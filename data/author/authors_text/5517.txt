c? 2002 Association for Computational Linguistics
Toward an Aposynthesis of Topic
Continuity and Intrasentential Anaphora
Eleni Miltsakaki?
University of Pennsylvania
The problem of proposing referents for anaphoric expressions has been extensively researched in
the literature and significant insights have been gained through the various approaches. However,
no single model is capable of handling all the cases. We argue that this is due to a failure of the
models to identify two distinct processes. Drawing on current insights and empirical data from
various languages we propose an aposynthetic1 model of discourse in which topic continuity,
computed across units, and focusing preferences internal to these units are subject to different
mechanisms. The observed focusing preferences across the units (i.e., intersententially) are best
modeled structurally, along the lines suggested in centering theory. The focusing mechanism
within the unit is subject to preferences projected by the semantics of the verbs and the connectives
in the unit as suggested in semantic/pragmatic focusing accounts. We show that this distinction
not only overcomes important problems in anaphora resolution but also reconciles seemingly
contradictory experimental results reported in the literature. We specify a model of anaphora
resolution that interleaves the two mechanisms. We test the central hypotheses of the proposed
model with an experimental study in English and a corpus-based study in Greek.
1. The Problem
Extensive research reported in the anaphora resolution literature has focused on the
problem of proposing referents for pronominals.2 First, centering, formulated as a
model of the relationship between attentional state and form of referring expressions,
was utilized as the basis of an algorithm for binding pronominals on the intersen-
tential level (Brennan, Walker-Friedman, and Pollard 1987). The proposed algorithm
(henceforth the BFP algorithm) gives the correct interpretation for the pronominal he
in example (1), stating a preference to resolve the pronominal to Max rather than Fred.
(1) a. Max is waiting for Fred.
b. He invited him for dinner.
It was soon observed, however, that the BFP algorithm was not capable of handling
cases of intrasentential anaphora such as in (2) (adapted from Suri, McCoy, and De-
Cristofaro [1999]).
? Institute of Research in Cognitive Science, Philadelphia, PA 19104. E-mail: elenimi@unagi.cis.upenn.edu
1 ?Aposynthesis? is a Greek word that means ?decomposition,? that is, pulling apart the components
that constitute what appears to be a uniform entity.
2 Although a significant amount of research in anaphora resolution has been carried out in statistical
approaches, reviewing such approaches is well beyond the scope of the current article.
320
Computational Linguistics Volume 28, Number 3
(2) a. Dodge was robbed by an ex-convict.
b. The ex-convict tied him up
c. because he wasn?t cooperating.
d. Then he took all the money and ran.
The centering-based BFP algorithm would have a preference to resolve he in (2d) to
Dodge and not to the ex-convict, based on a preference for a Continue transition.
Alternative approaches to anaphora resolution have sought to account for the re-
solution facts by proposing a semantic/pragmatic rather than structural mechanism.
Stevenson et al (2000) argue that both verbs and connectives have focusing proper-
ties affecting the preferred interpretation of pronominals. So in (3), the verb focusing
highlights Bill, since Bill is the person associated with the endpoint of the event of crit-
icizing. The connective, so, directs attention to the consequences and hence reinforces
the focus on Bill.
(3) a. John criticized Bill,
b. so he tried to correct the fault.
The semantic/pragmatic focusing account runs into the type of problem demon-
strated in (4), where the preferred interpretation for he is John, that is, the structural
subject, independent of semantic/pragmatic factors.3 In such discourses it seems that
a structural account is at play (in the sense of Grosz and Sidner [1986]).
(4) a. John criticized Bill.
b. Next, he insulted Susan.
This article sets out to explicate the behavior of pronominals demonstrated in the
above examples. Gaining significant insights from current research in anaphora reso-
lution, we reconcile what seem to be contradictory findings in a model according to
which inter- and intrasentential anaphora are not subject to the same mechanism. We
argue that the shortcomings of the proposed algorithms are due to confounding two
distinct processes, namely, topic continuity and the internal structure of the sentence.4
We conclude that intersentential anaphora is subject to structural constraints, whereas
intrasentential anaphora is subject to grammatical as well as semantic/pragmatic con-
straints. We define the notion of discourse unit and propose a two-level approach to
anaphora resolution. Within the unit, anaphora resolution is performed locally and is
constrained by the grammar and semantic properties of the predicates and the subor-
dinate conjunctions. This process outputs unresolved anaphoric expressions for which
potential referents are picked from a centering-style ranked list of entities constructed
in the previous unit.
3 Experimental results regarding these cases are reported in Stevenson et al (2000).
4 We use the term ?topic? to describe a centered entity, that is, the entity that the discourse is ?about.?
The notion of a centered entity is a discourse construct distinct from ?topic? or ?theme? as defined in
information structure. Elsewhere we have introduced the term ?attention structure in discourse? to
describe mechanisms, linguistic or nonlinguistic, that language users employ to navigate the hearer?s
attention in discourse. Topic continuity is derivative of attention structure in discourse. In this article,
however, we have opted for the more transparent term ?topic continuity,? as it describes the
phenomenon we are mostly concerned with in pretheoretical terms.
321
Miltsakaki Topic Continuity and Intrasentential Anaphora
The article is organized as follows. In Section 2 we give a brief overview of
centering-based models of anaphora resolution, discuss their shortcomings, and con-
trast them with the semantic-focusing account suggested in Stevenson et al (2000). In
Section 3, we present the discourse model we adopt and the specifications we propose
for anaphora resolution across and within centering update units. In Section 4, we test
the central hypotheses of the proposed model in two studies: an experimental study
in English and a corpus-based study in Greek. We conclude with a general discussion
in Section 5.
2. Issues and Insights in Anaphora Resolution
2.1 The BFP Algorithm
Brennan, Walker-Friedman, and Pollard (1987) were the first to use the centering model
as the basis for an anaphora resolution algorithm. The centering model (Grosz and
Sidner 1986; Grosz, Joshi, and Weinstein 1983) makes the following assumptions:
1. A discourse segment consists of a sequence of utterances, U1 , . . . , Un.
2. For each utterance, a ranked list of evoked discourse entities is
constructed, designated as the Cf list.
3. The highest-ranked element of the Cf list is called the preferred center
(Cp).
4. The highest-ranked entity in the Cf list of Ui?1 realized in Ui is the
backward-looking center (Cb).
There are several types of topic transitions from one utterance to the next depending
on whether the Cb is retained over two consecutive utterances Un?1 and Un and
whether this Cb is also the Cp of Un (see Table 1). The distinction between a Smooth
Shift and a Rough Shift is due to Brennan, Walker-Friedman, and Pollard (1987), who
observed that the centering model generates ambiguity in cases such as (5):
(5) a. Brennan drives an Alfa Romeo.
b. She drives too fast.
c. Friedman races her on weekends.
d. She often beats her.
Adding weight to the status of the Cp in (5c) makes it possible to resolve the pronom-
inal she in (5d) successfully to Friedman. We will return to the issue of ambiguity
shortly.
The BFP algorithm consists of three basic steps:
1. Generate possible Cb-Cf combinations.
2. Filter by constraints (e.g., contra-indexing, sortal predicates, centering
rules and constraints).
3. Rank by transition orderings (Continue > Retain > Smooth Shift >
Rough Shift).
322
Computational Linguistics Volume 28, Number 3
Table 1
Centering transitions.
Cb(Ui) = Cb(Ui?1 ) Cb(Ui) = Cb(Ui?1 )
Cb(Ui) = Cp(Ui) Continue Smooth Shift
Cb(Ui) = Cp(Ui) Retain Rough Shift
Some of the shortcomings of the BFP algorithm are discussed by Prasad and Strube
(2000), who observe that it makes two strategic errors. Prasad and Strube?s observa-
tions are made with respect to Hindi but hold in English and Modern Greek, as shown
in (6) and (7), respectively.
The first of these errors occurs in cases in which Cb(Ui?1 ) is different from
Cp(Ui?1 ). In such cases, the preference for a Continue transition is responsible for
the pronominal in Ui being resolved to the Cb(Ui?1 ) and not to the Cp(Ui?1 ).
(6) a. Elleni saw Maryj at school.
b. Maryj didn?t talk to heri.
c. Shej took herj friends and walked away.
(7) a. I Elenii ide ti Mariaj sto sholio.
the Eleni saw the Maria at-the school.
Elenii saw Mariaj at school.
b. I Mariaj den tisi milise.
the Maria not to-her talked.
Mariaj didn?t talk to heri.
c. NULLj pire tis files tisj ki NULLj efige.
NULL took the friends her and NULL left.
Shej took herj friends and left.
There is an important observation to be made here, which we present as the first
indication for the distinction between topic continuity and anaphora resolution. On
the one hand, the BFP centering-based algorithm makes a resolution error opting for
a Continue transition in (6c) and (7c). On the other hand, anaphora aside, the topic
transition identified by centering is intuitively correct. In (6) and (7), the discourse is
initiated with Ellen as the current topic, Maria is introduced as an entity related to the
current topic, and then the discourse shifts to Maria to elaborate on her doings. The
shift is in fact anticipated by the promotion of Maria from the object position in (6a)
and (7a) to the subject position in (6b) and (7b).
The second error observed by Prasad and Strube (2000) is that the BFP algorithm
generates ambiguity when Ui?1 is discourse initial. Example (8) is given as illustration.
(8) a. John gave a lot of his property to George.
b. His current salary exceeded the average salary by a lot.
Given that the Cb in the discourse initial (8a) is unspecified, Continue transitions are
generated when resolving his to either John or George. At this point, the BFP algorithm
is not capable of reaching a decision.
323
Miltsakaki Topic Continuity and Intrasentential Anaphora
The solution we propose for the two problems is simple: the preferred antecedent
for the pronominal in Ui is the highest-ranked entity in Ui?1 that is compatible with
the anaphoric expression. Compatibility is defined in terms of agreement features
(number and gender in the case of English). The proposed solution is consistent with
the centering model. The most relevant centering notion for anaphora resolution is
the pronoun rule, which stipulates that if an entity is realized as a pronoun, then so
is the Cb. Opting for resolution to the highest-ranked entity in the previous entity is
precisely supported by the pronoun rule because the highest-ranked entity realized
in the following utterance is the Cb. On the other hand, using centering transitions
for anaphora resolution does not necessarily follow from the original formulation of
centering. Centering transitions, as originally formulated and as confirmed by the data
discussed above, are best at identifying degrees of topic continuity. There is no a pri-
ori reason to expect that they will perform equally well in identifying pronominal
referents. This is because assuming maximal coherence (preference for Continue tran-
sitions) overlooks properties of attention structure in discourse: strategies that hearers
use to signal attention shifts to new centers while maintaining coherence. A Smooth
Shift may be intended and signaled appropriately by, for example, promoting a proper
name from object to subject position. Interpreting pronominals in accordance with the
pronoun rule as suggested here exploits precisely such strategies.
We conclude from this section that although centering transitions successfully
identify topic continuity in discourse, in anaphora resolution the most useful centering
notion is not the transitions themselves but the Cf list ranking in combination with
the pronoun rule.
2.2 Functional Centering
Strube and Hahn (1996, 1999) elaborate on the nature of the Cf list and propose a
centering-based model of anaphora resolution in which the Cf ranking is based not
on grammatical function but on functional information status. They recast centering
notions in terms of Danes??s (1974) trichotomy between given information, theme, and
new information. The Cb(Ui), the most highly ranked element of Cf(Ui?1 ) realized in
Ui, corresponds to the element that represents given information. The Cp(Ui) corre-
sponds to the theme of Ui. The rhematic elements of Ui are the ones not contained
in Ui?1 . Although the original motivation for the functional recast of centering was
due to German, a free-word-order language, Strube and Hahn (1996) claim that the
functional framework is superior because fixed- and free-word-order languages can
be accounted for using the same principles. They argue against Walker, Iida, and Cote
(1994), in which the Cf ranking is viewed as a language-specific parameter that needs
to be set.
In what follows we will remain agnostic to the suitability of the functional center-
ing framework for German. We will show, however, that functional centering is not
the appropriate framework for all free-word-order languages, much less for languages
universally. We bring in evidence from Modern Greek, a free-word-order language.
To identify the factors determining Cf ranking in Greek, we employ Rambow?s
(1993) diagnostic5 to test whether surface word order or grammatical function is the
most reliable indicator of salience. The relevant examples for the Greek version of
5 Rambow suggests that the order of entities in the position between finite and nonfinite verbs in
German (Mittelfeld) affects their salience. Gender in German is grammaticized, so Rambow constructs
an example with two same-gender entities in Mittelfeld and uses an ambiguous pronoun in subsequent
discourse to determine which of the two entities is more salient. The constructed example is given
below. ?Fem? indicates that the noun phrases are gender marked ?feminine.?
324
Computational Linguistics Volume 28, Number 3
Rambow?s diagnostic are shown in (9) and (10). The null pronominal in (9b) and (10b)
resolves to the subject irrespective of its surface position.6 The relevant indicator of
salience in the Cf list appears to be grammatical function, at least subjecthood.7
(9) a. I prosfati diefthetisii veltioni tin ikonomiki politikij?
the recent arrangement improve the economic policy?
Does the recent arrangementi improve the economic policyj?
b. Ohi, (nulli) ine aneparkis.
No, (it) is inadequate.
No, iti is inadequate.
(10) a. Tin ikonomiki politikij tij veltioni i prosfati diefthetisii?
the economic policy it-(clitic) improve the recent arrangement?
Does the recent arrangementi improve the economic policyj?
b. Ohi, (nulli) ine aneparkis.
No, (it) is inadequate.
No, iti is inadequate.
Further evidence for the role of grammatical function in Greek comes from syntac-
tic objects.8 In Greek (and also in Turkish), a strong pronominal or a full noun phrase
(NP) must be used to promote the object of Ui?1 to the subject position of Ui.9 As the
infelicitous interpretations (indicated by the pound sign) show in (11b), reference to
the object Yorgo becomes felicitous only with the use of name repetition or a strong
(1) a. Glauben Sie, dass [eine solche Massnahme]i [der russischen Wirtshaft]j helfen kann?
think you that a such measure-Fem the Russian economy-Fem help can?
Do you think that such a measure can help the Russian economy?
b. Nein, siei ist viel zu primitiv.
no, she is much too primitive.
No, it?s much too primitive.
6 Gender and lexical considerations are controlled. Both economical policy and arrangement are feminine
and they can both be inadequate. Also, we have presented the diagnostic test and confirmed the
judgment with a sizable group of native speakers of Greek attending the 15th International Symposium
on Theoretical and Applied Linguistics (Miltsakaki 2001).
7 It is interesting that in Turkish, another free-word-order language, it has also been shown (Turan 1998)
that the strongest indicator of salience is subjecthood.
8 Greek has two pronominal systems: weak pronouns that must cliticize to the verb and strong pronouns
that are syntactically similar to full NPs. Dropped subjects are considered part of the system of weak
pronouns. In Miltsakaki (2000), we argue that speakers of various languages use available nominal and
pronominal forms and prosodic features in spoken language to signal attention structure in discourse.
Greek speakers with a three-way distinction in their nominal system (i.e., full noun phrases and weak
and strong pronominals) use strong pronominals to signal reference to an entity previously evoked in
discourse that is not, however, the most salient entity. This use of strong pronominals is equivalent to
certain prosodic effects in English. As noted by an anonymous reviewer, there is extensive literature on
the effects of prosody in pronominal interpretation. For example, it has been observed that prominent
stress on the pronominals in (1) yields cospecification of he with Bill and him with John.
(1) John criticized Bill. Then, HE criticized HIM.
The need to recruit special prosody to achieve resolution to Bill indicates that structural focusing is
indeed at work projecting strong ?default? focusing preferences. In (1), there is sufficient semantic
information to help the hearer arrive at the intended interpretation. If there was no default
interpretation available at hand there would be no need to evoke prosodic effects. Once the linguistic
encoding of speakers? strategies for building attention structure in discourse are identified,
incorporating them in the centering framework should be trivial.
9 A ?full NP? is any noun phrase that contains a head noun, either common or proper.
325
Miltsakaki Topic Continuity and Intrasentential Anaphora
pronominal, shown in (11c) and (11d).10 We take this as further evidence that objects
rank lower than subjects in Greek.
(11) O Yannisi proskalese ton Yorgoj.
the John invited the Yorgo.
Johni invited Georgej.
a. nulli tuj prosfere ena poto.
he him offered a drink.
Hei offered himj a drink.
b. #null j #tu i prosfere ena poto.
he him offered a drink.
Hej offered himi a drink.
c. O Yorgosi tui prosfere ena poto.
the George him offered a drink.
George offered himi a drink.
d. Ekinosj tui prosfere ena poto.
he-strong him offered a drink.
HEj offered himi a drink.
Finally, to test the current results against the functional centering alternative, we
replace the definite subject in (9) with an indefinite noun phrase. As shown in (12),
the subject is an indefinite noun phrase representing new (or hearer-new) information
and the object is a definite phrase, encoding old (or hearer-old) information. The null
pronominal in (12b) resolves to the subject of (12a), and the information status of the
potential antecedents is disregarded.
(12) a. Mia kainourgia diefthetisii tha veltiosi tin ikonomiki politikij?
a new arrangement will improve the economic policy?
Will a new arrangementi improve the economic policyj?
b. Ohi, (nulli) tha ine aneparkis.
No, (it) will be inadequate.
No, it will be inadequate.
That the information status is not the relevant factor in discourse salience, at least
not cross-linguistically, is also confirmed in Turan (1998) for Turkish and in Prasad and
Strube (2000) for Hindi. In both of these languages, the relevant factor for the ranking
of elements in the Cf list is grammatical function.
We conclude that information status (or hearer status) is not universally the most
important factor determining discourse salience (in Cf ranking). Given the facts of
pronominalization, we maintain that, at least for English, Greek, Hindi, and Turkish,
grammatical function can most reliably determine the relative salience of entities.
10 Empirical evidence for the use of strong pronominals to signal reference to nonsalient entities in Greek
is provided in Dimitriadis (1996). Further functions of strong pronominals in Greek are identified in
Miltsakaki (1999, 2001).
326
Computational Linguistics Volume 28, Number 3
2.3 The S-list Algorithm
A further modification of the centering model is proposed by Strube (1998), who re-
places the functions of the backward-looking center and the centering transitions with
the ordering among elements of what he calls the S-list, that is, the list of salient
discourse entities. The S-list ranking criteria define a preference for hearer-old over
hearer-new discourse entities and are intended to reflect the attentional state of the
hearer at any given point in discourse processing. The S-list is generated incremen-
tally and is updated every time an anaphoric element is resolved. Anaphoric elements
are resolved with a lookup in the S-list. The elements of the S-list are looked up for
compatibility in the order determined by their information status ranking (hearer-old
entities are looked up before hearer-new entities). When the analysis of the utterance
is finished (processed left to right), the discourse entities that are not realized in the
utterance are removed. Strube (1998) claims that the incremental generation and pro-
cessing of the S-list enables his system to handle inter- and intrasentential anaphora
without any further specifications.
Although the S-list has the merit of avoiding ambiguities caused by the way the
Cb and the centering transitions interact, it is not capable of handling intrasentential
anaphora without any further specifications, as claimed in Strube (1998). Stevenson et
al. (2000) report experimental results pointing out cases in which focus preferences are
projected by verbs and connectives. Neither a grammatical function ordering nor an
information-based ordering is adequate to handle such cases. To illustrate the point,
we quote an example, shown in (13), from Stevenson et al (2000). We construct the S-
list ranking the elements according to grammatical function (information status would
not distinguish between the two proper names).11
(13) a. Keni admired Geoffj because hej won the prize.
b. Geoffj impressed Keni because hej won the prize.
In both (13a) and (13b) the pronominal resolves to Geoff, the verb argument with
the stimulus role. The ordering in the S-list in (13a), however, is Ken > Geoff, so the
S-list algorithm will resolve the subsequent pronominal to the higher-ranked element
at the time of processing, in this case, Ken. In fairness to the S-list algorithm, this is
a problem for any centering-based algorithm that attempts to handle intrasentential
anaphora according to a fixed ranking of entities in a salience list.12
Apparently, certain discourse algorithms relying on a fixed ordering of potential
antecedents are not capable of resolving anaphora successfully. In sections 4 and 5, we
argue that such cases are most commonly identified intrasententially.
2.4 Revised Algorithms for Focus Tracking/Revised Algorithms for Pronoun
Resolution
Based on previous work (Suri and McCoy 1994), Suri, McCoy, and DeCristofaro (1999)
propose a methodology of extending their Revised Algorithms for Focus Tracking/
Revised Algorithms for Pronoun Resolution (RAFT/RAPR) to handle focusing prop-
erties of complex sentences. To determine how their framework should be extended
to handle complex sentences, they develop a methodology specifically designed to
11 This strategy was also adopted by Prasad and Strube (2000) in the implementation of the S-list
algorithm for Hindi.
12 It is conceivable that a discourse can be constructed in which the semantics will force a similar pattern
of resolution intersententially. Hudson-D?Zmura and Tanenhaus (1998), however, report experimental
results that show that in such cases, sentence processing is slowed down.
327
Miltsakaki Topic Continuity and Intrasentential Anaphora
determine how people process complex sentences. The central question they pose is
whether a complex sentence should be processed as a multiple sentence or as a single
sentence. They specifically investigated the ?SX because SY? type of complex sentence
as well as its interaction with the sentences occurring in the immediately previous and
subsequent discourse.
(14) (S1) Dodge was robbed by an ex-convict the other night.
(15) (S2) The ex-convict tied him up because he wasn?t cooperating.
(16) (S3) Then he took all the money and ran.
Suri, McCoy, and DeCristofaro?s findings indicate that the pronoun resolution facts
within S2 are consistent with the expectations of both centering and RAFT/RAPR. On
completing the processing of the SY clause, however, the most salient entity for the
following discourse is not picked from SY. Based on these findings, they propose the
prefer-SX hypothesis to extend RAFT/RAPR.
Although the prefer-SX hypothesis repairs the algorithm with respect to the con-
struction in question, it seems to be missing a generalization regarding inconsistencies
observed within versus across sentences. We return to this issue in Section 4.
2.5 Stevenson et al?s Semantic/Pragmatic Focusing
Stevenson et al (2000) investigate the interaction between structural, thematic, and re-
lational preferences in interpreting pronouns and connectives in discourse. Stevenson,
Crawley, and Kleinman (1994) have argued that the crucial factors underlying fo-
cusing mechanisms in discourse are semantic/pragmatic factors. Semantic/pragmatic
focusing assumes that verbs and connectives project their own focusing preferences.
Verbs project focus preferences to the entities associated with the endpoint or con-
sequence of the described event. The focusing preferences of the connective depend
on its meaning. For example, connectives like because direct attention to the cause of
the previously described event, and connectives like so direct attention to the conse-
quences of the event. Thus in a sentence like (17), the verb projects a focus preference
for Bill, because Bill is the person associated with the endpoint of the event of criti-
cizing. The connective, so, directs attention to the consequences, reinforcing the focus
on Bill, which is then picked as the most preferred antecedent for the interpretation
of the subsequent pronominal.
(17) John criticized Bill so he tried to correct the fault.
By way of demonstration, let us turn our attention to action and state verbs.
The semantic/pragmatic focusing account predicts that sentences with action verbs
focus on the entity associated with the endpoint of the event, namely, the patient,
independent of its structural position. This focus is maintained when the connective
is so. In one of Stevenson et al?s (2000) experiments, it is shown that in cases such
as (18a) the pronominal he picks the patient as its referent both when it is introduced
in the previous clause as a subject and when it is introduced there as an object, as
in (18b).
(18) a. Patricki was hit by Josephj so hei cried.
b. Josephj hit Patricki so hei cried.
328
Computational Linguistics Volume 28, Number 3
A similar pattern is observed with state verbs, shown in (19), where he in the con-
tinuation is interpreted as the experiencer of the event independent of its structural
position.
(19) a. Keni admired Geoffj so hei gave him the prize.
b. Keni impressed Geoffj so hej gave him the prize.
So the experimental evidence supports Stevenson et al?s view that the focusing prop-
erties of verbs affect the interpretation of pronominals.
Hudson-D?Zmura and Tanenhaus (1998), however, report experimental results
that, at first blush, contradict this view. They conducted a similar experiment to test
whether subject-object or stimulus-experiencer is the crucial distinction for pronom-
inal interpretation. Subjects were given sentence (20) followed by the continuations
(20a)?(20b) and were asked to judge the continuations for naturalness.
(20) Max despises Ross.
a. He always gives Ross a hard time.
b. He always gives Max a hard time.
Hudson-D?Zmura and Tanenhaus?s results show that there is a strong preference for
the subject interpretation independent of the thematic role.
What are we to conclude from these inconsistent results? The results show that the
same type of verb (i.e., a state verb) in some cases projects its own focus preference
(e.g., experiencer), but in other cases it does not. In order to account for the facts, one
option would be to continue stretching structural focusing to account for the facts.
Another option would be to continue stretching semantic focusing. In the following
section, we propose an aposynthetic model for anaphora resolution in which we divide
the labor of anaphoric interpretation between the two mechanisms and define the
domains of their applicability.
3. The Proposal: Aposynthesis
3.1 Outline of the Discourse Model
We assume that the discourse is organized hierarchically in linear and embedded
segments as specified in Grosz and Sidner (1986). We also adopt the centering view
of local-discourse coherence to model topic continuity in discourse. According to the
centering model each segment consists of a sequence of utterances. The size of an utter-
ance, however, is left unspecified. Because transitions are computed for each utterance,
we will rename utterances as centering update units and argue that a centering update
unit consists of a matrix clause and all the dependent clauses associated with it. For
each update unit a list of forward-looking centers is constructed and ranked according
to the salience of each. Consistent with the proposed definition of unit, we argue that
entities evoked in subordinate clauses are of lower salience than entities evoked in the
matrix clause and are ranked accordingly. The proposed centering specifications have
the following corollaries:
1. The linear order of subordinate clauses relative to the matrix clause does
not affect the salience status of the entities.
329
Miltsakaki Topic Continuity and Intrasentential Anaphora
2. Entities evoked in subordinate clauses are available as potential links
between the current and previous or subsequent discourse.
3. Topic shifts must be established in matrix clauses.
4. Backward anaphora in subordinate clauses is no longer ?backward,? as
anaphors in subordinate clauses are processed before main clauses
independent of their linear order.
Finally, we assume that anaphora across units obeys centering?s pronoun rule. How-
ever, we do not adopt the BFP algorithm for anaphora resolution across units. Instead,
as suggested in Section 2.1, the preferred antecedent for a pronominal in Ui is the
highest-ranked entity in Ui?1 modulo agreement features.
The remainder of this section is organized as follows. First, we briefly review
Kameyama?s tensed adjunct hypothesis, which states that subordinate clauses are in-
dependent processing units and argue that on the basis of new empirical evidence the
hypothesis cannot be maintained. Next, we present evidence in support of a new def-
inition of the update unit. Data from English, Greek, and Japanese show that treating
subordinate clauses as independent units yields counterintuitive centering transitions
and violations of the pronoun rule.
3.1.1 The Centering Update Unit. Defining the update unit within the framework
of the centering model became central in very early work, because centering was
adopted and modified mainly to account for anaphora resolution. Given that anaphoric
elements occur in all types of clauses, it was crucial that the size of the update unit
be constrained to enable the handling of intrasentential anaphora. To a large extent,
efforts to identify the appropriate size for the unit were often dictated by needs specific
to anaphora resolution algorithms.
Centering was not originally formulated, however, as a model of anaphora resolu-
tion. For purposes of testing the suitability of the relevant unit in centering, it would
be desirable to derive a model that yields transitions that reflect our intuitions about
perceived discourse coherence, as well as the degree of the processing load required
by the hearer/reader at any given time in discourse processing. Reflecting degrees of
continuity is not a concern for anaphora resolution algorithms.
Kameyama (1993, 1998) was concerned with the problem of intrasentential cen-
tering and, in particular, the definition of the appropriate update unit when complex
sentences are processed. Kameyama suggested breaking up complex sentences accord-
ing to the following hypotheses:
1. Conjoined and adjoined tensed clauses form independent units.
2. Tenseless subordinate clauses, report complements, and relative clauses
belong to the update unit containing the matrix clause.
With regard to her tensed adjunct hypothesis, which treated tensed adjunct clauses (for
reasons of convenience, we will henceforth use the term ?subordinate? to refer to this
class of clauses) as independent units, Kameyama brings in support from backward
anaphora. She argues that the tensed adjunct hypothesis predicts that the pronoun in
the fronted subordinate clause in (21c), for example, is anaphorically dependent on
an entity already introduced in the immediate discourse and not on the subject of the
330
Computational Linguistics Volume 28, Number 3
main clause to which it is attached:
(21) a. Kerni began reading a lot about the history and philosophy of
Communism
b. but never 0i felt there was anything he as an individual could do
about it.
c. When hei attended the Christina Anti-Communist Crusade school here
about six months ago
d. Jimi became convinced that an individual can do something
constructive in the ideological battle
e. and 0i set out to do it.
This view of backward anaphora, in fact, was strongly professed by Kuno (1972),
who asserted that there was no genuine backward anaphora: the referent of an ap-
parent cataphoric pronoun must appear in the previous discourse. Kameyama?s (also
Kuno?s) argument is weak in two respects. First, it is not empirically tested that in
cases of backward anaphora the antecedent is found in the immediate discourse.
Carden (1982) and van Hoek (1997) provide empirical evidence of pronouns that
are the first mention of their referent in discourse. Most recently, Tanaka (2000) re-
ported that in the cataphora data retrieved from the Anaphoric Treebank, out of
133 total occurrences of personal pronouns encoded as ?cataphoric,? 47 (35.3%) were
?first mentioned.? Among the 47 cases of first-mention cataphora, 6 instances were
discourse initial.13
Secondly, this account leaves the use of a full NP in Kameyama?s main clause
(21d) unexplained (Kern and Jim have the same referent). Full NPs and proper names
occurring in Continue transitions have been observed to signify a segment bound-
ary. Assuming that segment boundaries do not occur between a main clause and
a subordinate clause associated with it, the use of a full NP in (21d) remains
puzzling.
Empirical evidence in support of Kameyama?s hypothesis that tensed subordinate
clauses should be treated as independent processing units was brought forth by Di
Eugenio (1990, 1998), who carried out centering studies in Italian. Di Eugenio (1990)
proposed that the alternation of null and overt pronominal subjects in Italian could be
explained in terms of centering transitions. Typically, a null subject signals a Continue
and a strong pronoun a Retain or a Shift.14
Following Kameyama (1993), Di Eugenio treats subordinate clauses as indepen-
dent update units. Her motivation for doing so comes from the following example,
in which the use of a strong pronoun in the main clause cannot be explained if the
preceding adjunct is not treated as an independent update unit. The translation (taken
from Di Eugenio [1998]) is literal but not word for word. For the utterance preceding
(22) the Cb(Ui?1 ) = vicinaj (neighbor-fem) and Cf(Ui?1 ) = vicinaj.
13 The Anaphoric Treebank is a corpus of a collection of news reports, annotated with, among other
things, type of anaphoric relations. It was developed by UCREL (Unit for Computer Research on the
English Language) at Lancaster University, collaborating with the IBM T.J. Watson Research Center.
14 Di Eugenio collapsed the distinction between Smooth and Rough Shifts. The reader is referred,
however, to Miltsakaki and Kukich (2000a, 2000b) for a discussion of the significance of Rough Shifts in
the evaluation of text coherence.
331
Miltsakaki Topic Continuity and Intrasentential Anaphora
(22) a. Prima che i pigronii siano seduti a tavola a far colazione,
Before the lazy onesi sit down to have breakfast,
b. leij e via col suoj calessino alle altre cascine della tenuta.
shej has left with herj buggy for the other farmhouses on the
property.
In Miltsakaki (2001), we report the results of a centering study in Greek. One
of the surprising findings in this study was that a few strong pronouns appeared
in Continue transitions. The result was surprising because the overall distribution of
nominal and pronominal forms revealed that weak pronouns were most common in
Continue transitions, whereas strong pronouns, full noun phrases, and proper nouns
were associated with Rough Shift transitions. On closer inspection, we observed that in
six out of the eight instances of strong pronouns in Continue transitions, the referent of
the strong pronoun was contrasted on some property with some other entity belonging
to a previously evoked set of entities.15 Although the sample is too small to draw any
definitive conclusions, we can at least entertain the hypothesis that strong pronouns
in Italian serve a similar function. If this is true, then an alternative explanation is
available for Di Eugenio?s data: in (22b), she, the most salient entity in the current
discourse, is contrasted with the lazy ones, in (22a), on the property of ?laziness.? It
turns out that the hypothesis that the strong pronoun does not signal a Rough Shift
transition is confirmed by the preceding discourse, in which the ?vicina? appears as the
most salient entity, realized with multiple dropped subjects. The discourse immediately
preceding (22) is shown in (23).16
(23) a. NULLj e? una donna non solo graziosa ma anche energica e dotata di
spirito pratico;
and not only is shej pretty but also energetic and endowed with a
pragmatic spirit;
b. NULLj e la combinazione di tutto cio? e?, a dir poco, efficace.
and the combination of all these qualities is effective, to say the least.
c. NULLj si alza all?alba per sovrintendere a che si dia da mangiare alle
bestie, si faccia il burro, si mandi via il latte che deve essere venduto;
una quantita? di cose fatte mentre il piu? della gente se la dorme della
grossa.
Shej gets up at dawn to supervise that the cows are fed, that the
butter is made, that the milk to be sold is sent away; a lot of things
done while most people sleep soundly.
We now turn to English and Greek to show that treating subordinate clauses as
independent centering units yields counterintuitive topic transitions. First, consider
the constructed example from English shown in (24).
15 We ignored one further instance of a strong pronominal in a Continue transition, as in that case the
strong pronominal headed a relative clause and its use was forced by the grammar.
16 Many thanks to Barbara Di Eugenio (personal communication) for providing me with the extra data in
(23). I presume that Di Eugenio?s coding of the null realization in (23b) is based on the inferable
information that the noun phrase ?la combinazione di tutto cio?? refers to herj qualities.
332
Computational Linguistics Volume 28, Number 3
(24) Sequence:
main-subordinate-main
a. John had a terrible headache.
Cb = ?
Cf = John > headache
Transition = none
b. When the meeting was over,
Cb = none
Cf = meeting
Transition = Rough Shift
c. he rushed to the pharmacy
store.
Cb = none
Cf = John
Transition = Rough Shift
(25) Sequence:
main-main-subordinate
a. John had a terrible headache.
Cb = ?
Cf = John > headache
Transition = none
b. He rushed to the pharmacy
store
Cb = John
Cf = John > pharmacy store
Transition = Continue
c. when the meeting was over.
Cb = none
Cf = meeting
Transition = Rough Shift
Allowing the subordinate clause to function as a single update unit yields a sequence
of two Rough Shifts, which is diagnostic of a highly discontinuous discourse. Fur-
ther, if indeed there are two Rough Shift transitions in this discourse, the use of the
pronominal in the third unit is puzzling. A sequence of two Rough Shift transitions
in this short discourse is counterintuitive and unexpected given that of all centering
transitions, Rough Shifts in particular have been shown to (a) disfavor pronominal
reference (Walker, Iida, and Cote 1994; Di Eugenio 1998; and Miltsakaki 1999, among
others), (b) be rare in corpora, to the extent that the transition has been ignored by
some researchers (Di Eugenio 1998 and Hurewitz 1998, among others), and (c) be re-
liable measures of low coherence in student essays (Miltsakaki and Kukich 2000a). In
addition, simply reversing the order of the clauses, shown in (25), causes an unex-
pected improvement, with one Rough Shift transition being replaced with a Continue.
Assuming that the two discourses demonstrate a similar degree of continuity in the
topic structure (they are both about ?John?), we would expect the transitions to reflect
this similarity when, in fact, they do not.
Presumably, the introduction of a new discourse entity, ?meeting,? in the time-
clause does not interfere with discourse continuity, nor does it project a preference
for a shift of topic, as the Cp normally does when it instantiates an entity different
from the current Cb. Notice that if we process the subordinate clause in the same unit
as the relevant main clause, we compute a Continue transition independent of the
linear position of the subordinate clause, as the entities introduced in the main clause
rank higher than the entities introduced in the subordinate clause. The computation
is shown in (26).
(26) a. John had a terrible headache.
Cb = ?
Cf = John > headache
Transition = none
b. When the meeting was over, he rushed to the pharmacy store.
Cb = John
Cf = John > pharmacy store > meeting
Transition = Continue
333
Miltsakaki Topic Continuity and Intrasentential Anaphora
Similar examples were identified in data collected from a short story in Greek
(Miltsakaki 2001, 1999). Example (27), shown below, is indicative.
(27) a. Ki epeza me tis bukles mu.
and I-was-playing with the curls my
And I was playing with my hair.
Cb = I, Cp = I, Tr = Continue
b. Eno ekini pethenan apo to krio,
while they were-dying from the cold
While they were dying from the cold,
Cb = none, Cp = THEY, Tr = Rough Shift
c. ego voltariza stin paralia,
I was-strolling on-the beach
I was strolling on the beach,
Cb = NONE, Cp = I, Tr = Rough Shift
d. ki i eforia pu esthanomun den ihe to teri tis
and the euphoria that I-was feeling not have the partner its
and the euphoria that I was feeling was unequaled.
Cb = I, Cp = EUPHORIA, Tr = Rough Shift
Again, processing the while clause in (27b) as an independent unit yields three Rough
Shift transitions in the subsequent discourse, reflecting a highly discontinuous dis-
course. When (27b) and (27c) are processed as a single unit, the resulting sequence of
transitions for the entire discourse is a much improved Continue-Continue-Retain.
Further evidence in support of the proposed definition of the update unit comes
from cross-linguistic observations on anaphora resolution. The most striking examples
come from Japanese.17 In Japanese, topics and subjects are lexically marked (wa and
ga, respectively), and null subjects are allowed. Note that subordinate clauses must
precede the main clause. Consider the Japanese discourse (28). Crucially, the referent
of the null subject in the second main clause resolves to the topic-marked subject of the
first main clause, ignoring the subject-marked subject of the intermediate subordinate
clause.
(28) a. Taroo wa tyotto okotteiru youdesu
Taroo TOP a-little upset look
Taroo looks a little upset.
b. Jiroo ga rippana osiro o tukutteiru node
Jiroo SUB great castle OBJ is-making because
Since Jiroo is making a great castle,
c. ZERO urayamasiino desu
ZERO jealous is
(he-Taroo) is jealous.
17 Thanks to Kimiko Nakanishi for providing me with the data. In a centering study she conducted in
Japanese, she also concluded that treating subordinate clauses as independent units would yield a
highly incoherent Japanese discourse.
334
Computational Linguistics Volume 28, Number 3
In section 2.4, a similar case was also identified in English. It is repeated here as
(29d). Again, the referent of he in (29d) is cospecified with ex-convict, the subject of the
previous main clause. If the because clause were processed independently, then the
most salient referent available for the interpretation of the anaphoric in (29d) should
be Dodge. Manipulating the semantics in the second main clause to make resolution to
Dodge the most plausible choice does not seem sufficient to warrant felicitous pronomi-
nalization, as has been shown experimentally in Suri, McCoy, and DeCristofaro (1999),
demonstrated here in (30). In (30), he is not the preferred form for reference to Dodge
despite the fact that Dodge is the most natural referent for the argument of the predicate
screaming for help in this context.
(29) a. Dodge was robbed by an ex-convict.
b. The ex-convict tied him up
c. because he wasn?t cooperating.
d. Then he took all the money and ran.
(30) a. Dodge was robbed by an ex-convict the other night.
b. The ex-convict tied him up because he wasn?t cooperating.
c. #Then he started screaming for help.
The low salience of subordinate clause entities is further confirmed in the experi-
mental results reported in Suri, McCoy, and DeCristofaro (1999). In their experiment,
participants judge that a natural way to refer to Dodge in (31c) is by name repetition.
(31) a. Dodge was robbed by an ex-convict the other night.
b. The ex-convict tied him up because he wasn?t cooperating.
c. Then Dodge started screaming for help.
Finally, defining the main clause and its associated subordinate clauses as a single
unit points to interesting new directions in understanding backward anaphora. With
the exception of a few modal contexts shown in (34),18 backward anaphora is most
commonly found in preposed subordinate clauses (32) and not in sequences of main
clauses (33). From the unit definition we propose, it follows that surface backward
anaphora is no longer ?backward? once the Cf list is constructed and ranked. The
referent of the pronoun in such cases appears lower in the Cf list ranking and, in
fact, looks backward for an antecedent, as any other normal pronoun would. To illus-
trate the point, the Cf list for (32) contains John > shower > he-referent. The pronoun
looks back for an antecedent, intrasententially, and resolves to the only compatible
antecedent available, John.
(32) As soon as he arrived, John jumped into the shower.
(33) #He arrived and John jumped into the shower.
(34) He-i couldn?t have imagined it at the time but John Smith-i turned out to
be elected President in less than three years.
18 Thanks to Ellen Prince for pointing out this example.
335
Miltsakaki Topic Continuity and Intrasentential Anaphora
3.1.2 Discourse Salience versus Information Structure. In the previous section, we
argued that the linear position of the subordinate clause does not affect topic continuity.
This position leads to another question: if the linear position of subordinate clauses
does not improve topic continuity, then what is the function of clause order variation?
Let us briefly turn our attention to surface word order within a single clause. It is
commonly assumed that for each language there is an underlying canonical order of
the basic constituents. In an SVO language like Greek, the canonical order of the verb
and its arguments is subject-verb-object. This, of course, is not always the attested
surface order. In syntactic theories, it is commonly assumed that surface word order
is derived by various movement operations. Some movement operations are dictated
by the syntax of each language and are necessary to yield grammatical sentences. It
is also common, however, especially in free-word-order languages, that movement
is syntactically optional and the surface word order is used to satisfy information-
packaging needs (for example, to arrange the information into old-new or ground-
focus or mark open propositions). Note that when this happens, it is only the surface
word order that is altered and not the basic relation of the arguments to the predicate.
To give an example from English, in (35) the internal argument of the verb (the object)
has been fronted, but its original relation to the verb has remained the same.
(35) Chocolate Mary hates.
Moving to the sentential level, we entertain the hypothesis that the same principle
dictates the position of the clauses relative to each other. Each dependent clause stands
in a specific relation to the main clause, and this relation is not altered by the order
in which the clause appears on the surface. In discourse grammars, this insight is
captured in the discourse Lexicalized Tree Adjoining Grammar (LTAG) treatment of
subordinate conjunctions. In discourse LTAGs, subordinate conjunctions are treated
as predicates, anchoring initial trees containing the main and the subordinate clause
as arguments. Each subordinate conjunction may anchor a family of trees to reflect
variations in the surface order of the substituted argument clauses, but the predicate-
argument relation remains the same (Webber and Joshi 1998; Webber et al, 1999a,
1999b).
The above discussion relates to the definition of the centering update unit in the
following way. The centering model keeps track of center continuations and center
shifts. In other words, it keeps track of discourse salience. If we dissociate salience
from information structure, the relevant unit for computing salience is at the sentence
level, which we can visualize as a horizontal level (see Figure 1). The relative order of
independent/dependent clauses is determined by information structuring, a process
possibly orthogonal to the computing of salience. Subordinate links are not relevant
to the salience mechanism. Salience is computed paratactically.
A natural consequence of this model is that one can introduce referents on the
vertical level without affecting the status of the salient entity on the horizontal level.
It follows that changes of topic must be established at the horizontal level. Such a
conception of the salience structure suggests that text processing is not strictly incre-
mental, as commonly assumed. Although it is possible that the Cf list is constructed
incrementally, the final ranking is determined only after the sentence is complete.
Admittedly, the distinction between discourse salience and information packaging
is hard to establish because of the inevitable overlap between information status and
salience: attention centers, for example, tend to be discourse old. Still, there are other
aspects of information packaging pertaining to clause order (e.g., temporal or logical
sequences, open proposition frames inherited from previous discourse) that do not
336
Computational Linguistics Volume 28, Number 3
Figure 1
Salience model.
necessarily relate to the salience of the participating entities. Although a great deal
of additional work is required to understand the precise nature of the interaction
between salience and information structure, we believe that we obtain a significant
gain in keeping the two processes distinct.
3.2 Outline of the Anaphora Resolution Model
In Sections 2 and 3, we discussed a number of challenging cases for anaphora res-
olution, including some puzzling experimental data. We raised the question of how
the data are to be reconciled. We are now able to offer an explanation. The basic
idea is that topic continuity and intrasentential anaphora are handled by two distinct
mechanisms. Topic continuity is computed across centering update units. Anaphoric
reference spanning across update units relates to topic continuity and is therefore de-
termined structurally in accordance with centering rules and constraints. Within the
unit, anaphora is constrained by focusing preferences projected by the matrix predi-
cate and the extended arguments of the predicate that can be locally realized through
subordination.
This basic outline is sufficient to explain (most of) the data we have seen so far. The
experiments reported in Stevenson et al (2000), which show a main effect of thematic
focusing, involve the interpretation of anaphoric expressions in subordinate clauses.
On the other hand, Hudson-D?Zmura and Tanenhaus?s (1998) experiments on similar
types of verbs show a main effect of structural focusing. The difference between the
two sets of experiments is that Hudson-D?Zmura and Tanenhaus?s (1998) experiments
involve sequences of main clauses, whereas in Stevenson et al (2000) the relevant
experiments involve subordinate clauses. Furthermore, Stevenson et al (2000) report
results on a different set of experiments showing a main effect of structural focusing,
and these are precisely the experiments containing sequences of main clauses. Further,
Suri, McCoy, and DeCristofaro?s (1999) ?SX because SY? construction indicates that
the referent appearing in the subordinate clause is not the preferred focus in the
subsequent discourse, whereas resolution to the subject of the main SX clause yields
the desired interpretation.
The remainder of this section is organized as follows. First, we provide definitions
for the basic tenets of the model we propose and describe the basic steps required for
combining the two mechanisms in a single anaphora resolution algorithm. Next, we
337
Miltsakaki Topic Continuity and Intrasentential Anaphora
discuss some remaining issues raised by the English connective so and certain types
of preposed subordinate clauses.
3.2.1 Algorithm and Model Specifications. Discourse consists of a sequence of seg-
ments. Each segment consists of a sequence of centering update units. A single center-
ing update unit consists of one main clause and all its associated dependent clauses.
Dependent clauses are of three types: sentential complements of verbs, relative clauses,
and subordinate clauses. Sentential complements of verbs and relative clauses are iden-
tified syntactically. Subordinate clauses are introduced with subordinate conjunctions.
To identify subordinate conjunctions, we apply the reversibility test: a tensed clause
is introduced by a subordinator when the clause it introduces can be preposed.19
For example, in (36), although is classified as a subordinator and the although clause
is classified as a subordinate clause because placing the although clause before the
main clause retains grammaticality. Conversely, however in (38) is not classified as a
subordinator, because preposing the clause with which it is associated yields ungram-
maticality.
(36) John traveled by air although he is afraid of flying.
(37) Although he is afraid of flying, John traveled by air.
(38) John traveled by air. However, he is afraid of flying.
(39) #However, he is afraid of flying. John traveled by air.
Update units are identified and numbered. For each identified update unit the list
of forward-looking centers is constructed and its members are ranked according to
the ranking rule for English. The ?M? prefix stands for ?main clause,? and the ?S?
prefix stands for ?subordinate clause.? The relevant ranking of the various types of
dependent clauses is currently left unspecified.
Ranking Rule for English
M-Subject > M-indirect object > M-direct object > M-other >
S1-subject > S1-indirect object > S1-direct object > S1-other >
S2-subject > . . .
For complex NPs, we assume left-to-right ranking of entities, as suggested in Walker
and Prince (1996).20
Given the above input for N units, Ui=1 ...N, the anaphora resolution algorithm
starts at the last identified unit. The basic steps are specified below. Some of the steps
require information that is obtainable by currently available natural language systems:
syntactic parsers, morphological analyzers, automated proper name identification, and
electronic lexical databases such as WordNet (to check animacy, for example, as would
be necessary for the ranking of entities in Greek). Others, such as understanding the
focusing preferences of verbs and connectives as well as identifying thematic roles
will, of course, await further research.
19 ?Reversibility? is identified as a characteristic of subordinate clauses in Quirk et al (1972).
20 Complex NPs are noun phrases containing multiple nouns, for example, John?s father.
338
Computational Linguistics Volume 28, Number 3
0. Start at the last identified unit Ui with i = N.
1. Identify pronominal expressions in the rightmost subordinate
clause.
2. Input antecedents from the Cf list.
3. Apply grammar-driven constraints (number and gender
agreement, contra-indexing, etc.) to reduce list of potential
antecedents.
4. Resolve from right to left to the first available antecedent inside
the subordinate clause. Output unresolved pronominals.
5. Using the Cf list resolve pronominals according to semantic fo-
cusing constraints. Output unresolved pronominals.
6. If there is another subordinate clause to process go to step 1.
7. Identify pronominals in the main clause. Apply grammar-driven
constraints (number and gender agreement, contra-indexing,
etc.) to reduce list of potential antecedents. Resolve from right
to left to the first available antecedent inside the current clause.
Output unresolved pronominals.
8. Input Cf list of potential antecedents from previous unit.
9. Apply grammar-driven constraints to reduce list of potential
antecedents.
10. Resolve pronominals starting from the leftmost to the highest-
ranked element of the list of available antecedents.
11. If an antecedent is found go to step 13.
12. If the list of potential antecedents is empty and there is a unit
to process go to step 8, else mark unknown.
13. If Ui is the first unit U1 terminate, else start processing Ui?1
and go to step 1.
By way of demonstration, we apply the algorithm to resolve the anaphoric ex-
pressions in discourse (2), repeated here in (40)?(42).
(40) Dodge was robbed by an ex-convict.
(41) The ex-convict tied him-3 up because he-2 wasn?t cooperating.
(42) Then he-1 took all the money and ran.
? Step 0 applies. Move to step 1.
? No subordinate clause is identified. Jump to step 7.
? Step 7 applies. The pronoun HE-1 is identified. There is no potential
antecedent in the current clause. Move to step 8.
? Step 8 applies. The Cf list from the previous unit contains
EX-CONVICT > DODGE.
? Step 9 applies. Grammar constraints do not reduce the list of potential
antecedents.
? Step 10 applies. HE-1 resolves to the EX-CONVICT.
? Step 13 applies. Move to step 1.
? Step 1 applies. The pronoun HE-2 is identified.
339
Miltsakaki Topic Continuity and Intrasentential Anaphora
? Step 2 applies. The Cf list is empty (it contains only the unresolved
pronoun HE-2).
? Steps 3 and 4 apply vacuously. There are no potential antecedents in the
current clause.
? Step 5 applies. The Cf list contains HIM-3 > EX-CONVICT because of
semantic focusing. HE-2 resolves to HIM-3.
? Step 7 applies. HIM-3 is identified. Grammar constraints apply and
contra-index EX-CONVICT with HIM-3.
? Step 8 applies. The Cf list from the previous unit contains
DODGE > EX-CONVICT.
? Step 9 applies. Grammar constraints do not reduce the list of potential
antecedents.
? Step 10 applies. HIM-3 resolves to DODGE.
? Steps 11?13 apply. The algorithm terminates.
3.3 Comparison with Related Algorithms
The crucial difference between our approach and related anaphora resolution algo-
rithms is in the treatment of subordinate clauses. Whereas steps 7?10 are similar to
other approaches that opt to resolve a pronoun to the highest-ranked element of the
Cf list of the previous clause, the resolution process described in steps 0?7 and the
Cf ranking assumptions described earlier are not. As indicated in the ranking rule
for English set forth in Section 3.2.1, (a) subordinate clauses are part of the same unit
containing the main clause with which they are associated, and (b) there is a single Cf
ranking list for both the main and the subordinate clauses. Because the entities in the
subordinate clauses rank lower than the entities in the main clause, the linear position
of the subordinate clause does not affect the resolution process. We have seen that this
?restoring? of a basic clause order results in virtually eliminating backward anaphora,
which in other approaches requires special treatment.21 Also, intrasentential anaphora
is preferred in the cases of anaphoric elements occurring in subordinate clauses but
not in main clauses (assuming grammatical filtering), again irrespective of their linear
order.
We will now demonstrate these differences with respect to Lappin and Leass?s
(1994) and Hobbs?s (1978) algorithms, which are conceptually the closest to our ap-
proach. Lappin and Leass?s RAP (Resolution of Anaphora Procedure) algorithm ap-
plies to the output of McCord?s (1990) Slot Grammar parser and utilizes measures of
salience derived from syntactic structure and a simple model of attentional state. Po-
tential anaphor antecedents receive a salience score on which they are evaluated. The
scoring system penalizes backward anaphora and rewards parallel syntactic positions
and intrasentential antecedents (sentence recency).
As we have already mentioned, backward anaphora need not receive any special
treatment in our approach. Lappin and Leass penalize cases of backward anaphora
severely, which seems to work well on empirical grounds, presumably because back-
ward anaphora is rather rare. In absence of an explicit method of identifying real
cases of backward anaphora, however, the system is likely to miss such cases. In our
21 Assuming that backward anaphora is restricted to subordinate clauses. Special treatment is required for
the but clauses discussed in section 3.1.1, example (34).
340
Computational Linguistics Volume 28, Number 3
approach, this is not a problem, because the Cf ranking of the processing unit im-
plicitly identifies all real cases of backward anaphora and converts them into forward
anaphora.
Further, some of the limitations of the system discussed by Lappin and Leass
involve cases of intersentential anaphora such as the following:
(43) a. This green indicator is lit when the controller is on.
b. It shows that the DC power supply voltages are at the correct levels.
The RAP algorithm resolves the pronoun it in (43b) to the controller in (43a). This is
because, in RAP, the subject of the main as well as the when clause in (43a) are of equal
salience. In this case, the controller wins because it is more recent. In our approach, it
would resolve to the highest-ranked entity of the previous unit, which in this case is
correctly identified as the green indicator. This is because the when clause is not treated
as an independent unit. The entities evoked in the when clause are linearly but not
structurally more recent.
Hobbs?s (1978) syntactic algorithm is based on a well-defined search procedure
(left-to-right in most cases, breadth-first) applied on the surface parse tree. The al-
gorithm has three main components. The first component treats reflexive pronouns
by constraining the search procedure with special configurational requirements. The
second component takes over when the antecedent of an anaphor is to be found in
previous sentences, and the third component searches subparts of the parse tree in
cycles until the highest clause is reached.
Intersententially, Hobbs?s syntactic algorithm favors subjects over objects, as sub-
jects are higher up in the parse tree than objects. Intersententially, our approach and
Hobbs?s algorithm would opt for the same type of antecedent. As Lappin and Le-
ass (1994) have pointed out, however, the syntactic search procedure seems to work
pretty well in English because grammatical order corresponds to phrase order. For
other languages, either free-word-order languages like Greek or languages in which
salience is determined by other factors (e.g., information status, as has been argued for
German [Strube 1998]), Hobbs?s search procedure would fail, because it is too rigid to
accommodate linguistic variation in marking salience. Even for languages like English,
the relevant salience of entities may be undermined by nonsyntactic factors. As has
already been suggested by Turan (1998), among others, certain types of NPs are less
salient than others independent of their grammatical function (e.g., indefinite quan-
tified expressions and impersonal pronouns). The flexibility of constructing lists of
entities according to salience both optimizes the capabilities of an anaphora resolution
algorithm and is best suited to accommodate the multiplicity of factors that may have
to be taken into account in determining reference salience.
Hobbs?s algorithm is, in effect, similar to our approach in the treatment of subor-
dinate clauses. Subordinate clauses belong to the same parse tree as the main clause to
which they are subordinate. This is equivalent to our claim that subordinate clauses are
not independent processing units. With respect to backward anaphora, in particular,
Hobbs?s use of the ?command? relation achieves the same result as our lower rank-
ing of entities appearing in subordinate clauses. The subject of a subordinate clause
would be lower in the parse tree than the subject of the main clause, independent
of the linear position of either. So, for example, in (44), the pronoun would correctly
resolve to Susan. In a case like (45), however, Hobbs?s algorithm would always resolve
the pronoun to Susan, since the search procedure has no way of making a distinc-
tion between different types of subordinate connectives (or verbs) and their effect on
reference salience.
341
Miltsakaki Topic Continuity and Intrasentential Anaphora
(44) After she phoned Barbara, Susan went out for dinner.
(45) Susan criticized Barbara because she was lazy.
3.4 Some Remaining Issues
As mentioned above, the proposed model for anaphora resolution accounts comfort-
ably for the results reported in Stevenson et al (2000) except, however, for the exper-
iment involving the connective so.
According to the reversibility test, so is classified as a subordinate conjunction
depending on its interpretation. In English, so denotes two relations: consequence and
purpose. The examples below indicate that only purpose-so behaves as a subordinate
conjunction. 22
(46) I had to give up my job so I could be happy again.
(47) So I could be happy again, I had to give up my job.
(48) I had just been to the bank, so I had money.
(49) #So I had money, I had just been to the bank.
The anaphora resolution model we propose predicts that the interpretation of
pronouns in consequence-so sentences is determined structurally. This prediction was
not borne out. Stevenson et al (2000) report a main effect of semantic focusing in
consequence-so continuations.
There are two options available to explain the data. First, we may hypothesize that
subordination is determined on structural grounds, in which case it is likely that lan-
guages may arbitrarily characterize their set of subordinate conjunctions. Under this
option, we may hypothesize that so in English is uniformly a subordinate conjunction
and then set out to investigate the implications of such a hypothesis on empirical
grounds. Alternatively, we may hypothesize that the crucial factor in characterizing
subordination is by its semantic properties, that is, the type of relation it establishes
with the proposition denoted in the main clause. This second option seems intuitively
appealing and more promising in explaining this otherwise puzzling linguistic phe-
nomenon, namely, the structural distinction between main and subordinate clauses. It
runs into the following problem, however.
In Modern Greek, the equivalent conjunction for the English so is etsi or ki etsi (=
?and so?), which is not polysemous and is not a subordinate conjunction. Greek etsi
links clauses paratactically (i.e., links sequences of main clauses). The examples below
show that Greek behaves differently from English in the so cases.
(50) #I Mariai htipise tin Elenij, ki etsi NULLj evale ta klamata.
the Maria hit the Eleni and so she put the tears.
Mariai hit Elenij and so shej started crying.
(51) I Mariai xilokopithike apo tin Elenij ki etsi NULLi evale ta klamata.
the Maria was-hit by the Eleni and so she put the tears.
Mariai was hit by Elenij and so shei started crying.
22 Although in many cases preposing a purpose-so clause seems unnatural, at least for some native
speakers of English preposing of a purpose-so clause is possible, given the appropriate context. For all
native speakers we have consulted, there is a marked difference in the acceptability of (47) and (49).
342
Computational Linguistics Volume 28, Number 3
The Modern Greek data show that the null subject in the so clause cannot be inter-
preted as the object of the previous clause. If subordination was to be defined on
semantic grounds, then we should not expect focusing differences between the two
languages, but in fact we do.
Finally, we notice that Greek is much like English when the second clause is linked
through other types of subordination, as shown in (52)?(53):
(52) I Mariai htipise tin Elenij giati NULLj ekane ataxies.
the Maria hit the Eleni because she did naughty-things
Mariai hit Elenij because shej was being naughty
(53) I Elenij xilokopithike apo ti Mariai giati NULLj ekane ataxies.
the Eleni was-hit by the Maria because she did naughty-things.
Elenij was hit by Mariai because shej was being naughty.
The reason for the difference between the two languages with respect to so clauses is
hard to explain. This difficulty in understanding the cross-linguistic variation is also
telling of our fundamental lack of understanding subordination in languages. In this
article, we do not claim to understand the intricacies of subordination any better. In
the next section, it is shown that the distinction between main and subordinate clauses
is in the right direction. It is not yet clear, however, what property of subordination?
structural, semantic or other?is responsible for the pattern we observe. We will leave
this issue open for future work.
Another issue that requires special attention in the proposed account pertains to
some special cases of preposed subordinate clauses. Example (54) presents a problem
for the proposed model because the antecedent of the subject pronoun in the matrix
clause is the subject of the preposed subordinate clause.
(54) After Susan phoned Barbara, she went out for dinner.
The ranking in the Cf list for (54) is she-referent > Susan > Barbara. In effect, what
we are faced with here is analogous to backward anaphora. In its current form, how-
ever, the proposed algorithm would process the subordinate clause first and would
then move to the matrix clause. The matrix clause contains a pronoun and no pos-
sible antecedent, so on completing the processing of the unit, the algorithm would
output the unresolved pronoun from the matrix clause and would continue searching
for an antecedent in the previous unit. Such cases can be identified easily by even
shallow parsing and be fixed locally by forcing resolution to the highest entity in the
current unit (i.e., Susan). Also, as a reviewer has suggested, the algorithm presented
in Section 3.2.1 could be modified so that in step 2 the Cf list includes all possible
antecedents from the current utterance Ui. With this modification, (54) would be pro-
cessed correctly, but as the same reviewer points out, this does not explain the contrast
in (55):
(55) a. Susan phoned Barbara. Then, she went out for dinner.
b. Susan phoned Barbara before she went out for dinner.
c. After Susan phoned Barbara, she went out for dinner.
Example (55a) is an instance of intersentential anaphora, and there is a subject
reference for the pronoun as predicted. Example (55b) is a case of intrasentential
343
Miltsakaki Topic Continuity and Intrasentential Anaphora
anaphora, and there is no clear subject reference. Example (55c) is another instance
of intrasentential anaphora, but in this case the subject preference is clearly on a par
with the intersentential case in (55a). Whatever required modification to the algorithm
will prove to be more useful, the fact remains that the similarity between (55a) and
(55c) remains unexplained in purely structural terms. We suspect that the difference
between (55b) and (55c) and the similarity between (55a) and (55c) is the result of
an interaction with a discourse function of subordinate clauses. Subordinate clauses
normally convey background information and do not by themselves move the nar-
rative forward. They also have the property of enabling information to appear in a
?nonnatural order? with respect to the event(s) of the main clause. A ?natural order?
for temporal connections would be to express events in the order in which they hap-
pened. For causal connections, a natural order would be to express the cause before
the effect. So it seems plausible to hypothesize that subordinate structures can be used
to introduce background (or presupposed) information and even discourse-new char-
acters without disturbing the narrative structure of the discourse and the salience of
the centers of attention already established in the narrative. If this line of thinking
is on the right track, then it is possible that the similarity between (55a) and (55c) is
due to the fact that both sequences of clauses reflect the linear succession of events.
The preposed after-clause does not disturb the natural temporal order of events both
of which are predicated of the same center, which in this case is introduced in the
subordinate clause. Further empirical work is clearly needed to evaluate this line of
explanation.
4. Empirical Studies
In this section we report the results of two empirical studies designed specifically
to test the central hypotheses of the proposed model: (a) that anaphoric reference
that spans across centering update units is determined structurally (as specified in
Section 3.2.1) and (b) that subordinate clauses do not form independent processing
units. In the first study, in Section 4.1, we report the results of a sentence completion
experiment in English. We quantify over the interpretation of a subject pronoun to
one of two ambiguous antecedents evoked in the preceding clause. Two factors were
analyzed, type of clause and semantic type of connection, in four conditions. The re-
sults show a strong main effect for type of clause. In the main-clause conditions, the
subject pronoun resolved to the subject antecedent of the previous sentence. In the
subordinate-clause conditions, the interpretation of the subject pronoun was varied.
In the second study, we tested the same hypotheses on a Greek corpus. We selected
three types of subordinate clauses under conditions similar to the ones set in the ex-
perimental study. We contrasted anaphoric interpretation in subordinate clauses with
anaphoric interpretation in main clauses. The results provide strong evidence for the
accuracy of the centering-based algorithm proposed for anaphora resolution across
units. The results also confirm that anaphoric interpretation in subordinate clauses is
not determined structurally.
4.1 Experimental Data
The aim of the experiment discussed in this section is to investigate the hypothe-
sis that subject pronouns in main clauses follow a different pattern of interpretation
from subject pronouns in subordinate clauses. The participants in this experiment read
sentences containing sequences of two clauses. In two of the four conditions of the
experiment, the sequence consisted of one main and one subordinate clause. In the
other two, the sequence consisted of two main clauses. In all the conditions, the first
344
Computational Linguistics Volume 28, Number 3
clause introduced two individuals of the same gender. The second clause contained
a subject pronoun of the same gender as the individuals introduced in the first main
clause. Participants were asked to complete the second clause. We were interested in
the interpretation of the subject pronominal in the second clause.
For this experiment, two subordinate conjunctions were selected from two se-
mantic classes, namely, time and contrast. Also, two adverbial conjunctions, then and
however, were selected from the same semantic classes to introduce the second clause
in the main-main conditions.
We predicted that the pronominal in the main clause would consistently be inter-
preted as the subject of the preceding main clause. We also predicted that the interpre-
tation of the subject pronoun in the subordinate clause would vary. In all critical items
the verb of the main clause belonged to the same category (see below), so any varia-
tion pattern observed across the two subordinate conjunctions would be attributed to
the focusing preferences projected by the semantics of the subordinate conjunctions.
The grouping of connectives into semantic types, namely, then, when for time and
however, although for contrast and structural types, then, however for main clauses and
when, although for subordinate clauses, enables us to compare and contrast the effect of
two factors: semantic type and type of clause. A main effect of semantic type would
mean that the interpretation of the subject pronoun is primarily determined by the
semantics of the connectives. Conversely, a main effect of type of clause would mean
that the interpretation of the subject pronominal is primarily determined by the type
of clause.
4.1.1 Study and Results. Sixteen adults, native speakers of English, participated in
the experiment. They were asked to perform a sentence completion task as described
in the previous section. Each participant received a form containing 36 fillers and 12
critical items, 6 per type of clause and 6 per semantic type. The example below demon-
strates a complete set of conditions; each form contained three such sets interspersed
with fillers. Each main clause appeared in all conditions across participants, and each
condition appeared three times within participants. The order of appearance of critical
items was varied in two LISTS (two variations in the order of appearance of critical
items).
a. The groom hit the best man violently. However, he. . . (contrast, main-main)
b. The groom hit the best man violently
although he. . . (contrast, main-subordinate)
c. The groom hit the best man violently. Then, he. . . (time, main-main)
d. The groom hit the best man violently when he. . . (time, main-subordinate)
All main clauses contained an action verb with two human arguments of the same
gender. An adverbial phrase was added at the end of the main clause to achieve nat-
uralness. The arguments of the verbs were always realized as role NPs to minimize
referential ambiguity in the continuations. In general, sentence continuation involv-
ing role NPs tends to disambiguate the intended reference. Nevertheless, an average
of two ambiguous continuations were identified per participant. On completing the
experiment, the participants were asked to identify explicitly the intended referent of
the ambiguous continuations.
Figure 2 summarizes the distribution of the data within participants. The y-axis
shows the number of references to the main-clause subject. The lighter grey columns
show the distribution of the interpretation of the main-clause subject pronoun, and the
345
Miltsakaki Topic Continuity and Intrasentential Anaphora
Figure 2
Experiment on anaphora.
darker grey columns show the distribution of the interpretation of the subordinate-
clause subject. For convenience, we labeled the main-main clause condition parataxis
and the main-subordinate clause condition hypotaxis.23
Analyses of variance revealed a strong main effect of type of CLAUSE: the pronoun
resolved to the subject of the previous clause significantly more often when it appeared
in a main clause (F = 74.16, df = 1, 15, p < .0001). Semantic type showed only a
marginal effect (F = 4.59, df = 1, 15, p < .049).
With regard to the comparison based on the semantic similarity between how-
ever/although and then/when, it turned out that in several cases when clauses were
assigned a causal interpretation. For example, the when clause continuation in (56) is
interpreted as giving the cause of the event described in the preceding main clause,
rather than the time specification.
(56) The father shook the son vigorously when he saw him lying on the
ground.
For this reason, it was hard to pursue the semantic-based comparison between the
paratactic then and the hypotactic when, and we therefore did not make any further
analyses within the semantic-type factor. Since it would be hard to control the inter-
pretation of the connectives used in this type of experiment, in future experiments
we plan to give up on the effort to pair connectives semantically. Instead, we plan to
23 Parataxis and hypotaxis are terms borrowed from traditional grammars to describe the two types of
connections. Main clauses are linked to each other through parataxis, whereas a subordinate clause is
linked to its superordinate clause through hypotaxis.
346
Computational Linguistics Volume 28, Number 3
include a larger number of subordinate conjunctions in order to be able to generalize
more reliably over the whole class of subordinate conjunctions.
4.1.2 Discussion. The results of this experiment show that structural focusing is promi-
nent across main clauses. The analyses of variance revealed a strong main effect for
the type of clause despite the fact that, in the experiment, we included main-clause
continuations introduced with however. Given the verb type in the first main clause,
we in fact stretched structural focusing to its limits, as it would be reasonable to expect
that the contrast relation established with however would shift attention to the second
individual. For example:
(57) The Popei tapped the priestj on the shoulder. However, hej ignored him.
Still, the results clearly indicate that the type of connection affects the interpretation
of the pronoun in the second main clause across, or despite, semantic types. Also, the
variation of pronominal reference across types of subordinate conjunctions indicates
that the interpretation of anaphoric expressions in subordinate clauses is determined
by other, apparently nonstructural, factors.24
Finally, the results are consistent with the hypothesis that structural focusing de-
termines the interpretation of pronominals across, but not within, processing units.
According to the discourse model suggested in Section 3.1, a main clause is an in-
dependent processing unit, so a sequence of main clauses constitutes a sequence of
units. On the other hand, a subordinate clause does not form an independent unit, so
a sequence containing a main and a subordinate clause is simply internal to the unit.
The reference variation observed in the subordinate conditions is then expected from
the model.
Unfortunately, in this experiment, we lost the contrast for the type of connection
within semantic types because of the noise created by the causal interpretation of
some when clauses. To enable the conclusions of this experiment to be generalized
across the entire classes of paratactic and hypotactic connectives, this experiment will
be repeated with a larger set of connectives.
An interesting extension of this work would be to investigate whether structural
focusing is active within units, before it is eventually overridden by semantic focusing,
or whether focusing preferences projected by the semantics of the verbs and connec-
tives are immediately accessed during on-line processing.
4.2 Corpus-Based Data
The corpus-based study reported in this section is based on a Greek corpus built with
text from Greek newspapers available on the Web. The corpus consists of approxi-
mately 800,000 words downloaded from the sites of the Greek newspapers Eleftherotipia
and To Vima.
The aim of the corpus study is similar to that of the experimental study reported
in the previous section. Greek allows dropped subjects, which yields higher referen-
tial ambiguity than English pronouns. We were therefore able to collect a reasonable
number of tokens fulfilling the conditions of the experimental study and to test our
hypotheses against naturally occurring data.
24 It is understood that in the main-subordinate clause sequences, the subordinate clause is linked to the
main clause in question and not to some subsequent main clause (as, for example, in cases in which
the subordinate clause is preposed).
347
Miltsakaki Topic Continuity and Intrasentential Anaphora
As in the experimental study, we wanted to compare and contrast the interpre-
tation of anaphoric expressions in a main clause with the interpretation of anaphoric
expressions in a subordinate clause. Unlike in the experimental study, however, the
search of anaphoric expressions in main and subordinate clauses was not restricted to
subject pronouns.
4.2.1 Data Collection and Coding. Greek has two pronominal systems: weak and
strong. Weak pronominals include dropped subjects and object clitics. Clitics are marked
with case, gender, and number features and attach to the verb. Direct-object clitics are
case-marked ?accusative,? and indirect or ?dative? clitics are case marked ?genitive.?
Strong pronominals are also marked with case, gender, and number features, but syn-
tactically, they behave as common nouns. Their functions are mentioned in Sections 2.2
and 3.1.1. In this study, only weak pronominals were included.
We established the following requirements for the data set of this study: (a) the
subordinate clause or second main clause contains a third-person dropped subject or
weak pronominal, and (b) the preceding main clause or any of its other associated sub-
ordinate clauses contains at least two competing antecedents. A competing antecedent
is defined as a full noun phrase, dropped subject, or weak pronominal that agrees in
gender and number with the anaphoric expression.
For anaphoric reference in main-main and main-subordinate sequences, ideally
we would have liked to include only those tokens in which the second main or sub-
ordinate clause under investigation was preceded by a unit containing only a main
clause. Imposing this extra constraint, however, would have invalidated a large num-
ber of the already limited number of tokens, so we decided against doing so. Although
conducting a second pass of the data, with the purpose of studying further the char-
acteristics of these antecedents, would have been useful, for the purposes of this study
it was not crucial. Further, a consistent pattern of reference in main-main sequences
including cases in which competing antecedents are present in intervening subordi-
nate clauses provides further evidence that entities introduced in subordinate clauses
do not override the salience of the main-clause entities. We will provide an example
to illustrate the point in the next section.
For the data set with main-subordinate sequences, we extracted three types of sub-
ordinate clauses introduced by the following subordinate conjunctions: otan (?when?),
yati (?because?), and oste (?so that?). The final data set included only tokens that fulfilled
the requirements described above.
For the data set with main-main sequences, we randomly selected files from the
corpus subdirectories and included tokens that fulfilled the requirements described
above. The selection process was terminated when the number of qualifying tokens
approximated one hundred.
Two coders, both native speakers of Greek, marked on the data set the antecedent
of the anaphoric expressions. One of the coders was the author and the other was a
naive, nonlinguist speaker of Greek. Intercoder reliability was particularly high (98%).
We attribute the high intercoder reliability to the fact that discourse-deictic expressions,
known to lower intercoder reliability, were not included in this study.25 The few cases of
disagreement between the coders involved either instances perceived as ambiguous
by the coders or abstract complex NPs about which there was disagreement as to
25 Discourse-deictic expressions include demonstratives, such as this and that, used to refer to chunks of
previous discourse. Discourse-deictic expressions in Greek are identical in form with neuter strong
pronominals. Dropped subjects can refer to discourse-deictic expressions, but such cases were excluded.
348
Computational Linguistics Volume 28, Number 3
whether the antecedent was the possessor or the possessee. Such cases were excluded
from the final data set.
The final dataset included 88 instances of main-main sequences and 108 instances
of main-subordinate sequences broken up as follows: 48 otan clauses, 17 yati clauses,
and 43 oste clauses.
4.2.2 Ranking Antecedents and Coding. Based on earlier work on the ranking of
entities in Greek (Miltsakaki 1999), the competing antecedents were ranked according
to the following rule:
Ranking Rule for Greek
Empathy > Subject > Indirect Object > Direct Object >
Indefinite Quantified NPs, Nonspecific Indefinites
Under Empathy were classified dative subjects of psych verbs. Such verbs are easily
identified from a normally short exhaustive list that can be enumerated for each lan-
guage. In our data, we encountered only the verb like from this verb category.26
All entities introduced in subordinate clauses associated with the main clause are
ranked using the same rule but lower than the main-clause entities. So, for example, if
the evoked entities are main subject, main object, and subordinate subject, the Cf list
is ranked as follows: main subject > main object > subordinate subject. It is not clear
what the ranking would be in cases of multiple subordinate clauses, but this extra
ranking specification was not crucial for the current study.
What was crucial for the study was the ranking of entities evoked within complex
NPs. Greek complex NPs are normally constructed with two nouns: the ?possessor,?
marked with genitive, and the ?possessee,? marked with nominative, accusative, or,
more rarely, genitive, depending on its grammatical role. The possessee always pre-
cedes the possessor.27 Noun-noun modification is not allowed in Greek. In complex
NPs, animate referents rank higher than inanimates. In all other cases, possessor ranks
higher than possessee. For clarification we present an example below, followed by the
ranking of the evoked entities:
(58) I mitera tis Marias ipe sto Yani oti o Giorgos den tha erhotan.
the mother of-the Maria said to-the John that the George not would come
Maria?s mother told John that George would not come.
Maria > mother > John > George
The salience ranking as specified above was then used for a second pass of coding
done by the author. For each set of candidate antecedents, the intended referent was
marked as either ?preferred antecedent? (designated by ?Ap?) or ?nonpreferred an-
tecedent? (designated by ?Anp?). The referent of an anaphoric expression was marked
26 By way of demonstration, the expression I like John in Greek is glossed as ?me-genitive like-3rd singular
John-nominative.? In the Greek example, the experiencer of the psych verb is analyzed as subject
despite its genitive marking. Such subjects are known as dative subjects. Modern Greek has lost the
dative case, whose function is now performed by the genitive case.
27 We use the terms possessor and possessee here for convenience to label the structural position of nouns
in complex NPs. These terms, however, do not always describe the semantic relationship between two
nouns. For example, in John?s participation, John can hardly be characterized as the possessor, but in
Greek participation would always precede John and would be case-marked genitive.
349
Miltsakaki Topic Continuity and Intrasentential Anaphora
as a preferred antecedent when it was the highest-ranked entity in the set of competing
antecedents. The referent of an anaphoric expression was marked as a nonpreferred an-
tecedent when it was not the highest-ranked entity in the set of competing antecedents.
In most cases, the set of candidate antecedents included only two candidates, so sub-
categorizing nonpreferred antecedents was not crucial.
In what follows, example (59) is demonstrative of cases in which the referent of
the anaphoric expression in the second main clause is marked ?Ap.? The competing
antecedents in (59a) are ta opla and anthropus, because they have the same number and
gender as the anaphoric tus in (59b). The NP ta opla is the preferred antecedent because
it is the highest-ranked element in the list of potential antecedents and is the intended
referent of the anaphoric. We report this particular example for the additional reason
that it shows that, outside complex NPs, animacy is not a factor in determining the
ranking of entities, even in cases in which the semantics of the verb taking the referent
of the anaphoric as an argument favors the human, in this case, antecedent. Example
(60) also demonstrates a case of reference to Ap. Here, the competing antecedents are
both male characters and semantically plausible subjects of the verb egrafe (?wrote?).
Note that the assumed ranking receives further support with this example, since the
anaphoric resolves to the subject of the previous clause and not to the most recent,
equally plausible entity.
(59) a. [Ta opla]i ine kataskevasmena ya na skotonun [anthropus]j.
the guns are made in-order to kill people.
Gunsi are made to kill peoplej.
b. Aftos ine o skopos [tus]i.
This is the goal their.
This is theiri goal.
(60) a. [O Turen]i vriskete apo filosofiki apopsi ston antipoda.
the Turen is-placed from philosophical view at-the opposite-side
[tu Popper]j.
of-the Popper.
From a philosophical point of view Tourrainei is the very opposite of
Popperj.
b. Prosfata [0]i egrafe oti iparhun dio idon dianoumeni
recently 0 wrote that there-are two types of-intellectuals.
Recently, hei wrote that there are two types of intellectuals.
In (61) the referent of the dropped subject in (61b) is marked ?Anp.? The list of
competing antecedents in (61a) contains PAOK, the name of a football team, and Pikulin
Ortith, the name of a player, both being singular and masculine. In this case, number
agreement with the verb is sufficient to create ambiguity, because Greek verbs are
marked for number but not gender. Also, in Greek, subject collective nouns marked
singular always take a singular verb. The intended referent is Anp, because it is ranked
lower than the subject PAOK.
(61) a. Ya mia sira praxeon [o PAOK]i kali [ton Pikulin Ortith]j na
For a series of-deeds the PAOK summons the Pikulin Ortith to
350
Computational Linguistics Volume 28, Number 3
apologithi amesa,
confess immediately,
PAOK is asking Pikulin Ortith to confess immediately for a series of
things,
b. yati [0]j ehi prokalesi megisti agonistiki ke ithiki zimia.
because 0 has caused enormous competitive and moral damage.
because [he]j has caused enormous damage (to the team) both morally
and in the championship.
Finally, example (62) demonstrates that competing antecedents in dependent clauses
do not override the salience of main-clause antecedents. Note that [i kinonikes igesies]
is a perfectly natural candidate for the subject of the verb pistevun.
(62) a. [I esiodoxi]i pistevun oti ehun dimiurgithi [i kinonikes igesies]j
the ambitious believe that have been-created the social leaderships
pu mporun na antiparatethun stin katestimeni exusia.
which can to object to-the established leadership.
[The ambitious ones]i believe that there have been formed [social
authorities]j
which can stand up to the established leadership/political power.
b. [NULL]i pistevun oti o agonas tus den ehi akrivos kerdithi alla
NULL believe that the fight their not has exactly been-won but
oti NULL vriskete se ?dromo horis epistrofi.?
that NULL is-found in ?road without return.?
[They]i believe that their fight has not exactly been won but that it is at
a point with no return.
In the next section we present the results of the analysis of the distribution of
anaphoric references based on the values of Ap and Anp.
4.3 Results and Discussion
Table 2 shows the distribution of anaphoric reference in the experiment described in
Section 4.2. The first column shows the number of times the anaphoric expression
resolves to the preferred antecedent (Ap). The second column shows the number of
times the anaphoric expression resolves to a nonpreferred antecedent (Anp), and the
third column summarizes the total number of tokens per condition.
The corpus-based results support the hypothesis that anaphora does not obey
the same rules in main and subordinate clauses.28 Clearly, the preferred antecedent
as defined structurally is a strong predictor of the referent of main-clause anaphoric
expressions, whereas the picture appears more complicated in subordinate clauses.
In the main-main condition, the Anp instances have interesting properties in com-
mon. Four out of the seven Anp cases involved complex NPs in which both competing
antecedents belong to the complex NP construction. It turned out that the ranking
we assumed for complex NPs did not always predict the intended referent correctly.
For example, in (63), the ranking of the complex NP i simetohi tu k. Avramopulu (Mr.
Avramopoulos?s participation) is Avramopulos > Simetohi because Avramopulos (the
current mayor of Athens) is animate and ranks higher. The intended referent of the
28 Not surprisingly, chi-square gives a highly significant p < 0.0005.
351
Miltsakaki Topic Continuity and Intrasentential Anaphora
Table 2
Reference in main and subordinate clauses.
Ap Anp Total
Main-main 81 (92%) 7 (7%) 88
Main-subordinate 55 (51%) 53 (49%) 108
dropped subject in the co-ordinated clause, however, is simetohi. A possible explana-
tion for this pattern is to analyze (63) as verb phrase (VP) coordination, in which case
the two VPs share the same subject.
(63) a. Apo afto prokiptei oti [i simetohi]i [tu k. Avramopulu]j
From this concludes that the participation of-the Mr. Avramopulos
stis prosehis ekloges epireazi apofasistika tin tihi tis ND
at-the next elections affects decisively the fate of-the ND-(name of
political party)
From this it is concluded that Mr. Avramopoulos?s participation at the
next elections decisively affects the fate of ND
b. ke [0]i evnoi antistihos to PaSoK.
and 0 favors correspondingly the PaSoK-(name of political party)
and [it]i favors PaSoK at the same time.
The same phenomenon was observed, however, in cases with no VP coordination,
as shown in (64). Again, in this case the anaphoric resolves to koma (?political party?)
and not to Avramopulos, as would be expected. A possible explanation here is that the
concept political party is not ?inanimate? in the sense that it denotes a particular group
of people. In this case, it would be an animate possessor, and the ranking would work
as expected. In conclusion, although it seems possible that the ranking of complex
NPs can be fixed reliably, taking into account special cases of VP coordination and
animacy of collective nouns, the number of instances of such special cases is too small
to draw any definitive conclusions.
(64) a. [To koma]i [tu Avramopulu]i emfanizete se thesi na anadihthi
the party of-the Avramopulos appears in position to be-promoted
se paragonta pu tha tropopiisi tus orus tu politiku pehnidiu.
to factor which will change the terms of-the political game.
Avramopoulos?s political party appears to be in a position to get
promoted to a factor that will change the terms of the political game.
b. Me to 14,7% pu pistonete os ?prothesi psifu? [0]i katagrafi
with the 14.7% which gets-credited as ?intention of-vote? 0 records
axiologi apihisi protu kan anadihthun ta politika haraktiristika
significant appeal before even get-revealed the political characteristics
tu.
its.
With the 14.7% which gets recorded as ?vote intention?, [it]i records a
significant appeal even before its political characteristics are shown.
The remaining cases of reference to Anp involved complex discourses in which
either inferrable information was needed or the referent was placed in an adverbial
352
Computational Linguistics Volume 28, Number 3
located in the same clause as the anaphoric itself. The following example contains
instances of both cases:
(65) a. Legete oti [o ?Mihanismos?]i metaferotan stin Arhea
it-is-said that the ?Mechanism? was-being-transfered to-the Ancient
Romi gia na epidihthi ston Kikerona
Rome in-order that be-shown to-the Kikerona
It is said that the ?Mechanism? was being transfered to Ancient Rome
in order to be shown to Cicero
b. ala to plio vithistike exo apo ta Kithira.
but the boat sank outside of the Kithira.
but the boat sank off (the coast of) Kithira.
c. To navagio entopistike stis arhes tu eona
the shipwreck was-located at-the beginning of the century
The shipwreck was found at the beginning of the century
d. ke meta tin anelkisi mathimatiki ke arheologi [ton]i
and after the hoisting mathematicians and archaeologists it
anasinthesan.
they-reconstructed.
and after the hoisting, mathematicians and archaeologists
reconstructed iti.
The pronoun in (65d) resolves to Mihanismos in (65a). The entity Mihanismos is evoked
much more recently as an inferrable entity in (65d), however?the hoisting of the
Mechanism?and it appears in the same clause as the anaphoric itself. Such complex
cases are extremely rare and generally very hard to resolve with a structure-based
algorithm.
To complete the analysis of the data, we further broke down the distribution of
reference to Ap and Anp for each subordinate clause. The results are shown in Table 3.
Chi-square shows no significant differences among the three types of subordinate
clauses (p < 0.182).
These results indicate that the focusing preferences of the connectives do not by
themselves predict the interpretation of the anaphoric expressions. They are, however,
consistent with Stevenson et al?s (2000) conclusions that the effect of the connective
on the interpretation of pronominals depends on the event structure of the preceding
clause, either reinforcing or reducing the effect of the verb focusing projections. Lack
of correlations between subordinate type and anaphora resolution is not surprising,
since the data included various types of verbs.
Table 3
Distribution of Ap/Anp.
Ap Anp Total
Main-when(otan) 23 (48%) 25 (52%) 48
Main-because(yati) 6 (35%) 11 (65%) 17
Main-so that(oste) 26 (60%) 17 (40%) 43
353
Miltsakaki Topic Continuity and Intrasentential Anaphora
5. General Discussion
The interpretation of anaphoric expressions in natural language processing is not a
trivial problem. Extensive research in past years has made significant contributions
to our understanding of the phenomenon, and a considerable number of theoreti-
cally motivated and/or corpus-based anaphora resolution algorithms have been built
with more or less success. The task remains a challenge, however, and the slow
rate of improvement in the performance of anaphora resolution systems is somewhat
alarming.
The detailed review of the literature provided in Section 2 revealed that many of
the complications and inconsistencies in anaphora resolution start when algorithms
are faced with anaphoric elements in complex sentences. In particular, we saw that
the interpretation of anaphoric expressions in certain types of clauses would defy any
algorithm based on registers of NPs and a uniform lookup mechanism.
The main contribution of this article is precisely the distinction and specification
of two systems that determine preferences for anaphoric interpretation. Contra earlier
views on the status of subordinate clauses, we argued that subordinate clauses do
not constitute independent processing units. In fact, subordinate clauses can be seen
as filling up extended argument positions required by the predicate of the matrix
clause, and, in this respect, intrasentential relationships that hold between predicates
and participating entities should be expected to be closely determined on semantic
grounds. We identified the boundaries of the basic discourse units with the boundaries
of the unit containing a matrix clause and all its dependent clauses and suggested that
anaphoric interpretation within this unit is determined semantically by the focusing
properties of the verbs and connectives.
On the other hand, topic continuity, as evaluated in the centering model, requires
rather arbitrary specifications of salience to facilitate discourse processing and effi-
cient integration of meaning to the previous discourse. Discourses grow enormous
very quickly. Unrestricted semantic representations and the resulting inferencing load
imposed by exploding semantic computations would considerably slow down dis-
course processing (Kohlhase and Koller 2000). The notion of salience, in the sense
of centering (Joshi and Kuhn 1979), is arguably crucial for efficient processing not
only for natural language processing systems, but also for humans. Topic continuity
therefore is evaluated using a salience mechanism operating across processing units,
and we showed that this mechanism is structural and best defined in centering terms.
We then argued that anaphoric reference that spans across units is also determined
structurally.
Regarding centering-based anaphora resolution algorithms, which seem the best
candidates for resolving anaphora across units, a few technical problems were dis-
cussed in Section 2. We suggested, however, that these problems can be fixed easily
and proposed that the algorithm should select as the preferred antecedent the highest-
ranked entity in the previous unit. This modification is, in fact, consistent with cen-
tering?s pronoun rule and at the same time does not rely on the assumption that text
is maximally coherent.
The corpus-based study reported in Section 5.2 was designed to test the hypothesis
that two mechanisms are indeed at work and also to evaluate the strengths of the
modified centering-based algorithm for resolving anaphoric reference across units.
The results were robust despite the moderate sample size, prescribing a route for a
number of future projects in this direction, the most challenging of which will probably
be understanding the structural and semantic properties of subordination and its role
in the organization, representation, and structure of discourse.
354
Computational Linguistics Volume 28, Number 3
Acknowledgments
I am most grateful to Ellen Prince and
Aravind Joshi as well as to Robin Clark,
Barbara Grosz, Michael Kohlhase,
Alexander Koller, and Alistair Knott for
very useful and stimulating discussions. I
would also like to thank Felicia Hurewitz,
Jesse Snedeker, and John Trueswell for their
generous help in the design and analysis of
the experiment as well as the other
members of the Gleitmans? CHEESE
seminar in the Psychology Department,
University of Pennsylvania, for their
continuous support and valuable feedback.
Finally, I am grateful to three anonymous
reviewers, whose thoughtful and
constructive criticism helped me improve
the quality of this article significantly. The
research on which this article is based was
supported by the Institute of Research in
Cognitive Science, grant no. NSF-SBR
8920230, University of Pennsylvania.
References
Brennan, Susan, Marilyn Walker-Friedman,
and Carl Pollard. 1987. ?A centering
approach to pronouns.? In Proceedings of
the 25th Annual Meeting of the Association for
Computational Linguistics, pages 155?162,
Stanford, California.
Carden, Guy. 1982. Backwards anaphora in
discourse context. Journal of Linguistics
18:361?387.
Danes?, Frantisek. 1974. ?Functional sentence
perspective and the organization of the
text.? In F. Danes?, editor, Papers on
Functional Sentence Perspective. Academia,
Prague, pages 106?128.
Di Eugenio, Barbara. 1990. ?Centering
theory and the Italian pronominal
system.? In Proceedings of the 13th
International Conference on Computational
Linguistics (COLING 90), pages 270?275,
Helsinki.
Di Eugenio, Barbara. 1998. ?Centering in
Italian.? In M. Walker, A. Joshi, and
E. Prince, editors, Centering Theory in
Discourse. Clarendon Press, Oxford, pages
115?137.
Dimitriadis, Alexis. 1996. ?When pro-drop
languages don?t: Overt pronominal
subjects and pragmatic inference.? In
Proceedings of the 32nd Meeting of the Chicago
Linguistics Society, Chicago, pages 33?47.
Grosz, Barbara, Aravind Joshi, and Scott
Weinstein. 1983. ?Providing a unified
account of definite noun phrases in
discourse.? In Proceedings of the 21st
Annual Meeting of the Association for
Computational Linguistics, pages 44?50,
MIT Press, Cambridge, Massachusetts.
Grosz, Barbara and Candace Sidner. 1986.
Attentions, intentions and the structure of
discourse. Computational Linguistics
12(3):175?204.
Hobbs, Jerry. 1978. Resolving pronoun
references. Lingua 44:311?338.
Hudson-D?Zmura, Susan and Michael
Tanenhaus. 1998. ?Assigning antecedents
to ambiguous pronouns: The role of the
center of attention as a default
assignment.? In M. Walker, A. Joshi, and
E. Prince, editors, Centering Theory in
Discourse. Clarendon Press, Oxford, pages
199?226.
Hurewitz, Felicia. 1998. ?A quantitative look
at discourse coherence.? In M. Walker,
A. Joshi, and E. Prince, editors, Centering
Theory in Discourse. Clarendon Press,
Oxford, pages 273?291.
Joshi, Aravind and Steven Kuhn. 1979.
?Centered logic: The role of entity
centered sentence representation in
natural language inferencing.? In Sixth
International Joint Conference on Artificial
Intelligence, pages 435?439, Tokyo.
Kameyama, Megumi. 1993. ?Intrasentential
centering.? In Proceedings of the Workshop
on Centering, University of Pennsylvania,
Philadelphia.
Kameyama, Megumi. 1998. ?Intrasentential
centering: A case study.? In M. Walker,
A. Joshi, and E. Prince, editors, Centering
Theory in Discourse. Clarendon Press,
Oxford, pages 89?112.
Kohlhase, Michael and Alexander Koller.
2000. ?Towards a tableaux machine for
language understanding.? In Proceedings
of the Second Workshop on Inference in
Computational Semantics (ICOS-2), pages
57?88, Dagstuhl, Germany.
Kuno, Susumu. 1972. Functional sentence
perspective: A case study from Japanese
and English. Linguistic Inquiry 3:269?320.
Lappin, Shalom and Herbert Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics
20(4):535?561.
McCord, Michael. 1990. ?Slot grammar: A
system for simpler construction of
practical natural language grammars.? In
R. Studer, editor, Natural Language and
Logic: International Scientific Symposium.
Lecture Notes in Computer Science.
Berlin, Springer Verlag, pages 118?145.
Miltsakaki, Eleni. 1999. ?Locating topics in
text processing.? In Computational
Linguistics in the Netherlands: Selected Papers
from the Tenth CLIN Meeting (CLIN ?99),
pages 127?138, Utrecht, Netherlands.
Miltsakaki, Eleni. 2000. Attention Structure
355
Miltsakaki Topic Continuity and Intrasentential Anaphora
in Discourse: A Cross-Linguistic
Investigation. Dissertation topic proposal,
University of Pennsylvania, Philadelphia.
Miltsakaki, Eleni. 2001. ?Centering in
Greek.? In Proceedings of the 15th
International Symposium on Theoretical and
Applied Linguistics, Thessaloniki, Greece.
Miltsakaki, Eleni and Karen Kukich. 2000a.
?Automated evaluation of coherence in
student essays.? In Proceedings of the
Workshop on Language Rescources and Tools
in Educational Applications, LREC 2000,
pages 7?14, Athens.
Miltsakaki, Eleni and Karen Kukich. 2000b.
?The role of centering theory?s rough shift
in the teaching and evaluation of writing
skills.? In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL 2000), pages 408?415,
Hong Kong.
Prasad, Rashmi and Michael Strube. 2000.
?Pronoun resolution in Hindi.? In
A. Williams and E. Kaiser, editors,
Working Papers in Linguistics, volume 6.
University of Pennsylvania, pages
189?208.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1972. A
Grammar of Contemporary English.
Longman, London.
Rambow, Owen. 1993. Pragmatic aspects of
scrambling and topicalization in German.
In Workshop on Centering Theory in
Naturally Occurring Discourse, Institute of
Research in Cognitive Science, University
of Pennslylvania.
Stevenson, Rosemary, Rosalind Crawley,
and David Kleinman. 1994. Thematic
roles, focusing and the representation of
events. Language and Cognitive Processes
9:519?548.
Stevenson, Rosemary, Alistair Knott, Jon
Oberlander, and Sharon McDonald. 2000.
Interpreting pronouns and connectives:
Interactions among focusing, thematic
roles and coherence relations. Language
and Cognitive Processes 15(3):225?262.
Strube, Michael. 1998. ?Never look back: An
alternative to centering.? In Proceedings of
the 17th International Conference on
Computational Linguistics and the 36th
Annual Meeting of the Association for
Computational Linguistics (COLING-ACL
?98), pages 1251?1257, Montreal.
Strube, Michael and Udo Hahn. 1996.
?Functional centering.? In Proceedings of
the 34th Annual Conference of the Association
for Computational Linguistics (ACL ?96),
pages 270?277, Santa Cruz.
Strube, Michael and Udo Hahn. 1999.
Functional centering: Grounding
referential coherence in information
structure. Computational Linguistics
25(3):309?344.
Suri, Linda and Kathleen McCoy. 1994.
RAFT/RAPR and centering: A
comparison and discussion of problems
related to processing complex sentences.
Computational Linguistics 20(2):301?317.
Suri, Linda, Kathleen McCoy, and Jon
DeCristofaro. 1999. A methodology for
extending focusing frameworks.
Computational Linguistics 25(2):173?194.
Tanaka, Izumi. 2000. ?Cataphoric personal
pronouns in English news reportage.? In
Proceedings of the Third International
Discourse Anaphora and Anaphor Resolution
Conference, DAARC 2000, volume 12,
pages 108?117, Lancaster, England.
Turan, Umit. 1998. ?Ranking
forward-looking centers in Turkish:
Universal and language specific
properties.? In M. Walker, A. Joshi, and
E. Prince, editors, Centering Theory in
Discourse. Clarendon Press, Oxford, pages
139?160.
van Hoek, Karen. 1997. Anaphora and
Conceptual Structure. University of
Chicago Press, Chicago.
Walker, Marilyn, Masayo Iida, and Sharon
Cote. 1994. Japanese discourse and the
process of centering. Computational
Linguistics 20(2):193?233.
Walker, Marilyn and Ellen Prince. 1996. ?A
bilateral approach to givenness: A
hearer-status algorithm and a centering
algorithm.? In T. Fretheim and J. Gundel,
editors, Reference and Referent Accessibility.
John Benjamins, Amsterdam, pages
291?306.
Webber, Bonnie and Aravind Joshi. 1998.
?Anchoring a lexicalized tree adjoining
grammar for discourse.? In ACL/COLING
Workshop on Discourse Relations and
Discourse Markers, pages 8?92, Montreal,
Quebec, Canada.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999a.
?Discourse relations: A structural and
presuppositional account using
lexicalized TAG.? In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 41?48,
College Park, Maryland.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999b. ?What
are little texts made of? A structural and
presuppositional account using
lexicalized TAG.? In Proceedings of the
International Workshop on Levels of
Representation in Discourse (LORID ?99),
pages 145?149, Edinburgh.
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
The Role of Centering Theory's Rough-Shift in the Teaching
and Evaluation of Writing Skills
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@unagi.cis.upenn.edu
Karen Kukich
Educatinal Testing Service
Princeton, NJ 08541 USA
kkukich@ets.org
Abstract
Existing software systems for automated essay scor-
ing can provide NLP researchers with opportunities
to test certain theoretical hypotheses, including some
derived from Centering Theory. In this study we em-
ploy ETS's e-rater essay scoring system to examine
whether local discourse coherence, as dened by a
measure of Rough-Shift transitions, might be a sig-
nicant contributor to the evaluation of essays. Our
positive results indicate that Rough-Shifts do indeed
capture a source of incoherence, one that has not been
closely examined in the Centering literature. These re-
sults not only justify Rough-Shifts as a valid transition
type, but they also support the original formulation of
Centering as a measure of discourse continuity even in
pronominal-free text.
1 Introduction
The task of evaluating student's writ-
ing ability has traditionally been a labor-
intensive human endeavor. However, sev-
eral dierent software systems, e.g., PEG
Page and Peterson (1995), Intelligent Essay
Assessor
1
and e-rater
2
, are now being used
to perform this task fully automatically. Fur-
thermore, by at least one measure, these soft-
ware systems evaluate student essays with the
same degree of accuracy as human experts.
That is, computer-generated scores tend to
match human expert scores as frequently as
two human scores match each other (Burstein
et al, 1998).
Essay scoring systems such as these can
provide NLP researchers with opportunities
to test certain theoretical hypotheses and to
explore a variety of practical issues in compu-
tational linguistics. In this study, we employ
the e-rater essay scoring system to test a hy-
1
http://lsa.colorado.edu.
2
http://www.ets.org/research/erater.html
pothesis related to Centering Theory (Joshi
and Weinstein, 1981; Grosz et al, 1983, in-
ter alia). We focus on Centering Theory's
Rough-Shift transition which is the least well
studied among the four transition types. In
particular, we examine whether the discourse
coherence found in an essay, as dened by a
measure of relative proportion of Rough-Shift
transitions, might be a signicant contributor
to the accuracy of computer-generated essay
scores. Our positive nding validates the role
of the Rough-Shift transition and suggests a
route for exploring Centering Theory's prac-
tical applicability to writing evaluation and
instruction.
2 The e-rater essay scoring system
One goal of automatic essay scoring systems
such as e-rater is to represent the criteria that
human experts use to evaluate essays. The
writing features that e-rater evaluates were
specically chosen to reect scoring criteria
for the essay portion of the Graduate Manage-
ment Admissions Test (GMAT). These cri-
teria are articulated in GMAT test prepara-
tion materials at http://www.gmat.org. In
e-rater, syntactic variety is represented by
features that quantify occurrences of clause
types. Logical organization and clear transi-
tions are represented by features that quan-
tify cue words in certain syntactic construc-
tions. The existence of main and supporting
points is represented by features that detect
where new points begin and where they are
developed. E-rater also includes features that
quantify the appropriateness of the vocabu-
lary content of an essay.
One feature of writing valued by writing
experts that is not explicitly represented in
the current version of e-rater is local coher-
ence. Centering Theory provides an algo-
rithm for computing local coherence in writ-
ten discourse. Our study investigates the ap-
plicability of Centering Theory's local coher-
ence measure to essay evaluation by determin-
ing the eect of adding this new feature to
e-rater's existing array of features.
3 Overview of Centering
A synthesis of two dierent lines of work
(Joshi and Kuhn, 1979; Joshi and Weinstein,
1981) and (Sidner, 1979; Grosz, 1977; Grosz
and Sidner, 1986) yielded the formulation
of Centering Theory as a model for moni-
toring local focus in discourse. The Cen-
tering model was designed to account for
those aspects of processing that are respon-
sible for the dierence in the perceived co-
herence of discourses such as those demon-
strated in (1) and (2) below (examples from
Hudson-D'Zmura (1988)).
(1) a. John went to his favorite music store to
buy a piano.
b. He had frequented the store for many
years.
c. He was excited that he could nally buy a
piano.
d. He arrived just as the store was closing for
the day.
(2) a. John went to his favorite music store to
buy a piano.
b. It was a store John had frequented for
many years.
c. He was excited that he could nally buy a
piano.
d. It was closing just as John arrived.
Discourse (1) is intuitively more coherent
than discourse (2). This dierence may be
seen to arise from the dierent degrees of con-
tinuity in what the discourse is about. Dis-
course (1) centers a single individual (John)
whereas discourse (2) seems to focus in and
out on dierent entities (John, store, John,
store). Centering is designed to capture these
uctuations in continuity.
4 The Centering model
In this section, we present the basic def-
initions and common assumptions in Cen-
tering as discussed in the literature (e.g.,
Walker et al (1998)). We present the as-
sumptions and modications we made for this
study in Section 6.1.
4.1 Discourse segments and entities
Discourse consists of a sequence of textual
segments and each segment consists of a se-
quence of utterances. In Centering The-
ory, utterances are designated by U
i
  U
n
.
Each utterance U
i
evokes a set of dis-
course entities, the FORWARD-LOOKING
CENTERS, designated by Cf(U
i
). The
members of the Cf set are ranked accord-
ing to discourse salience. (Ranking is de-
scribed in Section 4.4.)The highest-ranked
member of the Cf set is the PREFERRED
CENTER, Cp. A BACKWARD-LOOKING
CENTER, Cb,is also identied for utterance
U
i
. The highest ranked entity in the pre-
vious utterance, Cf(U
i 1
), that is realized
in the current utterance, U
i
, is its des-
ignated BACKWARD-LOOKING CENTER,
Cb. The BACKWARD-LOOKING CEN-
TER is a special member of the Cf set because
it represents the discourse entity that U
i
is
about, what in the literature is often called
the 'topic' (Reinhart, 1981; Horn, 1986).
The Cp for a given utterance may be iden-
tical with its Cb, but not necessarily so. It
is precisely this distinction between looking
back in the discourse with the Cb and pro-
jecting preferences for interpretations in the
subsequent discourse with the Cp that pro-
vides the key element in computing local co-
herence in discourse.
4.2 Centering transitions
Four types of transitions, reecting four de-
grees of coherence, are dened in Centering.
They are shown in transition ordering rule
(1). The rules for computing the transitions
are shown in Table 1.
(1) Transition ordering rule: Continue
is preferred to Retain, which is preferred to
Smooth-Shift, which is preferred to Rough-
Shift.
Centering denes one more rule, the Pro-
noun rule which we will discuss in detail in
Section 5.
Cb(Ui)=Cb(Ui-1) Cb(Ui)6=Cb(Ui-1)
Cb(Ui)=Cp Continue Smooth-Shift
Cb(Ui)6=Cp Retain Rough-Shift
Table 1: Table of transitions
4.3 Utterance
In early formulations of Centering Theory,
the 'utterance' was not dened explicitly. In
subsequent work (Kameyama, 1998), the ut-
terance was dened as, roughly, the tensed
clause with relative clauses and clausal com-
plements as exceptions. Based on crosslin-
guistic studies, Miltsakaki (1999) dened the
utterance as the traditional 'sentence', i.e.,
the main clause and its accompanying subor-
dinate and adjunct clauses constitute a single
utterance.
4.4 Cf ranking
As mentioned earlier, the PREFERRED
CENTER of an utterance is dened as the
highest ranked member of the Cf set. The
ranking of the Cf members is determined
by the salience status of the entities in the
utterance and may vary crosslinguistically.
Kameyama (1985) and Brennan et al (1987)
proposed that the Cf ranking for English is
determined by grammatical function as fol-
lows:
(2) Rule for ranking of
forward-looking centers: SUBJ>IND.
OBJ>OBJ>OTHERS
Later crosslinguistic studies based on em-
pirical work (Di Eugenio, 1998; Turan, 1995;
Kameyama, 1985) determined the following
detailed ranking, with QIS standing for quan-
tied indenite subjects (people, everyone
etc) and PRO-ARB (we, you) for arbitrary
plural pronominals.
(3)Revised rule for the ranking of
forward-looking centers: SUBJ>IND.
OBJ>OBJ>OTHERS>QIS, PRO-ARB.
4.4.1 Complex NPs
In the case of complex NPs, which have
the property of evoking multiple discourse en-
tities (e.g. his mother, software industry),
the working hypothesis commonly assumed
(e.g. Walker and Prince (1995)) is ordering
from left to right.
3
5 The role of Rough-Shift
transitions
As mentioned briey earlier, the Centering
model includes one more rule, the Pronoun
Rule given in (4).
(4) Pronoun Rule: If some element of
Cf(Ui-1) is realized as a pronoun in Ui, then
so is the Cb(Ui).
The Pronoun Rule reects the intuition
that pronominals are felicitously used to re-
fer to discourse-salient entities. As a result,
Cbs are often pronominalized, or even deleted
(if the grammar allows it). Rule (4) then
predicts that if there is only one pronoun in
an utterance, this pronoun must realize the
Cb. The Pronoun Rule and the distribution
of forms (denite/indenite NPs and pronom-
inals) over transition types plays a signicant
role in the development of anaphora resolu-
tion algorithms in NLP. Note that the utility
of the Pronoun Rule and the Centering transi-
tions in anaphora resolution algorithms relies
heavily on the assumption that the texts un-
der consideration are maximally coherent. In
maximally coherent texts, however, Rough-
Shifts transitions are rare, and even in less
than maximally coherent texts they occur
infrequently. For this reason the distinc-
tion between Smooth-Shifts and Rough-Shifts
was collapsed in previous work (Di Eugenio,
1998; Hurewitz, 1998, inter alia). The status
of Rough-Shift transitions in the Centering
model was therefore unclear, receiving only
negative evidence: Rough-Shifts are valid be-
cause they are found to be rare in coherent
discourse.
In this study we gain insights pertaining
to the nature of the Rough-Shifts precisely
because we are forced to drop the coherence
assumption. Our data consist of student es-
says whose degree of coherence is under eval-
uation and therefore cannot be assumed. Us-
ing students' paragraph marking as segment
boundaries, we 'centered' 100 GMAT essays.
The average length of these essays was about
3
But see also Di Eugenio (1998) for the treatment
of complex NPs in Italian.
Def. Phr. Indef. Phr. Prons
Rough-Shifts 75 120 16
Total 195 16
Table 2: Distribution of forms over Rough-Shifts
250 words. In the next section we show
that Rough-Shift transitions provide a reli-
able measure of incoherence, correlating well
with scores provided by writing experts.
One of the crucial insights was that, in
our data, the incoherence detected by the
Rough-Shift measure is not due to violations
of the Pronominal Rule or infelicitous use of
pronominal forms in general. In Table 2,
we report the results of the distribution of
forms over Rough-Shift transitions. Out of
the 211 Rough-Shift transitions, found in the
set of 100 essays, in 195 occasions the Cp
was a nominal phrase, either denite or indef-
inite. Pronominals occurred in only 16 cases
of which 6 cases instantiated the pronominals
'we' or 'you' in their generic sense. Table 2
strongly indicates that student essays were
not incoherent in terms of the processing load
imposed on the reader to resolve anaphoric
references. Instead, the incoherence in the es-
says was due to discontinuities in students'
essays caused by their introducing too many
undeveloped topics within what should be a
conceptually uniform segment, i.e. their para-
graphs. This is, in fact, what Rough-Shift
picked up.
These results not only justify Rough-Shifts
as a valid transition type but they also sup-
port the original formulation of Centering as
a measure of discourse continuity even when
anaphora resoluion is not an issue. It seems
that Rough-Shifts are capturing a source of
incoherence that has been overlooked in the
Centering literature. The processing load in
the Rough-Shift cases reported here is not
increased by the eort required to resolve
anaphoric reference but instead by the eort
required to nd the relevant topic connections
in a discourse bombarded with a rapid suc-
cession of multiple entities. That is, Rough-
Shifts are the result of absent or extremely
short-lived Cbs. We interpret the Rough-
Shift transitions in this context as a reection
of the incoherence perceived by the reader
when s/he is unable to identify the topic (fo-
cus) structure of the discourse. This is a
signicant insight which opens up new av-
enues for practical applications of the Cen-
tering model.
6 The e-rater Centering study
In an earlier preliminary study, we applied the
Centering algorithm manually to a sample of
36 GMAT essays to explore the hypothesis
that the Centering model provides a reason-
able measure of coherence (or lack of) reect-
ing the evaluation performed by human raters
with respect to the corresponding require-
ments described in the instructions for human
raters. We observed that essays with higher
scores tended to have signicantly lower per-
centages of ROUGH-SHIFTs than essays with
lower scores. As expected, the distribution of
the other types of transitions was not signif-
icant. In general, CONTINUEs, RETAINs,
and SMOOTH-SHIFTs do not yield incoher-
ent discourses (in fact, an essay with only
CONTINUE transitions might sound rather
boring!).
In this study we test the hypothesis that
a predictor variable derived from Centering
can signicantly improve the performance of
e-rater. Since we are in fact proposing Cen-
tering's ROUGH-SHIFTs as a predictor vari-
able, our model, strictly speaking, measures
incoherence.
The corpus for our study came from a
pool of essays written by students taking the
GMAT test. We randomly selected a total
of 100 essays, covering the full range of the
scoring scale, where 1 is lowest and 6 is high-
est (see appendix). We applied the Center-
ing algorithm to all 100 essays, calculated the
percentage of ROUGH-SHIFTs in each essay
and then ran multiple regression to evaluate
the contribution of the proposed variable to
the e-rater's performance.
6.1 Centering assumptions and
modications
Utterance. Following Miltsakaki (1999), we
assume that the each utterance consists of one
main clause and all its subordinate and ad-
junct clauses.
Cf ranking. We assumed the Cf ranking
given in (3).
A modication we made involved the sta-
tus of the pronominal I.
4
We observed that
in low-scored essays the rst person pronom-
inal I was used extensively, normally present-
ing personal narratives. However, personal
narratives were unsuited to this essay writing
task and were assigned lower scores by ex-
pert readers. The extensive use of I in the
subject position produced an unwanted eect
of high coherence. We prescriptively decided
to penalize the use of I's in order to better
reect the coherence demands made by the
particular writing task. The way to penal-
ize was to omit I's. As a result, coherence
was measured with respect to the treatment
of the remaining entities in the I-containing
utterances. This gave us the desired result of
being able to distinguish those I-containing
utterances which made coherent transitions
with respect to the entities they were talking
about and those that did not.
Lack of Fit
Source
DF Sum of
Squares
Mean
Square
F-
Ratio
Lack of Fit 71 53.55 0.75 1.30
Pure Error 24 13.83 0.57 Prob>F
Total Error 95 67.38 0.23
Max RSq
0.94
Parameter
Estimates
Term
Esti-
mate
Std
Error
t-
Ratio
Prob>
jtj
Intercept 1.46 0.37 3.92 0.0002
E-RATER 0.80 0.06 11.91 <.0001
ROUGH -0.013 0.0041 -3.32 0.0013
Eect Test
Source
Nparm
DF Sum of
Squares
F-
Ratio
Prob>
F
E-RATER 1 1 100.56 141.77 <.0001
ROUGH 1 1 7.81 11.01 0.0013
Table 3: Regression
Segments. Segment boundaries are ex-
4
In fact, a similar modication has been proposed
by Hurewitz (1998) and Walker (1998) observed that
the use of I in sentences such as 'I believe that...', 'I
think that...' do not aect the focus structure of the
text.
tremely hard to identify in an accurate and
principled way. Furthermore, existing algo-
rithms (Morris and Hirst, 1991; Youmans,
1991; Hearst, 1994; Kozima, 1993; Reynar,
1994; Passonneau and Litman, 1997; Passon-
neau, 1998) rely heavily on the assumption of
textual coherence. In our case, textual coher-
ence cannot be assumed. Given that text or-
ganization is also part of the evaluation of the
essays, we decided to use the students' para-
graph breaks to locate segment boundaries.
6.2 Implementation
For this study, we decided to manually tag
coreferring expressions despite the availabil-
ity of coreference algorithms. We made this
decision because a poor performance of the
coreference algorithm would give us distorted
results and we would not be able to test our
hypothesis. For the same reason, we manu-
ally tagged the Preferred centers as Cp. We
only needed to mark all the other entities as
OTHER. This information was adequate for
the computation of the Cb and all of the tran-
sitions.
Discourse segmentation and the implemen-
tation of the Centering algorithm for the com-
putation of the transitions were automated.
Segments boundaries were marked at para-
graph breaks and the transitions were calcu-
lated according to the instructions given in
Table 1. As output, the system computed
the percentage of Rough-Shifts for each es-
say. The percentage of Rough-Shifts was cal-
culated as the number of Rough-Shifts over
the total number of identied transitions in
the essay.
7 Study results
In the appendix, we give the percentages of
Rough-Shifts (ROUGH) for each of the actual
student essays (100) on which we tested the
ROUGH variable in the regression discussed
below. The HUMAN (HUM) column con-
tains the essay scores given by human raters
and the EARTER (E-R) column contains the
corresponding score assigned by the e-rater.
Comparing HUMAN and ROUGH, we ob-
serve that essays with scores from the higher
end of the scale tend to have lower percent-
ages of Rough-Shifts than the ones from the
lower end. To evaluate that this observa-
tion can be utilized to improve the e-rater's
performance, we regressed X=E-RATER and
X=ROUGH (the predictors) by Y=HUMAN.
The results of the regression are shown in Ta-
ble 3. The 'Estimate' cell contains the coef-
cients assigned for each variable. The coef-
cient for ROUGH is negative, thus penaliz-
ing occurrences of Rough-Shifts in the essays.
The t-test ('t-ratio' in Table 3) for ROUGH
has a highly signicant p-value (p<0.0013) for
these 100 essays suggesting that the added
variable ROUGH can contribute to the ac-
curacy of the model. The magnitude of the
contribution indicated by this regression is
approximately 0.5 point, a reasonalby siz-
able eect given the scoring scale (1-6). Ad-
ditional work is needed to precisely quan-
tify the contribution of ROUGH. That would
involve incorporating the ROUGH variable
into the building of a new e-rater model and
comapring the results of the new model to the
original e-rater model.
As a preliminary test of the predictability
of the model, we jacknifed the data. We per-
formed 100 tests with ERATER as the sole
variable leaving out one essay each time and
recorded the prediction of the model for that
essay. We repeated the procedure using both
variables. The predicted values for ERATER
alone and ERATER+ROUGH are shown in
columns PrH/E and PrH/E+R respectively
in Table 4. In comparing the predictions, we
observe that, indeed, 57 % of the predicted
values shown in the PrH/E+R column are
better approximations of the HUMAN scores,
especially in the cases where the ERATER's
score is discrepant by 2 points from the HU-
MAN score.
8 Discussion
Our positive nding, namely that Centering
Theory's measure of relative proportion of
Rough-Shift transitions is indeed a signi-
cant contributor to the accuracy of computer-
generated essay scores, has several practical
and theoretical implications. Clearly, it in-
dicates that adding a local coherence feature
to e-rater could signicantly improve e-rater's
scoring accuracy. Note, however, that over-
all scores and coherence scores need not be
strongly correlated. Indeed, our data contain
several examples of essays with high coher-
ence scores but low overall scores and vice
versa.
We briey reviewed these cases with several
ETS writing assessment experts to gain their
insights into the value of pursuing this work
further. In an eort to maximize the use of
their time with us, we carefully selected three
pairs of essays to elicit specic information.
One pair included two high-scoring (6) essays,
one with a high coherence score and the other
with a low coherence score. Another pair in-
cluded two essays with low coherence scores
but diering overall scores (a 5 and a 6). A
nal pair was carefully chosen to include one
essay with an overall score of 3 that made
several main points but did not develop them
fully or coherently, and another essay with an
overall score of 4 that made only one main
point but did develop it fully and coherently.
After briey describing the Rough-Shift co-
herence measure and without revealing either
the overall scores or the coherence scores of
the essay pairs, we asked our experts for their
comments on the overall scores and coherence
of the essays. In all cases, our experts pre-
cisely identied the scores the essays had been
given. In the rst case, they agreed with the
high Centering coherence measure, but one
expert disagreed with the low Centering co-
herence measure. For that essay, one expert
noted that "coherence comes and goes" while
another found coherence in a "chronological
organization of examples" (a notion beyond
the domain of Centering Theory). In the sec-
ond case, our experts' judgments conrmed
the Rough-Shift coherence measure. In the
third case, our experts specically identied
both the coherence and the development as-
pects as determinants of the essays' scores. In
general, our experts felt that the development
of an automated coherence measure would be
a useful instructional aid.
The advantage of the Rough-Shift metric
over other quantied components of the e-
rater is that it can be appropriately translated
into instructive feedback for the student. In
an interactive tutorial system, segments con-
taining Rough-Shift transitions can be high-
lighted and supplementary instructional com-
ments will guide the student into revising the
relevant section paying attention to topic dis-
continuities.
9 Future work
Our study prescribes a route for several fu-
ture research projects. Some, such as the
need to improve on fully automated tech-
niques for noun phrase/discourse entity iden-
tication and coreference resolution, are es-
sential for converting this measure of local co-
herence to a fully automated procedure. Oth-
ers, not explicitly discussed here, such as the
status of discourse deictic expressions, nom-
inalization resolution, and global coherence
studies are fair game for basic, theoretical re-
search.
Acknowledgements
We would like to thank Jill Burstein who provided us
with the essay set and human and e-rater scores used
in this study; Mary Fowles, Peter Cooper, and Seth
Weiner who provided us with the valuable insights
of their writing assessment expertise; Henry Brown
who kindly discussed some statistical issues with us;
Ramin Hemat who provided perl code for automati-
cally computing Centering transitions and the Rough-
Shift measure for each essay. We are grateful to Ar-
avind Joshi and Alistair Knott for useful discussions.
References
S. Brennan, M. Walker-Friedman, and C. Pollard.
1987. A Centering approach to pronouns. In Pro-
ceedings of the 25th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 155{162.
Stanford, Calif.
J. Burstein, K. Kukich, S. Wol, M. Chodorow,
L. Braden-Harder, M.D. Harris, and C. Lu. 1998.
Automated essay scoring using a hybrid feature
identication technique. In Annual Meeting of the
Association for Computational Linguistics, Mon-
treal, Canada, August.
B. Di Eugenio. 1998. Centering in Italian. In Center-
ing Theory in Discourse, pages 115{137. Clarendon
Press, Oxford.
B. Grosz and C. Sidner. 1986. Attentions, intentions
and the structure of discourse. Computational Lin-
guistics, 12:175{204.
B. Grosz, A. Joshi, and S. Weinstein. 1983. Provid-
ing a unied account of denite noun phrases in
discourse. In Annual Meeting of the Association
for Computational Linguistics, pages 44{50.
B. Grosz. 1977. The representation and use of focus
in language underastanding. Technical Report No.
151, Menlo Park, Calif., SRI International.
M. Hearst. 1994. Multiparagraph segmentation of
expository text. In Proc. of the 32nd ACL.
L. Horn. 1986. Presupposition, theme and variations.
In Chicago Linguistics Society, volume 22, pages
168{192.
S. Hudson-D'Zmura. 1988. The Structure of Dis-
course and Anaphor Resolution: The Discourse
Center and the Roles of Nouns and Pronouns.
Ph.D. thesis, University of Rochester.
F. Hurewitz. 1998. A quantitative look at discourse
coherence. In M. Walker, A. Joshi, and E. Prince,
editors, Centering Theory in Discourse, chapter 14.
Clarendon Press, Oxford.
A. Joshi and S. Kuhn. 1979. Centered logic: The
role of entity centered sentence representation in
natural language inferencing. In 6th International
Joint Conference on Articial Intelligence, pages
435{439.
A. Joshi and S. Weinstein. 1981. Control of infer-
ence: Role of some aspects of discourse structure:
Centering. In 7th International Joint Conference
on Articial Intelligence, pages 385{387.
M. Kameyama. 1985. Zero Anaphora: The Case of
Japanese. Ph.D. thesis, Stanford University.
M. Kameyama. 1998. Intrasentential Centering: A
case study. In M. Walker, A. Joshi, and E. Prince,
editors, Centering Theory in Discourse, pages 89{
112. Clarendon Press: Oxford.
H. Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Proc. of the 31st ACL
(Student Session), pages 286{288.
E. Miltsakaki. 1999. Locating topics in text process-
ing. In Proceedings of Computational Linguistics in
the Netherlands (CLIN'99).
J. Morris and G. Hirst. 1991. Lexical cohesion com-
puted by thesaural relations as an indicator of the
structure of the text. Computational Linguistics,
17:21{28.
E. B. Page and N. Peterson. 1995. The computer
moves into essay grading: Updating the ancient
test. Phi Delta Kappan, March:561{565.
R. Passonneau and D. Litman. 1997. Discourse seg-
mentation by human and automated means. Com-
putational Linguistics, 23(1):103{139.
R. Passonneau. 1998. Interaction of discourse struc-
ture with explicitness of discourse anaphoric noun
phrases. In M. Walker, A. Joshi, and E. Prince,
editors, Centering Theory in Discourse, pages 327{
358. Clarendon Press: Oxford.
T. Reinhart. 1981. Pragmatics and linguistics: An
analysis of sentence topics. Philosophica, 27:53{94.
J. Reynar. 1994. An automatic method of nding
topic boundaaries. In Proc. of 32nd ACL (Studen
Session), pages 331{333.
C. Sidner. 1979. Toward a computational theory of
denite anaphora comprehension in English. Tech-
nical Report No. AI-TR-537, Cambridge, Mass.
MIT Press.
U. Turan. 1995. Null vs. Overt Subjects in Turk-
ish Discourse: A Centering Analysis. Ph.D. thesis,
University of Pennsylvania.
M. Walker and E. Prince. 1995. A bilateral approach
to givenness: A hearer-status algorithm and a Cen-
tering algorithm. In T. Fretheim and J. Gundel,
editors, Reference and Referent Accessibility. Ams-
terdam: John Benjamins.
M. Walker, A. Joshi, and E. Prince (eds). 1998. Cen-
tering Theory in Discourse. Clarendon Press: Ox-
ford.
M. Walker. 1998. Centering : Anaphora resolution
and discourse structure. In M. Walker, A. Joshi,
and E. Prince, editors, Centering Theory in Dis-
course, pages 401{35. Clarendon Press: Oxford.
G. Youmans. 1991. A new tool for discourse ana-
lyis: The vocabulary-management prole. Lan-
guage, 67:763{789.
HUM E-R ROUGH PrH/E PrH/E+R
6 5 15 5.05 5.26
6 6 22 5.9921 5.9928
6 6 15 5.99 6.09
6 6 22 5.9921 5.9928
6 6 24 5.99 5.96
6 4 22 4.13 4.35
6 4 13 4.13 4.46
6 6 28 5.99 5.90
6 5 30 5.0577 5.0594
6 4 30 4.13 4.24
6 4 0 4.13 4.62
6 5 20 5.05 5.19
6 6 21 5.99 6.00
6 6 50 5.99 5.58
6 6 25 5.99 5.94
6 5 21 5.05 5.18
6 6 6 5.99 6.22
6 5 35 5.05 4.98
6 5 25 5.05 5.12
6 5 30 5.057 5.059
5 4 15 4.14 4.46
5 5 7 5.07 5.40
5 4 5 4.14 4.60
5 5 38 5.07 4.96
5 4 40 4.14 4.12
5 5 45 5.07 4.86
5 6 27 6.02 5.95
5 4 30 4.28 4.14
5 5 21 5.07 5.20
5 5 16 5.07 5.27
5 5 20 5.07 5.22
5 6 32 6.02 5.88
5 4 40 4.143 4.148
5 4 10 4.14 4.53
5 4 23 4.14 4.35
5 5 20 5.07 5.22
5 6 25 6.02 5.98
5 4 25 4.14 4.33
5 5 50 5.07 4.79
5 6 10 6.02 6.20
4 3 11 3.22 3.71
4 5 45 5.09 4.88
4 4 46 4.15 4.04
4 3 50 3.22 3.17
4 3 36 3.22 3.37
4 3 33 3.22 3.41
4 5 42 5.09 4.92
4 3 50 3.22 3.17
4 4 36 4.15 4.18
4 4 40 4.15 4.13
HUM E-R ROUGH PrH/E PrH/E+R
4 3 11 3.22 3.71
4 3 75 3.22 2.79
4 4 38 4.15 4.16
4 3 62 3.22 3.00
4 4 12 4.15 4.53
4 4 40 4.15 4.13
4 5 48 5.09 4.84
4 3 9 3.22 3.74
4 3 81 3.22 2.69
4 3 100 3.22 2.34
3 3 55 3.24 3.11
3 4 30 4.16 4.28
3 4 81 4.16 3.59
3 4 42 4.16 4.11
3 3 50 3.24 3.18
3 3 66 3.24 2.96
3 3 42 3.24 3.30
3 2 40 2.30 2.50
3 3 75 3.24 2.83
3 3 40 3.24 3.33
3 3 78 3.24 2.78
3 3 62 3.24 3.02
3 2 55 2.30 2.29
3 2 30 2.30 2.64
3 3 ? 3.29 ?
3 5 45 5.11 4.91
3 3 80 3.24 2.75
3 2 37 2.30 2.54
3 3 75 3.24 2.83
3 2 50 2.30 2.36
2 2 67 2.32 2.14
2 2 67 2.32 2.14
2 4 78 4.17 3.68
2 3 67 3.25 2.97
2 3 41 3.25 3.33
2 2 ? 2.32 ?
2 1 67 1.37 1.30
2 2 20 2.32 2.84
2 2 42 2.32 2.50
2 2 50 2.32 2.39
1 2 50 2.35 2.41
1 2 0 2.35 3.29
1 1 67 1.42 1.35
1 3 71 3.26 2.95
1 3 57 3.26 3.12
1 0 100 0.44 -0.03
1 1 85 1.42 1.09
1 1 67 1.42 1.35
1 2 57 2.35 2.31
1 1 0 1.42 2.48
Table 4: Table with the human scores (HUM), the e-rater scores (E-R), the Rough-Shift measure (ROUGH),
the (jacknifed) predicted values using e-rater as the only variable (PrH/E) and the (jacknifed) predicted values
using the e-rater and the added variable Rough-Shift (PrH/E+R). The ROUGH measure is the percentage of
Rough-Shifts over the total number of identied transitions. The question mark appears where no transitions
were identied.
Annotation and Data Mining of the Penn Discourse TreeBank
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104 USA
rjprasad@linc.cis.upenn.edu
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@linc.cis.upenn.edu
Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
joshi@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
The Penn Discourse TreeBank (PDTB) is a new re-
source built on top of the Penn Wall Street Journal
corpus, in which discourse connectives are anno-
tated along with their arguments. Its use of stand-
off annotation allows integration with a stand-off
version of the Penn TreeBank (syntactic structure)
and PropBank (verbs and their arguments), which
adds value for both linguistic discovery and dis-
course modeling. Here we describe the PDTB and
some experiments in linguistic discovery based on
the PDTB alone, as well as on the linked PTB and
PDTB corpora.
1 Introduction
Large scale annotated corpora such as the Penn
TreeBank (Marcus et al, 1993) have played a cen-
tral role in speech and natural language research.
However, with the demand for more powerful NLP
applications comes a need for greater richness in
annotation ? hence, the development of PropBank
(Kingsbury and Palmer, 2002), which adds basic se-
mantics to the PTB in the form of verb predicate-
argument annotation and eventually similar annota-
tion of nominalizations. We have been developing
yet another annotation layer above these both. The
Penn Discourse TreeBank (PDTB) adds low-level
discourse structure and semantics through the anno-
tation of discourse connectives and their arguments,
using connective-specific semantic role labels. With
this added knowledge, the PDTB (together with the
PTB and PropBank) should support more in-depth
NLP research and more powerful applications.
Work on the PDTB is grounded in a lexical-
ized approach to discourse ? DLTAG (Webber and
Joshi, 1998; Webber et al, 1999a; Webber et al,
2000; Webber et al, 2003). Here, low-level dis-
course structure and semantics are taken to re-
sult (in part) from composing elementary predicate-
argument relations whose predicates come mainly
from discourse connectives1 and whose arguments
1Despite this, we have deliberately adopted a policy of hav-
come from units of discourse ? clausal, sentential
or multi-sentential units. The PDTB therefore dif-
fers from the RST-annotated corpus (Carlson et al,
2003) which starts with (abstract) rhetorical rela-
tions (Mann and Thompson, 1988) and annotates a
subset of the Penn WSJ corpus with those relations
that can be taken to hold between (primarily) pairs
of discourse spans identified in the corpus.
The current paper focuses on what can be dis-
covered through analyzing PDTB annotation, both
on its own and together with the Penn TreeBank.
Section 2 of the paper briefly reviews the theo-
retical background of the project, its current state,
the guidelines given to annotators, the annotation
tool they used (WordFreak), and the extent of inter-
annotator agreement. Section 3 shows how we have
used PDTB annotation, along with the PTB, to ex-
tract several features pertaining to discourse con-
nectives and their arguments, and discusses the rel-
evance of these features for NLP research and ap-
plications. Section 4 concludes with the summary.
2 Project overview
2.1 Theoretical background
The PDTB project builds on basic ideas presented
in Webber and Joshi (1998), Webber et al (1999b)
and Webber et al (2003) ? that connectives are
discourse-level predicates which project predicate-
argument structure on a par with verbs at the sen-
tence level. Webber and Joshi (1998) propose a
tree-adjoining grammar for discourse (DLTAG) in
which compositional aspects of discourse meaning
are formally defined, thus teasing apart composi-
tional from non-compositional layers of meaning.
In this framework, connectives are grouped into nat-
ural classes depending on the structure that they
project at the discourse level. Subordinate and coor-
dinating conjunctions, for example, require two ar-
ing the annotations independent of the DLTAG structural de-
scriptions for two reasons: (1) to make the annotated cor-
pus useful to researchers working in different frameworks and
(2) to simplify the annotators? task, thereby increasing inter-
annotator reliability.
guments that can be identified structurally from ad-
jacent units of discourse. What Webber et al (2003)
call anaphoric connectives (discourse adverbials,
such as otherwise, instead, furthermore, etc.) also
require two arguments ? one derived structurally,
and the other derived anaphorically from the pre-
ceding discourse. The crucial contribution of this
framework to the design of PDTB is what can be
seen as a bottom-up approach to discourse structure.
Specifically, instead of appealing to an abstract (and
arbitrary) set of discourse relations whose identifi-
cation may confound multiple sources of discourse
meaning, we start with the annotation of discourse
connectives and their arguments, thus exposing a
clearly defined level of discourse representation.
2.2 Project description
The PTDB project began in November 2002. The
first phase, including pilot annotations and prelim-
inary development of guidelines, was completed in
May 2003, and we expect to release the PDTB by
November 2005. Intermediate versions of the an-
notated corpus will be made available for feedback
from the community.
The PDTB corpus will include annotations of
four types of connectives: subordinating and co-
ordinating conjunctions, adverbial connectives and
implicit connectives. The final number of annota-
tions will amount to approximately 30K: 20K anno-
tations of the 250 types explicit connectives identi-
fied in the corpus and 10K annotations of implicit
connectives. The final version of the corpus will
also characterize the semantic role of each argu-
ment.
To date, we have annotated 10 explicit connec-
tives (therefore, as a result, instead, otherwise, nev-
ertheless, because, although, even though, when, so
that), amounting to a total of 2717 annotations, as
well as 386 tokens of implicit connectives. Anno-
tations have been performed by two to four annota-
tors.
2.3 Annotation guidelines
The annotation guidelines for PDTB have
been revised considerably since the pilot
phase of the project in May 2003. The cur-
rent version of the guidelines is available at
http://www.cis.upenn.edu/   pdtb. Below
we outline basic points from the guidelines.
What counts as a discourse connective? We
count as discourse connectives (1) all subordinat-
ing and coordinating conjunctions, (2) all discourse
adverbials, and (3) all inter-sentential implicit con-
nectives. Discourse adverbials include only those
adverbials which convey relationships between two
abstract objects such as events, states, propositions,
etc. (Asher, 1993). For instance, in Example 1, as
a result conveys a cause-effect relation between the
event of limiting the size of industries and that of
industries operating out of small, expensive, and in-
efficient units. In contrast, the semantic interpreta-
tion of the clausal adverbial strangely in Example 2
only requires a single event/state which it classifies
in the set of strange events/states.2
(1) [In the past, the socialist policies of the govern-
ment strictly limited the size of new steel mills,
petrochemical plants, car factories and other in-
dustrial concerns to conserve resources and re-
strict the profits businessmen could make]. As
a result, [industry operated out of small, expen-
sive, highly inefficient industrial units].
(2) Strangely, conventional wisdom inside the Belt-
way regards these transfer payments as ?uncon-
trollable? or ?nondiscretionary.?
Implicit connectives are taken to occur between
adjacent sentences not related by any explicit con-
nective. They are annotated with whatever explicit
connective the annotator feels could be inserted,
with the original meaning retained. Assessment of
inter-annotator agreement groups these annotations
into five coarse classes (Miltsakaki et al, 2004).
Currently, we are not annotating implicit connec-
tives intra-sententially (such as between a main
clause and a free adjunct) or across paragraphs.
What counts as a legal argument? The sim-
plest argument to a connective is what we take to
be the minimum unit of discourse. Because we
take discourse relations to hold between abstract
objects, we require that an argument contain at least
one clause-level predication (usually a verb ? tensed
or untensed), though it may span as much as a se-
quence of clauses or sentences. The two exceptions
are nominal phrases that express an event or a state,
and discourse deictics that denote an abstract ob-
ject.
What we describe to annotators as arguments to
discourse connectives are actually the textual span
from which the argument is derived (Webber et al,
1999a; Webber et al, 2003). This is especially clear
in the case of the first argument of instead in (3),
which does not actually include the negation, al-
though it is part of the selected text.3
2For a more detailed discussion of how discourse adver-
bials can be distinguished from clausal adverbials, see Forbes
(2003).
3For a corpus-based study of the arguments of instead, see
(Miltsakaki et al, 2003).
(3) [No price for the new shares has been set]. In-
stead, [the companies will leave it up to the mar-
ketplace to decide].
How far does an argument extend? One par-
ticularly significant addition to the guidelines came
as a result of differences among annotators as to
how large a span constituted the argument of a con-
nective. During pilot annotations, annotators used
three annotation tags: CONN for the connective
and ARG1 and ARG2 for the two arguments. To
this set, we have added two optional tags, SUP1
and SUP2 (supplementary), for cases when the an-
notator wants to mark textual spans s/he considers
to be useful, supplementary information for the in-
terpretation of an argument. Examples (4) and (5)
demonstrate its use. The arguments are shown in
square brackets, while spans providing supplemen-
tary information are shown in parentheses.4
(4) Although [started in 1965], [Wedtech didn?t re-
ally get rolling until 1975] (when Mr. Neuberger
discovered the Federal Government?s Section 8
minority business program).
(5) Because [mutual fund trades don?t take effect un-
til the market close] (? in this case, at 4 p.m. ?)
[these shareholders effectively stayed put].
2.4 Inter-Annotation Reliability
An extensive discussion of inter-annotator reliabil-
ity in the PDTB is presented in (Miltsakaki et al,
2004). The three things that are relevant to the dis-
cussion here of using the PDTB for linguistic dis-
covery are (1) the agreement criterion, (2) the level
of inter-annotator agreement, and (3) the types of
inter-annotator variation.
With respect to agreement, we did not use the
kappa statistic (Siegel and Castellan, 1988) because
it requires the data tokens to be classified into dis-
crete categories and PDTB annotation involves se-
lecting a span of text whose length is not prescribed
a priori.5 Instead of kappa, we assessed inter-
annotator agreement using an exact match crite-
rion: for any ARG1 or ARG2 token, agreement was
recorded as 1 when both annotators made identical
4SUP annotations have not been used in the current
experiments.
5Carlson et al (2003) avoid this by using two sets of cat-
egories: one set in which there is a separate category for each
span that could constitute an elementary discourse unit, and one
set in which there is only a separate category for each span that
at least one annotator has selected. Because the arguments of
connectives tend to be longer and hence more variable than the
elementary spans used in the RST-corpus, we do not see any
gain from introducing the first set of categories, and the second
set is equivalent to our exact match criterion.
textual selections for the annotation and 0 when the
annotators made non-identical selections.
Treating ARG1 and ARG2 annotations as inde-
pendent tokens for assessment, the total number of
inter-annotator judgments assessed for explicit con-
nectives was twice the number of connective tokens,
i.e, 5434. In this measure, we achieved a high-level
of agreement on the arguments to subordinate con-
junctions (92.4%), while lower agreement on ad-
verbials (71.8%).6 This difference between the two
types is not surprising, since locating the anaphoric
(ARG1) argument of adverbial connectives is be-
lieved to be a harder task than that of locating the
arguments of subordinating conjunctions. For ex-
ample, the anaphoric argument of the adverbial con-
nectives may be located in some non-adjacent span
of text, even several paragraphs away.
A detailed analysis of inter-annotator variation
shows that most of the disagreements (79%) in-
volved Partial Overlap ? that is, text that is com-
mon to what is selected separately by each annota-
tor. Partial overlap subsumes categories such as (a)
higher verb, where one of the annotators included
some extra clausal material that contained a higher
governing predicate, (b) dependent clause, where
one of the annotators included extra clausal mate-
rial which was syntactically dependent on the clause
selected by both, and (c) parenthetical, where one
of the annotators included text that occurred in the
middle of the other annotator?s selection. Example 6
illustrates a case of higher verb disagreement.
(6) a. [he knew the RDF was neither rapid nor de-
ployable nor a force] ? even though [it cost
$8 billion or $10 billion a year].
b. he knew [the RDF was neither rapid nor de-
ployable nor a force] ? even though [it cost
$8 billion or $10 billion a year].
The partial overlap disagreements are important
with respect to the experiments described in the next
section, because most of this variation turns out to
be irrelevant to the experiments. We will elaborate
on this further in the next section.
3 Data Mining
PDTB annotation indicates two things: the argu-
ments of each explicit discourse connective and the
lexical tokens that actually play a role as discourse
connectives. It should be clear that the former
6In Miltsakaki et al (2004), we have reported on the anno-
tation of implicit connectives as well. We achieved 72% agree-
ment on the use of explicit expressions in place of the implicit
connectives. More details on the implicit connective annotation
can be found in this work.
cannot be derived automatically from existing re-
sources, since determining the size and location of
the arguments is not simply a matter of sentential
syntax or verb predicate argument relations. But
the latter is also a non-trivial feature because every
lexical item that functions as a discourse connective
also has a range of other functions. While some of
these functions correlate with POS-tags other than
those used in annotating connectives, the PTB POS-
tags themselves cannot always be reliably distin-
guished, given inconsistencies in how the lexical
items are analyzed.
We believe that the PDTB annotation can con-
tribute to a range of linguistic discovery and lan-
guage modeling tasks, such as
 providing empirical evidence for the DLTAG
claim that discourse adverbials get one argu-
ment anaphorically, while structural connec-
tives such as conjunctions establish relations
between adjacent units of text (Creswell et al,
2002).
 acquiring common usage patterns of connec-
tives and identifying their dependencies, in or-
der to support ?natural? choices in Natural
Language Generation (di Eugenio et al, 1997;
Moser and Moore, 1995; Williams and Reiter,
2003).
 developing decision procedures for resolving
and interpreting discourse adverbials (Milt-
sakaki et al, 2003) which can be built on top of
discourse parsing systems (Forbes et al, 2003).
 developing ?word sense disambiguation? pro-
cedures for distinguishing among different
senses of a connective and hence interpret-
ing connectives correctly (e.g., distinguishing
between temporal and explanatory since, be-
tween hypothetical and counterfactual if, be-
tween epistemic and semantic because, etc.)
 providing empirical evidence for theories of
anaphoric phenomena such as verb phrase el-
lipsis that see them as sensitive to the type of
discourse relation in which they are expressed
(Hardt and Romero, 2002; Kehler, 2002).
The value of carrying out such studies using a sin-
gle corpus with multiple layers of annotation is that
relationships between phenomena are clearer. (The
downside is focusing on a single genre ? newspa-
per text ? and a particular ?house style? ? that of
the Wall Street Journal. However, developing the
PDTB may help facilitate the production of more
such corpora, through an initial pass of automatic
annotation, followed by manual correction, much
as was done in developing the PTB (Marcus et al,
1993).)
Here we present some preliminary experiments
we have carried out on the current version of the
PDTB. We automatically extracted features asso-
ciated with discourse connectives and their argu-
ments, both from the PDTB annotation alone as well
as from the integrated annotation of the PDTB and
PTB. The findings reveal novel patterns regarding
the location and size of the arguments of discourse
connectives and suggest additional experiments.
The multi-layered annotations for PDTB, PTB
(and soon to be available PropBank) are rendered in
XML within a ?stand-off? annotation architecture
in which multiple (independently conducted) anno-
tations refer to the same primary document. Word-
Freak directly renders the PDTB annotations in the
stand-off XML representation, but for the syntactic
layer, the PTB phrase structure constituent annota-
tions had to first be converted to the XML stand-off
representation.7
For preparing the connective tokens for data min-
ing, we started with the 2717 annotations for the
10 explicit connectives reported in Section 2.2 and
extracted those tokens on which we achieved full
?exact match? agreement as well as ?partial over-
lap? agreement on both the arguments (cf. Sec-
tion 2.4). We felt justified in combining both sets
because ?partial overlap? disagreements, which oc-
curred mostly within sentences, did not make any
overall difference to the features that were extracted.
The total number of tokens we obtained from this
was 2688. 51 tokens on this set had to be thrown out
since the official release of the Penn TreeBank did
not have the corresponding syntactic annotations for
these tokens.8 From the remaining 2637 tokens, we
extracted two sets of features, one for adverbials
(229 tokens) and the other for subordinating con-
junctions (2408 tokens).
For the adverbials, we wanted to determine
whether the results reported in earlier work
(Creswell et al, 2002) held up. Among other
things, this work examined whether (1) anaphoric
arguments could be reliably annotated, to facili-
tate the development of robust anaphora resolu-
tion algorithms, and (2) there were differences be-
7Thanks to Jeremy Lacivita for implementing the represen-
tation of PTB in stand-off XML form. The stand-off represen-
tation of PTB will be released together with the PDTB corpus.
8Researchers who are currently conducting or are planning
to conduct multi-layered annotations or experiments with the
Penn TreeBank should be aware that the official release con-
tains more source and PoS-tagged files than the parsed files.
Future annotations of the PDTB will only be performed on texts
that are parsed.
tween the type, size and location of the arguments
of anaphoric (adverbial) connectives and those of
structural connectives.
The high inter-annotator agreement reported in
this earlier study has now been confirmed by the
PDTB annotation (cf. Section 2.4). As for the other,
we automatically extracted some of the same fea-
tures that were hand-annotated in Creswell et al
(2002) to determine the distribution of these con-
nectives with respect to their position (POS) and
the size and location (LOC) of their anaphoric argu-
ments. These features are further described below:
POS: pertains to the position of the connective in
its host argument, i.e., the argument in which it oc-
curs.9 POS can take three defined values: INIT for
argument-initial position (Examples 7-9), MED for
argument-medial position (Examples 10-11), and
FINAL for argument-final position (Examples 12
and 13). Note that the host argument of the con-
nective is a sentence in Example 8 and 9, a VP con-
junct in Example 7, a free adjunct in Example 10,
the main clause of a sentence in Example 11, a sub-
ordinate clause in Example 12, and finally, the first
of the two coordinated sentences in Example 13.
LOC: pertains to the size and location of the
anaphoric argument of the connective. LOC can
take four defined values: SS for when the anaphoric
argument occurs in the same sentence as the con-
nective (Examples 7, 10 and 11), PS for when the
argument occurs in the immediately previous sen-
tence (Examples 12 and 13), PP for when the argu-
ment occurs in the immediately preceding sequence
of sentences (Example 8), and NC for when the ar-
gument occurs in some non-contiguous sentence(s)
(Example 9). A sentence is defined as minimally
a main clause and all of its attached subordinate
clauses, if any. Coordinated main clauses, by this
definition, are treated as separate sentences. Note
that according to the definition of the LOC feature,
the anaphoric argument may constitute the entire
sentence(s), as in Examples 8, 9 and 13, or it may be
part of the sentence(s), as in Examples 7 and 10-12.
An important aspect of the LOC feature is that
it involved the multi-layering of PDTB and PTB,
since the PDTB itself contains no information about
syntactic constituency or even sentence boundaries.
For deriving the LOC feature values, we needed in-
formation not only about the sentence boundaries
of texts, but also about coordinated clause bound-
aries, which requires accessing sentence-internal
constituents.
9We achieved 94.1% agreement on the host argument
(ARG2) annotations.
(7) INIT-SS: Rep. John LaFalce (D., N.Y.) said Mr.
Johnson refused [to testify jointly with Mr. Mul-
ford] and instead [asked to appear after the Trea-
sury official had completed his testimony].
(8) INIT-PP: [But Mr. Treybig questions whether
that will be enough to stop Tandem?s first main-
frame from taking on some of the functions that
large organizations previously sought from Big
Blue?s machines. ?The answer isn?t price re-
ductions, but new systems,? he said]. Never-
theless, [Tandem faces a variety of challenges,
the biggest being that customers generally view
the company?s computers as complementary to
IBM?s mainframes].
(9) INIT-NC: [For years, costume jewelry makers
fought a losing battle]. Jewelry displays in de-
partment stores were often cluttered and unin-
spired. And the merchandise was, well, fake.
As a result, [marketers of faux gems steadily lost
space in department stores to more fashionable
rivals ? cosmetics makers].
(10) MED-SS: Investors usually don?t want [to take
physical delivery of a contract], [preferring in-
stead to profit from its price swings and then end
any obligation to take delivery or make delivery
as it nears expiration].
(11) MED-SS: Although [bond prices weren?t as
volatile on Tuesday trading as stock prices],
[traders nevertheless said action also was much
slower yesterday in the Treasury market].
(12) FIN-PS: Buyers can look forward to double-
digit annual returns if [they are right]. But they
will have disappointing returns or even losses if
[interest rates rise] instead.
(13) FIN-PS: [Tons of delectably rotting potatoes,
barley and wheat will fill damp barns across the
land as thousands of farmers turn the state?s buy-
ers away]. [Many a piglet won?t be born] as a re-
sult, and many a ham will never hang in a butcher
shop.
The distribution of the POS feature values across
the different connectives, given in Table 1, shows
that the connectives in this set occurred predomi-
nantly in the initial position of their host argument.
The question of whether or not these different po-
sitions correlate with any aspect of the informa-
tion structure of the arguments (Forbes et al, 2003;
Kruijff-Korbayova? and Webber, 2001) is, however,
an open one and will need to be explored further
with the PDTB annotations.
INIT MED FIN TOTAL
201 (87.8%) 13 (5.7%) 15 (6.5%) 229
Table 1: Distribution of the Position (POS) of Dis-
course Adverbials
CONN SS PS PP NC Total
nevertheless 3 (9.7%) 17 (54.8%) 3 (9.7%) 8 (25.8%) 31
otherwise 2 (11.1%) 14 (77.8%) 1 (5.6%) 1 (5.6%) 18
as a result 3 (4.8%) 44 (69.8%) 5 (7.9%) 12 (19%) 63
therefore 11 (55%) 7 (35%) 1 (5%) 1 (5%) 20
instead 22 (22.7%) 62 (63.9%) 2 (2.1%) 11 (11.3%) 97
TOTAL 41 (17.9%) 144 (62.9%) 12 (5.2%) 33 (14.4%) 229
Table 2: Distribution for Location (LOC) of Anaphoric Argument of Adverbial Connectives
The distribution of the LOC values across the dif-
ferent connectives is shown in Table 2. We first look
at all the connectives taken together (i.e., the final
TOTAL row) and focus on differences in LOC and
what such differences suggest.
The first thing that is evident from the TOTAL
row in Table 2 is the significant proportion of ARG1
tokens that occur in a position non-adjacent to the
discourse adverbial (NC = 14.4%). This accords
with the results in (Creswell et al, 2002), in terms
of providing evidence that discourse adverbials (un-
like structural connectives) are not getting both their
arguments from structurally defined positions.
The second point that is evident from the TOTAL
row is the significant proportion of ARG1 tokens
in SS location. This includes instances of ARG1
in complement clauses (Example 7), subordinate
clauses (Example 11), relative clauses (both restric-
tive and non-restrictive, as in Example 14), pre-
ceding VP conjuncts (Example 15), and from main
clauses, where the adverbial is attached to a free ad-
junct, as in Example 16.
(14) [  The British pound], [pressured by last week?s
resignations of key Thatcher administration of-
ficials], nevertheless [  rose Monday to $1.5820
from Friday?s $1.5795].10
(15) Appealing to a young audience, [he scraps an
old reference to Ozzie and Harriet] and instead
[quotes the Grateful Dead].
(16) [The transmogrified brokers never let the C-word
cross their lips], instead [stressing such terms as
?safe,? ?insured? and ?guaranteed?].
While one might want to argue that the latter is
no different from adjacent full clauses and hence
should be treated the same as a location in the pre-
vious sentence (i.e., LOC=PS), the other SS cases
provide additional evidence for an anaphoric anal-
ysis of these discourse adverbials since there al-
ready exists a separate structural relation in each
case. Furthermore, in Example 7, the arguments of
the conjunction and, though not yet addressed by
our annotators, differ from the arguments of instead.
10The subscripts on the bracketed spans in this example indi-
cate discontinuous parts of the host argument of nevertheless.
Any attempt to treat instead as a structural connec-
tive will produce a syntactic analysis with crossing
branches ? a source of both theoretical and practical
(parsing) problems (Forbes et al, 2003).
Turning now to the individual analysis of adver-
bials, Table 2 shows that the 4 connectives other
than therefore pattern rather similarly with respect
to the location of the anaphoric argument (SS,
PS, PP, NC). All of them except therefore have
their antecedent predominantly in the previous sen-
tence (between 54.8% and 77.8%). The question
is whether the difference in how therefore patterns
? i.e., drawing its antecedent 55% of the time from
the same sentence ? is simply a consequence of hav-
ing such few data points (i.e., only 20) or a matter of
?house style? (with all the examples from the Wall
Street Journal) or a difference that is theoretically
motivated. If the answer lies in house style or the-
ory, then it is relevant to work in natural language
generation. Further annotation and analysis of ad-
verbials and their arguments in the PDTB will pro-
vide more information as to this puzzle.
At the start of this section, we indicated five dif-
ferent areas in which PDTB annotation could con-
tribute to linguistic discovery and language model-
ing. This data mining experiment illustrates the first
three, as well as providing information relevant to
further development of discourse parsing systems
and natural language generation systems. For fu-
ture work, we intend to explore further the extrac-
tion and study of other features related to discourse
adverbials. Two features that we are currently work-
ing to extract automatically pertain to (a) the co-
occurrence of discourse adverbials with other con-
nectives in the host argument, and (b) the syntac-
tic type and depth of the anaphoric arguments, such
as whether the argument was a finite or non-finite
complement clause, a relative clause, or a finite or
non-finite subordinate clause etc.
For the subordinating conjunctions (Table 3), we
extracted features pertaining to the relative position
of the two arguments of the conjunction. Subordi-
nating conjunctions often take their arguments in
the same sentence with the subordinate clause as
one argument and the main clause as its other ar-
gument. However, the subordinate clause can either
occur to the right of the main clause, i.e., postposed,
as in Example 17, or it can occur preposed, i.e., be-
fore the main clause, as in Example 18.
(17) ARG1-ARG2: But Sen. McCain says [Mr.
Keating broke off their friendship abruptly in
1987], because [the senator refused to press the
thrift executive?s case as vigorously as Mr. Keat-
ing wanted].
(18) ARG2-ARG1: Because [Swiss and EC insurers
are widely present on each other?s markets], [the
accord isn?t expected to substantially increase
near-term competition].
The distribution of the relative position of the
arguments of these connectives, given in Table 3,
shows significant differences across the connec-
tives.
CONN ARG1-ARG2 ARG2-ARG1 Total
when 545 (54%) 465 (46%) 1010
because 822 (90%) 93 (10%) 915
even though 77 (75%) 26 (25%) 103
although 129 (37%) 218 (63%) 347
so that 33 (100%) 0 (0%) 33
Total 1606 (67%) 802 (33%) 2408
Table 3: Distribution for Argument order for Subor-
dinating Conjunctions
There are a few interesting things to note here.
First, even if one considers only the four subordi-
nating conjunctions with  100 tokens, no two of
them pattern in the same way.
Second, with when, the almost equal distribution
of preposed and postposed tokens suggests either
free variation of the two patterns or different uses
of the two patterns, with each use favoring a differ-
ent pattern. The latter would accord with a theo-
retical distinction that has been made between post-
posed when expressing a purely temporal relation
between the two clauses, and preposed when ex-
pressing a contingent relation between them (Moens
and Steedman, 1988). Integrated evidence from the
PTB and PropBank may help distinguish the two
possibilities.
Third, there is a striking contrast between the pat-
terning of although and even though, especially if
one assumes that even though (like even when, even
after, even if, etc.) involves application of the topi-
calizer even to the subordinate clause, just as it can
apply to other constituents. Further annotation and
analysis of the PDTB will reveal whether all subor-
dinating conjunctions that co-occur with even pat-
tern like even though, or whether this is specific to
the concessive.
Finally, when Williams and Reiter (2003) exam-
ined 342 texts from the RST annotation of the Penn
TreeBank corpus (Carlson et al, 2003), they re-
ported that 77% of the instances of concessive re-
lations that they examined appeared in the order
ARG2-ARG1. (The eleven instances of although
that they examined and the three instances of even
though appeared in concessive relations, along with
instances of but, despite, however, etc.) If we were
to collapse together all instances of although and
even though annotated in the PDTB (totalling 450),
we would find that 46% (206) patterned as ARG1-
ARG2, and 54% of them (244) patterned as ARG2-
ARG1. This might lead us to draw a similar con-
clusion to Williams and Reiter (2003). But it would
also disguise the fact noted above that although and
even though pattern oppositely to one another. This
suggests (1) that making the feature extraction pro-
cedure specific to particular connectives, as in the
PDTB, will reveal distributional patterns that are
lost when more abstract relations are the focus of the
annotation, and (2) that a larger set of annotated to-
kens can show more reliable distributional patterns.
In sum, data mining of PDTB with respect to sub-
ordinating conjunctions has shown radically differ-
ent distribution patterns regarding the relative po-
sition of the arguments. Some of these have con-
firmed and strengthened previous theoretical claims
and some have suggested new and promising re-
search directions. Further work in this area will also
be extremely relevant for NLG sentence planning
components employing discourse relations (Walker
et al (2003), Stent et al (2004), among others),
where the sentence planner needs to make decisions
regarding cue placement. Finally, while our ap-
proach is ?syntactic?, with the distribution of the
connectives and their arguments being explored in
terms of whether they are subordinating conjunc-
tions, coordinating conjunctions, or adverbial con-
nectives, one can also explore the patterning of
connectives in terms of semantic categories, once
their semantic role annotation is complete (cf. Sec-
tion 2.2). The latter could be especially interesting
to cross-linguistic studies of discourse, as well as
to applications such as multilingual generation and
MT are envisaged.11
4 Summary
In this paper we have presented the Penn Dis-
course TreeBank (PDTB), a large-scale discourse-
level annotated corpus that is being developed to-
wards the creation of a multi-layered annotated cor-
pus, integrating the Penn TreeBank, PropBank and
11We thank an anonymous reviewer for pointing this out.
the PDTB. The PDTB encodes low-level discourse
structure information, marking discourse connec-
tives as indicators of discourse relations, and their
arguments. We have reported high inter-annotator
agreement for the PDTB annotation. Our data min-
ing experience and preliminary results show that the
multi-layered corpora is a rich source of information
that can be exploited towards the development of
powerful and efficient natural language understand-
ing and generation systems as well as towards large-
scale corpus-based research.
Acknowledgments
We are very grateful to Tom Morton and Jeremy
Lacivita for the development and modification of
the WordFreak annotation tool. Special thanks to
Jeremy for providing continuous technical support.
Thanks are also due to our annotators, Cassandre
Creswell, Driya Amandita, John Laury, Emily Paw-
ley, Alan Lee, Alex Derenzy and Steve Pettington.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a Discourse-Tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Jan van Kuppevelt and Ronnie Smith, edi-
tors, Current Directions in Discourse and Dialogue.
Kluwer Academic Publishers.
Cassandre Creswell, Katherine Forbes, Eleni Miltsakaki,
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2002. The Discourse Anaphoric Properties of Con-
nectives. In Proceedings of DAARC2002. Edic?o?es
Colibri.
Barbara di Eugenio, Johanna D. Moore, and Massimo
Paolucci. 1997. Learning Features that Predict Cue
Usage. In Proceedings of ACL/EACL 97.
Kate Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop
Sarkar, Aravind Joshi, and Bonnie Webber. 2003. D-
LTAG System: Discourse Parsing with a Lexicalized
Tree-Adjoining Grammar. Journal of Logic, Lan-
guage and Information, 12(3):261?279.
Kate Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics,
University of Pennsylvania.
Dan Hardt and Maribel Romero. 2002. Ellipsis and the
Structure of Discourse. In Proceedings of Sinn und
Bedeutung VI.
Andrew Kehler. 2002. Coherence, Reference and the
Theory of Grammar. CSLI Publications.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of LREC-02.
Ivana Kruijff-Korbayova? and Bonnie Webber. 2001. In-
formation Structure and the Semantics of ?otherwise?.
In Proceedings of ESSLLI 2001: Workshop on Infor-
mation Structure, Discourse Structure and Discourse
Semantics.
William Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory. Toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Eleni Miltsakaki, Cassandre Creswell, Kate Forbes, Ar-
avind Joshi, and Bonnie Webber. 2003. Anaphoric
Arguments of Discourse Connectives: Semantic
Properties of Antecedents versus Non-Antecedents.
In Proceedings of the Computational Treatment of
Anaphora Workshop, EACL 2003.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating Discourse Con-
nectives and their Arguments. In Proceedings of the
NAACL/HLT Workshop: Frontiers in Corpus Annota-
tion.
Marc Moens and Mark Steedman. 1988. Temporal On-
tology and Temporal Reference. Computational Lin-
guistics, 14(2):15?28.
Megan G. Moser and Johanna Moore. 1995. Investi-
gating Cue Selection and Placement in Tutorial Dis-
course. In Proceedings of ACL95.
Sidney Siegel and N. J. Castellan. 1988. Nonparama-
teric Statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of ACL-2004.
Marilyn Walker, Rashmi Prasad, and Amanda Stent.
2003. A Trainable Generator for Recommendations
in Multimodal Dialogue. In Eurospeech, 2003.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
Lexicalized Tree-Adjoining Grammar for Discourse.
In ACL/COLING Workshop on Discourse Relations
and Discourse Markers, Montreal.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999a. Discourse Relations: A Struc-
tural and Presuppositional Account Using Lexicalized
TAG. In Proceedings of ACL-99.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999b. What are Little Texts Made of? A
Structural and Presuppositional Account Using Lex-
icalized TAG. In Proceedings of the International
Workshop on Levels of Representation in Discourse
(LORID ?99).
Bonnie Webber, Alistair Knott, and Aravind Joshi.
2000. Multiple Discourse Connectives in a Lexi-
calized Grammar for Discourse. In Proceedings of
IWCS-00.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and Discourse Struc-
ture. Computational Linguistics, 29:545?587.
Sandra Williams and Ehud Reiter. 2003. A Corpus
Analysis of Discourse Relations for Natural Language
Generation. In Proceedings of Corpus Linguistics.
Annotating Discourse Connectives And Their Arguments
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, PA 19104 USA
elenimi@linc.cis.upenn.edu
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104 USA
rjprasad@linc.cis.upenn.edu
Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
joshi@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
This paper describes a new, large scale
discourse-level annotation project ? the Penn
Discourse TreeBank (PDTB). We present an
approach to annotating a level of discourse
structure that is based on identifying discourse
connectives and their arguments. The PDTB
is being built directly on top of the Penn Tree-
Bank and Propbank, thus supporting the extrac-
tion of useful syntactic and semantic features
and providing a richer substrate for the devel-
opment and evaluation of practical algorithms.
We provide a detailed preliminary analysis of
inter-annotator agreement ? both the level of
agreement and the types of inter-annotator vari-
ation.
1 Introduction
Large scale annotated corpora have played a critical role
in speech and natural language research. The Penn Tree-
Bank (PTB) is an example of such a resource with world-
wide impact on natural language processing (Marcus et
al., 1993). However, the PTB deals with text only at
the sentence level: with the demand for more power-
ful NLP applications comes a need for greater richness
in annotation. At the sentence level, Penn Propbank
is adding predicate-argument annotation to sentences in
PTB (Kingsbury and Palmer, 2002). At the discourse-
level are efforts to produce corpora annotated with rhetor-
ical relations (Carlson et al, 2003). This paper describes
a more basic discourse-level annotation project ? the
Penn Discourse TreeBank (PDTB) ? that aims to produce
a large-scale corpus in which discourse connectives are
annotated, along with their arguments.
There have been several approaches to describing dis-
course in terms of discourse relations (Mann and Thomp-
son, 1988; Asher and Lascarides, 1998; Polanyi and
van den Berg, 1996). In these approaches, the additional
meaning the discourse contributes beyond the sentence
derives from discourse relations. Specification of the dis-
course relations for a discourse thus constitutes a descrip-
tion of a certain level of discourse structure.
Rather than starting from (abstract) discourse rela-
tions, we describe an approach to annotating a large-
scale corpus in terms of a more basic characterisation
of discourse structure in terms of discourse connectives
and their arguments. The motivation for such an ap-
proach stems from work by Webber and Joshi (1998),
Webber et al (1999a), Webber et al (2000) which inte-
grates sentence level structures with discourse level struc-
ture (using tree-adjoining grammars for both cases, LTAG
and DLTAG, respectively).1 This allows structural com-
position and its associated semantic composition at the
sentence level to be smoothly carried over to the dis-
course level, a goal also shared by Gardent (1997),
Schilder (1997) and Polanyi and van den Berg (1996),
among others.2
Discourse connectives and their arguments can be suc-
cessfully annotated with high reliability (cf. Section
4). This is not surprising, given that the task resem-
bles that of annotating verbs and their arguments at
the sentence level (Kingsbury and Palmer, 2002). In
fact, we use a fine-grained, lexically grounded annota-
tion in which argument labels are specific to the dis-
1In the PDTB annotations, we have deliberately adopted
a policy to make the annotations independent of the DLTAG
framework for two reasons: (1) to make the annotated corpus
widely useful to researchers working in different frameworks
and (2) to make the annotators? task easier, thereby increasing
interannotator reliability.
2However, the approaches in Gardent (1997),
Schilder (1997), and Polanyi and van den Berg (1996) are
different in two ways: a) the process by which discourse
derives compositional aspects of meaning is considered entirely
separate from how clauses do so, and b) only two mechanisms
are used for deriving discourse semantics ? compositional
semantics and inference.
course connectives involved, in much the same way as
in Kingsbury and Palmer (2002). In contrast, a recent
attempt (Carlson et al, 2003) at using RST-type rela-
tions for annotating a much smaller corpus has already
revealed difficulties involved in reliably annotating more
abstract discourse relations. Moreover, this type of anno-
tation does not contain any record of the basis on which
a relation was assigned.
The paper is organized as follows. Section 2 provides
a brief overview of the fundamental ideas that provide
the basis for the design of the PDTB annotation. Section
3 gives a detailed description of the annotation project,
including information about the size of the corpus, com-
pleted annotations as well as annotation instructions as
formulated in the guidelines. Section 4 presents data
analysis based on current annotations as well as results
from inter-annotator agreement. Section 5 wraps up with
a summary of the work.
2 Theoretical background
The annotation project presented in this paper builds
on basic ideas presented in Webber and Joshi (1998),
Webber et al (1999b) and Webber et al (2003) ? that
connectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at the
sentence level. Webber and Joshi (1998) propose a tree-
adjoining grammar for discourse (DLTAG) in which
compositional aspects of discourse meaning are for-
mally defined, thus teasing apart compositional from non-
compositional layers of meaning. In this framework, con-
nectives are grouped into natural classes depending on the
structure that they project at the discourse level. Subordi-
nate and coordinating conjunctions, for example, require
two arguments that can be identified structurally from ad-
jacent units of discourse. What Webber et al (2003) call
anaphoric discourse connectives (some, but not all, dis-
course adverbials, such as ?otherwise?, ?instead?, ?fur-
thermore?, etc.) also require two arguments, but only one
of them derives structurally. For the complete interpreta-
tion of these connectives, their other argument needs to
be recovered. The crucial contribution of this framework
to the design of the current project is what can be seen
as a bottom-up approach to discourse structure. Specifi-
cally, instead of appealing to an abstract (and arbitrary)
set of discourse relations whose identification involves
confounding multiple sources of discourse meaning, we
start with the annotation of discourse connectives and
their arguments, thus exposing a clearly defined level of
discourse representation.
3 Project description
The PTDB project began in November 2002. The first
phase, including pilot annotations and preliminary devel-
opment of guidelines, was completed in May 2003. The
PDTB is expected to be released by November 2005. In-
termediate versions of the annotated corpus will be made
available for receiving feedback.
The PDTB corpus will include annotations of four
types of connectives: subordinating conjunctions, coor-
dinating conjunctions, adverbial connectives and implicit
connectives. We specify each of these types in more de-
tail in Section 3.1. The final number of annotations in
the corpus will amount to approximately 30,000; 10,000
implicit connectives and 20,000 annotations of the 250
explicit connectives identified in the corpus. The final
version of the corpus will also contain characterizations
of the semantic roles associated with the arguments of
each type of connective.
In this paper we present the results of annotating 10
explicit connectives, amounting to a total of 2717 anno-
tations, as well as 386 tokens of implicit connectives. The
set of 10 connectives comprises the adverbial connectives
?therefore?, ?as a result?, ?instead?, ?otherwise?, ?never-
theless?, and the subordinate conjunctions ?because?, ?al-
though?, ?even though?, ?when?, and ?so that?. In all
cases, annotations have been performed by four annota-
tors. While this slows down the annotation process con-
siderably, the nature, significance and magnitude of the
project as well as the well-known complexity of discourse
annotation tasks impels us to strive for maximum relia-
bility, achieved by having the task performed by multiple
annotators.3
Individual annotation proceeds one connective at a
time. The annotation tool WordFreak4 is used to iden-
tify all instances of the given connective in the corpus,
and these are then annotated independently and manu-
ally by four annotators. This way, the annotators quickly
gain experience with that connective and develop a better
understanding of its predicate-argument characteristics.
Similarly, for the annotation of implicit connectives, all
instances (as specified in the guidelines, see Section 3.2)
are identified one file at a time. For this task, the anno-
tators are required to read the entire file so that they can
make well-informed and reliable decisions about the im-
plicit connectives and their arguments. In addition, after
the arguments of each implicit connective have been iden-
tified, the annotators provide, if possible, an explicit con-
nective (or other suitable expression) that best expresses
the inferred relation. As with explicit connectives, anno-
tations of implicit connectives are done by four annota-
3When inter-annotator consistency has stabilized, we intend
to reduce the number of annotators to three, or maybe two at the
minimum.
4WordFreak was developed by Tom Morton at the University
of Pennsylvania. It has been substantially modified by Jeremy
Lacivita to fit the needs of the PDTB project. A snapshot of the
tool can be seen at http://www.cis.upenn.edu/?pdtb.
tors.
Compared with Propbank?s annotation of verb
predicate-argument structures, annotation of arguments
of discourse predicates is different in interesting ways.
Propbank annotators have to determine the number of ar-
guments required by each verb. In contrast, discourse
connectives exhibit a clear predicate-argument structure
requiring only two arguments. The main challenge we
have discovered for annotating discourse connectives is
determining the extent of their arguments. Even subor-
dinate conjunctions whose arguments never cross a sen-
tence boundary may sometimes be the source of disagree-
ment between annotators.
In what follows, we present a brief overview of the
classes of connectives that we annotate, followed by
highlights of the annotation manual and relevant corpus
examples.
3.1 Discourse connectives
We classify discourse connectives into four classes: sub-
ordinate and coordinating conjunctions, adverbials and
implicit connectives. Examples of each type are given be-
low, with their arguments shown in square brackets and
the connectives, in italics.
3.1.1 Subordinate conjunctions
Subordinate conjunctions introduce clauses that are
syntactically dependent on a main clause. The most com-
mon types of relations that they express are temporal
(e.g., ?when?, ?as soon as?), causal e.g., ?because?), con-
cessive (e.g., ?although?, ?even though?), purpose (e.g.,
?so that?, ?in order that?) and conditional (e.g., ?if?, ?un-
less). Clauses introduced with a subordinate conjunction
may be preposed (or, more rarely, interposed) with re-
spect to the main clause, as shown in (1).
(1) Because [the drought reduced U.S. stockpiles], [they
have more than enough storage space for their new
crop], and that permits them to wait for prices to rise.
3.1.2 Coordinating conjunctions
Coordinating conjunctions are ones such as ?and?,
?but?, and ?or?. Example (2) shows the annotation of an
instance of the conjunction ?and?.
(2) [William Gates and Paul Allen in 1975 developed
an early language-housekeeper system for PCs], and
[Gates became an industry billionaire six years after
IBM adapted one of these versions in 1981].
Instances of coordinating conjunctions which coordi-
nate nominal or other non-clausal constituents are ex-
cluded from annotation. We also exclude cases of VP-
coordination because in such cases the arguments of the
connective can be retrieved automatically from the syn-
tactic layer.
3.1.3 Adverbial connectives
Adverbial connectives are sentence-modifying adverbs
which express a discourse relation (Forbes, 2003). The
class of adverbial connectives includes ?however?, ?there-
fore?, ?then?, ?otherwise?, etc. In this class, we have also
included prepositional phrases with a similar sentence
modifying function such as ?as a result?, ?in addition?,
?in fact?, etc. Example (3) shows the annotation of an
instance of the adverbial connective ?as a result?.
(3) ...[many analysts expected energy prices to rise at the
consumer level too]. As a result, [many economists
were expecting the consumer price index to increase
significantly more than it did].
The arguments of adverbial connectives may or may
not be adjacent to the sentence containing the connective.
In a few cases, an argument may be found one or two
paragraphs away from the connective.
3.1.4 Implicit connectives
Implicit connectives are identified between adjacent
sentences with no explicit connectives.5 The annotation
of implicit connectives is intended to capture the connec-
tion between two sentences appearing in adjacent posi-
tions. For example, in (4), the two adjacent sentences
are connected in a way similar to having the explicit con-
nective ?but? contrasting them. Indeed, for implicit con-
nectives, annotators are asked to provide, when possible,
an explicit connective that best describes the inferred re-
lation. The explicit connective provided in (4) was ?in
contrast?.
(4) ...[The $6 billion that some 40 companies are looking to
raise in the year ending March 31 compares with only
$2.7 billion raised on the capital market in the previous
fiscal year]. IMPLICIT-(In contrast) [In fiscal 1984 be-
fore Mr. Gandhi came to power, only $810 million was
raised].
3.2 Annotation guidelines
The annotation guidelines for PDTB have been revised
considerably since the pilot phase of the project in May
2003. The current version of the guidelines is available at
http://www.cis.upenn.edu/?pdtb. Below we out-
line the basic points.
3.2.1 What counts as a discourse connective?
We count as discourse connectives (1) all subordinat-
ing and coordinating conjunctions, (2) certain adverbials,
and (3) implicit connectives. The adverbials include only
those which convey a relation between events or states.
For example, in (5) ?as a result? conveys a cause-effect re-
lation between the event of limiting the size of new steel
5There are, of course, other implicit connectives that we are
not taking into account.
mills and that of the industry operating out of small, ex-
pensive and highly inefficient units. In contrast, the se-
mantic interpretation of ?strangely? in (6) only requires a
single event/state which it classifies in the set of strange
events/states.6
(5) [In the past, the socialist policies of the government
strictly limited the size of new steel mills, petrochem-
ical plants, car factories and other industrial concerns to
conserve resources and restrict the profits businessmen
could make]. As a result, industry operated out of small,
expensive, highly inefficient industrial units.
(6) Strangely, conventional wisdom inside the Beltway re-
gards these transfer payments as ?uncontrollable? or
?nondiscretionary.?
The guidelines also highlight instances of lexical items
with multiple functions, only one of which is as a dis-
course connective. For example, ?when? can either serve
as a subordinate conjunction or introduce a relative clause
modifying a nominal phrase, as in (7), where the when-
clause modifies the nominal ?1985?.7Here we again ben-
efit from building discourse annotation on top of Penn
TreeBank because the syntactic annotation of when-
clauses distinguishes the two functions: When-relatives
are marked as NP-modifiers adjoining to an NP, whereas
adverbial when-clauses adjoin to a sentential node.
(7) Attorneys have argued since 1985, when the law took
effect.
Similarly, some since-clauses function as NP modifiers
as shown in (8). In such cases, ?since? is not annotated as
a connective. As in the case of when-clauses, instances of
NP modifying since-clauses can be identified in the Penn
TreeBank by virtue of their syntactic annotation.
(8) In the decade since the communist nation emerged from
isolation, its burgeoning trade with the West has lifted
Hong Kong?s status as a regional business partner.
Finally, implicit connectives count as connectives.
They are identified between adjacent sentences which do
not contain any other explicit connectives. Currently, we
are not annotating implicit connectives intra-sententially,
such as between the matrix clause and free adjunct in Ex-
ample (9). We plan to incorporate annotations of implicit
intra-sentential connectives at a later stage of the project.
(9) Second, they channel monthly mortgage payments into
semiannual payments, reducing the administrative bur-
den on investors.
6For a more detailed discussion of the basis for distin-
guishing discourse adverbials from clausal adverbials, see
Forbes (2003).
7In cases of when-relatives, a when-clause can be annotated
as SUP (see Section 3.2.3).
3.2.2 What counts as a legal argument?
Because we take discourse relations to hold between
abstract objects, we require that an argument contains at
least one predicate along with its arguments. Of course,
a sequence of clauses or sentences may also form a legal
argument, containing multiple predicates.
Because our annotations are done directly on top of
the Penn TreeBank, annotators may select as an argument
certain textual spans that appear to exclude one or more
arguments of the predicate. These are cases in which
these arguments are directly retrievable from the syntac-
tic annotation. Thus, we are able to select only the pred-
icates that are required for the interpretation of the dis-
course connective and simultaneously access their argu-
ments for the complete interpretation of the clause while
keeping the annotations of single arguments simple and
maximally contiguous. In (10), for example, the relative
clause is marked as one of the two arguments of the con-
nective ?even though?. The subject of the verb in the rela-
tive clause is directly retrievable from the Penn TreeBank
annotation. Similarly, in (11) the subject of the infinitival
clause is also available from the syntactic representation.
(10) Workers described ?clouds of dust? [that hung over
parts of the factory] even though [exhaust fans venti-
lated the air].
(11) The average maturity for funds open only to institutions,
considered by some [to be a stronger indicator] because
[those managers watch the market closely], reached a
high point for the year ? 33 days.
There are two exceptions to the requirement that an
argument include a verb ? these are nominal phrases that
express an event or a state, and discourse deictics that
denote an event or state. In (12), for example, the nominal
phrase ?fainting spells? can be marked as a legal argument
of the connective ?when? because the phrase expresses an
event of fainting.
(12) Its symptoms include a cold sweat at the sound of de-
bate, clammy hands in the face of congressional crit-
icism, and [fainting spells] when [someone writes the
word ?controversy.?]
Discourse deictic expressions are forms such as ?this?
and ?that? that can be used to denote the interpretation
of clausal textual spans from the preceding discourse.
In (13), for example, ?that? denotes the interpretation of
the sentence immediately preceding it. Our annotators
are guided to make argument selections that assume that
anaphoric and deictic expressions have been resolved.
Thus, in (13), they are able to select ?That?s? as one ar-
gument of the connective ?because?.
(13) Airline stocks typically sell at a discount of about one-
third to the stock market?s price-earnings ratio ? which
is currently about 13 times earnings. [That?s] because
[airline earnings, like those of auto makers, have been
subject to the cyclical ups-and-downs of the economy].
The annotators are also informed that in some cases,
an argument of a connective must be derived from the
selected textual span (Webber et al, 1999a; Webber et al,
2003). This is the case for the first argument of ?instead?
in (14), which does not include the negation, although it
is contained in the selected text.8
(14) [No price for the new shares has been set]. Instead, [the
companies will leave it up to the marketplace to decide].
In sum, legal arguments can be groups of sentences,
single sentences (a main clause and its subordinate
clauses), single clauses (tensed or non-tensed), NPs that
specify events or situations, and discourse deictic expres-
sions.
3.2.3 How far does an argument extend?
One particularly significant addition to the guidelines
came as a result of differences among annotators as to
how large a span constituted the argument of a connec-
tive. During pilot annotations, annotators used three an-
notation tags: CONN for the connective and ARG1 and
ARG2 for the two arguments. To this set, we have added
the optional tags SUP1, SUP2 (supplementary) for cases
when the annotator wants to mark textual spans s/he con-
siders to be useful, supplementary information for the
interpretation an argument. Example (15) demonstrates
the use of SUP1. Arguments are shown in square brack-
ets, while spans providing supplementary information are
shown in parentheses.
(15) Although [started in 1965], [Wedtech didn?t really get
rolling until 1975] (when Mr. Neuberger discovered the
Federal Government?s Section 8 minority business pro-
gram).
4 Data analysis
To test the reliability of the annotation, we first con-
sidered the kappa statistic (Siegel and Castellan, 1988)
which is used extensively in empirical studies of dis-
course (Carletta, 1996). The kappa coefficient provides
an inter-annotator agreement figure for any number of an-
notators by measuring pairwise agreement between them
and by correcting for chance expected agreement. How-
ever, the statistic requires the data tokens to be classified
into discrete categories, and as a result, we could not ap-
ply it to our data since the PDTB annotation tokens can-
not be classified as such. Rather, annotation in the PDTB
constitutes either selection of a span of text for the ar-
guments of connectives which can be of indeterminate
length or providing explicit expressions for implicit con-
nectives from an open-ended class of expressions.
8For a preliminary corpus-based analysis of the arguments
of ?instead?, see Miltsakaki et al (2003).
Instead, we have assessed inter-annotator agreement in
terms of agreement/disagreement on span or named ex-
pression identity for each token as a percentage of the
pairs of spans or expressions that actually matched ver-
sus those that should have. For the argument annotations,
we use a most conservative measure - the exact match
criterion. In addition, we also used different diagnostics
for the argument annotations for the explicit connectives,
reporting percentage agreement on different classes of to-
kens, such as those in which the first argument (ARG1)
annotations and second argument (ARG2) annotations
were counted independently, as well as those in which the
ARG1 and ARG2 annotations (for each connective) were
counted together as a single token. For all the argument
annotations, the computation of agreement excluded the
supplementary annotations (cf. Section 3.2.3).
We present here agreement results on ARG1 and
ARG2 annotations by two annotators for the annotation
of ten explicit connectives, amounting to a total of 2717
annotations, and 368 annotations of implicit connectives,
including agreement results on the explicit expression the
annotators used in in place of the implicit connectives as
well as the ARG1 and ARG2 annotations of the implicit
connectives.9 The ten explicit connectives include 5 sub-
ordinating conjunctions (when, because, even though, al-
though, and so that) and 5 adverbials (nevertheless, oth-
erwise, instead, therefore, and as a result).
4.1 Inter-annotator Agreement
4.1.1 Explicit connectives
For the explicit connective annotations, we used two
diagnostics for measuring inter-annotator agreement. In
the first diagnostic , we took the class of tokens as the to-
tal number of argument annotations, treating ARG1 and
ARG2 annotations as independent tokens. The total num-
ber of tokens in this class is therefore twice the number
of connective tokens, i.e, 5434. We recorded agreement
using the exact match criterion. That is, for any ARG1
or ARG2 token, agreement was recorded as 1 when both
annotators made identical textual selections for the an-
notation and 0 when the annotators made non-identical
selections.
We achieved 90.2% agreement (4900/5434 tokens)
on the annotations for this class. Agreement on only
ARG1 tokens was 86.3%, and agreement on only ARG2
tokens was 94.1%. Further distribution of the agree-
ments by connective is given in Table 1. Connectives
are grouped in the table by type (subordinating conjunc-
tion (SUBCONJ) and adverbial (ADV)). The second col-
9Right now SUP1 and SUP2 annotations are for our use only
and are not included in the current evaluations. Additional an-
notations by another 2 annotators are currently underway. The
2 annotators of the explicit connectives are different from the 2
annotators of the implicit connectives.
umn gives the number of agreeing tokens for each con-
nective and the third column gives the total number of
(ARG1+ARG2) tokens available for that connective. The
last column gives the percent agreement for the connec-
tive in that row, i.e., as a percentage of tokens for which
agreement was 1 (column 2) versus the total number of
tokens for that connective (column 3).
CONNECTIVES AGR No. Conn. Total %AGR
when 1877 2032 92.4%
because 1703 1824 93.4%
even though 194 206 94.1%
although 635 704 90.1%
so that 66 74 89.2%
TOTAL SUBCONJ 4469 4834 92.4%
nevertheless 56 94 59.6%
otherwise 44 46 95.7%
instead 172 236 72.9%
as a result 110 168 65.5%
therefore 49 56 87.5%
TOTAL ADV. 431 600 71.8%
OVERALL TOTAL 4900 5434 90.2%
Table 1: Distribution of Agreement by Connective, with
ARG1 and ARG2 Annotations Counted Independently
The table shows that we achieved high agreement
on argument annotations of subordinating conjunctions
(92.4%). Average agreement on the adverbials was lower
(71.8%). This difference between the two types is not sur-
prising, since locating the anaphoric (ARG1) argument of
adverbial connectives is believed to be a harder task than
that of locating the arguments of subordinating conjunc-
tions. For example, the anaphoric argument of the ad-
verbial connectives may be located in some non-adjacent
span of text, even several paragraphs away. Arguments of
subordinating conjunctions, on the other hand, can most
often be found in spans of text adjacent to the connective.
The table also shows that there was uniform agreement
across the different subordinating conjunctions (roughly
90%), whereas the adverbials showed more variation.
In particular, agreement on otherwise and therefore was
high (95.7% and 87.5% respectively), while lower for
the other three adverbials, instead (72.9%), as a result
(65.5%), and nevertheless (59.6%). This suggests either
greater variability in how these adverbials are interpreted
or greater complexity in their interpretation, which results
in more variability when people are forced to associate an
interpretation with a particular text span.
We also computed agreement using a second more
conservative diagnostic in which we took the class of to-
kens as the total number of connective tokens (2717) so
that the ARG1 and ARG2 annotations for each connec-
tive were treated together as part of the same token. Here
again, we recorded agreement using the exact match mea-
sure. That is, for any connective token, agreement was
recorded as 1 when both annotators made identical tex-
tual selections for the annotation of both arguments and
0 when the annotators made non-identical selections for
any one or both arguments.
We achieved 82.8% agreement (2249/2717 tokens) on
the annotations for this class. Table 2 gives the distribu-
tion of the agreements by connective. The table shows
relatively lower agreements when compared with the first
diagnostic, for both subordinating conjunctions (86%) as
well as adverbials (57%). However, this difference is un-
derstandable since the token class as defined for this di-
agnostic yields a stricter measure of agreement.
CONNECTIVES AGR No. Conn. Total %AGR
when 868 1016 86.4%
because 804 912 88.2%
even though 91 103 88.3%
although 288 352 81.8%
so that 27 34 79.4%
TOTAL SUBCONJ 2078 2417 86.0%
nevertheless 18 47 38.3%
otherwise 21 23 91.3%
instead 72 118 61.0%
as a result 38 84 45.2%
therefore 22 28 78.6%
TOTAL ADV. 171 300 57.0%
OVERALL TOTAL 2249 2717 82.8%
Table 2: Distribution of Agreement by Connective, with
ARG1 and ARG2 Annotations Counted Together
We classified disagreements into 4 major types. The
result of classifying the 534 disagreements from Diag-
nostic 1 (Table 1) is given in Table 3. The third column
gives the percent of the total disagreements for each type.
DISAGREEMENT TYPE No. %
Missing Annotations 72 13.5%
No Overlap 30 5.6%
Partial Overlap
Parentheticals 53 9.9%
higher verb 181 33.9%
dependent clause 182 34.1%
Other 6 1.1%
Unresolved 10 1.9%
TOTAL 534 100%
Table 3: Disagreement Classification
The majority of disagreements (79%) were due to
Partial Overlap, which subsumes the categories Higher
Verb, Dependent Clause, Parenthetical and Other. Par-
tial Overlap means that there was partial overlap in the
annotations selected by the two annotators. Higher verb
includes tokens where one of the annotators included the
governing predicate for the clause marked by both anno-
tators. The higher clause occurred on the left or right pe-
riphery of the lower clause. Dependent Clause includes
tokens where one of the annotators included extra clausal
material that is syntactically dependent on the clause that
was selected by both, and that occurs on the left or right
periphery of the common text. Parenthetical means
that one of the annotators included a medial parentheti-
cal, while the other did not. The intervening text could be
the main as well as the dependent clause. An example is
provided below:
(16) Bankers said [warrants for Hong Kong stocks are at-
tractive] because [they give foreign investors], wary of
volatility in the colony?s stock market, [an opportunity
to buy shares without taking too great a risk].
(17) Bankers said [warrants for Hong Kong stocks are at-
tractive] because [they give foreign investors, wary of
volatility in the colony?s stock market, an opportunity
to buy shares without taking too great a risk].
Other included tokens with partial overlap between an-
notations, but in addition included a combination of more
than type, such as higher verb+dependent clause.
Note that disagreements that contain a partial over-
lap could be counted as agreeing tokens if we relaxed
the more conservative exact match measure to a partial
match measure. Our subjective view was that in several
cases, the ?extra? textual material, especially those fit-
ting the dependent clause and parenthetical category did
not make any significant semantic contribution in terms
of their inclusion or exclusion in the argument. With the
partial match measure, excluding these cases reduces the
disagreements to half the given number, giving us 94.5%
agreement overall.
The No Overlap tokens were cases of true disagree-
ment in that there was no overlap in the annotations se-
lected by the annotators. These tokens constituted 5.6%
of the disagreements. Examples (18) and (19) shows the
two annotations for a token in which there was no over-
lap in the ARG1 annotation. Missing Annotations also
constituted a substantial proportion of the disagreements
(13.5%) and was used for tokens where the annotation
was missing for one annotator. Note that these don?t re-
ally count as disagreement, since all connectives are pre-
theoretically assumed to require two arguments. Unre-
solved includes tokens which have introduced new issues
for the annotation guidelines and cannot be resolved at
this time. These include issues such as how to treat com-
paratives, certain types of adjunct clauses, certain types
of nominalizations etc.
(18) [The word ?death? cannot be escaped entirely by the
industry], but salesmen dodge it wherever possible or
cloak it in euphemisms, [preferring to talk about ?sav-
ings? and ?investment?] instead.
(19) The word ?death? cannot be escaped entirely by the
industry, but salesmen dodge it wherever possible or
[cloak it in euphemisms], preferring [to talk about ?sav-
ings? and ?investment?] instead.
4.1.2 Implicit connectives
For the 386 tokens of implicit connectives, we ana-
lyzed inter-annotator agreement between two annotators
for (a) the explicit connectives they provided in place of
an implicit connective, and (b) the argument annotations
of the implicit connectives.
As a preliminary step in analyzing agreement on the
type of explicit connective provided by the annotators in
place of an implicit connective, we considered 5 groups
of connectives conveying : a) additional information
(e.g., ?furthermore?, ?in addition?) b) cause-effect rela-
tions (e.g., ?because?, ?as a result?), c) temporal relations
(e.g., ?then?, ?simultaneously?), d) contrastive relations
(e.g., ?however?, ?although?), and e) restatement or sum-
marization (e.g., ?in other words?, ?in sum?). 10 Agree-
ment was then computed on these basic groups of con-
nectives.11 From the total of 386 tokens of implicit con-
nectives, 9 were excluded from the analysis due to tech-
nical error (missing annotation). For the remaining 307
tokens, we achieved 72% agreement on the type of ex-
plicit connective that best conveyed the interpretation of
the implicit connective.
For the argument annotations of the implicit connec-
tives, we present agreement results from using the first
diagnostic used for the explicit connectives. That is, we
counted ARG1 and ARG2 annotations as independent to-
kens and computed percent agreement using the exact
match criterion. On the 772 ARG1 and ARG2 tokens,
we achieved 85.1% (657/772) agreement between 2 an-
notators. The analysis of the 115 disagreements is given
in Table 4. Note that here again, the number of disagree-
ments reduces to half using the partial match measure for
the parenthetical and dependent clause classes, giving us
92.6% agreement overall.
DISAGREEMENT TYPE No. %
Missing Annotations 6 5.2%
No Overlap 2 1.7%
Partial Overlap
parenthetical 13 11.3%
higher verb 24 20.9%
dependent clause 44 38.3%
sentence 19 16.5%
other 3 2.6%
Unresolved 4 3.5%
TOTAL 115 100%
Table 4: Disagreement Classification for Implicit Con-
nective ARG Annotations
10These groups are based on types of coherence relations de-
rived from corpus-based distributions of connectives presented
in (Knott, 1996). Initially, we also considered a group of con-
nectives expressing hypothetical relations but no such connec-
tives were identified in the annotations.
11Some polysemous connectives such as ?while? and ?in fact?
appeared in more than one group.
5 Summary
In this paper we presented a new and innovative
discourse-level annotation project, the Penn Discourse
TreeBank (PDTB), in which discourse connectives and
their arguments are annotated, thereby defining a clear
level of discourse structure that can be reliably annotated
for a large corpus. Our inter-annotator results confirm our
expectations of high agreement and annotation reliability.
At a later stage of the project, we plan to provide seman-
tic characterizations of the arguments of connectives and
resolve any cases of polysemy that might arise.
Acknowledgments
We are very grateful to Tom Morton and Jeremy Lacivita
for the development and special modification of the
WordFreak annotation tool. Special thanks to Jeremy for
providing continuous technical support. Thanks are also
due to our annotators, Cassie Creswell, Driya Amandita,
John Laury, Emily Pawley, Alan Lee, Alex Derenzy and
Steve Pettington. Also, many thanks to Katherine Forbes
Riley and Jean Carletta for their comments and sugges-
tions. Finally, we would like to thank the reviewers for
their very useful comments. This work was partially sup-
ported by NSF Grant EIA 02-24417.
References
Nicholas Asher and Alex Lascarides. 1998. The seman-
tics and pragmatics of presupposition. Journal of Se-
mantics, 15(3):239?300.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks. Computational Linguistics, 22:249?254.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski,
2003. Current Directions in Discourse and Dialogue,
chapter Building a Discourse-Tagged Corpus in the
Framework of Rhetorical Structure Theory. Kluwer
Academic Publishers.
Kate Forbes. 2003. Discourse Semantics of S-Modifying
Adverbials. Ph.D. thesis, Department of Linguistics,
University of Pennsylvania.
Claire Gardent. 1997. Discourse tree adjoining gram-
mars. Claus 89, University of the Saarlandes, Saar-
brucken.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Third International Confer-
ence on Language Resources and Evaluation, LREC-
02, Las Palmas, Canary Islands, Spain.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. thesis,
University of Edinburgh.
William Mann and Sandra Thompson. 1988. Rhetori-
cal structure theory. toward a functional theory of text
organization. Text, 8(3):243?281.
Mitch Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: the Penn Treebank. Computational
Linguistics, 19:313?330.
Eleni Miltsakaki, Cassandre Creswell, Kate Forbes, Ar-
avind Joshi, and Bonnie Webber. 2003. Anaphoric
arguments of discourse connectives: Semantic prop-
erties of antecedents versus non-antecedents. In Pro-
ceedings of the Computational Treatment of Anaphora
Workshop, EACL 2003, Budapest.
Livia Polanyi and Martin van den Berg. 1996. Discourse
structure and discourse interpretation. In Proceedings
of the Tenth Amsterdam Colloquium, University of Am-
sterdam, pages 113?131.
Frank Schilder. 1997. Discourse tree grammar or how
to get attached to a discourse? In Proceedings of the
the second International Workshop on Computational
Semantics (IWCS-II), Tilburg, The Netherlands, pages
261?273.
Sidney Siegel and N. J. Castellan. 1988. Nonparama-
teric Statistics for the Behavioral Sciences. McGraw-
Hill, 2nd edition.
Bonnie Webber and Aravind Joshi. 1998. Anchoring a
lexicalized tree adjoining grammar for discourse. In
ACL/COLING Workshop on Discourse Relations and
Discourse Markers, Montreal, pages 8?92. Montreal,
Canada.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999a. Discourse relations: A struc-
tural and presuppositional account using lexicalized
TAG. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, Mary-
land, pages 41?48. College Park MD.
Bonnie Webber, Alistair Knott, Matthew Stone, and Ar-
avind Joshi. 1999b. What are little texts made of? A
structural and presuppositional account using lexical-
ized TAG. In Proceedings of the International Work-
shop on Levels of Representation in Discourse (LORID
?99), Edinburgh, pages 145?149.
Bonnie Webber, Alistair Knott, and Aravind Joshi. 2000.
Multiple discourse connectives in a lexicalized gram-
mar for discourse. In Proceedings of the Third Interna-
tional Workshop on Computational Semantics, Tilburg,
The Netherlands.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29?36,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Attribution and the (Non-)Alignment of Syntactic and Discourse Arguments
of Connectives
Nikhil Dinesh and Alan Lee and Eleni Miltsakaki and Rashmi Prasad and Aravind Joshi
University of Pennsylvania
Philadelphia, PA 19104 USA
fnikhild,aleewk,elenimi,rjprasad,joshig@linc.cis.upenn.edu
Bonnie Webber
University of Edinburgh
Edinburgh, EH8 9LW Scotland
bonnie@inf.ed.ac.uk
Abstract
The annotations of the Penn Discourse
Treebank (PDTB) include (1) discourse
connectives and their arguments, and (2)
attribution of each argument of each con-
nective and of the relation it denotes. Be-
cause the PDTB covers the same text as
the Penn TreeBank WSJ corpus, syntac-
tic and discourse annotation can be com-
pared. This has revealed significant dif-
ferences between syntactic structure and
discourse structure, in terms of the argu-
ments of connectives, due in large part to
attribution. We describe these differences,
an algorithm for detecting them, and fi-
nally some experimental results. These re-
sults have implications for automating dis-
course annotation based on syntactic an-
notation.
1 Introduction
The overall goal of the Penn Discourse Treebank
(PDTB) is to annotate the million word WSJ cor-
pus in the Penn TreeBank (Marcus et al, 1993) with
a layer of discourse annotations. A preliminary re-
port on this project was presented at the 2004 work-
shop on Frontiers in Corpus Annotation (Miltsakaki
et al, 2004a), where we described our annotation
of discourse connectives (both explicit and implicit)
along with their (clausal) arguments.
Further work done since then includes the an-
notation of attribution: that is, who has expressed
each argument to a discourse connective (the writer
or some other speaker or author) and who has ex-
pressed the discourse relation itself. These ascrip-
tions need not be the same. Of particular interest is
the fact that attribution may or may not play a role
in the relation established by a connective. This may
lead to a lack of congruence between arguments at
the syntactic and the discourse levels. The issue of
congruence is of interest both from the perspective
of annotation (where it means that, even within a
single sentence, one cannot merely transfer the an-
notation of syntactic arguments of a subordinate or
coordinate conjunction to its discourse arguments),
and from the perspective of inferences that these an-
notations will support in future applications of the
PDTB.
The paper is organized as follows. We give a brief
overview of the annotation of connectives and their
arguments in the PDTB in Section 2. In Section 3,
we describe the annotation of the attribution of the
arguments of a connective and the relation it con-
veys. In Sections 4 and 5, we describe mismatches
that arise between the discourse arguments of a con-
nective and the syntactic annotation as provided by
the Penn TreeBank (PTB), in the cases where all the
arguments of the connective are in the same sen-
tence. In Section 6, we will discuss some implica-
tions of these issues for the theory and practice of
discourse annotation and their relevance even at the
level of sentence-bound annotation.
2 Overview of the PDTB
The PDTB builds on the DLTAG approach to dis-
course structure (Webber and Joshi, 1998; Webber
et al, 1999; Webber et al, 2003) in which con-
nectives are discourse-level predicates which project
predicate-argument structure on a par with verbs at
29
the sentence level. Initial work on the PDTB has
been described in Miltsakaki et al (2004a), Milt-
sakaki et al (2004b), Prasad et al (2004).
The key contribution of the PDTB design frame-
work is its bottom-up approach to discourse struc-
ture: Instead of appealing to an abstract (and arbi-
trary) set of discourse relations whose identification
may confound multiple sources of discourse mean-
ing, we start with the annotation of discourse con-
nectives and their arguments, thus exposing a clearly
defined level of discourse representation.
The PDTB annotates as explicit discourse connec-
tives all subordinating conjunctions, coordinating
conjunctions and discourse adverbials. These pred-
icates establish relations between two abstract ob-
jects such as events, states and propositions (Asher,
1993).1
We use Conn to denote the connective, and Arg1
and Arg2 to denote the textual spans from which the
abstract object arguments are computed.2 In (1), the
subordinating conjunction since establishes a tem-
poral relation between the event of the earthquake
hitting and a state where no music is played by a
certain woman. In all the examples in this paper, as
in (1), Arg1 is italicized, Arg2 is in boldface, and
Conn is underlined.
(1) She hasn?t played any music since the earthquake
hit.
What counts as a legal argument? Since we take
discourse relations to hold between abstract objects,
we require that an argument contains at least one
clause-level predication (usually a verb ? tensed or
untensed), though it may span as much as a sequence
of clauses or sentences. The two exceptions are
nominal phrases that express an event or a state, and
discourse deictics that denote an abstract object.
1For example, discourse adverbials like as a result are dis-
tinguished from clausal adverbials like strangely which require
only a single abstract object (Forbes, 2003).
2Each connective has exactly two arguments. The argument
that appears in the clause syntactically associated with the con-
nective, we call Arg2. The other argument is called Arg1. Both
Arg1 and Arg2 can be in the same sentence, as is the case for
subordinating conjunctions (e.g., because). The linear order of
the arguments will be Arg2 Arg1 if the subordinate clause ap-
pears sentence initially; Arg1 Arg2 if the subordinate clause ap-
pears sentence finally; and undefined if it appears sentence me-
dially. For an adverbial connective like however, Arg1 is in the
prior discourse. Hence, the linear order of its arguments will be
Arg1 Arg2.
Because our annotation is on the same corpus as
the PTB, annotators may select as arguments textual
spans that omit content that can be recovered from
syntax. In (2), for example, the relative clause is
selected as Arg1 of even though, and its subject can
be recovered from its syntactic analysis in the PTB.
In (3), the subject of the infinitival clause in Arg1 is
similarly available.
(2) Workers described ?clouds of blue dust? that hung
over parts of the factory even though exhaust fans
ventilated the air.
(3) The average maturity for funds open only to institu-
tions, considered by some to be a stronger indicator
because those managers watch the market closely,
reached a high point for the year ? 33 days.
The PDTB also annotates implicit connectives be-
tween adjacent sentences where no explicit connec-
tive occurs. For example, in (4), the two sentences
are contrasted in a way similar to having an explicit
connective like but occurring between them. Anno-
tators are asked to provide, when possible, an ex-
plicit connective that best describes the relation, and
in this case in contrast was chosen.
(4) The $6 billion that some 40 companies are looking to
raise in the year ending March 21 compares with only
$2.7 billion raise on the capital market in the previous
year. IMPLICIT - in contrast In fiscal 1984, before
Mr. Gandhi came into power, only $810 million
was raised.
When complete, the PDTB will contain approxi-
mately 35K annotations: 15K annotations of the 100
explicit connectives identified in the corpus and 20K
annotations of implicit connectives.3
3 Annotation of attribution
Wiebe and her colleagues have pointed out the
importance of ascribing beliefs and assertions ex-
pressed in text to the agent(s) holding or making
them (Riloff and Wiebe, 2003; Wiebe et al, 2004;
Wiebe et al, 2005). They have also gone a consid-
erable way towards specifying how such subjective
material should be annotated (Wiebe, 2002). Since
we take discourse connectives to convey semantic
predicate-argument relations between abstract ob-
jects, one can distinguish a variety of cases depend-
ing on the attribution of the discourse relation or its
3The annotation guidelines for the PDTB are available at
http://www.cis.upenn.edu/pdtb.
30
arguments; that is, whether the relation or arguments
are ascribed to the author of the text or someone
other than the author.
Case 1: The relation and both arguments are at-
tributed to the same source. In (5), the concessive
relation between Arg1 and Arg2, anchored on the
connective even though is attributed to the speaker
Dick Mayer, because he is quoted as having said
it. Even where a connective and its arguments are
not included in a single quotation, the attribution can
still be marked explicitly as shown in (6), where only
Arg2 is quoted directly but both Arg1 and Arg2 can
be attibuted to Mr. Prideaux. Attribution to some
speaker can also be marked in reported speech as
shown in the annotation of so that in (7).
(5) ?Now, Philip Morris Kraft General Foods? parent
company is committed to the coffee business and to
increased advertising for Maxwell House,? says Dick
Mayer, president of the General Foods USA division.
?Even though brand loyalty is rather strong for cof-
fee, we need advertising to maintain and strengthen
it.?
(6) B.A.T isn?t predicting a postponement because the
units ?are quality businesses and we are en-
couraged by the breadth of inquiries,? said Mr.
Prideaux.
(7) Like other large Valley companies, Intel also noted
that it has factories in several parts of the nation,
so that a breakdown at one location shouldn?t leave
customers in a total pinch.
Wherever there is a clear indication that a relation
is attributed to someone other than the author of the
text, we annotate the relation with the feature value
SA for ?speaker attribution? which is the case for
(5), (6), and (7). The arguments in these examples
are given the feature value IN to indicate that they
?inherit? the attribution of the relation. If the rela-
tion and its arguments are attributed to the writer,
they are given the feature values WA and IN respec-
tively.
Relations are attributed to the writer of the text by
default. Such cases include many instances of re-
lations whose attribution is ambiguous between the
writer or some other speaker. In (8), for example,
we cannot tell if the relation anchored on although
is attributed to the spokeswoman or the author of the
text. As a default, we always take it to be attributed
to the writer.
Case 2: One or both arguments have a different at-
tribution value from the relation. While the default
value for the attribution of an argument is the attribu-
tion of its relation, it can differ as in (8). Here, as in-
dicated above, the relation is attributed to the writer
(annotated WA) by default, but Arg2 is attributed to
Delmed (annotated SA, for some speaker other than
the writer, and other than the one establishing the
relation).
(8) The current distribution arrangement ends in March
1990 , although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
Annotating the corpus with attribution is neces-
sary because in many cases the text containing the
source of attribution is located in a different sen-
tence. Such is the case for (5) where the relation
conveyed by even though, and its arguments are at-
tributed to Dick Mayer.
We are also adding attribution values to the anno-
tation of the implicit connectives. Implicit connec-
tives express relations that are inferred by the reader.
In such cases, the author intends for the reader to
infer a discourse relation. As with explicit connec-
tives, we have found it useful to distinguish implicit
relations intended by the writer of the article from
those intended by some other author or speaker. To
give an example, the implicit relation in (9) is at-
tributed to the writer. However, in (10) both Arg1
and Arg2 have been expressed by the speaker whose
speech is being quoted. In this case, the implicit re-
lation is attributed to the speaker.
(9) Investors in stock funds didn?t panic the week-
end after mid-October?s 190-point market plunge.
IMPLICIT-instead Most of those who left stock
funds simply switched into money market funds.
(10) ?People say they swim, and that may mean they?ve
been to the beach this year,? Fitness and Sports. ?It?s
hard to know if people are responding truthfully.
IMPLICIT-because People are too embarrassed to
say they haven?t done anything.?
The annotation of attribution is currently under-
way. The final version of the PDTB will include an-
notations of attribution for all the annotated connec-
tives and their arguments.
Note that in the Rhetorical Structure Theory
(RST) annotation scheme (Carlson et al, 2003), at-
tribution is treated as a discourse relation. We, on
the other hand, do not treat attribution as a discourse
31
relation. In PDTB, discourse relations (associated
with an explicit or implicit connective) hold between
two abstracts objects, such as events, states, etc. At-
tribution relates a proposition to an entity, not to an-
other proposition, event, etc. This is an important
difference between the two frameworks. One conse-
quence of this difference is briefly discussed in Foot-
note 4 in the next section.
4 Arguments of Subordinating
Conjunctions in the PTB
A natural question that arises with the annotation
of arguments of subordinating conjunctions (SUB-
CONJS) in the PDTB is to what extent they can be
detected directly from the syntactic annotation in the
PTB. In the simplest case, Arg2 of a SUBCONJ is its
complement in the syntactic representation. This is
indeed the case for (11), where since is analyzed as
a preposition in the PTB taking an S complement
which is Arg2 in the PDTB, as shown in Figure 1.
(11) Since the budget measures cash flow, a new $1 di-
rect loan is treated as a $1 expenditure.
Furthermore, in (11), since together with its com-
plement (Arg2) is analyzed as an SBAR which mod-
ifies the clause a new $1 direct loan is treated as a
$1 expenditure, and this clause is Arg1 in the PDTB.
Can the arguments always be detected in this
way? In this section, we present statistics showing
that this is not the case and an analysis that shows
that this lack of congruence between the PDTB and
the PTB is not just a matter of annotator disagree-
ment.
Consider example (12), where the PTB requires
annotators to include the verb of attribution said
and its subject Delmed in the complement of al-
though. But although as a discourse connective de-
nies the expectation that the supply of dialysis prod-
ucts will be discontinued when the distribution ar-
rangement ends. It does not convey the expectation
that Delmed will not say such things. On the other
hand, in (13), the contrast established by while is be-
tween the opinions of two entities i.e., advocates and
their opponents.4
4This distinction is hard to capture in an RST-based pars-
ing framework (Marcu, 2000). According to the RST-based an-
notation scheme (Carlson et al, 2003) ?although Delmed said?
and ?while opponents argued? are elementary discourse units
(12) The current distribution arrangement ends in March
1990, although Delmed said it will continue to pro-
vide some supplies of the peritoneal dialysis prod-
ucts to National Medical, the spokeswoman said.
(13) Advocates said the 90-cent-an-hour rise, to $4.25 an
hour by April 1991, is too small for the working poor,
while opponents argued that the increase will still
hurt small business and cost many thousands of
jobs.
In Section 5, we will identify additional cases. What
we will then argue is that it will be insufficient to
train an algorithm for identifying discourse argu-
ments simply on the basis of syntactically analysed
text.
We now present preliminary measurements of
these and other mismatches between the two corpora
for SUBCONJS. To do this we describe a procedural
algorithm which builds on the idea presented at the
start of this section. The statistics are preliminary in
that only the annotations of a single annotator were
considered, and we have not attempted to exclude
cases in which annotators disagree.
We consider only those SUBCONJS for which both
arguments are located in the same sentence as the
connective (which is the case for approximately 99%
of the annotated instances). The syntactic configura-
tion of such relations pattern in a way shown in Fig-
ure 1. Note that it is not necessary for any of Conn,
Arg1, or Arg2 to have a single node in the parse tree
that dominates it exactly. In Figure 1 we do obtain a
single node for Conn, and Arg2 but for Arg1, it is
the set of nodes fNP; V Pg that dominate it exactly.
Connectives like so that, and even if are not domi-
nated by a single node, and cases where the annota-
tor has decided that a (parenthetical) clausal element
is not minimally necessary to the interpretation of
Arg2 will necessitate choosing multiple nodes that
dominate Arg2 exactly.
Given the node(s) in the parse tree that dominate
Conn (fINg in Figure 1), the algorithm we present
tries to find node(s) in the parse tree that dominate
Arg1 and Arg2 exactly using the operation of tree
subtraction (Sections 4.1, and 4.2). We then discuss
its execution on (11) in Section 4.3.
annotated in the same way: as satellites of the relation Attribu-
tion. RST does not recognize that satellite segments, such as
the ones given above, sometimes participate in a higher RST
relation along with their nuclei and sometimes not.
32
S12
SBAR NP
A new $1 direct
loan
VP
is treated as a
$1 expenditure
IN S
2
the budget mea-
sures cash flowsince
Given N
Conn
= fINg, our goal is to find N
Arg1
=
fNP; V Pg, and N
Arg2
= fS
2
g. Steps:
 h
Conn
= IN
 x
Conn+Arg2
= SBAR  parent(h
Conn
)
 x
Conn+Arg1+Arg2
= S
12
 lowest Ancestor
parent(x
Conn+Arg2
)
with la-
bel S or SBAR. Note that x 2 Ancestor
x
 N
Arg2
= x
Conn+Arg2
 N
Conn
= SBAR  fINg
= fS
2
g
 N
Arg1
= x
Conn+Arg1+Arg2
  fx
Conn+Arg2
g
= S
12
  fSBARg
= fNP; V Pg
Figure 1: The syntactic configuration for (11), and the execution of the tree subtraction algorithm on this configuration.
4.1 Tree subtraction
We will now define the operation of tree subtraction
the graphical intuition for which is given in Figure
2. Let T be the set of nodes in the tree.
Definition 4.1. The ancestors of any node t 2 T ,
denoted by Ancestor
t
 T is a set of nodes such
that t 2 Ancestor
t
and parent(u; t) ) ([u 2
Ancestor
t
] ^ [Ancestor
u
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 89?97,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Real-Time Web Text Classification and Analysis of Reading Difficulty
Eleni Miltsakaki
Graduate School of Education
Universisty of Pennsylvania,
Philadelphia, PA 19104, USA.
elenimi@seas.upenn.edu
Audrey Troutt
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
atroutt@seas.upenn.edu
Abstract
The automatic analysis and categorization of
web text has witnessed a booming interest due
to the increased text availability of different
formats, content, genre and authorship. We
present a new tool that searches the web and
performs in real-time a) html-free text extrac-
tion, b) classification for thematic content and
c) evaluation of expected reading difficulty.
This tool will be useful to adolescent and adult
low-level reading students who face, among
other challenges, a troubling lack of reading
material for their age, interests and reading
level.
1 Introduction
According to the National Center for Education
Statistics, 29% of high school seniors in public
schools across America were below basic achieve-
ment in reading in 2005 (U.S. Department of Edu-
cation 2005). Once these students enter high school,
their reading problems, which began much earlier
in their education, are compounded by many fac-
tors including a lack of suitable reading material for
their age, interests and reading level. Most mate-
rial written at a lower reading level is designed for
much younger students; high-school students find it
boring or embarrassing. On the other hand material
designed for older students, while probably more in-
teresting, is incomprehensible to such a student and
leads to frustration and self-doubt. The internet is
a vast resource for potential reading material and is
often utilized by educators in the classroom, but it is
not currently possible to filter the results of a search
engine query by levels of readability. Instead, the
software that some schools have adopted restricts
students to lists and directories of hand-selected edu-
cational sites. This severely limits the content avail-
able to students and requires near-constant mainte-
nance to keep current with new information avail-
able on the web.
We are developing a new system, Read-X, that
searches the web and performs in real-time a) html-
free text extraction, b) classification for thematic
content and c) evaluation of expected reading dif-
ficulty. For the thematic classification task we col-
lected a manually labeled corpus to train and com-
pare three text classifiers. Our system is part of
larger research effort to improve existing readabil-
ity metrics by taking into account the profile of the
reader. As a first step in this direction, we computed
vocabulary frequencies per thematic area. We use
these frequencies to predict unknown words for the
reader relative to her familiarity with thematic areas
(Toreador). These tools (Read-X and Toreador) will
be useful to adolescent and adult low-level reading
students who face, among other challenges, a trou-
bling lack of reading material for their age, interests
and reading level.
The remainder of the paper is organized as fol-
lows: first we will describe our motivation for cre-
ating Read-X and Toreador, which is based on stud-
ies that show that older struggling readers can make
improvements in literacy and that those improve-
ments can have a profound impact on their lives.
Next we will describe existing technologies for liter-
acy improvement and research related to our current
project. Finally, we will give a detailed description
89
of Read-X and Toreador, including our methods of
evaluating the readability of texts, thematically clas-
sifying the texts and modeling reader profiles into
readability predictions, before concluding with an
outline of future work.
2 Educational motivation
Low reading proficiency is a widespread problem
evident in the performance of adolescents in U.S.
schools. The National Center for Education Statis-
tics (NCES) in 2005, the latest year for which data
is available, reports that only 29% of eight graders
in the United States achieved proficient or above
reading, meaning the remaining 71% of students
had only part of the reading skills needed for pro-
ficient work at their level or less (Snyder et al,
2006). (Hasselbring and Goin, 2004) reported that
?as many as 20 percent of 17-year-olds have been
estimated to be functionally illiterate, and 44 per-
cent of all high-school students have been described
as semi-literate?. Reading below grade level is a se-
rious problem for adolescents as it may hinder com-
prehension of textbooks and classroom materials in
all fields. (Denti, 2004) mentions that ?most high
school textbooks are written at the tenth through
twelfth grade levels with some textbooks used for
U. S. government written at the seventeenth grade
level?. Reading skills are tied to academics suc-
cess and are highly correlated with with ?higher in-
come and less unemployment, increased access to
lifelong learning, greater amounts of personal read-
ing for pleasure, and increased civic participation?
(Strucker et al, 2007).
Recent research has shown that it is possible
to identify adult literacy students on the brink of
achieving reading fluency in order to provide them
with concentrated instruction, dramatically improv-
ing their chances of attaining a high quality of life
(Strucker et al, 2007). (Weinstein and Walberg,
1993) studied the factors related to achievement in
reading and found that ?frequent and extensive en-
gagement in literacy-promoting activities as a young
adult was associated with higher scores on literacy
outcomes (independent of earlier-fixed characteris-
tics and experiences),? which implies that through
ample reading exercise students can achieve literacy
regardless of their background.
The current and future versions of the system that
we are developing uses natural language processing
techniques to provide learning tools for struggling
readers. The web is the single most varied resource
of content and style, ranging from academic papers
to personal blogs, and is thus likely to contain in-
teresting reading material for every user and reading
ability. The system presented here is the first to our
knowledge which performs in real time a)keyword
search, b)thematic classification and c)analysis of
reading difficulty. We also present a second sys-
tem which analyzes vocabulary difficulty according
to reader?s prior familiarity with thematic content.
3 Related work
In this section we discuss two main systems that are
most closely related to our work on text classifica-
tion and analysis of readability.
NetTrekker is a commercially available search
tool especially designed for K-12 students and ed-
ucators.1 NetTrekker?s search engine has access to
a database of web links which have been manually
selected and organized by education professionals.
The links are organized thematically per grade level
and their readability level is evaluated on a scale of
1-5. Level 1 corresponds to reading ability of grades
1-3 and 5 to reading ability of grades 11-13. Net-
trekker has been adopted by many school districts
in the U.S., because it offers a safe way for K-12
students to access only web content that is age ap-
popriate and academically relevant. On the other
hand, because the process of web search and classi-
fication is not automated, it is practically impossible
for NetTrekker to dynamically update its database so
that new material posted on the web can be included.
However, Nettrekker?s manual classification of web
links is a valuable resource of manually labeled data.
In our project, we use this resource to build labeled
dataset for training statistical classifiers. We discuss
the construction and use of this corpus in more detail
in Section 5.1).
The REAP tutor, developed at the Language Tech-
nologies Institute at Carnegie Mellon, is designed to
assist second language learners to build new vocabu-
lary and facilitates student specific practice sessions
(Collins-Thompson and Callan, 2004), (Heilman et
1Available at http://www.nettrekker.com.
90
al., 2006). The tutor allows the user to search for
textual passages as well as other text retrieved from
the web that contains specific vocabulary items. The
educational gain for students practicing with the tu-
tor has been shown in several studies (e.g., (Heil-
man et al, 2006)). Like NetTrekker, REAP retrieves
and classifies web text off-line. Unlike, Nettrekker,
however, textual analysis is automated. REAP?s in-
formation retrieval system (Collins-Thompson and
Callan, 2004) contains material from about 5 million
pages gathered with web crawling methods. The
data have been annotated and indexed off-line. An-
notations include readability level computed with an
earlier version of the method developed by (Heilman
et al, 2007), (Heilman et al, 2006) described be-
low, rough topic categorizations (e.g., fiction, non-
fiction) and some elements of grammatical structure
(e.g., part-of-speech tagging).
(Heilman et al, 2007) experiment with a system
for evaluation of reading difficulty which employs
both grammatical features and vocabulary. The
grammatical features built in the model were iden-
tified from grammar books used in three ESL lev-
els. (Heilman et al, 2007) find that while the vo-
cabulary model alone outperformed the grammar-
based model, the combined model performed best.
All models performed better in English text and less
well in ESL text. It would be very interesting to in-
tegrate this system with Read-X and evaluate its per-
formance.
To address issues specific to struggling read-
ers, (Hasselbring and Goin, 2004) developed
the Peabody Literacy Lab (PLL), a completely
computer-based program, using a variety of tech-
nologies to help students improve their ability to
read. We will not elaborate further on this work
because the PPL?s focus in not in developing new
technologies. PLL develops experimental programs
using existing technologies.
4 Read-X project overview
In the Read-X project, we have developed two tools
which are currently independent of each other. The
first tool Read-X, performs a web search and classi-
fies text as detailed in (5.1). The second tool Tore-
ador, analyzes input text and predicts vocabulary dif-
ficulty based on grade or theme-specific vocabulary
frequencies. The vocabulary predicted to be unfa-
miliar can be clicked on. This action activates a dic-
tionary look-up search on Wordnet whose display is
part of the tool?s interface. More details and screen-
shots are given in (??).
5 Description of Read-X
Below we describe in detail the technical compo-
nents of Read-X: internet search, text extraction and
analysis of readability.
5.1 Read-X: Web search and text classification
Internet search. Read-X performs a search of the
internet using the Yahoo! Web Services. When
the search button is clicked or the enter key de-
pressed after typing in a keyword, Read-X sends a
search request to Yahoo! including the keywords
and the number of results to return and receives re-
sults including titles and URLs of matching web-
sites in an XML document. The Yahoo! Web
Service is freely available for non-commercial use
with a limit of 5000 requests per day. If Read-X
is deployed for use by a wide number of users, it
may be necessary to purchase the ability to process
more requests with Yahoo or another search engine.
Read-X is currently available at http://net-
read.blogspot.com.
Text extraction. Read-X then retrieves the html,
xml, doc or PDF document stored at each URL
and extracts the human-readable text.2 text is ex-
tracted from html and xml documents using the
scraper provided by Generation Java by Henri Yan-
dell, see www.generationjava.com. The Microsoft
Word document scraper is part of the Apache Jakarta
project by the Apache Software Foundation, see
www.apache.org. The PDF scraper is part of the
Apache Lucene project, see www.pdfbox.org. All
three of these external tools are available under a
common public license as open source software un-
der the condition that any software that makes use of
the tools must also make the source code available to
users.
2Being able to identify appopriate web pages whose content
is reading material and not ?junk? is a non-trivial task. (Petersen
and Ostendorf, 2006) use a classifier for this task with moderate
success. We ?read? the structure of the html text to decide if the
content is appropriate and when in doubt, we err on the side of
throwing out potentially useful content.
91
Readability analysis. For printed materials, there
are a number of readability formulas used to mea-
sure the difficulty of a given text; the New Dale-
Chall Readability Formula, The Fry Readability
Formula, the Gunning-Fog Index, the Automated
Readability Index, and the Flesch Kincaid Reading
Ease Formula are a few examples. Usually these for-
mulas count the number of syllables, long sentences,
or difficult words in randomly selected passages
of the text. To automate the process of readabil-
ity analysis, we chose three Readability algorithms:
Lix, Rix, see (Anderson, 1983), and Coleman-Liau,
(Coleman and Liau, 1975), which were best suited
for fast calculation and provide the user with either
an approximate grade level for the text or a readabil-
ity classification of very easy, easy, standard, diffi-
cult or very difficult. When each text is analyzed by
Read-X the following statistics are computed: to-
tal number of sentences, total number of words, to-
tal number of long words (seven or more characters,
and total number of letters in the text. Below we de-
scribe how each of the three readability scores are
calculated using these statistics. Steps taken to de-
velop more sophisticated measures for future imple-
mentations are presented in Section 7).
Lix readability formula: The Lix readability al-
gorithm distinguishes between five levels of read-
ability: very easy, easy, standard, difficult, or very
difficult. If W is the number of words, LW is the
number of long words (7 or more characters), and
S is the number of sentences, them the Lix index is
LIX = W/S + (100 * LW) / W. An index of 0-24
corresponds to a very easy text, 25-34 is easy, 35-44
standard, 45-54 difficult, and 55 or more is consid-
ered very difficult.
Rix readability formula: The Rix readability
formula consists of the ratio of long words to sen-
tences, where long words are defined as 7 or more
characters. The ratio is translated into a grade level
as indicated in Table (1).
Coleman-Liau readability formula: The
Coleman-Liau readability formula is similar to the
Rix formula in that it gives the approximate grade
level of the text. Unlike the Lix and Rix formulas,
the Coleman-Liau formula requires the random
selection of a 100 word excerpt from the text.
Before the grade level can be calculated, the cloze
percent must be estimated for this selection. The
Ratio GradelLevel
7.2 and above College
6.2 and above 12
5.3 and above 11
4.5 and above 10
3.7 and above 9
3.0 and above 8
2.4 and above 7
1.8 and above 6
1.3 and above 5
0.8 and above 4
0.5 and above 3
0.2 and above 2
Below 0.2 1
Table 1: Rix translation to grade level
Classifier Supercategories Subcategories
Naive Bayes 66% 30%
MaxEnt 78% 66%
MIRA 76% 58%
Table 2: Performance of text classifiers.
cloze percent is the percent of words that, if deleted
from the text, can be correctly filled in by a college
undergraduate. If L is the number of letters in the
100 word sample and S is the number of sentences,
then the estimated cloze percent is C = 141.8491
- 0.214590 * L + 1.079812 * S. The grade level
can be calculated using the Coleman-Liau formula,
where grade level is -27.4004 * C + 23.06395. In
the SYS display we round the final result to the
nearest whole grade level.
6 Text classification
The automated classification of text into predefined
categories has witnessed strong interest in the past
ten years. The most dominant approach to this prob-
lem is based on machine learning techniques. Clas-
sifiers are built which learn from a prelabeled set of
data the characteristics of the categories. The perfor-
mance of commonly used classifiers varies depend-
ing on the data and the nature of the task. For the text
classification task in Read-X, we a) built a corpus of
prelabeled thematic categories and b)compared the
performance of three classifiers to evaluate their per-
92
formance on this task.
We collected a corpus of approximately 3.4 mil-
lion words and organized it into two sets of label-
ing categories. We hand collected a subset of labels
(most appropriate for a text classification task) from
the set of labels used for the organization of web text
in NetTrekker (see 3). We retrieved text for each
category by following the listed web links in Net-
Trekker and manually extracting text from the sites.
Our corpus is organized into a small hierarchy, with
two sets of labels: a)labels for supercategories and
b)labels for subcategories. There are 8 supercate-
gories (Arts, Career and business, Literature, Phi-
losophy and religion, Science, Social studies, Sports
and health, Technology) and 41 subcategories (e.g.,
the subcategories for Literature are Art Criticism,
Art History, Dance, Music, Theater). Subcategories
are a proper subset of supercategories but in the clas-
sification experiments reported below the classifiers
trained independently in the two data sets.
We trained three classifiers for this task: a Naive
Bayes classifier, a Maximum Entropy classifier and
MIRA, a new online learning algorithm that incor-
porates a measure of confidence in the algorithm(for
details see (Crammer et al, 2008)). 3 The perfor-
mance of the classifiers trained on the supercate-
gories and subcategories data is shown in Table (2).
All classifiers perform reasonably well in the super-
categories classification task but are outperformed
by the MaxEnt classifier in both the supercategories
and subcategories classifications. The Naive Bayes
classifiers performs worst in both tasks. As ex-
pected, the performance of the classifiers deterio-
rates substantially for the subcategories task. This
is expected due to the large number of labels and the
small size of data available for each subcategory. We
expect that as we collect more data the performance
of the classifiers for this task will improve. In an ear-
lier implementation of Read-X, thematic classifica-
tion was a coarser three-way classificaition task (lit-
erature, science, sports). In that implementation the
MaxEnt classifier performed at 93% and the Naive
Bayes classifier performed at 88% correct. In future
implementations of the tool, we will make available
3We gratefully acknowledge MALLET, a collection of
statistical NLP tools written in Java, publicly available at
http://mallet.cs.umass.edu and Mark Dredze for
his help installing and running MIRA on our data.
all three levels thematic classification.
6.1 Runtime and interface
The first implementation of Read-X, coded in Java,
has been made publicly available. The jar file is
called from the web through a link and runs on Win-
dows XP or Vista with Java Runtime Environment 6
and internet connection. Search results and analysis
are returned within a few seconds to a maximum of a
minute or two depending on the speed of the connec-
tion. The Read-X interface allows the user to con-
strain the search by selecting number of returned re-
sults and level of reading difficulty. A screenshot of
Read-X (cropped for anonymity) is shown in Figure
(1). The rightmost column is clickable and shows
the retrieved html-free text in an editor. From this
editor the text can be saved and further edited on the
user?s computer.
7 Description of Toreador
The analysis of reading difficulty based on standard
readability formulas gives a quick and easy way to
measure reading difficulty but it is problematic in
several ways. First, readability formulas compute
superficial features of word and sentence length. It
is easy to show that such features fail to distin-
guish between sentences which have similar word
and sentence lengths but differ in ease of interpreta-
tion. Garden path sentences, bountiful in the linguis-
tic literature, demonstrate this point. Example (1) is
harder to read than example (2) although the latter is
a longer sentence.
(1) She told me a little white lie will come back
to haunt me.
(2) She told me that a little white lie will come
back to haunt me.
Secondly, it is well known that there are aspects
of textual coherence such as topic continuity and
rhetorical structure which are not captured in counts
of words and sentences (e.g., (Higgins et al, 2004),
(Miltsakaki and Kukich, 2004))
Thirdly, readability formulas do not take into ac-
count the profile of the reader. For example, a reader
who has read a lot of literary texts will have less dif-
ficulty reading new literary text than a reader, with a
similar educational background, who has never read
93
Figure 1: Search results and analysis of readability
any literature. In this section, we discuss the first
step we have taken towards making more reliable
predictions on text readability given the profile of
the reader.
Readers who are familiar with specific thematic
areas, are more likely to know vocabulary that is
recurring in these areas. So, if we have vocabu-
lary frequency counts per thematic area, we are in a
better position to predict difficult words for specific
readers given their reading profiles. Vocabulary fre-
quency lists are often used by test developers as an
indicator of text difficulty, based on the assumption
that less frequent words are more likely to be un-
known. However, these lists are built from a variety
of themes and cannot be customized for the reader.
We have computed vocabulary frequencies for all
supercategories in the thematically labeled corpus.
The top 10 most frequent words per supercategory
are shown in Table (3). Vocabulary frequencies per
grade level have also been computed but not shown
here.
Toreador is a tool which runs independently of
Read-X and it?s designed to predict unknown vocab-
ulary for specific reader and grade profiles currently
specified by the user. A screenshot of Toreador is
shown in Figure (2). The interface shows two tabs
labeled ?Enter text here? and ?Read text here?. The
?Enter text here? tab allows the user to customize
vocabulary difficulty predictions by selecting the de-
sired grade or theme.4 Then, text can be copied from
another source and pasted in the window of the tool.
The tool will analyze the text and in a few seconds
return the results fo the analysis in the tab labeled
?Read text here?, shown in Figure (3). Toreador
checks the vocabulary frequency of the words in the
pasted text and returns the text highlighted with the
words that do not rank high in the vocabulary fre-
quency index for the chosen categories (grade or
theme). The highlighted words are clickable. When
they are clicked, they entry information from Word-
Net appears on the right panel. The system has
not been evaluated yet so some tuning will be re-
quired to determine the optimal cut-off frequency
point for highlighting words. An option is also avail-
able to deactivate highlights for ease of read or read-
ing for global meaning. Words that the system has
4The screenshot in Figure (2) shows an earlier version of the
tool where only three thematic categories were available.
94
Figure 2: Text analysis of vocabulary difficulty
Arts Career and Business Literature Philosophy Science Social Studies Sports, Health Technology
Word Freq Word Freq Word Freq Word Freq Word Freq Word Freq Word Freq Word Freq
musical 166 product 257 seemed 1398 argument 174 trees 831 behavior 258 players 508 software 584
leonardo 166 income 205 myself 1257 knowledge 158 bacteria 641 states 247 league 443 computer 432
instrument 155 market 194 friend 1255 augustine 148 used 560 psychoanalytic 222 player 435 site 333
horn 149 price 182 looked 1231 belief 141 growth 486 social 198 soccer 396 video 308
banjo 128 cash 178 things 1153 memory 130 acid 476 clemency 167 football 359 games 303
american 122 analysis 171 caesar 1059 truth 130 years 472 psychology 157 games 320 used 220
used 119 resources 165 going 1051 logic 129 alfalfa 386 psychotherapy 147 teams 292 systems 200
nature 111 positioning 164 having 1050 things 125 crop 368 united 132 national 273 programming 174
artist 104 used 153 asked 1023 existence 115 species 341 society 131 years 263 using 172
wright 98 sales 151 indeed 995 informal 113 acre 332 court 113 season 224 engineering 170
Table 3: 10 top most frequent words per thematic category.
not seen before, count as unknown and can be erro-
neously highlighted (for example, the verb ?give? in
the screenshot example). We are currently running
evaluation studies with a group of volunteers. While
we recognize that the readability formulas currently
implemented in Read-X are inadequate measures of
expected reading difficulty, Toreador is not designed
as an improvement over Read-X but as a component
measuring expected vocabulary difficulty. Other
factors contributing to reading difficulty such as syn-
tactic complexity, propositional density and rhetor-
ical structure will be modeled separately in the fu-
ture.
8 Summary and future work
In this paper we presented preliminary versions of
two tools developed to assist struggling readers iden-
tify text that is at the desired level of reading diffi-
culty while at the same time interesting and relevant
to their interests. Read-X is, to our knowledge, the
first system designed to locate, classify and analyze
reading difficulty of web text in real time, i.e., per-
forming the web search and text analysis in seconds.
Toreador analyzes the vocabulary of given text and
predicts which words are likely to be difficult for the
reader. The contribution of Toreador is that its pre-
dictions are based on vocabulary frequencies calcu-
lated per thematic area and are different depending
on the reader?s prior familiarity with the thematic ar-
eas.
We emphasize the shortcomings of the exist-
ing readability formulas, currently implemented in
Read-X, and the need to develop more sophisticated
measures of reading difficulty. We recognize that
perceived difficulty is the result of many factors,
which need to be analyzed and modeled separately.
95
Figure 3: Text analysis of vocabulary difficutly
Our goal in this research project is not to provide a
single readability score. Instead, we aim at buidling
models for multiple factors and provide individual
evaluation for each, e.g., measures of syntactic com-
plexity, ambiguity, propositional density, vocabu-
lary difficulty, required amount of inference to iden-
tify discourse relations and prior knowledge of the
reader.
In future work, several studies are needed. To
achieve satisfactory performance for the fine grained
thematic categories, we are collecting more data. We
also plan to run the subcategories classification not
as an independent classificaition task but as subclas-
sification task on supercategories. We expect that the
accuracy of the classifier will improve but we also
expect that for very fine thematic distinctions alter-
native approaches may be be required (e.g., give spe-
cial weights for key vocabulary that will distinguish
between sports subthemes) or develop new classi-
fication features beyond statistical analysis of word
distributions.
More sophisticated textual, semantic and dis-
course organization features need to be explored
which will reflect the perceived coherence of the text
beyond the choice of words and sentence level struc-
ture. The recently released Penn Discourse Tree-
bank 2.0 (Prasad et al, 2008)) 5 is a rich source with
annotations of explicit and implicit discourse con-
nectives and semantic labels which can be used to
identify useful discourse features. Finally, more so-
phisticated models are needed of reader profiles and
how they impact the perceived reading difficulty of
the text.
9 Acknowledgments
We are grateful to Mark Dredze for his help run-
ning MIRA and Ani Nenkoca for useful discussions
on readability. We thank the CLUNCH group at
the Computer and Information Science department
at the University of Pennsylvaniaand and two re-
viewers for their very useful feedback. This work is
partially funded by the GAPSA/Provosts Award for
Interdisciplinary Innovation to Audrey Troutt, Uni-
versity of Pennsylvania.
References
Jonathan Anderson. 1983. Lix and rix: Variations of
a little-known readability index. Journal of Reading,
26(6):490?496.
M Coleman and T. Liau. 1975. A computer readabil-
ity formula designed for machine scoring. Journal of
Applied Psychology, 60:283?284.
5Project site, http://www.seas.upenn.edu/?pdtb
96
K. Collins-Thompson and J. Callan. 2004. Informa-
tion retrieval for language tutoring: An overview of
the REAP project. In Proceedings of the Twenty Sev-
enth Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(poster descritpion.
Koby Crammer, Mark Dredze, John Blitzer, and Fer-
nando Pereira. 2008. Batch performance for an on-
line price. In The NIPS 2007 Workshop on Efficient
Machine Learning.
Lou Denti. 2004. Introduction: Pointing the way: Teach-
ing reading to struggling readers at the secondary level.
Reading and Writing Quarterly, 20:109?112.
Ted Hasselbring and Laura Goin. 2004. Literacy instruc-
tion for older struggling readers: What is the role of
technology? Reading and Writing Quarterly, 20:123?
144.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2006. Classroom success of an intelligent
tutoring system for lexical practice and reading com-
prehension. In Proceedings of the Ninth International
Conference on Spoken Language Processing.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammatical
features to improve readability measures for first and
second language texts. In Proceedings of the Human
Language Technology Conference. Rochester, NY.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In Proceedings of the Hu-
man Language Technology and North American As-
sociation for Computational Linguistics Conference
(HLT/NAACL 2004).
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring systems.
Natural Language Engineering, 10(1).
Sarah Petersen and Mari Ostendorf. 2006. Assessing the
reading level of web pages. In Proceedings of Inter-
speech 2006 (poster), pages 833?836.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
T. D. Snyder, A.G. Tan, and C.M. Hoffman. 2006. Digest
of education statistics 2005 (nces 2006-030). In U.S.
Department of Education, National Center for Edu-
cation Statistics. Washington, DC: U.S. Government
Printing Office.
John Strucker, Yamamoto Kentaro, and Irwin Kirsch.
2007. The relationship of the component skills of
reading to ials performance: Tipping points and five
classes of adult literacy learners. In NCSALL Reports
29. Boston: National Center for the Study of Adult
Learning and Literacy (NCSALL).
Thomas Weinstein and Herbert J. Walberg. 1993. Practi-
cal literacy of young adults: educational antecedents
and influences. Journal of Research in Reading,
16(1):3?19.
97
Coling 2010: Demonstration Volume, pages 41?44,
Beijing, August 2010
Antelogue: Pronoun Resolution for Text and Dialogue
Eleni Miltsakaki
University of Pennsylvania
elenimi@seas.upenn.edu
Abstract
Antelogue is a pronoun resolution prototype de-
signed to be released as off-the-shelf software to
be used autonomously or integrated with larger
anaphora resolution or other NLP systems. It has
modules to handle pronouns in both text and dia-
logue. In Antelogue, the problem of pronoun reso-
lution is addressed as a two-step process: a) acquir-
ing information about properties of words and the
entities they represent and b) determining an algo-
rithm that utilizes these features to make resolution
decisions. A hybrid approach is implemented that
combines known statistical and machine learning
techniques for feature acquisition and a symbolic
algorithm for resolution.
1 Introduction
Pronoun resolution is the well-known problem of
identifying antecedents for pronominal references
in text or dialogue. We present a prototype of
new system for pronoun resolution, Antelogue,
that handles both text and dialogues. In our ap-
proach, pronoun resolution is done in two steps:
a) feature acquisition of properties of words and
the entities they represent and b) resolution algo-
rithm. We adopt a hybrid approach to the problem,
using statistical and machine learning techniques
widely available in the NLP literature to collect
features and a symbolic algorithm informed by
prior research in anaphora resolution and models
of entity salience to appropriately rank and evalu-
ate antecedents.
The design and architecture of Antelogue is
modular and flexible and will soon be released
for off-the-shelf use as an independent compo-
nent or for possible integration of larger anaphora
resolution systems, such as the GuiTAR (Gen-
eral Tool for Anaphora Resolution) (Poesio and
Kabadjov, 2004) that currently is released with
(Mitkov et al, 2002)?s statistical pronoun resolu-
tion algorithm, MARS, that processes pronouns in
text. Motivation for building a new algorithm for
text and dialogues has been the problem of align-
ment between caption dialogues and stage direc-
tions on one hand and video content in movies on
the other. While pronoun resolution in stage direc-
tions proved to be a fairly easy task, in dialogues
we are facing the following challenges:
1. Part of speech taggers trained on text (typically
the Wall Street Journal texts of Penn Treebank)
perform poorly on dialogues, primarily due to the
fragmented nature of spoken language. As a result
NP tags are overgenerated.
2. Fragmentary speech and disfluencies or false
starts common in dialogues cannot be handled by
parsers trained on text.
3. First and second person pronouns are common.
Special algorithms are needed to handle them.
4. Special addressee patterns need to be identified
to block first and second person named references
(e.g., ?Hey, John, where did he go??) becoming
antecedents for third person pronouns.
5. In dialogues, pronouns can be used for ref-
erence to people or objects that are visually but
not textually accessible. Special algorithms are
needed to identify when an antecedent is not
present in the text.
6. Pronouns are used for reference to people or
objects that are visually salient in the scene but not
mentioned explicitly in the dialogue, i.e., there are
no textual antecedents.
7. Multi-party dialogues, sometimes 3rd person
pronouns are used to refer to other speakers. It is
hard to identify when an instance of a 3rd person
pronoun has an antecedent in the prior discourse
41
or another speaker.
In what follows, we present the system?s de-
sign and architecture and the components that
have already been implemented. In the demo, the
users will be able to use Antelogue?s GUI to enter
their own data and evaluate the system?s perfor-
mance in real time. The current version handles
first, second, and third person singular pronouns,
including a classification recognizing referential
and non-referential instances of ?it?. Antelogue
does not, yet, handle plural pronouns or recognize
impersonal uses of singular ?you?.
Resource 
Processor
Resource 
Processor
Resource 
Processor
Input text
Antelogue Repository
Pronoun Resolution
XML-
annotation
E-Grid 
representation
Resource Resource Resource
Figure 1: General System Architecture
2 System design
The problem of pronoun resolution is addressed
as a two-step process: a) acquiring information
about properties of words and the entities they
represent and b) determining an algorithm that uti-
lizes these features to make resolution decisions.
A hybrid approach is implemented that combines
known statistical and machine learning techniques
for feature acquisition and a symbolic algorithm
for resolution.
For the feature acquisition step, any number
of feature acquisition sub-modules can be imple-
mented. The architecture is flexible such that new
feature acquisition modules can be added as they
may become available or deemed crucial for spe-
cific applications. The demo version acquires fea-
tures from a sentence tokenizer, word tokenizer,
NER tagger, gender and number database and
POS tagger. For every sub-module a correspond-
ing parser analyzes the output of the submodules
to retrieve the features and store them in the Ante-
logue repository.
The resolution step implements an algorithm
for utilizing the features in the repository to make
resolution decisions. The resolution module needs
to communicate only with the repository to get
feature information and outputs xml annotated
text or, what we call, e-grid output in which pro-
nouns have been replaced by their antecedents. If
the identified antecedent is a pronoun, it is fur-
ther looked-up until a non-pronominal antecedent
is found. A pronominal antecedent is shown only
in case there is no nominal antecedent available.
The architecture of Antelogue is illustrated in
Fig. 1. Antelogue can be set to perform pro-
noun resolution in both dialogue and text. A pre-
processing step is required to ensure that the files
are in the appropriate format. Because Antelogue
was built to perform pronoun resolution in the di-
alogues and stage directions of screenplays, the
pre-processing steps required to extract dialogues
and text from the TV seriesLost, are available.
3 System architecture
Feature acquisition Sentence and word tok-
enization: built based on (Ratnaparkhi, 1996).
To address dialogue idiosyncrasies, sentence to-
kenization is forced to respect speaker turns thus
blocking forming sentences across speaker turns.
Word processor. This module processes the word
tokenized file and creates an indexed entry for ev-
ery word in the Antelogue repository.
Named Entity Recognizer tagging (NER): We in-
tegrated Stanford?s NER tagger (Finkel et al,
2005).
NER processor. This module processor the NER
tagged file and associates identified NER tags
with the corresponding words in the Antelogue
repository.
Gender and Animacy processor: This modules
collects gender information from the gender cor-
pus1 (Bergsma and Lin, 2006) and checks a self-
1http://www.cs.ualberta.ca/?bergsma/
Gender.
42
made corpus for profession (teacher, doctor, etc)
and family relations (mother, father, etc), ex-
tracted from web searches. In the gender corpus,
gender and number data are collected statistically
and are not always reliable. We developed a sim-
ple confidence metric to evaluate the reliability of
the gender and number data. If the ratio of the
highest probability to the sum of all other proba-
bilities is lower than 60% we mark gender or num-
ber unknown.2 Part-of-speech tagging (POS). We
trained (Ratnaparkhi, 1996)?s POS tagger on di-
alogue data obtained from the English CTS Tree-
bank with Structural Metadata released by LDC in
2009. POS parser. This modules parses the POS-
tagged input and updates the Antelogue reposi-
tory.
Pronoun resolution The pronoun resolution
submodule, currently, has three submodules: a)
first and second person pronouns, b) third person
singular masculine and feminine pronouns, and c)
third person singular neuter pronouns.
For the first and second person pronouns, Ante-
logue identifies and resolves all instances of ?I? to
the speaker name and all instances of ?you? to the
next speaker. It there is no other speaker (when
?you? is in the last turn), the algorithm will pick
the speaker from the previous turn. If there is no
previous turn, it is declared unresolvable.
For the third person ?he? and ?she? module, the
algorithmAntelogue searches for pronouns back-
wards starting at the last sentence of the dialogue.
For every sentence we construct a list of potential
antecedents identified as nouns or pronouns by the
POS tagger. A number of filters, then apply, to fil-
ter out incompatible antecedents. A category of
incompatible antecedents for ?he? and ?she? that
is almost unique to dialogues are addressee ref-
erences. We identify references to addressee us-
ing surface punctuation features. Resolution starts
with a look-up at antecedents of the current sen-
tences, processing them from left-to-right. If the
first antecedent is identified in the human cor-
pus and has compatible gender information, it is
picked. If not, the gender corpus is searched for
reliable matches. Once a match is identified, it
2(Charniak and Elsner, 2009)?s system ?learns? gender in-
formation using Expectation Maximization.
is filtered by NER. The gender corpus often as-
signs feminine or masculine gender to common
nouns. Only those entities that have a NER tag
pass the compatibility test. If no compatible an-
tecedent is found in the current sentence, Antel-
ogue continues search in the previous sentence. If
the dialogues have scene boundaries, as the case
is in Lost, the search for an antecedents stops at
a scene boundary. Otherwise it will not stop be-
fore the first sentence of the dialogue is reached.
If no compatible antecedent is found, it is declared
?unresolvable?. Correctly declaring pronouns un-
resolvable is extremely useful in dialogues, espe-
cially from movies, in which a referent of a third
person pronoun may be visually available but not
introduced in the prior discourse. Correctly un-
resolvable feminine and masculine pronouns sig-
nal a cue for search in the visuals scene, a cross-
modal direction that we are pursuing as part of fu-
ture work.
For the third person ?it?, we first need to ad-
dress the issue of identifying referential and non-
referential instances of ?it?.3 Non-referential in-
stances of ?it? include pleonastic ?it? (e.g., ?it
rains?, or ?it is certain that...?) and references to
a verbal or other clausal antecedent (e.g., ?it? in
?Mary got the award. It?s wonderful!). For the
?it? classification task, we follow (Bergsma et al,
2008)? approach. We generate 4 and 5 word pat-
terns out using the found occurrences of ?it? then
replace ?it/its? with ?they/theirs/them?. Frequen-
cies of the substituted versions are computed us-
ing data from the Google n-gram corpus. If substi-
tutions with ?they/theirs/them? are not common,
?it? is classified as non-referential.
Antelogue outputs a)an XML file with annota-
tions of entities, pronouns and antecedents, and
b)an ?e-grid representation file? in which all pro-
nouns have been replaced with their referents. In
the XML file, pronouns are either resolved or
declared unresolvable if no antecedent is identi-
fied. The pronoun ?it? can, additionally, be de-
clared non-referential. The e-grid representation
file is useful for evaluating text coherence using
the file directly as input to the (Barzilay and La-
pata, 2008)?s e-grid model, a direction we want
3For simplicity, we are sloppy here using the term non-
referential to mean non-referring to a nominal entity.
43
to take in the future to explore its strengths in
automatically identifying scene boundaries. De-
spite well-known problems in making meaningful
comparisons in pronoun resolution systems, An-
telogue?s performance is comparable to some of
the highest reported performances, either identify-
ing correctly an antecedent or correctly declaring
a pronoun unresolvable or non-referential in 85%
of 600 annotated pronouns.
Text module: Antelogue?s architecture for re-
solving pronouns in text is identical to dialogues
except that a)the pre-processing text extracts text
from the stage directions in the screenplay, b)
addressee patterns are not used to filter out an-
tecedents for ?he? and ?she? and instances of ?I?
and ?you? are ignored. In the future we plan to
implement resolution of ?I? and ?you? as well as
a dialogue style resolution of ?he? and ?she? for
instances of embedded speech. These instances
were extremely rare in our data but they need to
be catered for in the future. Antelogue?s perfor-
mance exceeds 90% for stage directions because
stage directions are relatively simple and fairly
unambiguous. For this reason, a syntactic parse
which slows down the system considerably was
not used. However, to retain similar levels of per-
formance in different domains, the use of syntac-
tic parse will be needed.
4 Antelogue API and demo
Antelogue is implemented in Java. Its API in-
cludes an executable file, an empty database for
the repository and command line instructions for
running the system. The dialogue POS tagger is
also available. The other feature acquisition sub-
modules, text POS tagger, NER tagger and gen-
der database are publicly available. Antelogue
makes use of the google n-gram corpus, available
through the Linguistic Data Consortium.4
As an off-the-shelf application, designed both
for integration but also for experimentation, eval-
uation and comparison with other systems, Ante-
logue runs on a single unix command. The user
is prompted to choose the dialogue or text module
and then is asked to determine the path with the
4http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13
data. Antelogue returns annotated files with re-
solved pronouns in seconds for a reasonably sized
file (approx. 2,000-3,000 words) or in couple of
minutes for very large files. These processing
time estimates apply to the demo version. Pro-
cessing time will carry depending on the number
of submodule implemented in the feature acquisi-
tion step.
For the demo, we built a special Graphical User
Interface. In the left part of the GUI, the user can
either type in his or her own text or dialogue, paste
text or dialogue, or select a local file. There are se-
lections for the text/dialogue mode and xml/e-grid
outputs. Antelogue performs pronoun resolution
in real time and show the results on the right hand
side part of the GUI.
Acknowledgments: Special thanks to Ben
Taskar for his help and guidance in this project
and to NSF IIS-0803538 grant for financial
support.
References
Barzilay, R. and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. Computational
Linguistics.
Bergsma, S. and D. Lin. 2006. Bootstrapping path-
based pronoun resolution. In ACL?06, pages 33?40.
Bergsma, S., D. Lin, and R. Goebel. 2008. Distribu-
tional identification of non-referential pronouns. In
ACL?08, pages 10?18.
Charniak, E. and M. Elsner. 2009. Em works for pro-
noun resolution. In Proceedings of EACL 2009.
Finkel, J.R., T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. Ann Arbor,
100.
Mitkov, R., R. Evans, and C. Orasan. 2002. A new,
fully automatic version of Mitkov?s knowledge-poor
pronoun resolution method. Lecture notes in com-
puter science, pages 168?186.
Poesio, M. and M.A. Kabadjov. 2004. A general-
purpose, off-the-shelf anaphora resolution module:
Implementation and preliminary evaluation. In
Proc. of the 4th International Conference on Lan-
guage Resources and Evaluation. Lisbon, Portugal.
Citeseer.
Ratnaparkhi, A. 1996. A maximum entropy model
for part-of-speech tagging. In In Proceedings of
EMNLP?96, pages 133?142.
44
Proceedings of the EACL 2009 Demonstrations Session, pages 49?52,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Matching Readers? Preferences and Reading Skills with Appropriate Web
Texts
Eleni Miltsakaki
University of Pennsylvania
Philadelphia, U.S.A.
elenimi@seas.upenn.edu
Abstract
This paper describes Read-X, a system designed to
identify text that is appropriate for the reader given
his thematic choices and the reading ability asso-
ciated with his educational background. To our
knowledge, Read-X is the first web-based system
that performs real-time searches and returns results
classified thematically and by reading level within
seconds. To facilitate educators or students search-
ing for reading material at specific reading levels,
Read-X extracts the text from the html, pdf, doc,
or xml format and makes available a text editor for
viewing and editing the extracted text.
1 Introduction
The automatic analysis and categorization of web
text has witnessed a booming interest due to in-
creased text availability of different formats (txt,
ppt, pdf, etc), content, genre and authorship. The
web is witnessing an unprecedented explosion in
text variability. Texts are contributed by users of
varied reading and writing skills as opposed to the
earlier days of the Internet when text was mostly
published by companies or institutions. The age
range of web users has also widened to include
very young school and sometimes pre-school aged
readers. In schools the use of the Internet is
now common to many classes and homework as-
signments. However, while the relevance of web
search results to given keywords has improved
substantially over the past decade, the appropri-
ateness of the results is uncatered for. On a key-
word search for ?snakes? the same results will be
given whether the user is a seven year old elemen-
tary school kid or a snake expert.
Prior work on assessing reading level includes
(Heilman et al, 2007) who experiment with a sys-
tem that employs grammatical features and vocab-
ulary to predict readability. The system is part of
the the REAP tutor, designed to help ESL learn-
ers improve their vocabulary skills. REAP?s infor-
mation retrieval system (Collins-Thompson and
Callan, 2004) is based on web data that have been
annotated and indexed off-line. Also, relatedly,
(Schwarm and Ostendorf, 2005) use a statistical
language model to train SVM classifiers to clas-
sify text for grade levels 2-5. The classifier?s pre-
cision ranges from 38%- 75% depending on the
grade level.
In this demo, we present Read-X, a system de-
signed to evaluate if text retrieved from the web
is appropriate for the intended reader. Our sys-
tem analyzes web text and returns the thematic
area and the expected reading difficulty of the re-
trieved texts. 1 To our knowledge, Read-X is the
first system that performs in real time a)keyword
search, b)thematic classification and c)analysis of
reading difficulty. Search results and analyses are
returned within a few seconds to a maximum of a
minute or two depending on the speed of the con-
nection. Read-X is enhanced with an added com-
ponent which predicts difficult vocabulary given
the user?s educational level and familiarity with
specific thematic areas.
2 Web search and text classification
Internet search. Read-X uses Yahoo! Web Ser-
vices to execute the keyword search. When the
search button is clicked or the enter key depressed
after typing in a keyword, Read-X sends a search
request to Yahoo! including the keywords and, op-
tionally, the number of results to return.
Text extraction. The html, xml, doc or pdf doc-
uments stored at each URL are then extracted in a
cleaned-up, tag-free, text format. At this stage a
decision is made as to whether a web page con-
tains reading material and not ?junk?. This is a
non-trivial task. (Petersen and Ostendorf, 2006)
use a classifier for this task with moderate success.
We ?read? the structure of the html text to decide if
the content is appropriate and when in doubt, we
1A demo video can be accessed at the blogsite
www.eacl08demo.blogspot.com.
49
Figure 1: Search results and analysis of readability
err on the side of throwing out potentially useful
content.
Readability analysis. For printed materials,
there are a number of readability formulas used
to measure the difficulty of a given text; the New
Dale-Chall Readability Formula, The Fry Read-
ability Formula, the Gunning-Fog Index, the Au-
tomated Readability Index, and the Flesch Kin-
caid Reading Ease Formula are a few examples
(see (DuBay, 2007) for an overview and refer-
ences). Usually, these formulas count the number
of syllables, long sentences, or difficult words in
randomly selected passages of the text. To auto-
mate the process of readability analysis, we chose
three Readability algorithms: Lix, RIX, see (An-
derson, 1983), and Coleman-Liau, (Coleman and
Liau, 1975), which were best suited for fast cal-
culation and provide the user with either an ap-
proximate grade level for the text or a readability
classification of very easy, easy, standard, difficult
or very difficult. When each text is analyzed, the
following statistics are computed: total number
of sentences, total number of words, total number
of long words (seven or more characters), and to-
tal number of letters in the text. Steps have been
taken to develop more sophisticated measures for
future implementations. Our current research aims
at implementing more sophisticated reading diffi-
culty measures, including reader?s familiarity with
the topic, metrics of propositional density and dis-
course coherence, without compromising speed of
Formula r3 r4 r5
Lix 10.2 (9-11) 11.7 (10-13) 11.1 (9-12)
RIX 10.2 (8-13) 12.3 (10-13) 11.5 (10-13)
Coleman-Liau 11.65 (9.2-13.3) 12.67 (12.2-13.1) 12.6 (11.4-14.1)
All 10.6 12.3 11.7
Table 1: Comparison of scores from three read-
ability formulae.
processing.
To evaluate the performance of the reading
scores we used as groundtruth a corpus of web-
texts classified for readability levels r3, r4, r5 cor-
responding to grade levels 7-8, 9-10, and 11-13 re-
spectively.2 The content of the corpus is a collec-
tion of web-sites with educational content, picked
by secondary education teachers. For 90 docu-
ments, randomly selected from levels 3-5 (30 per
level), we computed the scores predicted by Lix,
RIX and Coleman-Liau.
The average scores assigned by the three formu-
las are shown in Table (1). The numbers in paren-
theses show the range of scores assigned by each
formula for the collection of documents under
each reading level. The average score of all formu-
las for r3 is 10.6 which is sufficiently differentiated
from the average 12.3. for r4. The average score of
all formulas for r5, however, is 11.7, which cannot
be used to differentiate r4 from r5. These results
indicate that at least by comparison to the data in
2With the exception of Spache and Powers-Sumner-Kearl
test, all other readability formulas are not designed for low
grade readability levels.
50
Classifier Basic categories Subcategories
Naive Bayes 66% 30%
MaxEnt 78% 66%
MIRA 76% 58%
Table 2: Performance of text classifiers.
our corpus, the formulas can make reasonable dis-
tinctions between middle school and high school
grades but they cannot make finer distinctions be-
tween different high-school grades. A more reli-
able form of evaluation is currently underway. We
have designed self-paced reading experiments for
different readability scores produced by five for-
mulas (RIX, Lix, Coleman-Liau, Flesch-Kincaid
and Dale-Chall). Formulas whose predictions will
more closely reflect reading times for text compre-
hension will be preferred and form the basis for
a better metric in the future. In the current im-
plementation, Read-X reports the scores for each
formula in a separate column. Other readability
features modeling aspects of discourse coherence
(e.g.,(Miltsakaki and Kukich, 2004), (Barzilay and
Lapata, 2008), (Bruss et al, 2004), (Pitler and
Nenkova, 2008)) can also be integrated after psy-
cholinguistic evaluation studies are completed and
their computation of such features can be made in
real time.
Text classification For the text classification
task, we a) built a corpus of prelabeled thematic
categories and b) compared the performance of
three classifiers to evaluate their suitability for the-
matic classification task.3
We collected a corpus of approximately 3.4 mil-
lion words. The corpus contains text extracted
from web-pages that were previously manually
classified per school subject area by educators.
We organized it into a small thematic hierarchy,
with three sets of labels: a) labels for supercat-
egories, b) labels for basic categories and c) la-
bels for subcategories. There are 3 supercategories
(Literature, Science, Sports), 8 basic categories
(Arts, Career and Business, Literature, Philosophy
and Religion, Science, Social studies, Sports and
health, Technology) and 41 subcategories (e.g.,
the subcategories for Literature are Art Criticism,
Art History, Dance, Music, Theater).
The performance of the classifiers trained on the
basic categories and subcategories data is shown
3We gratefully acknowledge MALLET, a collection of
statistical NLP tools written in Java, publicly available at
http://mallet.cs.umass.edu and Mark Dredze for
his help installing and running MIRA on our data.
in Table (2). All classifiers perform reasonably
well in the basic categories classification task but
are outperformed by the MaxEnt classifier in both
the basic categories and subcategories classifica-
tions. The supercategories classification by Max-
Ent (not shown in the Table) is 93%. As expected,
the performance of the classifiers deteriorates sub-
stantially for the subcategories task. This is ex-
pected due to the large number of labels and the
small size of data available for each subcategory.
We expect that as we collect more data the perfor-
mance of the classifiers for this task will improve.
In the demo version, Read-X uses only the Max-
Ent classifier to assign thematic labels and reports
results for the super categories and basic cate-
gories, which have been tested and shown to be
reliable.
3 Predicting difficult words given
reader?s background
The analysis of reading difficulty based on stan-
dard readability formulas gives a quick and easy
way to measure reading difficulty but these formu-
las lack sophistication and sensitivity to the abili-
ties and background of readers. They are reason-
ably good at making rough distinctions between
-standardly defined- middle, high-school or col-
lege levels but they fall short in predicting reading
ease or difficulty for specific readers. For exam-
ple, a reader who is familiar with literary texts will
have less difficulty reading new literary text than
a reader, with a similar educational background,
who has never read any literary works. In this
section, we discuss the first step we have taken
towards making more reliable evaluations of text
readability given the profile of the reader.
Readers who are familiar with specific thematic
areas, are more likely to know vocabulary that is
recurring in these areas. So, if we have vocab-
ulary frequency counts per thematic area, we are
in a better position to predict difficult words for
specific readers given their reading profiles. Vo-
cabulary frequency lists are often used by test de-
velopers as an indicator of text difficulty, based on
the assumption that less frequent words are more
likely to be unknown. However, these lists are
built from a variety of themes and cannot be cus-
tomized for the reader. We have computed vocab-
ulary frequencies for all the basic thematic cate-
gories in our corpus. The top 10 most frequent
words per supercategory are shown in Table (3).
51
Arts Career and Business Literature Philosophy Science Social Studies Sports, Health Technology
Word Freq Word Freq Word Freq Word Freq t Word Freq Word Freq Word Freq Word Freq
musical 166 product 257 seemed 1398 argument 174 trees 831 behavior 258 players 508 software 584
leonardo 166 income 205 myself 1257 knowledge 158 bacteria 641 states 247 league 443 computer 432
instrument 155 market 194 friend 1255 augustine 148 used 560 psychoanalytic 222 player 435 site 333
horn 149 price 182 looked 1231 belief 141 growth 486 social 198 soccer 396 video 308
banjo 128 cash 178 things 1153 memory 130 acid 476 clemency 167 football 359 games 303
american 122 analysis 171 caesar 1059 truth 130 years 472 psychology 157 games 320 used 220
used 119 resources 165 going 1051 logic 129 alfalfa 386 psychotherapy 147 teams 292 systems 200
nature 111 positioning 164 having 1050 things 125 crop 368 united 132 national 273 programming 174
artist 104 used 153 asked 1023 existence 115 species 341 society 131 years 263 using 172
wright 98 sales 151 indeed 995 informal 113 acre 332 court 113 season 224 engineering 170
Table 3: 10 top most frequent words per thematic category.
Vocabulary frequencies per grade level have also
been computed but they are not shown here.
We have added a special component to the
Read-X architecture, which is designed to pre-
dict unknown vocabulary given the reader?s ed-
ucational background or familiarity with one (or
more) of the basic themes. The interface al-
lows you to select a web search result for further
analysis. The user can customize vocabulary dif-
ficulty predictions by selecting the desired grade
or theme. Then, the text is analyzed and, in a
few seconds, it returns the results of the analysis.
The vocabulary evaluator checks the vocabulary
frequency of the words in the text and highlights
the words that do not rank high in the vocabulary
frequency index for the chosen categories (grade
or theme). The highlighted words are clickable.
When they are clicked, the entry information from
WordNet appears on the right panel. The system
has not been evaluated yet so some tuning will
be required to determine the optimal cut-off fre-
quency point for highlighting words.
4 Future work
A major obstacle in developing better readability
models is the lack of reliable ?groundtruth? data.
Annotated data are very scarce but even such data
are only partially useful as it is not known if inter-
annotator agreement for readability levels would
be high. To address this issue we are currently
running a battery of self-paced reading and eye-
tracking experiments a) to evaluate which, if any,
readability formulas accurately predict differences
in reading times b)to test new hypotheses about
possible factors affecting the perceived difficulty
of a text, including vocabulary familiarity, propo-
sitional density and discourse coherence.
Acknowledgments
Audrey Troutt developed the software for Read-
X under a GAPSA Provost?s Award for Interdisci-
plinary Innovation, University of Pennsylvania.
References
Jonathan Anderson. 1983. Lix and rix: Variations of a little-
known readability index. Journal of Reading, 26(6):490?
496.
Regina Barzilay and Mirella Lapata. 2008. Modeling lo-
cal coherence: An entity-based approach. Computational
Linguistics.
M. Bruss, M. J. Albers, and D. S.McNamara. 2004. Changes
in scientific articles over two hundred years: A coh-metrix
analysis. In Proceedings of the 22nd Annual International
Conference on Design of Communication: the Engineer-
ing of Quality Documentation, pages 104?109. New York:
ACM Press.
M Coleman and T. Liau. 1975. A computer readability for-
mula designed for machine scoring. Journal of Applied
Psychology, 60:283?284.
K. Collins-Thompson and J. Callan. 2004. Information re-
trieval for language tutoring: An overview of the REAP
project. In Proceedings of the Twenty Seventh Annual In-
ternational ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (poster descritpion.
William DuBay. 2007. Smart Language: Readers, Read-
ability, and the Grading of Text. BookSurge Publishing.
overview of readability formulas and references.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Eske-
nazi. 2007. Combining lexical and grammatical features
to improve readability measures for first and second lan-
guage texts. In Proceedings of the Human Language Tech-
nology Conference. Rochester, NY.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Natural
Language Engineering, 10(1).
Sarah Petersen and Mari Ostendorf. 2006. Assessing the
reading level of web pages. In Proceedings of Interspeech
2006 (poster), pages 833?836.
Emily Pitler and Ani Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings of EMNLP, 2008.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading level
assessment using support vector machines and statistical
language models. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational Lin-
guistics, pages 523?530.
52
Refining the Meaning of Sense
Labels in PDTB: ?Concession?
Livio Robaldo
University of Turin (Italy)
email: robaldo@di.unito.it
Eleni Miltsakaki
University of Pennsylvania (USA)
email: elenimi@linc.cis.upenn.edu
Jerry R. Hobbs
University of Southern California (USA)
email: hobbs@isi.edu
Abstract
The most recent release of PDTB 2.0 contains annotations of senses of
connectives. The PDTB 2.0 manual describes the hierarchical set of
senses used in the annotation and offers rough semantic descriptions of
each label. In this paper, we refine the semantics of concession sub-
stantially and offer a formal description of concessive relations and the
associated inferences drawn by the reader, utilizing basic notions from
Hobbs?s logic, including the distinction between causes and causal com-
plexes (Hobbs, 2005). This work is part of a larger project on the se-
mantics of connectives which aims at developing formal descriptions of
discourse relations, useful for processing real data.
207
208 Robaldo, Miltsakaki, and Hobbs
1 Introduction
As the demand for more powerful NLP applications increases, there is also an in-
creasing need to develop algorithms for automated processing of discourse relations
and models for deriving the inferences drawn by the reader. PDTB 2.0 (Prasad et al,
2008), released in January 2008, contains annotations of discourse connectives and
their arguments, attribution, and sense labels giving rough semantic descriptions of
the connectives. The availability of such a richly annotated corpus promises to boost
our understanding of the structure and meaning of discourse and will facilitate the
development of efficient algorithms for identifying discourse connectives and their
arguments.
However, in order to be able to derive appropriate inferences associated with dis-
course relations, we need to develop useful semantic analyses of the meaning of con-
nectives so that they will generate the same range of inferences made by humans. In
this paper we take a first step in that direction, offering a simple formal analysis of
concessive relations, thus refining the semantics of the concessive sense labels used
in PDTB 2.0. Our analysis uses basic notions of causality developed in Hobbs (1998,
2005), capitalizing on the distinction between causes and causal complexes and on
the semantics of defeasible causality. Concessive meaning involves the failure of a
general defeasible causal relation in this specific instance.
The paper is organized as follows. Section 2 gives an overview of the PDTB 2.0,
focusing on the annotation of the senses of connectives, especially ?concession?. In
Section 3, we present an overview of the framework we are adopting for our formal
analysis, namely, Hobbs?s logic of causality, and our basic claims about how the se-
mantics of defeasible causality contributes to the semantics of concession. Section 4
presents the semantic analysis of ?concession?. In Section 5, we report briefly on the
distribution of concessive labels in PDTB 2.0 and conclude in Section 6.
2 Sense labels in PDTB
The Penn Discourse Treebank provides annotations of the argument structure of dis-
course connectives, attribution (e.g., ?ownership? of the relation by the writer or other
individual), and semantic labels for all the annotated connectives (Prasad et al, 2008).
This annotation of discourse connectives and their arguments draws on a lexical ap-
proach to discourse structure (Webber et al, 2003; Webber and Joshi, 2003), viewing
discourse connectives as discourse-level predicates that take two abstract objects such
as events, states, and propositions (Asher, 1993) as their arguments.
Two major types of discourse connectives are annotated in PDTB: a) explicit con-
nectives including subordinate conjunctions, coordinate conjunctions and adverbials,
and b) implicit connectives that are inserted between two adjacent sentences to cap-
ture the meaning of the inferred relation when no explicit connective is present. The
PDTB 2.0 is, to date, the largest annotation effort at the discourse level, including ap-
proximately 40,000 triples in the form (Connective, Arg1, Arg2). Arg2 is the second
argument in the text in the case of coordinating conjunctions, and is the complement
of subordinating conjunctions. In the case of adverbs, Arg2 is the element which the
adverb modifies syntactically. In cases of ambiguity, sense labels indicate the intended
sense in the given context. In all other cases, sense labels provide semantic descrip-
Refining the Meaning of Sense Labels in PDTB: ?Concession? 209
tions of the relations conveyed by the connectives, both explicit and implicit.
The tagset of senses is organized hierarchically (Miltsakaki et al, 2008). The top
level, or class level, has four tags representing four major semantic classes: ?TEMPO-
RAL?, ?CONTINGENCY?, ?COMPARISON? and ?EXPANSION?. For each class,
a second level of types is defined to further refine the semantics of the class levels.
For example, ?CONTINGENCY? has two types ?Cause? (relating two situations via
a direct cause-effect relation) and ?Condition? (relating a hypothetical scenario with
its (possible) consequences). A third level of subtype specifies the semantic contribu-
tion of each argument. For ?CONTINGENCY?, its ?Cause? type has two subtypes ?
?reason? (which applies when the connective indicates that the situation specified in
Arg2 is interpreted as the cause of the situation specified in Arg1, as often with the
connective because) and ?result? (which is used when the connective indicates that
the situation described in Arg2 is interpreted as the result of the situation presented in
Arg1). That is, ?reason? occurs when Arg2 causes Arg1; ?result? occurs when Arg1
causes Arg2.
Connectives can also be used to relate arguments pragmatically as in John is in
the house because the lights are on or If you?re thirsty, there?s beer in the fridge,
where the relation involbes the belief in or the telling of the condition rather than the
condition itself. For these rhetorical or pragmatic uses of connectives, a small set of
pragmatic sense tags has been defined ? specifically, ?Pragmatic Cause?, ?Pragmatic
Condition?, ?Pragmatic Contrast? and ?Pragmatic Concession?.
2.1 ?Concession? in PDTB
?Concession? is a type of the class-level category ?COMPARISON?. The class tag
?COMPARISON? applies when the connective indicates that a discourse relation is
established between Arg1 and Arg2 in order to highlight prominent differences be-
tween the two situations. Semantically, the truth of both arguments is independent of
the connective or the established relation. ?COMPARISON? has two types that further
specify its semantics. In some cases, Arg1 and Arg2 share a predicate or a property
and the difference is highlighted with respect to the values assigned to this property.
This interpretation is tagged with the type ?Contrast?.
There are also cases in which the highlighted differences are related to expectations
raised by one argument which are then denied by the other. This intepretation is
tagged with the type ?Concession?. According to the description in the PDTB 2.0
manual, the type ?Concession? applies when the connective indicates that one of the
arguments describes a situation A which normally causesC, while the other asserts (or
implies) ?C. Alternatively, one argument denotes a fact that triggers a set of potential
consequences, while the other denies one or more of them.
Two ?Concession? subtypes are defined in terms of the argument creating an ex-
pectation and the one denying it. Specifically, when Arg2 creates an expectation that
Arg1 denies, it is tagged as ?expectation?, shown in (1.c-d). When Arg1 creates an
expectation that Arg2 denies, it is tagged as ?contra-expectation?, shown in (1.e-f).
Examples (1.a-b) are made-up sentences we use for explanation and will be discussed
here and in the next section. All other examples are taken from PDTB 2.0. Each dis-
course fragment in (1) distinguishes between a discourse connective (underlined), and
two sentence-arguments: Arg1 (italics) and Arg2 (boldface).
210 Robaldo, Miltsakaki, and Hobbs
(1) a. Although John studied hard, he did not pass the exam. (expectation)
b. Although running is considered healthy, it is not advisable for persons
with heart problems. (expectation)
c. Although they represent only 2% of the population, they control nearly
one-third of discretionary income. (expectation)
d. While acquiring a big brand-name company can be a shortcut to
growth, it can also bring a host of unforeseen problems (expectation)
e. The Texas oilman has acquired a 26.2% stake valued at more than $1.2
billion in an automotive-lighting company, Koito Manufacturing Co.
But he has failed to gain any influence at the company. (contra-
expectation)
f. Mr. Cannell?s allegations of cheating ?are purely without foundation?,
and based on unfair inferences. However the state will begin keeping
closer track of achievement-test preparation booklets next spring..
(contra-expectation)
(1.a) is an example of ?expectation?: Arg2 (John studied hard) creates the expecta-
tion that John passed the exam, which is precisely denied by Arg1. The same holds
for (1.b-d). Note that (1.b), unlike (1.a, c-d), expresses a general concessive relation,
i.e., it does not refer to particular contingent events. (1.e-f) are instances of contra-
expectation, where the expectation is created by Arg1. In (1.e), the fact that the Texas
oilman acquired the indicated stake value creates the expectation that he gained influ-
ence at the company, while, in (1.f), since Mr. Cannell?s allegations of cheating are
purely without foundation (in the speaker?s judgement), we do not expect the state to
start tracking the test preparation.
3 Toward a formal definition of ?Concession?
Based on our analysis of the range of PDTB tokens tagged with a concessive label, we
offer here a more detailed semantic analysis of the meaning of concessive relations.
Since the direction of the concessive relation is not relevant, the argument that creates
the expectation and the argument that denies it are respectively termed as Argcexp and
Argdexp. We claim that a concessive relation arises from a contrast between the effects
of two causal relations cc and cd holding in the domain. c and d stand for ?creates? and
?denies?, respectively. The relation denoted by cc is the causal relation that creates the
expectation, and cd the one that denies it. The effects of these causal relations, as well
as their causes, are taken to be eventualities1.
In this paper, we use the letter e for most eventualities, possibly with some subscript
or superscript.2 We make use of the subscripts x1 and x2, respectively, to distinguish
between the causes and the effects in a causal relation cx. Therefore, the causes in
cc and cd are indicated by ec1 and ed1 respectively, and the effects by ec2 and ed2,
respectively. ec2 is the ?created expectation?; its cause ec1 is conveyed by Argcexp. ed2
is an eventuality that denies ec2, and it is explicitly described in Argdexp. The cause of
1The term ?eventuality? is borrowed from (Bach, 1981). It covers both standard notions of ?state? and
?event?.
2As we will see, also causal relations are eventualities; so the names cc and cd are an exception to this
rule.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 211
ed2, i.e., ed1, is usually unknown. Also ec2 is, in principle, unknown, but in most cases
it can be taken as the negation of ed2.
For instance, in the context of (1.a), the eventuality John studied hard (ec1) creates
the expectation John passed the exam (ec2). Nevertheless, Argdexp says that John did
not pass the exam actually (ed2). The reason of ed2 is unknown and has to be found in
the context. In other words, the context, whether explicit or inferred, should include
another eventuality that caused John?s failure, despite his studying hard. For example,
the next sentence might be John was very tired during the exam (ed1).
In order to formalize this account of concession, we need a defeasible notion of
causality. Many authors propose such an account of causality, e.g. (Achinstein, 1965;
Shoham, 1990; Simon, 1991; Bell, 1999, 2003), and Giunchiglia et al (2004). The ac-
count we use is that of Hobbs (2005). This distinguishes between the monotonic, pre-
cise notion of ?causal complex? and the nonmonotonic, defeasible notion of ?cause?.
The former gives us mathematical rigor; the latter is more useful for everyday rea-
soning and can be characterized in terms of the former. As Hobbs (2005) explains,
when we flip a switch to turn on a light, we say that flipping the switch ?caused? the
light to turn on. But for this to happen, many other factors had to be in place. The
bulb had to be intact, the switch had to be connected to the bulb, the power had to
be on in the city, and so on. The set of all the states and events that have to hold or
happen for an effect e to happen are called the ?causal complex? of e. Thus, the flip-
ping of the switch and the normal states of the bulb, the wiring, and the power supply
would all be in the causal complex for the turning on of the light. In a causal complex,
the majority of participating eventualities are normally true and therefore presumed
to hold. In the light bulb case, unless otherwise indicated, it is normally true that the
bulb is not burnt out, that the wiring is intact, that the power is on in the city, and so
on. But the light switch could be on or off; neither can be presumed. Those eventu-
alities that cannot normally be assumed to be true are identified as causes (cf. Kayser
and Nouioua, 2008). They are useful in planning, because they are often the actions
that the planner or some other agent must perform. They are useful in explanation
and prediction because they frequently constitute the new information. They are less
useful in diagnosis, where the whole causal complex has to be considered.
Note that in practice, we can never specify all the eventualities in a causal complex
for an event. So while the notion of causal complex gives us a precise way of thinking
about causality, it is not adequate for the kind of practical reasoning we do in planning,
explaining, and predicting. For this, we need the defeasible notion of ?cause?.
3.1 Background on Hobbs?s logic
Hobbs (1998) proposed a wide coverage logical framework for natural language based
on the notion of reification. Reification is the action of making states and events first-
class individuals in the logic, so they can be referred to by constants and variables.
We ?reify? eventualities, from the Latin word ?re(s)? for ?thing?: we take them to
be things. The framework distinguishes two parallel sets of predicates: primed and
unprimed. The unprimed predicates are the ordinary predicates we are used to in
logical representations of language. For example, (give a b c) says that a gives b to c.
When we assert this, we are saying that it actually takes place in the real world. The
primed predicate is used to talk about the reified eventualities. The expression (give?
212 Robaldo, Miltsakaki, and Hobbs
e a b c) says that e is a giving event by a of b to c. Eventualities may be possible
or actual. When they are actual, this is simply one of their properties. To say that a
state e actually obtains in the real world or that an event e actually occurs in the real
world, we write (Rexist e). That is, e really exists in the real world. If I want to fly, my
wanting really exists, but my flying does not. This is represented as:3
(Rexist e) ? (want? e I e1) ? (fly? e1 I)
Therefore, contrary to (p x), (p? e x) does not say that e actually occurs, only that if
it did, it would be a ?p? event. The relation between primed and unprimed predicates
is then formalized by the following axiom schema:
(forall (x) (iff (p x) (exists(e) (and(p? e x)(Rexist e)))))
Eventualities can be treated as the objects of human thoughts. Reified eventualities
are inserted as parameters of such predicates as believe, think, want, etc. These predi-
cates can be applied in a recursive fashion. The fact that John believes that Jack wants
to eat an ice cream is represented as an eventuality e such that4
(believe? e John e1) ? (want? e1 Jack e2) ?
(eat? e2 Jack Ic) ? (iceCream? e3 Ic)
In Hobbs?s notation, every relation on eventualities, including logical operators, causal
and temporal relations, and even tense and aspect, may be reified into another eventu-
ality. For instance, by asserting (imply? e e1 e2), we reify the implication from e1 to e2
into an eventuality e. e has to be thought as ?the state holding between e1 and e2 such
that whenever e1 really exists, e2 really exists too?. Negation is represented as (not? e1
e2): e1 is the eventuality of the e2?s not existing. Some problems arise with negation,
in that what is generally negated is an eventuality type rather than an eventuality token
or instance. In order to deal with more general cases of concession, we will refer to
eventualities that are inconsistentwith other ones. Two eventualities e1 and e2 are said
to be inconsistent iff they (respectively) imply two other eventualities e3 and e4 such
that e3 is the negation of e4. The definition is as follows:
(forall (e1 e3)
(iff (inconsistent e1 e2)
(and (eventuality e1) (eventuality e2)
(exists (e3 e4) (and (imply e1 e3)
(imply e2 e4)(not? e3 e4))))))
3.2 Typical elements, eventuality types and tokens
Among the things we can think about are both specific eventualities, like Fido is bark-
ing, and general or abstract types of eventualities, like Dogs bark. We do not want to
treat these as radically different kinds of entities. We would like both, at some level, to
3In order to increase readability, we will often make use of the symbol ? in place of the unprimed
predicate and.
4The formula expresses the de-re reading of the sentence, where e1, e2 , e3 , John, Jack, Ic are first order
constants.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 213
be treated simply as eventualities that can be the content of thoughts. To this end, the
logical framework includes the notion of typical element (from Hobbs (1983, 1995,
1998)). The typical element of a set is the reification of the universally quantified
variable ranging over the elements of the set (cf. McCarthy (1977)). Typical elements
are first-order individuals. The introduction of typical elements arises from the need
to move from the standard set-theoretic notation
s = {x | p(x) }
or its logical equivalent,
(forall (x) (iff (member x s) (p x)))
to a simple statement that p is true of a ?typical element? of s by reifying typical
elements. The principal property of typical elements is that all properties of typical
elements are inherited by the real members of the set.
It is important not to confuse the concept of typical element with the standard con-
cept of ?prototype?, which allows defeasibility, i.e., properties that are not inherited
by all of the real members of the set. Asserting a predicate on a typical element of a
set is logically equivalent to the multiple assertions of that predicate on all elements
of the set. Talking about typical elements of sets of eventualities leads to the distinc-
tion between eventuality types and eventuality tokens. The logic defines the following
concepts, for which we omit formal details5: a) Eventualities types (aka abstract even-
tualities): eventualities that involve at least one typical element among their arguments
or arguments of their arguments (we can call these ?parameters?), b) Partially instan-
tiated eventuality types (aka partial instances): a particular kind of eventuality type
resulting from instantiating some of the parameters of the abstract eventuality either
with real members of their sets or with typical elements of subsets, and c) Eventuality
tokens (aka instances: a particular kind of partially instantiated eventuality type with
no parameters. It is a consequence of universal instantiation that any property that
holds of an eventuality type is true of any partial instance of it.
Hobbs?s logical framework is particularly suitable to the study of the semantics of
discourse connectives, in that it allows focusing on their meaning while leaving under-
specified the details about the eventualities involved. In other words, we can simply
assume the existence of two eventualities e1 and e2 coming from the two arguments
Arg1 and Arg2 respectively. e1 and e2 may be either eventuality tokens, on atomic
arguments, as in (1.a), or eventuality tokens, on collective arguments, as in (1.c), or
(partially instantiated) eventuality types, as in (1.b), or any other kind of eventuality.
The semantics of concession proposed below uniformily applies to all these cases.
3.3 Hobbs?s Account of Causality
The account of causality described above in the introduction is represented in terms
of two predicates: (cause? cx ex1 ex2) and (causalComplex s ex2). cause? says that cx is
the state holding between ex1 and ex2 such that the former is a non-presumable cause
5Actually, ?instance? is slightly more general, since if s is a set, x is its typical element, and y is a
member of s, y is an instance of x, even though it is not an eventuality. Nevertheless, in this paper we
assume ?instances? and ?eventuality tokens? to be synonymous.
214 Robaldo, Miltsakaki, and Hobbs
of the latter. causalComplex says that s is the set of all presumable or non-presumable
eventualities that are involved in causing ex2. Obviously, ex1 belongs to s. Thus, in the
light example, the predicate cause applies to the flipping of the switch, while the states
of the bulb, the wiring, and the power supply would all be in the causal complex s.
Several axioms characterize the predicates cause and causalComplex. Some of them
relate causality with time6, some relate causality with probability, and so on Hobbs
(2005).
It is clear that the theory must not include an axiom stating that, whenever a causal
relation cx and its cause ex1 really exist, the corresponding effect ex2 really exists too.
The inclusion of such an axiom would lead to a non-defeasible causality. Rather, we
need an axiom stating that an effect really exists just in case all the eventualities in its
causal complex really exist:
(forall (s e)
(if (and (causalComplex s e)
(forall (e1) (if (member e1 s) (Rexist e1)))
(Rexist e)))
Nevertheless, as pointed out above, we can never specify all the eventualities in a
causal complex. Even in simple sentences like (1.a), the eventualities in the causal
complex are not easy to list, and the real causes may not coincide with what we think
the causes are in that context. For example, recalling our analysis of (1.a) above:
ec1=?John studied hard?
ec2=?John passed the exam?
ed1=?John was tired during the exam?
ed2=?John did not pass the exam?
cc=?ec1 causes ec2?; cd=?ed1 causes ed2?
One approach at this point would be to say that both ec1 and the negation of ed1
belong to the causal complex of ec2, with ec1 being the non-presumable cause of ec2.
But this would mean that not being tired during exams is a kind of ?precondition? for
passing exams by studying hard, which is obviously false in many contexts. Note,
however, that there is an arbitrary quality to what we designate as being in a causal
complex, because causality forms chains and we can start the chain at any point. John
was tired caused the situation that he did not manage to concentrate, which caused
the situation that he made a lot of errors in the exam, which caused the situation that
the teacher decided to fail him. One could argue that the last of these eventualities is
the real cause of ed2. Similarly, one could argue that ec1 is not the real cause of ec2:
John studied hard causes the situation that he makes few errors in the exam . . . and the
teacher decides not to fail him. The predicate cause is defeasibly transitive, however,
so these considerations do not affect our account of concession. Furthermore, we do
not take the negation of ed1 as necessarily belonging to the causal complex for ec2.
Rather, we claim that ed1, besides being the cause of ed2, is the cause of another
eventuality edp that is inconsistent with an element ecp in the causal complex for ec2.
6As argued also by Giordano and Schwind (2004), the effect caused by an eventuality can take place in
the current or in a subsequent instant.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 215
In (1.a), ecp may be simply John does not have any particular health problem that
jeopardizes his passing the exam. ed1 caused both John?s failure and an health status
that jeopardizes the passing of his exam. This is what we mean here by ?denying of
an expectation?.
In our analysis of concession, we distinguish between abstract causalities like hard
studying causes passing exams, and causality tokens like John?s tiredness caused
John?s failure. Note that asserting (Rexist c) on an abstract causal relation c amounts
to asserting (Rexist c?) for any (partial) instance c? of c. But recall that cause is only
defeasible. Both the abstract causal principle and its partial instance are simplified
stand-ins for rules that involve entire causal complexes, not all of whose elements may
obtain. Thus, just because hard studying causes passing exams, we cannot invariably
conclude that if John really studied, he really passed the exam.
4 The meaning of concessive relations
Our basic claim is that the meaning of concessive relations is triggered by a contrast
between two causal relations cc and cd such that one or more eventualities in the causal
complex of ec2 (the expectation created by cc), is denied by ed2 (the effect of cd). cc,
cd , ed2, and ec1 (the cause in cc) really exist in the world, or are at least believed to
exist by the speaker/writer. Furthermore, all eventualities in the causal complex for
ed2, including the non-presumable cause ed1, which is unknown in many cases, really
exist too. Argcexp conveys ec1, while Argdexp conveys ed2.
We also claim that in all cases of concession it seems that what really creates the
expectation is a causal relation cac that is an abstraction of cc. cc really exists in the
world precisely because cac really exists and cc is a partial instance of it. In other words,
the real existence of cc is inherited from cac. On the other hand, there is not necessarily
an abstract counterpart cad for cd that also really exists in the world. For instance,
in (1.a), it seems that what creates the expectation is the assumption that the causal
relation studying hard causes passing exams (cac) really exists in the context. John?s
hard studying causes John?s passing exams (cc) is just an instance of cac. This instance
really exists in the world too. However, since causality is defeasible, the fact that John
really studied hard (ec1) does not entail the real existence of John really passed the
exam (ec2). In fact, this is precisely denied by Argdexp: John did not pass the exam
(ed2). The cause of John?s failure, e.g., John?s tiredness (ed1), is (or is the cause of
an eventuality edp that is) inconsistent with an element ecp of the causal complex for
(ec2), namely, John does not have any particural health problem that jeopardizes the
passing of his exam. Note that we do not necessarily infer that being tired causes
failing an exam: tiredness was the cause of the failure in this particular scenario only.
Therefore, we assert that cd really exists, but we do not advocate the existence of a
more abstract causal relation cad that really exists too.
To summarize, the semantics of concession we propose is formalized in (2). The
conjuncts (Rexist cc) and (Rexist ed1) have been omitted in (2) because they may be
inferred from (Rexist cac) and (Rexist ed2). sc is the causal complex associated with
cc. ec1 and ed2 are given to us in Argcexp and Argdexp respectively, while all other
eventualities may be inferred by abduction from the contextual knowledge; some hints
about how this may be done are provided in Hobbs (2005).
216 Robaldo, Miltsakaki, and Hobbs
(2) (exist (cc ca c ec1 ec2 cd ed1 ed2 sc ecp edp)
(cause? cc ec1 ec2) ? (cause? cd ed1 ed2) ? (Rexist ca c) ?
(partialInstance cc ca c) ? (Rexist cd) ? (Rexist ec1) ?
(Rexist ed2) ? (cause ed1 edp) ? (Rexist edp) ?
(inconsistent ec2 ed2) ? (causalComplex sc ec2) ?
(memberecp sc) ? (inconsistent edp ecp))
Let us now examine how the semantics given in (2) applies for corpus examples
tagged as ?expectation? or ?contra-expectation?. Let us analyze (1.b) in the light of
the semantics proposed in (2). The abstract causality that creates the expectation (ca
c) is Something that is considered healthy for humans is advisable for them7. This
is partially instantiated in Since running is considered healthy for persons with heart
problems, it is advisable for them (cc). Nevertheless, the fact that running is really
considered healthy in the context (ec1) does not suffice to assert that running is really
advisable for persons with heart problems (ec2). There is a particular reason why
running is not advisable for persons with heart problems (ed2), e.g. their hearts do
not tolerate a heartbeat increase (ed1). Since running causes a heartbeat increase, the
heart can tolerate a heartbeat increase (ecp) is in the causal complex for ec2 and it is
inconsistent with ed2.
Similarly, in (1.c), which is taken from the PDTB, it is true that representing a
low percentage of the population causes controlling low percentage of income (cac).
Therefore, they represent 2% of population (ec1) causes they control low percentage
of income (ec2). Nevertheless, ec2 does not really exists in the context, in that it is
inconsistent with they control nearly one-third of income (ed2). There must be another
reason for why ec2 does not hold. For instance, either they are very rich, or they do
not have as many basic expenses as other people, or a more complex condition. This
unknown cause, i.e. ed1, both makes ed2 true and ec2 false in the context.
The last example highlights the point that finding the eventualities involved in (2) is
strongly dependent upon contextual knowledge. 2% is not taken to be a low percentage
in any context. For instance, 2% mercury in the water may be considered a high
percentage of pollution. Analogously, one third may be considered a high percentage
in that context, especially if compared with 2% of population, but it may be a low or
medium percentage in many other contexts. The analysis of examples (1.d-e) in terms
of the definition in (2) is analogous.
5 A survey of concessive relations in PDTB 2.0
PDTB 2.0 contains 1193 tokens of explicit connectives which are annotated with one
sense tagged as ?Concession?, ?contra-expectation? and ?expectation?. There are also
another 20 tokens that have been annotated with double senses, one of which is the
concessive type or subtypes. Table (1) shows the distribution of concessive labels for
the 1193 tokens. Explicit connectives with a concessive label assigned to less than
10 tokens are grouped under ?other?. The rest of the connectives shown in Table (1)
amount to 98% of all ?contra-expectation? and 95% of all ?expectation? tokens. The
7This is a paraphrase of Something being considered healthy for humans causes it to be advisable for
humans.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 217
Table 1: Concessive labels in PDTB 2.0
CONN ?contra-exp.? ?exp.? ?Concession? Total
although 21 132 1 154 (13%)
but 494 12 2 508 (42.5%)
even if 3 31 1 35 (3%)
even though 15 52 5 72 (6%)
however 70 2 5 77 (6.5%)
nevertheless 19 0 0 19 (1.5%)
nonetheless 17 0 0 17 (1.5%)
still 79 2 1 82 (7%)
though 30 53 1 84 (7%)
while 3 79 1 83 (7%)
yet 32 0 0 32 (2.5%)
other 13 17 0 30 (2.5%)
Total 796 380 17 1193
most common connective annotated with the ?Concession? type or one of its two sub-
types is ?but? with 508 tokens (42% of all concessive labels), followed by ?although?
with 154 tokens (13% of all concesive labels).
We are currently evaluating the robustness of the proposed refined semantics for
concessive labels in PDTB 2.0 starting with the most the most common concessive
connectives. While the validation process for the entire corpus is still work in progress,
preliminary results on 25% of ?but? tokens indicate that the semantics of concession
based on defeasible causality applies straightforwardly to more than 60% of the data.
In future work, we hope to be able to offer a more comprehensive account of all the
concessive labels in PDTB 2.0 including cases of concession in which the created
expectation arises from an implication rather than from a causal relation (about 23%),
as in (3)
(3) Although working for U.S. intelligence, Mr. Noriega was hardly helping the
U.S. exclusively. (expectation)
In (3), it is strange to say that working for U.S. intelligence normally ?causes?
helping U.S. exclusively. Rather, the former seems a kind of necessary condition or
job requirement for the latter: working for U.S. intelligence implies (among other
things) helping U.S. exclusively. Suppose that someone discovers that Mr. Noriega is
not helping the U.S. exclusively. Mr. Noriega is arguably breaking a rule or flauting an
expectation. Therefore, working for U.S. intelligence ?implies? rather than ?causes?
helping U.S. exclusively.
It is unsurprising that there are cases of concession based on implication rather than
causality, because the two concepts are very close to each other. One could think of
implication as a kind of abstract, informational, or ?denatured? causality. Both obey a
kind of (defeasible) modus ponens. When the cause or antecedent happens or holds,
so does the effect or consequent. The other key property of causal complexes is that
218 Robaldo, Miltsakaki, and Hobbs
all the eventualities in it are relevant, in a sense that is made precise in Hobbs (2005).
This notoriously does not hold for material implication, but as many have argued,
it probably does hold for felicitous uses of our everyday notion of implication. In
addition, there are easy conversions between causality and implication. If A causes B,
then the fact that A happens (defeasibly) implies that B happens. If P implies Q in the
everyday sense, then one?s belief in P (defeasibly) causes one?s belief in Q. In fact,
implicational cases of concession could be viewed as instances of metonymy, where
?believe? is the coercion relation, and hence really causal cases of concession.
6 Conclusion
We presented a formal description of the meaning of concession, a substantial refine-
ment of the rough semantics given in the manual of sense annotations of connectives
in PDTB 2.0. Our analysis builds on Hobbs?s logic of defeasible causality enabled
by the crucial distinction between causes and causal complexes. Our basic claim is
that concession is triggered by the contrast between two causal relations. The causal
relation between the content of one argument of the relation and some implicit even-
tuality (the expectation created based on the content of the argument) and the content
of another causal relation, that between the eventuality described in second argument
and its implicit cause. This second causal relation picks an element of the causal com-
plex that we would normally assume to hold and challenges it, hence the notion of
defeasible causality.
This work illustrates the mutual benefit that corpus annotation and formal analysis
can provide to each other. Corpus examples constitute a forcing function on the formal
analysis; definitions must accommodate the complexities one finds in the real world.
On the other hand, all good annotation rests on solid theory, and formal analysis can
help in the adjudication of difficult examples. The particular analysis we give in this
paper for the concession relation can clarify issues that arise in annotation, and can
also form the basis for recognizing these relations using a knowledge-rich inferencing
system.
References
Achinstein, P. (1965). ?Defeasible? Problems. The Journal of Philosophy 62(21),
629?633.
Asher, N. (1993). Reference to Abstract Objects. Kluwer, Dordrecht.
Bach, E. (1981). On Time, Tense, and Aspect: An Essay in English Metaphysics. In
P. Cole (Ed.), Radical Pragmatics, pp. 63?81. Academic Press, New York.
Bell, J. (1999). Primary and secondary events. In M. Thielscher (Ed.), Proc. of the
IJCAI-99 Workshop on Nonmonotonic Reasoning, Action and Change, pp. 65?72.
Bell, J. (2003). A common sense theory of causation. In P. Blackburn, C. Ghidini,
R. Turner, and F. Giunchiglia (Eds.), Modeling and Using Context: Fourth Interna-
tional and Interdisciplinary Conference, Context 2003, Berlin, pp. 40?53. Springer-
Verlag.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 219
Giordano, L. and C. Schwind (2004). Conditional logic of actions and causation.
Artificial Intelligence 157(1?2), 239?279.
Giunchiglia, E., J. Lee, V. Lifschitz, N. McCain, and H. Turner (2004). Nonmonotonic
causal theories. Artificial Intelligence 153(1?2), 49?104.
Hobbs, J. (1983). An Improper Treatment of Quantification in Ordinary English. In
Proc. of the 21st Annual Meeting of the Association for Computational Linguistics,
Cambridge, Massachusetts, pp. 57?63.
Hobbs, J. (1995). Monotone Decreasing Quantifiers in a Scope-Free Logical Form. In
K. van Deemter and S. Peters (Eds.), Semantic Ambiguity and Underspecification,
CSLI Lecture Notes, pp. 55?76. CSLI.
Hobbs, J. (1998). The Logical Notation: Ontological Promiscuity. In Discourse and
Inference, Chapter 2.
Hobbs, J. (2005). Towards a Useful Notion of Causality for Lexical Semantics. Jour-
nal of Semantics 22(2), 181?209.
Kayser, D. and F. Nouioua (2008). From the Description of an Accident to its Causes.
submitted to Artificial Intelligence.
McCarthy, J. (1977). Epistemological Problems of Artificial Intelligence. In Proc.
of International Joint Conference on Artificial Intelligence, Cambridge, Mas-
sachusetts, pp. 1038?1044.
Miltsakaki, E., L. Robaldo, A. Lee, and A. Joshi (2008). Sense Annotation in the
Penn Discourse Treebank. In Proc. of Computational Linguistics and Intelligent
Text Processing, Volume 4919 of LNCS, pp. 275?286. Springer.
Prasad, R., N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber
(2008). The Penn Discourse Treebank 2.0. In Proc. of the 6th Int. Conf. on Lan-
guage Resources and Evaluation.
Prasad, R., E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi, B. Webber, and L. Robaldo
(2008). The Penn Discourse Treebank 2.0. Annotation Manual. Technical Report
IRCS-06-01, IRCS Technical Report, Institute of Research in Cognitive Science,
University of Pennsylvania.
Shoham, Y. (1990). Nonmonotonic reasoning and causation. Cognitive Science 14,
213?252.
Simon, H. (1991). Nonmonotonic reasoning and causation: Comment. Cognitive
Science 49, 517?528.
Webber, B. and A. Joshi (2003). Anchoring a lexicalized tree-adjoining grammar for
discourse. In M. Stede, L. Wanner, and E. Hovy (Eds.), Discourse Relations and
Discourse Markers: Proceedings of the Conference, pp. 86?92.
Webber, B., A. Joshi, M. Stone, and A. Knott (2003). Anaphora and discourse struc-
ture. Computational Linguistics 29(4), 545?587.
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 49?57,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Do NLP and machine learning improve traditional readability formulas?
Thomas Franc?ois
University of Pennsylvania
CENTAL, UCLouvain
3401 Walnut Street Suite 400A
Philadelphia, PA 19104, US
frthomas@sas.upenn.edu
Eleni Miltsakaki
University of Pennsylvania & Choosito!
3401 Walnut Street Suite 400A
Philadelphia, PA 19104, US
elenimi@seas.upenn.edu
Abstract
Readability formulas are methods used to
match texts with the readers? reading level.
Several methodological paradigms have pre-
viously been investigated in the field. The
most popular paradigm dates several decades
back and gave rise to well known readability
formulas such as the Flesch formula (among
several others). This paper compares this ap-
proach (henceforth ?classic?) with an emerg-
ing paradigm which uses sophisticated NLP-
enabled features and machine learning tech-
niques. Our experiments, carried on a corpus
of texts for French as a foreign language, yield
four main results: (1) the new readability for-
mula performed better than the ?classic? for-
mula; (2) ?non-classic? features were slightly
more informative than ?classic? features; (3)
modern machine learning algorithms did not
improve the explanatory power of our read-
ability model, but allowed to better classify
new observations; and (4) combining ?classic?
and ?non-classic? features resulted in a signif-
icant gain in performance.
1 Introduction
Readability studies date back to the 1920?s and have
already spawned probably more than a hundred pa-
pers with research on the development of efficient
methods to match readers and texts relative to their
reading difficulty. During this period of time, sev-
eral methodological trends have appeared in suc-
cession (reviewed in Klare (1963; 1984), DuBay
(2004)). We can group these trends in three ma-
jor approaches: the ?classic studies?, the ?structuro-
cognitivist paradigm? and the ?AI readability?, a
term suggested by Franc?ois (2011a).
The classic period started right after the seminal
work of Vogel and Washburne (1928) and Gray and
Leary (1935) and is characterized by an ideal of sim-
plicity. The models (readability formulas) proposed
to predict text difficulty for a given population are
kept simple, using multiple linear regression with
two, or sometimes, three predictors. The predictors
are simple surface features, such as the average num-
ber of syllables per word and the average number of
words per sentence. The Flesch (1948) and Dale and
Chall (1948) formulas are probably the best-known
examples of this period.
With the rise of cognitivism in psychological
sciences in the 70?s and 80?s, new dimensions of
texts are highlighted such as coherence, cohesion,
and other discourse aspects. This led some schol-
ars (Kintsch and Vipond, 1979; Redish and Selzer,
1985) to adopt a critical attitude to classic readabil-
ity formulas which could only take into account su-
perficial features, ignoring other important aspects
contributing to text difficulty. Kintsch and Vipond
(1979) and Kemper (1983), among others, suggested
new features for readability, based on those newly
discovered text dimensions. However, despite the
fact that the proposed models made use of more so-
phisticated features, they failed to outperform the
classic formulas. It is probably not coincidental that
after these attempts readability research efforts de-
clined in the 90s.
More recently, however, the development of ef-
ficient natural language processing (NLP) systems
and the success of machine learning methods led to
49
a resurgence of interest in readability as it became
clear that these developments could impact the de-
sign and performance of readability measures. Sev-
eral studies (Si and Callan, 2001; Collins-Thompson
and Callan, 2005; Schwarm and Ostendorf, 2005;
Feng et al, 2010) have used NLP-enabled feature
extraction and state-of-the-art machine learning al-
gorithms and have reported significant gains in per-
formance, suggesting that the AI approach might be
superior to previous attempts.
Going beyond reports of performance which are
often hard to compare due to a lack of a common
gold standard, we are interested in investigating AI
approaches more closely with the aim of understand-
ing the reasons behind the reported superiority over
classic formulas. AI readability systems use NLP
for richer feature extraction and a machine learning
algorithm. Given that the classic formulas are also
statistical, is performance boosted because of the ad-
dition of NLP-enabled feature extraction or by better
machine learning algorithms? In this paper, we re-
port initial findings of three experiments designed to
explore this question.
The paper is organized as follows. Section 2 re-
views previous findings in the field and the challenge
of providing a uniform explanation for these find-
ings. Section 3 gives a brief overview of prior work
on French readability, which is the context of our
experiments (evaluating the readability of French
texts). Because there is no prior work comparing
classic formulas with AI readablity measures for
French, we first report the results of this compari-
son in Section 3. Then, we proceed with the results
of three experiments (2-4), comparing the contribu-
tions of the AI enabled features with features used
in classic formulas, different machine learning al-
gorithms and the interactions of features with algo-
rithms. There results are reported in Sections 4, 5,
and 6, respectively. We conclude in Section 7 with a
summary of the main findings and future work.
2 Previous findings
Several readability studies in the past decade have
reported a performance gain when using NLP-
enabled features, language models, and machine
learning algorithms to evaluate the reading difficulty
of a variety of texts (Si and Callan, 2001; Collins-
Thompson and Callan, 2005; Schwarm and Osten-
dorf, 2005; Heilman et al, 2008; Feng et al, 2010).
A first explanation for this superiority would be
related to the new predictors used in recent mod-
els. Classic formulas relied mostly on surface lexical
and syntactic variables such as the average number
of words per sentence, the average number of letters
per word, the proportion of given POS tags in the
text or the proportion of out-of-simple-vocabulary
words. In the AI paradigm, several new features
have been added, including language models, parse
tree-based predictors, probability of discourse rela-
tions, estimates of text coherence, etc. It is rea-
sonable to assume that these new features capture a
wider range of readability factors thus bringing into
the models more and, possibly, better information.
However, the evidence from comparative studies
is not consistent on this question. In several cases,
AI models include features central to classic formu-
las which, when isolated, appear to be the stronger
predictors in the models. An exception to this trend
is the work of Pitler and Nenkova (2008) who re-
ported non-significant correlation for the mean num-
ber of words per sentence (r = 0.1637, p = 0.3874)
and the mean number of characters per word (r =
?0.0859, p = 0.6519). In their study, though, they
used text quality rather than text difficulty as the de-
pendent variable. The data consisted solely of text
from the Wall Street Journal which is ?intended for
an educated adult audience? text labelled for de-
grees of reading fluency. Feng et al (2010) com-
pared a set of similar variables and observed that
language models performed better than classic for-
mula features but classic formula features outper-
formed those based on parsing information. Collins-
Thompson and Callan (2005) found that the classic
type-token ratio or number of words not in the 3000-
words Dale list appeared to perform better than their
language model on a corpus from readers, but were
poorer predictors on web-extracted texts.
In languages other than English, Franc?ois (2011b)
surveyed a wide range of features for French and
reports that the feature that uses a limited vocabu-
lary list (just like in some classic formulas) has a
stronger correlation with reading difficulty that a un-
igram model and the best performing syntactic fea-
ture was the average number of words per sentences.
Aluisio et al (2010), also, found that the best corre-
50
late with difficulty was the average number of words
per sentence. All in all, while there is sufficient ev-
idence that the AI paradigm outperforms the classis
formulas, classic features have often been shown to
make the single strongest predictors.
An alternative explanation could be that, by com-
parison to the simpler statistical analyses that deter-
mined the coefficients of the classic formulas, ma-
chine learning algorithms, such as support machine
vector (SVM) or logistic regression are more sophis-
ticated and better able to learn the regularities in
training data, thus building more accurate models.
Work in this direction has been of smaller scale but
already reporting inconsistent results. Heilman et al
(2008) considered the performance of linear regres-
sion, ordinal and multinomial logistic regression,
and found the latter to be more efficient. However,
Kate et al (2010) obtained contradictory findings,
showing that regression-based algorithms perform
better, especially when regression trees are used for
bagging. For French, Franc?ois (2011b) found that
SVMs were more efficient than linear regression, or-
dinal and multinomial logistic regression, boosting,
and bagging.
Finally, it is quite possible that there are interac-
tions between types of features and types of statis-
tical algorithms and these interactions are primarily
responsible for the better performance.
In what follows, we present the results of three
studies (experiments 2-4), comparing the contribu-
tions of the AI enabled features with features used
in classic formulas, different machine learning al-
gorithms and the interactions of features with algo-
rithms. As mentioned earlier, all the studies have
been done on French data, consisting of text ex-
tracted from levelled FFL textbooks (French as For-
eign Language). Because there is no prior work
comparing classic formulas with AI readability mea-
sures for FFL, we first report the results of this com-
parison in the next section (experiment 1).
3 Experiment 1: Model comparison for
FFL
To compute a classic readability formula for FFL,
we used the formula proposed for French by Kandel
and Moles (1958). We compared the results of this
formula with the AI model trained on the FFL data
used by Franc?ois (2011b).
The Kandel and Moles (1958) formula is an adap-
tation of the Flesch formula for French, based on a
study of a bilingual corpus:
Y = 207? 1.015lp? 0.736lm (1)
where Y is a readability score ranging from 100
(easiest) to 0 (harder); lp is the average number of
words per sentence and lm is the average number of
syllables per 100 words. Although this formula is
not specifically designed for FFL, we chose to im-
plement it over formulas proposed for FFL (Tharp,
1939; Uitdenbogerd, 2005). FFL-specific formu-
las are optimized for English-speaking learners of
French while our dataset is agnostic to the native
language of the learners.
The computation of the Kandel and Moles (1958)
formula requires a syllabification system for French.
Due to unavailability of such a system for French,
we adopted a hybrid syllabification method. For
words included in Lexique (New et al, 2004), we
used the gold syllabification included in the dictio-
nary. For all other words, we generated API pho-
netic representations with espeak 1, and then applied
the syllabification tool used for Lexique3 (Pallier,
1999). The accuracy of this process exceeded 98%.
For the comparison with an AI model, we ex-
tracted the same 46 features (see Table 2 for the
complete list) used in Franc?ois? model 2 and trained
a SVM model.
For all the study, the gold-standard consisted of
data taken from textbooks and labeled according to
the classification made by the publishers. The cor-
pus includes a wide range of texts, including ex-
tracts from novels, newspapers articles, songs, mail,
dialogue, etc. The difficulty levels are defined by
the Common European Framework of Reference for
Languages (CEFR) (Council of Europe, 2001) as
follows: A1 (Breakthrough); A2 (Waystage); B1
(Threshold); B2 (Vantage); C1 (Effective Opera-
tional Proficiency) and C2 (Mastery). The test cor-
pus includes 68 texts per level, for a total of 408 doc-
uments (see Table 1).
We applied both readability models to this test
corpus. Assessing and comparing the performance
1Available at: http://espeak.sourceforge.net/.
2Details on how to implement these features can be found in
Franc?ois (2011b).
51
A1 A2 B1 B2 C1 C2 Total
68(10, 827) 68(12, 045) 68(17, 781) 68(25, 546) 68(92, 327) 68(39, 044) 408(127, 681)
Table 1: Distribution of the number of texts and tokens per level in our test corpus.
of the two models with accuracy scores (acc), as is
common in classification tasks, has proved challeng-
ing and, in the end, uninformative. This is because
the Kandel and Moles formula?s output scores are
not an ordinal variable, but intervals. To compute
accuracy we would have to define a set of rather
arbitrary cut off points in the intervals and corre-
spond them with level boundaries. We tried three
approaches to achieve this task. First, we used
correspondences between Flesch scores and seven
difficulty levels proposed for French by de Land-
sheere (1963): ?very easy? (70 to 80) to ?very dif-
ficult? (-20 to 10). Collapsing the ?difficult? and
?very difficult? categories into one, we were able
to roughly match this scale with the A1-C2 scale.
The second method was similar, except that those
levels were mapped on the values from the original
Flesch scale instead of the one adapted for French.
The third approach was to estimate normal distribu-
tion parameters ?j and ?j for each level j for the
Kandel and Moles? formula output scores obtained
on our corpus. The class membership of a given ob-
servation i was then computed as follows:
arg
6
max
j=1
P (i ? j | N(?j , ?j)) (2)
Since the parameters were trained on the same cor-
pus used for the evaluation, this computation should
yield optimal class membership thresholds for our
data.
Given the limitations of all three approaches, it is
not surprising that accuracy scores were very low:
9% for the first and 12% for the second, which is
worse than random (16.6%). The third approach
gave a much improved accuracy score, 33%, but still
quite low. The problem is that, in a continuous for-
mula, predictions that are very close to the actual
will be classified as errors if they fall on the wrong
side of the cut off threshold. These results are, in
any case, clearly inferior to the AI formula based on
SVM, which classified correctly 49% of the texts.
A more suitable evaluation measure for a contin-
uous formula would be to compute the multiple cor-
relation (R). The multiple correlation indicates the
extent to which predictions are close to the actual
classes, and, when R2 is used, it describes the per-
centage of the dependent variable variation which
is explained by the model. Kandel and Moles? for-
mula got a slightly better performance (R = 0.551),
which is still substantially lower that the score (R =
0.728) obtained for the SVM model. To check if
the difference between the two correlation scores
was significant, we applied the Hotelling?s T-test for
dependent correlation (Hotelling, 1940) (required
given that the two models were evaluated on the
same data). The result of the test is highly signif-
icant (t = ?19.5; p = 1.83e?60), confirming that
the SVM model performed better that the classic for-
mula.
Finally, we computed a partial Spearman corre-
lation for both models. We considered the output
of each model as a single variable and we could,
therefore, evaluate the relative predictive power of
each variable when the other variable is controlled.
The partial correlation for the Kandel and Moles for-
mula is very low (? = ?0.11; p = 0.04) while
the SVM model retains a good partial correlation
(? = ?0.53; p < 0.001).
4 Experiment 2: Comparison of features
In this section, we compared the contribution of the
features used in classic formulas with the more so-
phisticated NLP-enabled features used in the ma-
chine learning models of readability. Given that the
features used in classic formulas are very easy to
compute and require minimal processing by com-
parison to the NLP features that require heavy pre-
processing (e.g., parsing), we are, also, interested in
finding out how much gain we obtain from the NLP
features. A consideration that becomes important
for tasks requiring real time evaluation of reading
difficulty.
To evaluate the relative contribution of each set
of features, we experiment with two sets of fea-
tures (see Table 2. We labeled as ?classic?, not only
52
Family Tag Description of the variable ? Linear
Classic
PA-Alterego
Proportion of absent words from a list
0.652 No
of easy words from AlterEgo1
X90FFFC 90th percentile of inflected forms for content words only ?0.641 No
X75FFFC 75th percentile of inflected forms for content words only ?0.63 No
PA-Goug2000
Proportion of absent words from 2000 first
0.597 No
of Gougenheim et al (1964)?s list
MedianFFFC Median of the frequencies of inflected content words ?0.56 Yes
PM8 Pourcentage of words longer than 8 characters 0.525 No
NL90P
Length of the word corresponding to
0.521 No
the 90th percentile of word lengths
NLM Mean number of letters per word 0.483 Yes
IQFFFC Interquartile range of the frequencies of inflected content words 0.405 No
MeanFFFC Mean of the frequencies of inflected content words ?0.319 No
TTR Type-token ratio based on lemma 0.284 No
NMP Mean number of words per sentence 0.618 No
NWS90 Length (in words) of the 90th percentile sentence 0.61 No
PL30 Percentage of sentences longer than 30 words 0.56 Yes
PRE/PRO Ratio of prepositions and pronouns 0.345 Yes
GRAM/PRO Ratio of grammatical words and pronouns 0.34 Yes
ART/PRO Ratio of articles and pronouns 0.326 Yes
PRE/ALL Proportions of prepositions in the text 0.326 Yes
PRE/LEX Ratio of prepositions and lexical words 0.322 Yes
ART/LEX Ratio of articles and lexical words 0.31 Yes
PRE/GRAM Ratio of prepositions and grammatical words 0.304 Yes
NOM-NAM/ART Ratio of nouns (common and proper) and gramm. words ?0.29 Yes
PP1P2 Percentage of P1 and P2 personal pronouns ?0.333 No
PP2 Percentage of P2 personal pronouns ?0.325 Yes
PPD Percentage of personal pronouns of dialogue 0.318 No
BINGUI Presence of commas 0, 462 No
Non-classic
Unigram Probability of the text sequence based on unigrams 0.546 No
MeanNGProb-G Average probability of the text bigrams based on Google 0.407 Yes
FCNeigh75 75th percentile of the cumulated frequency of neighbors per word ?0.306 Yes
MedNeigh+Freq Median number of more frequent neighbor for words ?0.229 Yes
Neigh+Freq90 90th percentile of more frequent neighbor for words ?0.192 Yes
PPres Presence of at least one present participle in the text 0.44 No
PPres-C Proportion of present participle among verbs 0.41 Yes
PPasse Presence of at least one past participle 0.388 No
Infi Presence of at least one infinive 0.341 No
Impf Presence of at least one imperfect 0.272 No
Subp Presence of at least one subjunctive present 0.266 Yes
Futur Presence of at least one future 0.252 No
Cond Presence of at least one conditional 0.227 No
PasseSim Presence of at least one simple past 0.146 No
Imperatif Presence of at least one imperative 0.019 Yes
Subi Presence of at least one subjunctive imperfect 0.049 Yes
avLocalLsa-Lem Average intersentential cohesion measured via LSA 0, 63 No
ConcDens
Estimate of the conceptual density
0.253 Yes
with Denside?es (Lee et al, 2010)
NAColl Proportion of MWE having the structure NOUN ADJ 0.286 Yes
NCPW Average number of MWEs per word 0.135 Yes
Table 2: List of the 46 features used by Franc?ois (2011b) in his model. The Spearman correlation reported here also
comes from this study.
53
the features that are commonly used in traditional
formulas like Flesch (length of words and number
of words per sentence) but also other easy to com-
pute features that were identified in readability work.
Specifically, in the ?classic? set we include num-
ber of personal pronouns (given as a list) (Gray and
Leary, 1935), the Type Token Ratio (TTR) (Lively
and Pressey, 1923), or even simple ratios of POS
(Bormuth, 1966).
The ?non-classic? set includes more complex
NLP-enabled features (coherence measured through
LSA, MWE, n-grams, etc.) and features suggested
by the structuro-cognitivist research (e.g., informa-
tion about tense and variables based on orthograph-
ical neighbors).
For evaluation, we first computed and compared
the average bivariate correlations of both sets. This
test yielded a better correlation for the classic fea-
tures (r? = 0.48 over the non-classic features r? =
0.29)
As a second test, we trained a SVM model on each
set and evaluated performances in a ten-fold cross-
validation. For this test, we reduced the number of
classic features by six to equal the number of pre-
dictors of the non-classic set. Our hypothesis was
the SVM model using non-classic features would
outperform the classic set because the non-classic
features bring richer information. This assumption
was not strictly confirmed as the non-classic set per-
formed only slightly better than the classic set. The
difference in the correlation scores was small (0.01)
and non-significant (t(9) = 0.49; p = 0.32), but the
difference in accuracy was larger (3.8%) and close to
significance (t(9) = 1.50; p = 0.08). Then, in an ef-
fort to pin down the source of the SVM gain that did
not come out in the comparison above, we defined a
SVM baseline model (b) that included only two typ-
ical features of the classic set: the average number
of letter per word (NLM) and the average number of
word per sentence (NMP). Then, for each of the i
remaining variables (44), we trained a model mi in-
cluding three predictors: NLM, NMP, and i. The
difference between the correlation of the baseline
model and that of the model mi was interpreted as
the information gain carried by the feature i. There-
fore, for both sets, of cardinality Ns, we computed:
?Ns
i=1R(mi)?R(b)
Ns
(3)
where R(mi) is the multiple correlation of model
mi.
Our assumption was that, if the non-classic set
brings in more varied information, every predictor
should, on average, improve more theR of the base-
line model, while the classic variables, more redun-
dant with NLM and NP, would be less efficient. In
this test, the mean gain for R was 0.017 for the clas-
sic set and 0.022 for the non-classic set. Although
the difference was once more small, this test yielded
a similar trend than the previous test.
As a final test, we compared the performance of
the SVM model trained only on the ?classic? set
with the SVM trained on both sets. In this case,
the improvement was significant (t(9) = 3.82; p =
0.002) with accuracy rising from 37.5% to 49%. Al-
though this test does not help us decide on the nature
of the gain as it could be coming just from the in-
creased number of features, it shows that combining
?classic? and ?non-classic? variables is valuable.
5 Experiment 3: Comparison of statistical
models
In this section, we explore the hypothesis that AI
models outperform classic formulas because they
use better statistical algorithms. We compare the
performance of a?classic? algorithm, multiple linear
regression, with the performance of a machine learn-
ing algorithm, in this case SVM. Note that an SVMs
have an advantage over linear regression for features
non-linearly related with difficulty. Bormuth (1966,
98-102) showed that several classic features, espe-
cially those focusing on the word level, were indeed
non-linear. To control for linearity, we split the 46
features into a linear and a non-linear subset, using
the Guilford?s F test for linearity (Guilford, 1965)
and an ? = 0.05. This classification yielded two
equal sets of 23 variables (see Table 2). In Table
3, we report the performance of the four models in
terms of R, accuracy, and adjacent accuracy. Fol-
lowing, Heilman et al (2008), we define ?adjacent
accuracy? as the proportion of predictions that were
within one level of the assigned label in the corpus.
54
Model R Acc. Adj. acc.
Linear
LR 0.58 27% 72%
SVM 0.64 38% 73%
Non-Linear
LR 0.75 36% 81%
SVM 0.70 44% 76%
Table 3: Multiple correlation coefficient (R), accuracy
and adjacent accuracy for linear regression and SVM
models, using the set of features either linearly or non
linearly related to difficulty.
Adjacent accuracy is closer toR as it is less sensitive
to minor classification errors.
Our results showed a contradictory pattern, yield-
ing a different result depending on type of evalu-
tion: accuracy or R and adjacent accuracy. With
respect to accuracy scores, the SVM performed bet-
ter in the classification task, with a significant per-
formance gain for both linear (gain = 9%; t(9) =
2.42; p = 0.02) and non-linear features (gain = 8%;
t(9) = 3.01; p = 0.007). On the other hand, the dif-
ference in R was non-significant for linear (gain =
0.06; t(9) = 0.80; p = 0.22) and even negative and
close to significance for non-linear (gain = ?0.05;
t(9) = 1.61; p = 0.07). In the light of these re-
sults, linear regression (LR) appears to be as effi-
cient as SVM accounting for variation in the depen-
dant variable (their R2 are pretty similar), but pro-
duces poorer predictions.
This is an interesting finding, which suggests that
the contradictory results in prior literature with re-
gard to performance of different readability mod-
els (see Section 2) might be related to the evalua-
tion measure used. Heilman et al (2008, 7), who
compared linear and logistic regressions, found that
the R of the linear model was significantly higher
than the R of the logistic model (p < 0.01). In con-
trast, the logistic model behaved significantly better
(p < 0.01) in terms of adjacent accuracy. Similarly,
Kate and al. (2010, 548), which used R as evalua-
tion measure, reported that their preliminary results
?verified that regression performed better than clas-
sification?. Once they compared linear regression
and SVM regression, they noticed similar correla-
tions for both techniques (respectively 0.7984 and
0.7915).
To conclude this section, our findings suggest that
(1) linear regression and SVM are comparable in ac-
counting for the variance of text difficulty and (2)
SVM has significantly better accuracy scores than
linear regression.
6 Experiment 4: Combined evaluation
In Experiment 2, we saw that ?non-classic? features
are slightly, but non-significantly, better than the
?classic? features. In Experiment 3, we saw that
SVM performs better than linear regression when
the evaluation is done by accuracy but both demon-
strate similar explanatory power in accounting for
the variation. In this section, we report evaluation
results for four models, derived by combining two
sets of features, classic and non-classic, with two al-
gorithms, linear regression and SVM. The results are
shown in Table (4).
The results are consistent with the findings in
the previous sections. When evaluated with accu-
racy scores SVM performs better with both classic
(t(9) = 3.15; p = 0.006) and non-classic features
(t(9) = 3.32; p = 0.004). The larger effect obtained
for the non-classic features might be due to an in-
teraction, i.e., an SVM trained with non-classic fea-
tures might be better at discriminating reading lev-
els. However, with respect to R, both algorithms are
similar, with linear regression outperforming SVM
in adjacent accuracy (non-significant). Linear re-
gression and SVM, then, appear to have equal ex-
planatory power.
As regards the type of features, the explanatory
power of both models seems to increase with non-
classic features as shown in the increased R, al-
though significance is not reached (t(9) = 0.49; p =
0.32 for the regression and t(9) = 1.5; p = 0.08 for
the SVM).
7 General discussion and conclusions
Recent readability studies have provided prelimi-
nary evidence that the evaluation of readability us-
ing NLP-enabled features and sophisticated machine
learning algorithms outperform the classic readabil-
ity formulas, such as Flesch, which rely on surface
textual features. In this paper, we reported a number
of experiments the purpose of which was to identify
the source of this performance gain.
Specifically, we compared the performance of
classic and non-classic features and the performance
55
Model R Acc. Adj. acc.
Classic
LR 0.66 30.6% 78%
SVM 0.67 37.5% 76%
Non-classic
LR 0.68 32% 76%
SVM 0.68 41.8% 73%
Table 4: Multiple correlation coefficient (R), accuracy and adjacent accuracy for linear regression and SVM models
with either the classic or the non-classic set of predictors.
of two statistical algorithms: linear regression (used
in classic formulas) and SVM (in the context of FFL
readability). Our results indicate that classic features
are strong single predictors of readability. While
we were not able to show that the non-classic fea-
tures are better predictors by themselves, our find-
ings show that leaving out non-classic features has a
significant negative impact on the performance. The
best performance was obtained when both classic
and non-classic features were used.
Our experiments on the comparison of the two
statistical algorithms showed that the SVM outper-
forms linear regression by a measure of accuracy,
but the two algorithms are comparable in explana-
tory power accounting for the same amount of vari-
ability. This observation accounts for contradictory
conclusions reported in previous work. Our study
shows that different evaluation measures can lead to
quite different conclusions.
Finally, our comparison of four models derived
by combining linear regression and SVM with ?clas-
sic? and ?non-classic? features confirms the signif-
icant contribution of ?non-classic? features and the
SVM algorithm to classification accuracy. However,
by a measure of adjacent accuracy and explanatory
power, the two algorithms are comparable.
From a practical application point of view, it
would be interesting to try these algorithms in web
applications that process large amounts of text in
real time (e.g., READ-X (Miltsakaki, 2009)) to eval-
uate the trade-offs between accuracy and efficiency.
Acknowledgments
We would like to acknowledge the invaluable help of
Bernadette Dehottay for the collection of the corpus,
as well as the Belgian American Educational Foun-
dation that supported Dr. Thomas Franc?ois with a
Fellowship during this work.
References
S. Aluisio, L. Specia, C. Gasperin, and C. Scarton. 2010.
Readability assessment for text simplification. In Fifth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 1?9, Los Angeles.
J.R. Bormuth. 1966. Readability: A new approach.
Reading research quarterly, 1(3):79?132.
K. Collins-Thompson and J. Callan. 2005. Predict-
ing reading difficulty with statistical language models.
Journal of the American Society for Information Sci-
ence and Technology, 56(13):1448?1462.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Press Syndicate of the University of
Cambridge.
E. Dale and J.S. Chall. 1948. A formula for predicting
readability. Educational research bulletin, 27(1):11?
28.
G. de Landsheere. 1963. Pour une application des tests
de lisibilite? de Flesch a` la langue franc?aise. Le Travail
Humain, 26:141?154.
W.H. DuBay. 2004. The principles of read-
ability. Impact Information. Disponible sur
http://www.nald.ca/library/research/readab/readab.pdf.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A Comparison of Features for Automatic Read-
ability Assessment. In COLING 2010: Poster Volume,
pages 276?284.
R. Flesch. 1948. A new readability yardstick. Journal of
Applied Psychology, 32(3):221?233.
T. Franc?ois. 2011a. La lisibilite? computationnelle
: un renouveau pour la lisibilite? du franc?ais langue
premie`re et seconde ? International Journal of Ap-
plied Linguistics (ITL), 160:75?99.
T. Franc?ois. 2011b. Les apports du traitement au-
tomatique du langage a` la lisibilite? du franais langue
e?trange`re. Ph.D. thesis, Universite? Catholique de Lou-
vain. Thesis Supervisors : Ce?drick Fairon and Anne
Catherine Simon.
G. Gougenheim, R. Miche?a, P. Rivenc, and A. Sauvageot.
1964. Le?laboration du franc?ais fondamental (1er
degre?). Didier, Paris.
56
W.S. Gray and B.E. Leary. 1935. What makes a book
readable. University of Chicago Press, Chicago: Illi-
nois.
J.P. Guilford. 1965. Fundamental statistics in psychol-
ogy and education. McGraw-Hill, New-York.
M. Heilman, K. Collins-Thompson, and M. Eskenazi.
2008. An analysis of statistical models and features
for reading difficulty prediction. In Proceedings of the
Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 1?8.
H. Hotelling. 1940. The selection of variates for use in
prediction with some comments on the general prob-
lem of nuisance parameters. The Annals of Mathemat-
ical Statistics, 11(3):271?283.
L. Kandel and A. Moles. 1958. Application de l?indice
de Flesch a` la langue franc?aise. Cahiers E?tudes de
Radio-Te?le?vision, 19:253?274.
R. Kate, X. Luo, S. Patwardhan, M. Franz, R. Florian,
R. Mooney, S. Roukos, and C. Welty. 2010. Learn-
ing to predict readability using diverse linguistic fea-
tures. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 546?554.
S. Kemper. 1983. Measuring the inference load of a text.
Journal of Educational Psychology, 75(3):391?401.
W. Kintsch and D. Vipond. 1979. Reading comprehen-
sion and readability in educational practice and psy-
chological theory. In L.G. Nilsson, editor, Perspec-
tives on Memory Research, pages 329?365. Lawrence
Erlbaum, Hillsdale, NJ.
G.R.. Klare. 1963. The Measurement of Readability.
Iowa State University Press, Ames, IA.
G.R. Klare. 1984. Readability. In P.D. Pearson, R. Barr,
M. L. Kamil, P. Mosenthal, and R. Dykstra, edi-
tors, Handbook of Reading Research, pages 681?744.
Longman, New York.
H. Lee, P. Gambette, E. Maille?, and C. Thuillier. 2010.
Denside?es: calcul automatique de la densite? des ide?es
dans un corpus oral. In Actes de la douxime Rencon-
tre des tudiants Chercheurs en Informatique pour le
Traitement Automatique des langues (RECITAL).
B.A. Lively and S.L. Pressey. 1923. A method for mea-
suring the vocabulary burden of textbooks. Educa-
tional Administration and Supervision, 9:389?398.
E. Miltsakaki. 2009. Matching readers? preferences and
reading skills with appropriate web texts. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Demonstrations Session, pages 49?52.
B. New, C. Pallier, M. Brysbaert, and L. Ferrand. 2004.
Lexique 2: A new French lexical database. Behav-
ior Research Methods, Instruments, & Computers,
36(3):516.
C. Pallier. 1999. Syllabation des repre?sentations
phone?tiques de brulex et de lexique. Technical report,
Technical Report, update 2004. Lien: http://www. pal-
lier. org/ressources/syllabif/syllabation. pdf.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 186?195.
J.C. Redish and J. Selzer. 1985. The place of readability
formulas in technical communication. Technical com-
munication, 32(4):46?52.
S.E. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and statis-
tical language models. Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 523?530.
L. Si and J. Callan. 2001. A statistical model for sci-
entific readability. In Proceedings of the Tenth Inter-
national Conference on Information and Knowledge
Management, pages 574?576. ACM New York, NY,
USA.
J.B. Tharp. 1939. The Measurement of Vocabulary Dif-
ficulty. Modern Language Journal, pages 169?178.
S. Uitdenbogerd. 2005. Readability of French as a for-
eign language and its uses. In Proceedings of the Aus-
tralian Document Computing Symposium, pages 19?
25.
M. Vogel and C. Washburne. 1928. An objective method
of determining grade placement of children?s reading
material. The Elementary School Journal, 28(5):373?
381.
57
