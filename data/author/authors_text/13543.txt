Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 379?383,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment
Shujian Huang
State Key Laboratory for
Novel Software Technology
Nanjing University
huangsj@nlp.nju.edu.cn
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
vogel@cs.cmu.edu
Jiajun Chen
State Key Laboratory for
Novel Software Technology
Nanjing University
chenjj@nlp.nju.edu.cn
Abstract
Word alignment has an exponentially large
search space, which often makes exact infer-
ence infeasible. Recent studies have shown
that inversion transduction grammars are rea-
sonable constraints for word alignment, and
that the constrained space could be efficiently
searched using synchronous parsing algo-
rithms. However, spurious ambiguity may oc-
cur in synchronous parsing and cause prob-
lems in both search efficiency and accuracy. In
this paper, we conduct a detailed study of the
causes of spurious ambiguity and how it ef-
fects parsing and discriminative learning. We
also propose a variant of the grammar which
eliminates those ambiguities. Our grammar
shows advantages over previous grammars in
both synthetic and real-world experiments.
1 Introduction
In statistical machine translation, word alignment at-
tempts to find word correspondences in parallel sen-
tence pairs. The search space of word alignment
will grow exponentially with the length of source
and target sentences, which makes the inference for
complex models infeasible (Brown et al, 1993). Re-
cently, inversion transduction grammars (Wu, 1997),
namely ITG, have been used to constrain the search
space for word alignment (Zhang and Gildea, 2005;
Cherry and Lin, 2007; Haghighi et al, 2009; Liu et
al., 2010). ITG is a family of grammars in which the
right hand side of the rule is either two nonterminals
or a terminal sequence. The most general case of the
ITG family is the bracketing transduction grammar
A? [AA] | ?AA? | e/f | /f | e/
Figure 1: BTG rules. [AA] denotes a monotone concate-
nation and ?AA? denotes an inverted concatenation.
(BTG, Figure 1), which has only one nonterminal
symbol.
Synchronous parsing of ITG may generate a large
number of different derivations for the same under-
lying word alignment. This is often referred to as
the spurious ambiguity problem. Calculating and
saving those derivations will slow down the parsing
speed significantly. Furthermore, spurious deriva-
tions may fill up the n-best list and supersede po-
tentially good results, making it harder to find the
best alignment. Besides, over-counting those spu-
rious derivations will also affect the likelihood es-
timation. In order to reduce spurious derivations,
Wu (1997), Haghighi et al (2009), Liu et al (2010)
propose different variations of the grammar. These
grammars have different behaviors in parsing effi-
ciency and accuracy, but so far no detailed compari-
son between them has been done.
In this paper, we formally analyze alignments un-
der ITG constraints and the different causes of spu-
rious ambiguity for those alignments. We do an em-
pirical study of the influence of spurious ambiguity
on parsing and discriminative learning by compar-
ing different grammars in both synthetic and real-
data experiments. To our knowledge, this is the first
in-depth analysis on this specific issue. A new vari-
ant of the grammar is proposed, which efficiently re-
moves all spurious ambiguities. Our grammar shows
advantages over previous ones in both experiments.
379
AA
A A
A
e1 e2 e3
f1 f2 f3
A
A A
A A
e1 e2 e3
f1 f2 f3
A
A
A A
A
e1 e2 e3
f1 f2 f3
A
A A
A A
e1 e2 e3
f1 f2 f3
Figure 2: Possible monotone/inverted t-splits (dashed
lines) under BTG, causing branching ambiguities.
2 ITG Alignment Family
By lexical rules like A ? e/f , each ITG derivation
actually represents a unique alignment between the
two sequences. Thus the family of ITG derivations
represents a family of word alignment.
Definition 1. The ITG alignment family is a set of
word alignments that has at least one BTG deriva-
tion.
ITG alignment family is only a subset of word
alignments because there are cases, known as inside-
outside alignments (Wu, 1997), that could not be
represented by any ITG derivation. On the other
hand, an ITG alignment may have multiple deriva-
tions.
Definition 2. For a given grammar G, spurious am-
biguity in word alignment is the case where two or
more derivations d1, d2, ... dk of G have the same
underlying word alignmentA. A grammarG is non-
spurious if for any given word alignment, there exist
at most one derivation under G.
In any given derivation, an ITG rule applies by ei-
ther generating a bilingual word pair (lexical rules)
or splitting the current alignment into two parts,
which will recursively generate two sub-derivations
(transition rules).
Definition 3. Applying a monotone (or inverted)
concatenation transition rule forms a monotone t-
split (or inverted t-split) of the original alignment
(Figure 2).
3 Causes of Spurious Ambiguity
3.1 Branching Ambiguity
As shown in Figure 2, left-branching and right-
branching will produce different derivations under
A? [AB] | [BB] | [CB] | [AC] | [BC] | [CC]
B ? ?AA? | ?BA? | ?CA? | ?AC? | ?BC? | ?CC?
C ? e/f | /f | e/
Figure 3: A Left heavy Grammar (LG).
BTG, but yield the same word alignment. Branching
ambiguity was identified and solved in Wu (1997),
using the grammar in Figure 3, denoted as LG. LG
uses two separate non-terminals for monotone and
inverted concatenation, respectively. It only allows
left branching of such non-terminals, by excluding
rules like A? [BA].
Theorem 1. For each ITG alignment A, in which
all the words are aligned, LG will produce a unique
derivation.
Proof: Induction on n, the length of A. Case n=1
is trivial. Induction hypothesis: the theorem holds
for any A with length less than n.
For A of length n, let s be the right most t-split
which splits A into S1 and S2. s exists because A is
an ITG alignment. Assume that there exists another
t-split s?, splitting A into S11 and (S12S2). Because
A is fixed and fully aligned, it is easy to see that if
s is a monotone t-split, s? could only be monotone,
and S12 and S2 in the right sub-derivation of t-split s?
could only be combined by monotone concatenation
as well. So s? will have a right branching of mono-
tone concatenation, which contradicts with the def-
inition of LG because right branching of monotone
concatenations is prohibited. A similar contradic-
tion occurs if s is an inverted t-split. Thus s should
be the unique t-split forA. By I.H., S1 and S2 have a
unique derivation, because their lengths are less than
n. Thus the derivation for A will be unique.
3.2 Null-word Attachment Ambiguity
Definition 4. For any given sentence pair (e, f) and
its alignment A, let (e?, f ?) be the sentence pairs
with all null-aligned words removed from (e, f).
The alignment skeletonAS is the alignment between
(e?, f ?) that preserves all links in A.
From Theorem 1 we know that every ITG align-
ment has a unique LG derivation for its alignment
skeleton (Figure 4 (c)).
However, because of the lexical or syntactic dif-
ferences between languages, some words may have
380
AC B
A
C C
C
e1/ e2 e3 e4
f1 f2 f3
(a)
B
A
A
C C
C
C
e1/ e2 e3 e4
f1 f2 f3
(b)
B
A
C
C01
Ct C01
C
C
e1/ e2 e3 e4
f1 f2 f3
(c)
Figure 4: Null-word attachment for the same alignment.
((a) and (b) are spurious derivations under LG caused
by null-aligned words attachment. (c) shows the unique
derivation under LGFN. The dotted lines have omitted
some unary rules for simplicity. The dashed box marks
the alignment skeleton.)
A? [AB] | [BB] | [CB] | [AC] | [BC] | [CC]
B ? ?AA? | ?BA? | ?CA? | ?AC? | ?BC? | ?CC?
C ? C01 | [Cs C]
C01 ? C00 | [Ct C01]
C00 ? e/f, Ct ? e/, Cs ? /f
Figure 5: A Left heavy Grammar with Fixed Null-word
attachment (LGFN).
no explicit correspondence in the other language and
tend to stay unaligned. These null-aligned words,
also called singletons, should be attached to some
other nodes in the derivation. It will produce dif-
ferent derivations if those null-aligned words are at-
tached by different rules, or to different nodes.
Haghighi et al (2009) give some restrictions on
null-aligned word attachment. However, they fail to
restrict the node to which the null-aligned word is
attached, e.g. the cases (a) and (b) in Figure 4.
3.3 LGFN Grammar
We propose here a new variant of ITG, denoted as
LGFN (Figure 5). Our grammar takes similar tran-
sition rules as LG and efficiently constrains the at-
tachment of null-aligned words. We will empirically
compare those different grammars in the next sec-
tion.
Lemma 1. LGFN has a unique mapping from the
derivation of any given ITG alignment A to the
derivation of its alignment skeleton AS .
Proof: LGFN maps the null-aligned source word
sequence, Cs1 , Cs2 , ..., Csk , the null-aligned target
word sequence, Ct1 , Ct2 , ..., Ctk? , together with the
aligned word-pair C00 that directly follows, to the
nodeC exactly in the way of Equation 1. The brack-
ets indicate monotone concatenations.
C ? [Cs1 ...[Csk [Ct1 ...[Ctk?C00]...]]...] (1)
The mapping exists when every null-aligned se-
quence has an aligned word-pair after it. Thus it
requires an artificial word at the end of the sentence.
Note that our grammar attaches null-aligned
words in a right-branching manner, which means it
builds the span only when there is an aligned word-
pair. After initialization, any newly-built span will
contain at least one aligned word-pair. Compara-
tively, the grammar in Liu et al (2010) uses a left-
branching manner. It may generate more spans that
only contain null-aligned words, which makes it less
efficient than ours.
Theorem 2. LGFN has a unique derivation for each
ITG alignment, i.e. LGFN is non-spurious.
Proof: Derived directly from Definition 4, Theo-
rem 1 and Lemma 1.
4 Experiments
4.1 Synthetic Experiments
We automatically generated 1000 fully aligned ITG
alignments of length 20 by generating random per-
mutations first and checking ITG constraints using a
linear time algorithm (Zhang et al, 2006). Sparser
alignments were generated by random removal of
alignment links according to a given null-aligned
word ratio. Four grammars were used to parse these
alignments, namely LG (Wu, 1997), HaG (Haghighi
et al, 2009), LiuG (Liu et al, 2010) and LGFN (Sec-
tion 3.3).
Table 1 shows the average number of derivations
per alignment generated under LG and HaG. The
number of derivations produced by LG increased
dramatically because LG has no restrictions on null-
aligned word attachment. HaG also produced a large
number of spurious derivations as the number of
null-aligned words increased. Both LiuG and LGFN
produced a unique derivation for each alignment, as
expected. One interpretation is that in order to get
381
% 0 5 10 15 20 25
LG 1 42.2 1920.8 9914.1+ 10000+ 10000+
HaG 1 3.5 10.9 34.1 89.2 219.9
Table 1: Average #derivations per alignment for LG and
HaG v.s. Percentage of unaligned words. (+ marked
parses have reached the beam size limit of 10000.)
600
s)
Ha
G
Liu
G
200300400500
sing time (
Ha
G
Liu
G
LF
G
LG
0100
0
5
10
15
20
25
Par
P
t
f
ll
li
d
d
Per
cen
tag
e o
f n
ull
-al
ign
ed 
wo
rds
 
Figure 6: Total parsing time (in seconds) v.s. Percentage
of un-aligned words.
the 10-best alignments for sentence pairs that have
10% of words unaligned, the top 109 HaG deriva-
tions should be generated, while the top 10 LiuG or
LGFN derivations are already enough.
Figure 6 shows the total parsing time using each
grammar. LG and HaG showed better performances
when most of the words were aligned because their
grammars are simpler and less constrained. How-
ever, when the number of null-aligned words in-
creased, the parsing times for LG and HaG became
much longer, caused by the calculation of the large
number of spurious derivations. Parsings using LG
for 10 and 15 percent of null-aligned words took
around 15 and 80 minutes, respectively, which can-
not be plotted in the same scale with other gram-
mars. The parsing times of LGFN and LiuG also
slowly increased, but parsing LGFN consistently
took less time than LiuG.
It should be noticed that the above results came
from parsing according to some given alignment.
When searching without knowing the correct align-
ment, it is possible for every word to stay unaligned,
which makes spurious ambiguity a much more seri-
ous issue.
4.2 Discriminative Learning Experiments
To further study how spurious ambiguity affects the
discriminative learning, we implemented a frame-
work following Haghighi et al (2009). We used
a log-linear model, with features like IBM model1
020.21 01
7
0.180.190.
2
AE R
0.150.160.1
7
1
6
11
16
A
Ha
G-
20be
st
LF
G-
1bes
t
LF
G-
20be
st
Num
ber o
f it
era
tio
ns
Figure 7: Test set AER after each iteration.
probabilities (collected from FBIS data), relative
distances, matchings of high frequency words,
matchings of pos-tags, etc. Online training was
performed using the margin infused relaxed algo-
rithm (Crammer et al, 2006), MIRA. For each
sentence pair (e, f), we optimized with alignment
results generated from the nbest parsing results.
Alignment error rate (Och and Ney, 2003), AER,
was used as the loss function. We ran MIRA train-
ing for 20 iterations and evaluated the alignments of
the best-scored derivations on the test set using the
average weights.
We used the manually aligned Chinese-English
corpus in NIST MT02 evaluation. The first 200 sen-
tence pairs were used for training, and the last 150
for testing. There are, on average, 10.3% words stay
null-aligned in each sentence, but if restricted to sure
links the average ratio increases to 22.6%.
We compared training using LGFN with 1-best,
20-best and HaG with 20-best (Figure 7). Train-
ing with HaG only obtained similar results with 1-
best trained LGFN, which demonstrated that spu-
rious ambiguity highly affected the nbest list here,
resulting in a less accurate training. Actually, the
20-best parsing using HaG only generated 4.53 dif-
ferent alignments on average. 20-best training us-
ing LGFN converged quickly after the first few it-
erations and obtained an AER score (17.23) better
than other systems, which is also lower than the re-
fined IBM Model 4 result (19.07).
We also trained a similar discriminative model but
extended the lexical rule of LGFN to accept at max-
imum 3 consecutive words. The model was used
to align FBIS data for machine translation exper-
iments. Without initializing by phrases extracted
from existing alignments (Cherry and Lin, 2007) or
using complicated block features (Haghighi et al,
382
2009), we further reduced AER on the test set to
12.25. An average improvement of 0.52 BLEU (Pa-
pineni et al, 2002) score and 2.05 TER (Snover
et al, 2006) score over 5 test sets for a typical
phrase-based translation system, Moses (Koehn et
al., 2003), validated the effectiveness of our experi-
ments.
5 Conclusion
Great efforts have been made in reducing spurious
ambiguities in parsing combinatory categorial gram-
mar (Karttunen, 1986; Eisner, 1996). However, to
our knowledge, we give the first detailed analysis on
spurious ambiguity of word alignment. Empirical
comparisons between different grammars also vali-
dates our analysis.
This paper makes its own contribution in demon-
strating that spurious ambiguity has a negative im-
pact on discriminative learning. We will continue
working on this line of research and improve our
discriminative learning model in the future, for ex-
ample, by adding more phrase level features.
It is worth noting that the definition of spuri-
ous ambiguity actually varies for different tasks. In
some cases, e.g. bilingual chunking, keeping differ-
ent null-aligned word attachments could be useful.
It will also be interesting to explore spurious ambi-
guity and its effects in those different tasks.
Acknowledgments
The authors would like to thank Alon Lavie, Qin
Gao and the anonymous reviewers for their valu-
able comments. This work is supported by the Na-
tional Natural Science Foundation of China (No.
61003112), the National Fundamental Research
Program of China (2010CB327903) and by NSF un-
der the CluE program, award IIS 084450.
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of the NAACL-HLT 2007/AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion, SSST ?07, pages 17?24, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585, December.
Jason Eisner. 1996. Efficient normal-form parsing for
combinatory categorial grammar. In Proceedings of
the 34th annual meeting on Association for Compu-
tational Linguistics, ACL ?96, pages 79?86, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Aria Haghighi, John Blitzer, and Dan Klein. 2009. Bet-
ter word alignments with supervised itg models. In
Association for Computational Linguistics, Singapore.
Lauri Karttunen. 1986. Radical lexicalism. Technical
Report CSLI-86-68, Stanford University.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Shujie Liu, Chi-Ho Li, and Ming Zhou. 2010. Dis-
criminative pruning for discriminative itg alignment.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ?10,
pages 316?324, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Matthew Snover, Bonnie J. Dorr, and Richard Schwartz.
2006. A study of translation edit rate with targeted
human annotation. In Proceedings of AMTA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23:377?403, September.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexi-
calized inversion transduction grammar for alignment.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, ACL ?05,
pages 475?482, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Compu-
tational Linguistics, pages 256?263, Morristown, NJ,
USA. Association for Computational Linguistics.
383
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 285?290,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Enhancing Statistical Machine Translation with Character Alignment 
 
Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang, Jiajun Chen 
State Key Laboratory for Novel Software Technology, 
Department of Computer Science and Technology, 
Nanjing University, Nanjing, 210046, China 
{xin,tanggc,dxy,huangsj,chenjj}@nlp.nju.edu.cn 
 
  
Abstract 
The dominant practice of statistical machine 
translation (SMT) uses the same Chinese word 
segmentation specification in both alignment 
and translation rule induction steps in building 
Chinese-English SMT system, which may suf-
fer from a suboptimal problem that word seg-
mentation better for alignment is not necessarily 
better for translation. To tackle this, we propose 
a framework that uses two different segmenta-
tion specifications for alignment and translation 
respectively: we use Chinese character as the 
basic unit for alignment, and then convert this 
alignment to conventional word alignment for 
translation rule induction. Experimentally, our 
approach outperformed two baselines: fully 
word-based system (using word for both 
alignment and translation) and fully charac-
ter-based system, in terms of alignment quality 
and translation performance. 
1 Introduction 
Chinese Word segmentation is a necessary step in 
Chinese-English statistical machine translation 
(SMT) because Chinese sentences do not delimit 
words by spaces. The key characteristic of a Chi-
nese word segmenter is the segmentation specifi-
cation1. As depicted in Figure 1(a), the dominant 
practice of SMT uses the same word segmentation 
for both word alignment and translation rule induc-
tion. For brevity, we will refer to the word seg-
mentation of the bilingual corpus as word segmen-
tation for alignment (WSA for short), because it 
determines the basic tokens for alignment; and refer 
to the word segmentation of the aligned corpus as 
word segmentation for rules (WSR for short), be-
cause it determines the basic tokens of translation 
                                                          
1 We hereafter use ?word segmentation? for short. 
rules2, which also determines how the translation 
rules would be matched by the source sentences. 
It is widely accepted that word segmentation with 
a higher F-score will not necessarily yield better 
translation performance (Chang et al, 2008; Zhang 
et al, 2008; Xiao et al, 2010). Therefore, many 
approaches have been proposed to learn word 
segmentation suitable for SMT. These approaches 
were either complicated (Ma et al, 2007; Chang et 
al., 2008; Ma and Way, 2009; Paul et al, 2010), or 
of high computational complexity (Chung and 
Gildea 2009; Duan et al, 2010). Moreover, they 
implicitly assumed that WSA and WSR should be 
equal. This requirement may lead to a suboptimal 
problem that word segmentation better for align-
ment is not necessarily better for translation. 
To tackle this, we propose a framework that uses 
different word segmentation specifications as WSA 
and WSR respectively, as shown Figure 1(b). We 
investigate a solution in this framework: first, we 
use Chinese character as the basic unit for align-
ment, viz. character alignment; second, we use a 
simple method (Elming and Habash, 2007) to 
convert the character alignment to conventional 
word alignment for translation rule induction. In the 
                                                          
2 Interestingly, word is also a basic token in syntax-based rules. 
Word alignment 
Bilingual Corpus 
Aligned Corpus 
WSA
Translation Rules
WSA 
WSR 
Rule induction 
Decoding 
Translation Results WSR
Word alignment 
Bilingual Corpus 
Aligned Corpus 
WSA
Translation Rules 
WSA
WSR
Rule induction 
Decoding 
Translation Results WSR
Aligned Corpus 
WSR
Conversion 
(b) WSA?WSR 
Figure 1. WSA and WSR in SMT pipeline
(a)  WSA=WSR 
285
experiment, our approach consistently outper-
formed two baselines with three different word 
segmenters: fully word-based system (using word 
for both alignment and translation) and fully char-
acter-based system, in terms of alignment quality 
and translation performance. 
The remainder of this paper is structured as fol-
lows: Section 2 analyzes the influences of WSA and 
WSR on SMT respectively; Section 3 discusses 
how to convert character alignment to word align-
ment; Section 4 presents experimental results, fol-
lowed by conclusions and future work in section 5. 
2 Understanding WSA and WSR 
We propose a solution to tackle the suboptimal 
problem: using Chinese character for alignment 
while using Chinese word for translation. Character 
alignment differs from conventional word align-
ment in the basic tokens of the Chinese side of the 
training corpus3. Table 1 compares the token dis-
tributions of character-based corpus (CCorpus) and 
word-based corpus (WCorpus). We see that the 
WCorpus has a longer-tailed distribution than the 
CCorpus. More than 70% of the unique tokens ap-
pear less than 5 times in WCorpus. However, over 
half of the tokens appear more than or equal to 5 
times in the CCorpus.  This indicates that modeling 
word alignment could suffer more from data 
sparsity than modeling character alignment.  
Table 2 shows the numbers of the unique tokens 
(#UT) and unique bilingual token pairs (#UTP) of 
the two corpora. Consider two extensively features, 
fertility and translation features, which are exten-
sively used by many state-of-the-art word aligners. 
The number of parameters w.r.t. fertility features 
grows linearly with #UT while the number of pa-
rameters w.r.t. translation features grows linearly 
with #UTP. We compare #UT and #UTP of both 
corpora in Table 2. As can be seen, CCorpus has 
less UT and UTP than WCorpus, i.e. character 
alignment model has a compact parameterization 
than word alignment model, where the compactness 
of parameterization is shown very important in sta-
tistical modeling (Collins, 1999). 
Another advantage of character alignment is the 
reduction in alignment errors caused by word seg- 
                                                          
3 Several works have proposed to use character (letter) on both 
sides of the parallel corpus for SMT between similar (European) 
languages (Vilar et al, 2007; Tiedemann, 2009), however, 
Chinese is not similar to English. 
Frequency Characters (%) Words (%) 
1 27.22 45.39 
2 11.13 14.61 
3 6.18 6.47 
4 4.26 4.32 
5(+) 50.21 29.21 
Table 1 Token distribution of CCorpus and WCorpus 
 
Stats. Characters Words 
#UT 9.7K 88.1K 
#UTP 15.8M 24.2M 
Table 2 #UT and #UTP in CCorpus and WCorpus 
 
mentation errors. For example, ??? (Cheney)? 
and ?? (will)? are wrongly merged into one word 
???  by the word segmenter, and ??? 
wrongly aligns to a comma in English sentence in 
the word alignment; However, both ? and ? align 
to ?Cheney? correctly in the character alignment. 
However, this kind of errors cannot be fixed by 
methods which learn new words by packing already 
segmented words, such as word packing (Ma et al, 
2007) and Pseudo-word (Duan et al, 2010). 
As character could preserve more meanings than 
word in Chinese, it seems that a character can be 
wrongly aligned to many English words by the 
aligner. However, we found this can be avoided to a 
great extent by the basic features (co-occurrence 
and distortion) used by many alignment models. For 
example, we observed that the four characters of the 
non-compositional word ????? (Arafat)? align 
to Arafat correctly, although these characters pre-
serve different meanings from that of Arafat. This 
can be attributed to the frequent co-occurrence (192 
times) of these characters and Arafat in CCorpus. 
Moreover,?  usually means France in Chinese, 
thus it may co-occur very often with France in 
CCorpus. If both France and Arafat appear in the 
English sentence, ? may wrongly align to France. 
However, if ? aligns to Arafat, ? will probably 
align to Arafat, because aligning ? to Arafat could 
result in a lower distortion cost than aligning it to 
France. 
Different from alignment, translation is a pattern 
matching procedure (Lopez, 2008). WSR deter-
mines how the translation rules would be matched 
by the source sentences. For example, if we use 
translation rules with character as WSR to translate 
name entities such as the non-compositional word 
????, i.e. translating literally, we may get a 
wrong translation. That?s because the linguistic 
286
knowledge that the four characters convey a spe-
cific meaning different from the characters has been 
lost, which cannot always be totally recovered even 
by using phrase in phrase-based SMT systems (see 
Chang et al (2008) for detail). Duan et al (2010) 
and Paul et al, (2010) further pointed out that 
coarser-grained segmentation of the source sen-
tence do help capture more contexts in translation. 
Therefore, rather than using character, using 
coarser-grained, at least as coarser as the conven-
tional word, as WSR is quite necessary. 
3 Converting Character Alignment to Word 
Alignment 
In order to use word as WSR, we employ the same 
method as Elming and Habash (2007)4 to convert 
the character alignment (CA) to its word-based 
version (CA?) for translation rule induction. The 
conversion is very intuitive: for every Eng-
lish-Chinese word pair ??, ?? in the sentence pair, 
we align ? to ? as a link in CA?, if and only if there 
is at least one Chinese character of ? aligns to ? in 
CA.  
Given two different segmentations A and B of the 
same sentence, it is easy to prove that if every word 
in A is finer-grained than the word of B at the cor-
responding position, the conversion is unambiguity 
(we omit the proof due to space limitation). As 
character is a finer-grained than its original word, 
character alignment can always be converted to 
alignment based on any word segmentation. 
Therefore, our approach can be naturally scaled to 
syntax-based system by converting character 
alignment to word alignment where the word seg-
mentation is consistent with the parsers. 
We compare CA with the conventional word 
alignment (WA) as follows: We hand-align some 
sentence pairs as the evaluation set based on char-
acters (ESChar), and converted it to the evaluation 
set based on word (ESWord) using the above con-
version method. It is worth noting that comparing 
CA and WA by evaluating CA on ESChar and 
evaluating WA on ESWord is meaningless, because 
the basic tokens in CA and WA are different. 
However, based on the conversion method, com-
paring CA with WA can be accomplished by evalu-
ating both CA? and WA on ESWord. 
                                                          
4 They used this conversion for word alignment combination 
only, no translation results were reported. 
4 Experiments 
4.1 Setup 
FBIS corpus (LDC2003E14) (210K sentence pairs) 
was used for small-scale task. A large bilingual 
corpus of our lab (1.9M sentence pairs) was used for 
large-scale task. The NIST?06 and NIST?08 test sets 
were used as the development set and test set re-
spectively. The Chinese portions of all these data 
were preprocessed by character segmenter (CHAR), 
ICTCLAS word segmenter 5  (ICT) and Stanford 
word segmenters with CTB  and PKU specifica-
tions6 respectively. The first 100 sentence pairs of 
the hand-aligned set in Haghighi et al (2009) were 
hand-aligned as ESChar, which is converted to 
three ESWords based on three segmentations re-
spectively. These ESWords were appended to 
training corpus with the corresponding word seg-
mentation for evaluation purpose. 
Both character and word alignment were per-
formed by GIZA++ (Och and Ney, 2003) enhanced 
with gdf heuristics to combine bidirectional align-
ments (Koehn et al, 2003). A 5-gram language 
model was trained from the Xinhua portion of 
Gigaword corpus. A phrase-based MT decoder 
similar to (Koehn et al, 2007) was used with the 
decoding weights optimized by MERT (Och, 2003). 
4.2 Evaluation 
We first evaluate the alignment quality. The method 
discussed in section 3 was used to compare char-
acter and word alignment. As can be seen from 
Table 3, the systems using character as WSA out-
performed the ones using word as WSA in both 
small-scale (row 3-5) and large-scale task (row 6-8) 
with all segmentations. This gain can be attributed 
to the small vocabulary size (sparsity) for character 
alignment. The observation is consistent with 
Koehn (2005) which claimed that there is a negative 
correlation between the vocabulary size and trans-
lation performance without explicitly distinguish-
ing WSA and WSR. 
We then evaluated the translation performance. 
The baselines are fully word-based MT systems 
(WordSys), i.e. using word as both WSA and WSR, 
and fully character-based systems (CharSys). Table  
 
                                                          
5 http://www.ictclas.org/ 
6 http://nlp.stanford.edu/software/segmenter.shtml 
287
  Word alignment Character alignment 
  P R F P R F 
S 
CTB 76.0 81.9 78.9 78.2 85.2 81.8 
PKU 76.1 82.0 79.0 78.0 86.1 81.9 
ICT 75.2 80.8 78.0 78.7 86.3 82.3 
L 
CTB 79.6 85.6 82.5 82.2 90.6 86.2 
PKU 80.0 85.4 82.6 81.3 89.5 85.2 
ICT 80.0 85.0 82.4 81.3 89.7 85.3 
Table 3 Alignment evaluation. Precision (P), recall (R), 
and F-score (F) with ? ? 0.5 (Fraser and Marcu, 2007) 
 
 WSA WSR CTB PKU ICT 
S word word 21.52 20.99 20.95char word 22.04 21.98 22.04
L word word 22.07 22.86 22.23 char word 23.41 23.51 23.05 
Table 4 Translation evaluation of WordSys and pro-
posed system using BLEU-SBP (Chiang et al, 2008) 
 
4 compares WordSys to our proposed system. Sig-
nificant testing was carried out using bootstrap 
re-sampling method proposed by Koehn (2004) 
with a 95% confidence level. We see that our pro-
posed systems outperformed WordSys in all seg-
mentation specifications settings. Table 5 lists the 
results of CharSys in small-scale task. In this setting, 
we gradually set the phrase length and the distortion 
limits of the phrase-based decoder (context size) to 
7, 9, 11 and 13, in order to remove the disadvantage 
of shorter context size of using character as WSR 
for fair comparison with WordSys as suggested by 
Duan et al (2010). Comparing Table 4 and 5, we 
see that all CharSys underperformed WordSys. This 
observation is consistent with Chang et al (2008) 
which claimed that using characters, even with 
large phrase length (up to 13 in our experiment) 
cannot always capture everything a Chinese word 
segmenter can do, and using word for translation is 
quite necessary. We also see that CharSys under-
performed our proposed systems, that?s because the 
harm of using character as WSR outweighed the 
benefit of using character as WSA, which indicated 
that word segmentation better for alignment is not 
necessarily better for translation, and vice versa. 
We finally compared our approaches to Ma et al 
(2007) and Ma and Way (2009), which proposed 
?packed word (PW)? and ?bilingual motivated 
word (BS)? respectively. Both methods iteratively 
learn word segmentation and alignment alterna-
tively, with the former starting from word-based 
corpus and the latter starting from characters-based 
corpus. Therefore, PW can be experimented on all 
segmentations. Table 6 lists their results in small- 
Context Size 7 9 11 13 
BLEU 20.90 21.19 20.89 21.09 
Table 5 Translation evaluation of CharSys. 
 
System WSA WSR CTB PKU ICT 
WordSys word word 21.52 20.99 20.95
Proposed char word 22.04 21.98 22.04
PW PW PW 21.24 21.24 21.19 
Char+PW char PW 22.46 21.87 21.97 
BS BS BS 19.76 
Char+BS char BS 20.19 
Table 6 Comparison with other works 
 
scale task, we see that both PW and BS underper-
formed our approach. This may be attributed to the 
low recall of the learned BS or PW in their ap-
proaches. BS underperformed both two baselines, 
one reason is that Ma and Way (2009) also em-
ployed word lattice decoding techniques (Dyer et al, 
2008) to tackle the low recall of BS, which was 
removed from our experiments for fair comparison. 
Interestingly, we found that using character as 
WSA and BS as WSR (Char+BS), a moderate gain 
(+0.43 point) was achieved compared with fully 
BS-based system; and using character as WSA and 
PW as WSR (Char+PW), significant gains were 
achieved compared with fully PW-based system, 
the result of CTB segmentation in this setting even 
outperformed our proposed approach (+0.42 point). 
This observation indicated that in our framework, 
better combinations of WSA and WSR can be found 
to achieve better translation performance. 
5 Conclusions and Future Work 
We proposed a SMT framework that uses character 
for alignment and word for translation, which im-
proved both alignment quality and translation per-
formance. We believe that in this framework, using 
other finer-grained segmentation, with fewer am-
biguities than character, would better parameterize 
the alignment models, while using other coars-
er-grained segmentation as WSR can help capture 
more linguistic knowledge than word to get better 
translation. We also believe that our approach, if 
integrated with combination techniques (Dyer et al, 
2008; Xi et al, 2011), can yield better results. 
 
Acknowledgments 
We thank ACL reviewers. This work is supported 
by the National Natural Science Foundation of 
China (No. 61003112), the National Fundamental 
Research Program of China (2010CB327903). 
288
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della 
Peitra, and Robert L. Mercer. 1993. The mathematics 
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2), pages 
263-311. 
Pi-Chuan Chang, Michel Galley, and Christopher D. 
Manning. 2008. Optimizing Chinese word segmenta-
tion for machine translation performance.  In Pro-
ceedings of third workshop on SMT, pages 224-232. 
David Chiang, Steve DeNeefe, Yee Seng Chan and 
Hwee Tou Ng. 2008. Decomposability of Translation 
Metrics for Improved Evaluation and Efficient Algo-
rithms. In Proceedings of Conference on Empirical 
Methods in Natural Language Processing, pages 
610-619. 
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of Conference on Empirical Methods in 
Natural Language Processing, pages 718-726. 
Michael Collins. 1999. Head-driven statistical models 
for natural language parsing. Ph.D. thesis, University 
of Pennsylvania. 
Xiangyu  Duan, Min Zhang,  and  Haizhou Li. 2010. 
Pseudo-word for phrase-based machine translation. In 
Proceedings of the Association for Computational 
Linguistics, pages 148-156. 
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 
2008. Generalizing word lattice translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 1012-1020. 
Jakob Elming and Nizar Habash. 2007. Combination of 
statistical word alignments based on multiple pre-
processing schemes. In Proceedings of the Associa-
tion for Computational Linguistics, pages 25-28. 
Alexander Fraser and Daniel Marcu. 2007. Squibs and 
Discussions: Measuring Word Alignment Quality for 
Statistical Machine Translation. In Computational 
Linguistics, 33(3), pages 293-303. 
Aria Haghighi, John Blitzer, John DeNero, and Dan 
Klein. 2009. Better word alignments with supervised 
ITG models. In Proceedings of the Association for 
Computational Linguistics, pages 923-931. 
Phillip Koehn, H. Hoang, A. Birch, C. Callison-Burch, 
M. Federico, N. Bertoldi, B. Cowan,W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. 
Herbst. 2007. Moses: Open source toolkit for statis-
tical machine translation. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 177-180.  
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
Conference on Empirical Methods on Natural Lan-
guage Processing, pages 388-395. 
Philipp Koehn. 2005. Europarl: A parallel corpus for 
statistical machine translation. In Proceedings of the 
MT Summit. 
Adam David Lopez. 2008. Machine translation by pat-
tern matching. Ph.D. thesis, University of Maryland. 
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. 
Bootstrapping word alignment via word packing. In 
Proceedings of the Association for Computational 
Linguistics, pages 304-311. 
Yanjun Ma and Andy Way. 2009. Bilingually motivated 
domain-adapted word segmentation for statistical 
machine translation. In Proceedings of the Conference 
of the European Chapter of the ACL, pages 549-557. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of the 
Association for Computational Linguistics, pages 
440-447. 
Franz Josef Och and Hermann  Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1), pages 19-51. 
Michael Paul, Andrew Finch and Eiichiro Sumita. 2010. 
Integration of multiple bilingually-learned segmenta-
tion schemes into statistical machine translation. In 
Proceedings of the Joint Fifth Workshop on Statistical 
Machine Translation and MetricsMATR, pages 
400-408. 
J?rg Tiedemann. 2009. Character-based PSMT for 
closely related languages. In Proceedings of the An-
nual Conference of the European Association for 
machine Translation, pages 12-19. 
David Vilar, Jan-T. Peter and Hermann Ney. 2007. Can 
we translate letters? In Proceedings of the Second 
Workshop on Statistical Machine Translation, pages 
33-39. 
Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu 
and Shouxun Lin. 2010.  Joint tokenization and 
translation. In Proceedings of the 23rd International 
Conference on Computational Linguistics, pages 
1200-1208. 
Ning  Xi, Guangchao  Tang,  Boyuan  Li, and  Yinggong 
Zhao. 2011. Word alignment combination over mul-
tiple word segmentation. In Proceedings of the ACL 
2011 Student Session, pages 1-5. 
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 
2008. Improved statistical machine translation by 
multiple Chinese word segmentation. In Proceedings 
289
of the Third Workshop on Statistical Machine Trans-
lation, pages 216-223. 
 
290
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135?143,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improving Word Alignment by Semi-supervised Ensemble
Shujian Huang1, Kangxi Li2, Xinyu Dai1, Jiajun Chen1
1State Key Laboratory for Novel Software Technology at Nanjing University
Nanjing 210093, P.R.China
{huangsj,daixy,chenjj}@nlp.nju.edu.cn
2School of Foreign Studies, Nanjing University
Nanjing 210093, P.R.China
richardlkx@126.com
Abstract
Supervised learning has been recently
used to improve the performance of word
alignment. However, due to the limited
amount of labeled data, the performance
of ?pure? supervised learning, which only
used labeled data, is limited. As a re-
sult, many existing methods employ fea-
tures learnt from a large amount of unla-
beled data to assist the task. In this pa-
per, we propose a semi-supervised ensem-
ble method to better incorporate both la-
beled and unlabeled data during learning.
Firstly, we employ an ensemble learning
framework, which effectively uses align-
ment results from different unsupervised
alignment models. We then propose to
use a semi-supervised learning method,
namely Tri-training, to train classifiers us-
ing both labeled and unlabeled data col-
laboratively and further improve the result.
Experimental results show that our meth-
ods can substantially improve the quality
of word alignment. The final translation
quality of a phrase-based translation sys-
tem is slightly improved, as well.
1 Introduction
Word alignment is the process of learning bilin-
gual word correspondences. Conventional word
alignment process is treated as an unsupervised
learning task, which automatically learns the cor-
respondences between bilingual words using an
EM style algorithm (Brown et al, 1993; Vogel
et al, 1996; Och and Ney, 2003). Recently, su-
pervised learning methods have been used to im-
prove the performance. They firstly re-formalize
word alignment as some kind of classification
task. Then the labeled data is used to train the
classification model, which is finally used to clas-
sify unseen test data (Liu et al, 2005; Taskar et
al., 2005; Moore, 2005; Cherry and Lin, 2006;
Haghighi et al, 2009).
It is well understood that the performance of
supervised learning relies heavily on the fea-
ture set. As more and more features are added
into the model, more data is needed for train-
ing. However, due to the expensive cost of la-
beling, we usually cannot get as much labeled
word alignment data as we want. This may limit
the performance of supervised methods (Wu et
al., 2006). One possible alternative is to use
features learnt in some unsupervised manner to
help the task. For example, Moore (2005) uses
statistics like log-likelihood-ratio and conditional-
likelihood-probability to measure word associa-
tions; Liu et al (2005) and Taskar et al (2005)
use results from IBM Model 3 and Model 4, re-
spectively.
Ayan and Dorr (2006) propose another way of
incorporating unlabeled data. They first train some
existing alignment models, e.g. IBM Model4 and
Hidden Markov Model, using unlabeled data. The
results of these models are then combined using a
maximum entropy classifier, which is trained us-
ing labeled data. This method is highly efficient
in training because it only makes decisions on
alignment links from existing models and avoids
searching the entire alignment space.
In this paper, we follow Ayan and Dorr (2006)?s
idea of combining multiple alignment results. And
we use more features, such as bi-lexical features,
which help capture more information from unla-
beled data. To further improve the decision mak-
ing during combination, we propose to use a semi-
supervised strategy, namely Tri-training (Zhou
and Li, 2005), which ensembles three classifiers
using both labeled and unlabeled data. More
specifically, Tri-training iteratively trains three
classifiers and labels all the unlabeled instances.
It then uses some instances among the unlabeled
ones to expand the labeled training set of each in-
135
dividual classifier. As word alignment task usually
faces a huge parallel corpus, which contains mil-
lions of unlabeled instances, we develop specific
algorithms to adapt Tri-training for this large scale
task.
The next section introduces the supervised
alignment combination framework; Section 3
presents our semi-supervised learning algorithm.
We show the experiments and results in Section
4; briefly overview related work in Section 5 and
conclude in the last section.
2 Word Alignment as a Classification
Task
2.1 Modeling
Given a sentence pair (e, f), where e =
e1, e2, . . . , eI and f = f1, f2, . . . , fJ , an align-
ment link ai,j indicates the translation correspon-
dence between words ei and fj . Word alignment
is to learn the correct alignment A between e and
f , which is a set of such alignment links.
As the number of possible alignment links
grows exponentially with the length of e and f , we
restrict the candidate set using results from several
existing alignment models. Note that, all the mod-
els we employ are unsupervised models. We will
refer to them as sub-models in the rest of this pa-
per.
Let A = {A1, A2, . . . , An} be a set of align-
ment results from sub-models; AI and AU be the
intersection and union of these results, respec-
tively. We define our learning task as: for each
alignment link ai,j in the candidate set AC =
AU?AI , deciding whether ai,j should be included
in the alignment result. We use a random variable
yi,j (or simply y) to indicate whether an alignment
link ai,j ? AC is correct. A Maximum Entropy
model is employed to directly model the distribu-
tion of y. The probability of y is defined in For-
mula 1, where hm(y, e, f ,A, i, j) is the mth fea-
ture function, and ?m is the corresponding weight.
p(y|e, f ,A, i, j)
=
exp
?M
m=1 ?mhm(y, e, f ,A, i, j)
?
y??{0,1} exp
?M
m=1 ?mhm(y?, e, f ,A, i, j)
(1)
While Ayan and Dorr (Ayan and Dorr, 2006)
make decisions on each alignment link in AU , we
take a different strategy by assuming that all the
alignment links in AI are correct, which means
alignment links in AI are always included in the
combination result. One reason for using this
strategy is that it makes no sense to exclude an
alignment link, which all the sub-models vote for
including. Also, links in AI usually have a good
quality (In our experiment, AI can always achieve
an accuracy higher than 96%). On the other hand,
because AI is decided before the supervised learn-
ing starts, it will be able to provide evidence for
making decisions on candidate links.
Also note that, Formula 1 is based on the as-
sumption that given AI , the decision on each y is
independent of each other. This is the crucial point
that saves us from searching the whole alignment
space. We take this assumption so that the Tri-
training strategy can be easily applied.
2.2 Features
For ensemble, the most important features are the
decisions of sub-models. We also use some other
features, such as POS tags, neighborhood infor-
mation, etc. Details of the features for a given link
ai,j are listed below.
Decision of sub-models: Whether ai,j exists in
the result of kth sub-model Ak. Besides in-
dividual features for each model, we also in-
clude features describing the combination of
sub-models? decisions. For example, if we
have 3 sub-models, there will be 8 features
indicating the decisions of all the sub-models
as 000, 001, 010, . . . , 111.
Part of speech tags: POS tags of previous, cur-
rent and next words in both languages. We
also include features describing the POS tag
pairs of previous, current and next word pairs
in the two languages.
Neighborhood: Whether each neighbor link ex-
ists in the intersection AI . Neighbor links re-
fer to links in a 3*3 window with (i, j) in the
center.
Fertilities: The number of words that ei (or fj) is
aligned to in AI .
Relative distance: The relative distance between
ei and fj , which is calculated as abs(i/I ?
j/J).
Conditional Link Probability (CLP) : The con-
ditional link probability (Moore, 2005) of ei
136
and fj . CLP of word e and f is estimated on
an aligned parallel corpus using Formula 2,
CLPd(e, f) =
link(e, f)? d
cooc(e, f)
(2)
where link(e, f) is the number of times e and
f are linked in the aligned corpus; cooc(e, f)
is the number of times e and f appear in
the same sentence pair; d is a discount-
ing constant which is set to 0.4 following
Moore (2005). We estimate these counts on
our set of unlabeled data, with the union of
all sub-model results AU as the alignment.
Union is used in order to get a better link cov-
erage. Probabilities are computed only for
those words that occur at least twice in the
parallel corpus.
bi-lexical features: The lexical word pair ei-fj .
Lexical features have been proved to be useful in
tasks such as parsing and name entity recognition.
Taskar et al (2005) also employ similar bi-lexical
features of the top 5 non-punctuation words for
word alignment. Using bi-lexicons for arbitrary
word pairs will capture more evidence from the
data; although it results in a huge feature set which
may suffer from data sparseness. In the next sec-
tion, we introduce a semi-supervised strategy will
may alleviate this problem and further improve the
learning procedure.
3 Semi-supervised methods
Semi-supervised methods aim at using unlabeled
instances to assist the supervised learning. One
of the prominent achievements in this area is
the Co-training paradigm proposed by Blum and
Mitchell (1998). Co-training applies when the fea-
tures of an instance can be naturally divided into
two sufficient and redundant subsets. Two weak
classifiers can be trained using each subset of fea-
tures and strengthened using unlabeled data. Blum
and Mitchell (1998) prove the effectiveness of this
algorithm, under the assumption that features in
one set is conditionally independent of features in
the other set. Intuitively speaking, if this condi-
tional independence assumption holds, the most
confident instance of one classifier will act as a
random instance for the other classifier. Thus it
can be safely used to expand the training set of the
other classifier.
The standard Co-training algorithm requires a
naturally splitting in the feature set, which is hard
to meet in most scenarios, including the task of
word alignment. Variations include using random
split feature sets or two different classification al-
gorithms. In this paper, we use the other Co-
training style algorithm called Tri-training, which
requires neither sufficient and redundant views nor
different classification algorithms.
3.1 Tri-training
Similar with Co-training, the basic idea of Tri-
training (Zhou and Li, 2005) is to iteratively ex-
pand the labeled training set for the next-round
training based on the decisions of the current clas-
sifiers. However, Tri-training employs three clas-
sifiers instead of two. To get diverse initial classi-
fiers, the training set of each classifier is initially
generated via bootstrap sampling from the origi-
nal labeled training set and updated separately. In
each round, these three classifiers are used to clas-
sify all the unlabeled instances. An unlabeled in-
stance is added to the training set of any classifier
if the other two classifiers agree on the labeling
of this example. So there is no need to explicitly
measure the confidence of any individual classi-
fier, which might be a problem for some learning
algorithms. Zhou and Li (2005) also give a termi-
nate criterion derived from PAC analysis. As the
algorithm goes, the number of labeled instances
increases, which may bring in more bi-lexical fea-
tures and alleviate the problem of data sparseness.
3.2 Tri-training for Word Alignment
One crucial problem for word alignment is the
huge amount of unlabeled instances. Typical par-
allel corpus for word alignment contains at least
hundreds of thousands of sentence pairs, with each
sentence pair containing tens of instances. That
makes a large set of millions of instances. There-
fore, we develop a modified version of Tri-training
algorithm using sampling techniques, which can
work well with such large scale data. A sketch of
our algorithm is shown in Figure 1.
The algorithm takes original labeled instance
set L, unlabeled sentence set SU , sub-model re-
sults As for each s in SU and a sampling ratio r as
input. Fk represents the kth classifier. Variables
with superscript i represent their values during the
ith iteration.
Line 2 initializes candidate instance set AC,s of
each sentence s to be the difference set between
137
Input: L, SU , As for each s and sampling ratio r.
1: for all sentence s in SU do
2: A0C,s ? AU,s ?AI,s //initializing candidate set
3: end for
4: for all l ? {1, 2, 3} do
5: L0l ? Subsample(L, 0.33)
6: F 0l ? Train(L0l )
7: end for
8: repeat
9: for all l ? {1, 2, 3} do
10: Let m,n ? {1, 2, 3} and m ?= n ?= l; Lil = ?
11: for all sentence s in SU do
12: for all instance a in Ai?1C,s do
13: if F i?1m (a) = F i?1n (a) then
14: Ai?1C,s ? A
i?1
C,s ? {(a, F
i?1
m (a))}
15: Lil ? Lil ? {(a, F i?1m (a))}
16: end if
17: end for
18: end for
19: end for
20: for all l ? {1, 2, 3} do
21: Lil ? Subsampling(Lil, r) ? Li?1l
22: F il ? Train(Lil)
23: AiC,s ? Ai?1C,s
24: end for
25: until all AiC,s are unchanged or empty
Output: F (x)? argmaxy?{0,1}
?
l:Fl(x)=y
1
Figure 1: Modified Tri-training Algorithm
AU,s and AI,s. In line 5-6, sub-samplings are per-
formed on the original labeled set L and the ini-
tial classifier F 0l is trained using the sampling re-
sults. In each iteration, the algorithm labels each
instance in the candidate set AiC,s for each clas-
sifier with the other two classifiers trained in last
iteration. Instances are removed from the candi-
date set and added to the labeled training set (Lil)
of classifier l, if they are given the same label by
the other two classifiers (line 13-16).
A sub-sampling is performed before the labeled
training set is used for training (line 21), which
means all the instances in Lil are accepted as cor-
rect, but only part of them are added into the train-
ing set. The sampling rate is controlled by a pa-
rameter r, which we empirically set to 0.01 in all
our experiments. The classifier is then re-trained
using the augmented training set Lil (line 22). The
algorithm iterates until all instances in the candi-
date sets get labeled or the candidate sets do not
change since the last iteration (line 25). The result-
ing classifiers can be used to label new instances
via majority voting.
Our algorithm differs from Zhou and Li (2005)
in the following three aspects. First of all, com-
paring to the original bootstrap sampling initial-
ization, we use a more aggressive strategy, which
Source Usage Sent. Pairs Cand. Links
LDC Train 288111 8.8M
NIST?02 Train 200 5,849
NIST?02 Eval 291 7,797
Table 1: Data used in the experiment
actually divides the original labeled set into three
parts. This strategy ensures that initial classifiers
are trained using different sets of instances and
maximizes the diversity between classifiers. We
will compare these two initializations in the ex-
periments section. Secondly, we introduce sam-
pling techniques for the huge number of unlabeled
instances. Sampling is essential for maintain-
ing a reasonable growing speed of training data
and keeping the computation physically feasible.
Thirdly, because the original terminate criterion
requires an error estimation process in each iter-
ation, we adapt the much simpler terminate cri-
terion of standard Co-training into our algorithm,
which iterates until all the unlabeled data are fi-
nally labeled or the candidate sets do not change
since the last iteration. In other words, our algo-
rithm inherits both the benefits of using three clas-
sifiers and the simplicity of using Co-training style
termination criterion. Parallel computing tech-
niques are also used during the processing of un-
labeled data to speed up the computation.
4 Experiments and Results
4.1 Data and Evaluation Methodology
All our experiments are conducted on the lan-
guage pair of Chinese and English. For training
alignment systems, a parallel corpus coming from
LDC2005T10 and LDC2005T14 is used as un-
labeled training data. Labeled data comes from
NIST Open MT Eval?02, which has 491 labeled
sentence pairs. The first 200 labeled sentence pairs
are used as labeled training data and the rest are
used for evaluation (Table 1). The number of can-
didate alignment links in each data set is also listed
in Table 1. These candidate alignment links are
generated using the three sub-models described in
Section 4.2.
The quality of word alignment is evaluated in
terms of alignment error rate (AER) (Och and Ney,
2003), classifier?s accuracy and recall of correct
decisions. Formula 3 shows the definition of AER,
where P and S refer to the set of possible and sure
alignment links, respectively. In our experiments,
138
ModelName AER Dev AER Test Accuracy Recall F1
Model4C2E 0.4269 0.4196 0.4898 0.3114 0.3808
Model4E2C 0.3715 0.3592 0.5642 0.5368 0.5502
BerkeleyAl. 0.3075 0.2939 0.7064 0.6377 0.6703
Model4GDF 0.3328 0.3336 0.6059 0.6184 0.6121
Supervised 0.2291 0.2430 0.8124 0.7027 0.7536
Table 2: Experiments of Sub-models
ModelName AER Dev AER Test Accuracy Recall F1
Supervised 0.2291 0.2430 0.8124 0.7027 0.7536
BerkeleyAl. 0.3075 0.2939 0.7064 0.6377 0.6703
Tri-Bootstrap0 0.2301 0.2488 0.8030 0.6858 0.7398
Tri-Divide0 0.2458 0.2525 0.8002 0.6630 0.7251
Tri-Bootstrap 0.2264 0.2468 0.7934 0.7449 0.7684
Tri-Divide 0.2416 0.2494 0.7832 0.7605 0.7717
Table 3: Experiments of Semi-supervised Models
we treat all alignment links as sure links.
AER = 1? |A ? P |+ |A ? S|
|A|+ |S|
(3)
We also define a F1 score to be the harmonic mean
of classifier?s accuracy and recall of correct deci-
sions (Formula 4).
F1 =
2 ? accuracy ? recall
accuracy + recall
(4)
We also evaluate the machine translation quality
using unlabeled data (in Table 1) and these align-
ment results as aligned training data. We use
multi-references data sets from NIST Open MT
Evaluation as development and test data. The En-
glish side of the parallel corpus is trained into
a language model using SRILM (Stolcke, 2002).
Moses (Koehn et al, 2003) is used for decoding.
Translation quality is measured by BLEU4 score
ignoring the case.
4.2 Experiments of Sub-models
We use the following three sub-models: bidi-
rectional results of Giza++ (Och and Ney,
2003) Model4, namely Model4C2E and
Model4E2C, and the joint training result of
BerkeleyAligner (Liang et al, 2006) (Berke-
leyAl.). To evaluate AER, all three data sets
listed in Table 1 are combined and used for the
unsupervised training of each sub-model.
Table 2 presents the alignment quality of those
sub-models, as well as a supervised ensemble of
them, as described in Section 2.1. We use the sym-
metrized IBM Model4 results by the grow-diag-
final-and heuristic as our baseline (Model4GDF).
Scores in Table 2 show the great improvement
of supervised learning, which reduce the align-
ment error rate significantly (more than 5% AER
points from the best sub-model, i.e. Berke-
leyAligner). This result is consistent with Ayan
and Dorr (2006)?s experiments. It is quite reason-
able that supervised model achieves a much higher
classification accuracy of 0.8124 than any unsu-
pervised sub-model. Besides, it also achieves the
highest recall of correct alignment links (0.7027).
4.3 Experiments of Semi-supervised Models
We present our experiment results on semi-
supervised models in Table 3. The two strategies
of generating initial classifiers are compared. Tri-
Bootstrap is the model using the original boot-
strap sampling initialization; and Tri-Divide is
the model using the dividing initialization as de-
scribed in Section 3.2. Items with superscripts 0
indicate models before the first iteration, i.e. ini-
tial models. The scores of BerkeleyAligner and
the supervised model are also included for com-
parison.
In general, all supervised and semi-supervised
models achieve better results than the best sub-
model, which proves the effectiveness of labeled
training data. It is also reasonable that initial mod-
els are not as good as the supervised model, be-
cause they only use part of the labeled data for
training. After the iterative training, both the two
139
0 1000 2000 3000 4000 5000 60000.4
0.5
0.6
0.7
0.8
0.9
Training Instances Number
Score
s(F?1, 
Accura
cy, Rec
all)
 
 
F?1RecallAccuracy
(a)
0 0.5 1 1.5 2 2.5 3x 1050.73
0.74
0.75
0.76
0.77
0.78
0.79
Number of sentences
F?1 s
cores
 
 
Tri?DivideSupervisedTri?Bootstrap
(b)
Figure 2: (a) Experiments on the Size of Labeled Training Data in Supervised Training; (b) Experiments
on the Size of Unlabeled Data in Tri-training
Tri-training models get a significant increase in
recall. We attribute this to the use of bi-lexical
features described in Section 2.2. Analysis of
the resulting model shows that the number of
bi-lexical features increases from around 300 to
nearly 7,800 after Tri-training. It demonstrates
that semi-supervised algorithms are able to learn
more bi-lexical features automatically from the
unlabeled data, which may help recognize more
translation equivalences. However, we also notice
that the accuracy drops a little after Tri-training.
This might also be caused by the large set of bi-
lexical features, which may contain some noises.
In the comparison of initialization strategies,
the dividing strategy achieves a much higher re-
call of 0.7605, which is also the highest among
all models. It also achieves the best F1 score of
0.7717, higher than the bootstrap sampling strat-
egy (0.7684). This result confirms that diversity of
initial classifiers is important for Co-training style
algorithms.
4.4 Experiments on the Size of Data
4.4.1 Size of Labeled Data
We design this experiment to see how the size of
labeled data affects the supervised training proce-
dure. Our labeled training set contains 5,800 train-
ing instances. We randomly sample different sets
of instances from the whole set and perform the
supervised training.
The alignment results are plotted in Figure 2a.
Basically, both accuracy and recall increase with
the size of labeled data. However, we also find that
the increase of all the scores gets slower when the
number of training instances exceeds 3,000. One
possible explanation for this is that the training
set itself is too small and contains redundant in-
stances, which may prevent further improvement.
We can see in the Section 4.4.2 that the scores can
be largely improved when more data is added.
4.4.2 Size of Unlabeled Data
For better understanding the effect of unlabeled
data, we run the Tri-training algorithm on unla-
beled corpus of different sizes. The original un-
labeled corpus contains about 288 thousand sen-
tence pairs. We create 12 sub-corpus of it with
different sizes by selecting certain amounts of sen-
tences from the beginning. Our smallest sub-
corpus consists of the first 5,000 sentence pairs of
the original corpus; while the largest sub-corpus
contains the first 275 thousand sentence pairs. The
alignment results on these different sub-corpus are
evaluated (See Figure 2b).
The result shows that as the size of unlabeled
data grows, the F1 score of Tri-Divide increases
from around 0.74 to 0.772. The F1 score of Tri-
Bootstrap also gets a similar increase. This proves
that adding unlabeled data does help the learning
process. The result also suggests that when the
size of unlabeled data is small, both Tri-Bootstrap
and Tri-Divide get lower scores than the super-
vised model. This is because the Tri-training mod-
els only use part of the labeled data for the training
of each individual classifier, while the supervised
model use the whole set. We can see that when
there are more than 50 thousand unlabeled sen-
tence pairs, both Tri-training models outperform
the supervised model significantly.
140
ModelName Dev04 Test05 Test06 Test08
Model4C2E 24.54 17.10 17.52 14.59
Model4E2C 26.54 19.00 20.18 16.56
BerkeleyAl. 26.19 20.08 19.65 16.70
Model4GDF 26.75 20.67 20.58 17.05
Supervised 27.07 20.00 19.47 16.13
Tri-Bootstrap 26.88 20.49 20.76 17.31
Tri-Divide 27.04 20.96 20.79 17.18
Table 4: Experiments on machine translation (BLEU4 scores in percentage)
Note that, both experiments on data size show
some unsteadiness during the learning process.
We attribute this mainly to the random sampling
we use in the algorithm. As there are, in all, about
8.8 million instances , it is highly possible that
some of these instances are redundant or noisy.
And because our random sampling does not dis-
tinguish different instances, the quality of result-
ing model may get affected if these redundant or
noisy instances are selected and added to the train-
ing set.
4.5 Experiments on Machine Translation
We compare the machine translation results of
each sub-models, supervised models and semi-
supervised models in Table 4. Among sub-models,
BerkeleyAligner gets better BLEU4 scores in al-
most all the data sets except TEST06, which
agrees with its highest F1 score among all sub-
models. The supervised method gets the highest
BLEU score of 27.07 on the dev set. However, its
performance on the test sets is a bit lower than that
of BerkeleyAligner.
As we expect, our two semi-supervised mod-
els achieve highest scores on almost all the data
sets, which are also higher than the commonly
used grow-diag-final-and symmetrization of IBM
Model 4. More specifically, Tri-Divide is the
best of all systems. It gets a dev score of 27.04,
which is comparable with the highest one (27.07).
Tri-Divide also gets the highest BLEU scores
on Test05 and Test06 (20.96 and 20.79, respec-
tively), which are nearly 1 point higher than all
sub-models. The other Tri-training model, Tri-
Bootstrap, gets the highest score on Test08, which
is also significantly better than those sub-models.
Despite the large improvement in F1 score, our
two Tri-training models only get slightly better
score than the well-known Model4GDF. This kind
of inconsistence between AER or F1 scores and
BLEU scores is a known issue in machine trans-
lation community (Fraser and Marcu, 2007). One
possible explanation is that both AER or F1 are
0-1 loss functions, which means missing one link
and adding one redundant link will get the same
penalty. And more importantly, every wrong link
receives the same penalty under these metrics.
However, these different errors may have different
effects on the machine translation quality. Thus,
improving alignment quality according to AER or
F1 may not directly lead to an increase of BLEU
scores. The relationship among these metrics are
still under investigation.
5 Related work
Previous work mainly focuses on supervised
learning of word alignment. Liu et al (2005)
propose a log-linear model for the alignment be-
tween two sentences, in which different features
can be used to describe the alignment quality.
Moore (2005) proposes a similar framework, but
with more features and a different search method.
Other models such as SVM and CRF are also
used (Taskar et al, 2005; Cherry and Lin, 2006;
Haghighi et al, 2009). For alignment ensemble,
Wu and Wang (2005) introduce a boosting ap-
proach, in which the labeled data is used to cal-
culate the weight of each sub-model.
These researches all focus on the modeling of
alignment structure and employ some strategy to
search for the optimal alignment. Our main con-
tribution here is the use Co-training style semi-
supervised methods to assist the ensemble learn-
ing framework of Ayan and Dorr (2006). Although
we use a maximum entropy model in our experi-
ment, other models like SVM and CRF can also
be incorporated into our learning framework.
In the area of semi-supervised learning of word
alignment, Callison-Burch et al (2004) compare
the results of interpolating statistical machine
141
translation models learnt from labeled and unla-
beled data, respectively. Wu et al (2006) propose
a modified boosting algorithm, where two differ-
ent models are also trained using labeled and un-
labeled data respectively and interpolated. Fraser
and Marcu (2006) propose an EMD algorithm,
where labeled data is used for discriminative re-
ranking. It should be pointed out that these pieces
of work all use two separate processes for learn-
ing with labeled and unlabeled data. They either
train and interpolate two separate models or re-
rank previously learnt models with labeled data
only. Our proposed semi-supervised strategy is
able to incorporate both labeled and unlabeled data
in the same process, which is in a different line of
thinking.
6 Conclusions and Future Work
Semi-supervised techniques are useful when there
is a large amount of unlabeled data. In this
paper, we introduce a semi-supervised learning
method, called Tri-training, to improve the word
alignment combination task. Although experi-
ments have proved the effectiveness of our meth-
ods, there is one defect that should be mentioned.
As we previously assume that all the decisions
on alignment links are independent of each other
(in Section 2.1), our model are only able to cap-
ture link level evidence like bi-lexical features.
Some global features, such as final word fertil-
ity, cannot be integrated into the current frame-
work. In the future, we plan to apply our semi-
supervised strategy in more complicated learning
frameworks, which are able to capture those global
features.
Currently we use a random sampling to handle
the 8.8 million instances. We will also explore
better and more aggressive sampling techniques,
which may lead to more stable training results and
also enable us to process larger corpus.
Acknowledgments
The authors would like to thank Dr. Ming Li,
Mr. Junming Xu and the anonymous reviewers for
their valuable comments. This work is supported
by the National Fundamental Research Program
of China(2010CB327903) and the Scientific Re-
search Foundation of Graduate School of Nanjing
University(2008CL08).
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A max-
imum entropy approach to combining word align-
ments. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 96?103, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Com-
putational Learning Theory, pages 92?100. Morgan
Kaufmann Publishers.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matic of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In ACL
?04: Proceedings of the 42nd Annual Meeting on As-
sociation for Computational Linguistics, page 175,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 105?112,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, pages 769?776, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Comput. Linguist., 33(3):293?303.
Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better word alignments with supervised itg models.
In Association for Computational Linguistics, Sin-
gapore.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In Robert C. Moore,
Jeff A. Bilmes, Jennifer Chu-Carroll, and Mark
Sanderson, editors, HLT-NAACL. The Association
for Computational Linguistics.
142
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 459?
466, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 81?88, Morristown, NJ, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51.
A. Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, page
901 904.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In HLT ?05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
73?80, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in sta-
tistical translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 836?841.
Hua Wu and Haifeng Wang. 2005. Boosting statistical
word alignment. In Proceedings of MT SUMMIT X,
pages 364?371, Phuket Island, Thailand, September.
HuaWu, HaifengWang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and un-
labeled data. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 913?
920, Sydney, Australia, July. Association for Com-
putational Linguistics.
Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Ex-
ploiting unlabeled data using three classifiers. vol-
ume 17, pages 1529?1541, Piscataway, NJ, USA.
IEEE Educational Activities Department.
143
