Lenient Default Unification for Robust Processing
within Unification Based Grammar Formalisms
Takashi NINOMIYA,?? Yusuke MIYAO,? and Jun?ichi TSUJII??
? Department of Computer Science, University of Tokyo
? CREST, Japan Science and Technology Corporation
e-mail: {ninomi, yusuke, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes new default unification, lenient
default unification. It works efficiently, and gives
more informative results because it maximizes the
amount of information in the result, while other de-
fault unification maximizes it in the default. We also
describe robust processing within the framework of
HPSG. We extract grammar rules from the results of
robust parsing using lenient default unification. The
results of a series of experiments show that parsing
with the extracted rules works robustly, and the cov-
erage of a manually-developed HPSG grammar for
Penn Treebank was greatly increased with a little
overgeneration.
1 Introduction
Parsing has often been considered to be crucial
for natural language processing, thus, efficient and
wide coverage parsing has been extensively pur-
sued in natural language literature. This study aims
at robust processing within the Head-driven Phrase
Structure Grammar (HPSG) to extend the cover-
age of manually-developed HPSG grammars. The
meaning of ?robust processing? is not limited to ro-
bust processing for ill-formed sentences found in
a spoken language, but includes robust processing
for sentences which are well-formed but beyond the
grammar writer?s expectation.
Studies of robust parsing within unification-based
grammars have been explored by many researchers
(Douglas and Dale, 1992; Imaichi and Matsumoto,
1995). They classified the errors found in analyzing
ill-formed sentences into several categories to make
them tractable, e.g., constraint violation, missing or
extra elements, etc. In this paper, we focus on re-
covery from the constraint violation errors, which is
a violation of feature values. All errors in agreement
fall into this category. Since many of the grammat-
ical components in HPSG are written as constraints
represented by feature structures, many of the er-
rors are expected to be recovered by the recovery of
constraint violation errors.
This paper proposes two new types of default
unification and describes their application to robust
processing. Default unification was originally stud-
ied to develop a system of lexical semantics to deal
with the default inheritance in a lexicon, but it is
also desirable for the recovery of such constraint vi-
olation errors due to the following merits: i) default
unification is always well-defined, and ii) a feature
structure is relaxed such that the amount of infor-
mation is maximized. From the viewpoint of robust
processing, an amount of lost information can be re-
garded as a cost (i.e., penalty) of robust processing.
In other words, default unification tries to minimize
the cost. Given a strict feature structure F and a
default feature structure G, default unification is de-
fined as unification that satisfies the following (writ-
ten as F <unionsq G): 1) It is always defined. 2) All strict
information is preserved. That is, F v (F <unionsq G). 3) It
reduces to standard unification in the case of F and
G being consistent. That is, (F <unionsq G) = (F unionsqG) if
F unionsqG is defined. With these definitions, Douglas?
relaxation technique can be regarded as a sort of de-
fault unification. They classify constraints into nec-
essary constraints and optional constraints, which
can be regarded as strict information and default in-
formation in the definition of default unification.
Carpenter (1993) gave concise and comprehen-
sive definitions of default unification. However, the
problem in Carpenter?s default unification is that it
tries to maximize the amount of information in a de-
fault feature structure, not the result of default uni-
fication. Consider the case where a grammar rule
is the default feature structure and the daughters are
the strict feature structure. The head feature prin-
ciple can be described as the structure-sharing be-
tween the values of the head feature in a mother
and in a head daughter. The set of constraints that
represent the head feature principle consists of only
one element. When we lose just one element in the
head feature principle, a large amount of informa-
tion in the daughter?s substructure is not propagated
to its mother. As Copestake (1993) mentioned, an-
other problem in Carpenter?s default unification is
that the time complexity for finding the optimal an-
swer of default unification is exponential because
we have to verify the unifiability of the power set of
constraints in a default feature structure.
Here, we propose ideal lenient default unifica-
tion, which tries to maximize the amount of infor-
mation of a result, not the amount of default infor-
mation. Thus, the problem of losing a large amount
of information in structure-sharing never arises. We
also propose lenient default unification whose algo-
rithm is much more efficient than the ideal one. Its
time complexity is linear to the size of the strict fea-
ture structure and the default feature structure. In-
stead, the amount of information of a result derived
by lenient default unification is equal to or less than
that of the ideal one.
We apply lenient default unification to robust pro-
cessing. Given an HPSG grammar, our approach
takes two steps; i) extraction of grammar rules from
the results of robust parsing using lenient default
unification for applying the HPSG grammar rules
(offline parsing), and ii) runtime parsing using the
HPSG grammar with the extracted rules. The ex-
tracted rules work robustly since they reflect the ef-
fects of recovery rules applied during offline robust
parsing and the conditions in which they are ap-
plied.
Sections 3 and 4 describe our default unification.
Our robust parsing is explained in Section 5. Sec-
tion 6 shows a series of experiments of robust pars-
ing with default unification.
2 Background
Default unification has been investigated by many
researchers (Bouma, 1990; Russell et al, 1991;
Copestake, 1993; Carpenter, 1993; Lascarides and
Copestake, 1999) in the context of developing lexi-
cal semantics. Here, we first explain the definition
given by Carpenter (1993) because his definition is
both concise and comprehensive.
2.1 Carpenter?s Default Unification
Carpenter proposed two types of default unification,
credulous default unification and skeptical default
unification.
(Credulous Default Unification)
F <unionsqc G =
{
F unionsqG?
??? G? v G is maximal such thatF unionsqG? is defined
}
(Skeptical Default Unification)
F <unionsqs G = ?(F <unionsqc G)
F is called a strict feature structure, whose in-
formation must not be lost, and G is called a de-
fault feature structure, whose information might be
lost but as little as possible so that F and G can be
unified. A credulous default unification operation
is greedy in that it tries to maximize the amount of
information it retains from the default feature struc-
ture. This definition returns a set of feature struc-
tures rather than a unique feature structure.
Skeptical default unification simply generalizes
the set of feature structures which results from cred-
ulous default unification. The definition of skeptical
default unification leads to a unique result. The de-
fault information which can be found in every result
of credulous default unification remains. Following
is an example of skeptical default unification.
[F: a] <unionsqs
[
F: 1 b
G: 1
H: c
]
= u
{ [F: a
G: b
H: c
]
,
[
F: 1 a
G: 1
H: c
]}
=
[F: a
G: ?
H: c
]
2.2 Forced Unification
Forced unification is another way to unify incon-
sistent feature structures. Forced unification always
succeeds by supposing the existence of the top type
(the most specific type) in a type hierarchy. Unifi-
cation of any pair of types is defined in the type hi-
erarchy, and therefore unification of any pair of fea-
ture structures is defined. One example is described
by Imaichi and Matsumoto (1995) (they call it cost-
based unification). Their unification always suc-
ceeds by supposing the top type, and it also keeps
the information about inconsistent types. Forced
unification can be regarded as one of the toughest
robust processing because it always succeeds and
never loses the information embedded in feature
structures. The drawback of forced unification is
the postprocessing of parsing, i.e., feature structures
with top types are not tractable. We write Funionsq f G for
the forced unification of F and G.
3 Ideal Lenient Default Unification
In this section, we explain our default unification,
ideal lenient default unification. Ideal lenient de-
fault unification tries to maximize the amount of
information of the result, subsuming the result of
forced unification. In other words, ideal lenient de-
fault unification tries to generate a result as similar
as possible to the result of forced unification such
that the result is defined in the type hierarchy with-
out the top type. Formally, we have:
Definition 3.1 Ideal Lenient Default Unification
F <unionsqi G = ?
{
F unionsqG?
?????
G? v f (Funionsq f G) is maximal
such that F unionsqG? is defined
without the top type
}
where v f is a subsumption relation where the top
type is defined.
From the definition of skeptical default unifica-
tion, ideal lenient default unification is equivalent
to F <unionsqs (Funionsq f G) assuming that skeptical default uni-
fication does not add the default information that in-
cludes the top type to the strict information.
Consider the following feature structures.
F =
?
???
F:
[F:a
G:b
H:c
]
G:
[F:a
G:a
H:c
]
?
???,G =
[
F: 1
G: 1
]
In the case of Carpenter?s default unification, the
results of skeptical and credulous default unification
become as follows: F <unionsqs G = F,F <unionsqc G = {F}. This
is because G is generalized to the bottom feature
structure, and hence the result is equivalent to the
strict feature structure.
With ideal lenient default unification, the result
becomes as follows.
F <unionsqi G =
?
?????
F:
[
F: 1 a
G:b
H: 2 c
]
G:
[
F: 1
G:a
H: 2
]
?
?????
v f
?
?F: 1
[F:a
G:>
H:c
]
G: 1
?
?
Note that the result of ideal lenient default unifica-
tion subsumes the result of forced unification.
As we can see in the example, ideal lenient de-
fault unification tries to keep as much information
of the structure-sharing as possible (ideal lenient
default unification succeeds in preserving the struc-
ture-sharing tagged as 1 and 2 though skeptical and
credulous default unification fail to capture it).
4 Lenient Default Unification
The optimal answer for ideal lenient default unifica-
tion can be found by calculating F <unionsqs (Funionsq f G). As
Copestake (1993) mentioned, the time complexity
of skeptical default unification is exponential, and
therefore the time complexity of ideal lenient de-
fault unification is also exponential.
As other researchers pursued efficient default uni-
fication (Bouma, 1990; Russell et al, 1991; Copes-
take, 1993), we also propose another definition of
default unification, which we call lenient default
unification. An algorithm derived for it finds its an-
swer efficiently.
Given a strict feature structure F and a default
feature structure G, let H be the result of forced uni-
fication, i.e., H = Funionsq f G. We define topnode(H)
as a function that returns the fail points (the nodes
that are assigned the top type in H), f pnode(H)
as a function that returns the fail path nodes (the
nodes from which a fail point can be reached), and
f pchild(H) as a a function that returns all the nodes
that are not fail path nodes but the immediate chil-
dren of fail path nodes.
Consider the following feature structures.
F =
?
????
F:F:
[F:F:a
G:G:b
H:H:c
]
G:
[
G:
[F:F:a
G:G:a
H:H:c
]
H:H:a
]
?
????,G =
[
F:F: 1?
G:G: 1
]
Figure 1 shows F , G and H in the graph notation.
This figure also shows the nodes that correspond to
topnode(H), f pnode(H) and f pchild(H).
 


 

 




F =

 
G =
	





H =












	




	


)(Htopnode? )(Hfpnode?
Deep Linguistic Analysis for the Accurate Identification of
Predicate-Argument Relations
Yusuke Miyao
Department of Computer Science
University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science
University of Tokyo
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper evaluates the accuracy of HPSG
parsing in terms of the identification of
predicate-argument relations. We could directly
compare the output of HPSG parsing with Prop-
Bank annotations, by assuming a unique map-
ping from HPSG semantic representation into
PropBank annotation. Even though PropBank
was not used for the training of a disambigua-
tion model, an HPSG parser achieved the ac-
curacy competitive with existing studies on the
task of identifying PropBank annotations.
1 Introduction
Recently, deep linguistic analysis has successfully
been applied to real-world texts. Several parsers
have been implemented in various grammar for-
malisms and empirical evaluation has been re-
ported: LFG (Riezler et al, 2002; Cahill et al,
2002; Burke et al, 2004), LTAG (Chiang, 2000),
CCG (Hockenmaier and Steedman, 2002b; Clark et
al., 2002; Hockenmaier, 2003), and HPSG (Miyao
et al, 2003; Malouf and van Noord, 2004). How-
ever, their accuracy was still below the state-of-the-
art PCFG parsers (Collins, 1999; Charniak, 2000) in
terms of the PARSEVAL score. Since deep parsers
can output deeper representation of the structure of
a sentence, such as predicate argument structures,
several studies reported the accuracy of predicate-
argument relations using a treebank developed for
each formalism. However, resources used for the
evaluation were not available for other formalisms,
and the results cannot be compared with each other.
In this paper, we employ PropBank (Kingsbury
and Palmer, 2002) for the evaluation of the accu-
racy of HPSG parsing. In the PropBank, semantic
arguments of a predicate and their semantic roles
are manually annotated. Since the PropBank has
been developed independently of any grammar for-
malisms, the results are comparable with other pub-
lished results using the same test data.
Interestingly, several studies suggested that the
identification of PropBank annotations would re-
quire linguistically-motivated features that can be
obtained by deep linguistic analysis (Gildea and
Hockenmaier, 2003; Chen and Rambow, 2003).
They employed a CCG (Steedman, 2000) or LTAG
(Schabes et al, 1988) parser to acquire syntac-
tic/semantic structures, which would be passed to
statistical classifier as features. That is, they used
deep analysis as a preprocessor to obtain useful fea-
tures for training a probabilistic model or statistical
classifier of a semantic argument identifier. These
results imply the superiority of deep linguistic anal-
ysis for this task.
Although the statistical approach seems a reason-
able way for developing an accurate identifier of
PropBank annotations, this study aims at establish-
ing a method of directly comparing the outputs of
HPSG parsing with the PropBank annotation in or-
der to explicitly demonstrate the availability of deep
parsers. That is, we do not apply statistical model
nor machine learning to the post-processing of the
output of HPSG parsing. By eliminating the effect
of post-processing, we can directly evaluate the ac-
curacy of deep linguistic analysis.
Section 2 introduces recent advances in deep lin-
guistic analysis and the development of semanti-
cally annotated corpora. Section 3 describes the de-
tails of the implementation of an HPSG parser eval-
uated in this study. Section 4 discusses a problem in
adopting PropBank for the performance evaluation
of deep linguistic parsers and proposes its solution.
Section 5 reports empirical evaluation of the accu-
racy of the HPSG parser.
2 Deep linguistic analysis and
semantically annotated corpora
Riezler et al (2002) reported the successful applica-
tion of a hand-crafted LFG (Bresnan, 1982) gram-
mar to the parsing of the Penn Treebank (Marcus
et al, 1994) by exploiting various techniques for
robust parsing. The study was impressive because
most researchers had believed that deep linguistic
analysis of real-world text was impossible. Their
success owed much to a consistent effort to main-
tain a wide-coverage LFG grammar, as well as var-
SVP
have
to
choose
this particular moment
S
NP VP
VP
NP
they
NP-1
did n?t
*-1
VP
VP
ARG0-choose
ARG1-chooseARG0-choose
REL-choose
Figure 1: Annotation of the PropBank
ious techniques for robust parsing.
However, the manual development of wide-
coverage linguistic grammars is still a difficult task.
Recent progress in deep linguistic analysis has
mainly depended on the acquisition of lexicalized
grammars from annotated corpora (Xia, 1999; Chen
and Vijay-Shanker, 2000; Chiang, 2000; Hocken-
maier and Steedman, 2002a; Cahill et al, 2002;
Frank et al, 2003; Miyao et al, 2004). This ap-
proach not only allows for the low-cost develop-
ment of wide-coverage grammars, but also provides
the training data for statistical modeling as a by-
product. Thus, we now have a basis for integrating
statistical language modeling with deep linguistic
analysis. To date, accurate parsers have been devel-
oped for LTAG (Chiang, 2000), CCG (Hockenmaier
and Steedman, 2002b; Clark et al, 2002; Hocken-
maier, 2003), and LFG (Cahill et al, 2002; Burke et
al., 2004). Those studies have opened up the appli-
cation of deep linguistic analysis to practical use.
However, the accuracy of those parsers was still
below PCFG parsers (Collins, 1999; Charniak,
2000) in terms of the PARSEVAL score, i.e., labeled
bracketing accuracy of CFG-style parse trees. Since
one advantage of deep parsers is that they can out-
put a sort of semantic representation, e.g. predicate-
argument structures, several studies have reported
the accuracy of predicate-argument relations (Hock-
enmaier and Steedman, 2002b; Clark et al, 2002;
Hockenmaier, 2003; Miyao et al, 2003). However,
their evaluation employed a treebank developed for
a specific grammar formalism. Hence, those results
cannot be compared fairly with parsers based on
other formalisms including PCFG parsers.
At the same time, following the great success
of machine learning approaches in NLP, many re-
search efforts are being devoted to developing vari-
ous annotated corpora. Notably, several projects are
underway to annotate large corpora with semantic
information such as semantic relations of words and
coreferences.
PropBank (Kingsbury and Palmer, 2002) and
FrameNet (Baker et al, 1998) are large English cor-
pora annotated with the semantic relations of words
in a sentence. Figure 1 shows an example of the
annotation of the PropBank. As the target text of
the PropBank is the same as the Penn Treebank, a
syntactic structure is given by the Penn Treebank.
The PropBank includes additional annotations rep-
resenting a predicate and its semantic arguments in
a syntactic tree. For example, in Figure 1, REL de-
notes a predicate, ?choose?, and ARG   represents
its semantic arguments: ?they? for the 0th argument
(i.e., subject) and ?this particular moment? for the
1st argument (i.e., object).
Existing studies applied statistical classifiers to
the identification of the PropBank or FrameNet an-
notations. Similar to many methods of applying ma-
chine learning to NLP tasks, they first formulated
the task as identifying in a sentence each argument
of a given predicate. Then, parameters of the iden-
tifier were learned from the annotated corpus. Fea-
tures of a statistical model were defined as a pat-
tern on a partial structure of the syntactic tree output
by an automatic parser (Gildea and Palmer, 2002;
Gildea and Jurafsky, 2002).
Several studies proposed the use of deep linguis-
tic features, such as predicate-argument relations
output by a CCG parser (Gildea and Hockenmaier,
2003) and derivation trees output by an LTAG parser
(Chen and Rambow, 2003). Both studies reported
that the identification accuracy improved by in-
troducing such deep linguistic features. Although
deep analysis has not outperformed PCFG parsers in
terms of the accuracy of surface structure, these re-
sults are implicitly supporting the necessity of deep
linguistic analysis for the recognition of semantic
relations.
However, these results do not directly reflect the
performance of deep parsers. Since these corpora
provide deeper structure of a sentence than surface
parse trees, they would be suitable for the evalua-
tion of deep parsers. In Section 4, we explore the
possibility of using the PropBank for the evaluation
of an HPSG parser.
3 Implementation of an HPSG parser
This study evaluates the accuracy of a general-
purpose HPSG parser that outputs predicate argu-
ment structures. While details have been explained
in other papers (Miyao et al, 2003; Miyao et al,
2004), in the remainder of this section, we briefly
review the grammar and the disambiguation model
of our HPSG parser.
SVP
have
to
choose
this particular moment
S
NP VP
VP
NP
they
NP-1
did n?t
*-1
VP
VParg
head
head
head head
head
head
head
arg
arg
arg
arg
mod
 
have
to
choose
this particular moment
they
did n?t
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  noun
SUBJ  < >
COMPS  < >
HEAD  verb
SUBJ  <    >2
HEAD  verb
SUBJ  < _ >
HEAD  verb
SUBJ  <    >2
HEAD  verb
SUBJ  <    >1
HEAD  verb
SUBJ  <    >1
HEAD  noun
SUBJ  < >
COMPS  < >
head-comp
head-comp
head-comp
head-comp
subject-head
1
 
have to
they
did n?t
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  noun
SUBJ  < >
COMPS  < >
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS <    >
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  <    >
COMPS  < >
1
1
2
2
HEAD  verb
SUBJ  < >
COMPS <    >
1
3
HEAD  verb
SUBJ  <    >
COMPS  < >
13
1
choose this particular moment
HEAD  noun
SUBJ  < >
COMPS  < >
4
HEAD  verb
SUBJ  < >
COMPS <    >
1
4
Figure 2: Extracting HPSG lexical entries from the
Penn Treebank-style parse tree
3.1 Grammar
The grammar used in this paper follows the theory
of HPSG (Pollard and Sag, 1994), and is extracted
from the Penn Treebank (Miyao et al, 2004). In
this approach, a treebank is annotated with partially
specified HPSG derivations using heuristic rules.
By inversely applying schemata to the derivations,
partially specified constraints are percolated and in-
tegrated into lexical entries, and a large HPSG-style
lexicon is extracted from the treebank.
Figure 2 shows an example of extracting HPSG
lexical entries from a Penn Treebank-style parse
tree. Firstly, given a parse tree (the top of the fig-
ure), we annotate partial specifications on an HPSG
derivation (the middle). Then, HPSG schemata are
applied to each branching in the derivation. Finally,
COMPS <                         >
SUBJ <                         >
PHON  ?choose?
HEAD  verb
REL  choose
ARG0
ARG1
HEAD  noun
SEM 1
HEAD  noun
SEM 2
SEM 1
2
Figure 3: Mapping from syntactic arguments to se-
mantic arguments
we get lexical entries for all of the words in the tree
(the bottom).
As shown in the figure, we can also obtain com-
plete HPSG derivation trees, i.e., an HPSG tree-
bank. It is available for the machine learning of dis-
ambiguation models, and can also be used for the
evaluation of HPSG parsing.
In an HPSG grammar, syntax-to-semantics map-
pings are implemented in lexical entries. For exam-
ple, when we have a lexical entries for ?choose?
as shown in Figure 3, the lexical entry includes
mappings from syntactic arguments (SUBJ and
COMPS features) into a predicate-argument struc-
ture (ARG0 and ARG1 features). Argument labels
in a predicate-argument structure are basically de-
fined in a left-to-right order of syntactic realizations,
while if we had a cue for a movement in the Penn
Treebank, arguments are put in its canonical posi-
tion in a predicate-argument structure.
3.2 Disambiguation model
By grammar extraction, we are able to obtain a large
lexicon together with complete derivation trees of
HPSG, i.e, an HPSG treebank. The HPSG treebank
can then be used as training data for the machine
learning of the disambiguation model.
Following recent research about disambiguation
models on linguistic grammars (Abney, 1997; John-
son et al, 1999; Riezler et al, 2002; Clark and Cur-
ran, 2003; Miyao et al, 2003; Malouf and van No-
ord, 2004), we apply a log-linear model or maxi-
mum entropy model (Berger et al, 1996) on HPSG
derivations. We represent an HPSG sign as a tu-
ple 	
 , where  is a lexical sign of the
head word, 
 is a part-of-speech, and  is a sym-
bol representing the structure of the sign (mostly
corresponding to nonterminal symbols of the Penn
Treebank). Given an HPSG schema  and the dis-
tance  between the head words of the head/non-
head daughter constituents, each (binary) branch-
ing of an HPSG derivation is represented as a tuple

 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 43?46
Manchester, August 2008
Word Sense Disambiguation for All Words using Tree-Structured
Conditional Random Fields
Jun Hatori
?
Yusuke Miyao
?
Jun?ichi Tsujii
???
?
Graduate School of Interdisciplinary Information Studies, University of Tokyo
?
Graduate School of Information Science and Technology, University of Tokyo
?
National Centre for Text Mining / 131 Princess Street, Manchester, M1 7DN, UK
?
School of Computer Science, University of Manchester
{hatori,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We propose a supervised word sense
disambiguation (WSD) method using
tree-structured conditional random fields
(TCRFs). By applying TCRFs to a
sentence described as a dependency tree
structure, we conduct WSD as a labeling
problem on tree structures. To incorpo-
rate dependencies between word senses,
we introduce a set of features on tree
edges, in combination with coarse-grained
tagsets, and show that these contribute
to an improvement in WSD accuracy.
We also show that the tree-structured
model outperforms the linear-chain model.
Experiments on the SENSEVAL-3 data
set show that our TCRF model performs
comparably with state-of-the-art WSD
systems.
1 Introduction
Word sense disambiguation (WSD) is one of the
fundamental underlying problems in computa-
tional linguistics. The task of WSD is to determine
the appropriate sense for each polysemous word
within a given text.
Traditionally, there are two task settings for
WSD: the lexical sample task, in which only one
targeted word is disambiguated given its context,
and the all-words task, in which all content words
within a text are disambiguated. Whilst most of
the WSD research so far has been toward the lex-
ical sample task, the all-words task has received
c
? Jun Hatori, Yusuke Miyao, and Jun?ichi Tsu-
jii, 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
relatively less attention, suffering from a serious
knowledge bottleneck problem. Since it is con-
sidered to be a necessary step toward practical ap-
plications, there is an urgent need to improve the
performance of WSD systems that can handle the
all-words task.
In this paper, we propose a novel approach for
the all-words task based on tree-structured condi-
tional random fields (TCRFs). Our TCRF model
incorporates the inter-word sense dependencies, in
combination with WORDNET hierarchical infor-
mation and a coarse-grained tagset, namely super-
senses, by which we can alleviate the data sparse-
ness problem.
2 Background
2.1 Inter-word sense dependencies
Since the all-words task requires us to disam-
biguate all content words, it seems reasonable to
assume that we could perform better WSD by con-
sidering the sense dependencies among words, and
optimizing word senses over the whole sentence.
Specifically, we base our model on the assumption
that there are strong sense dependencies between a
head word and its dependents in a dependency tree;
therefore, we employ the dependency tree struc-
tures for modeling the sense dependencies.
There have been a few WSD systems that incor-
porate the inter-word sense dependencies (e.g. Mi-
halcea and Faruque (2004)). However, to the ex-
tent of our knowledge, their effectiveness has not
explicitly examined thus far for supervised WSD.
2.2 WORDNET information
Supersense A supersense corresponds to the
lexicographers? file ID in WORDNET, with which
each noun or verb synset is associated. Since
43
they are originally introduced for ease of lexicog-
raphers? work, their classification is fairly gen-
eral, but not too abstract, and is hence expected
to act as good coarse-grained semantic categories.
The numbers of the supersenses are 26 and 15
for nouns and verbs. The effectiveness of the
use of supersenses and other coarse-grained tagsets
for WSD has been recently shown by several re-
searchers (e.g. Kohomban and Lee (2005), Cia-
ramita and Altun (2006), and Mihalcea et al
(2007)).
Sense number A sense number is the number of
a sense of a word in WORDNET. Since senses of a
word are ordered according to frequency, the sense
number can act as a powerful feature for WSD,
which offers a preference for frequent senses, and
especially as a back-off feature, which enables our
model to output the first sense when no other fea-
ture is available for that word.
2.3 Tree-structured CRFs
Conditional Random Fields (CRFs) are graph-
based probabilistic discriminative models pro-
posed by Lafferty et al (2001).
Tree-structured CRFs (TCRFs) are different
from widely used linear-chain CRFs, in that the
probabilistic variables are organized in a tree struc-
ture rather than in a linear sequence. Therefore, we
can consider them more appropriate for modeling
the semantics of sentences, which cannot be repre-
sented by linear structures.
Although TCRFs have not yet been applied to
WSD, they have already been applied to some NLP
tasks, such as semantic annotation (Tang et al,
2006), proving to be useful in modeling the seman-
tic structure of a text.
Formulation In CRFs, the conditional probabil-
ity of a label set y for an observation sequence x
is calculated by
p(y|x) =
1
Z(x)
exp
[
?
e?E,j
?
j
f
j
(e,x,y)
+
?
v?V,k
?
k
g
k
(v,x,y)
]
(1)
where E and V are the sets of edges and vertices,
f
j
and g
k
are the feature vectors for an edge and a
vertex, ?
j
and ?
k
are the weight vectors for them,
and Z(x) is the normalization function. For a de-
tailed description of TCRFs, see Tang et al (2006).
ROOT
destroy
confidenceman
the in
bank
ROOT
destroy
confidenceman
bank
<NMOD>
<ROOT> <ROOT>
<NMOD>
<NMOD> : in<PMOD>
<SBJ> <OBJ>
<SBJ> <OBJ>
Figure 1: An example sentence described as a de-
pendency tree structure.
3 WSD Model using Tree-structured
CRFs
3.1 Overview
Let us consider the following sentence.
(i) The man destroys confidence in banks.
In the beginning, we parse a given sentence by
using a dependency parser. The left-hand side of
Figure 1 shows the dependency tree for Sentence
(i) in the CoNLL-X dependency format.
Next, we convert the outputted tree into a tree of
content words, as illustrated in the right-hand side
of Figure 1, since our WSD task does not focus on
the disambiguation of function words.
Finally, we conduct WSD as a labeling task on
tree structures, by maximizing the probability of
a tree of word senses, given scores for vertex and
edge features.
3.2 Sense Labels
Using the information in WORDNET, we define
four sense labels for a word: a sense s
1
(v), a synset
s
2
(v), a topmost synset s
3
(v), and a supersense
s
4
(v). A topmost synset s
3
(v) is the superordi-
nate synset at the topmost level in the WORDNET
hierarchy, and note that a supersense s
4
(v) is only
available for nouns and verbs. We incorporate all
these labels together into the vertex and edge fea-
tures described in the following sections.
3.3 Vertex features
Most of the vertex features we use are those used
by Lee and Ng (2002). All these features are com-
bined with each of the four sense labels s
n
(v), and
incorporated as g
k
in Equation (1).
? Word form, lemma, and part of speech.
? Word forms, lemmas, and parts of speech of
the head and dependents in a dependency tree.
44
#sentences #words
Development 470 5,178
Brown-1 10,712 100,804
Brown-2 8,956 85,481
SENSEVAL-3 300 2,081
Table 1: Statistics of the corpora.
? Bag-of-words within 60-words window.
? Parts-of-speech of neighboring six words.
? Local n-gram within neighboring six words.
Additionally, we include as a vertex feature the
sense number, introduced in Section 2.2.
3.4 Edge features
For each edge, all possible sense bigrams
(i.e. s
1
(v)-s
1
(v
?
),s
1
(v)-s
2
(v
?
),? ? ? ,s
4
(v)-s
4
(v
?
)),
and the combination of sense bigrams with de-
pendency relation labels (e.g. ?SUB,? ?NMOD?)
and/or removed function words in between (e.g.
?of,? ?in?) are defined as edge features, which cor-
respond to f
j
in Equation (1).
4 Experiment
4.1 Experimental settings
In the experiment, we use as our main evalua-
tion data set the Brown-1 and Brown-2 sections of
SEMCOR. The last files in the five largest cate-
gories in Brown-1 are used for development, and
the rest of Brown-1 and all files in Brown-2 are al-
ternately used for training and testing. We also use
the SENSEVAL-3 English all-words data (Snyder
and Palmer, 2004) for testing, in order to compare
the performance of our model with other systems.
The statistics of the data sets are shown in Table 1.
All sentences are parsed by the Sagae?s depen-
dency parser (Sagae and Tsujii, 2007), and the
TCRF model is trained using Amis (Miyao and
Tsujii, 2002). During the development phase, we
tune the parameter of L
2
regularization for CRFs.
Note that, in all experiments, we try all content
words annotated with WORDNET synsets; there-
fore, the recalls are always equal to the precisions.
4.2 Results
First, we trained and evaluated our models on
SEMCOR. Table 2 shows the overall performance
of our models. BASELINE model is the first sense
baseline. NO-EDGE model uses only the ver-
tex features, while each of the Sn-EDGE models
makes use of the edge features associated with
System Recall
PNNL (Tratz et al, 2007) 67.0%
Simil-Prime (Kohomban and Lee, 2005) 66.1%
ALL-EDGE 65.5%
GAMBL (Decadt et al, 2004) 65.2%
SENSELEARNER (Mihalcea et al,2004) 64.6%
BASELINE 62.2%
Table 3: The comparison of the performance of
WSD systems evaluated on the SENSEVAL-3 En-
glish all-words test set.
a sense label s
n
, where n ? {1, 2, 3, 4}. The
ALL-EDGE model incorporates all possible com-
binations of sense labels. The only difference
in the ALL-EDGE? model is that it omits fea-
tures associated with dependency relation labels,
so that we can compare the performance with the
ALL-EDGE?(Linear) model, which is based on the
linear-chain model.
In the experiment, all models with one or more
edge features outperformed both the NO-EDGE
and BASELINE model. The ALL-EDGE model
achieved 75.78% and 77.49% recalls for the two
data sets, with 0.41% and 0.43% improvements
over the NO-EDGE model. By the stratified shuf-
fling test (Cohen, 1995), these differences are
shown to be statistically significant
1
, with the
exception of S3-EDGE model. Also, the tree-
structured model ALL-EDGE? is shown to outper-
form the linear-chain model ALL-EDGE?(Linear)
by 0.13% for both data sets (p = 0.013, 0.006).
Finally, we trained our models on the Brown-1
and Brown-2 sections, and evaluated them on the
SENSEVAL-3 English all-words task data. Table 3
shows the comparison of our model with the state-
of-the-art WSD systems. Considering the differ-
ence in the amount of training data, we can con-
clude that the performance of our TCRF model
is comparable to state-of-the-art WSD systems,
for all systems in Table 3 other than Simil-Prime
(Kohomban and Lee, 2005)
2
utilizes other sense-
annotated data, such as the SENSEVAL data sets
and example sentences in WORDNET.
1
Although some of the improvements seem marginal, they
are still statistically significant. This is probably because
sense bigram features are rarely active, given the size of the
training corpus, and most of the system outputs are the first
senses. Indeed, 91.3% of the outputs of ALL-EDGE model
are the first senses, for example.
2
Kohomban and Lee (2005) used almost the same train-
ing data as our system, but they utilize the instance weighting
technique and the combination of several classifiers, which
our system does not.
45
Training set Brown-1 Brown-2
Testing set Brown-2 Brown-1
Model Recall Offset #correct Recall Offset #correct
ALL-EDGE? 75.77% 0.40%  64766/85481 77.45% 0.39%  78077/100804
ALL-EDGE? (Linear) 75.64% 0.27%  64662/85481 77.32% 0.26%  77944/100804
ALL-EDGE 75.78% 0.41%  64779/85481 77.49% 0.43%  78114/100804
S4-EDGE 75.46% 0.09%  64507/85481 77.15% 0.09%  77769/100804
S3-EDGE 75.40% 0.03% ? 64452/85481 77.13% 0.07%  77750/100804
S2-EDGE 75.45% 0.08%  64494/85481 77.12% 0.06%  77738/100804
S1-EDGE 75.44% 0.07%  64491/85481 77.10% 0.04% > 77724/100804
NO-EDGE 75.37% 0.00% 64427/85481 77.06% 0.00% 77677/100804
BASELINE 74.36% 63567/85481 75.91% 76524/100804
Table 2: The performance of our system trained and evaluated on SEMCOR. The statistical significance
of the improvement over NO-EDGE model is shown in the ?Offset? fields, where ?,? ?>,? and ??? denote
p < 0.01, p < 0.05, and p ? 0.05, respectively.
5 Conclusion
In this paper, we proposed a novel approach for the
all-words WSD based on TCRFs. Our proposals
are twofold: one is to apply tree-structured CRFs
to dependency trees, and the other is to use bigrams
of fine- and coarse-grained senses as edge features.
In our experiment, the sense dependency fea-
tures are shown to improve the WSD accuracy.
Since the combination with coarse-grained tagsets
are also proved to be effective, they can be used to
alleviate the data sparseness problem. Moreover,
we explicitly proved that the tree-structured model
outperforms the linear-chain model, indicating that
dependency trees are more appropriate for repre-
senting semantic dependencies.
Although our model is based on a simple frame-
work, its performance is comparable to state-of-
the-art WSD systems. Since we can use addition-
ally other sense-annotated resources and sophisti-
cated machine learning techniques, our model still
has a great potential for improvement.
References
Ciaramita, M. and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In Proc. of the
Conf. on Empirical Methods in Natural Language
Processing (EMNLP).
Cohen, P. R. 1995. Empirical methods for artificial
intelligence. MIT Press.
Decadt, B., V. Hoste, W. Daelemans, and A. V. den
Bosch. 2004. GAMBL, genetic algorithm optimiza-
tion of memory-based WSD. In Senseval-3: Third
Int?l Workshop on the Evaluation of Systems for the
Semantic Analysis of Text.
Kohomban, U. S. and W. S. Lee. 2005. Learning se-
mantic classes for word sense disambiguation. In
Proc. of the 43rd Annual Meeting on Association for
Computational Linguistics (ACL).
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of 18th
Int?l Conf. on Machine Learning (ICML).
Lee, Y. K. and H. T. Ng. 2002. An empirical evalu-
ation of knowledge sources and learning algorithms
for word sense disambiguation. In Proc. of the Conf.
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Mihalcea, R. and E. Faruque. 2004. SenseLearner:
Minimally supervised word sense disambiguation
for all words in open text. In Proc. of ACL/SIGLEX
Senseval-3, Barcelona, Spain, July.
Mihalcea, R., A. Csomai, and M. Ciaramita. 2007.
UNT-Yahoo: SuperSenseLearner: Combining
SenseLearner with SuperSense and other coarse se-
mantic features. In Proc. of the 4th Int?l Workshop
on the Semantic Evaluations (SemEval-2007).
Miyao, Y. and J. Tsujii. 2002. Maximum entropy esti-
mation for feature forests. In Proc. of Human Lan-
guage Technology Conf. (HLT 2002).
Sagae, K. and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser en-
sembles. In Proc. of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007.
Snyder, B. and M. Palmer. 2004. The english all-words
task. In Senseval-3: Third Int?l Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text.
Tang, J., M. Hong, J. Li, and B. Liang. 2006. Tree-
structured conditional random fields for semantic an-
notation. In Proc. of the 5th Int?l Semantic Web Conf.
Tratz, S., A. Sanfilippo, M. Gregory, A. Chappell,
C. Posse, and P. Whitney. 2007. PNNL: A super-
vised maximum entropy approach to word sense dis-
ambiguation. In Proc. of the 4th Int?l Workshop on
Semantic Evaluations (SemEval-2007).
46
Coling 2008: Companion volume ? Posters and Demonstrations, pages 63?66
Manchester, August 2008
Exact Inference for Multi-label Classification using Sparse Graphical
Models
Yusuke Miyao? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Center for Text Mining, UK
{yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes a parameter estima-
tion method for multi-label classification
that does not rely on approximate infer-
ence. It is known that multi-label clas-
sification involving label correlation fea-
tures is intractable, because the graphi-
cal model for this problem is a complete
graph. Our solution is to exploit the spar-
sity of features, and express a model struc-
ture for each object by using a sparse
graph. We can thereby apply the junc-
tion tree algorithm, allowing for efficient
exact inference on sparse graphs. Exper-
iments on three data sets for text catego-
rization demonstrated that our method in-
creases the accuracy for text categorization
with a reasonable cost.
1 Introduction
This paper describes an exact inference method
for multi-label classification (Schapire and Singer,
2000; Ghamrawi and McCallum, 2005), into
which label correlation features are incorporated.
In general, directly solving this problem is compu-
tationally intractable, because the graphical model
for this problem is a complete graph. Neverthe-
less, an important characteristic of this problem,
in particular for text categorization, is that only a
limited number of features are active; i.e., non-
zero, for a given object x. This sparsity of fea-
tures is a desirable characteristic, because we can
remove the edges of the graphical model when no
corresponding features are active. We can there-
fore expect that a graphical model for each object
is a sparse graph. When a graph is sparse, we
can apply the junction tree algorithm (Cowell et
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
al., 1999), allowing for efficient exact inference on
sparse graphs.
Our method is evaluated on three data sets for
text categorization; one is from clinical texts, and
the others are from newswire articles. We ob-
serve the trade-off between accuracy and training
cost, while changing the number of label correla-
tion features to be included.
2 Multi-label Classification
Given a set of labels, L = {l
1
, . . . , l
|L|
}, multi-
label classification is the task of assigning a sub-
set y ? L to a document x. In the framework of
statistical machine learning, this problem can be
formulated as a problem of maximizing a scoring
function ?:
y? = argmax
y
?(x, y) = argmax
y
?(f(x, y)). (1)
As is usually the case in statistical machine
learning, we represent a probabilistic event,
?x, y?, with a feature vector, f(x, y) =
?f
1
(x, y), . . . , f
|f |
(x, y)?. In text categorization,
most effective features represent a frequency of a
word w in a document; i.e.,
f
l,w
(x, y) =
{
c
x
(w) if l ? y,
0 otherwise,
where c
x
(w) is a frequency of w in x.
The most popular method for multi-label classi-
fication is to create |L| binary classifiers, each of
which determines whether or not to assign a single
label (Yang and Pedersen, 1997). However, since
the decision for each label is independent of the de-
cision for other labels, this method cannot be sen-
sitive to label correlations, or the tendency of label
cooccurrences.
A recent research effort has been devoted to
the modeling of label correlations. While a num-
ber of approaches have been proposed for deal-
ing with label correlations (see Tsoumakas and
63
Katakis (2007) for the comprehensive survey), the
intuitively-appealing method is to incorporate fea-
tures on two labels into the model (Ghamrawi and
McCallum, 2005). The following label correlation
feature indicates a cooccurrence of two labels and
a word:
f
l,l
?
,w
(x, y) =
{
c
x
(w) if l, l? ? y,
0 otherwise.
3 A Method for Exact Inference
A critical difficulty encountered in the model with
label correlation features is the computational cost
for training and decoding. When features on every
pair of labels are included in the model, its graph-
ical model becomes a complete graph, which in-
dicates that the exact inference for this model is
NP-hard. However, not all edges are necessary
in actual inference, because of the sparsity of fea-
tures. That is, we can remove edges between l and
l
? when no corresponding features are active; i.e.,
f
l,l
?
,w
(x, y) = 0 for all w. In text categorization,
when feature selection is performed, many edges
can be removed because of this characteristic.
Therefore, our idea is to enjoy this sparsity of
features. We construct a graphical model for each
document, and put edges only when one or more
features are active on the corresponding label pair.
When a graph is sparse, we can apply a method
for exact inference, such as the junction tree al-
gorithm (Cowell et al, 1999). The junction tree
algorithm is a generic algorithm for exact infer-
ence on any graphical model, and it allows for ef-
ficient inference on sparse graphs. The method
converts a graph into a junction tree, which is a
tree of cliques in the original graph. When we
have a junction tree for each document, we can
efficiently perform belief propagation in order to
compute argmax in Equation (1), or the marginal
probabilities of cliques and labels, necessary for
the parameter estimation of machine learning clas-
sifiers, including perceptrons (Collins, 2002), and
maximum entropy models (Berger et al, 1996).
The computational complexity of the inference on
junction trees is proportional to the exponential of
the tree width, which is the maximum number of
labels in a clique, minus one.
An essential idea of this method is that a graph-
ical model is constructed for each document. Even
when features are defined on all pairs of labels,
active features for a specific document are lim-
ited. When combined with feature selection, this
# train # test # labels card.
cmc2007 978 976 45 1.23
reuters10 6,490 2,545 10 1.10
reuters90 7,770 3,019 90 1.24
Table 1: Statistics of evaluation data sets
? ? c
cmc2007 1,000 10 0
reuters10 5,000 20 5
reuters90 5,000 80 5
Table 2: Parameters for evaluation data sets
method greatly increases the sparsity of the result-
ing graphs, which is key to efficiency.
A weakness of this method comes from the as-
sumption of feature sparseness. We are forced to
apply feature selection, which is considered effec-
tive in text categorization, but not necessarily for
other tasks. The design of features is also restricted
in order to ensure the sparsity of features.
4 Experiments
4.1 Experimental Settings
We evaluate our method for multi-label classifica-
tion using three data sets for text categorization.
Table 1 shows the statistics of these data. In this
table, ?card.? denotes the average number of la-
bels assigned to a document.
cmc2007 is a data set used in the Computa-
tional Medicine Center (CMC) Challenge 2007
(Pestian et al, 2007)1. This challenge aimed at
the assignment of ICD-9-CM codes, such as cough
and pneumonia, to clinical free texts. It should be
noted that this data is controlled, so that both train-
ing and test sets include the exact same label com-
binations, and the number of combinations is 90.
This indicates that this task can be solved as a clas-
sification of 90 classes. However, since this is an
unrealistic situation for actual applications, we do
not rely on this characteristic in this work.
reuters10 and reuters90 are taken from
the Reuters-21578 collection,2 which is a popu-
lar benchmark for text categorization. This text
collection consists of newswire articles, and each
document is assigned topic categories, such as
grain and ship. We split the data into training and
test sets, according to the so-called ModApte split.
1Available at http://www.computationalmedicine.org
2Available at http://www.daviddlewis.com/resources/
testcollections/reuters21578/
64
cmc2007
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 82.79 69.88 83.09 69.06
100 83.49 70.70 83.68 70.39
200 82.95 69.67 83.67 70.18
400 83.03 69.98 83.49 70.49
800 83.51 71.41 83.58 70.70
1600 83.10 70.49 83.56 71.00
3200 80.74 66.70 82.02 69.57
reuters10
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 94.23 89.71 93.71 88.76
500 94.22 89.98 93.80 89.19
1000 94.43 90.37 94.07 89.55
2000 94.46 90.61 94.04 89.94
4000 94.12 90.26 94.12 89.98
8000 94.14 90.61 94.50 90.81
16000 93.92 90.29 94.30 90.88
reuters90
BPM ME
? micro-F1 sub. acc. micro-F1 sub. acc.
0 84.07 77.91 86.83 79.50
500 84.96 78.27 86.89 79.66
1000 85.38 78.70 86.94 79.99
2000 85.73 79.79 86.55 79.93
4000 85.72 79.73 86.54 80.23
8000 85.90 80.19 86.77 80.39
16000 86.17 80.52 ? ?
Table 3: Accuracy for cmc2007, reuters10,
and reuters90
From this data, we create two data sets. The first
set, reuters10, is a subset of the ModApte split,
to which the 10 largest categories are assigned.
The other, reuters90, consists of documents
that are labeled by 90 categories, having at least
one document in each of the training and test sets.
In the following experiments, we run two ma-
chine learning classifiers: Bayes Point Machines
(BPM) (Herbrich et al, 2001), and the maximum
entropy model (ME) (Berger et al, 1996). For
BPM, we run 100 averaged perceptrons (Collins,
2002) with 10 iterations for each. For ME, the
orthant-wise quasi-Newton method (Andrew and
Gao, 2007) is applied, with the hyper parameter
for l
1
regularization fixed to 1.0.
We use word unigram features that represent the
frequency of a particular word in a target docu-
ment. We also use features that indicate the non-
existence of a word, which we found effective in
preliminary experiments; feature f
l,w?
(x, y) is 1 if
l ? y and w is not included in the document x.
Words are stemmed and number expressions are
normalized to a unique symbol. Words are not
used if they are included in the stopword list (322
cmc2007
? max. width avg. width time (sec.)
0 0 0.00 90
100 2 1.17 132
200 3 1.51 145
400 3 1.71 165
800 4 2.11 200
1600 5 2.93 427
3200 4 3.99 2280
reuters10
? max. width avg. width time (sec.)
0 0 0.00 787
500 2 1.72 1378
1000 3 2.00 1752
2000 4 2.16 2594
4000 6 2.90 7183
8000 6 4.22 21555
16000 6 5.67 116535
reuters90
? max. width avg. width time (sec.)
0 0 0.00 26172
500 5 1.74 28067
1000 6 2.24 38510
2000 6 3.22 42479
4000 8 3.68 60029
8000 14 4.56 153268
16000 17 6.39 ?
Table 4: Tree width and training time for
cmc2007, reuters10, and reuters90
words), or they occur fewer than a threshold, c, in
training data. We set c = 5 for reuters10 and
reuters90, following previous works (Gham-
rawi and McCallum, 2005), while c = 0 for
cmc2007, because the data is small.
These features are selected according to av-
eraged mutual information (information gain),
which is the most popular method in previous
works (Yang and Pedersen, 1997; Ghamrawi and
McCallum, 2005). For each label, features are
sorted according to this score, and top-ranked fea-
tures are included in the model. By preliminary
experiments, we fixed parameters, ? for word uni-
gram features and ? for non-existence features, for
each data set, as shown in Table 2.
The same method is applied to the selection of
label correlation features. In the following experi-
ments, we observe the accuracy and training time
by changing the threshold parameter ? for the se-
lection of label correlation features.
4.2 Results
Table 33 shows microaveraged F-scores (micro-
F1) and subset accuracies (sub. acc.) (Ghamrawi
and McCallum, 2005) while varying ?, the num-
3The experiment with ? = 16000 for ME was not per-
formed due to its cost (estimated time is approx. two weeks).
65
ber of label correlation features. In all data sets
and with all classifiers, the accuracy is increased
by incorporating label correlation features. The re-
sults also demonstrate that the accuracy saturates,
or even decreases, with large ?. This indicates that
the feature selection is necessary not only for ob-
taining efficiency, but also for higher accuracy.
Table 4 shows tree widths, and the time for the
training of the ME models. As shown, the graph-
ical model is represented effectively with sparse
graphs, even when the number of label correlation
features is increased. With these results, we can
conclude that our method can model label correla-
tions with a tractable cost.
The accuracy for cmc2007 is significantly bet-
ter than the results reported in Patrick et al (2007)
(micro-F1=81.1) in a similar setting, in which only
word unigram features are used. Our best result is
approaching the results of Crammer et al (2007)
(micro-F1=84.6), which exploits various linguisti-
cally motivated features. Numerous results have
been reported for reuters10, and most of them
report the microaveraged F-score around 91 to 94,
while our best result is comparable to the state-of-
the-art accuracy. For reuters90, Ghamrawi and
McCallum (2005) achieved an improvement in the
microaveraged F-score from 86.34 to 87.01, which
is comparable to our result.
5 Conclusion
This paper described a method for the exact infer-
ence for multi-label classification with label corre-
lation features. Experimental results on text cate-
gorization with the CMC challenge data and the
Reuters-21578 text collection demonstrated that
our method improves the accuracy for text cate-
gorization with a tractable cost. The availability
of exact inference enables us to apply various ma-
chine learning methods not yet investigated in this
paper, including support vector machines.
From the perspective of machine learning re-
search, feature selection methods should be recon-
sidered. While we used a feature selection method
that is widely accepted in text categorization re-
search, it has no direct connection with machine
learning models. Since feature selection methods
motivated by the optimization criteria of machine
learning models have been proposed (Riezler and
Vasserman, 2004), we expect that the integration
of our proposal with those methods will open up a
new framework for multi-label classification.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Grant-in-Aid for Young Scientists (MEXT,
Japan).
References
Andrew, G. and J. Gao. 2007. Scalable training of
l
1
-regularized log-linear models. In 24th Annual In-
ternational Conference on Machine Learning.
Berger, A. L., S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to natu-
ral language processing. Computational Linguistics,
22(1):39?71.
Collins, M. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In 2002 Conference on
Empirical Methods in Natural Language Processing.
Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J.
Spiegelhalter. 1999. Probabilistic Networks and Ex-
pert Systems. Springer-Verlag, New York.
Crammer, K., M. Dredze, K. Ganchev, and P. P. Taluk-
dar. 2007. Automatic code assignment to medical
text. In BioNLP 2007, pages 129?136.
Ghamrawi, N. and A. McCallum. 2005. Collective
multi-label classification. In ACM 14th Conference
on Information and Knowledge Management.
Herbrich, R., T. Graepel, and C. Campbell. 2001.
Bayes point machines. Journal of Machine Learn-
ing Research, 1:245?279.
Patrick, J., Y. Zhang, and Y. Wang. 2007. Evaluat-
ing feature types for encoding clinical notes. In 10th
Conference of the Pacific Association for Computa-
tional Linguistics, pages 218?225.
Pestian, J. P., C. Brew, P. Matykiewicz, DJ Hovermale,
N. Johnson, K. B. Cohen, and W. Duch. 2007.
A shared task involving multi-label classification of
clinical free text. In BioNLP 2007, pages 97?104.
Riezler, S. and A. Vasserman. 2004. Gradient fea-
ture testing and l
1
regularization for maximum en-
tropy parsing. In 42nd Meeting of the Association
for Computational Linguistics.
Schapire, R. E. and Y. Singer. 2000. Boostexter: a
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
Tsoumakas, G. and I. Katakis. 2007. Multi-label clas-
sification: an overview. Journal of Data Warehous-
ing and Mining, 3(3):1?13.
Yang, Y. and J. O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
14th International Conference on Machine Learn-
ing, pages 412?420.
66
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 121?130,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Rich Feature Vector for Protein-Protein Interaction Extraction from
Multiple Corpora
Makoto Miwa1 Rune S?tre1 Yusuke Miyao1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo, Japan
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan.
2School of Computer Science, University of Manchester, UK
3National Center for Text Mining, UK
{mmiwa,rune.saetre,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
Because of the importance of protein-
protein interaction (PPI) extraction from
text, many corpora have been proposed
with slightly differing definitions of pro-
teins and PPI. Since no single corpus is
large enough to saturate a machine learn-
ing system, it is necessary to learn from
multiple different corpora. In this paper,
we propose a solution to this challenge.
We designed a rich feature vector, and we
applied a support vector machine modi-
fied for corpus weighting (SVM-CW) to
complete the task of multiple corpora PPI
extraction. The rich feature vector, made
from multiple useful kernels, is used to
express the important information for PPI
extraction, and the system with our fea-
ture vector was shown to be both faster
and more accurate than the original kernel-
based system, even when using just a sin-
gle corpus. SVM-CW learns from one cor-
pus, while using other corpora for support.
SVM-CW is simple, but it is more effec-
tive than other methods that have been suc-
cessfully applied to other NLP tasks ear-
lier. With the feature vector and SVM-
CW, our system achieved the best perfor-
mance among all state-of-the-art PPI ex-
traction systems reported so far.
1 Introduction
The performance of an information extraction pro-
gram is highly dependent on various factors, in-
cluding text types (abstracts, complete articles, re-
ports, etc.), exact definitions of the information to
be extracted, shared sub-topics of the text collec-
tions from which information is to be extracted.
Even if two corpora are annotated in terms of the
same type of information by two groups, the per-
formance of a program trained by one corpus is
unlikely to be reproduced in the other corpus. On
the other hand, from a practical point of view, it is
worth while to effectively use multiple existing an-
notated corpora together, because it is very costly
to make new annotations.
One problem with several different corpora is
protein-protein interaction (PPI) extraction from
text. While PPIs play a critical role in un-
derstanding the working of cells in diverse bio-
logical contexts, the manual construction of PPI
databases such as BIND, DIP, HPRD, IntAct, and
MINT (Mathivanan et al, 2006) is known to be
very time-consuming and labor-intensive. The au-
tomatic extraction of PPI from published papers
has therefore been a major research topic in Natu-
ral Language Processing for Biology (BioNLP).
Among several PPI extraction task settings, the
most common is sentence-based, pair-wise PPI ex-
traction. At least four annotated corpora have been
provided for this setting: AIMed (Bunescu et al,
2005), HPRD50 (Fundel et al, 2006), IEPA (Ding
et al, 2002), and LLL (Ne?dellec, 2005). Each of
these corpora have been used as the standard cor-
pus for training and testing PPI programs. More-
over, several corpora are annotated for more types
of events than just for PPI. Such examples include
BioInfer (Pyysalo et al, 2007), and GENIA (Kim
et al, 2008a), and they can be reorganized into PPI
corpora. Even though all of these corpora were
made for PPI extraction, they were constructed
based on different definitions of proteins and PPI,
which reflect different biological research inter-
ests (Pyysalo et al, 2008).
Research on PPI extraction so far has revealed
that the performance on each of the corpora could
121
benefit from additional examples (Airola et al,
2008). Learning from multiple annotated cor-
pora could lead to better PPI extraction perfor-
mance. Various research paradigms such as induc-
tive transfer learning (ITL) and domain adaptation
(DA) have mainly focused on how to effectively
use corpora annotated by other groups, by reduc-
ing the incompatibilities (Pan and Yang, 2008).
In this paper, we propose the extraction of PPIs
from multiple different corpora. We design a rich
feature vector, and as an ITL method, we ap-
ply a support vector machine (SVM) modified for
corpus weighting (SVM-CW) (Schweikert et al,
2008), in order to evaluate the use of multiple cor-
pora for the PPI extraction task. Our rich feature
vector is made from multiple useful kernels, each
of which is based on multiple parser inputs, pro-
posed by Miwa et al (2008). The system with our
feature vector was better than or at least compa-
rable to the state-of-the-art PPI extraction systems
on every corpus. The system is a good starting
point to use the multiple corpora. Using one of the
corpora as the target corpus, SVM-CW weights
the remaining corpora (we call them the source
corpora) with ?goodness? for training on the tar-
get corpus. While SVM-CW is simple, we show
that SVM-CW can improve the performance of the
system more effectively and more efficiently than
other methods proven to be successful in other
NLP tasks earlier. As a result, SVM-CW with our
feature vector is comprised of a PPI system with
five different models, of which each model is su-
perior to the best model in the original PPI extrac-
tion task, which used only the single corpus.
2 Related Works
While sentence-based, pair-wise PPI extraction
was initially tackled by using simple methods
based on co-occurrences, lately, more sophisti-
cated machine learning systems augmented by
NLP techniques have been applied (Bunescu et al,
2005). The task has been tackled as a classifica-
tion problem. To pull out useful information from
NLP tools including taggers and parsers, several
kernels have been applied to calculate the similar-
ity between PPI pairs. Miwa et al (2008) recently
proposed the use of multiple kernels using multi-
ple parsers. This outperformed other systems on
the AIMed, which is the most frequently used cor-
pus for the PPI extraction task, by a wide margin.
To improve the performance using external
Classification
Result
Training
Data
Feature 
vector
Raw Texts
Parsers
Classifier
Test 
Data
Raw Texts
Model
Pair Information
Pair Information
Label
Figure 1: Overview of our PPI extraction system
training data, many ITL and DA methods have
been proposed. Most of ITL methods assume that
the feature space is same, and that the labels may
be different in only some examples, while most of
DA methods assume that the labels are the same,
and that the feature space is different. Among the
methods, we use adaptive SVM (aSVM) (Yang et
al., 2007), singular value decomposition (SVD)
based alternating structure optimization (SVD-
ASO) (Ando et al, 2005), and transfer AdaBoost
(TrAdaBoost) (Dai et al, 2007) to compare with
SVM-CW. We do not use semi-supervised learn-
ing (SSL) methods, because it would be consid-
erably costly to generate enough clean unlabeled
data needed for SSL (Erkan et al, 2007). aSVM
is seen as a promising DA method among sev-
eral modifications of SVM including SVM-CW.
aSVM tries to find a model that is close to the one
made from other classification problems. SVD-
ASO is one of the most successful SSL, DA, or
multi-task learning methods in NLP. The method
tries to find an additional useful feature space by
solving auxiliary problems that are close to the tar-
get problem. With well-designed auxiliary prob-
lems, the method has been applied to text clas-
sification, text chunking, and word sense disam-
biguation (Ando, 2006). The method was reported
to perform better than or comparable to the best
state-of-the-art systems in all of these tasks. TrAd-
aBoost was proposed as an ITL method. In train-
ing, the method reduces the effect of incompatible
examples by decreasing their weights, and thereby
tries to use useful examples from source corpora.
The method has been applied to text classifica-
tion, and the reported performance was better than
SVM and transductive SVM (Dai et al, 2007).
3 PPI Extraction System
The target task of our system is a sentence-based,
pair-wise PPI extraction. It is formulated as a clas-
sification problem that judges whether a given pair
122
XPGp1 protein interacts with multiple subunits of
TFIIHprot and with CSBp2 protein.
Figure 2: A sentence including an interacting pro-
tein pair (p1, p2). (AIMed PMID 8652557, 9th
sentence, 3rd pair)
BOW
v-walks
e-walks
Graph BOW
v-walks
e-walks
Graph
Normalization
Parsers
KSDEPEnju
a sentence including a pair
feature vector
BOW Graph BOW
v-walks
e-walks
Graph
v-walks
e-walks
Figure 3: Extraction of a feature vector from the
target sentence
of proteins in a sentence is interacting or not. Fig-
ure 2 shows an example of a sentence in which the
given pair (p1 and p2) actually interacts.
Figure 1 shows the overview of the proposed
PPI extraction system. As a classifier using a sin-
gle corpus, we use the 2-norm soft-margin lin-
ear SVM (L2-SVM) classifier, with the dual co-
ordinate decent (DCD) method, by Hsieh et al
(2008). In this section, we explain the two main
features: the feature vector, and the corpus weight-
ing method for multiple corpora.
3.1 Feature Vector
We propose a feature vector with three types of
features, corresponding to the three different ker-
nels, which were each combined with the two
parsers: the Enju 2.3.0, and KSDEP beta 1 (Miyao
et al, 2008); this feature vector is used because the
kernels with these parsers were shown to be effec-
tive for PPI extraction by Miwa et al (2008), and
because it is important to start from a good per-
formance single corpus system. Both parsers were
retrained using the GENIA Treebank corpus pro-
vided by Kim et al (2003). By using our linear
feature vector, we can perform calculations faster
by using fast linear classifiers like L2-SVM, and
we also obtain a more accurate extraction, than by
using the original kernel method.
Figure 3 summarizes the way in which the fea-
ture vector is constructed. The system extracts
Bag-of-Words (BOW), shortest path (SP), and
graph features from the output of two parsers. The
PROT M:1, and M:1, interact M:1, multiple M:1,
of M:1, protein M:1, subunit M:1, with M:2, pro-
tein A:1
Figure 4: Bag-of-Words features of the pair in Fig-
ure 2 with their positions (B:Before, M:in the Mid-
dle of, A:After) and frequencies.
NMOD SBJ
rNMOD
ENTITY1 protein interact ENTITY2protein protein
ENTITY1 protein interacts with multiple and with ENTITY2 protein .
NMOD SBJ
COOD
COORD
NMOD
PMOD
NMOD SBJ
rNMOD
protein interact protein
SBJ rCOOD
rPMOD
V-walks  
E-walks
???
???
???
Figure 5: Vertex walks, edge walks in the upper
shortest path between the proteins in the parse tree
by KSDEP. The walks and their subsets are used
as the shortest path features of the pair in Figure 2.
output is grouped according to the feature-type
and parser, and each group of features is separately
normalized by the L2-norm1. Finally, all values
are put into a single feature vector, and the whole
feature vector is then also normalized by the L2-
norm. The features are constructed by using pred-
icate argument structures (PAS) from Enju, and by
using the dependency trees from KSDEP.
3.1.1 Bag-of-Words (BOW) Features
The BOW feature includes the lemma form of a
word, its relative position to the target pair of pro-
teins (Before, Middle, After), and its frequency in
the target sentence. BOW features form the BOW
kernel in the original kernel method. BOW fea-
tures for the pair in Figure 2 are shown in Figure 4.
3.1.2 Shortest Path (SP) Features
SP features include vertex walks (v-walks), edge
walks (e-walks), and their subsets (Kim et al,
2008b) on the target pair in a parse structure, and
represent the connection between the pair. The
features are the subsets of the tree kernels on the
shortest path (S?tre et al, 2007). Figure 5 illus-
trates the shortest path between the pair in Fig-
ure 2, and its v-walks and e-walks extracted from
the shortest path in the parse tree by KSDEP. A
v-walk includes two lemmas and their link, while
1The vector normalized by the L2-norm is also called a
unit vector.
123
an e-walk includes a lemma and its two links. The
links indicates the predicate argument relations for
PAS, and the dependencies for dependency trees.
3.1.3 Graph Features
Graph features are made from the all-paths graph
kernel proposed by Airola et al (2008). The ker-
nel represents the target pair using graph matrices
based on two subgraphs, and the graph features are
all the non-zero elements in the graph matrices.
The two subgraphs are a parse structure sub-
graph (PSS) and a linear order subgraph (LOS).
Figure 6 describes the subgraphs of the sentence
parsed by KSDEP in Figure 2. PSS represents the
parse structure of a sentence. PSS has word ver-
tices or link vertices. A word vertex contains its
lemma and its part-of-speech (POS), while a link
vertex contains its link. Additionally, both types
of vertices contain their positions relative to the
shortest path. The ?IP?s in the vertices on the
shortest path represent the positions, and the ver-
tices are differentiated from the other vertices like
?P?, ?CC?, and ?and:CC? in Figure 6. LOS repre-
sents the word sequence in the sentence. LOS has
word vertices, each of which contains its lemma,
its relative position to the target pair, and its POS.
Each subgraph is represented by a graph matrix
G as follows:
G = L
T
?
?
n=1
A
n
L, (1)
where L is a N?L label matrix, A is an N?N
edge matrix, N represents the number of vertices,
and L represents the number of labels. The la-
bel of a vertex includes all information described
above (e.g. ?ENTITY1:NN:IP? in Figure 6). If
two vertices have exactly same information, the
labels will be same. G can be calculated effi-
ciently by using the Neumann Series (Airola et al,
2008). The label matrix represents the correspon-
dence between labels and vertices. L
ij
is 1 if the
i-th vertex corresponds to the j-th label, and 0 oth-
erwise. The edge matrix represents the connection
between the pairs of vertices. A
ij
is a weight w
ij
(0.9 or 0.3 in Figure 6 (Airola et al, 2008)) if the
i-th vertex is connected to the j-th vertex, and 0
otherwise. By this calculation, G
ij
represent the
sum of the weights of all paths between the i-th
label and the j-th label.
A B H I L
positive 1,000 2,534 163 335 164
all 5,834 9,653 433 817 330
Table 1: The sizes of used PPI corpora. A:AIMed,
B:BioInfer, H:HPRD50, I:IEPA, and L:LLL.
50
60
70
80
90
100
0 20 40 60 80 100
% examples
AImed (F)
BioInfer (F)
AImed (AUC)
BioInfer (AUC)
Figure 7: Learning curves on two large corpora.
The x-axis is related to the percentage of the ex-
amples in a corpus. The curves are obtained by a
10-fold CV with a random split.
3.2 Corpus Weighting for Mixing Corpora
Table 1 shows the sizes of the PPI corpora that we
used. Their widely-ranged differences including
the sizes were manually analyzed by Pyysalo et
al. (2008). While AIMed, HPRD50, IEPA, and
LLL were all annotated as PPI corpora, BioInfer in
its original form contains much more fine-grained
information than does just the PPI. BioInfer was
transformed into a PPI corpus by a program, so
making it the largest of the five. Among them,
AIMed alone was created by annotating whole ab-
stracts, while the other corpora were made by an-
notating single sentences selected from abstracts.
Figure 7 shows the learning curves on two large
corpora: AIMed and BioInfer. The curves are
obtained by performing a 10-fold cross valida-
tion (CV) on each corpus, with random splits, us-
ing our system. The curves show that the perfor-
mances can benefit from the additional examples.
To get a better PPI extraction system for a chosen
target, we need to draw useful shared information
from external source corpora. We refer to exam-
ples in the source corpora as ?source examples?,
and examples in a target corpus as ?target exam-
ples?. Among the corpora, we assume that the la-
bels in some examples are incompatible, and that
their distributions are also different, but that the
feature space is shared.
In order to draw useful information from the
source corpora to get a better model for the target
124
ENTITY1
NN
IP
protein
NN
IP
interact
VBZ
IP
with
IN
IP
multiple
JJ
subunit
NNS
of
IN
PROT
NN
and
CC
with
IN
IP
ENTITY2
NN
IP
protein
NN
IP
.
.
NMOD
IP
SBJ
IP
COOD
IP
PMOD
NMOD NMOD
PMOD
CC
COORD
IP
NMOD
IP
PMOD
IP
P
ENTITY1
NN
protein
NN
M
interact
VBZ
M
with
IN
M
multiple
JJ
M
subunit
NNS
M
of
IN
M
PROT
NN
M
and
CC
M
with
IN
M
ENTITY2
NN
protein
NN
A
.
.
0.9,            0.3
IP: In shortest Path, B:Before, M:in the Middle of, A:After
Figure 6: Parse structure subgraph and linear order subgraph to extract graph features of the pair in
Figure 2. The parse structure subgraph is from the parse tree by KSDEP.
corpus, we use SVM-CW, which has been used
as a DA method. Given a set of instance-label
pairs (xi, yi), i = 1, . . ., ls + lt, xi?Rn, and
y
i
?{?1,+1}, we solve the following problem:
min
w
1
2
w
T
w + C
s
ls
?
i=1
`
i
+ C
t
ls+lt
?
i=ls+1
`
i
, (2)
where w is a weight vector, ` is a loss function,
and ls and lt are the numbers of source and target
examples respectively. C
s
? 0 and C
t
? 0 are
penalty parameters. We use a squared hinge loss
`
i
= max(0, 1? y
i
w
T
xi)2. Here, the source cor-
pora are treated as one corpus. The problem, ex-
cluding the second term, is equal to L2-SVM. The
problem can be solved using the DCD method.
As an ITL method, SVM-CW weights each cor-
pus, and tries to benefit from the source corpora,
by adjusting the effect of their compatibility and
incompatibility. For the adjustment, these penalty
parameters should be set properly. Since we are
unaware of the widely ranged differences among
the corpora, we empirically estimated them by
performing 10-fold CV on the training data.
4 Evaluation
4.1 Evaluation Settings
We used five corpora for evaluation: AIMed,
BioInfer, HPRD50, IEPA, and LLL. For the com-
parison with other methods, we report the F-
score (%), and the area under the receiver op-
erating characteristic (ROC) curve (AUC) (%)
using (abstract-wise) a 10-fold CV and a one-
answer-per-occurrence criterion. These measures
are commonly used for the PPI extraction tasks.
The F-score is a harmonic mean of Precision and
Recall. The ROC curve is a plot of a true posi-
tive rate (TPR) vs a false positive rate (FPR) for
different thresholds. We tuned the regularization
parameters of all classifiers by performing a 10-
fold CV on the training data using a random split.
The other parameters were fixed, and we report the
highest of the macro-averaged F-scores as our fi-
nal F-score. For 10-fold CV, we split the corpora
as recommended by Airola et al (2008).
4.2 PPI Extraction on a Single Corpus
In this section, we evaluate our system on a single
corpus, in order to evaluate our feature vector and
to justify the use of the following modules: nor-
malization methods and classification methods.
First, we compare our preprocessing method
with other preprocessing methods to confirm how
our preprocessing method improves the perfor-
mance. Our method produced 64.2% in F-score
using L2-SVM on AIMed. Scaling all features in-
dividually to have a maximal absolute value of 1,
produced only 44.2% in the F-score, while nor-
malizing the feature vector by L2-norm produced
61.5% in the F-score. Both methods were inferior
to our method, because the values of features in
the same group should be treated together, and be-
cause the values of features in the different groups
should not have a big discrepancy. Weighting each
125
L2 L1 LR AP CW
F 64.2 64.0 64.2 62.7 63.0
AUC 89.1 88.8 89.0 88.5 87.8
Table 2: Classification performance on AIMed us-
ing five different linear classifiers. The F-score (F)
and Area Under the ROC curve (AUC) are shown.
L2 is L2-SVM, L1 is L1-SVM, LR is logistic re-
gression, AP is averaged perceptron, and CW is
confidence weighted linear classification.
group with different values can produce better re-
sults, as will be explored in our future work.
Next, using our feature vector, we applied
five different linear classifiers to extract PPI
from AIMed: L2-SVM, 1-norm soft-margin
SVM (L1-SVM), logistic regression (LR) (Fan
et al, 2008), averaged perceptron (AP) (Collins,
2002), and confidence weighted linear classifica-
tion (CW) (Dredze et al, 2008). Table 2 indicates
the performance of these classifiers on AIMed.
We employed better settings for the task than did
the original methods for AP and CW. We used a
Widrow-Hoff learning rule (Bishop, 1995) for AP,
and we performed one iteration for CW. L2-SVM
is as good as, if not better, than other classifiers (F-
score and AUC). In the least, L2-SVM is as fast as
these classifiers. AP and CW are worse than the
other three methods, because they require a large
number of examples, and are un-suitable for the
current task. This result indicates that all linear
classifiers, with the exception of AP and CW, per-
form almost equally, when using our feature vec-
tor.
Finally, we implemented the kernel method by
Miwa et al (2008). For a 10-fold CV on AIMed,
the running time was 9,507 seconds, and the per-
formance was 61.5% F-score and 87.1% AUC.
Our system used 4,702 seconds, and the perfor-
mance was 64.2% F-score and 89.1% AUC. This
result displayed that our system, with L2-SVM,
and our new feature vector, is better, and faster,
than the kernel-based system.
4.3 Evaluation of Corpus Weighting
In this section, we first apply each model from a
source corpus to a target corpus, to show how dif-
ferent the corpora are. We then evaluate SVM-CW
by comparing it with three other methods (see Sec-
tion 2) with limited features, and apply it to every
corpus.
0
10
20
30
40
50
60
70
80
90
AIMed BioInfer HPRD50 IEPA LLL
F
Target corpus
AIMed
BioInfer
HPRD50
IEPA
LLL
co-occ
Model
Figure 8: F-score on a target corpus using a model
on a source corpus. For the comparison, we show
the 10-fold CV result on each target corpus and
co-occurrences. The regularization parameter was
fixed to 1.
First, we apply the model from a source corpus
to a target corpus. Figure 8 shows how the model
from a source corpus performs on the target cor-
pus. Interestingly, the model from IEPA performs
better on LLL than the model from LLL itself. All
the results showed that using different corpora (ex-
cept IEPA) is worse than just using the same cor-
pora. However, the cross-corpora scores are still
better than the co-occurrences base-line, which in-
dicates that the corpora share some information,
even though they are not fully compatible.
Next, we compare SVM-CW with three other
methods: aSVM, SVD-ASO, and TrAdaBoost.
For this comparison, we used our feature vec-
tor without including the graph features, because
SVD-ASO and TrAdaBoost require large compu-
tational resources. We applied SVD-ASO and
TrAdaBoost in the following way. As for SVD-
ASO, we made 400 auxiliary problems from the
labels of each corpus by splitting features ran-
domly, and extracted 50 additional features each
for 4 feature groups. In total, we made new 200
additional features from 2,000 auxiliary problems.
As recommended by Ando et al (2005), we re-
moved negative weights, performed SVD to each
feature group, and iterated ASO once. Since Ad-
aBoost easily overfitted with our rich feature vec-
tor, we applied soft margins (Ratsch et al, 2001)
to TrAdaBoost. The update parameter for source
examples was calculated using the update param-
eter on the training data in AdaBoost and the orig-
inal parameter in TrAdaBoost. This ensures that
the parameter would be the same as the original
parameter, when the C value in the soft margin ap-
proaches infinity.
126
aSVM SVD-ASO TrAdaBoost SVM-CW L2-SVM
F AUC F AUC F AUC F AUC F AUC
AIMed 63.6 88.4 62.9 88.3 63.4 88.4 64.0 88.6 63.2 88.4
BioInfer 66.5 85.2 65.7 85.1 66.1 85.2 66.7 85.4 66.2 85.1
HPRD50 71.2 84.3 68.7 80.8 72.6 85.3 72.7 86.4 67.2 80.7
IEPA 73.8 85.4 72.3 83.8 74.3 86.3 75.2 85.9 73.0 84.7
LLL 85.9 89.2 79.3 85.5 86.5 88.8 86.9 90.3 80.3 86.3
Table 3: Comparison of methods on multiple corpora. Our feature vector without graph features is used.
The source corpora with the best F-scores are reported for aSVM, TrAdaBoost, and SVM-CW.
F-score AUC
A B H I L all A B H I L all
A (64.2) 64.0 64.7 65.2 63.7 64.2 (89.1) 89.5 89.2 89.3 89.0 89.4
B 67.9 (67.6) 67.9 67.9 67.7 68.3 86.2 (86.1) 86.2 86.3 86.2 86.4
H 71.3 71.2 (69.7) 74.1 70.8 74.9 84.7 85.0 (82.8) 85.0 83.4 87.9
I 74.4 75.6 73.7 (74.4) 74.4 76.6 86.7 87.1 85.4 (85.6) 86.9 87.8
L 83.2 85.9 82.0 86.7 (80.5) 84.1 86.3 87.1 87.4 90.8 (86.0) 86.2
Table 4: F-score and AUC by SVM-CW. Rows correspond to a target corpus, and columns a source
corpus. A:AIMed, B:BioInfer, H:HPRD50, I:IEPA, and L:LLL corpora. ?all? signifies that all source
corpora are used as one source corpus, ignoring the differences among the corpora. For the comparison,
we show the 10-fold CV result on each target corpus.
In Table 3, we demonstrate the results of the
comparison. SVM-CW improved the classifica-
tion performance at least as much as all the other
methods. The improvement is mainly attributed to
the aggressive use of source examples while learn-
ing the model. Some source examples can be used
as training data, as indicated in Figure 8. SVM-
CW does not set the restriction between C
s
and
C
t
in Equation (2), so it can use source exam-
ples aggressively while learning the model. Since
aSVM transfers a model, and SVD-ASO transfers
an additional feature space, aSVM and SVD-ASO
do not use the source examples while learning the
model. In addition to the difference in the data us-
age, the settings of aSVM and SVD-ASO do not
match the current task. As for aSVM, the DA as-
sumption (that the labels are the same) does not
match the task. In SVD-ASO, the numbers of both
source examples and auxiliary problems are much
smaller than those reported by Ando et al (2005).
TrAdaBoost uses the source examples while learn-
ing the model, but never increases the weight of
the examples, and it attempts to reduce their ef-
fects.
Finally, we apply SVM-CW to all corpora using
all features. Table 4 summarizes the F-score and
AUC by SVM-CW with all features. SVM-CW
is especially effective for small corpora, show-
ing that SVM-CW can adapt source corpora to a
small annotated target corpus. The improvement
on AIMed is small compared to the improvement
on BioInfer, even though these corpora are sim-
ilar in size. One of the reasons for this is that
whole abstracts are annotated in AIMed, therefore
making the examples biased. The difference be-
tween L2-SVM and SVM-CW + IEPA on AIMed
is small, but statistically, it is significant (McNe-
mar test (McNemar, 1947), P = 0.0081). In the
cases of HPRD50 + IEPA, LLL + IEPA, and two
folds in BioInfer + IEPA, C
s
is larger than C
t
in
Equation (2). This is worth noting, because the
source corpus is more weighted than the target cor-
pus, and the prediction performance on the tar-
get corpus is improved. Most methods put more
trust in the target corpus than in the source cor-
pus, and our results show that this setting is not al-
ways effective for mixing corpora. The results also
indicate that IEPA contains more useful informa-
tion for extracting PPI than other corpora, and that
using source examples aggressively is important
for these combinations. We compared the results
of L2-SVM and SVM-CW + IEPA on AIMed,
and found that 38 pairs were described as ?inter-
action? or ?binding? in the sentences among 61
127
SVM-CW L2-SVM Airola et al
F AUC F AUC F AUC
A 65.2 89.3 64.2 89.1 56.4 84.8
B 68.3 86.4 67.6 86.1 61.3 81.9
H 74.9 87.9 69.7 82.8 63.4 79.7
I 76.6 87.8 74.4 85.6 75.1 85.1
L 86.7 90.8 80.5 86.0 76.8 83.4
Table 6: Comparison with the results by Airola
et al (2008). A:AIMed, B:BioInfer, H:HPRD50,
I:IEPA, and L:LLL corpora. The results with the
highest F-score from Table 4 are reported as the
results for SVM-CW.
newly found pairs. This analysis is evidence that
IEPA contains instances to help find such inter-
actions, and that SVM-CW helps to collect gold
pairs that lack enough supporting instances in a
single corpus, by adding instances from other cor-
pora. SVM-CW missed coreferential relations that
were also missed by L2-SVM. This can be at-
tributed to the fact that the coreferential informa-
tion is not stored in our current feature vector; so
we need an even more expressive feature space.
This is left as future work.
SVM-CW is effective on most corpus combi-
nations, and all the models from single corpora
can be improved by adding other source corpora.
This result is impressive, because the baselines by
L2-SVM on just single corpora are already better
than or at least comparable to other state-of-the-art
PPI extraction systems, and also because the vari-
ety of the differences among different corpora is
quite wide depending on various factors including
annotation policies of the corpora (Pyysalo et al,
2008). The results suggest that SVM-CW is useful
as an ITL method.
4.4 Comparison with Other PPI Systems
We compare our system with other previously
published PPI extraction systems. Tables 5 and
6 summarize the comparison. Table 5 summa-
rizes the comparison of several PPI extraction sys-
tems evaluated on the AIMed corpus. As indi-
cated, the performance of the heavy kernel method
is lower than our fast rich feature-vector method.
Our system is, to the extent of our knowledge, the
best performing PPI extraction system evaluated
on the AIMed corpus, both in terms of AUC and
F-scores. Airola et al (2008) first reported results
using all five corpora. We cannot directly com-
pare our result with the F-score results, because
they tuned the threshold, but our system still out-
performs the system by Airola et al (2008) on ev-
ery corpus in AUC values. The results also indi-
cate that our system outperforms other systems on
all PPI corpora, and that both the rich feature vec-
tor and the corpus weighting are effective for the
PPI extraction task.
5 Conclusion
In this paper, we proposed a PPI extraction system
with a rich feature vector, using a corpus weight-
ing method (SVM-CW) for combining the mul-
tiple PPI corpora. The feature vector extracts as
much information as possible from the main train-
ing corpus, and SVM-CW incorporate other exter-
nal source corpora in order to improve the perfor-
mance of the classifier on the main target corpus.
To the extent of our knowledge, this is the first ap-
plication of ITL and DA methods to PPI extrac-
tion. As a result, the system, with SVM-CW and
the feature vector, outperformed all other PPI ex-
traction systems on all of the corpora. The PPI
corpora share some information, and it is shown
to be effective to add other source corpora when
working with a specific target corpus.
The main contributions of this paper are: 1)
conducting experiments in extracting PPI using
multiple corpora, 2) suggesting a rich feature
vector using several previously proposed features
and normalization methods, 3) the combination of
SVM with corpus weighting and the new feature
vector improved results on this task compared with
prior work.
There are many differences among the corpora
that we used, and some of the differences are still
unresolved. For further improvement, it would be
necessary to investigate what is shared and what
is different among the corpora. The SVM-CW
method, and the PPI extraction system, can be ap-
plied generally to other classification tasks, and
to other binary relation extraction tasks, without
the need for modification. There are several other
tasks in which many different corpora, which at
first glance seem compatible, exist. By apply-
ing SVM-CW to such corpora, we will analyze
which differences can be resolved by SVM-CW,
and what differences require a manual resolution.
For the PPI extraction system, we found many
false negatives that need to be resolved. For fur-
ther improvement, we need to analyze the cause
128
positive all P R F AUC
SVM-CW 1,000 5,834 60.0 71.9 65.2 89.3
L2-SVM 1,000 5,834 62.7 66.6 64.2 89.1
(Miwa et al, 2008) 1,005 5,648 60.4 69.3 64.2 (61.5) 87.9 (87.1)
(Miyao et al, 2008) 1,059 5,648 54.9 65.5 59.5
(Airola et al, 2008) 1,000 5,834 52.9 61.8 56.4 84.8
(S?tre et al, 2007) 1,068 5,631 64.3 44.1 52.0
(Erkan et al, 2007) 951 4,020 59.6 60.7 60.0
(Bunescu and Mooney, 2005) 65.0 46.4 54.2
Table 5: Comparison with previous PPI extraction results on the AIMed corpus. The numbers of positive
and all examples, precision (P), recall (R), F-score (F), and AUC are shown. The result with the highest
F-score from Table 4 is reported as the result for SVM-CW. The scores in the parentheses of Miwa et al
(2008) indicate the result using the same 10-fold splits as our result, as indicated in Section 4.2.
of these false negatives more deeply, and design a
more discriminative feature space. This is left as a
future direction of our work.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Scientific Research (C) (General) (MEXT, Japan).
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interac-
tion extraction with evaluation of cross corpus learn-
ing. BMC Bioinformatics.
Rie Kubota Ando, Tong Zhang, and Peter Bartlett.
2005. A framework for learning predictive struc-
tures from multiple tasks and unlabeled data. Jour-
nal of Machine Learning Research, 6:1817?1853.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL-X),
pages 77?84, June.
C. M. Bishop. 1995. Neural Networks for Pattern
Recognition. Oxford University Press.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In
NIPS 2005.
Razvan C. Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP 2002,
pages 1?8.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong
Yu. 2007. Boosting for transfer learning. In ICML
2007, pages 193?200.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML 2008, pages 264?271.
Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extract-
ing protein interaction sentences using dependency
parsing. In EMNLP 2007.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2006. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
dual coordinate descent method for large-scale lin-
ear SVM. In ICML 2008, pages 408?415.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19:i180?i182.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008a. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
129
Seonho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Suresh Mathivanan, Balamurugan Periaswamy, TKB
Gandhi, Kumaran Kandasamy, Shubha Suresh, Riaz
Mohmood, YL Ramachandra, and Akhilesh Pandey.
2006. An evaluation of human protein-protein inter-
action data in the public domain. BMC Bioinformat-
ics, 7 Suppl 5:S19.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157, June.
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining mul-
tiple layers of syntactic information for protein-
protein interaction extraction. In Proceedings of the
Third International Symposium on Semantic Mining
in Biomedicine (SMBM 2008), pages 101?108.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya
Matsuzaki, and Jun?ichi Tsujii. 2008. Task-
oriented evaluation of syntactic parsers and their
representations. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguistics
(ACL?08:HLT).
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL?05 Workshop.
Sinno Jialin Pan and Qiang Yang. 2008. A survey on
transfer learning. Technical Report HKUST-CS08-
08, Department of Computer Science and Engineer-
ing, Hong Kong University of Science and Technol-
ogy, Hong Kong, China, November.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein inter-
action corpora. In BMC Bioinformatics, volume
9(Suppl 3), page S6.
Gunnar Ratsch, Takashi Onoda, and Klaus-Robert
Muller. 2001. Soft margins for adaboost. Machine
Learning, 42(3):287?320.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Syntactic features for protein-protein interaction ex-
traction. In LBM 2007 short papers.
Gabriele Schweikert, Christian Widmer, Bernhard
Scho?lkopf, and Gunnar Ra?tsch. 2008. An empir-
ical analysis of domain adaptation algorithms for
genomic sequence analysis. In NIPS, pages 1433?
1440.
Jun Yang, Rong Yan, and Alexander G. Hauptmann.
2007. Cross-domain video concept detection using
adaptive SVMs. In MULTIMEDIA ?07: Proceed-
ings of the 15th international conference on Multi-
media, pages 188?197.
130
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1162?1171,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Descriptive and Empirical Approaches to Capturing Underlying
Dependencies among Parsing Errors
Tadayoshi Hara1 Yusuke Miyao1
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN
2School of Computer Science, University of Manchester
3NaCTeM (National Center for Text Mining)
{harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii1,2,3
Abstract
In this paper, we provide descriptive and
empirical approaches to effectively ex-
tracting underlying dependencies among
parsing errors. In the descriptive ap-
proach, we define some combinations of
error patterns and extract them from given
errors. In the empirical approach, on the
other hand, we re-parse a sentence with
a target error corrected and observe er-
rors corrected together. Experiments on
an HPSG parser show that each of these
approaches can clarify the dependencies
among individual errors from each point
of view. Moreover, the comparison be-
tween the results of the two approaches
shows that combining these approaches
can achieve a more detailed error analysis.
1 Introduction
For any kind of technology, analyzing causes of
errors given by a system is a very helpful process
for improving its performance. In recent sophisti-
cated parsing technologies, the process has taken
on more and more important roles since critical
ideas for parsing performance have already been
introduced and the researches are now focusing on
exploring the rest of the pieces for making addi-
tional improvements.
In most cases for parsers? error analysis, re-
searchers associate output errors with failures in
handling certain linguistic phenomena and attempt
to avoid them by adding or modifying correspond-
ing settings of their parsers. However, such an
analysis cannot been done so smoothly since pars-
ing errors sometimes depend on each other and the
underlying dependencies behind superficial phe-
nomena cannot be captured easily.
In this paper, we propose descriptive and em-
pirical approaches to effective extraction of de-
pendencies among parsing errors and engage in a
deeper error analysis with them. In our descriptive
approach, we define various combinations of error
patterns as organized error phenomena on the ba-
sis of linguistic knowledge, and then extract such
combinations from given errors. In our empirical
approach, on the other and, we re-parse a sentence
under the condition where a target error is cor-
rected, and errors which are additionally corrected
are regarded as dependent errors. By capturing de-
pendencies among parsing errors through system-
atic approaches, we can effectively collect errors
which are related to the same linguistic properties.
In the experiments, we applied both of our ap-
proaches to an HPSG parser Enju (Miyao and Tsu-
jii, 2005; Ninomiya et al, 2006), and then evalu-
ated the obtained error classes. After examining
the individual approaches, we explored the com-
bination of them.
2 Parser and its evaluation
A parser is a system which interprets structures
of given sentences from some grammatical or in
some cases semantical viewpoints, and interpreted
structures are utilized as essential information for
various natural language tasks such as informa-
tion extraction, machine translation, and so on.
In most cases, an output structure of a parser is
based on a certain grammatical framework such as
CFG, CCG (Steedman, 2000), LFG (Kaplan and
Bresnan, 1995) or HPSG (Pollard and Sag, 1994).
Since such a framework can usually produce more
than one probable structure for a sentence, a parser
1162
John aux_arg12
ARG1 ARG2
verb_arg1
ARG1
has : come :
Figure 1: Predicate argument relations
Abbr. Full Abbr. Full
aux auxiliary lgs logical subject
verb verb coord coordination
prep prepositional conj conjunction
det determiner argN
1
... take argument(s)
adj adjunction (N
1
th, ...)
app apposition mod modify a word
relative relative
Table 1: Descriptions for predicate types
often utilizes some kind of disambiguation model
for choosing the best one.
While various parsers take different manners
in capturing linguistic phenomena based on their
frameworks, they are at least required to obtain
some kinds of relations between the words in sen-
tences. On the basis of the requirements, a parser
is usually evaluated on how correctly it gives in-
tended linguistic relations. ?Predicate argument
relation? is one of the most common evaluation
measurements for a parser since it is a very fun-
damental linguistic behavior and is less dependent
on parser systems. This measure divides linguis-
tic structural phenomena in a sentence into min-
imal predicative events. In one predicate argu-
ment relation, a word which represents an event
(predicate) takes some words as participants (argu-
ments). Although no fixed formulation exists for
the relations, there are to a large extent common
conceptions for them based on linguistic knowl-
edge among researchers.
Figure 1 shows an example of predicate argu-
ment relations given by Enju. In the sentence
?John has come.?, ?has? is a predicate of type
?aux arg12? and takes ?John? and ?come? as the
first and second arguments. ?come? is also a pred-
icate of the type ?verb arg1? and takes ?John? as
the first and the only argument. In this formalism,
each predicate type is represented as a combina-
tion of ?the grammatical nature of a word? and
?the arguments which it takes,? which are repre-
sented by the descriptions in Table 1. ?aux arg12?
in Figure 1 indicates that it is an auxiliary word
and takes two arguments ?ARG1? and ?ARG2.?
In order to improve the performance of a parser,
analyzing parsing errors is very much worth the
I watched the girl on TV Correct answer:
ARG1 ARG2
ARG1 ARG2
I watched the girl on TV Parser output:
ARG1 ARG2
ARG1 ARG2
Obtain inconsistent outputs as errors
Error: I watched the girl on TV 
ARG1
ARG1 Error
Figure 2: An example of parsing errors
Error: The book on which read the shelf  I yesterdayARG1
ARG2
ARG2ARG1Figure 3: Co-occurring parsing errors
effort. Since the errors are output according to
a given evaluation measurement such as ?predi-
cate argument relation,? we researchers carefully
explore them and infer the linguistic phenom-
ena which cause the erroneous outputs. Figure 2
shows an example of parsing errors for sentence ?I
watched the girl on TV.? Note that the errors are
based on predicate argument relations as shown
above and that the predicate types are abbreviated
in this figure. When we focus on the error output,
we can observe that ?ARG1? of predicate ?on?
was mistaken by the parser. In this case, ?ARG1?
represents a modifiee of the preposition, and we
then conclude that the ill attachment of a prepo-
sitional phrase caused this error. By continuing
such error analysis, weak points of the parser are
revealed and can be useful clues for further im-
provements.
However, in most researches on parsing tech-
nologies, error analysis has been limited to narrow
and shallow explorations since there are various
dependencies behind erroneous outputs. In Fig-
ure 3, for example, two errors were given: wrong
outputs for ?ARG1? of ?which? and ?ARG2? of
?read.? Both of these two errors originated from
the fact that the relative clause took a wrong an-
tecedent ?the shelf.? In this sentence, the former
1163
Error:
ARG1ARG1
They completed the sale of for 
ARG1
ARG1
it to him $1,000 
Confliction
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Analysis 2: (Impossible)
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Analysis 1: (Possible)
Can each error occur independently?
ARG1
ARG1
ARG1 ARG1
Figure 4: Sketch of error propagation
?ARG1? directly corresponds to the antecedent
while the latter ?ARG2? indirectly referred to the
same antecedent as the object of the verb ?read.?
The two predicate argument relations thus took the
same word as their common arguments, and there-
fore the two errors co-occurred.
On the other hand, one-way inductive relations
also exist among errors. In Figure 4, ?ARG1? of
?for? and ?to? were mistaken by a parser. We can
know that each of the errors was caused by an ill
attachment of a prepositional phrase with the same
analysis as shown in Figure 2. What is important
in this example is the manner in their occurrences.
The former error can appear by itself (Analysis 1)
while the latter cannot because of the structural
conflict with the former error (Analysis 2). The
appearance of the latter error thus induces that of
the former error. In error analysis, we have to cor-
rectly capture such various relations, which leads
us to a costly and less rewarding analysis.
In order to make advancements on this prob-
lem, we propose two types of approaches to real-
izing a deeper error analysis on parsing. In the ex-
periments, we examine our approaches for actual
errors which are given by the HPSG parser Enju
(Miyao and Tsujii, 2005; Ninomiya et al, 2006).
Enju was developed for capturing detailed syntac-
tic or semantic properties and relations for a sen-
tence with an HPSG framework (Pollard and Sag,
1994). In this research, we focus on error analysis
based on predicate argument relations, and in the
experiments with Enju, utilize the relations which
Erroneous phenomena Matched patterns
[Argument selection]
Prepositional attachment ARG1 of prep arg
Adjunction attachment ARG1 of adj arg
Conjunction attachment ARG1 of conj arg
Head selection for ARG1 of det arg
noun phrase
Coordination ARG1/2 of coord arg
[Predicate type selection]
Preposition/Adjunction prep arg / adj arg
Gerund acts as modifier/not verb mod arg / verb arg
Coordination/conjunction coord arg / conj arg
# of arguments prep argX / prep argY
for preposition (X 6= Y )
Adjunction/adjunctive noun adj arg / noun arg
[More structural errors]
To-infinitive for see Figure 7
modifier/argument of verb
Subject for passive sentence see Figure 8
or not
[Others]
Comma any error around ?,?
Relative clause attachment see Figure 9
Table 2: Patterns defined for descriptive approach
are represented in parsed tree structures.
3 Two approaches for error analysis
In this section, we propose two approaches for er-
ror analysis which enable us to capture underlying
dependencies among parsing errors. Our descrip-
tive approach matches the patterns of error com-
binations with given parsing errors and collects
matched erroneous participants. Our empirical ap-
proach, on the other hand, detects co-occurring
errors by re-parsing a sentence under a situation
where each of the errors is forcibly corrected.
3.1 Descriptive approach
Our descriptive approach for capturing dependen-
cies among parsing errors is to extract certain rep-
resentative structures of errors and collect the er-
rors which involve them. Parsing errors have a ten-
dency to occur with certain patterns of structures
representing linguistic phenomena. We first define
such patterns through observations with a part of
error outputs, and then match them with the rest.
Table 2 summarizes the patterns for erroneous
phenomena which we defined for matching in
the experiments. In the table, the patterns for
14 phenomena are given and classified into four
types according to their matching manners. Each
of the patterns for ?Argument selection? examine
whether a focused argument for a certain predi-
cate type is erroneous or not. Figure 5 shows the
pattern for ?Prepositional attachment,? which col-
1164
prep_arg
ARG1 Error
Parser output: ?
They completed the sale of for :
ARG1ARG1
it to : him $1,000 
Pattern:
prep_arg12 prep_arg12Correct output:
ARG1ARG1
They completed the sale of for :it to : him $1,000 prep_arg12 prep_arg12Parser output:
Example:
Figure 5: Pattern for ?Prepositional attachment?
gerund:     verb_argParser output: gerund: verb_mod_argCorrect answer:
(Patterns of correct answer and parser output can be interchanged)Pattern:
Example:
The customers walk the door
a   package   for   them
expecting: verb_mod_arg123 you to havein ARG1
MOD ARG2
ARG3
Parser output:
Correct output:
The customers walk the door
a   package   for   them
expecting:     verb_arg123 you to haveinNot exist 
ARG2
ARG3
ARG1
(MOD)
Figure 6: Pattern for ?Gerund acts as modifier or
not?
lects wrong ARG1 for predicate type ?prep arg?.
From the sentence in the figure, we can obtain
two errors for ?Prepositional attachment? around
prepositions ?to? and ?for.? On the other hand,
each ?Predicate type selection? pattern collects er-
rors around a word whose predicate type is erro-
neous. Figure 6 shows the pattern for ?Gerund
acts as modifier or not,? which collects errors
around gerunds whose predicate types are erro-
neous. From the example sentence in the figure,
we can obtain an erroneous predicate type for ?ex-
pecting? and collect errors around it for ?Gerund
acts as modifier or not.?
We can implement more structural errors than
simple argument or predicate type selections. Fig-
ures 7 and 8 show the patterns for ?To-infinitive
for modifier/argument of verb? and ?Subject for
passive sentence or not? respectively. The pat-
tern for the latter phenomenon collects errors on
recognitions of prepositional phrases which be-
have as subjects for passive expressions. The pat-
tern collects errors not only around prepositions
but also around the verbs which take the preposi-
Parser output: aux_arg12to :verb1 ?ARG3 verb2
Correct output: aux_mod_arg12
MOD
to :
ARG2
Unknown subject ARG1 ARG1
verb1 ? verb2
The  figures  ? were  adjusted to : remove ...aux_arg12
Example:
Parser output:
Correct answer:
ARG3
The  figures  ? were  adjusted to : remove ...aux_mod_arg12 
MOD ARG2
Unknown subject ARG1 ARG1
Pattern: (Patterns of correct answer and parser output can be interchanged)
Figure 7: Pattern for ?To-infinitive for modi-
fier/argument of verb?
Example:
Pattern:
Parser output: prep_arg12Unknown subject verb1 ?ARG1ARG1 ?
Correct output: lgs_arg2 ARG2verb1 ? ?ARG1
A  50-state  study  released in  September  by : Friends  ?
Unknown subject ARG1ARG1 prep_arg12Parser output:
Correct answer: A  50-state  study  released in  September  by : Friends  ?ARG1ARG2 lgs_arg12ARG2
(Patterns of correct answer and parser output can be interchanged)
Figure 8: Pattern for ?Subject for passive sentence
or not?
tional phrases as a subject.
Since these patterns are based on linguistic
knowledge given by a human, the process could
provide a relatively precise analysis with a lower
cost than a totally manual analysis.
3.2 Empirical approach
Our empirical approach, on the other hand, briefly
traces the parsing process which results in each of
the target errors. We collect co-occurring errors
as strongly relevant ones, and then extract depen-
dencies among the obtained groups. Parsing errors
could originate from wrong processing at certain
stages in the parsing, and errors with a common
origin would by necessity appear together. We re-
parse a target sentence under the condition where a
certain error is forcibly corrected and then collect
errors which are corrected together as the ?rela-
tive? ones. An error group where all errors are
relative to each other can be regarded as a ?co-
occurring error group.? Errors in the same co-
1165
Example:
Pattern:
relative_arg1
ARG1
Parser output: ARG1/2
Error
Parser output:
Correct answer:
The book on relative_arg1 read ARG2the shelf  I yesterdayARG1
ARG2
ARG1
which :
The book on relative_arg1 read the shelf  I yesterdaywhich :
Figure 9: Pattern for ?Relative clause attachment?
our work force
Error 1
Re-parse a sentence under the condition whereeach error is forcibly corrected 
Error 1
Error 2
Error 3
Correct Error 2
Error 1
Error 1
Extract co-occurring error groups and inductive relations 
Error 4 Error 1
Error 4Error 3
Error 3Correct
Correct
Correct
corrected together
corrected together
corrected together
corrected together
,
,
,
Error 1 Error 2 Error 3 Error 4
today
ARG1
Correct answer:
It    has    no    bearing on
our work force todayonParser output: ARG1 ARG1ARG2
ARG2 ARG1 ARG1 ARG1
It    has    no    bearing
Error 2 Error 3 Error 4 Error 5
Error 5 Error 4Correct corrected togetherError 1 Error 3Error 2, , ,
Error 4,
Error 2 Error 4,,
Error 2 Error 3,,
Error 5Induce
Co-occurring error group Co-occurring error group
Figure 10: An image of our empirical approach
occurring error group are expected to participate
in the same phenomenon. Dependencies among
errors are then expected to be summarized with in-
ductions among co-occurring error groups.
Figure 10 shows an image of this approach. In
this example, ?today? should modify noun phrase
?our work force? while the parser decided that ?to-
day? was also in the noun phrase. As a result, there
are five errors: three wrong outputs for ?ARG2?
of ?on? (Error 1) and ?ARG1? of ?our? (Error 2)
and ?work? (Error 3), excess relation ?ARG1? of
?force? (Error 4), and missing relation ?ARG1? for
?today? (Error 5). By correcting each of the errors
1, 2, 3 and 4, all of these errors are corrected to-
gether, and therefore classified into the same co-
occurring error group. Although error 5 cannot
participate in the group, correcting error 5 can cor-
rect all of the errors in the group, and therefore an
# ofError types Errors Patterns
? Analyzed 2,078 1,671
[Argument selection]
Prepositional attachment 579 579
Adjunction attachment 261 261
Conjunction attachment 43 40
Head selection for noun phrase 30 30
Coordination 202 184
[Predicate type selection]
Preposition/Adjunction 108 54
Gerund acts as modifier or not 84 31
Coordination/conjunction 54 27
# of arguments for preposition 51 17
Adjunction/adjunctive noun 13 13
[More structural errors]
To-infinitive for 120 22
modifier/argument of verb
Subject for passive sentence 8 3
or not
[Others]
Comma 444 372
Relative clause attachment 102 38
? Unanalyzed 2,631 ?
Total 4,709 ?
Table 3: Errors extracted with descriptive analysis
inductive relation is given from error 5 to the co-
occurring error group. We can then finally obtain
the inductive relations as shown at the bottom of
Figure 10. This approach can trace the actual be-
havior of the parser precisely, and can therefore
capture underlying dependencies which cannot be
found only by observing error outputs.
4 Experiments
We applied our approaches to parsing errors given
by the HPSG parser Enju, which was trained on
the Penn Treebank (Marcus et al, 1994) section
2-21. We first examined each approach, and then
explored the combination of the approaches.
4.1 Evaluation of descriptive approach
We examined our descriptive approach. We first
parsed sentences in the Penn Treebank section 22
with Enju, and then observed the errors. Based on
the observation, we next described the patterns as
shown in Section 3. After that, we parsed section
0 and then applied the patterns to the errors.
Table 3 summarizes the extracted errors. As the
table shows, with the 14 error patterns, we suc-
cessfully matched 1,671 locations in error outputs
and covered 2,078 of 4,709 errors, which com-
prised of more than 40% of the total errors. This
was the first step of the application of our ap-
proach, and in the future work we would like to
1166
Evaluated sentences (erroneous) 1,811 (1,009)
Errors (Correctable) 4,709 (3,085)
Co-occurring errors 1,978
Extracted inductive relations 501
F-score (LP/LR) 90.69 (90.78/93.59)
Table 4: Summary of our empirical approach




       	 
 










Figure 11: Frequency of each size of co-occurring
error group
add more patterns for capturing more phenomena.
When we focused on individual patterns, we
could observe that the simple error phenomena
such as the attachments were dominant. The first
reason for this would be that such phenomena
were among minimal linguistic events. This would
make the phenomena components of other more
complex ones. The second reason for the dom-
inance would be that the patterns for these error
phenomena were easy to implement only with ar-
gument inconsistencies, and only one or a few pat-
terns could cover every probable error. Among
these dominant error types, the number of prepo-
sitional attachments was outstanding. The er-
ror types which required matching with predicate
types were fewer than the attachment errors since
the limited patterns on the predicate types would
narrow the possible linguistic behavior of the can-
didate words. When we focus on more structural
errors, the table shows that the rates of the partici-
pant errors to matched locations were much larger
than those for simpler pattern errors. Once our pat-
terns matches, they could collect many errors at
the same time.
4.2 Evaluation of empirical approach
Next, we applied our empirical approach in the
same settings as in the previous section. We first
parsed sentences in section 0 and then applied our
approach to the obtained errors. In the experi-
ments, some errors could not be forcibly corrected
by our approach. The parser ?cut off? less proba-
ble parse substructures before giving the predicate
Sentence: The  asbestos  fiber  ,  crocidolite ,  is  unusually  resilient  once  it  enters the    
lungs  ,  with  even  brief  exposures  to  it  causing  symptoms  that  show  up  decades  later
,  researchers  said
(a)(b)
(c) (d)
(a) fiber      , : crocidoliteapp_arg12
fiber      , : crocidolitecoord_arg12
Correct answer:
Parser output:
is     usually     resilient     ? the     lungs        ,        with(b)
symptoms    that     show : up    decades    later(c)
Parser output:
Correct answer: verb_arg1
symptoms    that     show : up    decades    laterverb_arg12
(d)
ARG1 ARG2
ARG1 ARG2
ARG1 ARG1
ARG1 ARG2
ARG1 ARG1
Correct answer:
Parser output: is     usually     resilient     ? the     lungs        ,        withARG1 ARG1
Correct answer:
Parser output:
It    causing    symptoms    that    show    up    decades    laterARG1
It    causing    symptoms    that    show    up    decades    laterARG1
Figure 12: Obtained co-occurring error groups
argument relation for reducing the cost of parsing.
In this research, we ignored the errors which were
subject to such ?cut off? as ?uncorrectable? ones,
and focused only on the remaining ?correctable?
errors. In our future work, we would like to con-
sider the ?uncorrectable? errors.
Table 4 shows the summary of the analysis with
our approach. Enju gave 4,709 errors for section
0. Among these errors, the correctable errors were
3,085, and from these errors, we successfully ob-
tained 1,978 co-occurring error groups and 501 in-
ductive relations. Figure 11 shows the frequency
for each size of co-occurring groups. About a half
of the groups contains only single errors, which
would indicate that the errors could have only one-
way inductive relations with other errors. The rest
of this section explores examples of the obtained
co-occurring error groups and inductive relations.
Figure 12 shows an example of the extracted co-
occurring error groups. For the sentence shown at
the top of the figure, Enju gave seven errors. By
introducing our empirical approach, these errors
were definitely classified into four co-occurring er-
ror groups (a) to (d), and there were no inductive
relations detected among them. Group (a) contains
two errors on the comma?s local behavior as ap-
position or coordination. Group (b) contains the
errors on the words which gave almost the same
attachment behaviors. Group (c) contains the er-
rors on whether the verb ?show? took ?decades?
1167
Error types # of correctable errors # of independent errors Correction effect (errors)
[Argument selection]
Prepositional attachment 531 397 766
Adjunction attachment 196 111 352
Conjunction attachment 33 12 79
Head selection for noun phrase 22 0 84
Coordination 146 62 323
[Predicate type selection]
Preposition/Adjunction 72 30 114
Gerund acts as modifier or not 39 18 62
Coordination/conjunction 36 16 61
# of arguments for preposition 24 23 26
Adjunction/adjunctive noun 8 6 10
[More structural errors]
To-infinitive for 75 27 87
modifier/argument of verb
Subject for passive sentence or not 8 3 9
[Others]
Comma 372 147 723
Relative clause attachment 84 27 119
Total 1,646 979 ?
Table 5: Induction relations between errors for each linguistic phenomenon and other errors
Sentence: She  says  she  offered  Mrs.  Yeargin a  quiet  resignation
and  thought  she  could  help  save  her  teaching  certificate(a) (b)
Correcting (a) induced correcting (b)
(b) Correct answer:
Parser output:
? thought  she  could  help   save : her  teaching  certificateverb_arg123
? thought  she  could  help   save : her  teaching  certificateverb_arg12
ARG1 ARG2
ARG1
ARG1 ARG2 ARG3
(a) Correct answer:
Parser output:
? thought   she   could     help : save   her   teaching   certificateverb_arg12
? thought   she   could     help : save   her   teaching   certificateaux_arg12
ARG1 ARG2
ARG2 ARG2
ARG1 ARG2
ARG2ARG2
Figure 13: Inductive relation between obtained co-
occurring error groups
as its object or not. Group (d) contains an error on
the attachment of the adverb ?later?. Regardless
of the overlap of the regions in the sentence for
(c) and (d), our approach successfully classified
the errors into the two independent groups. With
our approach, it would be empirically shown that
the errors in each group actually co-occurred and
the group was independent. This would enable us
to concentrate on each of the co-occurring error
groups without paying attention to the influences
from the errors in other groups.
Figure 13 shows another example of the anal-
ysis with our empirical approach. In this case, 8
errors for a sentence were classified into two co-
occurring error groups (a) and (b), and our ap-
proach showed that correction in group (a) re-
sulted in correcting group (b) together. The errors
in group (a) were on whether ?help? behaved as an
auxiliary or pure verbal role. The errors in group
(b) were on whether ?save? took only one object
?her teaching certificate,? or two objects ?her? and
?teaching certificate.? Between group (a) and (b),
no ?structural? conflict could be found when cor-
recting only each of the groups. We could then
guess that the inductive relation between these two
groups was implicitly given by the disambigua-
tion model of the parser. By dividing the errors
into minimum units and clarifying the effects of
correcting a target error, error analysis with our
empirical approach could suggest some policy for
parser improvements.
4.3 Combination of two approaches
On the basis of the experiments shown in the pre-
vious sections, we would like to explore possibili-
ties for obtaining a more detailed analysis by com-
bining the two approaches.
4.3.1 Interactions between a target linguistic
phenomenon and other errors
Our descriptive approach could classify the pars-
ing errors according to the linguistic phenomena
they participated in. We then attempt to reveal how
such classified errors interacted with other errors
from the viewpoints of our empirical approach. In
order to enable the analysis by our empirical ap-
proach, we focused only on the correctable errors.
1168
Sentence: It  invests  heavily  in  dollar-denominated  securities  overseas  and  is
currently  waiving  management  fees  ,  which  boosts  its  yield (a)(b)(a)
It  invests  heavily  in  dollar-denominated  securities    overseas :adj_arg1
?Adjunction attachment?
ARG1
ARG1
Pattern matched: 
is  currently  waiving  management  fees              ,         which           boosts   its  yield
(b)
?Comma? , ?Relative clause attachment?Pattern matched: 
ARG1ARG1ARG1
ARG1ARG1ARG1
Error:
Error:
Figure 14: Combination of results given by de-
scriptive and empirical approaches (1)
Table 5 reports the degree to which the classi-
fied errors were related to other individual errors.
The leftmost numbers show the numbers of cor-
rectable errors, which were the focused errors in
the experiments. The central numbers show the
numbers of ?independent? errors, that is, the errors
which could be corrected only by correcting them-
selves. The rightmost numbers show ?correction
effects,? that is, the number of errors which would
consequently be corrected if all of the errors for
the focused phenomena were forcibly corrected.
?Independent? errors are obtained by collecting
error phenomena groups which consist of unions
of co-occurring error groups and each error in
which is not induced by other errors. Figure 14
shows an example of ?independent? errors. For
the sentence at the top of the figure, the parser had
four errors on ARG1 of ?overseas,? the comma,
?which? and ?boosts.? Our empirical approach
then classified these errors into two co-occurring
error groups (a) and (b), and there was no induc-
tive relation between the groups. Our descrip-
tive approach, on the other hand, matched all of
the errors with the patterns for ?Adjunction at-
tachment,? ?Comma? and ?Relative clause attach-
ment.? Since the error for the ?Adjunction attach-
ment? equals to a co-occurring group (a) and is not
induced by other errors, the error is ?independent.?
Table 5 shows that, for ?Prepositional attach-
ment?, ?Adjunction attachments,? ?# of argu-
ments for preposition? and ?Adjunction/adjunctive
noun,? more than half of the errors for the focused
phenomena are ?independent.? Containing many
?independent? errors would mean that the parser
should handle these phenomena further more in-
tensively as an independent event.
Sentence: Clark  J.  Vitulli was  named  senior  vice  president  and  general  manager 
of  this  U.S.  sales  and  marketing  arm  of  Japanese  auto  Maker  Mazda  Motor  Corp(b) (a)
(b)
(a)
senior  vice  president  and  general  manager  of  this  U.S.  sales   and :coord_arg12
?Coordination? (fragment)
ARG1
ARG1
Pattern matched: 
Correcting (a) induced correcting (b)
manager   of     this : U.S.   sales    and : marketing  arm  of
?Coordination? (fragment),
?Head selection of noun phrase?Pattern matched: 
det_arg1 coord_arg12
ARG2ARG1ARG2 ARG1
ARG2 ARG1 ARG1 ARG1 ARG2
Error:
Error:
Figure 15: Combination of results given by de-
scriptive and empirical approaches (2)
The ?correction effect? for a focused linguistic
phenomenon can be obtained by counting errors in
the union of the correctable error set for the phe-
nomenon and the error sets which were induced by
the individual errors in the set. We would show an
example of correction effect in Figure 15. In the
figure, the parser had six errors for the sentence
at the top: three false outputs for ARG1 of ?and,?
?this? and ?U.S.,? two false outputs for ARG2 of
?of? and ?and,? and missing output for ARG1 of
?sales.? Our empirical approach classified these
errors into two co-occurring error groups (a) and
(b), and extracted an inductive relation from (a) to
(b). Our descriptive approach, on the other hand,
matched two errors on ?and? with pattern ?Coor-
dination? and one error on ?this? with ?Head se-
lection for noun phrase.? When we focus on the
error for ?Head selection of noun phrase? in co-
occurring group (a), the correction of the error in-
duced the rest of the errors in (a), and further in-
duced the error in (b) according to the inductive
relation from (a) to (b). Therefore, a ?correction
effect? for the error results in six errors.
Table 5 shows that, for ?Conjunction attach-
ment,? ?Head selection for noun phrase? and ?Co-
ordination,? each ?correction effect? results in
more than twice the forcibly corrected errors. Im-
proving the parser so that it can resolve such high-
correction-effect erroneous phenomena may ad-
ditionally improve the parsing performances to a
great extent. On the other hand, ?Head selection
for noun phrase? contains no ?independent? error,
and therefore could not be handled independently
of other erroneous phenomena at all. Consider-
1169
ing the effects from outer events might make the
treatment of ?Head selection for noun phrase? a
more complicated process than other phenomena,
regardless of its high ?correction effect.?
Table 5 would thus suggest which phenomenon
we should resolve preferentially from the three
points of view: the number of errors, the number
of ?independent? errors and its ?correction effect.?
Considering these points, ?Prepositional attach-
ment? seems most preferable for handling first.
4.3.2 Possibilities for further analysis
Since the errors for the phenomenon were system-
atically collected with our descriptive approach,
we can work on further focused error analyses
which would answer such questions as ?Which
preposition causes most errors in attachments??,
?Which pair of a correct answer and an erroneous
output for predicate argument relations can occur
most frequently??, and so on. Our descriptive ap-
proach would enable us to thoroughly obtain such
analyses with more closely-defined patterns. In
addition, our empirical approach would clarify the
influences of the obtained error properties on the
parser?s behaviors. The results of the focused anal-
yses might reasonably lead us to the features that
can be captured as parameters for model training,
or policies for re-ranking the parse candidates.
The combination of our approaches would give
us interesting clues for planning effective strate-
gies for improving the parser. Our challenges for
combining the two approaches are now in the pre-
liminary stage and there would be many possibili-
ties for further detailed analysis.
5 Related work
Although there have been many researches which
analyzed errors on their own systems in the part of
the experiments, there have been few researches
which focused mainly on error analysis itself.
In the field of parsing, McDonald and Nivre
(2007) compared parsing errors between graph-
based and transition-based parsers. They observed
the accuracy transitions from various points of
view, and the obtained statistical data suggested
that error propagation seemed to occur in the
graph structures of parsing outputs. Our research
proceeded for one step in this point, and attempted
to reveal the way of the propagations. In exam-
ining the combination of the two types of pars-
ing, McDonald and Nivre (2007) utilized similar
approaches to our empirical analysis. They al-
lowed a parser to give only structures given by
the parsers. They implemented the ideas for eval-
uating the parser?s potentials whereas we imple-
mented the ideas for observing error propagations.
Dredze et al (2007) showed the possibility
that many parsing errors in the domain adaptation
tasks came from inconsistencies between annota-
tion manners of training resources. Such findings
would further suggest that, comparing given errors
without considering the inconsistencies could lead
to the misunderstanding of what occurs in domain
transitions. The summarized error dependencies
given by our approaches would be useful clues for
extracting such domain-dependent error phenom-
ena.
Gime?nez and Ma`rquez (2008) proposed an au-
tomatic error analysis approach in machine trans-
lation (MT) technologies. They were developing
a metric set which could capture features in MT
outputs at different linguistic levels with different
levels of granularity. As we considered the parsing
systems, they explored the way to resolve costly
and non-rewarding error analysis in the MT field.
One of their objectives was to enable researchers
to easily access detailed linguistic reports on their
systems and to concentrate only on analyses for
the system improvements. From this point of view,
our research might provide an introduction into
such rewarding analysis in parsing.
6 Conclusions
We proposed empirical and descriptive approaches
to extracting dependencies among parsing errors.
In the experiments, with each of our approaches,
we successfully obtained relevant errors. More-
over, the possibility was shown that the combina-
tion of our approaches would give a more detailed
error analysis which would bring us useful clues
for parser improvements.
In our future work, we will improve the per-
formance of our approaches by adding more pat-
terns for the descriptive approach and by handling
uncorrectable errors for the empirical approach.
With the obtained robust information, we will ex-
plore rewarding ways for parser improvements.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
1170
References
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Joa?o V. Grac?a, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proceedings of the
CoNLL Shared Task Session of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 1051?1055.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards
heterogeneous automatic MT error analysis. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
pages 1894?1901.
Ronald M. Kaplan and Joan Bresnan. 1995. Lexical-
functional grammar: A formal system for gram-
matical representation. Formal Issues in Lexical-
Functional Grammar, pages 29?130.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
ARPA Human Language Technology Workshop.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 83?90.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 155?163.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
1171
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1328?1337,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Supervised Learning of a Probabilistic Lexicon of Verb Semantic Classes
Yusuke Miyao
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
University of Tokyo
University of Manchester
National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
tsujii@is.s.u-tokyo.ac.jp
Abstract
The work presented in this paper explores
a supervised method for learning a prob-
abilistic model of a lexicon of VerbNet
classes. We intend for the probabilis-
tic model to provide a probability dis-
tribution of verb-class associations, over
known and unknown verbs, including pol-
ysemous words. In our approach, train-
ing instances are obtained from an ex-
isting lexicon and/or from an annotated
corpus, while the features, which repre-
sent syntactic frames, semantic similarity,
and selectional preferences, are extracted
from unannotated corpora. Our model
is evaluated in type-level verb classifica-
tion tasks: we measure the prediction ac-
curacy of VerbNet classes for unknown
verbs, and also measure the dissimilarity
between the learned and observed proba-
bility distributions. We empirically com-
pare several settings for model learning,
while we vary the use of features, source
corpora for feature extraction, and disam-
biguated corpora. In the task of verb clas-
sification into all VerbNet classes, our best
model achieved a 10.69% error reduction
in the classification accuracy, over the pre-
viously proposed model.
1 Introduction
Lexicons are invaluable resources for semantic
processing. In many cases, lexicons are neces-
sary to restrict a set of semantic classes to be as-
signed to a word. In fact, a considerable number of
works on semantic processing implicitly or explic-
itly presupposes the availability of a lexicon, such
as in word sense disambiguation (WSD) (Mc-
Carthy et al, 2004), and in token-level verb class
disambiguation (Lapata and Brew, 2004; Girju et
al., 2005; Li and Brew, 2007; Abend et al, 2008).
In other words, those methods are heavily de-
pendent on the availability of a semantic lexicon.
Therefore, recent research efforts have invested in
developing semantic resources, such as WordNet
(Fellbaum, 1998), FrameNet (Baker et al, 1998),
and VerbNet (Kipper et al, 2000; Kipper-Schuler,
2005), which greatly advanced research in seman-
tic processing. However, the construction of such
resources is expensive, and it is unrealistic to pre-
suppose the availability of full-coverage lexicons;
this is the case because unknown words always ap-
pear in real texts, and word-semantics associations
may vary (Abend et al, 2008).
This paper explores a method for the supervised
learning of a probabilistic model for the VerbNet
lexicon. We target the automatic classification of
arbitrary verbs, including polysemous verbs, into
all VerbNet classes; further, we target the esti-
mation of a probabilistic model, which represents
the saliences of verb-class associations for polyse-
mous verbs. In our approach, an existing lexicon
and/or an annotated corpus are used as the training
data. Since VerbNet classes are designed to rep-
resent the distinctions in the syntactic frames that
verbs can take, features, representing the statistics
of syntactic frames, are extracted from the unan-
notated corpora. Additionally, as the classes rep-
resent semantic commonalities, semantically in-
spired features, like distributionally similar words,
are used. These features can be considered as a
generalized representation of verbs, and we ex-
pect that the obtained probabilistic model predicts
VerbNet classes of the unknown words.
Our model is evaluated in two tasks of type-
level verb classification: one is the classification
of monosemous verbs into a small subset of the
classes, which was studied in some previous works
(Joanis and Stevenson, 2003; Joanis et al, 2008).
The other task is the classification of all verbs into
the full set of VerbNet classes, which has not yet
1328
been attempted. In the experiments, training in-
stances are obtained from VerbNet and/or Sem-
Link (Loper et al, 2007), while features are ex-
tracted from the British National Corpus or from
Wall Street Journal. We empirically compare sev-
eral settings for model learning by varying the
set of features, the source domain and the size
of a corpus for feature extraction, and the use of
the token-level statistics obtained from a manually
disambiguated corpus. We also provide the anal-
ysis of the remaining errors, which will lead us to
further improve the supervised learning of a prob-
abilistic semantic lexicon.
Supervised methods for automatic verb classifi-
cation have been extensively investigated (Steven-
son et al, 1999; Stevenson and Merlo, 1999;
Merlo and Stevenson, 2001; Stevenson and Joa-
nis, 2003; Joanis and Stevenson, 2003; Joanis et
al., 2008). However, their focus has been lim-
ited to a small subset of verb classes, and a lim-
ited number of monosemous verbs. The main con-
tributions of the present work are: i) to provide
empirical results for the automatic classification
of all verbs, including polysemous ones, into all
VerbNet classes, and ii) to empirically explore the
effective settings for the supervised learning of a
probabilistic lexicon of verb semantic classes.
2 Background
2.1 Verb lexicon
Levin?s (1993) work on verb classification has
broadened the field of computational research that
concerns the relationships between the syntactic
and semantic structures of verbs. The principal
idea behind the work is that the meanings of verbs
can be identified by observing possible syntactic
frames that the verbs can take. In other words,
with the knowledge of syntactic frames, verbs can
be semantically classified. This idea provided the
computational linguistics community with crite-
ria for the definition and the classification of verb
semantics; it has subsequently resulted in the re-
search of the induction of verb classes (Korhonen
and Briscoe, 2004), and the construction of a verb
lexicon based on Levin?s criteria.
VerbNet (Kipper et al, 2000; Kipper-Schuler,
2005) is a lexicon of verbs organized into classes
that share the same syntactic behaviors and seman-
tics. The design of classes originates from Levin
(1993), though the design has been considerably
reorganized and extends beyond the original clas-
43 Emission
43.1 Light Emission
beam, glow, sparkle, . . .
43.2 Sound Emission
blare, chime, jangle, . . .
. . .
44 Destroy
annihilate, destroy, ravage, . . .
45 Change of State
. . .
47 Existence
47.1 Exist
exist, persist, remain, . . .
47.2 Entity-Specific Modes Being
bloom, breathe, foam, . . .
47.3 Modes of Being with Motion
jiggle, sway, waft, . . .
. . .
Figure 1: VerbNet classes
43.2 Sound Emission
Theme V
Theme V P:loc Location
P:loc Location V Theme
there V Theme P:loc Location
Agent V Theme
Theme V Oblique
Location V with Theme
47.3 Modes of Being with Motion
Theme V
Theme V P:loc Location
P:loc Location V Theme
there V Theme
Agent V Theme
Figure 2: Syntactic frames for VerbNet classes
sification. The classes therefore cover more En-
glish verbs, and the classification should be more
consistent (Korhonen and Briscoe, 2004; Kipper
et al, 2006).
The current version of VerbNet includes 270
classes.1 Figure 1 shows a part of the classes of
VerbNet. The top-level categories, e.g. Emis-
sion and Destroy, represent a coarse classifica-
tion of verb semantics. They are further classi-
fied into verb classes, each of which expresses
a group of verbs sharing syntactic frames. Fig-
ure 2 shows an excerpt from VerbNet, which rep-
resents the possible syntactic frames for the Sound
Emission class, including ?chime? and ?jangle,?
and the Modes of Being with Motion class, in-
cluding ?jiggle? and ?waft.? In this figure, each
line represents a syntactic frame, where Agent,
1Throughout this paper, we refer to VerbNet 2.3. Sub-
classes are ignored in this work, following the setting of
Abend et al (2008).
1329
. . . the walls still shook;VN=47.3 and an evacuation
alarm blared;VN=43.2 outside.
Suddenly the woman begins;VN=55.1 swaying
;VN=47.3 and then . . .
Figure 3: An excerpt from SemLink
Theme, and Location indicate the thematic
roles, V denotes a verb, and P specifies a prepo-
sition. P:loc defines locative prepositions such
as: ?in? and ?at.? For example, the second syn-
tactic frame of Sound Emission, i.e., Theme V
P:loc Location, corresponds to the follow-
ing sentence:
1. The coins jangled in my pocket.
Theme corresponds to ?the coins,? V to ?jangled,?
P:loc to ?in,? and Location to ?my pocket.?
While VerbNet provides associations between
verbs and semantic classes, SemLink (Loper et
al., 2007) additionally provides mappings among
VerbNet, FrameNet (Baker et al, 1998), PropBank
(Palmer et al, 2005), and WordNet (Fellbaum,
1998). Since FrameNet and PropBank include an-
notated instances of sentences, SemLink can be
considered as a corpus annotated with VerbNet
classes. Figure 3 presents some annotated sen-
tences obtained from SemLink. For example, the
annotation ?blared;VN=43.2? indicates that the
occurrence of ?blare? in this context is classified
as Sound Emission.
2.2 Related work
There has been much research effort invested in
the automatic classification of verbs into lexical
semantic classes, in a supervised or unsupervised
way. The present work inherits the spirit of the su-
pervised approaches to verb classification (Steven-
son et al, 1999; Stevenson and Merlo, 1999;
Merlo and Stevenson, 2001; Stevenson and Joanis,
2003; Joanis and Stevenson, 2003; Joanis et al,
2008). Our learning framework basically follows
the above listed works: features are obtained from
an unannotated (automatically parsed) corpus, and
gold verb-class associations are used as training
instances for machine learning classifiers, such as
decision trees and support vector machines. How-
ever, those works targeted a small subset of Levin
classes, and a limited number of monosemous
verbs; for example, Merlo and Stevenson (2001)
studied three classes and 59 verbs, and Joanis et al
(2008) focused on 14 classes and 835 verbs. Al-
though these works provided a theoretical frame-
work for supervised verb classification, their re-
sults were not readily available for practical ap-
plications, because of the limitation in the cover-
age of the targeted classes/verbs on real texts. On
the contrary, we target the classification of arbi-
trary verbs, including polysemous verbs, into all
VerbNet classes (270 in total). In this realistic sit-
uation, we will empirically compare settings for
model learning, in order to explore effective con-
ditions to obtain better models.
Another difference from the aforementioned
works is that we aim at obtaining a probabilis-
tic model, which represents saliences of classes
of polysemous verbs. Lapata and Brew (2004)
and Li and Brew (2007) focused on this issue,
and described methods for inducing probabilities
of verb-class associations. The obtained proba-
bilistic model was intended to be incorporated into
a token-level disambiguation model. Their meth-
ods claimed to be unsupervised, meaning that the
induction of a probabilistic lexicon did not re-
quire any hand-annotated corpora. In fact, how-
ever, their methods relied on the existence of a
full-coverage lexicon, both in training and running
time. In their methods, a lexicon was necessary
for restricting possible classes to which each word
belongs. Since most verbs are associated with
only a couple of classes, such a restriction signif-
icantly reduces the search space, and the problem
becomes much easier to solve. This presupposi-
tion is implicitly or explicitly used in other seman-
tic disambiguation tasks (McCarthy et al, 2004),
but it is unrealistic for practical applications.
Clustering methods have also been extensively
researched for verb classification (Stevenson and
Merlo, 1999; Schulte im Walde, 2000; McCarthy,
2001; Korhonen, 2002; Korhonen et al, 2003;
Schulte im Walde, 2003). The extensive research
is in large part due to the intuition that the set of
classes could not be fixed beforehand. In partic-
ular, it is often problematic to define a static set
of semantic classes. However, it is reasonable to
assume that the set of VerbNet classes is fixed, be-
cause Levin-type classes are more static than on-
tological classes, like in WordNet synsets. There-
fore, we can apply supervised classification meth-
ods to our task. It is true that the current VerbNet
classes are imperfect and require revisions, but in
this work we adopt them as they are, because as
1330
time advances, more stable classifications will be-
come available.
The problem focused in this work has a close re-
lationship with automatic thesaurus/ontology ex-
pansion. In fact, we evaluate our method in the
task of automatic verb classification, which can
be considered as lexicon expansion. The most
prominent difference of the present work from the-
saurus/ontology expansion is that the number of
classes is much smaller in our problem, and the set
of verb classes can be assumed to be fixed. These
characteristics indicate that our problem is easier
and more well-defined than is the case for auto-
matic thesaurus/ontology expansion.
Supervised approaches to token-level verb class
disambiguation have recently been addressed
(Girju et al, 2005; Abend et al, 2008), largely ow-
ing to SemLink. Their approaches fundamentally
follow traditional supervised WSD methods: ex-
tracting features representing the context in which
the target word appears, and training a classifica-
tion model with an annotated corpus. While those
works achieved an impressive accuracy (more than
95%), the results may not necessarily indicate the
method?s effectiveness; rather, it may imply the
importance of a lexicon. In fact, these works re-
strict their target to verb tokens, in which the cor-
rect class exists in a given lexicon, and they only
consider candidate classes that are registered in the
lexicon. This setting reduces the ambiguity signif-
icantly, and the problem becomes much easier to
handle; for example, approximately half of verb
tokens are monosemous in their setting. Thus, a
simple baseline achieves very high accuracy fig-
ures. However, in our preliminary experiment
on token-level verb classification with unknown
verbs, we found that the accuracy for unknown
verbs (i.e., lemmas not included in the VerbNet
lexicon) is catastrophically low. This indicates
that VerbNet and SemLink are insufficient for un-
known verbs, and that we cannot expect the avail-
ability of a full-coverage lexicon in the real world.
Instead of a static lexicon, our probabilistic model
is intended to be used as a prior distribution for the
token-level disambiguation, as in Lapata and Brew
(2004)?s model.
3 A probabilistic model for verb
semantic classes
In this work, supervised learning is applied to the
probabilistic modeling of a lexicon of verb seman-
tic classes. We do not presuppose the existence of
a full-coverage lexicon; instead, we use an existing
lexicon for the training data. Combined with fea-
tures extracted from unannotated corpora, a proba-
bilistic model is learned from the existing lexicon.
Like other supervised learning applications, our
probabilistic lexicon can predict classes for words
that are not included in the original lexicon.
Our model is defined in the following way. We
assume that the set, C, of verb classes is fixed,
while a set of verbs is unfixed. With this assump-
tion, probabilistic modeling can be reduced to a
classification problem. Specifically, the goal is to
obtain a probability distribution, p(c|v), of verb
class c ? C for a given verb (lemma) v. We
can therefore apply well-known supervised learn-
ing methods to estimate p(c|v).
This probability is modeled in the form of a log-
linear model.
p(c|v) =
1
Z
exp
(
?
i
?
i
f
i
(c, v)
)
,
where f
i
(c, v) are features that represent charac-
teristics of c and v, and ?
i
are model parameters
that express weights of the corresponding features.
Model parameters can be estimated when train-
ing instances, i.e., pairs ?c, v?, and features,
f
i
(c, v), for each instance are given. Therefore,
what we have to do is to prepare the training in-
stances ?c, v?, and effective features f
i
(c, v) that
contribute to the better estimation of probabili-
ties. In token tagging tasks, both training instances
and features are extracted from annotated corpora.
However, since our goal is the probabilistic mod-
eling of a lexicon, we have to determine how to
derive the training instances and features for lexi-
con entries, to be discussed in the next section.
For the parameter estimation of log-linear mod-
els, we applied the stochastic gradient descent
method. A hyperparameter for l
2
-regularization
was tuned to minimize the KL-divergence (see
Section 4.4) for the development set.
4 Experiment design
In this work, we empirically compare several set-
tings for the learning of the above probabilistic
model, in the two tasks of automatic verb classi-
fication. In what follows, we explain the train-
ing/test data, corpora for extracting features, and
the design of the features and evaluation tasks.
The measures for evaluation are also introduced.
1331
1 sound_emission-43.2 chime
0.5 sound_emission-43.2 blare
0.5 manner_speaking-37.3 blare
0.5 modes_of_being_with_motion-47.3 sway
0.5 urge-58.1 sway
1 sound_emission-43.2 chime
0.7 sound_emission-43.2 blare
0.3 manner_speaking-37.3 blare
0.6 modes_of_being_with_motion-47.3 sway
0.4 urge-58.1 sway
Figure 4: Training instances obtained from Verb-
Net (upper) and VerbNet+SemLink (lower)
4.1 Data
As our goal is the supervised learning of a lexicon
of verb semantic classes, VerbNet is used as the
training/test data. In addition, since we aim at rep-
resenting the saliences of verb-class associations
with probabilities, the gold probabilities are nec-
essary. For this purpose, we count the occurrences
of each verb-class association in the VerbNet-
PropBank token mappings in the subset of the
SemLink corresponding to sections 2 through 21
of Penn Treebank (Marcus et al, 1994). Fre-
quency counts are normalized for each lemma,
with the Laplace smoothing (the parameter is 0.5).
In this work, we compare the two settings for
creating training instances. By comparing the re-
sults of these settings, we evaluate the necessity
of an annotated corpus for learning a probabilistic
lexicon of verb semantic classes.
VerbNet We collect all ?c, v? pairs registered in
VerbNet. For each v, all of the associated
classes are assigned equal weights (see the
upper part of Figure 4).
VerbNet+SemLink Each pair ?c, v? in VerbNet
is weighted by the normalized frequency ob-
tained from SemLink (see the lower part of
Figure 4).
Because VerbNet classes represent groups of
syntactic frames, and it is impossible to guess the
verb class by referring to only one occurrence in
a text, it is necessary to have statistics over a suf-
ficient amount of a corpus. Hence, features are
extracted from a large unannotated corpus. In this
paper, we use the following two corpora:
WSJ Wall Street Journal newspaper articles
(around 40 million words).
BNC British National Corpus, which is a bal-
anced corpus of around 100 million words.
In addition to the variance of the corpus domains,
we vary the size of the corpus to observe the ef-
fect of increasing the corpus size. These corpora
are automatically parsed by Enju 2.3.1 (Miyao and
Tsujii, 2008), and the features are extracted from
the parsing results.
4.2 Features
Levin-like classes, including VerbNet, are de-
signed to represent distinctions in syntactic frames
and alternations. Hence, if we were given the per-
fect knowledge of the possible syntactic frames,
verbs can be classified into the correct classes al-
most perfectly (Dorr and Jones, 1996). Previ-
ous works thus proposed features that express the
corpus statistics of syntactic frames. However,
class boundaries are subtle in some cases; several
classes share syntactic frames with each other to a
large extent.
For example, the classes shown in Figure 2 have
very similar syntactic frames. The difference is in-
dicated in the last two frames of Sound Emission,
although they appear much less frequently in real
texts. Therefore, it is difficult to accurately capture
the distinctions between these classes, if we are
only provided with the statistics of the syntactic
frames that appear in real texts. In this case, how-
ever, it is easy to observe that the verbs of these
classes have different selectional preferences; that
is, the Theme of Sound Emission verbs would
be objects that make sounds, while the Theme of
Modes of Being with Motion is likely to be ob-
jects that move.2 Although Levin?s classification
initially focused on syntactic alternations, the re-
sulting classes represent some semantic common-
alities. Hence, it would be reasonable to design
features that capture such semantic characteristics.
In this work, we re-implemented the following
features proposed by Joanis et al (2008) as the
starting point.
Syntactic slot Features to count the occurrences
of each syntactic slot, such as subject, ob-
ject, and prepositional phrases. For the sub-
ject slot, we also count its transitive and in-
transitive usages separately. Additionally, we
count the appearances of reflexive pronouns
and semantically empty constituents (it and
2Syntactic frames in VerbNet include specifications of se-
lectional preferences, such as animate and place, although
we do not explicitly use them, because it is not apparent to
determine the members of these semantic classes.
1332
Syntactic slot subj:0.885
intrans-subj:0.578
Slot overlap overlap-subj-obj:0.299
overlap-obj-in:0.074
Tense, voice, aspect pos-VBG:0.307
pos-VBD:0.290
Animacy anim-subj:0.244
anim-obj:0.057
Slot POS subj-PRP:0.270
subj-NN:0.270
Syntactic frame NP_V:0.326
NP_V_NP:0.307
Similar word sim-rock:0.090
sim-swing:0.083
Slot class subj-C82:0.219
obj-C12:0.081
Figure 5: Example of features for ?sway?
there). Differently from Joanis et al (2008),
we consider non-nominal arguments, such as
sentential and adjectival complements.
Slot overlap Features to measure the overlap in
words (lemmas) between two syntactic slots
of the verb. They are intended to approxi-
mate argument alternations, such as the erga-
tive alternation. For example, for the alter-
nation ?The sky cleared?/?The clouds cleared
from the sky,? a feature to indicate the overlap
between the subject slot and the from slot is
added (Joanis et al, 2008). The value of this
feature is computed by the method of Merlo
and Stevenson (2001).
Tense, voice, aspect Features to approximate the
tendency of the tense, voice, and aspect of
the target verb. The Penn Treebank POS tags
for verbs (VB, VBP, VBZ, VBG, VBD, and
VBN) are counted. In addition, included are
the frequency of the co-occurrences with an
adverb or an auxiliary verb, and the count of
usages as a noun or an adjective.
Animacy Features to measure the frequency of
animate arguments for each syntactic slot.
Personal pronouns except it are counted as
animate, following Joanis et al (2008), while
named entity recognition was not used.
Examples of these features are shown in Figure 5.
For details, refer to Joanis et al (2008).
The above features mainly represent syntactic
behaviors of target verbs. Since our target classes
are broader than in the previous works, we further
enhance the syntactic features. Additionally, as
discussed above, semantically motivated features
may present strong clues to distinguish among
syntactically similar classes. We therefore include
the following four types of feature; the first two
are syntactic, while the other two are intended to
capture semantic characteristics:
Slot POS In addition to the syntactic slot fea-
tures, we add features that represent a com-
bination of a syntactic slot and the POS of
its head word. Since VerbNet includes ex-
tended classes that take verbal and adjecti-
val arguments, the POSs of arguments would
provide a strong clue to discriminate among
these syntactic frames.
Syntactic frame The number of arguments and
their syntactic categories. This feature was
mentioned as a baseline in Joanis et al
(2008), but we include it in our model.
Similar word Similar words (lemmas) to the tar-
get verb. Similar words are automatically
obtained from a corpus (the same corpus as
used for feature extraction) by Lin (1998)?s
method. This feature is motivated by the
hypothesis that distributionally similar words
tend to be classified into the same class. Be-
cause Lin?s method is based on the similar-
ity of words in syntactic slots, the obtained
similar words are expected to represent a verb
class that share selectional preferences.
Slot class Semantic classes of the head words of
the arguments. This feature is also intended
to approximate selectional preferences. The
semantic classes are obtained by clustering
nouns, verbs, and adjectives into 200, 100,
and 50 classes respectively, by using the k-
medoid method with Lin (1998)?s similarity.
Figure 5 shows an example of the features for
?sway,? extracted from the BNC corpus.3 Feature
values are defined as relative frequencies for each
lemma; while, for similar word features, feature
values are weighted by Lin?s similarity measure.
4.3 Tasks
We evaluate our model in the tasks of auto-
matic verb classification (a.k.a. lexicon expan-
sion): given gold verb-class associations for some
set of verbs, we predict the classes for unknown
3
?C82? and ?C12? are automatically assigned cluster
names.
1333
Verb class Levin class number
Recipient 13.1, 13.3
Admire 31.2
Amuse 31.1
Run 51.3.2
Sound Emission 43.2
Light and Substance Emission 43.1, 43.4
Cheat 10.6
Steal and Remove 10.5, 10.1
Wipe 10.4.1, 10.4.2
Spray/Load 9.7
Fill 9.8
Other Verbs of Putting 9.1?6
Change of State 45.1?4
Object Drop 26.1, 26.3, 26.7
Table 1: 14 classes used in Joanis et al (2008) and
their corresponding Levin class numbers
verbs. While our main target is the full set of Verb-
Net classes, we also show results for the task stud-
ied in the previous work.
14-class task The task to classify (almost)
monosemous verbs into 14 classes. Refer to
Table 1 for the definition of the 14 classes.
Following Joanis et al (2008)?s task def-
inition, we removed verbs that belong to
multiple classes in these 14 classes, and also
removed overly polysemous verbs (in our
experiment, verb-class associations that have
the relative frequency that is less than 0.5
in SemLink are removed). For each class,
member verbs are randomly split into 50%
(training), 25% (development), and 25%
(final test) sets.
All-class task The task to classify all target verbs
into 268 classes.4 Any verbs that did not
occur at least 100 times in the BNC cor-
pus were removed.5 The remaining verbs
(2517 words) are randomly split into 80%
(training), 10% (development), and 10% (fi-
nal test) sets, under the constraint that at least
one instance for each class is included in the
training set.6
4.4 Evaluation measures
For the 14-class task, we simply measure the clas-
sification accuracy. However, the evaluation in the
4Two classes (Being Dressed and Debone) are not used in
the experiments because no lemmas belonged to these classes
after filtering by the frequency in BNC.
5This is the same preprocessing as Joanis et al (2008),
although we use VerbNet, while Joanis et al (2008) used the
original Levin classifications.
6Because polysemous verbs belong to multiple classes,
the class-wise data split was not adopted for the all-class task.
all-class task is not trivial, because verbs may be
assigned multiple classes.
Since our purpose is to obtain a probabilistic
model rather than to classify monosemous verbs,
the evaluation criterion should be sensitive to the
probabilistic distribution on the test data. In this
paper, we adopt two evaluation measures. One
is the top-N weighted accuracy; we count the
number of correct pairs ?c, v? in the N -best out-
puts from the model (where N is the number of
gold classes for each lemma), where each count is
weighted by the relative frequency (i.e., the counts
in SemLink) of the pair in the test set. For exam-
ple, in the case for ?blare? in Figure 4, if the model
states that Sound Emission has the largest prob-
ability, we get 0.7 points. If Manner Speaking
has the largest probability, we instead obtain 0.3
points. Intuitively, the score is higher when the
model presents larger probabilities to classes with
higher relative frequencies. This measure is simi-
lar to the top-N precision in information retrieval;
it evaluates the ranked output by the model. It
is intuitively interpretable, but is insufficient for
evaluating the quality of probability distributions.
The other measure is KL-divergence, which is
popularly used for measuring the dissimilarity be-
tween two probability distributions. This is de-
fined as follows:
KL(p||q) =
?
x
p(x) log(p(x))? p(x) log(q(x)).
In the experiments, this measure is applied, with
the assumption that p is the relative frequency
of ?c, v? in the test set, and that q is the esti-
mated probability distribution. Although the KL-
divergence is not a true distance metric, it is suf-
ficient for measuring the fitting of the estimated
model to the true distribution. We report the
KL-divergence averaged over all verbs in the test
set. Since this measure indicates a dissimilarity, a
smaller value is better. When p and q are equiva-
lent, KL(p||q) = 0.
5 Experimental results
Table 2 shows the accuracy obtained for the 14-
class task. The first column denotes the incorpo-
rated features (?Joanis et al?s features? or ?All fea-
tures?), and the sources of the features (?WSJ? or
?BNC?). The two baseline results are also given:
?Baseline (random)? indicates that classes are ran-
domly output, and ?Baseline (majority)? indicates
1334
Accuracy
Baseline (random) 7.14
Baseline (majority) 26.47
Joanis et al?s features/WSJ 56.86
Joanis et al?s features/BNC 64.22
All features/WSJ 60.29
All features/BNC 68.14
Table 2: Accuracy for the 14-class task
Accuracy KL
Baseline (random) 0.37 ?
Baseline (majority) 8.69 ?
Joanis et al?s features/WSJ 30.26 3.65
Joanis et al?s features/BNC 35.66 3.32
All features/WSJ 34.07 3.37
All features/BNC 42.54 2.99
Table 3: Accuracy and KL-divergence for the all-
class task (the VerbNet+SemLink setting)
that the majority class (i.e., the class that has the
largest number of member verbs) is output to every
lemma. While these figures cannot be compared
directly to the previous works due to the difference
in the preprocessing, Joanis et al (2008) achieved
58.4% accuracy for the 14-class task. Table 3 and
4 present the results for the all-class task. Table 3
gives the accuracy and KL-divergence achieved
by the model trained with the VerbNet+SemLink
training instances, while Table 4 presents the same
measures by the training instances created from
VerbNet only.
Our models performed substantially better on
both tasks than the baseline models. The results
also proved that the features we proposed in this
paper contributed to the further improvement of
the model from Joanis et al (2008). In the all-class
task with the VerbNet+SemLink setting, our fea-
tures achieved 10.69% error reduction in the accu-
racy over Joanis et al (2008)?s features. Another
interesting fact is that the model with BNC con-
sistently outperformed the model with WSJ. This
outcome is somewhat surprising, provided that the
relative frequencies in the training/test sets are cre-
ated from the WSJ portion of SemLink. The rea-
son for this is independent of the corpus size, as
will be shown below. When comparing Table 3
and 4, we can see that using SemLink statistics
resulted in a slightly better model. This result
is predictable, because the evaluation measures
are sensitive to the relative frequencies estimated
from SemLink. However, the difference remained
small. In both of the tasks and the evaluation mea-
sures, the best model was achieved when we use
Accuracy KL
Baseline (random) 0.37 ?
Baseline (majority) 8.69 ?
Joanis et al?s features/WSJ 29.65 3.67
Joanis et al?s features/BNC 35.78 3.34
All features/WSJ 34.53 3.40
All features/BNC 42.38 3.02
Table 4: Accuracy and KL-divergence for the all-
class task (the VerbNet only setting)
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100
Acc
ura
cy
Corpus size (M words)
Accuracy (Joanis et al?s features, WSJ)Accuracy (Joanis et al?s features, BNC)Accuracy (all features, WSJ)Accuracy (all features, BNC)
Figure 6: Corpus size vs. accuracy
all the features extracted from BNC, and create
training instances from VerbNet+SemLink.
Figure 6 and 7 plot the accuracy and KL-
divergence against the size of the unannotated cor-
pus used for feature extraction. The result clearly
indicates that the learning curve still grows at the
corpus size with 100 million words (especially for
the all features + BNC setting), which indicates
that better models are obtained by increasing the
size of the unannotated corpora.
Therefore, we can claim that the differences be-
tween the domains and the size of the unannotated
corpora are more influential than the availability of
the annotated corpora. This indicates that learning
only from a lexicon would be a viable solution,
when a token-disambiguated corpus like SemLink
is unavailable.
Table 5 shows the contribution of each feature
group. BNC is used for feature extraction, and
VerbNet+SemLink is used for the creation of train-
ing instances. The results demonstrated the effec-
tiveness of the slot POS features, and in particular,
for the all-class task, most likely because Verb-
Net covers verbs that take non-nominal arguments.
Additionally, the similar word features contributed
equally or more in both of the tasks. This result
suggests that we were reasonable in hypothesizing
that distributionally similar words tend to be clas-
1335
 2.5
 3
 3.5
 4
 4.5
 0  20  40  60  80  100
KL-
dive
rge
nce
Corpus size (M words)
KL (Joanis et al?s features, WSJ)KL (Joanis et al?s features, BNC)KL (all features, WSJ)KL (all features, BNC)
Figure 7: Corpus size vs. KL-divergence
14-classes All classes
Accuracy Accuracy KL
Baseline (random) 7.14 0.37 ?
Baseline (majority) 26.47 8.69 ?
Joanis et al?s features 64.22 35.66 3.32
+ Slot POS 66.67 38.77 3.18
+ Syntactic frame 64.71 35.99 3.29
+ Similar word 68.14 37.88 3.10
+ Slot class 64.71 36.51 3.26
All features 68.14 42.54 2.99
Table 5: Contribution of features
sified into the same class. Slot classes also con-
tributed to a slight improvement, indicating that
selectional preferences are effective clues for pre-
dicting VerbNet classes. The result of the ?All fea-
tures? model for the all-class task attests that these
features worked collaboratively, and using them
all resulted in a considerably better model.
From the analysis of the confusion matrix for
the outputs by our best model, we identified sev-
eral reasons for the remaining misclassification er-
rors. A major portion of the errors were caused by
confusing the classes that take the same preposi-
tions. Examples of these errors include:
? Other Change of State verbs were misclas-
sified into the Butter class: ?embalm,? ?lam-
inate.? (they take ?with? phrases)
? Judgement verbs were misclassified into the
Characterize class: ?acclaim,? ?hail.? (they
take ?as? phrases)
Since prepositions are strong features for auto-
matic verb classification (Joanis et al, 2008), the
classes that take the same prepositions remained
confusing. The discovery of the features to dis-
criminate among these classes would be crucial for
further improvement.
Another major error is in classifying verbs into
Other Change of State. Examples include:
? Amuse verbs: ?impair,? ?recharge.?
? Herd verbs: ?aggregate,? ?mass.?
Because Other Change of State is one of the
biggest classes, supervised learning tends to place
a high probability to this class. Therefore, when
strong clues do not exist, verbs tend to be mis-
classified into this class. In addition, this class is
not syntactically/semantically homogeneous, and
is likely to introduce noise in the machine learn-
ing classifier. A possible solution to this problem
would be to exclude this class from the classifica-
tion, and to process the class separately.
6 Conclusions
We presented a method for the supervised learn-
ing of a probabilistic model for a lexicon of Verb-
Net classes. By combining verb-class associa-
tions from VerbNet and SemLink, and features ex-
tracted from a large unannotated corpus, we could
successfully train a log-linear model in a super-
vised way. The experimental results attested to
our success that features proposed in this paper
worked effectively in obtaining a better probabil-
ity distribution. Not only syntactic features, but
also semantic features were shown to be effective.
While each of these features could increase the ac-
curacy, they collaboratively contributed to a large
improvement. In the all-class task, we obtained
10.69% error reduction in the classification accu-
racy over Joanis et al (2008)?s model. We also ob-
served the trend that a larger corpus for feature ex-
traction led to a better model, indicating that a bet-
ter model will be obtained by increasing the size of
an unannotated corpus.
We could identify the effective features and set-
tings for this problem, but the classification into
all VerbNet classes remained challenging. One
possible direction for this research topic would be
to use our model for the semi-automatic construc-
tion of verb lexicons, with the help of human cura-
tion. However, there is also a demand for explor-
ing other types of features that can discriminate
among confusing classes.
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research and Grant-
in-Aid for Young Scientists (MEXT, Japan).
1336
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2008.
A supervised algorithm for verb disambiguation into
VerbNet classes. In Proceedings of COLING 2008,
pages 9?16.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL 1998.
Bonnie J. Dorr and Doug Jones. 1996. Role of word
sense disambiguation in lexical acquisition: Predict-
ing semantics from syntactic cues. In Proceedings
of COLING-96, pages 322?327.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Roxana Girju, Dan Roth, and Mark Sammons. 2005.
Token-level disambiguation of VerbNet classes. In
The Interdisciplinary Workshop on Verb Features
and Verb Classes.
Eric Joanis and Suzanne Stevenson. 2003. A general
feature space for automatic verb classification. In
Proceedings of EACL 2003, pages 163?170.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic
verb classification. Natural Language Engineering,
14(3):337?367.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of 17th National Conference on Ar-
tificial Intelligence.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending VerbNet with
novel verb classes. In Proceedings of LREC 2006.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. the-
sis, Computer and Information Science Department,
University of Pennsylvania.
Anna Korhonen and Ted Briscoe. 2004. Extended
lexical-semantic classification of English verbs. In
Proceedings of the HLT/NAACL Workshop on Com-
putational Lexical Semantics.
Anna Korhonen, Yuval Krymolowski, and Zvika
Marx. 2003. Clustering polysemic subcategoriza-
tion frame distributions semantically. In Proceed-
ings of ACL 2003.
Anna Korhonen. 2002. Semantically motivated
subcategorization acquisition. In Proceedings of
the Workshop on Unsupervised Lexical Acquisition,
pages 51?58.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?75.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
Juanguo Li and Chris Brew. 2007. Disambiguating
Levin verbs using untagged data. In Proceedings of
RANLP 2007.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL
1998.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics, Tilburg, the Netherlands.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of ACL 2004.
Diana McCarthy. 2001. Lexical Acquisition at the
Syntax-Semantics Interface: Diathesis Alternations,
Subcategorization Frames and Selectional Prefer-
ences. Ph.D. thesis, University of Sussex.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb-classification based on statistical distri-
bution of argument structure. Computational Lin-
guistics, 27(3):373?408.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, 31(1).
Sabine Schulte im Walde. 2000. Clustering verbs se-
mantically according to their alternation behavior.
In Proceedings of COLING 2000, pages 747?753.
Sabine Schulte im Walde. 2003. Experiments on the
choice of features for learning verb classes. In Pro-
ceedings of EACL 2003, pages 315?322.
Suzanne Stevenson and Eric Joanis. 2003. Semi-
supervised verb class discovery using noisy features.
In Proceedings of CoNLL 2003, pages 71?78.
Suzanne Stevenson and Paola Merlo. 1999. Automatic
verb classification using grammatical features. In
Proceedings of EACL 1999, pages 45?52.
Suzanne Stevenson, Paola Merlo, Natalia Kariaeva,
and Kamin Whitehouse. 1999. Supervised learning
of lexical semantic verb classes using frequency dis-
tributions. In Proceedings of SigLex99: Standardiz-
ing Lexical Resources, pages 15?22.
1337
127
128
129
130
Adapting a Probabilistic Disambiguation Model
of an HPSG Parser to a New Domain
Tadayoshi Hara1, Yusuke Miyao1, and Jun?ichi Tsujii1,2,3
1 Department of Computer Science, University of Tokyo,
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
2 CREST, JST (Japan Science and Technology Agency),
Honcho, 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
3 School of Informatics, University of Manchester,
POBox 88, Sackville St, Manchester, M60 1QD, UK
Abstract. This paper describes a method of adapting a domain-inde-
pendent HPSG parser to a biomedical domain. Without modifying the
grammar and the probabilistic model of the original HPSG parser, we
develop a log-linear model with additional features on a treebank of the
biomedical domain. Since the treebank of the target domain is limited, we
need to exploit an original disambiguation model that was trained on a
larger treebank. Our model incorporates the original model as a reference
probabilistic distribution. The experimental results for our model trained
with a small amount of a treebank demonstrated an improvement in
parsing accuracy.
1 Introduction
Natural language processing (NLP) is being demanded in various fields, such
as biomedical research, patent application, and WWW, because an unmanage-
able amount of information is being published in unstructured data, i.e., natural
language texts. To exploit latent information in these, the assistance of NLP
technologies is highly required. However, an obstacle is the lack of portability
of NLP tools. In general, NLP tools specialized to each domain were developed
from scratch, or adapted by considerable human effort. This is because linguistic
resources for each domain, such as a treebank, have not been sufficiently devel-
oped yet. Since dealing with various kinds of domains is an almost intractable
job, sufficient resources can not be expected.
The method presented in this paper is the development of disambiguation
models of an HPSG parser by combining a disambiguation model of an original
parser with a new model adapting to a new domain. Although the training of a
disambiguation model of a parser requires a sufficient amount of a treebank, its
construction requires a considerable human effort. Hence, we exploit the original
disambiguation model that was trained with a larger, but domain-independent
treebank. Since the original disambiguation model contains rich information of
general grammatical constraints, we try to use its information in developing a
disambiguation model for a new domain.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 199?210, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
200 T. Hara, Y. Miyao, and J. Tsujii
Our disambiguation model is a log-linear model into which the original disam-
biguation model is incorporated as a reference distribution. However, we cannot
simply estimate this model, because of the problem that has been discussed in
studies of the probabilistic modeling of unification-based grammars [1,2]. That
is, the exponential explosion of parse candidates assigned by the grammar makes
the estimation intractable. The previous studies solved the problem by applying
a dynamic programming algorithm to a packed representation of parse trees. In
this paper, we borrow their idea, and define reference distribution on a packed
structure. With this method, the log-linear model with a reference distribution
can be estimated by using dynamic programming.
In the experiments, we used an HPSG parser originally trained with the
Penn Treebank [3], and evaluated a disambiguation model trained with the GE-
NIA treebank [4], which consisted of abstracts of biomedical papers. First, we
measured the accuracy of parsing and the time required for parameter estima-
tion. For comparison, we also examined other possible models other than our
disambiguation model. Next, we varied the size of a training corpus in order to
evaluate the size sufficient for domain adaptation. Then, we varied feature sets
used for training and examined the parsing accuracy. Finally, we compared the
errors in the parsing results of our model with those of the original parser.
In Section 2, we introduce the disambiguation model of an HPSG parser. In
Section 3, we describe a method of adopting reference distribution for adapting
a probabilistic disambiguation model to a new domain. In Section 4, we examine
our method through experiments on the GENIA treebank.
2 An HPSG Parser
The HPSG parser used in this study is Enju [5]. The grammar of Enju was ex-
tracted from the Penn Treebank [3], which consisted of sentences collected from
The Wall Street Journal [6]. The disambiguation model of Enju was trained
on the same treebank. This means that the parser has been adapted to The
Wall Street Journal, and would be difficult to apply to other domains such
as biomedical papers that include different distribution of words and
their constraints.
In this study, we attempted the adaptation of a probabilistic disambiguation
model by fixing the grammar and the disambiguation model of the original
parser. The disambiguation model of Enju is based on a feature forest model
[2], which is a maximum entropy model [7] on packed forest structure. The
probability, pE(t|s), of producing the parse result t for a given sentence s is
defined as
pE(t|s) =
1
Zs
exp
(
?
i
?ifi(t, s)
)
Zs =
?
t??T (s)
exp
(
?
i
?ifi(t?, s)
)
,
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 201
Fig. 1. Chart for parsing ?he saw a girl with a telescope?
where T (s) is the set of parse candidates assigned to s. The feature function
fi(t, s) represents the characteristics of t and s, while the corresponding model
parameter ?i is its weight. Model parameters were estimated so as to maximize
the log-likelihood of the training data.
Estimation of the above model requires a set of training pairs ?ts, T (s)?, where
ts is the correct parse for the sentence s. While ts is provided by a treebank, T (s)
is computed by parsing each s in the treebank. However, the simple enumeration
of T (s) is impractical because the size of T (s) is exponential to the length of s.
To avoid an exponential explosion, Enju represented T (s) in a packed form of
HPSG parse trees [5]. In chart parsing, partial parse candidates are stored in a
chart, in which phrasal signs are identified and packed into an equivalence class
if they are determined to be equivalent and dominate the same word sequence.
A set of parse trees is then represented as a set of relations among equivalence
classes. Figure 1 shows a chart for parsing ?he saw a girl with a telescope?, where
the modifiee (?saw? or ?girl?) of ?with? is ambiguous. Each feature structure
expresses an equivalence class, and the arrows represent immediate-dominance
relations. The phrase, ?saw a girl with a telescope?, has two ambiguous subtrees
(A in the figure). Since the signs of the top-most nodes are equivalent, they are
packed into the same equivalence class. The ambiguity is represented as two
pairs of arrows that come out of the node.
A packed chart can be interpreted as an instance of a feature forest [2]. A
feature forest represents a set of exponentially-many trees in an ?and/or? graph
of a tractable size. A feature forest is formally defined as a tuple ?C, D, R, ?, ??,
where C is a set of conjunctive nodes, D is a set of disjunctive nodes, R ? C
is a set of root nodes1, ? : D ? 2C is a conjunctive daughter function, and
? : C ? 2D is a disjunctive daughter function.
1 For the ease of explanation, the definition of root node is slightly different from the
original.
202 T. Hara, Y. Miyao, and J. Tsujii
HEAD  prep
MOD  NP
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  prep
MOD  VP
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT
<NP,NP>
he
saw
c1
c3
c2
c4
c5 c6
c8c7
Fig. 2. Packed representation of HPSG parse trees in Figure 1
Figure 2 shows (a part of) the HPSG parse trees in Figure 1 represented
as a feature forest. Square boxes are conjunctive nodes, dotted lines express a
disjunctive daughter function, and solid arrows represent a conjunctive daughter
function.
Based on the definition, parse tree t of sentence s can be represented as the
set of conjunctive nodes in the feature forest. The probability pE(t|s) is then
redefined as
pE(t|s) =
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)
Zs =
?
t??T (s)
exp
(
?
c?t?
?
i
?ifi(c)
)
,
where fi(c) are alternative feature functions assigned to conjunctive nodes c ? C.
By using this redefined probability, a dynamic programming algorithm can be
applied to estimate p(t|T (s)) without unpacking the packed chart [2].
Feature functions in feature forest models are designed to capture the char-
acteristics of a conjunctive node. In HPSG parsing, it corresponds to a tuple of a
mother and its daughters. Enju uses features that are combinations of the atomic
features listed in Table 1. The following combinations are used for representing
the characteristics of the binary/unary rule applications.
fbinary =
?rule,dist,comma,
spanh, symh,wordh, posh, leh,
spann, symn, wordn, posn, len
?
funary = ?rule,sym,word,pos,le?
where suffixh andnmeans a headdaughter anda non-headdaughter, respectively.
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 203
Table 1. Templates of atomic features
rule the name of the applied schema
dist the distance between the head words of the daughters
comma whether a comma exists between daughters and/or inside of daughter phrases
span the number of words dominated by the phrase
sym the symbol of the phrasal category (e.g. NP, VP)
word the surface form of the head word
pos the part-of-speech of the head word
le the lexical entry assigned to the head word
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>HEAD  nounSUBCAT <>
he
transitiveVBD,saw,S,root =f
prep-mod-vpwith,IN,PP,3,
,transitiveVBD,saw,VP,3,
mod,3,0,-head
binary =f
c1
c2
c3 c4
Fig. 3. Example features
In addition, the following feature is used for expressing the condition of the
root node of the parse tree.
froot = ?sym,word,pos,le?
Figure 3 shows example features: froot is the feature for the root node, in
which the phrase symbol is S and the surface form, part-of-speech, and lexical
entry of the lexical head are ?saw?, VBD, and a transitive verb, respectively.
The fbinary is the feature for the binary rule application to ?saw a girl? and
?with a telescope?, in which the applied schema is the Head-Modifier Schema, the
head daughter is VP headed by ?saw?, and the non-head daughter is PP headed
by ?with?, whose part-of-speech is IN and the lexical entry is a VP-modifying
preposition.
3 Re-training of Disambiguation Models
The method of domain adaptation is to develop a new maximum entropy model
with incorporating an original model as a reference probabilistic distribution.
The idea of adaptation using a reference distribution has already been presented
204 T. Hara, Y. Miyao, and J. Tsujii
in several studies [8,9]. When we have a reference probabilistic model p0(t|s) and
are making a new model pM (t|s), the probability is defined as
pM (t|s) =
1
Z ?s
p0(t|s) exp
?
?
?
j
?jgj(t?, s)
?
?
where Z ?s =
?
t??T (s)
p0(t?|s) exp
?
?
?
j
?jgj(t?, s)
?
? .
Model parameters, ?j, are estimated so as to maximize the likelihood of the
training data as in ordinary maximum entropy models. The maximization of the
likelihood with the above model is equivalent to finding the model pM that is
closest to the reference probability p0 in terms of the Kullback-Leibler distance.
However, we cannot simply apply the above method to our task because the
parameter estimation requires the computation of the above probability for all
parse candidates T (s). As discussed in Section 2, the size of T (s) is exponentially
related to the length of s. This imposes a new problem, that is, we need to
enumerate p0(t|s) for all candidate parses. Obviously, this is intractable.
Since Enju represented a probabilistic disambiguation model in a packed
forest structure, we exploit that structure to represent our probabilistic model.
That is, we redefine pM with feature functions gj on conjunctive nodes as
pM (t|s) =
1
Z ?s
p0(t|s) exp
?
?
?
c?t
?
j
?jgj(c)
?
?
where Z ?s =
?
t??T (s)
p0(t|s) exp
?
?
?
c?t?
?
j
?jgj(c)
?
? .
HEAD  verb
SUBCAT
<NP,NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  noun
SUBCAT <>
HEAD  verb
SUBCAT <NP>
HEAD  verb
SUBCAT <NP>
HEAD  prep
MOD  VP
SUBCAT <>
HEAD  verb
SUBCAT <NP>HEAD  nounSUBCAT <>
he
c1
c2
c3 c4
t1 selected
t2 selected
? j cgjj )( 1?
?i cfii )( 1?
? j cgjj )( 4?? j cgjj )( 3?? j cgjj )( 2?
?i cfii )( 2? ?i cfii )( 3? ?i cfii )( 4?
Fig. 4. Example of importing a reference distribution into each conjunctive node
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 205
As described in Section 2, the original model, pE(t|s), is expressed in a packed
structure as
pE(t|s) =
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)
where Zs =
?
t??T (s)
exp
(
?
c?t
?
i
?ifi(c)
)
.
Then, p0(t|s) is substituted by pE(t|s), and pM (t|s) is formulated as
pM (t|s) =
1
Z ?s
{
1
Zs
exp
(
?
c?t
?
i
?ifi(c)
)}
exp
?
?
?
c?t
?
j
?jgj(c)
?
?
=
1
Z ?s ? Zs
exp
?
?
?
c?t
?
i
?ifi(c) +
?
c?t
?
j
?jgj(c)
?
?
=
1
Z ??s
exp
?
?
?
?
c?t
?
?
?
i
?ifi(c) +
?
j
?jgj(c)
?
?
?
?
?
where Z ??s = Zs ? Z ?s =
?
t?T (s)
exp
?
?
?
?
c?t
?
?
?
i
?ifi(c) +
?
j
?jgj(c)
?
?
?
?
?
.
With this form of pM (t|s), a dynamic programing algorithm can be applied.
For example, we show how to obtain probabilities of parse trees in the case of
Figure 4. For ease, we assume that there are only two disjunctive daughters
(dotted lines) that are of the top conjunctive node. The left disjunctive node
introduces a parse tree t1 that consists of conjunctive nodes {c1, c2, c3, . . . },
and the right one, t2 that consists of {c1, c2, c4, . . . }. To each conjunctive node
ck, a weight from the reference distribution
?
i ?ifi(ck) is assigned. Probability
pM (t1|s) and pM (t2|s) are then given as
pM (t1|s)=
1
Z ??s
exp
?
?
?
?
?
?
i
?ifi(c1) +
?
j
?jgj(c1)
?
?+
?
?
?
i
?ifi(c2) +
?
j
?jgj(c2)
?
?
+
?
?
?
i
?ifi(c3) +
?
j
?jgj(c3)
?
? + ? ? ?
?
?
?
pM (t2|s)=
1
Z ??s
exp
?
?
?
?
?
?
i
?ifi(c1) +
?
j
?jgj(c1)
?
?+
?
?
?
i
?ifi(c2) +
?
j
?jgj(c2)
?
?
+
?
?
?
i
?ifi(c4) +
?
j
?jgj(c4)
?
? + ? ? ?
?
?
?
.
206 T. Hara, Y. Miyao, and J. Tsujii
4 Experiments
We implemented the method described in Section 3. The original parser, Enju,
was developed on Section 02-21 of the Penn Treebank (39,832 sentences)[5]. For
the training of our model, we used the GENIA treebank [4], which consisted of
500 abstracts (4,446 sentences) extracted from MEDLINE. We divided the GENIA
treebank into three sets of 400, 50, and 50 abstracts (3,524, 455, and 467 sentences),
and these setswere used respectively as training, development, and final evaluation
data. The method of Gaussian MAP estimation [10] was used for smoothing.
The meta parameter ? of the Gaussian distribution was determined so as
to maximize the accuracy on the development set. In the following experiments,
we measured the accuracy of predicate-argument dependencies on the evaluation
set. The measure is labeled precision/recall (LP/LR), which is the same measure
as previous work [11,5] that evaluated the accuracy of lexicalized grammars on
the Penn Treebank.
First, we measured the accuracy of parsing and the time required for pa-
rameter estimation. Table 2 compares the results of the following estimation
methods.
Table 2. Accuracy and time cost for various estimation methods
F-score Training Parsing time (sec.)
GENIA Corpus Penn Treebank time (sec.) GENIA Corpus Penn Treebank
Our method 86.87 86.81 2,278 611 3,165
Combined 86.32 86.09 29,421 424 2,757
GENIA only 85.72 42.49 1,694 332 8,183
Original model 85.10 87.16 137,038 515 2,554
85
85.2
85.4
85.6
85.8
86
86.2
86.4
86.6
86.8
87
0 500 1000 1500 2000 2500 3000 3500
training sentences
F-
sc
ore
RULE WORDh + WORDn RULE + WORDh + WORDn
Fig. 5. Corpus size vs. Accuracy
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 207
Table 3. Accuracy with atomic feature templates
Features LP LR F-score diff.
RULE 85.42 84.87 85.15 +0.05
DIST 85.29 84.77 85.03 ?0.07
COMMA 85.45 84.86 85.15 +0.05
SPANh+SPANn 85.58 85.02 85.30 +0.20
SYMBOLh+SYMBOLn 85.01 84.56 84.78 ?0.32
WORDh+WORDn 86.59 86.07 86.33 +1.23
WORDh 85.48 84.98 85.23 +0.13
WORDn 85.44 84.64 85.04 ?0.06
POSh+POSn 85.23 84.77 85.00 ?0.10
LEh+LEn 85.42 85.06 85.24 +0.14
None 85.39 84.82 85.10
Table 4. Accuracy with the combination of RULE and other features
Features LP LR F-score diff.
RULE+DIST 85.41 84.85 85.13 +0.03
RULE+COMMA 85.92 85.15 85.53 +0.43
RULE+SPANh+SPANn 85.33 84.82 85.07 ?0.03
RULE+SYMBOLh+SYMBOLn 85.43 85.00 85.21 +0.11
RULE+WORDh+WORDn 87.12 86.62 86.87 +1.77
RULE + WORDh 85.74 84.94 85.34 +0.24
RULE + WORDn 85.10 84.60 84.85 ?0.25
RULE+POSh+POSn 85.51 85.08 85.29 +0.19
RULE+LEh+LEn 85.48 85.08 85.28 +0.18
None 85.39 84.82 85.10
Our method: training with our method
Combined: training Enju model with the training corpus replaced by the com-
bination of the GENIA corpus and the Penn Treebank
GENIA only: training Enju model with the training corpus replaced by the
GENIA corpus only
Original Model: training an original Enju model
The table shows the accuracy and the parsing time for the GENIA corpus and
the Penn Treebank Section 23, and also shows the time required for the training
of the model. The additional feature used in our method was RULE+WORDh+
WORDn, which will be explained later. In the ?Combined? method, we could
not train the model with the original training parameters (n = 20,  = 0.98 in
[5]) because the estimator ran out of memory. Hence, we reduced the parameters
to n = 10,  = 0.95.
For the GENIA corpus, our model gave the higher accuracy than the origi-
nal model and the other estimation methods, while for the Penn Treebank, our
model gave a little lower accuracy than the original model. This result indicates
that our model was more adapted to the specific domain. The ?GENIA only?
208 T. Hara, Y. Miyao, and J. Tsujii
Table 5. Accuracy with the combination of WORD and another feature
Features LP LR F-score diff.
WORDh+WORDn+RULE 87.12 86.62 86.87 +1.77
WORDh+WORDn+DIST 86.41 85.86 86.14 +1.04
WORDh+WORDn+COMMA 86.91 86.38 86.64 +1.54
WORDh+WORDn+SPANh+SPANn 85.77 85.22 85.49 +0.39
WORDh+WORDn+SYMBOLh+SYMBOLn 86.58 85.70 86.14 +1.04
WORDh+WORDn+POSh+POSn 86.53 85.99 86.26 +1.16
WORDh+WORDn+LEh+LEn 86.16 85.68 85.92 +0.82
None 85.39 84.82 85.10
Table 6. Errors in our model and Enju
Total errors Common errors Errors not in
the other model
Our model 1179 1050 129
Original model 1338 1050 288
method gave significantly lower accuracy. We expect that the method clearly
lacked the amount of the training corpus for obtaining generic grammatical
information.
The ?Combined? method achieved the accuracy close to our method. How-
ever, it is notable that our method took much less time for the training of the
model since ours did not need to handle the Penn Treebank. Instead, our method
exploited the original model of Enju, which was trained on the Penn Treebank,
and this resulted in much less cost of training.
Next, we changed the size of the GENIA treebank for training: 40, 80, 120,
160, 200, 240, 280, 320, 360, and 400 abstracts. Figure 5 shows the accuracy when
the size of the training data was changed. We can say that, for those feature sets
giving remarkable accuracy in the experiments, the accuracy edged upwards with
the size of the training corpus, and the trend does not seem to converge even if
more than 400 abstracts exist. If we choose more complex feature sets for higher
accuracy, data sparseness will occur and an even larger corpus will be needed.
These findings indicate that we can further improve the accuracy by using a
larger treebank and a proper feature set.
Table 3 shows the accuracy of models with only atomic feature templates.
The bottom of the table gives the accuracy attained by the original parser.
When we focus on the WORD features, we can see the combination of WORDh
and WORDn improved the accuracy significantly, although each of the features
by itself did not improve so much. DIST, SYMBOL, and POS feature templates
lowered the accuracy. The other feature templates improved the accuracy, though
not as well as the WORD templates.
Table 4 shows that the RULE feature combined with one or more other
features often gave a little higher accuracy than the RULE feature gave by
itself, though not as well as the WORD features.
Adapting a Probabilistic Disambiguation Model of an HPSG Parser 209
Table 5 shows that the WORD features combined with one or more other
features gave remarkable improvement to the accuracy as a whole. RULE and
COMMA features gave even higher accuracy than with only the WORD features.
Our results revealed that the WORD features were crucial for the adaptation to
the biomedical domain. We expect that this was because the biomedical domain
had a different distribution of words, while more generic grammatical constraints
were not significantly different from other domains.
Table 6 shows the comparison of the number of errors of our model with those
of the original model in parsing the GENIA corpus. Though our model gave less
errors than the original model, our model introduced a certain amount of new
errors. In future work, we need to investigate manually those errors to find more
suitable feature templates without losing the information in the original model.
5 Conclusions
We have presented a method of adapting a domain-independent HPSG parser
to a biomedical domain. Since the treebank of the new domain was limited,
we exploited an original disambiguation model. The new model was trained
on a biomedical treebank, and was combined with the original model by using
it as a reference distribution of a log-linear model. The experimental results
demonstrated our new model was adapted to the target domain, and was superior
to other adaptation methods in accuracy and the cost of training time. With our
model, the parsing accuracy for the target domain improved by 1.77 point with
the treebank of 3,524 sentences. Since the accuracy did not seem to saturate, we
will further improve the accuracy by increasing the size of the domain-dependent
treebank. In addition, the experimental results showed that the WORD feature
significantly contributed to the accuracy improvement.
We examined only a few feature templates, and we must search for further
more feature templates. Not only the new combinations of the atomic features
but also new types of features, which may be domain-dependent such as named
entities, will be possible.
References
1. Geman, S., Johnson, M.: Dynamic programming for parsing and estimation of
stochastic unification-based grammars. In: Proc. 40th ACL. (2002)
2. Miyao, Y., Tsujii, J.: Maximum entropy estimation for feature forests. In: Proc.
HLT 2002. (2002)
3. Marcus, M., Kim, G., Marcinkiewicz, M.A., MacIntyre, R., Bies, A., Ferguson, M.,
Katz, K., Schasberger, B.: The Penn Treebank: Annotating predicate argument
structure. In: ARPA Human Language Technology Workshop. (1994)
4. Kim, J.D., Ohta, T., Teteisi, Y., Tsujii, J.: Genia corpus - a semantically annotated
corpus for bio-textmining. Bioinformatics 19 (2003) i180?i182
5. Miyao, Y., Tsujii, J.: Probabilistic disambiguation models for wide-coverage HPSG
parsing. In: Proc. ACL 2005. (2005)
210 T. Hara, Y. Miyao, and J. Tsujii
6. Miyao, Y., Ninomiya, T., Tsujii, J.: Corpus-oriented grammar development for
acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In:
Proc. IJCNLP-04. (2004)
7. Berger, A.L., Pietra, S.A.D., Pietra, V.J.D.: A maximum entropy approach to
natural language processing. Computational Linguistics 22 (1996) 39?71
8. Jelinek, F.: Statistical Methods for Speech Recognition. The MIT Press (1998)
9. Johnson, M., Riezler, S.: Exploiting auxiliary distributions in stochastic unification-
based grammars. In: Proc. 1st NAACL. (2000)
10. Chen, S., Rosenfeld, R.: A gaussian prior for smoothing maximum entropy models.
Technical Report CMUCS-99-108, Carnegie Mellon University (1999)
11. Clark, S., Curran, J.R.: Parsing the WSJ using CCG and log-linear models. In:
Proc. 42nd ACL. (2004)
TOWARDS DATA AND GOAL ORIENTED ANALYSIS:  
TOOL INTER-OPERABILITY AND COMBINATORIAL 
COMPARISON 
Yoshinobu Kano1      Ngan Nguyen1      Rune S?tre1       Kazuhiro Yoshida1 
Keiichiro Fukamachi1      Yusuke Miyao1       Yoshimasa Tsuruoka3   
Sophia Ananiadou2,3        Jun?ichi Tsujii1,2,3 
 
1Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester 
PO Box 88, Sackville St, MANCHESTER M60 1QD, UK 
 
3NaCTeM (National Centre for Text Mining), Manchester Interdisciplinary Biocentre, 
University of Manchester, 131 Princess St, MANCHESTER M1 7DN, UK 
 
{kano,nltngan,satre,kyoshida,keif,yusuke,tsujii} 
@is.s.u-tokyo.ac.jp 
{yoshimasa.tsuruoka,sophia.ananiadou}@manchester.ac.uk 
 
Abstract 
Recently, NLP researches have advanced 
using F-scores, precisions, and recalls with 
gold standard data as evaluation measures. 
However, such evaluations cannot capture 
the different behaviors of varying NLP 
tools or the different behaviors of a NLP 
tool that depends on the data and domain in 
which it works. Because an increasing 
number of tools are available nowadays, it 
has become increasingly important to grasp 
these behavioral differences, in order to 
select a suitable set of tools, which forms a 
complex workflow for a specific purpose. 
In order to observe such differences, we 
need to integrate available combinations of 
tools into a workflow and to compare the 
combinatorial results. Although generic 
frameworks like UIMA (Unstructured 
Information Management Architecture) 
provide interoperability to solve this 
problem, the solution they provide is only 
partial. In order for truly interoperable 
toolkits to become a reality, we also need 
sharable and comparable type systems with 
an automatic combinatorial comparison 
generator, which would allow systematic 
comparisons of available tools. In this 
paper, we describe such an environment, 
which we developed based on UIMA, and 
we show its feasibility through an example 
of a protein-protein interaction (PPI) 
extraction system. 
1 Introduction 
Recently, an increasing number of TM/NLP tools 
such as part-of-speech (POS) taggers (Tsuruoka et 
al., 2005), named entity recognizers (NERs) 
(Settles, 2005) syntactic parsers (Hara et al, 2005) 
and relation or event extractors (ERs) have been 
developed. Nevertheless, it is still very difficult to 
integrate independently developed tools into an 
aggregated application that achieves a specific 
task. The difficulties are caused not only by 
differences in programming platforms and 
different input/output data formats, but also by the 
lack of higher level interoperability among 
modules developed by different groups.  
859
UIMA, Unstructured Information Management 
Architecture (Lally and Ferrucci, 2004), which was 
originally developed by IBM and has recently 
become an open project in OASIS and Apache, 
provides a promising framework for tool 
integration. Although it has a set of useful 
functionalities, UIMA only provides a generic 
framework, thus it requires a user community to 
develop their own platforms with a set of actual 
software modules. A few attempts have already 
been made to establish platforms, e.g. the CMU 
UIMA component repository 1 , GATE 
(Cunningham et al, 2002) with its UIMA 
interoperability layer, etc.  
However, simply wrapping existing modules to 
be UIMA compliant does not offer a complete 
solution. Most of TM/NLP tasks are composite in 
nature, and can only be solved by combining 
several modules. Users need to test a large number 
of combinations of tools in order to pick the most 
suitable combination for their specific task. 
Although types and type systems are the only 
way to represent meanings in the UIMA 
framework, UIMA does not provide any specific 
types, except for a few purely primitive types. In 
this paper, we propose a way to design sharable 
type systems. A sharable type system designed in 
this way can provide the interoperability between 
independently developed tools with fewer losses in 
information, thus allowing for the combinations of 
tools and comparisons on these combinations. 
We show how our automatic comparison 
generator works based on a type system designed in 
that way. Taking the extraction of protein-protein 
                                                 
1 http://uima.lti.cs.cmu.edu/ 
interaction (PPI) as a typical example of a 
composite task, we illustrate how our platform 
helps users to observe the differences between 
tools and to construct a system for their own needs. 
2 Motivation and Background 
2.1 Goal and Data Oriented Evaluation, 
Module Selection and Inter-operability 
There are standard evaluation metrics for NLP 
modules such as precision, recall and F-value. For 
basic tasks like sentence splitting, POS tagging, 
and named-entity recognition, these metrics can be 
estimated using existing gold-standard test sets.  
Conversely, accuracy measurements based on 
the standard test sets are sometimes deceptive, 
since its accuracy may change significantly in 
practice, depending on the types of text and the 
actual tasks at hand. Because these accuracy 
metrics do not take into account the importance of 
the different types of errors to any particular 
application, the practical utility of two systems 
with seemingly similar levels of accuracy may in 
fact differ significantly. To users and developers 
alike, a detailed examination of how systems 
perform (on the text they would like to process) is 
often more important than standard metrics and 
test sets. Naturally, far greater weight is placed in 
measuring the end-to-end performance of a 
composite system than in measuring the 
performance of the individual components. 
In reality, because the selection of modules 
usually affects the performance of the entire 
system, it is crucial to carefully select modules that 
are appropriate for a given task. This is the main 
reason for having a collection of interoperable 
 
 
TOOL-SPECIFIC TYPES
PennPOS 
Penn verb1 ? ?
POS 
tcas.uima.Annotation 
-begin: int  -end: int 
SyntacticAnnotation SemanticAnnotation 
Sentence Phrase Token NamedEntity Relation 
-ent: FSArray<NamedEntity>
POSToken 
-pos: POS 
RichToken 
uima.jcas.cas.TOP 
UnknownPOS 
-base: String 
-posType: String 
ToolAToken
Verb Noun ?.. 
ToolBPOSToken
Protein 
ToolCProtein
ProteinProteinInteraction
ToolDPPI
Figure 1. Part of our type system 
860
modules. We need to show how the ultimate 
performance will be affected by the selection of 
different modules and show the best combination 
of modules in terms of the performance of the 
whole aggregated system for the task at hand. 
 Since the number of possible combinations of 
component modules is typically large, the system 
has to be able to enumerate and execute them 
semi-automatically. This requires a higher level of 
interoperability of individual modules than just 
wrapping them for UIMA.  
2.2 UIMA 
2.2.1 CAS and Type System 
The UIMA framework uses the ?stand-off 
annotation? style (Ferrucci et al, 2006). The raw 
text in a document is kept unchanged during the 
analysis process, and when the processing of the 
text is performed, the result is added as new stand-
off annotations with references to their positions in 
the raw text. A Common Analysis Structure (CAS) 
maintains a set of these annotations, which in itself 
are objects. The annotation objects in a CAS 
belong to types that are defined separately in a 
hierarchical type system. The features of an 
annotation2  object have values that are typed as 
well. 
2.2.2 Component and Capability 
Each UIMA Component has the capability 
property which describes what types of objects the 
component may take as the input and what types of 
objects it produces as the output. For example, a 
named entity recognizer detects named entities in 
                                                 
tools. Types should be defined in a distinct and 
2 In the UIMA framework, Annotation is a base type which 
has begin and end offset values. In this paper we call any 
objects (any subtype of TOP) as annotations. 
the text and outputs annotation objects of the type 
NamedEntity. 
It is possible to deploy any UIMA component as 
a SOAP web service, so that we can combine a 
remote component on a web service with the local 
component freely inside a UIMA-based system.  
3 Integration Platform and Comparators 
3.1 Sharable and Comparable Type System 
Although UIMA provides a set of useful 
functionalities for an integration platform of 
TM/NLP tools, users still have to develop the 
actual platform by using these functionalities 
effectively. There are several decisions for the 
designer to make an integration platform. 
Determining how to use types in UIMA is a 
crucial decision. Our decision is to keep different 
type systems by individual groups as they are, if 
necessary; we require that individual type systems 
have to be related through a sharable type system, 
which our platform defines. Such a shared type 
system can bridge modules with different type 
systems, though the bridging module may lose 
some information during the translation process.  
Whether such a sharable type system can be 
defined or not is dependent on the nature of each 
problem.  For example, a sharable type system for 
POS tags in English can be defined rather easily, 
since most of POS-related modules (such as POS 
taggers, shallow parsers, etc.) more or less follow 
the well established types defined by the Penn 
Treebank (Marcus et al, 1993) tag set. 
Figure 1 shows a part of our sharable type 
system. We deliberately define a highly organized 
type hierarchy as described above.  
Secondly we should consider that the type 
system may be used to compare a similar sort of 
Comparable Tools 
Sentence 
Detector
Deep 
Parser 
Named  
Entity 
Recognizer 
POS 
Tagger 
PPI 
Extractor 
AImed 
Collection 
Reader 
Comparator 
Evaluator 
Tokenizer 
Figure 2. PPI system workflow  
(conceptual) 
Figure 3.  
Basic example pattern
Comparable Tools
OpenNLP 
Sentence 
Detector 
Enju ABNER 
Stepp 
Tagger
UIMA 
Tokenizer
Figure 4.  
Complex tool example 
Comparable Tools 
GENIA 
Tagger 
OpenNLP 
Sentence 
Detector 
Enju NER 
POS 
Tagger
Tokenizer
Figure 5.  
Branch flow pattern 
Comparable Tools
OpenNLP 
S.D. 
UIMA 
Tokenizer
Enju ABNER 
Stepp 
Tagger
GENIA 
S.D. 
861
hierarchical manner. For example, both tokenizers 
and POS taggers output an object of type Token, 
but their roles are different when we assume a 
cascaded pipeline. We defined Token as a 
supertvpe, POSToken as subtypes of Token. Each 
tool should have an individual type to make clear 
which tool generated which instance, because each 
tool may have a slightly different definition. This 
is important because the capabilities are 
represented by these types, and the capabilities are 
the only attributes which are machine readable. 
3.2 General Combinatorial Comparison 
stem is defined in the previously 
tually shows the workflow of our 
wh
 pattern expansion mechanism which 
ge
cases, a single tool can play two or 
m
                                                
Generator 
Even if the type sy
described way, there are still some issues to 
consider when comparing tools. We illustrate these 
issues using the PPI workflow that we utilized in 
our experiments. 
Figure 2 concep
ole PPI system. If we can prepare two or more 
components for some type of the components in 
the workflow (e.g. two sentence detectors and three 
POS taggers), then we can make combinations of 
these tools to form a multiplied number of 
workflow patterns (2x3 = 6 patterns). See Table 1 
for the details of UIMA components used in our 
experiments. 
We made a
nerates possible workflow patterns automatically 
from a user-defined comparable workflow. A 
comparable workflow is a special workflow that 
explicitly specifies which set of components 
should be compared. Then, users just need to group 
comparable components (e.g. ABNER3 and MedT-
NER as a comparable NER group) without making 
any modifications to the original UIMA 
components. This aggregation of comparable 
components is controlled by our custom workflow 
controller.  
In some 
ore roles (e.g. the GENIA Tagger performs 
tokenization, POS tagging, and NER; see Figure 
4). It may be possible to decompose the original 
tool into single roles, but in most cases it is 
difficult and unnatural to decompose such a 
 
ponent requires two or more input 
ty
4 Experiments and Results 
 using our PPI 
e have several 
co
igure 6 show a part of the 
co
Table 2.   
3 In the example figures, ABNER requires Sentence to 
make the explanation clearer, though ABNER does not 
require it in actual usage. 
complex tool. We designed our comparator to 
detect possible input combinations automatically 
by the types of previously generated annotations, 
and the input capability of each posterior 
component. As described in the previous section, 
the component should have appropriate 
capabilities with proper types in order to permit 
this detection.  
When a com
pes (e.g. our PPI extractor requires outputs of a 
deep parser and a protein NER system), there 
could be different components used in the prior 
flow (e.g. OpenNLP and GENIA sentence 
detectors in Figure 5). Our comparator also 
calculates such cases automatically. 
 OO UO GOO U G A
UU 8 89 8
We have performed experiments
extraction system as an example (Kano et al, 
2008). It is similar to our BioCreative PPI system 
(S?tre et al, 2006) but differs in that we have 
deconstructed the original system into seven 
different components (Figure 2).  
As summarized in Table 1, w
mparable components and the AImed corpus as 
the gold standard data. In this case, possible 
combination workflow patterns are POSToken for 
36, PPI for 589, etc.   
Table 2, 3, 4 and F
mparison result screenshots between these 
patterns on 20 articles from the AImed corpus. In 
the tables, abbreviations like ?OOG? stands for a 
workflow of O(Sentence) -> O(Token) - 
Sentence
comparisons (%). 
Table 3. Part of Token
comparisons, 
precision/recall (%).
OOO UOS GOO 
UUO 87/74 81/68 85/68 
GUG 74/65 73/65 78/65 
GGO 92/95 81/84 97/95 
OGO 100/100 89/88 100/94 
G 0 0 - 85
U
 9/75 /75 8/70
GU 89/75 89/75 88/70
GG 92/95 91/95 97/95
OG 
86 - 0 7
A 6 6 60 -
O - 10 10/100 99/99 00/9481 0 7
Table 4. Part of POSToken comparisons, 
precision/recall (%) 
862
G(POSToken), where O stands for OpenNLP, G 
stands for Genia, U stands for UIMA, etc.  
When neither of the compared results include 
th
e comparison on Sentences 
sh
%  
0 
e gold standard data (AImed in this case), the 
comparison results show a similarity of the tools 
for this specific task and data, rather than an 
evaluation. Even if we lack an annotated corpus, it 
is possible to run the tools and compare the results 
in order to understand the characteristics of the 
tools depending on the corpus and the tool 
combinations.  
Although th
ows low scores of similarities, Tokens are 
almost the same; it means that input sentence 
boundaries do not affect tokenizations so much. 
POSToken similarities drop approximately 0-10
100 
  
                      100
Fi  6  NER (Protein) comp rison di
ences in 
5 Conclusion and Future Work 
ponents, 
 design, which the UIMA 
fra
   0  
gure . a stribution of 
precisions (x-axis, %) and recalls (y-axis, %). 
from the similarities in Token; the differ
Token are mainly apostrophes and punctuations; 
POSTokens are different because each POS 
tagger uses a slightly different set of tags: normal 
Penn tagset for Stepp tagger, BioPenn tagset 
(includes new tags for hyphenation) for GENIA 
tagger, and an original apostrophe tag for 
OpenNLP tagger. 
NLP tasks typically consist of many com
and it is necessary to show which set of tools are 
most suitable for each specific task and data. 
Although UIMA provides a general framework 
with much functionality for interoperability, we 
still need to build an environment that enables the 
combinations and comparisons of tools for a 
specific task.  
The type system
mework does not provide, is one of the most 
critical issues on interoperability. We have thus 
proposed a way to design a sharable and 
comparable type system. Such a type system allows 
for the automatic combinations of any UIMA 
compliant components and for the comparisons of 
these combinations, when the components have 
proper capabilities within the type system. We are 
Sentence Token POSToken RichToken Protein Phrase PPI
GENIA Tagger: Trained on the WSJ, GENIA and PennBioIE corpora (POS). Uses Maximum Entropy (Berger 
et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). Trained on GENIA corpus (Sentence 
Splitter). 
Enju: HPSG parser with predicate argument structures as well as phrase structures. Although trained with Penn 
Treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (Hara 
et al, 2005). 
STePP Tagger: Based on probabilistic models, tuned to biomedical text trained by WSJ, GENIA (Kim et al, 
2003)  and PennBioIE corpora. 
MedT-NER: Statistical recognizer trained on the JNLPBA data. 
ABNER: From the University of Wisconsin (Settles, 2005), wrapped by the Center for Computational 
Pharmacology at the University of Colorado.  
Akane++: A new version of the AKANE system (Yakushiji, 2006), trained with SVMlight-TK (Joachims, 1999; 
Bunescu and Mooney, 2006; Moschitti, 2006) and the AImed Corpus. 
UIMA Examples: Provided in the Apache UIMA example. Sentence Splitter and Tokenizer. 
OpenNLP Tools: Part of the OpenNLP project (http://opennlp.sourceforge.net/), from Apache UIMA examples. 
AImed Corpus: 225 Medline abstracts with proteins and PPIs annotated (Bunescu and Mooney, 2006).   
Legend:         Input type(s) required for that tool          Input type(s) required optionally          Output type(s)  
Table 1. List of UIMA Components used in our experiment. 
863
preparing to make a portion of the components and 
services described in this paper publicly available 
(http://www-tsujii.is.s.u-tokyo.ac.jp/uima/). 
The final system shows which combination of 
co
or this work includes 
co
cknowledgments 
e wish to thank Dr. Lawrence Hunter?s text 
References 
Vincent J. Della Pietra, and Stephen 
IT 
 Mooney. 
on." Edited 
tcheva, and V. 
ls and 
m Lally, Daniel Gruhl, and Edward 
RC24122. (2006). 
ilistic disambiguation model of an 
t, 
e 
." MIT Press, (1999): 169-
ls 
ser: a tool comparator, using protein-protein 
i. "Introduction to the Bio-Entity 
d 
ics 
 i180-
le Application with the Unstructured Information 
l 43, 
ng a Large Annotated Corpus of 
ractical 
. (2006). 
oko 
 
cally tagging genes, proteins, and other entity 
rsity 
, 
ust Part-of-
tion 
University of Tokyo, (2006).  
mponents has the best score, and also generates 
comparative results. This helps users to grasp the 
characteristics and differences among tools, which 
cannot be easily observed by the widely used F-
score evaluations only. 
Future directions f
mbining the output of several modules of the 
same kind (such as NERs) to obtain better results, 
collecting other tools developed by other groups 
using the sharable type system, making machine 
learning tools UIMA compliant, and making grid 
computing available with UIMA workflows to 
increase the entire performance without modifying 
the original UIMA components. 
 
A
 
W
mining group at the Center for Computational 
Pharmacology for discussing with us and making 
their tools available for this research. This work 
was partially supported by NaCTeM (the UK 
National Centre for Text Mining), Grant-in-Aid for 
Specially Promoted Research (MEXT, Japan) and 
Genome Network Project (MEXT, Japan). 
NaCTeM is jointly funded by 
JISC/BBSRC/EPSRC. 
Berger, Adam L., 
A. Della Pietra. "A maximum entropy approach to 
natural language processing." Comput. Linguist. (M
Press) 22, no. 1 (1996): 39-71. 
Bunescu, Razvan, and Raymond
"Subsequence Kernels for Relation Extracti
by Weiss Y., Scholkopf B. and Platt J., 171-178. 
Cambridge, MA: MIT Press, (2006). 
Cunningham, H., D. Maynard, K. Bon
Tablan. "GATE: A framework and graphical 
development environment for robust NLP too
applications." Proceedings of the 40th Anniversary 
Meeting of the Association for Computational 
Linguistics. (2002). 
Ferrucci, David, Ada
Epstein. "Towards an Interoperability Standard for Text 
and Multi-Modal Analytics." IBM Research Report, 
Hara, Tadayoshi, Yusuke Miyao, and Jun'ichi Tsujii. 
"Adapting a probab
HPSG parser to a new domain." Edited by Dale Rober
Wong Kam-Fai, Su Jian and Yee Oi. Natural Languag
Processing IJCNLP 2005. Jeju Island, Korea: Springer-
Verlag, (2005). 199-210. 
Joachims, Thorsten. "Making large-scale support vector 
machine learning practical
184. 
Kano, Yoshinobu, et al "Filling the gaps between too
and u
interaction as an example." Proceedings of The Pacific 
Symposium on Biocomputing (PSB). Hawaii, USA, To 
appear, (2008). 
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa Tsuruoka, 
and Yuka Tateis
Recognition Task at JNLPBA." Proceedings of the 
International Workshop on Natural Language 
Processing. Geneva, Switzerland, (2004). 70-75. 
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, an
Jun'ichi Tsujii. "GENIA corpus - a semantically 
annotated corpus for bio-textmining." Bioinformat
(Oxford University Press) 19, no. suppl. 1 (2003):
i182. 
Lally, Adam, and David Ferrucci. "Building an 
Examp
Management Architecture." IBM Systems Journa
no. 3 (2004): 455-475. 
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann 
Marcinkiewicz. "Buildi
English: The Penn Treebank." Computational 
Linguistics 19, no. 2 (1993): 313-330. 
Moschitti, Alessandro. "Making Tree Kernels P
for Natural Language Learning." EACL
S?tre, Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiroh Matsubayashi, and Tom
Ohta. "AKANE System: Protein-Protein Interaction
Pairs in BioCreAtIvE2 Challenge." Proceedings of the 
Second BioCreative Challenge Evaluation Workshop. 
(2007). 
Settles, B. "ABNER: an open source tool for 
automati
names in text." Bioinformatics (Oxford Unive
Press) 21, no. 14 (2005): 3191-3192. 
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim
and Tomoko Ohta. "Developing a Rob
Speech Tagger for Biomedical Text." Advances in 
Informatics - 10th Panhellenic Conference on 
Informatics. Volos, Greece, (2005). 382-392. 
Yakushiji, Akane. "Relation Information Extrac
Using Deep Syntactic Analysis." PhD Thesis, 
864
Feature Forest Models for Probabilistic
HPSG Parsing
Yusuke Miyao?
University of Tokyo
Jun?ichi Tsujii??
University of Tokyo
University of Manchester
Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit com-
plicated data structures, such as typed feature structures. This prevents us from applying
common methods of probabilistic modeling in which a complete structure is divided into sub-
structures under the assumption of statistical independence among sub-structures. For example,
part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing
is split into applications of CFG rules. These methods have relied on the structure of the target
problem, namely lattices or trees, and cannot be applied to graph structures including typed fea-
ture structures.
This article proposes the feature forest model as a solution to the problem of probabilistic
modeling of complex data structures including typed feature structures. The feature forest model
provides a method for probabilistic modeling without the independence assumption when prob-
abilistic events are represented with feature forests. Feature forests are generic data structures
that represent ambiguous trees in a packed forest structure. Feature forest models are maximum
entropy models defined over feature forests. A dynamic programming algorithm is proposed for
maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of
any data structures is possible when they are represented by feature forests.
This article also describes methods for representing HPSG syntactic structures and
predicate?argument structures with feature forests. Hence, we describe a complete strategy for
developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is
empirically evaluated through parsing experiments on the Penn Treebank, and the promise of
applicability to parsing of real-world sentences is discussed.
1. Introduction
Following the successful development of wide-coverage lexicalized grammars (Riezler
et al 2000; Hockenmaier and Steedman 2002; Burke et al 2004; Miyao, Ninomiya, and
? Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan.
E-mail: yusuke@is.s.u-tokyo.ac.jp.
?? Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan.
E-mail: tsujii@is.s.u-tokyo.ac.jp.
Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication:
5 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
Tsujii 2005), statistical modeling of these grammars is attracting considerable attention.
This is because natural language processing applications usually require disambiguated
or ranked parse results, and statistical modeling of syntactic/semantic preference is one
of the most promising methods for disambiguation.
The focus of this article is the problem of probabilistic modeling of wide-coverage
HPSG parsing. Although previous studies have proposed maximum entropy mod-
els (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen,
Toutanova, et al 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003;
Malouf and van Noord 2004), the straightforward application of maximum entropy
models to wide-coverage HPSG parsing is infeasible because estimation of maximum
entropymodels is computationally expensive, especially when targeting wide-coverage
parsing. In general, complete structures, such as transition sequences inMarkovmodels
and parse trees, have an exponential number of ambiguities. This causes an exponential
explosion when estimating the parameters of maximum entropy models. We therefore
require solutions to make model estimation tractable.
This article first proposes feature forest models, which are a general solution to
the problem of maximum entropy modeling of tree structures (Miyao and Tsujii 2002).
Our algorithm avoids exponential explosion by representing probabilistic events with
feature forests, which are packed representations of tree structures. When complete
structures are represented with feature forests of a tractable size, the parameters of
maximum entropy models are efficiently estimated without unpacking the feature
forests. This is due to dynamic programming similar to the algorithm for computing
inside/outside probabilities in PCFG parsing.
The latter half of this article (Section 4) is on the application of feature forest
models to disambiguation in wide-coverage HPSG parsing. We describe methods for
representing HPSG parse trees and predicate?argument structures using feature forests
(Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the
parameter estimation algorithm for feature forest models, these methods constitute a
complete procedure for the probabilistic modeling of wide-coverage HPSG parsing.
The methods we propose here were applied to an English HPSG parser, Enju (Tsujii
Laboratory 2004). We report on an extensive evaluation of the parser through parsing
experiments on theWall Street Journal portion of the Penn Treebank (Marcus et al 1994).
The content of this article is an extended version of our earlier work reported
in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The
major contribution of this article is a strict mathematical definition of the feature forest
model and the parameter estimation algorithm, which are substantially refined and
extended from Miyao and Tsujii (2002). Another contribution is that this article thor-
oughly discusses the relationships between the feature forest model and its application
to HPSG parsing. We also provide an extensive empirical evaluation of the resulting
HPSG parsing approach using real-world text.
Section 2 discusses a problem of conventional probabilistic models for lexicalized
grammars. Section 3 proposes feature forest models for solving this problem. Section 4
describes the application of feature forest models to probabilistic HPSG parsing. Sec-
tion 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6
introduces research related to our proposals. Section 7 concludes.
2. Problem
Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now be-
coming the de facto standard approach for disambiguation models for lexicalized or
36
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
feature structure grammars (Johnson et al 1999; Riezler et al 2000, 2002; Geman and
Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al 2004; Carroll and Oepen
2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al 2002;
Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord
2004) have also adopted log-linear models. This is because these grammar formalisms
exploit feature structures to represent linguistic constraints. Such constraints are known
to introduce inconsistencies in probabilistic models estimated using simple relative
frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable
choice for credible probabilistic models. It also allows various overlapping features to
be incorporated, and we can expect higher accuracy in disambiguation.
A maximum entropy model gives a probabilistic distribution that maximizes the
likelihood of training data under given feature functions. Given training data E =
{?x, y?}, a maximum entropy model gives conditional probability p(y|x) as follows.
Definition 1 (Maximum entropy model)
A maximum entropy model is defined as the solution of the following optimization
problem.
pM(y|x) = argmax
p
?
?
?
?
?
?x,y??E
p?(x, y) log p(y|x)
?
?
?
where:
p(y|x) = 1
Z(x)
exp
(
?
i
?i fi(x, y)
)
Z(x) =
?
y?Y(x)
exp
(
?
i
?i fi(x, y)
)
In this definition, p?(x, y) is the relative frequency of ?x, y? in the training data. fi is a
feature function, which represents a characteristic of probabilistic events by mapping
an event into a real value. ?i is the model parameter of a corresponding feature function
fi, and is determined so as to maximize the likelihood of the training data (i.e., the
optimization in this definition). Y(x) is a set of y for given x; for example, in parsing, x is
a given sentence and Y(x) is a parse forest for x. An advantage of maximum entropy
models is that feature functions can represent any characteristics of events. That is,
independence assumptions are unnecessary for the design of feature functions. Hence,
this method provides a principled solution for the estimation of consistent probabilistic
distributions over feature structure grammars.
The remaining issue is how to estimate parameters. Several numerical algorithms,
such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved
Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limited-
memory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright
1999), have been proposed for parameter estimation. Although the algorithm proposed
in the present article is applicable to all of the above algorithms, we used L-BFGS for
experiments.
However, a computational problem arises in these parameter estimation algo-
rithms. The size of Y(x) (i.e., the number of parse trees for a sentence) is generally
37
Computational Linguistics Volume 34, Number 1
very large. This is because local ambiguities in parse trees potentially cause exponential
growth in the number of structures assigned to sub-sequences of words, resulting in
billions of structures for whole sentences. For example, when we apply rewriting rule
S ? NP VP, and the left NP and the right VP, respectively, have n and m ambiguous
subtrees, the result of the rule application generates n?m trees.
This is problematic because the complexity of parameter estimation is proportional
to the size of Y(x). The cost of the parameter estimation algorithms is bound by the
computation ofmodel expectation, ?i, given as (Malouf 2002):
?i =
?
x?X
p?(x)
?
y?Y(x)
fi(x, y)p(y|x)
=
?
x?X
p?(x)
?
y?Y(x)
fi(x, y)
1
Z(x)
exp
?
?
?
j
?j fj(x, y)
?
? (1)
As shown in this definition, the computation of model expectation requires the summa-
tion over Y(x) for every x in the training data. The complexity of the overall estimation
algorithm is O( ?|Y| ?|F||E|), where ?|Y| and ?|F| are the average numbers of y and activated
features for an event, respectively, and |E| is the number of events. When Y(x) grows
exponentially, the parameter estimation becomes intractable.
In PCFGs, the problem of computing probabilities of parse trees is avoided by using
a dynamic programming algorithm for computing inside/outside probabilities (Baker
1979). With the algorithm, the computation becomes tractable. We can expect that the
same approach would be effective for maximum entropy models as well.
This notion yields a novel algorithm for parameter estimation for maximum en-
tropy models, as described in the next section.
3. Feature Forest Model
Our solution to the problem is a dynamic programming algorithm for computing
inside/outside ?-products. Inside/outside ?-products roughly correspond to inside/
outside probabilities in PCFGs. In maximum entropy models, a probability is defined
as a normalized product of ?
fj
j (= exp(?j fj)). Hence, similar to the algorithm of computing
inside/outside probabilities, we can compute exp
(
?
j ?j fj
)
, which we define as the
?-product, for each node in a tree structure. If we can compute ?-products at a tractable
cost, the model expectation ?i is also computed at a tractable cost.
We first define the notion of a feature forest, a packed representation of a set
of an exponential number of tree structures. Feature forests correspond to packed
charts in CFG parsing. Because feature forests are generalized representations of forest
structures, the notion is not only applicable to syntactic parsing but also to sequence
tagging, such as POS tagging and named entity recognition (which will be discussed in
Section 6). We then define inside/outside ?-products that represent the ?-products of
partial structures of a feature forest. Inside?-products correspond to inside probabilities
in PCFG, and represent the summation of ?-products of the daughter sub-trees. Outside
?-products correspond to outside probabilities in PCFG, and represent the summation
of ?-products in the upper part of the feature forest. Both can be computed incre-
mentally by a dynamic programming algorithm similar to the algorithm for computing
38
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
inside/outside probabilities in PCFG. Given inside/outside ?-products of all nodes in
a feature forest, the model expectation ?i is easily computed by multiplying them for
each node.
3.1 Feature Forest
To describe the algorithm, we first define the notion of a feature forest, the generalized
representation of features in a packed forest structure. Feature forests are used for
enumerating possible structures of events, that is, they correspond to Y(x) in Equation 1.
Definition 2 (Feature forest)
A feature forest ? is a tuple ?C,D, r,?, ??, where:
 C is a set of conjunctive nodes,
 D is a set of disjunctive nodes,
 r is the root node: r ? C,
 ? : D ? 2C is a conjunctive daughter function,
 ? : C ? 2D is a disjunctive daughter function.
We denote a feature forest for x as ?(x). For example, ?(x) can represent the set of all
possible tag sequences of a given sentence x, or the set of all parse trees of x. A feature
forest is an acyclic graph, and unpacked structures extracted from a feature forest are
trees. We also assume that terminal nodes of feature forests are conjunctive nodes. That
is, disjunctive nodes must have daughters (i.e., ?(d) = ? for all d ? D).
A feature forest represents a set of trees of conjunctive nodes in a packed structure.
Conjunctive nodes correspond to entities such as states in Markov chains and nodes
in CFG trees. Feature functions are assigned to conjunctive nodes and express their
characteristics. Disjunctive nodes are for enumerating alternative choices. Conjunctive/
disjunctive daughter functions represent immediate relations of conjunctive and dis-
junctive nodes. By selecting a conjunctive node as a child of each disjunctive node, we
can extract a tree consisting of conjunctive nodes from a feature forest.
Figure 1 shows an example of a feature forest. Each disjunctive node enumerates
alternative nodes, which are conjunctive nodes. Each conjunctive node has disjunctive
Figure 1
A feature forest.
39
Computational Linguistics Volume 34, Number 1
Figure 2
Unpacked trees.
nodes as its daughters. The feature forest in Figure 1 represents a set of 2? 2? 2 = 8
unpacked trees shown in Figure 2. For example, by selecting the left-most conjunctive
node at each disjunctive node, we extract an unpacked tree (c1, c2, c4, c6). An unpacked
tree is represented as a set of conjunctive nodes. Generally, a feature forest represents
an exponential number of trees with a polynomial number of nodes. Thus, complete
structures, such as tag sequences and parse trees with ambiguities, can be represented
in a tractable form.
Feature functions are defined over conjunctive nodes.1
Definition 3 (Feature function for feature forests)
A feature function for a feature forest is:
fi : C ? R
Hence, together with feature functions, a feature forest represents a set of trees of
features.
Feature forests may be regarded as a packed chart in CFG parsing. Although feature
forests have the same structure as PCFG parse forests, nodes in feature forests do not
necessarily correspond to nodes in PCFG parse forests. In fact, in Sections 4.2 and 4.3, we
will demonstrate that syntactic structures and predicate?argument structures in HPSG
can be represented with tractable-size feature forests. The actual interpretation of a node
in a feature forest may thus be ignored in the following discussion. Our algorithm is
applicable whenever feature forests are of a tractable size. The descriptive power of
feature forests will be discussed again in Section 6.
Asmentioned, a feature forest is a packed representation of trees of features.We first
define model expectations, ?i, on a set of unpacked trees, and then show that they can
be computed without unpacking feature forests. We denote an unpacked tree as a set,
c ? C, of conjunctive nodes. Our concern is only the set of features associated with each
conjunctive node, and the shape of the tree structure is irrelevant to the computation of
probabilities of unpacked trees. Hence, we do not distinguish an unpacked tree from a
set of conjunctive nodes.
The collection of unpacked trees represented by a feature forest is defined as amulti-
set of unpacked trees because we allow multiple occurrences of equivalent unpacked
1 Feature functions may also be conditioned on x. In this case, feature functions can be written as fi(c, x).
For simplicity, we omit x in the following discussion.
40
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
trees in a feature forest.2 Given multisets of unpacked trees, A,B, we define the union
and the product as follows.
A? B ? A ? B
A? B ? {a ? b|a ? A,b ? B}
Intuitively, the first operation is a collection of trees, and the second lists all combina-
tions of trees in A and B. It is trivial that they satisfy commutative, associative, and
distributive laws.
A? B = B? A
A? B = B? A
A? (B? C) = (A? B)? C
A? (B? C) = (A? B)? C
A? (B? C) = (A? B)? (A? C)
We denote a set of unpacked trees rooted at node n ? C ?D as ?(n). ?(n) is de-
fined recursively. For a terminal node c ? C, obviously ?(c) = {{c}}. For an internal
conjunctive node c ? C, an unpacked tree is a combination of trees, each of which is
selected from a disjunctive daughter. Hence, a set of all unpacked trees is represented
as a product of trees from disjunctive daughters.
?(c) = {{c}} ?
?
d??(c)
?(d)
A disjunctive node d ? D represents alternatives of packed trees, and obviously a set
of its unpacked trees is represented as a union of the daughter trees, that is, ?(d) =
?
c??(d)?(c).
To summarize, a set of unpacked trees is defined formally as follows.
Definition 4 (Unpacked tree)
Given a feature forest ? = ?C,D, r,?, ??, a set ?(n) of unpacked trees rooted at node
n ? C ?D is defined recursively as follows.
 If n ? C is a terminal, that is, ?(n) = ?,
?(n) ? {{n}}
 If n ? C,
?(n) ? {{n}} ?
?
d??(n)
?(d)
2 In fact, no feature forests include equivalent unpacked trees if no disjunctive nodes have identical
daughter nodes. Thus we may define a set of unpacked trees as an ordinary set, although the details
are omitted here for simplicity.
41
Computational Linguistics Volume 34, Number 1
 If n ? D,
?(n) ?
?
c??(n)
?(c)
Feature forests are directed acyclic graphs and, as such, this definition does not include
a loop. Hence, ?(n) is properly defined.
A set of all unpacked trees is then represented by ?(r); henceforth, we denote ?(r)
as ?(?), or just ?when it is not confusing in context. Figure 3 shows ?(?) of the feature
forest in Figure 1. Following Definition 4, the first element of each set is the root node,
c1, and the rest are elements of the product of {c2, c3}, {c4, c5}, and {c6, c7}. Each set in
Figure 3 corresponds to a tree in Figure 2.
Given this formalization, the feature function for an unpacked tree is defined as
follows.
Definition 5 (Feature function for unpacked tree)
The feature function fi for an unpacked tree, c ? ?(?) is defined as:
fi(c) =
?
c?c
fi(c)
Because c ? ?(?) corresponds to y of the conventional maximum entropy model, this
function substitutes for fi(x, y) in the conventional model. Once a feature function for
an unpacked tree is given, a model expectation is defined as in the traditional model.
Definition 6 (Model expectation of feature forests)
The model expectation ?i for a set of feature forests {?(x)} is defined as:
?i =
?
x?X
p?(x)
?
c??(?(x))
fi(c)p(c|x)
=
?
x?X
p?(x)
?
c??(?(x))
fi(c)
1
Z(x)
exp
?
?
?
j
?j fj(c)
?
?
where Z(x) =
?
c??(?(x))
exp
?
?
?
j
?j fj(c)
?
?
It is evident that the naive computation of model expectations requires exponential
time complexity because the number of unpacked trees (i.e., |?(?)|) is exponentially
related to the number of nodes in the feature forest ?. We therefore need an algorithm
for computing model expectations without unpacking a feature forest.
Figure 3
Unpacked trees represented as sets of conjunctive nodes.
42
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 4
Inside/outside at node c2 in a feature forest.
3.2 Dynamic Programming
To efficiently compute model expectations, we incorporate an approach similar to the
dynamic programming algorithm for computing inside/outside probabilities in PCFGs.
We first define the notion of inside/outside of a feature forest. Figure 4 illustrates this
concept, which is similar to the analogous concept in PCFGs.3 Inside denotes a set of
partial trees (sets of conjunctive nodes) derived from node c2. Outside denotes a set of
partial trees that derive node c2. That is, outside trees are partial trees of complements
of inside trees.
We denote a set of inside trees at node n as ?(n), and that of outside trees as o(n).
Definition 7 (Inside trees)
We define a set ?(n) of inside trees rooted at node n ? C ?D as a set of unpacked trees
rooted at n.
?(n) ? ?(n)
Definition 8 (Outside trees)
We define a set o(n) of outside trees rooted at node n ? C ?D as follows.
o(r) ? {?}
o(c) ?
?
d???1(c)
o(d)
o(d) ?
?
c???1(d)
?
?
?
{{c}} ? o(c)?
?
d???(c),d? =d
?(d?)
?
?
?
3 A node may have multiple outside trees in general as in the case of CFGs, although Figure 4 shows only
one outside tree of c2 for simplicity.
43
Computational Linguistics Volume 34, Number 1
In the definition, ??1 and ??1 denote mothers of conjunctive and disjunctive nodes,
respectively. Formally,
??1(c) ? {d|c ? ?(d)}
??1(d) ? {c|d ? ?(c)}
Next, inside/outside ?-products are defined for conjunctive and disjunctive nodes.
The inside (or outside) ?-products are the summation of exp
(
?
j ?j fj(c)
)
of all inside (or
outside) trees c.
Definition 9 (Inside/outside ?-product)
An inside ?-product at conjunctive node c ? C is
?c =
?
c??(c)
exp
?
?
?
j
?j fj(c)
?
?
An outside ?-product is
?c =
?
c?o(c)
exp
?
?
?
j
?j fj(c)
?
?
Similarly, inside/outside ?-products at disjunctive node d ? D are defined as follows:
?d =
?
c??(d)
exp
?
?
?
j
?j fj(c)
?
?
?d =
?
c?o(d)
exp
?
?
?
j
?j fj(c)
?
?
We can derive that the model expectations of a feature forest are computed as the
product of the inside and outside ?-products.
Theorem 1 (Model expectation of feature forests)
The model expectation ?i of a feature forest?(x) = ?Cx,Dx, rx,?x, ?x? is computed as the
product of inside and outside ?-products as follows:
?i =
?
x?X
p?(x) 1
Z(x)
?
c?Cx
fi(c)?c?c
where Z(x) = ?rx
44
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 5
Incremental computation of inside ?-products at conjunctive node c2.
Figure 6
Incremental computation of inside ?-products at disjunctive node d4.
This equation shows a method for efficiently computing model expectations by
traversing conjunctive nodes without unpacking the forest, if the inside/outside
?-products are given. The remaining issue is how to efficiently compute inside/outside
?-products.
Fortunately, inside/outside ?-products can be incrementally computed by dynamic
programming without unpacking feature forests. Figure 5 shows the process of com-
puting the inside ?-product at a conjunctive node from the inside ?-products of its
daughter nodes. Because the inside of a conjunctive node is a set of the combinations of
all of its descendants, the ?-product is computed by multiplying the ?-products of the
daughter trees. The following equation is derived.
?c =
?
?
?
d??(c)
?d
?
? exp
?
?
?
j
?j fj(c)
?
?
The inside of a disjunctive node is the collection of the inside trees of its daughter nodes.
Hence, the inside ?-product at disjunctive node d ? D is computed as follows (Figure 6).
?d =
?
c??(d)
?c
45
Computational Linguistics Volume 34, Number 1
Theorem 2 (Inside ?-product)
The inside ?-product ?c at a conjunctive node c is computed by the following equation
if ?d is given for all daughter disjunctive nodes d ? ?(c).
?c =
?
?
?
d??(c)
?d
?
? exp
?
?
?
j
?j fj(c)
?
?
The inside ?-product ?d at a disjunctive node d is computed by the following equation
if ?c is given for all daughter conjunctive nodes c ? ?(d).
?d =
?
c??(d)
?c
The outside of a disjunctive node is equivalent to the outside of its daughter nodes.
Hence, the outside ?-product of a disjunctive node is propagated to its daughter con-
junctive nodes (Figure 7).
?c =
?
{d|c??(d)}
?d
The computation of the outside ?-product of a disjunctive node is somewhat com-
plicated. As shown in Figure 8, the outside trees of a disjunctive node are all com-
binations of
 the outside trees of the mother nodes, and
 the inside trees of the sister nodes.
Figure 7
Incremental computation of outside ?-products at conjunctive node c2.
46
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 8
Incremental computation of outside ?-products at disjunctive node d4.
From this, we find:
?d =
?
{c|d??(c)}
?
?
?
?
?
?
?
?c exp
?
?
?
j
?j fj(c)
?
?
?
d???(c)
d? =d
?d?
?
?
?
?
?
?
?
We finally find the following theorem for the computation of outside ?-products.
Theorem 3 (Outside ?-product)
The outside ?-product ?c at conjunctive node c is computed by the following equation
if ?d is given for all mother disjunctive nodes, that is, all d such that c ? ?(d).
?c =
?
{d|c??(d)}
?d
The outside ?-product ?d at disjunctive node d is computed by the following equation
if ?c is given for all mother conjunctive nodes, that is, all c such that d ? ?(c), and ?d?
for all sibling disjunctive nodes d?.
?d =
?
{c|d??(c)}
?
?
?
?
?
?
?
?c exp
?
?
?
j
?j fj(c)
?
?
?
d???(c)
d? =d
?d?
?
?
?
?
?
?
?
Figure 9 shows the overall algorithm for estimating the parameters, given a set
of feature forests. The key point of the algorithm is to compute inside ?-products ?
and outside ?-products ? for each node in C, and not for all unpacked trees. The func-
tions inside product and outside product compute ? and ? efficiently by dynamic
programming.
Note that the order in which nodes are traversed is important for incremental com-
putation, although it is not shown in Figure 9. The computation for the daughter
nodes and mother nodes must be completed before computing the inside and outside
47
Computational Linguistics Volume 34, Number 1
Figure 9
Algorithm for computing model expectations of feature forests.
?-products, respectively. This constraint is easily solved using any topological sort
algorithm. A topological sort is applied once at the beginning. The result of the sorting
does not affect the cost and the result of estimation. In our implementation, we assume
that conjunctive/disjunctive nodes are already ordered from the root node in input data.
The complexity of this algorithm is O(( ?|C|+ ?|D|) ?|F||E|), where ?|C| and ?|D| are the
average numbers of conjunctive and disjunctive nodes, respectively. This is tractable
when ?|C| and ?|D| are of a reasonable size. As noted in this section, the number of
48
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
nodes in a feature forest is usually polynomial even when that of the unpacked trees
is exponential. Thus we can efficiently compute model expectations with polynomial
computational complexity.
4. Probabilistic HPSG Parsing
Following previous studies on probabilistic models for HPSG (Oepen, Toutanova, et al
2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van
Noord 2004), we apply a maximum entropy model to HPSG parse disambiguation. The
probability, p(t|w), of producing parse result t of a given sentence w is defined as
p(t|w) = 1
Zw
p0(t|w) exp
(
?
i
?i fi(t,w)
)
where
Zw =
?
t??T(w)
p0(t
?|w) exp
(
?
i
?i fi(t
?,w)
)
where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution)
and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) rep-
resents the characteristics of t and w, and the corresponding model parameter ?i is its
weight. Model parameters that maximize the log-likelihood of the training data are
computed using a numerical optimization method (Malouf 2002).
Estimation of the model requires a set of pairs ?tw,T(w)?, where tw is the correct
parse for a sentencew. Whereas tw is provided by a treebank, T(w) has to be computed
by parsing each w in the treebank. Previous studies assumed T(w) could be enumer-
ated; however, this assumption is impractical because the size of T(w) is exponentially
related to the length of w.
Our solution here is to apply the feature forest model of Section 3 to the probabilistic
modeling of HPSG parsing. Section 4.1 briefly introduces HPSG. Section 4.2 and 4.3
describe how to represent HPSG parse trees and predicate?argument structures by
feature forests. Together with the parameter estimation algorithm in Section 3, these
methods constitute a complete method for probabilistic disambiguation. We also ad-
dress a method for accelerating the construction of feature forests for all treebank
sentences in Section 4.4. The design of feature functions will be given in Section 4.5.
4.1 HPSG
HPSG (Pollard and Sag 1994; Sag,Wasow, and Bender 2003) is a syntactic theory that fol-
lows the lexicalist framework. In HPSG, linguistic entities, such as words and phrases,
are denoted by signs, which are represented by typed feature structures (Carpenter
1992). Signs are a formal representation of combinations of phonological forms and
syntactic/semantic structures, and express which phonological form signifies which
syntactic/semantic structure. Figure 10 shows the lexical sign for loves. The geometry
of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word,
MOD denotes modifiee constraints, and SPR, SUBJ, and COMPS describe constraints
of a specifier, a syntactic subject, and complements, respectively. CONT denotes the
49
Computational Linguistics Volume 34, Number 1
Figure 10
Lexical entry for the transitive verb loves.
Figure 11
Simplified representation of the lexical entry in Figure 10.
predicate?argument structure of a phrase/sentence. The notation of CONT in this article
is borrowed from that of Minimal Recursion Semantics (Copestake et al 2006): HOOK
represents a structure accessed by other phrases, and RELS describes the remaining
structure of the semantics. In what follows, we represent signs in a reduced form as
shown in Figure 11, because of the large size of typical HPSG signs, which often include
information not immediately relevant to the point being discussed. We will only show
attributes that are relevant to an explanation, expecting that readers can fill in the values
of suppressed attributes.
50
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
In our actual implementation of the HPSG grammar, lexical/phrasal signs contain
additional attributes that are not defined in the standard HPSG theory but are used
by a disambiguation model. Examples include the surface form of lexical heads, and
the type of lexical entry assigned to lexical heads, which are respectively used for
computing the features WORD and LE introduced in Section 4.5. By incorporating ad-
ditional attributes into signs, we can straightforwardly compute feature functions for
each sign. This allows for a simple mapping between a parsing chart and a feature forest
as described subsequently. However, this might increase the size of parse forests and
therefore decrease parsing efficiency, because differences between additional attributes
interfere with equivalence relations for ambiguity packing.
4.2 Packed Representation of HPSG Parse Trees
We represent an HPSG parse tree with a set of tuples ?m, l, r?, where m, l, and r are the
signs of the mother, left daughter, and right daughter, respectively.4 In chart parsing,
partial parse candidates are stored in a chart, in which phrasal signs are identified and
packed into equivalence classes if they are judged to be equivalent and dominate the
sameword sequences. A set of parse trees is then represented as a set of relations among
equivalence classes.5
Figure 12 shows a chart for parsing he saw a girl with a telescope, where the modifiee
of with is ambiguous (saw or girl). Each feature structure expresses an equivalence class,
and the arrows represent immediate-dominance relations. The phrase, saw a girl with
a telescope, has two trees (A in the figure). Because the signs of the top-most nodes are
equivalent, they are packed into an equivalence class. The ambiguity is represented as
the two pairs of arrows leaving the node A.
A set of HPSG parse trees is represented in a chart as a tuple ?E,Er,??, where E is a
set of equivalence classes, Er ? E is a set of root nodes, and ? : E? 2E?E is a function
to represent immediate-dominance relations.
Our representation of a chart can be interpreted as an instance of a feature forest.
We map the tuple ?em, el, er?, which corresponds to ?m, l, r?, into a conjunctive node.
Figure 13 shows (a part of) the HPSG parse trees in Figure 12 represented as a feature
forest. Square boxes (ci) are conjunctive nodes, and di disjunctive nodes. A solid arrow
represents a disjunctive daughter function, and a dotted line expresses a conjunctive
daughter function.
Formally, a chart ?E,Er,?? is mapped into a feature forest ?C,D,R,?, ?? as follows.6
 C = {?em, el, er?|em ? E ? (el, er) ? ?(em)} ? {w|w ? w}
 D = E
 R = {?em, el, er?|em ? Er ? ?em, el, er? ? C}
4 For simplicity, only binary trees are considered. Extension to unary and n-ary (n > 2) trees is trivial.
5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985),
and we will discuss a method for encoding CONT in a feature forest in Section 4.3. We also assume that
parse trees are packed according to equivalence relations rather than subsumption relations (Oepen and
Carroll 2000). We cannot simply map parse forests packed under subsumption into feature forests,
because they over-generate possible unpacked trees.
6 For ease of explanation, the definition of the root node is different from the original definition given
in Section 3. In this section, we define R as a set of conjunctive nodes rather than a single node r. The
definition here is translated into the original definition by introducing a dummy root node r? that has
no features and only one disjunctive daughter whose daughters are R.
51
Computational Linguistics Volume 34, Number 1
Figure 12
Chart for parsing he saw a girl with a telescope.
Figure 13
Feature forest representation of HPSG parse trees in Figure 12.
52
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
 ?(em) =
{
{?em, el, er?|(el, er) ? ?(em)} if ?(em) = ?
{w|em is a lexical entry for w} otherwise
 ?(c) =
{
{el, er} if c = ?em, el, er?
? if c ? w
Onemay claim that restricting the domain of feature functions to ?em, el, er? limits the
flexibility of feature design. Although this is true to some extent, it does not necessarily
mean the impossibility of incorporating features on nonlocal dependencies into the
model. This is because a feature forest model does not assume probabilistic indepen-
dence of conjunctive nodes. This means that we can unpack a part of the forest without
changing the model. Actually, we successfully developed a probabilistic model includ-
ing features on nonlocal predicate?argument dependencies, as described subsequently.
4.3 Packed Representation of Predicate?Argument Structures
With the method previously described, we can represent an HPSG parsing chart with
a feature forest. However, equivalence classes in a chart might increase exponentially
because predicate?argument structures in HPSG signs represent the semantic relations
of all words that the phrase dominates. For example, Figure 14 shows phrasal signs with
predicate?argument structures for saw a girl with a telescope. In the chart in Figure 12,
these signs are packed into an equivalence class. However, Figure 14 shows that the
values of CONT, that is, predicate?argument structures, have different values, and the
signs as they are cannot be equivalent. As seen in this example, predicate?argument
structures prevent us from packing signs into equivalence classes.
In this section, we apply the feature forest model to predicate?argument structures,
which may include reentrant structures and non-local dependencies. It is theoretically
difficult to apply the feature forest model to predicate?argument structures; a feature
forest cannot represent graph structures that include reentrant structures in a straight-
forward manner. However, if predicate?argument structures are constructed as in the
manner described subsequently, they can be represented by feature forests of a tracta-
ble size.
Feature forests can represent predicate?argument structures if we assume some
locality and monotonicity in the composition of predicate?argument structures.
Locality: In each step of composition of a predicate?argument structure, only a
limited depth of the daughters? predicate?argument structures are referred to.
That is, local structures in the deep descendent phrases may be ignored to
construct larger phrases. This assumption means that predicate?argument
structures can be packed into conjunctive nodes by ignoring local structures.
Figure 14
Signs with predicate?argument structures.
53
Computational Linguistics Volume 34, Number 1
Monotonicity: All relations in the daughters? predicate?argument structures
are percolated to the mother. That is, none of the predicate?argument
relations in the daughter phrases disappear in the mother. Thus
predicate?argument structures of descendent phrases can be located at
lower nodes in a feature forest.
Predicate?argument structures usually satisfy the above conditions, evenwhen they
include non-local dependencies. For example, Figure 15 shows HPSG lexical entries
for the wh-extraction of the object of love (left) and for the control construction of try
(right). The first condition is satisfied because both lexical entries refer to CONT|HOOK
of argument signs in SUBJ, COMPS, and SLASH. None of the lexical entries directly
access ARGX of the arguments. The second condition is also satisfied because the values
of CONT|HOOK of all of the argument signs are percolated to ARGX of the mother. In
addition, the elements in CONT|RELS are percolated to the mother by the Semantic Prin-
ciple. Compositional semantics usually satisfies the above conditions, including MRS
(Copestake et al 1995, 2006). The composition of MRS refers to HOOK, and no internal
structures of daughters. The Semantic Principle of MRS also assures that all semantic
relations in RELS are percolated to the mother. When these conditions are satisfied,
semantics may include any constraints, such as selectional restrictions, although the
grammar we used in the experiments does not include semantic restrictions to constrain
parse forests.
Under these conditions, local structures of predicate?argument structures are en-
coded into a conjunctive node when the values of all of its arguments have been
instantiated. We introduce the notion of inactives to denote such local structures.
Definition 10 (Inactives)
An inactive is a subset of predicate?argument structures in which all arguments have
been instantiated.
Because inactive parts will not change during the rest of the parsing process, they can
be placed in a conjunctive node. By placing newly generated inactives into correspond-
ing conjunctive nodes, a set of predicate?argument structures can be represented in a
feature forest by packing local ambiguities, and non-local dependencies are preserved.
Figure 16 illustrates a process of parsing the sentence She ignored the fact that I wanted
to dispute, where dispute has an ambiguity (dispute1, intransitive, and dispute2, transitive)
Figure 15
Lexical entries including non-local relations.
54
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 16
Process of composing predicate?argument structures.
Figure 17
Predicate?argument structures of dispute.
and factmay optionally take a complementizer phrase.7 The predicate?argument struc-
tures for dispute1 and dispute2 are shown in Figure 17. Curly braces express the am-
biguities of partially constructed predicate?argument structures. The resulting feature
forest is shown in Figure 18. The boxes denote conjunctive nodes and dx represent
disjunctive nodes.
The clause I wanted to dispute has two possible predicate?argument structures: one
corresponding to dispute1 (? in Figure 16) and the other corresponding to dispute2 (?
in Figure 16). The nodes of the predicate?argument structure ? are all instantiated, that
is, it contains only inactives. The corresponding conjunctive node (?? in Figure 18) has
two inactives, for want and dispute1. The other structure ? has an unfilled object in the
argument (ARG28) of dispute2, which will be filled by the non-local dependency. Hence,
the corresponding conjunctive node ?? has only one inactive corresponding to want,
and the remaining part that corresponds to dispute2 is passed on for further processing.
When we process the phrase the fact that I wanted to dispute, the object of dispute2 is filled
by fact (? in Figure 16), and the predicate?argument structure of dispute2 is then placed
into a conjunctive node (?? in Figure 18).
7 In Figure 16, feature structures of different nodes of parse trees are assigned distinct variables, even when
they are from the same lexical entries. This is because feature structures are copied during chart parsing.
Although these variables are from the same lexical entry, it is copied to several chart items, and hence
there are no structure sharings among them.
8 ? (bottom) represents an uninstantiated value (Carpenter 1992).
55
Computational Linguistics Volume 34, Number 1
Figure 18
A feature forest representation of predicate?argument structures.
One of the beneficial characteristics of this packed representation is that the rep-
resentation is isomorphic to the parsing process, that is, a chart. Hence, we can assign
features of HPSG parse trees to a conjunctive node, together with features of predicate?
argument structures. In Section 5, we will investigate the contribution of features on
parse trees and predicate?argument structures to the disambiguation of HPSG parsing.
4.4 Filtering by Preliminary Distribution
The method just described is the essence of our solution for the tractable estimation
of maximum entropy models on exponentially many HPSG parse trees. However,
the problem of computational cost remains. Construction of feature forests requires
parsing of all of the sentences in a treebank. Despite the development of methods to
improve HPSG parsing efficiency (Oepen, Flickinger, et al 2002), exhaustive parsing of
all sentences is still expensive.
We assume that computation of parse trees with low probabilities can be omitted
in the estimation stage because T(w) can be approximated by parse trees with high
probabilities. To achieve this, we first prepared a preliminary probabilistic model whose
estimation did not require the parsing of a treebank. The preliminary model was used
to reduce the search space for parsing a training treebank.
The preliminary model in this study is a unigram model, p?(t|w) =
?
w?w p(l|w),
where w ? w is a word in the sentence w, and l is a lexical entry assigned to w. This
model is estimated by counting the relative frequencies of lexical entries used for w in
the training data. Hence, the estimation does not require parsing of a treebank. Actually,
we use a maximum entropymodel to compute this probability as described in Section 5.
56
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
The preliminarymodel is used for filtering lexical entries whenwe parse a treebank.
Given this model, we restrict the number of lexical entries used to parse a treebank.With
a threshold n for the number of lexical entries and a threshold  for the probability,
lexical entries are assigned to a word in descending order of probability, until the
number of assigned entries exceeds n, or the accumulated probability exceeds . If this
procedure does not assign a lexical entry necessary to produce a correct parse (i.e., an
oracle lexical entry), it is added to the list of lexical entries. It should be noted that oracle
lexical entries are given by the HPSG treebank. This assures that the filtering method
does not exclude correct parse trees from parse forests.
Figure 19 shows an example of filtering the lexical entries assigned to saw. With  =
0.95, four lexical entries are assigned. Although the lexicon includes other lexical entries,
such as a verbal entry taking a sentential complement (p = 0.01 in the figure), they are
filtered out. Although this method reduces the time required for parsing a treebank, this
approximation causes bias in the training data and results in lower accuracy. The trade-
off between parsing cost and accuracy will be examined experimentally in Section 5.4.
We have several ways to integrate p? with the estimated model p(t|T(w)). In the
experiments, we will empirically compare the following methods in terms of accuracy
and estimation time.
Filtering only: The unigram probability p? is used only for filtering in training.
Product: The probability is defined as the product of p? and the estimated model p.
Reference distribution: p? is used as a reference distribution of p.
Feature function: log p? is used as a feature function of p. This method has been
shown to be a generalization of the reference distribution method (Johnson
and Riezler 2000).
4.5 Features
Feature functions in maximum entropy models are designed to capture the characteris-
tics of ?em, el, er?. In this article, we investigate combinations of the atomic features listed
Figure 19
Filtering of lexical entries for saw.
57
Computational Linguistics Volume 34, Number 1
Table 1
Templates for atomic features.
RULE name of the applied schema
DIST distance between the head words of the daughters
COMMA whether a comma exists between daughters and/or inside of daughter phrases
SPAN number of words dominated by the phrase
SYM symbol of the phrasal category (e.g., NP, VP)
WORD surface form of the head word
POS part-of-speech of the head word
LE lexical entry assigned to the head word
ARG argument label of a predicate
in Table 1. The following combinations are used for representing the characteristics of
binary/unary schema applications.
fbinary =
?
RULE,DIST,COMMA,
SPANl, SYMl, WORDl, POSl, LEl,
SPANr, SYMr, WORDr, POSr, LEr
?
funary = ?RULE,SYM,WORD,POS,LE?
where subscripts l and r denote left and right daughters.
In addition, the following is used for expressing the condition of the root node of
the parse tree.
froot = ?SYM,WORD,POS,LE?
Feature functions to capture predicate?argument dependencies are represented as
follows:
fpa =
?
ARG, DIST, WORDp, POSp, LEp, WORDa, POSa, LEa
?
where subscripts p and a represent predicate and argument, respectively.
Figure 20 shows examples: froot is for the root node, in which the phrase symbol
is S and the surface form, part-of-speech, and lexical entry of the lexical head are saw,
VBD, and a transitive verb, respectively. fbinary is for the binary rule application to saw a
girl and with a telescope, in which the applied schema is the Head-Modifier Schema, the
left daughter is VP headed by saw, and the right daughter is PP headed by with, whose
part-of-speech is IN and whose lexical entry is a VP-modifying preposition.
Figure 21 shows example features for predicate?argument structures. The figure
shows features assigned to the conjunctive node denoted as ?? in Figure 18. Because
inactive structures in the node have three predicate?argument relations, three features
are activated. The first one is for the relation of want and I, where the label of the relation
is ARG1, the distance between the head words is 1, the surface string and the POS of
58
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 20
Example features for binary schema application and root condition.
Figure 21
Example features for predicate?argument structures.
the predicate are want and VBD, and those of the argument are I and PRP. The second
and the third features are for the other two relations. We may include features on more
than two relations, such as the dependencies among want, I, and dispute, although such
features are not incorporated currently.
In our implementation, some of the atomic features are abstracted (i.e., ignored) for
smoothing. Tables 2, 3, and 4 show the full set of templates of combined features used in
the experiments. Each row represents the template for a feature function. A check indi-
cates the atomic feature is incorporated, and a hyphen indicates the feature is ignored.
59
Computational Linguistics Volume 34, Number 1
Table 2
Feature templates for binary schema (left) and unary schema (right).
RULE DIST COMMA SPAN SYM WORD POS LE
? ? ?
? ?
? ? ?
? ? ?
? ?
? ?
?? ? ?
? ?
?
?
?
? ? ?
?
? ?
? ??
?
? ?
?
? ? ?
?
?
? ?
?
? ?
??
?
? ?
?
?
?
?
?
?
? ? ? ?
? ?? ? ?
? ? ?
? ?
? ? ?
? ? ?
?
?? ? ?
? ? ? ?
?
? ? ?
?
?
? ? ??
?
? ?
? ?
? ?
?
?
? ?
? ?
?
??
?
? ?
? ? ?
?
?
?
? ? ?
? ? ?
RULE SYM WORD POS LE
?
?
? ? ?
?
?
? ?
??
?
?
?
?
? ? ?
? ??
? ?
? ?
?
? ?
?
??
? ? ?
?
? ?
? ? ?
Table 3
Feature templates for root condition.
SYM WORD POS LE
?
? ? ?
?
? ?
?
?
?
?
?
? ?
? ?
? ?
? ?
? ?
?
?
? ? ?
?
?
? ? ?
Table 4
Feature templates for predicate?argument dependencies.
ARG DIST WORD POS LE
? ? ? ? ?
? ? ?
?
?
? ?
?
? ?
?
?
? ? ?
?
?
?
?
?
?
? ?
? ?
? ? ? ?
?? ? ?
? ?? ?
?
?
??
?
? ?
??
?
?
? ??
? ?
?
?
60
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
5. Experiments
This section presents experimental results on the parsing accuracy attained by the
feature forest models. In all of the following experiments, we use the HPSG grammar
developed by the method of Miyao, Ninomiya, and Tsujii (2005). Section 5.1 describes
how this grammarwas developed. Section 5.2 explains other aspects of the experimental
settings. In Sections 5.3 to 5.7, we report results of the experiments on HPSG parsing.
5.1 The HPSG Grammar
In the following experiments, we use Enju 2.1 (Tsujii Laboratory 2004), which is a wide-
coverage HPSG grammar extracted from the Penn Treebank by the method of Miyao,
Ninomiya, and Tsujii (2005). In this method, we convert the Penn Treebank into an
HPSG treebank, and collect HPSG lexical entries from terminal nodes of the HPSG
treebank. Figure 22 illustrates the process of treebank conversion and lexicon collection.
We first convert and fertilize parse trees of the Penn Treebank. This step identifies
syntactic constructions that require special treatment in HPSG, such as raising/control
and long-distance dependencies. These constructions are then annotated with typed
feature structures so that they conform to the HPSG analysis. Next, we apply HPSG
schemas and principles, and obtain fully specified HPSG parse trees. This step solves
feature structure constraints given in the previous step, and fills unspecified constraints.
Failures of schema/principle applications indicate that the annotated constraints do not
Figure 22
Extracting HPSG lexical entries from the Penn Treebank.
61
Computational Linguistics Volume 34, Number 1
conform to the HPSG analysis, and require revisions. Finally, we obtain lexical entries
from the HPSG parse trees. The terminal nodes of HPSG parse trees are collected, and
they are generalized by removing word-specific or context-specific constraints.
An advantage of this method is that a wide-coverage HPSG lexicon is obtained
because lexical entries are extracted from real-world sentences. Obtained lexical entries
are guaranteed to construct well-formed HPSG parse trees because HPSG schemas
and principles are successfully applied during the development of the HPSG treebank.
Another notable feature is that we can additionally obtain an HPSG treebank, which
can be used as training data for disambiguation models. In the following experiments,
this HPSG treebank is used for the training of maximum entropy models.
The lexicon used in the following experiments was extracted from Sections 02?21
of the Wall Street Journal portion of the Penn Treebank. This lexicon can assign correct
lexical entries to 99.09% of words in the HPSG treebank converted from Penn Treebank
Section 23. This number expresses ?lexical coverage? in the strong sense defined by
Hockenmaier and Steedman (2002). In this notion of ?coverage,? this lexicon has 84.1%
sentential coverage, where this means that the lexicon can assign correct lexical entries
to all of the words in a sentence. Although the parser might produce parse results for
uncovered sentences, these parse results cannot be completely correct.
5.2 Experimental Settings
The data for the training of the disambiguation models was the HPSG treebank derived
from Sections 02?21 of the Wall Street Journal portion of the Penn Treebank, that is, the
same set used for lexicon extraction. For training of the disambiguation models, we
eliminated sentences of 40 words or more and sentences for which the parser could not
produce the correct parses. The resulting training set consists of 33,604 sentences (when
n = 10 and  = 0.95; see Section 5.4 for details). The treebanks derived from Sections
22 and 23 were used as the development and final test sets, respectively. Following
previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000),
accuracy is measured for sentences of less than 40 words and for those with less than
100 words. Table 5 shows the specifications of the test data.
The measure for evaluating parsing accuracy is precision/recall of predicate?
argument dependencies output by the parser. A predicate?argument dependency is
defined as a tuple ?wh,wn,?,??, where wh is the head word of the predicate, wn is the
head word of the argument, ? is the type of the predicate (e.g., adjective, intransitive
verb), and ? is an argument label (MODARG, ARG1, . . ., ARG4). For example, He tried
running has three dependencies as follows:
 ?tried, he, transitive verb,ARG1?
Table 5
Specification of test data for the evaluation of parsing accuracy.
No. of Sentences Avg. Length
Test set (Section 23, < 40 words) 2,144 20.52
Test set (Section 23, < 100 words) 2,299 22.23
Development set (Section 22, < 40 words) 1,525 20.69
Development set (Section 22, < 100 words) 1,641 22.43
62
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
 ?tried, running, transitive verb,ARG2?
 ?running, he, intransitive verb,ARG1?
Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser,
and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identified
regardless of ? and ?. F-score is the harmonic mean of LP and LR. Sentence accuracy
is the exact match accuracy of complete predicate?argument relations in a sentence.
These measures correspond to those used in other studies measuring the accuracy of
predicate?argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman
2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al 2004),
although exact figures cannot be compared directly because the definitions of depen-
dencies are different. All predicate?argument dependencies in a sentence are the target
of evaluation except quotation marks and periods. The accuracy is measured by parsing
test sentences with gold-standard part-of-speech tags from the Penn Treebank unless
otherwise noted.
The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its
hyper-parameter was tuned for each model to maximize F-score for the development
set. The algorithm for parameter estimation was the limited-memory BFGS method
(Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with
the LiLFeS library (Makino et al 2002), and various speed-up techniques for HPSG
parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and
Tsujii 2004; Ninomiya et al 2005). Other efficient parsing techniques, including global
thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were
not used. The results obtained using these techniques are given in Ninomiya et al A
limit on the number of constituents was set for time-out; the parser stopped parsing
when the number of constituents created during parsing exceeded 50,000. In such a
case, the parser output nothing, and the recall was computed as zero.
Features occurring more than twice were included in the probabilistic models. A
method of filtering lexical entries was applied to the parsing of training data (Sec-
tion 4.4). Unless otherwise noted, parameters for filtering were n = 10 and  = 0.95, and
a reference distribution method was applied. The unigram model, p0(t|s), for filtering is
a maximum entropy model with two feature templates, ?WORD, POS, LE? and ?POS, LE?.
The model includes 24,847 features.
5.3 Efficacy of Feature Forest Models
Tables 6 and 7 show parsing accuracy for the test set. In the tables, ?Syntactic features?
denotes a model with syntactic features, that is, fbinary, funary, and froot introduced
Table 6
Accuracy of predicate?argument relations (test set, <40 words).
LP LR UP UR F-score Sentence acc.
Baseline 78.10 77.39 82.83 82.08 77.74 18.3
Syntactic features 86.92 86.28 90.53 89.87 86.60 36.3
Semantic features 84.29 83.74 88.32 87.75 84.01 30.9
All 86.54 86.02 90.32 89.78 86.28 36.0
63
Computational Linguistics Volume 34, Number 1
Table 7
Accuracy of predicate?argument relations (test set, <100 words).
LP LR UP UR F-score Sentence acc.
Baseline 77.58 76.84 82.22 81.43 77.21 17.1
Syntactic features 86.47 85.83 90.06 89.40 86.15 34.1
Semantic features 83.81 83.26 87.75 87.16 83.53 28.9
All 86.13 85.59 89.85 89.29 85.86 33.8
in Section 4.5. ?Semantic features? represents a model with features on predicate?
argument structures, that is, fpa given in Table 4. ?All? is a model with both syntactic
and semantic features. The ?Baseline? row shows the results for the reference model,
p0(t|s), used for lexical entry filtering in the estimation of the other models. This model
is considered as a simple application of a traditional PCFG-style model; that is, p(r) = 1
for any rule r in the construction rules of the HPSG grammar.
The results demonstrate that feature forest models have significantly higher ac-
curacy than a baseline model. Comparing ?Syntactic features? with ?Semantic fea-
tures,? we see that the former model attained significantly higher accuracy than the
latter. This indicates that syntactic features are more important for overall accuracy.
We will examine the contributions of each atomic feature of the syntactic features in
Section 5.5.
Features on predicate?argument relations were generally considered as important
for the accurate disambiguation of syntactic structures. For example, PP-attachment
ambiguity cannot be resolved with only syntactic preferences. However, the results
show that a model with only semantic features performs significantly worse than one
with syntactic features. Even when combined with syntactic features, semantic features
do not improve accuracy. Obviously, semantic preferences are necessary for accurate
parsing, but the features used in this work were not sufficient to capture semantic pref-
erences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies
may be too sparse to capture semantic preferences.
For reference, our results are competitive with the best corresponding results re-
ported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our
results cannot be compared directly with other grammar formalisms because each
formalism represents predicate?argument dependencies differently. In contrast with the
results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly
lower than precision. This may have resulted from the HPSG grammar having stricter
feature constraints and the parser not being able to produce parse results for around
1% of the sentences. To improve recall, we need techniques to deal with these 1% of
sentences.
Table 8 gives the computation/space costs of model estimation. ?Estimation time?
indicates user times required for running the parameter estimation algorithm. ?No. of
feature occurrences? denotes the total number of occurrences of features in the training
data, and ?Data size? gives the sizes of the compressed files of training data. We can
conclude that feature forest models are estimated at a tractable computational cost and
a reasonable data size, even when a model includes semantic features including non-
local dependencies. The results reveal that feature forest models essentially solve the
problem of the estimation of probabilistic models of sentence structures.
64
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Table 8
Computation/space costs of model estimation.
No. of features Estimation No. of feature Data size
time (sec.) occurrences (MB)
Baseline 24,847 499 6,948,364 21
Syntactic features 599,104 511 127,497,615 727
Semantic features 334,821 278 176,534,753 375
All 933,925 716 304,032,368 1,093
Table 9
Estimation method vs. accuracy and estimation time.
LP LR F-score Estimation time (sec.)
Filtering only 51.70 49.89 50.78 449
Product 86.50 85.94 86.22 1,568
Reference distribution 86.92 86.28 86.60 511
Feature function 84.81 84.09 84.45 945
5.4 Comparison of Filtering Methods
Table 9 compares the estimation methods introduced in Section 4.4. In all of the follow-
ing experiments, we show the accuracy for the test set (<40 words) only. Table 9 reveals
that our method achieves significantly lower accuracy when it is used only for filtering
in the training phrase. One reason is that the feature forest model prefers lexical entries
that are filtered out in the training phase, because they are always oracle lexical entries
in the training. This means that we must incorporate the preference of filtering into the
final parse selection. As shown in Table 9, the models combined with a preliminary
model achieved sufficient accuracy. The reference distribution method achieved higher
accuracy and lower cost. The feature function method achieved lower accuracy in our
experiments. A possible reason for this is that a hyper-parameter of the prior was set to
the same value for all the features including the feature of the log-probability given by
the preliminary distribution.
Tables 10 and 11 show the results of changing the filtering threshold. We can
determine the correlation between the estimation/parsing cost and accuracy. In our
experiment, n ? 10 and  ? 0.90 seem necessary to preserve the F-score over 86.0.
5.5 Contribution of Features
Table 12 shows the accuracy with different feature sets. Accuracy was measured for 15
models with some atomic features removed from the final model. The last row denotes
the accuracy attained by the unigram model (i.e., the reference distribution). The num-
bers in bold type represent a significant difference from the final model according to
stratified shuffling tests with the Bonferroni correction (Cohen 1995) with p-value < .05
for 32 pairwise comparisons. The results indicate that DIST, COMMA, SPAN, WORD, and
65
Computational Linguistics Volume 34, Number 1
Table 10
Filtering threshold vs. accuracy.
n, LP LR F-score Sentence acc.
5, 0.80 85.09 84.30 84.69 32.4
5, 0.90 85.44 84.61 85.02 32.5
5, 0.95 85.52 84.66 85.09 32.7
5, 0.98 85.50 84.63 85.06 32.6
10, 0.80 85.60 84.65 85.12 32.5
10, 0.90 86.49 85.92 86.20 34.7
10, 0.95 86.92 86.28 86.60 36.3
10, 0.98 87.18 86.66 86.92 37.7
15, 0.80 85.59 84.63 85.11 32.4
15, 0.90 86.48 85.80 86.14 35.7
15, 0.95 87.21 86.68 86.94 37.0
15, 0.98 87.69 87.16 87.42 39.2
Table 11
Filtering threshold vs. estimation cost.
n, Estimation time (sec.) Parsing time (sec.) Data size (MB)
5, 0.80 108 5,103 341
5, 0.90 150 6,242 407
5, 0.95 190 7,724 469
5, 0.98 259 9,604 549
10, 0.80 130 6,003 370
10, 0.90 268 8,855 511
10, 0.95 511 15,393 727
10, 0.98 1,395 36,009 1,230
15, 0.80 123 6,298 372
15, 0.90 259 9,543 526
15, 0.95 735 20,508 854
15, 0.98 3,777 86,844 2,031
POS features contributed to the final accuracy, although the differences were slight. In
contrast, RULE, SYM, and LE features did not affect accuracy. However, when each was
removed together with another feature, the accuracy decreased drastically. This implies
that such features carry overlapping information.
5.6 Factors for Parsing Accuracy
Table 13 shows parsing accuracy for covered and uncovered sentences. As defined in
Section 5.1, ?covered? indicates that the HPSG lexicon has all correct lexical entries for a
sentence. In other words, for covered sentences, exactly correct parse trees are obtained
if the disambiguation model worked perfectly. The result reveals clear differences in
accuracy between covered and uncovered sentences. The F-score for covered sentences
is around 2.5 points higher than the overall F-score, whereas the F-score is more than
10 points lower for uncovered sentences. This result indicates improvement of lexicon
quality is an important factor for higher accuracy.
66
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Table 12
Accuracy with different feature sets.
Features LP LR F-score Sentence acc. No. of features
All 86.92 86.28 86.60 36.3 599,104
?RULE 86.83 86.19 86.51 36.3 596,446
?DIST 86.52 85.96 86.24 35.7 579,666
?COMMA 86.31 85.81 86.06 34.4 584,040
?SPAN 86.32 85.75 86.03 35.5 559,490
?SYM 86.74 86.16 86.45 35.4 406,545
?WORD 86.39 85.77 86.08 35.3 91,004
?POS 86.18 85.61 85.89 34.1 406,545
?LE 86.91 86.32 86.61 36.8 387,938
?DIST,SPAN 85.39 84.82 85.10 33.1 270,467
?DIST,SPAN,COMMA 83.75 83.25 83.50 28.9 261,968
?RULE,DIST,SPAN,COMMA 83.44 82.93 83.18 27.6 259,372
?WORD,LE 86.40 85.81 86.10 34.7 25,429
?WORD,POS 85.44 84.87 85.15 32.7 40,102
?WORD,POS,LE 84.68 84.12 84.40 31.1 8,899
?SYM,WORD,POS,LE 82.77 82.14 82.45 24.9 1,914
None 78.10 77.39 77.74 18.3 0
Table 13
Accuracy for covered/uncovered sentences.
LP LR F-score Sentence acc. No. of sentences
covered sentences 89.36 88.96 89.16 42.2 1,825
uncovered sentences 75.57 74.04 74.80 2.5 319
Figure 23 shows the learning curve. A feature set was fixed, and the parameter of
the Gaussian prior was optimized for each model. High accuracy is attained even with a
small training set, and the accuracy seems to be saturated. This indicates that we cannot
further improve the accuracy simply by increasing the size of the training data set. The
exploration of new types of features is necessary for higher accuracy. It should also be
noted that the upper bound of the accuracy is not 100%, because the grammar cannot
produce completely correct parse results for uncovered sentences.
Figure 24 shows the accuracy for each sentence length. It is apparent from this
figure that the accuracy is significantly higher for sentences with less than 10 words.
This implies that experiments with only short sentences overestimate the performance
of parsers. Sentences with at least 10 words are necessary to properly evaluate the
performance of parsing real-world texts. The accuracies for the sentences with more
than 10 words are not very different, although data points for sentences with more than
50 words are not reliable.
Table 14 shows the accuracies for predicate?argument relations when parts-
of-speech tags are assigned automatically by a maximum-entropy-based parts-of-
speech tagger (Tsuruoka and Tsujii 2005). The results indicate a drop of about three
points in labeled precision/recall (a two-point drop in unlabeled precision/recall).
A reason why we observed larger accuracy drops in labeled precision/recall is that
67
Computational Linguistics Volume 34, Number 1
Figure 23
Corpus size vs. accuracy.
Figure 24
Sentence length vs. accuracy.
predicate?argument relations are fragile with respect to parts-of-speech errors because
predicate types (e.g., adjective, intransitive verb) are determined depending on the
parts-of-speech of predicate words. Although our current parsing strategy assumes that
parts-of-speech are given beforehand, for higher accuracy in real application contexts,
we will need a method for determining parts-of-speech and parse trees jointly.
Table 14
Accuracy with automatic parts-of-speech tags (test set).
LP LR UP UR F-score Sentence acc.
<40 words 83.88 82.84 88.83 87.73 83.36 30.1
<100 words 83.45 82.40 88.37 87.26 82.92 28.2
68
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
5.7 Analysis of Disambiguation Errors
Table 15 shows amanual classification of the causes of disambiguation errors in 100 sen-
tences randomly chosen from Section 00. In our evaluation, one error source may cause
multiple dependency errors. For example, if an incorrect lexical entry is assigned to a
verb, all of the argument dependencies of the verb are counted as errors. The numbers
in the table include such double-counting. Figure 25 shows examples of disambiguation
errors. The figure shows output from the parser.
Major causes are classified into three types: attachment ambiguity, argument/
modifier distinction, and lexical ambiguity. As attachment ambiguities are well-known
error sources, PP-attachment is the largest source of errors in our evaluation. Our
disambiguation model cannot accurately resolve PP-attachment ambiguities because it
does not include dependencies among a modifiee and the argument of the preposition.
Because previous studies revealed that such dependencies are effective features for
PP-attachment resolution, we should incorporate them into our model. Some of the
attachment ambiguities, including adjective and adverb, should also be resolved
with an extension of features. However, we cannot identify any effective features
for the disambiguation of attachment of verbal phrases, including relative clauses,
verb phrases, subordinate clauses, and to-infinitives. For example, Figure 25 shows
an example error of the attachment of a relative clause. The correct answer is that the
Table 15
Classification of disambiguation errors.
Error cause No. of errors
Attachment ambiguity prepositional phrase 32
relative clause 14
adjective 7
adverb 6
verb phrase 5
subordinate clause 3
to-infinitive 3
others 6
Argument/modifier distinction to-infinitive 19
noun phrase 7
verb phrase 7
subordinate clause 7
others 9
Lexical ambiguity preposition/modifier 13
verb subcategorization frame 13
participle/adjective 12
others 6
Test set errors errors of treebank conversion 18
errors of Penn Treebank 4
Comma 32
Noun phrase identification 15
Coordination/insertion 15
Zero-pronoun resolution 9
Others 4
69
Computational Linguistics Volume 34, Number 1
Figure 25
Examples of disambiguation errors.
subject of yielded is acre, but this cannot be determined only by the relation among yield,
grapes, and acre. The resolution of these errors requires a novel type of feature function.
Errors of argument/modifier distinction are prominent in deep syntactic analysis,
because arguments and modifiers are not explicitly distinguished in the evaluation of
CFG parsers. Figure 25 shows an example of the argument/modifier distinction of a
to-infinitive clause. In this case, the to-infinitive clause is a complement of tempts. The
subcategorization frame of tempts seems responsible for this problem. However, the
disambiguation model wrongly assigned a lexical entry for a transitive verb because
of the sparseness of the training data (tempts occurred only once in the training data).
The resolution of this sort of ambiguity requires the refinement of a probabilistic model
of lexical entries. Errors of verb phrases and subordinate clauses are similar to this
example. Errors of argument/modifier distinction of noun phrases aremainly caused by
temporal nouns and cardinal numbers. The resolution of these errors seems to require
the identification of temporal expressions and usage of cardinal numbers.
Errors of lexical ambiguities were mainly caused by idioms. For example, in Fig-
ure 25, compared with is a compound preposition, but the parser recognized it as a
verb phrase. This indicates that the grammar or the disambiguation model requires
the special treatment of idioms. Errors of verb subcategorization frames were mainly
caused by difficult constructions such as insertions. Figure 25 shows that the parser
could not identify the inserted clause (says John Siegel. . .) and a lexical entry for a
declarative transitive verb was chosen.
Attachment errors of commas are also significant. It should be noted that commas
were ignored in the evaluation of CFG parsers. We did not eliminate punctuation
from the evaluation because punctuation sometimes contributes to semantics, as
in coordination and insertion. In this error analysis, errors of commas representing
coordination/insertion are classified into ?coordination/insertion,? and ?comma? in-
dicates errors that do not contribute to the computation of semantics.
Errors of noun phrase identification mean that a noun phrase was split into two
phrases. These errors were mainly caused by the indirect effects of other errors.
70
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Errors of identifying coordination/insertion structures sometimes resulted in
catastrophic analyses. While accurate analysis of such constructions is indispensable,
it is also known to be difficult because disambiguation of coordination/insertion
requires the computation of preferences over global structures, such as the similarity
of syntactic/semantic structure of coordinates. Incorporating features for representing
the similarity of global structures is difficult for feature forest models.
Zero-pronoun resolution is also a difficult problem. However, we found that
most were indirectly caused by errors of argument/modifier distinction in to-infinitive
clauses.
A significant portion of the errors discussed above cannot be resolved by the fea-
tures we investigated in this study, and the design of other features will be necessary
for improving parsing accuracy.
6. Discussion
6.1 Probabilistic Modeling of Complete Structures
The model described in this article was first published in Miyao and Tsujii (2002), and
has been applied to probabilistic models for parsing with lexicalized grammars. Appli-
cations to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al
2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained
higher accuracy than other models. These researchers applied feature forests to repre-
sentations of the packed parse results of LFG and the dependency/derivation structures
of CCG. Their work demonstrated the applicability and effectiveness of feature forest
models in parsing with wide-coverage lexicalized grammars. Feature forest models
were also shown to be effective for wide-coverage sentence realization (Nakanishi,
Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic
enough to be applied to natural language processing tasks other than parsing.
The work of Geman and Johnson (2002) independently developed a dynamic pro-
gramming algorithm for maximum entropy models. The solution was similar to our
approach, although their method was designed to traverse LFG parse results repre-
sented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995).
The difference between the two approaches is that feature forests use a simpler generic
data structure to represent packed forest structures. Therefore, without assuming what
feature forests represent, our algorithm can be applied to various tasks, including
theirs.
Another approach to the probabilistic modeling of complete structures is a method
of approximation. The work on whole sentence maximum entropy models (Rosenfeld
1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimate
parameters of maximum entropy models on whole sentence structures. However, the
algorithm suffered from slow convergence, and the model was basically a sequence
model. It could not produce a solution for complex structures as our model can.
We should also mention Conditional Random Fields (CRFs) (Lafferty, McCallum,
and Pereira 2001) for solving a similar problem in the context of maximum entropy
Markov models. Their solution was an algorithm similar to the computation of
forward/backward probabilities of hidden Markov models (HMMs). Their algorithm is
a special case of our algorithm in which each conjunctive node has only one daughter.
This is obvious because feature forests can represent Markov chains. In an analogy,
CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs.
71
Computational Linguistics Volume 34, Number 1
Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also
regarded as instances of feature forest models. This fact implies that our algorithm is
applicable to not only parsing but also to other tasks. CRFs are now widely used for
sequence-based tasks, such as parts-of-speech tagging and named entity recognition,
and have been shown to achieve the best performance in various tasks (McCallum and
Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al 2003; Sha and Pereira
2003; Peng and McCallum 2004; Roark et al 2004; Settles 2004; Sutton, Rohanimanesh,
and McCallum 2004). These results suggest that the method proposed in the present
article will achieve high accuracy when applied to various statistical models with
tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton,
Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for
extending feature forest models. The purpose of dynamic CRFs is to incorporate feature
functions that are not represented locally, and the solution is to apply a variational
method, which is an algorithm of numerical computation, to obtain approximate so-
lutions. A similar method may be developed to overcome a bottleneck of feature forest
models, that is, the fact that feature functions are localized to conjunctive nodes.
The structure of feature forests is common in natural language processing and
computational linguistics. As is easily seen, lattices, Markov chains, and CFG parse
trees are represented by feature forests. Furthermore, because conjunctive nodes do
not necessarily represent CFG nodes or rules and terminals of feature forests need
not be words, feature forests can express any forest structures in which ambiguities
are packed in local structures. Examples include the derivation trees of LTAG and
CCG. Chiang (2003) proved that feature forests could be considered as the derivation
forests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi
1987; Weir 1988). LCFRSs define a wide variety of grammars, including LTAG and
CCG, while preserving polynomial-time complexity of parsing. This demonstrates that
feature forest models are applicable to probabilistic models far beyond PCFGs. Feature
forests are also isomorphic to support graphs (or explanation graphs) used in the graphical
EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic pro-
gramming language, PRISM (Sato and Kameya 1997), is converted into support graphs,
and parameters of probabilistic models are automatically learned by an EM algorithm.
Support graphs have been proved to represent various statistical structural models, in-
cluding HMMs, PCFGs, Bayesian networks, and many other graphical structures (Sato
and Kameya 2001; Sato 2005). Taken together, these results imply the high applicability
of feature forest models to various real tasks.
Because feature forests have a structure isomorphic to parse forests of PCFG, it
might seem that they can represent only immediate dominance relations of CFG rules
as in PCFG, resulting in only a slight, trivial extension of PCFG. As described herein,
however, feature forests can represent structures beyond CFG parse trees. Furthermore,
because feature forests are a generalized representation of ambiguous structures, each
node in a feature forest need not correspond to a node in a PCFG parse forest. That is,
a node in a feature forest may represent any linguistic entity, including a fragment of a
syntactic structure, a semantic relation, or other sentence-level information.
The idea of feature forest models could be applied to non-probabilistic machine
learning methods. Taskar et al (2004) proposed a dynamic programming algorithm
for the learning of large-margin classifiers including support vector machines (Vapnik
1995), and presented its application to disambiguation in CFG parsing. Their algorithm
resembles feature forest models; an optimization function is computed by a dynamic
programing algorithmwithout unpacking packed forest structures. From the discussion
in this article, it is evident that if the main part of an update formula is represented
72
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
with (the exponential of) linear combinations, a method similar to feature forest models
should be applicable.
6.2 Probabilistic Parsing with Lexicalized Grammars
Before the advent of feature forest models, studies on probabilistic models of HPSG
adopted conventional maximum entropy models to select the most probable parse from
parse candidates given by HPSG grammars (Oepen, Toutanova, et al 2002; Toutanova
and Manning 2002; Baldridge and Osborne 2003). The difference between these studies
and our work is that we used feature forests to avoid the exponential increase in the
number of structures that results from unpacked parse results. These studies ignored
the problem of exponential explosion; in fact, training sets in these studies were very
small and consisted only of short sentences. A possible approach to avoid this problem
is to develop a fully restrictive grammar that never causes an exponential explosion, al-
though the development of such a grammar requires considerable effort and it cannot be
acquired from treebanks using existing approaches.We think that exponential explosion
is inevitable, particularly with the large-scale wide-coverage grammars required to an-
alyze real-world texts. In such cases, these methods of model estimation are intractable.
Another approach to estimating log-linear models for HPSG was to extract a small
informative sample from the original set T(w) (Osborne 2000). The method was suc-
cessfully applied to Dutch HPSG parsing (Malouf and van Noord 2004). A possible
problem with this method is in the approximation of exponentially many parse trees by
a polynomial-size sample. However, their method has an advantage in that any features
on parse results can be incorporated into a model, whereas our method forces feature
functions to be defined locally on conjunctive nodes. We will discuss the trade-off
between the approximation solution and the locality of feature functions in Section 6.3.
Non-probabilistic statistical classifiers have also been applied to disambiguation in
HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector
machines (Toutanova, Markova, and Manning 2004). However, the problem of expo-
nential explosion is also inevitable using their methods. As described in Section 6.1, an
approach similar to ours may be applied, following the study of Taskar et al (2004).
A series of studies on parsing with LFG (Johnson et al 1999; Riezler et al 2000,
2002) also proposed a maximum entropy model for probabilistic modeling of LFG pars-
ing. However, similarly to the previous studies on HPSG parsing, these groups had
no solution to the problem of exponential explosion of unpacked parse results. As dis-
cussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum
entropy estimation for packed representations of LFG parses.
Recent studies on CCG have proposed probabilistic models of dependency struc-
tures or predicate?argument dependencies, which are essentially the same as the
predicate?argument structures described in the present article. Clark, Hockenmaier, and
Steedman (2002) attempted the modeling of dependency structures, but the model was
inconsistent because of the violation of the independence assumption. Hockenmaier
(2003) proposed a consistent generative model of predicate?argument structures. The
probability of a non-local dependency was conditioned on multiple words to preserve
the consistency of the probability model; that is, probability p(I|want, dispute) in Sec-
tion 4.3 was directly estimated. The problem was that such probabilities could not be
estimated directly from the data due to data sparseness, and a heuristic method had
to be employed. Probabilities were therefore estimated as the average of individual
probabilities conditioned on a single word. Another problem is that the model is no
longer consistent when unification constraints such as those in HPSG are introduced.
73
Computational Linguistics Volume 34, Number 1
Our solution is free of these problems, and is applicable to various grammars, not only
HPSG and CCG.
Most of the state-of-the-art studies on parsing with lexicalized grammars have
adopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al 2004; Riezler
and Vasserman 2004). Their methods of translating parse results into feature forests are
basically the same as our method described in Section 4, and details differ because
different grammar theories represent syntactic structures differently. They reported
higher accuracy in parsing the Penn Treebank than the previous methods introduced
herein, and these results attest the effectiveness of feature forest models in practical
deep parsing. A remaining problem is that no studies could provide empirical compar-
isons across grammar theories. The above studies and our research evaluated parsing
accuracy on their own test sets. The construction of theory-independent standard test
sets requires enormous effort because we must establish theory-independent criteria
such as agreed definitions of phrases and headedness. Although this issue is beyond
the scope of the present article, it is a fundamental obstacle to the transparency of these
studies on parsing.
Clark and Curran (2004a) described a method for reducing the cost of parsing a
training treebank without sacrificing accuracy in the context of CCG parsing. They first
assigned each word a small number of supertags, corresponding to lexical entries in
our case, and parsed supertagged sentences. Because they did not use the probabilities of
supertags in a parsing stage, their method corresponds to our ?filtering only? method.
The difference from our approach is that they also applied the supertagger in a parsing
stage. We suppose that this was crucial for high accuracy in their approach, although
empirical investigation is necessary.
6.3 Trade-Off between Dynamic Programming and Feature Locality
The proposed algorithm is an essential solution to the problem of estimating probabilis-
tic models on exponentially many complete structures. However, the applicability of
this algorithm relies on the constraint that features are defined locally in conjunctive
nodes. As discussed in Section 6.1, this does not necessarily mean that features in our
model can represent only the immediate-dominance relations of CFG rules, because
conjunctive nodesmay encode any fragments of complete structures. In fact, we demon-
strated in Section 4.3 that certain assumptions allowed us to encode non-local predicate?
argument dependencies in tractable-size feature forests. In addition, although in the
experiments we used only features on bilexical dependencies, the method described in
Section 4.3 allows us to define any features on a predicate and all of its arguments, such
as a ternary relation among a subject, a verb, and a complement (e.g., the relation among
I, want, and dispute1 in Figure 21), and a generalized relation among semantic classes
of a predicate and its arguments. This is because a predicate and all of its arguments
are included in a conjunctive node, and feature functions can represent any relations
expressed within a conjunctive node.
Whenwe definemore global features, such as co-occurrences of structures at distant
places in a sentence, conjunctive nodes must be expanded so that they include all
structures that are necessary to define these features. However, this obviously increases
the number of conjunctive nodes, and consequently, the cost of parameter estimation
increases. In an extreme case, for example, if we define features on any co-occurrences
of partial parse trees, the full unpacking of parse forests would be necessary, and pa-
rameter estimation would be intractable. This indicates that there is a trade-off between
the locality of features and the cost of estimation. That is, larger context features might
74
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
contribute to higher accuracy, while they inflate the size of feature forests and increase
the cost of parameter estimation.
Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000;
Malouf and van Noord 2004) allow us to define any features on complete structures
without any constraints. However, they force us to employ approximation methods
for tractable computation. The effectiveness of those techniques therefore relies on
convergence speed and approximation errors, which may vary depending on the char-
acteristics of target problems and features.
It is an open research question whether dynamic programming or sampling can
deliver a better balance of estimation efficiency and accuracy. The answer will differ in
different problems. When most effective features can be represented locally in tractable-
size feature forests, dynamic programming methods including ours are suitable.
However, when global context features are indispensable for high accuracy, sampling
methods might be better. We should also investigate compromise solutions such as
dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh,
and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson
2005). There is no analytical way of predicting the best solution, and it must be
investigated experimentally for each target task.
7. Conclusion
A dynamic programming algorithm was presented for maximum entropy modeling
and shown to provide a solution to the parameter estimation of probabilistic models of
complete structures without the independence assumption. We first defined the notion
of a feature forest, which is a packed representation of an exponential number of trees of
features. When training data is represented with feature forests, model parameters are
estimated at a tractable cost without unpacking the forests. Themethod provides a more
flexible modeling scheme than previous methods of application of maximum entropy
models to natural language processing. Furthermore, it is applicable to complex data
structures where an event is difficult to decompose into independent sub-events.
We also demonstrated that feature forest models are applicable to probabilistic mod-
eling of linguistic structures such as the syntactic structures of HPSG and predicate?
argument structures including non-local dependencies. The presented approach can
be regarded as a general solution to the probabilistic modeling of syntactic analysis
with lexicalized grammars. Table 16 summarizes the best performance of the HPSG
parser described in this article. The parser demonstrated impressively high coverage
and accuracy for real-world texts. We therefore conclude that the HPSG parser for
English is moving toward a practical level of use in real-world applications. Recently,
the applicability of the HPSG parser to practical applications, such as information
extraction and retrieval, has also been demonstrated (Miyao et al 2006; Yakushiji et al
2006; Chun 2007).
Table 16
Final results.
Parsing accuracy for Section 23 (<40 words)
# parsed sentences 2,137/2,144 (99.7%)
Precision/recall 87.69%/87.16%
Sentential accuracy 39.2%
75
Computational Linguistics Volume 34, Number 1
From our extensive investigation of HPSG parsing, we observed that exploration
of new types of features is indispensable to further improvement of parsing accuracy.
A possible research direction is to encode larger contexts of parse trees, which has
been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova,
and Manning 2004). Future work includes not only the investigation of these features
but also the abstraction of predicate?argument dependencies using semantic classes.
Experimental results also suggest that an improvement in grammar coverage is crucial
for higher accuracy. This indicates that an improvement in the quality of the grammar
is a key factor for the improvement of parsing accuracy.
The feature forest model provides new insight into the relationship between a
linguistic structure and a unit of probability. Traditionally, a unit of probability was
implicitly assumed to correspond to a meaningful linguistic structure; a tagging of a
word or an application of a rewriting rule. One reason for the assumption is to enable
dynamic programming algorithms, such as the Viterbi algorithm. The probability of a
complete structure must be decomposed into atomic structures in which ambiguities
are limited to a tractable size. Another reason is to estimate plausible probabilities.
Because a probability is defined over atomic structures, they should also be meaning-
ful so as to be assigned a probability. In feature forest models, however, conjunctive
nodes are responsible for the former, whereas feature functions are responsible for the
latter. Although feature functions must be defined locally in conjunctive nodes, they
are not necessarily equivalent. Conjunctive nodes may represent any fragments of a
complete structure, which are not necessarily linguistically meaningful. They should
be designed to pack ambiguities and enable us to define useful features. Meanwhile,
feature functions indicate an atomic unit of probability, and are designed to capture
statistical regularity of the target problem. We expect the separation of a unit of prob-
ability from linguistic structures to open up a new framework for flexible probabilistic
modeling.
Acknowledgments
The authors wish to thank the anonymous
reviewers of Computational Linguistics for
their helpful comments and discussions. We
would also like to thank Takashi Ninomiya
and Kenji Sagae for their precious support.
References
Abney, Steven P. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Baker, James K. 1979. Trainable grammars
for speech recognition. In Jared J. Wolf
and Dennis H. Klatt, editors, Speech
Communication Papers Presented at the 97th
Meeting of the Acoustical Society of America.
MIT Press, Cambridge, MA, pages 547?550.
Baldridge, Jason and Miles Osborne. 2003.
Active learning for HPSG parse selection.
In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL
2003, pages 17?24, Edmonton, Canada.
Berger, AdamL., StephenA. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith,
and Andy Way. 2004. Treebank-based
acquisition of wide-coverage, probabilistic
LFG resources: Project overview,
results and evaluation. In Proceedings
of the IJCNLP-04 Workshop ?Beyond Shallow
Analyses?, Hainan Island. Available
at www-tsujii.is.s.u-tokyo.ac.jp/bsa.
Carpenter, Bob. 1992. The Logic of Typed
Feature Structures. Cambridge University
Press, Cambridge, England.
Carroll, John and Stephan Oepen. 2005.
High efficiency realization for a
wide-coverage unification grammar.
In Proceedings of the 2nd International
Joint Conference on Natural Language
Processing (IJCNLP-05), pages 165?176,
Jeju Island.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings
of the First Conference on North American
Chapter of the Association for Computational
76
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Linguistics (NAACL 2000), pages 132?139,
Seattle, WA.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing
and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL 2005), pages 173?180, Ann Arbor, MI.
Chen, Stanley and Ronald Rosenfeld.
1999a. A Gaussian prior for smoothing
maximum entropy models. Technical
Report CMUCS-99-108, Carnegie
Mellon University.
Chen, Stanley F. and Ronald Rosenfeld.
1999b. Efficient sampling and feature
selection in whole sentence maximum
entropy language models. In Proceedings
of the 1999 IEEE International Conference on
Acoustics, Speech, and Signal Processing,
pages 549?552, Phoenix, AZ.
Chiang, David. 2003. Mildly context sensitive
grammars for estimating maximum
entropy parsing models. In Proceedings of
the 8th Conference on Formal Grammar,
pages 19?31, Vienna.
Chun, Hong-Woo. 2007.Mining Literature for
Disease-Gene Relations. Ph.D. thesis,
University of Tokyo.
Clark, Stephen and James R. Curran. 2003.
Log-linear models for wide-coverage
CCG parsing. In Proceedings of the 2003
Conference on Empirical Methods in
Natural Language Processing (EMNLP 2003),
pages 97?104, Sapporo.
Clark, Stephen and James R. Curran.
2004a. The importance of supertagging
for wide-coverage CCG parsing.
In Proceedings of the 20th International
Conference on Computational Linguistics
(COLING 2004), pages 282?288, Geneva.
Clark, Stephen and James R. Curran. 2004b.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 104?111,
Barcelona.
Clark, Stephen, Julia Hockenmaier, and
Mark Steedman. 2002. Building deep
dependency structures with a wide-
coverage CCG parser. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002),
pages 327?334, Philadephia.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting
of the Association for Computational
Linguistics (ACL?97), pages 16?23,
Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania.
Collins, Michael. 2000. Discriminative
reranking for natural language
parsing. In Proceedings of the Seventeenth
International Conference on Machine
Learning, pages 175?182, Palo Alto, CA.
Collins,Michael. 2003. Head-driven statistical
models for natural language parsing.
Computational Linguistics, 29(4):589?637.
Copestake, Ann, Dan Flickinger,
Rob Malouf, Susanne Riehemann, and
Ivan Sag. 1995. Translation using minimal
recursion semantics. In Proceedings of the
Sixth International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI95), pages 15?32, Leuven.
Copestake, Ann, Dan Flickinger, Ivan A. Sag,
and Carl Pollard. 2006. Minimal recursion
semantics: An introduction. Research
on Language and Computation, 3(4):281?332.
Darroch, J. N. and D. Ratcliff. 1972.
Generalized iterative scaling for log-linear
models. The Annals of Mathematical
Statistics, 43(5):1470?1480.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
19(4):380?393.
Geman, Stuart and Mark Johnson. 2002.
Dynamic programming for parsing and
estimation of stochastic unification-based
grammars. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics (ACL 2002), pages 279?286,
Philadelphia, PA.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
2001 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001),
pages 167?202, Pittsburgh, PA.
Hockenmaier, Julia. 2003. Parsing with
generative models of predicate-argument
structure. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL 2003), pages 359?366,
Sapporo.
Hockenmaier, Julia and Mark Steedman.
2002. Acquiring compact lexicalized
grammars from a cleaner treebank.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC-2002), pages 1974?1981,
Las Palmas.
77
Computational Linguistics Volume 34, Number 1
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic ?unification-
based? grammars. In Proceedings
of the 37th Annual Meeting of the Association
for Computational Linguistics (ACL?99),
pages 535?541, College Park, Maryland.
Johnson, Mark and Stefan Riezler. 2000.
Exploiting auxiliary distributions in
stochastic unification-based grammars.
In Proceedings of the First Conference
on North American Chapter of the
Association for Computational Linguistics,
pages 154?161, Seattle, WA.
Kameya, Yoshitaka and Taisuke Sato.
2000. Efficient EM learning with tabulation
for parameterized logic programs.
In Proceedings of the 1st International
Conference on Computational Logic
(CL2000), volume 1861 of Lecture Notes
in Artificial Intelligence (LNAI),
pages 269?294, Imperial College, London.
Kaplan, Ronald M., Stefan Riezler, Tracy H.
King, John T. Maxwell, III, Alexander
Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow
and deep stochastic parsing. In Proceedings
of the Human Language Technology
Conference and the North American Chapter
of the Association for Computational
Linguistics (HLT-NAACL 2004),
pages 97?104, Boston, MA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of the International Conference
on Machine Learning 2001, pages 282?289,
Williams College, Williamstown, MA.
Makino, Takaki, Yusuke Miyao, Kentaro
Torisawa, and Jun?ichi Tsujii. 2002.
Native-code compilation of feature
structures. In Stephen Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans
Uszkoreit, editors, Collaborative
Language Engineering: A Case Study
in Efficient Grammar-based Parsing. CSLI
Publications, Palo Alto, CA, pages 49?80.
Malouf, Robert. 2002. A comparison
of algorithms for maximum entropy
parameter estimation. In Proceedings
of the Sixth Conference on Natural
Language Learning (CoNLL-2002),
pages 1?7, Taipei.
Malouf, Robert and Gertjan van Noord.
2004. Wide coverage parsing with
stochastic attribute value grammars.
In Proceedings of the IJCNLP-04 Workshop
?Beyond Shallow Analyses?, Hainan Island.
Available at www.tsujii.is.s.u-tokyo.
ac.jp/bsa.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate
argument structure. In Proceedings
of the Workshop on Human Language
Technology, pages 114?119, Plainsboro, NJ.
Maxwell John T., III and Ronald M. Kaplan.
1995. A method for disjunctive constraint
satisfaction. In Mary Dalrymple, Ronald
M. Kaplan, John T. Maxwell, III, and
Annie Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, number 47
in CSLI Lecture Notes Series. CSLI
Publications, Palo Alto, CA, chapter 14,
pages 381?481.
McCallum, Andrew and Wei Li. 2003.
Early results for named entity recognition
with conditional random fields, feature
induction and web-enhanced lexicons.
In Proceedings of the 7th Conference
on Natural Language Learning (CoNLL),
pages 188?191, Edmonton.
McCallum, Andrew, Khashayar
Rohanimanesh, and Charles Sutton.
2003. Dynamic conditional random fields
for jointly labeling multiple sequences. In
Proceedings of the Workshop on Syntax,
Semantics, Statistics at the 16th Annual
Conference on Neural Information Processing
Systems, Vancouver. Available at
www.cs.umasse.du/?mccallum/papers/
derf-nips03.pdf.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2003. Probabilistic modeling
of argument structures including
non-local dependencies. In Proceedings
of the International Conference on Recent
Advances in Natural Language Processing
(RANLP 2003), pages 285?291, Borovets.
Miyao, Yusuke, Takashi Ninomiya,
and Jun?ichi Tsujii. 2005. Corpus-oriented
grammar development for acquiring a
head-driven phrase structure grammar
from the Penn Treebank. In Natural
Language Processing - IJCNLP 2004,
pages 684?693, Hainan Island.
Miyao, Yusuke, Tomoko Ohta,
Katsuya Masuda, Yoshimasa Tsuruoka,
Kazuhiro Yoshida, Takashi Ninomiya,
and Jun?ichi Tsujii. 2006. Semantic retrieval
for the accurate identification of relational
concepts in massive textbases. In
Proceedings of the Joint Conference of the 21st
International Conference on Computational
Linguistics and the 44th Annual Meeting of the
78
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Association for Computational Linguistics
(COLING-ACL 2006), pages 1017?1024,
Sydney.
Miyao, Yusuke and Jun?ichi Tsujii. 2002.
Maximum entropy estimation for feature
forests. In Proceedings of the Human
Language Technology Conference (HLT-2002),
pages 292?297, San Diego, CA.
Miyao, Yusuke and Jun?ichi Tsujii. 2003.
A model of syntactic disambiguation
based on lexicalized grammars. In
Proceedings of the Seventh Conference on
Computational Natural Language Learning
(CoNLL-2003), pages 1?8, Edmonton.
Miyao, Yusuke and Jun?ichi Tsujii. 2005.
Probabilistic disambiguation models
for wide-coverage HPSG parsing. In
Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL 2005), pages 83?90, Ann Arbor, MI.
Nakanishi, Hiroko, Yusuke Miyao, and
Jun?ichi Tsujii. 2005. Probabilistic models
for disambiguation of an HPSG-based
chart generator. In Proceedings of the
9th International Workshop on Parsing
Technologies (IWPT 2005), pages 93?102,
Vancouver.
Ninomiya, Takashi, Yoshimasa Tsuruoka,
Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Efficacy of beam thresholding, unification
filtering and hybrid parsing in probabilistic
HPSG parsing. In Proceedings of the 9th
International Workshop on Parsing
Technologies, pages 103?114, Vancouver.
Nocedal, Jorge. 1980. Updating
quasi-Newton matrices with limited
storage.Mathematics of Computation,
35:773?782.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical Optimization. Springer,
New York.
Oepen, Stephan and John Carroll. 2000.
Ambiguity packing in constraint-based
parsing: practical results. In Proceedings
of the First Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL 2000), pages 162?169,
Seattle, WA.
Oepen, Stephan, Dan Flickinger, Jun?ichi
Tsujii, and Hans Uszkoreit, editors. 2002.
Collaborative Language Engineering: A Case
Study in Efficient Grammar-Based Processing.
CSLI Publications, Palo Alto, CA.
Oepen, Stephan, Kristina Toutanova,
Stuart Shieber, Christopher Manning, Dan
Flickinger, and Thorsten Brants. 2002. The
LinGO Redwoods treebankmotivation and
preliminary applications. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
volume 2, pages 1?5, Taipei.
Osborne, Miles. 2000. Estimation of
stochastic attribute-value grammar using
an informative sample. In Proceedings
of the 18th International Conference on
Computational Linguistics (COLING 2000),
volume 1, pages 586?592, Saarbru?cken.
Peng, Fuchun and Andrew McCallum. 2004.
Accurate information extraction from
research papers using conditional
random fields. In Proceedings of Human
Language Technology Conference and
North American Chapter of the Association
for Computational Linguistics (HLT/
NAACL-04), pages 329?336, Boston, MA.
Pinto, David, Andrew McCallum, Xen Lee,
and W. Bruce Croft. 2003. Table extraction
using conditional random fields. In
Proceedings of the 26th Annual International
ACM SIGIR Conference on Research
and Development in Information Retrieval
(SIGIR 2003), pages 235?242, Toronto.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago, IL.
Riezler, Stefan, Tracy H. King, Ronald M.
Kaplan, Richard Crouch, John T.
Maxwell, III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using
a lexical-functional grammar and
discriminative estimation techniques.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL 2002), pages 271?278,
Philadephia, PA.
Riezler, Stefan, Detlef Prescher, Jonas Kuhn,
and Mark Johnson. 2000. Lexicalized
stochastic modeling of constraint-based
grammars using log-linear measures
and EM training. In Proceedings of the
38th Annual Meeting of the Association
for Computational Linguistics (ACL 2000),
pages 480?487, Hong Kong.
Riezler, Stefan and Alexander Vasserman.
2004. Incremental feature selection and
l1 regularization for relaxed maximum-
entropy modeling. In Proceedings
of the 2004 Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2004), pages 174?181, Barcelona.
Roark, Brian, Murat Saraclar, Michael Collins,
and Mark Johnson. 2004. Discriminative
language modeling with conditional
random fields and the perceptron
algorithm. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 47?54,
Barcelona.
79
Computational Linguistics Volume 34, Number 1
Rosenfeld, Ronald. 1997. A whole sentence
maximum entropy language model. In
Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding,
pages 230?237, Santa Barbara, CA.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Syntactic Theory: A Formal
Introduction. Number 152 in CSLI Lecture
Notes. CSLI Publications, Standford, CA.
Sarawagi, Sunita and William W. Cohen.
2004. Semi-Markov conditional random
fields for information extraction. In
Proceedings of the 18th Annual Conference
on Neural Information Processing
Systems, pages 1185?1192, Vancouver.
Sato, Taisuke. 2005. A generic approach to em
learning for symbolic-statistical models. In
Proceedings of the 4th Learning Language in
Logic Workshop (LLL05), pages 21?28, Bonn.
Sato, Taisuke and Yoshitaka Kameya. 1997.
PRISM: a language for symbolic-statistical
modeling. In Proceedings of the 15th
International Joint Conference on Artificial
Intelligence (IJCAI ?97), pages 1330?1335,
Nagoya.
Sato, Taisuke and Yoshitaka Kameya. 2001.
Parameter learning of logic programs for
symbolic-statistical modeling. Journal of
Artificial Intelligence Research, 15:391?454.
Settles, Burr. 2004. Biomedical named entity
recognition using conditional random fields
and rich feature sets. In Proceedings of the
International Joint Workshop on Natural
Language Processing in Biomedicine and its
Applications (NLPBA), pages 104?107,
Geneva.
Sha, Fei and Fernando Pereira. 2003.
Shallow parsing with conditional
random fields. In Proceedings of the 2003
Human Language Technology Conference and
North American Chapter of the Association
for Computational Linguistics (HLT-NAACL
2003), pages 213?220, Edmonton.
Shieber, Stuart M. 1985. Using restriction
to extend parsing algorithms for
complex-feature-based formalisms. In
Proceedings of the 23rd Annual Meeting
on Association for Computational Linguistics,
pages 145?152, Chicago, IL.
Sutton, Charles, Khashayar Rohanimanesh,
and Andrew McCallum. 2004. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. In Proceedings
of the 21st International Conference
on Machine Learning (ICML 2004),
pages 783?790, Alberta.
Taskar, Ben, Dan Klein, Michael Collins,
Daphne Koller, and Chris Manning.
2004. Max-margin parsing. In Proceedings
of the 2004 Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2004), pages 1?8, Barcelona.
Toutanova, Kristina and Christopher
Manning. 2002. Feature selection for a
rich HPSG grammar using decision trees.
In Proceedings of the Sixth Conference on
Natural Language Lerning (CoNLL-2002),
pages 77?83, Taipei.
Toutanova, Kristina, Penka Markova,
and Christopher Manning. 2004. The
leaf projection path view of parse trees:
Exploring string kernels for HPSG parse
selection. In Proceedings of the 2004
Conference on Empirical Methods in
Natural Language Processing (EMNLP
2004), pages 166?173, Barcelona.
Tsujii Laboratory. 2004. Enju?A practical
HPSG parser. Available at http://www.
tsujii.is.s.u-tokyo.ac.jp/enju/.
Tsuruoka, Yoshimasa, Yusuke Miyao, and
Jun?ichi Tsujii. 2004. Towards efficient
probabilistic HPSG parsing: Integrating
semantic and syntatic preference
to guide the parsing. In Proceedings
of the IJCNLP-04 Workshop ?Beyond Shallow
Analyses?, Hainan Island. Available
at www.tsujii.is.s.u-tokyo.ac.jp/bsa.
Tsuruoka, Yoshimasa and Jun?ichi Tsujii.
2005. Bidirectional inference with the
easiest-first strategy for tagging sequence
data. In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP 2005),
pages 467?474, Vancouver.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York.
Vijay-Shanker, K., David J. Weir, and
Aravind K. Joshi. 1987. Characterizing
structural descriptions produced by
various grammatical formalisms. In
Proceedings of the 25th Annual Meeting
of the Association for Computational
Linguistics, pages 104?111, Palo Alto, CA.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania.
Yakushiji, Akane, Yusuke Miyao,
Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2006. Automatic construction of
predicate-argument structure patterns for
biomedical information extraction.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing (EMNLP 2006), pages 284?292,
Sydney.
80

Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
A Robust Retrieval Engine for Proximal and Structural Search
Katsuya Masuda? Takashi Ninomiya?? Yusuke Miyao? Tomoko Ohta?? Jun?ichi Tsujii??
? Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
? CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
{kmasuda,ninomi,yusuke,okap,tsujii}@is.s.u-tokyo.ac.jp
1 Introduction
In the text retrieval area including XML and Region Al-
gebra, many researchers pursued models for specifying
what kinds of information should appear in specified
structural positions and linear positions (Chinenyanga
and Kushmerick, 2001; Wolff et al, 1999; Theobald and
Weilkum, 2000; Clarke et al, 1995). The models at-
tracted many researchers because they are considered to
be basic frameworks for retrieving or extracting complex
information like events. However, unlike IR by keyword-
based search, their models are not robust, that is, they
support only exact matching of queries, while we would
like to know to what degree the contents in specified
structural positions are relevant to those in the query even
when the structure does not exactly match the query.
This paper describes a new ranked retrieval model
that enables proximal and structural search for structured
texts. We extend the model proposed in Region Alge-
bra to be robust by i) incorporating the idea of ranked-
ness in keyword-based search, and ii) expanding queries.
While in ordinary ranked retrieval models relevance mea-
sures are computed in terms of words, our model assumes
that they are defined in more general structural fragments,
i.e., extents (continuous fragments in a text) proposed in
Region Algebra. We decompose queries into subqueries
to allow the system not only to retrieve exactly matched
extents but also to retrieve partially matched ones. Our
model is robust like keyword-based search, and also en-
ables us to specify the structural and linear positions in
texts as done by Region Algebra.
The significance of this work is not in the development
of a new relevance measure nor in showing superiority
of structure-based search over keyword-based search, but
in the proposal of a framework for integrating proximal
and structural ranking models. Since the model treats all
types of structures in texts, not only ordinary text struc-
tures like ?title,? ?abstract,? ?authors,? etc., but also se-
mantic tags corresponding to recognized named entities
or events can also be used for indexing text fragments
and contribute to the relevance measure. Since extents
are treated similarly to keywords in traditional models,
our model will be integrated with any ranking and scala-
bility techniques used by keyword-based models.
We have implemented the ranking model in our re-
trieval engine, and had preliminary experiments to eval-
uate our model. Unfortunately, we used a rather small
corpus for the experiments. This is mainly because
there is no test collection of the structured query and
tag-annotated text. Instead, we used the GENIA cor-
pus (Ohta et al, 2002) as structured texts, which was
an XML document annotated with semantics tags in the
filed of biomedical science. The experiments show that
our model succeeded in retrieving the relevant answers
that an exact-matching model fails to retrieve because of
lack of robustness, and the relevant answers that a non-
structured model fails because of lack of structural spec-
ification.
2 A Ranking Model for Structured
Queries and Texts
This section describes the definition of the relevance be-
tween a document and a structured query represented by
the region algebra. The key idea is that a structured query
is decomposed into subqueries, and the relevance of the
whole query is represented as a vector of relevance mea-
sures of subqueries.
The region algebra (Clarke et al, 1995) is a set of op-
erators, which represent the relation between the extents
(i.e. regions in texts). In this paper, we suppose the re-
gion algebra has seven operators; four containment oper-
ators (?, ?, 6?, 6?) representing the containment relation
between the extents, two combination operators (4, 5)
corresponding to ?and? and ?or? operator of the boolean
model, and ordering operator (3) representing the order
of words or structures in the texts. For convenience of
explanation, we represent a query as a tree structure as
  
 
	
		
 
A Debug Tool for Practical Grammar Development
Akane Yakushiji? Yuka Tateisi?? Yusuke Miyao?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,yoshinag,tsujii}@is.s.u-tokyo.ac.jp
Naoki Yoshinaga? Jun?ichi Tsujii??
Abstract
We have developed willex, a tool that
helps grammar developers to work effi-
ciently by using annotated corpora and
recording parsing errors. Willex has two
major new functions. First, it decreases
ambiguity of the parsing results by com-
paring them to an annotated corpus and
removing wrong partial results both au-
tomatically and manually. Second, willex
accumulates parsing errors as data for the
developers to clarify the defects of the
grammar statistically. We applied willex
to a large-scale HPSG-style grammar as
an example.
1 Introduction
There is an increasing need for syntactical parsers
for practical usages, such as information extrac-
tion. For example, Yakushiji et al (2001) extracted
argument structures from biomedical papers using
a parser based on XHPSG (Tateisi et al, 1998),
which is a large-scale HPSG. Although large-scale
and general-purpose grammars have been devel-
oped, they have a problem of limited coverage.
The limits are derived from deficiencies of gram-
mars themselves. For example, XHPSG cannot treat
coordinations of verbs (ex. ?Molybdate slowed but
did not prevent the conversion.?) nor reduced rel-
atives (ex. ?Rb mutants derived from patients with
retinoblastoma.?). Finding these grammar defects
and modifying them require tremendous human ef-
fort.
Hence, we have developed willex that helps to im-
prove the general-purpose grammars. Willex has two
major functions. First, it reduces a human workload
to improve the general-purpose grammar through
using language intuition encoded in syntactically
tagged corpora in XML format. Second, it records
data of grammar defects to allow developers to have
a whole picture of parsing errors found in the target
corpora to save debugging time and effort by priori-
tizing them.
2 What Is the Ideal Grammar Debugging?
There are already other grammar developing tools,
such as a grammar writer of XTAG (Paroubek et al,
1992), ALEP (Schmidt et al, 1996), ConTroll (Go?tz
and Meurers, 1997), a tool by Nara Institute of Sci-
ence and Technology (Miyata et al, 1999), and [incr
tsdb()] (Oepen et al, 2002). But these tools have
following problems; they largely depend on human
debuggers? language intuition, they do not help users
to handle large amount of parsing results effectively,
and they let human debuggers correct the bugs one
after another manually and locally.
To cope with these shortcomings, willex proposes
an alternative method for more efficient debugging
process.
The workflow of the conventional grammar devel-
oping tools and willex are different in the following
ways. With the conventional tools, human debug-
gers must check each sentence to find out grammar
defects and modify them one by one. On the other
hand, with willex human debuggers check sentences
that are tagged with syntactical structure, one by
one, find grammar defects, and record them, while
willex collects the whole grammar defect records.
Then human debuggers modify the found grammar
defects. This process allows human debuggers to
make priority over defects that appear more fre-
quently in the corpora, or defects that are more crit-
ical for purposes of syntactical parsing. Indeed, it
is possible for human debuggers using the conven-
tional tools to collect and modify the defects but
willex saves the trouble of human debuggers to col-
lect defects to modify them more efficiently.
3 Functions of willex
To create the new debugging tool, we have extended
will (Imai et al, 1998). Will is a browser of parsing
results of grammars based on feature structures. Will
and willex are implemented in JAVA.
3.1 Using XML Tagged Corpora
Willex uses sentence boundaries, word chunking,
and POSs/labels encoded in XML tagged corpora.
First, with the information of sentence boundaries
and word chunking, ambiguity of sentences is re-
duced, and ambiguity at parsing phase is also re-
duced. A parser connected to willex is assumed to
produce only results consistent with the information.
An example is shown in Figure 1 (<su> is a senten-
tial tag and <np> is a tag for noun phrases).
I  saw  a girl  with a telescope
I  saw  a girl  with a telescope


<su> I saw <np> a girl with a telescope </np></su>
Figure 1: An example of pa sing results along with
word chunking
Next, willex compares POSs/labels encoded in
XML tags and parsing results, and deletes improper
parsing trees. Therefore, it reduces numbers of par-
tial parsing trees, which appear in the way of parsing
and should be checked by human debuggers. In ad-
dition, human debuggers can delete partial parsing
trees manually later. Figure 2 shows a concrete ex-
ample. (NP and S are labels for noun and sentential
phrases respectively.)
POS/label from Tagged Corpus
POSs/labels from Partial Results
<NP> A cat </NP> knows everything
A      cat
D      N N       V
A      cat
NP S  
Figure 2: An example of deletion by using
POSs/labels
3.2 Output of Grammar Defects
Willex has a function to output information of gram-
mar defects into a file in order to collect the de-
fects data and treat them statistically. In addition,
we can save a log of debugging experiences which
show what grammar defects are found.
An example of an output file is shown in Table
1. It includes sentence numbers, word ranges in
which parsing failed, and comments input by a hu-
man debugger. For example, the first row of the ta-
ble means that the sentence #0 has coordinations of
verb phrases at position #3?#12, which cannot be
parsed. ?OK? in the second row means the sen-
tence is parsed correctly (i.e., no grammar defects
are found in the sentence). The third row means that
the word #4 of the sentence #2 has no proper lexical
entry.
The word ranges are specified by human debug-
gers using a GUI, which shows parsing results in
CKY tables and parse trees. The comments are input
by human debuggers in a natural language or chosen
from the list of previous comments. A postprocess-
ing module of willex sorts the error data by the com-
ments to help statistical analysis.
Table 1: An example of file output
Sentence # Word # comment
0 3?12 V-V coordination
1 ? OK
2 4 no lexical entry
4 Experiments and Discussion
We have applied willex to rental-XTAG, an HPSG-
style grammar converted from the XTAG English
grammar (The XTAG Research Group, 2001) by a
grammar conversion (Yoshinaga and Miyao, 2001).1
The corpus used is MEDLINE abstracts with tags
based on a slightly modified version of GDA-
DTD2 (Hasida, 2003). The corpus is ?partially
parsed?; the attachments of prepositional phrases are
annotated manually.
The tags do not always specify the correct struc-
tures based on rental-XTAG (i.e., the grammar as-
sumed by tags is different from rental-XTAG), so we
prepared a POS/label conversion table. We can use
tagged corpora based on various grammars different
from the grammar that the parser is assuming by us-
ing POS/label conversion tables.
We investigated 208 sentences (average 24.2
words) from 26 abstracts. 73 sentences were parsed
successfully and got correct results. Thus the cover-
age was 35.1%.
4.1 Qualitative Evaluation
Willex received three major positive feedbacks from
a user; first, the function of restricting partial results
was helpful, as it allows human debuggers to check
fewer results, second, the function to delete incorrect
partial results manually was useful, because there
are some cases that tags do not specify POSs/labels,
and third, human debuggers could use the record-
ing function to make notes to analyze them carefully
later.
However, willex also received some negative eval-
uations; the process of locating the cause of pars-
ing failure in a sentence was found to be a bit trou-
blesome. Also, willex loses its accuracy if the hu-
man debuggers themselves have trouble understand-
ing the correct syntactical structure of a sentence.3
1Since XTAG and rental-XTAG generate equivalent parse
results for the same input, debugging rental-XTAG means de-
bugging XTAG itself.
2GDA has no tags which specify prepositional phrases, so
we add <prep> and <prepp>.
3Thus, we divided the process of identifying grammar de-
fects to two steps. First, a non-expert roughly classifies pars-
ing errors and records temporary memorandums. Then, the
non-expert shows typical examples of sentences in each class
to experts and identifies grammar defects based on experts? in-
ference. Here, we can make use of the recording function of
We found from these evaluations that the func-
tions of willex can be used effectively, though more
automation is needed.
4.2 Quantitative Evaluation
Figure 3 shows the decrease in partial parsing trees
caused by using the tagged corpus. (Data of 10 sen-
tences among the 208 sentences are shown.) The
graph shows that human workload was reduced by
using the tagged corpus.
0
5000
10000
15000
20000
25000
30000
35000
10 15 20 25 30 35 40
n
u
m
b
e
r
 
o
f
 
p
a
r
t
i
a
l
 
r
e
s
u
l
t
s
length of a sentence (number of words)
without any info.with chunk info.with chunk and POS/label info.
Figure 3: Examples of numbers of partial results
4.3 Defects of rental-XTAG
Table 2 shows the defects of rental-XTAG which are
found by using willex.
Table 2: The defects of rental-XTAG
the defects of rental-XTAG #
no lexical entry 62
cannot handle reduced relative 35
cannot handle V-V coordination 22
Adjective does not post-modify NP 9
cannot parse ?, but not? 4
cannot handle objective to-infinitive 3
?, which ...? does not post-modify NP 3
cannot handle reduced as-relative clause 2
cannot parse ?greater than?(?>?) 2
misc. 17
From this table, it is inferred that (1) lack of lexi-
cal entries, (2) inability to parse reduced relative and
willex.
(3) inability to parse coordinations of verbs are seri-
ous problems of rental-XTAG.
4.4 Conflicts Between the Modified GDA and
rental-XTAG
Conflicts between rental-XTAG and the grammar on
which the modified GDA based cause parsing fail-
ures. Statistics of the conflicts is shown in Table 3.
Table 3: Conflicts between the modified GDA and
rental-XTAG
modified GDA rental-XTAG #
adjectival phrase verbal phrase 36
bracketing except ?,? 10
bracketing of ?,? 8
treatment of omitted words 2
misc. 5
These conflicts cannot be resolved by a simple
POS/label conversion table. One resolution is insert-
ing a preprocess module that deletes and moves tags
which cause conflicts.
We do not consider these conflicts as grammar de-
fects but the difference of grammars to be absorbed
in the conversion phase.
5 Conclusion and Future Work
We developed a debug tool, willex, which uses XML
tagged corpora and outputs information of grammar
defects. By using tagged corpora, willex succeeded
to reduce human workload. And by recording gram-
mar defects, it provides debugging environment with
a bigger perspective. But there remains a prob-
lem that a simple POS/label conversion table is not
enough to resolve conflicts of a debugged grammar
and a grammar assumed by tags. The tool should
support to handle the complicated conflicts.
In the future, we will try to modify willex to infer
causes of parsing errors (semi-)automatically. It is
difficult to find a point of parsing failure automati-
cally, because subsentences that have no correspon-
dent partial results are not always the failed point.
Hence, we will expand willex to find the longest
subsentences that are parsed successfully. Words,
POS/labels and features of the subsentences can be
clues to infer the causes of parsing errors.
References
Thilo Go?tz and Walt Detmar Meurers. 1997. The Con-
Troll system as large grammar development platform.
In Proc. of Workshop on Computational Environments
for Grammar Development and Linguistic Engineer-
ing, pages 38?45.
Koiti Hasida. 2003. Global docu-
ment annotation (GDA). available in
http://www.i-content.org/GDA/.
Hisao Imai, Yusuke Miyao, and Jun?ichi Tsujii. 1998.
GUI for an HPSG parser. In Information Processing
Society of Japan SIG Notes NL-127, pages 173?178,
September. In Japanese.
Takashi Miyata, Kazuma Takaoka, and Yuji Mat-
sumoto. 1999. Implementation of GUI debugger for
unification-based grammar. In Information Process-
ing Society of Japan SIG Notes NL-129, pages 87?94,
January. In Japanese.
Stephan Oepen, Emily M. Bender, Uli Callmeier, Dan
Flickinger, and Melanie Siegel. 2002. Parallel dis-
tributed grammar engineering for practical applica-
tions. In Proc. of the Workshop on Grammar Engi-
neering and Evaluation, pages 15?21.
Patrick Paroubek, Yves Schabes, and Aravind K. Joshi.
1992. XTAG ? a graphical workbench for developing
Tree-Adjoining grammars. In Proc. of the 3rd Confer-
ence on Applied Natural Language Processing, pages
216?223.
Paul Schmidt, Axel Theofilidis, Sibylle Rieder, and
Thierry Declerck. 1996. Lean formalisms, linguis-
tic theory, and applications. Grammar development in
ALEP. In Proc. of COLING ?96, volume 1, pages
286?291.
Yuka Tateisi, Kentaro Torisawa, Yusuke Miyao, and
Jun?ichi Tsujii. 1998. Translating the XTAG english
grammar to HPSG. In Proc. of TAG+4 workshop,
pages 172?175.
The XTAG Research Group. 2001. A Lex-
icalized Tree Adjoining Grammar for English.
Technical Report IRCS Research Report 01-03,
IRCS, University of Pennsylvania. available in
http://www.cis.upenn.edu/?xtag/.
Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and
Jun?ichi Tsujii. 2001. Event extraction from biomedi-
cal papers using a full parser. In Pacific Symposium on
Biocomputing 2001, pages 408?419, January.
Naoki Yoshinaga and Yusuke Miyao. 2001. Grammar
conversion from LTAG to HPSG. In Proc. of the sixth
ESSLLI Student Session, pages 309?324.
Finding Anchor Verbs for Biomedical IE
Using Predicate-Argument Structures
Akane YAKUSHIJI? Yuka TATEISI?? Yusuke MIYAO?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Agency)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi TSUJII??
Abstract
For biomedical information extraction, most sys-
tems use syntactic patterns on verbs (anchor verbs)
and their arguments. Anchor verbs can be se-
lected by focusing on their arguments. We propose
to use predicate-argument structures (PASs), which
are outputs of a full parser, to obtain verbs and their
arguments. In this paper, we evaluated PAS method
by comparing it to a method using part of speech
(POSs) pattern matching. POS patterns produced
larger results with incorrect arguments, and the re-
sults will cause adverse effects on a phase selecting
appropriate verbs.
1 Introduction
Research in molecular-biology field is discovering
enormous amount of new facts, and thus there is
an increasing need for information extraction (IE)
technology to support database building and to find
novel knowledge in online journals.
To implement IE systems, we need to construct
extraction rules, i.e., rules to extract desired infor-
mation from processed resource. One subtask of the
construction is defining a set of anchor verbs, which
express realization of desired information in natural
language text.
In this paper, we propose a novel method of
finding anchor verbs: extracting anchor verbs from
predicate-argument structures (PASs) obtained by
full parsing. We here discuss only finding anchor
verbs, although our final purpose is construction
of extraction rules. Most anchor verbs take topi-
cal nouns, i.e., nouns describing target entities for
IE, as their arguments. Thus verbs which take top-
ical nouns can be candidates for anchor verbs. Our
method collects anchor verb candidates by choosing
PASs whose arguments are topical nouns. Then, se-
mantically inappropriate verbs are filtered out. We
leave this filtering phase as a future work, and dis-
cuss the acquisition of candidates. We have also in-
vestigated difference in verbs and their arguments
extracted by naive POS patterns and PAS method.
When anchor verbs are found based on whether
their arguments are topical nouns, like in (Hatzivas-
siloglou and Weng, 2002), it is important to obtain
correct arguments. Thus, in this paper, we set our
goal to obtain anchor verb candidates and their cor-
rect arguments.
2 Background
There are some works on acquiring extraction rules
automatically. Sudo et al (2003) acquired subtrees
derived from dependency trees as extraction rules
for IE in general domains. One problem of their sys-
tem is that dependency trees cannot treat non-local
dependencies, and thus rules acquired from the con-
structions are partial. Hatzivassiloglou and Weng
(2002) used frequency of collocation of verbs and
topical nouns and verb occurrence rates in several
domains to obtain anchor verbs for biological inter-
action. They used only POSs and word positions
to detect relations between verbs and topical nouns.
Their performance was 87.5% precision and 82.4%
recall. One of the reasons of errors they reported is
failures to detect verb-noun relations.
To avoid these problems, we decided to use PASs
obtained by full parsing to get precise relations be-
tween verbs and their arguments. The obtained pre-
cise relations will improve precision. In addition,
PASs obtained by full parsing can treat non-local
dependencies, thus recall will also be improved.
The sentence below is an example which sup-
ports advantage of full parsing. A gerund ?activat-
ing? takes a non-local semantic subject ?IL-4?. In
full parsing based on Head-Driven Phrase Structure
Grammar (HPSG) (Sag and Wasow, 1999), the sub-
ject of the whole sentence and the semantic subject
of ?activating? are shared, and thus we can extract
the subject of ?activating?.
IL-4 may mediate its biological effects by activat-
ing a tyrosine-phosphorylated DNA binding pro-
tein.
interacts
ARG1 it
1 with
MODIFY
ARG1      regions
2
1
of
MODIFY
ARG1 molecules
2
,
,
(a) (b) (c)
It interacts with non-polymorphic regions of major his-
tocompatibility complex class II molecules.
Figure 1: PAS examples
with
MODIFY
interacts
ARG1 it
ARG1 regions
Core verb
serves
ARG1      IL-5
1
ARG2
to
ARG1
ARG2
stimulate
ARG1
ARG2 binding
1
Core verb
1
Figure 2: Core verbs of PASs
3 Anchor Verb Finding by PASs
By using PASs, we extract candidates for anchor
verbs from a sentence in the following steps:
1. Obtain all PASs of a sentence by a full
parser. The PASs correspond not only to verbal
phrases but also other phrases such as preposi-
tional phrases.
2. Select PASs which take one or more topical
nouns as arguments.
3. From the selected PASs in Step 2, select PASs
which include one or more verbs.
4. Extract a core verb, which is the innermost ver-
bal predicate, from each of the chosen PASs.
In Step 1, we use a probabilistic HPSG parser
developed by Miyao et al (2003), (2004). PASs
obtained by the parser are illustrated in Figure 1.1
Bold words are predicates. Arguments of the predi-
cates are described in ARGn (n = 1, 2, . . .). MOD-
IFY denotes the modified PAS. Numbers in squares
denote shared structures. Examples of core verbs
are illustrated in Figure 2. We regard all arguments
in a PAS are arguments of the core verb.
Extraction of candidates for anchor verbs from
the sentence in Figure 1 is as follows. Here, ?re-
gions? and ?molecules? are topical nouns.
In Step 1, we obtain all the PASs, (a), (b) and (c),
in Figure 1.
1Here, named entities are regarded as chunked, and thus
internal structures of noun phrases are not illustrated.
Next, in Step 2, we check each argument of (a),
(b) and (c). (a) is discarded because it does not have
a topical noun argument.2 (b) is selected because
ARG1 ?regions? is a topical noun. Similarly, (c) is
selected because of ARG1 ?molecules?.
And then, in Step 3, we check each POS of a
predicate included in (b) and (c). (b) is selected be-
cause it has the verb ?interacts? in 1 which shares
the structure with (a). (c) is discarded because it
includes no verbs.
Finally, in Step 4, we extract a core verb from (b).
(b) includes 1 asMODIFY, and the predicate of 1
is the verb, ?interacts?. So we extract it.
4 Experiments
We investigated the verbs and their arguments ex-
tracted by PAS method and POS pattern matching,
which is less expressive in analyzing sentence struc-
tures but would be more robust.
For topical nouns and POSs, we used the GENIA
corpus (Kim et al, 2003), a corpus of annotated ab-
stracts taken from National Library of Medicine?s
MEDLINE database. We defined topical nouns as
the names tagged as protein, peptide, amino acid,
DNA, RNA, or nucleic acid. We chose PASs which
take one or more topical nouns as an argument or
arguments, and substrings matched by POS patterns
which include topical nouns. All names tagged in
the corpus were replaced by their head nouns in
order to reduce complexity of sentences and thus
reduce the task of the parser and the POS pattern
matcher.
4.1 Implementation of PAS method
We implemented PAS method on LiLFeS, a
unification-based programming system for typed
feature structures (Makino et al, 1998; Miyao et al,
2000).
The selection in Step 2 described in Section 3
is realized by matching PASs with nine PAS tem-
plates. Four of the templates are illustrated in Fig-
ure 3.
4.2 POS Pattern Method
We constructed a POS pattern matcher with a par-
tial verb chunking function according to (Hatzivas-
siloglou and Weng, 2002). Because the original
matcher has problems in recall (its verb group de-
tector has low coverage) and precision (it does not
consider other words to detect relations between
verb groups and topical nouns), we implemented
2(a) may be selected if the anaphora (?it?) is resolved. But
we regard anaphora resolving is too hard task as a subprocess
of finding anchor verbs.
*any*
ARG1 N1
N1 = topical noun
*any*
ARG1 N1
ARG2 N2
N1 = topical noun
or N2 = topical noun
? ?
*any*
MODIFY *any*
ARG1 N1
N1 = topical noun
*any*
MODIFY *any*
ARG1 N1
ARG2 N2
N1 = topical noun
or N2 = topical noun
Figure 3: PAS templates
N ? V G ? N
N ? V G
V G ? N
N : is a topical noun
V G: is a verb group which is accepted by a finite state
machine described in (Hatzivassiloglou andWeng, 2002)
or one of {VB, VBD, VBG, VBN, VBP, VBZ}
?: is 0?4 tokens which do not include {FW, NN, NNS,
NNP, NNPS, PRP, VBG, WP, *}
(Parts in Bold letters are added to the patterns of Hatzi-
vassiloglou and Weng (2002).)
Figure 4: POS patterns
our POS pattern matcher as a modified version of
one in (Hatzivassiloglou and Weng, 2002).
Figure 4 shows patterns in our experiment. The
last verb of V G is extracted if all of Ns are topical
nouns. Non-topical nouns are disregarded. Adding
candidates for verb groups raises recall of obtained
relations of verbs and their arguments. Restriction
on intervening tokens to non-nouns raises the preci-
sion, although it decreases the recall.
4.3 Experiment 1
We extracted last verbs of POS patterns and core
verbs of PASs with their arguments from 100 ab-
stracts (976 sentences) of the GENIA corpus. We
took up not the verbs only but tuples of the verbs
and their arguments (VAs), in order to estimate ef-
fect of the arguments on semantical filtering.
Results
The numbers of VAs extracted from the 100 ab-
stracts using POS patterns and PASs are shown in
Table 1. (Total ? VAs of verbs not extracted by the
other method) are not the same, because more than
one VA can be extracted on a verb in a sentence.
POS patterns method extracted more VAs, although
POS patterns PASs
Total 1127 766
VAs of verbs
not extracted 478 105
by the other
Table 1: Numbers of VAs extracted from the 100
abstracts
Appropriate Inappropriate Total
Correct 43 12 55
Incorrect 20 23 43
Total 63 35 98
Table 2: Numbers of VAs extracted by POS patterns
(in detail)
their correctness is not considered.
4.4 Experiment 2
For the first 10 abstracts (92 sentences), we man-
ually investigated whether extracted VAs are syn-
tactically or semantically correct. The investigation
was based on two criteria: ?appropriateness? based
on whether the extracted verb can be used for an an-
chor verb and ?correctness? based on whether the
syntactical analysis is correct, i.e., whether the ar-
guments were extracted correctly.
Based on human judgment, the verbs that rep-
resent interactions, events, and properties were se-
lected as semantically appropriate for anchor verbs,
and the others were treated as inappropriate. For ex-
ample, ?identified? in ?We identified ZEBRA pro-
tein.? is not appropriate and discarded.
We did not consider non-topical noun arguments
for POS pattern method, whereas we considered
them for PAS method. Thus decision on correctness
is stricter for PAS method.
Results
The manual investigation results on extracted
VAs from the 10 abstracts using POS patterns and
PASs are shown in Table 2 and 3 respectively.
POS patterns extracted more (98) VAs than PASs
(75), but many of the increment were from incor-
rect POS pattern matching. By POS patterns, 43
VAs (44%) were extracted based on incorrect anal-
ysis. On the other hand, by PASs, 20 VAs (27%)
were extracted incorrectly. Thus the ratio of VAs
extracted by syntactically correct analysis is larger
on PAS method.
POS pattern method extracted 38 VAs of verbs
not extracted by PAS method and 7 of them are cor-
rect. For PAS method, correspondent numbers are
Appropriate Inappropriate Total
Correct 44 11 55
Incorrect 14 6 20
Total 58 17 75
Table 3: Numbers of VAs extracted by PASs (in de-
tail)
11 and 4 respectively. Thus the increments tend to
be caused by incorrect analysis, and the tendency is
greater in POS pattern method.
Since not all of verbs that take topical nouns are
appropriate for anchor verbs, automatic filtering is
required. In the filtering phase that we leave as a
future work, we can use semantical classes and fre-
quencies of arguments of the verbs. The results with
syntactically incorrect arguments will cause adverse
effect on filtering because they express incorrect re-
lationship between verbs and arguments. Since the
numbers of extracted VAs after excluding the ones
with incorrect arguments are the same (55) between
PAS and POS pattern methods, it can be concluded
that the precision of PAS method is higher. Al-
though there are few (7) correct VAs which were
extracted by POS pattern method but not by PAS
method, we expect the number of such verbs can be
reduced using a larger corpus.
Examples of appropriate VAs extracted by only
one method are as follows: (A) is correct and (B)
incorrect, extracted by only POS pattern method,
and (C) is correct and (D) incorrect, extracted by
only PAS method. Bold words are extracted verbs
or predicates and italic words their extracted argu-
ments.
(A) This delay is associated with down-regulation
of many erythroid cell-specific genes, including
alpha- and beta-globin, band 3, band 4.1, and . . . .
(B) . . . show that several elements in the . . . region of
the IL-2R alpha gene contribute to IL-1 respon-
siveness, . . . .
(C) The CD4 coreceptor interacts with non-
polymorphic regions of . . . molecules on
non-polymorphic cells and contributes to T cell
activation.
(D) Whereas activation of the HIV-1 enhancer follow-
ing T-cell stimulation is mediated largely through
binding of the . . . factor NF-kappa B to two adja-
cent kappa B sites in . . . .
5 Conclusions
We have proposed a method of extracting anchor
verbs as elements of extraction rules for IE by us-
ing PASs obtained by full parsing. To compare
our method with more naive and robust methods,
we have extracted verbs and their arguments using
POS patterns and PASs. POS pattern method could
obtain more candidate verbs for anchor verbs, but
many of them were extracted with incorrect argu-
ments by incorrect matching. A later filtering pro-
cess benefits by precise relations between verbs and
their arguments which PASs obtained. The short-
coming of PAS method is expected to be reduced by
using a larger corpus, because verbs to extract will
appear many times in many forms. One of the future
works is to extend PAS method to handle events in
nominalized forms.
Acknowledgements
This work was partially supported by Grant-in-
Aid for Scientific Research on Priority Areas (C)
?Genome Information Science? from the Ministry
of Education, Culture, Sports, Science and Technol-
ogy of Japan.
References
Vasileios Hatzivassiloglou and Wubin Weng. 2002.
Learning anchor verbs for biological interaction
patterns from published text articles. Interna-
tional Journal of Medical Informatics, 67:19?32.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a se-
mantically annotated corpus for bio-textmining.
Bioinformatics, 19(suppl. 1):i180?i182.
Takaki Makino, Minoru Yoshida, Kentaro Tori-
sawa, and Jun-ichi Tsujii. 1998. LiLFeS ? to-
wards a practical HPSG parser. In Proceedings
of COLING-ACL?98.
Yusuke Miyao, Takaki Makino, Kentaro Torisawa,
and Jun-ichi Tsujii. 2000. The LiLFeS abstract
machine and its evaluation with the LinGO gram-
mar. Natural Language Engineering, 6(1):47 ?
61.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi
Tsujii. 2003. Probabilistic modeling of argument
structures including non-local dependencies. In
Proceedings of RANLP 2003, pages 285?291.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi
Tsujii. 2004. Corpus-oriented grammar develop-
ment for acquiring a Head-driven Phrase Struc-
ture Grammar from the Penn Treebank. In Pro-
ceedings of IJCNLP-04.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic
Theory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisition.
In Proceedings of ACL 2003, pages 224?231.
Proceedings of the 43rd Annual Meeting of the ACL, pages 75?82,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Probabilistic CFG with latent annotations
Takuya Matsuzaki
 
Yusuke Miyao
 
Jun?ichi Tsujii  
 
Graduate School of Information Science and Technology, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033

CREST, JST(Japan Science and Technology Agency)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012

matuzaki, yusuke, tsujii  @is.s.u-tokyo.ac.jp
Abstract
This paper defines a generative probabilis-
tic model of parse trees, which we call
PCFG-LA. This model is an extension of
PCFG in which non-terminal symbols are
augmented with latent variables. Fine-
grained CFG rules are automatically in-
duced from a parsed corpus by training a
PCFG-LA model using an EM-algorithm.
Because exact parsing with a PCFG-LA is
NP-hard, several approximations are de-
scribed and empirically compared. In ex-
periments using the Penn WSJ corpus, our
automatically trained model gave a per-
formance of 86.6% (F  , sentences  40
words), which is comparable to that of an
unlexicalized PCFG parser created using
extensive manual feature selection.
1 Introduction
Variants of PCFGs form the basis of several broad-
coverage and high-precision parsers (Collins, 1999;
Charniak, 1999; Klein and Manning, 2003). In those
parsers, the strong conditional independence as-
sumption made in vanilla treebank PCFGs is weak-
ened by annotating non-terminal symbols with many
?features? (Goodman, 1997; Johnson, 1998). Exam-
ples of such features are head words of constituents,
labels of ancestor and sibling nodes, and subcatego-
rization frames of lexical heads. Effective features
and their good combinations are normally explored
using trial-and-error.
This paper defines a generative model of parse
trees that we call PCFG with latent annotations
(PCFG-LA). This model is an extension of PCFG
models in which non-terminal symbols are anno-
tated with latent variables. The latent variables work
just like the features attached to non-terminal sym-
bols. A fine-grained PCFG is automatically induced
from parsed corpora by training a PCFG-LA model
using an EM-algorithm, which replaces the manual
feature selection used in previous research.
The main focus of this paper is to examine the
effectiveness of the automatically trained models in
parsing. Because exact inference with a PCFG-LA,
i.e., selection of the most probable parse, is NP-hard,
we are forced to use some approximation of it. We
empirically compared three different approximation
methods. One of the three methods gives a perfor-
mance of 86.6% (F  , sentences  40 words) on the
standard test set of the Penn WSJ corpus.
Utsuro et al (1996) proposed a method that auto-
matically selects a proper level of generalization of
non-terminal symbols of a PCFG, but they did not
report the results of parsing with the obtained PCFG.
Henderson?s parsing model (Henderson, 2003) has a
similar motivation as ours in that a derivation history
of a parse tree is compactly represented by induced
hidden variables (hidden layer activation of a neu-
ral network), although the details of his approach is
quite different from ours.
2 Probabilistic model
PCFG-LA is a generative probabilistic model of
parse trees. In this model, an observed parse tree
is considered as an incomplete data, and the corre-
75
	 

:

:

 
 
Proceedings of the 43rd Annual Meeting of the ACL, pages 83?90,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Probabilistic disambiguation models for wide-coverage HPSG parsing
Yusuke Miyao
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper reports the development of log-
linear models for the disambiguation in
wide-coverage HPSG parsing. The esti-
mation of log-linear models requires high
computational cost, especially with wide-
coverage grammars. Using techniques to
reduce the estimation cost, we trained the
models using 20 sections of Penn Tree-
bank. A series of experiments empiri-
cally evaluated the estimation techniques,
and also examined the performance of the
disambiguation models on the parsing of
real-world sentences.
1 Introduction
Head-Driven Phrase Structure Grammar (HPSG)
(Pollard and Sag, 1994) has been studied extensively
from both linguistic and computational points of
view. However, despite research on HPSG process-
ing efficiency (Oepen et al, 2002a), the application
of HPSG parsing is still limited to specific domains
and short sentences (Oepen et al, 2002b; Toutanova
and Manning, 2002). Scaling up HPSG parsing to
assess real-world texts is an emerging research field
with both theoretical and practical applications.
Recently, a wide-coverage grammar and a large
treebank have become available for English HPSG
(Miyao et al, 2004). A large treebank can be used as
training and test data for statistical models. There-
fore, we now have the basis for the development and
the evaluation of statistical disambiguation models
for wide-coverage HPSG parsing.
The aim of this paper is to report the development
of log-linear models for the disambiguation in wide-
coverage HPSG parsing, and their empirical evalua-
tion through the parsing of the Wall Street Journal of
Penn Treebank II (Marcus et al, 1994). This is chal-
lenging because the estimation of log-linear models
is computationally expensive, and we require solu-
tions to make the model estimation tractable. We
apply two techniques for reducing the training cost.
One is the estimation on a packed representation of
HPSG parse trees (Section 3). The other is the filter-
ing of parse candidates according to a preliminary
probability distribution (Section 4).
To our knowledge, this work provides the first re-
sults of extensive experiments of parsing Penn Tree-
bank with a probabilistic HPSG. The results from
the Wall Street Journal are significant because the
complexity of the sentences is different from that of
short sentences. Experiments of the parsing of real-
world sentences can properly evaluate the effective-
ness and possibility of parsing models for HPSG.
2 Disambiguation models for HPSG
Discriminative log-linear models are now becom-
ing a de facto standard for probabilistic disambigua-
tion models for deep parsing (Johnson et al, 1999;
Riezler et al, 2002; Geman and Johnson, 2002;
Miyao and Tsujii, 2002; Clark and Curran, 2004b;
Kaplan et al, 2004). Previous studies on prob-
abilistic models for HPSG (Toutanova and Man-
ning, 2002; Baldridge and Osborne, 2003; Malouf
and van Noord, 2004) also adopted log-linear mod-
els. HPSG exploits feature structures to represent
linguistic constraints. Such constraints are known
83
to introduce inconsistencies in probabilistic models
estimated using simple relative frequency (Abney,
1997). Log-linear models are required for credible
probabilistic models and are also beneficial for in-
corporating various overlapping features.
This study follows previous studies on the proba-
bilistic models for HPSG. The probability,    , of
producing the parse result  from a given sentence 
is defined as
    


 
 
 
    
 



  

  

 

 

 
   
 
 
 

   
 



 

 

 

 
where  
 
   is a reference distribution (usually as-
sumed to be a uniform distribution), and    is a set
of parse candidates assigned to . The feature func-
tion 

   represents the characteristics of  and ,
while the corresponding model parameter 

   is
its weight. Model parameters that maximize the log-
likelihood of the training data are computed using a
numerical optimization method (Malouf, 2002).
Estimation of the above model requires a set of
pairs 
 
   , where 
 
is the correct parse for sen-
tence . While 
 
is provided by a treebank,    is
computed by parsing each  in the treebank. Pre-
vious studies assumed    could be enumerated;
however, the assumption is impractical because the
size of    is exponentially related to the length
of . The problem of exponential explosion is in-
evitable in the wide-coverage parsing of real-world
texts because many parse candidates are produced to
support various constructions in long sentences.
3 Packed representation of HPSG parse
trees
To avoid exponential explosion, we represent   
in a packed form of HPSG parse trees. A parse tree
of HPSG is represented as a set of tuples  	 
,
where 	 and 
 are the signs of mother, left daugh-
ter, and right daughter, respectively1. In chart pars-
ing, partial parse candidates are stored in a chart, in
which phrasal signs are identified and packed into an
equivalence class if they are determined to be equiv-
alent and dominate the same word sequence. A set
1For simplicity, only binary trees are considered. Extension
to unary and  -ary (    ) trees is trivial.
Figure 1: Chart for parsing ?he saw a girl with a
telescope?
of parse trees is then represented as a set of relations
among equivalence classes.
Figure 1 shows a chart for parsing ?he saw a
girl with a telescope?, where the modifiee (?saw?
or ?girl?) of ?with? is ambiguous. Each feature
structure expresses an equivalence class, and the ar-
rows represent immediate-dominance relations. The
phrase, ?saw a girl with a telescope?, has two trees
(A in the figure). Since the signs of the top-most
nodes are equivalent, they are packed into an equiv-
alence class. The ambiguity is represented as two
pairs of arrows that come out of the node.
Formally, a set of HPSG parse trees is represented
in a chart as a tuple 

 , where  is a set
of equivalence classes, 

  is a set of root
nodes, and      is a function to repre-
sent immediate-dominance relations.
Our representation of the chart can be interpreted
as an instance of a feature forest (Miyao and Tsujii,
2002; Geman and Johnson, 2002). A feature for-
est is an ?and/or? graph to represent exponentially-
many tree structures in a packed form. If    is
represented in a feature forest,       can be esti-
mated using dynamic programming without unpack-
ing the chart. A feature forest is formally defined as
a tuple,   ?, where  is a set of conjunc-
tive nodes,  is a set of disjunctive nodes,   
is a set of root nodes2,      is a conjunctive
daughter function, and ?     is a disjunctive
2For the ease of explanation, the definition of root node is
slightly different from the original.
84
Figure 2: Packed representation of HPSG parse trees
in Figure 1
daughter function. The feature functions 

   are
assigned to conjunctive nodes.
The simplest way to map a chart of HPSG parse
trees into a feature forest is to map each equivalence
class    to a conjunctive node    . How-
ever, in HPSG parsing, important features for dis-
ambiguation are combinations of a mother and its
daughters, i.e.,  	 
. Hence, we map the tuple


 
	
 

, which corresponds to  	 
, into a
conjunctive node.
Figure 2 shows (a part of) the HPSG parse trees
in Figure 1 represented as a feature forest. Square
boxes are conjunctive nodes, dotted lines express a
disjunctive daughter function, and solid arrows rep-
resent a conjunctive daughter function.
The mapping is formally defined as follows.
   

 
	
 

 

   
	
 

 

	
 

   

	,
   ,
   

 
	
 

 

 

 

 
	
 

 
	,
   

  

 

    

 


 
	
 

 
	
 

 
	
 

   

		,
and
 ?  

 
	
 

 
	
 

	 

 
	
 

  	.
Figure 3: Filtering of lexical entries for ?saw?
4 Filtering by preliminary distribution
The above method allows for the tractable estima-
tion of log-linear models on exponentially-many
HPSG parse trees. However, despite the develop-
ment of methods to improve HPSG parsing effi-
ciency (Oepen et al, 2002a), the exhaustive parsing
of all sentences in a treebank is still expensive.
Our idea is that we can omit the computation
of parse trees with low probabilities in the estima-
tion stage because    can be approximated with
parse trees with high probabilities. To achieve this,
we first prepared a preliminary probabilistic model
whose estimation did not require the parsing of a
treebank. The preliminary model was used to reduce
the search space for parsing a training treebank.
The preliminary model in this study is a unigram
model, 	    


  
  	  where    is a
word in the sentence , and 	 is a lexical entry as-
signed to . This model can be estimated without
parsing a treebank.
Given this model, we restrict the number of lexi-
cal entries used to parse a treebank. With a thresh-
old  for the number of lexical entries and a thresh-
old  for the probability, lexical entries are assigned
to a word in descending order of probability, until
the number of assigned entries exceeds , or the ac-
cumulated probability exceeds . If the lexical en-
try necessary to produce the correct parse is not as-
signed, it is additionally assigned to the word.
Figure 3 shows an example of filtering lexical en-
tries assigned to ?saw?. With   
, four lexical
entries are assigned. Although the lexicon includes
other lexical entries, such as a verbal entry taking a
sentential complement (   

 in the figure), they
are filtered out. This method reduces the time for
85
RULE the name of the applied schema
DIST the distance between the head words of the
daughters
COMMA whether a comma exists between daughters
and/or inside of daughter phrases
SPAN the number of words dominated by the phrase
SYM the symbol of the phrasal category (e.g. NP, VP)
WORD the surface form of the head word
POS the part-of-speech of the head word
LE the lexical entry assigned to the head word
Table 1: Templates of atomic features
parsing a treebank, while this approximation causes
bias in the training data and results in lower accu-
racy. The trade-off between the parsing cost and the
accuracy will be examined experimentally.
We have several ways to integrate 	  with the esti-
mated model      . In the experiments, we will
empirically compare the following methods in terms
of accuracy and estimation time.
Filtering only The unigram probability 	  is used
only for filtering.
Product The probability is defined as the product of
	  and the estimated model  .
Reference distribution 	  is used as a reference dis-
tribution of  .
Feature function  	  is used as a feature function
of  . This method was shown to be a gener-
alization of the reference distribution method
(Johnson and Riezler, 2000).
5 Features
Feature functions in the log-linear models are de-
signed to capture the characteristics of 

 
	
 

.
In this paper, we investigate combinations of the
atomic features listed in Table 1. The following
combinations are used for representing the charac-
teristics of the binary/unary schema applications.
binary 
 RULE,DIST,COMMA
SPAN
	
 SYM
	
WORD
	
 POS
	
 LE
	

SPAN

 SYM

WORD

 POS

 LE


unary  RULE,SYM,WORD,POS,LE
In addition, the following is for expressing the con-
dition of the root node of the parse tree.
root  SYM,WORD,POS,LE
Figure 4: Example features
Figure 4 shows examples: root is for the root
node, in which the phrase symbol is S and the
surface form, part-of-speech, and lexical entry of
the lexical head are ?saw?, VBD, and a transitive
verb, respectively. binary is for the binary rule ap-
plication to ?saw a girl? and ?with a telescope?,
in which the applied schema is the Head-Modifier
Schema, the left daughter is VP headed by ?saw?,
and the right daughter is PP headed by ?with?,
whose part-of-speech is IN and the lexical entry is
a VP-modifying preposition.
In an actual implementation, some of the atomic
features are abstracted (i.e., ignored) for smoothing.
Table 2 shows a full set of templates of combined
features used in the experiments. Each row rep-
resents a template of a feature function. A check
means the atomic feature is incorporated while a hy-
phen means the feature is ignored.
Restricting the domain of feature functions to


 
	
 

 seems to limit the flexibility of feature
design. Although it is true to some extent, this does
not necessarily mean the impossibility of incorpo-
rating features on nonlocal dependencies into the
model. This is because a feature forest model does
not assume probabilistic independence of conjunc-
tive nodes. This means that we can unpack a part of
the forest without changing the model. Actually, in
our previous study (Miyao et al, 2003), we success-
fully developed a probabilistic model including fea-
tures on nonlocal predicate-argument dependencies.
However, since we could not observe significant im-
provements by incorporating nonlocal features, this
paper investigates only the features described above.
86
RULE DIST COMMA SPAN SYM WORD POS LE
  
? ?
  
  
? ?
 
?
  
? ?

?

  
?
 
? ?

?
 
?
  

?
 
?
 
?

?
 
?

?


?
   
? ?
  
? ? ?
 
  
? ? ?

?
  
? ? ? ?

  
?

? ? ?

?
 
? ?
 

?
 
? ?

?

?
 
? ? ?


?
  
? ? ?
RULE SYM WORD POS LE

?
  

?
 
?

?

?

  
? ?

? ?
 

? ?

?

? ? ?

 
? ? ?
SYM WORD POS LE
?
  
?
 
?
?

?

 
? ?
? ?
 
? ?

?
? ? ?


? ? ?
Table 2: Feature templates for binary schema (left), unary schema (center), and root condition (right)
Avg. length LP LR UP UR F-score
Section 22 ( 40 words) 20.69 87.18 86.23 90.67 89.68 86.70
Section 22 ( 100 words) 22.43 86.99 84.32 90.45 87.67 85.63
Section 23 ( 40 words) 20.52 87.12 85.45 90.65 88.91 86.27
Section 23 ( 100 words) 22.23 86.81 84.64 90.29 88.03 85.71
Table 3: Accuracy for development/test sets
6 Experiments
We used an HPSG grammar derived from Penn
Treebank (Marcus et al, 1994) Section 02-21
(39,832 sentences) by our method of grammar de-
velopment (Miyao et al, 2004). The training data
was the HPSG treebank derived from the same por-
tion of the Penn Treebank3. For the training, we
eliminated sentences with no less than 40 words and
for which the parser could not produce the correct
parse. The resulting training set consisted of 33,574
sentences. The treebanks derived from Sections 22
and 23 were used as the development (1,644 sen-
tences) and final test sets (2,299 sentences). We
measured the accuracy of predicate-argument de-
pendencies output by the parser. A dependency is
defined as a tuple 

  

, where  is the
predicate type (e.g., adjective, intransitive verb), 

is the head word of the predicate,  is the argument
label (MODARG, ARG1, ..., ARG4), and 

is the
head word of the argument. Labeled precision/recall
(LP/LR) is the ratio of tuples correctly identified by
the parser, while unlabeled precision/recall (UP/UR)
is the ratio of 

and 

correctly identified re-
gardless of  and . The F-score is the harmonic
mean of LP and LR. The accuracy was measured by
parsing test sentences with part-of-speech tags pro-
3The programs to make the grammar and the tree-
bank from Penn Treebank are available at http://www-
tsujii.is.s.u-tokyo.ac.jp/enju/.
vided by the treebank. The Gaussian prior was used
for smoothing (Chen and Rosenfeld, 1999), and its
hyper-parameter was tuned for each model to max-
imize the F-score for the development set. The op-
timization algorithm was the limited-memory BFGS
method (Nocedal and Wright, 1999). All the follow-
ing experiments were conducted on AMD Opteron
servers with a 2.0-GHz CPU and 12-GB memory.
Table 3 shows the accuracy for the develop-
ment/test sets. Features occurring more than twice
were included in the model (598,326 features). Fil-
tering was done by the reference distribution method
with   
 and   
. The unigram model
for filtering was a log-linear model with two feature
templates, WORD POS LE and POS LE (24,847
features). Our results cannot be strictly compared
with other grammar formalisms because each for-
malism represents predicate-argument dependencies
differently; for reference, our results are competi-
tive with the corresponding measures reported for
Combinatory Categorial Grammar (CCG) (LP/LR
= 86.6/86.3) (Clark and Curran, 2004b). Different
from the results of CCG and PCFG (Collins, 1999;
Charniak, 2000), the recall was clearly lower than
precision. This results from the HPSG grammar
having stricter feature constraints and the parser not
being able to produce parse results for around one
percent of the sentences. To improve recall, we need
techniques of robust processing with HPSG.
87
LP LR Estimationtime (sec.)
Filtering only 34.90 23.34 702
Product 86.71 85.55 1,758
Reference dist. 87.12 85.45 655
Feature function 84.89 83.06 1,203
Table 4: Estimation method vs. accuracy and esti-
mation time
   F-score Estimationtime (sec.)
Parsing
time
(sec.)
Memory
usage
(MB)
5, 0.80 84.31 161 7,827 2,377
5, 0.90 84.69 207 9,412 2,992
5, 0.95 84.70 240 12,027 3,648
5, 0.98 84.81 340 15,168 4,590
10, 0.80 84.79 164 8,858 2,658
10, 0.90 85.77 298 13,996 4,062
10, 0.95 86.27 654 25,308 6,324
10, 0.98 86.56 1,778 55,691 11,700
15, 0.80 84.68 180 9,337 2,676
15, 0.90 85.85 308 14,915 4,220
15, 0.95 86.68 854 32,757 7,766
Table 5: Filtering threshold vs. accuracy and esti-
mation time
Table 4 compares the estimation methods intro-
duced in Section 4. In all of the following exper-
iments, we show the accuracy for the test set (
40 words) only. Table 4 revealed that our simple
method of filtering caused a fatal bias in training
data when a preliminary distribution was used only
for filtering. However, the model combined with a
preliminary model achieved sufficient accuracy. The
reference distribution method achieved higher accu-
racy and lower cost. The feature function method
achieved lower accuracy in our experiments. A pos-
sible reason is that a hyper-parameter of the prior
was set to the same value for all the features includ-
ing the feature of the preliminary distribution.
Table 5 shows the results of changing the filter-
ing threshold. We can determine the correlation be-
tween the estimation/parsing cost and accuracy. In
our experiment,  
 
 and  
 

 seem neces-
sary to preserve the F-score over 
.
Figure 5 shows the accuracy for each sentence
length. It is apparent from this figure that the ac-
curacy was significantly higher for shorter sentences
( 10 words). This implies that experiments with
only short sentences overestimate the performance
of parsers. Sentences with at least 10 words are nec-
0.8
0.82
0.84
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
0 5 10 15 20 25 30 35 40 45
pr
ec
isi
on
/re
ca
ll
sentence length
precision
recall
Figure 5: Sentence length vs. accuracy
 70
 75
 80
 85
 90
 95
 100
 0  5000  10000  15000  20000  25000  30000  35000  40000
pr
ec
isi
on
/re
ca
ll
training sentences
precision
recall
Figure 6: Corpus size vs. accuracy
essary to properly evaluate the performance of pars-
ing real-world texts.
Figure 6 shows the learning curve. A feature set
was fixed, while the parameter of the prior was op-
timized for each model. High accuracy was attained
even with small data, and the accuracy seemed to
be saturated. This indicates that we cannot further
improve the accuracy simply by increasing training
data. The exploration of new types of features is
necessary for higher accuracy.
Table 6 shows the accuracy with difference fea-
ture sets. The accuracy was measured by removing
some of the atomic features from the final model.
The last row denotes the accuracy attained by the
preliminary model. The numbers in bold type rep-
resent that the difference from the final model was
significant according to stratified shuffling tests (Co-
hen, 1995) with p-value  

. The results indicate
that DIST, COMMA, SPAN, WORD, and POS features
contributed to the final accuracy, although the dif-
88
Features LP LR # features
All 87.12 85.45 623,173
?RULE 86.98 85.37 620,511
?DIST 86.74 85.09 603,748
?COMMA 86.55 84.77 608,117
?SPAN 86.53 84.98 583,638
?SYM 86.90 85.47 614,975
?WORD 86.67 84.98 116,044
?POS 86.36 84.71 430,876
?LE 87.03 85.37 412,290
?DIST,SPAN 85.54 84.02 294,971
?DIST,SPAN,
COMMA 83.94 82.44 286,489
?RULE,DIST,
SPAN,COMMA 83.61 81.98 283,897
?WORD,LE 86.48 84.91 50,258
?WORD,POS 85.56 83.94 64,915
?WORD,POS,LE 84.89 83.43 33,740
?SYM,WORD,
POS,LE 82.81 81.48 26,761
None 78.22 76.46 24,847
Table 6: Accuracy with different feature sets
ferences were slight. In contrast, RULE, SYM, and
LE features did not affect the accuracy. However,
if each of them was removed together with another
feature, the accuracy decreased drastically. This im-
plies that such features had overlapping information.
Table 7 shows the manual classification of the
causes of errors in 100 sentences randomly chosen
from the development set. In our evaluation, one
error source may cause multiple errors of dependen-
cies. For example, if a wrong lexical entry was as-
signed to a verb, all the argument dependencies of
the verb are counted as errors. The numbers in the
table include such double-counting. Major causes
were classified into three types: argument/modifier
distinction, attachment ambiguity, and lexical am-
biguity. While attachment/lexical ambiguities are
well-known causes, the other is peculiar to deep
parsing. Most of the errors cannot be resolved by
features we investigated in this study, and the design
of other features is crucial for further improvements.
7 Discussion and related work
Experiments on deep parsing of Penn Treebank have
been reported for Combinatory Categorial Grammar
(CCG) (Clark and Curran, 2004b) and Lexical Func-
tional Grammar (LFG) (Kaplan et al, 2004). They
developed log-linear models on a packed represen-
tation of parse forests, which is similar to our rep-
resentation. Although HPSG exploits further com-
plicated feature constraints and requires high com-
Error cause # of errors
Argument/modifier distinction 58
temporal noun 21
to-infinitive 15
others 22
Attachment 53
prepositional phrase 18
to-infinitive 10
relative clause 8
others 17
Lexical ambiguity 42
participle/adjective 15
preposition/modifier 14
others 13
Comma 19
Coordination 14
Noun phrase identification 13
Zero-pronoun resolution 9
Others 17
Table 7: Error analysis
putational cost, our work has proved that log-linear
models can be applied to HPSG parsing and attain
accurate and wide-coverage parsing.
Clark and Curran (2004a) described a method of
reducing the cost of parsing a training treebank in
the context of CCG parsing. They first assigned to
each word a small number of supertags, which cor-
respond to lexical entries in our case, and parsed su-
pertagged sentences. Since they did not mention the
probabilities of supertags, their method corresponds
to our ?filtering only? method. However, they also
applied the same supertagger in a parsing stage, and
this seemed to be crucial for high accuracy. This
means that they estimated the probability of produc-
ing a parse tree from a supertagged sentence.
Another approach to estimating log-linear mod-
els for HPSG is to extract a small informative sam-
ple from the original set    (Osborne, 2000).
Malouf and van Noord (2004) successfully applied
this method to German HPSG. The problem with
this method was in the approximation of exponen-
tially many parse trees by a polynomial-size sample.
However, their method has the advantage that any
features on a parse tree can be incorporated into the
model. The trade-off between approximation and lo-
cality of features is an outstanding problem.
Other discriminative classifiers were applied to
the disambiguation in HPSG parsing (Baldridge and
Osborne, 2003; Toutanova et al, 2004). The prob-
lem of exponential explosion is also inevitable for
89
their methods. An approach similar to ours may be
applied to them, following the study on the learning
of a discriminative classifier for a packed represen-
tation (Taskar et al, 2004).
As discussed in Section 6, exploration of other
features is indispensable to further improvements.
A possible direction is to encode larger contexts of
parse trees, which were shown to improve the accu-
racy (Toutanova and Manning, 2002; Toutanova et
al., 2004). Future work includes the investigation of
such features, as well as the abstraction of lexical
dependencies like semantic classes.
References
S. P. Abney. 1997. Stochastic attribute-value grammars.
Computational Linguistics, 23(4).
J. Baldridge and M. Osborne. 2003. Active learning for
HPSG parse selection. In CoNLL-03.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. NAACL-2000, pages 132?139.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models. Technical Re-
port CMUCS-99-108, Carnegie Mellon University.
S. Clark and J. R. Curran. 2004a. The importance of su-
pertagging for wide-coverage CCG parsing. In Proc.
COLING-04.
S. Clark and J. R. Curran. 2004b. Parsing the WSJ using
CCG and log-linear models. In Proc. 42th ACL.
P. R. Cohen. 1995. Empirical Methods for Artificial In-
telligence. MIT Press.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univ. of
Pennsylvania.
S. Geman and M. Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proc. 40th ACL.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distributions in stochastic unification-based grammars.
In Proc. 1st NAACL.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic ?unification-based?
grammars. In Proc. ACL?99, pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc.
HLT/NAACL?04.
R. Malouf and G. van Noord. 2004. Wide coverage pars-
ing with stochastic attribute value grammars. In Proc.
IJCNLP-04 Workshop ?Beyond Shallow Analyses?.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proc. CoNLL-
2002.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn Treebank: Annotating predicate argu-
ment structure. In ARPA Human Language Technol-
ogy Workshop.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estima-
tion for feature forests. In Proc. HLT 2002.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2003. Probabilistic
modeling of argument structures including non-local
dependencies. In Proc. RANLP 2003, pages 285?291.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2004. Corpus-
oriented grammar development for acquiring a Head-
driven Phrase Structure Grammar from the Penn Tree-
bank. In Proc. IJCNLP-04.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
S. Oepen, D. Flickinger, J. Tsujii, and H. Uszkoreit, ed-
itors. 2002a. Collaborative Language Engineering:
A Case Study in Efficient Grammar-Based Processing.
CSLI Publications.
S. Oepen, K. Toutanova, S. Shieber, C. Manning,
D. Flickinger, and T. Brants. 2002b. The LinGO,
Redwoods treebank. motivation and preliminary appli-
cations. In Proc. COLING 2002.
M. Osborne. 2000. Estimation of stochastic attribute-
value grammar using an informative sample. In Proc.
COLING 2000.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch,
J. T. Maxwell III, and M. Johnson. 2002. Pars-
ing the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques. In
Proc. 40th ACL.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In EMNLP 2004.
K. Toutanova and C. D. Manning. 2002. Feature selec-
tion for a rich HPSG grammar using decision trees. In
Proc. CoNLL-2002.
K. Toutanova, P. Markova, and C. Manning. 2004. The
leaf projection path view of parse trees: Exploring
string kernels for HPSG parse selection. In EMNLP
2004.
90
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 465?472,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving the Scalability of Semi-Markov Conditional
Random Fields for Named Entity Recognition
Daisuke Okanohara? Yusuke Miyao? Yoshimasa Tsuruoka ? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
?School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
?SORST, Solution Oriented Research for Science and Technology
Honcho 4-1-8, Kawaguchi-shi, Saitama, Japan
{hillbig,yusuke,tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents techniques to apply
semi-CRFs to Named Entity Recognition
tasks with a tractable computational cost.
Our framework can handle an NER task
that has long named entities and many
labels which increase the computational
cost. To reduce the computational cost,
we propose two techniques: the first is the
use of feature forests, which enables us to
pack feature-equivalent states, and the sec-
ond is the introduction of a filtering pro-
cess which significantly reduces the num-
ber of candidate states. This framework
allows us to use a rich set of features ex-
tracted from the chunk-based representa-
tion that can capture informative charac-
teristics of entities. We also introduce a
simple trick to transfer information about
distant entities by embedding label infor-
mation into non-entity labels. Experimen-
tal results show that our model achieves an
F-score of 71.48% on the JNLPBA 2004
shared task without using any external re-
sources or post-processing techniques.
1 Introduction
The rapid increase of information in the biomedi-
cal domain has emphasized the need for automated
information extraction techniques. In this paper
we focus on the Named Entity Recognition (NER)
task, which is the first step in tackling more com-
plex tasks such as relation extraction and knowl-
edge mining.
Biomedical NER (Bio-NER) tasks are, in gen-
eral, more difficult than ones in the news domain.
For example, the best F-score in the shared task of
Bio-NER in COLING 2004 JNLPBA (Kim et al,
2004) was 72.55% (Zhou and Su, 2004) 1, whereas
the best performance at MUC-6, in which systems
tried to identify general named entities such as
person or organization names, was an accuracy of
95% (Sundheim, 1995).
Many of the previous studies of Bio-NER tasks
have been based on machine learning techniques
including Hidden Markov Models (HMMs) (Bikel
et al, 1997), the dictionary HMM model (Kou et
al., 2005) and Maximum Entropy Markov Mod-
els (MEMMs) (Finkel et al, 2004). Among these
methods, conditional random fields (CRFs) (Laf-
ferty et al, 2001) have achieved good results (Kim
et al, 2005; Settles, 2004), presumably because
they are free from the so-called label bias problem
by using a global normalization.
Sarawagi and Cohen (2004) have recently in-
troduced semi-Markov conditional random fields
(semi-CRFs). They are defined on semi-Markov
chains and attach labels to the subsequences of a
sentence, rather than to the tokens2. The semi-
Markov formulation allows one to easily construct
entity-level features. Since the features can cap-
ture all the characteristics of a subsequence, we
can use, for example, a dictionary feature which
measures the similarity between a candidate seg-
ment and the closest element in the dictionary.
Kou et al (2005) have recently showed that semi-
CRFs perform better than CRFs in the task of
recognition of protein entities.
The main difficulty of applying semi-CRFs to
Bio-NER lies in the computational cost at training
1Krauthammer (2004) reported that the inter-annotator
agreement rate of human experts was 77.6% for bio-NLP,
which suggests that the upper bound of the F-score in a Bio-
NER task may be around 80%.
2Assuming that non-entity words are placed in unit-length
segments.
465
Table 1: Length distribution of entities in the train-
ing set of the shared task in 2004 JNLPBA
Length # entity Ratio
1 21646 42.19
2 15442 30.10
3 7530 14.68
4 3505 6.83
5 1379 2.69
6 732 1.43
7 409 0.80
8 252 0.49
>8 406 0.79
total 51301 100.00
because the number of named entity classes tends
to be large, and the training data typically contain
many long entities, which makes it difficult to enu-
merate all the entity candidates in training. Table
1 shows the length distribution of entities in the
training set of the shared task in 2004 JNLPBA.
Formally, the computational cost of training semi-
CRFs is O(KLN), where L is the upper bound
length of entities, N is the length of sentence and
K is the size of label set. And that of training in
first order semi-CRFs is O(K2LN). The increase
of the cost is used to transfer non-adjacent entity
information.
To improve the scalability of semi-CRFs, we
propose two techniques: the first is to intro-
duce a filtering process that significantly re-
duces the number of candidate entities by using
a ?lightweight? classifier, and the second is to
use feature forest (Miyao and Tsujii, 2002), with
which we pack the feature equivalent states. These
enable us to construct semi-CRF models for the
tasks where entity names may be long and many
class-labels exist at the same time. We also present
an extended version of semi-CRFs in which we
can make use of information about a preceding
named entity in defining features within the frame-
work of first order semi-CRFs. Since the preced-
ing entity is not necessarily adjacent to the current
entity, we achieve this by embedding the informa-
tion on preceding labels for named entities into the
labels for non-named entities.
2 CRFs and Semi-CRFs
CRFs are undirected graphical models that encode
a conditional probability distribution using a given
set of features. CRFs allow both discriminative
training and bi-directional flow of probabilistic in-
formation along the sequence. In NER, we of-
ten use linear-chain CRFs, which define the con-
ditional probability of a state sequence y = y1, ...,
yn given the observed sequence x = x1,...,xn by:
p(y|x, ?) = 1
Z(x) exp(?
n
i=1?j?jfj(yi?1, yi, x, i)),
(1)
where fj(yi?1, yi,x, i) is a feature function and
Z(x) is the normalization factor over all the state
sequences for the sequence x. The model parame-
ters are a set of real-valued weights ? = {?j}, each
of which represents the weight of a feature. All the
feature functions are real-valued and can use adja-
cent label information.
Semi-CRFs are actually a restricted version of
order-L CRFs in which all the labels in a chunk are
the same. We follow the definitions in (Sarawagi
and Cohen, 2004). Let s = ?s1, ..., sp? denote a
segmentation of x, where a segment sj = ?tj , uj ,
yj? consists of a start position tj , an end position
uj , and a label yj . We assume that segments have a
positive length bounded above by the pre-defined
upper bound L (tj ? uj , uj ? tj + 1 ? L) and
completely cover the sequence x without overlap-
ping, that is, s satisfies t1 = 1, up = |x|, and
tj+1 = uj + 1 for j = 1, ..., p ? 1. Semi-CRFs
define a conditional probability of a state sequence
y given an observed sequence x by:
p(y|x, ?) = 1
Z(x) exp(?j?i?ifi(sj)), (2)
where fi(sj) := fi(yj?1, yj ,x, tj , uj) is a fea-
ture function and Z(x) is the normalization factor
as defined for CRFs. The inference problem for
semi-CRFs can be solved by using a semi-Markov
analog of the usual Viterbi algorithm. The com-
putational cost for semi-CRFs is O(KLN) where
L is the upper bound length of entities, N is the
length of sentence and K is the number of label
set. If we use previous label information, the cost
becomes O(K2LN).
3 Using Non-Local Information in
Semi-CRFs
In conventional CRFs and semi-CRFs, one can
only use the information on the adjacent previ-
ous label when defining the features on a certain
state or entity. In NER tasks, however, informa-
tion about a distant entity is often more useful than
466
O protein O O DNA
O protein O-protein O-protein DNA
Figure 1: Modification of ?O? (other labels) to
transfer information on a preceding named entity.
information about the previous state (Finkel et al,
2005). For example, consider the sentence ?... in-
cluding Sp1 and CP1.? where the correct labels of
?Sp1? and ?CP1? are both ?protein?. It would be
useful if the model could utilize the (non-adjacent)
information about ?Sp1? being ?protein? to clas-
sify ?CP1? as ?protein?. On the other hand, in-
formation about adjacent labels does not necessar-
ily provide useful information because, in many
cases, the previous label of a named entity is ?O?,
which indicates a non-named entity. For 98.0% of
the named entities in the training data of the shared
task in the 2004 JNLPBA, the label of the preced-
ing entity was ?O?.
In order to incorporate such non-local informa-
tion into semi-CRFs, we take a simple approach.
We divide the label of ?O? into ?O-protein? and
?O? so that they convey the information on the
preceding named entity. Figure 1 shows an ex-
ample of this conversion, in which the two labels
for the third and fourth states are converted from
?O? to ?O-protein?. When we define the fea-
tures for the fifth state, we can use the informa-
tion on the preceding entity ?protein? by look-
ing at the fourth state. Since this modification
changes only the label set, we can do this within
the framework of semi-CRF models. This idea is
originally proposed in (Peshkin and Pfeffer, 2003).
However, they used a dynamic Bayesian network
(DBNs) rather than a semi-CRF, and semi-CRFs
are likely to have significantly better performance
than DBNs.
In previous work, such non-local information
has usually been employed at a post-processing
stage. This is because the use of long distance
dependency violates the locality of the model and
prevents us from using dynamic programming
techniques in training and inference. Skip-CRFs
(Sutton and McCallum, 2004) are a direct imple-
mentation of long distance effects to the model.
However, they need to determine the structure
for propagating non-local information in advance.
In a recent study by Finkel et al, (2005), non-
local information is encoded using an indepen-
dence model, and the inference is performed by
Gibbs sampling, which enables us to use a state-
of-the-art factored model and carry out training ef-
ficiently, but inference still incurs a considerable
computational cost. Since our model handles lim-
ited type of non-local information, i.e. the label
of the preceding entity, the model can be solved
without approximation.
4 Reduction of Training/Inference Cost
The straightforward implementation of this mod-
eling in semi-CRFs often results in a prohibitive
computational cost.
In biomedical documents, there are quite a few
entity names which consist of many words (names
of 8 words in length are not rare). This makes
it difficult for us to use semi-CRFs for biomedi-
cal NER, because we have to set L to be eight or
larger, where L is the upper bound of the length of
possible chunks in semi-CRFs. Moreover, in or-
der to take into account the dependency between
named entities of different classes appearing in a
sentence, we need to incorporate multiple labels
into a single probabilistic model. For example, in
the shared task in COLING 2004 JNLPBA (Kim
et al, 2004) the number of labels is six (?pro-
tein?, ?DNA?, ?RNA?, ?cell line?, ?cell type?
and ?other?). This also increases the computa-
tional cost of a semi-CRF model.
To reduce the computational cost, we propose
two methods (see Figure 2). The first is employing
a filtering process using a lightweight classifier to
remove unnecessary state candidates beforehand
(Figure 2 (2)), and the second is the using the fea-
ture forest model (Miyao and Tsujii, 2002) (Fig-
ure 2 (3)), which employs dynamic programming
at training ?as much as possible?.
4.1 Filtering with a naive Bayes classifier
We introduce a filtering process to remove low
probability candidate states. This is the first step
of our NER system. After this filtering step, we
construct semi-CRFs on the remaining candidate
states using a feature forest. Therefore the aim of
this filtering is to reduce the number of candidate
states, without removing correct entities. This idea
467
(1) Enumerate
Candidate States
(2) Filtering by
Na?ve Bayes
(3) Construct feature forest
Training/
Inference
: other : entity
: other with preceding entity information
Figure 2: The framework of our system. We first enumerate all possible candidate states, and then filter
out low probability states by using a light-weight classifier, and represent them by using feature forest.
Table 2: Features used in the naive Bayes Classi-
fier for the entity candidate: ws, ws+1, ..., we. spi
is the result of shallow parsing at wi.
Feature Name Example of Features
Start/End Word ws, we
Inside Word ws, ws+1, ... , we
Context Word ws?1, we+1
Start/End SP sps, spe
Inside SP sps, sps+1, ..., spe
Context SP sps?1, spe+1
is similar to the method proposed by Tsuruoka and
Tsujii (2005) for chunk parsing, in which implau-
sible phrase candidates are removed beforehand.
We construct a binary naive Bayes classifier us-
ing the same training data as those for semi-CRFs.
In training and inference, we enumerate all possi-
ble chunks (the max length of a chunk is L as for
semi-CRFs) and then classify those into ?entity?
or ?other?. Table 2 lists the features used in the
naive Bayes classifier. This process can be per-
formed independently of semi-CRFs
Since the purpose of the filtering is to reduce the
computational cost, rather than to achieve a good
F-score by itself, we chose the threshold probabil-
ity of filtering so that the recall of filtering results
would be near 100 %.
4.2 Feature Forest
In estimating semi-CRFs, we can use an efficient
dynamic programming algorithm, which is simi-
lar to the forward-backward algorithm (Sarawagi
and Cohen, 2004). The proposal here is a more
general framework for estimating sequential con-
ditional random fields.
This framework is based on the feature forest
DNA
protein
Other
DNA
protein
Other
: or node (disjunctive node)
: and node (conjunctive node)
pos i i+1
??
Figure 3: Example of feature forest representation
of linear chain CRFs. Feature functions are as-
signed to ?and? nodes.
protein
O-protein
protein
u
j
=8 
prev-entity:protein
u
j
=  8
prev-entity: protein
packed
pos
87 9
Figure 4: Example of packed representation of
semi-CRFs. The states that have the same end po-
sition and prev-entity label are packed.
model, which was originally proposed for disam-
biguation models for parsing (Miyao and Tsujii,
2002). A feature forest model is a maximum en-
tropy model defined over feature forests, which are
abstract representations of an exponential number
of sequence/tree structures. A feature forest is
an ?and/or? graph: in Figure 3, circles represent
468
?and? nodes (conjunctive nodes), while boxes de-
note ?or? nodes (disjunctive nodes). Feature func-
tions are assigned to ?and? nodes. We can use
the information of the previous ?and? node for de-
signing the feature functions through the previous
?or? node. Each sequence in a feature forest is
obtained by choosing a conjunctive node for each
disjunctive node. For example, Figure 3 represents
3 ? 3 = 9 sequences, since each disjunctive node
has three candidates. It should be noted that fea-
ture forests can represent an exponential number
of sequences with a polynomial number of con-
junctive/disjunctive nodes.
One can estimate a maximum entropy model for
the whole sequence with dynamic programming
by representing the probabilistic events, i.e. se-
quence of named entity tags, by feature forests
(Miyao and Tsujii, 2002).
In the previous work (Lafferty et al, 2001;
Sarawagi and Cohen, 2004), ?or? nodes are con-
sidered implicitly in the dynamic programming
framework. In feature forest models, ?or? nodes
are packed when they have same conditions. For
example, ?or? nodes are packed when they have
same end positions and same labels in the first or-
der semi-CRFs,
In general, we can pack different ?or? nodes that
yield equivalent feature functions in the follow-
ing nodes. In other words, ?or? nodes are packed
when the following states use partial information
on the preceding states. Consider the task of tag-
ging entity and O-entity, where the latter tag is ac-
tually O tags that distinguish the preceding named
entity tags. When we simply apply first-order
semi-CRFs, we must distinguish states that have
different previous states. However, when we want
to distinguish only the preceding named entity tags
rather than the immediate previous states, feature
forests can represent these events more compactly
(Figure 4). We can implement this as follows. In
each ?or? node, we generate the following ?and?
nodes and their feature functions. Then we check
whether there exist ?or? node which has same con-
ditions by using its information about ?end posi-
tion? and ?previous entity?. If so, we connect the
?and? node to the corresponding ?or? node. If not,
we generate a new ?or? node and continue the pro-
cess.
Since the states with label O-entity and entity
are packed, the computational cost of training in
our model (First order semi-CRFs) becomes the
half of the original one.
5 Experiments
5.1 Experimental Setting
Our experiments were performed on the training
and evaluation set provided by the shared task in
COLING 2004 JNLPBA (Kim et al, 2004). The
training data used in this shared task came from
the GENIA version 3.02 corpus. In the task there
are five semantic labels: protein, DNA, RNA,
cell line and cell type. The training set consists
of 2000 abstracts from MEDLINE, and the evalu-
ation set consists of 404 abstracts. We divided the
original training set into 1800 abstracts and 200
abstracts, and the former was used as the training
data and the latter as the development data. For
semi-CRFs, we used amis3 for training the semi-
CRF with feature-forest. We used GENIA taggar4
for POS-tagging and shallow parsing.
We set L = 10 for training and evaluation when
we do not state L explicitly , where L is the upper
bound of the length of possible chunks in semi-
CRFs.
5.2 Features
Table 3 lists the features used in our semi-CRFs.
We describe the chunk-dependent features in de-
tail, which cannot be encoded in token-level fea-
tures.
?Whole chunk? is the normalized names at-
tached to a chunk, which performs like the closed
dictionary. ?Length? and ?Length and End-
Word? capture the tendency of the length of a
named entity. ?Count feature? captures the ten-
dency for named entities to appear repeatedly in
the same sentence.
?Preceding Entity and Prev Word? are fea-
tures that capture specifically words for conjunc-
tions such as ?and? or ?, (comma)?, e.g., for the
phrase ?OCIM1 and K562?, both ?OCIM1? and
?K562? are assigned cell line labels. Even if
the model can determine only that ?OCIM1? is a
cell line , this feature helps ?K562? to be assigned
the label cell line.
5.3 Results
We first evaluated the filtering performance. Table
4 shows the result of the filtering on the training
3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/
4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
Note that the evaluation data are not used for training the GE-
NIA tagger.
469
Table 3: Feature templates used for the chunk s := ws ws+1 ... we where ws and we represent the words
at the beginning and ending of the target chunk respectively. pi is the part of speech tag of wi and sci is
the shallow parse result of wi.
Feature Name description of features
Non-Chunk Features
Word/POS/SC with Position BEGIN + ws, END + we, IN + ws+1, ..., IN + we?1, BEGIN + ps,...
Context Uni-gram/Bi-gram ws?1, we+1, ws?2 + ws?1, we+1 + we+2, ws?1 + we+1
Prefix/Suffix of Chunk 2/3-gram character prefix of ws, 2/3/4-gram character suffix of we
Orthography capitalization and word formation of ws...we
Chunk Features
Whole chunk ws + ws+1 + ... + we
Word/POS/SC End Bi-grams we?1 + we, pe?1 + pe, sce?1 + sce
Length, Length and End Word |s|, |s|+we
Count Feature the frequency of wsws+1..we in a sentence is greater than one
Preceding Entity Features
Preceding Entity /and Prev Word PrevState, PrevState + ws?1
Table 4: Filtering results using the naive Bayes
classifier. The number of entity candidates for the
training set was 4179662, and that of the develop-
ment set was 418628.
Training set
Threshold probability reduction ratio recall
1.0 ? 10?12 0.14 0.984
1.0 ? 10?15 0.20 0.993
Development set
Threshold probability reduction ratio recall
1.0 ? 10?12 0.14 0.985
1.0 ? 10?15 0.20 0.994
and evaluation data. The naive Bayes classifiers
effectively reduced the number of candidate states
with very few falsely removed correct entities.
We then examined the effect of filtering on the
final performance. In this experiment, we could
not examine the performance without filtering us-
ing all the training data, because training on all
the training data without filtering required much
larger memory resources (estimated to be about
80G Byte) than was possible for our experimental
setup. We thus compared the result of the recog-
nizers with and without filtering using only 2000
sentences as the training data. Table 5 shows the
result of the total system with different filtering
thresholds. The result indicates that the filtering
method achieved very well without decreasing the
overall performance.
We next evaluate the effect of filtering, chunk
information and non-local information on final
performance. Table 6 shows the performance re-
sult for the recognition task. L means the upper
bound of the length of possible chunks in semi-
CRFs. We note that we cannot examine the re-
sult of L = 10 without filtering because of the in-
tractable computational cost. The row ?w/o Chunk
Feature? shows the result of the system which does
not employ Chunk-Features in Table 3 at training
and inference. The row ?Preceding Entity? shows
the result of a system which uses Preceding En-
tity and Preceding Entity and Prev Word fea-
tures. The results indicate that the chunk features
contributed to the performance, and the filtering
process enables us to use full chunk representation
(L = 10). The results of McNemar?s test suggest
that the system with chunk features is significantly
better than the system without it (the p-value is
less than 1.0 < 10?4). The result of the preceding
entity information improves the performance. On
the other hand, the system with preceding infor-
mation is not significantly better than the system
without it5. Other non-local information may im-
prove performance with our framework and this is
a topic for future work.
Table 7 shows the result of the overall perfor-
mance in our best setting, which uses the infor-
mation about the preceding entity and 1.0?10?15
threshold probability for filtering. We note that the
result of our system is similar to those of other sys-
5The result of the classifier on development data is 74.64
(without preceding information) and 75.14 (with preceding
information).
470
Table 5: Performance with filtering on the development data. (< 1.0 ? 10?12) means the threshold
probability of the filtering is 1.0 ? 10?12.
Recall Precision F-score Memory Usage (MB) Training Time (s)
Small Training Data = 2000 sentences
Without filtering 65.77 72.80 69.10 4238 7463
Filtering (< 1.0 ? 10.0?12) 64.22 70.62 67.27 600 1080
Filtering (< 1.0 ? 10.0?15) 65.34 72.52 68.74 870 2154
All Training Data = 16713 sentences
Without filtering Not available Not available
Filtering (< 1.0 ? 10.0?12) 70.05 76.06 72.93 10444 14661
Filtering (< 1.0 ? 10.0?15) 72.09 78.47 75.14 15257 31636
Table 6: Overall performance on the evaluation set. L is the upper bound of the length of possible chunks
in semi-CRFs.
Recall Precision F-score
L < 5 64.33 65.51 64.92
L = 10 + Filtering (< 1.0 ? 10.0?12) 70.87 68.33 69.58
L = 10 + Filtering (< 1.0 ? 10.0?15) 72.59 70.16 71.36
w/o Chunk Feature 70.53 69.92 70.22
+ Preceding Entity 72.65 70.35 71.48
tems in several respects, that is, the performance of
cell line is not good, and the performance of the
right boundary identification (78.91% in F-score)
is better than that of the left boundary identifica-
tion (75.19% in F-score).
Table 8 shows a comparison between our sys-
tem and other state-of-the-art systems. Our sys-
tem has achieved a comparable performance to
these systems and would be still improved by us-
ing external resources or conducting pre/post pro-
cessing. For example, Zhou et. al (2004) used
post processing, abbreviation resolution and exter-
nal dictionary, and reported that they improved F-
score by 3.1%, 2.1% and 1.2% respectively. Kim
et. al (2005) used the original GENIA corpus
to employ the information about other semantic
classes for identifying term boundaries. Finkel
et. al (2004) used gazetteers, web-querying, sur-
rounding abstracts, and frequency counts from
the BNC corpus. Settles (2004) used seman-
tic domain knowledge of 17 types of lexicon.
Since our approach and the use of external re-
sources/knowledge do not conflict but are com-
plementary, examining the combination of those
techniques should be an interesting research topic.
Table 7: Performance of our system on the evalu-
ation set
Class Recall Precision F-score
protein 77.74 68.92 73.07
DNA 69.03 70.16 69.59
RNA 69.49 67.21 68.33
cell type 65.33 82.19 72.80
cell line 57.60 53.14 55.28
overall 72.65 70.35 71.48
Table 8: Comparison with other systems
System Recall Precision F-score
Zhou et. al (2004) 75.99 69.42 72.55
Our system 72.65 70.35 71.48
Kim et.al (2005) 72.77 69.68 71.19
Finkel et. al (2004) 68.56 71.62 70.06
Settles (2004) 70.3 69.3 69.8
471
6 Conclusion
In this paper, we have proposed a single proba-
bilistic model that can capture important charac-
teristics of biomedical named entities. To over-
come the prohibitive computational cost, we have
presented an efficient training framework and a fil-
tering method which enabled us to apply first or-
der semi-CRF models to sentences having many
labels and entities with long names. Our results
showed that our filtering method works very well
without decreasing the overall performance. Our
system achieved an F-score of 71.48% without the
use of gazetteers, post-processing or external re-
sources. The performance of our system came
close to that of the current best performing system
which makes extensive use of external resources
and rule based post-processing.
The contribution of the non-local information
introduced by our method was not significant in
the experiments. However, other types of non-
local information have also been shown to be ef-
fective (Finkel et al, 2005) and we will examine
the effectiveness of other non-local information
which can be embedded into label information.
As the next stage of our research, we hope to ap-
ply our method to shallow parsing, in which seg-
ments tend to be long and non-local information is
important.
References
Daniel M. Bikel, Richard Schwartz, and Ralph
Weischedel. 1997. Nymble: a high-performance
learning name-finder. In Proc. of the Fifth Confer-
ence on Applied Natural Language Processing.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malv-
ina Nissim, Gail Sinclair, and Christopher Man-
ning. 2004. Exploiting context for biomedical en-
tity recognition: From syntax to the web. In Proc. of
JNLPBA-04.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proc. of ACL 2005, pages 363?370.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proc. of JNLPBA-04, pages 70?75.
Seonho Kim, Juntae Yoon, Kyung-Mi Park, and Hae-
Chang Rim. 2005. Two-phase biomedical named
entity recognition using a hybrid method. In Proc. of
the Second International Joint Conference on Natu-
ral Language Processing (IJCNLP-05).
Zhenzhen Kou, William W. Cohen, and Robert F. Mur-
phy. 2005. High-recall protein entity recognition
using a dictionary. Bioinformatics 2005 21.
Micahel Krauthammer and Goran Nenadic. 2004.
Term identification in the biomedical literature. Jor-
nal of Biomedical Informatics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML 2001.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002.
Peshkin and Pfeffer. 2003. Bayesian information ex-
traction network. In IJCAI.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In NIPS 2004.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Proc. of JNLPBA-04.
Beth M. Sundheim. 1995. Overview of results of the
MUC-6 evaluation. In Sixth Message Understand-
ing Conference (MUC-6), pages 13?32.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. In ICML workshop on Sta-
tistical Relational Learning.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Chunk
parsing revisited. In Proceedings of the 9th Inter-
national Workshop on Parsing Technologies (IWPT
2005).
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recogni-
tion. In Proc. of JNLPBA-04.
472
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1017?1024,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Retrieval for the Accurate Identification of Relational Concepts
in Massive Textbases
Yusuke Miyao? Tomoko Ohta? Katsuya Masuda? Yoshimasa Tsuruoka?
Kazuhiro Yoshida? Takashi Ninomiya? Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?Information Technology Center, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{yusuke,okap,kmasuda,tsuruoka,kyoshida,ninomi,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper introduces a novel framework
for the accurate retrieval of relational con-
cepts from huge texts. Prior to retrieval,
all sentences are annotated with predicate
argument structures and ontological iden-
tifiers by applying a deep parser and a term
recognizer. During the run time, user re-
quests are converted into queries of region
algebra on these annotations. Structural
matching with pre-computed semantic an-
notations establishes the accurate and effi-
cient retrieval of relational concepts. This
framework was applied to a text retrieval
system for MEDLINE. Experiments on
the retrieval of biomedical correlations re-
vealed that the cost is sufficiently small for
real-time applications and that the retrieval
precision is significantly improved.
1 Introduction
Rapid expansion of text information has motivated
the development of efficient methods of access-
ing information in huge texts. Furthermore, user
demand has shifted toward the retrieval of more
precise and complex information, including re-
lational concepts. For example, biomedical re-
searchers deal with a massive quantity of publica-
tions; MEDLINE contains approximately 15 mil-
lion references to journal articles in life sciences,
and its size is rapidly increasing, at a rate of more
than 10% yearly (National Library of Medicine,
2005). Researchers would like to be able to
search this huge textbase for biomedical correla-
tions such as protein-protein or gene-disease asso-
ciations (Blaschke and Valencia, 2002; Hao et al,
2005; Chun et al, 2006). However, the framework
of traditional information retrieval (IR) has diffi-
culty with the accurate retrieval of such relational
concepts because relational concepts are essen-
tially determined by semantic relations between
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
The present paper demonstrates a framework
for the accurate real-time retrieval of relational
concepts from huge texts. Prior to retrieval, we
prepare a semantically annotated textbase by ap-
plying NLP tools including deep parsers and term
recognizers. That is, all sentences are annotated
in advance with semantic structures and are stored
in a structured database. User requests are con-
verted on the fly into patterns of these semantic
annotations, and texts are retrieved by matching
these patterns with the pre-computed semantic an-
notations. The accurate retrieval of relational con-
cepts is attained because we can precisely describe
relational concepts using semantic annotations. In
addition, real-time retrieval is possible because se-
mantic annotations are computed in advance.
This framework has been implemented for a
text retrieval system for MEDLINE. We first ap-
ply a deep parser (Miyao and Tsujii, 2005) and
a dictionary-based term recognizer (Tsuruoka and
Tsujii, 2004) to MEDLINE and obtain annotations
of predicate argument structures and ontological
identifiers of genes, gene products, diseases, and
events. We then provide a search engine for these
annotated sentences. User requests are converted
into queries of region algebra (Clarke et al, 1995)
extended with variables (Masuda et al, 2006) on
these annotations. A search engine for the ex-
tended region algebra efficiently finds sentences
having semantic annotations that match the input
queries. In this paper, we evaluate this system with
respect to the retrieval of biomedical correlations
1017
Symbol CRP
Name C-reactive protein, pentraxin-related
Species Homo sapiens
Synonym MGC88244, PTX1
Product C-reactive protein precursor, C-reactive
protein, pentraxin-related protein
External links EntrezGene:1401, GDB:119071, . . .
Table 1: An example GENA entry
and examine the effects of using predicate argu-
ment structures and ontological identifiers.
The need for the discovery of relational con-
cepts has been investigated intensively in Infor-
mation Extraction (IE). However, little research
has targeted on-demand retrieval from huge texts.
One difficulty is that IE techniques such as pat-
tern matching and machine learning require heav-
ier processing in order to be applied on the fly.
Another difficulty is that target information must
be formalized beforehand and each system is de-
signed for a specific task. For instance, an IE
system for protein-protein interactions is not use-
ful for finding gene-disease associations. Apart
from IE research, enrichment of texts with vari-
ous annotations has been proposed and is becom-
ing a new research area for information manage-
ment (IBM, 2005; TEI, 2004). The present study
basically examines this new direction in research.
The significant contribution of the present paper,
however, is to provide the first empirical results of
this framework for a real task with a huge textbase.
2 Background: Resources and Tools for
Semantic Annotations
The proposed system for the retrieval of relational
concepts is a product of recent developments in
NLP resources and tools. In this section, ontology
databases, deep parsers, and search algorithms for
structured data are introduced.
2.1 Ontology databases
Ontology databases are collections of words and
phrases in specific domains. Such databases have
been constructed extensively for the systematic
management of domain knowledge by organizing
textual expressions of ontological entities that are
detached from actual sentences.
For example, GENA (Koike and Takagi, 2004)
is a database of genes and gene products that
is semi-automatically collected from well-known
databases, including HUGO, OMIM, Genatlas,
Locuslink, GDB, MGI, FlyBase, WormBase,
Figure 1: An output of HPSG parsing
Figure 2: A predicate argument structure
CYGD, and SGD. Table 1 shows an example of
a GENA entry. ?Symbol? and ?Name? denote
short forms and nomenclatures of genes, respec-
tively. ?Species? represents the organism species
in which this gene is observed. ?Synonym? is a
list of synonyms and name variations. ?Product?
gives a list of products of this gene, such as pro-
teins coded by this gene. ?External links? pro-
vides links to other databases, and helps to obtain
detailed information from these databases. For
biomedical terms other than genes/gene products,
the Unified Medical Language System (UMLS)
meta-thesaurus (Lindberg et al, 1993) is a large
database that contains various names of biomedi-
cal and health-related concepts.
Ontology databases provide mappings be-
tween textual expressions and entities in the real
world. For example, Table 1 indicates that CRP,
MGC88244, and PTX1 denote the same gene con-
ceptually. Hence, these resources enable us to
canonicalize variations of textual expressions of
ontological entities.
2.2 Parsing technologies
Recently, state-of-the-art CFG parsers (Charniak
and Johnson, 2005) can compute phrase structures
of natural sentences at fairly high accuracy. These
parsers have been used in various NLP tasks in-
cluding IE and text mining. In addition, parsers
that compute deeper analyses, such as predicate
argument structures, have become available for
1018
the processing of real-world sentences (Miyao and
Tsujii, 2005). Predicate argument structures are
canonicalized representations of sentence mean-
ings, and express the semantic relations of words
explicitly. Figure 1 shows an output of an HPSG
parser (Miyao and Tsujii, 2005) for the sentence
?A normal serum CRP measurement does not ex-
clude deep vein thrombosis.? The dotted lines ex-
press predicate argument relations. For example,
the ARG1 arrow coming from ?exclude? points
to the noun phrase ?A normal serum CRP mea-
surement?, which indicates that the subject of ?ex-
clude? is this noun phrase, while such relations are
not explicitly represented by phrase structures.
Predicate argument structures are beneficial for
our purpose because they can represent relational
concepts in an abstract manner. For example, the
relational concept of ?CRP excludes thrombosis?
can be represented as a predicate argument struc-
ture, as shown in Figure 2. This structure is univer-
sal in various syntactic expressions, such as pas-
sivization (e.g., ?thrombosis is excluded by CRP?)
and relativization (e.g., ?thrombosis that CRP ex-
cludes?). Hence, we can abstract surface varia-
tions of sentences and describe relational concepts
in a canonicalized form.
2.3 Structural search algorithms
Search algorithms for structured texts have been
studied extensively, and examples include XML
databases with XPath (Clark and DeRose, 1999)
and XQuery (Boag et al, 2005), and region alge-
bra (Clarke et al, 1995). The present study fo-
cuses on region algebra extended with variables
(Masuda et al, 2006) because it provides an ef-
ficient search algorithm for tags with cross bound-
aries. When we annotate texts with various levels
of syntactic/semantic structures, cross boundaries
are inherently nonnegligible. In fact, as described
in Section 3, our system exploits annotations of
predicate argument structures and ontological en-
tities, which include substantial cross boundaries.
Region algebra is defined as a set of operators
on regions, i.e., word sequences. Table 2 shows
operators of the extended region algebra, where
A and B denote regions, and results of operations
are also regions. For example, ?A & B? denotes a
region that includes both A and B. Four contain-
ment operators, >, >>, <, and <<, represent an-
cestor/descendant relations in XML. For example,
?A > B? indicates that A is an ancestor of B. In
[tag] Region covered with ?<tag>?
A > B A containing B
A >> B A containing B (A is not nested)
A < B A contained by B
A << B A contained by B (B is not nested)
A - B Starting with A and ending with B
A & B A and B
A | B A or B
Table 2: Operators of the extended region algebra
[sentence] >>
(([word arg1="$subject"] > exclude) &
([phrase id="$subject"] > CRP))
Figure 3: A query of the extended region algebra
Figure 4: Matching with the query in Figure 3
search algorithms for region algebra, the cost of
retrieving the first answer is constant, and that of
an exhaustive search is bounded by the lowest fre-
quency of a word in a query (Clarke et al, 1995).
Variables in the extended region algebra allow
us to express shared structures and are necessary
in order to describe predicate argument structures.
For example, Figure 3 shows a formula in the ex-
tended region algebra that represents the predicate
argument structure of ?CRP excludes something.?
This formula indicates that a sentence contains a
region in which the word ?exclude? exists, the
first argument (?arg1?) phrase of which includes
the word ?CRP.? A predicate argument relation is
expressed by the variable, ?$subject.? Figure 4
shows a situation in which this formula is satisfied.
Three horizontal bars describe regions covered by
<sentence>, <phrase>, and <word> tags,
respectively. The dotted line denotes the relation
expressed by this variable. Given this formula as a
query, a search engine can retrieve sentences hav-
ing semantic annotations that satisfy this formula.
3 A Text Retrieval System for MEDLINE
While the above resources and tools have been de-
veloped independently, their collaboration opens
up a new framework for the retrieval of relational
concepts, as described below (Figure 5).
Off-line processing: Prior to retrieval, a deep
parser is applied to compute predicate argument
1019
Figure 5: Framework of semantic retrieval
structures, and a term recognizer is applied to cre-
ate mappings from textual expressions into identi-
fiers in ontology databases. Semantic annotations
are stored and indexed in a structured database for
the extended region algebra.
On-line processing: User input is converted into
queries of the extended region algebra. A search
engine retrieves sentences having semantic anno-
tations that match the queries.
This framework is applied to a text retrieval en-
gine for MEDLINE. MEDLINE is an exhaustive
database covering nearly 4,500 journals in the life
sciences and includes the bibliographies of arti-
cles, about half of which have abstracts. Research
on IE and text mining in biomedical science has
focused mainly on MEDLINE. In the present pa-
per, we target al articles indexed in MEDLINE at
the end of 2004 (14,785,094 articles). The follow-
ing sections explain in detail off-/on-line process-
ing for the text retrieval system for MEDLINE.
3.1 Off-line processing: HPSG parsing and
term recognition
We first parsed all sentences using an HPSG parser
(Miyao and Tsujii, 2005) to obtain their predi-
cate argument structures. Because our target is
biomedical texts, we re-trained a parser (Hara et
al., 2005) with the GENIA treebank (Tateisi et
al., 2005), and also applied a bidirectional part-of-
speech tagger (Tsuruoka and Tsujii, 2005) trained
with the GENIA treebank as a preprocessor.
Because parsing speed is still unrealistic for
parsing the entire MEDLINE on a single ma-
chine, we used two geographically separated com-
puter clusters having 170 nodes (340 Xeon CPUs).
These clusters are separately administered and not
dedicated for use in the present study. In order to
effectively use such an environment, GXP (Taura,
2004) was used to connect these clusters and dis-
tribute the load among them. Our processes were
given the lowest priority so that our task would not
disturb other users. We finished parsing the entire
MEDLINE in nine days (Ninomiya et al, 2006).
# entries (genes) 517,773
# entries (gene products) 171,711
# entries (diseases) 148,602
# expanded entries 4,467,855
Table 3: Sizes of ontologies used for term recog-
nition
Event type Expressions
influence effect, affect, role, response, . . .
regulation mediate, regulate, regulation, . . .
activation induce, activate, activation, . . .
Table 4: Event expression ontology
Next, we annotated technical terms, such as
genes and diseases, to create mappings to onto-
logical identifiers. A dictionary-based term recog-
nition algorithm (Tsuruoka and Tsujii, 2004) was
applied for this task. First, an expanded term
list was created by generating name variations of
terms in GENA and the UMLS meta-thesaurus1.
Table 3 shows the size of the original database and
the number of entries expanded by name varia-
tions. Terms in MEDLINE were then identified
by the longest matching of entries in this expanded
list with words/phrases in MEDLINE.
The necessity of ontologies is not limited to
nominal expressions. Various verbs are used for
expressing events. For example, activation events
of proteins can be expressed by ?activate,? ?en-
hance,? and other event expressions. Although the
numbers of verbs and their event types are much
smaller than those of technical terms, verbal ex-
pressions are important for the description of rela-
tional concepts. Since ontologies of event expres-
sions in this domain have not yet been constructed,
we developed an ontology from scratch. We inves-
tigated 500 abstracts extracted from MEDLINE,
and classified 167 frequent expressions, including
verbs and their nominalized forms, into 18 event
types. Table 4 shows a part of this ontology. These
expressions in MEDLINE were automatically an-
notated with event types.
As a result, we obtained semantically annotated
MEDLINE. Table 5 shows the size of the orig-
inal MEDLINE and semantic annotations. Fig-
ure 6 shows semantic annotations for the sentence
in Figure 1, where ?-? indicates nodes of XML,2
1We collected disease names by specifying a query with
the semantic type as ?Disease or Syndrome.?
2Although this example is shown in XML, this textbase
contains tags with cross boundaries because tags for predicate
argument structures and technical terms may overlap.
1020
# papers 14,785,094
# abstracts 7,291,857
# sentences 70,935,630
# words 1,462,626,934
# successfully parsed sentences 69,243,788
# predicate argument relations 1,510,233,701
# phrase tags 3,094,105,383
# terms (genes) 84,998,621
# terms (gene products) 27,471,488
# terms (diseases) 19,150,984
# terms (event expressions) 51,810,047
Size of the original MEDLINE 9.3 GByte
Size of the semantic annotations 292 GByte
Size of the index file for region algebra 954 GByte
Table 5: Sizes of the original and semantically an-
notated MEDLINE textbases
- <sentence sentence_id="e6e525">
- <phrase id="0" cat="S" head="15" lex_head="18">
- <phrase id="1" cat="NP" head="4" lex_head="14">
- <phrase id="2" cat="DT" head="3" lex_head="3">
- <word id="3" pos="DT" cat="DT" base="a" arg1="4">
- A
- <phrase id="4" cat="NP" head="7" lex_head="14">
- <phrase id="5" cat="AJ" head="6" lex_head="6">
- <word id="6" pos="JJ" cat="AJ" base="normal" arg1="7">
- normal
- <phrase id="7" cat="NP" head="10" lex_head="14">
- <phrase id="8" cat="NP" head="9" lex_head="9">
- <word id="9" pos="NN" cat="NP" base="serum" mod="10">
- serum
- <phrase id="10" cat="NP" head="13" lex_head="14">
- <phrase id="11" cat="NP" head="12" lex_head="12">
- <entity_name id="entity-1" type="gene"
gene_id="GHS003134" gene_symbol="CRP"
gene_name="C-reactive protein, pentraxin-related"
species="Homo sapiens"
db_site="EntrezGene:1401|GDB:119071|GenAtlas:CRP">
- <word id="12" pos="NN" cat="NP" base="crp" mod="13">
- CRP
- <phrase id="13" cat="NP" head="14" lex_head="14">
- <word id="14" pos="NN" cat="NP" base="measurement">
- measurement
- <phrase id="15" cat="VP" head="16" lex_head="18">
- <phrase id="16" cat="VP" head="17" lex_head="18">
- <phrase id="17" cat="VP" head="18" lex_head="18">
- <word id="18" pos="VBZ" cat="VP" base="do"
arg1="1" arg2="21">
- does
- <phrase id="19" cat="AV" head="20" lex_head="20">
- <word id="20" pos="RB" cat="AV" base="not" arg1="21">
- not
- <phrase id="21" cat="VP" head="22" lex_head="23">
- <phrase id="22" cat="VP" head="23" lex_head="23">
- <word id="23" pos="VB" cat="VP" base="exclude"
arg1="1" arg2="24">
- exclude
...
Figure 6: A semantically annotated sentence
although the latter half of the sentence is omitted
because of space limitations. Sentences are an-
notated with four tags,3 ?phrase,? ?word,? ?sen-
tence,? and ?entity name,? and their attributes as
given in Table 6. Predicate argument structures are
annotated as attributes, ?mod? and ?argX ,? which
point to the IDs of the argument phrases. For ex-
ample, in Figure 6, the <word> tag for ?exclude?
has the attributes arg1="1" and arg2="24",
which denote the IDs of the subject and object
phrases, respectively.
3Additional tags exist for representing document struc-
tures such as ?title? (details omitted).
Tag Attributes
phrase id, cat, head, lex head
word id, cat, pos, base, mod, argX , rel type
sentence sentence id
entity name id, type, gene id/disease id, gene symbol,
gene name, species, db site
Attribute Description
id unique identifier
cat syntactic category
head head daughter?s ID
lex head lexical head?s ID
pos part-of-speech
base base form of the word
mod ID of modifying phrase
argX ID of the X-th argument of the word
rel type event type
sentence id sentence?s ID
type whether gene, gene prod, or disease
gene id ID in GENA
disease id ID in the UMLS meta-thesaurus
gene symbol short form of the gene
gene name nomenclature of the gene
species species that have this gene
db site links to external databases
Table 6: Tags (upper) and attributes (lower) for
semantic annotations
3.2 On-line processing
The off-line processing described above results in
much simpler on-line processing. User input is
converted into queries of the extended region al-
gebra, and the converted queries are entered into a
search engine for the extended region algebra. The
implementation of a search engine is described in
detail in Masuda et al (2006).
Basically, given subject x, object y, and verb v,
the system creates the following query:
[sentence] >>
([word arg1="$subject" arg2="$object"
base="v"] &
([phrase id="$subject"] > x) &
([phrase id="$object"] > y))
Ontological identifiers are substituted for x, y,
and v, if possible. Nominal keywords, i.e., x and
y, are replaced by [entity_name gene_id="n"]
or [entity_name disease_id="n"], where n is
the ontological identifier of x or y. For verbal key-
words, base="v" is replaced by rel_type="r",
where r is the event type of v.
4 Evaluation
Our system is evaluated with respect to speed and
accuracy. Speed is indispensable for real-time in-
teractive text retrieval systems, and accuracy is key
for the motivation of semantic retrieval. That is,
our motivation for employing semantic retrieval
1021
Query No. User input
1 something inhibit ERK2
2 something trigger diabetes
3 adiponectin increase something
4 TNF activate IL6
5 dystrophin cause disease
6 macrophage induce something
7 something suppress MAP phosphorylation
8 something enhance p53 (negative)
Table 7: Queries for experiments
[sentence] >>
([word rel_type="activation"] &
[entity_name type="gene" gene_id="GHS019685"] &
[entity_name type="gene" gene_id="GHS009426"])
[sentence] >>
([word arg1="$subject" arg2="$object"
rel_type="activation"] &
([phrase id="$subject"] >
[entity_name type="gene" gene_id="GHS019685"]) &
([phrase cat="np" id="$object"] >
[entity_name type="gene" gene_id="GHS009426"]))
Figure 7: Queries of the extended region algebra
for Query 4-3 (upper: keyword search, lower: se-
mantic search)
was to provide a device for the accurate identifica-
tion of relational concepts. In particular, high pre-
cision is desired in text retrieval from huge texts
because users want to extract relevant information,
rather than collect exhaustive information.
We have two parameters to vary: whether to
use predicate argument structures and whether to
use ontological identifiers. The effect of using
predicate argument structures is evaluated by com-
paring ?keyword search? with ?semantic search.?
The former is a traditional style of IR, in which
sentences are retrieved by matching words in a
query with words in sentences. The latter is a
new feature of the present system, in which sen-
tences are retrieved by matching predicate argu-
ment relations in a query with those in a semanti-
cally annotated textbase. The effect of using onto-
logical identifiers is assessed by changing queries
of the extended region algebra. When we use the
term ontology, nominal keywords in queries are
replaced with ontological identifiers in GENA and
the UMLS meta-thesaurus. When we use the event
expression ontology, verbal keywords in queries
are replaced with event types.
Table 7 is a list of queries used in the follow-
ing experiments. Words in italics indicate a class
of words: ?something? indicates that any word
can appear, and disease indicates that any dis-
ease expression can appear. These queries were
selected by a biologist, and express typical re-
lational concepts that a biologist may wish to
find. Queries 1, 3, and 4 represent relations of
genes/proteins, where ERK2, adiponectin, TNF,
and IL6 are genes/proteins. Queries 2 and 5 de-
scribe relations concerning diseases, and Query 6
is a query that is not relevant to genes or diseases.
Query 7 expresses a complex relation concern-
ing a specific phenomena, i.e., phosphorylation,
of MAP. Query 8 describes a relation concerning
a gene, i.e., p53, while ?(negative)? indicates that
the target of retrieval is negative mentions. This is
expressed by ?not? modifying a predicate.
For example, Query 4 attempts to retrieve sen-
tences that mention the protein-protein interaction
?TNF activates IL6.? This is converted into queries
of the extended region algebra given in Figure 7.
The upper query is for keyword search and only
specifies the appearances of the three words. Note
that the keywords are translated into the ontolog-
ical identifiers, ?activation,? ?GHS019685,? and
?GHS009426.? The lower query is for semantic
search. The variables in ?arg1? and ?arg2? indi-
cate that ?GHS019685? and ?GHS009426? are the
subject and object, respectively, of ?activation?.
Table 8 summarizes the results of the experi-
ments. The postfixes of query numbers denote
whether ontological identifiers are used. X-1 used
no ontologies, and X-2 used only the term ontol-
ogy. X-3 used both the term and event expression
ontologies4. Comparison of X-1 and X-2 clarifies
the effect of using the term ontology. Comparison
of X-2 and X-3 shows the effect of the event ex-
pression ontology. The results for X-3 indicate
the maximum performance of the current system.
This table shows that the time required for the se-
mantic search for the first answer, shown as ?time
(first)? in seconds, was reasonably short. Thus,
the present framework is acceptable for real-time
text retrieval. The numbers of answers increased
when we used the ontologies, and this result indi-
cates the efficacy of both ontologies for obtaining
relational concepts written in various expressions.
Accuracy was measured by judgment by a bi-
ologist. At most 100 sentences were retrieved for
each query, and the results of keyword search and
semantic search were merged and shuffled. A bi-
ologist judged the shuffled sentences (1,839 sen-
tences in total) without knowing whether the sen-
4Query 5-1 is not tested because ?disease? requires
the term ontology, and Query 6-2 is not tested because
?macrophage? is not assigned an ontological identifier.
1022
Query Keyword search Semantic search
No. # ans. time (first/all) precision n-precision # ans. time (first/all) precision relative recall
1-1 252 0.00/ 1.5 74/100 (74%) 74/100 (74%) 143 0.01/ 2.5 96/100 (96%) 51/74 (69%)
1-2 348 0.00/ 1.9 61/100 (61%) 61/100 (61%) 174 0.01/ 3.1 89/100 (89%) 42/61 (69%)
1-3 884 0.00/ 3.2 50/100 (50%) 50/100 (50%) 292 0.01/ 5.3 91/100 (91%) 21/50 (42%)
2-1 125 0.00/ 1.8 45/100 (45%) 9/ 27 (33%) 27 0.02/ 2.9 23/ 27 (85%) 17/45 (38%)
2-2 113 0.00/ 2.9 40/100 (40%) 10/ 26 (38%) 26 0.06/ 4.0 22/ 26 (85%) 19/40 (48%)
2-3 6529 0.00/ 12.1 42/100 (42%) 42/100 (42%) 662 0.01/1527.4 76/100 (76%) 8/42 (19%)
3-1 287 0.00/ 1.5 20/100 (20%) 4/ 30 (13%) 30 0.05/ 2.4 23/ 30 (80%) 6/20 (30%)
3-2 309 0.01/ 2.1 21/100 (21%) 4/ 32 (13%) 32 0.10/ 3.5 26/ 32 (81%) 6/21 (29%)
3-3 338 0.01/ 2.2 24/100 (24%) 8/ 39 (21%) 39 0.05/ 3.6 32/ 39 (82%) 8/24 (33%)
4-1 4 0.26/ 1.5 0/ 4 (0%) 0/ 0 (?) 0 2.44/ 2.4 0/ 0 (?) 0/ 0 (?)
4-2 195 0.01/ 2.5 9/100 (9%) 1/ 6 (17%) 6 0.09/ 4.1 5/ 6 (83%) 2/ 9 (22%)
4-3 2063 0.00/ 7.5 5/100 (5%) 5/ 94 (5%) 94 0.02/ 10.5 89/ 94 (95%) 2/ 5 (40%)
5-2 287 0.08/ 6.3 73/100 (73%) 73/100 (73%) 116 0.05/ 14.7 97/100 (97%) 37/73 (51%)
5-3 602 0.01/ 15.9 50/100 (50%) 50/100 (50%) 122 0.05/ 14.2 96/100 (96%) 23/50 (46%)
6-1 10698 0.00/ 42.8 14/100 (14%) 14/100 (14%) 1559 0.01/3014.5 65/100 (65%) 10/14 (71%)
6-3 42106 0.00/3379.5 11/100 (11%) 11/100 (11%) 2776 0.01/5100.1 61/100 (61%) 5/11 (45%)
7 87 0.04/ 2.7 34/ 87 (39%) 7/ 15 (47%) 15 0.05/ 4.2 10/ 15 (67%) 10/34 (29%)
8 1812 0.01/ 7.6 19/100 (19%) 17/ 84 (20%) 84 0.20/ 29.2 73/ 84 (87%) 7/19 (37%)
Table 8: Number of retrieved sentences, retrieval time, and accuracy
tence was retrieved by keyword search or semantic
search. Without considering which words actually
matched the query, a sentence is judged to be cor-
rect when any part of the sentence expresses all of
the relations described by the query. The modality
of sentences was not distinguished, except in the
case of Query 8. These evaluation criteria may be
disadvantageous for the semantic search because
its ability to exactly recognize the participants of
relational concepts is not evaluated. Table 8 shows
the precision attained by keyword/semantic search
and n-precision, which denotes the precision of
the keyword search, in which the same number,
n, of outputs is taken as the semantic search. The
table also gives the relative recall of the semantic
search, which represents the ratio of sentences that
are correctly output by the semantic search among
those correctly output by the keyword search. This
does not necessarily represent the true recall be-
cause sentences not output by keyword search are
excluded. However, this is sufficient for the com-
parison of keyword search and semantic search.
The results show that the semantic search exhib-
ited impressive improvements in precision. The
precision was over 80% for most queries and was
nearly 100% for Queries 4 and 5. This indicates
that predicate argument structures are effective for
representing relational concepts precisely, espe-
cially for relations in which two entities are in-
volved. Relative recall was approximately 30?
50%, except for Query 2. In the following, we
will investigate the reasons for the residual errors.
Table 9 shows the classifications of the errors of
Disregarding of noun phrase structures 45
Term recognition errors 33
Parsing errors 11
Other reasons 8
Incorrect human judgment 7
Nominal expressions 41
Phrasal verb expressions 26
Inference required 24
Coreference resolution required 19
Parsing errors 16
Other reasons 15
Incorrect human judgment 10
Table 9: Error analysis (upper: 104 false positives,
lower: 151 false negatives)
semantic retrieval. The major reason for false pos-
itives was that our queries ignore internal struc-
tures of noun phrases. The system therefore re-
trieved noun phrases that do not directly mention
target entities. For example, ?the increased mor-
tality in patients with diabetes was caused by . . . ?
does not indicate the trigger of diabetes. Another
reason was term recognition errors. For exam-
ple, the system falsely retrieved sentences con-
taining ?p40,? which is sometimes, but not nec-
essarily used as a synonym for ?ERK2.? Ma-
chine learning-based term disambiguation will al-
leviate these errors. False negatives were caused
mainly by nominal expressions such as ?the in-
hibition of ERK2.? This is because the present
system does not convert user input into queries
on nominal expressions. Another major reason,
phrasal verb expressions such as ?lead to,? is also
a shortage of our current strategy of query cre-
ation. Because semantic annotations already in-
1023
clude linguistic structures of these expressions, the
present system can be improved further by creat-
ing queries on such expressions.
5 Conclusion
We demonstrated a text retrieval system for MED-
LINE that exploits pre-computed semantic anno-
tations5. Experimental results revealed that the
proposed system is sufficiently efficient for real-
time text retrieval and that the precision of re-
trieval was remarkably high. Analysis of resid-
ual errors showed that the handling of noun phrase
structures and the improvement of term recogni-
tion will increase retrieval accuracy. Although
the present paper focused on MEDLINE, the NLP
tools used in this system are domain/task indepen-
dent. This framework will thus be applicable to
other domains such as patent documents.
The present framework does not conflict with
conventional IR/IE techniques, and integration
with these techniques is expected to improve the
accuracy and usability of the proposed system. For
example, query expansion and relevancy feedback
can be integrated in a straightforward way in order
to improve accuracy. Document ranking is useful
for the readability of retrieved results. IE systems
can be applied off-line, in the manner of the deep
parser in our system, for annotating sentences with
target information of IE. Such annotations will en-
able us to retrieve higher-level concepts, such as
relationships among relational concepts.
Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Systems
Genomics? (MEXT, Japan), Genome Network
Project (NIG, Japan), and Solution-Oriented Re-
search for Science and Technology (JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
S. Boag, D. Chamberlin, M. F. Ferna?ndez, D. Florescu,
J. Robie, and J. Sime?on. 2005. XQuery 1.0: An
XML query language.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proc. ACL 2005.
5A web-based demo of our system is available on-line at:
http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
J. Clark and S. DeRose. 1999. XML Path Language
(XPath) version 1.0.
C. L. A. Clarke, G. V. Cormack, and F. J. Burkowski.
1995. An algebra for structured text search and a
framework for its implementation. The Computer
Journal, 38(1):43?56.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
T. Hara, Y. Miyao, and J. Tsujii. 2005. Adapting
a probabilistic disambiguation model of an HPSG
parser to a new domain. In Proc. IJCNLP 2005.
IBM, 2005. Unstructed Information Management Ar-
chitecture (UIMA) SDK User?s Guide and Refer-
ence.
A. Koike and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. In Proc.
Biolink 2004, pages 9?16.
D. A. Lindberg, B. L. Humphreys, and A. T. Mc-
Cray. 1993. The Unified Medical Language Sys-
tem. Methods in Inf. Med., 32(4):281?291.
K. Masuda, T. Ninomiya, Y. Miyao, T. Ohta, and
J. Tsujii. 2006. Nested region algebra extended with
variables. In Preparation.
Y. Miyao and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proc. 43rd ACL, pages 83?90.
National Library of Medicine. 2005. Fact Sheet MED-
LINE. Available at http://www.nlm.nih.
gov/pubs/factsheets/medline.html.
T. Ninomiya, Y. Tsuruoka, Y. Miyao, K. Taura, and
J. Tsujii. 2006. Fast and scalable HPSG parsing.
Traitement automatique des langues (TAL), 46(2).
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax annotation for the GENIA corpus. In Proc.
IJCNLP 2005, Companion volume, pages 222?227.
K. Taura. 2004. GXP : An interactive shell for the grid
environment. In Proc. IWIA2004, pages 59?67.
TEI Consortium, 2004. Text Encoding Initiative.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional infer-
ence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP 2005, pages
467?474.
1024
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 707?714,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Translating HPSG-style Outputs of a Robust Parser
into Typed Dynamic Logic
Manabu Sato? Daisuke Bekki? Yusuke Miyao? Jun?ichi Tsujii??
? Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
? Center for Evolutionary Cognitive Sciences, University of Tokyo
Komaba 3-8-1, Meguro-ku, Tokyo 153-8902, Japan
?School of Informatics, University of Manchester
PO Box 88, Sackville St, Manchester M60 1QD, UK
?SORST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
? {sa-ma, yusuke, tsujii}@is.s.u-tokyo.ac.jp
? bekki@ecs.c.u-tokyo.ac.jp
Abstract
The present paper proposes a method
by which to translate outputs of a ro-
bust HPSG parser into semantic rep-
resentations of Typed Dynamic Logic
(TDL), a dynamic plural semantics de-
fined in typed lambda calculus. With
its higher-order representations of con-
texts, TDL analyzes and describes
the inherently inter-sentential nature of
quantification and anaphora in a strictly
lexicalized and compositional manner.
The present study shows that the pro-
posed translation method successfully
combines robustness and descriptive ad-
equacy of contemporary semantics. The
present implementation achieves high
coverage, approximately 90%, for the
real text of the Penn Treebank corpus.
1 Introduction
Robust parsing technology is one result of the
recent fusion between symbolic and statistical
approaches in natural language processing and
has been applied to tasks such as information
extraction, information retrieval and machine
translation (Hockenmaier and Steedman, 2002;
Miyao et al, 2005). However, reflecting the
field boundary and unestablished interfaces be-
tween syntax and semantics in formal theory
of grammar, this fusion has achieved less in
semantics than in syntax.
For example, a system that translates the
output of a robust CCG parser into seman-
tic representations has been developed (Bos et
al., 2004). While its corpus-oriented parser at-
tained high coverage with respect to real text,
the expressive power of the resulting semantic
representations is confined to first-order predi-
cate logic.
The more elaborate tasks tied to discourse
information and plurality, such as resolution
of anaphora antecedent, scope ambiguity, pre-
supposition, topic and focus, are required to
refer to ?deeper? semantic structures, such as
dynamic semantics (Groenendijk and Stokhof,
1991).
However, most dynamic semantic theories
are not equipped with large-scale syntax that
covers more than a small fragment of target
languages. One of a few exceptions is Min-
imal Recursion Semantics (MRS) (Copestake
et al, 1999), which is compatible with large-
scale HPSG syntax (Pollard and Sag, 1994)
and has affinities with UDRS (Reyle, 1993).
For real text, however, its implementation, as
in the case of the ERG parser (Copestake
and Flickinger, 2000), restricts its target to the
static fragment of MRS and yet has a lower
coverage than corpus-oriented parsers (Baldwin,
to appear).
The lack of transparency between syntax and
discourse semantics appears to have created a
tension between the robustness of syntax and
the descriptive adequacy of semantics.
In the present paper, we will introduce
a robust method to obtain dynamic seman-
tic representations based on Typed Dynamic
Logic (TDL) (Bekki, 2000) from real text
by translating the outputs of a robust HPSG
parser (Miyao et al, 2005). Typed Dy-
namic Logic is a dynamic plural seman-
tics that formalizes the structure underlying
the semantic interactions between quantifica-
tion, plurality, bound variable/E-type anaphora
707
re?????e7?t xi1 ? ? ?xin ? ?G(i7?e)7?t .? gi7?e.g ? G? r
?
gx1, . . . ,gxm
?
? ? prop ? ?G(i7?e)7?t .? gi7?e.g ? G???hi7?e.h ? ?G?
?
? prop
...? prop
?
? ? ?G(i7?e)7?t .(? ? ? ?(?G))
re f
?
xi
?
[? prop] [? prop] ? ?G(i7?e)7?t .
?
?
?
i f G
?
x = ?G
?
x
then ? gi 7?e.g ? ?G? G?x = ?G
?
x
otherwise unde f ined
?
?
?
?
??
where prop ? ((i 7? e) 7? t) 7? (i 7? e) 7? t
g? ? G? 7?t ? Gg
G(i7?e) 7?t
.
xi ? ? de.?gi7?e.g ? G?gx = d
?
??
Figure 1: Propositions of TDL (Bekki, 2005)
and presuppositions. All of this complex
discourse/plurality-related information is encap-
sulated within higher-order structures in TDL,
and the analysis remains strictly lexical and
compositional, which makes its interface with
syntax transparent and straightforward. This is
a significant advantage for achieving robustness
in natural language processing.
2 Background
2.1 Typed Dynamic Logic
Figure 1 shows a number of propositions de-
fined in (Bekki, 2005), including atomic pred-
icate, negation, conjunction, and anaphoric ex-
pression. Typed Dynamic Logic is described in
typed lambda calculus (G?del?s System T) with
four ground types: e(entity), i(index), n(natural
number), and t(truth). While assignment func-
tions in static logic are functions in meta-
language from type e variables (in the case of
first-order logic) to objects in the domain De,
assignment functions in TDL are functions in
object-language from indices to entities. Typed
Dynamic Logic defines the notion context as
a set of assignment functions (an object of
type (i 7? e) 7? t) and a proposition as a func-
tion from context to context (an object of type
((i 7? e) 7? t) 7? (i 7? e) 7? t). The conjunctions
of two propositions are then defined as com-
posite functions thereof. This setting conforms
to the view of ?propositions as information
flow?, which is widely accepted in dynamic
semantics.
Since all of these higher-order notions are
described in lambda terms, the path for compo-
sitional type-theoretic semantics based on func-
tional application, functional composition and
type raising is clarified. The derivations of
TDL semantic representations for the sentences
?A boy ran. He tumbled.? are exemplified in
Figure 2 and Figure 3. With some instantia-
tion of variables, the semantic representations
of these two sentences are simply conjoined
and yield a single representation, as shown in
(1).
?
????
boy0x1s1
run0e1s1
agent 0e1x1
re f (x2) [ ]
?
tumble0e2s2
agent 0e2x2
?
?
????(1)
The propositions boy0x1s1, run
0e1s1 and
agent 0e1x1 roughly mean ?the entity referred
to by x1 is a boy in the situation s1?, ?the
event referred to by e1 is a running event in
the situation s1?, and ?the agent of event e1
is x1?, respectively.
The former part of (1) that corresponds to
the first sentence, filtering and testing the input
context, returns the updated context schema-
tized in (2). The updated context is then
passed to the latter part, which corresponds to
the second sentence as its input.
? ? ? x1 s1 e1 ? ? ?
john situation1 running1
john situation2 running2
...
...
...
(2)
This mechanism makes anaphoric expressions,
such as ?He? in ?He tumbles?, accessible to its
preceding context; namely, the descriptions of
their presuppositions can refer to the preceding
context compositionally. Moreover, the refer-
ents of the anaphoric expressions are correctly
calculated as a result of previous filtering and
testing.
708
?a?
? ni7?i7?p7?p.?wi 7?i7?i7?p7?p.
? ei.? si.? ? p.nx1s
?
wx1es?
?
?boy?
? xi.? si.? ? p.
?
boy0xs?
?
?wi7?i7?i7?p7?p.? ei.? si.? ? p.
?
boy0x1s
wx1es?
?
?ran?
? sb j(i7?i 7?i7?p7?p)7?i 7?i7?p7?p.
sb j
?
? xi.? ei.? si.? ? p.
"
run0es
agent 0ex?
#!
? ei.? si.? ? p.
?
??
boy0x1s1
run0es
agent 0ex1?
?
??
Figure 2: Derivation of a TDL semantic representation of ?A boy ran?.
?he?
?wi7?i7?i7?p7?p.
? ei.? si.? ? p.re f ?x2
?
[ ]
?
wx2es?
?
?tumbled?
? sb j(i7?i7?i7?p7?p)7?i7?i7?p7?p.
sb j
?
? xi.? ei.? si.? ? p.
"
tumble0es
agent 0ex?
#!
? ei.? si.? ? p.re f ?x2
?
[ ]
?
tumble0e2s2
agent 0e2x2
?
Figure 3: Derivation of TDL semantic representation of ?He tumbled?.
Although the antecedent for x2 is not de-
termined in this structure, the possible candi-
dates can be enumerated: x1, s1 and e1, which
precede x2. Since TDL seamlessly represents
linguistic notions such as ?entity?, ?event? and
?situation?, by indices, the anaphoric expres-
sions, such as ?the event? and ?that case?, can
be treated in the same manner.
2.2 Head-driven Phrase Structure
Grammar
Head-driven Phrase Structure Grammar (Pollard
and Sag, 1994) is a kind of lexicalized gram-
mar that consists of lexical items and a small
number of composition rules called schema.
Schemata and lexical items are all described
in typed feature structures and the unification
operation defined thereon.
?
?????
PHON ?boy?
SYN
SEM
?
??????
HEAD
?
noun
MOD h i
?
VAL
"
SUBJ h i
COMPS h i
SPR hdeti
#
SLASH h i
?
??????
?
?????
(3)
Figure 4 is an example of a parse tree,
where the feature structures marked with the
same boxed numbers have a shared struc-
ture. In the first stage of the derivation of
this tree, lexical items are assigned to each
of the strings, ?John? and ?runs.? Next, the
mother node, which dominates the two items,
?
??
PHON ?John runs?
HEAD 1
SUBJ h i
COMPS h i
?
??
?
??
PHON ?John?
HEAD noun
SUBJ h i
COMPS h i
?
?? : 2
?
???
PHON ?runs?
HEAD verb : 1
SUBJ h 2 i
COMPS h i
?
???
John runs
Figure 4: An HPSG parse tree
is generated by the application of Subject-Head
Schema. The recursive application of these op-
erations derives the entire tree.
3 Method
In this section, we present a method to de-
rive TDL semantic representations from HPSG
parse trees, adopting, in part, a previous
method (Bos et al, 2004). Basically, we first
assign TDL representations to lexical items that
are terminal nodes of a parse tree, and then
compose the TDL representation for the en-
tire tree according to the tree structure (Figure
5). One problematic aspect of this approach is
that the composition process of TDL semantic
representations and that of HPSG parse trees
are not identical. For example, in the HPSG
709
?
?
PHON ?John runs?
HEAD 1
SUBJ h i
COMPS h i
?
?
Subject-Head Schema
* ? e.? s.? ? .
re f (x1) [John0x1s1]
"
run0es
agent 0ex1?
#
?run
_empty_
+
Composition Rules
normal composition
word formation
nonlocal application
unary derivation
?
?
PHON ?John?
HEAD noun
SUBJ h i
COMPS h i
?
? : 2
?
??
PHON ?runs?
HEAD verb : 1
SUBJ h 2 i
COMPS h i
?
??
Assignment Rules
? ?w.? e.? s.? ? .
re f (x1) [John0x1s1] [wx1es? ]
?John
_empty_
?* ? sb j.sb j?
? x.? e.? s.? ? .
"
run0es
agent 0ex?
#!
?run
_empty_
+
John runs John runs
Figure 5: Example of the application of the rules
parser, a compound noun is regarded as two
distinct words, whereas in TDL, a compound
noun is regarded as one word. Long-distance
dependency is also treated differently in the
two systems. Furthermore, TDL has an opera-
tion called unary derivation to deal with empty
categories, whereas the HPSG parser does not
have such an operation.
In order to overcome these differences and
realize a straightforward composition of TDL
representations according to the HPSG parse
tree, we defined two extended composition
rules, word formation rule and non-local
application rule, and redefined TDL unary
derivation rules for the use in the HPSG
parser. At each step of the composition, one
composition rule is chosen from the set of
rules, based on the information of the schemata
applied to the HPSG tree and TDL represen-
tations of the constituents. In addition, we de-
fined extended TDL semantic representations,
referred to as TDL Extended Structures (TD-
LESs), to be paired with the extended compo-
sition rules.
In summary, the proposed method is com-
prised of TDLESs, assignment rules, composi-
tion rules, and unary derivation rules, as will
be elucidated in subsequent sections.
3.1 Data Structure
A TDLES is a tuple hT, p,ni, where T is an
extended TDL term, which can be either a
TDL term or a special value ? . Here, ?
is a value used by the word formation rule,
which indicates that the word is a word modi-
fier (See Section 3.3). In addition, p and n are
the necessary information for extended compo-
sition rules, where p is a matrix predicate in T
and is used by the word formation rule, and
n is a nonlocal argument, which takes either
a variable occurring in T or an empty value.
This element corresponds to the SLASH fea-
ture in HPSG and is used by the nonlocal
application rule.
The TDLES of the common noun ?boy? is
given in (4). The contents of the structure
are T , p and n, beginning at the top. In
(4), T corresponds to the TDL term of ?boy?
in Figure 2, p is the predicate boy, which is
identical to a predicate in the TDL term (the
identity relation between the two is indicated
by ???). If either T or p is changed, the other
will be changed accordingly. This mechanism
is a part of the word formation rule, which
offers advantages in creating a new predicate
from multiple words. Finally, n is an empty
value.
* ? x.? s.? ? .
?
?boy0xs?
?
?boy
_empty_
+
(4)
3.2 Assignment Rules
We define assignment rules to associate HPSG
lexical items with corresponding TDLESs. For
closed class words, such as ?a?, ?the? or
?not?, assignment rules are given in the form
of a template for each word as exemplified
below.
"
PHON ?a?
HEAD det
SPEC hnouni
#
?
* ? x.? s.? ? .
? ? n.?w.? e.? s.? ? .
nx1s
?
wx1es?
?
?
_empty_
_empty_
+(5)
710
Shown in (5) is an assignment rule for the
indefinite determiner ?a?. The upper half of
(5) shows a template of an HPSG lexical item
that specifies its phonetic form as ?a?, where
POS is a determiner and specifies a noun. A
TDLES is shown in the lower half of the fig-
ure. The TDL term slot of this structure is
identical to that of ?a? in Figure 2, while slots
for the matrix predicate and nonlocal argument
are empty.
For open class words, such as nouns, verbs,
adjectives, adverbs and others, assignment rules
are defined for each syntactic category.
?
?????
PHON P
HEAD noun
MOD hi
SUBJ hi
COMPS hi
SPR hdeti
?
?????
?
* ? x.? s.? ? .
?
?P0xs?
?
?P
_empty_
+
(6)
The assignment rule (6) is for common nouns.
The HPSG lexical item in the upper half of (6)
specifies that the phonetic form of this item is
a variable, P, that takes no arguments, does
not modify other words and takes a specifier.
Here, POS is a noun. In the TDLES assigned
to this item, an actual input word will be sub-
stituted for the variable P, from which the ma-
trix predicate P0 is produced. Note that we can
obtain the TDLES (4) by applying the rule of
(6) to the HPSG lexical item of (3).
As for verbs, a base TDL semantic represen-
tation is first assigned to a verb root, and the
representation is then modified by lexical rules
to reflect an inflected form of the verb. This
process corresponds to HPSG lexical rules for
verbs. Details are not presented herein due to
space limitations.
3.3 Composition Rules
We define three composition rules: the func-
tion application rule, the word formation
rule, and the nonlocal application rule.
Hereinafter, let SL = hTL, pL,nLi and SR =
hTR, pR,nRi be TDLESs of the left and the
right daughter nodes, respectively. In addition,
let SM be TDLESs of the mother node.
Function application rule: The composition
of TDL terms in the TDLESs is performed by
function application, in the same manner as in
the original TDL, as explained in Section 2.1.
Definition 3.1 (function application rule). If
Type
?
TL
?
= ? and Type?TR
?
= ? 7? ? then
SM =
* TRTL
pR
union
?
nL,nR
?
+
Else if Type
?
TL
?
= ? 7? ? and Type?TR
?
= ? then
SM =
* TLTR
pL
union
?
nL,nR
?
+
In Definition 3.1, Type(T ) is a function
that returns the type of TDL term T , and
union(nL,nR) is defined as:
union
?
nL,nR
?
=?
??
??
empty i f nL = nR = _empty_
n i f nL = n, nR = _empty_
n i f nL = _empty_, nR = n
unde f ined i f nL 6= _empty_, nR 6= _empty_
This function corresponds to the behavior of
the union of SLASH in HPSG. The composi-
tion in the right-hand side of Figure 5 is an
example of the application of this rule.
Word formation rule: In natural language,
it is often the case that a new word is cre-
ated by combining multiple words, for exam-
ple, ?orange juice?. This phenomenon is called
word formation. Typed Dynamic Logic and
the HPSG parser handle this phenomenon in
different ways. Typed Dynamic Logic does
not have any rule for word formation and re-
gards ?orange juice? as a single word, whereas
most parsers treat ?orange juice? as the sepa-
rate words ?orange? and ?juice?. This requires
a special composition rule for word formation
to be defined. Among the constituent words of
a compound word, we consider those that are
not HPSG heads as word modifiers and define
their value for T as ? . In addition, we apply
the word formation rule defined below.
Definition 3.2 (word formation rule). If
Type
?
TL
?
= ? then
SM =
* TR
concat
?
pL, pR
?
nR
+
Else if Type
?
TR
?
= ? then
SM =
* TL
concat
?
pL, pR
?
nL
+
711
concat (pL, pR) in Definition 3.2 is a func-
tion that returns a concatenation of pL and pR.
For example, the composition of a word mod-
ifier ?orange? (7) and and a common noun
?juice? (8) will generate the TDLES (9).
? ?
orange
_empty_
?
(7)
* ? x.? s.? ? .
?
? juice0xs?
?
? juice
_empty_
+
(8)
* ? x.? s.? ? .
?
?orange_ juice0xs?
?
?orange_ juice
_empty_
+
(9)
Nonlocal application rule: Typed Dynamic
Logic and HPSG also handle the phenomenon
of wh-movement differently. In HPSG, a wh-
phrase is treated as a value of SLASH, and
the value is kept until the Filler-Head Schema
are applied. In TDL, however, wh-movement
is handled by the functional composition rule.
In order to resolve the difference between
these two approaches, we define the nonlocal
application rule, a special rule that introduces
a slot relating to HPSG SLASH to TDLESs.
This slot becomes the third element of TD-
LESs. This rule is applied when the Filler-
Head Schema are applied in HPSG parse trees.
Definition 3.3 (nonlocal application rule).
If Type
?
TL
?
= (? 7? ? ) 7? ? , Type?TR
?
= ? ,
Type
?
nR
?
= ? and the Filler-Head Schema are applied
in HPSG, then
SM =
*
TL
?? nR.TR
?
pL
_empty_
+
3.4 Unary Derivation Rules
In TDL, type-shifting of a word or a phrase is
performed by composition with an empty cat-
egory (a category that has no phonetic form,
but has syntactic/semantic functions). For ex-
ample, the phrase ?this year? is a noun phrase
at the first stage and can be changed into a
verb modifier when combined with an empty
category. Since many of the type-shifting rules
are not available in HPSG, we defined unary
derivation rules in order to provide an equiva-
lent function to the type-shifting rules of TDL.
These unary rules are applied independently
with HPSG parse trees. (10) and (11) illus-
trate the unary derivation of ?this year?. (11)
Table 1: Number of implemented rules
assignment rules
HPSG-TDL template 51
for closed words 16
for open words 35
verb lexical rules 27
composition rules
binary composition rules 3
function application rule
word formation rule
nonlocal application rule
unary derivation rules 12
is derived from (10) using a unary derivation
rule.
? ?w.? e.? s.? ? .re f ?x1
??
?year0x1s1
??
wx1es?
?
?year
_empty_
?(10)
* ? v.? e.? s.? ? .
re f
?
x1
??
?year0x1s1
??
ves
?
mod 0ex1?
??
?year
_empty_
+
(11)
4 Experiment
The number of rules we have implemented is
shown in Table 1. We used the Penn Treebank
(Marcus, 1994) Section 22 (1,527 sentences) to
develop and evaluate the proposed method and
Section 23 (2,144 sentences) as the final test
set.
We measured the coverage of the construc-
tion of TDL semantic representations, in the
manner described in a previous study (Bos
et al, 2004). Although the best method for
strictly evaluating the proposed method is to
measure the agreement between the obtained
semantic representations and the intuitions of
the speaker/writer of the texts, this type of
evaluation could not be performed because of
insufficient resources. Instead, we measured
the rate of successful derivations as an indica-
tor of the coverage of the proposed system.
The sentences in the test set were parsed by
a robust HPSG parser (Miyao et al, 2005),
and HPSG parse trees were successfully gen-
erated for 2,122 (98.9%) sentences. The pro-
posed method was then applied to these parse
trees. Table 2 shows that 88.3% of the un-
712
Table 2: Coverage with respect to the test set
covered sentences 88.3 %
uncovered sentences 11.7 %
assignment failures 6.2 %
composition failures 5.5 %
word coverage 99.6 %
Table 3: Error analysis: the development set
# assignment failures 103
# unimplemented words 61
# TDL unsupporting words 17
# nonlinguistic HPSG lexical items 25
# composition failures 72
# unsupported compositions 20
# invalid assignments 36
# nonlinguistic parse trees 16
seen sentences are assigned TDL semantic rep-
resentations. Although this number is slightly
less than 92.3%, as reported by Bos et al,
(2004), it seems reasonable to say that the pro-
posed method attained a relatively high cover-
age, given the expressive power of TDL.
The construction of TDL semantic represen-
tations failed for 11.7% of the sentences. We
classified the causes of the failure into two
types. One of which is application failure of
the assignment rules (assignment failure); that
is, no assignment rules are applied to a num-
ber of HPSG lexical items, and so no TD-
LESs are assigned to these items. The other
is application failure of the composition rules
(composition failure). In this case, a type mis-
match occurred in the composition, and so a
TDLES was not derived.
Table 3 shows further classification of the
causes categorized into the two classes. We
manually investigated all of the failures in the
development set.
Assignment failures are caused by three fac-
tors. Most assignment failures occurred due to
the limitation in the number of the assignment
rules (as indicated by ?unimplemented words?
in the table). In this experiment, we did not
implement rules for infrequent HPSG lexical
items. We believe that this type of failure
will be resolved by increasing the number of
ref($1)[]
[lecture($2,$3) &
past($3) &
agent($2,$1) &
content($2,$4) &
ref($5)[]
[every($6)[ball($6,$4)]
[see($7,$4) &
present($4) &
agent($7,$5) &
theme($7,$6) &
tremendously($7,$4) &
ref($8)[]
[ref($9)[groove($9,$10)]
[be($11,$4) &
present($4) &
agent($11,$8) &
in($11,$9) &
when($11,$7)]]]]]
Figure 6: Output for the sentence: ?When
you?re in the groove, you see every ball
tremendously,? he lectured.
assignment rules. The second factor in the
table, ?TDL unsupported words?, refers to ex-
pressions that are not covered by the current
theory of TDL. In order to resolve this type of
failure, the development of TDL is required.
The third factor, ?nonlinguistic HPSG lexical
items? includes a small number of cases in
which TDLESs are not assigned to the words
that are categorized as nonlinguistic syntactic
categories by the HPSG parser. This problem
is caused by ill-formed outputs of the parser.
The composition failures can be further clas-
sified into three classes according to their
causative factors. The first factor is the ex-
istence of HPSG schemata for which we have
not yet implemented composition rules. These
failures will be fixed by extending of the def-
inition of our composition rules. The sec-
ond factor is type mismatches due to the un-
intended assignments of TDLESs to lexical
items. We need to further elaborate the as-
signment rules in order to deal with this prob-
lem. The third factor is parse trees that are
linguistically invalid.
The error analysis given above indicates that
we can further increase the coverage through
the improvement of the assignment/composition
rules.
Figure 6 shows an example of the output
for a sentence in the development set. The
variables $1, . . . ,$11 are indices that
713
represent entities, events and situations. For
example, $3 represents a situation and $2
represents the lecturing event that exists
in $3. past($3) requires that the sit-
uation is past. agent($2,$1) requires
that the entity $1 is the agent of $2.
content($2,$4) requires that $4 (as a
set of possible worlds) is the content of
$2. be($11,$4) refers to $4. Finally,
every($6)[ball($6,$4)][see($7,$4)
...] represents a generalized quantifier
?every ball?. The index $6 serves as an
antecedent both for bound-variable anaphora
within its scope and for E-type anaphora out-
side its scope. The entities that correspond to
the two occurrences of ?you? are represented
by $8 and $5. Their unification is left as
an anaphora resolution task that can be easily
solved by existing statistical or rule-based
methods, given the structural information of
the TDL semantic representation.
5 Conclusion
The present paper proposed a method by which
to translate HPSG-style outputs of a robust
parser (Miyao et al, 2005) into dynamic se-
mantic representations of TDL (Bekki, 2000).
We showed that our implementation achieved
high coverage, approximately 90%, for real
text of the Penn Treebank corpus and that the
resulting representations have sufficient expres-
sive power of contemporary semantic theory
involving quantification, plurality, inter/intra-
sentential anaphora and presupposition.
In the present study, we investigated the
possibility of achieving robustness and descrip-
tive adequacy of semantics. Although previ-
ously thought to have a trade-off relationship,
the present study proved that robustness and
descriptive adequacy of semantics are not in-
trinsically incompatible, given the transparency
between syntax and discourse semantics.
If the notion of robustness serves as a cri-
terion not only for the practical usefulness of
natural language processing but also for the
validity of linguistic theories, then the compo-
sitional transparency that penetrates all levels
of syntax, sentential semantics, and discourse
semantics, beyond the superficial difference be-
tween the laws that govern each of the levels,
might be reconsidered as an essential principle
of linguistic theories.
References
Timothy Baldwin, John Beavers, Emily M. Bender,
Dan Flickinger, Ara Kim and Stephan Oepen (to
appear) Beauty and the Beast: What running a
broad-coverage precision grammar over the BNC
taught us about the grammar ? and the cor-
pus, In Linguistic Evidence: Empirical, Theoreti-
cal, and Computational Perspectives, Mouton de
Gruyter.
Daisuke Bekki. 2000. Typed Dynamic Logic for
Compositional Grammar, Doctoral Dissertation,
University of Tokyo.
Daisuke Bekki. 2005. Typed Dynamic Logic and
Grammar: the Introduction, manuscript, Univer-
sity of Tokyo,
Johan Bos, Stephen Clark, Mark Steedman, James
R. Curran and Julia Hockenmaier. 2004. Wide-
Coverage Semantic Representations from a CCG
Parser, In Proc. COLING ?04, Geneva.
Ann Copestake, Dan Flickinger, Ivan A. Sag and
Carl Pollard. 1999. Minimal Recursion Seman-
tics: An introduction, manuscript.
Ann Copestake and Dan Flickinger. 2000.
An open-source grammar development environ-
ment and broad-coverage English grammar using
HPSG In Proc. LREC-2000, Athens.
Jeroen Groenendijk and Martin Stokhof. 1991. Dy-
namic Predicate Logic, In Linguistics and Philos-
ophy 14, pp.39-100.
Julia Hockenmaier and Mark Steedman. 2002. Ac-
quiring Compact Lexicalized Grammars from a
Cleaner Treebank, In Proc. LREC-2002, Las Pal-
mas.
Mitch Marcus. 1994. The Penn Treebank: A
revised corpus design for extracting predicate-
argument structure. In Proceedings of the ARPA
Human Language Technolog Workshop, Prince-
ton, NJ.
Yusuke Miyao, Takashi Ninomiya and Jun?ichi Tsu-
jii. 2005. Corpus-oriented Grammar Develop-
ment for Acquiring a Head-driven Phrase Struc-
ture Grammar from the Penn Treebank, in IJC-
NLP 2004, LNAI3248, pp.684-693. Springer-
Verlag.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar, Studies in Contem-
porary Linguistics. University of Chicago Press,
Chicago, London.
Uwe Reyle. 1993. Dealing with Ambiguities by
Underspecification: Construction, Representation
and Deduction, In Journal of Semantics 10,
pp.123-179.
714
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850?857,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Trimming CFG Parse Trees for Sentence Compression Using Machine
Learning Approaches
Yuya Unno1 Takashi Ninomiya2 Yusuke Miyao1 Jun?ichi Tsujii134
1Department of Computer Science, University of Tokyo
2Information Technology Center, University of Tokyo
3School of Informatics, University of Manchester
4SORST, JST
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
{unno, yusuke, tsujii}@is.s.u-tokyo.ac.jp
ninomi@r.dl.itc.u-tokyo.ac.jp
Abstract
Sentence compression is a task of creating
a short grammatical sentence by removing
extraneous words or phrases from an origi-
nal sentence while preserving its meaning.
Existing methods learn statistics on trim-
ming context-free grammar (CFG) rules.
However, these methods sometimes elim-
inate the original meaning by incorrectly
removing important parts of sentences, be-
cause trimming probabilities only depend
on parents? and daughters? non-terminals
in applied CFG rules. We apply a maxi-
mum entropy model to the above method.
Our method can easily include various
features, for example, other parts of a
parse tree or words the sentences contain.
We evaluated the method using manually
compressed sentences and human judg-
ments. We found that our method pro-
duced more grammatical and informative
compressed sentences than other methods.
1 Introduction
In most automatic summarization approaches, text
is summarized by extracting sentences from a
given document without modifying the sentences
themselves. Although these methods have been
significantly improved to extract good sentences
as summaries, they are not intended to shorten sen-
tences; i.e., the output often has redundant words
or phrases. These methods cannot be used to
make a shorter sentence from an input sentence or
for other applications such as generating headline
news (Dorr et al, 2003) or messages for the small
screens of mobile devices. We need to compress
sentences to obtain short and useful summaries.
This task is called sentence compression.
While several methods have been proposed for
sentence compression (Witbrock and Mittal, 1999;
Jing and McKeown, 1999; Vandeghinste and Pan,
2004), this paper focuses on Knight and Marcu?s
noisy-channel model (Knight and Marcu, 2000)
and presents an extension of their method. They
developed a probabilistic model for trimming a
CFG parse tree of an input sentence. Their
method drops words of input sentences but does
not change their order or change the words. They
use a parallel corpus that contains pairs of origi-
nal and compressed sentences. The method makes
CFG parse trees of both original and compressed
sentences and learns trimming probabilities from
these pairs. Although their method is concise and
well-defined, its accuracy is still unsatisfactory.
Their method has two problems. One is that prob-
abilities are calculated only from the frequencies
of applied CFG rules, and other characteristics like
whether the phrase includes negative words cannot
be introduced. The other problem is that the parse
trees of original and compressed sentences some-
times do not correspond.
To solve the former problem, we apply a maxi-
mum entropy model to Knight and Marcu?s model
to introduce machine learning features that are de-
fined not only for CFG rules but also for other
characteristics in a parse tree, such as the depth
from the root node or words it contains. To solve
the latter problem, we introduce a novel matching
method, the bottom-up method, to learn compli-
cated relations of two unmatched trees.
We evaluated each algorithm using the Ziff-
Davis corpus, which has long and short sentence
pairs. We compared our method with Knight and
Marcu?s method in terms of F -measures, bigram
F -measures, BLEU scores and human judgments.
850
2 Background
2.1 The Noisy-Channel Model for Sentence
Compression
Knight and Marcu proposed a sentence compres-
sion method using a noisy-channel model (Knight
and Marcu, 2000). This model assumes that a long
sentence was originally a short one and that the
longer sentence was generated because some un-
necessary words were added. Given a long sen-
tence l, it finds a short sentence s that maximizes
P (s|l). This is equivalent to finding the s that
maximizes P (s) ? P (l|s) in Bayes? Rule.
The expression P (s) is the source model, which
gives the probability that s is the original short
string. When s is ungrammatical, P (s) becomes
small. The expression P (l|s) is the channel
model, which gives the probability that s is ex-
panded to l. When s does not include important
words of l, P (l|s) has a low value.
In the Knight and Marcu?s model, a proba-
bilistic context-free grammar (PCFG) score and a
word-bigram score are incorporated as the source
model. To estimate the channel model, Knight
and Marcu used the Ziff-Davis parallel corpus,
which contains long sentences and corresponding
short sentences compressed by humans. Note that
each compressed sentence is a subsequence of the
corresponding original sentence. They first parse
both the original and compressed sentences using
a CFG parser to create parse trees. When two
nodes of the original and compressed trees have
the same non-terminals, and the daughter nodes of
the compressed tree are a subsequence of the orig-
inal tree, they count the node pair as a joint event.
For example, in Figure 1, the original parse tree
contains a rule rl = (B ? D E F ), and the com-pressed parse tree contains rs = (B ? D F ).They assume that rs was expanded into rl, andcount the node pairs as joint events. The expan-
sion probability of two rules is given by:
Pexpand (rl|rs) =
count(joint(rl, rs))
count(rs)
.
Finally, new subtrees grow from new daugh-
ter nodes in each expanded node. In Figure 1,
(E (G g) (H h)) grows from E. The PCFG
scores, Pcfg , of these subtrees are calculated.Then, each probability is assumed to be indepen-
dent of the others, and the channel model, P (l|s),
is calculated as the product of all expansion prob-
abilities of joint events and PCFG scores of new
A
B C
E FD
d
g h
f
c
A
B C
FD
d f
c
G H
Figure 1: Examples of original and compressed
parse trees.
subtrees:
P (l|s) =
?
(rl,rs)?R
Pexpand (rl|rs) ?
?
r?R?
Pcfg(r),
where R is the set of rule pairs, and R? is the set
of generation rules in new subtrees.
To compress an input sentence, they create a
tree with the highest score of all possible trees.
They pack all possible trees in a shared-forest
structure (Langkilde, 2000). The forest structure
is represented by an AND-OR tree, and it con-
tains many tree structures. The forest represen-
tation saves memory and makes calculation faster
because the trees share sub structures, and this can
reduce the total number of calculations.
They normalize each log probability using the
length of the compressed sentence; that is, they di-
vide the log probability by the length of the com-
pressed sentence.
Turner and Charniak (Turner and Charniak,
2005) added some special rules and applied this
method to unsupervised learning to overcome the
lack of training data. However their model also
has the same problem. McDonald (McDonald,
2006) independently proposed a new machine
learning approach. He does not trim input parse
trees but uses rich features about syntactic trees
and improved performance.
2.2 Maximum Entropy Model
The maximum entropy model (Berger et al, 1996)
estimates a probability distribution from training
data. The model creates the most ?uniform? distri-
bution within the constraints given by users. The
distribution with the maximum entropy is consid-
ered the most uniform.
Given two finite sets of event variables, X and
Y , we estimate their joint probability distribution,
P (x, y). An output, y (? Y), is produced, and
851
contextual information, x (? X ), is observed. To
represent whether the event (x, y) satisfies a cer-
tain feature, we introduce a feature function. A
feature function fi returns 1 iff the event (x, y) sat-isfies the feature i and returns 0 otherwise.
Given training data {(x1, y1), ? ? ? , (xn, yn)},we assume that the expectation of fi on the dis-tribution of the model conforms to that on the em-
pirical probability distribution P? (x, y). We select
the probability distribution that satisfies these con-
straints of all feature functions and maximizes its
entropy, H(P ) = ??x,y P (x, y) log (P (x, y)).
3 Methods
3.1 Maximum Entropy Model for Sentence
Compression
We describe a maximum entropy method as a
natural extension of Knight and Marcu?s noisy-
channel model (Knight and Marcu, 2000). Knight
and Marcu?s method uses only mother and daugh-
ter local relations in CFG parse trees. Therefore,
it sometimes eliminates the meanings of the origi-
nal sentences. For example, their method cannot
distinguish ?never? and ?always? because these
two adverbs are assigned the same non-terminals
in parse trees. However, if ?never? is removed
from a sentence, the meaning of the sentence com-
pletely changes. Turner and Charniak (Turner and
Charniak, 2005) revised and improved Knight and
Marcu?s algorithm; however, their algorithm also
uses only mother and daughter relations and has
the same problem. We use other information as
feature functions of the maximum entropy model,
and this model can deal with many features more
appropriately than using simple frequency.
Suppose that we trim a node in the original full
parse tree. For example, suppose we have a mother
node A and daughter nodes (B C D) that are de-
rived using a CFG rule. We must leave at least one
non-terminal in the daughter nodes. The trim can-
didates of this rule are the members of the set of
subsequences, Y , of (B C D), or the seven non-
terminal sequences below:
Y = {B,C,D,BC,BD,CD,BCD}.
For each y (? Y), such as (B C), the trimming
probability, P (y|Y) = Ptrim(A ? B C|A ?
B C D), is calculated by using the maximum en-
tropy model. We assume that these joint events are
independent of each other and calculate the proba-
bility that an original sentence, l, is compressed to
Description
1 the mother node
2 the current node
3 the daughter node sequence in the original sentence
and which daughters are removed
4 the daughter node sequence in the compressed sen-
tence
5 the number of daughter nodes
6 the depth from the root
7 the daughter non-terminals that are removed
8 the daughter terminals that are removed
9 whether the daughters are ?negative adverbs?, and
removed
10 tri-gram of daughter nodes
11 only one daughter exists, and its non-terminal is the
same as that of the current node
12 only one daughter exists, and its non-terminal is the
same as that of the mother node
13 how many daughter nodes are removed
14 the number of terminals the current node contains
15 whether the head daughter is removed
16 the left-most and the right-most daughters
17 the left and the right siblings
Table 1: Features for maximum entropy model.
s as the product of all trimming probabilities, like
in Knight and Marcu?s method.
P (s|l) =
?
(rs,rl)?R
Ptrim(rs|rl),
where R is the set of compressed and original rule
pairs in joint events. Note that our model does not
use Bayes? Rule or any language models.
For example, in Figure 1, the trimming proba-
bility is calculated as below:
P (s|l) = Ptrim(A ? B C|A ? B C)
?Ptrim(B ? D F |B ? D E F ).
To represent all summary candidates, we cre-
ate a compression forest as Knight and Marcu did.
We select the tree assigned the highest probability
from the forest.
Features in the maximum entropy model are de-
fined for a tree node and its surroundings. When
we process one node, or one non-terminal x, we
call it the current node. We focus on not only x
and its daughter nodes, but its mother node, its
sibling nodes, terminals of its subtree and so on.
The features we used are listed in Table 1.
Knight and Marcu divided the log probabilities
by the length of the summary. We extend this idea
so that we can change the output length flexibly.
We introduce a length parameter, ?, and define a
score S? as S?(s) = length(s)? log P (s|l), where
l is an input sentence to be shortened, and s is a
852
summary candidate. Because log P (s|l) is nega-
tive, short sentences obtain a high score for large
?, and long ones get a low score. The parameter
? can be negative or positive, and we can use it to
control the average length of outputs.
3.2 Bottom-Up Method
As explained in Section 2.1, in Knight and
Marcu?s method, both original and compressed
sentences are parsed, and correspondences of CFG
rules are identified. However, when the daugh-
ter nodes of a compressed rule are not a subse-
quence of the daughter nodes in the original one,
the method cannot learn this joint event. A com-
plex sentence is a typical example. A complex
sentence is a sentence that includes another sen-
tence as a part. An example of a parse tree of a
complex sentence and its compressed version is
shown in Figure 2. When we extract joint events
from these two trees, we cannot match the two
root nodes because the sequence of the daughter
nodes of the root node of the compressed parse
tree, (NP ADVP VP .), is not a subsequence
of the daughter nodes of the original parse tree,
(S , NP VP .). Turner and Charniak (Turner and
Charniak, 2005) solve this problem by appending
special rules that are applied when a mother node
and its daughter node have the same label. How-
ever, there are several types of such problems like
Figure 2. We need to extract these structures from
a training corpus.
We propose a bottom-up method to solve the
problem explained above. In our method, only
original sentences are parsed, and the parse trees
of compressed sentences are extracted from the
original parse trees. An example of this method
is shown in Figure 3. The original sentence is ?d
g h f c?, and its compressed sentence is ?d g c?.
First, each terminal in the parse tree of the original
sentence is marked if it exists in the compressed
sentence. In the figure, the marked terminals are
represented by circles. Second, each non-terminal
in the original parse tree is marked if it has at least
one marked terminal in its sub-trees. These are
represented as bold boxes in the figure. If non-
terminals contain marked non-terminals in their
sub-trees, these non-terminals are also marked re-
cursively. These marked non-terminals and termi-
nals compose a tree structure like that on the right-
hand side in the figure. These non-terminals rep-
resent joint events at each node.
S
S ,
,
NP VP
I said
.
.
S
.
.
NP VPADVP
I never think soNP VPADVP
I never think so
top top
Figure 2: Example of parse tree pair that cannot
be matched.
A
B C
E FD
G H
h
f
A
B C
ED
d
g
c
d
g
c
G
Figure 3: Example of bottom-up method.
Note that this ?tree? is not guaranteed to be
a grammatical ?parse tree? by the CFG gram-
mar. For example, from the tree of Figure 2,
(S (S ? ? ? ) (, , ) (NP I) (VP said) (. .)), a new
tree, (S (S ? ? ? ) (. .)), is extracted. However, the
rule (S ? S .) is ungrammatical.
4 Experiment
4.1 Evaluation Method
We evaluated each sentence compression method
using word F -measures, bigram F -measures, and
BLEU scores (Papineni et al, 2002). BLEU scores
are usually used for evaluating machine transla-
tion quality. A BLEU score is defined as the
weighted geometric average of n-gram precisions
with length penalties. We used from unigram to
4-gram precisions and uniform weights for the
BLEU scores.
ROUGE (Lin, 2004) is a set of recall-based cri-
teria that is mainly used for evaluating summa-
rization tasks. ROUGE-N uses average N-gram re-
call, and ROUGE-1 is word recall. ROUGE-L uses
the length of the longest common subsequence
(LCS) of the original and summarized sentences.
In our model, the length of the LCS is equal to
the number of common words, and ROUGE-L is
equal to the unigram F -measure because words
are not rearranged. ROUGE-L and ROUGE-1 are
supposed to be appropriate for the headline gener-
853
ation task (Lin, 2004). This is not our task, but it
is the most similar task in his paper.
We also evaluated the methods using human
judgments. The evaluator is not the author but not
a native English speaker. The judgment used the
same criteria as those in Knight and Marcu?s meth-
ods. We performed two experiments. In the first
experiment, evaluators scored from 1 to 5 points
the grammaticality of the compressed sentence. In
the second one, they scored from 1 to 5 points
how well the compressed sentence contained the
important words of the original one.
We used the parallel corpus used in Ref. (Knight
and Marcu, 2000). This corpus consists of sen-
tence pairs extracted automatically from the Ziff-
Davis corpus, a set of newspaper articles about
computer products. This corpus has 1087 sentence
pairs. Thirty-two of these sentences were used for
the human judgments in Knight and Marcu?s ex-
periment, and the same sentences were used for
our human judgments. The rest of the sentences
were randomly shuffled, and 527 sentence pairs
were used as a training corpus, 263 pairs as a de-
velopment corpus, and 264 pairs as a test corpus.
To parse these corpora, we used Charniak and
Johnson?s parser (Charniak and Johnson, 2005).
4.2 Settings of Two Experiments
We experimented with/without goal sentence
length for summaries.
In the first experiment, the system was given
only a sentence and no sentence length informa-
tion. The sentence compression problem without
the length information is a general task, but evalu-
ating it is difficult because the correct length of a
summary is not generally defined even by humans.
The following example shows this.
Original:?A font, on the other hand, is a subcate-
gory of a typeface, such as Helvetica Bold or Hel-
vetica Medium.?
Human: ?A font is a subcategory of a typeface,
such as Helvetica Bold.?
System: ?A font is a subcategory of a typeface.?
The ?such as? phrase is removed in this sys-
tem output, but it is not removed in the human
summary. Neither result is wrong, but in such
situations, the evaluation score of the system de-
creases. This is because the compression rate of
each algorithm is different, and evaluation scores
are affected by the lengths of system outputs. For
this reason, results with different lengths cannot be
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
F-
m
ea
su
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 4: F -measures and compression ratios.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
B
ig
ra
m
 F
-m
ea
su
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 5: Bigram F -measures and compression
ratios.
compared easily. We therefore examined the rela-
tions between the average compression ratios and
evaluation scores for all methods by changing the
system summary length with the different length
parameter ? introduced in Section 3.1.
In the second experiment, the system was given
a sentence and the length for the compressed sen-
tence. We compressed each input sentence to the
length of the sentence in its goal summary. This
sentence compression problem is easier than that
in which the system can generate sentences of any
length. We selected the highest-scored sentence
from the sentences of length l. Note that the re-
calls, precisions and F-measures have the same
scores in this setting.
4.3 Results of Experiments
The results of the experiment without the sen-
tence length information are shown in Figure 4,
5 and 6. Noisy-channel indicates the results of the
noisy-channel model, ME indicates the results of
the maximum-entropy method, and ME + bottom-
up indicates the results of the maximum-entropy
854
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 s
co
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 6: BLEU scores and compression ratios.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
Noisy-channel ME ME+bottom-up
F-measure
bigram-F-measure
BLEU
Figure 7: Results of experiments with length in-
formation.
method with the bottom-up method. We used the
length parameter, ?, introduced in Section 3.1, and
obtained a set of summaries with different aver-
age lengths. We plotted the compression ratios
and three scores in the figures. In these figures,
a compression ratio is the ratio of the total num-
ber of words in compressed sentences to the total
number of words in the original sentences.
In these figures, our maximum entropy meth-
ods obtained higher scores than the noisy-channel
model at all compression ratios. The maximum
entropy method with the bottom-up method obtain
the highest scores on these three measures.
The results of the experiment with the sentence
length information are shown in Figure 7. In this
experiment, the scores of the maximum entropy
methods were higher than the scores of the noisy-
channel model. The maximum entropy method
with the bottom-up method achieved the highest
scores on each measure.
The results of the human judgments are shown
in Table 2. In this experiment, each length of out-
put is same as the length of goal sentence. The
Method Grammar Importance
Human 4.94 4.31
Noisy-channel 3.81 3.38
ME 3.88 3.38
ME + bottom-up 4.22 4.06
Table 2: Results of human judgments.
maximum entropy with the bottom-up method ob-
tained the highest scores of the three methods. We
did t-tests (5% significance). Between the noisy-
channel model and the maximum entropy with the
bottom-up method, importance is significantly dif-
ferent but grammaticality is not. Between the hu-
man and the maximum entropy with the bottom-
up method, grammaticality is significantly differ-
ent but importance is not. There are no significant
differences between the noisy-channel model and
the maximum entropy model.
4.3.1 Problem of Negative Adverbs
One problem of the noisy-channel model is that
it cannot distinguish the meanings of removed
words. That is, it sometimes removes semantically
important words, such as ?not? and ?never?, be-
cause the expansion probability depends only on
non-terminals of parent and daughter nodes.
For example, our test corpus includes 15 sen-
tences that contain ?not?. The noisy-channel
model removed six ?not?s, and the meanings of
the sentences were reversed. However, the two
maximum entropy methods removed only one
?not? because they have ?negative adverb? as a
feature in their models. The first example in Ta-
ble 3 shows one of these sentences. In this exam-
ple, only Noisy-channel removed ?not?.
4.3.2 Effect of Bottom-Up Method
Our bottom-up method achieved the highest
accuracy, in terms of F -measures, bigram F -
measures, BLEU scores and human judgments.
The results were fairly good, especially when it
summarized complex sentences, which have sen-
tences as parts. The second example in Table 3 is
a typical complex sentence. In this example, only
ME + bottom-up correctly remove ?he said?.
Most of the complex sentences were correctly
compressed by the bottom-up method, but a few
sentences like the third example in Table 3 were
not. In this example, the original sentence was
parsed as shown in Figure 8 (left). If this sen-
tence is compressed to the human output, its parse
tree has to be like that in Figure 8 (middle) using
855
Original a file or application ?? alias ??
similar in effect to the ms-dos path
statement provides a visible icon in
folders where an aliased application
does not actually reside .
Human a file or application alias provides
a visible icon in folders where an
aliased application does not actually
reside .
Noisy-
channel
a similar in effect to ms-dos
statement provides a visible icon in
folders where an aliased application
does reside .
ME a or application alias statement
provides a visible icon in folders
where an aliased application does not
actually reside .
ME +
bottom-up
a file or application statement
provides a visible icon in folders
where an aliased application does not
actually reside .
Original the user can then abort the
transmission , he said .
Human the user can then abort the
transmission .
Noisy-
channel
the user can abort the transmission
said .
ME the user can abort the transmission
said .
ME +
bottom-up
the user can then abort the
transmission .
Original it is likely that both companies will
work on integrating multimedia with
database technologies .
Human both companies will work on
integrating multimedia with database
technologies .
Noisy-
channel
it is likely that both companies will
work on integrating .
ME it is likely that both companies will
work on integrating .
ME +
bottom-up
it is will work on integrating
multimedia with database technologies
.
Table 3: Examples of compressed sentences.
our method. When a parse tree is too long from
the root to the leaves like this, some nodes are
trimmed but others are not because we assume that
each trimming probability is independent. The
compressed sentence is ungrammatical, as in the
third example in Table 3.
We have to constrain such ungrammatical sen-
tences or introduce another rule that reconstructs
a short tree as in Figure 8 (right). That is, we in-
troduce a new transformation rule that compresses
(A1 (B (C (A2 ? ? ? )))) to (A2 ? ? ? ).
4.4 Comparison with Original Results
We compared our results with Knight and Marcu?s
original results. They implemented two methods:
one is the noisy-channel model and the other is
a decision-based model. Each model produced
32 compressed sentences, and we calculated F -
measures, bigram F -measures, and BLEU scores.
We used the length parameter ? = 0.5 for the
maximum-entropy method and ? = ?0.25 for
S
VP
is ADJP SBAR
likely that S
both companies 
will ...
S
It
both companies 
will ...
S
VP
SBAR
S
both companies 
will ...
(left) (middle) (right)
Figure 8: Parse trees of complicated complex sen-
tences.
Method Comp. F-measure bigram F-
measure
BLEU
Noisy-
channel
70.19% 68.80 55.96 44.54
Decision-
based
57.26% 71.25 61.93 58.21
ME 66.51% 73.10 62.86 53.51
ME +
bottom-up
58.14% 78.58 70.30 65.26
Human 53.59%
Table 4: Comparison with original results.
the maximum-entropy method with the bottom-up
method. These two values were determined using
experiments on the development set, which did not
contain the 32 test sentences.
The results are shown in Table 4. Noisy-channel
indicates the results of Knight and Marcu?s noisy-
channel model, and Decision-based indicates the
results of Knight and Marcu?s decision-based
model. Comp. indicates the compression ratio of
each result. Our two methods achieved higher ac-
curacy than the noisy-channel model. The results
of the decision-based model and our maximum-
entropy method were not significantly different.
Our maximum-entropy method with the bottom-
up method achieved the highest accuracy.
4.5 Corpus Size and Output Accuracy
In general, using more training data improves the
accuracy of outputs and using less data results in
low accuracy. Our experiment has the problem
that the training corpus was small. To study the re-
lation between training corpus size and accuracy,
we experimented using different training corpus
sizes and compared accuracy of the output.
Figure 9 shows the relations between training
corpus size and three scores, F -measures, bigram
F -measures and BLEU scores, when we used the
maximum entropy method with the bottom-up
method. This graph suggests that the accuracy in-
856
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  100  200  300  400  500  600  700  800
Sc
or
e
Size of training corpus
BLEU score
F-measure
bigram F-measure
Figure 9: Relation between training corpus size
and evaluation score.
creases when the corpus size is increased. Over
about 600 sentences, the increase becomes slower.
The graph shows that the training corpus was
large enough for this study. However, if we intro-
duced other specific features, such as lexical fea-
tures, a larger corpus would be required.
5 Conclusion
We presented a maximum entropy model to ex-
tend the sentence compression methods described
by Knight and Marcu (Knight and Marcu, 2000).
Our proposals are two-fold. First, our maxi-
mum entropy model allows us to incorporate var-
ious characteristics, such as a mother node or the
depth from a root node, into a probabilistic model
for determining which part of an input sentence
is removed. Second, our bottom-up method of
matching original and compressed parse trees can
match tree structures that cannot be matched using
Knight and Marcu?s method.
The experimental results show that our maxi-
mum entropy method improved the accuracy of
sentence compression as determined by three eval-
uation criteria: F -measures, bigram F -measures
and BLEU scores. Using our bottom-up method
further improved accuracy and produced short
summaries that could not be produced by previ-
ous methods. However, we need to modify this
model to appropriately process more complicated
sentences because some sentences were not cor-
rectly summarized. Human judgments showed
that the maximum entropy model with the bottom-
up method provided more grammatical and more
informative summaries than other methods.
Though our training corpus was small, our ex-
periments demonstrated that the data was suffi-
cient. To improve our approaches, we can intro-
duce more feature functions, especially more se-
mantic or lexical features, and to deal with these
features, we need a larger corpus.
Acknowledgements
We would like to thank Prof. Kevin Knight and
Prof. Daniel Marcu for providing their parallel
corpus and the experimental results.
References
A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra.1996. A Maximum Entropy Approach to NaturalLanguage Processing. Computational Linguistics,22(1):39?71.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.
In Proc. of ACL?05, pages 173?180.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge Trim-
mer: A Parse-and-Trim Approach to Headline Gen-eration. In Proc. of DUC 2003, pages 1?8.
H. Jing and K. R. McKeown. 1999. The decomposi-
tion of human-written summary sentences. In Proc.of SIGIR?99, pages 129?136.
K. Knight and D. Marcu. 2000. Statistics-Based Sum-marization - Step One: Sentence Compression. InProc. of AAAI/IAAI?00, pages 703?710.
I. Langkilde. 2000. Forest-Based Statistical SentenceGeneration. In Proc. of NAACL?00, pages 170?177.
C. Lin. 2004. ROUGE: A Package for AutomaticEvaluation of Summaries. In Text SummarizationBranches Out: Proc. of ACL?04 Workshop, pages
74?81.
R. McDonald. 2006. Discriminative Sentence Com-
pression with Soft Syntactic Evidence. In Proc. ofEACL?06.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a Method for Automatic Evaluation of Ma-chine Translation. In Proc. of ACL?02, pages 311?318.
J. Turner and E. Charniak. 2005. Supervised and Un-supervised Learning for Sentence Compression. InProc. of ACL?05, pages 290?297.
V. Vandeghinste and Y. Pan. 2004. Sentence Com-pression for Automated Subtitling: A Hybrid Ap-
proach. In Text Summarization Branches Out: Proc.of ACL?04 Workshop, pages 89?95.
M. J. Witbrock and V. O. Mittal. 1999. Ultra-Summarization: A Statistical Approach to Generat-ing Highly Condensed Non-Extractive Summaries.
In Proc. of SIGIR?99, pages 315?316.
857
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624?631,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
HPSG Parsing with Shallow Dependency Constraints
Kenji Sagae1 and Yusuke Miyao1 and Jun?ichi Tsujii1,2,3
1Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
2School of Computer Science, University of Manchester
3National Center for Text Mining
{sagae,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We present a novel framework that com-
bines strengths from surface syntactic pars-
ing and deep syntactic parsing to increase
deep parsing accuracy, specifically by com-
bining dependency and HPSG parsing. We
show that by using surface dependencies to
constrain the application of wide-coverage
HPSG rules, we can benefit from a num-
ber of parsing techniques designed for high-
accuracy dependency parsing, while actu-
ally performing deep syntactic analysis. Our
framework results in a 1.4% absolute im-
provement over a state-of-the-art approach
for wide coverage HPSG parsing.
1 Introduction
Several efficient, accurate and robust approaches to
data-driven dependency parsing have been proposed
recently (Nivre and Scholz, 2004; McDonald et al,
2005; Buchholz and Marsi, 2006) for syntactic anal-
ysis of natural language using bilexical dependency
relations (Eisner, 1996). Much of the appeal of these
approaches is tied to the use of a simple formalism,
which allows for the use of efficient parsing algo-
rithms, as well as straightforward ways to train dis-
criminative models to perform disambiguation. At
the same time, there is growing interest in pars-
ing with more sophisticated lexicalized grammar
formalisms, such as Lexical Functional Grammar
(LFG) (Bresnan, 1982), Lexicalized Tree Adjoin-
ing Grammar (LTAG) (Schabes et al, 1988), Head-
driven Phrase Structure Grammar (HPSG) (Pollard
and Sag, 1994) and Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000), which represent deep
syntactic structures that cannot be expressed in a
shallower formalism designed to represent only as-
pects of surface syntax, such as the dependency
formalism used in current mainstream dependency
parsing.
We present a novel framework that combines
strengths from surface syntactic parsing and deep
syntactic parsing, specifically by combining depen-
dency and HPSG parsing. We show that, by us-
ing surface dependencies to constrain the applica-
tion of wide-coverage HPSG rules, we can bene-
fit from a number of parsing techniques designed
for high-accuracy dependency parsing, while actu-
ally performing deep syntactic analysis. From the
point of view of HPSG parsing, accuracy can be im-
proved significantly through the use of highly ac-
curate discriminative dependency models, without
the difficulties involved in adapting these models
to a more complex and linguistically sophisticated
formalism. In addition, improvements in depen-
dency parsing accuracy are converted directly into
improvements in HPSG parsing accuracy. From the
point of view of dependency parsing, the applica-
tion of HPSG rules to structures generated by a sur-
face dependency model provides a principled and
linguistically motivated way to identify deep syntac-
tic phenomena, such as long-distance dependencies,
raising and control.
We begin by describing our dependency and
HPSG parsing approaches in section 2. In section
3, we present our framework for HPSG parsing with
shallow dependency constraints, and in section 4 we
624
Figure 1: HPSG parsing
evaluate this framework empirically. Sections 5 and
6 discuss related work and conclusions.
2 Fast dependency parsing and
wide-coverage HPSG parsing
2.1 Data-driven dependency parsing
Because we use dependency parsing as a step in
deep parsing, it is important that we choose a pars-
ing approach that is not only accurate, but also effi-
cient. The deterministic shift/reduce classifier-based
dependency parsing approach (Nivre and Scholz,
2004) has been shown to offer state-of-the-art accu-
racy (Nivre et al, 2006) with high efficiency due to
a greedy search strategy. Our approach is based on
Nivre and Scholz?s approach, using support vector
machines for classification of shift/reduce actions.
2.2 Wide-coverage HPSG parsing
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemas explain general
construction rules, and a large number of lexical en-
tries express word-specific syntactic/semantic con-
straints. Figure 1 shows an example of the process
of HPSG parsing. First, lexical entries are assigned
to each word in a sentence. In Figure 1, lexical
entries express subcategorization frames and pred-
icate argument structures. Parsing proceeds by ap-
plying schemas to lexical entries. In this example,
the Head-Complement Schema is applied to the lex-
ical entries of ?tried? and ?running?. We then obtain
a phrasal structure for ?tried running?. By repeat-
edly applying schemas to lexical/phrasal structures,
Figure 2: Extracting HPSG lexical entries from the
Penn Treebank
we finally obtain an HPSG parse tree that covers the
entire sentence.
In this paper, we use an HPSG parser developed
by Miyao and Tsujii (2005). This parser has a wide-
coverage HPSG lexicon which is extracted from the
Penn Treebank. Figure 2 illustrates their method
for extraction of HPSG lexical entries. First, given
a parse tree from the Penn Treebank (top), HPSG-
style constraints are added and an HPSG-style parse
tree is obtained (middle). Lexical entries are then ex-
tracted from the terminal nodes of the HPSG parse
tree (bottom). This way, in addition to a wide-
coverage lexicon, we also obtain an HPSG treebank,
which can be used as training data for disambigua-
tion models.
The disambiguation model of this parser is based
on a maximum entropy model (Berger et al, 1996).
The probability p(T |W ) of an HPSG parse tree T
for the sentence W = ?w1, . . . , wn? is given as:
p(T |W ) = p(T |L,W )p(L|W )
=
1
Z
exp
(
?
i
?ifi(T )
)
?
j
p(lj |W ),
where L = ?l1, . . . , ln? are lexical entries and
625
p(li|W ) is the supertagging probability, i.e., the
probability of assignining the lexical entry li to wi
(Ninomiya et al, 2006). The probability p(T |L,W )
is a maximum entropy model on HPSG parse trees,
where Z is a normalization factor, and feature func-
tions fi(T ) represent syntactic characteristics, such
as head words, lengths of phrases, and applied
schemas. Given the HPSG treebank as training data,
the model parameters ?i are estimated so as to maxi-
mize the log-likelihood of the training data (Malouf,
2002).
3 HPSG parsing with dependency
constraints
While a number of fairly straightforward models can
be applied successfully to dependency parsing, de-
signing and training HPSG parsing models has been
regarded as a significantly more complex task. Al-
though it seems intuitive that a more sophisticated
linguistic formalism should be more difficult to pa-
rameterize properly, we argue that the difference in
complexity between HPSG and dependency struc-
tures can be seen as incremental, and that the use
of accurate and efficient techniques to determine the
surface dependency structure of a sentence provides
valuable information that aids HPSG disambigua-
tion. This is largely because HPSG is based on a lex-
icalized grammar formalism, and as such its syntac-
tic structures have an underlying dependency back-
bone. However, HPSG syntactic structures includes
long-distance dependencies, and the underlying de-
pendency structure described by and HPSG structure
is a directed acyclic graph, not a dependency tree (as
used by mainstream approaches to data-driven de-
pendency parsing). This difference manifests itself
in words that have multiple heads. For example, in
the sentence I tried to run, the pronoun I is a depen-
dent of tried and of run. This makes it possible to
represent that I is the subject of both verbs, precisely
the kind of information that cannot be represented in
dependency parsing. If we ignore long-distance de-
pendencies, however, HPSG structures can be seen
as lexicalized trees that can be easily converted into
dependency trees.
Given that for an HPSG representation of the syn-
tactic structure of a sentence we can determine a
dependency tree by removing long-distance depen-
dencies, we can use dependency parsing techniques
(such as the deterministic dependency parsing ap-
proach mentioned in section 2.1) to determine the
underlying dependency trees in HPSG structures.
This is the basis for the parsing framework presented
here. In this approach, deep dependency analysis
is done in two stages. First, a dependency parser
determines the shallow dependency tree for the in-
put sentence. This shallow dependency tree corre-
sponds to the underlying dependency graph of the
HPSG structure for the input sentence, without de-
pendencies that roughly correspond to deep syntax.
The second step is to perform HPSG parsing, as
described in section 2.2, but using the shallow de-
pendency tree to constrain the application of HPSG
rules. We now discuss these two steps in more detail.
3.1 Determining shallow dependencies in
HPSG structures using dependency parsing
In order to apply a data-driven dependency ap-
proach to the task of identifying the shallow de-
pendency tree in HPSG structures, we first need a
corpus of such dependency trees to serve as train-
ing data. We created a dependency training corpus
based on the Penn Treebank (Marcus et al, 1993),
or more specifically on the HPSG Treebank gener-
ated from the Penn Treebank (see section 2.2). For
each HPSG structure in the HPSG Treebank, a de-
pendency tree is extracted in two steps. First, the
HPSG tree is converted into a CFG-style tree, sim-
ply by removing long-distance dependency links be-
tween nodes. A dependency tree is then extracted
from the resulting lexicalized CFG-style tree, as is
commonly done for converting constituent trees into
dependency trees after the application of a head-
percolation table (Collins, 1999).
Once a dependency training corpus is available,
it is used to train a dependency parser as described
in section 2.1. This is done by training a classifier
to determine parser actions based on local features
that represent the current state of the parser (Nivre
and Scholz, 2004; Sagae and Lavie, 2005). Train-
ing data for the classifier is obtained by applying the
parsing algorithm over the training sentences (for
which the correct dependency structures are known)
and recording the appropriate parser actions that re-
sult in the formation of the correct dependency trees,
coupled with the features that represent the state of
626
the parser mentioned in section 2.1. An evaluation
of the resulting dependency parser and its efficacy in
aiding HPSG parsing is presented in section 4.
3.2 Parsing with dependency constraints
Given a set of dependencies, the bottom-up process
of HPSG parsing can be constrained so that it does
not violate the given dependencies. This can be
achieved by a simple extension of the parsing algo-
rithm, as follows. During parsing, we store the lex-
ical head of each partial parse tree. In each schema
application, we can determine which child is the
head; for example, the left child is the head when
we apply the Head-Complement Schema. Given this
information and lexical heads, the parser can iden-
tify the dependency produced by this schema appli-
cation, and can therefore judge whether the schema
application violates the dependency constraints.
This method forces the HPSG parser to produce
parse trees that strictly conform to the output of
the dependency parser. However, this means that
the HPSG parser outputs no successful parse results
when it cannot find the parse tree that is completely
consistent with the given dependencies. This situ-
ation may occur when the dependency parser pro-
duces structures that are not covered in the HPSG
grammar. This is especially likely with a fully data-
driven dependency parser that uses local classifica-
tion, since its output may not be globally consistent
grammatically. In addition, the HPSG grammar is
extracted from the HPSG Treebank using a corpus-
based procedure, and it does not necessarily cover
all possible grammatical phenomena in unseen text
(Miyao and Tsujii, 2005).
We therefore propose an extension of this ap-
proach that uses predetermined dependencies as soft
constraints. Violations of schema applications are
detected in the same way as before, but instead of
strictly prohibiting schema applications, we penal-
ize the log-likelihood of partial parse trees created
by schema applications that violate the dependen-
cies constraints. Given a negative value ?, we add
? to the log-probability of a partial parse tree when
the schema application violates the dependency con-
straints. That is, when a parse tree violates n depen-
dencies, the log-probability of the parse tree is low-
ered by n?. The meta parameter ? is determined so
as to maximize the accuracy on the development set.
Soft dependency constraints can be implemented
as explained above as a straightforward extension of
the parsing algorithm. In addition, it is easily inte-
grated with beam thresholding methods of parsing.
Because beam thresholding discards partial parse
trees that have low log-probabilities, we can ex-
pect that the parser would discard partial parse trees
based on violation of the dependency constraints.
4 Experiments
We evaluate the accuracy of HPSG parsing with de-
pendency constraints on the HPSG Treebank (Miyao
et al, 2003), which is extracted from the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993)1. Sections 02-21 were used for training
(for HPSG and dependency parsers), section 22 was
used as development data, and final testing was per-
formed on section 23. Following previous work on
wide-coverage parsing with lexicalized grammars
using the Penn Treebank, we evaluate the parser by
measuring the accuracy of predicate-argument rela-
tions in the parser?s output. A predicate-argument
relation is defined as a tuple ??,wh, a, wa?, where
? is the predicate type (e.g. adjective, intransitive
verb), wh is the head word of the predicate, a is the
argument label (MODARG, ARG1, ... , ARG4), and
wa is the head word of the argument. Labeled pre-
cision (LP)/labeled recall (LR) is the ratio of tuples
correctly identified by the parser. These predicate-
argument relations cover the full range of syntactic
dependencies produced by the HPSG parser (includ-
ing, long-distance dependencies, raising and control,
in addition to surface dependencies).
In the experiments presented in this section, in-
put sentences were automatically tagged with parts-
of-speech with about 97% accuracy, using a max-
imum entropy POS tagger. We also report results
on parsing text with gold standard POS tags, where
explicitly noted. This provides an upper-bound on
what can be expected if a more sophisticated multi-
tagging scheme (James R. Curran and Vadas, 2006)
is used, instead of hard assignment of single tags in
a preprocessing step as done here.
1The extraction software can be obtained from http://www-
tsujii.is.s.u-tokyo.ac.jp/enju.
627
4.1 Baseline
HPSG parsing results using the same HPSG gram-
mar and treebank have recently been reported by
Miyao and Tsujii (2005) and Ninomia et al (2006).
By running the HPSG parser described in section 2.2
on the development data without dependency con-
straints, we obtain similar values of LP (86.8%) and
LR (85.6%) as those reported by Miyao and Tsu-
jii (Miyao and Tsujii, 2005). Using the extremely
lexicalized framework of (Ninomiya et al, 2006) by
performing supertagging before parsing, we obtain
similar accuracy as Ninomiya et al (87.1% LP and
85.9% LR).
4.2 Dependency constraints and the penalty
parameter
Parsing the development data with hard dependency
constraints confirmed the intuition that these con-
straints often describe dependency structures that do
not conform to HPSG schema used in parsing, re-
sulting in parse failures. To determine the upper-
bound on HPSG parsing with hard dependency con-
straints, we set the HPSG parser to disallow the ap-
plication of any rules that result in the creation of
dependencies that violate gold standard dependen-
cies. This results in high precision (96.7%), but re-
call is low (82.3%) due to parse failures caused by
lack of grammatical coverage 2. Using dependen-
cies produced by the shift-reduce SVM parser, we
obtain 91.5% LP and 65.7% LR. This represents a
large gain in precision over the baseline, but an even
greater loss in recall, which limits the usefulness of
the parser, and severely hurts the appeal of hard con-
straints.
We focus the rest of our experiments on parsing
with soft dependency constraints. As explained in
section 3, this involves setting the penalty parame-
ter ?. During parsing, we subtract ? from the log-
probability of applying any schema that violates the
dependency constraints given to the HPSG parser.
Figure 3 illustrates the effect of ? when gold stan-
dard dependencies (and gold standard POS tags) are
used. We note that setting ? = 0 causes the parser
2Although the HPSG grammar does not have perfect cov-
erage of unseen text, it supports complete and mostly correct
analyses for all sentences in the development set. However,
when we require completely correct analyses by using hard con-
straints, lack of coverage may cause parse failures.
8990919293949596 0
5
10
15
20
25
30
35
Penal
ty
Accuracy
Precis
ion
Recal
l
F-sco
re
Figure 3: The effect of ? on HPSG parsing con-
strained by gold standard dependencies.
to ignore dependency constraints, providing base-
line performance. Conversely, setting a high enough
value (? = 30 is sufficient, in practice) causes any
substructures that violate the dependency constraints
to be used only when they are absolutely neces-
sary to produce a valid parse for the input sentence.
In figure 3, this corresponds to an upper-bound on
the accuracy of parsing with soft dependency con-
straints (94.7% f-score), since gold standard depen-
dencies are used.
We set ? empirically with simple hill climbing on
the development set. Because it is expected that the
optimal value of ? depends on the accuracy of the
surface dependency parser, we set separate values
for parsing with a POS tagger or with gold standard
POS tags. Figure 4 shows the accuracy of HPSG
predicate-argument relations obtained with depen-
dency constraints determined by dependency pars-
ing with gold standard POS tags. With both au-
tomatically assigned and gold standard POS tags,
we observe an improvement of about 0.6% in pre-
cision, recall and f-score, when the optimal ? value
is used in each case. While this corresponds to a rel-
ative error reduction of over 6% (or 12%, if we con-
sider the upper-bound dictated by imperfect gram-
matical coverage), a more interesting aspect of this
framework is that it allows techniques designed for
improving dependency accuracy to improve HPSG
parsing accuracy directly, as we illustrate next.
628
89.489.689.89090.290.490.690.891 0
0.5
1
1.5
2
2.5
3
3.5
Penal
ty
Accuracy
Precis
ion
Recal
l
F-sco
re
Figure 4: The effect of ? on HPSG parsing con-
strained by the output of a dependency parser using
gold standard POS tags.
4.3 Determining constraints with dependency
parser combination
Parser combination has been shown to be a power-
ful way to obtain very high accuracy in dependency
parsing (Sagae and Lavie, 2006). Using dependency
constraints allows us to improve HPSG parsing ac-
curacy simply by using an existing parser combina-
tion approach. As a first step, we train two addi-
tional parsers with the dependencies extracted from
the HPSG Treebank. The first uses the same shift-
reduce framework described in section 2.1, but it
process the input from right to left (RL). This has
been found to work well in previous work on depen-
dency parser combination (Zeman and Z?abokrtsky?,
2005; Sagae and Lavie, 2006). The second parser
is MSTParser, the large-margin maximum spanning
tree parser described in (McDonald et al, 2005)3.
We examine the use of two combination schemes:
one using two parsers, and one using three parsers.
The first combination approach is to keep only de-
pendencies for which there is agreement between the
two parsers. In other words, dependencies that are
proposed by one parser but not the other are simply
discarded. Using the left-to-right shift-reduce parser
and MSTParser, we find that this results in very high
precision of surface dependencies on the develop-
ment data. In the second approach, combination of
3Downloaded from http://sourceforge.net/projects/mstparser
the three dependency parsers is done according to
the maximum spanning tree combination scheme of
Sagae and Lavie (2006), which results in high accu-
racy of surface dependencies. For each of the com-
bination approaches, we use the resulting dependen-
cies as constraints for HPSG parsing, determining
the optimal value of ? on the development set in
the same way as done for a single parser. Table 1
summarizes our experiments on development data
using parser combinations to produce dependency
constraints 4. The two combination approaches are
denoted as C1 and C2.
Parser Dep ? HPSG Diff
none (baseline) ? ? 86.5 ?
LR shift-reduce 91.2 1.5 87.1 0.6
RL shift-reduce 90.1 ? ?
MSTParser 91.0 ? ?
C1 (agreement) 96.8* 2.5 87.4 0.9
C2 (MST) 92.4 2.5 87.4 0.9
Table 1: Summary of results on development data.
* The shallow accuracy of combination C1 corre-
sponds to the dependency precision (no dependen-
cies were reported for 8% of all words in the devel-
opment set).
4.4 Results
Having determined ? values on development data
for the shift-reduce dependency parser, the two-
parser agreement combination, and the three-parser
maximum spanning tree combination, we parse the
test data (section 23) using these three different
sources of dependency constraints for HPSG pars-
ing. Our final results are shown in table 2, where
we also include the results published in (Ninomiya
et al, 2006) for comparison purposes, and the result
of using dependency constraints obtained with gold
standard POS tags.
By using two unlabeled dependency parsers to
provide soft dependency constraints, we obtain a
1% absolute improvement in precision and recall of
predicate-argument identification in HPSG parsing
over a strong baseline. Our baseline approach out-
performed previously published results on this test
4The accuracy figures for the dependency parsers is ex-
pressed as unlabeled accuracy of the surface dependencies only,
and are not comparable to the HPSG parsing accuracy figures
629
Parser LP LR F-score
HPSG Baseline 87.4 87.0 87.2
Shift-Reduce + HPSG 88.2 87.7 87.9
C1 + HPSG 88.5 88.0 88.2
C2 + HPSG 88.4 87.9 88.1
Baseline(gold) 89.8 89.4 89.6
Shift-Reduce(gold) 90.62 90.23 90.42
C1+HPSG(gold) 90.9 90.4 90.6
C2+HPSG(gold) 90.8 90.4 90.6
Miyao and Tsujii, 2005 85.0 84.3 84.6
Ninomiya et al, 2006 87.4 86.3 86.8
Table 2: Final results on test set. The first set of
results show our HPSG baseline and HPSG with soft
dependency constraints using three different sources
of dependency constraints. The second set of results
show the accuracy of the same parsers when gold
part-of-speech tags are used. The third set of results
is from existing published models on the same data.
set, and our best performing combination scheme
obtains an absolute improvement of 1.4% over the
best previously published results using the HPSG
Treebank. It is interesting to note that the results ob-
tained with dependency parser combinations C1 and
C2 were very similar, even though in C1 only two
parsers were used, and constraints were provided for
about 92% of shallow dependencies (with accuracy
higher than 96%). Clearly, precision is crucial in de-
pendency constraints.
Finally, although it is necessary to perform de-
pendency parsing to pre-compute dependency con-
straints, the total time required to perform the en-
tire process of HPSG parsing with dependency con-
straints is close to that of the baseline HPSG ap-
proach. This is due to two reasons: (1) the de-
pendency parsing approaches used to pre-compute
constraints are several times faster than the baseline
HPSG approach, and (2) the HPSG portion of the
process is significantly faster when dependency con-
straints are used, since the constraints help sharpen
the search space, making search more efficient. Us-
ing the baseline HPSG approach, it takes approx-
imately 25 minutes to parse the test set. The to-
tal time required to parse the test set using HPSG
with dependency constraints generated by the shift-
reduce parser is 27 minutes. With combination C1,
parsing time increases to 30 minutes, since two de-
pendency parsers are used sequentially.
5 Related work
There are other approaches that combine shallow
processing with deep parsing (Crysmann et al,
2002; Frank et al, 2003; Daum et al, 2003) to im-
prove parsing efficiency. Typically, shallow parsing
is used to create robust minimal recursion seman-
tics, which are used as constraints to limit ambigu-
ity during parsing. Our approach, in contrast, uses
syntactic dependencies to achieve a significant im-
provement in the accuracy of wide-coverage HPSG
parsing. Additionally, our approach is in many
ways similar to supertagging (Bangalore and Joshi,
1999), which uses sequence labeling techniques as
an efficient way to pre-compute parsing constraints
(specifically, the assignment of lexical entries to in-
put words).
6 Conclusion
We have presented a novel framework for taking ad-
vantage of the strengths of a shallow parsing ap-
proach and a deep parsing approach. We have
shown that by constraining the application of rules
in HPSG parsing according to results from a depen-
dency parser, we can significantly improve the ac-
curacy of deep parsing by using shallow syntactic
analyses.
To illustrate how this framework allows for im-
provements in the accuracy of dependency parsing
to be used directly to improve the accuracy of HPSG
parsing, we showed that by combining the results of
different dependency parsers using the search-based
parsing ensemble approach of (Sagae and Lavie,
2006), we obtain improved HPSG parsing accuracy
as a result of the improved dependency accuracy.
Although we have focused on the use of HPSG
and dependency parsing, the general framework pre-
sented here can be applied to other lexicalized gram-
mar formalisms, such as LTAG, CCG and LFG.
Acknowledgements
This research was partially supported by Grant-in-
Aid for Specially Promoted Research 18002007.
630
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
Amaximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Joan Bresnan. 1982. The mental representation of gram-
matical relations. MIT Press.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Natural Language
Learning. New York, NY.
M. Collins. 1999. Head-Driven Models for Natural Lan-
guage Parsing. Phd thesis, University of Pennsylva-
nia.
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan
Mueller, Guenter Neumann, Jakub Piskorski, Ulrich
Schaefer, Melanie Siegel, Hans Uszkoreit, Feiyu Xu,
Markus Becker, and Hans-Ulrich Krieger. 2002. An
integrated architecture for shallow and deep process-
ing. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002).
Michael Daum, Kilian A. Foth, and Wolfgang Menzel.
2003. Constraint-based integration of deep and shal-
low parsing techniques. In Proceedings of the 10th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2003).
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of the International Conference on Computational Lin-
guistics (COLING?96). Copenhagen, Denmark.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Schaefer. 2003. Integrated
shallow and deep parsing: TopP meets HPSG. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2003), pages
104?111.
Stephen Clark James R. Curran and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In
Proceedings of COLING/ACL 2006. Sydney, Aus-
tralia.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of the 2002 Conference on Natural Language
Learning.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewics.
1993. Building a large annotated corpus of english:
The penn treebank. Computational Linguistics, 19.
Ryan McDonald, Fernando Pereira, K. Ribarov, and
J. Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technolo-
gies/Empirical Methods in Natural Language Process-
ing (HLT-EMNLP). Vancouver, Canada.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proceedings of the 42nd Meeting of the Associ-
ation for Computational Linguistics. Ann Arbor, MI.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Corpus oriented grammar development for
aquiring a head-driven phrase structure grammar from
the penn treebank. In Proceedings of the Tenth Con-
ference on Natural Language Learning.
T. Ninomiya, T. Matsuzaki, Y. Tsuruoka, Y. Miyao, and
J. Tsujii. 2006. Extremely lexicalized models for ac-
curate and fast hpsg parsing. In Proceedings of the
2006 Conference on Empirical Methods for Natural
Language Processing (EMNLP 2006).
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 64?70. Geneva, Switzerland.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
the Tenth Conference on Natural Language Learning.
New York, NY.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies. Vancouver, BC.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of the 2006 Meeting of
the North American ACL. New York, NY.
Yves Schabes, Anne Abeille, and Aravind Joshi. 1988.
Parsing strategies with lexicalized grammars: Appli-
cation to tree adjoining grammars. In Proceedings of
12th COLING.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Zeman and Zdenek Z?abokrtsky?. 2005. Improving
parsing accuracy by combining diverse dependency
parsers. In Proceedings of the International Workshop
on Parsing Technologies. Vancouver, Canada.
631
Proceedings of ACL-08: HLT, pages 46?54,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Task-oriented Evaluation of Syntactic Parsers and Their Representations
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a comparative evalua-
tion of several state-of-the-art English parsers
based on different frameworks. Our approach
is to measure the impact of each parser when it
is used as a component of an information ex-
traction system that performs protein-protein
interaction (PPI) identification in biomedical
papers. We evaluate eight parsers (based on
dependency parsing, phrase structure parsing,
or deep parsing) using five different parse rep-
resentations. We run a PPI system with several
combinations of parser and parse representa-
tion, and examine their impact on PPI identi-
fication accuracy. Our experiments show that
the levels of accuracy obtained with these dif-
ferent parsers are similar, but that accuracy
improvements vary when the parsers are re-
trained with domain-specific data.
1 Introduction
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame-
works (Charniak, 2000; Klein and Manning, 2003;
Charniak and Johnson, 2005; Petrov and Klein,
2007), but also include dependency parsers (Mc-
Donald and Pereira, 2006; Nivre and Nilsson, 2005;
Sagae and Tsujii, 2007) and deep parsers (Kaplan
et al, 2004; Clark and Curran, 2004; Miyao and
Tsujii, 2008). However, efforts to perform extensive
comparisons of syntactic parsers based on different
frameworks have been limited. The most popular
method for parser comparison involves the direct
measurement of the parser output accuracy in terms
of metrics such as bracketing precision and recall, or
dependency accuracy. This assumes the existence of
a gold-standard test corpus, such as the Penn Tree-
bank (Marcus et al, 1994). It is difficult to apply
this method to compare parsers based on different
frameworks, because parse representations are often
framework-specific and differ from parser to parser
(Ringger et al, 2004). The lack of such comparisons
is a serious obstacle for NLP researchers in choosing
an appropriate parser for their purposes.
In this paper, we present a comparative evalua-
tion of syntactic parsers and their output represen-
tations based on different frameworks: dependency
parsing, phrase structure parsing, and deep pars-
ing. Our approach to parser evaluation is to mea-
sure accuracy improvement in the task of identify-
ing protein-protein interaction (PPI) information in
biomedical papers, by incorporating the output of
different parsers as statistical features in a machine
learning classifier (Yakushiji et al, 2005; Katrenko
and Adriaans, 2006; Erkan et al, 2007; S?tre et al,
2007). PPI identification is a reasonable task for
parser evaluation, because it is a typical information
extraction (IE) application, and because recent stud-
ies have shown the effectiveness of syntactic parsing
in this task. Since our evaluation method is applica-
ble to any parser output, and is grounded in a real
application, it allows for a fair comparison of syn-
tactic parsers based on different frameworks.
Parser evaluation in PPI extraction also illu-
minates domain portability. Most state-of-the-art
parsers for English were trained with the Wall Street
Journal (WSJ) portion of the Penn Treebank, and
high accuracy has been reported for WSJ text; how-
ever, these parsers rely on lexical information to at-
tain high accuracy, and it has been criticized that
these parsers may overfit to WSJ text (Gildea, 2001;
46
Klein and Manning, 2003). Another issue for dis-
cussion is the portability of training methods. When
training data in the target domain is available, as
is the case with the GENIA Treebank (Kim et al,
2003) for biomedical papers, a parser can be re-
trained to adapt to the target domain, and larger ac-
curacy improvements are expected, if the training
method is sufficiently general. We will examine
these two aspects of domain portability by compar-
ing the original parsers with the retrained parsers.
2 Syntactic Parsers and Their
Representations
This paper focuses on eight representative parsers
that are classified into three parsing frameworks:
dependency parsing, phrase structure parsing, and
deep parsing. In general, our evaluation methodol-
ogy can be applied to English parsers based on any
framework; however, in this paper, we chose parsers
that were originally developed and trained with the
Penn Treebank or its variants, since such parsers can
be re-trained with GENIA, thus allowing for us to
investigate the effect of domain adaptation.
2.1 Dependency parsing
Because the shared tasks of CoNLL-2006 and
CoNLL-2007 focused on data-driven dependency
parsing, it has recently been extensively studied in
parsing research. The aim of dependency pars-
ing is to compute a tree structure of a sentence
where nodes are words, and edges represent the re-
lations among words. Figure 1 shows a dependency
tree for the sentence ?IL-8 recognizes and activates
CXCR1.? An advantage of dependency parsing is
that dependency trees are a reasonable approxima-
tion of the semantics of sentences, and are readily
usable in NLP applications. Furthermore, the effi-
ciency of popular approaches to dependency pars-
ing compare favorable with those of phrase struc-
ture parsing or deep parsing. While a number of ap-
proaches have been proposed for dependency pars-
ing, this paper focuses on two typical methods.
MST McDonald and Pereira (2006)?s dependency
parser,1 based on the Eisner algorithm for projective
dependency parsing (Eisner, 1996) with the second-
order factorization.
1http://sourceforge.net/projects/mstparser
Figure 1: CoNLL-X dependency tree
Figure 2: Penn Treebank-style phrase structure tree
KSDEP Sagae and Tsujii (2007)?s dependency
parser,2 based on a probabilistic shift-reduce al-
gorithm extended by the pseudo-projective parsing
technique (Nivre and Nilsson, 2005).
2.2 Phrase structure parsing
Owing largely to the Penn Treebank, the mainstream
of data-driven parsing research has been dedicated
to the phrase structure parsing. These parsers output
Penn Treebank-style phrase structure trees, although
function tags and empty categories are stripped off
(Figure 2). While most of the state-of-the-art parsers
are based on probabilistic CFGs, the parameteriza-
tion of the probabilistic model of each parser varies.
In this work, we chose the following four parsers.
NO-RERANK Charniak (2000)?s parser, based on a
lexicalized PCFG model of phrase structure trees.3
The probabilities of CFG rules are parameterized on
carefully hand-tuned extensive information such as
lexical heads and symbols of ancestor/sibling nodes.
RERANK Charniak and Johnson (2005)?s rerank-
ing parser. The reranker of this parser receives n-
best4 parse results from NO-RERANK, and selects
the most likely result by using a maximum entropy
model with manually engineered features.
BERKELEY Berkeley?s parser (Petrov and Klein,
2007).5 The parameterization of this parser is op-
2http://www.cs.cmu.edu/?sagae/parser/
3http://bllip.cs.brown.edu/resources.shtml
4We set n = 50 in this paper.
5http://nlp.cs.berkeley.edu/Main.html#Parsing
47
Figure 3: Predicate argument structure
timized automatically by assigning latent variables
to each nonterminal node and estimating the param-
eters of the latent variables by the EM algorithm
(Matsuzaki et al, 2005).
STANFORD Stanford?s unlexicalized parser (Klein
and Manning, 2003).6 Unlike NO-RERANK, proba-
bilities are not parameterized on lexical heads.
2.3 Deep parsing
Recent research developments have allowed for ef-
ficient and robust deep parsing of real-world texts
(Kaplan et al, 2004; Clark and Curran, 2004; Miyao
and Tsujii, 2008). While deep parsers compute
theory-specific syntactic/semantic structures, pred-
icate argument structures (PAS) are often used in
parser evaluation and applications. PAS is a graph
structure that represents syntactic/semantic relations
among words (Figure 3). The concept is therefore
similar to CoNLL dependencies, though PAS ex-
presses deeper relations, and may include reentrant
structures. In this work, we chose the two versions
of the Enju parser (Miyao and Tsujii, 2008).
ENJU The HPSG parser that consists of an HPSG
grammar extracted from the Penn Treebank, and
a maximum entropy model trained with an HPSG
treebank derived from the Penn Treebank.7
ENJU-GENIA The HPSG parser adapted to
biomedical texts, by the method of Hara et al
(2007). Because this parser is trained with both
WSJ and GENIA, we compare it parsers that are
retrained with GENIA (see section 3.3).
3 Evaluation Methodology
In our approach to parser evaluation, we measure
the accuracy of a PPI extraction system, in which
6http://nlp.stanford.edu/software/lex-parser.
shtml
7http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
This study demonstrates that IL-8 recognizes and
activates CXCR1, CXCR2, and the Duffy antigen
by distinct mechanisms.
The molar ratio of serum retinol-binding protein
(RBP) to transthyretin (TTR) is not useful to as-
sess vitamin A status during infection in hospi-
talised children.
Figure 4: Sentences including protein names
ENTITY1(IL-8) SBJ?? recognizes OBJ?? ENTITY2(CXCR1)
Figure 5: Dependency path
the parser output is embedded as statistical features
of a machine learning classifier. We run a classi-
fier with features of every possible combination of a
parser and a parse representation, by applying con-
versions between representations when necessary.
We also measure the accuracy improvements ob-
tained by parser retraining with GENIA, to examine
the domain portability, and to evaluate the effective-
ness of domain adaptation.
3.1 PPI extraction
PPI extraction is an NLP task to identify protein
pairs that are mentioned as interacting in biomedical
papers. Because the number of biomedical papers is
growing rapidly, it is impossible for biomedical re-
searchers to read all papers relevant to their research;
thus, there is an emerging need for reliable IE tech-
nologies, such as PPI identification.
Figure 4 shows two sentences that include pro-
tein names: the former sentence mentions a protein
interaction, while the latter does not. Given a pro-
tein pair, PPI extraction is a task of binary classi-
fication; for example, ?IL-8, CXCR1? is a positive
example, and ?RBP, TTR? is a negative example.
Recent studies on PPI extraction demonstrated that
dependency relations between target proteins are ef-
fective features for machine learning classifiers (Ka-
trenko and Adriaans, 2006; Erkan et al, 2007; S?tre
et al, 2007). For the protein pair IL-8 and CXCR1
in Figure 4, a dependency parser outputs a depen-
dency tree shown in Figure 1. From this dependency
tree, we can extract a dependency path shown in Fig-
ure 5, which appears to be a strong clue in knowing
that these proteins are mentioned as interacting.
48
(dep_path (SBJ (ENTITY1 recognizes))
(rOBJ (recognizes ENTITY2)))
Figure 6: Tree representation of a dependency path
We follow the PPI extraction method of S?tre et
al. (2007), which is based on SVMs with SubSet
Tree Kernels (Collins and Duffy, 2002; Moschitti,
2006), while using different parsers and parse rep-
resentations. Two types of features are incorporated
in the classifier. The first is bag-of-words features,
which are regarded as a strong baseline for IE sys-
tems. Lemmas of words before, between and after
the pair of target proteins are included, and the linear
kernel is used for these features. These features are
commonly included in all of the models. Filtering
by a stop-word list is not applied because this setting
made the scores higher than S?tre et al (2007)?s set-
ting. The other type of feature is syntactic features.
For dependency-based parse representations, a de-
pendency path is encoded as a flat tree as depicted in
Figure 6 (prefix ?r? denotes reverse relations). Be-
cause a tree kernel measures the similarity of trees
by counting common subtrees, it is expected that the
system finds effective subsequences of dependency
paths. For the PTB representation, we directly en-
code phrase structure trees.
3.2 Conversion of parse representations
It is widely believed that the choice of representa-
tion format for parser output may greatly affect the
performance of applications, although this has not
been extensively investigated. We should therefore
evaluate the parser performance in multiple parse
representations. In this paper, we create multiple
parse representations by converting each parser?s de-
fault output into other representations when possi-
ble. This experiment can also be considered to be
a comparative evaluation of parse representations,
thus providing an indication for selecting an appro-
priate parse representation for similar IE tasks.
Figure 7 shows our scheme for representation
conversion. This paper focuses on five representa-
tions as described below.
CoNLL The dependency tree format used in the
2006 and 2007 CoNLL shared tasks on dependency
parsing. This is a representation format supported by
several data-driven dependency parsers. This repre-
Figure 7: Conversion of parse representations
Figure 8: Head dependencies
sentation is also obtained from Penn Treebank-style
trees by applying constituent-to-dependency conver-
sion8 (Johansson and Nugues, 2007). It should be
noted, however, that this conversion cannot work
perfectly with automatic parsing, because the con-
version program relies on function tags and empty
categories of the original Penn Treebank.
PTB Penn Treebank-style phrase structure trees
without function tags and empty nodes. This is the
default output format for phrase structure parsers.
We also create this representation by converting
ENJU?s output by tree structure matching, although
this conversion is not perfect because forms of PTB
and ENJU?s output are not necessarily compatible.
HD Dependency trees of syntactic heads (Fig-
ure 8). This representation is obtained by convert-
ing PTB trees. We first determine lexical heads of
nonterminal nodes by using Bikel?s implementation
of Collins? head detection algorithm9 (Bikel, 2004;
Collins, 1997). We then convert lexicalized trees
into dependencies between lexical heads.
SD The Stanford dependency format (Figure 9).
This format was originally proposed for extracting
dependency relations useful for practical applica-
tions (de Marneffe et al, 2006). A program to con-
vert PTB is attached to the Stanford parser. Although
the concept looks similar to CoNLL, this representa-
8http://nlp.cs.lth.se/pennconverter/
9http://www.cis.upenn.edu/?dbikel/software.
html
49
Figure 9: Stanford dependencies
tion does not necessarily form a tree structure, and is
designed to express more fine-grained relations such
as apposition. Research groups for biomedical NLP
recently adopted this representation for corpus anno-
tation (Pyysalo et al, 2007a) and parser evaluation
(Clegg and Shepherd, 2007; Pyysalo et al, 2007b).
PAS Predicate-argument structures. This is the de-
fault output format for ENJU and ENJU-GENIA.
Although only CoNLL is available for depen-
dency parsers, we can create four representations for
the phrase structure parsers, and five for the deep
parsers. Dotted arrows in Figure 7 indicate imper-
fect conversion, in which the conversion inherently
introduces errors, and may decrease the accuracy.
We should therefore take caution when comparing
the results obtained by imperfect conversion. We
also measure the accuracy obtained by the ensem-
ble of two parsers/representations. This experiment
indicates the differences and overlaps of information
conveyed by a parser or a parse representation.
3.3 Domain portability and parser retraining
Since the domain of our target text is different from
WSJ, our experiments also highlight the domain
portability of parsers. We run two versions of each
parser in order to investigate the two types of domain
portability. First, we run the original parsers trained
with WSJ10 (39832 sentences). The results in this
setting indicate the domain portability of the original
parsers. Next, we run parsers re-trained with GE-
NIA11 (8127 sentences), which is a Penn Treebank-
style treebank of biomedical paper abstracts. Accu-
racy improvements in this setting indicate the pos-
sibility of domain adaptation, and the portability of
the training methods of the parsers. Since the parsers
listed in Section 2 have programs for the training
10Some of the parser packages include parsing models
trained with extended data, but we used the models trained with
WSJ section 2-21 of the Penn Treebank.
11The domains of GENIA and AImed are not exactly the
same, because they are collected independently.
with a Penn Treebank-style treebank, we use those
programs as-is. Default parameter settings are used
for this parser re-training.
In preliminary experiments, we found that de-
pendency parsers attain higher dependency accuracy
when trained only with GENIA. We therefore only
input GENIA as the training data for the retraining
of dependency parsers. For the other parsers, we in-
put the concatenation of WSJ and GENIA for the
retraining, while the reranker of RERANK was not re-
trained due to its cost. Since the parsers other than
NO-RERANK and RERANK require an external POS
tagger, a WSJ-trained POS tagger is used with WSJ-
trained parsers, and geniatagger (Tsuruoka et al,
2005) is used with GENIA-retrained parsers.
4 Experiments
4.1 Experiment settings
In the following experiments, we used AImed
(Bunescu and Mooney, 2004), which is a popular
corpus for the evaluation of PPI extraction systems.
The corpus consists of 225 biomedical paper ab-
stracts (1970 sentences), which are sentence-split,
tokenized, and annotated with proteins and PPIs.
We use gold protein annotations given in the cor-
pus. Multi-word protein names are concatenated
and treated as single words. The accuracy is mea-
sured by abstract-wise 10-fold cross validation and
the one-answer-per-occurrence criterion (Giuliano
et al, 2006). A threshold for SVMs is moved to
adjust the balance of precision and recall, and the
maximum f-scores are reported for each setting.
4.2 Comparison of accuracy improvements
Tables 1 and 2 show the accuracy obtained by using
the output of each parser in each parse representa-
tion. The row ?baseline? indicates the accuracy ob-
tained with bag-of-words features. Table 3 shows
the time for parsing the entire AImed corpus, and
Table 4 shows the time required for 10-fold cross
validation with GENIA-retrained parsers.
When using the original WSJ-trained parsers (Ta-
ble 1), all parsers achieved almost the same level
of accuracy ? a significantly better result than the
baseline. To the extent of our knowledge, this is
the first result that proves that dependency parsing,
phrase structure parsing, and deep parsing perform
50
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 53.2/56.5/54.6 N/A N/A N/A N/A
KSDEP 49.3/63.0/55.2 N/A N/A N/A N/A
NO-RERANK 50.7/60.9/55.2 45.9/60.5/52.0 50.6/60.9/55.1 49.9/58.2/53.5 N/A
RERANK 53.6/59.2/56.1 47.0/58.9/52.1 48.1/65.8/55.4 50.7/62.7/55.9 N/A
BERKELEY 45.8/67.6/54.5 50.5/57.6/53.7 52.3/58.8/55.1 48.7/62.4/54.5 N/A
STANFORD 50.4/60.6/54.9 50.9/56.1/53.0 50.7/60.7/55.1 51.8/58.1/54.5 N/A
ENJU 52.6/58.0/55.0 48.7/58.8/53.1 57.2/51.9/54.2 52.2/58.1/54.8 48.9/64.1/55.3
Table 1: Accuracy on the PPI task with WSJ-trained parsers (precision/recall/f-score)
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 49.1/65.6/55.9 N/A N/A N/A N/A
KSDEP 51.6/67.5/58.3 N/A N/A N/A N/A
NO-RERANK 53.9/60.3/56.8 51.3/54.9/52.8 53.1/60.2/56.3 54.6/58.1/56.2 N/A
RERANK 52.8/61.5/56.6 48.3/58.0/52.6 52.1/60.3/55.7 53.0/61.1/56.7 N/A
BERKELEY 52.7/60.3/56.0 48.0/59.9/53.1 54.9/54.6/54.6 50.5/63.2/55.9 N/A
STANFORD 49.3/62.8/55.1 44.5/64.7/52.5 49.0/62.0/54.5 54.6/57.5/55.8 N/A
ENJU 54.4/59.7/56.7 48.3/60.6/53.6 56.7/55.6/56.0 54.4/59.3/56.6 52.0/63.8/57.2
ENJU-GENIA 56.4/57.4/56.7 46.5/63.9/53.7 53.4/60.2/56.4 55.2/58.3/56.5 57.5/59.8/58.4
Table 2: Accuracy on the PPI task with GENIA-retrained parsers (precision/recall/f-score)
WSJ-trained GENIA-retrained
MST 613 425
KSDEP 136 111
NO-RERANK 2049 1372
RERANK 2806 2125
BERKELEY 1118 1198
STANFORD 1411 1645
ENJU 1447 727
ENJU-GENIA 821
Table 3: Parsing time (sec.)
equally well in a real application. Among these
parsers, RERANK performed slightly better than the
other parsers, although the difference in the f-score
is small, while it requires much higher parsing cost.
When the parsers are retrained with GENIA (Ta-
ble 2), the accuracy increases significantly, demon-
strating that the WSJ-trained parsers are not suffi-
ciently domain-independent, and that domain adap-
tation is effective. It is an important observation that
the improvements by domain adaptation are larger
than the differences among the parsers in the pre-
vious experiment. Nevertheless, not all parsers had
their performance improved upon retraining. Parser
CoNLL PTB HD SD PAS
baseline 424
MST 809 N/A N/A N/A N/A
KSDEP 864 N/A N/A N/A N/A
NO-RERANK 851 4772 882 795 N/A
RERANK 849 4676 881 778 N/A
BERKELEY 869 4665 895 804 N/A
STANFORD 847 4614 886 799 N/A
ENJU 832 4611 884 789 1005
ENJU-GENIA 874 4624 895 783 1020
Table 4: Evaluation time (sec.)
retraining yielded only slight improvements for
RERANK, BERKELEY, and STANFORD, while larger
improvements were observed for MST, KSDEP, NO-
RERANK, and ENJU. Such results indicate the dif-
ferences in the portability of training methods. A
large improvement from ENJU to ENJU-GENIA shows
the effectiveness of the specifically designed do-
main adaptation method, suggesting that the other
parsers might also benefit from more sophisticated
approaches for domain adaptation.
While the accuracy level of PPI extraction is
the similar for the different parsers, parsing speed
51
RERANK ENJU
CoNLL HD SD CoNLL HD SD PAS
KSDEP CoNLL 58.5 (+0.2) 57.1 (?1.2) 58.4 (+0.1) 58.5 (+0.2) 58.0 (?0.3) 59.1 (+0.8) 59.0 (+0.7)
RERANK CoNLL 56.7 (+0.1) 57.1 (+0.4) 58.3 (+1.6) 57.3 (+0.7) 58.7 (+2.1) 59.5 (+2.3)
HD 56.8 (+0.1) 57.2 (+0.5) 56.5 (+0.5) 56.8 (+0.2) 57.6 (+0.4)
SD 58.3 (+1.6) 58.3 (+1.6) 56.9 (+0.2) 58.6 (+1.4)
ENJU CoNLL 57.0 (+0.3) 57.2 (+0.5) 58.4 (+1.2)
HD 57.1 (+0.5) 58.1 (+0.9)
SD 58.3 (+1.1)
Table 5: Results of parser/representation ensemble (f-score)
differs significantly. The dependency parsers are
much faster than the other parsers, while the phrase
structure parsers are relatively slower, and the deep
parsers are in between. It is noteworthy that the
dependency parsers achieved comparable accuracy
with the other parsers, while they are more efficient.
The experimental results also demonstrate that
PTB is significantly worse than the other represen-
tations with respect to cost for training/testing and
contributions to accuracy improvements. The con-
version from PTB to dependency-based representa-
tions is therefore desirable for this task, although it
is possible that better results might be obtained with
PTB if a different feature extraction mechanism is
used. Dependency-based representations are com-
petitive, while CoNLL seems superior to HD and SD
in spite of the imperfect conversion from PTB to
CoNLL. This might be a reason for the high per-
formances of the dependency parsers that directly
compute CoNLL dependencies. The results for ENJU-
CoNLL and ENJU-PAS show that PAS contributes to a
larger accuracy improvement, although this does not
necessarily mean the superiority of PAS, because two
imperfect conversions, i.e., PAS-to-PTB and PTB-to-
CoNLL, are applied for creating CoNLL.
4.3 Parser ensemble results
Table 5 shows the accuracy obtained with ensembles
of two parsers/representations (except the PTB for-
mat). Bracketed figures denote improvements from
the accuracy with a single parser/representation.
The results show that the task accuracy significantly
improves by parser/representation ensemble. Inter-
estingly, the accuracy improvements are observed
even for ensembles of different representations from
the same parser. This indicates that a single parse
representation is insufficient for expressing the true
Bag-of-words features 48.2/54.9/51.1
Yakushiji et al (2005) 33.7/33.1/33.4
Mitsumori et al (2006) 54.2/42.6/47.7
Giuliano et al (2006) 60.9/57.2/59.0
S?tre et al (2007) 64.3/44.1/52.0
This paper 54.9/65.5/59.5
Table 6: Comparison with previous results on PPI extrac-
tion (precision/recall/f-score)
potential of a parser. Effectiveness of the parser en-
semble is also attested by the fact that it resulted in
larger improvements. Further investigation of the
sources of these improvements will illustrate the ad-
vantages and disadvantages of these parsers and rep-
resentations, leading us to better parsing models and
a better design for parse representations.
4.4 Comparison with previous results on PPI
extraction
PPI extraction experiments on AImed have been re-
ported repeatedly, although the figures cannot be
compared directly because of the differences in data
preprocessing and the number of target protein pairs
(S?tre et al, 2007). Table 6 compares our best re-
sult with previously reported accuracy figures. Giu-
liano et al (2006) and Mitsumori et al (2006) do
not rely on syntactic parsing, while the former ap-
plied SVMs with kernels on surface strings and the
latter is similar to our baseline method. Bunescu and
Mooney (2005) applied SVMs with subsequence
kernels to the same task, although they provided
only a precision-recall graph, and its f-score is
around 50. Since we did not run experiments on
protein-pair-wise cross validation, our system can-
not be compared directly to the results reported
by Erkan et al (2007) and Katrenko and Adriaans
52
(2006), while S?tre et al (2007) presented better re-
sults than theirs in the same evaluation criterion.
5 Related Work
Though the evaluation of syntactic parsers has been
a major concern in the parsing community, and a
couple of works have recently presented the com-
parison of parsers based on different frameworks,
their methods were based on the comparison of the
parsing accuracy in terms of a certain intermediate
parse representation (Ringger et al, 2004; Kaplan
et al, 2004; Briscoe and Carroll, 2006; Clark and
Curran, 2007; Miyao et al, 2007; Clegg and Shep-
herd, 2007; Pyysalo et al, 2007b; Pyysalo et al,
2007a; Sagae et al, 2008). Such evaluation requires
gold standard data in an intermediate representation.
However, it has been argued that the conversion of
parsing results into an intermediate representation is
difficult and far from perfect.
The relationship between parsing accuracy and
task accuracy has been obscure for many years.
Quirk and Corston-Oliver (2006) investigated the
impact of parsing accuracy on statistical MT. How-
ever, this work was only concerned with a single de-
pendency parser, and did not focus on parsers based
on different frameworks.
6 Conclusion and Future Work
We have presented our attempts to evaluate syntac-
tic parsers and their representations that are based on
different frameworks; dependency parsing, phrase
structure parsing, or deep parsing. The basic idea
is to measure the accuracy improvements of the
PPI extraction task by incorporating the parser out-
put as statistical features of a machine learning
classifier. Experiments showed that state-of-the-
art parsers attain accuracy levels that are on par
with each other, while parsing speed differs sig-
nificantly. We also found that accuracy improve-
ments vary when parsers are retrained with domain-
specific data, indicating the importance of domain
adaptation and the differences in the portability of
parser training methods.
Although we restricted ourselves to parsers
trainable with Penn Treebank-style treebanks, our
methodology can be applied to any English parsers.
Candidates include RASP (Briscoe and Carroll,
2006), the C&C parser (Clark and Curran, 2004),
the XLE parser (Kaplan et al, 2004), MINIPAR
(Lin, 1998), and Link Parser (Sleator and Temperley,
1993; Pyysalo et al, 2006), but the domain adapta-
tion of these parsers is not straightforward. It is also
possible to evaluate unsupervised parsers, which is
attractive since evaluation of such parsers with gold-
standard data is extremely problematic.
A major drawback of our methodology is that
the evaluation is indirect and the results depend
on a selected task and its settings. This indicates
that different results might be obtained with other
tasks. Hence, we cannot conclude the superiority of
parsers/representations only with our results. In or-
der to obtain general ideas on parser performance,
experiments on other tasks are indispensable.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Grant-in-Aid for Young Scientists (MEXT, Japan).
References
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
T. Briscoe and J. Carroll. 2006. Evaluating the accu-
racy of an unlexicalized statistical parser on the PARC
DepBank. In COLING/ACL 2006 Poster Session.
R. Bunescu and R. J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL 2004, pages 439?446.
R. C. Bunescu and R. J. Mooney. 2005. Subsequence
kernels for relation extraction. In NIPS 2005.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
ACL 2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL-2000, pages 132?139.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In 42nd ACL.
S. Clark and J. R. Curran. 2007. Formalism-independent
parser evaluation with CCG and DepBank. In ACL
2007.
A. B. Clegg and A. J. Shepherd. 2007. Benchmark-
ing natural-language parsers for biological applica-
tions using dependency graphs. BMC Bioinformatics,
8:24.
53
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL 2002.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In 35th ACL.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COLING
1996.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interac-
tion sentences using dependency parsing. In EMNLP
2007.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP 2001, pages 167?202.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In EACL 2006.
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating im-
pact of re-training a lexical disambiguation model on
domain adaptation of an HPSG parser. In IWPT 2007.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA 2007.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell, and
A. Vasserman. 2004. Speed and accuracy in shallow
and deep stochastic parsing. In HLT/NAACL?04.
S. Katrenko and P. Adriaans. 2006. Learning relations
from biomedical corpora using dependency trees. In
KDECB, pages 61?80.
J.-D. Kim, T. Ohta, Y. Teteisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19:i180?182.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL 2003.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In LREC Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL 2005.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In EACL
2006.
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with SVM. IEICE - Trans.
Inf. Syst., E89-D(8):2464?2466.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Y. Miyao, K. Sagae, and J. Tsujii. 2007. Towards
framework-independent evaluation of deep linguistic
parsers. In Grammar Engineering across Frameworks
2007, pages 238?258.
A. Moschitti. 2006. Making tree kernels practical for
natural language processing. In EACL 2006.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In ACL 2005.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL 2007.
S. Pyysalo, T. Salakoski, S. Aubin, and A. Nazarenko.
2006. Lexical adaptation of link grammar to the
biomedical sublanguage: a comparative evaluation of
three approaches. BMC Bioinformatics, 7(Suppl. 3).
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007a. BioInfer: a cor-
pus for information extraction in the biomedical do-
main. BMC Bioinformatics, 8(50).
S. Pyysalo, F. Ginter, V. Laippala, K. Haverinen, J. Hei-
monen, and T. Salakoski. 2007b. On the unification of
syntactic annotations under the Stanford dependency
scheme: A case study on BioInfer and GENIA. In
BioNLP 2007, pages 25?32.
C. Quirk and S. Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical ma-
chine translation. In EMNLP 2006.
E. K. Ringger, R. C. Moore, E. Charniak, L. Vander-
wende, and H. Suzuki. 2004. Using the Penn Tree-
bank to evaluate non-treebank parsers. In LREC 2004.
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic
features for protein-protein interaction extraction. In
LBM 2007 short papers.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In EMNLP-CoNLL 2007.
K. Sagae, Y. Miyao, T. Matsuzaki, and J. Tsujii. 2008.
Challenges in mapping of syntactic representations
for framework-independent parser evaluation. In the
Workshop on Automated Syntatic Annotations for In-
teroperable Language Resources.
D. D. Sleator and D. Temperley. 1993. Parsing English
with a Link Grammar. In 3rd IWPT.
Y. Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. Develop-
ing a robust part-of-speech tagger for biomedical text.
In 10th Panhellenic Conference on Informatics.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In First International
Symposium on Semantic Mining in Biomedicine.
54
Resource sharing among HPSG and LTAG communities
by a method of grammar conversion from FB-LTAG to HPSG
Naoki Yoshinaga Yusuke Miyao
Department of Information Science, Graduate school of Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
fyoshinag, yusukeg@is.s.u-tokyo.ac.jp
Kentaro Torisawa
School of Information Science, Japan Advanced Institute of Science and Technology
Asahidai 1-1, Tatsunokuchi-cho, Noumi-gun, Ishikawa, 923-1292, Japan
Information and Human Behavior, PRESTO, Japan Science and Technology Corporation
Kawaguchi Hon-cho 4-1-8, Kawaguchi-shi, Saitama, 332-0012, Japan
torisawa@jaist.ac.jp
Jun?ichi Tsujii
Department of Computer Science, Graduate school of Information Science and Technology, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
CREST, JST (Japan Science and Technology Corporation)
Kawaguchi Hon-cho 4-1-8, Kawaguchi-shi, Saitama, 332-0012, Japan
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper describes the RenTAL sys-
tem, which enables sharing resources
in LTAG and HPSG formalisms by a
method of grammar conversion from
an FB-LTAG grammar to a strongly
equivalent HPSG-style grammar. The
system is applied to the latest version
of the XTAG English grammar. Ex-
perimental results show that the ob-
tained HPSG-style grammar success-
fully worked with an HPSG parser, and
achieved a drastic speed-up against an
LTAG parser. This system enables to
share not only grammars and lexicons
but also parsing techniques.
1 Introduction
This paper describes an approach for shar-
ing resources in various grammar formalisms
such as Feature-Based Lexicalized Tree Adjoin-
ing Grammar (FB-LTAG1) (Vijay-Shanker, 1987;
Vijay-Shanker and Joshi, 1988) and Head-Driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994) by a method of grammar conver-
sion. The RenTAL system automatically converts
an FB-LTAG grammar into a strongly equiva-
lent HPSG-style grammar (Yoshinaga and Miyao,
2001). Strong equivalence means that both gram-
mars generate exactly equivalent parse results,
and that we can share the LTAG grammars and
lexicons in HPSG applications. Our system can
reduce considerable workload to develop a huge
resource (grammars and lexicons) from scratch.
Our concern is, however, not limited to the
sharing of grammars and lexicons. Strongly
equivalent grammars enable the sharing of
ideas developed in each formalism. There
have been many studies on parsing tech-
niques (Poller and Becker, 1998; Flickinger et
al., 2000), ones on disambiguation models (Chi-
ang, 2000; Kanayama et al, 2000), and ones
on programming/grammar-development environ-
1In this paper, we use the term LTAG to refer to FB-
LTAG, if not confusing.
LTAG Resources
Grammar: 
Elementary tree 
templates
Lexicon
Type hierarchy 
extractor
Tree 
converter
Lexicon 
converter
RenTAL System
HPSG Resources
Grammar: 
Lexical entry 
templates
Lexicon
LTAG parsers HPSG parsers
Derivation trees Parse trees
Derivation 
translator
LTAG-based application
HPSG-based application
Figure 1: The RenTAL System: Overview
ment (Sarkar and Wintner, 1999; Doran et al,
2000; Makino et al, 1998). These works are re-
stricted to each closed community, and the rela-
tion between them is not well discussed. Investi-
gating the relation will be apparently valuable for
both communities.
In this paper, we show that the strongly equiv-
alent grammars enable the sharing of ?parsing
techniques?, which are dependent on each com-
putational framework and have never been shared
among HPSG and LTAG communities. We ap-
ply our system to the latest version of the XTAG
English grammar (The XTAG Research Group,
2001), which is a large-scale FB-LTAG gram-
mar. A parsing experiment shows that an efficient
HPSG parser with the obtained grammar achieved
a significant speed-up against an existing LTAG
parser (Yoshinaga et al, 2001). This result im-
plies that parsing techniques for HPSG are also
beneficial for LTAG parsing. We can say that the
grammar conversion enables us to share HPSG
parsing techniques in LTAG parsing.
Figure 1 depicts a brief sketch of the RenTAL
system. The system consists of the following four
modules: Tree converter, Type hierarchy extrac-
tor, Lexicon converter and Derivation translator.
The tree converter module is a core module of the
system, which is an implementation of the gram-
mar conversion algorithm given in Section 3. The
type hierarchy extractor module extracts the sym-
bols of the node, features, and feature values from
the LTAG elementary tree templates and lexicon,
and construct the type hierarchy from them. The
lexicon converter module converts LTAG elemen-
tary tree templates into HPSG lexical entries. The
derivation translator module takes HPSG parse
S
NP VP
V
run
VP
VP
V
can
*
NP
N
We
?1
?2
?1
anchor
foot node
*
substitution node
Initial tree
Auxiliary tree
Figure 2: Elementary trees
trees, and map them to LTAG derivation trees. All
modules other than the last one are related to the
conversion process from LTAG into HPSG, and
the last one enables to obtain LTAG analysis from
the obtained HPSG analysis.
Tateisi et al also translated LTAG into
HPSG (Tateisi et al, 1998). However, their
method depended on translator?s intuitive analy-
sis of the original grammar. Thus the transla-
tion was manual and grammar dependent. The
manual translation demanded considerable efforts
from the translator, and obscures the equiva-
lence between the original and obtained gram-
mars. Other works (Kasper et al, 1995; Becker
and Lopez, 2000) convert HPSG grammars into
LTAG grammars. However, given the greater ex-
pressive power of HPSG, it is impossible to con-
vert an arbitrary HPSG grammar into an LTAG
grammar. Therefore, a conversion from HPSG
into LTAG often requires some restrictions on the
HPSG grammar to suppress its generative capac-
ity. Thus, the conversion loses the equivalence of
the grammars, and we cannot gain the above ad-
vantages.
Section 2 reviews the source and the tar-
get grammar formalisms of the conversion algo-
rithm. Section 3 describes the conversion algo-
rithm which the core module in the RenTAL sys-
tem uses. Section 4 presents the evaluation of
the RenTAL system through experiments with the
XTAG English grammar. Section 5 concludes this
study and addresses future works.
2 Background
2.1 Feature-Based Lexicalized Tree
Adjoining Grammar (FB-LTAG)
LTAG (Schabes et al, 1988) is a grammar formal-
ism that provides syntactic analyses for a sentence
by composing elementary trees with two opera-
Arg :
we
can run
ID grammar rule
unify
Sym : NP
Arg : 
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :
Sym :
Arg :
2
3
2
unify
3
unify
ID grammar rule
we
can run
Sym : NP
Arg :
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :   NP
Sym : 
Arg :
Arg :
1
1
|
2
Arg :
2
unify
we can run
Sym : NP
Arg :
Sym : VP
Arg :   VP
Sym : VP
Arg :   NP
Arg :   NP
Arg :
Figure 6: Parsing with an HPSG grammar
S
NP
VP
V
run
NP
N
We
substitution
?1
?2
S
NP VP
V
run
N
We
Figure 3: Substitution
VP
VP
V
can
*
adjunction
?1
S
NP VP
V
run
N
We
S
NP
VP
VP
V
can
N
We
V
run
Figure 4: Adjunction
tions called substitution and adjunction. Elemen-
tary trees are classified into two types, initial trees
and auxiliary trees (Figure 2). An elementary tree
has at least one leaf node labeled with a terminal
symbol called an anchor (marked with ). In an
auxiliary tree, one leaf node is labeled with the
same symbol as the root node and is specially
marked as a foot node (marked with ). In an el-
ementary tree, leaf nodes with the exception of
anchors and the foot node are called substitution
nodes (marked with #).
Substitution replaces a substitution node with
another initial tree (Figure 3). Adjunction grafts
an auxiliary tree with the root node and foot
node labeled x onto an internal node of another
tree with the same symbol x (Figure 4). FB-
LTAG (Vijay-Shanker, 1987; Vijay-Shanker and
Joshi, 1988) is an extension of the LTAG formal-
ism. In FB-LTAG, each node in the elementary
trees has a feature structure, containing grammat-
ical constraints on the node. Figure 5 shows a
result of LTAG analysis, which is described not
derived tree
?2
?1?1
derivation tree
S
NP VP
VP
V
can
N
We
V
run
Figure 5: Derived trees and derivation trees
only by derived trees (i.e., parse trees) but also by
derivation trees. A derivation tree is a structural
description in LTAG and represents the history of
combinations of elementary trees.
There are several grammars developed in the
FB-LTAG formalism, including the XTAG En-
glish grammar, a large-scale grammar for En-
glish (The XTAG Research Group, 2001). The
XTAG group (Doran et al, 2000) at the Univer-
sity of Pennsylvania is also developing Korean,
Chinese, and Hindi grammars. Development of
a large-scale French grammar (Abeille? and Can-
dito, 2000) has also started at the University of
Pennsylvania and is expanded at University of
Paris 7.
2.2 Head-Driven Phrase Structure
Grammar (HPSG)
An HPSG grammar consists of lexical entries and
ID grammar rules, each of which is described
with typed feature structures (Carpenter, 1992). A
lexical entry for each word expresses the charac-
teristics of the word, such as the subcategorization
frame and the grammatical category. An ID gram-
mar rule represents a relation between a mother
and its daughters, and is independent of lexical
characteristics. Figure 6 illustrates an example of
bottom-up parsing with an HPSG grammar. First,
lexical entries for ?can? and ?run? are unified re-
spectively with the daughter feature structures of
Canonical elementary trees Non-canonical elementary trees
think
S
NP VP
V S
*
it
S
NP VP
N
V
VP
V
?
is
Non-anchored subtree
S
NP VP
V PP
P NP
for
look
PP S
P NP
a) Exception for Condition 1
b) Exception for Condition 2
Figure 7: A canonical elementary tree and exceptions
an ID grammar rule. The feature structure of the
mother node is determined as a result of these uni-
fications. The center of Figure 6 shows a rule ap-
plication to ?can run? and ?we?.
There are a variety of works on efficient pars-
ing with HPSG, which allow the use of HPSG-
based processing in practical application con-
texts (Flickinger et al, 2000). Stanford Univer-
sity is developing the English Resource Gram-
mar, an HPSG grammar for English, as a part
of the Linguistic Grammars Online (LinGO)
project (Flickinger, 2000). In practical con-
text, German, English, and Japanese HPSG-based
grammars are developed and used in the Verb-
mobil project (Kay et al, 1994). Our group
has developed a wide-coverage HPSG grammar
for Japanese (Mitsuishi et al, 1998), which is
used in a high-accuracy Japanese dependency an-
alyzer (Kanayama et al, 2000).
3 Grammar conversion
The grammar conversion from LTAG to
HPSG (Yoshinaga and Miyao, 2001) is the
core portion of the RenTAL system. The
conversion algorithm consists of:
1. Conversion of canonical elementary trees to
HPSG lexical entries.
2. Definition of ID grammar rules to emulate
substitution and adjunction.
3. Conversion of non-canonical elementary
trees to canonical ones.
The left-hand side of Figure 7 shows a canoni-
cal elementary tree, which satisfies the following
conditions:
Condition 1 A tree must have only one anchor.
Sym:
Arg:
Sym  :
Leaf :
Dir    :
right left
,
Foot?:
+
_
*
think
V S
VP
S
NP
V
think:
S
VP S
NP
foot node
anchor
trunk
*
substitution node
Sym  :
Leaf :
Dir    :
Foot?:
Figure 8: A conversion from a canonical elemen-
tary tree into an HPSG lexical entry
mother

Sym : 1
Arg : 2






h
Sym : 3
Arg : h i
i
substitution node
X
X
X
X
X
2
4
Arg :
*
2
4
Sym : 1
Leaf : 3
Dir : left
Foot? :  
3
5
j 2
+
3
5
trunk node
Figure 9: Left substitution rule
Condition 2 All branchings in a tree must con-
tain trunk nodes.
Trunk nodes are nodes on a trunk, which is a path
from an anchor to the root node (the thick lines in
Figure 7) (Kasper et al, 1995). Condition 1 guar-
antees that a canonical elementary tree has only
one trunk, and Condition 2 guarantees that each
branching consists of a trunk node, a leaf node,
and their mother (also a trunk node). The right-
hand side of Figure 7 shows elementary trees vi-
olating the conditions.
Canonical elementary trees can be directly con-
verted to HPSG lexical entries by regarding each
leaf node as a subcategorization element of the
anchor, and by encoding them into a list. Fig-
ure 8 shows an example of the conversion. By
following the trunk from the anchor ?think? to the
mother

Sym : 1
Arg : 2  3







Sym : 4
Arg : 3

foot node
P
P
P
P
P
2
4
Arg :
*
2
4
Sym : 1
Leaf : 4
Dir : left
Foot? : +
3
5
j 2
+
3
5
trunk node
 append
Figure 10: Left adjunction rule
root node labeled S, we store each branching in
a list. As shown in Figure 8, each branching is
specified by a leaf node and the mother node. A
feature Sym represents the non-terminal symbol
of the mother node. Features Leaf, Dir, Foot?
represent the leaf node; the non-terminal symbol,
the direction (on which side of the trunk node the
leaf node is), and the type (whether a foot node or
a substitution node), respectively.
Figures 9 and 10 show ID grammar rules to em-
ulate substitution and adjunction. These grammar
rules are independent of the original grammar be-
cause they don?t specify any characteristics spe-
cific to the original grammar.
In the substitution rule, the Sym feature of the
substitution node must have the value of the Leaf
feature 3 of the trunk node. The Arg feature of
the substitution node must be a null list, because
the substitution node must be unified only with
the node corresponding to the root node of the ini-
tial tree. The substitution rule percolates the tail
elements 2 of the Arg feature of a trunk node to
the mother in order to continue constructing the
tree.
In the adjunction rule, the Sym feature of a
foot node must have the same value as the Leaf
feature 4 . The value of the Arg feature of the
mother node is a concatenation list of both Arg
features 2 and 3 of its daughters because we
first construct the tree corresponding to the ad-
joining tree and next continue constructing the
tree corresponding to the adjoined tree. The value
?+? or ? ? of the Foot? feature explicitly de-
termines whether the next rule application is the
adjunction rule or the substitution rule.
Figure 11 shows an instance of rule applica-
tions. The thick line indicates the adjoined tree
(1) and the dashed line indicates the adjoining
Sym : NP
Arg : 
Sym : S
Arg : 
Sym : S
?1
2
1
5
3
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
2
1
Sym :        VP
Leaf :        S 
Dir :  right
Foot? :  +
Sym : NP
Arg : 
Sym : NP
Arg : 
Sym : V
Sym : S
Sym : VP
Sym : V
think:
loves:
you
? A
*
? B
4
4
7
7
8
6
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
5
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
2
1
5
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
2
1
3
6
8
Sym :        S
Leaf :        NP
Dir :  left
Foot? :  
3
6
Sym :        S
Leaf :        NP
Dir :  left
Foot? : 
,
5
Sym :        S
Leaf :        NP 
Dir :  left
Foot? :  
2
1
,
4
9
9
?1
he
?2
?4
?3
Arg :
Arg :
Arg : Arg :
Arg :
what
? C
Figure 11: An example of rule applications
S
NP
VP
V PP
P NP
for
S
NP VP
V
P NP
for
look look
cut off
PP
look_for
PP
look_for
identifier
Figure 12: Division of a multi-anchored elemen-
tary tree into single-anchored trees
tree (2). The adjunction rule is applied to con-
struct the branching marked with ?, where ?think?
takes as an argument a node whose Sym feature?s
value is S. By applying the adjunction rule, the
Arg feature of the mother node (B) becomes a
concatenation list of both Arg features of 1 ( 8 )
and 1 ( 5 ). Note that when the construction of
1 is completed, the Arg feature of the trunk node
(C) will be its former state (A). We can continue
constructing 1 as if nothing had happened.
Multi-anchored elementary trees, which violate
Condition 1, are divided into multiple canonical
elementary trees. We call the cutting nodes in the
divided trees cut-off nodes (Figure 12). Note that
a cut-off node is marked by an identifier to pre-
serve a co-occurrence relation among the multiple
anchors. Figure 12 shows an example of the con-
version of a multi-anchored elementary tree for a
compound expression ?look for?. We first select
an anchor ?look? as the syntactic head, and tra-
verse the tree along the trunk from the root node
S to the anchor ?look?. We then cut off the multi-
PAd
P
P
substitution
all candidate initial trees 
for substitution
, ?
non-anchored subtree
multi-anchored trees without non-anchored subtrees
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
breaking points
on
tonext
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
it
S
NP VP
N
V
is
VP
V
?
PP S
P NP
, ?
Ad
P
on
tonext
Figure 13: Combination of a non-anchored subtree into anchored trees
anchored elementary tree at the node PP, and cut-
off nodes PP in resulting single-anchored trees are
marked by an identifier look for.
Non-canonical elementary trees violating Con-
dition 2 have a non-anchored subtree which is
a subtree of depth 1 or above with no anchor.
A non-anchored subtree is converted into multi-
anchored trees by substituting the deepest node
(Figure 13). Substituted nodes are marked as
breaking points to remember that the nodes orig-
inate from the substitution nodes. In the resulting
trees, all subtrees are anchored so that we can ap-
ply the above conversion algorithms. Figure 13
shows a conversion of a non-canonical elemen-
tary tree for it-cleft. A substitution node P in the
non-anchored subtree is selected, and is substi-
tuted by each initial tree. The substituted node
P in resulting multi-anchored trees are marked as
breaking points.
The above algorithm gives the conversion of
LTAG, and it can be easily extended to handle an
FB-LTAG grammar by merely storing a feature
structure of each node into the Sym feature and
Leaf feature together with the non-terminal sym-
bol. Feature structure unification is executed by
ID grammar rules.
The strong equivalence is assured because only
substitution/adjunction operations performed in
LTAG are performed with the obtained HPSG-
style grammar. This is because each element
in the Arg feature selects only feature structures
corresponding to trees which can substitute/be
adjoined by each leaf node of an elementary
tree. By following a history of rule applications,
each combination of elementary trees in LTAG
derivation trees can be readily recovered. The
strong equivalence holds also for conversion of
non-canonical elementary trees. For trees violat-
ing Condition 1, we can distinguish the cut-off
Table 1: The classification of elementary tree
templates in the XTAG English grammar (LTAG)
and converted lexical entry templates correspond-
ing to them (HPSG): A: canonical elementary
trees, B: elementary trees violating only Condi-
tion 1, C: elementary trees violating only Condi-
tion 2, D: elementary trees violating both condi-
tions
Grammar A B C D Total
LTAG 326 764 54 50 1,194
HPSG 326 1,992 1,083 2,474 5,875
nodes from the substitution nodes owing to iden-
tifiers, which recover the co-occurrence relation
in the original elementary trees between the di-
vided trees. For trees violating Condition 2, we
can identify substitution nodes in a combined tree
because they are marked as breaking points, and
we can consider the combined tree as two trees in
the LTAG derivation.
4 Experiments
The RenTAL system is implemented in LiL-
FeS (Makino et al, 1998)2. LiLFeS is one of
the fastest inference engines for processing fea-
ture structure logic, and efficient HPSG parsers
have already been built on this system (Nishida
et al, 1999; Torisawa et al, 2000). We ap-
plied our system to the XTAG English gram-
mar (The XTAG Research Group, 2001)3, which
is a large-scale FB-LTAG grammar for English.
2The RenTAL system is available at:
http://www-tsujii.is.s.u-tokyo.ac.jp/rental/
3We used the grammar attached to the latest distribution
of an LTAG parser which we used for the parsing experi-
ment. The parser is available at:
ftp://ftp.cis.upenn.edu/pub/xtag/lem/lem-0.13.0.i686.tgz
Table 2: Parsing performance with the XTAG En-
glish grammar for the ATIS corpus.
Parser Parse Time (sec.)
lem 19.64
TNT 0.77
The XTAG English grammar consists of 1,194 4
elementary tree templates and around 45,000 lex-
ical items5. We successfully converted all the
elementary tree templates in the XTAG English
grammar to HPSG lexical entry templates. Ta-
ble 1 shows the classifications of elementary tree
templates of the XTAG English grammar, ac-
cording to the conditions we introduced in Sec-
tion 3, and also shows the number of correspond-
ing HPSG lexical entry templates. Conversion
took about 25 minutes CPU time on a 700 Mhz
Pentium III Xeon with four gigabytes main mem-
ory.
The original and the obtained grammar gener-
ated exactly the same number of derivation trees
in the parsing experiment with 457 sentences
from the ATIS corpus (Marcus et al, 1994)6 (the
average length is 6.32 words). This result empir-
ically attested the strong equivalence of our algo-
rithm.
Table 2 shows the average parsing time with
the LTAG and HPSG parsers. In Table 2, lem
refers to the LTAG parser (Sarkar et al, 2000),
ANSI C implementation of the two-phase pars-
ing algorithm that performs the head corner pars-
ing (van Noord, 1994) without features (phase
1), and then executes feature unification (phase
2). TNT refers to the HPSG parser (Torisawa et
al., 2000), C++ implementation of the two-phase
parsing algorithm that performs filtering with a
compiled CFG (phase 1) and then executes fea-
ture unification (phase 2). Table 2 clearly shows
that the HPSG parser is significantly faster than
the LTAG parser. This result implies that parsing
techniques for HPSG are also beneficial for LTAG
4We eliminated 32 elementary trees because the LTAG
parser cannot produce correct derivation trees with them.
5These lexical items are a subset of the original XTAG
English grammar distribution.
6We eliminated 59 sentences because of a time-out of
the parsers, and 61 sentences because the LTAG parser does
not produce correct derivation trees because of bugs in its
preprocessor.
parsing. We can say that the grammar conversion
enables us to share HPSG parsing techniques in
LTAG parsing. Another paper (Yoshinaga et al,
2001) describes the detailed analysis on the factor
of the difference of parsing performance.
5 Conclusion
We described the RenTAL system, a grammar
converter from FB-LTAG to HPSG. The grammar
conversion guarantees the strong equivalence, and
hence we can obtain an HPSG-style grammar
equivalent to existing LTAG grammars. Experi-
mental result showed that the system enabled to
share not only LTAG grammars, but also HPSG
parsing techniques. This system will enable a
variety of resource sharing such as the sharing
of the programming/grammar-development envi-
ronment (Makino et al, 1998; Sarkar and Wint-
ner, 1999) and grammar extraction methods from
bracketed corpora (Xia, 1999; Chen and Vijay-
Shanker, 2000; Neumann, 1998). Although our
system connects only FB-LTAG and HPSG, we
believe that our approach can be extended to other
formalisms such as Lexical-Functional Gram-
mar (Kaplan and Bresnan, 1982).
Acknowledgment The authors are indebted
to Mr. Anoop Sarkar for his help in using his
parser in our experiment. The authors would like
to thank anonymous reviewers for their valuable
comments and criticisms on this paper.
References
Anne Abeille? and Marie-He?le`ne Candito. 2000.
FTAG: A Lexicalized Tree Adjoining Grammar for
French. In Anne Abeille? and Owen Rambow, edi-
tors, Tree Adjoining Grammars: Formal, Computa-
tional and Linguistic Aspects, pages 305?329. CSLI
publications.
Tilman Becker and Patrice Lopez. 2000. Adapting
HPSG-to-TAG compilation to wide-coverage gram-
mars. In Proc. of TAG+5, pages 47?54.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge University Press.
John Chen and K. Vijay-Shanker. 2000. Automated
extraction of TAGs from the Penn Treebank. In
Proc. of IWPT 2000.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar.
In Proc. of ACL 2000, pages 456?463.
Christy Doran, Beth Ann Hockey, Anoop Sarkar,
B. Srinivas, and Fei Xia. 2000. Evolution of the
XTAG system. In Anne Abeille? and Owen Ram-
bow, editors, Tree Adjoining Grammars: Formal,
Computational and Linguistic Aspects, pages 371?
403. CSLI publications.
Dan Flickinger, Stephen Oepen, Jun?ichi Tsujii, and
Hans Uszkoreit, editors. 2000. Natural Language
Engineering ? Special Issue on Efficient Processing
with HPSG: Methods, Systems, Evaluation. Cam-
bridge University Press.
Dan Flickinger. 2000. On building a more effi-
cient grammar by exploiting types. Natural Lan-
guage Engineering ? Special Issue on Efficient Pro-
cessing with HPSG: Methods, Systems, Evaluation,
6(1):15?28.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsu-
isi, and Jun?ichi Tsujii. 2000. Hybrid Japanese
parser with hand-crafted grammar and statistics. In
Proc. of COLING 2000, pages 411?417.
Ronald Kaplan and Joan Bresnan. 1982. Lexical-
Functional Grammar: A formal system for gram-
matical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
pages 173?281. The MIT Press.
Robert Kasper, Bernd Kiefer, Klaus Netter, and
K. Vijay-Shanker. 1995. Compilation of HPSG to
TAG. In Proc. of ACL ?94, pages 92?99.
M. Kay, J. Gawron, and P. Norvig. 1994. Verbmo-
bil: A Translation System for Face-to-Face Dialog.
CSLI Publications.
Takaki Makino, Minoru Yoshida, Kentaro Torisawa,
and Jun?ichi Tsujii. 1998. LiLFeS ? towards a
practical HPSG parsers. In Proc. of COLING?ACL
?98, pages 807?811.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yutaka Mitsuishi, Kentaro Torisawa, and Jun?ichi Tsu-
jii. 1998. HPSG-style underspecified Japanese
grammar with wide coverage. In Proc. of
COLING?ACL ?98, pages 876?880.
Gu?ter Neumann. 1998. Automatic extraction of
stochastic lexcalized tree grammars from treebanks.
In Proc. of TAG+4, pages 120?123.
Kenji Nishida, Kentaro Torisawa, and Jun?ichi Tsujii.
1999. An efficient HPSG parsing algorithm with ar-
ray unification. In Proc. of NLPRS ?99, pages 144?
149.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press and CSLI Publications.
Peter Poller and Tilman Becker. 1998. Two-step TAG
parsing revisited. In Proc. of TAG+4, pages 143?
146.
Anoop Sarkar and Shuly Wintner. 1999. Typing as a
means for validating feature structures. In Proc.of
CLIN ?99, pages 159?167.
Anoop Sarkar, Fei Xia, and Aravind Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proc. of COL-
ING 2000, pages 37?42.
Yves Schabes, Anne Abeille, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? gram-
mars: Application to Tree Adjoining Grammars. In
Proc. of 12th COLING ?92, pages 578?583.
Yuka Tateisi, Kentaro Torisawa, Yusuke Miyao, and
Jun?ichi Tsujii. 1998. Translating the XTAG En-
glish grammar to HPSG. In Proc. of TAG+4, pages
172?175.
The XTAG Research Group. 2001. A Lex-
icalized Tree Adjoining Grammar for English.
http://www.cis.upenn.edu/?xtag/.
Kentaro Torisawa, Kenji Nishida, Yusuke Miyao, and
Jun?ichi Tsujii. 2000. An HPSG parser with CFG
filtering. Natural Language Engineering ? Special
Issue on Efficient Processing with HPSG: Methods,
Systems, Evaluation, 6(1):63?80.
Gertjan van Noord. 1994. Head corner parsing for
TAG. Computational Intelligence, 10(4):525?534.
K. Vijay-Shanker and Aravind K. Joshi. 1988. Fea-
ture structures based Tree Adjoining Grammars. In
Proc. of 12th COLING ?92, pages 714?719.
K. Vijay-Shanker. 1987. A Study of Tree Adjoining
Grammars. Ph.D. thesis, Department of Computer
& Information Science, University of Pennsylvania.
Fei Xia. 1999. Extracting Tree Adjoining Grammars
from bracketed corpora. In Proc. of NLPRS ?99,
pages 398?403.
Naoki Yoshinaga and Yusuke Miyao. 2001. Grammar
conversion from FB-LTAG to HPSG. In Proc. of
ESSLLI 2001 Student Session. To appear.
Naoki Yoshinaga, Yusuke Miyao, Kentaro Torisawa,
and Jun?ichi Tsujii. 2001. Efficient LTAG parsing
using HPSG parsers. In Proc. of PACLING 2001.
To appear.
A model of syntactic disambiguation based on lexicalized grammars
Yusuke Miyao
Department of Computer Science,
University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science,
University of Tokyo
CREST, JST
(Japan Science and Technology Corporation)
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper presents a new approach to syntac-
tic disambiguation based on lexicalized gram-
mars. While existing disambiguation mod-
els decompose the probability of parsing re-
sults into that of primitive dependencies of two
words, our model selects the most probable
parsing result from a set of candidates allowed
by a lexicalized grammar. Since parsing re-
sults given by the lexicalized grammar cannot
be decomposed into independent sub-events,
we apply a maximum entropy model for fea-
ture forests, which allows probabilistic model-
ing without the independence assumption. Our
approach provides a general method of produc-
ing a consistent probabilistic model of parsing
results given by lexicalized grammars.
1 Introduction
Recent studies on the automatic extraction of lexicalized
grammars (Xia, 1999; Chen and Vijay-Shanker, 2000;
Hockenmaier and Steedman, 2002a) allow the modeling
of syntactic disambiguation based on linguistically moti-
vated grammar theories including LTAG (Chiang, 2000)
and CCG (Clark et al, 2002; Hockenmaier and Steed-
man, 2002b). However, existing models of disambigua-
tion with lexicalized grammars are a mere extension of
lexicalized probabilistic context-free grammars (LPCFG)
(Collins, 1996; Collins, 1997; Charniak, 1997), which
are based on the decomposition of parsing results into the
syntactic/semantic dependencies of two words in a sen-
tence under the assumption of independence of the de-
pendencies. While LPCFG models have proved that the
incorporation of lexical associations (i.e., dependencies
of words) significantly improves the accuracy of parsing,
this idea has been naively inherited in the recent studies
on disambiguation models of lexicalized grammars.
However, the disambiguation models of lexicalized
grammars should be totally different from that of LPCFG,
because the grammars define the relation of syntax and
semantics, and can restrict the possible structure of pars-
ing results. Parsing results cannot simply be decomposed
into primitive dependencies, because the complete struc-
ture is determined by solving the syntactic constraints
of a complete sentence. For example, when we apply
a unification-based grammar, LPCFG-like modeling re-
sults in an inconsistent probability model because the
model assigns probabilities to parsing results not allowed
by the grammar (Abney, 1997). We have only two ways
of adhering to LPCFG models: preserve the consistency
of probability models by abandoning improvements to
the lexicalized grammars using complex constraints (Chi-
ang, 2000), or ignore the inconsistency in probability
models (Clark et al, 2002).
This paper provides a new model of syntactic disam-
biguation in which lexicalized grammars can restrict the
possible structures of parsing results. Our modeling aims
at providing grounds for i) producing a consistent proba-
bilistic model of lexicalized grammars, as well as ii) eval-
uating the contributions of syntactic and semantic prefer-
ences to syntactic disambiguation. The model is com-
posed of the syntax and semantics probabilities, which
represent syntactic and semantic preferences respectively.
The syntax probability is responsible for determining the
syntactic categories chosen by words in a sentence, and
the semantics probability selects the most plausible de-
pendencies of words from candidates allowed by the syn-
tactic categories yielded by the syntax probability. Since
the sequence of syntactic categories restricts the possi-
ble structure of parsing results, the semantics probabil-
ity is a conditional probability without decomposition
into the primitive dependencies of words. Recently used
machine learning methods including maximum entropy
models (Berger et al, 1996) and support vector machines
(Vapnik, 1995) provide grounds for this type of model-
ing, because it allows various dependent features to be
incorporated into the model without the independence as-
sumption.
The above approach, however, has a serious deficiency:
a lexicalized grammar assigns exponentially many pars-
ing results because of local ambiguities in a sentence,
which is problematic in estimating the parameters of a
probability model. To cope with this, we adopted an
algorithm of maximum entropy estimation for feature
forests (Miyao and Tsujii, 2002; Geman and Johnson,
2002), which allows parameters to be efficiently esti-
mated. The algorithm enables probabilistic modeling
of complete structures, such as transition sequences in
Markov models and parse trees, without dividing them
into independent sub-events. The algorithm avoids expo-
nential explosion by representing a probabilistic event by
a packed representation of a feature space. If a complete
structure is represented with a feature forest of a tractable
size, the parameters can be efficiently estimated by dy-
namic programming.
A series of studies on parsing with wide-coverage LFG
(Johnson et al, 1999; Riezler et al, 2000; Riezler et al,
2002) have had a similar motivation to ours. Their mod-
els have also been based on a discriminative model to
select a parsing result from all candidates given by the
grammar. A significant difference is that we apply max-
imum entropy estimation for feature forests to avoid the
inherent problem with estimation: the exponential explo-
sion of parsing results given by the grammar. They as-
sumed that parsing results would be suppressed to a rea-
sonable number through using heuristic rules, or by care-
fully implementing a fully restrictive and wide-coverage
grammar, which requires a considerable amount of effort
to develop. Our contention is that this problem can be
solved in a more sophisticated way as is discussed in this
paper. Another difference is that our model is separated
into syntax and semantics probabilities, which will ben-
efit computational/linguistic investigations into the rela-
tion between syntax and semantics, and allow separate
improvements to both models.
Overall, the approach taken in this paper is different
from existing models in the following respects.
? Since it does not require the assumption of inde-
pendence, the probability model is consistent with
lexicalized grammars with complex constraints in-
cluding unification-based grammar formalism. Our
model can assign consistent probabilities to parsing
results of lexicalized grammars, while the traditional
models assign probabilities to parsing results not al-
lowed by the grammar.
? Since the syntax and semantics probabilities are sep-
arate, we can improve them individually. For exam-
ple, the syntax model can be improved by smooth-
ing using the syntactic classes of words, while the
semantics model should be able to be improved by
using semantic classes. In addition, the model can
be a starting point that allows the theory of syntax
and semantics to be evaluated through consulting an
extensive corpus.
We evaluated the validity of our model through experi-
ments on a disambiguation task of parsing the Penn Tree-
bank (Marcus et al, 1994) with an automatically acquired
LTAG grammar. To assess the contribution of the syntax
and semantics probabilities to the accuracy of parsing and
to evaluate the validity of applying maximum entropy es-
timation for feature forests, we compared three models
trained with the same training set and the same set of fea-
tures. Following the experimental results, we concluded
that i) a parser with the syntax probability only achieved
high accuracy with the lexicalized grammar, ii) the in-
corporation of preferences for lexical association through
the semantics probability resulted in significant improve-
ments, and iii) our model recorded an accuracy that was
quite close to the traditional model, which indicated the
validity of applying maximum entropy estimation for fea-
ture forests.
In what follows, we first describe the existing models
for syntactic disambiguation, and discuss problems with
them in Section 2. We then define the general form for
parsing results of lexicalized grammars, and introduce
our model in Section 3. We prove the validity of our ap-
proach through a series of experiments in Section 4.
2 Traditional models for syntactic
disambiguation
This section reviews the existing models for syntactic dis-
ambiguation from the viewpoint of representing parsing
results of lexicalized grammars. In particular, we dis-
cuss how the models incorporate syntactic/semantic pref-
erences for syntactic disambiguation. The existing stud-
ies are based on the decomposition of parsing results into
primitive lexical dependencies where syntactic/semantic
preferences are combined. This traditional scheme of
syntactic disambiguation can be problematic with lexi-
calized grammars. Throughout the discussion, we refer
to the example sentence ?What does your student want to
write??, whose parse tree is in Figure 1.
2.1 Lexicalized parse trees
The first successful work on syntactic disambiguation
was based on lexicalized probabilistic context-free gram-
mar (LPCFG) (Collins, 1997; Charniak, 1997). Although
LPCFG is not exactly classified into lexicalized grammar
formalism, we should mention these studies since they
demonstrated that lexical dependencies were essential to
improving the accuracy of parsing.
what
does
your want
to write
S
S
S
VP
VP
NP
student
Figure 1: A parse tree for ?What does your student want
to write??
what
does
your want
to write
S
S
S
VP
VP
NP
student write
want
want
want
want
student
Figure 2: A lexicalized parse tree
A lexicalized parse tree is an extension of a parse tree
that is achieved by augmenting each non-terminal with its
lexical head. There is an example of a lexicalized parse
tree in Figure 2, which is a lexicalized version of the one
in Figure 1. A lexicalized parse tree is represented by
a set of branchings in the tree1: T = {?w
h
i
, w
n
i
, r
i
?},
where w
h
i
is a head word, w
n
i
the head word of a
non-head, and r
i
a grammar rule corresponding to each
branching. LPCFG models yield a probability of the
complete parse tree T = {?w
h
i
, w
n
i
, r
i
?} by the prod-
uct of probabilities of branchings in it.
p(T ) =
?
i
p(w
h
i
, w
n
i
, r
i
|?),
where ? is a condition of the probability, which is usually
the nonterminal symbol of the mother node. Since each
branching is augmented with the lexical heads of non-
terminals in the rule, the model can capture lexical de-
pendencies, which increase the accuracy. This is because
lexical dependencies approximately represent the seman-
tic preference of a sentence. As is well known, a syntactic
structure is not accurately disambiguated only with syn-
tactic preferences, and the incorporation of approximate
1For simplicity, we have assumed parse trees are only com-
posed of binary branchings.
semantic preferences was the key to improving the accu-
racy of syntactic disambiguation.
We should note that this model has the following three
disadvantages.
1. The model fails to represent some linguistic depen-
dencies, including long-distance dependencies and
argument/modifier distinctions. Since an existing
study incorporates these relations ad hoc (Collins,
1997), they are apparently crucial in accurate dis-
ambiguation. This is also problematic for providing
a sufficient representation of semantics.
2. The model assumes the statistical independence of
branchings, which is apparently not preserved. For
example, the ambiguity of PP-attachments should be
resolved by considering three words: the modifiee of
the PP, its preposition, and the object of the PP.
3. The preferences of syntax and semantics are com-
bined in the lexical dependencies of two words,
i.e., features for syntactic preference and those for
semantic preference are not distinguished in the
model. Lexicalized grammars formalize the con-
straints of the relations between syntax and seman-
tics, but the model does not assume the existence
of such constraints. The model prevents further im-
provements to the syntax/semantics models; in addi-
tion to the linguistic analysis of the relation between
syntax and semantics.
2.2 Derivation trees
Recent work on the automatic extraction of LTAG (Xia,
1999; Chen and Vijay-Shanker, 2000) and disambigua-
tion models (Chiang, 2000) has been the first on the sta-
tistical model for syntactic disambiguation based on lexi-
calized grammars. However, the models are based on the
lexical dependencies of elementary trees, which is a sim-
ple extension of the LPCFG. That is, the models are still
based on decomposition into primitive lexical dependen-
cies.
Derivation trees, the structural description in LTAG
(Schabes et al, 1988), represent the association of lex-
ical items i.e., elementary trees. In LTAG, all syntactic
constraints of words are described in an elementary tree,
and the dependencies of elementary trees, i.e., a deriva-
tion tree, describe the semantic relations of words more
directly than lexicalized parse trees. For example, Fig-
ure 3 has a derivation tree corresponding to the parse
tree in Figure 12. The dotted lines represent substitu-
tion while the solid lines represent adjunction. We should
note that the relations captured by ad-hoc augmentation
2The nodes in a derivation tree are denoted with the names
of the elementary trees, while we have omitted details.
what does student want to
write
your
Figure 3: A derivation tree
of lexicalized parse trees, such as the distinction of argu-
ments/modifiers and unbounded dependencies (Collins,
1997), are elegantly represented in derivation trees. For-
mally, a derivation tree is represented as a set of depen-
dencies: D = {??
i
, ?
?
j
, r
i
?}, where ?
i
is an elemen-
tary tree, ?
?
i
represents a node in ?
j
where substitu-
tion/adjunction has occurred, and r
i
is a label of the ap-
plied rule, i.e., adjunction or substitution.
A probability of derivation tree D = {??
i
, ?
?
j
, r
i
?} is
generally defined as follows (Schabes et al, 1988; Chi-
ang, 2000).
p(D) =
?
i
p(?
i
|?
?
j
, r
i
)
Note that each probability on the right represents the syn-
tactic/semantic preference of a dependency of two lexical
items. We can readily see that the model is very similar
to LPCFG models.
The first problem with LPCFG is partially solved
by this model, since the dependencies not represented
in LPCFG (e.g., long-distance dependencies and ar-
gument/modifier distinctions) are elegantly represented,
while some relations (e.g., the control relation between
?want? and ?student?) are not yet represented. However,
the other two problems remain unsolved in this model.
In particular, when we apply Feature-Based LTAG (FB-
LTAG), the above probability is no longer consistent be-
cause of the non-local constraints caused by feature uni-
fication (Abney, 1997).
2.3 Dependency structures
A disambiguation model for wide-coverage CCG (Clark
et al, 2002) aims at representing deep linguistic depen-
dencies including long-distance dependencies and con-
trol relations. This model can represent all the syntac-
tic/semantic dependencies of words in a sentence. How-
ever, the statistical model is still a mere extension of
LPCFG, i.e., it is based on decomposition into primitive
lexical dependencies.
In this model, a lexicalized grammar defines the map-
ping from a sentence into dependency structures, which
represent all the necessary dependencies of words in a
sentence, including long-distance dependencies and con-
trol relations. There is an example in Figure 4, which
what doesstudent want to
write
your
ARG1
ARG2
ARG1
MODIFY
MODIFY
Figure 4: A dependency structure
corresponds to the parse tree in Figure 1. Note that this
representation includes a dependency not represented in
the derivation tree (the control relation between ?want?
and ?student?). A dependency structure is formally de-
fined as a set of dependencies: S = {?w
h
i
, w
n
i
, ?
i
?},
where w
h
i
and w
n
i
are a head and argument word of the
dependency, and ?
i
is an argument position of the head
word filled by the argument word.
An existing model assigns a probability value to de-
pendency structure S = {?w
h
i
, w
n
i
, ?
i
?} as follows.
p =
?
i
p(w
n
i
|w
h
i
, ?
i
)
Primitive probability is approximated by the relative fre-
quency of lexical dependencies of two words in a training
corpus.
Since dependency structures include all necessary de-
pendency relations, the first problem with LPCFG is now
completely solved. However, the third problem still re-
mains unsolved. The probability of a complete parse tree
is defined as the product of probabilities of primitive de-
pendencies of two words. In addition, the second prob-
lem is getting worse; the independence assumption is ap-
parently violated in this model, since the possible depen-
dency structures are restricted by the grammar. The prob-
ability model is no longer consistent.
3 Probability Model based on Lexicalized
Grammars
This section introduces our model of syntactic disam-
biguation, which is based on the decomposition of the
parsing model into the syntax and semantics models. The
concept behind it is that the plausibility of a parsing re-
sult is determined by i) the plausibility of syntax, and ii)
selecting the most probable semantics from the structures
allowed by the given syntax. This section formalizes the
general form of statistical models for disambiguation of
parsing including lexicalized parse trees, derivation trees,
and dependency structures. Problems with the existing
models are then discussed, and our model is introduced.
Suppose that a set W of words and a set C of syn-
tactic categories (e.g., nonterminal symbols of CFG, ele-
mentary trees of LTAG, feature structures of HPSG (Sag
and Wasow, 1999)) are given. A lexicalized grammar is
Lexicalized parse tree
?write, what, S? write S?,
?write, does, S? does S?,
?write, student, S? NP VP?,
?student, your, NP? your student?,
?write, want, VP? want VP?,
?write, to, VP? to write?
Derivation tree
?write, what, SUBST?,
?write, does, ADJ?,
?write, student, SUBST?,
?student, your, ADJ?,
?write, want, ADJ?,
?write, to, ADJ?
Dependency structure
?write, what, ARG2?,
?write, does, MODIFY?,
?write, student, ARG1?,
?student, your, MODIFY?,
?write, want, MODIFY?,
?want, student, ARG1?,
?write, to, MODIFY?
Figure 5: Parsing results of lexicalized grammars
then defined as a tuple G = ?L, R?, where L = {l =
?w, c?|w ? W , c ? C} is a lexicon and R is a set of
grammar rules. A parsing result of lexicalized gram-
mars is defined as a labeled graph structure A = {a|a =
?l
h
, l
n
, d?}, where a is an edge representing the depen-
dency of head l
h
and argument l
n
labeled with d. For
example, the lexicalized parse tree in Figure 2 is repre-
sented in this form as in Figure 5, as well as the derivation
tree and the dependency structure.
Given the above definition, the existing models dis-
cussed in Section 2 yield a probability P (A|w) for given
sentence w as in the following general form.
P (A|w) =
?
a?A
p(a|?),
In short, the probability of the complete structure is de-
fined as the product of probabilities of lexical depen-
dencies. For example, p(a|?) corresponds to the prob-
ability of branchings in LPCFG models, that of substi-
tution/adjunction in derivation tree models, and that of
primitive dependencies in dependency structure models.
The models, however, have a crucial weakness with
lexicalized grammar formalism; probability values are
assigned to parsing results not allowed by the grammar,
i.e., the model is no longer consistent. Hence, the disam-
biguation model of lexicalized grammars should not be
decomposed into primitive lexical dependencies.
A possible solution to this problem is to directly es-
timate p(A|w) by applying a maximum entropy model
(Berger et al, 1996). However, such modeling will lead
us to extensive tweaking of features that is theoretically
unjustifiable, and will not contribute to the theoretical
investigation of the relations of syntax and semantics.
Since lexicalized grammars express all syntactic con-
straints by syntactic categories of words, we have as-
sumed that we first determine which syntactic category c
should be chosen, and then determine which argument re-
lations are likely to appear under the constraints imposed
by the syntactic categories. Formally,
p(A|w) = p(c|w)p(A|c).
The first probability in the above formula is the prob-
ability of syntactic categories, i.e., the probability of se-
lecting a sequence of syntactic categories in a sentence.
Since syntactic categories in lexicalized grammars deter-
mine the syntactic constraints of words, this expresses the
syntactic preference of each word in a sentence. Note that
our objective is not only to improve parsing accuracy but
also to investigate the relation between syntax and seman-
tics. We have not adopted the local contexts of words as
in the supertaggers in LTAG (Joshi and Srinivas, 1994)
because they partially include the semantic preferences
of a sentence. The probability is purely unigram to se-
lect the probable syntactic category for each word. The
probability is then given by the product of probabilities
to select a syntactic category for each word from a set of
candidate categories allowed by the lexicon.
p(c|w) =
?
i
p(c
i
|w
i
)
The second describes the probability of semantics,
which expresses the semantic preferences of relating the
words in a sentence. Note that the semantics probabil-
ity is dependent on the syntactic categories determined
by the syntax probability, because in lexicalized grammar
formalism, a series of syntactic categories determines the
possible structures of parsing results. Parsing results are
obtained by solving the constraints given by the grammar.
Hence, we cannot simply decompose semantics probabil-
ity into the dependency probabilities of two words. We
define semantics probability as a discriminative model
that selects the most probable parsing result from a set
of candidates given by parsing.
Since semantics probability cannot be decomposed
into independent sub-events, we applied a maximum en-
tropy model, which allowed probabilistic modeling with-
out the independence assumption. Using this model, we
can assign consistent probabilities to parsing results with
complex structures, such as ones represented with feature
structures (Abney, 1997; Johnson et al, 1999). Given
parsing result A, semantics probability is defined as fol-
lows:
p(A|c) = 1
Z
c
exp
?
?
?
s?S(A)
?(s)
?
?
Z
c
=
?
A
?
?A(c)
exp
?
?
?
s
?
?S(A
?
)
?(s?)
?
? ,
where S(A) is a set of connected subgraphs of A, ?(s)
is a weight of subgraph s, and A(c) is a set of parsing
results allowed by the sequence of syntactic categories c.
Since we aim at separating syntactic and semantic pref-
erences, feature functions for semantic probability distin-
guish only words, not syntactic categories. We should
note that subgraphs should not be limited to an edge, i.e.,
the lexical dependency of two words. By taking more
than one edge as a subgraph, we can represent the depen-
dency of more than two words, although existing mod-
els do not adopt such dependencies. Various ambigui-
ties should be resolved by considering the dependency
of more than two words; e.g. PP-attachment ambiguity
should be resolved by the dependency of three words.
Consequently, the probability model takes the follow-
ing form.
p(A|w) =
{
?
i
p(c
i
|w
i
)
}
?
?
?
1
Z
c
exp
?
?
?
s?S(A)
?(s)
?
?
?
?
?
However, this model has a crucial flaw: the maxi-
mum likelihood estimation of semantics probability is
intractable. This is because the estimation requires Z
c
to be computed, which requires summation over A(c),
exponentially many parsing results. To cope with this
problem, we applied an efficient algorithm of maximum
entropy estimation for feature forests (Miyao and Tsu-
jii, 2002; Geman and Johnson, 2002). This enabled
the tractable estimation of the above probability, when
a set of candidates are represented in a feature forest of a
tractable size.
Here, we should mention that the disadvantages of the
traditional models discussed in Section 2 have been com-
pletely solved by this model. It can be applied to any
parsing results given by a lexicalized grammar, does not
require the independence assumption, and is defined as a
combination of syntax and semantics probabilities, where
the semantics probability is a discriminative model that
selects a parsing result from the set of candidates given
by the syntax probability.
4 Experiments
The model proposed in Section 3 is generally applica-
ble to any lexicalized grammars, and this section reports
the evaluation of our model with a wide-coverage LTAG
grammar, which is automatically acquired from the Penn
Treebank (Marcus et al, 1994) Sections 02?21. The
grammar was acquired by an algorithm similar to (Xia,
1999), and consisted of 2,105 elementary trees, where
1,010 were initial trees and 1,095 were auxiliary ones.
The coverage of the grammar against Section 22 (1,700
sentences) was 92.6% (1,575 sentences) in a weak sense
(i.e., the grammar could output a structure consistent with
the bracketing in the test corpus), and 68.0% (1,156 sen-
tences) in a strong sense (i.e., the grammar could output
exactly the correct derivation).
Since the grammar acquisition algorithm could output
derivation trees for the sentences in the training corpus
(Section 02?21), we used them as a training set of the
probability model. The model of syntax probability was
estimated with syntactic categories appearing in the train-
ing set. For estimating the semantics probability, a parser
produced all possible derivation trees for each sequence
of syntactic categories (corresponding to each sentence)
in the training set, and the obtained derivation trees, i.e.,
A(c), are passed to a maximum entropy estimator. By ap-
plying the grammar acquisition algorithm to Section 22,
we obtained the derivation trees of the sentences in this
section, and from this set we prepared a test set by elim-
inating non-sententials, long sentences (including more
than 40 words), sentences not covered by the grammar,
and sentences that caused time-outs in parsing. The re-
sulting set consisted of 917 derivation trees.
The following three disambiguation models were pre-
pared using the training set.
syntax Only composed of the syntax probability, i.e.,
p(c|w)
traditional Similar to our model, but semantics proba-
bility p(A|c) was decomposed into the probabilities
of the primitive dependencies of two words as in the
traditional modeling, i.e., this model is an inconsis-
tent probability model
our model The model by maximum entropy estimation
for feature forests
The syntax probability was a unigram model, and con-
texts around the word such as previous words/categories
were not used. Hence, it includes only syntactic prefer-
ences of words. The semantics parts of traditional and
our model were maximum entropy models, where ex-
actly the same set of features were used, i.e., the differ-
ence between the two models was only in an event repre-
sentation: derivation trees were decomposed into primi-
tive dependencies in traditional, while in our model they
were represented by a feature forest without decompo-
sition. Hence, we can evaluate the effects of applying
maximum entropy estimation for feature forests by com-
paring our model with traditional. While our model al-
lowed features to be incorporated that were not limited
to the dependencies of two words (Section 3), the models
used throughout the experiments only included features
of the dependencies of two words. The semantics proba-
bilities were developed with two sets of features includ-
exact partial
syntax 73.4 77.3
traditional 79.2 83.4
our model 79.6 83.6
Table 1: Accuracy of dependencies (1)
exact partial
syntax 73.4 77.3
traditional 79.6 83.6
our model 78.9 82.8
Table 2: Accuracy of dependencies (2)
ing surface forms/POSs of words, the labels of dependen-
cies (substitution/adjunction), and the distance between
two words. The first feature set had 283,755 features
and the other had 150,156 features excluding fine-grained
features of the first set. There were 701,819 events for
traditional, and 32,371 for our model. The difference in
the number of events was caused by the difference in the
units of events, i.e., an event corresponded to a depen-
dency in traditional, while it corresponded to a sentence
in our model.
The parameters of the models were estimated by the
limited-memory BFGS algorithm (Nocedal, 1980) with
a Gaussian distribution as the prior probability distri-
bution for smoothing (Chen and Rosenfeld, 1999) im-
plemented in a maximum entropy estimator for feature
forests (Miyao, 2002). The estimation for traditional was
converged in 67 iterations in 127 seconds, and our model
in 29 iterations in 111 seconds on a Pentium III 1.26-GHz
CPU with 4 GB of memory. These results reveal that the
estimation with our model is comparatively efficient with
traditional. The parsing algorithm was CKY-style pars-
ing with beam thresholding, which was similar to ones
used in (Collins, 1996; Clark et al, 2002). Although
we needed to compute normalizing factor Z
c
to obtain
probability values, we used unnormalized products as the
preference score for beam thresholding, following (Clark
et al, 2002). We did not use any preprocessing such as
supertagging (Joshi and Srinivas, 1994) and the parser
searched for the most plausible derivation tree from the
derivation forest in terms of the probability given by the
combination of syntax and semantics probabilities.
Tables 1 and 2 list the accuracy of dependencies, i.e.,
edges in derivation trees, for each model with two sets
of features for the semantics model3. Since in derivation
trees each word in a sentence depends on one and only
one word (see Figure 3), the accuracy is the number of
3Since the features of the syntax part were not changed, the
results for syntax are exactly the same.
correct edges divided by the number of all edges in the
tree. The exact column indicates the ratio of dependen-
cies where the syntactic category, the argument position,
and the dependee head word of the argument word are
correctly output. The partial column shows the ratio of
dependencies where the words are related regardless of
the label. We should note that the exact measure is a very
stringent because the model must select the correct syn-
tactic category from 2,105 categories.
First, we can see that syntax achieved a high level of
accuracy although it was not quite sufficient yet. We
think this was because the grammar could adequately re-
strict the possible structure of parsing results, and the dis-
ambiguation model tried to search for the most probable
structure from the candidates allowed by the grammar.
Second, traditional and our model recorded significantly
higher accuracy than syntax. The accuracy of our model
was almost matched traditional, which proved the valid-
ity of probabilistic modeling with maximum entropy es-
timation for feature forests. The differences between tra-
ditional and our model were insignificant and the results
proved that a consistent probability model of parsing can
be built without the independence assumption, and attains
performance that rivals the traditional models in terms of
parsing accuracy.
We should note that accuracy can further be improved
with our model because it allows other features to be in-
corporated that were not used in these experiments be-
cause the model is not rely on the decomposition into
the dependencies of two words. Another possibility to
increase the accuracy is to refine the LTAG grammar. Al-
though we assumed that all syntactic constraints were
expressed with syntactic categories (Section 3), i.e., el-
ementary trees, the grammar used in the experiments
were not augmented with feature structures and not suffi-
ciently restrictive to eliminate syntactically invalid struc-
tures. Since our model did not include the preferences of
syntactic relations of words, we expect the refinement of
the grammar will greatly improve the accuracy.
5 Conclusion
This paper described a novel model for syntactic dis-
ambiguation based on lexicalized grammars, where the
model selects the most probable parsing result from the
candidates allowed by a lexicalized grammar. Since lex-
icalized grammars can restrict the possible structure of
parsing results, the probabilistic model cannot simply
be decomposed into independent events as in the ex-
isting disambiguation models for parsing. By apply-
ing a maximum entropy model for feature forests, we
achieved probabilistic modeling without decomposition.
Through experiments, we proved the syntax-only model
could record with high level of accuracy with a lexical-
ized grammar, and maximum entropy estimation for fea-
ture forests could attain competitive accuracy compared
to the traditional model. We see this work as the first step
in the application of linguistically motivated grammars to
the parsing of real-world texts as well as the evaluation of
linguistic theories by consulting extensive corpora.
Future work should include the application of our
model to other lexicalized grammars including HPSG.
The development of sophisticated parsing strategies is
also required to improve the accuracy and efficiency of
parsing. Since parsing results of lexicalized grammars
such as HPSG and CCG can include non-local dependen-
cies, we cannot simply apply well-known parsing strate-
gies, such as beam thresholding, which assume the local
computation of preference scores. Further investigations
must be left for future research.
References
Steven P. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4).
Adam L. Berger, Stephen A. Della Pietra, and Vincent.
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of 14th National Conference on Artificial Intelli-
gence, pages 598?603.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMUCS-99-108, Carnegie Mellon Uni-
versity.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of TAGs from the Penn Treebank. In Proceed-
ings of 6th IWPT.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of ACL 2000, pages 456?463.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of 40th
ACL.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proceedings of 34th
ACL, pages 184?191.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of 35th
ACL.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochastic
unification-based grammars. In Proceedings of 40th
ACL, pages 279?286.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of 3rd LREC.
Julia Hockenmaier and Mark Steedman. 2002b. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proceedings of 40th ACL,
pages 335?342.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of 37th
ACL, pages 535?541.
Aravind K. Joshi and B. Srinivas. 1994. Disambiguation
of super parts of speech (or supertags): Almost pars-
ing. In Proceedings of 17th COLING, pages 161?165.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proceedings of
HLT 2002.
Yusuke Miyao. 2002. Amis ? a maximum entropy es-
timator for feature forests. Available via http://www-
tsujii.is.s.u-tokyo.ac.jp/%7Eyusuke/amis/.
Jorge Nocedal. 1980. Updating quasi-Newton matrices
with limited storage. Mathematics of Computation,
35:773?783.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of 38th ACL.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of 40th ACL.
Ivan A. Sag and ThomasWasow. 1999. Syntactic Theory
? A Formal Introduction. CSLI Lecture Notes no. 92.
CSLI Publications.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized grammars?:
Application to tree adjoining grammars. In Proceed-
ings of 12th COLING, pages 578?583.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of 5th NLPRS.
An Efficient Clustering Algorithm for Class-based Language Models
Takuya Matsuzaki Yusuke Miyao
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
matuzaki,yusuke,tsujii@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Abstract
This paper defines a general form for class-
based probabilistic language models and pro-
poses an efficient algorithm for clustering
based on this. Our evaluation experiments re-
vealed that our method decreased computation
time drastically, while retaining accuracy.
1 Introduction
Clustering algorithms have been extensively studied in
the research area of natural language processing because
many researchers have proved that ?classes? obtained by
clustering can improve the performance of various NLP
tasks. Examples have been class-based -gram models
(Brown et al, 1992; Kneser and Ney, 1993), smooth-
ing techniques for structural disambiguation (Li and Abe,
1998) and word sense disambiguation (Shu?tze, 1998).
In this paper, we define a general form for class-based
probabilistic language models, and propose an efficient
and model-theoretic algorithm for clustering based on
this. The algorithm involves three operations, CLAS-
SIFY, MERGE, and SPLIT, all of which decreases the
optimization function based on the MDL principle (Ris-
sanen, 1984), and can efficiently find a point near the lo-
cal optimum. The algorithm is applicable to more general
tasks than existing studies (Li and Abe, 1998; Berkhin
and Becher, 2002), and computational costs are signifi-
cantly small, which allows its application to very large
corpora.
Clustering algorithms may be classified into three
types. The first is a type that uses various heuristic mea-
sure of similarity between the elements to be clustered
and has no interpretation as a probabilitymodel (Widdow,
2002). The resulting clusters from this type of method
are not guaranteed to work effectively as a component
of a statistical language model, because the similarity
used in clustering is not derived from the criterion in the
learning process of the statistical model, e.g. likelihood.
The second type has clear interpretation as a probability
model, but no criteria to determine the number of clusters
(Brown et al, 1992; Kneser and Ney, 1993). The perfor-
mance of methods of this type depend on the number of
clusters that must be specified before the clustering pro-
cess. It may prove rather troublesome to determine the
proper number of clusters in this type of method. The
third has interpretation as a probability model and uses
some statistically motivated model selection criteria to
determine the proper number of clusters. This type has
a clear advantage compared to the second. AutoClass
(Cheeseman and Stutz, 1996), the Bayesian model merg-
ing method (Stolcke and Omohundro, 1996) and Li?s
method (Li, 2002) are examples of this type. AutoClass
and the Bayesian model merging are based on soft clus-
tering models and Li?s method is based on a hard clus-
tering model. In general, computational costs for hard
clustering models are lower than that for soft clustering
models. However, the time complexity of Li?s method is
of cubic order in the size of the vocabulary. Therefore, it
is not practical to apply it to large corpora.
Our model and clustering algorithm provide a solution
to these problems with existing clustering algorithms.
Since the model has clear interpretation as a probability
model, the clustering algorithm uses MDL as clustering
criteria and using a combination of top-down clustering,
bottom-up clustering, and a K-means style exchange al-
gorithm, the method we propose can perform the cluster-
ing efficiently.
We evaluated the algorithm through experiments on
a disambiguation task of Japanese dependency analysis.
In the experiments, we observed that the proposed algo-
rithm?s computation time is roughly linear to the size of
the vocabulary, and it performed slightly better than the
existing method. Our main intention in the experiments
was to see improvements in terms of computational cost,
not in performance in the test task. We will show, in Sec-
tions 2 and 3, that the proposed method can be applied
to a broader range of tasks than the test task we evalu-
ate in the experiments in Section 4. We need further ex-
periments to determine the performance of the proposed
method with more general tasks.
2 Probability model
2.1 Class-based language modeling
Our probability model is a class-based model and it is an
extension of the model proposed by Li and Abe (1998).
We extend their two-dimensional class model to a multi-
dimensional class model, i.e., we incorporate an arbitrary
number of random variables in our model.
Although our probabilitymodel and learning algorithm
are general and not restricted to particular domains, we
mainly intend to use them in natural language process-
ing tasks where large amounts of lexical knowledge are
required. When we incorporate lexical information into
a model, we inevitably face the data-sparseness problem.
The idea of ?word class? (Brown et al, 1992) gives a gen-
eral solution to this problem. A word class is a group
of words which performs similarly in some linguistic
phenomena. Part-of-speech are well-known examples of
such classes. Incorporating word classes into linguistic
models yields good smoothing or, hopefully, meaningful
generalization from given samples.
2.2 Model definition
Let us introduce some notations to define our model. In
our model, we have considered  kinds of discrete ran-
dom variables 

 

     

and their joint distribu-
tion. 

denotes a set of possible values for the -th vari-
able 

. Our probability model assumes disjunctive par-
titions of each 

, which are denoted by 

?s. A disjunc-
tive partition   

 

     

 of  is a subset of


, and satisfies 

 

 	 
   and   



.
We call elements in a partition 

classes of elements in


. 


, or 

for short, denotes a class in 

which
contains an element   

.
With these notations, our probability model is ex-
pressed as:
 

 

 

 

     

 


  


 


     





 

 


 (1)
In this paper, we have considered a hard clusteringmodel,
i.e.,     for any   . Li & Abe?s model
(1998) is an instance of this joint probability model,
where   . Using more than 2 variables the model can
represent the probability for the co-occurrence of triplets,
such as subject, verb, object.
2.3 Clustering criterion
To determine the proper number of classes in each par-
tition 

     

, we need criteria other than the maxi-
mum likelihood criterion, because likelihood always be-
come greater when we use smaller classes. We can see
this class number decision problem as a model selection
problem and apply some statistically motivated model
selection criteria. As mentioned previously (following
Li and Abe (1998)) we used the MDL principle as our
clustering criterion.
Assume that we have  samples of co-occurrence
data:
  

 

 

     

  
        
The objective function in both clustering and parame-
ter estimations in our method is the description length,
, which is defined as follows:
   	

 
  (2)
where  denotes the model and 

 is the likelihood
of samples  under model  :


 
	


 

 

     

 (3)
The first term in Eq.2,  	

, is called the data
description length. The second term, , is called the
model description length, and when sample size  is
large, it can be approximated as
 


	
where  is the number of free parameters in model  .
We used this approximated form throughout this paper.
Given the number of classes, 

 

 for each  
     , we have





  free parameters for joint
probabilities    . Also, for each class , we
have   free parameters for conditional probabilities
 , where   . Thus, we have
 







   





 






 

 





 
Our learning algorithm tries to minimize  by
adjusting the parameters in the model, selecting partition


of each 

, and choosing the numbers of classes, 

in each partition 

.
3 Clustering algorithm
Our clustering algorithm is a combination of three ba-
sic operations: CLASSIFY, SPLIT and MERGE. We it-
eratively invoke these until a terminate condition is met.
Briefly, these three work as follows. The CLASSIFY
takes a partition  in  as input and improves the par-
tition by moving the elements in  from one class to an-
other. This operation is similar to one iteration in the K-
means algorithm. The MERGE takes a partition  as in-
put and successively chooses two classes 

and 

from
 and replaces themwith their union,



. The SPLIT
takes a class, , and tries to find the best division of 
into two new classes, which will decrease the description
length the most.
All of these three basic operations decrease the de-
scription length. Consequently, our overall algorithm
also decreases the description length monotonically and
stops when all three operations cause no decrease in de-
scription length. Strictly, this termination does not guar-
antee the resulting partitions to be even locally opti-
mal, because SPLIT operations do not perform exhaus-
tive searches in all possible divisions of a class. Doing
such an exhaustive search is almost impossible for a class
of modest size, because the time complexity of such an
exhaustive search is of exponential order to the size of the
class. However, by properly selecting the number of tri-
als in SPLIT, we can expect the results to approach some
local optimum.
It is clear that the way the three operations are com-
bined affects the performance of the resulting class-based
model and the computation time required in learning. In
this paper, we basically take a top-down, divisive strat-
egy, but at each stage of division we do CLASSIFY op-
erations on the set of classes at each stage. When we
cannot divide any classes and CLASSIFY cannot move
any elements, we invoke MERGE to merge classes that
are too finely divided. This top-down strategy can drasti-
cally decrease the amount of computation time compared
to the bottom-up approaches used by Brown et al (1992)
and Li and Abe (1998).
The following is the precise algorithm for our main
procedure:
Algorithm 1 MAIN PROCEDURE()
INPUT
 : an integer specifying the number of trials in a
SPLIT operation
OUTPUT
Partitions 

  

and estimated parameters in the
model
PROCEDURE
Step 0 

  

 	 INITIALIZE

 

 
Step 1 Do Step 2 through Step 3 until no change is made
through one iteration
Step 2 For     , do Step 2.1 through Step 2.2
Step 2.1 Do Step 2.1.1 until no change occurs through it
Step 2.1.1 For      , 

	 CLASSIFY


Step 2.2 For each   

,  	 SPLIT 
Step 3 For     , 

	 MERGE


Step 4 Return the resulting partitions with the parame-
ters in the model
In the Step 0 of the algorithm, INITIALIZE creates
the initial partitions of 

     

. It first divides each


     

into two classes and then applies CLASSIFY
to each partition 

     

one by one, while any ele-
ments can move.
The following subsections explain the algorithm for
the three basic operations in detail and show that they
decrease  monotonically.
3.1 Iterative classification
In this subsection, we explain a way of finding a local
optimum in the possible classification of elements in 

,
given the numbers of classes in partitions 

.
Given the number of classes, optimization in terms of
the description length (Eq.2) is just the same as optimiz-
ing the likelihood (Eq.3). We used a greedy algorithm
which monotonically increases the likelihood while
updating classification. Our method is a generalized
version of the previously reported K-means/EM-
algorithm-style, iterative-classification methods in
Kneser and Ney (1993), Berkhin and Becher (2002) and
Dhillon et al (2002). We demonstrate that the method is
applicable to more generic situations than those previ-
ously reported, where the number of random variables is
arbitrary.
To explain the algorithmmore fully, we define ?counter
functions? Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103?114,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficacy of Beam Thresholding, Unification Filtering and Hybrid
Parsing in Probabilistic HPSG Parsing
Takashi Ninomiya
CREST, JST
and
Department of Computer Science
The University of Tokyo
ninomi@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
CREST, JST
and
Department of Computer Science
The University of Tokyo
tsuruoka@is.s.u-tokyo.ac.jp
Yusuke Miyao
Department of Computer Science
The University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science
The University of Tokyo
and
School of Informatics
University of Manchester
and
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
We investigated the performance efficacy
of beam search parsing and deep parsing
techniques in probabilistic HPSG parsing
using the Penn treebank. We first tested
the beam thresholding and iterative pars-
ing developed for PCFG parsing with an
HPSG. Next, we tested three techniques
originally developed for deep parsing: quick
check, large constituent inhibition, and hy-
brid parsing with a CFG chunk parser. The
contributions of the large constituent inhi-
bition and global thresholding were not sig-
nificant, while the quick check and chunk
parser greatly contributed to total parsing
performance. The precision, recall and av-
erage parsing time for the Penn treebank
(Section 23) were 87.85%, 86.85%, and 360
ms, respectively.
1 Introduction
We investigated the performance efficacy of beam
search parsing and deep parsing techniques in
probabilistic head-driven phrase structure grammar
(HPSG) parsing for the Penn treebank. We first
applied beam thresholding techniques developed for
CFG parsing to HPSG parsing, including local
thresholding, global thresholding (Goodman, 1997),
and iterative parsing (Tsuruoka and Tsujii, 2005b).
Next, we applied parsing techniques developed for
deep parsing, including quick check (Malouf et al,
2000), large constituent inhibition (Kaplan et al,
2004) and hybrid parsing with a CFG chunk parser
(Daum et al, 2003; Frank et al, 2003; Frank, 2004).
The experiments showed how each technique con-
tributes to the final output of parsing in terms of
precision, recall, and speed for the Penn treebank.
Unification-based grammars have been extensively
studied in terms of linguistic formulation and com-
putation efficiency. Although they provide precise
linguistic structures of sentences, their processing is
considered expensive because of the detailed descrip-
tions. Since efficiency is of particular concern in prac-
tical applications, a number of studies have focused
on improving the parsing efficiency of unification-
based grammars (Oepen et al, 2002). Although sig-
nificant improvements in efficiency have been made,
parsing speed is still not high enough for practical
applications.
The recent introduction of probabilistic models of
wide-coverage unification-based grammars (Malouf
and van Noord, 2004; Kaplan et al, 2004; Miyao
and Tsujii, 2005) has opened up the novel possibil-
ity of increasing parsing speed by guiding the search
path using probabilities. That is, since we often re-
quire only the most probable parse result, we can
compute partial parse results that are likely to con-
tribute to the final parse result. This approach has
been extensively studied in the field of probabilistic
103
CFG (PCFG) parsing, such as Viterbi parsing and
beam thresholding.
While many methods of probabilistic parsing for
unification-based grammars have been developed,
their strategy is to first perform exhaustive pars-
ing without using probabilities and then select the
highest probability parse. The behavior of their al-
gorithms is like that of the Viterbi algorithm for
PCFG parsing, so the correct parse with the high-
est probability is guaranteed. The interesting point
of this approach is that, once the exhaustive pars-
ing is completed, the probabilities of non-local de-
pendencies, which cannot be computed during pars-
ing, are computed after making a packed parse for-
est. Probabilistic models where probabilities are as-
signed to the CFG backbone of the unification-based
grammar have been developed (Kasper et al, 1996;
Briscoe and Carroll, 1993; Kiefer et al, 2002), and
the most probable parse is found by PCFG parsing.
This model is based on PCFG and not probabilis-
tic unification-based grammar parsing. Geman and
Johnson (Geman and Johnson, 2002) proposed a dy-
namic programming algorithm for finding the most
probable parse in a packed parse forest generated by
unification-based grammars without expanding the
forest. However, the efficiency of this algorithm is
inherently limited by the inefficiency of exhaustive
parsing.
In this paper we describe the performance of beam
thresholding, including iterative parsing, in proba-
bilistic HPSG parsing for a large-scale corpora, the
Penn treebank. We show how techniques developed
for efficient deep parsing can improve the efficiency
of probabilistic parsing. These techniques were eval-
uated in experiments on the Penn Treebank (Marcus
et al, 1994) with the wide-coverage HPSG parser de-
veloped by Miyao et al (Miyao et al, 2005; Miyao
and Tsujii, 2005).
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical en-
tries express word-specific characteristics. The struc-
tures of sentences are explained using combinations
of schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
Figure 1 shows an example of HPSG parsing of
the sentence ?Spring has come.? First, each of the
lexical entries for ?has? and ?come? is unified with a
daughter feature structure of the Head-Complement
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  noun
SUBJ  < >
COMPS  < >
1
=?
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing
Schema. Unification provides the phrasal sign of
the mother. The sign of the larger constituent is
obtained by repeatedly applying schemata to lexi-
cal/phrasal signs. Finally, the parse result is output
as a phrasal sign that dominates the sentence.
Given set W of words and set F of feature struc-
tures, an HPSG is formulated as a tuple, G = ?L,R?,
where
L = {l = ?w,F ?|w ? W, F ? F} is a set of lexical
entries, and
R is a set of schemata, i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of phrasal
signs, i.e., feature structures, as a result of parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Miyao et al, 2003; Mal-
ouf and van Noord, 2004; Kaplan et al, 2004; Miyao
and Tsujii, 2005) defined a probabilistic model of
unification-based grammars as a log-linear model or
maximum entropy model (Berger et al, 1996). The
probability of parse result T assigned to given sen-
tence w = ?w1, . . . , wn? is
p(T |w) = 1Zw
exp
(
?
i
?ifi(T )
)
Zw =
?
T ?
exp
(
?
i
?ifi(T ?)
)
,
where ?i is a model parameter, and fi is a feature
function that represents a characteristic of parse tree
T . Intuitively, the probability is defined as the nor-
malized product of the weights exp(?i) when a char-
acteristic corresponding to fi appears in parse result
T . Model parameters ?i are estimated using numer-
104
ical optimization methods (Malouf, 2002) so as to
maximize the log-likelihood of the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the computa-
tion of p(T |w) for all parse candidates assigned to
sentence w. Because the number of parse candidates
is exponentially related to the length of the sentence,
the estimation is intractable for long sentences.
To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). They assumed that features are functions
on nodes in a packed parse forest. That is, parse tree
T is represented by a set of nodes, i.e., T = {c}, and
the parse forest is represented by an and/or graph
of the nodes. From this assumption, we can redefine
the probability as
p(T |w) = 1Zw
exp
(
?
c?T
?
i
?ifi(c)
)
Zw =
?
T ?
exp
(
?
c?T ?
?
i
?ifi(c)
)
.
A packed parse forest has a structure similar to a
chart of CFG parsing, and c corresponds to an edge
in the chart. This assumption corresponds to the
independence assumption in PCFG; that is, only
a nonterminal symbol of a mother is considered in
further processing by ignoring the structure of its
daughters. With this assumption, we can compute
the figures of merit (FOMs) of partial parse results.
This assumption restricts the possibility of feature
functions that represent non-local dependencies ex-
pressed in a parse result. Since unification-based
grammars can express semantic relations, such as
predicate-argument relations, in their structure, the
assumption unjustifiably restricts the flexibility of
probabilistic modeling. However, previous research
(Miyao et al, 2003; Clark and Curran, 2004; Kaplan
et al, 2004) showed that predicate-argument rela-
tions can be represented under the assumption of
feature locality. We thus assumed the locality of fea-
ture functions and exploited it for the efficient search
of probable parse results.
3 Techniques for efficient deep
parsing
Many of the techniques for improving the parsing
efficiency of deep linguistic analysis have been de-
veloped in the framework of lexicalized grammars
such as lexical functional grammar (LFG) (Bresnan,
1982), lexicalized tree adjoining grammar (LTAG)
(Shabes et al, 1988), HPSG (Pollard and Sag, 1994)
or combinatory categorial grammar (CCG) (Steed-
man, 2000). Most of them were developed for ex-
haustive parsing, i.e., producing all parse results that
are given by the grammar (Matsumoto et al, 1983;
Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer
et al, 1999; Malouf et al, 2000; Torisawa et al, 2000;
Oepen et al, 2002; Penn and Munteanu, 2003). The
strategy of exhaustive parsing has been widely used
in grammar development and in parameter training
for probabilistic models.
We tested three of these techniques.
Quick check Quick check filters out non-unifiable
feature structures (Malouf et al, 2000). Sup-
pose we have two non-unifiable feature struc-
tures. They are destructively unified by travers-
ing and modifying them, and then finally they
are found to be not unifiable in the middle of the
unification process. Quick check quickly judges
their unifiability by peeping the values of the
given paths. If one of the path values is not
unifiable, the two feature structures cannot be
unified because of the necessary condition of uni-
fication. In our implementation of quick check,
each edge had two types of arrays. One con-
tained the path values of the edge?s sign; we
call this the sign array. The other contained the
path values of the right daughter of a schema
such that its left daughter is unified with the
edge?s sign; we call this a schema array. When
we apply a schema to two edges, e1 and e2, the
schema array of e1 and the sign array of e2 are
quickly checked. If it fails, then quick check re-
turns a unification failure. If it succeeds, the
signs are unified with the schemata, and the re-
sult of unification is returned.
Large constituent inhibition (Kaplan et al,
2004) It is unlikely for a large medial edge to
contribute to the final parsing result if it spans
more than 20 words and is not adjacent to the
beginning or ending of the sentence. Large
constituent inhibition prevents the parser from
generating medial edges that span more than
some word length.
HPSG parsing with a CFG chunk parser A
hybrid of deep parsing and shallow parsing
was recently found to improve the efficiency
of deep parsing (Daum et al, 2003; Frank et
al., 2003; Frank, 2004). As a preprocessor, the
shallow parsing must be very fast and achieve
high precision but not high recall so that the
105
procedure Viterbi(?w1, . . . , wn?, ?L?, R?, ?, ?, ?)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? pi[i, k], Ft ? pi[k, j], r ? R
if F = r(Fs, Ft) has succeeded
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
Figure 2: Pseudo-code of Viterbi algorithms for probabilistic HPSG parsing
total parsing performance in terms of precision,
recall and speed is not degraded. Because there
is trade-off between speed and accuracy in
this approach, the total parsing performance
for large-scale corpora like the Penn treebank
should be measured. We introduce a CFG
chunk parser (Tsuruoka and Tsujii, 2005a) as a
preprocessor of HPSG parsing. Chunk parsers
meet the requirements for preprocessors; they
are very fast and have high precision. The
grammar for the chunk parser is automatically
extracted from the CFG treebank translated
from the HPSG treebank, which is generated
during grammar extraction from the Penn
treebank. The principal idea of using the chunk
parser is to use the bracket information, i.e.,
parse trees without non-terminal symbols, and
prevent the HPSG parser from generating edges
that cross brackets.
4 Beam thresholding for HPSG
parsing
4.1 Simple beam thresholding
Many algorithms for improving the efficiency of
PCFG parsing have been extensively investigated.
They include grammar compilation (Tomita, 1986;
Nederhof, 2000), the Viterbi algorithm, controlling
search strategies without FOM such as left-corner
parsing (Rosenkrantz and Lewis II, 1970) or head-
corner parsing (Kay, 1989; van Noord, 1997), and
with FOM such as the beam search, the best-first
search or A* search (Chitrao and Grishman, 1990;
Caraballo and Charniak, 1998; Collins, 1999; Rat-
naparkhi, 1999; Charniak, 2000; Roark, 2001; Klein
and Manning, 2003). The beam search and best-
first search algorithms significantly reduce the time
required for finding the best parse at the cost of los-
ing the guarantee of finding the correct parse.
The CYK algorithm, which is essentially a bottom-
up parser, is a natural choice for non-probabilistic
HPSG parsers. Many of the constraints are ex-
pressed as lexical entries in HPSG, and bottom-up
parsers can use those constraints to reduce the search
space in the early stages of parsing.
For PCFG, extending the CYK algorithm to out-
put the Viterbi parse is straightforward (Ney, 1991;
Jurafsky and Martin, 2000). The parser can effi-
ciently calculate the Viterbi parse by taking the max-
imum of the probabilities of the same nonterminal
symbol in each cell. With the probabilistic model
defined in Section 2, we can also define the Viterbi
search for unification-based grammars (Geman and
Johnson, 2002). Figure 2 shows the pseudo-code of
Viterbi algorithm. The pi[i, j] represents the set of
partial parse results that cover words wi+1, . . . , wj ,
and ?[i, j, F ] stores the maximum FOM of partial
parse result F at cell (i, j). Feature functions are
defined over lexical entries and results of rule appli-
cations, which correspond to conjunctive nodes in a
feature forest. The FOM of a newly created partial
parse, F , is computed by summing the values of ? of
the daughters and an additional FOM of F .
The Viterbi algorithm enables various pruning
techniques to be used for efficient parsing. Beam
thresholding (Goodman, 1997) is a simple and effec-
tive technique for pruning edges during parsing. In
each cell of the chart, the method keeps only a por-
tion of the edges which have higher FOMs compared
to the other edges in the same cell.
106
procedure BeamThresholding(?w1, . . . , wn?, ?L?, R?, ?, ?, ?)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? pi[i, k], Ft ? pi[k, j], r ? R
if F = r(Fs, Ft) has succeeded
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
LocalThresholding(?, ?)
GlobalThresholding(n, ?)
procedure LocalThresholding(?, ?)
sort pi[i, j] according to ?[i, j, F ]
pi[i, j]? {pi[i, j]1, . . . , pi[i, j]?}
?max = maxF ?[i, j, F ]
foreach F ? pi[i, j]
if ?[i, j, F ] < ?max ? ?
pi[i, j]? pi[i, j]\{F}
procedure GlobalThresholding(n, ?)
f [0..n]? {0,?? ??, . . . ,??}
b[0..n]? {??,??, . . . ,??, 0}
#forward
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
f [j]? max(f [j], f [i] + ?[i, j, F ])
#backward
for i = n? 1 to 0
for j = i + 1 to n
foreach F ? pi[i, j]
b[i]? max(b[i], b[j] + ?[i, j, F ])
#global thresholding
?max = f [n]
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
if f [i] + ?[i, j, F ] + b[j] < ?max ? ? then
pi[i, j]? pi[i, j]\{F}
Figure 3: Pseudo-code of local beam search and global beam search algorithms for probabilistic HPSG
parsing
107
procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ??, ??, ??, ?last, ?last, ?last)
?? ?0; ? ? ?0; ? ? ?0
loop while ? ? ?last and ? ? ?last and ? ? ?last
call BeamThresholding(w, G, ?, ?, ?)
if pi[1, n] 6= ? then exit
?? ? + ??; ? ? ? + ??; ? ? ? + ??
Figure 4: Pseudo-code of iterative beam thresholding
We tested three selection schemes for deciding
which edges to keep in each cell.
Local thresholding by number of edges Each
cell keeps the top ? edges based on their FOMs.
Local thresholding by beam width Each cell
keeps the edges whose FOM is greater than
?max ? ?, where ?max is the highest FOM
among the edges in the cell.
Global thresholding by beam width Each cell
keeps the edges whose global FOM is greater
than ?max??, where ?max is the highest global
FOM in the chart.
Figure 3 shows the pseudo-code of local beam
search, and global beam search algorithms for prob-
abilistic HPSG parsing. The code for local thresh-
olding is inserted at the end of the computation for
each cell. In Figure 3, pi[i, j]k denotes the k-th ele-
ment in sorted set pi[i, j]. We first take the first ?
elements that have higher FOMs and then remove
the elements with FOMs lower than ?max ? ?.
Global thresholding is also used for pruning edges,
and was originally proposed for CFG parsing (Good-
man, 1997). It prunes edges based on their global
FOM and the best global FOM in the chart. The
global FOM of an edge is defined as its FOM plus its
forward and backward FOMs, where the forward and
backward FOMs are rough estimations of the outside
FOM of the edge. The global thresholding is per-
formed immediately after each line of the CYK chart
is completed. The forward FOM is calculated first,
and then the backward FOM is calculated. Finally,
all edges with a global FOM lower than ?max ? ?
are pruned. Figure 3 gives further details of the al-
gorithm.
4.2 Iterative beam thresholding
We tested the iterative beam thresholding proposed
by Tsuruoka and Tsujii (2005b). We started the
parsing with a narrow beam. If the parser output
results, they were taken as the final parse results. If
the parser did not output any results, we widened the
Table 1: Abbreviations used in experimental results
num local beam thresholding by number
width local beam thresholding by width
global global beam thresholding by width
iterative iterative parsing with local beam
thresholding by number and width
chp parsing with CFG chunk parser
beam, and reran the parsing. We continued widen-
ing the beam until the parser output results or the
beam width reached some limit.
The pseudo-code is presented in Figure 4. It calls
the beam thresholding procedure shown in Figure 3
and increases parameters ? and ? until the parser
outputs results, i.e., pi[1, n] 6= ?.
Preserved iterative parsing Our implemented
CFG parser with iterative parsing cleared the
chart and edges at every iteration although the
parser regenerated the same edges using those
generated in the previous iteration. This is
because the computational cost of regenerating
edges is smaller than that of reusing edges to
which the rules have already been applied. For
HPSG parsing, the regenerating cost is even
greater than that for CFG parsing. In our
implementation of HPSG parsing, the chart
and edges were not cleared during the iterative
parsing. Instead, the pruned edges were marked
as thresholded ones. The parser counted the
number of iterations, and when edges were
generated, they were marked with the iteration
number, which we call the generation. If
edges were thresholded out, the generation was
replaced with the current iteration number plus
1. Suppose we have two edges, e1 and e2. The
grammar rules are applied iff both e1 and e2 are
not thresholded out, and the generation of e1
or e2 is equal to the current iteration number.
Figure 5 shows the pseudo-code of preserved
iterative parsing.
108
procedure BeamThresholding(?w1, . . . , wn?, ?L?, R?, ?, ?, ?, iternum)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if gen[i, k, Fs] = iternum ? gen[k, j, Ft] = iternum
if F = r(Fs, Ft) has succeeded
gen[i, j, F ]? iternum
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
LocalThresholding(?, ?, iternum)
GlobalThresholding(n, ?, iternum)
procedure LocalThresholding(?, ?, iternum)
sort pi[i, j] according to ?[i, j, F ]
?[i, j]? {pi[i, j]1, . . . , pi[i, j]?}
?max = maxF ?[i, j, F ]
foreach F ? ?[i, j]
if ?[i, j, F ] < ?max ? ?
?[i, j]? ?[i, j]\{F}
foreach F ? (pi[i, j]? ?[i, j])
gen[i, j, F ]? iternum + 1
procedure GlobalThresholding(n, ?, iternum)
f [0..n]? {0,?? ??, . . . ,??}
b[0..n]? {??,??, . . . ,??, 0}
#forward
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
f [j]? max(f [j], f [i] + ?[i, j, F ])
#backward
for i = n? 1 to 0
for j = i + 1 to n
foreach F ? pi[i, j]
b[i]? max(b[i], b[j] + ?[i, j, F ])
#global thresholding
?max = f [n]
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? ?[i, j]
if f [i] + ?[i, j, F ] + b[j] < ?max ? ? then
?[i, j]? ?[i, j]\{F}
foreach F ? (pi[i, j]? ?[i, j])
gen[i, j, F ]? iternum + 1
procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ??, ??, ??, ?last, ?last, ?last)
?? ?0; ? ? ?0; ? ? ?0; iternum = 0
loop while ? ? ?last and ? ? ?last and ? ? ?last
call BeamThresholding(w, G, ?, ?, ?, iternum)
if pi[1, n] 6= ? then exit
?? ? + ??; ? ? ? + ??; ? ? ? + ??; iternum? iternum + 1
Figure 5: Pseudo-code of preserved iterative parsing for HPSG
109
Table 2: Experimental results for development set (section 22) and test set (section 23)
Precision Recall F-score Avg. Time (ms) No. of failed sentences
development set 88.21% 87.32% 87.76% 360 12
test set 87.85% 86.85% 87.35% 360 15









        
  	 
  	      	  
         



	














        
  	 
  	      	  
         



	





Figure 7: Parsing time for the sentences in Section 24 of less than 15 words of Viterbi parsing (none) (Left)
and iterative parsing (iterative) (Right)

 
 
 


 
 
	 

 
 
               
                      Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155?163,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extremely Lexicalized Models for Accurate and Fast HPSG Parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
Yoshimasa Tsuruoka
School of Informatics
University of Manchester
Yusuke Miyao
Department of Computer Science
University of Tokyo
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
SORST, Japan Science and Technology Agency
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
{ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes an extremely lexi-
calized probabilistic model for fast and
accurate HPSG parsing. In this model,
the probabilities of parse trees are de-
fined with only the probabilities of select-
ing lexical entries. The proposed model
is very simple, and experiments revealed
that the implemented parser runs around
four times faster than the previous model
and that the proposed model has a high
accuracy comparable to that of the previ-
ous model for probabilistic HPSG, which
is defined over phrase structures. We
also developed a hybrid of our probabilis-
tic model and the conventional phrase-
structure-based model. The hybrid model
is not only significantly faster but also sig-
nificantly more accurate by two points of
precision and recall compared to the pre-
vious model.
1 Introduction
For the last decade, accurate and wide-coverage
parsing for real-world text has been intensively
and extensively pursued. In most of state-of-the-
art parsers, probabilistic events are defined over
phrase structures because phrase structures are
supposed to dominate syntactic configurations of
sentences. For example, probabilities were de-
fined over grammar rules in probabilistic CFG
(Collins, 1999; Klein and Manning, 2003; Char-
niak and Johnson, 2005) or over complex phrase
structures of head-driven phrase structure gram-
mar (HPSG) or combinatory categorial grammar
(CCG) (Clark and Curran, 2004b; Malouf and van
Noord, 2004; Miyao and Tsujii, 2005). Although
these studies vary in the design of the probabilistic
models, the fundamental conception of probabilis-
tic modeling is intended to capture characteristics
of phrase structures or grammar rules. Although
lexical information, such as head words, is known
to significantly improve the parsing accuracy, it
was also used to augment information on phrase
structures.
Another interesting approach to this problem
was using supertagging (Clark and Curran, 2004b;
Clark and Curran, 2004a; Wang and Harper, 2004;
Nasr and Rambow, 2004), which was originally
developed for lexicalized tree adjoining grammars
(LTAG) (Bangalore and Joshi, 1999). Supertag-
ging is a process where words in an input sen-
tence are tagged with ?supertags,? which are lex-
ical entries in lexicalized grammars, e.g., elemen-
tary trees in LTAG, lexical categories in CCG,
and lexical entries in HPSG. Supertagging was,
in the first place, a technique to reduce the cost
of parsing with lexicalized grammars; ambiguity
in assigning lexical entries to words is reduced
by the light-weight process of supertagging be-
fore the heavy process of parsing. Bangalore and
Joshi (1999) claimed that if words can be assigned
correct supertags, syntactic parsing is almost triv-
ial. What this means is that if supertags are cor-
rectly assigned, syntactic structures are almost de-
155
termined because supertags include rich syntac-
tic information such as subcategorization frames.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated
in the case of a CCG parser (Clark and Curran,
2004a) with the result of a drastic improvement in
the parsing speed. Wang and Harper (2004) also
demonstrated the effects of supertagging with a
statistical constraint dependency grammar (CDG)
parser. They achieved accuracy as high as the
state-of-the-art parsers. However, a supertagger it-
self was used as an external tagger that enumerates
candidates of lexical entries or filters out unlikely
lexical entries just to help parsing, and the best
parse trees were selected mainly according to the
probabilistic model for phrase structures or depen-
dencies with/without the probabilistic model for
supertagging.
We investigate an extreme case of HPSG pars-
ing in which the probabilistic model is defined
with only the probabilities of lexical entry selec-
tion; i.e., the model is never sensitive to charac-
teristics of phrase structures. The model is simply
defined as the product of the supertagging proba-
bilities, which are provided by the discriminative
method with machine learning features of word
trigrams and part-of-speech (POS) 5-grams as de-
fined in the CCG supertagging (Clark and Curran,
2004a). The model is implemented in an HPSG
parser instead of the phrase-structure-based prob-
abilistic model; i.e., the parser returns the parse
tree assigned the highest probability of supertag-
ging among the parse trees licensed by an HPSG.
Though the model uses only the probabilities of
lexical entry selection, the experiments revealed
that it was as accurate as the previous phrase-
structure-based model. Interestingly, this means
that accurate parsing is possible using rather sim-
ple mechanisms.
We also tested a hybrid model of the su-
pertagging and the previous phrase-structure-
based probabilistic model. In the hybrid model,
the probabilities of the previous model are mul-
tiplied by the supertagging probabilities instead
of a preliminary probabilistic model, which is in-
troduced to help the process of estimation by fil-
tering unlikely lexical entries (Miyao and Tsujii,
2005). In the previous model, the preliminary
probabilistic model is defined as the probability
of unigram supertagging. So, the hybrid model
can be regarded as an extension of supertagging
from unigram to n-gram. The hybrid model can
also be regarded as a variant of the statistical CDG
parser (Wang, 2003; Wang and Harper, 2004), in
which the parse tree probabilities are defined as
the product of the supertagging probabilities and
the dependency probabilities. In the experiments,
we observed that the hybrid model significantly
improved the parsing speed, by around three to
four times speed-ups, and accuracy, by around two
points in both precision and recall, over the pre-
vious model. This implies that finer probabilistic
model of lexical entry selection can improve the
phrase-structure-based model.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemata describe gen-
eral construction rules, and a large number of
lexical entries express word-specific characteris-
tics. The structures of sentences are explained us-
ing combinations of schemata and lexical entries.
Both schemata and lexical entries are represented
by typed feature structures, and constraints repre-
sented by feature structures are checked with uni-
fication.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
parse result is output as a phrasal sign that domi-
nates the sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries de-
termine the dominant syntactic structures.
156
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model
or maximum entropy model (Berger et al, 1996).
The probability that a parse result T is assigned to
a given sentence w = ?w1, . . . , wn? is
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature
function that represents a characteristic of parse
tree T , and Zw is the sum over the set of all pos-
sible parse trees for the sentence. Intuitively, the
probability is defined as the normalized product
of the weights exp(?u) when a characteristic cor-
responding to fu appears in parse result T . The
model parameters, ?u, are estimated using numer-
ical optimization methods (Malouf, 2002) to max-
imize the log-likelihood of the training data.
However, the above model cannot be easily es-
timated because the estimation requires the com-
putation of p(T |w) for all parse candidates as-
signed to sentence w. Because the number of
parse candidates is exponentially related to the
length of the sentence, the estimation is intractable
for long sentences. To make the model estimation
tractable, Geman and Johnson (Geman and John-
son, 2002) and Miyao and Tsujii (Miyao and Tsu-
jii, 2002) proposed a dynamic programming algo-
rithm for estimating p(T |w). Miyao and Tsujii
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(2005) also introduced a preliminary probabilistic
model p0(T |w) whose estimation does not require
the parsing of a treebank. This model is intro-
duced as a reference distribution of the probabilis-
tic HPSG model; i.e., the computation of parse
trees given low probabilities by the model is omit-
ted in the estimation stage. We have
(Previous probabilistic HPSG)
phpsg?(T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in T
and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model
with the probabilistic HPSG model of Miyao and
Tsujii (2005). The features used in their model are
combinations of the feature templates listed in Ta-
ble 1. The feature templates fbinary and funary
are defined for constituents at binary and unary
branches, froot is a feature template set for the
root nodes of parse trees, and flex is a feature tem-
plate set for calculating the preliminary probabilis-
tic model. An example of features applied to the
parse tree for the sentence ?Spring has come? is
shown in Figure 2.
157
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Features.
3 Extremely lexicalized probabilistic
models
In the experiments, we tested parsing with the pre-
vious model for the probabilistic HPSG explained
in Section 2 and other three types of probabilis-
tic models defined with the probabilities of lexi-
cal entry selection. The first one is the simplest
probabilistic model, which is defined with only
the probabilities of lexical entry selection. It is
defined simply as the product of the probabilities
of selecting all lexical entries in the sentence; i.e.,
the model does not use the probabilities of phrase
structures like the previous models.
Given a set of lexical entries, L, a sentence,
w = ?w1, . . . , wn?, and the probabilistic model
of lexical entry selection, p(li ? L|w, i), the first
model is formally defined as follows:
(Model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi
in T and p(li|w, i) is the probability of selecting
lexical entry li for wi.
The second model is defined as the product of
the probabilities of selecting all lexical entries in
the sentence and the root node probability of the
parse tree. That is, the second model is also de-
fined without the probabilities on phrase struc-
tures:
(Model 2)
pmodel2(T |w) =
1
Zmodel2 pmodel1(T |w) exp
?
??
?
u
(fu?froot)
?ufu(T )
?
??
Zmodel2 =
?
T ?
pmodel1(T ?|w) exp
?
??
?
u
(fu?froot)
?ufu(T ?)
?
?? ,
where Zmodel2 is the sum over the set of all pos-
sible parse trees for the sentence.
The third model is a hybrid of model 1 and the
previous model. The probabilities of the lexical
entries in the previous model are replaced with the
probabilities of lexical entry selection:
(Model 3)
pmodel3(T |w) =
1
Zmodel3 pmodel1(T |w) exp
(?
u
?ufu(T )
)
Zmodel3 =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In this study, the same model parameters used
in the previous model were used for phrase struc-
tures.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic Model of Lexical Entry Selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
158
fexlex =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
combinations of feature templates
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Features for the probabilities of lexical
entry selection.
procedure Parsing(?w1, . . . , wn?, ?L,R?, ?, ?, ?, ?, ?)
for i = 1 to n
foreach F ? ? {F |?wi, F ? ? L}
p =
?
u ?ufu(F
?)
pi[i? 1, i] ? pi[i? 1, i] ? {F ?}
if (p > ?[i? 1, i, F ?]) then
?[i? 1, i, F ?] ? p
LocalThresholding(i? 1, i,?, ?)
for d = 1 to n
for i = 0 to n? d
j = i+ d
for k = i+ 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if F = r(Fs, Ft) has succeeded
p = ?[i, k, Fs] + ?[k, j, Ft] +
?
u ?ufu(F )
pi[i, j] ? pi[i, j] ? {F}
if (p > ?[i, j, F ]) then
?[i, j, F ] ? p
LocalThresholding(i, j,?, ?)
GlobalThresholding(i, n, ?)
procedure IterativeParsing(w, G, ?0, ?0, ?0, ?0, ?0, ??, ??, ??,
??, ??, ?last, ?last, ?last, ?last, ?last)? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0;
loop while ? ? ?last and ? ? ?last and ? ? ?last and ? ? ?last
and ? ? ?last
call Parsing(w, G, ?, ?, ?, ?, ?)
if pi[1, n] 6= ? then exit
? ? ?+??; ? ? ? +??;
? ? ?+??; ? ? ? +??; ? ? ? +??;
Figure 3: Pseudo-code of iterative parsing for
HPSG.
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical en-
tries for the word wi. The feature templates used
in our model are listed in Table 2 and are word
trigrams and POS 5-grams.
4 Experiments
4.1 Implementation
We implemented the iterative parsing algorithm
(Ninomiya et al, 2005) for the probabilistic HPSG
models. It first starts parsing with a narrow beam.
If the parsing fails, then the beam is widened, and
parsing continues until the parser outputs results
or the beam width reaches some limit. Though
the probabilities of lexical entry selection are in-
troduced, the algorithm for the presented proba-
bilistic models is almost the same as the original
iterative parsing algorithm.
The pseudo-code of the algorithm is shown in
Figure 3. In the figure, the pi[i, j] represents
the set of partial parse results that cover words
wi+1, . . . , wj , and ?[i, j, F ] stores the maximum
figure-of-merit (FOM) of partial parse result F
at cell (i, j). The probability of lexical entry
F is computed as ?u ?ufu(F ) for the previous
model, as shown in the figure. The probability
of a lexical entry for models 1, 2, and 3 is com-
puted as the probability of lexical entry selection,
p(F |w, i). The FOM of a newly created partial
parse, F , is computed by summing the values of
? of the daughters and an additional FOM of F if
the model is the previous model or model 3. The
FOM for models 1 and 2 is computed by only sum-
ming the values of ? of the daughters; i.e., weights
exp(?u) in the figure are assigned zero. The terms
? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs
in the chart cell. The terms ? and ? are the thresh-
olds of the number and the beam width of lexical
entries, and ? is the beam width for global thresh-
olding (Goodman, 1997).
4.2 Evaluation
We evaluated the speed and accuracy of parsing
with extremely lexicalized models by using Enju
2.1, the HPSG grammar for English (Miyao et al,
2005; Miyao and Tsujii, 2005). The lexicon of
the grammar was extracted from Sections 02-21 of
the Penn Treebank (Marcus et al, 1994) (39,832
sentences). The grammar consisted of 3,797 lex-
ical entries for 10,536 words1. The probabilis-
tic models were trained using the same portion of
the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and other tech-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by ap-
plying lexical rules to observed lexical entries in the HPSG
treebank (Nakanishi et al, 2004). The lexicon, however, in-
cluded many lexical entries that do not appear in the HPSG
treebank. The HPSG treebank is used for training the prob-
abilistic model for lexical entry selection, and hence, those
lexical entries that do not appear in the treebank are rarely
selected by the probabilistic model. The ?effective? tag set
size, therefore, is around 1,361, the number of lexical entries
without those never-seen lexical entries.
159
No. of tested sentences Total No. of Avg. length of tested sentences
? 40 words ? 100 words sentences ? 40 words ? 100 words
Section 23 2,162 (94.04%) 2,299 (100.00%) 2,299 20.7 22.2
Section 24 1,157 (92.78%) 1,245 (99.84%) 1,247 21.2 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (? 40 + Gold POSs) Section 23 (? 100 + Gold POSs)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 87.65 86.97 91.13 90.42 468 87.26 86.50 90.73 89.93 604
model 1 87.54 86.85 90.38 89.66 111 87.23 86.47 90.05 89.27 129
model 2 87.71 87.02 90.51 89.80 109 87.38 86.62 90.17 89.39 130
model 3 89.79 88.97 92.66 91.81 132 89.48 88.58 92.33 91.40 152
Section 23 (? 40 + POS tagger) Section 23 (? 100 + POS tagger)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 85.33 84.83 89.93 89.41 509 84.96 84.25 89.55 88.80 674
model 1 85.26 84.31 89.17 88.18 133 85.00 84.01 88.85 87.82 154
model 2 85.37 84.42 89.25 88.26 134 85.08 84.09 88.91 87.88 155
model 3 87.66 86.53 91.61 90.43 155 87.35 86.29 91.24 90.13 183
Table 4: Experimental results for Section 23.
niques for deep parsing2. The parameters for beam
searching were determined manually by trial and
error using Section 22: ?0 = 4,?? = 4, ?last =
20, ?0 = 1.0,?? = 2.5, ?last = 11.0, ?0 =
12,?? = 4, ?last = 28, ?0 = 6.0,?? =
2.25, ?last = 15.0, ?0 = 8.0,?? = 3.0, and
?last = 20.0. With these thresholding parame-
ters, the parser iterated at most five times for each
sentence.
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tu-
ple ??,wh, a, wa?, where ? is the predicate type
(e.g., adjective, intransitive verb), wh is the head
word of the predicate, a is the argument label
(MODARG, ARG1, ..., ARG4), and wa is the
head word of the argument. Labeled precision
(LP)/labeled recall (LR) is the ratio of tuples cor-
rectly identified by the parser3. Unlabeled pre-
cision (UP)/unlabeled recall (UR) is the ratio of
tuples without the predicate type and the argu-
ment label. This evaluation scheme was the
same as used in previous evaluations of lexicalized
grammars (Hockenmaier, 2003; Clark and Cur-
2Deep parsing techniques include quick check (Malouf
et al, 2000) and large constituent inhibition (Kaplan et al,
2004) as described by Ninomiya et al (2005), but hybrid
parsing with a CFG chunk parser was not used. This is be-
cause we did not observe a significant improvement for the
development set by the hybrid parsing and observed only a
small improvement in the parsing speed by around 10 ms.
3When parsing fails, precision and recall are evaluated,
although nothing is output by the parser; i.e., recall decreases
greatly.
ran, 2004b; Miyao and Tsujii, 2005). The ex-
periments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the
Treebank was used as the development set, and
the performance was evaluated using sentences of
? 40 and 100 words in Section 23. The perfor-
mance of each parsing technique was analyzed us-
ing the sentences in Section 24 of ? 100 words.
Table 3 details the numbers and average lengths of
the tested sentences of ? 40 and 100 words in Sec-
tions 23 and 24, and the total numbers of sentences
in Sections 23 and 24.
The parsing performance for Section 23 is
shown in Table 4. The upper half of the table
shows the performance using the correct POSs in
the Penn Treebank, and the lower half shows the
performance using the POSs given by a POS tag-
ger (Tsuruoka and Tsujii, 2005). The left and
right sides of the table show the performances for
the sentences of ? 40 and ? 100 words. Our
models significantly increased not only the pars-
ing speed but also the parsing accuracy. Model
3 was around three to four times faster and had
around two points higher precision and recall than
the previous model. Surprisingly, model 1, which
used only lexical information, was very fast and
as accurate as the previous model. Model 2 also
improved the accuracy slightly without informa-
tion of phrase structures. When the automatic POS
tagger was introduced, both precision and recall
dropped by around 2 points, but the tendency to-
wards improved speed and accuracy was again ob-
160
76.00%
78.00%
80.00%
82.00%
84.00%
86.00%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F
-
s
c
o
r
e
previous model
model 1
model 2
model 3
Figure 4: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
served.
The unlabeled precisions and recalls of the pre-
vious model and models 1, 2, and 3 were signifi-
cantly different as measured using stratified shuf-
fling tests (Cohen, 1995) with p-values < 0.05.
The labeled precisions and recalls were signifi-
cantly different among models 1, 2, and 3 and
between the previous model and model 3, but
were not significantly different between the previ-
ous model and model 1 and between the previous
model and model 2.
The average parsing time and labeled F-score
curves of each probabilistic model for the sen-
tences in Section 24 of? 100 words are graphed in
Figure 4. The superiority of our models is clearly
observed in the figure. Model 3 performed sig-
nificantly better than the previous model. Models
1 and 2 were significantly faster with almost the
same accuracy as the previous model.
5 Discussion
5.1 Supertagging
Our probabilistic model of lexical entry selection
can be used as an independent classifier for select-
ing lexical entries, which is called the supertag-
ger (Bangalore and Joshi, 1999; Clark and Curran,
2004b). The CCG supertagger uses a maximum
entropy classifier and is similar to our model.
We evaluated the performance of our probabilis-
tic model as a supertagger. The accuracy of the re-
sulting supertagger on our development set (Sec-
tion 22) is given in Table 5 and Table 6. The test
sentences were automatically POS-tagged. Re-
sults of other supertaggers for automatically ex-
test data accuracy (%)
HPSG supertagger 22 87.51
(this paper)
CCG supertagger 00/23 91.70 / 91.45
(Curran and Clark, 2003)
LTAG supertagger 22/23 86.01 / 86.27
(Shen and Joshi, 2003)
Table 5: Accuracy of single-tag supertaggers. The
numbers under ?test data? are the PTB section
numbers of the test data.
? tags/word word acc. (%) sentence acc. (%)
1e-1 1.30 92.64 34.98
1e-2 2.11 95.08 46.11
1e-3 4.66 96.22 51.95
1e-4 10.72 96.83 55.66
1e-5 19.93 96.95 56.20
Table 6: Accuracy of multi-supertagging.
tracted lexicalized grammars are listed in Table 5.
Table 6 gives the average number of supertags as-
signed to a word, the per-word accuracy, and the
sentence accuracy for several values of ?, which is
a parameter to determine how many lexical entries
are assigned.
When compared with other supertag sets of au-
tomatically extracted lexicalized grammars, the
(effective) size of our supertag set, 1,361 lexical
entries, is between the CCG supertag set (398 cat-
egories) used by Curran and Clark (2003) and the
LTAG supertag set (2920 elementary trees) used
by Shen and Joshi (2003). The relative order based
on the sizes of the tag sets exactly matches the or-
der based on the accuracies of corresponding su-
pertaggers.
161
5.2 Efficacy of extremely lexicalized models
The implemented parsers of models 1 and 2 were
around four times faster than the previous model
without a loss of accuracy. However, what sur-
prised us is not the speed of the models, but
the fact that they were as accurate as the previ-
ous model, though they do not use any phrase-
structure-based probabilities. We think that the
correct parse is more likely to be selected if the
correct lexical entries are assigned high probabil-
ities because lexical entries include specific infor-
mation about subcategorization frames and syn-
tactic alternation, such as wh-movement and pas-
sivization, that likely determines the dominant
structures of parse trees. Another possible rea-
son for the accuracy is the constraints placed by
unification-based grammars. That is, incorrect
parse trees were suppressed by the constraints.
The best performer in terms of speed and ac-
curacy was model 3. The increased speed was,
of course, possible for the same reasons as the
speeds of models 1 and 2. An unexpected but
very impressive result was the significant improve-
ment of accuracy by two points in precision and
recall, which is hard to attain by tweaking param-
eters or hacking features. This may be because
the phrase structure information and lexical in-
formation complementarily improved the model.
The lexical information includes more specific in-
formation about the syntactic alternation, and the
phrase structure information includes information
about the syntactic structures, such as the dis-
tances of head words or the sizes of phrases.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. We ex-
emplified the dominance of lexical information in
real syntactic parsing, i.e., syntactic parsing with-
out gold-supertags, by showing that the proba-
bilities of lexical entry selection dominantly con-
tributed to syntactic parsing.
The CCG supertagging demonstrated fast and
accurate parsing for the probabilistic CCG (Clark
and Curran, 2004a). They used the supertag-
ger for eliminating candidates of lexical entries,
and the probabilities of parse trees were calcu-
lated using the phrase-structure-based model with-
out the probabilities of lexical entry selection. Our
study is essentially different from theirs in that the
probabilities of lexical entry selection have been
demonstrated to dominantly contribute to the dis-
ambiguation of phrase structures.
We have not yet investigated whether our results
can be reproduced with other lexicalized gram-
mars. Our results might hold only for HPSG be-
cause HPSG has strict feature constraints and has
lexical entries with rich syntactic information such
as wh-movement.
6 Conclusion
We developed an extremely lexicalized probabilis-
tic model for fast and accurate HPSG parsing.
The model is very simple. The probabilities of
parse trees are defined with only the probabili-
ties of selecting lexical entries, which are trained
by the discriminative methods in the log-linear
model with features of word trigrams and POS 5-
grams as defined in the CCG supertagging. Ex-
periments revealed that the model achieved im-
pressive accuracy as high as that of the previous
model for the probabilistic HPSG and that the im-
plemented parser runs around four times faster.
This indicates that accurate and fast parsing is pos-
sible using rather simple mechanisms. In addi-
tion, we provided another probabilistic model, in
which the probabilities for the leaf nodes in a parse
tree are given by the probabilities of supertag-
ging, and the probabilities for the intermediate
nodes are given by the previous phrase-structure-
based model. The experiments demonstrated not
only speeds significantly increased by three to four
times but also impressive improvement in parsing
accuracy by around two points in precision and re-
call.
We hope that this research provides a novel ap-
proach to deterministic parsing in which only lex-
ical selection and little phrasal information with-
out packed representations dominates the parsing
strategy.
References
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23(4):597?
618.
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
162
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL?05, pages 173?180.
Stephen Clark and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Proc.
of ACL?04, pages 104?111.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Univ. of Pennsylvania.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proc. of EACL?03, pages 91?98.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Proc.
of ACL ?99, pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL?03,
pages 423?430.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Proc. of IJCNLP-04 Workshop ?Be-
yond Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake.
2000. Efficient feature structure operations with-
out compilation. Journal of Natural Language En-
gineering, 6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii,
2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok Lee
and Oi Yee Kwong (Eds.), Natural Language Pro-
cessing - IJCNLP 2004 LNAI 3248, chapter Corpus-
oriented Grammar Development for Acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lex-
ical rules on parsing with a treebank grammar. In
Proc. of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic hpsg parsing. In Proc. of IWPT
2005, pages 103?114.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling
of constraint-based grammars using log-linear mea-
sures and EM training. In Proc. of ACL?00, pages
480?487.
Libin Shen and Aravind K. Joshi. 2003. A SNoW
based supertagger with application to NP chunking.
In Proc. of ACL?03, pages 505?512.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proc. of HLT/EMNLP
2005, pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statisti-
cal constraint dependency grammar (CDG) parser.
In Proc. of ACL?04 Incremental Parsing work-
shop: Bringing Engineering and Cognition To-
gether, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Gram-
mar. Ph.D. thesis, Purdue University.
163
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284?292,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Construction of Predicate-argument Structure Patterns
for Biomedical Information Extraction
Akane Yakushiji? ? Yusuke Miyao? Tomoko Ohta?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
? School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
{akane, yusuke, okap, yucca, tsujii}@is.s.u-tokyo.ac.jp
Yuka Tateisi? ? Jun?ichi Tsujii? ?
Abstract
This paper presents a method of automat-
ically constructing information extraction
patterns on predicate-argument structures
(PASs) obtained by full parsing from a
smaller training corpus. Because PASs
represent generalized structures for syn-
tactical variants, patterns on PASs are ex-
pected to be more generalized than those
on surface words. In addition, patterns
are divided into components to improve
recall and we introduce a Support Vector
Machine to learn a prediction model using
pattern matching results. In this paper, we
present experimental results and analyze
them on how well protein-protein interac-
tions were extracted from MEDLINE ab-
stracts. The results demonstrated that our
method improved accuracy compared to a
machine learning approach using surface
word/part-of-speech patterns.
1 Introduction
One primitive approach to Information Extrac-
tion (IE) is to manually craft numerous extrac-
tion patterns for particular applications and this
is presently one of the main streams of biomedi-
cal IE (Blaschke and Valencia, 2002; Koike et al,
2003). Although such IE attempts have demon-
strated near-practical performance, the same sets
of patterns cannot be applied to different kinds of
information. A real-world task requires several
kinds of IE, thus manually engineering extraction
Current Affiliation:
? FUJITSU LABORATORIES LTD.
? Faculty of Informatics, Kogakuin University
patterns, which is tedious and time-consuming
process, is not really practical.
Techniques based on machine learning (Zhou et
al., 2005; Hao et al, 2005; Bunescu and Mooney,
2006) are expected to alleviate this problem in
manually crafted IE. However, in most cases, the
cost of manually crafting patterns is simply trans-
ferred to that for constructing a large amount of
training data, which requires tedious amount of
manual labor to annotate text.
To systematically reduce the necessary amount
of training data, we divided the task of construct-
ing extraction patterns into a subtask that general
natural language processing techniques can solve
and a subtask that has specific properties accord-
ing to the information to be extracted. The former
subtask is of full parsing (i.e. recognizing syntactic
structures of sentences), and the latter subtask is of
constructing specific extraction patterns (i.e. find-
ing clue words to extract information) based on the
obtained syntactic structures.
We adopted full parsing from various levels
of parsing, because we believe that it offers the
best utility to generalize sentences into normal-
ized syntactic relations. We also divided patterns
into components to improve recall and we intro-
duced machine learning with a Support Vector
Machine (SVM) to learn a prediction model us-
ing the matching results of extraction patterns. As
an actual IE task, we extracted pairs of interacting
protein names from biomedical text.
2 Full Parsing
2.1 Necessity for Full Parsing
A technique that many previous approaches have
used is shallow parsing (Koike et al, 2003; Yao
et al, 2004; Zhou et al, 2005). Their assertion is
284
Distance Count (%) Sum (%)
?1 54 5.0 5.0
0 8 0.7 5.7
1 170 15.7 21.4
2?5 337 31.1 52.5
6?10 267 24.6 77.1
11? 248 22.9 100.0
Distance ?1 means protein word has been annotated as in-
teracting with itself (e.g. ?actin polymerization?). Distance 0
means words of the interacting proteins are directly next to
one another. Multi-word protein names are concatenated as
long as they do not cross tags to annotate proteins.
Table 1: Distance between Interacting Proteins
that shallow parsers are more robust and would be
sufficient for IE. However, their claims that shal-
low parsers are sufficient, or that full parsers do
not contribute to application tasks, have not been
fully proved by experimental results.
Zhou et al (2005) argued that most informa-
tion useful for IE derived from full parsing was
shallow. However, they only used dependency
trees and paths on full parse trees in their experi-
ment. Such structures did not include information
of semantic subjects/objects, which full parsing
can recognize. Additionally, most relations they
extracted from the ACE corpus (Linguistic Data
Consortium, 2005) on broadcasts and newswires
were within very short word-distance (70% where
two entities are embedded in each other or sep-
arated by at most one word), and therefore shal-
low information was beneficial. However, Table 1
shows that the word distance is long between in-
teracting protein names annotated on the AImed
corpus (Bunescu and Mooney, 2004), and we have
to treat long-distance relations for information like
protein-protein interactions.
Full parsing is more effective for acquiring gen-
eralized data from long-length words than shallow
parsing. The sentences at left in Figure 1 exem-
plify the advantages of full parsing. The gerund
?activating? in the last sentence takes a non-local
semantic subject ?ENTITY1?, and shallow parsing
cannot recognize this relation because ?ENTITY1?
and ?activating? are in different phrases. Full pars-
ing, on the other hand, can identify both the sub-
ject of the whole sentence and the semantic subject
of ?activating? have been shared.
2.2 Predicate-argument Structures
We applied Enju (Tsujii Laboratory, 2005a) as
a full parser which outputs predicate-argument
structures (PASs). PASs are well normalized
forms that represent syntactic relations. Enju
is based on Head-driven Phrase Structure Gram-
mar (Sag and Wasow, 1999), and it has been
trained on the Penn Treebank (PTB) (Marcus et
al., 1994) and a biomedical corpus, the GENIA
Treebank (GTB) (Tsujii Laboratory, 2005b). We
used a part-of-speech (POS) tagger trained on the
GENIA corpus (Tsujii Laboratory, 2005b) as a
preprocessor for Enju. On predicate-argument re-
lations, Enju achieved 88.0% precision and 87.2%
recall on PTB, and 87.1% precision and 85.4% re-
call on GTB.
The illustration at right in Figure 1 is a PAS
example, which represents the relation between
?activate?, ?ENTITY1? and ?ENTITY2? of all sen-
tences to the left. The predicate and its argu-
ments are words converted to their base forms,
augmented by their POSs. The arrows denote
the connections from predicates to their arguments
and the types of arguments are indicated as arrow
labels, i.e., ARGn (n = 1, 2, . . .), MOD. For ex-
ample, the semantic subject of a transitive verb is
ARG1 and the semantic object is ARG2.
What is important here is, thanks to the strong
normalization of syntactic variations, that we can
expect that the construction algorithm for extract-
ing patterns that works on PASs will need a much
smaller training corpus than those working on
surface-word sequences. Furthermore, because of
the reduced diversity of surface-word sequences at
the PAS level, any IE system at this level should
demonstrate improved recall.
3 Related Work
Sudo et al (2003), Culotta and Sorensen (2004)
and Bunescu and Mooney (2005) acquired sub-
structures derived from dependency trees as ex-
traction patterns for IE in general domains. Their
approaches were similar to our approach using
PASs derived from full parsing. However, one
problem with their systems is that they could
not treat non-local dependencies such as seman-
tic subjects of gerund constructions (discussed in
Section 2), and thus rules acquired from the con-
structions were partial.
Bunescu and Mooney (2006) also learned ex-
traction patterns for protein-protein interactions
by SVM with a generalized subsequence kernel.
Their patterns are sequences of words, POSs, en-
tity types, etc., and they heuristically restricted
length and word positions of the patterns. Al-
285
ENTITY1 recognizes and activates ENTITY2.
ENTITY2 activated by ENTITY1 are not well characterized.
The herpesvirus encodes a functional ENTITY1 that activates human ENTITY2.
ENTITY1 can functionally cooperate to synergistically activate ENTITY2.
The ENTITY1 plays key roles by activating ENTITY2.
ENTITY1/NN
activate/VB ENTITY2/NN
ARG1 ARG2
Figure 1: Syntactical Variations of ?activate?
though they achieved about 60% precision and
about 40% recall, these heuristic restrictions could
not be guaranteed to be applied to other IE tasks.
Hao et al (2005) learned extraction patterns
for protein-protein interactions as sequences of
words, POSs, entity tags and gaps by dynamic
programming, and reduced/merged them using a
minimum description length-based algorithm. Al-
though they achieved 79.8% precision and 59.5%
recall, sentences in their test corpus have too
many positive instances and some of the pat-
terns they claimed to have been successfully con-
structed went against linguistic or biomedical in-
tuition. (e.g. ?ENTITY1 and interacts with EN-
TITY2? should be replaced by a more general pat-
tern because they aimed to reduce the number of
patterns.)
4 Method
We automatically construct patterns to extract
protein-protein interactions from an annotated
training corpus. The corpus needs to be tagged to
denote which protein words are interacting pairs.
We follow five steps in constructing extraction
patterns from the training corpus. (1) Sentences
in the training corpus are parsed into PASs and
we extract raw patterns from the PASs. (2) We
divide the raw patterns to generate both combi-
nation and fragmental patterns. Because obtained
patterns include inappropriate ones (wrongly gen-
erated or too general), (3) we apply both kinds of
patterns to PASs of sentences in the training cor-
pus, (4) calculate the scores for matching results
of combination patterns, and (5) make a prediction
model with SVM using these matching results and
scores.
We extract pairs of interacting proteins from a
target text in the actual IE phase, in three steps.
(1) Sentences in the target corpus are parsed into
PASs. (2) We apply both kinds of extraction pat-
terns to these PASs and (3) calculate scores for
combination pattern matching. (4) We use the pre-
diction model to predict interacting pairs.
ENTITY1
ENTITY2
CD4/NN protein/NN
interact/VB
with/IN polymorphic/JJ
region/NN
of/INMHCII/NN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
ARG1
Parsing Result
Raw Pattern
CD4 protein interacts with polymorphic regions of MHCII .
ENTITY1
ENTITY2
Sentence in Training Corpus
protein/NN
interact/VB
with/IN
region/NN
of/IN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
(1) (2) (3) (4) (5) (6)
p
0
p
1
p
2
p
3
p
4
p
5
p
6
ENTITY2/NN
ENTITY1/NN
Figure 2: Extraction of Raw Pattern
4.1 Full Parsing and Extraction of Raw
Patterns
As the first step in both the construction phase and
application phase of extraction patterns, we parse
sentences into PASs using Enju.1 We label all
PASs of the protein names as protein PASs.
After parsing, we extract the smallest set of
PASs, which connect words that denote interact-
ing proteins, and make it a raw pattern. We take
the same method to extract and refine raw patterns
as Yakushiji et al (2005). Connecting means we
can trace predicate-argument relations from one
protein word to the other in an interacting pair.
The procedure to obtain a raw pattern (p0, . . . , pn)
is as follows:
predicate(p): PASs that have p as their argument
argument(p): PASs that p has as its arguments
1. pi = p0 is the PAS of a word correspondent
to one of interacting proteins, and we obtain
candidates of the raw pattern as follows:
1-1. If pi is of the word of the other interact-
ing protein, (p0, . . . , pi) is a candidate
of the raw pattern.
1-2. If not, make pattern candidates
for each pi+1 ? predicate(pi) ?
argument(pi) ? {p0, . . . , pi} by
returning to 1-1.
2. Select the pattern candidate of the smallest
set as the raw pattern.
1Before parsing, we concatenate each multi-word protein
name into the one word as long as the concatenation does not
cross name boundaries.
286
3. Substitute variables (ENTITY1, ENTITY2) for
the predicates of PASs correspondent to the
interacting proteins.
The lower part of Figure 2 shows an example
of the extraction of a raw pattern. ?CD4? and
?MHCII? are words representing interacting pro-
teins. First, we set the PAS of ?CD4? as p0.
argument(p0) includes the PAS of ?protein?, and
we set it as p1 (in other words, tracing the arrow
(1)). Next, predicate(p1) includes the PAS of ?in-
teract? (tracing the arrow (2) back), so we set it
as p2. We continue similarly until we reach the
PAS of ?MHCII? (p6). The result of the extracted
raw pattern is the set of p0, . . . , p6 with substitut-
ing variables ENTITY1 and ENTITY2 for ?CD4?
and ?MHCII?.
There are some cases where an extracted raw
pattern is not appropriate and we need to re-
fine it. One case is when unnecessary coordi-
nations/parentheses are included in the pattern,
e.g. two interactions are described in a combined
representation (?ENTITY1 binds this protein and
ENTITY2?). Another is when two interacting pro-
teins are connected directly by a conjunction or
only one protein participates in an interaction. In
such cases, we refine patterns by unfolding of co-
ordinations/parentheses and extension of patterns,
respectively. We have omitted detailed explana-
tions because of space limitations. The details are
described in the work of Yakushiji et al (2005).
4.2 Division of Patterns
Division for generating combination patterns is
based on observation of Yakushiji et al (2005) that
there are many cases where combinations of verbs
and certain nouns form IE patterns. In the work
of Yakushiji et al (2005), we divided only patterns
that include only one verb. We have extended the
division process to also treat nominal patterns or
patterns that include more than one verb.
Combination patterns are not appropriate for
utilizing individual word information because they
are always used in rather strictly combined ways.
Therefore we have newly introduced fragmental
patterns which consist of independent PASs from
raw patterns, in order to use individual word infor-
mation for higher recall.
4.2.1 Division for Generating Combination
Patterns
Raw patterns are divided into some compo-
nents and the components are combined to con-
ENTITY1/NN protein/NN interact/VBwith/IN region/NN
of/IN
ENTITY2/NN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
*/VBwith/IN
ARG1ARG2
*/NN
ENTITY/NN
protein/NN
MOD
region/NN of/INENTITY/NN
ARG1
ARG2
interact/VB
ARG1
=
*/NN
*/VB
ARG1
*/NN
=
$X
$X
Main
Prep
Entity
Entity
Entity
MainEntity Main
Main
Entity
Raw Pattern
Combination Pattern
Figure 3: Division of Raw Pattern into Combina-
tion Pattern Components (Entity-Main-Entity)
struct combination patterns according to types of
the division. There are three types of division of
raw patterns for generating combination patterns.
These are:
(a) Two-entity Division
(a-1) Entity-Main-Entity Division
(a-2) Main-Entity-Entity Division
(b) Single-entity Division, and
(c) No Division (Naive Patterns).
Most raw patterns, where entities are at both
ends of the patterns, are divided into Entity-Main-
Entity. Main-Entity-Entity are for the cases where
there are PASs other than entities at the ends of
the patterns (e.g. ?interaction between ENTITY1
and ENTITY2?). Single-entity is a special Main-
Entity-Entity for interactions with only one partic-
ipant (e.g. ?ENTITY1 dimerization?).
There is an example of Entity-Main-Entity divi-
sion in Figure 3. First, the main component from
the raw pattern is the syntactic head PAS of the
raw pattern. If the raw pattern corresponds to a
sentence, the syntactic head PAS is the PAS of the
main verb. We underspecify the arguments of the
main component, to enable them to unify with the
PASs of any words with the same POSs. Next, if
there are PASs of prepositions connecting to the
main component, they become prep components.
If there is no PAS of a preposition next to the main
component on the connecting link from the main
component to an entity, we make the pseudo PAS
of a null preposition the prep component. The left
prep component ($X) in Figure 3 is a pseudo PAS
of a null preposition. We also underspecify the ar-
guments of prep components. Finally, the remain-
ing two parts, which are typically noun phrases, of
the raw pattern become entity components. PASs
287
corresponding to the entities of the original pair
are labeled as only unifiable with the entities of
other pairs.
Main-Entity-Entity division is similar, except
we distinguish only one prep component as a
double-prep component and the PAS of the coor-
dinate conjunction between entities becomes the
coord component. Single-entity division is simi-
lar to Main-Entity-Entity division and the differ-
ence is that single-entity division produces no co-
ord and one entity component. Naive patterns are
patterns without division, where no division can be
applied (e.g. ?ENTITY1/NN in/IN complexes/NN
with/IN ENTITY2/NN?).
All PASs on boundaries of components are la-
beled to determine which PAS on a boundary of
another component can be unified. Labels are rep-
resented by subscriptions in Figure 3. These re-
strictions on component connection are used in the
step of constructing combination patterns.
Constructing combination patterns by combin-
ing components is equal to reconstructing orig-
inal raw patterns with the original combination
of components, or constructing new raw patterns
with new combinations of components. For exam-
ple, an Entity-Main-Entity pattern is constructed
by combination of any main, any two prep and any
two entity components. Actually, this construction
process by combination is executed in the pattern
matching step. That is, we do not off-line con-
struct all possible combination patterns from the
components and only construct the combination
patterns that are able to match the target.
4.2.2 Division for Generating Fragmental
Patterns
A raw pattern is splitted into individual PASs
and each PAS becomes a fragmental pattern. We
also prepare underspecified patterns where one or
more of the arguments of the original are under-
specified, i.e., are able to match any words of
the same POSs and the same label of protein/not-
protein. We underspecify the PASs of entities in
fragmental patterns to enable them to unify with
any PASs with the same POSs and a protein la-
bel, although in combination patterns we retain the
PASs of entities as only unifiable with entities of
pairs. This is because fragmental patterns are de-
signed to be less strict than combination patterns.
4.3 Pattern Matching
Matching of combination patterns is executed as
a process to match and combine combination pat-
tern components according to their division types
(Entity-Main-Entity, Main-Entity-Entity, Single-
entity and No Division). Fragmental matching is
matching all fragmental patterns to PASs derived
from sentences.
4.4 Scoring for Combination Matching
We next calculate the score of each combination
matching to estimate the adequacy of the combina-
tion of components. This is because new combina-
tion of components may form inadequate patterns.
(e.g. ?ENTITY1 be ENTITY2? can be formed of
components from ?ENTITY1 be ENTITY2 recep-
tor?.) Scores are derived from the results of com-
bination matching to the source training corpus.
We apply the combination patterns to the train-
ing corpus, and count pairs of True Positives (TP)
and False Positives (FP). The scores are calculated
basically by the following formula:
Score = TP/(TP + FP ) + ? ? TP
This formula is based on the precision of the pat-
tern on the training corpus, i.e., an estimated pre-
cision on a test corpus. ? works for smoothing,
that is, to accept only patterns of large TP when
FP = 0. ? is set as 0.01 empirically. The formula
is similar to the Apriori algorithm (Agrawal and
Srikant, 1995) that learns association rules from a
database. The first term corresponds to the confi-
dence of the algorithm, and the second term corre-
sponds to the support.
For patterns where TP = FP = 0, which
are not matched to PASs in the training corpus
(i.e., newly produced by combinations of com-
ponents), we estimates TP ? and FP ? by using
the confidence of the main and entity compo-
nents. This is because main and entity components
tend to contain pattern meanings, whereas prep,
double-prep and coord components are rather
functional. The formulas to calculate the scores
for all cases are:
Score =
8
>
>
>
<
>
>
>
:
TP/(TP + FP ) + ? ? TP
(TP + FP ?= 0)
TP ?/(TP ? + FP ?)
(TP = FP = 0, TP ? + FP ? ?= 0)
0 (TP = FP = TP ? = FP ? = 0)
288
Combination Pattern
(1) Combination of components in combination
matching
(2) Main component in combination matching
(3) Entity components in combination matching
(4) Score for combination matching (SCORE)
Fragmental Pattern
(5) Matched fragmental patterns
(6) Number of PASs of example that are not matched
in fragmental matching
Raw Pattern
(7) Length of raw pattern derived from example
Table 2: Features for SVM Learning of Prediction
Model
TP ? =
8
>
<
>
:
TP ?main + TP ?entity1(+TP ?entity2)
(for Two-entity, Single-entity)
0 (for Naive)
FP ? = (similar to TP ? but TP ?x is replaced by FP ?x)
TP ?main =
8
>
>
>
>
>
<
>
>
>
>
>
:
TPmain:two/(TPmain:two + FPmain:two)
 
TPmain:two + FPmain:two ?= 0,
for Two-entity
!
TPmain:single/(TPmain:single + FPmain:single)
 
TPmain:single + FPmain:single ?= 0,
for Single-entity
!
0 (other cases)
TP ?entityi =
8
>
<
>
:
TPentityi/(TPentityi + FPentityi)
?
TPentityi + FPentityi ?= 0
?
0 (other cases)
FP ?x =
?
similar to TP ?x but TP ?y in the
numerators is replaced by FP ?y
?
? TP : number of TPs by the combination of components
? TPmain:two: sum of TPs by two-entity combinations
that include the same main component
? TPmain:single: sum of TPs by single-entity combina-
tions that include the same main component
? TPentityi: sum of TPs by combinations that include
the same entity component which is not the straight en-
tity component
? FPx: similar to TPx but TP is replaced by FP
The entity component ?ENTITY/NN?, which
only consists of the PAS of an entity, adds no infor-
mation to combinations of components. We call
this component a straight entity component and
exclude its effect from the scores.
4.5 Construction of Prediction Model
We use an SVM to learn a prediction model to de-
termine whether a new protein pair is interacting.
We used SV M light (Joachims, 1999) with an rbf
kernel, which is known as the best kernel for most
tasks. The prediction model is based on the fea-
tures of Table 2.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Prec
ision
Recall
ALLSCOREERK
Figure 4: Results of IE Experiment
ENTITY1
FGF-2/NN
bind well to FGFR1 but
interact/VB with/IN
poorly/RB
ENTITY2
KGFR/NN
ARG1 ARG1 ARG2
Figure 5: Example Demonstrating Advantages of
Full Parsing
5 Results and Discussion
5.1 Experimental Results on the AImed
Corpus
To evaluate extraction patterns automatically con-
structed with our method, we used the AImed cor-
pus, which consists of 225 MEDLINE (U.S. Na-
tional Library of Medicine, 2006) abstracts (1969
sentences) annotated with protein names and
protein-protein interactions, for the training/test
corpora. We used tags for the protein names given.
We measured the accuracy of the IE task using
the same criterion as Bunescu andMooney (2006),
who used an SVM to construct extraction patterns
on word/POS/type sequences from the AImed cor-
pus. That is, an extracted interaction from an ab-
stract is correct if the proteins are tagged as inter-
acting with each other somewhere in that abstract
(document-level measure).
Figure 4 plots our 10-fold cross validation and
the results of Bunescu and Mooney (2006). The
line ALL represents results when we used all fea-
tures for SVM learning. The line SCORE repre-
sents results when we extracted pairs with higher
combination matching scores than various thresh-
old values. And the line ERK represents results
by Bunescu and Mooney (2006).
The line ALL obtained our best overall F-
measure 57.3%, with 71.8% precision and 48.4%
recall. Our method was significantly better than
Bunescu and Mooney (2006) for precision be-
289
tween 50% and 80%. It also needs to be noted
that SCORE, which did not use SVM learning
and only used the combination patterns, achieved
performance comparable to that by Bunescu and
Mooney (2006) for the precision range from 50%
to 80%. And for this range, introducing the frag-
mental patterns with SVM learning raised the re-
call. This range of precision is practical for the
IE task, because precision is more important than
recall for significant interactions that tend to be
described in many abstracts (as shown by the
next experiment), and too-low recall accompa-
nying too-high precision requires an excessively
large source text.
Figure 5 shows the advantage of introducing
full parsing. ?FGF-2? and ?KGFR? is an interact-
ing protein pair. The pattern ?ENTITY1 interact
with ENTITY2? based on PASs successfully ex-
tracts this pair. However, it is difficult to extract
this pair with patterns based on surface words, be-
cause there are 5 words between ?FGF-2? and ?in-
teract?.
5.2 Experimental Results on Abstracts of
MEDLINE
We also conducted an experiment to extract in-
teracting protein pairs from a large amount of
biomedical text, i.e. about 14 million titles and
8 million abstracts in MEDLINE. We constructed
combination patterns from all 225 abstracts of the
AImed corpus, and calculated a threshold value
of combination scores that produced about 70%
precision and 30% recall on the training corpus.
We extracted protein pairs with higher combi-
nation scores than the threshold value. We ex-
cluded single-protein interactions to reduce time
consumption and we used a protein name recog-
nizer in this experiment2.
We compared the extracted pairs with a man-
ually curated database, Reactome (Joshi-Tope et
al., 2005), which published 16,564 human pro-
tein interaction pairs as pairs of Entrez Gene
IDs (U.S. National Library of Medicine, 2006).
We converted our extracted protein pairs into pairs
of Entrez Gene IDs by the protein name recog-
nizer.3 Because there may be pairs missed by Re-
2Because protein names were recognized after the pars-
ing, multi-word protein names were not concatenated.
3Although the same protein names are used for humans
and other species, these are considered to be human proteins
without checking the context. This is a fair assumption be-
cause Reactome itself infers human interaction events from
experiments on model organisms such as mice.
Total 89
Parsing Error/Failure 35
(Related to coordinations) (14)
Lack of Combination Pattern Component 33
Requiring Anaphora Resolution 9
Error in Prediction Model 8
Requiring Attributive Adjectives 5
Others 10
More than one cause can occur in one error, thus the sum of
all causes is larger than the total number of False Negatives.
Table 3: Causes of Error for FNs
actome or pairs that our processed text did not in-
clude, we excluded extracted pairs of IDs that are
not included in Reactome and excluded Reactome
pairs of IDs that do not co-occur in the sentences
of our processed text.
After this postprocessing, we found that we had
extracted 7775 human protein pairs. Of them, 155
pairs were also included in Reactome ([a] pseudo
TPs) and 7620 pairs were not included in Reac-
tome ([b] pseudo FPs). 947 pairs of Reactome
were not extracted by our system ([c] pseudo False
Negatives (FNs)). However, these results included
pairs that Reactome missed or those that only co-
occurred and were not interacting pairs in the text.
There may also have been errors with ID assign-
ment.
To determine such cases, a biologist investi-
gated 100 pairs randomly selected from pairs of
pseudo TPs, FPs and FNs retaining their ratio of
numbers. She also checked correctness of the as-
signed IDs. 2 pairs were selected from pseudo
TPs, 88 pairs were from pseudo FPs and 10 pairs
were from pseudo FNs. The biologist found that
57 pairs were actual TPs (2 pairs of pseudo TPs
and 55 pairs of pseudo FPs) and 32 pairs were ac-
tual FPs of the pseudo FPs. Thus, the precision
was 64.0% in this sample set. Furthermore, even
if we assume that all pseudo FNs are actual FNs,
the recall can be estimated by actual TPs / (actual
TPs + pseudo FNs) ? 100 = 83.8%.
These results mean that the recall of an IE sys-
tem for interacting proteins is improved for a large
amount of text even if it is low for a small corpus.
Thus, this justifies our assertion that a high degree
of precision in the low-recall range is important.
5.3 Error Analysis
Tables 3 and 4 list causes of error for FNs/FPs on
a test set of the AImed corpus using the predic-
tion model with the best F-measure with all the
290
Total 35
Requiring Attributive Adjectives 13
Corpus Error 11
Error in Prediction Model 5
Requiring Negation Words 2
Parsing Error 1
Others 3
Table 4: Causes of Error for FPs
features. Different to Subsection 5.1, we individ-
ually checked each occurring pair of interacting
proteins. The biggest problems were parsing er-
ror/failure, lack of necessary patterns and learning
of inappropriate patterns.
5.3.1 Parsing Error
As listed in Table 3, 14 (40%) of the 35 pars-
ing errors/failures were related to coordinations.
Many of these were caused by differences in the
characteristics of the PTB/GTB, the training cor-
pora for Enju, and the AImed Corpus. For ex-
ample, Enju failed to obtain the correct structure
for ?the ENTITY1 / ENTITY1 complex? because
words in the PTB/GTB are not segmented with
?/? and Enju could not be trained on such a case.
One method to solve this problem is to avoid seg-
menting words with ?/? and introducing extraction
patterns based on surface characters, such as ?EN-
TITY1/ENTITY2 complex?.
Parsing errors are intrinsic problems to IE meth-
ods using parsing. However, from Table 3, we can
conclude that the key to gaining better accuracy
is refining of the method with which the PAS pat-
terns are constructed (there were 46 related FNs)
rather than improving parsing (there were 35 FNs).
5.3.2 Lack of Necessary Patterns and
Learning of Inappropriate Patterns
There are two different reasons causing the
problems with the lack of necessary patterns and
the learning of inappropriate patterns: (1) the
training corpus was not sufficiently large to sat-
urate IE accuracy and (2) our method of pattern
construction was too limited.
Effect of Training Corpus Size To investigate
whether the training corpus was large enough to
maximize IE accuracy, we conducted experiments
on training corpora of various sizes. Figure 6 plots
graphs of F-measures by SCORE and Figure 7
plots the number of combination patterns on train-
ing corpora of various sizes. From Figures 6 and 7,
the training corpus (207 abstracts at a maximum)
 0.35
 0.4
 0.45
 0.5
 0.55
 0  50  100  150  200
F-m
eas
ure
 by
 SC
OR
E
Training Corpus Size (Number of Abstracts)
Figure 6: Effect of Training Corpus Size (1)
 0
 100
 200
 300
 400
 500
 600
 0  50  100  150  200
Nu
mb
er
Training Corpus Size (Number of Abstracts)
Raw Patterns (before division)Main ComponentEntity ComponentOther ComponentNaive Pattern
Figure 7: Effect of Training Corpus Size (2)
is not large enough. Thus increasing corpus size
will further improve IE accuracy.
Limitation of the Present Pattern Construc-
tion The limitations with our pattern construc-
tion method are revealed by the fact that we
could not achieve a high precision like Bunescu
and Mooney (2006) within the high-recall range.
Compared to theirs, one of our problems is that our
method could not handle attributives. One exam-
ple is ?binding property of ENTITY1 to ENTITY2?.
We could not obtain ?binding? because the small-
est set of PASs connecting ?ENTITY1? and ?EN-
TITY2? includes only the PASs of ?property?, ?of?
and ?to?. To handle these attributives, we need dis-
tinguish necessary attributives from those that are
general4 by semantic analysis or bootstrapping.
Another approach to improve our method is to
include local information in sentences, such as
surface words between protein names. Zhao and
Grishman (2005) reported that adding local infor-
mation to deep syntactic information improved IE
results. This approach is also applicable to IE in
other domains, where related entities are in a short
4Consider the case where a source sentence for a pattern is
?ENTITY1 is an important homodimeric protein.? (?homod-
imeric? represents that two molecules of ?ENTITY1? interact
with each other.)
291
distance like the work of Zhou et al (2005).
6 Conclusion
We proposed the use of PASs to construct pat-
terns as extraction rules, utilizing their ability to
abstract syntactical variants with the same rela-
tion. In addition, we divided the patterns for gen-
eralization, and used matching results for SVM
learning. In experiments on extracting of protein-
protein interactions, we obtained 71.8% precision
and 48.4% recall on a small corpus and 64.0% pre-
cision and 83.8% recall estimated on a large text,
which demonstrated the obvious advantages of our
method.
Acknowledgement
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
R. Agrawal and R Srikant. 1995. Mining Sequential
Patterns. In Proc. the 11th International Conference
on Data Engineering, pages 3?14.
Christian Blaschke and Alfonso Valencia. 2002. The
Frame-Based Module of the SUISEKI Informa-
tion Extraction System. IEEE Intelligent Systems,
17(2):14?20.
Razvan Bunescu and Raymond J. Mooney. 2004.
Collective information extraction with relational
markov networks. In Proc. ACL?04, pages 439?446.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Ex-
traction. In Proc. HLT/EMNLP 2005, pages 724?
731.
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. In Advances
in Neural Information Processing Systems 18, pages
171?178. MIT Press.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. ACL?04,
pages 423?429.
Yu Hao, Xiaoyan Zhu, Minlie Huang, and Ming
Li. 2005. Discovering patterns to extract protein-
protein interactions from the literature: Part II.
Bioinformatics, 21(15):3294?3300.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Advances in Kernel Methods
? Support Vector Learning. MIT-Press.
G Joshi-Tope, M Gillespie, I Vastrik, P D?Eustachio,
E Schmidt, B de Bono, B Jassal, GR Gopinath,
GR Wu, L Matthews, S Lewis, E Birney, and Stein
L. 2005. Reactome: a knowledgebase of biologi-
cal pathways. Nucleic Acids Research, 33(Database
Issue):D428?D432.
Asako Koike, Yoshiyuki Kobayashi, and Toshihisa
Takagi. 2003. Kinase Pathway Database: An
Integrated Protein-Kinase and NLP-Based Protein-
Interaction Resource. Genome Research, 13:1231?
1243.
Linguistic Data Consortium. 2005. ACE Program.
http://projects.ldc.upenn.edu/ace/.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proc. AAI ?94.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic The-
ory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In
Proc. ACL 2003, pages 224?231.
Tsujii Laboratory. 2005a. Enju - A practical HPSG
parser. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.
Tsujii Laboratory. 2005b. GENIA Project.
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.
U.S. National Library of Medicine. 2006. PubMed.
http://www.pubmed.gov.
Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and
Jun?ichi Tsujii. 2005. Biomedical information ex-
traction with predicate-argument structure patterns.
In Proc. SMBM 2005, pages 60?69.
Daming Yao, Jingbo Wang, Yanmei Lu, Nathan No-
ble, Huandong Sun, Xiaoyan Zhu, Nan Lin, Don-
ald G. Payan, Ming Li, and Kunbin Qu. 2004. Path-
wayFinder: Paving The Way Towards Automatic
Pathway Extraction. In Bioinformatics 2004: Proc.
the 2nd APBC, volume 29 of CRPIT, pages 53?62.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proc. ACL?05, pages 419?426.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proc. ACL?05, pages 427?434.
292
Proceedings of the 10th Conference on Parsing Technologies, pages 11?22,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Evaluating Impact of Re-training a Lexical Disambiguation Model
on Domain Adaptation of an HPSG Parser
Tadayoshi Hara1 Yusuke Miyao1 Jun?ichi Tsujii1;2;3
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan
2School of Computer Science, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
3NaCTeM(National Center for Text Mining)
Manchester Interdisciplinary Biocentre, University of Manchester
131 Princess St, MANCHESTER M1 7DN, UK
E-mail: fharasan, yusuke, tsujiig@is.s.u-tokyo.ac.jp
Abstract
This paper describes an effective approach
to adapting an HPSG parser trained on the
Penn Treebank to a biomedical domain. In
this approach, we train probabilities of lex-
ical entry assignments to words in a tar-
get domain and then incorporate them into
the original parser. Experimental results
show that this method can obtain higher
parsing accuracy than previous work on do-
main adaptation for parsing the same data.
Moreover, the results show that the combi-
nation of the proposed method and the exist-
ing method achieves parsing accuracy that is
as high as that of an HPSG parser retrained
from scratch, but with much lower training
cost. We also evaluated our method in the
Brown corpus to show the portability of our
approach in another domain.
1 Introduction
Domain portability is an important aspect of the ap-
plicability of NLP tools to practical tasks. There-
fore, domain adaptation methods have recently been
proposed in several NLP areas, e.g., word sense dis-
ambiguation (Chan and Ng, 2006), statistical pars-
ing (Lease and Charniak, 2005; McClosky et al,
2006), and lexicalized-grammar parsing (Johnson
and Riezler, 2000; Hara et al, 2005). Their aim was
to re-train a probabilistic model for a new domain at
low cost, and more or less successfully improved the
accuracy for the domain.
In this paper, we propose a method for adapting
an HPSG parser (Miyao and Tsujii, 2002; Ninomiya
et al, 2006) trained on the WSJ section of the Penn
Treebank (Marcus et al, 1994) to a biomedical do-
main. Our method re-trains a probabilistic model of
lexical entry assignments to words in a target do-
main, and incorporates it into the original parser.
The model of lexical entry assignments is a log-
linear model re-trained with machine learning fea-
tures only of word n-grams. Hence, the cost for the
re-training is much lower than the cost of training
the entire disambiguation model from scratch.
In the experiments, we used an HPSG parser orig-
inally trained with the Penn Treebank, and evaluated
a disambiguation model re-trained with the GENIA
treebank (Kim et al, 2003), which consists of ab-
stracts of biomedical papers. We varied the size of
a training corpus, and measured the transition of the
parsing accuracy and the cost required for parameter
estimation. For comparison, we also examined other
possible approaches to adapting the same parser. In
addition, we applied our approach to the Brown cor-
pus (Kucera and Francis, 1967) in order to examine
portability of our approach.
The experimental results revealed that by sim-
ply re-training the probabilistic model of lexical en-
try assignments we achieve higher parsing accuracy
than with a previously proposed adaptation method.
In addition, combined with the existing adaptation
method, our approach achieves accuracy as high as
that obtained by re-training the original parser from
scratch, but with much lower training cost. In this
paper, we report these experimental results in detail,
and discuss how disambiguation models of lexical
entry assignments contribute to domain adaptation.
In recent years, it has been shown that lexical in-
11
formation plays a very important role for high accu-
racy of lexicalized grammar parsing. Bangalore and
Joshi (1999) indicated that, correct disambiguation
with supertagging, i.e., assignment of lexical entries
before parsing, enabled effective LTAG (Lexical-
ized Tree-Adjoining Grammar) parsing. Clark and
Curran (2004a) showed that supertagging reduced
cost for training and execution of a CCG (Combina-
tory Categorial Grammar) parser while keeping ac-
curacy. Clark and Curran (2006) showed that a CCG
parser trained on data derived from lexical category
sequences alone was only slightly less accurate than
one trained on complete dependency structures. Ni-
nomiya et al (2006) also succeeded in significantly
improving speed and accuracy of HPSG parsing by
using supertagging probabilities. These results indi-
cate that the probability of lexical entry assignments
is essential for parse disambiguation.
Such usefulness of lexical information has also
been shown for domain adaptation methods. Lease
and Charniak (2005) showed how existing domain-
specific lexical resources on a target domain may be
leveraged to augment PTB-training: part-of-speech
tags, dictionary collocations, and named-entities.
Our findings basically follow the above results. The
contribution of this paper is to provide empirical re-
sults of the relationships among domain variation,
probability of lexical entry assignment, training data
size, and training cost. In particular, this paper em-
pirically shows how much in-domain corpus is re-
quired for satisfiable performance.
In Section 2, we introduce an HPSG parser and
describe an existing method for domain adaptation.
In Section 3, we show our methods of re-training
a lexical disambiguation model and incorporating
it into the original model. In Section 4, we exam-
ine our method through experiments on the GENIA
treebank. In Section 5, we examine the portability
of our method through experiments on the Brown
corpus. In Section 6, we showed several recent re-
searches related to domain adaptation.
2 An HPSG Parser
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of grammar rules describe
general construction rules, and a large number of
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
Grammar Rule
3
1
Unification
HEAD 
SUBCAT < >
1
2
HEAD 
SUBCAT < >
3
2
HEAD 
SUBCAT < >
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
John has come
HEAD verb
SUBCAT <noun>
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
Lexical Entries
John has come
John has come
Figure 1: Parsing a sentence ?John has come.?
HEAD verb
SUBCAT <noun>
HEAD noun
SUBCAT <>
HEAD verb
SUBCAT <verb>
HEAD verb
SUBCAT <noun>
John has come
HEAD verb
SUBCAT <>
Figure 2: An HPSG parse tree for a sentence ?John
has come.?
lexical entries express word-specific characteristics.
The structures of sentences are explained using com-
binations of grammar rules and lexical entries.
Figure 1 shows an example of HPSG parsing of
the sentence ?John has come.? First, as shown at the
top of the figure, an HPSG parser assigns a lexical
entry to each word in this sentence. Next, a gram-
mar rule is assigned and applied to lexical entries. At
the middle of this figure, the grammar rule is applied
to the lexical entries for ?has? and ?come.? We then
obtain the structure represented at the bottom of the
figure. After that, the application of grammar rules
is done iteratively, and then we can finally obtain the
parse tree as is shown in Figure 2. In practice, since
two or more parse candidates can be given for one
sentence, a disambiguation model gives probabili-
ties to these candidates, and a candidate given the
highest probability is then chosen as a correct parse.
12
The HPSG parser used in this study is Ninomiya
et al (2006), which is based on Enju (Miyao and
Tsujii, 2005). Lexical entries of Enju were extracted
from the Penn Treebank (Marcus et al, 1994), which
consists of sentences collected from The Wall Street
Journal (Miyao et al, 2004). The disambiguation
model of Enju was trained on the same treebank.
The disambiguation model of Enju is based on
a feature forest model (Miyao and Tsujii, 2002),
which is a log-linear model (Berger et al, 1996) on
packed forest structure. The probability, p
E
(tjw),
of producing the parse result t for a given sentence
w = hw
1
; :::; w
u
i is defined as
p
E
(tjw) =
1
Z
s
Y
i
p
lex
(l
i
jw; i)  q
syn
(tjl);
Z
s
=
X
t2T (w)
Y
i
p
lex
(l
i
jw; i)  q
syn
(tjl)
where l = hl
1
; :::; l
u
i is a list of lexical entries as-
signed to w, p
lex
(l
i
jw; i) is a probabilistic model
giving the probability that lexical entry l
i
is assigned
to word w
i
, q
syn
(tjl) is an unnormalized log-linear
model of tree construction and gives the possibil-
ity that parse candidate t is produced from lexical
entries l, and T (w) is a set of parse candidates as-
signed to w. With a treebank of a target domain as
training data, model parameters of p
lex
and q
syn
are
estimated so as to maximize the log-likelihood of the
training data.
Probabilistic model p
lex
is defined as a log-linear
model as follows.
p
lex
(l
i
jw; i) =
1
Z
w
i
exp
 
X
j

j
f
j
(l
i
;w; i)
!
;
Z
w
i
=
X
l
i
2L(w
i
)
exp
 
X
j

j
f
j
(l
i
;w; i)
!
;
where L(w
i
) is a set of lexical entries which can
be assigned to word w
i
. Before training this model,
L(w
i
) for all w
i
are extracted from the training tree-
bank. The feature function f
j
(l
i
;w; i) represents the
characteristics of l
i
, w and w
i
, while corresponding

j
is its weight. For the feature functions, instead of
using unigram features adopted in Miyao and Tsujii
(2005), Ninomiya et al (2006) used ?word trigram?
and ?POS 5-gram? features which are listed in Ta-
ble 1. With the revised Enju model, they achieved
Table 1: Features for the probabilities of lexical en-
try selection
surrounding words w
 1
w
0
w
1
(word trigram)
surrounding POS tags p
 2
p
 1
p
0
p
1
p
2
(POS 5-gram)
combinations w
 1
w
0
; w
0
w
1
; p
 1
w
0
; p
0
w
0
;
p
1
w
0
; p
0
p
1
p
2
p
3
; p
 2
p
 1
p
0
;
p
 1
p
0
p
1
; p
0
p
1
p
2
; p
 2
p
 1
;
p
 1
p
0
; p
0
p
1
; p
1
p
2
parsing accuracy as high as Miyao and Tsujii (2005),
with around four times faster parsing speed.
Johnson and Riezler (2000) suggested the pos-
sibility of the method for adapting a stochastic
unification-based grammar including HPSG to an-
other domain. They incorporated auxiliary distribu-
tions as additional features for an original log-linear
model, and then attempted to assign proper weights
to the new features. With this approach, they suc-
ceeded in decreasing to a degree indistinguishable
sentences for a target grammar.
Our previous work proposed a method for adapt-
ing an HPSG parser trained on the Penn Treebank
to a biomedical domain (Hara et al, 2005). We
re-trained a disambiguation model of tree construc-
tion, i.e., q
syn
, for the target domain. In this ap-
proach, q
syn
of the original parser was used as a
reference distribution (Jelinek, 1998) of another log-
linear model, and the new model was trained using a
target treebank. Since re-training used only a small
treebank of the target domain, the cost was small and
parsing accuracy was successfully improved.
3 Re-training of a Disambiguation Model
of Lexical Entry Assignments
Our idea of domain adaptation is to train a disam-
biguation model of lexical entry assignments for the
target domain and then incorporate it into the origi-
nal parser. Since Enju includes the disambiguation
model of lexical entry assignments as p
lex
, we can
implement our method in Enju by training another
disambiguation model p0
lex
(l
i
jw; i) of lexical entry
assignments for the biomedical domain, and then re-
placing the original p
lex
with the newly trained p0
lex
.
In this paper, for p0
lex
, we train a disambigua-
tion model p
lex mix
(l
i
jw; i) of lexical entry assign-
ments. p
lex mix
is a maximum entropy model and
the feature functions for it is the same as p
lex
as
13
given in Table 1. With these feature functions, we
train p
lex mix
on the treebanks both of the original
and biomedical domains.
In the experiments, we examine the contribution
of our method to parsing accuracy. In addition, we
implement several other possible methods for com-
parison of the performances.
baseline: use the original model of Enju
GENIA only: execute the same method of training
the disambiguation model of Enju, using only
the GENIA treebank
Mixture: execute the same method of training the
disambiguation model of Enju, using both of
the Penn Treebank and the GENIA treebank (a
kind of smoothing method)
HMT05: execute the method proposed in our pre-
vious work (Hara et al, 2005)
Our method: replace p
lex
in the original model
with p
lex mix
, while leaving q
syn
as it is
Our method (GENIA): replace p
lex
in the original
model with p
lex genia
, which is a probabilistic
model of lexical entry assignments trained only
with the GENIA treebank, while leaving q
syn
as it is
Our method + GENIA: replace p
lex
in the original
model with p
lex mix
and q
syn
with q
syn genia
,
which is a disambiguation model of tree con-
struction trained with the GENIA treebank
Our method + HMT05: replace p
lex
in the orig-
inal model with p
lex mix
and q
syn
with the
model re-trained with our previous method
(Hara et al, 2005) (the combination of our
method and the ?HMT05? method)
baseline (lex): use only p
lex
as a disambiguation
model
GENIA only (lex): use only p
lex genia
as a disam-
biguation model, which is a probabilistic model
of lexical entry assignments trained only with
the GENIA treebank
Mixture (lex): use only p
lex mix
as a disambigua-
tion model
The ?baseline? method does no adaptation to the
biomedical domain, and therefore gives lower pars-
ing accuracy for the domain than for the original do-
main. This method is regarded as the baseline of
the experiments. The ?GENIA only? method relies
solely on the treebank for the biomedical domain,
and therefore it cannot work well with the small tree-
bank. The ?Mixture? method is a kind of smoothing
method using all available training data at the same
time, and therefore the method can give the highest
accuracy of the three, which would be regarded as
the ideal accuracy with the naive methods. However,
training this model is expected to be very costly.
The ?baseline (lex),? ?GENIA only (lex),? and
?Mixture (lex)? approaches rely solely on models of
lexical entry assignments, and show lower accuracy
than those that contain both of models of lexical en-
try assignments and tree constructions. These ap-
proaches can be utilized as indicators of importance
of combining the two types of models.
Our previous work (Hara et al, 2005) showed that
the model trained with the ?HMT05? method can
give higher accuracy than the ?baseline? method,
even with the small amount of the treebanks in the
biomedical domain. The model also takes much less
cost to train than with the ?Mixture? method. How-
ever, they reported that the method could not give as
high accuracy as the ?Mixture? method.
4 Experiments with the GENIA Corpus
4.1 Experimental Settings
We implemented the models shown in Section 3,
and then evaluated the performance of them. The
original parser, Enju, was developed on Section 02-
21 of the Penn Treebank (39,832 sentences) (Miyao
and Tsujii, 2005; Ninomiya et al, 2006). For
training those models, we used the GENIA tree-
bank (Kim et al, 2003), which consisted of 1,200
abstracts (10,848 sentences) extracted from MED-
LINE. We divided it into three sets of 900, 150, and
150 abstracts (8,127, 1,361, and 1,360 sentences),
and these sets were used respectively as training, de-
velopment, and final evaluation data. The method
of Gaussian MAP estimation (Chen and Rosenfeld,
1999) was used for smoothing. The meta parameter
 of the Gaussian distribution was determined so as
to maximize the accuracy on the development set.
14
  
  
  
  
  
   
  
 
                 
	 
                   
 Proceedings of the 10th Conference on Parsing Technologies, pages 60?68,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A log-linear model with an n-gram reference distribution for accurate HPSG
parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
ninomi@r.dl.itc.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yusuke Miyao
Department of Computer Science
University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
NaCTeM (National Center for Text Mining)
tsujii@is.s.u-tokyo.ac.jp
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
Abstract
This paper describes a log-linear model with
an n-gram reference distribution for accurate
probabilistic HPSG parsing. In the model,
the n-gram reference distribution is simply
defined as the product of the probabilities
of selecting lexical entries, which are pro-
vided by the discriminative method with ma-
chine learning features of word and POS
n-gram as defined in the CCG/HPSG/CDG
supertagging. Recently, supertagging be-
comes well known to drastically improve
the parsing accuracy and speed, but su-
pertagging techniques were heuristically in-
troduced, and hence the probabilistic mod-
els for parse trees were not well defined.
We introduce the supertagging probabilities
as a reference distribution for the log-linear
model of the probabilistic HPSG. This is the
first model which properly incorporates the
supertagging probabilities into parse tree?s
probabilistic model.
1 Introduction
For the last decade, fast, accurate and wide-coverage
parsing for real-world text has been pursued in
sophisticated grammar formalisms, such as head-
driven phrase structure grammar (HPSG) (Pollard
and Sag, 1994), combinatory categorial grammar
(CCG) (Steedman, 2000) and lexical function gram-
mar (LFG) (Bresnan, 1982). They are preferred
because they give precise and in-depth analyses
for explaining linguistic phenomena, such as pas-
sivization, control verbs and relative clauses. The
main difficulty of developing parsers in these for-
malisms was how to model a well-defined proba-
bilistic model for graph structures such as feature
structures. This was overcome by a probabilistic
model which provides probabilities of discriminat-
ing a correct parse tree among candidates of parse
trees in a log-linear model or maximum entropy
model (Berger et al, 1996) with many features for
parse trees (Abney, 1997; Johnson et al, 1999; Rie-
zler et al, 2000; Malouf and van Noord, 2004; Ka-
plan et al, 2004; Miyao and Tsujii, 2005). Follow-
ing this discriminative approach, techniques for effi-
ciency were investigated for estimation (Geman and
Johnson, 2002; Miyao and Tsujii, 2002; Malouf and
van Noord, 2004) and parsing (Clark and Curran,
2004b; Clark and Curran, 2004a; Ninomiya et al,
2005).
An interesting approach to the problem of parsing
efficiency was using supertagging (Clark and Cur-
60
ran, 2004b; Clark and Curran, 2004a; Wang, 2003;
Wang and Harper, 2004; Nasr and Rambow, 2004;
Ninomiya et al, 2006; Foth et al, 2006; Foth and
Menzel, 2006), which was originally developed for
lexicalized tree adjoining grammars (LTAG) (Ban-
galore and Joshi, 1999). Supertagging is a process
where words in an input sentence are tagged with
?supertags,? which are lexical entries in lexicalized
grammars, e.g., elementary trees in LTAG, lexical
categories in CCG, and lexical entries in HPSG. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated in
the case of a CCG parser (Clark and Curran, 2004a)
with the result of a drastic improvement in the pars-
ing speed. Wang and Harper (2004) also demon-
strated the effects of supertagging with a statisti-
cal constraint dependency grammar (CDG) parser
by showing accuracy as high as the state-of-the-art
parsers, and Foth et al (2006) and Foth and Menzel
(2006) reported that accuracy was significantly im-
proved by incorporating the supertagging probabili-
ties into manually tuned Weighted CDG. Ninomiya
et al (2006) showed the parsing model using only
supertagging probabilities could achieve accuracy as
high as the probabilistic model for phrase structures.
This means that syntactic structures are almost de-
termined by supertags as is claimed by Bangalore
and Joshi (1999). However, supertaggers themselves
were heuristically used as an external tagger. They
filter out unlikely lexical entries just to help parsing
(Clark and Curran, 2004a), or the probabilistic mod-
els for phrase structures were trained independently
of the supertagger?s probabilistic models (Wang and
Harper, 2004; Ninomiya et al, 2006). In the case of
supertagging of Weighted CDG (Foth et al, 2006),
parameters for Weighted CDG are manually tuned,
i.e., their model is not a well-defined probabilistic
model.
We propose a log-linear model for probabilistic
HPSG parsing in which the supertagging probabil-
ities are introduced as a reference distribution for
the probabilistic HPSG. The reference distribution is
simply defined as the product of the probabilities of
selecting lexical entries, which are provided by the
discriminative method with machine learning fea-
tures of word and part-of-speech (POS) n-gram as
defined in the CCG/HPSG/CDG supertagging. This
is the first model which properly incorporates the su-
pertagging probabilities into parse tree?s probabilis-
tic model. We compared our model with the proba-
bilistic model for phrase structures (Miyao and Tsu-
jii, 2005). This model uses word and POS unigram
for its reference distribution, i.e., the probabilities of
unigram supertagging. Our model can be regarded
as an extension of a unigram reference distribution
to an n-gram reference distribution with features that
are used in supertagging. We also compared with a
probabilistic model in (Ninomiya et al, 2006). The
probabilities of their model are defined as the prod-
uct of probabilities of supertagging and probabilities
of the probabilistic model for phrase structures, but
their model was trained independently of supertag-
ging probabilities, i.e., the supertagging probabili-
ties are not used for reference distributions.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical entries
express word-specific characteristics. The structures
of sentences are explained using combinations of
schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly applying
schemata to lexical/phrasal signs. Finally, the parse
result is output as a phrasal sign that dominates the
sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
61
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries deter-
mine the dominant syntactic structures.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model or
maximum entropy model (Berger et al, 1996). The
probability that a parse result T is assigned to a
given sentence w = ?w1, . . . , wn? is
(Probabilistic HPSG)
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature func-
tion that represents a characteristic of parse tree T ,
and Zw is the sum over the set of all possible parse
trees for the sentence. Intuitively, the probability
is defined as the normalized product of the weights
exp(?u) when a characteristic corresponding to fu
appears in parse result T . The model parameters, ?u,
are estimated using numerical optimization methods
(Malouf, 2002) to maximize the log-likelihood of
the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the compu-
tation of p(T |w) for all parse candidates assigned
to sentence w. Because the number of parse can-
didates is exponentially related to the length of the
sentence, the estimation is intractable for long sen-
tences. To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). Miyao and Tsujii (2005) also introduced a
preliminary probabilistic model p0(T |w) whose es-
timation does not require the parsing of a treebank.
This model is introduced as a reference distribution
(Jelinek, 1998; Johnson and Riezler, 2000) of the
probabilistic HPSG model; i.e., the computation of
parse trees given low probabilities by the model is
omitted in the estimation stage (Miyao and Tsujii,
2005), or a probabilistic model can be augmented
by several distributions estimated from the larger
and simpler corpus (Johnson and Riezler, 2000). In
(Miyao and Tsujii, 2005), p0(T |w) is defined as the
product of probabilities of selecting lexical entries
with word and POS unigram features:
(Miyao and Tsujii (2005)?s model)
puniref (T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in
T and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model with
other two types of probabilistic models using a su-
pertagger (Ninomiya et al, 2006). The first one is
the simplest probabilistic model, which is defined
with only the probabilities of lexical entry selec-
tion. It is defined simply as the product of the prob-
abilities of selecting all lexical entries in the sen-
tence; i.e., the model does not use the probabilities
of phrase structures like the probabilistic models ex-
plained above. Given a set of lexical entries, L, a
sentence, w = ?w1, . . . , wn?, and the probabilistic
model of lexical entry selection, p(li ? L|w, i), the
first model is formally defined as follows:
62
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(Ninomiya et al (2006)?s model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi in T
and p(li|w, i) is the probability of selecting lexical
entry li for wi.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic model of lexical entry selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical entries
for the word wi.
The second model is a hybrid model of supertag-
ging and the probabilistic HPSG. The probabilities
are given as the product of Ninomiya et al (2006)?s
model 1 and the probabilistic HPSG.
(Ninomiya et al (2006)?s model 3)
pmodel3(T |w) = pmodel1(T |w)phpsg(T |w)
In the experiments, we compared our model with
Miyao and Tsujii (2005)?s model and Ninomiya et
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
fsptag =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Feature templates.
al. (2006)?s model 1 and 3. The features used in our
model and their model are combinations of the fea-
ture templates listed in Table 1 and Table 2. The
feature templates fbinary and funary are defined for
constituents at binary and unary branches, froot is a
feature template set for the root nodes of parse trees.
flex is a feature template set for calculating the uni-
gram reference distribution and is used in Miyao and
Tsujii (2005)?s model. fsptag is a feature template
set for calculating the probabilities of selecting lex-
ical entries in Ninomiya et al (2006)?s model 1 and
3. The feature templates in fsptag are word trigrams
and POS 5-grams. An example of features applied
to the parse tree for the sentence ?Spring has come?
is shown in Figure 2.
3 Probabilistic HPSG with an n-gram
reference distribution
In this section, we propose a probabilistic model
with an n-gram reference distribution for probabilis-
tic HPSG parsing. This is an extension of Miyao
and Tsujii (2005)?s model by replacing the unigram
reference distribution with an n-gram reference dis-
tribution. Our model is formally defined as follows:
63
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
combinations of feature templates for fsptag
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Combinations of feature templates.
(Probabilistic HPSG with an n-gram reference distribution)
pnref (T |w) =
1
Znref pmodel1(T |w) exp
(?
u
?ufu(T )
)
Znref =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In our model, Ninomiya et al (2006)?s model 1
is used as a reference distribution. The probabilis-
tic model of lexical entry selection and its feature
templates are the same as defined in Ninomiya et al
(2006)?s model 1.
The formula of our model is the same as Ni-
nomiya et al (2006)?s model 3. But, their model
is not a probabilistic model with a reference distri-
bution. Both our model and their model consist of
the probabilities for lexical entries (= pmodel1(T |w))
and the probabilities for phrase structures (= the rest
of each formula). The only difference between our
model and their model is the way of how to train
model parameters for phrase structures. In both our
model and their model, the parameters for lexical en-
tries (= the parameters of pmodel1(T |w)) are first es-
timated from the word and POS sequences indepen-
dently of the parameters for phrase structures. That
is, the estimated parameters for lexical entries are
the same in both models, and hence the probabilities
of pmodel1(T |w) of both models are the same. Note
that the parameters for lexical entries will never be
updated after this estimation stage; i.e., the parame-
ters for lexical entries are not estimated in the same
time with the parameters for phrase structures. The
difference of our model and their model is the esti-
mation of parameters for phrase structures. In our
model, given the probabilities for lexical entries, the
parameters for phrase structures are estimated so as
to maximize the entire probabilistic model (= the
product of the probabilities for lexical entries and
the probabilities for phrase structures) in the train-
ing corpus. In their model, the parameters for phrase
structures are trained without using the probabili-
ties for lexical entries, i.e., the parameters for phrase
structures are estimated so as to maximize the prob-
abilities for phrase structures only. That is, the pa-
rameters for lexical entries and the parameters for
phrase structures are trained independently in their
model.
Miyao and Tsujii (2005)?s model also uses a ref-
erence distribution, but with word and POS unigram
features, as is explained in the previous section. The
only difference between our model and Miyao and
Tsujii (2005)?s model is that our model uses se-
quences of word and POS tags as n-gram features
for selecting lexical entries in the same way as su-
pertagging does.
4 Experiments
We evaluated the speed and accuracy of parsing
by using Enju 2.1, the HPSG grammar for English
(Miyao et al, 2005; Miyao and Tsujii, 2005). The
lexicon of the grammar was extracted from Sec-
tions 02-21 of the Penn Treebank (Marcus et al,
1994) (39,832 sentences). The grammar consisted
of 3,797 lexical entries for 10,536 words1. The prob-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by apply-
ing lexical rules to observed lexical entries in the HPSG tree-
bank (Nakanishi et al, 2004). The lexicon, however, included
many lexical entries that do not appear in the HPSG treebank.
64
No. of tested sentences Total No. of sentences Avg. length of tested sentences
Section 23 2,299 (100.00%) 2,299 22.2
Section 24 1,245 (99.84%) 1,247 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (Gold POSs)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)?s model 87.26 86.50 86.88 90.73 89.93 90.33 604
Ninomiya et al (2006)?s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129
Ninomiya et al (2006)?s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152
our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234
our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379
Section 23 (POS tagger)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)?s model 84.96 84.25 84.60 89.55 88.80 89.17 674
Ninomiya et al (2006)?s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154
Ninomiya et al (2006)?s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183
Matsuzaki et al (2007)?s model 86.93 86.47 86.70 - - - 30
our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260
our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821
Table 4: Experimental results for Section 23.
abilistic models were trained using the same portion
of the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and quick check
(Malouf et al, 2000).
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tuple
??,wh, a, wa?, where ? is the predicate type (e.g.,
adjective, intransitive verb), wh is the head word of
the predicate, a is the argument label (MODARG,
ARG1, ..., ARG4), and wa is the head word of
the argument. Labeled precision (LP)/labeled re-
call (LR) is the ratio of tuples correctly identified
by the parser2. Unlabeled precision (UP)/unlabeled
recall (UR) is the ratio of tuples without the pred-
icate type and the argument label. This evaluation
scheme was the same as used in previous evaluations
of lexicalized grammars (Hockenmaier, 2003; Clark
The HPSG treebank is used for training the probabilistic model
for lexical entry selection, and hence, those lexical entries that
do not appear in the treebank are rarely selected by the proba-
bilistic model. The ?effective? tag set size, therefore, is around
1,361, the number of lexical entries without those never-seen
lexical entries.
2When parsing fails, precision and recall are evaluated, al-
though nothing is output by the parser; i.e., recall decreases
greatly.
and Curran, 2004b; Miyao and Tsujii, 2005). The
experiments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the Tree-
bank was used as the development set, and the per-
formance was evaluated using sentences of ? 100
words in Section 23. The performance of each
model was analyzed using the sentences in Section
24 of ? 100 words. Table 3 details the numbers
and average lengths of the tested sentences of ? 100
words in Sections 23 and 24, and the total numbers
of sentences in Sections 23 and 24.
The parsing performance for Section 23 is shown
in Table 4. The upper half of the table shows the per-
formance using the correct POSs in the Penn Tree-
bank, and the lower half shows the performance us-
ing the POSs given by a POS tagger (Tsuruoka and
Tsujii, 2005). LF and UF in the figure are labeled
F-score and unlabeled F-score. F-score is the har-
monic mean of precision and recall. We evaluated
our model in two settings. One is implemented with
a narrow beam width (?our model 1? in the figure),
and the other is implemented with a wider beam
width (?our model 2? in the figure)3. ?our model
3The beam thresholding parameters for ?our model 1? are
?0 = 10,?? = 5, ?last = 30, ?0 = 5.0,?? = 2.5, ?last =15.0, ?0 = 10,?? = 5, ?last = 30, ?0 = 5.0,?? =2.5, ?last = 15.0, ?0 = 6.0,?? = 3.5, and ?last = 20.0.
65
83.00%
83.50%
84.00%
84.50%
85.00%
85.50%
86.00%
86.50%
87.00%
87.50%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F-sc
ore
Miyao and Tsujii(2005)'s modelNinomiya et al(2006)'s model 1Ninomiya et al(2006)'s model 3
our model
Figure 3: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
1? was introduced to measure the performance with
balanced F-score and speed, which we think appro-
priate for practical use. ?our model 2? was intro-
duced to measure how high the precision and re-
call could reach by sacrificing speed. Our mod-
els increased the parsing accuracy. ?our model 1?
was around 2.6 times faster and had around 2.65
points higher F-score than Miyao and Tsujii (2005)?s
model. ?our model 2? was around 2.3 times slower
but had around 2.9 points higher F-score than Miyao
and Tsujii (2005)?s model. We must admit that the
difference between our models and Ninomiya et al
(2006)?s model 3 was not as great as the differ-
ence from Miyao and Tsujii (2005)?s model, but ?our
model 1? achieved 0.56 points higher F-score, and
?our model 2? achieved 0.8 points higher F-score.
When the automatic POS tagger was introduced, F-
score dropped by around 2.4 points for all models.
We also compared our model with Matsuzaki et
al. (2007)?s model. Matsuzaki et al (2007) pro-
The terms ? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs in the chart
cell. The terms ? and ? are the thresholds of the number and
the beam width of lexical entries, and ? is the beam width for
global thresholding (Goodman, 1997). The terms with suffixes
0 are the initial values. The parser iterates parsing until it suc-
ceeds to generate a parse tree. The parameters increase for each
iteration by the terms prefixed by ?, and parsing finishes when
the parameters reach the terms with suffixes last. Details of the
parameters are written in (Ninomiya et al, 2005). The beam
thresholding parameters for ?our model 2? are ?0 = 18,?? =
6, ?last = 42, ?0 = 9.0,?? = 3.0, ?last = 21.0, ?0 =18,?? = 6, ?last = 42, ?0 = 9.0,?? = 3.0, ?last = 21.0.
In ?our model 2?, the global thresholding was not used.
posed a technique for efficient HPSG parsing with
supertagging and CFG filtering. Their results with
the same grammar and servers are also listed in the
lower half of Table 4. They achieved drastic im-
provement in efficiency. Their parser ran around 6
times faster than Ninomiya et al (2006)?s model 3,
9 times faster than ?our model 1? and 60 times faster
than ?our model 2.? Instead, our models achieved
better accuracy. ?our model 1? had around 0.5 higher
F-score, and ?our model 2? had around 0.8 points
higher F-score. Their efficiency is mainly due to
elimination of ungrammatical lexical entries by the
CFG filtering. They first parse a sentence with a
CFG grammar compiled from an HPSG grammar,
and then eliminate lexical entries that are not in the
parsed CFG trees. Obviously, this technique can
also be applied to the HPSG parsing of our mod-
els. We think that efficiency of HPSG parsing with
our models will be drastically improved by applying
this technique.
The average parsing time and labeled F-score
curves of each probabilistic model for the sentences
in Section 24 of ? 100 words are graphed in Fig-
ure 3. The graph clearly shows the difference of
our model and other models. As seen in the graph,
our model achieved higher F-score than other model
when beam threshold was widen. This implies that
other models were probably difficult to reach the F-
score of ?our model 1? and ?our model 2? for Section
23 even if we changed the beam thresholding param-
eters. However, F-score of our model dropped eas-
66
ily when we narrow down the beam threshold, com-
pared to other models. We think that this is mainly
due to its bad implementation of parser interface.
The n-gram reference distribution is incorporated
into the kernel of the parser, but the n-gram fea-
tures and a maximum entropy estimator are defined
in other modules; n-gram features are defined in a
grammar module, and a maximum entropy estimator
for the n-gram reference distribution is implemented
with a general-purpose maximum entropy estimator
module. Consequently, strings that represent the n-
gram information are very frequently changed into
feature structures and vice versa when they go in and
out of the kernel of the parser. On the other hand, Ni-
nomiya et al (2006)?s model 3 uses the supertagger
as an external module. Once the parser acquires the
supertagger?s outputs, the n-gram information never
goes in and out of the kernel. This advantage of Ni-
nomiya et al (2006)?s model can apparently be im-
plemented in our model, but this requires many parts
of rewriting of the implemented parser. We estimate
that the overhead of the interface is around from 50
to 80 ms/sentence. We think that re-implementation
of the parser will improve the parsing speed as esti-
mated. In Figure 3, the line of our model crosses the
line of Ninomiya et al (2006)?s model. If the esti-
mation is correct, our model will be faster and more
accurate so that the lines in the figure do not cross.
Speed-up in our model is left as a future work.
5 Conclusion
We proposed a probabilistic model in which su-
pertagging is consistently integrated into the prob-
abilistic model for HPSG. In the model, the n-gram
reference distribution is simply defined as the prod-
uct of the probabilities of selecting lexical entries
with machine learning features of word and POS n-
gram as defined in the CCG/HPSG/CDG supertag-
ging. We conducted experiments on the Penn Tree-
bank with a wide-coverage HPSG parser. In the ex-
periments, we compared our model with the prob-
abilistic HPSG with a unigram reference distribu-
tion (Miyao and Tsujii, 2005) and the probabilistic
HPSG with supertagging (Ninomiya et al, 2006).
Though our model was not as fast as Ninomiya
et al (2006)?s models, it achieved the highest ac-
curacy among them. Our model had around 2.65
points higher F-score than Miyao and Tsujii (2005)?s
model and around 0.56 points higher F-score than
the Ninomiya et al (2006)?s model 3. When we sac-
rifice parsing speed, our model achieved around 2.9
points higher F-score than Miyao and Tsujii (2005)?s
model and around 0.8 points higher F-score than Ni-
nomiya et al (2006)?s model 3. Our model achieved
higher F-score because parameters for phrase struc-
tures in our model are trained with the supertagging
probabilities, which are not in other models.
References
Steven P. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Joan Bresnan. 1982. The Mental Representation of
Grammatical Relations. MIT Press, Cambridge, MA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proc. of
ACL?04, pages 104?111.
Killian Foth and Wolfgang Menzel. 2006. Hybrid pars-
ing: Using probabilistic models as predictors for a
symbolic parser. In Proc. of COLING-ACL 2006.
Killian Foth, Tomas By, and Wolfgang Menzel. 2006.
Guiding a constraint dependency parser with su-
pertags. In Proc. of COLING-ACL 2006.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
F. Jelinek. 1998. Statistical Methods for Speech Recog-
nition. The MIT Press.
67
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proc. of NAACL-2000, pages 154?161.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proc. of ACL ?99,
pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In Proc. of IJCNLP-04 Workshop ?Beyond
Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake. 2000.
Efficient feature structure operations without compi-
lation. Journal of Natural Language Engineering,
6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG parsing with supertagging and
CFG-filtering. In Proc. of IJCAI 2007, pages 1671?
1676.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proc. of HLT
2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii, 2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok
Lee and Oi Yee Kwong (Eds.), Natural Language
Processing - IJCNLP 2004 LNAI 3248, chapter
Corpus-oriented Grammar Development for Acquir-
ing a Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lexi-
cal rules on parsing with a treebank grammar. In Proc.
of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
and Jun?ichi Tsujii. 2005. Efficacy of beam threshold-
ing, unification filtering and hybrid parsing in proba-
bilistic HPSG parsing. In Proc. of IWPT 2005, pages
103?114.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006. Ex-
tremely lexicalized models for accurate and fast HPSG
parsing. In Proc. of EMNLP 2006, pages 155?163.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proc. of ACL?00, pages 480?487.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proc. of HLT/EMNLP 2005,
pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proc.
of ACL?04 Incremental Parsing workshop: Bringing
Engineering and Cognition Together, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Grammar.
Ph.D. thesis, Purdue University.
68
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evaluating the Effects of Treebank Size in a Practical Application for 
Parsing 
Kenji Sagae1, Yusuke Miyao1, Rune S?tre1 and Jun'ichi Tsujii1,2,3 
1Department of Computer Science, Univerisity of Tokyo, Japan 
2School of Computer Science, University of Manchester 
3National Center for Text Mining, Manchester, UK 
{sagae,yusuke,rune.saetre,tsujii@is.s.u-tokyo.ac.jp} 
 
 
 
 
 
Abstract 
Natural language processing modules such as 
part-of-speech taggers, named-entity recog-
nizers and syntactic parsers are commonly 
evaluated in isolation, under the assumption 
that artificial evaluation metrics for individual 
parts are predictive of practical performance 
of more complex language technology sys-
tems that perform practical tasks. Although 
this is an important issue in the design and en-
gineering of systems that use natural language 
input, it is often unclear how the accuracy of 
an end-user application is affected by parame-
ters that affect individual NLP modules.  We 
explore this issue in the context of a specific 
task by examining the relationship between 
the accuracy of a syntactic parser and the 
overall performance of an information extrac-
tion system for biomedical text that includes 
the parser as one of its components.  We 
present an empirical investigation of the rela-
tionship between factors that affect the accu-
racy of syntactic analysis, and how the 
difference in parse accuracy affects the overall 
system.   
1 Introduction 
Software systems that perform practical tasks with 
natural language input often include, in addition to 
task-specific components, a pipeline of basic natu-
ral language processing modules, such as part-of-
speech taggers, named-entity recognizers, syntactic 
parsers and semantic-role labelers.  Although such 
building blocks of larger language technology so-
lutions are usually carefully evaluated in isolation 
using standard test sets, the impact of improve-
ments in each individual module on the overall 
performance of end-to-end systems is less well 
understood.  While the effects of the amount of 
training data, search beam widths and various ma-
chine learning frameworks have been explored in 
detail with respect to speed and accuracy in basic 
natural language processing tasks, how these trade-
offs in individual modules affect the performance 
of the larger systems they compose is an issue that 
has received relatively little attention.  This issue, 
however, is of great practical importance in the 
effective design and engineering of complex soft-
ware systems that deal with natural language.   
In this paper we explore some of these issues 
empirically in an information extraction task in the 
biomedical domain, the identification of protein- 
protein interactions (PPI) mentioned in papers ab-
stracts from MEDLINE, a large database of bio-
medical papers.  Due in large part to the creation of 
biomedical treebanks (Kulick et al, 2004; Tateisi 
et al, 2005) and rapid progress of data-driven 
parsers (Lease and Charniak, 2005; Nivre et al, 
2007), there are now fast, robust and accurate syn-
tactic parsers for text in the biomedical domain.  
Recent research shows that parsing accuracy of 
biomedical corpora is now between 80% and 90% 
(Clegg and Shepherd, 2007; Pyysalo et al, 2007; 
Sagae et al, 2008).  Intuitively, syntactic relation-
ships between words should be valuable in deter-
mining possible interactions between entities 
present in text.  Recent PPI extraction systems 
have confirmed this intuition (Erkan et al, 2007; 
S?tre et al, 2007; Katrenko and Adriaans, 2006).     
While it is now relatively clear that syntactic 
parsing is useful in practical tasks that use natural 
language corpora in bioinformatics, several ques-
14
tions remain as to research issues that affect the 
design and testing of end-user applications, includ-
ing how syntactic analyses should be used in a 
practical setting, whether further improvements in 
parsing technologies will result in further im-
provements in practical systems, whether it is im-
portant to continue the development of treebanks 
and parser adaptation techniques for the biomedi-
cal domain, and how much effort should be spent 
on comparing and benchmarking parsers for bio-
medical data.  We attempt to shed some light on 
these matters by presenting experiments that show 
the relationship of the accuracy of a dependency 
parser and the accuracy of the larger PPI system 
that includes the parser.  We investigate the effects 
of domain-specific treebank size (the amount of 
available manually annotated training data for syn-
tactic parsers) and final system performance, and 
obtain results that should be informative to re-
searchers in bioinformatics who rely on existing 
NLP resources to design information extraction 
systems, as well as to members of the parsing 
community who are interested in the practical im-
pact of parsing research. 
In section 2 we discuss our motivation and re-
lated efforts.  Section 3 describes the system for 
identification of protein-protein interactions used 
in our experiments, and in section 4 describes the 
syntactic parser that provides the analyses for the 
PPI system, and the data used to train the parser.  
We describe our experiments, results and analysis 
in section 5, and conclude in section 6.  
2 Motivation and related work 
While recent work has addressed questions relating 
to the use of different parsers or different types of 
syntactic representations in the PPI extraction task 
(S?tre et al, 2007, Miyao et al, 2008), little con-
crete evidence has been provided for potential ben-
efits of improved parsers or additional resources 
for training syntactic parsers.  In fact, although 
there is increasing interest in parser evaluation in 
the biomedical domain in terms of precision/recall 
of brackets and dependency accuracy (Clegg and 
Shepherd, 2007; Pyysalo et al, 2007; Sagae et al, 
2008), the relationship between these evaluation 
metrics and the performance of practical informa-
tion extraction systems remains unclear.  In the 
parsing community, relatively small accuracy gains 
are often reported as success stories, but again, the 
precise impact of such improvements on practical 
tasks in bioinformatics has not been established. 
One aspect of this issue is the question of do-
main portability and domain adaptation for parsers 
and other NLP modules.  Clegg and Shepherd 
(2007) mention that available statistical parsers 
appear to overfit to the newswire domain, because 
of their extensive use of the Wall Street Journal 
portion of the Penn Treebank (Marcus et al, 1994) 
during development and training.  While this claim 
is supported by convincing evaluations that show 
that parsers trained on the WSJ Penn Treebank 
alone perform poorly on biomedical text in terms 
of accuracy of dependencies or bracketing of 
phrase structure, the benefits of using domain-
specific data in terms of practical system perfor-
mance have not been quantified.  These expected 
benefits drive the development of domain-specific 
resources, such as the GENIA treebank (Tateisi et 
al., 2005), and parser domain adaption (Hara et al, 
2007), which are of clear importance in parsing 
research, but of largely unconfirmed impact on 
practical systems. 
Quirk and Corston-Oliver (2006) examine a 
similar issue, the relationship between parser accu-
racy and overall system accuracy in syntax-
informed machine translation.  Their research is 
similar to the work presented here, but they fo-
cused on the use of varying amounts of out-of-
domain training data for the parser, measuring how 
a translation system for technical text performed 
when its syntactic parser was trained with varying 
amounts of Wall Street Journal text.  Our work, in 
contrast, investigates the use of domain-specific 
training material in parsers for biomedical text, a 
domain where significant amounts of effort are 
allocated for development of domain-specific NLP 
resources in hope that such resources will result in 
better overall performance in practical systems.  
3 A PPI extraction system based on syn-
tactic parsing 
PPI extraction is an NLP task to identify protein 
pairs that are mentioned as interacting in biomedi-
cal papers.  Figure 2 shows two sentences that in-
clude protein names: the former sentence mentions 
a protein interaction, while the latter does not.  
Given a protein pair, PPI extraction is a task of 
binary classification; for example, <IL-8, CXCR1> 
15
is a positive example, and <RBP, TTR> is a ne-
gative example. 
Following recent work on using dependency 
parsing in systems that identify protein interactions 
in biomedical text (Erkan et al, 2007; S?tre et al, 
2007; Katrenko and Adriaans, 2006), we have built 
a system for PPI extraction that uses dependency 
relations as features. As exemplified, for the pro-
tein pair IL-8 and CXCR1 in the first sentence of 
Figure 2, a dependency parser outputs a dependen-
cy tree shown in Figure 1.  From this dependency 
tree, we can extract a dependency path between 
IL-8 and CXCR1 (Figure 3), which appears to be 
a strong clue in knowing that these proteins are 
mentioned as interacting. 
The system we use in this paper is similar to the 
one described in S?tre et al (2007), except that it 
uses syntactic dependency paths obtained with a 
dependency parser, but not predicate-argument 
paths based on deep-parsing.  This method is based 
on SVM with SubSet Tree Kernels (Collins, 2002; 
Moschitti, 2006).  A dependency path is encoded 
as a flat tree as depicted in Figure 4. Because a tree 
kernel measures the similarity of trees by counting 
common subtrees, it is expected that the system 
finds effective subsequences of dependency paths.   
In addition to syntactic dependency features, we 
incorporate bag-of-words features, which are re-
garded as a strong baseline for IE systems.  We use 
lemmas of words before, between and after the pair 
of target proteins. 
In this paper, we use Aimed (Bunescu and 
Mooney, 2004), which is a popular benchmark for 
the evaluation of PPI extraction systems.  The 
Aimed corpus consists of 225 biomedical paper 
abstracts (1970 sentences), which are sentence-
split, tokenized, and annotated with proteins and 
PPIs.  
4 A data-driven dependency parser for 
biomedical text 
The parser we used as component of our PPI ex-
traction system was a shift-reduce dependency 
parser that uses maximum entropy models to de-
termine the parser?s actions.  Our overall parsing 
approach uses a best-first probabilistic shift-reduce 
algorithm, working left-to right to find labeled de-
pendencies one at a time. The algorithm is essen-
tially a dependency version of the constituent 
parsing algorithm for probabilistic parsing with 
LR-like data-driven models described by Sagae 
and Lavie (2006).  This dependency parser has 
been shown to have state-of-the-art accuracy in the 
CoNLL shared tasks on dependency parsing 
(Buchholz and Marsi, 2006; Nivre, 2007). Sagae 
and Tsujii (2007) present a detailed description of 
the parsing approach used in our work, including 
the parsing algorithm and the features used to clas-
sify parser actions.  In summary, the parser uses an 
algorithm similar to the LR parsing algorithm 
(Knuth, 1965), keeping a stack of partially built 
syntactic structures, and a queue of remaining in-
put tokens.  At each step in the parsing process, the 
parser can apply a shift action (remove a token 
from the front of the queue and place it on top of 
the stack), or a reduce action (pop the two topmost 
This study demonstrates that IL-8 recognizes 
and activates CXCR1, CXCR2, and the Duf-
fy antigen by distinct mechanisms. 
 
The molar ratio of serum retinol-binding pro-
tein (RBP) to transthyretin (TTR) is not 
useful to assess vitamin A status during infec-
tion in hospitalized children. 
Figure 2: Example sentences with protein names 
Figure 1: A dependency tree 
ROOT  IL-8  recognizes  and  activates  CXCR1 
ROOT 
SBJ 
OBJ 
COORD 
CC 
ENTITY1(IL-8)    recognizes   ENTITY2(CXCR1) 
Figure 3: A dependency path between protein names 
SBJ OBJ 
16
stack items, and push a new item composed of the 
two popped items combined in a single structure). 
This parsing approach is very similar to the one 
used successfully by Nivre et al (2006), but we 
use a maximum entropy classifier (Berger et al, 
1996) to determine parser actions, which makes 
parsing considerably faster. In addition, our pars-
ing approach performs a search over the space of 
possible parser actions, while Nivre et al?s ap-
proach is deterministic. 
The parser was trained using 8,000 sentences 
from the GENIA Treebank (Tateisi et al, 2005), 
which contains abstracts of papers taken from 
MEDLINE, annotated with syntactic structures.  
To determine the effects of training set size on the 
parser, and consequently on the PPI extraction sys-
tem, we trained several parsing models with differ-
ent amounts of GENIA Treebank data.  We started 
with 100 sentences, and increased the training set 
by 100 sentence increments, up to 1,000 sentences.  
From that point, we increased the training set by 
1,000 sentence increments.  Figure 5 shows the 
labeled dependency accuracy for the varying sizes 
of training sets.  The accuracy was measured on a 
portion of the GENIA Treebank reserved as devel-
opment data.  The result clearly demonstrates that 
the increase in the size of the training set contri-
butes to increasing parse accuracy.  Training the 
parser with only 100 sentences results in parse ac-
curacy of about 72.5%.  Accuracy rises sharply 
with additional training data until the size of the 
training set reaches about 1,000 sentences (about 
82.5% accuracy).  From there, accuracy climbs 
consistently, but slowly, until 85.6% accuracy is 
reached with 8,000 sentences of training data. 
It should be noted that parser accuracy on the 
Aimed data used in our PPI extraction experiments 
may be slightly lower, since the domain of the 
GENIA Treebank is not exactly the same as the 
Aimed corpus.  Both of them were extracted from 
MEDLINE, but the criteria for data selection were 
not the same in the two corpora, creating possible 
differences in sub-domains.  We also note that the 
accuracy of a parser trained with more than 40,000 
sentences from the Wall Street Journal portion of 
the Penn Treebank is under 79%, a level equivalent 
to that obtained by training the parser with only 
500 sentences of GENIA data. 
 
 
Figure 5: Data size vs. parse accuracy 
 
5 Experiments and Results 
In this section we present our PPI extraction expe-
riments applying the dependency parsers trained 
with the different amounts of the GENIA Treebank 
in our PPI system.  As we mentioned, the GENIA 
Treebank is used for training the parser, while the 
Aimed is used for training and evaluation of PPI 
extraction.  A part-of-speech tagger trained with 
GENIA and PennBioIE was used.  We do not ap-
ply automatic protein name detection, and instead 
use the gold-standard protein annotations in the 
Aimed corpus.  Before running a parser, multiword 
protein names are concatenated and treated as sin-
gle words. As described in Section 3, bag-of-words 
and syntactic dependency paths are fed as features 
to the PPI classifier. The accuracy of PPI extrac-
tion is measured by the abstract-wise 10-fold cross 
validation (S?tre et al 2007). 
When we use the part-of-speech tagger and the 
dependency parser trained with WSJ, the accuracy 
(F-score) of PPI extraction on this data set is 55.2.  
The accuracy increases to 56.9 when we train the 
part-of-speech tagger with GENIA and Penn BioIE, 
while using the WSJ-trained parser.  This confirms 
the claims by Lease and Charniak (2005) that sub-
sentential lexical analysis alone is helpful in adapt-
ing WSJ parsers to the biomedical domain.  While 
Lease and Charniak looked only at parse accuracy, 
70
75
80
85
90
0 2000 4000 6000 8000
Figure 4: A tree kernel representation of the dependency 
path 
(dep_path (SBJ (ENTITY1 ecognizes)) 
(rOBJ (recognizes ENTITY2))) 
17
our result shows that the increase in parse accuracy 
is, as expected, beneficial in practice. 
Figure 6 shows the relationship between the 
amount of parser training data and the F-score for 
the PPI extraction.  The result shows that the accu-
racy of PPI extraction increases with the use of 
more sentences to train the parser.    The best accu-
racy was obtained when using 4,000 sentences, 
where parsing accuracy is around 84.3.  Although 
it may appear that further increasing the training 
data for the parser may not improve the PPI extrac-
tion accuracy (since only small and inconsistent 
variations in F-score are observed in Figure 6), 
when we plot the curves shown in Figures 5 and 6 
in a single graph (Figure 7), we see that the two 
curves match each other to a large extent.  This is 
supported by the strong correlation between parse 
accuracy and PPI accuracy observed in Figure 8.  
While this suggests that training the parser with a 
larger treebank may result in improved accuracy in 
PPI extraction, we observe that a 1% absolute im-
provement in parser accuracy corresponds roughly 
to a 0.25 improvement in PPI extraction F-score.  
Figure 5 indicates that to obtain even a 1% im-
provement in parser accuracy by using more train-
ing data, the size of the treebank would have to 
increase significantly. 
Although the results presented so far seem to 
suggest the need for a large data annotation effort 
to achieve a meaningful improvement in PPI ex-
traction accuracy, there are other ways to improve 
the overall accuracy of the system without an im-
provement in parser accuracy.  One obvious alter-
native is to increase the size of the PPI-annotated 
corpus (which is distinct from the treebank used to 
train the parser).  As mentioned in section 3, our 
system is trained using the Aimed corpus, which 
contains 225 abstracts from biomedical papers with 
manual annotations indicating interactions between 
proteins.  Pairs of proteins with no interaction de-
scribed in the text are used as negative examples, 
and pairs of proteins described as interacting are 
used as positive examples.  The corpus contains a 
total of roughly 9,000 examples.  Figure 9 shows 
how the overall system accuracy varies when dif-
ferent amounts of training data (varying amounts 
of training examples) are used to train the PPI sys-
tem (keeping the parse accuracy constant, using all 
of the available training data in the GENIA tree-
bank to train the parser).  While Figure 5 indicates 
that a significant improvement in parse accuracy 
requires a large increase in the treebank used to 
train the parser, and Figure 7 shows that improve-
ments in PPI extraction accuracy may require a 
sizable improvement in parse accuracy, Figure 9 
suggests that even a relatively small increase in the 
PPI corpus may lead to a significant improvement 
in PPI extraction accuracy. 
 
Figure 6: Parser training data size vs. PPI extraction 
accuracy 
 
 
 
Figure 7: Parser training data size vs. parser accuracy 
and PPI extraction accuracy 
 
 
 
Figure 8: Parse accuracy vs. PPI extraction accuracy 
 
53
54
55
56
57
58
0 2000 4000 6000 8000
53
54
55
56
57
58
68
72
76
80
84
88
0 5000 10000
Parser
PPI F-score
53
54
55
56
57
58
70 75 80 85 90
18
 
 
Figure 9: Number of PPI training examples vs. PPI ex-
traction accuracy 
 
While some of the conclusions that can be 
drawn from these results may be somewhat sur-
prising, most are entirely expected.  However, even 
in these straightforward cases, our experiments 
provide some empirical evidence and concrete 
quantitative analysis to complement intuition.  We 
see that using domain-specific training data for the 
parsing component for the PPI extraction system 
produces superior results, compared to using train-
ing data from the WSJ Penn Treebank.  When the 
parser trained on WSJ sentences is used, PPI ex-
traction accuracy is about 55, compared to over 57 
when sentences from biomedical papers are used.  
This corresponds fairly closely to the differences in 
parser accuracy: the accuracy of the parser trained 
on 500 sentences from GENIA is about the same 
as the accuracy of the parser trained on the entire 
WSJ Penn Treebank, and when these parsers are 
used in the PPI extraction system, they result in 
similar overall task accuracy.  However, the results 
obtained when a domain-specific POS tagger is 
combined with a parser trained with out-of-domain 
data, overall PPI results are nearly at the same lev-
el as those obtained with domain-specific training 
data (just below 57 with a domain-specific POS 
tagger and out-of-domain parser, and just above 57 
for domain-specific POS tagger and parser).  At 
the same time, the argument against annotating 
domain-specific data for parsers in new domains is 
not a strong one, since higher accuracy levels (for 
both the parser and the overall system) can be ob-
tained with a relatively small amount of domain-
specific data. 
Figures 5, 6 and 7 also suggest that additional 
efforts in improving parser accuracy (through the 
use of feature engineering, other machine learning 
techniques, or an increase in the size of its training 
set) could improve PPI extraction accuracy, but a 
large improvement in parser accuracy may be re-
quired.  When we combine these results with the 
findings obtained by Miyao et al (2008), they sug-
gest that a better way to improve the overall sys-
tem is to spend more effort in designing a specific 
syntactic representation that addresses the needs of 
the system, instead of using a generic representa-
tion designed for measuring parser accuracy.  
Another potentially fruitful course of action is to 
design more sophisticated and effective ways for 
information extraction systems to use NLP tools, 
rather than simply extracting features that corres-
pond to small fragments of syntactic trees.  Of 
course, making proper use of natural language 
analysis is a considerable challenge, but one that 
should be kept in mind through the design of prac-
tical systems that use NLP components. 
6 Conclusion 
This paper presented empirical results on the rela-
tionship between the amount of training data used 
to create a dependency parser, and the accuracy of 
a system that performs identification of protein-
protein interactions using the dependency parser.  
We trained a dependency parser with different 
amounts of data from the GENIA Treebank to es-
tablish how the improvement in parse accuracy 
corresponds to improvement in practical task per-
formance in this information extraction task.  
While parsing accuracy clearly increased with 
larger amounts of data, and is likely to continue 
increasing with additional annotation of data for 
the GENIA Treebank, the trend in the accuracy of 
PPI extraction indicates that a sizable improvement 
in parse accuracy may be necessary for improved 
detection of protein interactions. 
When combined with recent findings by Miyao 
et al (2008), our results indicate that further work 
in designing PPI extraction systems that use syn-
tactic dependency features would benefit from 
more adequate syntactic representations or more 
sophisticated use of NLP than simple extraction of 
syntactic subtrees.  Furthermore, to improve accu-
racy in this task, efforts on data annotation should 
focus on task-specific data (manual annotation of 
40
45
50
55
60
65
0 5000 10000 15000
19
protein interactions in biomedical papers), rather 
than on additional training data for syntactic pars-
ers.  While annotation of parser training data might 
seems like a cost-effective choice, since improved 
parser results might be beneficial in a number of 
systems where the parser can be used, our results 
show that, in this particular task, efforts should be 
focused elsewhere, such as the annotation of addi-
tion PPI data.  
Acknowledgements 
We thank the anonymous reviewers for their in-
sightful comments.  This work was partially sup-
ported by Grant-in-Aid for Specially Promoted 
Research (MEXT, Japan), Genome Network 
Project (MEXT, Japan), and Grant-in-Aid for 
Young Scientists (MEXT, Japan). 
References 
 
Berger, A., S. A. Della Pietra, and V. J. Della Pietra. 
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 
22(1):39?71. 
Clegg, A. and Shepherd, A. 2007. Benchmarking natu-
ral-language parsers for biological applications using 
dependency graphs. BMC Bioinformatics, 8:24. 
Erkan, G., A. Ozgur, and D. R. Radev. 2007. Semisu-
pervised classification for extracting protein interac-
tion sentences using dependency parsing. In 
Proceedings of CoNLL-EMNLP 2007. 
Hara, T., Miyao, Y and Tsujii, J. 2007. Evaluating Im-
pact of Re-training a Lexical Disambiguation Model 
on Domain Adaptation of an HPSG Parser. In Pro-
ceedings of the International Conference on Parsing 
Technologies (IWPT). 
Katrenko, S. and P. W. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency 
trees. In Proceedings of the first workshop on Know-
ledge Discovery and Emergent Complexity in BioIn-
formatics (KDECB), pages 61?80. 
Kulick, S., A. Bies, M. Liberman, M. Mandel, R. 
McDonald, M. Palmer, A. Schein and L. Ungar. 2004. 
Integrated Annotation for Biomedical Information 
Extraction. In Proceedings of Biolink 2004: Linking 
Biological Literature, Ontologies and Databases 
(HLT-NAACL workshop). 
Lease, M. and Charniak, E. 2005. Parsing Biomedical 
Literature. In R. Dale, K.-F. Wong, J. Su, and O. 
Kwong, editors, Proceedings of the 2nd International 
Joint Conference on Natural Language Processing 
(IJCNLP'05), volume 3651 of Lecture Notes in 
Computer Science, pages 58 ? 69. 
Miyao, Y., S?tre, R., Sagae, K., Matsuzaki, T. and Tsu-
jii, J. 2008. Task-Oriented Evaluation of Syntactic 
Parsers and Their Representations.  In Proceedings of 
the 46th Annual Meeting of the Association for Com-
putational Linguistics. 
Nivre, J., Hall, J., Kubler, S., McDonald, R., Nilsson, J., 
Riedel, S. and Yuret, D. 2007. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proceedings 
the CoNLL 2007 Shared Task in EMNLP-CoNLL. 
Nivre, Joakim, Johan Hall, Jens Nilsson, Gulsen Eryi-
git,and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector 
machines. In Proceedings of the Tenth Conference on 
Computational Natural Language Learning, shared 
task session. 
Pyysalo S., Ginter F., Haverinen K., Heimonen J., Sala-
koski T. and Laippala V. 2007. On the unification of 
syntactic annotations under the Stanford dependency 
scheme: A case study on BioInfer and GENIA.  In 
Proceedings of BioNLP 2007: Biological, Transla-
tional and Clinical Language Processing. 
Quirk, C. and Corston-Oliver S. 2006. The impact of 
parse quality on syntactically-informed statistical 
machine translation. In Proceedings of EMNLP 2007. 
S?tre, R., Sagae, K., and Tsujii, J. 2007. Syntactic fea-
tures for protein-protein interaction extraction.  In 
Proceedings of the International Symposium on Lan-
guages in Biology and Medicine (LBM short oral 
presentations). 
Sagae, K. and Lavie, A. 2006. A best-first probabilistic 
shift-reduce parser. In Proceedings of the 
COLING/ACL 2006 Main Conference Poster Ses-
sions, pages 691?698, Sydney, Australia, July. Asso-
ciation for Computational Linguistics. 
Sagae, K., Miyao, Y. and Tsujii, J. 2008. Challenges in 
Mapping of Syntactic Representations for Frame-
work-Independent Parser Evaluation. In Proceedings 
of the Workshop on Automated Syntatic Annotations 
for Interoperable Language Resources at the First 
International Conference on Global Interoperability 
for Language Resources (ICGL'08). 
Tateisi, Y., Yakushiji, A., Ohta, T., and Tsujii, J. 2005. 
Syntax annotation for the GENIA corpus. In Pro-
ceedings Second International Joint Conference on 
Natural Language Processing: Companion Volume 
including Posters/Demos and tutorial abstracts. 
20
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 180?191,
Paris, October 2009. c?2009 Association for Computational Linguistics
Effective Analysis of Causes and Inter-dependencies of Parsing Errors
Tadayoshi Hara1 Yusuke Miyao1
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN
2School of Computer Science, University of Manchester
3NaCTeM (National Center for Text Mining)
{harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii1,2,3
Abstract
In this paper, we propose two methods for
analyzing errors in parsing. One is to clas-
sify errors into categories which grammar
developers can easily associate with de-
fects in grammar or a parsing model and
thus its improvement. The other is to
discover inter-dependencies among errors,
and thus grammar developers can focus on
errors which are crucial for improving the
performance of a parsing model.
The first method uses patterns of er-
rors to associate them with categories of
causes for those errors, such as errors in
scope determination of coordination, PP-
attachment, identification of antecedent of
relative clauses, etc. On the other hand,
the second method, which is based on re-
parsing with one of observed errors cor-
rected, assesses inter-dependencies among
errors by examining which other errors
were to be corrected as a result if a spe-
cific error was corrected.
Experiments show that these two meth-
ods are complementary and by being com-
bined, they can provide useful clues as to
how to improve a given grammar.
1 Introduction
In any kind of complex systems, analyzing causes
of errors is a crucial step for improving its perfor-
mance. In recent sophisticated parsing technolo-
gies, the step of error analysis has been becoming
more and more convoluted and time-consuming,
if not impossible. While common performance
evaluation measures such as F-values are useful to
compare the performance of systems or evaluate
improvement of a system, they hardly give useful
clues as to how to improve a system. Evaluation
measures usually assume uniform units such as the
number of correctly or incorrectly recognized con-
stituent boundaries and their labels, or in a similar
vein, dependency links among words and their la-
bels, and then compute single values such as the F-
value. These values do not give any insights as to
where the weaknesses exist in a parsing model. As
a result, the improvement process takes the form
of time consuming trial-error cycles.
Once grammar developers know the actual dis-
tribution of errors across different categories such
as PP-attachment, complement/adjunct distinc-
tion, gerund/participle distinction, etc., they can
think of focused and systematic improvement of
a parsing model.
Another problem of the F-value in terms of
uniform units is that it does not take inter-
dependencies among errors into consideration. In
particular, for parsers based on grammar for-
malisms such as LFG (Kaplan and Bresnan, 1995),
HPSG (Pollard and Sag, 1994), or CCG (Steed-
man, 2000), units (eg. single predicate-argument
links) are inter-related through hierarchical struc-
tures and structure sharing assumed by these for-
malisms. Single errors are inherently propagated
to other sets of errors. This is also the case, though
to a lesser extent, for parsing models in which
shallow parsing is followed by another component
for semantic label assignment.
In order to address these two issues, we propose
two methods in this paper. One is to recognize
cause categories of errors and the other is to cap-
ture inter-dependencies among errors. The former
method defines various patterns of errors to iden-
tify categories of error causes. The latter method
re-parses a sentence with a single target error cor-
rected, and regards the errors which are corrected
in re-parse as errors dependent on the target.
Although these two methods are implemented
for a specific parser using HPSG (Miyao and Tsu-
jii, 2005; Ninomiya et al, 2006), the same ideas
can be applied to any type of parsing models.
180
Predicate 
Sentence: John    has    come
Predicative event 1:
Predicative event 2:
Word:  hasGrammatical nature:  auxiliary# of arguments:  2
Argument 1 John
Argument 2 come
Predicate 
Word:  comeGrammatical nature:  verb# of arguments:  1
Argument 1 John
Predicate-argument relations
Figure 1: Predicate-argument relations
John aux_2args
ARG1 ARG2
verb_1arg
ARG1
has : come :
Figure 2: Representation of predicate-argument
relations
In the following, Section 2 introduces a parser
and its evaluation metrics, Section 3 illustrates dif-
ficulties in analyzing parsing errors based on com-
mon evaluation measures, and Section 4 proposes
the two methods for effective error analysis. Sec-
tion 5 presents experimental results which show
how our methods work for analyzing actual pars-
ing errors. Section 6 and Section 7 illustrate fur-
ther application of these methods to related topics.
Section 8 summarizes this research and indicates
some of future directions.
2 A parser and its evaluation
A parser is a system which interprets given sen-
tences in terms of structures derived from syn-
tactic or in some cases semantic viewpoints, and
structures constructed as a result are used as es-
sential information for various tasks of natural lan-
guage processing such as information extraction,
machine translation, and so on.
In this paper, we address issues involved in im-
proving the performance of a parser which pro-
duces structural representations deeper than sur-
face constituent structures. Such a parser is called
a ?deep parser.? In many deep parsers, the output
structure is defined by a linguistics-based gram-
mar framework such as CFG, CCG (Steedman,
2000), LFG (Kaplan and Bresnan, 1995) or HPSG
Abbr. Full Abbr. Full
aux auxiliary conj conjunction
prep prepositional lgs logical subject
verb verb app apposition
coord coordination relative relative
det determiner Narg(s) takes N arguments
adj adjunction mod modifies a word
Table 1: Descriptions for predicate types
(Pollard and Sag, 1994). Alternatively, some deep
parsing models assume staged processing in which
a stage of shallow parsing is followed by a stage of
semantic role labeling, which assigns labels indi-
cating semantic relationships between predicates
and their arguments. In either case, we assume a
parser to produce a single ?deep? structural rep-
resentation for a given sentence, which is chosen
from a set of possible interpretations as the most
probable one by a disambiguation model.
For evaluation of the performance of a parser,
various metrics have been introduced according
to the structure captured by a given grammar
formalism or a system of semantic labels. In
most cases, instead of examining correctness for
a whole structure, a parser is evaluated in terms of
the F-value which shows how correctly it recog-
nizes relationships among words and assigns ?la-
bels? to the relationships in the structure. In this
paper, we assume a certain type of ?predicate-
argument relation.?
In this measurement, a structure given for a
sentence is decomposed into a set of predicative
words and their arguments. A predicate takes
other words as its arguments. In our representa-
tion, the arguments are labeled by semantically
neutral labels such as ARGn(n = 1...5) and
MOD. In this representation, a basic unit is a
triplet, such as
<Predicate:PredicateType,
ArgumentLabel,
Argument>,
where ?Predicate? and ?Argument? are surface
words. As shown in the examples in Section 4,
?PredicateType? bears extra information concern-
ing the syntactic construction in which the triplet
is embedded. ARG1-ARG5 express relations be-
tween a Head and its complement, while MOD ex-
presses a relation between an Adjunct and its mod-
ifiee. Since all dependency relations are expressed
by triplets, triplets contain not only semantic de-
181
I saw a girl with a telescope Correct answer:
ARG1 ARG2
ARG1 ARG2
Parser output:
ARG1 ARG2
ARG1 ARG2Compare
Error (25%): 
ARG1
ARG1
Correct (75%):
ARG1 ARG2
ARG2ARG1 ARG2
ARG2
I saw a girl with a telescope 
I saw a girl with a telescope 
I saw a girl with a telescope 
Figure 3: An example of parsing performance
evaluations
pendencies but also many dependencies which are
essentially syntactic in nature. Figure 1 shows an
example used in Miyao and Tsujii (2005) and Ni-
nomiya et al (2006).
This example shows predicate-argument rela-
tions for ?John has come.? There are two pred-
icates in this sentence, ?has? and ?come?. The
word ?has?, which is used as an auxiliary verb,
takes two words, ?John? and ?come?, as its ar-
guments, and therefore two triplets of predicate-
argument relation, <has ARG1 John> and <has
ARG2 come>. As for the predicative word
?come?, we have one triplet <come ARG1 John>.
Note that, in this HPSG analysis, the auxiliary
verb ?has? is analyzed in such a way that it takes
one NP as subject and one VP as complement,
and that the subject of the auxiliary verb is shared
by the verb (?come?) in VP as its subject (Fig-
ure 2). The fact that ?has? in this sentence is an
auxiliary verb is indicated by the ?PredicateType?,
aux 2args. A ?PredicateType? consists of a type
and the number of arguments it takes (Table 1).
3 Difficulties in analyzing parsing errors
Figure 3 shows an example of the evaluation of
the parser based on these predicate-argument rela-
tions. Note that the predicate types are abbreviated
in this figure. In the sentence ?I saw a girl with a
telescope?, there should be four triplets for the two
predicates, ?saw? and ?with,? each of which takes
Error:
They completed the sale of for 
ARG1ARG1
it to him $1,000 
Conflict
Analysis 2: (Impossible)
They completed the sale of for ARG1ARG1
it to him $1,000 
Analysis 1: (Possible) ARG1ARG1
ARG1ARG1
Can each error occur independently?
They completed the sale of for ARG1 ARG1
it to him $1,000 
ARG1ARG1
Figure 4: Sketch of error propagation
The book on which read the shelf  I yesterdayARG1 ARG2
ARG2ARG1
Error:
Figure 5: Parsing errors around one relative clause
attachment
two arguments. Although the parser output does
indeed contain four triplets, the first argument of
?with? is not the correct one. Thus, this output is
erroneous, with the F-value of 75%.
While the F-value thus computed is fine for cap-
turing the performance of a parser, it does not offer
any help for improving its performance.
First, because it does not give any indica-
tion on what portion of erroneous triplets are in
PP-attachment, complement/adjunct distinction,
gerund/participle distinction, etc., one cannot de-
termine which part of a parsing model should be
improved. In order to identify error categories, we
have to manually compare a parsing output with
a correct parse and classify them. Consider again
the example in Figure 3. We can easily observe
that ?ARG1? of predicate ?with? was mistaken. In
this case, the word linked via ?ARG1? represents
a modifiee of the prepositional phrase, and thereby
we conclude that the error is in PP-attachment.
While the process looks straightforward for this
simple sentence and error, to perform such a man-
ual inspection for all sentences and more complex
types of errors is costly, and becomes inhibitive
when the size of a test set of sentences is realisti-
182
cally large.
Another problem with the F-value is that it ig-
nores inter-dependencies among errors. Since the
F-value does not consider inter-dependencies, one
cannot determine which errors are more crucial
than others in terms of the performance of the sys-
tem as a whole.
A simple example of inter-dependency is shown
in Figure 4. ?ARG1? of ?for? and ?to? were mis-
taken by a parser, both of which can be classified
as PP-attachments as in Figure 3. However, the
two errors are not independent. The former error
can occur by itself (Analysis 1) while the latter
cannot because of the structural conflict with the
former (Analysis 2). The occurrence of the latter
error thus forces the former.
Moreover, inter-dependency in a deep parser
based on linguistics-based formalisms can be
complicated. Error propagation is ingrained in
grammar itself. Consider Figure 5. In this exam-
ple, a wrong decision on the antecedent of a rela-
tive clause results in a wrong triplet of the predi-
cate in the embedded clause with the antecedent.
That is, the two erroneous triplets, one of the
?ARG1? of ?which? and the other of the ?ARG2?
of ?read,? were caused by a single wrong deci-
sion of the antecedent of a relative clause. Such
a propagation of errors can be even more compli-
cated, for example, when the predicate in the rela-
tive clause is a control verb.
In the following section we propose two meth-
ods for analyzing errors. Although both meth-
ods are implemented for the specific parser Enju
(Miyao and Tsujii, 2005; Ninomiya et al, 2006),
the same ideas can be implemented for any parsing
model.
4 Methods for effective error analysis
4.1 Recognizing categories of error causes
While the Enju parser produces rich feature struc-
tures as output, the performance is evaluated by
the F-value in terms of basic units of predicate-
argment structure. As we illustrated in Section 2,
the basic unit is a triplet in the following form.
<Predicate:PredicateType,
ArgumentLabel,
Argument>
We illustrated in Section 2 how we can identify
errors in PP-attachment simply by examining a
The  car  was  designed to : use   it  for ...
Correct output:
aux_2argsto :[ verb1 ] ?ARG3 [ verb2 ]Parser output:
aux_mod_2args
MOD
to :
ARG2
Unknown subject ARG1 ARG1
[ verb1 ] ? [ verb2 ]
aux_2args
Example:
Parser output:
Correct answer:
ARG3
The  car  was  designed to : use   it  for ...aux_mod_2args
MOD ARG2
Unknown subject ARG1 ARG1
Pattern:
(Patterns of correct answer and parser output can be interchanged)
Figure 6: Pattern for ?To-infinitive for modi-
fier/argument of verb?
triplet produced by the parser with the correspond-
ing triplet in the gold standard parse.
However, in more complex cases, we have to
consider a set of mismatched triplets collectively
in order to map errors to meaningful error causes.
The following are typical examples of error causes
and pattern rules which identify them.
(1) Interpretation of Infinitival Clauses as Adjunct
or Complement
Two different types of interpretations of the in-
finitival clauses are explicitly indicated by ?Predi-
cateType.? Consider the following two sentences.
(a) [Infinitival clause as an adjunct of the main
clause]
The car was designed (by John) to use it for
business trips.
(b) [Infinitival clause as an argument of catena-
tive verb]
The car is designed to run fast.
In both sentences, ?to? is treated as a predicate to
represent the infinitival clauses in triplets. How-
ever, Enju marks the ?PredicateType? of (a) as
?aux-mod-2args,? while it marks the predicate
simply as ?aux-2args? in (b). Furthermore, the
linkage between the main clause and the infinitival
clause is treated differently. In (a), the infinitival
clause takes the main clause with relation MOD,
while in (b) the main clause takes the infinitival
183
[ gerund ]: verb_Narg(s)Parser output: [ gerund ]: verb_mod_Narg(s)Correct answer:(Patterns of correct answer and parser output can be interchanged)
Pattern:
Example:
The customers walk the door
a   package   for   them
expecting: verb_mod_3args
you to have
in MOD
ARG1
ARG2 ARG3
Parser output:
Correct output:
The customers walk the door
a   package   for   them
expecting: verb_3args
you to have
in
Not exist 
ARG2 ARG3
ARG1 (MOD)
? ?
? ?
Figure 7: Pattern for ?Gerund acts as modifier or
not?
clause as ARG3. Furthermore, in the catenative
verb interpretation of ?designed?, the deep object
(the surface subject in this example) fills ARG1
of the verb in the infinitival clause (complement),
while in the adjunct interpretation, the deep sub-
ject which is missing in this sentence occupies
the same role. Consequently, a single erroneous
choice between these two interpretations results in
a set of mismatched triplets.
We recognize such a set of mismatched triplets
by a pattern rule (Figure 6) and map them to this
type of error cause.
(2) Interpretation of Gerund-Participle interpreta-
tions
A treatment similar to (1) is taken for different
interpretations of Gerund. Interpretation as Ad-
junct of a main clause is signaled by the ?Predi-
cateType? verb-mod-*, while an interpretation as
a modifier of a noun is represented by the ?Predi-
cateType? verb (Figure 7).
(3) Interpretation of ?by?
A prepositional phrase with ?by? in a passive
clause can be interpreted as a deep subject, while
the same phrase can be interpreted as an ordinary
PP phrase that is used as an adjunct. The first in-
terpretation is marked by the ?PredicateType? lgs
(logical subject) which takes only one argument.
The relationship between the passivized verb and
the deep subject is captured by ARG1 which goes
Example:
Pattern:
Correct output:
Parser output: prep_2args
Unknown subject
[ verb1 ] ?ARG1ARG1 ?
lgs_1arg ARG1[ verb1 ] ? ?ARG1
A 50-state study released in September  by : Friends ?
Unknown subject
? ARG1ARG1
prep_2argsParser output:
Correct answer:
A 50-state study released in September  by : Friends ??
ARG1ARG1
lgs_1argARG1
(Patterns of correct answer and parser output can be interchanged)
Figure 8: Pattern for ?Subject for passive sentence
or not?
Example:
Pattern:
relative_1arg
ARG1
Parser output: ARG1/2
Error
Parser output:
Correct answer:
The  book on which : read 
ARG1
the  shelf  I yesterday
ARG2
The  book on which : read 
ARG1
the  shelf  I yesterday
ARG2
relative_1arg
relative_1arg
Error
? ?
Figure 9: Pattern for ?Relative clause attachment?
from the verb to the noun phrase. On the other
hand, in the interpretation as an ordinary PP, the
preposition as predicate links the main verb and
NP via ARG1 and ARG2, respectively (Figure 8).
Again, a set of mismatched triplets should be
mapped to a single cause of errors via a pattern
rule.
(4) Antecedent of a Relative Clause
This type of error is manifested by two mis-
matched triplets with different predicates. This is
because a wrong choice of antecedent for a rela-
tive clause results in a wrong link for the trace of
the relative clause.
Since a relative clause pronoun is treated as a
184
Cause categories Patterns
[Argument selection]
Prepositional attachment ARG1 of prep *
Adjunction attachment ARG1 of adj *
Conjunction attachment ARG1 of conj *
Head selection for noun phrase ARG1 of det *
Coordination ARG1/2 of coord *
[Predicate type selection]
Preposition/Adjunction prep * ? adj *
Gerund acts as modifier/not verb mod Narg(s)
? verb Narg(s)
Coordination/conjunction coord * ? conj *
# of arguments for preposition prep Marg(s)
? prep Narg(s)
Adjunction/adjunctive noun adj * ? noun *
[More structural errors]
To-infinitive for see Figure 6
modifier/argument of verb
Subject for passive sentence/not see Figure 8
[Others]
Comma any error around ?,?
Relative clause attachment see Figure 9
Table 2: Defined patterns for cause categories
predicate which takes the antecedent as its single
argument, identification of error type can be done
simply by looking at ARG1. However, since the
errors usually propagate to the triplets that contain
their traces, we have to map them together to the
single error (Figure 9).
Table 2 shows the errors across different types
which our current version of pattern rules can
identify.
4.2 Capturing inter-dependencies among
errors
Some inter-dependencies among erroneous
triplets are ingrained in grammar, such as the case
of antecedent of a relative clause in (4) in Section
4.1. Some are caused by general constraints such
as the projection principle in dependency structure
(Figure 4 in Section 2).
Regardless of causes of dependencies, to recog-
nize inter-dependencies among errors is a crucial
step of effective error analysis.
Our method consists of the following four steps:
[Step 1] Re-parsing a target sentence: A given sen-
tence is re-parsed under the condition where an er-
ror is forcibly corrected.
[Step 2] Forming a network of inter-dependencies
of errors: By comparing the new parse result (a
set of triplets) with the initial parse result, this
step creates a directed graph of errors in the ini-
1
Re-parse a sentence under the condition whereeach error is forcibly corrected 
1
2
3
Correct 2
1
1
Form inter-dependent error groups anderror propagation network
4 1
433CorrectCorrect
Correct
disappear
disappear
disappear
disappear
,
,
,
1 2 3 4
ARG1our work force todayon
Errors:
ARG1 ARG1ARG2
ARG2 ARG1 ARG1 ARG1It  has  no  bearing
2 3 4
5
5 4Correct disappear1 32, , ,
4,
2 4,,
2 3,,
5Propagation
Resultant network:
Inter-dependent error group Inter-dependent error group
(a)
(b)
(c)
Inter-dependency among errors:re-parse
re-parse
re-parse
re-parse
re-parse
Figure 10: Schema of capturing inter-
dependencies
tial parse. A directed link shows that correction of
the error in the starting node produces a new parse
result in which the error in the receiving node of
the link disappears.
[Step 3] Forming groups of inter-dependent errors:
This step recognizes a group of inter-dependent er-
rors which forms a directed circle in the network
created by [Step 2].
[Step 4] Forming a network of error propagation:
This step creates a new network by reducing each
of inter-dependent error groups of [Step 3] to a sin-
gle node.
Figure 10 illustrates how these steps work. In
this example, while ?today? should modify the
noun phrase ?our work force?, the initial parse
wrongly takes ?today? as the head noun of the
whole noun phrase. As a result, there are five er-
rors; three wrong outputs, ?ARG2? of ?on? (Er-
ror 1), ?ARG1? of ?our? (Error 2) and ?ARG1?
of ?work? (Error 3). There is an extra triplet for
?ARG1? of ?force? (Error 4), and a triplet for
?ARG1? of ?today? (Error 5) is missing (Figure
10 (a)).
Figure 10 (b) shows inter-dependencies among
the errors recognized by [Step 2], and Figure 10
185
# ofCause categories of errors Errors Locations
Classified 2,078 1,671
[Argument selection]
Prepositional attachment 579 579
Adjunction attachment 261 261
Conjunction attachment 43 40
Head selection for noun phrase 30 30
Coordination 202 184
[Predicate type selection]
Preposition/Adjunction 108 54
Gerund acts as modifier/not 84 31
Coordination/conjunction 54 27
# of arguments for preposition 51 17
Adjunction/adjunctive noun 13 13
[More structural errors]
To-infinitive for 120 22
modifier/argument of verb
Subject for passive sentence/not 8 3
[Others]
Comma 444 372
Relative clause attachment 102 38
Unclassified 2,631 ?
Total 4,709 ?
Table 3: Errors classified into cause categories
(c) shows what the resultant network looks like.
An inter-dependent error group of 1, 2, 3 and 4 is
recognized by [Step 3] and represented as a single
node. Error 5 is propagated to this node in the final
network.
5 Experiments
We applied our methods to the analyses of actual
errors produced by Enju. This version of Enju was
trained on the Penn Treebank (Marcus et al, 1994)
Section 2-21.
5.1 Observation of identified cause categories
We first parsed sentences in PTB Section 22, and
based on the observation of errors, we defined the
patterns in Section 4. We then parsed sentences in
Section 0. The errors in Section 0 were mapped to
error cause categories by the pattern rules created
for Section 22.
Table 3 summarizes the distribution across the
causes of errors. The left and right numbers in the
table show the number of erroneous triplets clas-
sified into the categories and the frequency of the
patterns matched, respectively. The table shows
that, with the 14 pattern rules, we successfully ob-
served 1,671 hits and 2,078 erroneous triplets are
dealt with by these hits. This amounts to more
than 40% erroneous triplets (2,078/4,709). Since
this was the first attempt, we expect the coverage
can be easily improved by adding new patterns.
Evaluated sentences (erroneous) 1,811 (1,009)
Errors (Correctable) 4,709 (3,085)
Inter-dependent error groups 1,978
Correction propagations 501
F-score (LP/LR) 90.69 (90.78/90.59)
Table 4: Summary of inter-dependencies




       	 
 










Figure 11: Frequency of each size of inter-
dependent error group
From the table, we can observe that a signif-
icant portion of errors is covered by simple types
of error causes such as PP-attachment and Adjunct
attachment. They are simple in the sense that the
number of erroneous triplets treated and the fre-
quency of the pattern application coincide. How-
ever, their conceived significance may be over-
rated. These simple types may constitute parts of
more complex error causes. Furthermore, since
pattern rules for these simple causes are easy to
prepare and have already been covered by the cur-
rent version, most of the remaining 60% of the er-
roneous triplets are likely to require patterns for
more complex causes.
On the other hand, patterns for complex causes
collect more erroneous triplets once they are fired.
This tendency is more noticeable in structural pat-
terns of errors. For example, in ?To-infinitive for
modifier/argument of verb,? there were only 22
hits for the pattern, while the number of erroneous
triplets is 120. This implies five triplets per hit.
This is because, in a deep parser, a wrong choice
between adjunct or complement interpretations of
a to-infinitival clause affects the interpretation of
implicit arguments in the clause through control.
Though expected, such detailed observations show
how differences between shallow and deep parsers
may affect evaluation methods and the methods of
analyzing errors.
5.2 Observation of inter-dependencies
In the inter-dependency experiments we per-
formed, some errors could not be forcibly cor-
rected by our method. This was because the parser
186
The  asbestos  fiber  ,  crocidolite ,  is  unusually  resilient  once
it  enters  the  lungs  ,  with  even  brief  exposures  to  it causing
symptoms  that  show  up  decades  later  ,  researchers  said  .
(a) (b)(c) (d)
? fiber      , : crocidolite ?app_2args
? fiber      , : crocidolite ?coord_2args
Correct answer:
Parser output:
? is   usually   resilient   ? the   lungs      ,       with   ?
? symptoms   that    show : up   decades   later  ?
Parser output:
Correct answer: verb_1arg
? symptoms   that    show : up   decades   later  ?verb_2args
? it   causing   symptoms   that   show   up   decades   later  ?
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Inter-dependent error group (c):
Inter-dependent error group (d):
ARG1 ARG2
ARG1 ARG2
ARG1ARG1
ARG1ARG1
ARG1
ARG1
ARG1
ARG2
ARG1
ARG1
Figure 12: Obtained inter-dependent error groups
we use prunes less probable parse substructures
during parsing. In some cases, even if we gave a
large positive value to a triplet which should be in-
cluded in the final parse, parsing paths which can
contain the triplet were pruned before. In this re-
search, we ignored such errors as ?uncorrectable?
ones, and focused on the remaining ?correctable?
errors.
Table 4 shows a summary of the analysis. As the
previous experiment, Enju produced 4,709 errors
for Section 0, of which 3,085 were correctable. By
applying the method illustrated in Section 4.2, we
obtained 1,978 inter-dependent error groups and
501 correction propagation relationships among
the groups.
Figure 11 shows the frequency of the size of
inter-dependent error groups. About half of the
groups contain only single errors which could
have only one-way correction propagations with
other errors or were completely independent of
other errors.
Figure 12 shows an example of the extracted
inter-dependent error groups. For the sentence
shown at the top, Enju gave seven errors. By ap-
plying the method in Section 4.2, these errors were
grouped into four inter-dependent error groups (a)
to (d), and no correction propagations were de-
She  says  she  offered  Mrs.  Yeargin a  quiet  resignation  and
thought  she  could  help  save  her  teaching  certificate .(a) (b)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):Correct answer:
Parser output:
? she  could  help  save : her  teaching  certificate .verb_3args
? she  could  help  save : her  teaching  certificate .verb_2args
Correct answer:
Parser output:
? thought   she  could   help : save   ?verb_2args
? thought   she  could   help : save   ?aux_2args
Correction propagation from (a) to (b)
ARG2
ARG2
ARG2
ARG2
ARG1 ARG2
ARG1 ARG2
ARG3ARG1
ARG1
ARG2
ARG2
ARG1
Figure 13: Correction propagation between ob-
tained inter-dependent error groups
tected among them. Group (a) contains two errors
on the comma?s local behavior as apposition or co-
ordination. Group (b) contains the errors on the
words which give almost the same attachment be-
haviors. Group (c) contains the errors on whether
the verb ?show? took ?decades? as its object or
not. Group (d) contains an error on the attachment
of the adverb ?later?. Regardless of the overlap
of the regions in the sentence for (c) and (d), our
approach successfully grouped the errors into two
independent groups. The method shows that the
errors in each group are inter-dependent, but er-
rors in one group are independent of those in an-
other. This enables us to concentrate on each of
the co-occurring error groups separately, without
minding the errors in other groups.
Figure 13 shows another example. In this ex-
ample, eight errors for a sentence were classified
into two inter-dependent error groups (a) and (b).
Moreover, it shows that the correction of group (a)
results in correction of group (b).
The errors in group (a) were related to the
choice as to whether ?help? had an auxiliary or
a pure verbal role. The errors in group (b) were
related with the choice as to whether ?save? took
only one object (?her teaching certificate?) or two
objects (?her? and ?teaching certificate?). Be-
tween group (a) and (b), no ?structural? con-
187
It  invests  heavily  in  dollar-denominated  securities  overseas  and
is  currently  waiving  management  fees  ,  which  boosts  its yield .
(a)(b)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Adjunction attachmentCause categories: 
Comma, Relative clause attachmentCause categories: 
It  invests  heavily  in  ? securities  overseas : ?adj_1argARG1 ARG1
? is  currently  waiving  management  fees   ,   which   boosts  ?
ARG1ARG1ARG1
ARG1ARG1ARG1
Figure 14: Combining our two methods (1)
flict could arise when correcting only each of the
groups. We could then hypothesize that the cor-
rection propagation between the two groups were
caused by the disambiguation model.
By dividing the errors into minimal units and
clarifying the effects of correcting a target error,
we can conclude that the inter-dependent group
(a) should be handled first for effective improve-
ment of the parser. In such a way, obtained inter-
dependencies among errors can suggest an effec-
tive strategy for parser improvement.
5.3 Combination of the two methods
By combining the two methods described in Sec-
tion 4.1 and 4.2, we can see how each error cause
affects the performance of a parser. The results
are summarized in Table 5. The leftmost column
in the table shows the numbers of errors in terms
of triplets, which are the same as the leftmost col-
umn in Table 3.
The ?independence rate? shows the ratio of er-
roneous triplets in the category which are not af-
fected by correction of other erroneous triplets. On
the other hand, the ?correction effect? shows how
many erroneous triplets would be corrected if one
of the erroneous triplets in the category was cor-
rected. These two columns are computed by using
the error propagation network constructed in Sec-
tion 4.2. That is, by using the network we obtain
the number of erroneous triplets to be corrected if
a given erroneous triplet in the category was cor-
rected, sum up these numbers and then calculate
the average number of expected side-effect correc-
Clark  J.  Vitulli was  named  senior  vice  president  and  general  
manager  of   this  U.S.  sales  and  marketing  arm of Japanese
auto Maker Mazda Motor Corp .
(b)(a)
Sentence:
Inter-dependent error group (a):
Inter-dependent error group (b):
Coordination (fragment)Head selection for noun phraseCause categories: 
? president  ? of  this  U.S.  sales   and : ?coord_2argsARG1ARG1
of   this : U.S. sales and : marketing armdet_1arg coord_2args
ARG2ARG1ARG2 ARG1
ARG2 ARG1 ARG1 ARG1 ARG2
Correction propagation from (a) to (b)
Coordination (fragment)Cause categories: 
Figure 15: Combining our two methods (2)
tion per erroneous triplet in the category.
Figure 14 shows an example of independent er-
rors. For the sentence at the top, the parser pro-
duced four errors. The method in Section 4.2
successfully discovered two inter-dependent error
groups (a) and (b). There was no error propaga-
tion relation between the two groups. On the other
hand, the method in Section 4.1 associated all of
these four errors with the categories of ?Adjunc-
tion attachment,? ?Comma? and ?Relative clause
attachment,? and the error for the ?Adjunction at-
tachment? corresponds to the inter-dependent er-
ror group (a). Because this group is not a receiving
node of any propagation in the network, the error
is regarded as an ?independent? one.
Independent errors mean that, if a new parsing
model could correct them, the correction would
not be destroyed by other errors which remain in
the new parsing model.
The correction effect shows the opposite and
desirable effect of the nature of the dependency
among errors which the propagation network rep-
resents. This means that, if one of erroneous
triplets in the category was corrected, the correc-
tion would be amplified through propagation, and
as a result other errors would also be corrected.
We show an example of the correction effect in
Figure 15. In the figure, the parser had six errors;
three false outputs for ARG1 of ?and,? ?this? and
188
Independence Correction Expected rangeCause categories of errors # of errors
rate (%) effect (%) of error correction
[Argument selection]
Prepositional attachment 579 74.8 144.3 625.0 - 835.5
Adjunction attachment 261 56.6 179.6 265.3 - 468.8
Conjunction attachment 43 36.4 239.4 37.5 - 102.9
Head selection for noun phrase 30 0.0 381.8 0.0 - 114.5
Coordination 202 42.5 221.2 189.9 - 446.8
[Predicate type selection]
Preposition/Adjunction 108 41.7 158.3 71.3 - 171.0
Gerund acts as modifier/not 84 46.2 159.0 61.7 - 133.0
Coordination/conjunction 54 44.4 169.4 40.6 - 91.5
# of arguments for preposition 51 95.8 108.3 52.9 - 55.2
Adjunction/adjunctive noun 13 75.0 125.0 12.2 - 16.3
[More structural errors]
To-infinitive for 120 36.0 116.0 50.1 - 139.2
modifier/argument of verb
Subject for passive sentence/not 8 37.5 112.5 3.4 - 9.0
[Others]
Comma 444 39.5 194.4 341.0 - 863.1
Relative clause attachment 102 32.1 141.7 46.4 - 144.5
Table 5: Correction propagations between errors for each cause category and the other errors
?U.S.,? two false outputs for ARG2 of ?of? and
?and,? and a missing output for ARG1 of ?sales.?
Our method for inter-dependencies classified these
errors into two inter-dependent error groups (a)
and (b), and extracted an correction propagation
from (a) to (b). Our method for cause categories,
on the other hand, associated two errors of ?and?
with the category ?Coordination? and one error of
?this? with the category ?Head selection for noun
phrase.? When we correct an error in the interde-
pendent error group (a), the correction leads to not
only correction of the other errors in (a) but also
correction of the error in (b) via correction prop-
agation from (a) to (b). Therefore, a correction
effect of an error in group (a) results in 6.0.
On the basis of the above considerations, we es-
timated the range of the effect which an error cor-
rection in each category has. The minimum of ex-
pected correction range in Table 5 is given by the
product of the number of erroneous triplets in the
category, the independence rate and the correction
effect. On the other hand, the maximum is given
by the product of the number of erroneous triplets
in the category and the correction effect. This as-
sumes that all corrections made in the category are
not cancelled by other errors, while the figure in
the minimum are based on the assumption that all
corrections made in the category, except for the in-
dependent ones, are cancelled by other errors.
Table 5 would thus suggest which categories
should be resolved with high priority, from three
points of view: the number of errors in the cat-
egory, the number of independent errors, and the
correction effect.
6 Further applications of our methods
In this section, as an example of the further ap-
plication of our methods, we attempt to analyze
parsing behaviors in domain adaptation from the
viewpoints of error cause categories.
In Hara et al (2007), we proposed a method for
adapting Enju to a target domain, and then suc-
ceeded in improving the parser performance for
the GENIA corpus (Kim et al, 2003), a biomed-
ical domain. Table 6 summarizes the parsing re-
sults for three types of settings respectively: pars-
ing PTB with Enju (?Enju for PTB?), parsing GE-
NIA with Enju (?Enju for GENIA?), and parsing
GENIA with the adapted model (?Adapted for GE-
NIA?). We then analyzed the performance transi-
tion among these settings from the viewpoint of
the cause categories given in Section 4.1 (Table 7).
In order to compare the error frequencies among
different settings, we took the percentage of target
errors in all of the evaluated triplets. The signed
values between the two settings show how much
the errors increased when moving from the left set-
tings to the right ones.
When we focus on the transition from ?Enju
for PTB? to ?Enju for GENIA,? we can observe
that the change in the domain resulted in a dif-
ferent distribution of error causes. The errors for
most categories increased, and in particular, the er-
rors for ?Prepositional attachment? and ?Coordi-
189
Enju for PTB Enju for GENIA Adapted for GENIA
Evaluated sentences 1,811 842 842
Evaluated triplets 44,934 22,230 22,230
Errors 4,709 3,120 2,229
F-score (LP/LR) 90.69 (90.78/90.59) 87.41 (87.60/87.22) 90.93 (91.10/90.76)
Table 6: Summary of parsing performances for domain and model variations
Rate of errors against total examined relations in test set (%)Cause categories of errors Enju for PTB ?? Enju for GENIA ?? Adapted for GENIA
Classified 4.62 +2.60? 7.22 ?1.80? 5.42
[Argument selection]
Prepositional attachment 1.29 +0.93? 2.22 ?0.64? 1.58
Adjunction attachment 0.58 +0.38? 0.96 ?0.20? 0.76
Conjunction attachment 0.10 ?0.04? 0.06 ?0.04? 0.02
Head selection for noun phrase 0.07 +0.17? 0.24 ?0.06? 0.18
Coordination 0.45 +0.59? 1.04 ?0.25? 0.79
[Predicate type selection]
Preposition/Adjunction 0.24 +0.08? 0.32 ?0.06? 0.26
Gerund acts as modifier/not 0.19 ?0.07? 0.12 +0.01? 0.13
Coordination/conjunction 0.12 ?0.00? 0.12 ?0.07? 0.05
# of arguments for preposition 0.11 ?0.02? 0.09 ?0.00? 0.09
Adjunction/adjunctive noun 0.03 +0.19? 0.22 ?0.08? 0.14
[More structural errors]
To-infinitive for 0.27 +0.02? 0.29 ?0.09? 0.20
modifier/argument of verb
Subject for passive sentence/not 0.02 +0.34? 0.36 +0.01? 0.37
[Others]
Comma 0.99 ?0.03? 0.96 ?0.31? 0.65
Relative clause attachment 0.23 +0.05? 0.28 ?0.03? 0.25
Unclassified 5.86 +0.96? 6.82 ?2.22? 4.60
Total (Classified + Unclassified) 10.48 +3.56? 14.04 ?4.01? 10.03
Table 7: Error distributions for domain and model variations
nation? increased remarkably. On the other hand,
the transition from ?Enju for GENIA? to ?Adapted
for GENIA? shows that their adaptation method
succeeded in reducing the errors for most cate-
gories to some extent. However, for ?Preposi-
tional attachment,? ?Coordination,? and ?Subject
for passive sentence or not,? there were still no-
ticeable gaps in error distribution between ?Enju
for PTB? and ?Adapted for GENIA.? This would
mean that the adapted model requires further per-
formance improvement if we expect the same level
of performances for those categories as the parser
originally obtained in PTB.
We could thus capture some biases of cause
categories which occur in domain transition or
in domain adaptation, which would not be clari-
fied by F-score evaluation methods. With inter-
dependencies given by the method described in
Section 4.2, the above analysis would be useful
for effectively exploring further adaptation.
7 Related works
Although there have been many researchers who
analyzed errors in their own systems in the experi-
ments, there has been little research which focused
on error analysis itself.
In the field of parsing, McDonald and Nivre
(2007) compared parsing errors between graph-
based and transition-based parsers. They consid-
ered accuracy transitions from various points of
view, and the obtained statistical data suggested
that error propagation seemed to occur in the
graph structures of parsing outputs. Our research
proceeded one step further and attempted to re-
veal the nature of the propagations. In examin-
ing the combination of the two types of parsing,
they utilized approaches similar to our method for
capturing inter-dependencies of errors. They al-
lowed a parser to give only structures produced by
the parsers and utilized the ideas for evaluating the
parser?s potentials, whereas we utilized it for ob-
serving error propagations.
Dredze et al (2007) showed that many of the
parsing errors in domain adaptation tasks may
come from inconsistencies between the annota-
tions of training resources. This would sug-
gest that just error comparisons without consider-
ing the inconsistencies could lead to a misunder-
190
standing of what happens in domain transitions.
The summarized error cause categories and inter-
dependencies given by our methods would be use-
ful clues for extracting such domain-dependent er-
ror phenomena.
When we look into other research areas in nat-
ural language processing, Gime?nez and Ma`rquez
(2008) proposed an automatic error analysis ap-
proach in machine translation (MT) technologies.
They developed a metric set which could capture
features in MT outputs at different linguistic lev-
els with different levels of granularity. Like we
considered parsing systems, they explored ways to
resolve costly and rewardless error analysis in the
MT field. One of their objectives was to enable
researchers to easily obtain detailed linguistic re-
ports on the behavior of their systems, and to con-
centrate on analyses for the system improvements.
8 Conclusion
We proposed two methods for analyzing parsing
errors. One is to assign errors to cause categories,
and the other is to capture inter-dependencies
among errors. The first method defines error pat-
terns to identify cause categories and then asso-
ciates errors involved in the patterns with the cor-
responding categories. The second method re-
parses a sentence with a target error corrected, and
regards errors corrected together as dependent on
the target.
In our experiments with an HPSG parser, we
successfully associated more than 40% of the er-
rors with 14 cause categories, and captured 1,978
inter-dependent error groups. Moreover, the com-
bination of our methods gave a more detailed error
analysis for effective improvement of the parser.
In our future work, we would give more pat-
tern rules for classifying a large percentage of er-
rors into cause categories, and incorporate uncor-
rectable errors into inter-dependency analysis. Af-
ter improving the analytical facilities of our indi-
vidual methods, we would explore the possibil-
ity of combining the methods for obtaining more
powerful and detailed clues on how to improve
parsing performance.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
References
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Joa?o V. Grac?a, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proceedings of the
CoNLL Shared Task Session of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 1051?1055.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards het-
erogeneous automatic mt error analysis. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08), pages 1894?1901.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of 10th International Confer-
ence on Parsing Technologies (IWPT 2007), pages
11?22.
Ronald M. Kaplan and Joan Bresnan. 1995. Lexical-
functional grammar: A formal system for gram-
matical representation. Formal Issues in Lexical-
Functional Grammar, pages 29?130.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1):i180?i182.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
ARPA Human Language Technology Workshop.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 83?90.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 155?163.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
191
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93?102,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Probabilistic models for disambiguation of an HPSG-based chart generator
Hiroko Nakanishi
 
 
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku
Tokyo 113-0033, Japan
Yusuke Miyao
 

CREST, JST
Honcho 4-1-8, Kawaguchi-shi
Saitama 332-0012, Japan
n165, yusuke, tsujii@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii  

School of Informatics
University of Manchester
POBox 88, Sackville St
MANCHESTER M60 1QD, UK
Abstract
We describe probabilistic models for a
chart generator based on HPSG. Within
the research field of parsing with lex-
icalized grammars such as HPSG, re-
cent developments have achieved efficient
estimation of probabilistic models and
high-speed parsing guided by probabilis-
tic models. The focus of this paper is
to show that two essential techniques ?
model estimation on packed parse forests
and beam search during parsing ? are suc-
cessfully exported to the task of natural
language generation. Additionally, we re-
port empirical evaluation of the perfor-
mance of several disambiguation models
and how the performance changes accord-
ing to the feature set used in the models
and the size of training data.
1 Introduction
Surface realization is the final stage of natural lan-
guage generation which receives a semantic rep-
resentation and outputs a corresponding sentence
where all words are properly inflected and ordered.
This paper presents log-linear models to address the
ambiguity which arises when HPSG (Head-driven
Phrase Structure Grammar (Pollard and Sag, 1994))
is applied to sentence generation. Usually a single
semantic representation can be realized as several
sentences. For example, consider the following two
sentences generated from the same input.
 The complicated language in the huge new law
has muddied the fight.
 The language in the huge new law complicated
has muddied the fight.
The latter is not an appropriate realization because
?complicated? tends to be wrongly interpreted to
modify ?law?. Therefore the generator needs to se-
lect a candidate sentence which is more fluent and
easier to understand than others.
In principle, we need to enumerate all alternative
realizations in order to estimate a log-linear model
for generation. It therefore requires high compu-
tational cost to estimate a probabilistic model for a
wide-coverage grammar because there are consider-
able ambiguities and the alternative realizations are
hard to enumerate explicitly. Moreover, even after
the model has been estimated, to explore all possible
candidates in runtime is also expensive. The same
problems also arise with HPSG parsing, and recent
studies (Tsuruoka et al, 2004; Miyao and Tsujii,
2005; Ninomiya et al, 2005) proposed a number of
solutions including the methods of estimating log-
linear models using packed forests of parse trees and
pruning improbable candidates during parsing.
The aim of this paper is to apply these techniques
to generation. Since parsing and generation both
output the best probable tree under some constraints,
we expect that techniques that work effectively in
parsing are also beneficial for generation. First, we
enabled estimation of log-linear models with less
cost by representing a set of generation trees in a
packed forest. The forest representation was ob-
tained by adopting chart generation (Kay, 1996; Car-
93
roll et al, 1999) where ambiguous candidates are
packed into an equivalence class and mapping a
chart into a forest in the same way as parsing. Sec-
ond, we reduced the search space in runtime by
adopting iterative beam search (Tsuruoka and Tsu-
jii, 2004) that efficiently pruned improbable candi-
dates. We evaluated the generator on the Penn Tree-
bank (Marcus et al, 1993), which is highly reliable
corpus consisting of real-world texts.
Through a series of experiments, we compared
the performance of several disambiguation mod-
els following an existing study (Velldal and Oepen,
2005) and examined how the performance changed
according to the size of training data, the feature set,
and the beam width. Comparing the latter half of the
experimental results with those on parsing (Miyao
and Tsujii, 2005), we investigated similarities and
differences between probabilistic models for parsing
and generation. The results indicated that the tech-
niques exported from parsing to generation worked
well while the effects were slightly different in de-
tail.
The Nitrogen system (Langkilde and Knight,
1998; Langkilde, 2000) maps semantic relations to a
packed forest containing all realizations and selects
the best one with a bigram model. Our method ex-
tends their approach in that we can utilize syntactic
features in the disambiguation model in addition to
the bigram.
From the perspective of using a lexicalized gram-
mar developed for parsing and importing pars-
ing techniques, our method is similar to the fol-
lowing approaches. The Fergus system (Banga-
lore and Rambow, 2000) uses LTAG (Lexicalized
Tree Adjoining Grammar (Schabes et al, 1988))
for generating a word lattice containing realizations
and selects the best one using a trigram model.
White and Baldridge (2003) developed a chart gen-
erator for CCG (Combinatory Categorial Gram-
mar (Steedman, 2000)) and proposed several tech-
niques for efficient generation such as best-first
search, beam thresholding and chunking the input
logical forms (White, 2004). Although some of the
techniques look effective, the models to rank can-
didates are still limited to simple language mod-
els. Carroll et al (1999) developed a chart gen-
erator using HPSG. After the generator outputs all
the sentences the grammar allows, the ranking mod-
?????? xh eINDEXR EL??
??
??
?
?
??
??
??
?
?
p a s t
y
x
e
bu y
T ENS E
A R G 2
A R G 1
INDEX
R EL
??
?
?
?
??
?
?
?
y
z
t h e
A R G 1
INDEX
R EL ?????? ybookINDEXR EL
Figure 1: PASs for ?He bought the book.?
ule (Velldal and Oepen, 2005) selects the best one
using a log-linear model. Their model is trained us-
ing only 864 sentences where all realizations can be
explicitly enumerated.
As a grammar is extended to support more lin-
guistic phenomena and to achieve higher cover-
age, the number of alternative realizations increases
and the enumeration requires much higher compu-
tational cost. Moreover, using a variety of syntactic
features also increases the cost. By representing a
set of realizations compactly with a packed forest,
we trained the models with rich features on a large
corpus using a wide-coverage grammar.
2 Background
This section describes background of this work in-
cluding the representation of the input to our gener-
ator, the algorithm of chart generation, and proba-
bilistic models for HPSG.
2.1 Predicate-argument structures
The grammar we adopted is the Enju grammar,
which is an English HPSG grammar extracted from
the Penn Treebank by Miyao et al (2004). In
parsing a sentence with the Enju grammar, seman-
tic relations of words is output included in a parse
tree. The semantic relations are represented by a
set of predicate-argument structures (PASs), which
in turn becomes the input to our generator. Figure
1 shows an example input to our generator which
corresponds to the sentence ?He bought the book.?,
which consists of four predicates. REL expresses
the base form of the word corresponding to the pred-
icate. INDEX expresses a semantic variable to iden-
tify each word in the set of relations. ARG1 and
ARG2 express relationships between the predicate
and its arguments, e.g., the circled part in Figure 1
shows ?he? is the subject of ?buy? in this example.
The other constraints in the parse tree are omitted
in the input for the generator. Since PASs abstract
94
away superficial differences, generation from a set
of PASs contains ambiguities in the order of modi-
fiers like the example in Section 1 or the syntactic
categories of phrases. For example, the PASs in Fig-
ure 1 can generate the NP, ?the book he bought.?
When processing the input PASs, we split a single
PAS into a set of relations like (1) representing the
first PAS in Figure 1.
 
	
	Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 788?796,
Beijing, August 2010
Entity-Focused Sentence Simplification for Relation Extraction
Makoto Miwa1 Rune S?tre1 Yusuke Miyao2 Jun?ichi Tsujii1,3,4
1Department of Computer Science, The University of Tokyo
2National Institute of Informatics
3School of Computer Science, University of Manchester
4National Center for Text Mining
mmiwa@is.s.u-tokyo.ac.jp,rune.saetre@is.s.u-tokyo.ac.jp,
yusuke@nii.ac.jp,tsujii@is.s.u-tokyo.ac.jp
Abstract
Relations between entities in text have
been widely researched in the natu-
ral language processing and information-
extraction communities. The region con-
necting a pair of entities (in a parsed
sentence) is often used to construct ker-
nels or feature vectors that can recognize
and extract interesting relations. Such re-
gions are useful, but they can also incor-
porate unnecessary distracting informa-
tion. In this paper, we propose a rule-
based method to remove the information
that is unnecessary for relation extraction.
Protein?protein interaction (PPI) is used
as an example relation extraction problem.
A dozen simple rules are defined on out-
put from a deep parser. Each rule specif-
ically examines the entities in one target
interaction pair. These simple rules were
tested using several PPI corpora. The PPI
extraction performance was improved on
all the PPI corpora.
1 Introduction
Relation extraction (RE) is the task of finding a
relevant semantic relation between two given tar-
get entities in a sentence (Sarawagi, 2008). Some
example relation types are person?organization
relations (Doddington et al, 2004), protein?
protein interactions (PPI), and disease?gene as-
sociations (DGA) (Chun et al, 2006). Among
the possible RE tasks, we chose the PPI extrac-
tion problem. PPI extraction is a major RE task;
around 10 corpora have been published for train-
ing and evaluation of PPI extraction systems.
Recently, machine-learning methods, boosted
by NLP techniques, have proved to be effec-
tive for RE. These methods are usually intended
to highlight or select the relation-related regions
in parsed sentences using feature vectors or ker-
nels. The shortest paths between a pair of enti-
ties (Bunescu and Mooney, 2005) or pair-enclosed
trees (Zhang et al, 2006) are widely used as focus
regions. These regions are useful, but they can in-
clude unnecessary sub-paths such as appositions,
which cause noisy features.
In this paper, we propose a method to remove
information that is deemed unnecessary for RE.
Instead of selecting the whole region between
a target pair, the target sentence is simplified
into simpler, pair-related, sentences using general,
task-independent, rules. By addressing particu-
larly the target entities, the rules do not affect im-
portant relation-related expressions between the
target entities. We show how rules of two groups
can be easily defined using the analytical capabil-
ity of a deep parser with specific examination of
the target entities. Rules of the first group can re-
place a sentence with a simpler sentence, still in-
cluding the two target entities. The other group of
rules can replace a large region (phrase) represent-
ing one target entity, with just a simple mention of
that target entity. With only a dozen simple rules,
we show that we can solve several simple well-
known problems in RE, and that we can improve
the performance of RE on all corpora in our PPI
test-set.
788
2 Related Works
The general paths, such as the shortest path or
pair-enclosed trees (Section 1), can only cover
a part of the necessary information for relation
extraction. Recent machine-learning methods
specifically examine how to extract the missing
information without adding too much noise. To
find more representative regions, some informa-
tion from outside the original regions must be
included. Several tree kernels have been pro-
posed to extract such regions from the parse
structure (Zhang et al, 2006). Also the graph
kernel method emphasizes internal paths with-
out ignoring outside information (Airola et al,
2008). Composite kernels have been used to com-
bine original information with outside informa-
tion (Zhang et al, 2006; Miwa et al, 2009).
The approaches described above are useful,
but they can include unnecessary information that
distracts learning. Jonnalagadda and Gonzalez
(2009) applied bioSimplify to relation extraction.
BioSimplify is developed to improve their link
grammar parser by simplifying the target sentence
in a general manner, so their method might re-
move important information for a given target re-
lation. For example, they might accidentally sim-
plify a noun phrase that is needed to extract the
relation. Still, they improved overall PPI extrac-
tion recall using such simplifications.
To remove unnecessary information from a sen-
tence, some works have addressed sentence sim-
plification by iteratively removing unnecessary
phrases. Most of this work is not task-specific;
it is intended to compress all information in a tar-
get sentence into a few words (Dorr et al, 2003;
Vanderwende et al, 2007). Among them, Vickrey
and Koller (2008) applied sentence simplification
to semantic role labeling. With retaining all argu-
ments of a verb, Vickrey simplified the sentence
by removing some information outside of the verb
and arguments.
3 Entity-Focused Sentence
Simplification
We simplify a target sentence using simple rules
applicable to the output of a deep parser called
Mogura (Matsuzaki et al, 2007), to remove noisy
information for relation extraction. Our method
relies on the deep parser; the rules depend on the
Head-driven Phrase Structure Grammar (HPSG)
used by Mogura, and all the rules are written for
the parser Enju XML output format. The deep
parser can produce deep syntactic and semantic
information, so we can define generally applica-
ble comprehensive rules on HPSG with specific
examination of the entities.
For sentence simplification in relation extrac-
tion, the meaning of the target sentence itself is
less important than maintaining the truth-value of
the relation (interact or not). For that purpose,
we define rules of two groups: clause-selection
rules and entity-phrase rules. A clause-selection
rule constructs a simpler sentence (still includ-
ing both target entities) by removing noisy infor-
mation before and after the relevant clause. An
entity-phrase rule simplifies an entity-containing
region without changing the truth-value of the re-
lation. By addressing the target entities particu-
larly, we can define rules for many applications,
and we can simplify target sentences with less
danger of losing relation-related mentions. The
rules are summarized in Table 1.
Our method is different from the sentence sim-
plification in other systems (ref. Section 2). First,
our method relies on the parser, while bioSimplify
by Jonnalagadda and Gonzalez (2009) is devel-
oped for the improvement of their parser. Second,
our method tries to keep only the relation-related
regions, unlike other general systems including
bioSimplify which tried to keep all information in
a sentence. Third, our entity-phrase rules modify
only the entity-containing phrases, while Vickrey
and Koller (2008) tries to remove all information
outside of the target verb and arguments.
3.1 Clause-selection Rules
In compound or complex sentences, it is natural
to assume that one clause includes both the target
entities and the relation-related information. It can
also be assumed that the remaining sentence parts,
outside the clause, contain less related (or noisy)
information. The clause-selection rules simplify a
sentence by retaining only the clause that includes
the target entities (and by discarding the remain-
der of the sentence). We define three types of
789
Rule Group Rule Type # Example (original? simplified )
Sentence Clause 1 We show that A interacts with B.? A interacts with B.
Clause Selection Relative Clause 2 ... A that interacts with B.? A interacts with B.
Copula 1 A is a protein that interacts with B.? A interacts with B.
Apposition 2 a protein, A? A
Entity Phrase Exemplification 4 proteins, such as A? AParentheses 2 a protein (A)? A
Coordination 3 protein and A? A
Table 1: Rules for Sentence Simplification. (# is the rule count. A and B are the target entities.)
(a) S
bbbbbbb \\\\\\\
... VP
bbbbbbb \\\\\\\
N*
ccccc [[[[[
Vcc
77
(copular) ...
bbbbbbb \\\\\\\
... ENTITY ... N* S-REL
bbbbbbb \\\\\\\
NP-REL
NN
...
ccccc [[[[[
... ENTITY ...
A is a protein that interacts with B .
(b) S
bbbbbbb \\\\\\\
N*
ccccc [[[[[
...
ccccc [[[[[
... ENTITY ... ... ENTITY ...
A interacts with B .
Figure 1: Copula Rule. (a) is simplified to (b).
The arrows represent predicate?argument rela-
tions.
(a) N*
bbbbbbb \\\\\\\
N* ...
bbbbbbb ]]]]]]]]]]]]]
PN
RR
55(apposition) N*
ccccc [[[[[
... ENTITY ...
protein , A
(b) N*
ccccc [[[[[
... ENTITY ...
A
Figure 2: Apposition Rule.
clause-selection rules for sentence clauses, rela-
tive clauses, and copula. The sentence clause rule
finds the (smallest) clause that includes both tar-
get entities. It then replaces the original sentence
with the clause. The relative clause rules con-
struct a simple sentence from a relative clause and
the antecedent. If this simple sentence includes
the target entities, it is used instead of the orig-
inal sentence. We define two rules for the case
where the antecedent is the subject of the relative
clause. One rule is used when the relative clause
includes both the target entities. The other rule is
used when the antecedent includes one target en-
tity and the relative clause includes the other tar-
get entity. The copula rule is for sentences that
include copular verbs (e.g. be, is, become, etc).
The rule constructs a simple sentence from a rel-
ative clause with the subject of the copular verb
as the antecedent subject of the clause. The rule
replaces the target sentence with the constructed
sentence, if the relative clause includes one target
entity and the subject of a copular verb includes
the other target entity, as shown in Figure 1.
3.2 Entity-phrase Rules
Even the simple clauses (or paths between two
target entities) include redundant or noisy expres-
sions that can distract relation extraction. Some
of these expressions are related to the target enti-
ties, but because they do not affect the truth-value
of the relation, they can be deleted to make the
path simple and clear. The target problem affects
which expressions can be removed. We define
four types of rules for appositions, exemplifica-
tions, parentheses, and coordinations. Two appo-
sition rules are defined to select the correct ele-
ment from an appositional expression. One ele-
ment modifies or defines the other element in ap-
position, but the two elements represent the same
information from the viewpoint of PPI. If the tar-
get entity is in one of these elements, removing the
other element does not affect the truth-value of the
interaction. Many of these apposition expressions
are identified by the deep parser. The rule to se-
lect the last element is presented in Figure 2. Four
exemplification rules are defined for the two ma-
jor types of expressions using the phrases ?includ-
ing? or ?such as?. Exemplification is represented
by hyponymy or hypernymy. As for appositions,
the truth-value of the interaction does not change
whether we use the specific mention or the hyper-
class that the mention represents. Two parenthe-
ses rules are defined. Parentheses are useful for
synonyms, hyponyms, or hypernyms (ref. the two
790
1: S ? input sentence
2: repeat
3: reset rules {apply all the rules again}
4: P ? parse S
5: repeat
6: r ? next rule {null if no more rules}
7: if r is applicable to P then
8: P ? apply r to P
9: S ? sentence extracted from P
10: break (Goto 3)
11: end if
12: until r is null
13: until r is null
14: return S
Figure 3: Pseudo-code for sentence simplifica-
tion.
former rules). Three coordination rules are de-
fined. Removing other phrases from coordinated
expressions that include a target entity does not
affect the truth-value of the target relation. Two
rules are defined for simple coordination between
two phrases (e.g. select left or right phrase), and
one rule is defined to (recursively) remove one
element from lists of more than two coordinated
phrases (while maintaining the coordinating con-
junction, e.g. ?and?).
3.3 Sentence Simplification
To simplify a sentence, we apply rules repeatedly
until no more applications are possible as pre-
sented in Figure 3. After one application of one
rule, the simplified sentence is re-parsed before
attempting to apply all the rules again. This is be-
cause we require a consistent parse tree as a start-
ing point for additional applications of the rules,
and because a parser can produce more reliable
output for a partly simplified sentence than for the
original sentence. Using this method, we can also
backtrack and seek out conversion errors by exam-
ining the cascade of partly simplified sentences.
4 Evaluation
To elucidate the effect of the sentence simplifi-
cation, we applied the rules to five PPI corpora
and evaluated the PPI extraction performance. We
then analyzed the errors. The evaluation settings
will be explained in Section 4.1. The results of the
PPI extraction will be explained in Section 4.2. Fi-
nally, the deeper analysis results will be presented
in Section 4.3.
4.1 Experimental Settings
The state-of-the-art PPI extraction system
AkaneRE by Miwa et al (2009) was used to
evaluate our approach. The system uses a com-
bination of three feature vectors: bag-of-words
(BOW), shortest path (SP), and graph features.
Classification models are trained with a support
vector machine (SVM), and AkaneRE (with
Mogura) is used with default parameter settings.
The following two systems are used for a state-
of-the-art comparison: AkaneRE with multiple
parsers and corpora (Miwa et al, 2009), and
Airola et al (2008) single-parser, single-corpus
system.
The rules were evaluated on the BioIn-
fer (Pyysalo et al, 2007), AIMed (Bunescu et al,
2005), IEPA (Ding et al, 2002), HPRD50 (Fun-
del et al, 2006), and LLL (Ne?dellec, 2005) cor-
pora1. Table 2 shows the number of positive (in-
teracting) vs. all pairs. One duplicated abstract in
the AIMed corpus was removed.
These corpora have several differences in their
definition of entities and relations (Pyysalo et al,
2008). In fact, BioInfer and AIMed target al oc-
curring entities related to the corpora (proteins,
genes, etc). On the other hand, IEPA, HPRD50,
and LLL only use limited named entities, based
either on a list of entity names or on a named en-
tity recognizer. Only BioInfer is annotated for
other event types in addition to PPI, including
static relations such as protein family member-
ship. The sentence lengths are also different. The
duplicated pair-containing sentences contain the
following numbers of words on average: 35.8 in
BioInfer, 31.3 in AIMed, 31.8 in IEPA, 26.5 in
HPRD50, and 33.4 in LLL.
For BioInfer, AIMed, and IEPA, each corpus is
split into training, development, and test datasets2.
The training dataset from AIMed was the only
training dataset used for validating the rules. The
development datasets are used for error analysis.
The evaluation was done on the test dataset, with
models trained using training and development
1http://mars.cs.utu.fi/PPICorpora/
GraphKernel.html
2This split method will be made public later.
791
BioInfer AIMed IEPA HPRD50 LLL
pos all pos all pos all pos all pos all
training 1,848 7,108 684 4,072 256 630 - - - -
development 256 928 102 608 23 51 - - - -
test 425 1,618 194 1,095 56 136 - - - -
all 2,534 9,653 980 5,775 335 817 163 433 164 330
Table 2: Number of positive (pos) vs. all possible sentence pairs in used PPI corpora.
BioInfer AIMed IEPA
Rule Applied F AUC Applied F AUC Applied F AUC
No Application 0 62.5 83.0 0 61.2 87.9 0 73.4 82.5
Clause Selection 4,313 63.5 83.9 2,569 62.5 88.2 307 75.0 83.7
Entity Phrase 22,066 60.5 80.9 7,784 61.2 86.1 1,031 72.7 83.3
ALL 26,281 62.9 82.1 10,783 60.2 85.7 1,343 75.4 85.7
Table 3: Performance of PPI Extraction on test datasets. ?Applied? represents the number of times the
rules are applied on the corpus. ?No Application? means PPI extraction without sentence simplification.
ALL is the case all rules are used. The top scores for each corpus are shown in bold.
datasets). Ten-fold cross-validation (CV) was
done to facilitate comparison with other existing
systems. For HPRD50 and LLL, there are insuf-
ficient examples to split the data, so we use these
corpora only for comparing the scores and statis-
tics. We split the corpora for the CV, and mea-
sured the F -score (%) and area under the receiver
operating characteristic (ROC) curve (AUC) as
recommended in (Airola et al, 2008). We count
each occurrence as one example because the cor-
rect interactions must be extracted for each occur-
rence if the same protein name occurs multiple
times in a sentence.
In the experiments, the rules are applied in the
following order: sentence?clause, exemplifica-
tion, apposition, parentheses, coordination, cop-
ula, and relative-clause rules. Furthermore, if the
same rule is applicable in different parts of the
parse tree, then the rule is first applied closest to
the leaf-nodes (deepest first). The order of the
rules is arbitrary; changing it does not affect the
results much. We conducted five experiments us-
ing the training and development dataset in IEPA,
each time with a random shuffling of the order of
the rules; the results were 77.8?0.26 in F -score
and 85.9?0.55 in AUC.
4.2 Performance of PPI Extraction
The performance after rule application was bet-
ter than the baseline (no application) on all the
corpora, and most rules could be frequently ap-
plied. We show the PPI extraction performance on
Rule Applied F AUC
No Application 0 72.9 84.5
Sentence Clause 145 71.6 83.8
Relative Clause 7 73.3 84.1
Copula 0 72.9 84.5
Clause Selection 152 71.4 83.4
Apposition 64 73.2 84.6
Exemplification 33 72.9 84.7
Parentheses 90 72.9 85.1
Coordination 417 73.6 85.4
Entity Phrase 605 74.1 86.6
ALL 763 75.0 86.6
Table 4: Performance of PPI Extraction on
HPRD50.
Rule Applied F AUC
No Application 0 79.0 84.6
Sentence Clause 135 81.3 85.2
Relative Clause 42 78.8 84.6
Copula 0 79.0 84.6
Clause Selection 178 81.0 85.6
Apposition 197 79.6 83.9
Exemplification 0 79.0 84.6
Parentheses 56 79.5 85.8
Coordination 322 84.2 89.4
Entity Phrase 602 83.8 90.1
ALL 761 82.9 90.5
Table 5: Performance of PPI Extraction on LLL.
BioInfer, AIMed, and IEPA with rules of different
groups in Table 3. The effect of using rules of
different types for PPI extraction from HPRD50
and LLL is reported in Table 4 and Table 5. Ta-
ble 6 shows the number of times each rule was
applied in an ?apply all-rules? experiment. The
usability of the rules depends on the corpus, and
different combinations of rules produce different
792
Rule B AIMed IEPA H LLL
S. Cl. 3,960 2,346 300 150 135
R. Cl. 287 212 17 5 24
Copula 60 57 1 0 0
Cl. Sel. 4,307 2,615 318 155 159
Appos. 3,845 1,100 99 69 198
Exempl. 383 127 11 33 0
Paren. 2,721 2,158 235 91 88
Coord. 15,025 4,783 680 415 316
E. Foc. 21,974 8,168 1,025 608 602
Sum 26,281 10,783 1,343 763 761
Table 6: Distribution of the number of rules ap-
plied when all rules are applied. B:BioInfer, and
H:HPRD50 corpora.
Rules Miwa et al Airola et al
F AUC F AUC F AUC
B 60.0 79.8 68.3 86.4 61.3 81.9
A 54.9 83.7 65.2 89.3 56.4 84.8
I 77.8 88.7 76.6 87.8 75.1 85.1
H 75.0 86.6 74.9 87.9 63.4 79.7
L 82.9 90.5 86.7 90.8 76.8 83.4
Table 7: Comparison with the results by Miwa et
al. (2009) and Airola et al (2008). The results
with all rules are reported.
results. For the clause-selection rules, the per-
formance was as good as or better than the base-
line for all corpora, except for HPRD50, which
indicates that the pair-containing clauses also in-
clude most of the important information for PPI
extraction. Clause selection rules alone could im-
prove the overall performance for the BioInfer and
AIMed corpora. Entity-phrase rules greatly im-
proved the performance on the IEPA, HPRD50,
and LLL corpora, although these rules degraded
the performance on the BioInfer and AIMed cor-
pora. These phenomena hold even if we use small
parts of the two corpora, so this is not because of
the size of the corpora.
We compare our results with the results by
Miwa et al (2009) and Airola et al (2008) in Ta-
ble 7. On three of five corpora, our method pro-
vides better results than the state-of-the-art system
by Airola et al (2008), and also provides com-
parable results to those obtained using multiple
parsers and corpora (Miwa et al, 2009) despite
the fact that our method uses one parser and one
corpus at a time. We cannot directly compare our
result with Jonnalagadda and Gonzalez (2009) be-
cause the evaluation scheme, the baseline system,
[FP?TN][Sentence, Parenthesis, Coordination] To
characterize the AAV functions mediating this effect,
cloned AAV type 2 wild-type or mutant genomes were
transfected into simian virus 40 (SV40)-transformed
hamster cells together with the six HSV replication genes
(encoding UL5, UL8, major DNA-binding protein, DNA
polymerase, UL42 , and UL52) which together are
necessary and sufficient for the induction of SV40 DNA
amplification (R. Heilbronn and H. zur Hausen, J. Virol.
63:3683-3692, 1989). (BioInfer.d760.s0)
[TP?FN][Coordination] Both the GT155-calnexin and
the GT155-CAP-60 interactions were dependent on the
presence of a correctly modified oligosaccharide group
on GT155, a characteristic of many calnexin interactions.
(AIMed.d167.s1408)
[TN?TN][Coordination, Parenthesis] Leptin may act as
a negative feedback signal to the hypothalamic control of
appetite through suppression of neuropeptide Y (NPY)
secretion and stimulation of cocaine and amphetamine
regulated transcript (CART) . (IEPA.d190.s454)
Figure 4: A rule-related error, a critical error, and
a parser-related error. Regions removed by the
rules are underlined, and target proteins are shown
in bold. Predictions, applied rules, and sentence
IDs are shown.
[FN?TP][Sentence, Coordination] WASp contains a
binding motif for the Rho GTPase CDC42Hs as well as
verprolin / cofilin-like actin-regulatory domains , but no
specific actin structure regulated by CDC42Hs-WASp has
been identified. (BioInfer.d795.s0)
[FN?TP][Parenthesis, Apposition] The protein Raf-1 , a
key mediator of mitogenesis and differentiation, associates
with p21ras (refs 1-3) . (AIMed.d124.s1055)
[FN?TP][Sentence, Parenthesis] On the basis of
far-Western blot and plasmon resonance (BIAcore)
experiments, we show here that recombinant bovine
prion protein (bPrP) (25-242) strongly interacts with the
catalytic alpha/alpha? subunits of protein kinase CK2
(also termed ?casein kinase 2?) (IEPA.d197.s479)
Figure 5: Correctly simplified cases. The first
sentence is a difficult (not PPI) relation, which is
typed as ?Similar? in the BioInfer corpus.
and test parts differ.
4.3 Analysis
We trained models using the training datasets
and classified the examples in the development
datasets. Two types of analysis were performed
based on these results: simplification-based and
classification-based analysis.
For the simplification-based analysis, we com-
pared positive (interacting) and negative pair sen-
tences that produce the exact same (inconsistent)
sentence after protein names normalization and
793
BioInfer AIMed IEPA
Before simplification FN FP TP TN FN FP TP TN FN FP TP TN Not AffectedAfter simplification TP TN FN FP TP TN FN FP TP TN FN FP
No Error 18 2 3 35 14 21 21 8 3 2 0 4 32
No Application 3 2 0 3 0 7 8 0 0 1 0 1 7
Number of Errors 0 2 0 32 4 2 1 4 0 0 0 0 1
Number of Pairs 21 6 3 70 18 30 30 12 3 3 0 5 40
Coordination 0 0 0 20 4 2 1 0 0 0 0 0 1
Sentence 0 2 0 4 0 0 0 4 0 0 0 0 0
Parenthesis 0 0 0 5 0 0 0 0 0 0 0 0 0
Exemplification 0 0 0 2 0 0 0 0 0 0 0 0 0
Apposition 0 0 0 1 0 0 0 0 0 0 0 0 0
Table 8: Distribution of sentence simplification errors compared to unsimplified predictions with their
types (on the three development datasets). TP, True Positive; TN, True Negative; FN, False Negative;
FP, False Positive. ?No Error? means that simplification was correct; ?No Application? means that no
rule could be applied; Other rule names mean that an error resulted from that rule application. ?Not
Affected? means that the prediction outcome did not change.
simplification in the training dataset. The numbers
of such inconsistent sentences are 7 for BioIn-
fer, 78 for AIMed, and 1 for IEPA. The few in-
consistencies in BioInfer and IEPA are from er-
rors by the rules, mainly triggered by parse errors.
The frequent inconsistencies in AIMed are mostly
from inconsistent annotations. For example, even
if all coordinated proteins are either interacting or
not, only the first protein mention is annotated as
interacting.
For the classification-based analysis, we
specifically examine simplified pairs that were
predicted differently before and after the simplifi-
cation. Pairs predicted differently before and after
rule application were selected: 100 random pairs
from BioInfer and all 90 pairs from AIMed. For
IEPA, all 51 pairs are reported. Simplified results
are classified as errors when the rules affect a re-
gion unrelated to the entities in the smallest sen-
tence clause. The results of analysis are shown in
Table 8. There were 34 errors in BioInfer, and 11
errors in AIMed. Among the errors, there were
five critical errors (in two sentences, in AIMed).
Critical errors mean that the pairs lost relation-
related mentions, and the errors are the only er-
rors which caused the changes in the truth-value
of the relation. There was also a rule-related er-
ror (in BioInfer), which means that rules with cor-
rect parse results affect a region unrelated to the
entities, and parse errors (parser-related errors).
Figure 4 shows the rule-related error in BioInfer,
one critical error in AIMed, and one parser-related
error in IEPA.
5 Discussion
Our end goal is to provide consistent relation
extraction for real tasks. Here we discuss the
?safety? of applying our simplification rules, the
difficulties in the BioInfer and AIMed corpora, the
reduction of errors, and the requirements for such
a general (PPI) extraction system.
Our rules are applicable to sentences, with little
danger of changing the relation-related mentions.
Figure 5 shows three successfully simplified cases
(?No Error? cases from Table 8). The sentence
simplification leaves sufficient information to de-
termine the value of the relation in these exam-
ples. Relation-related mentions remained for most
of the simplification error cases. There were only
five critical errors, which changed the truth-value
of the relation, out of 46 errors in 241 pairs shown
in Table 8. Please note that some rules can be
dangerous for other relation extraction tasks. For
example, the sentence clause rule could remove
modality information (negation, speculation, etc.)
modifying the clause, but there are few such cases
in the PPI corpora (see Table 8). Also, the task of
hedge detection (Morante and Daelemans, 2009)
can be solved separately, in the original sentences,
after the interacting pairs have been found. For
example, in the BioNLP shared task challenge
and the BioInfer corpus, interaction detection and
modality are treated as two different tasks. Once
other NLP tasks, like static relation (Pyysalo et
794
al., 2009) or coreference resolution, become good
enough, they can supplement or even substitute
some of the proposed rules.
There are different difficulties in the BioInfer
and AIMed corpora. BioInfer includes more com-
plicated sentences and problems than the other
corpora do, because 1) the apposition, coordi-
nation, and exemplification rules are more fre-
quently used in the BioInfer corpus than in the
other corpora (shown in Table 6), 2) there were
more errors in the BioInfer corpus than in other
corpora among the simplified sentences (shown
in Table 8), and 3) BioInfer has more words per
sentence and more relation types than the other
corpora. AIMed contains several annotation in-
consistencies as explained in Section 4.3. These
inconsistencies must be removed to properly eval-
uate the effect of our method.
Simplification errors are mostly caused by
parse errors. Our rule specifically examines a part
of parser output; a probability is attached to the
part. The probability is useful for defining the or-
der of rule applications, and the n-best results by
the parser are useful to fix major errors such as co-
ordination errors. By using these modifications of
rule applications and by continuous improvement
in parsing technology for the biomedical domain,
the performance on the BioInfer and AIMed cor-
pora will be improved also for the all rules case.
The PPI extraction system lost the ability to
capture some of the relation-related expressions
left by the simplification rules. This indicates
that the system used to extract some relations (be-
fore simplification) by using back-off features like
bag-of-words. The system can reduce bad effects
caused by parse errors, but it also captures the an-
notation inconsistencies in AIMed. Our simpli-
fication (without errors) can capture more general
expressions needed for relation extraction. To pro-
vide consistent PPI relation extraction in a general
setting (e.g. for multiple corpora or for other pub-
lic text collections), the parse errors must be dealt
with, and a relation extraction system that can cap-
ture (only) general relation-related expressions is
needed.
6 Conclusion
We proposed a method to simplify sentences, par-
ticularly addressing the target entities for relation
extraction. Using a few simple rules applicable
to the output of a deep parser called Mogura,
we showed that sentence simplification is effec-
tive for relation extraction. Applying all the rules
improved the performance on three of the five
corpora, while applying only the clause-selection
rules raised the performance for the remaining two
corpora as well. We analyzed the simplification
results, and showed that the simple rules are ap-
plicable with little danger of changing the truth-
values of the interactions.
The main contributions of this paper are: 1) ex-
planation of general sentence simplification rules
using HPSG for relation extraction, 2) presenting
evidence that application of the rules improve re-
lation extraction performance, and 3) presentation
of an error analysis from two viewpoints: simpli-
fication and classification results.
As future work, we are planning to refine and
complete the current set of rules, and to cover
the shortcomings of the deep parser. Using these
rules, we can then make better use of the parser?s
capabilities. We will also attempt to apply our
simplification rules to other relation extraction
problems than those of PPI.
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan), Genome Network Project (MEXT, Japan),
and Scientific Research (C) (General) (MEXT,
Japan).
795
References
Airola, Antti, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
A graph kernel for protein-protein interaction ex-
traction. In Proceedings of the BioNLP 2008 work-
shop.
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In HLT ?05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
724?731.
Bunescu, Razvan C., Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine, 33(2):139?155.
Chun, Hong-Woo, Yoshimasa Tsuruoka, Jin-Dong
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,
and Jun?ichi Tsujii. 2006. Extraction of gene-
disease relations from medline using domain dictio-
naries and machine learning. In The Pacific Sympo-
sium on Biocomputing (PSB), pages 4?15.
Ding, J., D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) program: Tasks, data, and evalua-
tion. In Proceedings of LREC?04, pages 837?840.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In in Proceedings of Work-
shop on Automatic Summarization, pages 1?8.
Fundel, Katrin, Robert Ku?ffner, and Ralf Zimmer.
2006. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Jonnalagadda, Siddhartha and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of the 3rd
International Symposium on Languages in Biology
and Medicine, pages 109?114, November.
Matsuzaki, Takuya, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertag-
ging and cfg-filtering. In IJCAI?07: Proceedings of
the 20th international joint conference on Artifical
intelligence, pages 1671?1676, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Miwa, Makoto, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009. Protein-protein interac-
tion extraction by leveraging multiple kernels and
parsers. International Journal of Medical Informat-
ics, June.
Morante, Roser and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Ne?dellec, Claire. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL?05 Workshop.
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Pyysalo, Sampo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein inter-
action corpora. In BMC Bioinformatics, volume
9(Suppl 3), page S6.
Pyysalo, Sampo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece
in the biomedical information extraction puzzle.
In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 1?9, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Sarawagi, Sunita. 2008. Information extraction.
Foundations and Trends in Databases, 1(3):261?
377.
Vanderwende, Lucy, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplifica-
tion and lexical expansion. Inf. Process. Manage.,
43(6):1606?1618.
Vickrey, David and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL-08: HLT, pages 344?352, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 825?832. Association for
Computational Linguistics.
796
Coling 2010: Poster Volume, pages 1417?1425,
Beijing, August 2010
Semi-automatically Developing Chinese HPSG Grammar from the Penn Chinese Treebank for Deep Parsing 
Kun Yu1 Yusuke Miyao2 Xiangli Wang1 Takuya Matsuzaki1 Junichi Tsujii1,3 1. The University of Tokyo 2. National Institute of Informatics yusuke@nii.ac.jp {kunyu, xiangli, matuzaki, tsujii} @is.s.u-tokyo.ac.jp 3. The University of Manchester  Abstract In this paper, we introduce our recent work on Chinese HPSG grammar development through treebank conversion. By manually defining grammatical constraints and anno-tation rules, we convert the bracketing trees in the Penn Chinese Treebank (CTB) to be an HPSG treebank. Then, a large-scale lexi-con is automatically extracted from the HPSG treebank. Experimental results on the CTB 6.0 show that a HPSG lexicon was successfully extracted with 97.24% accu-racy; furthermore, the obtained lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. 1 Introduction Precise, in-depth syntactic and semantic analysis has become important in many NLP applications. Deep parsing provides a way of simultaneously obtaining both the semantic relation and syntac-tic structure. Thus, the method has become more popular among researchers recently (Miyao and Tsujii, 2006; Matsuzaki et al, 2007; Clark and Curran, 2004; Kaplan et al, 2004).  This paper introduces our recent work on deep parsing for Chinese, specifically focusing on the development of a large-scale grammar, based on the HPSG theory (Pollard and Sag, 1994). Be-cause it takes a decade to manually develop an HPSG grammar that achieves sufficient coverage for real-world text, we use a semi-automatic ap-proach, which has successfully been pursued for English (Miyao, 2006; Miyao et al, 2005; Xia, 1999; Hockenmaier and Steedman, 2002; Chen and Shanker, 2000; Chiang, 2000) and other lan-guages (Guo et al, 2007; Cramer and Zhang, 2009; Hockenmaier, 2006; Rehbei and Genabith, 2009; Schluter and Genabith, 2009).  The following lists our method of approach: (1) define a skeleton of the grammar (in this 
work, the structure of sign, grammatical princi-ples and schemas), (2) convert the CTB (Xue et al, 2002) into an HPSG-style treebank, (3) automatically extract a large-scale lexicon from the obtained treebank. Experiments were performed to evaluate the quality of the grammar developed from the CTB 6.0. More than 95% of the sentences in the CTB could be successfully converted, and the ex-tracted lexicon was 97.24% accurate. The ex-tracted lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. Since grammar engineering has many specific problems in each language, although we used the similar method applied in other languages to de-velop a Chinese HPSG grammar, it is very dif-ferent from applying, such as statistical parsing models, to a new language. Lots of efforts have been done for the specific characteristics of Chi-nese. The contribution of our work is to describe these issues. As a result, a skeleton design of Chinese HPSG is proposed, and for the first time, a robust and wide-coverage Chinese HPSG grammar is developed from real-world text.  2 Design of Grammatical Constraints for Chinese HPSG Because of the lack of a comprehensive HPSG-based syntactic theory for Chinese, we extended the original HPSG (Pollard and Sag, 1994) to analyze the specific linguistic phenomena in Chinese. Due to space limitations, we will pro-vide a brief sampling of our extensions, and dis-cuss several selected constructions.  2.1 Sign, Principles, and Schemas Sign, which is a data structure to express gram-matical constraints of words/phrases, is modified and extended for the analysis of Chinese specific constructions, as shown in Figure 1. PHON, MOD, SPEC, SUBJ, MARKING, and SLASH are 
1417
features defined in the original HPSG, and they represent the phonological information of a word, the constraints on the modifiee, the speci-fiee, the subject, the marker, and the long-distance dependency, respectively. COMPS, which represents the constraints on comple-ments, is divided into LCOMPS and RCOMPS, to distinguish between left and right comple-ments. Aspect, question, and negation particles are treated as markers as done in (Gao, 2000), which are distinguished by ASPECT, QUESTION, and NEGATION. CONT is also originated from Pollard and Sag (1994), although it is used to represent semantic structures with predicate-argument dependencies. TOPIC and CONJ are extended features that represent the constraints on the topic and the conjuncts of co-ordination. FILLER is another extended feature that records the grammatical function of the moved argument in a long-distance dependency. 
 Figure 1. HPSG sign for Chinese. The principles, including Phonology Princi-ple, Valence Principle, Head Feature Principle, and Nonlocal Feature Principle, are imple-mented in our Chinese HPSG grammar as de-fined in (Pollard and Sag, 1994). Semantic Principle is slightly modified so that it composes predicate-argument structures. 14 schemas are defined in our grammar, among which the Coord-Empty-Conj Schema, Relative-Head Schema, Empty-Relativizer Schema, and Topic-Head Schema are designed specifically for Chinese. The other 10 schemas are borrowed from the original HPSG theory. 15 Chinese constructions are considered in our current grammar (refer to Table 1). A detailed description of some particular constructions will be provided in the following subsection.  2.2 An HPSG Analysis for Chinese  2.2.1 BA Construction The BA construction moves the object of a verb to the pre-verbal position. For example, the sen-
tence in Figure 2 with the original word order is ??/I ?/read ? ?/book?. There were three popular ways to address the BA construction: as a verb (Huang, 1991; Bender, 2000), preposition (Gao, 1992), and case marker (Gao, 2000). Since the aspect markers, such as ???, cannot attach to BA, we exclude the analysis of treating BA as a verb. Because BA, like prepositions, always ap-pears before a noun phrase, we therefore follow the analysis in Gao (1992), and treat BA as a preposition. As shown in Figure 2, BA takes a moved object as a complement, and attaches to the verb as a left-complement. 
 (I read the book.) Figure 21. Analysis of BA construction. 2.2.2 BEI Construction The BEI construction is used to make the passive voice of a sentence. Because the aspect marker also cannot attach to BEI, we do not treat BEI as a verb, as done in the CTB. Similar to the analy-sis of BA construction, we regard BEI as a preposition that attaches to the verb as a left-complement. Additionally, because we can insert a clause ???/Li ?/send ?/person? between the moved object ??/he? and the verb ??/beat?, as is the case for ??/he ?/BEI ??/Li ?/send ?/person ?/beat ? (He was beaten by the person that is sent by Li)?, we treat the relation between the moved object and the verb as a long-distance dependency. Figure 3 exem-plifies our analysis of the BEI construction, in which the Filler-Head Schema is used to handle the long-distance dependency, and the FILLER feature is used to record that the role of the moved argument. 
 (The book is read by me.)  Figure 3. Analysis of BEI construction.  2.2.3 Topic Construction As indicated in Li and Thompson (1989), a topic refers to the theme of a sentence, which always                                                            1 In the figures in this paper, we will show only selected features that are relevant to the explanation.  
1418
appears before the subject. The difference be-tween the topic and subject is the subject must always have a direct semantic relationship with the verb in a sentence, whereas the topic does not. There are two types of topic constructions. In the first type, the topic does not fill any argu-ment slots of the verb, such as the topic ???/elephant? in Figure 4. In the second type, the topic has a semantic relationship with the verb. For example, in the sentence ??/he ?/I ??/like (I like him)?, the topic ??/he? is also an object of ???/like?. For the first type, we define the Topic-Head Schema to describe the topic construction (refer to Figure 4). For the second type, we follow the same analysis as in English, and use the Filler-Head Schema.   
 (The nose of an elephant is long.) Figure 4. Analysis of topic construction. 2.2.4 Serial Verb Construction In contrast to the definition of serial verb con-struction in Li and Thompson (1989), we specify a serial verb construction as a special type of verb phrase coordination, which describes sev-eral separate events with no conjunctions inside. Similar to ordinary coordination, the verb phrases in a serial verb construction share the same syntactic subject (Muller and Lipenkova, 2009), topic, and left-complement. We define Coord-Empty-Conj Schema to deal with it. Fig-ure 5 shows an example analysis. 
 (I go to the book store and buy a book.) Figure 5. Analysis of serial verb construction. 2.2.5 Relative Clause In Chinese, a relative clause is marked by a rela-tivizer ??? and exists in the left of the head noun. Because Chinese noun phrases are right-headed in general, we analyze a relative clause as a nominalization that modifies a head noun (Li and Thompson, 1989). Inside of a relative clause, the relativizer is treated as head. When the rela-tivizer is omitted, we define a unary schema, Empty-Relativizer Schema, which functions by combining a relative clause with an empty rela-
tivizer. Furthermore, we introduce a Relative-Head Schema to handle the long-distance de-pendency for the extracted argument2 (refer to Figure 6).  
 (the book that I buy) Figure 6. Analysis of relative clause. 3 Converting the CTB into an HPSG Treebank 3.1 Partially-specified Derivation Tree Annotation In order to convert the CTB into an HPSG tree-bank, we first annotate the bracketing trees in the CTB to be partially-specified derivation trees3, which conform to the grammatical constraints designed in Section 2. Three types of rules are defined to fulfill this annotation. 
 (I read the book that he wrote.) Figure 7. The CTB annotation for a sentence. 
 Figure 8. Partially-specified derivation tree for Figure 7. For example, Figure 7 shows the bracketing tree of a sentence in the CTB, while Figure 8 shows the partially-specified derivation tree after re-annotation.                                                            2 The extracted adjunct is not treated as a long-distance dependency in our current grammar. 3 Partially-specified derivation tree means a tree structure that is annotated with schema names and some features of the HPSG signs (Miyao, 2006). 
1419
3.1.1 Rules for Annotation Conversion  In the CTB, there exist some annotations that do not coincide with our HPSG analysis for Chi-nese. Therefore, we define pattern rules to con-vert the annotations in the CTB to fit with our HPSG analysis. 76 annotation rules are defined for 15 Chinese constructions (refer to Table 2). Due to page constraints, we focus on the constructions that we discussed in Section 2. Construction Rule # Relative clause 20 BEI construction 21 Coordination 7 Subject/object control 5 Non-verbal predicate 4 Logical subject 3 Right node raising 3 Parenthesis 3 BA construction 3 Aspect/question/negation particle 2 Subordination 1 Serial Verb construction 1 Modal verb 1 Topic construction 1 Apposition 1 Table 1. Chinese constructions and annotation rules. Rules for BA and BEI Construction As analyzed in Section 2, we treat BA and BEI as prepositions that attach to the verb as left-complements. However, in the CTB, BA and BEI are annotated as verbs that take a sentential complement (Xue and Xia, 2000). By applying the annotation rules, the BA/BEI and the subject of the sentential complement of BA/BEI are re-annotated as a prepositional phrase (as indicated in the dash-boxed part in Figure 9). 
 (I read the book.) Figure 9. Conversion of BA construction. 
 (He is regarded as a friend by me.) Figure 10. Verb division in BEI construction. In addition, in the CTB, some BA/BEI con-structions are not annotated with trace, which 
makes it difficult to retrieve the semantic relation between the verb and the moved object. The principal reason for this is that the moved object in these constructions has a semantic relation with only part of the verb. For example, in Fig-ure 10, the moved noun ??/he? is the object of ??/regard?, but not for ???/regard as?. Analy-sis shows that only a closed set of characters (e.g. ??/as?)  can be attached to verbs in such a case. Therefore, we manually collect these char-acters from the CTB, and then define pattern rules to automatically split the verb, which ends with the collected characters, in the BA and BEI construction. Finally, we annotate trace for the split verb. Figure 10 exemplifies the conversion of an example sentence. Rules for Topic Construction In the CTB, a functional tag ?TPC? is used to indicate a topic (Xue and Xia, 2000). Therefore, we use this functional tag to detect topic phrases during conversion. Rules for Serial Verb Construction We define pattern rules to detect the parallel verb phrases with no conjunction inside (as shown in Figure 11), and treat these verb phrases as a se-rial verb construction. However, when the verb in the first phrase is a modal verb, such as the case of ??/I ?/want to ??/sing (I want to sing)?, the parallel verb phrases should not be treated as a serial verb construction. Therefore, a list of modal verbs is manually collected from the CTB to filter out these exceptional cases dur-ing conversion.  
 (go downstairs and eat meal) Figure 11. An example of parallel verb phrases. Rules for Relative Clause  
 (the book that he wrote) Figure 12. Conversion of relative clause. We define annotation rules to slightly modify the annotation of a relative clause in CTB, as shown in Figure 12, to make the tree structure easy to be 
1420
analyzed. Furthermore, in CTB, relative clauses are annotated with both extracted arguments and extracted adjuncts. But in our grammar, we only deal with extracted arguments, and the gap in a relative clause (as indicated in the dash-boxed part in Figure 12). When the extracted phrase is an adjunct of the relative clause, we simply view the clause as a modifier of the extracted phrase. 3.1.2 Rules for Correcting Inconsistency  There are some inconsistencies in the annotation of the CTB, which presents difficulties for per-forming the derivation tree annotation. There-fore, we define 49 rules, as done in (Hockenmaier and Steedman, 2002) for English, to mitigate inconsistencies before annotation (re-fer to Table 3).  3.1.3 Rules for Assisting Annotation We also define 48 rules (refer to Table 2), which are similar to the rules used in (Miyao, 2006) for English, to help the derivation tree annotation. For example, 12 pattern rules are defined to as-sign the schemas to corresponding constituents. Rule Type Rule Description Rule # Fix tree annotation 37 Fix phrase tag annotation 5 Fix functional tag annotation 5 Rules for correcting inconsistent annotation Fix POS tag annotation 2 Slash recognization 27 Schema assignment 12 Head/Argument/Modifier marking 8 Rules for assisting  annotation Binarization 1 Table 2. Rules for correcting inconsistency and assisting annotation. 3.2 HPSG Treebank Acquisition In this phase, the schemas and principles are ap-plied to the annotated partially-specified trees, in order to fill out unspecified constraints and vali-date the consistency of the annotated constraints. In effect, an HPSG treebank is obtained. For instance, by applying the Head-Complement Schema to the dash-boxed nodes in Figure 8, the constraints of the right daughter are percolated to RCOMPS of the left daughter (as indicated as 4 in Figure 13). After applying the schemas and the principles to the whole tree in Figure 8, a HPSG derivation tree is acquired (re-fer to Figure 13).  3.3 Lexicon Extraction  With the HPSG treebank acquired in Section 3.2, we automatically collect lexical entries as the combination of words and lexical entry templates from the terminal nodes of the derivation trees. For example, from the HPSG derivation tree 
shown in Figure 13, we obtain a lexical entry for the word ??/write? as shown in Figure 14. 
 Figure 13. HPSG derivation tree for Figure 8. 
 Figure 14. Lexical entry extracted for the word ??/write?. 3.3.1 Lexical Entry Template Expansion 
 (a) Lexical entry template for the verb in BEI construction 
 (b) Lexical entry template for the verb in original word order Figure 15. Application of a lexical rule. Some Chinese constructions change the word order of sentences, such as the BA/BEI construc-tions. Therefore, we apply lexical rules (Naka-nishi et al, 2004) to the lexical entry templates to convert them into those for the original word order, and expand the lexical entry templates consequently. 18 lexical rules are defined for the verbs in the BA/BEI constructions. For example, by applying a lexical rule to the lexical entry template in Figure 15(a), the moved object indi-
1421
cated by SLASH is restored into RCOMPS, and the subject introduced by BEI in LCOMPS is restored into SUBJ (refer to Figure 15(b)). 3.3.2 Mapping of Semantics In our grammar, we use predicate-argument de-pendencies for semantic representation. 44 types of predicate-argument relations are defined to represent the semantic structures of 13 classes of words. For example, we define a predicate-argument relation ?verb_arg12?, in which a verb takes two arguments ?ARG1? and ?ARG2?, to ex-press the semantics of transitive verbs. 72 se-mantics mapping rules are defined to associate these predicate-argument relations with the lexi-cal entry templates. Figure 16 exemplifies a se-mantics mapping rule. The input of this rule is the lexical entry template (as shown in the left part), and the output is a predicate-argument rela-tion ?verb_arg12? (as shown in the right part), which associates the syntactic arguments SUBJ and SLASH with the semantic arguments ARG1 and ARG2 (as indicated by 1 and 2 in Figure 16). 
 Figure 16. A semantics mapping rule. 4 Evaluation 4.1 Experimental Setting We used the CTB 6.0 for HPSG grammar devel-opment and evaluation. We split the corpus into development, testing, and training data sets, fol-lowing the recommendation from the corpus author. The development data was used to tune the design of grammar constraints and the anno-tation rules. However, the testing data set was reserved for further evaluation on parsing. Thus, the training data was further divided into two parts for training and testing in this work. During the evaluation, unknown words were handled in the same way as done in (Hockenmaier and Steedman, 2002).  4.2 Evaluation Metrics In order to verify the quality of the grammar de-veloped in our work, we evaluated the extracted lexicon by the accuracy for assessing the semi-automatic conversion process, and the coverage for quantifying the upper-bound coverage of the future HPSG parser based on this grammar.  The accuracy of the extracted lexicon was evaluated by lexical accuracy, which counts the 
number of the correct lexical entries among all the obtained lexical entries.  In addition, two evaluation metrics as used in (Hockenmaier and Steedman, 2002; Xia, 1999; Miyao, 2006) were used to evaluate the coverage of the obtained lexicon. The first one is lexical coverage (Hockenmaier and Steedman, 2002; Xia, 1999), which means that the percentage that the lexical entries extracted from the testing data are covered by the lexical entries acquired from the training data. The second one is sentential coverage (Miyao, 2006): a sentence is consid-ered to be covered only when the lexical entries of all the words in this sentence are covered.  4.3 Results of Accuracy Since there was no gold standard data for the automatic evaluation of accuracy, we randomly selected 100 sentences from the testing data, and manually checked the lexical entries extracted from these sentences. Results show that 1,558 lexical entries were extracted at 97.24% (1,515/1,558) accuracy.  Error analysis shows all the incorrect lexical entries came from the error in the derivation tree annotation. For example, our current design failed to find the correct boundary of coordinated noun phrases when the word ??/etc? was at-tached at the end, such as ???/property right ??/selling ? ??/assets ??/renting ?/etc (property right selling and assets renting etc.)?. We will improve the derivation tree anno-tation to solve this issue. 4.4 Results of Coverage Table 3 shows the coverage of the extracted lexi-cal entries, which indicates that a large HPSG lexicon was successfully extracted from the CTB for unseen text, with reasonable coverage. The statistics of the HPSG lexicon extraction in our experiments (refer to Table 4) also indicates that we successfully extracted lexical entries from more than 95% of the sentences in the CTB.  Among all the uncovered lexical entries, 78.55% are for content words, such as verb and noun. In addition, the classification of uncovered lexical entries in Table 4 indicates that about 1/3 of the uncovered lexical entries came from the unknown lexical entry templates (?+w/-t?). We analyzed the 193 ?+w/-t? failures in the testing data, among which 169 failures resulted from the shortage of training data, which indicated that the correct lexical entry template did not appear in 
1422
the training data. The learning curve in Figure 17 shows that we can resolve this issue by enlarging the training data. The other 24 failures came from the error in the derivation tree annotation. For example, our current grammar failed at de-tecting the coordinated clauses when they were separated by a colon. We will be able to reduce this type of failure by improving the derivation tree annotation. Uncovered Lexical Entries Sent. Cov. Lex. Cov. +w/+t +w/-t 76.51% 98.51% 1.05% 0.43% Table 34. Coverage of extracted HPSG lexicon. Data Set Total Sent # Succeed Sent # Word # Lexical Entry Template # Training 20,230 19,257(95.19%) 510,815 4,836 Develop 2,067 2,009(97.19%) 55,714 1,582 Testing 2,000 1,941(97.05%) 44,924 1,163 Table 4. Statistics of HPSG lexicon extraction.  
 Figure 17. Lexical coverage (Y axis) vs. corpus size (X axis). 
 Figure 18. A lexical entry template extracted from testing data. The other type of failures (?+w/+t?) indicate that a word was incorrectly associated with a lexical entry template, even though both of them existed in the training data. Error analysis shows that 64.39% of failures were related to verbs. For example, for a relative clause ???/invest ??/Taiwan ? ??/businessman (the busi-nessman that invests Taiwan)? in the testing data, we associated a lexical entry template as shown in Figure 18 with the verb ???/invest?. In the training data, however, the lexical entry template shown in Figure 18 cannot be extracted for ???/invest?, since this word never appears in a relative clause with an extracted subject. Intro-ducing lexical rules to expand the lexical entry template of verbs in a relative clause is a possible way to solve this problem. 4.5 Comparison with Previous Work Guo?s work (Guo et al, 2007; Guo, 2009) is the only previous work on Chinese lexicalized                                                            4 ?+w/+t? means both the word and lexical entry template have been seen in the lexicon. ?+w/-t? means only the word has been seen in the lexicon (Hockenmaier and Steedman, 2002). 
grammar development from the CTB, which in-duced wide-coverage LFG resources from the CTB. By using the hand-made gold-standard f-structures of 200 sentences from the CTB 5.1, the LFG f-structures developed in Guo?s work achieved 96.34% precision and 96.46% recall for unseen text (Guo, 2009). In our work, we applied the similar strategy in evaluating the accuracy of the developed Chinese HPSG grammar, which achieved 97.24% lexical accuracy on 100 unseen sentences from the CTB 6.0. When evaluating the coverage of our grammar, we used a much larger data set (including 2,000 unseen sen-tences), and achieved 98.51% lexical coverage. Although these results cannot be compared to Guo?s work directly because of the different size and content of data set, it indicates that the Chi-nese HPSG grammar developed in our work is comparable in quality with Guo?s work. In addition, there were previous works about developing lexicalized grammar for English. Considering the small size of the CTB, in com-parison to the Penn Treebank used in the previ-ous works, the results listed in Table 5 verify that, the quality of the Chinese HPSG grammar developed in our work is comparable to these previous works.  Previous Work Sent. Cov. Lex. Cov. Miyao (2006) 82.50% 98.97% Hockenmaier and Steedman (2002) - 98.50% Xia (1999) - 96.20% Table 5. Evaluation results of previous work.  4.6 Discussion There are still some sentences in the CTB from which we failed to extract lexical entries. We analyzed the 59 failed sentences in the testing data and listed the reasons in Table 6.  Reason Sent # Error in the derivation tree annotation 31 Short of semantics mapping rule 23 Inconsistent annotation in the CTB 5 Table 6. Reasons for lexicon extraction failures. The principal reason for 31 sentence failures, is the error in the derivation tree annotation. For instance, our current annotation rules could con-vert the regular relative clause shown in Figure 12. Nonetheless, when the relative clause is in-side of a parenthesis, such as ?? ??/primitive ? ???/method (the method that is primi-tive)?, the annotation rules failed at finding the extracted head noun to create a derivation tree. This type of failure can be reduced by improving the annotation rules. 
1423
The second reason, for which 23 sentences failed, is the shortage of the semantics mapping rules. For example, we did not define semantics mapping rule for a classifier that acts as a predi-cate with two topics. This type of failure can be reduced by adding semantic mapping rules.  The last reason for sentence failures is incon-sistencies in the CTB annotation. In our future work, these inconsistencies will be collected to enrich our inconsistency correction rules.  In addition to the reasons above, some sen-tences with special constructions in the devel-opment and training data also could not be analyzed by our current grammar, since the spe-cial construction is difficult for the current HPSG to analyze. The special constructions include the argument-cluster coordination shown in Figure 19. Introducing the similar rules used in CCG (Hockenmaier and Steedman, 2002) could be a possible solution to this problem. 
 (have 177 intrant projects and 6.4 billion investments) Figure 19. An argument-cluster coordination in CTB. 5 Related Work  To the extent of our knowledge, the only previ-ous work about developing Chinese lexicalized grammar from treebanks is Guo?s work (Guo et al, 2007; Guo, 2009). An LFG-based parsing using wide-coverage LFG approximations in-duced from the CTB was done in this work. However, they did not train a deep parser based on the LFG resources obtained in their work, but relied on an external PCFG parser to create c-structure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). In contrast to Guo?s work, we paid particular attention to a different grammar framework, i.e. HPSG, with the analysis of more Chinese constructions, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other lan-guages. For example, by using the Penn Tree-bank, Miyao et al (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG 
specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) ac-quired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained wide-coverage LFG resources from a French Tree-bank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a compara-tive result to state-of-the-art works for English.  There are some researchers who worked on Chinese HPSG grammar development manually. Zhang (2004) implemented a Chinese HPSG grammar using the LinGO Grammar matrix (Bender et al, 2002). Only a few basic construc-tions were considered, and a small lexicon was constructed in this work. Li (1997) and Wang et al (2009) designed frameworks for Chinese HPSG grammar; however, only small grammars were implemented in these works. Furthermore, some linguistic works focused mainly on the discussion of specific Chinese constructions in the HPSG or LFG framework, without implementing a grammar for real-world text (Bender, 2000; Gao, 2000; Li and McFe-tridge, 1995; Li, 1995; Xue and McFetridge, 1995; Wang and Liu, 2007; Ng, 1997; Muller and Lipenkova, 2009; Liu, 1996; Kit, 1998). 6 Conclusion and Future Work  In this paper, we described the semi-automatic development of a Chinese HPSG grammar from the CTB. Grammatical constraints are first de-signed by hand. Then, we convert the bracketing trees in the CTB into an HPSG treebank, by us-ing pre-defined annotation rules. Lastly, we automatically extract lexical entries from the HPSG treebank. We evaluated our work on the CTB 6.0. Results indicated that a large HPSG lexicon was successfully extracted with a 97.24% accuracy. Furthermore, our grammar achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text.  This is an ongoing work, and there are some future works under consideration, including en-riching the design of annotation rules, introduc-ing more semantics mapping rules, and adding lexical rules. In addition, the work on Chinese HPSG parsing is on-going, within which the Chinese HPSG grammar developed in this work will be available soon. 
1424
References  Emily Bender. 2000. The Syntax of Madarin Ba: Reconsid-ering the Verbal Analysis. Journal of East Asian Lin-guistics. 9(2): 105-145. Emily Bender, Dan Flickinger, and Stephan Oepen. 2002. The Grammar Matrix: An Open-source Starter-lit for the Rapid Development of Cross-linguistically Consistent Broad-coverage Precision Grammars. Procedings of the Workshop on Grammar Engineering and Evaluation. John Chen and Vijay K. Shanker. 2004. Automated Extrac-tion of TAGs from the Penn Treebank. Proceedings of the 6th IWPT. David Chiang. 2000. Statistical Parsing with an Automati-cally-extracted Tree Adjoining Grammar. Proceedings of the 38th ACL. 456-463. Stephen Clark and James R. Curran. 2004. Parsing the WSJ Using CCG and Log-linear Models. Proceedings of the 42nd ACL. Bart Cramer and Yi Zhang. 2009. Construction of a German HPSG Grammar from a Detailed Treebank. Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks. Qian Gao. 1992. Chinese Ba Construction: its Syntax and Semantics. Technical report.  Qian Gao. 2000. Argument Structure, HPSG and Chinese Grammar. Ph.D. Thesis. Ohio State University. Yuqing Guo. 2009. Treebank-based acquisition of Chinese LFG Resources for Parsing and Generation. Ph.D. The-sis. Dublin City University. Yuqing Guo, Josef van Genabith and Haifeng Wang. 2007. Acquisition of Wide-Coverage, Robust, Probabilistic Lexical-Functional Grammar Resources for Chinese. Proceedings of the 12th International Lexical Functional Grammar Conference (LFG 2007). 214-232. Julia Hockenmaier. 2006. Creating a CCGbank and a wide-coverage CCG lexicon for German Proceedings of COLING/ACL 2006. Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Tree-bank. Proceedings of the 3rd LREC. C-R Huang. 1991. Madarin Chinese and the Lexical Map-ping Theory: A Study of the Interaction of Morphology and Argument Changing. Bulletin of the Institute of His-tory and Philosophy 62. Ronald M. Kaplan et al 2004. Speed and Accuracy in Shal-low and Deep Stochastic Parsing. Proceedings of HLT/NAACL 2004. Chunyu Kit. 1998. Ba and Bei as Multi-valence Preposi-tions in Chinese. Studia Linguistica Sinica: 497-522.  Wei Li. 1995. Esperanto Inflection and its Interface in HPSG. Proceedings of the 11th North West Linguistics Conference. Wei Li. 1997. Outline of an HPSG-style Chinese Reversible Grammar. Proceedings of the 13th North West Linguis-tics Conference. Wei Li and Paul McFetridge. 1995. Handling Chinese NP Predicate in HPSG. Proceedings of PACLING-II. Charles N. Li and Sandra A. Thompson. 1989. Mandarin Chinese: A Functional Reference Grammar. University of California Press, London, England. 
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2007. Efficient HPSG Parsing with Supertagging and CFG-filtering. Proceedings of the 20th IJCAI. Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-oriented Grammar Development and Feature Forest Model. Ph.D. Thesis. The University of Tokyo. Yusuke Miyao, Takashi Ninomiya and Junichi Tsujii. 2005. Corpus-oriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. Natural Language Processing - IJCNLP 2005: 684-693. Yusuke Miyao and Junichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. Computational Linguistics. 34(1): 35-80. Stefan Muller and Janna Lipenkova. 2009. Serial Verb Con-structions in Chinese: A HPSG Account. Proceedings of the 16th International Conference on Head-Driven Phrase Structure Grammar. 234-254. Hiroko Nakanishi, Yusuke Miyao and Junichi Tsujii. 2004. An Empirical Investigation of the Effect of Lexical Rules on Parsing with a Treebank Grammar. Proceed-ings of the 3rd TLT. 103-114. Say K. Ng. 1997. A Double-specifier Account of Chinese NPs Using Head-driven Phrase Structure Grammar. Master Thesis. Department of Linguistics, University of Edinburgh. Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. The University of Chicago Press and CSLI Publications, Chicago, IL and Stanford, CA. Ines Rehbein and Josef van Genabith. 2009. Automatic Acquisition of LFG Resources for German ? As Good as it Gets. Proceedings of the 14th International Lexical Functional Grammar Conference (LFG 2009). Natalie Schluter and Josef van Genabith. 2008. Treebank-based Acquisition of LFG Parsing Resources for French. Proceedings of the 6th LREC. Mark Steedman. 2000. The Syntactic Process. The MIT Press. Xiangli Wang et al 2009. Design of Chinese HPSG Frame-work for Data-driven Parsing. Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation. Lulu Wang and Haitao Liu. 2007. A Description of Chinese NPs Using Head-driven Phrase Structure Grammar. Pro-ceedings of the 14th International Conference on Head-Driven Phrase Structure Grammar. 287-305. Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. Proceedings of the 5th NLPRS. Nianwen Xue, Fudong Chiou, and Martha Palmer. 2002. Building a Large-scale Annotated Chinese Corpus. Pro-ceedings of COLING 2002. Ping Xue and Paul McFetridge. 1995. DP Structure, HPSG, and the Chinese NP. Proceedings of the 14th Annual Conference of Canadian Linguistics Association. Nianwen Xue and Fei Xia. 2000. The Bracketing Guidelines for the Penn Chinese Treebank. Yi Zhang. 2004. Starting to Implement Chinese Resource Grammar using LKB and LinGO Grammar Matrix. Technical report.   
1425
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2140?2150, Dublin, Ireland, August 23-29 2014.
Left-corner Transitions on Dependency Parsing
Hiroshi Noji and Yusuke Miyao
Department of Informatics
The Graduate University for Advanced Studies
National Institute of Informatics, Tokyo, Japan
{noji,yusuke}@nii.ac.jp
Abstract
We propose a transition system for dependency parsing with a left-corner parsing strategy. Unlike
parsers with conventional transition systems, such as arc-standard or arc-eager, a parser with our
system correctly predicts the processing difficulties people have, such as of center-embedding.
We characterize our transition system by comparing its oracle behaviors with those of other tran-
sition systems on treebanks of 18 typologically diverse languages. A crosslinguistical analysis
confirms the universality of the claim that a parser with our system requires less memory for
parsing naturally occurring sentences.
1 Introduction
It is sometimes argued that transition-based dependency parsing is appealing not only from an engi-
neering perspective due to its efficiency, but also from a scientific perspective: These parsers process a
sentence incrementally similar to a human parser, which have motivated several studies concerning their
cognitive plausibility (Nivre, 2004; Boston and Hale, 2007; Boston et al., 2008). A cognitively plausible
dependency parser is attractive for many reasons, one of the most important being that dependency tree-
banks are available in many languages, so it is suitable for crosslinguistical studies of human language
processing (Keller, 2010). However, current transition systems based on shift-reduce actions fully or
partially employ a bottom-up strategy
1
, which is problematic from a psycholinguistical point of view:
Bottom-up or top-down strategies are known to fail in predicting the difficulty for certain sentences, such
as center-embedding, which people have troubles in comprehending (Abney and Johnson, 1991).
We propose a transition system for dependency parsing with a left-corner strategy. For constituency
parsing, unlike other strategies, the arc-eager left-corner strategy is known to correctly predict processing
difficulties people have (Abney and Johnson, 1991). To the best of our knowledge, however, the idea of
left-corner strategy has not been introduced in the dependency parsing literature. We define the memory
cost for a transition system as the number of unconnected subtrees on a stack. Under this condition, the
proposed system incurs non-constant memory cost only when encountering center-embedded structures.
After developing the transition system, we characterize it by looking into the following question: Is
it true that naturally occurring sentences can be parsed on this system with a lower memory overhead?
This should be true under the assumptions that 1) people avoid generating a sentence that causes diffi-
culty for them, and 2) center-embedding is a kind of such structure. Specifically, we focus on analyzing
the oracle transitions of the system, i.e., parser actions to recover the gold dependency tree for a sen-
tence. In English, it is known that left-corner transformed treebank sentences can be parsed with less
memory (Schuler et al., 2010), but our focus in this paper is on the language universality of the claim in
a crosslingual setting. Two different but relevant motivations exist for this analysis. The first is to an-
swer the following scientific question: Is the claim that people tend to avoid generating center-embedded
sentences language universal? This is unclear since the observation that a center-embedded sentence is
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The top-down parser of Hayashi et al. (2012) is an exception, but its processing is not incremental.
2140
difficult to comprehend is from psycholinguistic studies mainly on English. The second motivation is
to verify whether a parser with the developed system can be viable for crosslinguistical study of human
language processing. There is evidence that a human parser cannot store elements of a small constant
number, such as three or four (Cowan, 2001). If our system confirms to such a severe constraint, we may
claim its cognitive plausibility across languages. We will pursue these questions using the multilingual
dependency treebanks from the CoNLL-X and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et
al., 2007).
In short, our contributions of this paper can be sketched as follows:
1. We formulate a transition system for dependency parsing with a left-corner strategy.
2. We characterize our transition system with its memory cost by simulating oracle transitions along
with other transition systems on the CoNLL multilingual treebanks. This is the first empirical study
of required memory for left-corner parsing in a crosslinguistical setting.
2 Memory cost of Parsing Algorithms
In this work, we focus on the memory cost for dependency parsing transition systems. While there
have been many studies concerning the memory cost for an algorithm in constituency parsing (Abney
and Johnson, 1991; Resnik, 1992), the same kind of study is rare in dependency parsing. This section
discusses the memory cost for the current dependency parsing transition systems. Before that, we first
review the known results in constituency parsing regarding memory cost.
2.1 Center-Embedding and the Left-Corner Strategy
10X
12d6X
8c2X
4b1a
3
5
7
9
11
13
2X
5X
9X
12d8c
10
13
4b
6
11
1a
3
7
2X
9X
12d5X
7c4b
6
8
10 13
1a
3
11
The structures on the right side are called left-branching,
right-branching, and center-embedding, respectively. Peo-
ple have difficulty when parsing center-embedded struc-
tures, while no difficulty with right-branching or left-
branching structures. An example of a center-embedded sentence is the rat [the cat [the dog chased]
bit] ate the cheese, which is difficult, but if we rewrite it as the cheese was eaten [by the rat [that bit the
cat [that chased the dog]]], which is a kind of right-branching structure, the parse becomes easier.
Abney and Johnson (1991) showed that top-down or bottom-up strategies
2
fail to predict this result.
For example, for the right-branching structure, a bottom-up strategy requires O(n) memory, since it
must first construct a subtree of c and d, but the center-embedded structure requires less memory. The
arc-eager left-corner strategy correctly predicts the difficulty of a center-embedded structure, which is
characterized by the following order of recognitions of nodes and arcs:
1. A node is enumerated when the subtree of its first child has been enumerated.
2. An arc is enumerated when two nodes it connects have been enumerated.
The numbers on the trees above indicate the order of recognition for this strategy. We can see that it
requires a constant memory for both right-branching and left-branching structures. For example, for
the right-branching structure, it reaches 7 after reading b, which means that a and b are connected by a
subtree. On the other hand, for the center-embedded structure, it reaches 6 after reading b, but a and b
cannot be connected at this point, requiring extra memory.
2.2 Transition-based Dependency Parsing
Next, we summarize the issues with current transition systems for dependency parsing with regards to
their memory cost. A transition-based dependency parser processes a sentence on a transition system,
which is defined as a set of configurations and a set of transitions between configurations (Nivre, 2008).
Each configuration has a stack preserving constructed subtrees on which we define the memory cost as a
function for each system.
2
We should distinguish between two types of characterizations of parsing: strategy and algorithm. A parsing strategy is
an abstract notion that defines ?a way of enumerating the nodes and arcs of parse trees? (Abney and Johnson, 1991), while
a parsing algorithm defines the implementation of that strategy, typically with push-down automata (Johnson-Laird, 1983;
Resnik, 1992). A parsing strategy is useful for characterizing the properties of each parser, and we concentrate on the strategy
for exposition of constituency parsing. For dependency parsing, we mainly discuss the algorithm, i.e., the transition system.
2141
a b c
(a)
a b c
(b)
a b c
(c)
X
X
cb
a
(d)
Figure 1: (a)?(c): Right-branching dependency
trees for three words; (d): the corresponding CNF.
Arc-standard Arc-eager Left-corner
left-branching O(1) O(1) O(1)
right-branching O(n) O(1 ? n) O(1)
center-embedding O(n) O(1 ? n) O(n)
Table 1: Order of the memory cost for each struc-
ture for each transition system. O(1 ? n) means
that it processes some structures with a constant
cost but some with a non-constant cost.
Arc-Standard We define the memory cost as the number of elements on the stack since all stack
elements are disjoint. In this setting, we can see that the arc-standard system has a problem for a right-
branching structure, such as a
y
b
y
c
y
? ? ? , in which the system first pushes all words on the stack before
connecting each pair of words, requiring O(n) memory. Nivre (2004) discussed the problem with this
system in greater detail, observing that its stack size grows when processing structures that become right-
branching when converted to the Chomsky normal form (CNF) of a context-free grammar (CFG). Figure
1 lists those dependency structures for three words, for which the system must construct a subtree of b
and c before connecting a to either, requiring extra memory. This is because the system builds a tree
bottom-up: each token collects all dependents before being attached to its head. In fact, the arc-standard
system is essentially equivalent to the push-down automaton of a CFG in the CNF with a bottom-up
strategy (Nivre, 2004), so it has the same property as the bottom-up parser for a CFG.
Arc-Eager In the arc-eager system, the stack contains sequences of tokens comprising connected com-
ponents, so we can define the memory cost as the number of connected components on the stack. With
this definition, we can partially resolve the problem with the arc-standard system. The arc-eager system
does not incur any cost for processing the structure in Figure1(a) and a
y
b
y
c
y
? ? ? since it can connect
all tokens on the stack (Nivre, 2004). Because its construction is no longer pure bottom-up, it is difficult
to formally characterize the cost based on the type of tree structure. However, this transition system can-
not correctly predict difficulties with center-embedding because the cost never increases as long as all
dependency arcs are left-to-right, e.g., a sentence a
y
b
y
c d becomes center-embedding when converted
to a CNF, but it does not incur any cost for it. Note that this system still incurs cost for some right-
branching structures, such as in Figures 1(b?c), and some center-embedded structures. Therefore, for the
arc-eager system, it is complicated to discuss the required order of memory cost. We summarize these
results in Table 1. Our goal is to develop an algorithm with the properties of the last column, requiring
non-constant memory for only center-embedded structures.
Other systems All systems where stack elements cannot be connected have the same problem as the
arc-standard system because of their bottom-up constructions, including the hybrid system of Kuhlmann
et al. (2011). Kitagawa and Tanaka-Ishii (2010) and Sartorio et al. (2013) present an interesting variant,
which attaches a node to another node that may not be the head of a subtree on the stack. We can use
the same reasoning for the arc-eager system for these systems: they sometimes do not incur costs for
center-embedded structures, while they incur a non-constant cost for some right-branching structures.
3 Left-corner Dependency Parsing
We now discuss the construction of our transition system with the left-corner strategy. Resnik (1992)
proposed a push-down recognizer for a CFG. In the following, we instead characterize his algorithm by
inference rules, which are more intuitive and helpful to adapt the idea for dependency parsing.
Prediction:
B
?????? A?B C
A
CB
Composition:
A
CB D
???????? C?D E
A
C
ED
B
Prediction and Composition There are two
characteristic operations in the push-down recog-
nizer of Resnik (1992): prediction and composi-
tion. We show inference rules of these operations
on the right side:
Prediction is used to predict the parent node and
the sibling of a recognized subtree when the sub-
2142
SHIFT (?, j|?,A) 7? (?|?j?, ?, A)
INSERT (?|??
?
1
|i|x(?)?), j|?,A) 7? (?|??
?
1
|i|j?, ?, A ? {(i, j)} ? {?
k??
(j, k))
LEFT-PRED (?|??
11
, ? ? ? ?, ?, A) 7? (?|?x(?
11
)?, ?, A)
RIGHT-PRED (?|??
11
, ? ? ? ?, ?, A) 7? (?|??
11
, x(?)?, ?, A)
LEFT-COMP (?|??
?
2
|x(?)?|??
11
, ? ? ? ?, ?, A) 7? (?|??
?
2
|x(? ? {?
11
})?, ?, A)
RIGHT-COMP (?|??
?
2
|x(?)?|??
11
, ? ? ? ?, ?, A) 7? (?|??
?
2
|?
11
|x(?)?, ?, A ? {?
k??
(?
11
, k)})
Figure 2: Actions of the left-corner transition system.
LEFT-PRED: RIGHT-PRED:
a
x
a
a
X(x)
xa
a
a
x
a
X(a)
xa
LEFT-COMP: RIGHT-COMP:
a
b
x
b
x
a
a
X(b)
xb
X(b)
X(x)
xa
b
a
b
x
b
a
x
a
X(b)
xb
X(b)
X(a)
xa
b
Figure 3: Correspondences of reduce actions between dependency and CFG. Nonterminal X(t) means
that its lexical head is t. We only show minimal example subtrees for simplicity. However, a can have
an arbitrary number of children, so can b or x, as long as x is on a right spine and has no right children.
tree is complete and its parent node is not yet recognized. Composition composes two subtrees by first
predicting the parent and the sibling of a recognized subtree then immediately connecting trees by iden-
tifying the same node on two trees (C, in this case). This is used when the parent node of a completed
subtree has already been predicted as a part of another tree in a top-down fashion.
Dummy Node We now turn to the discussion of dependency parsing. The key characteristic of our
transition system is the introduction of a dummy node on a subtree, which is needed to represent a
subtree containing some predicted structures as in constituency subtrees for Resnik?s recognizer. To get
an intuition of the parser actions, we present a simulation of transitions for the sentence in Figure 1(b),
of which current systems fail to predict its difficulty. Our system first shifts a then conducts a kind of
prediction operation, resulting in a subtree
x
a
, where x is a dummy node. This means that we predict
that a will become a left dependent of an incoming word. Next, it shifts b to the stack then conducts a
composition operation to obtain a tree
x
a
b
. It finally inserts c to the position of x, recovering the tree.
Transition system As in many other transition systems, a configuration for our system is a tuple c =
(?, ?,A), where ? is a stack, and we use a vertical bar to signify an append operation, e.g., ? = ?
?
|?
1
denoting ?
1
is the top most element of the stack ?, and ? is an input buffer consisting of token indexes
not processed yet. ? = j|?
?
means j is a first element of ?, and A ? V
w
? V
w
is a set of arcs given V
w
,
a set of token indexes for a sentence w.
Stack:
w
2
w
1
x
w
3
w
5
w
4
w
6
w
7
? = [?2, x({3, 5})?, ?6, 7?]
? = [8, 9, ? ? ? , n]
A = {(2, 1), (5, 4), (6, 7)}
Each element of a stack is a list representing a right
spine of a subtree, as in Kitagawa and Tanaka-Ishii
(2010) and Sartorio et al. (2013). A right spine ?
i
=
??
i1
, ?
i2
, ? ? ? , ?
ik
? consists of all nodes in a descending
path from the head of ?
i
, i.e., ?
i1
, taking the rightmost
child at each step. We also write ?
i
= ?
?
i
|?
ik
meaning that ?
ik
is the right most node of spine ?
i
. Each
element of ?
i
is an index of a token in a sentence, or a dummy node x(?), where ? is a set of the left
dependents of x. The figure above depicts an example of the configuration, where the i-th word in a
sentence is written as w
i
on the stack.
In the following, we say a right spine ?
i
is complete if it does not contain any dummy nodes, while
?
i
containing a dummy node is referred to as incomplete. Our transition system uses six actions, two of
2143
which are shift actions and four are reduce actions. All actions are defined in Figure 2.
Shift Actions There are two kinds of shift actions: SHIFT and INSERT
3
. SHIFT moves a token from
the top of the buffer to the stack. INSERT replaces a dummy node on the top of the stack with a token
from the top of the buffer. This adds arcs from/to tokens connected to the dummy node. Note that this
action can be performed for a configuration where x(?) is the top of ?
1
or ? is empty, in which case
arcs (i, j) or ?
k??
(j, k) are not added. Resnik (1992) does not define this action, but instead uses a
verification operation (Rule 9). One can view our INSERT action as a composition of two actions: SHIFT
and a verification. We note that after these shift actions, the top element of the stack must be complete.
Reduce Actions Reduce actions create new arcs for subtrees on the stack. LEFT-PRED and RIGHT-
PRED correspond to the predictions of the CFG counterpart. Figure 3 describes these transitions for
minimal subtrees. LEFT-PRED assigns a dummy node x as the head of a (this corresponds to ?
11
), while
RIGHT-PRED creates x as a new right dependent. When we convert the resulting tree into a CNF, we can
see that the difference between these two operations lies in the predicted parent node of a: LEFT-PRED
predicts a nonterminal X(x), i.e., it predicts that the head of this subtree is the head of the predicting
sibling node, while RIGHT-PRED predicts that the head is a. Note that different from CFG rules, we do
not have to predict the actual sibling node; rather, we can abstract this predicted node as a dummy node
x. A similar correspondence holds between the composition actions: RIGHT-COMP and LEFT-COMP.
We note that to obtain a valid tree, shift and reduce actions must be performed alternatively. We
can prove this as follows: Let c = (?|?
2
|?
1
, ?, A). Since reduce actions turn an incomplete ?
1
into a
complete subtree, we cannot perform two consecutive reduce actions. Shift actions make ?
1
complete.
After a shift action, we cannot perform INSERT since it requires ?
i
to be incomplete; if we perform
SHIFT, the top two elements on the stack become complete, but we cannot connect these two trees since
the only way to connect two trees on the stack is composition, but this requires ?
2
to be incomplete.
Defining Oracle The oracle for a transition system is a function that returns a correct action given the
current configuration and a set of gold arcs. It is typically used for training a parser (Nivre, 2008), but
we define it to analyze the behavior of our system on treebank sentences.
First, we show that our system has the spurious ambiguity, and discuss its implications. Consider a
sentence a
x
b
y
c, which can be parsed with two different action sequences as follows:
1. SHIFT? LEFT-PRED? INSERT? RIGHT-PRED? INSERT
2. SHIFT? LEFT-PRED? SHIFT? RIGHT-COMP? INSERT
The former INSERTs b at step 3, then RIGHT-PREDs to wait for a right dependent (c). The latter, on the
other hand, SHIFTs b at step 3, then RIGHT-COMPs to combine two subtrees (a
x
x and b) to obtain a
tree a
x
b
y
x. These ambiguities between action sequences and the resulting tree are referred to as the
spurious ambiguity. Next, we analyze the underlying differences between these two operations. We
argue that the difference lies in the form of the recognized constituency tree: The former RIGHT-PREDs
at step 4, which means that it recognizes a constituency of the form ((a b) c), while the latter recognizes
(a (b c)) due to its RIGHT-COMP operation. Therefore, the spurious ambiguity of our system is caused by
the ambiguity of converting a dependency tree to a constituency tree. Recently, some transition systems
have exploited similar ambiguities using dynamic oracles (Goldberg and Nivre, 2013; Sartorio et al.,
2013; Honnibal et al., 2013). The same type of analysis might be possible for our system, but we leave
it for future work; here we only present a static oracle and discuss its properties.
Since our system performs shift and reduce actions interchangeably, we need two functions to define
the oracle. Let c = (?|?
2
|?
1
, ?, A). The next shift action is determined as follows:
? INSERT: if ?
1
= ??
?
1
|i|x(?)? and (i, j) ? A
g
and j has no dependents in ? (if i exists) or ?k ?
?; (j, k) ? A
g
(otherwise).
? SHIFT: otherwise.
The next reduce action is determined as follows:
3
We use small caps to refer to a specific action, e.g., SHIFT, while ?shift? refers to an action type.
2144
? LEFT-COMP: if ?
2
= ??
?
2
|i|x(?)?, ?
1
= ??
11
, ? ? ? ?, ?
11
has no dependents in ?, and ?
11
can be a
left dependent of x: i?s next dependent is the head of ?
11
(if i exists) or k ? ? and ?
11
share the
same head (otherwise).
? RIGHT-COMP: if ?
2
= ??
?
2
|i|x(?)?, ?
1
= ??
11
, ? ? ? ?, ?
11
has one more dependent in ?, and ?
11
can
be insertable at the position of x: (i, ?
11
) ? A
g
or ?k ? ?; (?
11
, k) ? A
g
.
? RIGHT-PRED: if ?
1
= ??
11
, ? ? ? ? and ?
11
has one more dependent in ?.
? LEFT-PRED: otherwise.
Each condition checks whether we can obtain the gold dependency arcs after the transition. This oracle
follows the strategy of ?compose or insert when possible?. As we saw in the example, sometimes INSERT
and SHIFT both can be valid to recover the gold arcs; however, we always select INSERT. Sometimes the
same ambiguity occurs between LEFT-COMP and LEFT-PRED or RIGHT-COMP and RIGHT-PRED, but
we always prefer composition.
As we saw above, the spurious ambiguity of our system occurs when the conversion from a depen-
dency tree to a constituency tree is not deterministic. This oracle has the property that its recognized
constituency tree corresponds to the one that can be obtained by constructing all left-arcs first given a de-
pendency tree. For example, in the above example for a
x
b
y
c, we select action sequences 1 to recognize
a constituency of ((a b) c). We can prove this property by showing that the algorithm always collects all
left-arcs for a head before any right-arcs; Not doing INSERT or composition when possible means that
we create a right-arc for a head when the left-arcs are not yet completed. We can also verify that this
algorithm can parse all no-center-embedded sentences in a CNF converted in this manner with the stack
depth never exceeding three, requiring non-constant memory for only center-embedded structures.
4 Memory Cost Analysis
To characterize our transition system, we compare it to other systems by observing the incurred memory
cost during running oracle transitions for sentences on a set of typologically diverse languages. For this
analysis, we aim to verify the language universality of the claim: naturally occurring sentences should
be parsed with a left-corner parser with less required memory.
Settings We collect 18 treebanks from the CoNLL-X and 2007 shared tasks (Buchholz and Marsi,
2006; Nivre et al., 2007). Some languages were covered by both shared tasks; we use only 2007 data.
We remove sentences with non-projective arcs (Nivre, 2008) or without any root nodes. We follow the
common practice adding a dummy root token to each sentence. This token is placed at the end of each
sentence, as in Ballesteros and Nivre (2013), since it does not change the cost on sentences with one root
token on all systems.
We compare three transition systems: arc-standard, arc-eager, and left-corner. For each system, we
perform oracle transitions for all sentences and languages, measuring the memory cost for each configu-
ration defined as follows. For the arc-standard and left-corner systems, we use the number of elements on
the stack. This arc-standard system uses the original formulation of Nivre (2003), connecting two items
on the stack at the reduce action. For the arc-eager system, we use the number of connected components.
The system can create a subtree at the beginning of a buffer, in which case we add 1 to the cost.
We run a static oracle for each system. For the left-corner system, we implemented the algorithm
presented in Section 3. For the arc-standard and arc-eager systems, we implemented an oracle preferring
reduce actions over shift, which can minimizes the memory cost.
Memory costs for general sentences For each language, we count the number of configurations for
each memory cost during performing oracles on all sentences. In Figure 4, we show the cumulative
frequencies of configurations having each memory cost (see solid lines in the figure). These lines can
answer the question: What memory cost is required to cover X% of configurations when recovering all
gold trees? Note that comparing absolute values are not meaningful since the minimal cost to construct
an arc is different for each system, e.g., the arc-standard system requires at least two items on the stack,
2145
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Arabic
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Basque
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Bulgarian
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Catalan
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Chinese
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Czech
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Danish
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Dutch
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
English
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Greek
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Hungarian
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Italian
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Japanese
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Portuguese
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Slovene
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Spanish
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Swedish
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Turkish
Arc-standard
Arc-standard (random)
Arc-eager
Arc-eager (random)
Left-corner
Left-corner (random)
Figure 4: Crosslinguistic comparison of cumulative frequency of memory cost when processing all sen-
tences for each system. For example, in Arabic, for the arc-eager system, around 90% of all config-
urations incurred memory cost of ? 5. Dotted lines (random) are results on the sentences, which are
randomly reordered while preserving the graph structure and projectivity.
while the arc-eager system can create a right arc if the stack contains one element. Instead, we focus on
the universality of each system?s behavior for different languages.
As we discussed in section 2.2, the arc-standard system can process only left-branching structures with
a constant memory, which are typical in head-final languages such as Japanese or Turkish, and we can
2146
see this tendency. The system behaves poorly in many other languages.
The arc-eager and left-corner systems behave similarly for many languages, but we can see that there
are some languages for which the left-corner system behaves similarly to other languages, while the arc-
eager system requires larger cost; Arabic, Hungarian, or Japanese, for example. In fact, except Arabic,
the left-corner system reaches 98% of configurations with a memory cost of? 3, which indicates that the
property of the left-corner system requiring less memory is more universal than that of other systems.
Comparison to randomized sentences One might wonder that the results above come from the nature
of left-corner parsing reducing the stack size, not from the bias in language avoiding center-embedded
structures. To partially answer this question, we conduct another experiment comparing oracle transitions
on original treebank sentences and on wrong sentences. We create these wrong sentences by using the
method from Gildea and Temperley (2007). We reorder words in each sentence by first extracting a
directed graph then randomly reordering the children of each node while preserving projectivity. The
dotted lines in Figure 4 denotes the results of randomized sentences for each system.
There are notable differences in required memory between original and random sentences for many
languages. This result indicates that our system can parse with less memory for only naturally occurring
sentences. For Chinese and Hungarian, the differences are subtle. However, the differences are also
small for the other systems, which implies that these corpora have some biases on graphs reducing the
differences.
5 Related Work and Discussion
To the best of our knowledge, parsing with a left-corner strategy has only been studied for constituency.
Roark (2001) proposed a top-down parser for a CFG with a left-corner grammar transform (Johnson,
1998), which is essentially the same as left-corner parsing but enables several extensions in a unified
framework. Roark et al. (2009) studied the psychological plausibility of Roark?s parser, observing that it
fits well to human reading time data. Another model with a left-corner strategy is Schuler et al. (2010):
they observed that the transformed grammar of English requires only limited memory, proposing a finite
state approximation with a hierarchical hidden Markov model. This parser was later extended by van
Schijndel and Schuler (2013), which defined a left-corner parser for constituency with shift and reduce
actions. In fact, they used the same kind of actions as our transition system: shift, insert, predict, and
composition. Though they did not mentioned explicitly, we showed how to construct a left-corner parsing
algorithm with these actions by decomposing the push-down recognizer of Resnik (1992). These are
examples of broad-coverage parsing models with cognitively plausibility, which has recently received
considerable attention in interdisciplinary research on psycholinguistics and computational linguistics
(Schuler et al., 2010; Keller, 2010; Demberg et al., 2013).
Differently from previous models, our target is dependency. A dependency-based cognitively plausible
model is attractive, especially from a crosslinguistical viewpoint. Keller (2010) argued that current
models only work for English, or German in few exceptions, and the importance of crosslinguistically
valid models of human language processing. There has been some attempts to use a transition system for
studying human language processing (Boston and Hale, 2007; Boston et al., 2008), so it is interesting to
compare automatic parsing behaviors with various transition systems to human processing.
We introduced a dummy node for representing a subtree with an unknown head or dependent. Re-
cently, Menzel and colleagues (Beuck and Menzel, 2013; Kohn and Menzel, 2014) have also studied
dependency parsing with a dummy node. While conceptually similar, the aim of introducing a dummy
node is different between our approach and theirs: We need a dummy node to represent a subtree cor-
responding to that in Resnik?s algorithm, while they introduced it to confirm that every dependency tree
on a sentence prefix is fully connected. This difference leads to a technical difference; a subtree of their
parser can contain more than one dummy node, while we restrict each subtree to containing only one
dummy node on a right spine.
Our experiments in section 4 can be considered as a study on functional biases existing in language or
language evolution (Jaeger and Tily, 2011). In computational linguistics, Gildea and Temperley (2007;
2010) examined the bias on general sentences called dependency length minimization (DLM), which
2147
argues that grammar should favor the dependency structures that reduce the sum of dependency arc
lengths. They reordered English and German treebank sentences with various criteria: original, random
with projectivity, and optimal that minimizes the sum of dependency lengths. They observed that the
word order of English fits very well to the optimal ordering, while German does not. We examined the
universality of the bias to reduce memory cost for left-corner parsing. Although we cannot compare the
incurred cost with the optimal reordered sentences, our results on original sentences, in which there are
few configurations requiring the stack depth? 4, suggest the bias to avoid center-embedded structures is
language universal. It will be interesting to analyze in more detail the relationships between DLM and the
bias of our system since the two biases are not independent, e.g., center-embed structures typically appear
with longer dependencies. Are there languages that do not hold DLM while requiring less memory, or
vice versa? For these analyses, we might have to take care of the grammar construction, e.g., there
are several definitions for coordination structures for dependency grammars (Popel et al., 2013). The
functional views discussed above might shed some light on the desired construction for these cases.
6 Conclusion
We have pointed out that the memory cost on current transition systems for dependency parsing do not
coincide with observations in people, proposing a system with a left-corner strategy. Our crosslinguistical
analysis confirms the universality of the claim that people avoid generating center-embedded sentences,
which also suggests that it is worthy for crosslinguistical studies of human language processing.
As a next stage, we are seeking to train a parser model as in other transition systems with a discrimi-
native framework such as a structured perceptron (Zhang and Nivre, 2011; Huang and Sagae, 2010). A
parser with our transition system might also be attractive for the problem of grammar induction, where
recovering dependency trees are a central problem (Klein and Manning, 2004), and where some linguis-
tic biases have been exploited, such as reducibility (Mare?cek and
?
Zabokrtsk?y, 2012) or acoustic cues
(Pate and Goldwater, 2013). Recently, Cohen et al. (2011) showed how to interpret shift-reduce actions
as a generative model; combining their idea and our transition system might enable the model to exploit
memory biases that exist in natural sentences.
Finally, dependency grammars are suitable for treating non-projective structures. Extensions for tran-
sition systems have been proposed to handle non-projective structures with additional actions (Attardi,
2006; Nivre, 2009). Although our system cannot handle non-projective structures, a similar extension
might be possible, which would enable a left-corner analysis for non-projective structures.
Acknowledgements
We thank Pontus Stenetorp and anonymous reviewers for their valuable feedbacks on a preliminary
version of this paper.
References
Steven Abney and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. Journal
of Psycholinguistic Research, 20(3):233?250.
Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings
of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 166?170, New York
City, June. Association for Computational Linguistics.
Miguel Ballesteros and Joakim Nivre. 2013. Going to the roots of dependency parsing. Computational Linguis-
tics, 39(1):5?13.
Niels Beuck and Wolfgang Menzel. 2013. Structural prediction in incremental dependency parsing. In Alexander
Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 7816 of Lecture Notes in
Computer Science, pages 245?257. Springer Berlin Heidelberg.
Marisa Ferrara Boston and John T. Hale. 2007. Garden-pathing in a statistical dependency parser. In Proceedings
of the Midwest Computational Linguistics Colloquium, West Lafayette, IN. Midwest Computational Linguistics
Colloquium.
2148
Marisa Ferrara Boston, John T. Hale, Umesh Patil, Reinhold Kliegl, and Shravan Vasishth. 2008. Parsing costs as
predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus. Journal of Eye Movement
Research, 2(1):1?12.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149?164, New York
City, June. Association for Computational Linguistics.
Shay B. Cohen, Carlos G?omez-Rodr??guez, and Giorgio Satta. 2011. Exact inference for generative probabilistic
non-projective dependency parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1234?1245, Edinburgh, Scotland, UK., July. Association for Computational
Linguistics.
Nelson Cowan. 2001. The magical number 4 in short-term memory: A reconsideration of mental storage capacity.
Behavioral and Brain Sciences, 24(1):87?114.
Vera Demberg, Frank Keller, and Alexander Koller. 2013. Incremental, predictive parsing with psycholinguisti-
cally motivated tree-adjoining grammar. Computational Linguistics, 39(4):1025?1066.
Daniel Gildea and David Temperley. 2007. Optimizing grammars for minimum dependency length. In Proceed-
ings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 184?191, Prague, Czech
Republic, June. Association for Computational Linguistics.
Daniel Gildea and David Temperley. 2010. Do grammars minimize dependency length? Cognitive Science,
34(2):286?310.
Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. TACL,
1:403?414.
Katsuhiko Hayashi, Taro Watanabe, Masayuki Asahara, and Yuji Matsumoto. 2012. Head-driven transition-
based parsing with top-down prediction. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 657?665, Jeju Island, Korea, July. Association for
Computational Linguistics.
Matthew Honnibal, Yoav Goldberg, and Mark Johnson. 2013. A non-monotonic arc-eager transition system
for dependency parsing. In Proceedings of the Seventeenth Conference on Computational Natural Language
Learning, pages 163?172, Sofia, Bulgaria, August. Association for Computational Linguistics.
Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086, Uppsala,
Sweden, July. Association for Computational Linguistics.
T.Florian Jaeger and Harry Tily. 2011. On language utility: Processing complexity and communicative efficiency.
Wiley Interdisciplinary Reviews: Cognitive Science, 2(3):323?335.
P. N. Johnson-Laird. 1983. Mental models: towards a cognitive science of language, inference, and consciousness.
Harvard University Press, Cambridge, MA, USA.
Mark Johnson. 1998. Finite-state approximation of constraint-based grammars using left-corner grammar trans-
forms. In Christian Boitet and Pete Whitelock, editors, COLING-ACL, pages 619?623. Morgan Kaufmann
Publishers / ACL.
Frank Keller. 2010. Cognitively plausible models of human language processing. In Proceedings of the ACL 2010
Conference Short Papers, pages 60?67, Uppsala, Sweden, July. Association for Computational Linguistics.
Kotaro Kitagawa and Kumiko Tanaka-Ishii. 2010. Tree-based deterministic dependency parsing ? an application
to nivre?s method ?. In Proceedings of the ACL 2010 Conference Short Papers, pages 189?193, Uppsala,
Sweden, July. Association for Computational Linguistics.
Dan Klein and Christopher Manning. 2004. Corpus-based induction of syntactic structure: Models of depen-
dency and constituency. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 478?485, Barcelona, Spain, July.
Arne Kohn and Wolfgang Menzel. 2014. Incremental predictive parsing with turboparser. In Proceedings of the
ACL 2014 Conference Short Papers, Baltimore, USA, June. Association for Computational Linguistics.
2149
Marco Kuhlmann, Carlos G?omez-Rodr??guez, and Giorgio Satta. 2011. Dynamic programming algorithms for
transition-based dependency parsers. In Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies, pages 673?682, Portland, Oregon, USA, June. Association
for Computational Linguistics.
David Mare?cek and Zden?ek
?
Zabokrtsk?y. 2012. Exploiting reducibility in unsupervised dependency parsing. In
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 297?307, Jeju Island, Korea, July. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT), pages 149?160.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Frank Keller, Stephen Clark,
Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bring-
ing Engineering and Cognition Together, pages 50?57, Barcelona, Spain, July. Association for Computational
Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics,
34(4):513?553.
Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 351?359, Suntec, Singapore, August. Association for Computational
Linguistics.
John K. Pate and Sharon Goldwater. 2013. Unsupervised dependency parsing with acoustic cues. TACL, 1:63?74.
Martin Popel, David Mare?cek, Jan
?
St?ep?anek, Daniel Zeman, and Zden?ek
?
Zabokrtsk?y. 2013. Coordination struc-
tures in dependency treebanks. In ACL, pages 517?527.
Philip Resnik. 1992. Left-corner parsing and psychological plausibility. In COLING, pages 191?197.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic
expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324?333, Singapore,
August. Association for Computational Linguistics.
Brian Edward Roark. 2001. Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, and
Applications. Ph.D. thesis, Providence, RI, USA. AAI3006783.
Francesco Sartorio, Giorgio Satta, and Joakim Nivre. 2013. A transition-based dependency parser using a dynamic
parsing strategy. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 135?144, Sofia, Bulgaria, August. Association for Computational Linguistics.
William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broad-coverage parsing using
human-like memory constraints. Computational Linguistics, 36(1):1?30.
Marten van Schijndel and William Schuler. 2013. An analysis of frequency- and memory-based processing costs.
In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 95?105, Atlanta, Georgia, June. Association for Computa-
tional Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies, pages 188?193, Portland, Oregon, USA, June. Association for Computational Linguistics.
2150
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1180?1190,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improvements to the Bayesian Topic N -gram Models
Hiroshi Noji??
noji@nii.ac.jp
Daichi Mochihashi??
daichi@ism.ac.jp
?Graduate University for Advanced Studies
?National Institute of Informatics, Tokyo, Japan
?The Institute of Statistical Mathematics, Tokyo, Japan
Yusuke Miyao??
yusuke@nii.ac.jp
Abstract
One of the language phenomena that n-gram
language model fails to capture is the topic in-
formation of a given situation. We advance the
previous study of the Bayesian topic language
model by Wallach (2006) in two directions:
one, investigating new priors to alleviate the
sparseness problem caused by dividing all n-
grams into exclusive topics, and two, develop-
ing a novel Gibbs sampler that enables moving
multiple n-grams across different documents
to another topic. Our blocked sampler can
efficiently search for higher probability space
even with higher order n-grams. In terms of
modeling assumption, we found it is effective
to assign a topic to only some parts of a docu-
ment.
1 Introduction
N -gram language model is still ubiquitous in NLP,
but due to its simplicity it fails to capture some im-
portant aspects of language, such as difference of
word usage in different situations, sentence level
syntactic correctness, and so on. Toward language
model that can consider such a more global con-
text, many extensions have been proposed from
lexical pattern adaptation, e.g., adding cache (Je-
linek et al, 1991) or topic information (Gildea and
Hofmann, 1999; Wallach, 2006), to grammaticality
aware models (Pauls and Klein, 2012).
Topic language models are important for use in
e.g., unsupervised language model adaptation: we
want a language model that can adapt to the do-
main or topic of the current situation (e.g., a doc-
ument in SMT or a conversation in ASR) automat-
ically and select the appropriate words using both
topic and syntactic context. Wallach (2006) is one
such model, which generate each word based on lo-
cal context and global topic information to capture
the difference of lexical usage among different top-
ics.
However, Wallach?s experiments were limited to
bigrams, a toy setting for language models, and ex-
periments with higher-order n-grams have not yet
been sufficiently studied, which we investigate in
this paper. In particular, we point out the two funda-
mental problems caused when extending Wallach?s
model to a higher-order: sparseness caused by di-
viding all n-grams into exclusive topics, and local
minima caused by the deep hierarchy of the model.
On resolving these problems, we make several con-
tributions to both computational linguistics and ma-
chine learning.
To address the first problem, we investigate incor-
porating a global language model for ease of sparse-
ness, along with some priors on a suffix tree to cap-
ture the difference of topicality for each context,
which include an unsupervised extension of the dou-
bly hierarchical Pitman-Yor language model (Wood
and Teh, 2009), a Bayesian generative model for su-
pervised language model adaptation. For the sec-
ond inference problem, we develop a novel blocked
Gibbs sampler. When the number of topics is K
and vocabulary size is V , n-gram topic model has
O(KV n) parameters, which grow exponentially to
n, making the local minima problem even more se-
vere. Our sampler resolves this problem by moving
many customers in the hierarchical Chinese restau-
rant process at a time.
We evaluate various models by incremental cal-
culation of test document perplexity on 3 types of
corpora having different size and diversity. By com-
bining the proposed prior and the sampling method,
our Bayesian model achieve much higher accura-
cies than the naive extension of Wallach (2006) and
shows results competitive with the unigram rescal-
ing (Gildea and Hofmann, 1999), which require
1180
huge computational cost at prediction, with much
faster prediction time.
2 Basic Models
All models presented in this paper are based on the
Bayesian n-gram language model, the hierarchical
Pitman-Yor process language model (HPYLM). In
the following, we first introduce the HPYLM, and
then discuss the topic model extension of Wallach
(2006) with HPYLM.
2.1 HPYLM
Let us first define some notations. W is a vocabulary
set, V = |W | is the size of that set, and u, v, w ?W
represent the word type.
The HPYLM is a Bayesian treatment of the n-
gram language model. The generative story starts
with the unigram word distribution G?, which is
a V -dimensional multinomial where G?(w) repre-
sents the probability of word w. The model first
generates this distribution from the PYP as G? ?
PYP(a, b,G0), where G0 is a V -dimensional uni-
form distribution (G0(u) = 1V ;?u ? W ) and
acts as a prior for G? and a, b are hyperparameters
called discount and concentration, respectively. It
then generates all bigram distributions {Gu}u?W as
Gu ? PYP(a, b,G?). Given this distributions, it
successively generates 3-gram distributions Guv ?
PYP(a, b,Gu) for all (u, v) ? W 2 pairs, which
encode a natural assumption that contexts having
common suffix have similar word distributions. For
example, two contexts ?he is? and ?she is?, which
share the suffix ?is?, are generated from the same
(bigram) distribution Gis, so they would have simi-
lar word distributions. This process continues until
the context length reaches n ? 1 where n is a pre-
specified n-gram order (if n = 3, the above example
is a complete process). We often generalize this pro-
cess using two contexts h and h? as
Gh ? PYP(a, b,Gh?), (1)
where h = ah?, in which a is a leftmost word of h.
We are interested in the posterior word distribu-
tion following a context h. Our training corpus w
is a collection of n-grams, from which we can cal-
culate the posterior p(w|h,w), which is often ex-
plained with the Chinese restaurant process (CRP):
p(w|h,w) = chw ? athwch? + b
+
ath? + b
ch? + b
p(w|h?,w),
(2)
where chw is an observed count of n-gram hw called
customers, while thw is a hidden variable called ta-
bles. ch? and th? represents marginal counts: ch? =?
w chw and th? =
?
w thw. This form is very
similar to the well-known Kneser-Ney smoothing,
and actually the Kneser-Ney can be understood as a
heuristic approximation of the HPYLM. This char-
acteristic enables us to build the state-of-the-art lan-
guage model into a more complex generative model.
2.2 Wallach (2006) with HPYLM
Wallach (2006) is a generative model for a docu-
ment collection that combines the topic model with
a Bayesian n-gram language model. The latent
Dirichlet alocation (LDA) (Blei et al, 2003) is the
most basic topic model, which generates each word
in a document based on a unigram word distribution
defined by a topic allocated to that word. The bi-
gram topic model of Wallach (2006) simply replaces
this unigram word distribution (a multinomial) for
each topic with a bigram word distribution 1. In
other words, ordinary LDA generates word condi-
tioning only on the latent topic, whereas the bigram
topic model generates conditioning on both the la-
tent topic and the previous word, as in the bigram
language model. Extending this model with a higher
order n-gram is trivial; all we have to do is to replace
the bigram language model for each topic with an n-
gram language model.
The formal description of the generative story of
this n-gram topic model is as follows. First, for
each topic k ? 1, ? ? ? ,K, where K is the num-
ber of topics, the model generates an n-gram lan-
guage model Gkh.2 These n-gram models are gen-
erated by the PYP, so Gkh ? PYP(a, b,Gkh?) holds.
The model then generate a document collection. For
each document j ? 1, ? ? ? , D, it generates a K-
1This is the model called prior 2 in Wallach (2006); it con-
sistently outperformed the other prior. Wallach used the Dirich-
let language model as each topic, but we only explore the model
with HPYLM because its superiority to the Dirichlet language
model has been well studied (Teh, 2006b).
2We sometimes denote Gkh to represent a language model of
topic k, not a specific multinomial for some context h, depend-
ing on the context.
1181
dimensional topic distribution ?j by a Dirichlet dis-
tribution Dir(?) where ? = (?1, ?2, ? ? ? , ?K) is a
prior. Finally, for each word position i ? 1, ? ? ? , Nj
where Nj is the number of words in document j, i-
th word?s topic assignment zji is chosen according
to ?j , then a word type wji is generated from Gzjihji
where hji is the last n? 1 words preceding wji. We
can summarize this process as follows:
1. Generate topics:
For each h ? ?, {W}, ? ? ? , {W}n?1:
For each k ? 1, ? ? ? ,K:
Gkh ? PYP(a, b,Gkh?)
2. Generate corpora:
For each document j ? 1, ? ? ?D:
?j ? Dir(?)
For each word position i ? 1, ? ? ? , Nj :
zji ? ?j
wji ? Gzjihji
3 Extended Models
One serious drawback of the n-gram topic model
presented in the previous section is sparseness. At
inference, as in LDA, we assign each n-gram a topic,
resulting in an exclusive clustering of n-grams in
the corpora. Roughly speaking, when the number
of topics is K and the number of all n-grams in the
training corpus is N , a language model of topic k,
Gkh is learned using only about O(N/K) instances
of the n-grams assigned the topic k, making each
Gkh much sparser and unreliable distribution.
One way to alleviate this problem is to place an-
other n-gram model, say G0h, which is shared with
all topic-specific n-gram models {Gkh}Kk=1. How-
ever, what is the best way to use this special distribu-
tion? We explore two different approaches to incor-
porate this distribution in the model presented in the
previous section. In one model, the HIERARCHICAL
model, G0h is used as a prior for all other n-gram
models, where G0h exploits global statistics across
all topics {Gkh}. In the other model, the SWITCH-
ING model, no statistics are shared across G0h and
{Gkh}, but some words are directly generated from
G0h regardless of the topic distribution.
3.1 HIERARCHICAL Model
Informally, what we want to do is to establish hier-
archies among the global G0h and other topics {Gkh}.
In Bayesian formalism, we can explain this using an
???
???
???
???
???
???
Figure 1: Variable dependencies of the HIERARCHICAL
model. {u, v} are word types, k is a topic and each Gkh
is a multinomial word distribution. For example, G2uv
represents a word distribution following the context uv
in topic 2.
abstract distribution F as Gkh ? F(G0h). The prob-
lem here is making the appropriate choice for the
distribution F . Each topic word distribution already
has hierarchies among n? 1-gram and n-gram con-
texts as Gkh ? PYP(a, b,Gkh?). A natural solution
to this problem is the doubly hierarchical Pitman-
Yor process (DHPYP) proposed in Wood and Teh
(2009). Using this distribution, the new generative
process of Gkh is
Gkh ? PYP(a, b, ?Gkh? + (1? ?)G0h), (3)
where ? is a new hyperparameter that determines
mixture weight. The dependencies among G0h and
{Gkh} are shown in Figure 1. Note that the genera-
tive process of G0h is the same as the HPYLM (1).
Let us clarify the DHPYP usage differences be-
tween our model and the previous work of Wood and
Teh (2009). A key difference is the problem setting:
Wood and Teh (2009) is aimed at the supervised
adaptation of a language model for a specific do-
main, whereas our goal is unsupervised adaptation.
In Wood and Teh (2009), each Gkh for k ? 1, 2, ? ? ?
corresponds to a language model of a specific do-
main and the training corpus for each k is pre-
specified and fixed. For ease of data sparseness of
domain-specific corpora, latent model G0h exploits
shared statistics amongGkh for k = 1, 2, ? ? ? . In con-
trast, with our model, each Gkh is a topic, so it must
perform the clustering of n-grams in addition to ex-
1182
ploiting the latent G0h. This makes inference harder
and requires more careful design of ?.
Modeling of ? We can better understand the role
of ? in (3) by considering the posterior predictive
form corresponds to (2), which is written as
p(w|h, k,w) = c
k
hw ? atkhw
ckh? + b
+
atkh? + b
ckh? + b
q(w|h, k,w),
(4)
q(w|h, k,w) = ?p(w|h?, k,w) + (1? ?)p(w|h, 0,w),
where c, t with superscript k corresponds to the
count existing in topic k. This shows us that ? de-
termines the back-off behavior: which probability
we should take into account: the shorter context of
the same topic Gkh? or the full context of the global
model G0h. Wood and Teh (2009) shares this vari-
able across all contexts of the same length, for each
k, but this assumption may not be the best. For ex-
ample, after the context ?in order?, we can predict
the word ?to? or ?that?, and this tendency is unaf-
fected by the topic. We call this property of context
the topicality and say that ?in order? has weak topi-
cality. Therefore, we place ? as a distinct value for
each context h, which we share across all topics. We
designate this ? determined by h ?h in the follow-
ing. Moreover, similar contexts may have similar
values of ?h. For example, the two contexts ?of the?
and ?in the?, which share the suffix ?the?, both have
a strong topicality3. We encode this assumption by
placing hierarchical Beta distributions on the suffix
tree across all topics:
?h ? Beta(??h? , ?(1? ?h?)) = DP(?, ?h?), (5)
where DP is the hierarchical Dirichlet process (Teh
et al, 2006), which has only two atoms in {0,1} and
? is a concentration parameter. As in HPYLM, we
place a uniform prior ?0 = 1/2 on the base distribu-
tion of the top node (?? ? DP(?, ?0)).
Having generated the topic component of the
model, the corpus generating process is the same as
the previous model because we only change the gen-
erating process of Gkh for k = 1, ? ? ? ,K.
3These words can be used very differently depending on the
context. For example, in a teen story, ?in the room? or ?in the
school? seems more dominant than ?in the corpora? or ?in the
topic?, which is likely to appear in this paper.
3.2 SWITCHING Model
Our second extension also exploits the globalG0h, al-
beit differently than the HIERARCHICAL model. In
this model, the relationship of G0h to the other {Gkh}
is flat, not hierarchical: G0h is a special topic that can
generate a word. The model first generates each lan-
guage model of k = 0, 1, 2, ? ? ? ,K independently
as Gkh ? PYP(a, b,Gkh?). When generating a word,
it first determines whether to use global model G0h
or topic model {Gkh}Kk=1. Here, we use the ?h in-
troduced above in a similar way: the probability of
selecting k = 0 for the next word is determined by
the previous context. This assumption seems natu-
ral; we expect theG0h to mainly generate common n-
grams, and the topicality of each context determines
how common that n-gram might be. The complete
generative process of this model is written as fol-
lows:
1. Generate topics:
For each h ? ?, {V }, ? ? ? , {V }n?1:
?h ? DP(?, ??h)
For each k ? 0, ? ? ? ,K:
Gkh ? PYP(a, b,Gkh?)
2. Generate corpora:
For each document j ? 1, ? ? ?D:
?j ? Dir(?)
For each word position i ? 1, ? ? ? , Nj :
lji ? Bern(?hji)
If lji = 0: zji = 0
If lji = 1: zji ? ?j
wji ? Gzjihji
The difference between the two models is their
usage of the global model G0h. For a better under-
standing of this, we provide a comparison of their
graphical models in Figure 2.
4 Inference
For posterior inference, we use the collapsed Gibbs
sampler. In our models, all the latent variables are
{Gkh, ?h, ?j , z,?}, where z is the set of topic assign-
ments and ? = {a, b, ?,?} are hyperparameters,
which are treated later. We collapse all multinomials
in the model, i.e., {Gkh, ?h, ?j}, in which Gkh and ?h
are replaced with the Chinese restaurant process of
PYP and DP respectively. Given the training corpus
w, the target posterior distribution is p(z,S|w,?),
where S is the set of seating arrangements of all
restaurants. To distinguish the two types of restau-
rant, in the following, we refer the restaurant to indi-
1183
(a) HIERARCHICAL (b) SWITCHING
Figure 2: Graphical model representations of our two models in the case of a 3-gram model. Edges that only exist in
one model are colored.
cate the collapsed state of Gkh (PYP), while we refer
the restaurant of ?h to indicates the collapsed state
of ?h (DP). We present two different types of sam-
pler: a token-based sampler and a table-based sam-
pler. For both samplers, we first explain in the case
of our basic model (Section 2.2), and later discuss
some notes on our extended models.
4.1 Token-based Sampler
The token-based sampler is almost identical to
the collapsed sampler of the LDA (Griffiths and
Steyvers, 2004). At each iteration, we consider the
following conditional distribution of zji given all
other topic assignments z?ji and S?ji, which is the
set of seating arrangements with a customer corre-
sponds to wji removed, as
p(zji|z?ji,S?ji) ? p(zji|z?ji)p(wji|zji, hji,S?ji),
(6)
where p(wji|zji, hji,S?ji) =
ckhw ? atkhw
ckh? + b
+
atkh? + b
ckh? + b
p(wji|zji, hji,S?ji) (7)
is a predictive word probability under the topic zji,
and
p(zji|z?ji) =
n?jijk + ?k
Nj ? 1 +
?
k? ?k?
, (8)
where n?jijk is the number of words that is assigned
topic k in document j excluding wji, which is the
same as the LDA. Given the sampled topic zji, we
update the language model of topic zji, by adding
customer wji to the restaurant specified by zji and
context hji. See Teh (2006a) for details of these cus-
tomer operations.
HIERARCHICAL Adding customer operation is
slightly changed: When a new table is added to a
restaurant, we must track the label l ? {0, 1} indi-
cating the parent restaurant of that table, and add the
customer corresponding to l to the restaurant of ?h.
See Wood and Teh (2009) for details of this opera-
tion.
SWITCHING We replace p(zji|z?ji) with
p(zji|z?ji) =
?
?
?
p(lji = 0|hji) (zji = 0)
p(lji = 1|hji) ?
n?jijk +?k
?
k 6=0 n
?ji
jk +
?
k? ?k?
(zji 6= 0),
(9)
where p(lji|hji) is a predictive of lji given by the
CRP of ?hji . We need not assign a label to a new
table, but rather we always add a customer to the
restaurant of ?h according to whether the sampled
topic is 0 or not.
4.2 Table-based Sampler
One problem with the token-based sampler is that
the seating arrangement of the internal restaurant
would never be changed unless a new table is cre-
ated (or an old table is removed) in its child restau-
rant. This probability is very low, particularly in
the restaurants of shallow depth (e.g., unigram or
1184
vConstruct a block
Move the block to the sampled topic
: customer
: table
Figure 3: Transition of the state of restaurants in the
table-based sampler when the number of topics is 2.
{u, v, w} are word types. Each box represents a restau-
rant where the type in the upper-right corner indicates the
context. In this case, we can change the topic of the three
3-grams (vvw, vvw, uvw) in some documents from 1 to
2 at the same time.
bigram restaurants) because these restaurants have
a larger number of customers and tables than those
of deep depth, leading to get stack in undesirable
local minima. For example, imagine a table in
the restaurant of context ?hidden? (depth is 2) and
some topic, served ?unit?. This table is connected
to tables in its child restaurants corresponding to
some 3-grams (e.g., ?of hidden unit? or ?train hid-
den unit?), whereas similar n-grams, such as those
of ?of hidden units? or ?train hidden units? might
be gathered in another topic, but collecting these n-
grams into the same topic might be difficult under
the token-based sampler. The table-based sampler
moves those different n-grams having common suf-
fixes jointly into another topic.
Figure 3 shows a transition of state by the table-
based sampler and Algorithm 4.2 depicts a high-
level description of one iteration. First, we select
a table in a restaurant, which is shown with a dotted
line in the figure. Next, we descend the tree to col-
lect the tables connected to the selected table, which
are pointed by arrows. Because this connection can-
not be preserved in common data structures for a
restaurant described in Teh (2006a) or Blunsom et
al. (2009), we select the child tables randomly. This
is correct because customers in CRP are exchange-
Algorithm 1 Table-based sampler
for all table in all restaurants do
Remove a customer from the parent restaurant.
Construct a block of seating arrangement S by de-
scending the tree recursively.
Sample topic assignment zS ? p(zS |S,S?S , z?S).
Move S to sampled topic, and add a customer to the
parent restaurant of the first selected table.
end for
able, so we can restore the parent-child relations ar-
bitrarily. We continue this process recursively until
reaching the leaf nodes, obtaining a block of seat-
ing arrangement S. After calculating the conditional
distribution, we sample new topic assignment for
this block. Finally, we move this block to the sam-
pled topic, which potentially changes the topic of
many words across different documents, which are
connected to customers in a block at leaf nodes (this
connection is also arbitrary).
Conditional distribution Let zS be the block of
topic assignments connected to S and zS be a vari-
able indicating the topic assignment. Thanks to the
exchangeability of all customers and tables in one
restaurant (Teh, 2006a), we can imagine that cus-
tomers and tables in S have been added to the restau-
rants last. We are interested in the following condi-
tional distribution: (conditioning ? is omitted)
p(zS = k?|S,S?S , z?S) ? p(S|S?S , k?)p(zS = k?|z?S),
where p(S|S?S , k?) is a product of customers? ac-
tions moving to another topic, which can be decom-
posed as:
p(S|S?S , k?) = p(w|k?, h)
?
s?S
p(s|k?) (10)
p(s|k?) =
?ts?1
i=0 (b+a(t
k?(?s)
hsw
+i))
?csi
j=1(j?a)
(b+ck
?(?s)
hsw?
)cs?
(11)
?
?ts?1
i=0 (b+a(t
k?(?s)
hsw
+i))
(b+ck
?(?s)
hsw?
)cs?
. (12)
Let us define some notations used above. Each
s ? S is a part of seating arrangements in a restau-
rant, there being ts tables, i-th of which with csi
customers, with hs as the corresponding context. A
restaurant of context h and topic k has tkhw tables
served dish w, i-th of which with ckhwi customers.
Superscripts ?s indicate excluding the contribution
1185
of customers in s, and xn = x(x+1) ? ? ? (x+n?1)
is the ascending factorial. In (10) p(w|k?, h) is the
parent distribution of the first selected table, and
the other p(s|k?) is the seating arrangement of cus-
tomers. The likelihood for changing topic assign-
ments across documents must also be considered,
which is p(zS = k?|z?S) and decomposed as:
p(zS = k?|z?S) =
?
j
(n?S
jk?
+?k? )
nj(S)
(N?Sj +
?
k ?k)
nj(S)
, (13)
where nj(S) is the number of word tokens con-
nected with S in document j.
HIERARCHICAL We skip tables on restaurants of
k = 0, because these tables are all from other topics
and we cannot construct a block. The effects of ?
can be ignored because these are shared by all topics.
SWITCHING In the SWITCHING, p(zS = k?|z?S)
cannot be calculated in a closed form because
p(lji|hji) in (9) would be changed dynamically
when adding customers. This problem is the same
one addressed by Blunsom and Cohn (2011), and we
follow the same approximation in which, when we
calculate the probability, we fractionally add tables
and customers recursively.
4.3 Inference of Hyperparameters
We also place a prior on each hyperparameter and
sample value from the posterior distribution for ev-
ery iteration. As in Teh (2006a), we set different
values of a and b for each depth of PYP, but share
across all topics and sample values with an auxiliary
variable method. We also set different value of ? for
each depth, on which we place Gamma(1, 1). We
make the topic prior ? asymmetric: ? = ??0;? ?
Gamma(1, 1),?0 ? Dir(1).
5 Related Work
HMM-LDA (Griffiths et al, 2005) is a composite
model of HMM and LDA that assumes the words
in a document are generated by HMM, where only
one state has a document-specific topic distribution.
Our SWITCHING model can be understood as a lex-
ical extension of HMM-LDA. It models the topical-
ity by context-specific binary random variables, not
by hidden states. Other n-gram topic models have
focused mainly on information retrieval. Wang et
min. training set test set
Corpus appear # types # docs # tokens # docs # tokens
Brown 4 19,759 470 1,157,225 30 70,795
NIPS 4 22,705 1500 5,088,786 50 167,730
BNC 10 33,071 6,162 12,783,130 100 202,994
Table 1: Corpus statistics after the pre-processing: We
replace words appearing less than min.appear times in
training + test documents, or appearing only in a test set
with an unknown token. All numbers are replaced with
#, while punctuations are remained.
al. (2007) is a topic model on automatically seg-
mented chunks. Lindsey et al (2012) extended this
model with the hierarchical Pitman-Yor prior. They
also used switching variables, but for a different pur-
pose: to determine the segmenting points. They treat
these variables completely independently, while our
model employs a hierarchical prior to share statisti-
cal strength among similar contexts.
Our primary interest is language model adapta-
tion, which has been studied mainly in the area of
speech processing. Conventionally, this adaptation
has relied on a heuristic combination of two sep-
arately trained models: an n-gram model p(w|h)
and a topic model p(w|d). The unigram rescal-
ing, which is a product model of these two mod-
els, perform better than more simpler models such
as linear interpolation (Gildea and Hofmann, 1999).
There are also some extensions to this method (Tam
and Schultz, 2009; Huang and Renals, 2008), but
these methods have one major drawback: at predic-
tion, the rescaling-based method requires normaliza-
tion across vocabulary at each word, which prohibits
use on applications requiring dynamic (incremental)
adaptation, e.g., settings where we have to update
the topic distribution as new inputs come in. Tam
and Schultz (2005) studied on this incremental set-
tings, but they employ an interpolation. The practi-
cal interest here is whether our Bayesian models can
rival the rescaling-based method in terms of predic-
tion power. We evaluate this in the next section.
6 Experiments
6.1 Settings
We test the effectiveness of presented models and
the blocked sampling method on unsupervised lan-
guage model adaptation settings. Specifically we
1186
0 2 4 6 8time (hr.)7.3e+06
7.5e+067.7e+067.9e+06
8.1e+06
negativel
og-likeliho
od
(a) Brown
0 8 16 24 32time (hr.)2.9e+07
3.1e+073.3e+073.5e+07
3.7e+07
negativel
og-likeliho
od
(b) NIPS
0 15 30 45 60time (hr.)8.0e+07
8.3e+078.6e+078.9e+07
9.2e+07
negativel
og-likeliho
od 3-gram Hpytmtoken3-gram Hpytm4-gram Hpytmtoken4-gram Hpytm
(c) BNC
10 50 100# topics205210
215220225230
235240245
testperpl
exity
(d) Brown
10 50 100# topics100105
110115120
125
testperpl
exity
(e) NIPS
10 50 100# topics130140
150160170
180190
testperpl
exity
HpylmHpytmtokenHpytmRescalingSwitchingHierarchical
(f) BNC
Figure 4: (a)?(c): Comparison of negative log-likelihoods at training of HPYTM (K = 50). Lower is better. HPYTM
is trained on both token- and table-based samplers, while HPYTMtoken is trained only on the token-based sampler.
(d)?(f): Test perplexity of various 3-gram models as a function of number of topics on each corpus.
concentrate on the dynamic adaptation: We update
the posterior of language model given previously ob-
served contexts, which might be decoded transcripts
at that point in ASR or MT.
We use three corpora: the Brown, BNC and NIPS.
The Brown and BNC are balanced corpora that con-
sist of documents of several genres from news to
romance. The Brown corpus comprises 15 cate-
gories. We selected two documents from each cate-
gory for the test set, and use other 470 documents for
the training set. For the NIPS, we randomly select
1,500 papers for training and 50 papers for testing.
For BNC, we first randomly selected 400 documents
from a written corpus and then split each document
into smaller documents every 100 sentences, leading
to 6,262 documents, from which we randomly se-
lected 100 documents for testing, and other are used
for training. See Table 1 for the pre-processing of
unknown types and the resulting corpus statistics.
For comparison, besides our proposed HIERAR-
CHICAL and SWITCHING models, we prepare vari-
ous models for baseline. HPYLM is a n-gram lan-
guage model without any topics. We call the model
without the global G0h introduced in Section 2.2
HPYTM. To see the effect of the table-based sam-
pler, we also prepare HPYTMtoken, which is trained
only on the token-based sampler. RESCALING is
the unigram rescaling. This is a product model of
an n-gram model p(w|h) and a topic model p(w|d),
where we learn each model separately and then com-
bine them by:
p(w|h, d) ?
(p(w|d)
p(w)
)?
p(w|h). (14)
We set ? in (14) to 0.7, which we tuned with the
Brown corpus.
6.2 Effects of Table-based Sampler
We first evaluate the effects of our blocked sam-
pler at training. For simplicity, we concentrate on
the HPYTM with K = 50. Table 4(a)?(c) shows
negative likelihoods of the model during training.
On all corpora, the model with the table-based sam-
pler reached the higher probability space with much
faster speed on both 3-gram and 4-gram models.
1187
6.3 Perplexity Results
Training For burn-in, we ran the sampler as fol-
lows: For HPYLM, we ran 100 Gibbs iterations. For
RESCALING, we ran 900 iterations on LDA and 100
iterations on HPYLM. For all other models, we ran
500 iterations of the Gibbs; HPYTMtoken is trained
only on the token-based sampler, while for other
models, the table-based sampler is performed after
the token-based sampler.
Evaluation We have to adapt to the topic dis-
tribution of unseen documents incrementally. Al-
though previous works have employed incremental
EM (Gildea and Hofmann, 1999; Tam and Schultz,
2005) because their inference is EM/VB-based, we
use the left-to-right method (Wallach et al, 2009),
which is a kind of particle filter updating the poste-
rior topic distribution of a test document. We set the
number of particles to 10 and resampled each parti-
cle every 10 words for all experiments. To get the
final perplexity, after burn-in, we sampled 10 sam-
ples every 10 iterations of Gibbs, calculated a test
perplexity for each sample, and averaged the results.
Comparison of 3-grams Figure 4(d)?(f) shows
perplexities when varying the number of top-
ics. Generally, compared to the HPYTMtoken, the
HPYTM got much perplexity gains, which again
confirm the effectiveness of our blocked sampler.
Both our proposed models, the HIERARCHICAL and
the SWITCHING, got better performances than the
HPYTM, which does not place the global model
G0h. Our SWITCHING model consistently performed
the best. The HIERARCHICAL performed somewhat
worse than the RESCALING when K become large,
but the SWITCHING outperformed that.
Comparison of 4-grams and beyond We sum-
marize the results with higher order n-grams in Ta-
ble 2, where we also show the time for prediction.
We fixed the number of topics K = 100 because
we saw that all models but HPYTMtoken performed
best at K = 100 when n = 3. Generally, the
results are consistent with those of n = 3. The
models with n = ? indicate a model extension
using the Bayesian variable-order language model
(Mochihashi and Sumita, 2008), which can naturally
be integrated with our generative models. By this
extension, we can prune unnecessary nodes stochas-
NIPS BNC
Model n PPL time PPL time
HPYLM 4 117.2 59 169.2 74
HPYLM ? 117.9 61 173.1 59
RESCALING 4 101.4 19009 130.3 36323
HPYTM 4 107.0 1004 133.1 980
HPYTM ? 107.2 1346 133.6 1232
HIERARCHICAL 4 106.3 1038 129.0 993
HIERARCHICAL ? 105.7 1337 129.3 1001
SWITCHING 4 100.0 1059 125.5 991
SWITCHING ? 100.4 1369 125.7 1006
Table 2: Comparison of perplexity and the time require
for prediction (in seconds). The number of topics is fixed
to 100 on all topic-based models.
tically during training. We can see that this ?-
gram did not hurt performances, but the sampled
model get much more compact; in BNC, the number
of nodes of the SWITCHING with 4-gram is about
7.9M, while the one with ?-gram is about 3.9M.
Note that our models require no explicit normaliza-
tion, thereby drastically reducing the time for pre-
diction compared to the RESCALING. This differ-
ence is especially remarkable when the vocabulary
size becomes large.
We can see that our SWITCHING performed con-
sistently better than the HIERARCHICAL. One rea-
son for this result might be the mismatch of pre-
diction of the topic distribution in the HIERARCHI-
CAL. The HIERARCHICAL must allocate some (not
global) topics to every word in a document, so even
the words to which the SWITCHING might allocate
the global topic (mainly function words; see below)
must be allocated to some other topics, causing a
mismatch of allocations of topic.
6.4 Qualitative Results
To observe the behavior in which the SWITCHING
allocates some words to the global topic, in Figure
5, we show the posterior of allocating the topic 0
or not at each word in a part of the NIPS training
corpus. We can see that the model elegantly identi-
fied content and function words, learning the topic
distribution appropriately using only semantic con-
texts. These same results in the HIERARCHICAL are
presented in Table 3, where we show some relations
between ?h and context h. Contexts that might be
likely to precede nouns have a higher value of ?h,
1188
there has been much recent work on measuring image statistics
and on learning probability distributions on images . we observe
that the mapping from images to statistics is many-to-one and
show it can be quantified by a phase space factor .
Figure 5: The posterior for assigning topic 0 or not in
NIPS by the ?-gram SWITCHING. Darker words indi-
cate a higher probability of not being assigned topic 0.
?h h
0.0?0.1 in spite, were unable, a sort, on behalf, . regardless
0.5?0.6 assumed it, rand mines, plans was, other excersises
0.9?1.0 that the, the existing, the new, their own, and spatial
Table 3: Some contexts h for various values of ?h in-
duced by the 3-gram HIERARCHICAL in BNC.
while prefixes of idioms have a lower value. The?-
gram extension gives us the posterior of n-gram or-
der p(n|h), which can be used to calculate the proba-
bility of a word ordering composing a phrase in topic
k as p(w, n|k, h) ? p(n|h)p(w|k, n, h). In Table
4, we show some higher probability topic-specific
phrases from the model trained on the NIPS.
7 Conclusion
We have presented modeling and algorithmic con-
tributions to the existing Bayesian n-gram topic
model. We explored two different priors to incor-
porate a global model, and found the effectiveness
of the flat structured model. We developed a novel
blocked Gibbs move for these types of models to ac-
celerate inference. We believe that this Gibbs op-
eration can be incorporated with other models hav-
ing a similar hierarchical structure. Empirically, we
demonstrate that by a careful model design and effi-
cient inference, a well-defined Bayesian model can
rival the conventional heuristics.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
0 46
according to ? support vectors
? ( # ) in high dimentional
? section # as decision function
techniques such as set of # observations
? ( b ) original data set
83 89
the hierarchical mixtures ? linear discriminant
the rbf units images per class
the gating networks multi-class classification
grown hme ? decision boundaries
the modular architecture references per class
Table 4: Topical phrases from NIPS induced by the ?-
gram SWITCHING model. ? is a symbol for the beginning
of a sentence and # represents a number.
Phil Blunsom, Trevor Cohn, Sharon Goldwater, and Mark
Johnson. 2009. A note on the implementation of hi-
erarchical dirichlet processes. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
337?340, Suntec, Singapore, August. Association for
Computational Linguistics.
Daniel Gildea and Thomas Hofmann. 1999. Topic-based
language models using em. In In Proceedings of EU-
ROSPEECH, pages 2167?2170.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl 1):5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In In Advances in Neural Information Pro-
cessing Systems 17, pages 537?544. MIT Press.
Songfang Huang and Steve Renals. 2008. Unsupervised
language model adaptation based on topic and role in-
formation in multiparty meetings. In in Proc. Inter-
speech08, pages 833?836.
F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. 1991.
A dynamic language model for speech recognition. In
Proceedings of the workshop on Speech and Natural
Language, HLT ?91, pages 293?295, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert Lindsey, William Headden, and Michael Stipice-
vic. 2012. A phrase-discovering topic model using hi-
erarchical pitman-yor processes. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 214?222, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Daichi Mochihashi and Eiichiro Sumita. 2008. The infi-
nite markov model. In J.C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1017?1024. MIT
Press, Cambridge, MA.
1189
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers - Volume 1, pages
959?968. Association for Computational Linguistics.
Yik-Cheung Tam and Tanja Schultz. 2005. Dynamic lan-
guage model adaptation using variational bayes infer-
ence. In INTERSPEECH, pages 5?8.
Yik-Cheung Tam and Tanja Schultz. 2009. Correlated
bigram lsa for unsupervised language model adapta-
tion. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, editors, Advances in Neural Information
Processing Systems 21, pages 1633?1640.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985?
992, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, ICML
?09, pages 1105?1112, New York, NY, USA. ACM.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the 23rd international
conference on Machine learning, ICML ?06, pages
977?984.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings
of the 2007 Seventh IEEE International Conference on
Data Mining, ICDM ?07, pages 697?702, Washington,
DC, USA. IEEE Computer Society.
Frank Wood and Yee Whye Teh. 2009. A hierarchi-
cal nonparametric Bayesian approach to statistical lan-
guage model domain adaptation. In Proceedings of the
International Conference on Artificial Intelligence and
Statistics, volume 12.
1190
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1374?1384,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Formalizing Word Sampling for Vocabulary Prediction
as Graph-based Active Learning
Yo Ehara
?
National Institute of
Information and Communications
Technology
ehara@nict.go.jp
Yusuke Miyao
National Institute of
Informatics
yusuke@nii.ac.jp
Hidekazu Oiwa
Issei Sato
Hiroshi Nakagawa
The University of Tokyo
{oiwa,sato}@r.dl.itc.u-tokyo.ac.jp
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
Predicting vocabulary of second language
learners is essential to support their lan-
guage learning; however, because of the
large size of language vocabularies, we
cannot collect information on the entire
vocabulary. For practical measurements,
we need to sample a small portion of
words from the entire vocabulary and pre-
dict the rest of the words. In this study, we
propose a novel framework for this sam-
pling method. Current methods rely on
simple heuristic techniques involving in-
flexible manual tuning by educational ex-
perts. We formalize these heuristic tech-
niques as a graph-based non-interactive
active learning method as applied to a spe-
cial graph. We show that by extending the
graph, we can support additional function-
ality such as incorporating domain speci-
ficity and sampling from multiple corpora.
In our experiments, we show that our ex-
tended methods outperform other methods
in terms of vocabulary prediction accuracy
when the number of samples is small.
1 Introduction
Predicting the vocabulary of second language
learners is essential to support them when they are
reading. Educational experts have been continu-
ously studying methods for measuring the size of
a learner?s vocabulary, i.e., the number of words
?
The main body of this work was done when the first
author was a Ph.D. candidate in the University of Tokyo and
the paper was later greatly revised when the first author was
a JSPS (Japan Society for the Promotion of Science) research
fellow (PD) at National Institute of Informatics. See http:
//yoehara.com/ for details.
the learner knows, over the decades (Meara and
Buxton, 1987; Laufer and Nation, 1999). Ehara
et al. (2012) formalized a more fine-grained mea-
surement task called vocabulary prediction. The
goal of this task is to predict whether a learner
knows a given word based on only a relatively
small portion of his/her vocabulary. This vocabu-
lary prediction task can be further used for predict-
ing the readability of texts. By predicting vocab-
ulary unknown to readers and showing the mean-
ing of those specific words to readers, Ehara et al.
(2013) showed that the number of documents that
learners can read increases.
Word sampling is essential for vocabulary pre-
diction. Because of the large size of language vo-
cabularies, we usually cannot collect information
on the entire vocabulary. For practical measure-
ments, we inevitably need to sample a small por-
tion of words from the entire vocabulary and then
predict the rest. We refer to this sampling tech-
nique as word sampling.
Word sampling can greatly affect the perfor-
mance of vocabulary prediction. For example, if
we consider only short everyday general domain
words such as ?cat? and ?dog? as samples, the rest
of the vocabulary is difficult to predict since learn-
ers likely know most of these words. To more ac-
curately measure a learner?s vocabulary, we ide-
ally must sample words that are representative of
the entire set of words. More specifically, we wish
to sample words such that if a learner knows these
words, he/she is likely to know the rest of the
words in the given vocabulary, and vice versa.
To our knowledge, however, all current studies
have relied on a simple heuristic method. In this
heuristic method, educational experts first some-
how create groups of words with the aim that the
words in a group are of similar difficulty for learn-
1374
ers. To create groups of words, the experts typi-
cally make use of word frequencies and sometimes
manually reclassify words based on experience.
Next, a fixed number of words are randomly sam-
pled from each group via a uniform distribution.
We call this approach heuristic word sampling.
In this study, we propose a novel framework
that formalizes word sampling as non-interactive
graph-based active learning based on weighted
graphs. In our approach, nodes of a graph corre-
spond to words, whereas the edge weights show
how similar the difficulty levels of a word pair
are. Unlike interactive active learning algorithms
used in the NLP community, which use expert an-
notators? human labels for sampling nodes, non-
interactive active learning algorithms exclude ex-
pert annotators? human labels from the protocol
(Ji and Han, 2012; Gu and Han, 2012). Given
a weighted graph and using only its structure,
without human labels, these algorithms sample
nodes that are important for classification with al-
gorithms called label propagation. Excluding an-
notators? human labels from the protocol is bene-
ficial for educational purposes since learners can
share the same set of sampled words via, for ex-
ample, printed handouts.
Formalizing the current methods as non-
interactive graph-based active learning enables us
to extend the sampling methods with additional
functionality that current methods cannot han-
dle without applying burdensome manual heuris-
tics because we can flexibly design the weighted
graphs fed to the active learning algorithms. In our
framework, this extension is achieved by extend-
ing the graph, namely, our framework can handle
domain specificity and multiple corpora.
Domains are important when one wants to mea-
sure the vocabulary of learners. For example, con-
sider measuring non-native English speakers tak-
ing computer science graduate courses. We may
want to measure their English vocabulary with an
emphasis on computer science rather than their
general English vocabulary. However, such an
extension is impossible via current methods, and
thus it is desirable to sample algorithms to be able
to handle domain specificity. Our framework can
incorporate domain specificity between words in
the form of edges between such words.
Handling multiple corpora is important when
we cannot single out which corpus we should rely
on. The current technique used by educational
experts to handle multiple corpora is to heuristi-
cally integrate multiple frequency lists from mul-
tiple corpora into a single list of words; however,
such manual integration is burdensome. Thus, au-
tomatic integration is desirable. Our framework
converts multiple corpora into graphs, merges
these graphs together, and then samples from the
merged graph.
Our contributions as presented in this paper are
summarized as follows:
1. We formalize word sampling for vocabulary
prediction as graph-based active learning.
2. Based on this formalization, we can perform
more flexible word sampling that can handle
domain specificity and multiple corpora.
The remaining parts of this paper are orga-
nized as follows. In ?2, we explain the problem
setting in detail. We first explain how existing
heuristic word sampling works and how it relies
on the cluster assumption from the viewpoint of
graphs. Then, we introduce existing graph-based
non-interactive active learning methods. In ?3,
we show that the existing heuristic word sampling
is merely a special case of a non-interactive ac-
tive learning method (Gu and Han, 2012). Pre-
cisely, the existing sampling is identical to the case
where a special graph called a ?multi-complete
graph? is fed to a non-interactive active learning
method. Since this method can take any weighted
graphs other than this special graph, this imme-
diately leads to a way of devising new sampling
methods by modifying graphs. ?4 explains exactly
how we can modify graphs for improving active
learning. ?5 evaluates the proposed method both
quantitatively and qualitatively, and ?6 concludes
our paper.
2 Problem Setting
2.1 Heuristic Word Sampling
A simple vocabulary estimation technique intro-
duced by educational experts is to use the fre-
quency rank of words in a corpus based on the
assumption that learners using words with similar
frequency ranks have a similar vocabulary (Laufer
and Nation, 1999). In accordance with this as-
sumption, they first group words by frequency
ranks in a corpus and then assume that words in
each group have a similar vocabulary status. For
example, they sampled words as follows:
1375
1. Rank words by frequency in a corpus.
2. Group words with frequency ranks from 1 to
1, 000 as Level 1000, words with frequency
ranks from 1, 001 to 2, 000 as Level 2000,
and so on.
3. Take 18 samples from Level 1000, another 18
samples from Level 2000, and so on.
The rationale behind this method is to treat
high-ranked and low-ranked words separately
rather than sample words from the entire vocabu-
lary. After sampling words, this sampling method
can be used for various measurements; for exam-
ple, Laufer and Nation (1999) used this method
to estimate the size of the learners? vocabulary
by simply adding 1, 000 ?
Correctly answered words
18
for
each level.
2.2 Cluster Assumption
In the previous subsection, we noted that existing
word sampling methods rely on the assumption
that words with similar frequency ranks are known
to learners whose familiar words are similar each
other. This assumption is known as the cluster as-
sumption in the field of graph studies (Zhou et al.,
2004).
To further describe the cluster assumption, we
first define graphs. A graph G = (V, E) consists
of a set of nodes (vertices) V and a set of edges E .
Here, each node has a label, and each edge has a
weight. A label denotes the category of its corre-
sponding node. For example, in binary classifica-
tion, a label is taken from {+1,?1}. A weight is
a real value; when the weight of an edge is large,
we describe the edge as being heavy.
The cluster assumption is an assumption that
heavily connected nodes in a graph should have
similar labels. In other words, the cluster as-
sumption states that weights of edges and labels
of nodes should be consistent.
We explain how the cluster assumption relates
to our task. In our application, each node corre-
sponds to a word. Labels of the nodes in a graph
denote the vocabulary of a learner. If he/she knows
a word, the label of the node corresponding to the
word is +1; if not, the label is ?1. The cluster
assumption in our application is that the heavier
the edge, the higher the similarity between users
familiar with the two words.
In this manner, existing word sampling meth-
ods implicitly assume cluster assumption. This
is therefore the underlying approach for reducing
the word sampling problem into graph-based ac-
tive learning. Since graphs allow for more flexible
modeling by changing the weights of edges, we
expect that more flexible word sampling will be
enabled by graph-based active learning.
2.3 Label Propagation
Since the graph-based active learning algorithms
are based on label propagation algorithms, we will
explain them first. Basically, given a weighted
graph, label propagation algorithms classify their
nodes in a weakly supervised manner. While the
graph-based active learning algorithm that we are
trying to use (Gu and Han, 2012) does not use la-
bel propagation algorithms? outputs directly, it is
tuned to be used with a state-of-the-art label prop-
agation method called Learning with Local and
Global Consistency (LLGC) (Zhou et al., 2004).
Label propagation algorithms predict the labels
of nodes from a few manually supervised labels
and graph weights. To this end, label propaga-
tion algorithms follow the following steps. First,
humans label a small subset of the nodes in the
graph. This subset of nodes is called the set of la-
beled nodes, and the remaining nodes are called
unlabeled nodes. Second, label propagation al-
gorithms propagate labels to the unlabeled nodes
based on edge weights. The rationale behind la-
bel propagation algorithms lies in cluster assump-
tion; as label propagation algorithms assume that
two nodes connected by a heavily weighted edge
should have similar labels, more heavily weighted
edges should propagate more labels.
We formalize Learning with Local and Global
Consistency (LLGC) (Zhou et al., 2004), one
of the state-of-the-art label propagation methods.
Here, for simplicity, suppose that we want to per-
form binary classification of nodes. Let N be the
total number of nodes in a graph. Then, we de-
note labels of each node by y
def
= (y
1
, . . . , y
N
)
>
.
For unlabeled nodes, y
i
is set to 0. For labeled
nodes, y
i
is set to +1 if the learner knows a word,
?1 if not. We also introduce a label propagation
(LP) score vector f = (f
1
, . . . , f
N
)
>
. This LP
score vector is the output of label propagation and
is real-valued. To obtain the classification result
from this real-valued LP score vector for an un-
labeled node (word) i, the learner is predicted to
know the word i if f
i
> 0, and he/she is predicted
to be unfamiliar with the word if f
i
? 0.
1376
Next, we formally define a normalized graph-
Laplacian matrix, which is used for penalization
based on the cluster assumption. Let an N ? N
-sized square matrix W be a weighted adjacency
matrix of G. W is symmetric and non-negative
definite; its diagonal elements W
i,i
= 0 and
all other elements are non-negative
1
. The graph
Laplacian of a normalized graph, known as a nor-
malized graph Laplacian matrix, is defined as
L
norm
W
def
= I?D
?
1
2
W
WD
?
1
2
W
. Here, D
W
is defined
as a diagonal matrix whose diagonal element is
(D
W
)
i,i
def
=
?
|V|
j=1
W
i,j
, and I denotes the iden-
tity matrix of the appropriate size. Note that a
normalized graph Laplacian L
norm
W
depends on the
weighted adjacency matrix W.
Then, LLGC can be formalized as a simple op-
timization problem as shown in Equation 1.
min
f
?f ? y?
2
2
+ ?f
>
L
norm
W
f (1)
Equation 1 consists of two terms. Intuitively,
the first term tries to make the LP score vector, the
final output f , as close as possible to the given la-
bels y. The second term is designed to meet the
cluster assumption: it penalizes the case where
two nodes with heavy edges have very different
LP scores. ? > 0 is the only hyper-parameter of
LLGC: it determines how strong the penalization
based on the cluster assumption should be. Thus,
in total, Equation 1 outputs an LP score vector f
considering both the labeled input y and the clus-
ter assumption of the given graph W: the heav-
ier an edge, the closer the scores of the two nodes
connected by the edge becomes.
2.4 Graph-based active learning algorithms
An important categorization of graph-based active
learning for applications is whether it is interactive
or non-interactive. Here, interactive approaches
use human labels during the learning process; they
present a node for humans to label, and based on
this label, the algorithms compute the next node to
be presented to the humans. Thus, in interactive
algorithms, human labeling and computations of
the next node must run concurrently.
Non-interactive algorithms do not use human
labels during the learning process. Given the
entire graph, these algorithms sample important
1
While all elements of a non-negative definite matrix are
not necessarily non-negative, we define all elements of W
as non-negative here, following the definition of Zhou et al.
(2004).
nodes for label propagation algorithms. Here, im-
portant nodes are the ones that minimize estimated
classification error of label propagation when the
nodes are labeled. Note that, unlike active learning
used in the NLP community, non-interactive active
learning algorithms exclude expert annotators? hu-
man labels from the protocol. While they exclude
expert annotators, they are still regarded as active
learning methods in the machine learning commu-
nity since they try to choose such nodes that are
beneficial for classification (Ji and Han, 2012; Gu
and Han, 2012).
For educational purposes, non-interactive algo-
rithms are preferred over interactive algorithms.
The main drawback of interactive algorithms is
that they must run concurrently with the hu-
man labeling. For our applications, this means
that the vocabulary tests for vocabulary prediction
must always be computerized. In contrast, non-
interactive algorithms allow us to have vocabulary
tests printed in the form of handouts, so we focus
on non-interactive algorithms throughout this pa-
per.
Compared with interactive algorithm studies,
such as Zhu et al. (2003), graph-based non-
interactive active learning algorithms have been
introduced in recent years. There has been a sem-
inal paper on non-interactive algorithms (Ji and
Han, 2012). We used Gu and Han?s algorithm be-
cause it reports higher accuracy for many tasks
with competitive computation times over Ji and
Han?s algorithm (Gu and Han, 2012).
These active learning methods share two basic
rules although their objective functions are dif-
ferent. First, these methods tend to select glob-
ally important nodes, also known as hubs. A no-
table example of global importance is the num-
ber of edges. Second, these methods tend to
avoid sampling nodes that are heavily connected
to previously sampled nodes. This is due to clus-
ter assumption, the assumption that similar nodes
should have similar labels, which suggests that it is
redundant to select nodes close to previously sam-
pled nodes; the labels of such nodes should be reli-
ably predicted from the previously sampled nodes.
Gu and Han?s algorithm, which is the algorithm
we used, also follows these rules. In this algo-
rithm, when considering the k-th sample, for every
node i in the current set of not-yet-chosen nodes, a
score score(k, i) is calculated, and the node with
the highest score is chosen. First, the score is de-
1377
signed to be large if the i-th node is globally im-
portant. In the algorithm, the global importance
of a node is measured by an eigenvalue decompo-
sition of the normalized graph-Laplacian, L
norm
.
Transformed from the graph?s adjacency matrix,
this matrix stores the graph?s global information.
Second, the score is designed to be smaller if the
i-th node is close to one of the previously sampled
nodes.
Score score (k, i) is defined as follows. We
perform eigenvalue decomposition beforehand.
L
norm
W
= U?U
>
, u
i
is the transpose of the i-th
row of U, and ?
i
is its corresponding eigenvalue.
score (k, i)
def
=
(
H
?1
k
u
i
)
>
?
?1
(
H
?1
k
u
i
)
1 + u
>
i
H
?1
k
u
i
(2)
In Equation 2, H
k
preserves information of the
previous k ? 1 samples. First, H
0
is a diag-
onal matrix whose i-th diagonal element is de-
fined as
1
(??
i
+1)
2
?1
where ? is a hyper-parameter.
H
0
weighs the score of globally important nodes
through the eigenvalue decomposition. Second,
H
k
is updated such that the scores of the nodes
distant from the previously taken samples are
higher. The precise update formula of H
k
follows.
i
k+1
is the index of the node sampled at k + 1-th
round. For the derivation of this formula, see Gu
and Han (2012).
H
?1
k+1
= H
?1
k
?
(
H
?1
k
u
i
k+1
) (
H
?1
k
u
i
k+1
)
>
1 + u
>
i
k+1
H
?1
k
u
i
k+1
(3)
Hyper-parameter ? determines how strong the
cluster assumption should be; the larger the value,
the more strongly the algorithm avoids selecting
nodes near previously selected samples over the
graph. Note that ? is inherited from the LLGC
2
algorithm (Zhou et al., 2004), i.e., the label prop-
agation algorithm that Gu and Han?s algorithm is
based on. From the optimization viewpoint, ? de-
termines the degree of penalization.
Remember that the score has nothing to do with
the LP scores described in ?2.3. score is used
to choose nodes used for training in the graph-
based non-interactive active learning. LP scores
are later used for classification by label propaga-
tion algorithms that use the chosen training nodes.
Throughout this paper, when we mean LP scores,
we explicitly write ?LP scores?. All the other
scores mean score.
2
Learning with Local and Global Consistency.
Figure 1: Converting frequency list into multiple-
complete graph.
3 Formalizing heuristic word sampling
as graph-based active learning
Figure 1 shows how to formalize a word frequency
list into a multiple complete graph. The word fre-
quency list is split into clusters, and each cluster
forms a complete graph. Each node in a graph cor-
responds to a word. By gathering all the complete
graphs, a multiple complete graph can be formed.
Multiple complete graph G
T,n
is defined as a
graph of T complete graphs, each of which con-
sists of n nodes fully connected within the n
nodes. An example of a multiple complete graph
can be seen in Figure 2. We can define the
Tn ? Tn adjacency matrix for multiple com-
plete graphs. W
complete
all
is defined as follows:
W
complete
all
def
=
?
?
?
?
?
?
W
complete
0 ? ? ? 0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0 ? ? ? 0 W
complete
?
?
?
?
?
?
(4)
W
complete
def
=
?
?
?
?
?
?
?
?
0 1 ? ? ? 1 1
1 0 1 ? ? ? 1
.
.
. 1
.
.
.
.
.
.
1
.
.
.
.
.
.
1
1 1 ? ? ? 1 0
?
?
?
?
?
?
?
?
(5)
We can see that W
complete
all
is a block-diagonal
matrix where each block is a n ? n matrix,
W
complete
.
Heuristic word sampling can be rewritten into
non-interactive active learning on graphs. Suppose
there are T groups, each of which has n words,
and we want to sample n
0
words from each. In
1378
Figure 2: Example of multi-complete graph, where Theorem 3.1 holds true. Here, T = 4, n = 5, and
k = 10; 10 light blue (light) nodes have already been sampled, and 10 blue (dark) nodes remain; the
11-th node is sampled uniformly randomly from the nodes within the red rectangles.
heuristic word sampling, for each group from T
groups, n
0
words are sampled from the n words
in the group uniformly randomly. Thus, there are
Tn
0
words in total.
Since heuristic word sampling takes a node
from each of the T groups, T concurrent sampling
processes are involved. For simplicity, we further
express the same sampling using only one sam-
pling process from the entire graph as follows:
? For every round, we sample words uniformly
randomly from the remaining words of the
groups where the number of samples selected
in previous rounds is least.
Figure 2 shows an example of this sampling
process. Here, the second and third groups from
the left are the groups in which the number of pre-
viously selected nodes is the least. This is because
they have only two previously selected nodes,
while the others have three. Thus, in the figure, the
remaining words of the groups are the nodes with
red rectangles. Randomly sampling one node from
the nodes with red rectangles means sampling a
node from the second or third group. We call the
set of nodes in a graph from which samples will be
taken in the next round a seed pool. Thus, in Fig-
ure 2, the set of nodes with red rectangles is the
seed pool. Nodes that have already been sampled
are taken out of the current seed pool.
Next, we more formally explain the seed pool
concept. We start sampling nodes from a multiple
complete graph via the algorithm presented by Gu
and Han. The initial seed pool is set to all nodes
in the graph, i.e., V . We sample one node in each
round; thus, k ? |V| nodes are selected by the k-
th round. Let t ? T be the index of the complete
graph in the multiple complete graph. Then, the
following theorem holds with  being a small pos-
itive value that substitutes the 0 eigenvalues in the
eigen decomposition.
Theorem 3.1 Let 0 <  < 1 and n ?
{2, 3, 4, . . .}. Then, among T complete graphs,
k mod T complete graphs have b
k
T
c + 1 sam-
ples, and the remaining graphs have b
k
T
c sam-
ples
3
. Moreover, the (k + 1)-th sample is taken
uniformly randomly from the remaining complete
graphs.
In Theorem 3.1,  > 0 is a substitute for the
0 eigenvalue of L
W
4
. Since  is a substitute for
the 0 eigenvalue, it is rational to assume 1 > .
Also, remember that n is the number of nodes in
one complete graph. The algorithm stops when
k = Tn
0
+ 1, i.e., at the Tn
0
+ 1-th round when
there are no remaining nodes to sample. Figure 2
shows an example of Theorem 3.1.
A proof of this theorem is presented in the sup-
plementary material. Briefly, in a multiple com-
plete graph, the score of a node depends only on
the complete graph or the cluster that the node
belongs to. Thus, we only have to consider one
complete graph in which k is the number of nodes
that have been already chosen. Then, mathe-
matical induction proves that, within one com-
plete graph, all the not-yet-chosen nodes have the
same score(k, i). Second, we have to show that
the score always decreases by taking a sample,
i.e., score(k, i) > score(k + 1, i). By a long
but straightforward calculation, we can express
score(k, i) by using only ?, , n, and k. Then, by
substituting the formula to score(k, i), we obtain
score(k, i)? score(k + 1, i) > 0.
4 Extending Graphs
In the previous section, we explained how to for-
malize heuristic word sampling as active learn-
ing on multiple complete graphs. This formaliza-
3
Here, both k and T are non-negative integers. Thus,
k%T denotes the remainder of the division of k by T , and
b
k
T
c is the quotient of the division.
4
In Gu and Han?s algorithm, they substitute the 0 eigen-
value with a small positive value , and they set  = 10
?6
.
1379
Figure 3: Example of merging two graphs.
tion can lead to better active learning by extend-
ing these graphs. In this section, we describe such
graph extensions.
We extend graphs by merging graphs. Figure 3
shows how to merge graphs. We define ?merging?
two weighted graphs as creating a weighted graph
whose adjacency matrix is the sum of the two ad-
jacency matrices of the two weighted graphs. This
suggests that an edge of the merged graph is sim-
ply the sum of the corresponding edges of the two
weighted graphs.
The merged graph is expected to inherit the
characteristics of its original graphs. Thus, ap-
plying graph-based active learning to the merged
graph is expected to sample nodes in accordance
with the characteristics of its original graphs.
For example, if we merge a graph representing
domain-specific relations and a multiple complete
graph representing difficulty grouping of words,
active learning from the resulting merged graph
is expected to sample words considering both do-
main specificity and difficulty grouping of words.
For another example, suppose we merge two
multiple complete graphs created from frequency
lists from two different corpora. Then, active
learning from the resulting merged graph is ex-
pected to sample words taking into account fre-
quency lists from both corpora.
5 Evaluation
We evaluate our proposed method both quantita-
tively and qualitatively. In the quantitative eval-
uation, we measure the prediction accuracy of
graphs. Note that the heuristic word sampling
method is identical to using Gu and Han?s algo-
rithm with a multiple complete graph; however,
our proposed graphs have enriched relations be-
tween words. In the qualitative evaluation, we ex-
plain in detail what words are appropriate as train-
ing examples for vocabulary prediction by pre-
senting sampled examples.
5.1 Quantitative evaluation
To evaluate the accuracy of vocabulary prediction,
we used the dataset that Ehara et al. (2010) and
Ehara et al. (2012) used. This dataset was gleaned
from questionnaires answered by 15 English as a
second language (ESL) learners. Every learner
was asked to answer how well he/she knew 11,999
English words. The data was collected in January
2009. One learner was unpaid, whereas the other
15 learners were paid. We used the data from the
15 paid learners since the data from the unpaid
learner was noisy. Most of the learners were na-
tive Japanese speakers and graduate students. Be-
cause most of the learners in this dataset were na-
tive Japanese speakers, words from SVL 12,000
(SPACE ALC Inc., 1998) were used for the learn-
ers in this dataset. Note that SVL 12,000 is a col-
lection of 12,000 words that are deemed important
for Japanese learners of English, as judged by na-
tive English teachers.
Next, we required frequency lists for the words
that appeared in the dataset. To create frequency
lists, lemmatization is important because the num-
ber of word types depends on the method used
to lemmatize the words. Note that in the field of
vocabulary measurement, lemmatization is mainly
performed by ignoring conjugation (Nation and
Beglar, 2007). Lemmatizing the dataset resulted
in a word list of 8,463 words. We adjusted the size
of the word list to a round 8,000 by removing 463
randomly chosen words. Note that all constituent
words were labeled by the 15 ESL learners.
We created the following four graphs by span-
ning edges among the 8, 000 words.
BNC multi-complete This graph corresponds to
heuristic word sampling and served as our
baseline. It is a multiple complete graph
comprising eight complete graphs, each of
which consisted of 1,000 words based on the
sorted frequency list from the British Na-
tional Corpus (BNC). We chose the BNC be-
cause the method presented by Nation and
Beglar was based on it (Nation and Beglar,
2007). Note that all edge weights are set to 1.
BNC+domain To form this graph, edges rep-
resenting domain specificity are added to
the ?BNC multi-complete? graph. For do-
main specificity, we used domain information
1380
from WordNet 3.0.
5
First, we extracted 102
domain-specific words under the ?computer?
domain among the 8,000 words and created
a complete graph consisting of these domain-
specific words. The edge weights of the com-
plete graph were set to 1. Next, we simply
merged
6
the complete graph consisting of the
domain-specific words with the ?BNC multi-
complete? graph.
BNC+COCA In addition to the ?BNC multi-
complete? graph, edges based on another cor-
pus, the Corpus of Contemporary American
English (COCA), were introduced. We first
created the COCA multi-complete graph, a
multiple complete graph consisting of eight
complete graphs, each of which consisted of
1,000 words based on the sorted frequency
list using COCA. The edge weights of the
COCA multi-complete graph were set to 1.
Next, we merged the BNC multi-complete
and COCA multi-complete graphs to form
the ?BNC + COCA graph?.
BNC+domain+COCA This graph is the graph
produced by merging the ?BNC + domain?
and ?BNC + COCA? graphs.
Note that our experiment setting differed from
the usual label propagation setting used for semi-
supervised learning because the purpose of our
task differed. In the usual label propagation set-
ting, the ?test? nodes (data) are prepared sepa-
rately from the training nodes to determine how
accurately the algorithm can classify forthcoming
or unseen nodes. However, in our setting, there
were no such forthcoming words. Of course, there
will always be words that do not emerge, even in a
large corpus; however, such rare words are too dif-
ficult for language learners to identify, and many
are proper nouns, which are not helpful for mea-
suring the vocabulary of second language learners.
Therefore, our focus here is to measure how
well the learners know a fixed set of words, that
is, the given 8,000 words. Even if an algorithm
can achieve high accuracy for words outside this
fixed set, we have no way of evaluating it using
the pooled annotations. Here, we want to measure,
from a fixed number of samples (e.g., 50), how ac-
curately an algorithm can predict a learner?s vo-
5
We used the NLTK toolkit http://nltk.org/ to extract the
domain information.
6
Definition of how to merge two graphs is in ?4.
Figure 4: Results of our quantitative experiments.
Vertical axis denotes accuracy, and horizontal axis
shows number of samples, i.e., training words.
cabulary for the entire 8,000 words. Thus, we
define accuracy to be the number of words that
each algorithm finds correctly divided by the vo-
cabulary size. We set hyper-parameter ? to 0.01
as Gu and Han (2012) did. Note that this hyper-
parameter is reportedly not sensitive to accuracy
(Zhou et al., 2011).
Figure 4 and Table 1 show the results of the
experiment over the different datasets. The ver-
tical axis in the figure denotes accuracy, whereas
the horizontal axis denotes the number of samples,
i.e., training words. Note that the accuracy is av-
eraged over 15 learners and that LLGC is used for
classification unless otherwise specified. For ex-
ample, ?BNC multi-complete? indicates that sam-
ples taken from the BNC multi-complete graph are
used for training, and LLGC is used for classifica-
tion. Note that ?BNC + domain + COCA (SVM)?
uses a support vector machine (SVM) for classifi-
cation, and ?BNC + domain + COCA (LR)? uses
logistic regression (LR) for classification. Among
many supervised machine learning methods, we
chose SVM and LR because SVM is widely used
in the NLP community, and LR was used for the-
oretical reasons (Ehara et al., 2012; Ehara et al.,
2013).
SVM and LR require features of a word
for classification while LLGC requires a
weighted graph of words. Since the graph
?BNC+domain+COCA? is made from three
features, namely the word frequencies of BNC
1381
Table 1: Results of our quantitative experiments. LLGC is used for classification unless otherwise spec-
ified. Bold letters indicate top accuracy. Asterisks (*) indicate that values are statistically significant
against baseline, heuristic sampling, i.e., ?BNC multi-complete? (using sign test p < 0.01).
10 15 20 30 40 50
BNC multi-complete 64.15 (%) 67.54 73.73 73.66 74.92 74.82
BNC+domain 65.27 71.88 72.88 75.02 76.03 * 75.95
BNC+COCA 73.45 74.10 74.57 74.90 74.96 75.29
BNC+domain+COCA 75.23 * 75.71 * 75.18 * 75.35 * 75.47 76.44 *
BNC+domain+COCA (SVM) 58.99 57.74 60.44 70.79 69.29 74.46
BNC+domain+COCA (LR) 60.29 61.74 59.27 69.17 70.63 73.42
and COCA corpora and whether a word is in the
computer domain, we used these features for the
features of SVM and LR in this experiment for a
fair comparison. When using word frequencies for
features, we used the logarithm of raw frequencies
since it is reported to work well (Ehara et al.,
2013). SVM and LR are also known to heavily
depend on a hyper-parameter called C, which
determines the strength of regularization. We
tried C = 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0,
and 100.0 for each of SVM and LR where the size
of training data is 50 and chose the C value that
performs best. As a result, we set C = 5.0 for
SVM and C = 50.0 for LR. Note that this setting
is advantageous for SVM and LR compared to
LLGC because the hyper-parameters of SVM
and LR are tuned while LLGC?s hyper-parameter
remains untuned. For the implementation of SVM
and LR, we used the ?scikit-learn? package in
Python
7
.
We first observed that our proposed methods
constantly outperform the baseline, heuristic word
sampling, i.e., ?BNC multi-complete? in Table 1.
This indicates that we successfully obtained bet-
ter accuracy by formalizing heuristic word sam-
pling as active learning and extending graphs. In
Table 1, the accuracy of the top-ranked methods
(shown using bold letters) is statistically signif-
icantly better than the accuracy of ?BNC multi-
complete? (using the sign test p < 0.01).
We then observed that ?BNC multi-complete?
and ?BNC + domain? show competitive accuracy
with sample sizes from 10 to 20; furthermore,
?BNC + domain? is slightly better than ?BNC
multi-complete? with sample sizes ranging from
30 to 50 (statistically significant p < 0.01 using
sign test). Next, we note that there is a trade-off
between domain and word frequency when choos-
7
http://scikit-learn.org/stable/
ing samples. More specifically, if we select too
many words from the domain, the measurement of
the general English ability of learners can be in-
accurate; conversely, if we select too many words
from the corpus-based word frequency list, while
the general English ability of learners is accu-
rately measured, we may obtain no information
on the learner?s vocabulary for the targeted do-
main. The competitive or slightly better accuracy
of ?BNC + domain? over ?BNC multi-complete?
shows that ?BNC + domain? could successfully
integrate domain information into the frequency-
based groups without deteriorating measurements
of general English ability.
We also observe that ?BNC + COCA? greatly
outperforms ?BNC multi-complete? when the
number of samples is 10. This shows that the inte-
gration of the two corpora, BNC and COCA (i.e.,
?BNC + COCA?), successfully increases the accu-
racy when there are only a small number of sam-
ples.
?BNC + domain + COCA? achieves the best ac-
curacy of all the graphs except when the number
of samples is 40. This indicates that the domain
information and the information from the COCA
corpus helped one another to improve the accuracy
because ?BNC + domain? and ?BNC + COCA? in-
troduce different types of domain information into
?BNC multi-complete.?
Finally, we observe that ?BNC + domain +
COCA (SVM)? and ?BNC + domain + COCA
(LR)? perform worse than LLGC over the same
dataset for all sample sizes, particularly when the
size of the training data is small. Since LLGC is
a semi-supervised classifier while SVM and LR
are not, SVM and LR perform poorly for small
amounts of training data. This result shows that
LLGC is appropriate for this task compared to
SVM because, in this task, an increase in the size
1382
Table 2: Computer-related samples in top 30 sam-
ples.
Name Num. of
Samples
Examples
BNC multi-
complete
0 -
BNC+domain 5 input, client, field,
background, regis-
ter
BNC+COCA 0 -
BNC+domain
+COCA
3 drive, client, com-
mand
of training data directly leads to an increased bur-
den on the human learners.
5.2 Qualitative evaluation
In this subsection, we qualitatively evaluate our
results to determine the types of nodes that are
sampled when domain specificity is introduced.
Specifically, we evaluate what words are selected
as samples in the ?BNC + domain? graph.
As noted above, in the ?BNC + domain? graph,
the computer science domain is introduced into
?BNC multi-complete? to measure learners? vo-
cabulary with a specific emphasis on the computer
science domain. Thus, it is desirable that some
words in the computer science domain are sam-
pled from the ?BNC + domain? graph; otherwise,
we need to predict the learners? vocabulary for
the computer science domain from general words
rather than those in the computer science domain,
which is extremely difficult.
Table 2 shows the number of words in the com-
puter science domain sampled in the first 30 sam-
ples. Note that only ?BNC + domain? and ?BNC
+ domain + COCA? select samples from the com-
puter science domain. This indicates that in the
other two methods, to measure vocabulary with
an emphasis on the computer science domain, we
need to predict learners? vocabulary from the gen-
eral words, which is almost impossible with only
30 samples. Furthermore, it is interesting to note
that ?BNC + domain? and ?BNC + domain +
COCA? select different samples from the com-
puter science domain, except for the word ?client,?
although originally the same computer science do-
main wordlist was introduced to both graphs.
Since ?BNC + domain? achieves competitive
or slightly better accuracy than ?BNC multi-
complete? in the quantitative analysis and the
qualitative analysis, we conclude that our method
can successfully introduce domain specificity into
the sampling methodology without reducing accu-
racy.
6 Conclusion
In this study, we propose a novel sampling frame-
work that measures the vocabulary of second lan-
guage learners. We call existing sampling meth-
ods heuristic sampling. This approach to sampling
ranks words from a single corpus by frequency and
creates groups of 1,000 words. Next, tens of words
are sampled from each group. This method as-
sumes that the relative difficulty of all 1,000 words
is the same.
In this paper, we introduce a novel sampling
method by showing that the existing heuristic sam-
pling approach is simply a special case of a graph-
based active learning algorithm by Gu and Han
(2012) applied to a special graph. We also pro-
pose a method to extend this graph to enable us to
handle domain specificity of words and multiple
corpora, which are difficult or impossible to han-
dle using current methods.
We evaluate our method both quantitatively and
qualitatively. In our quantitative evaluation, the
proposed method achieves higher prediction accu-
racy compared with the current approach to vo-
cabulary prediction. This suggests that our pro-
posed method can successfully make use of do-
main specificity and multiple corpora for pre-
dicting vocabulary. In our qualitative evaluation,
we examine the words sampled by our proposed
method and observe that targeted domain-specific
words are successfully sampled.
For our future work, because the graph used
in this paper was constructed manually, we plan
to automatically create a graph suitable for active
learning and classification. There are several algo-
rithms that create graphs from feature-based rep-
resentations of words, but these have never been
used for active learning of this task.
Acknowledgments
This work was supported by the Grant-in-Aid for
JSPS Fellows (JSPS KAKENHI Grant Number
12J09575).
1383
References
Yo Ehara, Nobuyuki Shimizu, Takashi Ninomiya, and
Hiroshi Nakagawa. 2010. Personalized reading
support for second-language web documents by col-
lective intelligence. In Proceedings of the 15th in-
ternational conference on Intelligent user interfaces
(IUI 2010), pages 51?60, Hong Kong, China. ACM.
Yo Ehara, Issei Sato, Hidekazu Oiwa, and Hiroshi Nak-
agawa. 2012. Mining words in the minds of second
language learners: learner-specific word difficulty.
In Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 2012),
Mumbai, India, December.
Yo Ehara, Nobuyuki Shimizu, Takashi Ninomiya, and
Hiroshi Nakagawa. 2013. Personalized reading
support for second-language web documents. ACM
Transactions on Intelligent Systems and Technology,
4(2).
Quanquan Gu and Jiawei Han. 2012. Towards active
learning on graphs: An error bound minimization
approach. In Proceedings of the IEEE International
Conference on Data Mining (ICDM) 2012.
Ming Ji and Jiawei Han. 2012. A variance minimiza-
tion criterion to active learning on graphs. In Pro-
ceedings of the 15th international conference on Ar-
tificial Intelligence and Statistics (AISTATS).
Batia Laufer and Paul Nation. 1999. A vocabulary-
size test of controlled productive ability. Language
testing, 16(1):33?51.
Paul Meara and Barbara Buxton. 1987. An alterna-
tive to multiple choice vocabulary tests. Language
Testing, 4(2):142?154.
Paul Nation and David Beglar. 2007. A vocabulary
size test. The Language Teacher, 31(7):9?13.
SPACE ALC Inc. 1998. Standard vocabulary list
12,000.
Dengyong Zhou, Oliver Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Sch?olkopf. 2004.
Learning with local and global consistency. In Pro-
ceedings in 18th Annual Conference on Neural In-
formation Processing Systems (NIPS), pages 321?
328.
Xueyuan Zhou, Mikhail Belkin, and Nathan Srebro.
2011. An iterated graph laplacian approach for rank-
ing on manifolds. In Proceedings of 17th ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD), pages 877?885.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahra-
mani. 2003. Combining active learning and semi-
supervised learning using gaussian fields and har-
monic functions. In Proceedings of ICML 2003
workshop on The Continuum from Labeled to Unla-
beled Data in Machine Learning and Data Mining.
1384
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 686?695,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Framework of Semantic Role Assignment based on Extended Lexical
Conceptual Structure: Comparison with VerbNet and FrameNet
Yuichiroh Matsubayashi? Yusuke Miyao? Akiko Aizawa?
?, National Institute of Informatics, Japan
{y-matsu,yusuke,aizawa}@nii.ac.jp
Abstract
Widely accepted resources for semantic
parsing, such as PropBank and FrameNet,
are not perfect as a semantic role label-
ing framework. Their semantic roles are
not strictly defined; therefore, their mean-
ings and semantic characteristics are un-
clear. In addition, it is presupposed that
a single semantic role is assigned to each
syntactic argument. This is not necessarily
true when we consider internal structures of
verb semantics. We propose a new frame-
work for semantic role annotation which
solves these problems by extending the the-
ory of lexical conceptual structure (LCS).
By comparing our framework with that of
existing resources, including VerbNet and
FrameNet, we demonstrate that our ex-
tended LCS framework can give a formal
definition of semantic role labels, and that
multiple roles of arguments can be repre-
sented strictly and naturally.
1 Introduction
Recent developments of large semantic resources
have accelerated empirical research on seman-
tic processing (Ma`rquez et al 2008). Specif-
ically, corpora with semantic role annotations,
such as PropBank (Kingsbury and Palmer, 2002)
and FrameNet (Ruppenhofer et al 2006), are in-
dispensable resources for semantic role labeling.
However, there are two topics we have to carefully
take into consideration regarding role assignment
frameworks: (1) clarity of semantic role meanings
and (2) the constraint that a single semantic role
is assigned to each syntactic argument.
While these resources are undoubtedly invalu-
able for empirical research on semantic process-
Sentence [John] threw [a ball] [from the window] .
Affection Agent Patient
Movement Source Theme Source/Path
PropBank Arg0 Arg1 Arg2
VerbNet Agent Theme Source
FrameNet Agent Theme Source
Table 1: Examples of single role assignments with ex-
isting resources.
ing, current usage of semantic labels for SRL sys-
tems is questionable from a theoretical viewpoint.
For example, most of the works on SRL have
used PropBank?s numerical role labels (Arg0 to
Arg5). However, the meanings of these numbers
depend on each verb in principle and PropBank
does not expect semantic consistency, namely on
Arg2 to Arg5. Moreover, Yi et al(2007) explic-
itly showed that Arg2 to Arg5 are semantically
inconsistent. The reason why such labels have
been used in SRL systems is that verb-specific
roles generally have a small number of instances
and are not suitable for learning. However, it is
necessary to avoid using inconsistent labels since
those labels confuse machine learners and can be
a cause of low accuracy in automatic process-
ing. In addition, clarity of the definition of roles
are particularly important for users to rationally
know how to use each role in their applications.
For this reasons, well-organized and generalized
labels grounded in linguistic characteristics are
needed in practice. Semantic roles of FrameNet
and VerbNet (Kipper et al 2000) are used more
consistently to some extent, but the definition of
the roles is not given in a formal manner and their
semantic characteristics are unclear.
Another somewhat related problem of existing
annotation frameworks is that it is presupposed
686
that a single semantic role is assigned to each syn-
tactic argument.1In fact, one syntactic argument
can play multiple roles in the event (or events) ex-
pressed by a verb. For example, Table 1 shows a
sentence containing the verb ?throw? and seman-
tic roles assigned to its arguments in each frame-
work. The table shows that each framework as-
signs a single role, such as Arg0 and Agent, to
each syntactic argument. However, we can ac-
quire information from this sentence that John
is an agent of the throwing event (the ?Affec-
tion? row), as well as a source of the movement
event of the ball (the ?Movement? row). Existing
frameworks of assigning single roles simply ig-
nore such information that verbs inherently have
in their semantics. We believe that giving a clear
definition of multiple argument roles would be
beneficial not only as a theoretical framework but
also for practical applications that require detailed
meanings derived from secondary roles.
This issue is also related to fragmentation and
the unclear definition of semantic roles in these
frameworks. As we exemplify in this paper, mul-
tiple semantic characteristics are conflated in a
single role label in these resources due to the man-
ner of single-role assignment. This means that se-
mantic roles of existing resources are not mono-
lithic and inherently not mutually independent,
but they share some semantic characteristics.
The aim of this paper is more on theoreti-
cal discussion for role-labeling frameworks rather
than introducing a new resource. We developed
a framework of verb lexical semantics, which is
an extension of the lexical conceptual structure
(LCS) theory, and compare it with other exist-
ing frameworks which are used in VerbNet and
FrameNet, as an annotation scheme of SRL. LCS
is a decomposition-based approach to verb se-
mantics and describes a meaning by composing
a set of primitive predicates. The advantage of
this approach is that primitive predicates and their
compositions are formally defined. As a result,
we can give a strict definition of semantic roles
by grounding them to lexical semantic structures
of verbs. In fact, we define semantic roles as ar-
gument slots in primitive predicates. With this ap-
1To be precise, FrameNet permits multiple-role assign-
ment, while it does not perform this systematically as we
show in Table 1. It mostly defines a single role label for a
corresponding syntactic argument, that plays multiple roles
in several sub-events in a verb.
proach, we demonstrate that some sort of seman-
tic characteristics that VerbNet and FrameNet in-
formally/implicitly describe in their roles can be
given formal definitions and that multiple argu-
ment roles can be represented strictly and natu-
rally by extending the LCS theory.
In the first half of this paper, we define our ex-
tended LCS framework and describe how it gives
a formal definition of roles and solves the problem
of multiple roles. In the latter half, we discuss
the analysis of the empirical data we collected
for 60 Japanese verbs and also discuss theoreti-
cal relationships with the frameworks of existing
resources. We discuss in detail the relationships
between our role labels and VerbNet?s thematic
roles. We also describe the relationship between
our framework and FrameNet, with regards to the
definitions of the relationships between semantic
frames.
2 Related works
There have been several attempts in linguistics
to assign multiple semantic properties to one ar-
gument. Gruber (1965) demonstrated the dis-
pensability of the constraint that an argument
takes only one semantic role, with some concrete
examples. Rozwadowska (1988) suggested an
approach of feature decomposition for semantic
roles using her three features of change, cause,
and sentient, and defined typical thematic roles
by combining these features. This approach made
it possible for us to classify semantic properties
across thematic roles. However, Levin and Rap-
paport Hovav (2005) argued that the number of
combinations using defined features is usually
larger than the actual number of possible com-
binations; therefore, feature decomposition ap-
proaches should predict possible feature combi-
nations.
Culicover and Wilkins (1984) divided their
roles into two groups, action and perceptional
roles, and explained that dual assignment of roles
always involves one role from each set. Jackend-
off (1990) proposed an LCS framework for rep-
resenting the meaning of a verb by using several
primitive predicates. Jackendoff also stated that
an LCS represents two tiers in its structure, action
tier and thematic tier, which are similar to Culi-
cover and Wilkins?s two sets. Essentially, these
two approaches distinguished roles related to ac-
tion and change, and successfully restricted com-
687
26
6
4
cause(affect(i,j), go(j,
2
6
4
from(locate(in(i)))
fromward(locate(at(k)))
toward(locate(at(l)))
3
7
5
))
3
7
7
5
Figure 1: LCS of the verb throw.
binations of roles by taking a role from each set.
Dorr (1997) created an LCS-based lexical re-
source as an interlingual representation for ma-
chine translation. This framework was also used
for text generation (Habash et al 2003). How-
ever, the problem of multiple-role assignment was
not completely solved on the resource. As a
comparison of different semantic structures, Dorr
(2001) and Hajic?ova? and Kuc?erova? (2002) ana-
lyzed the connection between LCS and PropBank
roles, and showed that the mapping between LCS
and PropBank roles was many to many correspon-
dence and roles can map only by comparing a
whole argument structure of a verb. Habash and
Dorr (2001) tried to map LCS structures into the-
matic roles by using their thematic hierarchy.
3 Multiple role expression using lexical
conceptual structure
Lexical conceptual structure is an approach to de-
scribe a generalized structure of an event or state
represented by a verb. A meaning of a verb is rep-
resented as a structure composed of several prim-
itive predicates. For example, the LCS structure
for the verb ?throw? is shown in Figure 1 and
includes the predicates cause, affect, go, from,
fromward, toward, locate, in, and at. The argu-
ments of primitive predicates are filled by core ar-
guments of the verb. This type of decomposition
approach enables us to represent a case that one
syntactic argument fills multiple slots in the struc-
ture. In Figure 1, the argument i appears twice in
the structure: as the first argument of affect and
the argument in from.
The primitives are designed to represent a full
or partial action-change-state chain, which con-
sists of a state, a change in or maintaining of a
state, or an action that changes/maintains a state.
Table 2 shows primitives that play important roles
to represent that chain. Some primitives embed
other primitives as their arguments and the seman-
tics of the entire structure of an LCS structure
is calculated according to the definition of each
primitive. For instance, the LCS structure in Fig-
Predicates Semantic Functions
state(x, y) First argument is in state specified by
second argument.
cause(x, y) Action in first argument causes change
specified in second argument.
act(x) First argument affects itself.
affect(x, y) First argument affects second argument.
react(x, y) First argument affects itself, due to the
effect from second argument.
go(x, y) First argument changes according to the
path described in the second argument.
from(x) Starting point of certain change event.
fromward(x) Direction of starting point.
via(x) Pass point of certain change event.
toward(x) Direction of end point.
to(x) End point of certain change event.
along(x) Linear-shaped path of change event.
Table 2: Major primitive predicates and their semantic
functions.
ure 1 represents the action changing the state of j.
The inner structure of the second argument of go
represents the path of the change.
The overall definition of our extended LCS
framework is shown in Figure 2.2 Basically, our
definition is based on Jackendoff?s LCS frame-
work (1990), but performed some simplifications
and added extensions. The modification is per-
formed in order to increase strictness and gen-
erality of representation and also a coverage for
various verbs appearing in a corpus. The main
differences between the two LCS frameworks are
as follows. In our extended LCS framework, (i)
the possible combinations of cause, act, affect,
react, and go are clearly restricted, (ii) multiple
actions or changes in an event can be described
by introducing a combination function (comb for
short), (iii) GO, STAY and INCH in Jackendoff?s
theory are incorporated into one function go, and
(iv) most of the change-of-state events are repre-
sented as a metaphor using a spatial transition.
The idea of a comb function comes from a nat-
ural extension of Jackendoff?s EXCH function.
In our case, comb is not limited to describing
a counter-transfer of the main event but can de-
scribe subordinate events occurring in relation to
the main event.3 We can also describe multiple
2Here we omitted the attributes taken by each predicate,
in order to simplify the explanation. We also omitted an
explanation for lower level primitives, such as STATE and
PLACE groups, which are not necessarily important for the
topic of this paper.
3In our extended LCS theory, we can describe multiple
688
LCS =
2
4
EVENT+
comb
h
EVENT
i
*
3
5
STATE =
8
>
>
>
<
>
>
>
:
be
locate(PLACE)
orient(PLACE)
extent(PLACE)
connect(arg)
9
>
>
>
=
>
>
>
;
EVENT =
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
8
>
>
>
<
>
>
>
:
state(arg, STATE)
go(arg, PATH)
cause(act(arg1), go(arg1, PATH))
cause(affect(arg1, arg2), go(arg2, PATH))
cause(react(arg1, arg2), go(arg1, PATH))
9
>
>
>
=
>
>
>
;
manner(constant)?
mean(constant)?
instrument(constant)?
purpose(EVENT)*
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
PLACE =
8
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
:
in(arg)
on(arg)
cover(arg)
fit(arg)
inscribed(arg)
beside(arg)
around(arg)
near(arg)
inside(arg)
at(arg)
9
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
;
PATH=
2
6
6
6
6
6
6
6
6
4
from(STATE)?
fromward(STATE)?
via(STATE)?
toward(STATE)?
to(STATE)?
along(arg)?
3
7
7
7
7
7
7
7
7
5
Figure 2: Description system of our LCS. Operators
+, ?, ? follow the basic regular expression syntax. {}
represents a choice of the elements.
main events if the agent does more than two ac-
tions simultaneously and all the actions are the
focus (e.g., John exchanges A with B). This ex-
tension is simple, but essential for creating LCS
structures of predicates appearing in actual data.
In our development of 60 Japanese predicates
(verb and verbal noun) frequently appearing in
Kyoto University Text Corpus (KTC) (Kurohashi
and Nagao, 1997) , 37.6% of the frames included
multiple events. By using the comb function, we
can express complicated events with predicate de-
composition and prevent missing (multiple) roles.
A key point for associating LCS framework
with the existing frameworks of semantic roles is
that each primitive predicate of LCS represents
a fundamental function in semantics. The func-
events in the semantic structure of a verb. However, gener-
ally, a verb focuses on one of those events and this makes
a semantic variation among verbs such as buy, sell, and pay
as well as difference of syntactic behavior of the arguments.
Therefore, focused event should be distinguished from the
others as lexical information. We expressed focused events
as main formulae (formulae that are not surrounded by a
comb function).
Role Description
Protagonist Entity which is viewpoint of verb.
Theme Entity in which its state or change of state
is mentioned.
State Current state of certain entity.
Actor Entity which performs action that
changes/maintains its state.
Effector Entity which performs action that
changes/maintains a state of another entity.
Patient Entity which is changed/maintained its
state by another entity.
Stimulus Entity which is cause of the action.
Source Starting point of certain change event.
Source dir Direction of starting point.
Middle Pass point of certain change event.
Goal End point of certain change event.
Goal dir Direction of end point.
Route Linear-shaped path of certain change event.
Table 3: Semantic role list for proposing extended LCS
framework.
tions of the arguments of the primitive predicates
can be explained using generalized semantic roles
such as typical thematic roles. In order to sim-
ply represent the semantic functions of the ar-
guments in the LCS primitives or make it eas-
ier to compare our extended LCS framework with
other SRL frameworks, we define a semantic role
set that corresponds to the semantic functions of
the primitive predicates in the LCS structure (Ta-
ble 3). We employed role names similarly to typ-
ical thematic roles in order to easily compare the
role sets, but the definition is different. Also, due
to the increase of the generality of LCS represen-
tation, we obtained clearer definition to explain a
correspondence between LCS primitives and typ-
ical thematic roles than the Jackendoff?s predi-
cates. Note that the core semantic information of
a verb represented by a LCS framework is em-
bodied directly in its LCS structure and the in-
formation decreases if the structure is mapped to
the semantic roles. The mapping is just for con-
trasting thematic roles. Each role is given an ob-
vious meaning and designed to fit to the upper-
level primitives of the LCS structure, which are
the arguments of EVENT and PATH functions. In
Table 4, we can see that these roles correspond al-
most one-to-one to the primitive arguments. One
special role is Protagonist, which does not match
an argument of a specific primitive. The Pro-
tagonist is assigned to the first argument in the
main formula to distinguish that formula from the
sub formulae. There are 13 defined roles, and
689
Predicate 1st arg 2nd arg
state Theme State
act Actor ?
affect Effector Patient
react Actor Stimulus
go Theme PATH
from Source ?
fromward Source dir ?
via Middle ?
toward Goal dir ?
to Goal ?
along Route ?
Table 4: Correspondence between semantic roles and
arguments of LCS primitives
this number is comparatively smaller than that in
VerbNet. The discussion with regard to this num-
ber is described in the next section.
Essentially, the semantic functions of the ar-
guments in LCS primitives are similar to those
of traditional, or basic, thematic roles. However,
there are two important differences. Our extended
LCS framework principally guarantees that the
primitive predicates do not contain any informa-
tion concerning (i) selectional preference and (ii)
complex structural relation of arguments. Primi-
tives are designed to purely represent a function
in an action-change-state chain, thus the informa-
tion of selectional preference is annotated to a dif-
ferent layer; specifically, it is directly annotated to
core arguments (e.g., we can annotate i with sel-
Pref(animate ? organization) in Figure 1). Also,
the semantic function is already decomposed and
the structural relation among the arguments is rep-
resented as a structure of primitives in LCS rep-
resentation. Therefore, each argument slot of
the primitive predicates does not include compli-
cated meanings and represents a primitive seman-
tic property which is highly functional. These
characteristics are necessary to ensure clarity of
the semantic role meanings. We believe that even
though there surely exists a certain type of com-
plex semantic role, it is reasonable to represent
that role based on decomposed properties.
In order to show an instance of our extended
LCS theory, we constructed a dictionary of LCS
structures for 60 Japanese verbs (including event
nouns) using our extended LCS framework. The
60 verbs were the most frequent verbs in KTC af-
ter excluding 100 most frequent ones.4 We cre-
4We omitted top 100 verbs since these most frequent ones
Role Single Multiple Grow (%)
Theme 21 108 414
State 1 1 0
Actor 12 13 8.3
Effector 73 92 26
Patient 77 79 2.5
Stimulus 0 0 0
Source 11 44 300
Source dir 4 4 0
Middle 1 8 700
Goal 42 81 93
Goal dir 2 3 50
Route 2 2 0
w/o Theme 225 327 45
Total 246 435 77
Table 5: Number of appearances of each role
ated the dictionary looking at the instances of
the target verbs in KTC. To increase the cover-
age of senses and case frames, we also consulted
the online Japanese dictionary Digital Daijisen5
and Kyoto university case frames (Kawahara and
Kurohashi, 2006) which is a compilation of case
frames automatically acquired from a huge web
corpus. There were 97 constructed frames in the
dictionary.
Then we analyzed how many roles are addi-
tionally assigned by permitting multiple role as-
signment (see Table 5). The numbers of assigned
roles for single role are calculated by counting
roles that appear first for each target argument in
the structure. Table 5 shows that the total number
of assigned roles is 1.77 times larger than single-
role assignment. The main reason is an increase in
Theme. For single-role assignment, Theme, in our
sense, in action verbs is always duplicated with
Actor/Patient. On the other hand, LCS strictly
divides a function for action and change; there-
fore the duplicated Theme is correctly annotated.
Moreover, we obtained a 45% increase even when
we did not count duplicated Theme. Most of in-
crease are a result from the increase in Source
and Goal. For example, Effectors of transmission
verbs are also annotated with a Source, and Effec-
tors of movement verbs are sometimes annotated
with Source or Goal.
contain a phonogram form (Hiragana form) of a certain verb
written with Kanji characters, and that phonogram form gen-
erally has a huge ambiguity because many different verbs
have same pronunciation in Japanese.
5Available at http://dictionary.goo.ne.jp/jn/.
690
Resource Frame-independent # of roles
LCS yes 13
VerbNet (v3.1) yes 30
FrameNet (r1.4) no 8884
Table 6: Number of roles in each resource.
4 Comparison with other resources
4.1 Number of semantic roles
The number of roles is related to the number of se-
mantic properties represented in a framework and
to the generality of that property. Table 6 lists the
number of semantic roles defined in our extended
LCS framework, VerbNet and FrameNet.
There are two ways to define semantic roles.
One is frame specific, where the definition of each
role depends on a specific lexical entry and such
a role is never used in the other frames. The other
is frame independent, which is to construct roles
whose semantic function is generalized across
all verbs. The number of roles in FrameNet is
comparatively large because it defines roles in a
frame-specific way. FrameNet respects individual
meanings of arguments rather than generality of
roles.
Compared with VerbNet, the number of roles
defined in our extended LCS framework is less
than half. However, this fact does not mean
that the representation ability of our framework is
lower than VerbNet. We manually checked and
listed a corresponding representation in our ex-
tended LCS framework for each thematic role in
VerbNet in Table 6. This table does not provide a
perfect or complete mapping between the roles in
these two frameworks because the mappings are
not based on annotated data. However, we can
roughly say that the VerbNet roles combine three
types of information, a function of the argument
in the action-change-state chain, selectional pref-
erence, and structural information of arguments,
which are in different layers in LCS representa-
tion. VerbNet has many roles whose functions in
the action-change-state chain are duplicated. For
example, Destination, Recipient, and Beneficiary
have the same property end-state (Goal in LCS)
of a changing event. The difference between such
roles comes from a specific sub-type of a chang-
ing event (possession), selectional preference, and
structural information among the arguments. By
distinguishing such roles, VerbNet roles may take
into account specific syntactic behaviors of cer-
tain semantic roles. Packing such complex infor-
mation to semantic roles is useful for analyzing
argument realization. However, from the view-
point of semantic representation, the clarity for
semantic properties provided using a predicate de-
composition approach is beneficial. The 13 roles
for the LCS approach is sufficient for obtaining
a function in the action-change-state chain. In
our LCS framework, selectional preference can
be assigned to arguments in an individual verb or
verb class level instead of role labels themselves
to maintain generality of semantic functions. In
addition, our extended LCS framework can easily
separate complex structural information from role
labels because LCS directly represents a structure
among the arguments. We can calculate the infor-
mation from the LCS structure instead of coding
it into role labels. As a result, our extended LCS
framework maintains generality of roles and the
number of roles is smaller than other frameworks.
4.2 Clarity of role meanings
We showed that an approach of predicate decom-
position used in LCS theory clarified role mean-
ings assigned to syntactic arguments. Moreover,
LCS achieves high generality of roles by separat-
ing selectional preference or structural informa-
tion from role labels. The complex meaning of
one syntactic argument is represented by multi-
ple appearances of the argument in an LCS struc-
ture. For example, we show an LCS structure
and a frame in VerbNet with regard to the verb
?buy? in Figure 3. The LCS structure consists
of four formulae. The first one is the main for-
mula and the others are sub-formulae that rep-
resent co-occurring actions. The semantic-role-
like representation of the structure is given in Ta-
ble 4: i = {Protagonist, Effector, Source, Goal},
j = {Patient,Theme}, k = {Effector, Source,
Goal}, and l = {Patient,Theme}. Selectional
preference is annotated to each argument as i:
selPref(animate ? organization), j: selPref(any),
k: selPref(animate ? organization), and l: sel-
Pref(valuable entity). If we want to represent the
information, such as ?Source of what??, then we
can extend the notation as Source(j) to refer to a
changing object.
On the other hand, VerbNet combines mul-
tiple types of information into a single role as
mentioned above. Also, the meaning of some
691
VerbNet role (# of uses) Representation in LCS
Actor (9), Actor1 (9), Actor2 (9) Actor or Effector in symmetric formulas in the structure
Agent (212) (Actor ? Effector) ? Protagonist
Asset (6) Theme ? Source of the change is (locate(in()) ? Protagonist) ?
selPref(valuable entity)
Beneficiary (9) (peripheral role ? (Goal ? locate(in()))) ? selPref(animate ? organization)
? ?(Actor ? Effector) ? a transferred entity is something beneficial
Cause (21) ((Effector ? selPref(?animate ? ?organization)) ? Stimulus ? peripheral role)
Destination (32) Goal
Experiencer (24) Actor of react()
Instrument (25) ((Effector ? selPref(?animate ? ?organization)) ? peripheral role)
Location (45) (Theme ? PATH roles ? peripheral role) ? selPref(location)
Material (6) Theme ? Source of a change ? The Goal of the change is locate(fit()) ?
the Goal fullfills selPref(physical object)
Patient (59), Patient 1(11) Patient ? Theme
Patient2 (11) (Source ? Goal) ? connect()
Predicate (23) Theme ? (Goal ? locate(fit())) ? peripheral role
Product (7) Theme ? (Goal ? locate(fit()) ? selPref(physical object))
Proposition (11) Theme
Recipient (33) Goal ? locate(in()) ? selPref(animate ? organization)
Source (34) Source
Theme (162) Theme
Theme1 (13), Theme2 (13) Both of the two is Theme ? Theme1 is Theme and Theme2 is State
Topic (18) Theme ? selPref(knowledge ? infromation)
Table 7: Relationship of roles between VerbNet and our LCS framework. VerbNet roles that appears more than
five times in frame definition are analyzed. Each relationship shown here is only a partial and consistent part of
the complete correspondence table. Note that complete table of mapping highly depends on each lexical entry
(or verb class). Here, locate(in()) generally means possession or recognizing.
roles depends more on selectional preference or
the structure of the arguments than a primitive
function in the action-change-state chain. Such
VerbNet roles are used for several different func-
tions depending on verbs and their alternations,
and it is therefore difficult to capture decomposed
properties from the role label without having spe-
cific lexical knowledge. Moreover, some seman-
tic functions, such as Mary is a Goal of the money
in Figure 3, are completely discarded from the
representation at the level of role labels.
There is another representation related to the
argument meanings in VerbNet. This representa-
tion is a type of predicate decomposition using its
original set of predicates, which are referred to as
semantic predicates. For example, the verb ?buy?
in Figure 3 has the predicates has possession,
transfer and cost for composing the meaning of
its event structure. The thematic roles are fillers
of the predicates? arguments, thus the semantic
predicates may implicitly provide additional func-
tions to the roles and possibly represent multiple
roles. Unfortunately, we cannot discover what
each argument of the semantic predicates exactly
means since the definition of each predicate is not
Example: ?John bought a book from Mary for $10.?
VerbNet: Agent V Theme {from} Source {for} Asset.
has possession(start(E), Source, Theme),
has possession(end(E), Agent, Theme),
transfer(during(E), Theme), cost(E, Asset)
LCS:
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
cause(aff(i:John, j:a book), go(j,
h
to(loc(in(i)))
i
))
comb
2
4cause(aff(i,l:$10), go(l,
"
from(loc(in(i)))
to(loc(at(k:Mary)))
#
))
3
5
comb
2
4cause(aff(k,j), go(j,
"
from(loc(in(k)))
to(loc(at(i)))
#
))
3
5
comb
?
cause(aff(k,l), go(l,
h
to(loc(in(k)))
i
))
?
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Figure 3: Comparison between the semantic predicate
representation and the LCS structure of the verb buy.
publicly available. A requirement for obtaining
implicit semantic functions from these semantic
predicates is clearly defining how the roles (or
functions) are calculated from these complex re-
lations of semantic predicates.
FrameNet does not use semantic roles general-
ized among all verbs or does not represent seman-
692
i: selPref(animate ? organization), j: selPref(any), k: selPref(animate ? organization), l:
selPref(valuable entity)
Figure 4: LCS of the verbs get, buy, sell, pay, and collect and their relationships calculated from the structures.
tic properties of roles using a predicate decom-
position approach, but defines specific roles for
each conceptual event/state to represent a specific
background of the roles in the event/state. How-
ever, at the same time, FrameNet defines several
types of parent-child relations between most of
the frames and between their roles; therefore, we
may say FrameNet implicitly describes a sort of
decomposed property using roles in highly gen-
eral or abstract frames and represents the inher-
itance of these semantic properties. One advan-
tage of this approach is that the inheritance of a
meaning between roles is controlled through the
relations, which are carefully maintained by hu-
man efforts, and is not restricted by the represen-
tation ability of the decomposition system. On the
other hand, the only way to represent generalized
properties of a certain semantic role is enumerat-
ing all inherited roles by tracing ancestors. Also,
a semantic relation between arguments in a cer-
tain frame, which is given by LCS structure and
semantic predicates of VerbNet, is only defined
by a natural language description for each frame
in FrameNet. From a CL point of view, we con-
sider that, at least, a certain level of formalization
of semantic relation of arguments is important for
utilize this information for application. LCS ap-
proach, or an approach using a well-defined pred-
icate decomposition, can explicitly describe se-
mantic properties and relationships between argu-
Figure 5: The frame relations among the verbs get,
buy, sell, pay, and collect in FrameNet.
ments in a lexical structure. The primitive proper-
ties can be clearly defined, even though the repre-
sentation ability is restricted under the generality
of roles.
In addition, the frame-to-frame relations in
FrameNet may be a useful resource for some ap-
plication tasks such as paraphrasing and entail-
ment. We argue that some types of relationships
between frames are automatically calculated us-
ing the LCS approach. For example, one of the
relations is based on an inclusion relation of two
LCS structures. Figure 4 shows automatically
calculated relations surrounding the verb ?buy?.
Note that we chose a sense related to a com-
mercial transaction, which means a exchange of
a goods and money, for each word in order to
compare the resulted relation graph with that of
FrameNet. We call relations among ?buy?, ?sell?,
?pay? and ?collect? as different viewpoints since
693
they contain exactly the same formulae, and the
only difference is the main formula. The rela-
tion between ?buy? and ?get? is defined as in-
heritance; a part of the child structure exactly
equals the parent structure. Interestingly, the re-
lations surrounding the ?buy? are similar to those
in FrameNet (see Figure 5). We cannot describe
all types of the relations we considered due to
space limitations. However, the point is that these
relationships are represented as rewriting rules
between the two LCS representations and thus
they are automatically calculated. Moreover, the
grounds for relations maintain clarity based on
concrete structural relations. A semantic relation
construction of frames based on structural rela-
tionships is another possible application of LCS
approaches that connects traditional LCS theo-
ries with resources representing a lexical network
such as FrameNet.
4.3 Consistency on semantic structures
Constructing a LCS dictionary is generally a dif-
ficult work since LCS has a high flexibility for
describing structures and different people tend to
write different structures for a single verb. We
maintained consistency of the dictionary by tak-
ing into account a similarity of the structures be-
tween the verbs that are in paraphrasing or entail-
ment relations. This idea was inspired by auto-
matic calculation of semantic relations of lexicon
as we mentioned above. We created a LCS struc-
ture for each lexical entry as we can calculate se-
mantic relations between related verbs and main-
tained high-level consistency among the verbs.
Using our extended LCS theory, we success-
fully created 97 frames for 60 predicates without
any extra modification. From this result, we be-
lieve that our extended theory is stable to some
extent. On the other hand, we found that an extra
extension of the LCS theory is needed for some
verbs to explain the different syntactic behaviors
of one verb. For example, a condition for a cer-
tain syntactic behavior of a verb related to re-
ciprocal alteration (see class 2.5 of Levin (Levin,
1993)) such as???? (connect) and?? (in-
tegrate) cannot be explained without considering
the number of entities in some arguments. Also,
some verbs need to define an order of the internal
events. For example, the Japanese verb ???
? (shuttle) means that going is a first action and
coming back is a second action. These are not
the problems that are directly related to a seman-
tic role annotation on that we focus in this paper,
but we plan to solve these problems with further
extensions.
5 Conclusion
We discussed the two problems in current labeling
approaches for argument-structure analysis: the
problems in clarity of role meanings and multiple-
role assignment. By focusing on the fact that an
approach of predicate decomposition is suitable
for solving these problems, we proposed a new
framework for semantic role assignment by ex-
tending Jackendoff?s LCS framework. The statis-
tics of our LCS dictionary for 60 Japanese verbs
showed that 37.6% of the created frames included
multiple events and the number of assigned roles
for one syntactic argument increased 77% from
that in single-role assignment.
Compared to the other resources such as Verb-
Net and FrameNet, the role definitions in our ex-
tended LCS framework are clearer since the prim-
itive predicates limit the meaning of each role to
a function in the action-change-state chain. We
also showed that LCS can separate three types of
information, the functions represented by primi-
tives, the selectional preference and structural re-
lation of arguments, which are conflated in role la-
bels in existing resources. As a potential of LCS,
we demonstrated that several types of frame re-
lations, which are similar to those in FrameNet,
are automatically calculated using the structural
relations between LCSs. We still must perform a
thorough investigation for enumerating relations
which can be represented in terms of rewriting
rules for LCS structures. However, automatic
construction of a consistent relation graph of se-
mantic frames may be possible based on lexical
structures.
We believe that this kind of decomposed analy-
sis will accelerate both fundamental and applica-
tion research on argument-structure analysis. As a
future work, we plan to expand the dictionary and
construct a corpus based on our LCS dictionary.
Acknowledgment
This work was partially supported by JSPS Grant-
in-Aid for Scientific Research #22800078.
694
References
P.W. Culicover and W.K. Wilkins. 1984. Locality in
linguistic theory. Academic Press.
Bonnie J. Dorr. 1997. Large-scale dictionary con-
struction for foreign language tutoring and inter-
lingual machine translation. Machine Translation,
12(4):271?322.
Bonnie J. Dorr. 2001. Lcs database. http://www.
umiacs.umd.edu/?bonnie/LCS Database Document
ation.html.
Jeffrey S Gruber. 1965. Studies in lexical relations.
Ph.D. thesis, MIT.
N. Habash and B. Dorr. 2001. Large scale language
independent generation using thematic hierarchies.
In Proceedings of MT summit VIII.
N. Habash, B. Dorr, and D. Traum. 2003. Hybrid
natural language generation from lexical conceptual
structures. Machine Translation, 18(2):81?128.
Eva Hajic?ova? and Ivona Kuc?erova?. 2002. Argu-
ment/valency structure in propbank, lcs database
and prague dependency treebank: A comparative
pilot study. In Proceedings of the Third Inter-
national Conference on Language Resources and
Evaluation (LREC 2002), pages 846?851.
Ray Jackendoff. 1990. Semantic Structures. The MIT
Press.
D. Kawahara and S. Kurohashi. 2006. Case frame
compilation from the web using high-performance
computing. In Proceedings of LREC-2006, pages
1344?1347.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of LREC-2002,
pages 1989?1993.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of the National Conference on Arti-
ficial Intelligence, pages 691?696. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto
university text corpus project. Proceedings of the
Annual Conference of JSAI, 11:58?61.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment realization. Cambridge University Press.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: an introduction to the special
issue. Computational linguistics, 34(2):145?159.
B. Rozwadowska. 1988. Thematic restrictions on de-
rived nominals. In W Wlikins, editor, Syntax and
Semantics, volume 21, pages 147?165. Academic
Press.
J. Ruppenhofer, M. Ellsworth, M.R.L. Petruck, C.R.
Johnson, and J. Scheffczyk. 2006. FrameNet II:
Extended Theory and Practice. Berkeley FrameNet
Release, 1.
Szu-ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In
Proceedings of HLT-NAACL 2007, pages 548?555.
695
Tutorial Abstracts of ACL 2010, page 1,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Wide-coverage NLP with Linguistically Expressive Grammars
Julia Hockenmaier
Department of Computer Science,
University of Illinois
juliahmr@illinois.edu
Yusuke Miyao
National Institute of Informatics
yusuke@nii.ac.jp
Josef van Genabith
Centre for Next Generation Localisation,
School of Computing,
Dublin City University
josef@computing.dcu.ie
1 Introduction
In recent years, there has been a lot of research
on wide-coverage statistical natural language
processing with linguistically expressive gram-
mars such as Combinatory Categorial Grammars
(CCG), Head-driven Phrase-Structure Grammars
(HPSG), Lexical-Functional Grammars (LFG)
and Tree-Adjoining Grammars (TAG). But al-
though many young researchers in natural lan-
guage processing are very well trained in machine
learning and statistical methods, they often lack
the necessary background to understand the lin-
guistic motivation behind these formalisms. Fur-
thermore, in many linguistics departments, syntax
is still taught from a purely Chomskian perspec-
tive. Additionally, research on these formalisms
often takes place within tightly-knit, formalism-
specific subcommunities. It is therefore often dif-
ficult for outsiders as well as experts to grasp the
commonalities of and differences between these
formalisms.
2 Content Overview
This tutorial overviews basic ideas of TAG/
CCG/LFG/HPSG, and provides attendees with a
comparison of these formalisms from a linguis-
tic and computational point of view. We start
from stating the motivation behind using these ex-
pressive grammar formalisms for NLP, contrast-
ing them with shallow formalisms like context-
free grammars. We introduce a common set of
examples illustrating various linguistic construc-
tions that elude context-free grammars, and reuse
them when introducing each formalism: bounded
and unbounded non-local dependencies that arise
through extraction and coordination, scrambling,
mappings to meaning representations, etc. In the
second half of the tutorial, we explain two key
technologies for wide-coverage NLP with these
grammar formalisms: grammar acquisition and
parsing models. Finally, we show NLP applica-
tions where these expressive grammar formalisms
provide additional benefits.
3 Tutorial Outline
1. Introduction: Why expressive grammars
2. Introduction to TAG
3. Introduction to CCG
4. Introduction to LFG
5. Introduction to HPSG
6. Inducing expressive grammars from corpora
7. Wide-coverage parsing with expressive
grammars
8. Applications
9. Summary
References
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith and Andy Way. 2008.
Wide-Coverage Deep Statistical Parsing using Au-
tomatic Dependency Structure Annotation. Compu-
tational Linguistics, 34(1). pp.81-124, MIT Press.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature For-
est Models for Probabilistic HPSG Parsing. Compu-
tational Linguistics, 34(1). pp.35-80, MIT Press.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3). pp.355-396, MIT
Press.
1
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440?448,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Bayesian Symbol-Refined Tree Substitution Grammars
for Syntactic Parsing
Hiroyuki Shindo? Yusuke Miyao? Akinori Fujino? Masaaki Nagata?
?NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
?National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
yusuke@nii.ac.jp
Abstract
We propose Symbol-Refined Tree Substitu-
tion Grammars (SR-TSGs) for syntactic pars-
ing. An SR-TSG is an extension of the con-
ventional TSG model where each nonterminal
symbol can be refined (subcategorized) to fit
the training data. We aim to provide a unified
model where TSG rules and symbol refine-
ment are learned from training data in a fully
automatic and consistent fashion. We present
a novel probabilistic SR-TSG model based
on the hierarchical Pitman-Yor Process to en-
code backoff smoothing from a fine-grained
SR-TSG to simpler CFG rules, and develop
an efficient training method based on Markov
Chain Monte Carlo (MCMC) sampling. Our
SR-TSG parser achieves an F1 score of 92.4%
in the Wall Street Journal (WSJ) English Penn
Treebank parsing task, which is a 7.7 point im-
provement over a conventional Bayesian TSG
parser, and better than state-of-the-art discrim-
inative reranking parsers.
1 Introduction
Syntactic parsing has played a central role in natural
language processing. The resulting syntactic analy-
sis can be used for various applications such as ma-
chine translation (Galley et al, 2004; DeNeefe and
Knight, 2009), sentence compression (Cohn and La-
pata, 2009; Yamangil and Shieber, 2010), and ques-
tion answering (Wang et al, 2007). Probabilistic
context-free grammar (PCFG) underlies many sta-
tistical parsers, however, it is well known that the
PCFG rules extracted from treebank data via maxi-
mum likelihood estimation do not perform well due
to unrealistic context freedom assumptions (Klein
and Manning, 2003).
In recent years, there has been an increasing inter-
est in tree substitution grammar (TSG) as an alter-
native to CFG for modeling syntax trees (Post and
Gildea, 2009; Tenenbaum et al, 2009; Cohn et al,
2010). TSG is a natural extension of CFG in which
nonterminal symbols can be rewritten (substituted)
with arbitrarily large tree fragments. These tree frag-
ments have great advantages over tiny CFG rules
since they can capture non-local contexts explic-
itly such as predicate-argument structures, idioms
and grammatical agreements (Cohn et al, 2010).
Previous work on TSG parsing (Cohn et al, 2010;
Post and Gildea, 2009; Bansal and Klein, 2010) has
consistently shown that a probabilistic TSG (PTSG)
parser is significantly more accurate than a PCFG
parser, but is still inferior to state-of-the-art parsers
(e.g., the Berkeley parser (Petrov et al, 2006) and
the Charniak parser (Charniak and Johnson, 2005)).
One major drawback of TSG is that the context free-
dom assumptions still remain at substitution sites,
that is, TSG tree fragments are generated that are
conditionally independent of all others given root
nonterminal symbols. Furthermore, when a sentence
is unparsable with large tree fragments, the PTSG
parser usually uses naive CFG rules derived from
its backoff model, which diminishes the benefits ob-
tained from large tree fragments.
On the other hand, current state-of-the-art parsers
use symbol refinement techniques (Johnson, 1998;
Collins, 2003; Matsuzaki et al, 2005). Symbol
refinement is a successful approach for weaken-
ing context freedom assumptions by dividing coarse
treebank symbols (e.g. NP and VP) into sub-
categories, rather than extracting large tree frag-
ments. As shown in several studies on TSG pars-
ing (Zuidema, 2007; Bansal and Klein, 2010), large
440
tree fragments and symbol refinement work comple-
mentarily for syntactic parsing. For example, Bansal
and Klein (2010) have reported that deterministic
symbol refinement with heuristics helps improve the
accuracy of a TSG parser.
In this paper, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. SR-TSG is an extension of the conventional
TSG model where each nonterminal symbol can be
refined (subcategorized) to fit the training data. Our
work differs from previous studies in that we focus
on a unified model where TSG rules and symbol re-
finement are learned from training data in a fully au-
tomatic and consistent fashion. We also propose a
novel probabilistic SR-TSG model with the hierar-
chical Pitman-Yor Process (Pitman and Yor, 1997),
namely a sort of nonparametric Bayesian model, to
encode backoff smoothing from a fine-grained SR-
TSG to simpler CFG rules, and develop an efficient
training method based on blocked MCMC sampling.
Our SR-TSG parser achieves an F1 score of
92.4% in the WSJ English Penn Treebank pars-
ing task, which is a 7.7 point improvement over a
conventional Bayesian TSG parser, and superior to
state-of-the-art discriminative reranking parsers.
2 Background and Related Work
Our SR-TSG work is built upon recent work on
Bayesian TSG induction from parse trees (Post and
Gildea, 2009; Cohn et al, 2010). We firstly review
the Bayesian TSG model used in that work, and then
present related work on TSGs and symbol refine-
ment.
A TSG consists of a 4-tuple, G = (T,N, S,R),
where T is a set of terminal symbols, N is a set of
nonterminal symbols, S ? N is the distinguished
start nonterminal symbol and R is a set of produc-
tions (a.k.a. rules). The productions take the form
of elementary trees i.e., tree fragments of height
? 1. The root and internal nodes of the elemen-
tary trees are labeled with nonterminal symbols, and
leaf nodes are labeled with either terminal or nonter-
minal symbols. Nonterminal leaves are referred to
as frontier nonterminals, and form the substitution
sites to be combined with other elementary trees.
A derivation is a process of forming a parse tree.
It starts with a root symbol and rewrites (substi-
tutes) nonterminal symbols with elementary trees
until there are no remaining frontier nonterminals.
Figure 1a shows an example parse tree and Figure
1b shows its example TSG derivation. Since differ-
ent derivations may produce the same parse tree, re-
cent work on TSG induction (Post and Gildea, 2009;
Cohn et al, 2010) employs a probabilistic model of
a TSG and predicts derivations from observed parse
trees in an unsupervised way.
A Probabilistic Tree Substitution Grammar
(PTSG) assigns a probability to each rule in the
grammar. The probability of a derivation is defined
as the product of the probabilities of its component
elementary trees as follows.
p (e) =
?
x?e?e
p (e |x) ,
where e = (e1, e2, . . .) is a sequence of elemen-
tary trees used for the derivation, x = root (e) is the
root symbol of e, and p (e |x) is the probability of
generating e given its root symbol x. As in a PCFG,
e is generated conditionally independent of all oth-
ers given x.
The posterior distribution over elementary trees
given a parse tree t can be computed by using the
Bayes? rule:
p (e |t) ? p (t |e) p (e) .
where p (t |e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the task
of TSG induction from parse trees turns out to con-
sist of modeling the prior distribution p (e). Recent
work on TSG induction defines p (e) as a nonpara-
metric Bayesian model such as the Dirichlet Pro-
cess (Ferguson, 1973) or the Pitman-Yor Process to
encourage sparse and compact grammars.
Several studies have combined TSG induction and
symbol refinement. An adaptor grammar (Johnson
et al, 2007a) is a sort of nonparametric Bayesian
TSG model with symbol refinement, and is thus
closely related to our SR-TSG model. However,
an adaptor grammar differs from ours in that all its
rules are complete: all leaf nodes must be termi-
nal symbols, while our model permits nonterminal
symbols as leaf nodes. Furthermore, adaptor gram-
mars have largely been applied to the task of unsu-
pervised structural induction from raw texts such as
441
(a) (b) (c)
Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of
(a). The refinement annotation is hyphenated with a nonterminal symbol.
morphology analysis, word segmentation (Johnson
and Goldwater, 2009), and dependency grammar in-
duction (Cohen et al, 2010), rather than constituent
syntax parsing.
An all-fragments grammar (Bansal and Klein,
2010) is another variant of TSG that aims to uti-
lize all possible subtrees as rules. It maps a TSG
to an implicit representation to make the grammar
tractable and practical for large-scale parsing. The
manual symbol refinement described in (Klein and
Manning, 2003) was applied to an all-fragments
grammar and this improved accuracy in the English
WSJ parsing task. As mentioned in the introduc-
tion, our model focuses on the automatic learning of
a TSG and symbol refinement without heuristics.
3 Symbol-Refined Tree Substitution
Grammars
In this section, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. Our SR-TSG model is an extension of
the conventional TSG model where every symbol of
the elementary trees can be refined to fit the train-
ing data. Figure 1c shows an example of SR-TSG
derivation. As with previous work on TSG induc-
tion, our task is the induction of SR-TSG deriva-
tions from a corpus of parse trees in an unsupervised
fashion. That is, we wish to infer the symbol sub-
categories of every node and substitution site (i.e.,
nodes where substitution occurs) from parse trees.
Extracted rules and their probabilities can be used to
parse new raw sentences.
3.1 Probabilistic Model
We define a probabilistic model of an SR-TSG based
on the Pitman-Yor Process (PYP) (Pitman and Yor,
1997), namely a sort of nonparametric Bayesian
model. The PYP produces power-law distributions,
which have been shown to be well-suited for such
uses as language modeling (Teh, 2006b), and TSG
induction (Cohn et al, 2010). One major issue as
regards modeling an SR-TSG is that the space of the
grammar rules will be very sparse since SR-TSG al-
lows for arbitrarily large tree fragments and also an
arbitrarily large set of symbol subcategories. To ad-
dress the sparseness problem, we employ a hierar-
chical PYP to encode a backoff scheme from the SR-
TSG rules to simpler CFG rules, inspired by recent
work on dependency parsing (Blunsom and Cohn,
2010).
Our model consists of a three-level hierarchy. Ta-
ble 1 shows an example of the SR-TSG rule and its
backoff tree fragments as an illustration of this three-
level hierarchy. The topmost level of our model is a
distribution over the SR-TSG rules as follows.
e |xk ? Gxk
Gxk ? PYP
(
dxk , ?xk , P
sr-tsg (? |xk )
)
,
where xk is a refined root symbol of an elemen-
tary tree e, while x is a raw nonterminal symbol
in the corpus and k = 0, 1, . . . is an index of the
symbol subcategory. Suppose x is NP and its sym-
bol subcategory is 0, then xk is NP0. The PYP has
three parameters: (dxk , ?xk , P
sr-tsg). P sr-tsg (? |xk )
442
SR-TSG SR-CFG RU-CFG
Table 1: Example three-level backoff.
is a base distribution over infinite space of symbol-
refined elementary trees rooted with xk, which pro-
vides the backoff probability of e. The remaining
parameters dxk and ?xk control the strength of the
base distribution.
The backoff probability P sr-tsg (e |xk ) is given by
the product of symbol-refined CFG (SR-CFG) rules
that e contains as follows.
P sr-tsg (e |xk ) =
?
f?F (e)
scf ?
?
i?I(e)
(1? sci)
? H (cfg-rules (e |xk ))
? |xk ? Hxk
Hxk ? PYP
(
dx, ?x, P
sr-cfg (? |xk )
)
,
where F (e) is a set of frontier nonterminal nodes
and I (e) is a set of internal nodes in e. cf and ci
are nonterminal symbols of nodes f and i, respec-
tively. sc is the probability of stopping the expan-
sion of a node labeled with c. SR-CFG rules are
CFG rules where every symbol is refined, as shown
in Table 1. The function cfg-rules (e |xk ) returns
the SR-CFG rules that e contains, which take the
form of xk ? ?. Each SR-CFG rule ? rooted
with xk is drawn from the backoff distribution Hxk ,
and Hxk is produced by the PYP with parameters:(
dx, ?x, P sr-cfg
)
. This distribution over the SR-CFG
rules forms the second level hierarchy of our model.
The backoff probability of the SR-CFG rule,
P sr-cfg (? |xk ), is given by the root-unrefined CFG
(RU-CFG) rule as follows,
P sr-cfg (? |xk ) = I (root-unrefine (? |xk ))
? |x ? Ix
Ix ? PYP
(
d?x, ?
?
x, P
ru-cfg (? |x )
)
,
where the function root-unrefine (? |xk ) returns
the RU-CFG rule of ?, which takes the form of x?
?. The RU-CFG rule is a CFG rule where the root
symbol is unrefined and all leaf nonterminal sym-
bols are refined, as shown in Table 1. Each RU-CFG
rule ? rooted with x is drawn from the backoff distri-
bution Ix, and Ix is produced by a PYP. This distri-
bution over the RU-CFG rules forms the third level
hierarchy of our model. Finally, we set the back-
off probability of the RU-CFG rule, P ru-cfg (? |x),
so that it is uniform as follows.
P ru-cfg (? |x ) =
1
|x? ?|
.
where |x? ?| is the number of RU-CFG rules
rooted with x. Overall, our hierarchical model en-
codes backoff smoothing consistently from the SR-
TSG rules to the SR-CFG rules, and from the SR-
CFG rules to the RU-CFG rules. As shown in (Blun-
som and Cohn, 2010; Cohen et al, 2010), the pars-
ing accuracy of the TSG model is strongly affected
by its backoff model. The effects of our hierarchical
backoff model on parsing performance are evaluated
in Section 5.
4 Inference
We use Markov Chain Monte Carlo (MCMC) sam-
pling to infer the SR-TSG derivations from parse
trees. MCMC sampling is a widely used approach
for obtaining random samples from a probability
distribution. In our case, we wish to obtain deriva-
tion samples of an SR-TSG from the posterior dis-
tribution, p (e |t,d,?, s).
The inference of the SR-TSG derivations corre-
sponds to inferring two kinds of latent variables:
latent symbol subcategories and latent substitution
443
sites. We first infer latent symbol subcategories for
every symbol in the parse trees, and then infer latent
substitution sites stepwise. During the inference of
symbol subcategories, every internal node is fixed as
a substitution site. After that, we unfix that assump-
tion and infer latent substitution sites given symbol-
refined parse trees. This stepwise learning is simple
and efficient in practice, but we believe that the joint
learning of both latent variables is possible, and we
will deal with this in future work. Here we describe
each inference algorithm in detail.
4.1 Inference of Symbol Subcategories
For the inference of latent symbol subcategories, we
adopt split and merge training (Petrov et al, 2006)
as follows. In each split-merge step, each symbol
is split into at most two subcategories. For exam-
ple, every NP symbol in the training data is split into
either NP0 or NP1 to maximize the posterior prob-
ability. After convergence, we measure the loss of
each split symbol in terms of the likelihood incurred
when removing it, then the smallest 50% of the
newly split symbols as regards that loss are merged
to avoid overfitting. The split-merge algorithm ter-
minates when the total number of steps reaches the
user-specified value.
In each splitting step, we use two types of blocked
MCMC algorithm: the sentence-level blocked
Metroporil-Hastings (MH) sampler and the tree-
level blocked Gibbs sampler, while (Petrov et al,
2006) use a different MLE-based model and the EM
algorithm. Our sampler iterates sentence-level sam-
pling and tree-level sampling alternately.
The sentence-level MH sampler is a recently pro-
posed algorithm for grammar induction (Johnson et
al., 2007b; Cohn et al, 2010). In this work, we apply
it to the training of symbol splitting. The MH sam-
pler consists of the following three steps: for each
sentence, 1) calculate the inside probability (Lari
and Young, 1991) in a bottom-up manner, 2) sample
a derivation tree in a top-down manner, and 3) ac-
cept or reject the derivation sample by using the MH
test. See (Cohn et al, 2010) for details. This sampler
simultaneously updates blocks of latent variables as-
sociated with a sentence, thus it can find MAP solu-
tions efficiently.
The tree-level blocked Gibbs sampler focuses on
the type of SR-TSG rules and simultaneously up-
dates all root and child nodes that are annotated
with the same SR-TSG rule. For example, the
sampler collects all nodes that are annotated with
S0 ? NP1VP2, then updates those nodes to an-
other subcategory such as S0 ? NP2VP0 according
to the posterior distribution. This sampler is simi-
lar to table label resampling (Johnson and Goldwa-
ter, 2009), but differs in that our sampler can update
multiple table labels simultaneously when multiple
tables are labeled with the same elementary tree.
The tree-level sampler also simultaneously updates
blocks of latent variables associated with the type of
SR-TSG rules, thus it can find MAP solutions effi-
ciently.
4.2 Inference of Substitution Sites
After the inference of symbol subcategories, we
use Gibbs sampling to infer the substitution sites of
parse trees as described in (Cohn and Lapata, 2009;
Post and Gildea, 2009). We assign a binary variable
to each internal node in the training data, which in-
dicates whether that node is a substitution site or not.
For each iteration, the Gibbs sampler works by sam-
pling the value of each binary variable in random
order. See (Cohn et al, 2010) for details.
During the inference, our sampler ignores
the symbol subcategories of internal nodes of
elementary trees since they do not affect the
derivation of the SR-TSG. For example, the
elementary trees ?(S0 (NP0 NNP0) VP0)? and
?(S0 (NP1 NNP0) VP0)? are regarded as being the
same when we calculate the generation probabilities
according to our model. This heuristics is help-
ful for finding large tree fragments and learning
compact grammars.
4.3 Hyperparameter Estimation
We treat hyperparameters {d,?} as random vari-
ables and update their values for every MCMC it-
eration. We place a prior on the hyperparameters as
follows: d ? Beta (1, 1), ? ? Gamma (1, 1). The
values of d and ? are optimized with the auxiliary
variable technique (Teh, 2006a).
444
5 Experiment
5.1 Settings
5.1.1 Data Preparation
We ran experiments on the Wall Street Journal
(WSJ) portion of the English Penn Treebank data
set (Marcus et al, 1993), using a standard data
split (sections 2?21 for training, 22 for development
and 23 for testing). We also used section 2 as a
small training set for evaluating the performance of
our model under low-resource conditions. Hence-
forth, we distinguish the small training set (section
2) from the full training set (sections 2-21). The tree-
bank data is right-binarized (Matsuzaki et al, 2005)
to construct grammars with only unary and binary
productions. We replace lexical words with count
? 5 in the training data with one of 50 unknown
words using lexical features, following (Petrov et al,
2006). We also split off all the function tags and
eliminated empty nodes from the data set, follow-
ing (Johnson, 1998).
5.1.2 Training and Parsing
For the inference of symbol subcategories, we
trained our model with the MCMC sampler by us-
ing 6 split-merge steps for the full training set and 3
split-merge steps for the small training set. There-
fore, each symbol can be subdivided into a maxi-
mum of 26 = 64 and 23 = 8 subcategories, respec-
tively. In each split-merge step, we initialized the
sampler by randomly splitting every symbol in two
subcategories and ran the MCMC sampler for 1000
iterations. After that, to infer the substitution sites,
we initialized the model with the final sample from
a run on the small training set, and used the Gibbs
sampler for 2000 iterations. We estimated the opti-
mal values of the stopping probabilities s by using
the development set.
We obtained the parsing results with the MAX-
RULE-PRODUCT algorithm (Petrov et al, 2006) by
using the SR-TSG rules extracted from our model.
We evaluated the accuracy of our parser by brack-
eting F1 score of predicted parse trees. We used
EVALB1 to compute the F1 score. In all our exper-
iments, we conducted ten independent runs to train
our model, and selected the one that performed best
on the development set in terms of parsing accuracy.
1http://nlp.cs.nyu.edu/evalb/
Model F1 (small) F1 (full)
CFG 61.9 63.6
*TSG 77.1 85.0
SR-TSG (P sr-tsg) 73.0 86.4
SR-TSG (P sr-tsg, P sr-cfg) 79.4 89.7
SR-TSG (P sr-tsg, P sr-cfg, P ru-cfg) 81.7 91.1
Table 2: Comparison of parsing accuracy with the
small and full training sets. *Our reimplementation
of (Cohn et al, 2010).
Figure 2: Histogram of SR-TSG and TSG rule sizes
on the small training set. The size is defined as the
number of CFG rules that the elementary tree con-
tains.
5.2 Results and Discussion
5.2.1 Comparison of SR-TSG with TSG
We compared the SR-TSG model with the CFG
and TSG models as regards parsing accuracy. We
also tested our model with three backoff hierarchy
settings to evaluate the effects of backoff smoothing
on parsing accuracy. Table 2 shows the F1 scores
of the CFG, TSG and SR-TSG parsers for small and
full training sets. In Table 2, SR-TSG (P sr-tsg) de-
notes that we used only the topmost level of the hi-
erarchy. Similary, SR-TSG (P sr-tsg, P sr-cfg) denotes
that we used only the P sr-tsg and P sr-cfg backoff mod-
els.
Our best model, SR-TSG (P sr-tsg, P sr-cfg, P ru-cfg),
outperformed both the CFG and TSG models on
both the small and large training sets. This result
suggests that the conventional TSG model trained
from the vanilla treebank is insufficient to resolve
445
Model F1 (? 40) F1 (all)
TSG (no symbol refinement)
Post and Gildea (2009) 82.6 -
Cohn et al (2010) 85.4 84.7
TSG with Symbol Refinement
Zuidema (2007) - *83.8
Bansal et al (2010) 88.7 88.1
SR-TSG (single) 91.6 91.1
SR-TSG (multiple) 92.9 92.4
CFG with Symbol Refinement
Collins (1999) 88.6 88.2
Petrov and Klein (2007) 90.6 90.1
Petrov (2010) - 91.8
Discriminative
Carreras et al (2008) - 91.1
Charniak and Johnson (2005) 92.0 91.4
Huang (2008) 92.3 91.7
Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the
development set (? 100).
structural ambiguities caused by coarse symbol an-
notations in a training corpus. As we expected, sym-
bol refinement can be helpful with the TSG model
for further fitting the training set and improving the
parsing accuracy.
The performance of the SR-TSG parser was
strongly affected by its backoff models. For exam-
ple, the simplest model, P sr-tsg, performed poorly
compared with our best model. This result suggests
that the SR-TSG rules extracted from the training
set are very sparse and cannot cover the space of
unknown syntax patterns in the testing set. There-
fore, sophisticated backoff modeling is essential for
the SR-TSG parser. Our hierarchical PYP model-
ing technique is a successful way to achieve back-
off smoothing from sparse SR-TSG rules to simpler
CFG rules, and offers the advantage of automatically
estimating the optimal backoff probabilities from the
training set.
We compared the rule sizes and frequencies of
SR-TSG with those of TSG. The rule sizes of SR-
TSG and TSG are defined as the number of CFG
rules that the elementary tree contains. Figure 2
shows a histogram of the SR-TSG and TSG rule
sizes (by unrefined token) on the small training set.
For example, SR-TSG rules: S1 ? NP0VP1 and
S0 ? NP1VP2 were considered to be the same to-
ken. In Figure 2, we can see that there are almost
the same number of SR-TSG rules and TSG rules
with size = 1. However, there are more SR-TSG
rules than TSG rules with size ? 2. This shows
that an SR-TSG can use various large tree fragments
depending on the context, which is specified by the
symbol subcategories.
5.2.2 Comparison of SR-TSG with Other
Models
We compared the accuracy of the SR-TSG parser
with that of conventional high-performance parsers.
Table 3 shows the F1 scores of an SR-TSG and con-
ventional parsers with the full training set. In Ta-
ble 3, SR-TSG (single) is a standard SR-TSG parser,
446
and SR-TSG (multiple) is a combination of sixteen
independently trained SR-TSG models, following
the work of (Petrov, 2010).
Our SR-TSG (single) parser achieved an F1 score
of 91.1%, which is a 6.4 point improvement over
the conventional Bayesian TSG parser reported by
(Cohn et al, 2010). Our model can be viewed as
an extension of Cohn?s work by the incorporation
of symbol refinement. Therefore, this result con-
firms that a TSG and symbol refinement work com-
plementarily in improving parsing accuracy. Com-
pared with a symbol-refined CFG model such as the
Berkeley parser (Petrov et al, 2006), the SR-TSG
model can use large tree fragments, which strength-
ens the probability of frequent syntax patterns in
the training set. Indeed, the few very large rules of
our model memorized full parse trees of sentences,
which were repeated in the training set.
The SR-TSG (single) is a pure generative model
of syntax trees but it achieved results comparable to
those of discriminative parsers. It should be noted
that discriminative reranking parsers such as (Char-
niak and Johnson, 2005) and (Huang, 2008) are con-
structed on a generative parser. The reranking parser
takes the k-best lists of candidate trees or a packed
forest produced by a baseline parser (usually a gen-
erative model), and then reranks the candidates us-
ing arbitrary features. Hence, we can expect that
combining our SR-TSG model with a discriminative
reranking parser would provide better performance
than SR-TSG alone.
Recently, (Petrov, 2010) has reported that com-
bining multiple grammars trained independently
gives significantly improved performance over a sin-
gle grammar alone. We applied his method (referred
to as a TREE-LEVEL inference) to the SR-TSG
model as follows. We first trained sixteen SR-TSG
models independently and produced a 100-best list
of the derivations for each model. Then, we erased
the subcategory information of parse trees and se-
lected the best tree that achieved the highest likeli-
hood under the product of sixteen models. The com-
bination model, SR-TSG (multiple), achieved an F1
score of 92.4%, which is a state-of-the-art result for
the WSJ parsing task. Compared with discriminative
reranking parsers, combining multiple grammars by
using the product model provides the advantage that
it does not require any additional training. Several
studies (Fossum and Knight, 2009; Zhang et al,
2009) have proposed different approaches that in-
volve combining k-best lists of candidate trees. We
will deal with those methods in future work.
Let us note the relation between SR-CFG, TSG
and SR-TSG. TSG is weakly equivalent to CFG and
generates the same set of strings. For example, the
TSG rule ?S ? (NP NNP) VP? with probability p
can be converted to the equivalent CFG rules as fol-
lows: ?S ? NPNNP VP ? with probability p and
?NPNNP ? NNP? with probability 1. From this
viewpoint, TSG utilizes surrounding symbols (NNP
of NPNNP in the above example) as latent variables
with which to capture context information. The
search space of learning a TSG given a parse tree
is O (2n) where n is the number of internal nodes
of the parse tree. On the other hand, an SR-CFG
utilizes an arbitrary index such as 0, 1, . . . as latent
variables and the search space is larger than that of a
TSG when the symbol refinement model allows for
more than two subcategories for each symbol. Our
experimental results comfirm that jointly modeling
both latent variables using our SR-TSG assists accu-
rate parsing.
6 Conclusion
We have presented an SR-TSG, which is an exten-
sion of the conventional TSG model where each
symbol of tree fragments can be automatically sub-
categorized to address the problem of the condi-
tional independence assumptions of a TSG. We pro-
posed a novel backoff modeling of an SR-TSG
based on the hierarchical Pitman-Yor Process and
sentence-level and tree-level blocked MCMC sam-
pling for training our model. Our best model sig-
nificantly outperformed the conventional TSG and
achieved state-of-the-art result in a WSJ parsing
task. Future work will involve examining the SR-
TSG model for different languages and for unsuper-
vised grammar induction.
Acknowledgements
We would like to thank Liang Huang for helpful
comments and the three anonymous reviewers for
thoughtful suggestions. We would also like to thank
Slav Petrov and Hui Zhang for answering our ques-
tions about their parsers.
447
References
Mohit Bansal and Dan Klein. 2010. Simple, Accurate
Parsing with an All-Fragments Grammar. In In Proc.
of ACL, pages 1098?1107.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP, pages 1204?1213.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. of ACL, 1:173?180.
Shay B Cohen, David M Blei, and Noah A Smith. 2010.
Variational Inference for Adaptor Grammars. In In
Proc. of HLT-NAACL, pages 564?572.
Trevor Cohn and Mirella Lapata. 2009. Sentence Com-
pression as Tree Transduction. Journal of Artificial
Intelligence Research, 34:637?674.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing Tree-Substitution Grammars. Journal
of Machine Learning Research, 11:3053?3096.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29:589?637.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
Tree Adjoining Machine Translation. In Proc. of
EMNLP, page 727.
Thomas S Ferguson. 1973. A Bayesian Analysis of
Some Nonparametric Problems. Annals of Statistics,
1:209?230.
Victoria Fossum and Kevin Knight. 2009. Combining
Constituent Parsers. In Proc. of HLT-NAACL, pages
253?256.
Michel Galley, Mark Hopkins, Kevin Knight, Daniel
Marcu, Los Angeles, and Marina Del Rey. 2004.
What?s in a Translation Rule? Information Sciences,
pages 273?280.
Liang Huang. 2008. Forest Reranking : Discriminative
Parsing with Non-Local Features. In Proc. of ACL,
19104:0.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In In Proc. of HLT-NAACL, pages 317?325.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007a. Adaptor Grammars : A Frame-
work for Specifying Compositional Nonparametric
Bayesian Models. Advances in Neural Information
Processing Systems 19, 19:641?648.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwa-
ter. 2007b. Bayesian Inference for PCFGs via Markov
chain Monte Carlo. In In Proc. of HLT-NAACL, pages
139?146.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D Manning. 2003. Accurate
Unlexicalized Parsing. In Proc. of ACL, 1:423?430.
K Lari and S J Young. 1991. Applications of Stochas-
tic Context-Free Grammars Using the Inside?Outside
Algorithm. Computer Speech and Language, 5:237?
257.
Mitchell P Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. of ACL, pages 75?82.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of ACL, pages
433?440.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Proc. of HLT-NAACL, pages 19?27.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25:855?900.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In In Proc. of ACL-
IJCNLP, pages 45?48.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
YW Teh. 2006b. A Hierarchical Bayesian Language
Model based on Pitman-Yor Processes. In Proc. of
ACL, 44:985?992.
J Tenenbaum, TJ O?Donnell, and ND Goodman. 2009.
Fragment Grammars: Exploring Computation and
Reuse in Language. MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series.
Mengqiu Wang, Noah A Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model ? A Quasi-
Synchronous Grammar for QA. In Proc. of EMNLP-
CoNLL, pages 22?32.
Elif Yamangil and Stuart M Shieber. 2010. Bayesian
Synchronous Tree-Substitution Grammar Induction
and Its Application to Sentence Compression. In In
Proc. of ACL, pages 937?947.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li.
2009. K-Best Combination of Syntactic Parsers. In
Proc. of EMNLP, pages 1552?1560.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proc. of EMNLP-CoNLL, pages 551?560.
448
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1045?1053,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Incremental Joint Approach to Word Segmentation, POS Tagging, and
Dependency Parsing in Chinese
Jun Hatori1 Takuya Matsuzaki2 Yusuke Miyao2 Jun?ichi Tsujii3
1University of Tokyo / 7-3-1 Hongo, Bunkyo, Tokyo, Japan
2National Institute of Informatics / 2-1-2 Hitotsubashi, Chiyoda, Tokyo, Japan
3Microsoft Research Asia / 5 Danling Street, Haidian District, Beijing, P.R. China
hatori@is.s.u-tokyo.ac.jp
{takuya-matsuzaki,yusuke}@nii.ac.jp jtsujii@microsoft.com
Abstract
We propose the first joint model for word segmen-
tation, POS tagging, and dependency parsing for
Chinese. Based on an extension of the incremental
joint model for POS tagging and dependency pars-
ing (Hatori et al, 2011), we propose an efficient
character-based decoding method that can combine
features from state-of-the-art segmentation, POS
tagging, and dependency parsing models. We also
describe our method to align comparable states in
the beam, and how we can combine features of dif-
ferent characteristics in our incremental framework.
In experiments using the Chinese Treebank (CTB),
we show that the accuracies of the three tasks can
be improved significantly over the baseline models,
particularly by 0.6% for POS tagging and 2.4% for
dependency parsing. We also perform comparison
experiments with the partially joint models.
1 Introduction
In processing natural languages that do not include
delimiters (e.g. spaces) between words, word seg-
mentation is the crucial first step that is necessary
to perform virtually all NLP tasks. Furthermore, the
word-level information is often augmented with the
POS tags, which, along with segmentation, form the
basic foundation of statistical NLP.
Because the tasks of word segmentation and POS
tagging have strong interactions, many studies have
been devoted to the task of joint word segmenta-
tion and POS tagging for languages such as Chi-
nese (e.g. Kruengkrai et al (2009)). This is because
some of the segmentation ambiguities cannot be re-
solved without considering the surrounding gram-
matical constructions encoded in a sequence of POS
tags. The joint approach to word segmentation and
POS tagging has been reported to improve word seg-
mentation and POS tagging accuracies by more than
1% in Chinese (Zhang and Clark, 2008). In addition,
some researchers recently proposed a joint approach
to Chinese POS tagging and dependency parsing (Li
et al, 2011; Hatori et al, 2011); particularly, Ha-
tori et al (2011) proposed an incremental approach
to this joint task, and showed that the joint approach
improves the accuracies of these two tasks.
In this context, it is natural to consider further
a question regarding the joint framework: how
strongly do the tasks of word segmentation and de-
pendency parsing interact? In the following Chinese
sentences:
S? ?sV  ?s ?Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1042?1051,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Multiple Dependency Corpora
for Inducing Wide-coverage Japanese CCG Resources
Sumire Uematsu?
uematsu@cks.u-tokyo.ac.jp
Takuya Matsuzaki?
takuya-matsuzaki@nii.ac.jp
Hiroki Hanaoka?
hanaoka@nii.ac.jp
Yusuke Miyao?
yusuke@nii.ac.jp
Hideki Mima?
mima@t-adm.t.u-tokyo.ac.jp
?The University of Tokyo
Hongo 7-3-1, Bunkyo, Tokyo, Japan
?National Institute of Infomatics
Hitotsubashi 2-1-2, Chiyoda, Tokyo, Japan
Abstract
This paper describes a method of in-
ducing wide-coverage CCG resources for
Japanese. While deep parsers with corpus-
induced grammars have been emerging
for some languages, those for Japanese
have not been widely studied, mainly be-
cause most Japanese syntactic resources
are dependency-based. Our method first
integrates multiple dependency-based cor-
pora into phrase structure trees and then
converts the trees into CCG derivations.
The method is empirically evaluated in
terms of the coverage of the obtained lexi-
con and the accuracy of parsing.
1 Introduction
Syntactic parsing for Japanese has been domi-
nated by a dependency-based pipeline in which
chunk-based dependency parsing is applied and
then semantic role labeling is performed on the de-
pendencies (Sasano and Kurohashi, 2011; Kawa-
hara and Kurohashi, 2011; Kudo and Matsumoto,
2002; Iida and Poesio, 2011; Hayashibe et al,
2011). This dominance is mainly because chunk-
based dependency analysis looks most appropriate
for Japanese syntax due to its morphosyntactic ty-
pology, which includes agglutination and scram-
bling (Bekki, 2010). However, it is also true that
this type of analysis has prevented us from deeper
syntactic analysis such as deep parsing (Clark and
Curran, 2007) and logical inference (Bos et al,
2004; Bos, 2007), both of which have been sur-
passing shallow parsing-based approaches in lan-
guages like English.
In this paper, we present our work on induc-
ing wide-coverage Japanese resources based on
combinatory categorial grammar (CCG) (Steed-
man, 2001). Our work is basically an extension of
a seminal work on CCGbank (Hockenmaier and
Steedman, 2007), in which the phrase structure
trees of the Penn Treebank (PTB) (Marcus et al,
1993) are converted into CCG derivations and a
wide-coverage CCG lexicon is then extracted from
these derivations. As CCGbank has enabled a va-
riety of outstanding works on wide-coverage deep
parsing for English, our resources are expected to
significantly contribute to Japanese deep parsing.
The application of the CCGbank method to
Japanese is not trivial, as resources like PTB are
not available in Japanese. The widely used re-
sources for parsing research are the Kyoto corpus
(Kawahara et al, 2002) and the NAIST text corpus
(Iida et al, 2007), both of which are based on the
dependency structures of chunks. Moreover, the
relation between chunk-based dependency struc-
tures and CCG derivations is not obvious.
In this work, we propose a method to integrate
multiple dependency-based corpora into phrase
structure trees augmented with predicate argument
relations. We can then convert the phrase structure
trees into CCG derivations. In the following, we
describe the details of the integration method as
well as Japanese-specific issues in the conversion
into CCG derivations. The method is empirically
evaluated in terms of the quality of the corpus con-
version, the coverage of the obtained lexicon, and
the accuracy of parsing with the obtained gram-
mar. Additionally, we discuss problems that re-
main in Japanese resources from the viewpoint of
developing CCG derivations.
There are three primary contributions of this pa-
per: 1) we show the first comprehensive results for
Japanese CCG parsing, 2) we present a methodol-
ogy for integrating multiple dependency-based re-
1042
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 1: A CCG derivation.
X/Y : f Y : a ? X : fa (>)
Y : a X\Y : a ? X : fa (<)
X/Y : f Y/Z : g ? X/Z : ?x.f(gx) (> B)
Y\Z : g X\Y : f ? X\Z : ?x.f(gx) (< B)
Figure 2: Combinatory rules (used in the current
implementation).
sources to induce CCG derivations, and 3) we in-
vestigate the possibility of further improving CCG
analysis by additional resources.
2 Background
2.1 Combinatory Categorial Grammar
CCG is a syntactic theory widely accepted in the
NLP field. A grammar based on CCG theory con-
sists of categories, which represent syntactic cat-
egories of words and phrases, and combinatory
rules, which are rules to combine the categories.
Categories are either ground categories like S and
NP or complex categories in the form of X/Y or
X\Y , where X and Y are the categories. Cate-
gory X/Y intuitively means that it becomes cat-
egory X when it is combined with another cat-
egory Y to its right, and X\Y means it takes a
category Y to its left. Categories are combined
by applying combinatory rules (Fig. 2) to form
categories for larger phrases. Figure 1 shows a
CCG analysis of a simple English sentence, which
is called a derivation. The verb give is assigned
category S\NP/NP/NP , which indicates that it
takes two NPs to its right, one NP to its left, and fi-
nally becomes S. Starting from lexical categories
assigned to words, we can obtain categories for
phrases by applying the rules recursively.
An important property of CCG is a clear inter-
face between syntax and semantics. As shown in
Fig. 1, each category is associated with a lambda
term of semantic representations, and each com-
binatory rule is associated with rules for semantic
composition. Since these rules are universal, we
can obtain different semantic representations by
switching the semantic representations of lexical
categories. This means that we can plug in a vari-
Sentence S Verb S\$ (e.g. S\NPga)
Noun phrase NP Post particle NPga|o|ni|to\NP
Auxiliary verb S\S
Table 1: Typical categories for Japanese syntax.
Cat. Feature Value Interpretation
NP case ga nominal
o accusative
ni dative
to comitative, complementizer, etc.
nc none
S form stem stem
base base
neg imperfect or negative
cont continuative
vo s causative
Table 2: Features for Japanese syntax (those used
in the examples in this paper).
ety of semantic theories with CCG-based syntactic
parsing (Bos et al, 2004).
2.2 CCG-based syntactic theory for Japanese
Bekki (2010) proposed a comprehensive theory
for Japanese syntax based on CCG. While the the-
ory is based on Steedman (2001), it provides con-
crete explanations for a variety of constructions of
Japanese, such as agglutination, scrambling, long-
distance dependencies, etc. (Fig. 3).
The ground categories in his theory are S, NP,
and CONJ (for conjunctions). Table 1 presents
typical lexical categories. While most of them
are obvious from the theory of CCG, categories
for auxiliary verbs require an explanation. In
Japanese, auxiliary verbs are extensively used to
express various semantic information, such as
tense and modality. They agglutinate to the main
verb in a sequential order. This is explained in
Bekki?s theory by the category S\S combined with
a main verb via the function composition rule
(<B). Syntactic features are assigned to categories
NP and S (Table 2). The feature case represents a
syntactic case of a noun phrase. The feature form
denotes an inflection form, and is necessary for
constraining the grammaticality of agglutination.
Our implementation of the grammar basically
follows Bekki (2010)?s theory. However, as a first
step in implementing a wide-coverage Japanese
parser, we focused on the frequent syntactic con-
structions that are necessary for computing pred-
icate argument relations, including agglutination,
inflection, scrambling, case alternation, etc. Other
details of the theory are largely simplified (Fig. 3),
1043
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 3: A simplified CCG analysis of the sentence ?The ambassador participated in the negotiation.?.
S ? NP/NP (RelExt)
S\NP1 ? NP1/ P1 (RelIn)
S ? S1/S1 (Con)
S\$1\NP1 ? (S1\$1\NP1)/(S1\$1\NP1) (ConCoord)
Figure 4: Type changing rules. The upper two are
for relative clauses and the others for continuous
clauses.
coordination and semantic representation in par-
ticular. The current implementation recognizes
coordinated verbs in continuous clauses (e.g., ??
???????????/he played the pia o and
sang?), but the treatment of other types of coor-
dination is largely simplified. For semantic repre-
sentation, we define predicate argument structures
(PASs) rather than the theory?s formal representa-
tion based on dynamic logic. Sophisticating our
semantic representation is left for future work.
For parsing efficiency, we modified the treat-
ment of some constructions so that empty el-
ements are excluded from the implementation.
First, we define type changing rules to produce
relative and continuous clauses (shown in Fig. 4).
The rules produce almost the same results as the
theory?s treatment, but without using empty ele-
ments (pro, etc.). We also used lexical rules to
treat pro-drop and scrambling. For the sentence in
Fig. 3, the deletion of the nominal phrase (??
?), the dative phrase (???), or both results in
valid sentences, and shuffling the two phrases does
so as well. Lexical entries with the scrambled or
dropped arguments are produced by lexical rules
in our implementation.
2.3 Linguistic resources for Japanese parsing
As described in Sec. 1, dependency-based analysis
has been accepted for Japanese syntax. Research
on Japanese parsing also relies on dependency-
based corpora. Among them, we used the follow-
ing resources in this work.
Kyoto corpus A news text corpus annotated
with morphological information, chunk bound-
Kyoto Corpus 
Chunk 
?? ? government NOM ?? ? ambassador ACC ?? ? negotiation DAT ?? ? ? ? participation do cause PAST 
NAIST Corpus 
Dep. 
Causer ARG-ga ARG-ni 
Figure 5: The Kyoto and NAIST annotations for
?The government had the ambassador participate
in the negotiation.?. Accusatives are labeled as
ARG-ga in causative (see Sec. 3.2).
aries, and dependency relations among chunks
(Fig. 5). The dependencies are classified into four
types: Para (coordination), A (apposition), I (ar-
gument cluster), and Dep (default). Most of the
dependencies are annotated as Dep.
NAIST text corpus A corpus annotated with
anaphora and coreference relations. The same set
as the Kyoto corpus is annotated.1 The corpus
only focuses on three cases: ?ga? (subject), ?o?
(direct object), and ?ni? (indirect object) (Fig. 5).
Japanese particle corpus (JP) (Hanaoka et al,
2010) A corpus annotated with distinct gram-
matical functions of the Japanese particle (postpo-
sition) ?to?. In Japanese, ?to? has many functions,
including a complementizer (similar to ?that?), a
subordinate conjunction (similar to ?then?), a co-
ordination conjunction (similar to ?and?), and a
case marker (similar to ?with?).
2.4 Related work
Research on Japanese deep parsing is fairly lim-
ited. Formal theories of Japanese syntax were
presented by Gunji (1987) based on Head-driven
Phrase Structure Grammar (HPSG) (Sag et al,
2003) and by Komagata (1999) based on CCG, al-
though their implementations in real-world pars-
ing have not been very successful. JACY (Siegel
1In fact, the NAIST text corpus includes additional texts,
but in this work we only use the news text section.
1044
and Bender, 2002) is a large-scale Japanese gram-
mar based on HPSG, but its semantics is tightly
embedded in the grammar and it is not as easy
to systematically switch them as it is in CCG.
Yoshida (2005) proposed methods for extracting
a wide-coverage lexicon based on HPSG from a
phrase structure treebank of Japanese. We largely
extended their work by exploiting the standard
chunk-based Japanese corpora and demonstrated
the first results for Japanese deep parsing with
grammar induced from large corpora.
Corpus-based acquisition of wide-coverage
CCG resources has enjoyed great success for En-
glish (Hockenmaier and Steedman, 2007). In
that method, PTB was converted into CCG-based
derivations from which a wide-coverage CCG lex-
icon was extracted. CCGbank has been used for
the development of wide-coverage CCG parsers
(Clark and Curran, 2007). The same methodology
has been applied to German (Hockenmaier, 2006),
Italian (Bos et al, 2009), and Turkish (C?ak?c?,
2005). Their treebanks are annotated with depen-
dencies of words, the conversion of which into
phrase structures is not a big concern. A notable
contribution of the present work is a method for in-
ducing CCG grammars from chunk-based depen-
dency structures, which is not obvious, as we dis-
cuss later in this paper.
CCG parsing provides not only predicate argu-
ment relations but also CCG derivations, which
can be used for various semantic processing tasks
(Bos et al, 2004; Bos, 2007). Our work consti-
tutes a starting point for such deep linguistic pro-
cessing for languages like Japanese.
3 Corpus integration and conversion
For wide-coverage CCG parsing, we need a)
a wide-coverage CCG lexicon, b) combinatory
rules, c) training data for parse disambiguation,
and d) a parser (e.g., a CKY parser). Since d) is
grammar- and language-independent, all we have
to develop for a new language is a)?c).
As we have adopted the method of CCGbank,
which relies on a source treebank to be converted
into CCG derivations, a critical issue to address is
the absence of a Japanese counterpart to PTB. We
only have chunk-based dependency corpora, and
their relationship to CCG analysis is not clear.
Our solution is to first integrate multiple
dependency-based resources and convert them
into a phrase structure treebank that is independent
ProperNoun 
????? Yeltsin 
NP 
ProperNoun 
??? Russia 
Noun 
???president 
PostP 
? DAT 
PP 
NP 
Aux 
??? not 
VP 
Verb 
?? forgive 
VerbSuffix 
? PASSIVE 
VP Aux 
? PAST 
VP 
?to Russian president Yeltsin? ?(one) was not forgiven? 
Figure 6: Internal structures of a nominal chunk
(left) and a verbal chunk (right).
of CCG analysis (Step 1). Next, we translate the
treebank into CCG derivations (Step 2). The idea
of Step 2 is similar to what has been done with
the English CCGbank, but obviously we have to
address language-specific issues.
3.1 Dependencies to phrase structure trees
We first integrate and convert available Japanese
corpora?namely, the Kyoto corpus, the NAIST
text corpus, and the JP corpus ?into a phrase
structure treebank, which is similar in spirit to
PTB. Our approach is to convert the depen-
dency structures of the Kyoto corpus into phrase
structures and then augment them with syntac-
tic/semantic roles from the other two corpora.
The conversion involves two steps: 1) recogniz-
ing the chunk-internal structures, and (2) convert-
ing inter-chunk dependencies into phrase struc-
tures. For 1), we don?t have any explicit infor-
mation in the Kyoto corpus although, in princi-
ple, each chunk has internal structures (Vadas and
Curran, 2007; Yamada et al, 2010). The lack of
a chunk-internal structure makes the dependency-
to-constituency conversion more complex than a
similar procedure by Bos et al (2009) that con-
verts an Italian dependency treebank into con-
stituency trees since their dependency trees are an-
notated down to the level of each word. For the
current implementation, we abandon the idea of
identifying exact structures and instead basically
rely on the following generic rules (Fig. 6):
Nominal chunks Compound nouns are first
formed as a right-branching phrase and
post-positions are then attached to it.
Verbal chunks Verbal chunks are analyzed as
left-branching structures.
The rules amount to assume that all but the last
word in a compound noun modify the head noun
(i.e., the last word) and that a verbal chunk is typ-
ically in a form V A1 . . . An, where V is a verb
1045
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP 
PP 
Noun 
?? process  
PostPcm  
? ACC  PP 
NP 
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP PP 
Noun 
?? process  
PostPcm  
? ACC  
Para Dep 
?$ proFess froP EirtK to deatK? 
Figure 7: From inter-chunk dependencies to a tree.
(or other predicative word) and Ais are auxiliaries
(see Fig. 6). We chose the left-branching structure
as default for verbal chunks because the semantic
scopes of the auxiliaries are generally in that or-
der (i.e., A1 has the narrowest scope). For both
cases, phrase symbols are percolated upward from
the right-most daughters of the branches (except
for a few cases like punctuation) because in almost
all cases the syntactic head of a Japanese phrase is
the right-most element.
In practice, we have found several patterns of
exceptions for the above rules. We implemented
exceptional patterns as a small CFG and deter-
mined the chunk-internal structures by determin-
istic parsing with the generic rules and the CFG.
For example, two of the rules we came up with are
rule A: Number ? PrefixOfNumber Number
rule B: ClassifierPhrase ? Number Classifier
in the precedence: rule A > B > generic rules.
Using the above, we bracket a compound noun
? ? ? ??
approximately thousand people death
PrefixOfNumber Number Classifier CommonNoun
?death of approximately one thousand people?
as in
(((? ?) ?) ??)
(((approximately thousand) people) death)
We can improve chunk-internal structures to some
extent by refining the CFG rules. A complete solu-
tion like the manual annotation by Vadas and Cur-
ran (2007) is left for future work.
The conversion of inter-chunk dependencies
into phrase structures may sound trivial, but it is
not necessarily easy when combined with chunk-
internal structures. The problem is to which node
in the internal structure of the head the dependent
dep modifier-type precedence
Para ??/PostPcm ??/PostPcm, */(Verb|Aux), ...
Dep */PostPcm */(Verb|Aux), */Noun, ...
Dep */PostPadnom */Noun, */(Verb|Aux), ...
Table 3: Rules to determine adjoin position.
PP 
Noun 
? dog 
PostP 
? DAT 
VP 
NP 
Adj  
?? 
white  
NP 
VP 
Noun 
? 
cat  
Verb 
?? say  
Aux 
? PAST 
VP PP 
Verb 
?? go!  
PostP 
? 
CMP  
ARG - to 
ARG - ni 
ARG - ga 
ARG - ga 
ARG - ga 
ARG - ga ARG - ni 
ARG - CLS  
NAIST 
JP 
Figure 8: Overlay of pred-arg structure annotation
(?The white cat who said ?Go!? to the dog.?).
tree is adjoined (Fig. 7). In the case shown in the
figure, three chunks are in the dependency relation
indicated by arrows on the top. The dotted arrows
show the nodes to which the subtrees are adjoined.
Without any human-created resources, we can-
not always determine the adjoin positions cor-
rectly. Therefore, as a compromise, we wound up
implementing approximate heuristic rules to deter-
mine the adjoin positions. Table 3 shows examples
of such rules. A rule specifies a precedence of the
possible adjoin nodes as an ordered list of patterns
on the lexical head of the subtree under an ad-
join position. The precedence is defined for each
combination of the type of the dependent phrase,
which is determined by its lexical head, and the
dependency type in the Kyoto corpus.
To select the adjoin position for the left-most
subtree in Fig. 7, for instance, we look up the
rule table using the dependency type, ?Para?, and
the lexical head of the modifier subtree, ? ??
/PostPcm?, as the key, and find the precedence ??
?/PostPcm, */(Verb|Aux), ...?. We thus select the
PP-node on the middle subtree indicated by the
dotted arrow because its lexical head (the right-
most word), ? ??/PostPcm?, matches the first
pattern in the precedence list. In general, we seek
for an adjoin node for each pattern p in the prece-
dence list, until we find a first match.
The semantic annotation given in the NAIST
corpus and the JP corpus is overlaid on the phrase
structure trees with slight modifications (Fig. 8).
1046
PP 
Noun 
?? negotiation 
PostPcm  
? DAT VP Noun 
?? participation  
Verb 
? do 
VerbSuffix 
? CAUSE  
Aux 
? PAST 
VP 
VP 
S 
NPni 
NP 
?? negotiation 
T1 
? DAT T4 
T5 
?? participation  
S?S 
? do 
S?S 
? CAUSE  
S?S 
? PAST 
T3 
T2 
S ? 
? 
?  or   ?B   
?  or   ?B   
?  or   ?B   
NPni 
NPnc 
?? negotiation 
NPni?NPnc 
? DAT 
Svo_s?NPni 
Svo_s?NPni 
?? participation  
Svo_s?Svo_s 
? do 
Scont?Svo_s 
? CAUSE  
Sbase?Scont 
? PAST 
Scont?NPni 
Sbase?NPni 
Sbase 
Step 2 - 1  
Step 2 - 2, 2 - 3  
Figure 9: A phrase structure into a CCG deriva-
tion.
In the figure, the annotation given in the two cor-
pora is shown inside the dotted box at the bottom.
We converted the predicate-argument annotations
given as labeled word-to-word dependencies into
the relations between the predicate words and their
argument phrases. The results are thus similar to
the annotation style of PropBank (Palmer et al,
2005). In the NAIST corpus, each pred-arg re-
lation is labeled with the argument-type (ga/o/ni)
and a flag indicating that the relation is medi-
ated by either a syntactic dependency or a zero
anaphora. For a relation of a predicate wp and its
argument wa in the NAIST corpus, the boundary
of the argument phrase is determined as follows:
1. If wa precedes wp and the relation is medi-
ated by a syntactic dep., select the maximum
PP that is formed by attaching one or more
postpositions to the NP headed by wa.
2. If wp precedes wa or the relation is mediated
by a zero anaphora, select the maximum NP
headed by wa that does not include wp.
In the figure, ??/dog?/DAT? is marked as the ni-
argument of the predicate ???/say? (Case 1), and
???/white ?/cat? is marked as its ga-argument
(Case 2). Case 1 is for the most basic construction,
where an argument PP precedes its predicate. Case
VP 
??    ? 
friend- DAT 
PP VP 
?? 
meet - BASE  
NPni ? 
VP 
10 ?    ? 
10 o?clock - TIME  
PP VP 
?? 
meet - BASE  
T/T ? 
X  
S 
??    ? 
friend- DAT 
NPni S?NPni 
?? 
meet - BASE  
S 
10 ?    ? 
10 o?clock - TIME  
S?S S 
?? 
meet - BASE  
?(to) Peet at ten? 
?(to) Peet a friend? 
Figure 10: An argument post particle phrase (PP)
(upper) and an adjunct PP (lower).
2 covers the relative clause construction, where a
relative clause precedes the head NP, the modifi-
cation of a noun by an adjective, and the relations
mediated by zero anaphora.
The JP corpus provides only the function label
to each particle ?to? in the text. We determined
the argument phrases marked by the ?to? particles
labeled as (nominal or clausal) argument-markers
in a similar way to Case 1 above and identified the
predicate words as the lexical heads of the phrases
to which the PPto phrases attach.
3.2 Phrase structures to CCG derivations
This step consists of three procedures (Fig. 9):
1. Add constraints on categories and features
to tree nodes as far as possible and assign a
combinatory rule to each branching.
2. Apply combinatory rules to all branching and
obtain CCG derivations.
3. Add feature constraints to terminal nodes.
3.2.1 Local constraint on derivations
According to the phrase structures, the first proce-
dure in Step 2 imposes restrictions on the resulting
CCG derivations. To describe the restrictions, we
focus on some of the notable constructions and il-
lustrate the restrictions for each of them.
Phrases headed by case marker particles A
phrase of this type must be either an argument
(Fig. 10, upper) or a modifier (Fig. 10, lower) of a
predicative. Distinction between the two is made
based on the pred-arg annotation of the predica-
tive. If a phrase is found to be an argument, 1) cat-
egory NP is assigned to the corresponding node,
2) the case feature of the category is given accord-
ing to the particle (in the case of Fig. 10 (upper),
1047
VP 
Verb 
?? 
Speak - NEG  
Aux 
??? 
not- CONT  
Aux 
? 
PAST- BASE  
VP 
S cont ?S  
S ba s e ?S  
?did not speak? 
? or ?B  
? or ?B  S cont ?NPg a  
Sneg ?NPg a  
?? 
Speak - NEG  
S cont ?Sneg  
??? 
not- CONT  
S b ase ?S cont  
? 
PAST- BASE  
S b ase ?NPg a  
Figure 11: An auxiliary verb and its conversion.
VP 
Verb 
?? inquire - NEG  
VerbSuffix 
??? cause - BASE  
??    ? her - DAT 
PP 
VP 
ARG - ga  
?(to) Kave Ker inTuire? 
? 
S?NPni[1 ] 
S?S 
??? cause - BASE  
??    ? her - DAT 
NPni[1 ] 
S 
S?NPni[1 ] 
?? inquire - NEG  
ga         [1]  
NPni[1 ] 
ga: [1 ] 
Figure 12: A causative construction.
ni for dative), and 3) the combinatory rule that
combines the particle phrase and the predicative
phrase is assigned backward function application
rule (<). Otherwise, a category T/T is assigned to
the corresponding modifier node and the rule will
be forward function application (>).
Auxiliary verbs As described in Sec. 2.2, an
auxiliary verb is always given the category S\S
and is combined with a verbal phrase via < or <B
(Fig. 11). Furthermore, we assign the form feature
value of the returning category S according to the
inflection form of the auxiliary. In the case shown
in the figure, Sbase\S is assigned for ??/PAST-
BASE? and Scont\S for ????/not-CONT?. As
a result of this restriction, we can obtain condi-
tions for every auxiliary agglutination because the
two form values in S\S are both restricted after
applying combinatory rules (Sec. 3.2.2).
Case alternations In addition to the argu-
ment/adjunct distinction illustrated above, a pro-
cess is needed for argument phrases of predicates
involving case alternation. Such predicates are
either causative (see Fig. 12) or passive verbs
and can be detected by voice annotation from the
NAIST corpus. For an argument of that type of
verb, its deep case (ga for Fig. 12) must be used
to construct the semantic representation, namely
the PAS. As well as assigning the shallow case
value (ni in Fig. 12) to the argument?s category
NP, as usual, we assign a restriction to the PAS
S?NPo[1] 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
NP 
? 
book 
NP 
NP[1]?NP[1] 
VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
Noun 
? 
book 
NP 
S?NP[1] 
NP[1]?NP[1] 
Noun 
? 
store 
NP 
?   ? 
book-ACC 
PP VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
VP X 
S 
NP?NP 
NP 
? 
store 
NP 
NP?NP 
?    ? 
book-ACC 
NPo 
S 
S?NPo 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
?a store wKere (,) EougKt tKe EooN? 
?a EooN wKiFK (,) EougKt? 
Figure 13: A relative clause with/without argu-
ment extraction (upper/lower, respectively).
of the verb so that the semantic argument corre-
sponding to the deep case is co-indexed with the
argument NP. These restrictions are then utilized
for PAS construction in Sec. 3.2.3.
Relative clauses A relative clause can be de-
tected as a subtree that has a VP as its left child
and an NP as its right child, as shown in Fig. 13.
The conversion of the subtree consists of 1) in-
serting a node on the top of the left VP (see the
right-hand side of Fig. 13), and 2) assigning the
appropriate unary rule to make the new node. The
difference between candidate rules RelExt and Re-
lIn (see Fig. 4) is whether the right-hand NP is
an obligatory argument of the VP or not, which
can be determined by the pred-arg annotation on
the predicate in the VP. In the upper example in
Fig. 13, RelIn is assigned because the right NP
?book? is annotated as an accusative argument of
the predicate ?buy?. In contrast, RelExt is as-
signed in the lower side in the figure because the
right NP ?store? is not annotated as an argument.
Continuous clauses A continuous clause can be
detected as a subtree with a VP of continuous form
as its left child and a VP as its right child. Its
conversion is similar to that of a relative clause,
and only differs in that the candidate rules are Con
and ConCoord. ConCoord generates a continu-
ous clause that shares arguments with the main
clause while Con produces one without shared ar-
guments. Rule assignment is done by comparing
the pred-arg annotations of the two phrases.
1048
Training Develop. Test
#Sentences 24,283 4,833 9,284
#Chunks 234,685 47,571 89,874
#Words 664,898 136,585 255,624
Table 4: Statistics of input linguistic resources.
3.2.2 Inverse application of rules
The second procedure in Step 2 begins with as-
signing a category S to the root node. A combi-
natory rule assigned to each branching is then ?in-
versely? applied so that the constraint assigned to
the parent transfers to the children.
3.2.3 Constraints on terminal nodes
The final process consists of a) imposing restric-
tions on the terminal category in order to instan-
tiate all the feature values, and b) constructing a
PAS for each verbal terminal. An example of pro-
cess a) includes setting the form features in the
verb category, such as S\NPni, according to the
inflection form of the verb. As for b), arguments
in a PAS are given according to the category and
the partial restriction. For instance, if a category
S\NPni is obtained for ???/inquire? (Fig. 12),
the PAS for ?inquire? is unary because the cate-
gory has one argument category (NPni), and the
category is co-indexed with the semantic argument
ga in the PAS due to the partial restriction depicted
in Sec. 3.2.1. As a result, a lexical entry is ob-
tained as?? ` S\NPni[1]: inquire([1]).
3.3 Lexical entries
Finally, lexical rules are applied to each of the ob-
tained lexical entries in order to reduce them to
the canonical form. Since words in the corpus (es-
pecially verbs) often involve pro-drop and scram-
bling, there are a lot of obtained entries that have
slightly varied categories yet share a PAS. We as-
sume that an obtained entry is a variation of the
canonical one and register the canonical entries in
the lexicon. We treat only subject deletion for pro-
drop because there is not sufficient information to
judge the deletion of other arguments. Scrambling
is simply treated as permutation of arguments.
4 Evaluation
We used the following for the implementation of
our resources: Kyoto corpus ver. 4.02, NAIST text
2http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?
Kyoto\%20University\%20Text\%20Corpus
Training Develop. Test
St.1 St.2 St.1 St.2 St.1 St.2
Sent. 24,283 24,116 4,833 4,803 9,284 9,245
Converted 24,116 22,820 4,803 4,559 9,245 8,769
Con. rate 99.3 94.6 99.4 94.9 99.6 94.9
Table 5: Statistics of corpus conversion.
Sentential Coverage
Covered Uncovered Cov. (%)
Devel. 3,920 639 85.99
Test 7,610 1,159 86.78
Lexical Coverage
Word Known Unknown
combi. cat. word
Devel. 127,144 126,383 682 79 0
Test 238,083 236,651 1,242 145 0
Table 6: Sentential and lexical coverage.
corpus ver. 1.53, and JP corpus ver. 1.04. The
integrated corpus is divided into training, devel-
opment, and final test sets following the standard
data split in previous works on Japanese depen-
dency parsing (Kudo and Matsumoto, 2002). The
details of these resources are shown in Table 4.
4.1 Corpus conversion and lexicon extraction
Table 5 shows the number of successful conver-
sions performed by our method. In total, we ob-
tained 22,820 CCG derivations from 24,283 sen-
tences (in the training set), resulting in the to-
tal conversion rate of 93.98%. The table shows
we lost more sentences in Step 2 than in Step 1.
This is natural because Step 2 imposed more re-
strictions on resulting structures and therefore de-
tected more discrepancies including compounding
errors. Our conversion rate is about 5.5 points
lower than the English counterpart (Hockenmaier
and Steedman, 2007). Manual investigation of the
sampled derivations would be beneficial for the
conversion improvement.
For the lexicon extraction from the CCGbank,
we obtained 699 types of lexical categories from
616,305 word tokens. After lexical reduction, the
number of categories decreased to 454, which in
turn may produce 5,342 categories by lexical ex-
pansion. The average number of categories for a
word type was 11.68 as a result.
4.2 Evaluation of coverage
Following the evaluation criteria in (Hockenmaier
and Steedman, 2007), we measured the coverage
3http://cl.naist.jp/nldata/corpus/
4https://alaginrc.nict.go.jp/resources/tocorpus/
tocorpusabstract.html
1049
of the grammar on unseen texts. First, we obtained
CCG derivations for evaluation sets by applying
our conversion method and then used these deriva-
tions as gold standard. Lexical coverage indicates
the number of words to which the grammar assigns
a gold standard category. Sentential coverage indi-
cates the number of sentences in which all words
are assigned gold standard categories 5.
Table 6 shows the evaluation results. Lexical
coverage was 99.40% with rare word treatment,
which is in the same level as the case of the En-
glish CCG parser C&C (Clark and Curran, 2007).
We also measured coverage in a ?weak? sense,
which means the number of sentences that are
given at least one analysis (not necessarily cor-
rect) by the obtained grammar. This number was
99.12 % and 99.06 % for the development and the
test set, respectively, which is sufficiently high for
wide-coverage parsing of real-world texts.
4.3 Evaluation of parsing accuracy
Finally, we evaluated the parsing accuracy. We
employed the parser and the supertagger of
(Miyao and Tsujii, 2008), specifically, its gen-
eralized modules for lexicalized grammars. We
trained log-linear models in the same way as
(Clark and Curran, 2007) using the training set as
training data. Feature sets were simply borrowed
from an English parser; no tuning was performed.
Following conventions in research on Japanese de-
pendency parsing, gold morphological analysis re-
sults were input to a parser. Following C&C, the
evaluation measure was precision and recall over
dependencies, where a dependency is defined as a
4-tuple: a head of a functor, a functor category, an
argument slot, and a head of an argument.
Table 7 shows the parsing accuracy on the de-
velopment and the test sets. The supertagging ac-
curacy is presented in the upper table. While our
coverage was almost the same as C&C, the perfor-
mance of our supertagger and parser was lower.
To improve the performance, tuning disambigua-
tion models for Japanese is a possible approach.
Comparing the parser?s performance with previ-
ous works on Japanese dependency parsing is dif-
ficult as our figures are not directly comparable
to theirs. Sassano and Kurohashi (2009) reported
the accuracy of their parser as 88.48 and 95.09
5Since a gold derivation can logically be obtained if gold
categories are assigned to all words in a sentence, sentential
coverage means that the obtained lexicon has the ability to
produce exactly correct derivations for those sentences.
Supertagging accuracy
Lex. Cov. Cat. Acc.
Devel. 99.40 90.86
Test 99.40 90.69
C&C 99.63 94.32
Overall performance
LP LR LF UP UR UF
Devel. 82.55 82.73 82.64 90.02 90.22 90.12
Test 82.40 82.59 82.50 89.95 90.15 90.05
C&C 88.34 86.96 87.64 93.74 92.28 93.00
Table 7: Parsing accuracy. LP, LR and LF refer to
labeled precision, recall, and F-score respectively.
UP, UR, and UF are for unlabeled.
in unlabeled chunk-based and word-based F1 re-
spectively. While our score of 90.05 in unlabeled
category dependency seems to be lower than their
word-based score, this is reasonable because our
category dependency includes more difficult prob-
lems, such as whether a subject PP is shared by
coordinated verbs. Thus, our parser is expected to
be capable of real-world Japanese text analysis as
well as dependency parsers.
5 Conclusion
In this paper, we proposed a method to induce
wide-coverage Japanese resources based on CCG
that will lead to deeper syntactic analysis for
Japanese and presented empirical evaluation in
terms of the quality of the obtained lexicon and
the parsing accuracy. Although our work is basi-
cally in line with CCGbank, the application of the
method to Japanese is not trivial due to the fact that
the relationship between chunk-based dependency
structures and CCG derivations is not obvious.
Our method integrates multiple dependency-
based resources to convert them into an integrated
phrase structure treebank. The obtained treebank
is then transformed into CCG derivations. The
empirical evaluation in Sec. 4 shows that our cor-
pus conversion successfully converts 94 % of the
corpus sentences and the coverage of the lexicon
is 99.4 %, which is sufficiently high for analyz-
ing real-world texts. A comparison of the parsing
accuracy with previous works on Japanese depen-
dency parsing and English CCG parsing indicates
that our parser can analyze real-world Japanese
texts fairly well and that there is room for improve-
ment in disambiguation models.
1050
References
Daisuke Bekki. 2010. Formal Theory of Japanese Syn-
tax. Kuroshio Shuppan. (In Japanese).
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING 2004, pages
1240?1246.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cate-
gorial grammar treebank for Italian. In Proceedings
of the Eighth International Workshop on Treebanks
and Linguistic Theories (TLT8), pages 27?38.
Johan Bos. 2007. Recognising textual entailment and
computational semantics. In Proceedings of Seventh
International Workshop on Computational Seman-
tics IWCS-7, page 1.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of ACL Stu-
dent Research Workshop, pages 73?78.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Takao Gunji. 1987. Japanese Phrase Structure Gram-
mar: A Unification-based Approach. D. Reidel.
Hiroki Hanaoka, Hideki Mima, and Jun?ichi Tsujii.
2010. A Japanese particle corpus built by example-
based annotation. In Proceedings of LREC 2010.
Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type.
In Proceedings of IJCNLP 2011, pages 201?209.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of the Joint Conference of COLING/ACL
2006.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of ACL-HLT 2011, pages 804?813.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text
corpus with predicate-argument and coreference re-
lations. In Proceedings of Linguistic Annotation
Workshop, pages 132?139.
Daisuke Kawahara and Sadao Kurohashi. 2011. Gen-
erative modeling of coordination by factoring paral-
lelism and selectional preferences. In Proceedings
of IJCNLP 2011.
Daisuke Kawahara, Sadao Kurohashi, and Koiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 495?498. (In Japanese).
Nobo Komagata. 1999. Information Structure in Texts:
A Computational Analysis of Contextual Appropri-
ateness in English and Japanese. Ph.D. thesis, Uni-
versity of Pennsylvania.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analyisis using cascaded chunking. In
Proceedings of CoNLL 2002.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction, 2nd
Edition. CSLI Publications.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of IJCNLP 2011.
Manabu Sassano and Sadao Kurohashi. 2009. A uni-
fied single scan algorithm for Japanese base phrase
chunking and dependency parsing. In Proceedings
of ACL-IJCNLP 2009.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL 2007, pages 240?247.
Emiko Yamada, Eiji Aramaki, Takeshi Imai, and
Kazuhiko Ohe. 2010. Internal structure of a disease
name and its application for ICD coding. Studies
in health technology and informatics, 160(2):1010?
1014.
Kazuhiro Yoshida. 2005. Corpus-oriented develop-
ment of Japanese HPSG parsers. In Proceedings of
the ACL Student Research Workshop.
1051
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 273?277,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building Japanese Textual Entailment Specialized Data Sets
for Inference of Basic Sentence Relations
Kimi Kaneko ? Yusuke Miyao ? Daisuke Bekki ?
? Ochanomizu University, Tokyo, Japan
? National Institute of Informatics, Tokyo, Japan
? {kaneko.kimi | bekki}@is.ocha.ac.jp
? yusuke@nii.ac.jp
Abstract
This paper proposes a methodology for
generating specialized Japanese data sets
for textual entailment, which consists of
pairs decomposed into basic sentence rela-
tions. We experimented with our method-
ology over a number of pairs taken from
the RITE-2 data set. We compared
our methodology with existing studies
in terms of agreement, frequencies and
times, and we evaluated its validity by in-
vestigating recognition accuracy.
1 Introduction
In recognizing textual entailment (RTE), auto-
mated systems assess whether a human reader
would consider that, given a snippet of text t1 and
some unspecified (but restricted) world knowl-
edge, a second snippet of text t2 is true. An ex-
ample is given below.
Ex. 1) Example of a sentence pair for RTE
? Label: Y
? t1: Shakespeare wrote Hamlet and Macbeth.
? t2: Shakespeare is the author of Hamlet.
?Label? on line 1 shows whether textual entail-
ment (TE) holds between t1 and t2. The pair is
labeled ?Y? if the pair exhibits TE and ?N? other-
wise.
It is difficult for computers to make such as-
sessments because pairs have multiple interrelated
basic sentence relations (BSRs, for detailed in-
formation on BSRs, see section 3). Recognizing
each BSRs in pairs exactly is difficult for com-
puters. Therefore, we should generate special-
ized data sets consisting of t1-t2 pairs decomposed
into BSRs and a methodology for generating such
data sets since such data and methodologies for
Japanese are unavailable at present.
This paper proposes a methodology for gener-
ating specialized Japanese data sets for TE that
consist of monothematic t1-t2 pairs (i.e., pairs in
which only one BSR relevant to the entailment
relation is highlighted and isolated). In addition,
we compare our methodology with existing stud-
ies and analyze its validity.
2 Existing Studies
Sammons et al(2010) point out that it is necessary
to establish a methodology for decomposing pairs
into chains of BSRs, and that establishing such
methodology will enable understanding of how
other existing studies can be combined to solve
problems in natural language processing and iden-
tification of currently unsolvable problems. Sam-
mons et al experimented with their methodology
over the RTE-5 data set and showed that the recog-
nition accuracy of a system trained with their spe-
cialized data set was higher than that of the system
trained with the original data set. In addition, Ben-
tivogli et al(2010) proposed a methodology for
classifying more details than was possible in the
study by Sammons et al.
However, these studies were based on only En-
glish data sets. In this regard, the word-order
rules and the grammar of many languages (such
as Japanese) are different from those of English.
We thus cannot assess the validity of methodolo-
gies for any Japanese data set because each lan-
guage has different usages. Therefore, it is neces-
sary to assess the validity of such methodologies
with specialized Japanese data sets.
Kotani et al (2008) generated specialized
Japanese data sets for RTE that were designed
such that each pair included only one BSR. How-
ever, in that approach the data set is generated ar-
tificially, and BSRs between pairs of real world
texts cannot be analyzed.
We develop our methodology by generating
specialized data sets from a collection of pairs
from RITE-21 binary class (BC) subtask data sets
containing sentences from Wikipedia. RITE-2 is
273
an evaluation-based workshop focusing on RTE.
Four subtasks are available in RITE-2, one of
which is the BC subtask whereby systems assess
whether there is TE between t1 and t2. The rea-
son why we apply our methodology to part of the
RITE-2 BC subtask data set is that we can con-
sider the validity of the methodology in view of
the recognition accuracy by using the data sets
generated in RITE-2 tasks, and that we can an-
alyze BSRs in real texts by using sentence pairs
extracted from Wikipedia.
3 Methodology
In this study, we extended and refined the method-
ology defined in Bentivogli et al(2010) and devel-
oped a methodology for generating Japanese data
sets broken down into BSRs and non-BSRs as de-
fined below.
Basic sentence relations (BSRs):
? Lexical: Synonymy, Hypernymy, Entailment,
Meronymy;
? Phrasal: Synonymy, Hypernymy, Entailment,
Meronymy, Nominalization, Corference;
? Syntactic: Scrambling, Case alteration, Modi-
fier, Transparent head, Clause, List, Apposi-
tion, Relative clause;
? Reasoning: Temporal, Spatial, Quantity, Im-
plicit relation, Inference;
Non-basic sentence relations (non-BSRs)?
? Disagreement: Lexical, Phrasal, Modal, Mod-
ifier, Temporal, Spatial, Quantity;
Mainly, we used relations defined in Bentivogli
et al(2010) and divided Synonymy, Hypernymy,
Entailment and Meronymy into Lexical and
Phrasal. The differences between our study and
Bentivogli et al(2010) are as follows. Demonymy
and Statements in Bentivogli et al(2010) were
not considered in our study because they were
not necessary for Japanese data sets. In addi-
tion, Scrambling, Entailment, Disagreement:
temporal, Disagreement: spatial and Disagree-
ment: quantity were newly added in our study.
Scrambling is a rule for changing the order of
phrases and clauses. Entailment is a rule whereby
the latter sentence is true whenever the former is
true (e.g., ?divorce?? ?marry?). Entailment is a
rule different from Synonymy, Hypernymy and
Meronymy.
The rules for decomposition are schematized as
follows:
1http://www.cl.ecei.tohoku.ac.jp/rite2/doku.php
? Break down pairs into BSRs in order to bring
t1 close to t2 gradually, as the interpretation
of the converted sentence becomes wider
? Label each pair of BSRs or non-BSRs
such that each pair is decomposed to ensure
that there are not multiple BSRs
An example is shown below, where the underlined
parts represent the revised points.
t1? ???????? ????? ? ????? ????
Shakespearenom Hamlet com Macbethacc writepast?Shakespeare wrote Hamlet and Macbeth.?[List] ???????? ?????? ????
Shakespearenom Hamletacc writepast?Shakespeare wrote Hamlet.?
t2?[Synonymy] ???????? ?????? ?? ????
?phrasal Shakespearenom Hamletgen authorcomp becop?Shakespeare is the author of Hamlet.?
Table 1: Example of a pair with TE
An example of a pair without TE is shown below.
t1? ?????? ???????? ???
Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Entailment] ?????? ???? ????
? phrasal Bulgarianom continental.statecomp becop?Bulgaria is a continental state.?
t2?[Disagreement] ?????? ?? ????
?lexical Bulgarianom island.countrycomp becop?Bulgaria is an island country.?
Table 2: Example of a pair without TE (Part 1)
To facilitate TE assessments like Table 3, non-
BSR labels were used in decomposing pairs. In
addition, we allowed labels to be used several
times when some BSRs in a pair are related to ?N?
assessments.
t1? ?????? ???????? ???
Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Disagreement] ?????? ???????? ???
?modal Bulgarianom Eurasia.continentdat becop?neg?Bulgaria is not on the Eurasian continent.?
t2?[Synonymy] ?????? ?????? ?????
?lexical Bulgarianom Europedat belongcop?neg?Bulgaria does not belong to Europe.?
Table 3: Example of a pair without TE (Part 2)
As mentioned above, the idea here is to decom-
pose pairs in order to bring t1 closer to t2, the
latter of which in principle has a wider semantic
scope. We prohibited the conversion of t2 because
it was possible to decompose the pairs such that
they could be true even if there was no TE. Never-
theless, since it is sometimes easier to convert t2,
274
we allowed the conversion of t2 in only the case
that t1 contradicted t2 and the scope of t2 did not
overlap with that of t1 even if t2 was converted and
TE would be unchanged. An example in case that
we allowed to convert t2 is shown below. Bold-
faced types in Table 4 shows that it becomes easy
to compare t1 with t2 by converting to t2.
t1? ??? ?????? ???????
Tomnom today breakfastacc eatpast?neg?Tom didn?t eat breakfast today.?
[Scrambling] ??? ??? ??? ???????
today Tomnom breakfastacc eatpast?neg?Today, Tom didn?t eat breakfast.?
t2? ??? ??? ??? ????
this.morning Tomnom breadacc eatpast?This morning, Tom ate bread and salad.?
[Entailment] ??? ??? ??? ????
?phrasal today Tomnom breakfastacc eatpast?Today, Tom ate breakfast.?
[Disagreement] ?????????????
?modal ?Today, Tom ate breakfast.?
Table 4: Example of conversion of t2
4 Results
4.1 Comparison with Existing Studies
We applied our methodology to 173 pairs from the
RITE-2 BC subtask data set. The pairs were de-
composed by one annotator, and the decomposed
pairs were assigned labels by two annotators. Dur-
ing labeling, we used the labels presented in Sec-
tion 3 and ?unknown? in cases where pairs could
not be labeled. Our methodology was developed
based on 112 pairs, and by using the other 61 pairs,
we evaluated the inter-annotator agreement as well
as the frequencies and times of decomposition.
The agreement for 241 monothematic pairs gen-
erated from 61 pairs amounted to 0.83 and was
computed as follows. The kappa coefficient for
them amounted 0.81.
Agreement = ?Agreed?? labels/Total 2
Bentivogli et al (2010) reported an agreement
rate of 0.78, although they computed the agree-
ment by using the Dice coefficient (Dice, 1945),
and therefore the results are not directly compara-
ble to ours. Nevertheless, the close values suggest
2Because the ?Agreed? pairs were clear to be classi-
fied as ?Agreed?, where ?Total? is the number of pairs la-
beled ?Agreed? subtracted from the number of labeled pairs.
?Agreed? labels is the number of pairs labeled ?Agreed? sub-
tract from the number of pairs with the same label assigned
by the two annotators.
that our methodology is comparable to that in Ben-
tivogli?s study in terms of agreement.
Table 5 shows the distribution of monothematic
pairs with respect to original Y/N pairs.
Or
igin
alp
air
s Monothematic pairs
Y N Total
Y (32) 116 ? 116
N (29) 96 29 125
Total (61) 212 29 241
Table 5: Distribution of monothematic pairs with
respect to original Y/N pairs
When the methodology was applied to 61 pairs,
a total of 241 and an average of 3.95 monothe-
matic pairs were derived. The average was slightly
greater than the 2.98 reported in (Bentivogli et al,
2010). For pairs originally labeled ?Y? and ?N?, an
average of 3.62 and 3.31 monothematic pairs were
derived, respectively. Both average values were
slightly higher than the values of 3.03 and 2.80 re-
ported in (Bentivogli et al, 2010). On the basis of
the small differences between the average values
in our study and those in (Bentivogli et al, 2010),
we are justified in saying that our methodology is
valid.
Table 6 3 shows the distribution of BSRs in t1-
t2 pairs in an existing study and the present study.
We can see from Table 6 thatCorferencewas seen
more frequently in Bentivogli?s study than in our
study, while Entailment and Scrambling were
seen more frequently in our study. This demon-
strates that differences between languages are rele-
vant to the distribution and classification of BSRs.
An average of 5 and 4 original pairs were de-
composed per hour in our study and Bentivogli?s
study, respectively. This indicates that the com-
plexity of our methodology is not much different
from that in Bentivogli et al(2010).
4.2 Evaluation of Accuracy in BSR
In the RITE-2 formal run4, 15 teams used our spe-
cialized data set for the evaluation of their systems.
Table 7 shows the average of F1 scores5 for each
BSR.
Scrambling and Modifier yielded high scores
(close to 90%). The score of List was also
3Because ?lexical? and ?phrasal? are classified together
in Bentivogli et al(2010), they are not shown separately in
Table 6.
4In RITE-2, data generated by our methodology were re-
leased as ?unit test data?.
5The traditional F1 score is the harmonic mean of preci-
sion and recall.
275
BSR Monothematic pairsBentivogli et al Present studyTotal Y N Total Y NSynonymy 25 22 3 45 45 0Hypernymy 5 3 2 5 5 0Entailment - - - 44 44 0Meronymy 7 4 3 1 1 0Nominalization 9 9 0 1 1 0Corference 49 48 1 3 3 0Scrambling - - - 15 15 0Case alteration 7 5 2 7 7 0Modifier 25 15 10 42 42 0Transparent head 6 6 0 1 1 0Clause 5 4 1 14 14 0List 1 1 0 3 3 0Apposition 3 2 1 1 1 0Relative clause 1 1 0 8 8 0Temporal 2 1 1 1 1 0Spatial 1 1 0 1 1 0Quantity 6 0 6 0 0 0Implicit relation 7 7 0 18 18 0Inference 40 26 14 2 2 0Disagreement: lexical/phrasal 3 0 3 27 0 27Disagreement: modal 1 0 1 1 0 1Disagreement: temporal - - - 1 0 1Disagreement: spatial - - - 0 0 0Disagreement: quantity - - - 0 0 0Demonymy 1 1 0 - - -Statements 1 1 0 - - -total 205 157 48 241 212 29
Table 6: Distribution of BSRs in t1-t2 pairs in an
existing study and in the present study using our
methodology
BSR F1(%) Monothematic MissPairsScrambling 89.6 15 4Modifier 88.8 42 0List 88.6 3 0Temporal 85.7 1 1Relative clause 85.4 8 2Clause 85.0 14 2Hypernymy: lexical 85.0 5 1Disagreement: phrasal 80.1 25 0Case alteration 79.9 7 2Synonymy: lexical 79.7 9 6Transparent head 78.6 1 2Implicit relation 75.7 18 2Synonymy: phrasal 73.6 36 9Corference 70.9 3 1Entailment: phrasal 70.2 44 7Disagreement: lexical 69.0 2 0Meronymy: lexical 64.3 1 1Nominalization 64.3 1 0Apposition 50.0 1 1Spatial 50.0 1 1Inference 40.5 2 2Disagreement: modal 35.7 1 0Disagreement: temporal 28.6 1 1Total - 241 41
Table 7: Average F1 scores in BSR and frequen-
cies of misclassifications by annotators
nearly 90%, although the data sets included only
3 instances. These scores were high because
pairs with these BSRs are easily recognized in
terms of syntactic structure. By contrast, Dis-
agreement: temporal, Disagreement: modal,
Inference, Spatial and Apposition yielded low
scores (less than 50%). The scores of Disagree-
ment: lexical, Nominalization and Disagree-
ment: Meronymy were about 50-70%. BSRs
that yielded scores of less than 70% occurred less
than 3 times, and those that yielded scores of not
more than 70% occurred 3 times or more, except
for Temporal and Transparent head. Therefore,
the frequencies of BSRs are related to F1 scores,
and we should consider how to build systems that
recognize infrequent BSRs accurately. In addi-
tion, F1 scores in Synonymy: phrasal and En-
tailment: phrasal are low, although these are la-
beled frequently. This is one possible direction of
future work.
Table 7 also shows the number of pairs in BSR
to which the two annotators assigned different la-
bels. For example, one annotator labeled t2 [Ap-
position] while the other labeled t2 [Spatial] in
the following pair:
Ex. 2) Example of a pair for RTE
? t1: Tokyo, the capital of Japan, is in Asia.
? t2: The capital of Japan is in Asia.
We can see from Table 7 that the F1 scores for
BSRs, which are often assessed as different by dif-
ferent people, are generally low, except for several
labels, such as Synonymy: lexical and Scram-
bling. For this reason, we can conjecture that
cases in which computers experience difficulty de-
termining the correct labels are correlated with
cases in which humans also experience such dif-
ficulty.
5 Conclusions
This paper presented a methodology for generat-
ing Japanese data sets broken down into BSRs
and Non-BSRs, and we conducted experiments in
which we applied our methodology to 61 pairs
extracted from the RITE-2 BC subtask data set.
We compared our method with that of Bentivogli
et al(2010) in terms of agreement as well as
frequencies and times of decomposition, and we
obtained similar results. This demonstrated that
our methodology is as feasible as Bentivogli et
al.(2010) and that differences between languages
emerge only as the different sets of labels and the
different distributions of BSRs. In addition, 241
monothematic pairs were recognized by comput-
ers, and we showed that both the frequencies of
BSRs and the rate of misclassification by humans
are relevant to F1 scores.
Decomposition patterns were not empirically
compared in the present study and will be investi-
gated in future work. We will also develop an RTE
inference system by using our specialized data set.
276
References
Bentivogli, L., Cabrio, E., Dagan, I, Giampiccolo, D.,
Leggio, M. L., Magnini,B. 2010. Building Textual
Entailment Specialized Data Sets: a Methodology
for Isolating Linguistic Phenomena Relevant to In-
ference. In Proceedings of LREC 2010, Valletta,
Malta.
Dagan, I, Glickman, O., Magnini, B. 2005. Recog-
nizing Textual Entailment Challenge. In Proc. of
the First PASCAL Challenges Workshop on RTE.
Southampton, U.K.
Kotani, M., Shibata, T., Nakata, T, Kurohashi, S. 2008.
Building Textual Entailment Japanese Data Sets and
Recognizing Reasoning Relations Based on Syn-
onymy Acquired Automatically. In Proceedings of
the 14th Annual Meeting of the Association for Nat-
ural Language Processing, Tokyo, Japan.
Magnini, B., Cabrio, E. 2009. Combining Special-
izedd Entailment Engines. In Proceedings of LTC
?09. Poznan, Poland.
Dice, L. R. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297-
302.
Mark Sammons, V.G.Vinod Vydiswaran, Dan Roth.
2010. ?Ask not what textual entailment can do for
you...?. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden, pp. 1199-1208.
277
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 79?89,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Logical Inference on Dependency-based Compositional Semantics
Ran Tian Yusuke Miyao Takuya Matsuzaki
National Institute of Informatics, Japan
{tianran,yusuke,takuya-matsuzaki}@nii.ac.jp
Abstract
Dependency-based Compositional Se-
mantics (DCS) is a framework of natural
language semantics with easy-to-process
structures as well as strict semantics. In
this paper, we equip the DCS framework
with logical inference, by defining ab-
stract denotations as an abstraction of
the computing process of denotations in
original DCS. An inference engine is built
to achieve inference on abstract denota-
tions. Furthermore, we propose a way to
generate on-the-fly knowledge in logical
inference, by combining our framework
with the idea of tree transformation.
Experiments on FraCaS and PASCAL
RTE datasets show promising results.
1 Introduction
Dependency-based Compositional Semantics
(DCS) provides an intuitive way to model seman-
tics of questions, by using simple dependency-like
trees (Liang et al, 2011). It is expressive enough
to represent complex natural language queries on
a relational database, yet simple enough to be
latently learned from question-answer pairs. In
this paper, we equip DCS with logical inference,
which, in one point of view, is ?the best way
of testing an NLP system?s semantic capacity?
(Cooper et al, 1996).
It should be noted that, however, a framework
primarily designed for question answering is not
readily suited for logical inference. Because, an-
swers returned by a query depend on the specific
database, but implication is independent of any
databases. For example, answers to the question
?What books are read by students??, should al-
ways be a subset of answers to ?What books are
ever read by anyone??, no matter how we store the
data of students and how many records of books
are there in our database.
Thus, our first step is to fix a notation which ab-
stracts the calculation process of DCS trees, so as
to clarify its meaning without the aid of any exist-
ing database. The idea is to borrow a minimal set
of operators from relational algebra (Codd, 1970),
which is already able to formulate the calculation
in DCS and define abstract denotation, which is
an abstraction of the computation of denotations
guided by DCS trees. Meanings of sentences then
can be represented by primary relations among
abstract denotations. This formulation keeps the
simpleness and computability of DCS trees mostly
unaffected; for example, our semantic calculation
for DCS trees is parallel to the denotation compu-
tation in original DCS.
An inference engine is built to handle inference
on abstract denotations. Moreover, to compensate
the lack of background knowledge in practical in-
ference, we combine our framework with the idea
of tree transformation (Bar-Haim et al, 2007), to
propose a way of generating knowledge in logical
representation from entailment rules (Szpektor et
al., 2007), which are by now typically considered
as syntactic rewriting rules.
We test our system on FraCaS (Cooper et al,
1996) and PASCAL RTE datasets (Dagan et al,
2006). The experiments show: (i) a competi-
tive performance on FraCaS dataset; (ii) a big
impact of our automatically generated on-the-fly
knowledge in achieving high recall for a logic-
based RTE system; and (iii) a result that outper-
forms state-of-the-art RTE system on RTE5 data.
Our whole system is publicly released and can
be downloaded from http://kmcs.nii.ac.
jp/tianran/tifmo/.
2 The Idea
In this section we describe the idea of represent-
ing natural language semantics by DCS trees, and
achieving inference by computing logical relations
among the corresponding abstract denotations.
79
SUBJreadstudent bookOBJARG ARG
Figure 1: The DCS tree of ?students read books?
student
ARG
Mark
John
Emily
...
book
ARG
A Tale of Two Cities
Ulysses
...
read
SUBJ OBJ
Mark New York Times
Mary A Tale of Two Cities
John Ulysses
... ...
Table 1: Databases of student, book, and read
2.1 DCS trees
DCS trees has been proposed to represent natu-
ral language semantics with a structure similar to
dependency trees (Liang et al, 2011) (Figure 1).
For the sentence ?students read books?, imagine
a database consists of three tables, namely, a set
of students, a set of books, and a set of ?reading?
events (Table 1). The DCS tree in Figure 1 is in-
terpreted as a command for querying these tables,
obtaining ?reading? entries whose ?SUBJ? field
is student and whose ?OBJ? field is book. The
result is a set {John reads Ulysses, . . .}, which is
called a denotation.
DCS trees can be extended to represent linguis-
tic phenomena such as quantification and coref-
erence, with additional markers introducing addi-
tional operations on tables. Figure 2 shows an ex-
ample with a quantifier ?every?, which is marked
as ??? on the edge (love)OBJ-ARG(dog) and in-
terpreted as a division operator q
OBJ
?
(?2.2). Op-
timistically, we believe DCS can provide a frame-
work of semantic representation with sufficiently
wide coverage for real-world texts.
The strict semantics of DCS trees brings us the
idea of applying DCS to logical inference. This is
not trivial, however, because DCS works under the
assumption that databases are explicitly available.
Obviously this is unrealistic for logical inference
on unrestricted texts, because we cannot prepare
a database for everything in the world. This fact
fairly restricts the applicable tasks of DCS.
Our solution is to redefine DCS trees without
the aid of any databases, by considering each node
of a DCS tree as a content word in a sentence (but
may no longer be a table in a specific database),
while each edge represents semantic relations be-
tween two words. The labels on both ends of
an edge, such as SUBJ (subject) and OBJ (ob-
ject), are considered as semantic roles of the cor-
SUBJreadstu enbueoOBJARG ARGotadARGOBJ
SUBJotadke?? ?t?OBJARG ARG
stu SUBJread ??OBJARG ARG ke??SUBJARG
T: H:?
Figure 2: DCS trees of ?Mary loves every dog?
(Left-Up), ?Tom has a dog? (Left-Down), and
?Tom has an animal that Mary loves? (Right).
responding words
1
. To formulate the database
querying process defined by a DCS tree, we pro-
vide formal semantics to DCS trees by employing
relational algebra (Codd, 1970) for representing
the query. As described below, we represent mean-
ings of sentences with abstract denotations, and
logical relations among sentences are computed
as relations among their abstract denotations. In
this way, we can perform inference over formulas
of relational algebra, without computing database
entries explicitly.
2.2 Abstract denotations
Abstract denotations are formulas constructed
from a minimal set of relational algebra (Codd,
1970) operators, which is already able to formu-
late the database queries defined by DCS trees.
For example, the semantics of ?students read
books? is given by the abstract denotation:
F
1
= read ? (student
SUBJ
? book
OBJ
),
where read, student and book denote sets repre-
sented by these words respectively, and w
r
repre-
sents the set w considered as the domain of the
semantic role r (e.g. book
OBJ
is the set of books
considered as objects). The operators? and? rep-
resent intersection and Cartesian product respec-
tively, both borrowed from relational algebra. It
is not hard to see the abstract denotation denotes
the intersection of the ?reading? set (as illustrated
by the ?read? table in Table 1) with the product of
?student? set and ?book? set, which results in the
same denotation as computed by the DCS tree in
Figure 1, i.e. {John reads Ulysses, . . .}. However,
the point is that F
1
itself is an algebraic formula
that does not depend on any concrete databases.
Formally, we introduce the following constants:
? W : a universal set containing all entities.
1
The semantic role ARG is specifically defined for denot-
ing nominal predicate.
80
example phrase abstract denotation / statement
compound noun pet fish pet ? fish
modification nice day day ? (W
ARG
? nice
MOD
)
temporal relation boys study at night study ? (boy
SUBJ
? night
TIME
)
relative clause books that book ? pi
OBJ
(read
students read ?(student
SUBJ
?W
OBJ
))
quantification all men die man ? pi
SUBJ
(die)
hypernym dog ? animal
derivation all criminals commit criminal ? pi
SUBJ
(commit?
a crime (W
SUBJ
? crime
OBJ
))
antonym rise ? fall
negation no dogs are hurt dog ? pi
OBJ
(hurt)
Table 2: Abstract denotations and statements
? Content words: a content word (e.g. read)
defines a set representing the word (e.g.
read={(x, y) | read(x, y)}).
In addition we introduce following functions:
? ?: the Cartesian product of two sets.
? ?: the intersection of two sets.
? pi
r
: projection onto domain of semantic role
r (e.g. pi
OBJ
(read) = {y | ?x; read(x, y)}).
Generally we admit projections onto multiple
semantics roles, denoted by pi
R
where R is a
set of semantic roles.
? ?
r
: relabeling (e.g. ?
OBJ
(book) = book
OBJ
).
? q
r
?
: the division operator, where q
r
?
(A,B) is
defined as the largest set X which satisfies
B
r
?X ? A.
2
This is used to formulate uni-
versal quantifiers, such as ?Mary loves every
dog? and ?books read by all students?.
An abstract denotation is then defined as finite ap-
plications of functions on either constants or other
abstract denotations.
2.3 Statements
As the semantics of DCS trees is formulated by
abstract denotations, the meanings of declarative
sentences are represented by statements on ab-
stract denotations. Statements are declarations
of some relations among abstract denotations, for
which we consider the following set relations:
Non-emptiness A 6= ?: the set A is not empty.
Subsumption A ? B: set A is subsumed by B.
3
Roughly speaking, the relations correspond to the
logical concepts satisfiability and entailment.
2
If A and B has the same dimension, q
?
(A,B) is either
? or {?} (0-dimension point set), depending on if A ? B.
3
Using division operator, subsumption can be represented
by non-emptiness, since for setsA,B of the same dimension,
q
?
(A,B) 6= ? ? A ? B.
Abstract denotations and statements are conve-
nient for representing semantics of various types
of expressions and linguistic knowledge. Some
examples are shown in Table 2.
4
2.4 Logical inference on DCS
Based on abstract denotations, we briefly describe
our process to apply DCS to textual inference.
2.4.1 Natural language to DCS trees
To obtain DCS trees from natural language, we
use Stanford CoreNLP
5
for dependency parsing
(Socher et al, 2013), and convert Stanford depen-
dencies to DCS trees by pattern matching on POS
tags and dependency labels.
6
Currently we use
the following semantic roles: ARG, SUBJ, OBJ,
IOBJ, TIME and MOD. The semantic role MOD
is used for any restrictive modifiers. Determiners
such as ?all?, ?every? and ?each? trigger quanti-
fiers, as shown in Figure 2.
2.4.2 DCS trees to statements
A DCS tree T = (N , E) is defined as a rooted tree,
where each node ? ? N is labeled with a content
word w(?) and each edge (?, ?
?
) ? E ? N ?
N is labeled with a pair of semantic roles (r, r
?
)
7
.
Here ? is the node nearer to the root. Furthermore,
for each edge (?, ?
?
) we can optionally assign a
quantification marker.
Abstract denotation of a DCS tree can be cal-
culated in a bottom-up manner. For example, the
abstract denotation of H in Figure 2 is calculated
from the leaf node Mary, and then:
Node love (Mary loves):
F
2
= love ? (Mary
SUBJ
?W
OBJ
)
Node animal (Animal that Mary loves):
F
3
= animal ? pi
OBJ
(F
2
)
Node have (Tom has an animal that Mary loves):
F
4
= have ? (Tom
SUBJ
? (F
3
)
OBJ
).
Formally, suppose the root ? of a DCS tree T has
children ?
1
, . . . , ?
n
, and edges (?, ?
1
), . . . , (?, ?
n
)
labeled by (r
1
, r
?
1
), . . . , (r
n
, r
?
n
), respectively. The
abstract denotation of T is defined as:
[[T ]]=w(?) ? (
n
?
i=1
?
r
i
(pi
r
?
i
([[T
?
i
]]))?W
R
?
\r
i
),
4
Negation and disjointness (???) are explained in ?2.5.
5
http://nlp.stanford.edu/software/
corenlp.shtml
6
In (Liang et al, 2011) DCS trees are learned from QA
pairs and database entries. We obtain DCS trees from depen-
dency trees, to bypass the need of a concrete database.
7
The definition differs slightly from the original Liang et
al. (2011), mainly for the sake of simplicity and clarity.
81
piOBJ
(F
4
) = F
3
? F
7
pi
OBJ
(F
6
) = dog ? F
7
T
F
6
6= ? Axiom 4
dog ? F
7
6= ?
T
dog ? pi
OBJ
(F
2
) dog ? animal Axiom 8
dog ? F
3
dog ? F
7
? F
3
? F
7
Axiom 6
F
3
? F
7
6= ? Axiom 4
F
4
6= ?
Figure 3: An example of proof using abstract denotations
1. W 6= ?
2. A ? B ? A
3. B
r
? q
r
?
(A,B) ? A
4. pi
R
(A) 6= ? ? A 6= ?
5. (A ? B & B ? C)? A ? C
6. (A ? B & A 6= ?)? B 6= ?
7. A ? B ? pi
R
(A) ? pi
R
(B)
8. (C ? A & C ? B)? C ? A ? B
Table 3: An excerpt of axioms
where T
?
i
is the subtree of T rooted at ?
i
, and
R
?
is the set of possible semantic roles for con-
tent word w(?) (e.g. R
love
= {SUBJ,OBJ}), and
W
R
?
\r
i
is the product of W which has dimension
R
?
\ r
i
(e.g. W
{SUBJ,OBJ}\SUBJ
= W
OBJ
).
When universal quantifiers are involved, we
need to add division operators to the formula.
If (?, ?
i
) is assigned by a quantification marker
???
8
, then the abstract denotation is
9
[[T ]]=q
r
i
?
(pi
R
?
\{r
1
,...,r
i?1
}
([[T
?
]]), pi
r
?
i
([[T
?
i
]])),
where T
?
is the same tree as T except that the
edge (?, ?
i
) is removed. For example, the ab-
stract denotation of the first sentence of T in Fig-
ure 2 (Mary loves every dog) is calculated from F
2
(Mary loves) as
F
5
= q
OBJ
?
(pi
OBJ
(F
2
),dog).
After the abstract denotation [[T ]] is calcu-
lated, the statement representing the meaning of
the sentence is defined as [[T ]] 6= ?. For ex-
ample, the statement of ?students read books?
is read ? (student
SUBJ
? book
OBJ
) 6= ?, and
the statement of ?Mary loves every dog? is
q
OBJ
?
(pi
OBJ
(F
2
),dog) 6= ?, which is logically
equivalent to dog ? pi
OBJ
(F
2
).
10
2.4.3 Logical inference
Since meanings of sentences are represented by
statements on abstract denotations, logical infer-
ence among sentences is reduced to deriving new
relations among abstract denotations. This is done
by applying axioms to known statements, and ap-
proximately 30 axioms are implemented (Table 3).
8
Multiple quantifiers can be processed similarly.
9
The result of [[T ]] depends on the order of the children
?
1
, . . . , ?
n
. Different orders correspond to readings of differ-
ent quantifier scopes.
10
See Footnote 2,3.
These are algebraic properties of abstract denota-
tions, among which we choose a set of axioms that
can be handled efficiently and enable most com-
mon types of inference seen in natural language.
For the example in Figure 2, by constructing the
following abstract denotations:
Tom has a dog:
F
6
= have ? (Tom
SUBJ
? dog
OBJ
)
Objects that Tom has:
F
7
= pi
OBJ
(have ? (Tom
SUBJ
?W
OBJ
)),
we can use the lexical knowledge dog ? animal,
the statements of T (i.e. dog ? pi
OBJ
(F
2
) and
F
6
6= ?), and the axioms in Table 3,
11
to prove
the statement of H (i.e. F
4
6= ?) (Figure 3).
We built an inference engine to perform logical
inference on abstract denotations as above. In this
logical system, we treat abstract denotations as
terms and statements as atomic sentences, which
are far more easier to handle than first order pred-
icate logic (FOL) formulas. Furthermore, all im-
plemented axioms are horn clauses, hence we can
employ forward-chaining, which is very efficient.
2.5 Extensions
Further extensions of our framework are made
to deal with additional linguistic phenomena, as
briefly explained below.
Negation To deal with negation in our forward-
chaining inference engine, we introduce one more
relation on abstract denotations, namely disjoint-
ness A ? B, meaning that A and B are dis-
joint sets. Using disjointness we implemented two
types of negations: (i) atomic negation, for each
content word w we allow negation w? of that word,
characterized by the property w ? w?; and (ii) root
negation, for a DCS tree T and its denotation [[T ]],
the negation of T is represented by T ? T , mean-
ing that T = ? in its effect.
Selection Selection operators in relational alge-
bra select a subset from a set to satisfy some spe-
11
Algebraic identities, such as pi
OBJ
(F
4
) = F
3
? F
7
and
pi
OBJ
(F
6
) = dog ? F
7
, are also axioms.
82
cific properties. This can be employed to rep-
resent linguistic phenomena such as downward
monotonicity and generalized quantifiers. In the
current system, we implement (i) superlatives,
e.g. s
highest
(mountain? (W
ARG
?Asia
MOD
)) (the
highest mountain in Asia) and (ii) numerics, e.g.
s
two
(pet ? fish) (two pet fish), where s
f
is a se-
lection marker. Selection operators are imple-
mented as markers assigned to abstract denota-
tions, with specially designed axioms. For ex-
ample superlatives satisfy the following property:
A ? B & s
highest
(B) ? A ? s
highest
(B) =
s
highest
(A). New rules can be added if necessary.
Coreference We use Stanford CoreNLP to re-
solve coreferences (Raghunathan et al, 2010),
whereas coreference is implemented as a special
type of selection. If a node ? in a DCS tree T be-
longs to a mention cluster m, we take the abstract
denotation [[T
?
]] and make a selection s
m
([[T
?
]]),
which is regarded as the abstract denotation of that
mention. Then all selections of the same mention
cluster are declared to be equal.
3 Generating On-the-fly Knowledge
Recognizing textual entailment (RTE) is the task
of determining whether a given textual statement
H can be inferred by a text passage T. For this,
our primary textual inference system operates as:
1. For a T-H pair, apply dependency parsing
and coreference resolution.
2. Perform rule-based conversion from depen-
dency parses to DCS trees, which are trans-
lated to statements on abstract denotations.
3. Use statements of T and linguistic knowledge
as premises, and try to prove statements of H
by our inference engine.
However, this method does not work for real-
world datasets such as PASCAL RTE (Dagan et
al., 2006), because of the knowledge bottleneck:
it is often the case that the lack of sufficient lin-
guistic knowledge causes failure of inference, thus
the system outputs ?no entailment? for almost all
pairs (Bos and Markert, 2005).
The transparent syntax-to-semantics interface
of DCS enables us to back off to NLP techniques
during inference for catching up the lack of knowl-
edge. We extract fragments of DCS trees as para-
phrase candidates, translate them back to linguis-
  
T/H DCS trees AbstractdenotationsParsingCoreference InferenceYes/NoOn-the-flyknowledge Axioms
Languageresources
Figure 4: RTE system
tic expressions, and apply distributional similar-
ity to judge their validity. In this way, our frame-
work combines distributional and logical seman-
tics, which is also the main subject of Lewis and
Steedman (2013) and Beltagy et al (2013).
As follows, our full system (Figure 4) addition-
ally invokes linguistic knowledge on-the-fly:
4. If H is not proven, compare DCS trees of T
and H, and generate path alignments.
5. Aligned paths are evaluated by a similar-
ity score to estimate their likelihood of be-
ing paraphrases. Path alignments with scores
higher than a threshold are accepted.
6. Convert accepted path alignments into state-
ments on abstract denotations, use them in
logical inference as new knowledge, and try
to prove H again.
3.1 Generating path alignments
On-the-fly knowledge is generated by aligning
paths in DCS trees. A path is considered as joining
two germs in a DCS tree, where a germ is defined
as a specific semantic role of a node. For example,
Figure 5 shows DCS trees of the following sen-
tences (a simplified pair from RTE2-dev):
T: Tropical storm Debby is blamed for deaths.
H: A storm has caused loss of life.
The germ OBJ(blame) and germ ARG(death) in
DCS tree of T are joined by the underscored path.
Two paths are aligned if the joined germs are
aligned, and we impose constraints on aligned
germs to inhibit meaningless alignments, as de-
scribed below.
3.2 Aligning germs by logical clues
Two germs are aligned if they are both at leaf
nodes (e.g. ARG(death) in T and ARG(life) in H,
Figure 5), or they already have part of their mean-
ings in common, by some logical clues.
83
  
readsT?: H?: ARGtunsb obnek?btt?ARG ARGOBJreadsARG ARG
IOBJ
eda??nuARG MOD
??b uarru?b
ARG
SUBJ
ARG
MOD
OBJ
Figure 5: Aligned paths (underscored by the solid
lines) and aligned germs (joined by the dotted line)
To formulate this properly, we define the ab-
stract denotation of a germ, which, intuitively, rep-
resents the meaning of the germ in the specific sen-
tence. The abstract denotation of a germ is defined
in a top-down manner: for the root node ? of a
DCS tree T , we define its denotation [[?]]
T
as the
denotation of the entire tree [[T ]]; for a non-root
node ? and its parent node ?, let the edge (?, ?) be
labeled by semantic roles (r, r
?
), then define
[[? ]]
T
= [[T
?
]] ? (?
r
?
(pi
r
([[?]]
T
))?W
R
?
\r
?).
Now for a germ r(?), the denotation is defined as
the projection of the denotation of node ? onto the
specific semantic role r: [[r(?)]]
T
= pi
r
([[?]]
T
).
For example, the abstract denotation of germ
ARG(book) in Figure 1 is defined as pi
ARG
(book?
pi
OBJ
(read?(student
SUBJ
?book
OBJ
))), meaning
?books read by students?. Similarly, denotation
of germ OBJ(blame) in T of Figure 5 indicates
the object of ?blame? as in the sentence ?Tropi-
cal storm Debby is blamed for death?, which is
a tropical storm, is Debby, etc. Technically, each
germ in a DCS tree indicates a variable when the
DCS tree is translated to a FOL formula, and the
abstract denotation of the germ corresponds to the
set of consistent values (Liang et al, 2011) of that
variable.
The logical clue to align germs is: if there exists
an abstract denotation, other than W , that is a su-
perset of both abstract denotations of two germs,
then the two germs can be aligned. A simple ex-
ample is that ARG(storm) in T can be aligned
to ARG(storm) in H, because their denotations
have a common superset other than W , namely
pi
ARG
(storm). A more complicated example is that
OBJ(blame) and SUBJ(cause) can be aligned,
because inference can induce [[OBJ(blame)]]
T
=
[[ARG(Debby)]]
T
= [[ARG(storm)]]
T
, as well as
[[SUBJ(cause)]]
H
= [[ARG(storm)]]
H
, so they also
have the common superset pi
ARG
(storm). How-
ever, for example, logical clues can avoid align-
ing ARG(storm) to ARG(loss), which is obviously
  
T?: T'?:
What is tropical storm, Debby,       and is blamed for death ]][[ What is tropical storm, Debby,           and cause loss of life ]][[?blame deathDebbyARG ARGOBJstormARG ARG
IOBJ
tropicalARG MOD
cause losslife
ARG
SUBJ
ARG
MOD
OBJDebbyARGstormARG ARGtropicalARG MOD
Figure 6: Tree transformation and generated on-
the-fly knowledge (subsumption of denotations
shown above the trees)
meaningless.
3.3 Scoring path alignments by similarity
Aligned paths are evaluated by a similarity score,
for which we use distributional similarity of the
words that appear in the paths (?4.1). Only path
alignments with high similarity scores can be ac-
cepted. Also, we only accept paths of length ? 5,
to prevent too long paths to be aligned.
3.4 Applying path alignments
Accepted aligned paths are converted into state-
ments, which are used as new knowledge. The
conversion is done by first performing a DCS tree
transformation according to the aligned paths, and
then declare a subsumption relation between the
denotations of aligned germs. For example, to ap-
ply the aligned path pair generated in Figure 5,
we use it to transform T into a new tree T? (Fig-
ure 6), and then the aligned germs, OBJ(blame)
in T and SUBJ(cause) in T?, will generate
the on-the-fly knowledge: [[OBJ(blame)]]
T
?
[[SUBJ(cause)]]
T?
.
Similar to the tree transformation based ap-
proach to RTE (Bar-Haim et al, 2007), this pro-
cess can also utilize lexical-syntactic entailment
rules (Szpektor et al, 2007). Furthermore, since
the on-the-fly knowledge is generated by trans-
formed pairs of DCS trees, all contexts are pre-
served: in Figure 6, though the tree transformation
can be seen as generated from the entailment rule
?X is blamed for death? X causes loss of life?, the
generated on-the-fly knowledge, as shown above
the trees, only fires with the additional condition
that X is a tropical storm and is Debby. Hence,
the process can also be used to generate knowl-
edge from context sensitive rules (Melamud et al,
2013), which are known to have higher quality
(Pantel et al, 2007; Clark and Harrison, 2009).
However, it should be noted that using on-the-
fly knowledge in logical inference is not a trivial
84
task. For example, the FOL formula of the rule ?X
is blamed for death? X causes loss of life? is:
?x; (?a; blame(x, a) & death(a))?
(?b, c; cause(x, b) & loss(b, c) & life(c)),
which is not a horn clause. The FOL formula for
the context-preserved rule in Figure 6 is even more
involved. Still, it can be efficiently treated by our
inference engine because as a statement, the for-
mula [[OBJ(blame)]]
T
? [[SUBJ(cause)]]
T?
is an
atomic sentence, more than a horn clause.
4 Experiments
In this section, we evaluate our system on FraCaS
(?4.2) and PASCAL RTE datasets (?4.3).
4.1 Language Resources
The lexical knowledge we use are synonyms, hy-
pernyms and antonyms extracted from WordNet
12
.
We also add axioms on named entities, stopwords,
numerics and superlatives. For example, named
entities are singletons, so we add axioms such as
?x; (x ? Tom & x 6= ?)? Tom ? x.
To calculate the similarity scores of path align-
ments, we use the sum of word vectors of the
words from each path, and calculate the cosine
similarity. For example, the similarity score of the
path alignment ?OBJ(blame)IOBJ-ARG(death)
? SUBJ(cause)OBJ-ARG(loss)MOD-ARG(life)? is
calculated as the cosine similarity of vectors
blame+death and cause+loss+life. Other struc-
tures in the paths, such as semantic roles, are ig-
nored in the calculation. The word vectors we
use are from Mikolov et al (2013)
13
(Mikolov13),
and additional results are also shown using Turian
et al (2010)
14
(Turian10). The threshold for ac-
cepted path alignments is set to 0.4, based on pre-
experiments on RTE development sets.
4.2 Experiments on FraCaS
The FraCaS test suite contains 346 inference prob-
lems divided into 9 sections, each focused on a cat-
egory of semantic phenomena. We use the data by
MacCartney and Manning (2007), and experiment
on the first section, Quantifiers, following Lewis
and Steedman (2013). This section has 44 single
premise and 30 multi premise problems. Most of
12
http://wordnet.princeton.edu/
13
http://code.google.com/p/word2vec/
14
http://metaoptimize.com/projects/
wordreprs/
Single Prem. Multi Prem.
Lewis13 70 50
MacCartney07 84.1 -
MacCartney08 97.7 -
Our Sys. 79.5 80.0
Table 4: Accuracy (%) on FraCaS
the problems do not require lexical knowledge, so
we use our primary textual inference system with-
out on-the-fly knowledge nor WordNet, to test the
performance of the DCS framework as formal se-
mantics. To obtain the three-valued output (i.e.
yes, no, and unknown), we output ?yes? if H is
proven, or try to prove the negation of H if H is
not proven. To negate H, we use the root negation
as described in ?2.5. If the negation of H is proven,
we output ?no?, otherwise we output ?unknown?.
The result is shown in Table 4. Since our sys-
tem uses an off-the-shelf dependency parser, and
semantic representations are obtained from sim-
ple rule-based conversion from dependency trees,
there will be only one (right or wrong) interpre-
tation in face of ambiguous sentences. Still, our
system outperforms Lewis and Steedman (2013)?s
probabilistic CCG-parser. Compared to MacCart-
ney and Manning (2007) and MacCartney and
Manning (2008), our system does not need a pre-
trained alignment model, and it improves by mak-
ing multi-sentence inferences. To sum up, the re-
sult shows that DCS is good at handling universal
quantifiers and negations.
Most errors are due to wrongly generated DCS
trees (e.g. wrongly assigned semantic roles) or
unimplemented quantifier triggers (e.g. ?neither?)
or generalized quantifiers (e.g. ?at least a few?).
These could be addressed by future work.
4.3 Experiments on PASCAL RTE datasets
On PASCAL RTE datasets, strict logical inference
is known to have very low recall (Bos and Markert,
2005), so on-the-fly knowledge is crucial in this
setting. We test the effect of on-the-fly knowledge
on RTE2, RTE3, RTE4 and RTE5 datasets, and
compare our system with other approaches.
4.3.1 Impact of on-the-fly knowledge
Results on test data are shown in Table 5. When
only primary knowledge is used in inference (the
first row), recalls are actually very low; After we
activate the on-the-fly knowledge, recalls jump to
over 50%, with a moderate fall of precision. As a
result, accuracies significantly increase.
85
RTE2 RTE3 RTE4 RTE5
Prec. Rec. Acc. Prec. Rec. Acc. Prec. Rec. Acc. Prec. Rec. Acc.
Primary 70.9 9.8 52.9 73.2 7.3 51.1 89.7 5.2 52.3 82.6 6.3 52.5
+On-the-fly 57.6 66.5 58.8 63.7 64.6 63.0 60.0 57.4 59.6 69.9 55.7 65.8
Table 5: Impact of on-the-fly knowledge
RTE2 RTE3 RTE4 RTE5
Bos06 60.6 - - -
MacCartney08 - 59.4 - -
Clark08 - - 56.5 -
Wang10 63.0 61.1 - -
Stern11 61.6 67.1 - 63.5
Stern12 - - - 64.0
Our Sys. 58.8 63.0 59.6 65.8
Table 6: Comparison with other systems
4.3.2 Comparison to other RTE systems
A comparison between our system and other RTE
systems is shown in Table 6. Bos06 (Bos and
Markert, 2006) is a hybrid system combining
deep features from a theorem prover and a model
builder, together with shallow features such as lex-
ical overlap and text length. MacCartney08 (Mac-
Cartney and Manning, 2008) uses natural logic to
calculate inference relations between two superfi-
cially aligned sentences. Clark08 (Clark and Har-
rison, 2008) is a logic-based system utilizing vari-
ous resources including WordNet and DIRT para-
phrases (Lin and Pantel, 2001), and is tolerant to
partially unproven H sentences in some degree.
All of the three systems pursue a logical approach,
while combining various techniques to achieve ro-
bustness. The result shows that our system has
comparable performance. On the other hand,
Wang10 (Wang and Manning, 2010) learns a tree-
edit model from training data, and captures entail-
ment relation by tree edit distance. Stern11 (Stern
and Dagan, 2011) and Stern12 (Stern et al, 2012)
extend this framework to utilize entailment rules
as tree transformations. These are more tailored
systems using machine learning with many hand-
crafted features. Still, our unsupervised system
outperforms the state-of-the-art on RTE5 dataset.
4.3.3 Analysis
Summing up test data from RTE2 to RTE5, Fig-
ure 7 shows the proportion of all proven pairs and
their precision. Less than 5% pairs can be proven
primarily, with a precision of 77%. Over 40%
pairs can be proven by one piece of on-the-fly
knowledge, yet pairs do exist in which more than
2 pieces are necessary. The precisions of 1 and 2
pieces on-the-fly knowledge application are over
  
0 1 2 >=30
0.10.2
0.30.4
0.50.6
0.70.8
0.9 Proportion of proven pairsPrecision
Applied on-the-fly knowledge
Figure 7: Proportion of proven pairs and their pre-
cision, w.r.t. pieces of on-the-fly knowledge.
60%, which is fairly high, given our rough estima-
tion of the similarity score. As a comparison, Dinu
and Wang (2009) studied the proportion of proven
pairs and precision by applying DIRT rules to tree
skeletons in RTE2 and RTE3 data. The proportion
is 8% with precision 65% on RTE2, and propor-
tion 6% with precision 72% on RTE3. Applied
by our logical system, the noisy on-the-fly knowl-
edge can achieve a precision comparable to higher
quality resources such as DIRT.
A major type of error is caused by the igno-
rance of semantic roles in calculation of simi-
larity scores. For example, though ?Italy beats
Kazakhstan? is not primarily proven from ?Italy
is defeated by Kazakhstan?, our system does
produce the path alignment ?SUBJ(beat)OBJ ?
OBJ(defeat)SUBJ? with a high similarity score.
The impact of such errors depends on the data
making methodology, though. It lowers precisions
in RTE2 and RTE3 data, particularly in ?IE? sub-
task (where precisions drop under 0.5). On the
other hand, it occurs less often in ?IR? subtask.
Finally, to see if we ?get lucky? on RTE5 data
in the choice of word vectors and thresholds, we
change the thresholds from 0.1 to 0.7 and draw
the precision-recall curve, using two types of word
vectors, Mikolov13 and Turian10. As shown in
Figure 8, though the precision drops for Turian10,
both curves show the pattern that our system keeps
gaining recall while maintaining precision to a cer-
tain level. Not too much ?magic? in Mikolov13 ac-
tually: for over 80% pairs, every node in DCS tree
of H can be covered by a path of length ? 5 that
86
  0 012 01> 01= 013 01. 014 01501.
01..
014
014.
015
015.
016
016. What tisropchlms,
789Prr
op89t
itn 
Figure 8: Precision-Recall curve.
has a corresponding path of length ? 5 in T with
a similarity score > 0.4.
5 Conclusion and Discussion
We have presented a method of deriving abstract
denotation from DCS trees, which enables logi-
cal inference on DCS, and we developed a textual
inference system based on the framework. Exper-
imental results have shown the power of the rep-
resentation that allows both strict inference as on
FraCaS data and robust reasoning as on RTE data.
Exploration of an appropriate meaning repre-
sentation for querying and reasoning on knowl-
edge bases has a long history. Description logic,
being less expressive than FOL but featuring more
efficient reasoning, is used as a theory base for Se-
mantic Web (W3C, 2012). Ideas similar to our
framework, including the use of sets in a repre-
sentation that benefits efficient reasoning, are also
found in description logic and knowledge repre-
sentation community (Baader et al, 2003; Sowa,
2000; Sukkarieh, 2003). To our knowledge, how-
ever, their applications to logical inference beyond
the use for database querying have not been much
explored in the context of NLP.
The pursue of a logic more suitable for natural
language inference is not new. For instance, Mac-
Cartney and Manning (2008) has implemented a
model of natural logic (Lakoff, 1970). While
being computationally efficient, various inference
patterns are out of the scope of their system.
Much work has been done in mapping natu-
ral language into database queries (Cai and Yates,
2013; Kwiatkowski et al, 2013; Poon, 2013).
Among these, the (?-)DCS (Liang et al, 2011;
Berant et al, 2013) framework defines algorithms
that transparently map a labeled tree to a database
querying procedure. Essentially, this is because
DCS trees restrict the querying process to a very
limited subset of possible operations. Our main
contribution, the abstract denotation of DCS trees,
can thus be considered as an attempt to charac-
terize a fragment of FOL that is suited for both
natural language inference and transparent syntax-
semantics mapping, through the choice of opera-
tions and relations on sets.
We have demonstrated the utility of logical in-
ference on DCS through the RTE task. A wide
variety of strategies tackling the RTE task have
been investigated (Androutsopoulos and Malaka-
siotis, 2010), including the comparison of surface
strings (Jijkoun and De Rijke, 2005), syntactic and
semantic structures (Haghighi et al, 2005; Snow
et al, 2006; Zanzotto et al, 2009; Burchardt et
al., 2009; Heilman and Smith, 2010; Wang and
Manning, 2010), semantic vectors (Erk and Pad?o,
2009) and logical representations (Bos and Mark-
ert, 2005; Raina et al, 2005; Tatu and Moldovan,
2005). Acquisition of basic knowledge for RTE
is also a huge stream of research (Lin and Pantel,
2001; Shinyama et al, 2002; Sudo et al, 2003;
Szpektor et al, 2004; Fujita et al, 2012; Weis-
man et al, 2012; Yan et al, 2013). These previ-
ous works include various techniques for acquir-
ing and incorporating different kinds of linguistic
and world knowledge, and further fight against the
knowledge bottleneck problem, e.g. by back-off
to shallower representations.
Logic-based RTE systems employ various ap-
proaches to bridge knowledge gaps. Bos and
Markert (2005) proposes features from a model
builder; Raina et al (2005) proposes an abduction
process; Tatu and Moldovan (2006) shows hand-
crafted rules could drastically improve the perfor-
mance of a logic-based RTE system.
As such, our current RTE system is at a proof-
of-concept stage, in that many of the above tech-
niques are yet to be implemented. Nonetheless,
we would like to emphasize that it already shows
performance competitive to state-of-the-art sys-
tems on one data set (RTE5). Other directions of
our future work include further exploitation of the
new semantic representation. For example, since
abstract denotations are readily suited for data
querying, they can be used to verify newly gen-
erated assumptions by fact search in a database.
This may open a way towards a hybrid approach
to RTE wherein logical inference is intermingled
with large scale database querying.
Acknowledgments This research was supported
by the Todai Robot Project at National Institute of
Informatics.
87
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. J. Artif. Int. Res., 38(1).
Franz Baader, Diego Calvanese, Deborah L. McGuin-
ness, Daniele Nardi, and Peter F. Patel-Schneider,
editors. 2003. The Description Logic Handbook:
Theory, Implementation, and Applications. Cam-
bridge University Press, New York, NY, USA.
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI 2007.
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets markov: Deep semantics with
probabilistic logical form. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of EMNLP
2013.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of EMNLP 2005.
Johan Bos and Katja Markert. 2006. When logical
inference helps determining textual entailment (and
when it doesnt). In Proceedings of the 2nd PASCAL
RTE Challenge Workshop.
Aljoscha Burchardt, Marco Pennacchiotti, Stefan
Thater, and Manfred Pinkal. 2009. Assessing the
impact of frame semantics on textual entailment.
Nat. Lang. Eng., 15(4).
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of ACL 2013.
Peter Clark and Phil Harrison. 2008. Recognizing tex-
tual entailment with logical inference. In Proceed-
ings of 2008 Text Analysis Conference (TAC?08).
Peter Clark and Phil Harrison. 2009. Large-scale ex-
traction and use of knowledge from text. In Pro-
ceedings of the Fifth International Conference on
Knowledge Capture (K-CAP?09).
E. F. Codd. 1970. A relational model of data for large
shared data banks. Commun. ACM, 13(6).
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris
Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp,
David Milward, Manfred Pinkal, Massimo Poesio,
and et al 1996. Using the framework. FraCaS De-
liverable D, 16.
Ido Dagan, O. Glickman, and B. Magnini. 2006. The
pascal recognising textual entailment challenge. In
Machine Learning Challenges. Evaluating Predic-
tive Uncertainty, Visual Object Classification, and
Recognising Tectual Entailment.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings of EACL 2009.
Katrin Erk and Sebastian Pad?o. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proceedings of the Work-
shop on Geometrical Models of Natural Language
Semantics.
Atsushi Fujita, Pierre Isabelle, and Roland Kuhn.
2012. Enlarging paraphrase collections through
generalization and instantiation. In Proceedings of
EMNLP 2012.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching.
In Proceedings of EMNLP 2005.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings
of NAACL 2010.
Valentin Jijkoun and Maarten De Rijke. 2005. Rec-
ognizing textual entailment: Is word similarity
enough? In Machine Learning Challenge Work-
shop, volume 3944 of LNCS, Springer.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
EMNLP 2013.
George Lakoff. 1970. Linguistics and natural logic.
Synthese, 22(1-2).
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of ACL, 1.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL 2011.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for question-answering. Nat. Lang.
Eng., 7(4).
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of Col-
ing 2008.
88
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of ACL 2013.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL
2013.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of NAACL 2007.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In Proceedings of ACL 2013.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Ju-
rafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of EMNLP 2010.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning
and abductive reasoning. In Proceedings of AAAI
2005.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of HLT 2002.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of NAACL 2006.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with compo-
sitional vector grammars. In Proceedings of ACL
2013.
John F. Sowa. 2000. Knowledge Representation:
Logical, Philosophical and Computational Founda-
tions. Brooks/Cole Publishing Co., Pacific Grove,
CA, USA.
Asher Stern and Ido Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In
Proceedings of RANLP 2011.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based in-
ference. In Proceedings of ACL 2012.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic ie pattern acquisition. In
Proceedings of ACL 2003.
JanaZ. Sukkarieh. 2003. An expressive efficient rep-
resentation: Bridging a gap between nlp and kr.
In Vasile Palade, RobertJ. Howlett, and Lakhmi
Jain, editors, Knowledge-Based Intelligent Informa-
tion and Engineering Systems. Springer Berlin Hei-
delberg.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP
2004.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of ACL 2007.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of EMNLP 2005.
Marta Tatu and Dan Moldovan. 2006. A logic-
based semantic approach to recognizing textual en-
tailment. In Proceedings of the COLING/ACL 2006.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL 2010.
W3C. 2012. Owl 2 web ontology language document
overview (second edition). www.w3.org/TR/owl2-
overview/.
Mengqiu Wang and Christopher Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of Coling 2010.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido
Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings of
EMNLP 2012.
Yulan Yan, Chikara Hashimoto, Kentaro Torisawa,
Takao Kawai, Jun?ichi Kazama, and Stijn De Saeger.
2013. Minimally supervised method for multilin-
gual paraphrase extraction from definition sentences
on the web. In Proceedings of NAACL 2013.
Fabio massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learn-
ing approach to textual entailment recognition. Nat.
Lang. Eng., 15(4).
89
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,
Dublin, Ireland, August 23-24, 2014.
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
Stephan Oepen
??
, Marco Kuhlmann
?
, Yusuke Miyao
?
, Daniel Zeman
?
,
Dan Flickinger
?
, Jan Haji
?
c
?
, Angelina Ivanova
?
, and Yi Zhang
?
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Link?ping University, Department of Computer and Information Science
?
National Institute of Informatics, Tokyo
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
?
Stanford University, Center for the Study of Language and Information
?
Nuance Communications Aachen GmbH
sdp-organizers@emmtee.net
Abstract
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate?argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
1 Background and Motivation
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),
1
seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
1
See http://alt.qcri.org/semeval2014/
task8/ for further technical details, information on how to
obtain the data, and official results.
63
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
A1 A2
(a) Partial semantic dependencies in PropBank and NomBank.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2 ARG3 ARG1
ARG2mwe _and_cARG1ARG1
BV
ARG1 implicit_conjARG1
(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
top
ARG1
ARG2
ARG1
ARG2
ARG2
ARG1
ARG1 ARG1 ARG1ARG1
ARG1
ARG2
ARG1
ARG2
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
(c) Enju Predicate?Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
RSTR
PAT
EXT
PAT
ACT
RSTR
ADDR
ADDR
ADDR
ADDR
APPS.m
APPS.m
CONJ.m
CONJ.m CONJ.m
top
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
Figure 1: Sample semantic dependency graphs for Example (1).
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea & Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena?
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction?typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ?semantic parsing?,
which Kate and Wong (2010) define as ?the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.?
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ?completeness? in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
2 Target Representations
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
64
we provide some background on the ?pedigree? and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00?
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource?
dubbed DeepBank by Flickinger et al. (2012)?
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ?lossy? conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen & L?nning, 2006), then to ?pure?
bi-lexical form?projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.
2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.
3
The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate?argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate?argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Haji
?
c et al., 2012)
4
is a set of parallel dependency trees over the WSJ
2
Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
3
See http://kmcs.nii.ac.jp/enju/.
4
See http://ufal.mff.cuni.cz/pcedt2.0/.
id form lemma pos top pred arg1 arg2
#20200002
1 Ms. Ms. NNP ? + _ _
2 Haag Haag NNP ? ? compound ARG1
3 plays play VBZ + + _ _
4 Elianti Elianti NNP ? ? _ ARG2
5 . . . ? ? _ _
Table 1: Tabular SDP data format (showing DM).
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
3 Graph Representation
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
x
1
, . . . , x
n
is a structure G = (V,E, `
V
, `
E
) where
V = {1, . . . , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E ? V ? V is a set of edges; and `
V
and `
E
are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label `
V
(i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label `
E
(i? j) of an edge i? j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Haji
?
c et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
65
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate?argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg
i
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
DM PAS PCEDT
(1) # labels 51 42 68
(2) % singletons 22.62 4.49 35.79
(3) # edge density 0.96 1.02 0.99
(4) %
g
trees 2.35 1.30 56.58
(5) %
g
projective 3.05 1.71 53.29
(6) %
g
fragmented 6.71 0.23 0.56
(7) %
n
reentrancies 27.35 29.40 9.27
(8) %
g
topless 0.28 0.02 0.00
(9) # top nodes 0.9972 0.9998 1.1237
(10) %
n
non-top roots 44.71 55.92 4.36
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
4 Data Sets
All three target representations are annotations of
the same text, Sections 00?21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and
?U.S., .?, and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00?20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.
5
In terms of distinctions
5
These statistics are obtained using the ?official? SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ?%
n
? denotes (non-singleton) node percentages, and
?%
g
? percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ?covered?, i.e. for any root j there is no edge i? k
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM ? .6425 .2612 ? .6719 .5675
PAS .6688 ? .2963 .6993 ? .5490
PCEDT .2636 .2963 ? .5743 .5630 ?
Table 3: Pairwise F
1
similarities, including punctu-
ation (upper right diagonals) or not (lower left).
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ?tree-oriented? than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ?top? predicate, and the whole graph should
?hang together?. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F
1
(dubbed
UF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i < j < k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;
6
while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ?local? top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ?upstairs? predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ?vanilla?
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel?
?
cukian analysis of coordinate structures
(Mel?
?
cuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ?standing in? for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6
In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
67
employee stock investment plans
compound compound compound
employee stock investment plans
ARG1
ARG1
ARG1
employee stock investment plans
ACT
PAT REG
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.
7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase?internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...] investments
in stock for employees, i.e. grouping the concept
7
As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ?shared? modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ?common stock?).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
?sentence? (or ?conventional?) meaning, on the one
hand, and ?speaker? (or ?occasion?) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
5 Task Setup
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit?including
graph input, basic statistics, and scoring?were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ?companion?
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
68
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen-
80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malm?
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate?role?argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ?virtual? dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and F
1
: LP, LR, LF; (b) unlabeled precision,
recall, and F
1
: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ?official? ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency F
1
scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example?provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ?companion? syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
6 Submissions and Results
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
69
Team Track Approach Resources
Link?ping C extension of Eisner?s algorithm for DAGs, edge-factored
structured perceptron
?
Potsdam C & O graph-to-tree transformation, Mate companion
Priberam C & O model with second-order features, decoding with dual decom-
position, MIRA
companion
Turku O cascade of SVM classifiers (dependency recognition, label
classification, top recognition)
companion,
syntactic n-grams,
word2vec
Alpage C & O transition-based parsing for DAGs, logistic regression, struc-
tured perceptron
companion,
Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma-
tion, parser ensemble
?
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
companion
Copenhagen-Malm? C graph-to-tree transformation, Mate ?
In-House O existing parsers developed by the organizers grammars
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.
8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
7 Overview of Approaches
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8
Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning?oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
70
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS> DM> PCEDT, which can be taken as
a tentative indicator of relative levels of ?parsabil-
ity?. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
?mere? sentence meaning) in at least some construc-
tions in PCEDT.
8 Conclusions and Outlook
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate?
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ?complete? predications
(in contrast to more lenient per-dependency F
1
used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ?gold? track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
Acknowledgements
We are grateful to ?eljko Agi
?
c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
References
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89 ? 97). Beijing, China.
Bohnet, B., & Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 ? 1465). Jeju
Island, Korea.
Buchholz, S., & Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149 ? 164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281 ? 332.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 ? 454). Genoa, Italy.
Fares, M., Oepen, S., & Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 ? 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., & Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
71
245 ? 288.
Grice, H. P. (1968). Utterer?s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 ? 242.
Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart?, M. A., M?rquez, L., . . . Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1 ? 18). Boulder, CO, USA.
Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,
O., Cinkov?, S., . . . ?abokrtsk?, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 ? 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 ?
11). Jeju, Republic of Korea.
Kate, R. J., & Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 ? 330.
Mel?
?
cuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., & Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 ?
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291 ? 330.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,
J., Riedel, S., & Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., & L?nning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71 ? 106.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 ? 440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., & Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 ? 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., & Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 ? 1567).
Vadas, D., & Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240 ? 247). Prague, Czech Re-
public.
72
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335?340,
Dublin, Ireland, August 23-24, 2014.
In-House: An Ensemble of Pre-Existing Off-the-Shelf Parsers
Yusuke Miyao
?
, Stephan Oepen
??
, and Daniel Zeman
?
?
National Institute of Informatics, Tokyo
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
yusuke@nii.ac.jp, oe@ifi.uio.no, zeman@ufal.mff.cuni.cz
Abstract
This submission to the open track of
Task 8 at SemEval 2014 seeks to connect
the Task to pre-existing, ?in-house? pars-
ing systems for the same types of target
semantic dependency graphs.
1 Background and Motivation
The three target representations for Task 8 at
SemEval 2014, Broad-Coverage Semantic Depen-
dency Parsing (SDP; Oepen et al., 2014), are
rooted in language engineering efforts that have
been under continuous development for at least
the past decade. The gold-standard semantic de-
pendency graphs used for training and testing in
the Task result from largely manual annotation, in
part re-purposing and adapting resources like the
Penn Treebank (PTB; Marcus et al., 1993), Prop-
Bank (Palmer et al., 2005), and others. But the
groups who prepared the SDP target data have also
worked in parallel on automated parsing systems
for these representations.
Thus, for each of the target representations,
there is a pre-existing parser, often developed in
parallel to the creation of the target dependency
graphs, viz. (a) for the DM representation, the
parser of the hand-engineered LinGO English Re-
source Grammar (ERG; Flickinger, 2000); (b) for
PAS, the Enju parsing system (Miyao, 2006), with
its probabilistic HPSG acquired through linguis-
tic projection of the PTB; and (c) for PCEDT,
the scenario for English analysis within the Treex
framework (Popel and ?abokrtsk?, 2010), com-
bining data-driven dependency parsing with hand-
engineered tectogrammatical conversion. At least
This work is licenced under a Creative Commons At-
tribution 4.0 International License; page numbers and the
proceedings footer are added by the organizers. http://
creativecommons.org/licenses/by/4.0/
for DM and PAS, these parsers have been exten-
sively engineered and applied successfully in a
variety of applications, hence represent relevant
points of comparison. Through this ?in-house?
submission (of our ?own? parsers to our ?own?
task), we hope to facilitate the comparison of dif-
ferent approaches submitted to the Task with this
pre-existing line of parser engineering.
2 DM: The English Resource Grammar
Semantic dependency graphs in the DM target rep-
resentation, DELPH-IN MRS-Derived Bi-Lexical
Dependencies, stem from a two-step ?reduc-
tion? (simplification) of the underspecified logical-
form meaning representations output natively by
the ERG parser, which implements the linguis-
tic framework of Head-Driven Phrase Structure
Grammar (HPSG; Pollard and Sag, 1994). Gold-
standard DM training and test data for the Task
were derived from the manually annotated Deep-
Bank Treebank (Flickinger et al., 2012), which
pairs Sections 00?21 of the venerable PTB Wall
Street Journal (WSJ) Corpus with complete ERG-
compatible HPSG syntactico-semantic analyses.
DeepBank as well as the ERG rely on Minimal Re-
cursion Semantics (MRS; Copestake et al., 2005)
for meaning representation, such that the exact
same post-processing steps could be applied to the
parser outputs as were used in originally reducing
the gold-standard MRSs from DeepBank into the
SDP bi-lexical semantic dependency graphs.
Parsing Setup The ERG parsing system is a hy-
brid, combining (a) the hand-built, broad-coverage
ERG with (b) an efficient chart parser for uni-
fication grammars and (c) a conditional proba-
bility distribution over candidate analyses. The
parser most commonly used with the ERG, called
PET (Callmeier, 2002),
1
constructs a complete,
1
The SDP test data was parsed using the 1212 release
of the ERG, using PET and converter versions from what
335
subsumption-based parse forest of partial HPSG
derivations (Oepen and Carroll, 2000), and then
extracts from the forest n-best lists (in globally
correct rank order) of complete analyses according
to a discriminative parse ranking model (Zhang et
al., 2007). For our experiments, we trained the
parse ranker on Sections 00?20 of DeepBank and
otherwise used the default, non-pruning develop-
ment configuration, which is optimized for accu-
racy. In this setup, ERG parsing on average takes
close to ten seconds per sentence.
Post-Parsing Conversion After parsing, MRSs
are reduced to DM bi-lexical semantic dependen-
cies in two steps. First, Oepen and L?nning
(2006) define a conversion to variable-free Ele-
mentary Dependency Structures (EDS), which (a)
maps each predication in the MRS logical-form
meaning representation to a node in a dependency
graph and (b) transforms argument relations rep-
resented by shared logical variables into directed
dependency links between graph nodes. This first
step of the conversion is ?mildly? lossy, in that
some scope-related information is discarded; the
EDS graph, however, will contain the same num-
ber of nodes and the same set of argument de-
pendencies as there are predications and semantic
role assignments in the original MRS. In particu-
lar, the EDS may still reflect non-lexical semantic
predications introduced by grammatical construc-
tions like covert quantifiers, nominalization, com-
pounding, or implicit conjunction.
2
Second, in another conversion step that is not
information-preserving, the EDS graphs are fur-
ther reduced into strictly bi-lexical form, i.e. a set
of directed, binary dependency relations holding
exclusively between lexical units. This conversion
is defined by Ivanova et al. (2012) and seeks to
(a) project some aspects of construction seman-
tics onto word-to-word dependencies (for example
introducing specific dependency types for com-
pounding or implicit conjunction) and (b) relate
the linguistically informed ERG-internal tokeniza-
tion to the conventions of the PTB.
3
Seeing as both
is called the LOGON SVN trunk as of January 2014; see
http://moin.delph-in.net/LogonTop for detail.
2
Conversely, semantically vacuous parts of the original
input (e.g. infinitival particles, complementizers, relative pro-
nouns, argument-marking prepositions, auxiliaries, and most
punctuation marks) were not represented in the MRS in the
first place, hence have no bearing on the conversion.
3
Adaptations of tokenization encompass splitting ?multi-
word? ERG tokens (like such as or ad hoc), as well as ?hiding?
ERG token boundaries at hyphens or slashes (e.g. 77-year-
conversion steps are by design lossy, DM seman-
tic dependency graphs present a true subset of the
information encoded in the full, original MRS.
3 PAS: The Enju Parsing System
Enju Predicate?Argument Structures (PAS) are
derived from the automatic HPSG-style annota-
tion of the PTB, which was primarily used for the
development of the Enju parsing system
4
(Miyao,
2006). A notable feature of this parser is that the
grammar is not developed by hand; instead, the
Enju HPSG-style treebank is first developed, and
the grammar (or, more precisely, the vast major-
ity of lexical entries) is automatically extracted
from the treebank (Miyao et al., 2004). In this
?projection? step, PTB annotations such as empty
categories and coindexation are used for deriv-
ing the semantic representations that correspond
to HPSG derivations. Its probabilistic model for
disambiguation is also trained using this treebank
(Miyao and Tsujii, 2008).
5
The PAS data set is an extraction of predicate?
argument structures from the Enju HPSG tree-
bank. The Enju parser outputs results in ?ready-
to-use? formats like phrase structure trees and
predicate?argument structures, as full HPSG anal-
yses are not friendly to users who are not famil-
iar with the HPSG theory. The gold-standard PAS
target data in the Task was developed using this
function; the conversion program from full HPSG
analyses to predicate?argument structures was ap-
plied to the Enju Treebank.
Predicate?argument structures (PAS) represent
word-to-word semantic dependencies, such as se-
mantic subject and object. Each dependency type
is represented with two elements: the type of the
predicate, such as verb and adjective, and the ar-
gument label, such as ARG1 and ARG2.
6
old), which the PTB does not split.
4
See http://kmcs.nii.ac.jp/enju/.
5
Abstractly similar to the ERG, the annotations of the
Enju treebank instantiate the linguistic theory of HPSG.
However, the two resources have been developed indepen-
dently and implementation details are quite different. The
most significant difference is that the Enju HPSG treebank is
developed by linguistic projection of PTB annotations, and
the Enju parser derived from the treebank; conversely, the
ERG was predominantly manually crafted, and it was later
applied in the DeepBank re-annotation of the WSJ Corpus.
6
Full details of the predicate?argument structures in the
Enju HPSG Treebank, are available in two documents linked
from the Enju web site (see above), viz. the Enju Output
Specification Manual and the XML Format Documentation.
336
Parsing Setup Basically we used the publicly
available package of the Enju parser ?as is? (see the
above web site). We did not change default pars-
ing parameters (beam width, etc.) and features.
However, the release version of the Enju parser is
trained with the HPSG treebank corresponding to
the Penn Treebank WSJ Sections 2?21, which in-
cludes the test set of the Task (Section 21). There-
fore, we re-trained the Enju parser using Sections
0?20, and used this re-trained parser in preparing
the PAS semantic dependency graphs in this en-
semble submission.
Post-Parsing Conversion The dependency for-
mat of the Enju parser is almost equivalent to what
is provided as the PAS data set in this shared task.
Therefore, the post-parsing conversion for the PAS
data involves only formatting, viz. (a) format con-
version into the tabular file format of the Task; and
(b) insertion of dummy relations for punctuation
tokens ignored in the output of Enju.
7
4 PCEDT: The Treex Parsing Scenario
The Prague Czech-English Dependency Treebank
(PCEDT; Haji
?
c et al., 2012)
8
is a set of parallel de-
pendency trees over the same WSJ texts from the
Penn Treebank, and their Czech translations. Sim-
ilarly to other treebanks in the Prague family, there
are two layers of syntactic annotation: analytical
(a-trees) and tectogrammatical (t-trees). Unlike
for the other two representations used in the Task,
for PCEDT there is no pre-existing parsing system
designed to deliver the full scale of annotations
of the SDP gold-standard data. The closest avail-
able match is a parsing scenario implemented in
the Treex natural language processing framework.
Parsing Setup Treex
9
(Popel and ?abokrtsk?,
2010) is a modular, open-source framework origi-
nally developed for transfer-based machine trans-
lation. It can accomplish any NLP-related task
by sequentially applying to the same piece of data
various blocks of code. Blocks operate on a com-
mon data structure and are chained in scenarios.
Some early experiments with scenarios for tec-
togrammatical analysis of English were described
by Klime? (2007). It is of interest that they report
7
The Enju parser ignores tokens tagged as ?.?, while
the PAS representation includes them with dummy relations;
thus, missing periods are inserted in post-processing by com-
parison to the original PTB token sequence.
8
See http://ufal.mff.cuni.cz/pcedt2.0/.
9
See http://ufal.mff.cuni.cz/treex/.
U.S. should regulate X more stringently than  Y
CPR
PAT
PRED
ACT
PAT
MANN
CPR
PAT
PRED
ACT
PAT
MANN CPR
Figure 1: PCEDT asserts two copies of the token
regulate (shown here as ?regulate? and ??, under-
lined). Projecting t-nodes onto the original tokens,
required by the SDP data format, means that the
 node will be merged with regulate. The edges
going to and from  will now lead to and from reg-
ulate (see the dotted arcs), which results in a cycle.
To get rid of the cycle, we skip  and connect di-
rectly its children, as shown in the final SDP graph
below the sentence.
an F
1
score of assigning functors (dependency la-
bels in PCEDT terminology) of 70.3%; however,
their results are not directly comparable to ours.
Due to the modular nature of Treex, there are
various conceivable scenarios to get the t-tree of
a sentence. We use the default scenario that con-
sists of 48 blocks: two initial blocks (reading the
input), one final block (writing the output), two
A2N blocks (named entity recognition), twelve
W2A blocks (dependency parsing at the analytical
layer) and 31 A2T and T2T blocks (creating the
t-tree based on the a-tree).
Most blocks are highly specialized in one par-
ticular subtask (e.g. there is a block just to make
sure that quotation marks are attached to the root
of the quoted subtree). A few blocks are respon-
sible for the bulk of the work. The a-tree is con-
structed by a block that contains the MST Parser
(McDonald et al., 2005), trained on the CoNLL
2007 English data (Nivre et al., 2007), i.e. Sec-
tions 2?11 of the PTB, converted to dependencies.
The annotation style of CoNLL 2007 differs from
PCEDT 2.0, and thus the unlabeled attachment
score of the analytical parser is only 66%.
Obviously one could expect better results if we
retrained the MST Parser directly on the PCEDT
a-trees, and on the whole training data. The only
reason why we did not do so was lack of time.
Our results thus really demonstrate what is avail-
able ?off-the-shelf?; on the other hand, the PCEDT
component of our ensemble fails to set any ?upper
bound? of output quality, as it definitely is not bet-
337
John brought and ate ripe apples and pears
ACT
CONJ
CONJ
PRED.m PRED.m
RSTR
PAT.m PAT.m
PAT
TOP TOP
PAT
PAT
ACT
ACT
CONJ.m CONJ.m
RSTR
RSTR
PAT
CONJ.m CONJ.m
Figure 2: Coordination in PCEDT t-tree (above)
and in the corresponding SDP graph (below).
ter informed than the other systems participating
in the Task.
Functor assignment is done heuristically, based
on POS tags and function words. The primary
focus of the scenario was on functors that could
help machine translation, thus it only generated
25 different labels (of the total set of 65 labels in
the SDP gold-standard data)
10
and left about 12%
of all nodes without functors. Precision peaks at
78% for ACT(or) relations, while the most fre-
quent error type (besides labelless dependencies)
is a falsely proposed RSTR(iction) relation. Both
ACT and RSTR are among the most frequent de-
pendency types in PCEDT.
Post-Parsing Conversion Once the t-tree has
been constructed, it is converted to the PCEDT
target representation of the Task, using the same
conversion code that was used to prepare the gold-
standard SDP data.
11
SDP graphs are defined over surface tokens but
the set of nodes of a t-tree need not correspond
one-to-one to the set of tokens. For example, there
are no t-nodes for punctuation and function words
(except in coordination); these tokens are rendered
as semantically vacuous in SDP, i.e. they do not
participate in edges. On the other hand, t-trees can
contain generated nodes, which represent elided
words and do not correspond to any surface to-
10
The system was able to output the following functors (or-
dered in the descending order of their frequency in the sys-
tem output): RSTR, PAT, ACT, CONJ.member, APP, MANN,
LOC, TWHEN, DISJ.member, BEN, RHEM, PREC, ACMP,
MEANS, ADVS.member, CPR, EXT, DIR3, CAUS, COND,
TSIN, REG, DIR2, CNCS, and TTILL.
11
In the SDP context, the target representation derived
from the PCEDT is called by the same name as the origi-
nal treebank; but note that the PCEDT semantic dependency
graphs only encode a subset of the information annotated at
the tectogrammatical layer of the full treebank.
DM PAS PCEDT
LF LM LF LM LF LM
Priberam .8916 .2685 .9176 .3783 .7790 .1068
In-House .9246 .4807 .9206 .4384 .4315 .0030
UF UM UF UM UF UM
Priberam .9032 .2990 .9281 .3924 .8903 .3071
In-House .9349 .5230 .9317 .4429 .6919 .0148
Table 1: End-to-end ?in-house? parsing results.
ken. Most generated nodes are leaves and, thus,
can simply be omitted from the SDP graphs. Other
generated nodes are copies of normal nodes and
they are linked to the same token to which the
source node is mapped. As a result, one token can
appear at several different positions in the tree; if
we project these occurrences into one node, the
graph will contain cycles. We decided to remove
all generated nodes causing cycles. Their chil-
dren are attached to their parents and inherit the
functor of the generated node (Figure 1). The con-
version procedure also removes cycles caused by
more fine-grained tokenization of the t-layer.
Furthermore, t-trees use technical edges to cap-
ture paratactic constructions where the relations
are not ?true? dependencies. The conversion pro-
cedure extracts true dependency relations: Each
conjunct is linked to the parent or to a shared child
of the coordination. In addition, there are also
links from the conjunction to the conjuncts and
they are labeled CONJ.m(ember). These links pre-
serve the paratactic structure (which can even be
nested) and the type of coordination. See Figure 2
for an example.
5 Results and Reflections
Seeing as our ?in-house? parsers are not directly
trained on the semantic dependency graphs pro-
vided for the Task, but rather are built from ad-
ditional linguistic resources, we submitted results
from the parsing pipelines sketched in Sections 2
to 4 above to the open SDP track. Table 1
summarizes parser performance in terms of la-
beled and unlabeled F
1
(LF and UF)
12
and full-
sentence exact match (LM and UM), comparing
to the best-performing submission (dubbed Prib-
eram; Martins and Almeida, 2014) to this track.
Judging by the official SDP evaluation metric, av-
erage labeled F
1
over the three representations,
our ensemble ranked last among six participating
12
Our ensemble members exhibit comparatively small dif-
ferences in recall vs. precision.
338
teams; in terms of unlabeled average F
1
, the ?in-
house? submission achieved the fourth rank.
As explained in the task description (Oepen et
al., 2014), parts of the WSJ Corpus were excluded
from the SDP training and testing data because
of gaps in the DeepBank and Enju treebanks, and
to exclude cyclic dependency graphs, which can
sometimes arise in the DM and PCEDT conver-
sions. For these reasons, one has to allow for the
possibility that the testing data is positively bi-
ased towards our ensemble members.
13
But even
with this caveat, it seems fair to observe that the
ERG and Enju parsers both are very competitive
for the DM and PAS target representations, respec-
tively, specifically so when judged in exact match
scores. A possible explanation for these results
lies in the depth of grammatical information avail-
able to these parsers, where DM or PAS seman-
tic dependency graphs are merely a simpliefied
view on the complete underlying HPSG analyses.
These parsers have performed well in earlier con-
trastive evaluation too (Miyao et al., 2007; Bender
et al., 2011; Ivanova et al., 2013; inter alios).
Results for the Treex English parsing scenario,
on the other hand, show that this ensemble mem-
ber is not fine-tuned for the PCEDT target rep-
resentation; due to the reasons mentioned above,
its performance even falls behind the shared task
baseline. As is evident from the comparison of
labeled vs. unlabeled F
1
scores, (a) the PCEDT
parser is comparatively stronger at recovering se-
mantic dependency structure than at assigning la-
bels, and (b) about the same appears to be the case
for the best-performing Priberam system (on this
target representation).
Acknowledgements
Data preparation and large-scale parsing in the
DM target representation was supported through
access to the ABEL high-performance computing
facilities at the University of Oslo, and we ac-
knowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. This project
has been supported by the infrastructural funding
13
There is no specific evidence that the WSJ sentences ex-
cluded in the Task for technical issues in either of the under-
lying treebanks or conversion procedures would be compara-
tively much easier to parse for other submissions than for the
members of our ?in-house? ensemble, but unlike other sys-
tems these parsers ?had a vote? in the selection of the data,
particularly so for the DM and PAS target representations.
by the Ministry of Education, Youth and Sports of
the Czech Republic (CEP ID LM2010013).
References
Bender, E. M., Flickinger, D., Oepen, S., and
Zhang, Y. (2011). Parser evaluation over local
and non-local deep dependencies in a large cor-
pus. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Pro-
cessing (p. 397 ? 408). Edinburgh, Scotland,
UK.
Callmeier, U. (2002). Preprocessing and encoding
techniques in PET. In S. Oepen, D. Flickinger,
J. Tsujii, and H. Uszkoreit (Eds.), Collabora-
tive language engineering. A case study in effi-
cient grammar-based processing (p. 127 ? 140).
Stanford, CA: CSLI Publications.
Copestake, A., Flickinger, D., Pollard, C., and
Sag, I. A. (2005). Minimal Recursion Seman-
tics. An introduction. Research on Language
and Computation, 3(4), 281 ? 332.
Flickinger, D. (2000). On building a more ef-
ficient grammar by exploiting types. Natural
Language Engineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., and Kordoni, V. (2012).
DeepBank. A dynamically annotated treebank
of the Wall Street Journal. In Proceedings of the
11th International Workshop on Treebanks and
Linguistic Theories (p. 85 ? 96). Lisbon, Portu-
gal: Edi??es Colibri.
Haji
?
c, J., Haji
?
cov?, E., Panevov?, J., Sgall, P.,
Bojar, O., Cinkov?, S., . . . ?abokrtsk?, Z.
(2012). Announcing Prague Czech-English De-
pendency Treebank 2.0. In Proceedings of the
8th International Conference on Language Re-
sources and Evaluation (p. 3153 ? 3160). Istan-
bul, Turkey.
Ivanova, A., Oepen, S., Dridan, R., Flickinger, D.,
and ?vrelid, L. (2013). On different approaches
to syntactic analysis into bi-lexical dependen-
cies. An empirical comparison of direct, PCFG-
based, and HPSG-based parsers. In Proceedings
of the 13th International Conference on Parsing
Technologies (p. 63 ? 72). Nara, Japan.
Ivanova, A., Oepen, S., ?vrelid, L., and
Flickinger, D. (2012). Who did what to whom?
339
A contrastive study of syntacto-semantic depen-
dencies. In Proceedings of the Sixth Linguistic
Annotation Workshop (p. 2 ? 11). Jeju, Republic
of Korea.
Klime?, V. (2007). Transformation-based tec-
togrammatical dependency analysis of English.
In V. Matou?ek and P. Mautner (Eds.), Text,
speech and dialogue 2007, LNAI 4629 (p. 15 ?
22). Berlin / Heidelberg, Germany: Springer.
Marcus, M., Santorini, B., and Marcinkiewicz,
M. A. (1993). Building a large annotated cor-
pora of English: The Penn Treebank. Computa-
tional Linguistics, 19, 313 ? 330.
Martins, A. F. T., and Almeida, M. S. C. (2014).
Priberam. A turbo semantic parser with second
order features. In Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation.
Dublin, Ireland.
McDonald, R., Pereira, F., Ribarov, K., and Haji
?
c,
J. (2005). Non-projective dependency parsing
using spanning tree algorithms. In Proceedings
of the Human Language Technology Conference
and Conference on Empirical Methods in Nat-
ural Language Processing (p. 523 ? 530). Van-
couver, British Columbia, Canada.
Miyao, Y. (2006). From linguistic theory to
syntactic analysis. Corpus-oriented grammar
development and feature forest model. Doc-
toral Dissertation, University of Tokyo, Tokyo,
Japan.
Miyao, Y., Ninomiya, T., and Tsujii, J. (2004).
Corpus-oriented grammar development for ac-
quiring a Head-Driven Phrase Structure Gram-
mar from the Penn Treebank. In Proceedings of
the 1st International Joint Conference on Natu-
ral Language Processing (p. 684 ? 693).
Miyao, Y., Sagae, K., and Tsujii, J. (2007).
Towards framework-independent evaluation of
deep linguistic parsers. In Proceedings of
the 2007 Workshop on Grammar Engineering
across Frameworks (p. 238 ? 258). Palo Alto,
California.
Miyao, Y., and Tsujii, J. (2008). Feature forest
models for probabilistic HPSG parsing. Com-
putational Linguistics, 34(1), 35 ? 80.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nils-
son, J., Riedel, S., and Yuret, D. (2007). The
CoNLL 2007 shared task on dependency pars-
ing. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Conference on Natural
Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., and Carroll, J. (2000). Ambiguity
packing in constraint-based parsing. Practical
results. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for
Computational Linguistics (p. 162 ? 169). Seat-
tle, WA, USA.
Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D.,
Flickinger, D., Haji
?
c, J., . . . Zhang, Y. (2014).
SemEval 2014 Task 8. Broad-coverage seman-
tic dependency parsing. In Proceedings of the
8th International Workshop on Semantic Evalu-
ation. Dublin, Ireland.
Oepen, S., and L?nning, J. T. (2006).
Discriminant-based MRS banking. In Proceed-
ings of the 5th International Conference on
Language Resources and Evaluation (p. 1250 ?
1255). Genoa, Italy.
Palmer, M., Gildea, D., and Kingsbury, P. (2005).
The Proposition Bank. A corpus annotated with
semantic roles. Computational Linguistics,
31(1), 71 ? 106.
Pollard, C., and Sag, I. A. (1994). Head-Driven
Phrase Structure Grammar. Chicago, USA:
The University of Chicago Press.
Popel, M., and ?abokrtsk?, Z. (2010). TectoMT.
Modular NLP framework. Advances in Natural
Language Processing, 293 ? 304.
Zhang, Y., Oepen, S., and Carroll, J. (2007).
Efficiency in unification-based n-best parsing.
In Proceedings of the 10th International Con-
ference on Parsing Technologies (p. 48 ? 59).
Prague, Czech Republic.
340
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 29?35
Manchester, August 2008
Parser Evaluation across Frameworks without Format Conversion
Wai Lok Tam
Interfaculty Initiative in
Information Studies
University of Tokyo
7-3-1 Hongo Bunkyo-ku
Tokyo 113-0033 Japan
Yo Sato
Dept of Computer Science
Queen Mary
University of London
Mile End Road
London E1 4NS, U.K.
Yusuke Miyao
Dept of Computer Science
University of Tokyo
7-3-1 Hongo Bunkyo-ku
Tokyo 113-0033 Japan
Jun-ichi Tsujii
Abstract
In the area of parser evaluation, formats
like GR and SD which are based on
dependencies, the simplest representation
of syntactic information, are proposed as
framework-independent metrics for parser
evaluation. The assumption behind these
proposals is that the simplicity of depen-
dencies would make conversion from syn-
tactic structures and semantic representa-
tions used in other formalisms to GR/SD a
easy job. But (Miyao et al, 2007) reports
that even conversion between these two
formats is not easy at all. Not to mention
that the 80% success rate of conversion
is not meaningful for parsers that boast
90% accuracy. In this paper, we make
an attempt at evaluation across frame-
works without format conversion. This
is achieved by generating a list of names
of phenomena with each parse. These
names of phenomena are matched against
the phenomena given in the gold stan-
dard. The number of matches found is used
for evaluating the parser that produces the
parses. The evaluation method is more ef-
fective than evaluation methods which in-
volve format conversion because the gen-
eration of names of phenomena from the
output of a parser loaded is done by a rec-
ognizer that has a 100% success rate of
recognizing a phenomenon illustrated by a
sentence. The success rate is made pos-
sible by the reuse of native codes: codes
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
used for writing the parser and rules of the
grammar loaded into the parser.
1 Introduction
The traditional evaluation method for a deep parser
is to test it against a list of sentences, each of which
is paired with a yes or no. The parser is evaluated
on the number of grammatical sentences it accepts
and that of ungrammatical sentences it rules out.
A problem with this approach to evaluation is that
it neither penalizes a parser for getting an analy-
sis wrong for a sentence nor rewards it for getting
it right. What prevents the NLP community from
working out a universally applicable reward and
penalty scheme is the absence of a gold standard
that can be used across frameworks. The correct-
ness of an analysis produced by a parser can only
be judged by matching it to the analysis produced
by linguists in syntactic structures and semantic
representations created specifically for the frame-
work on which the grammar is based. A match or
a mismatch between analyses produced by differ-
ent parsers based on different frameworks does not
lend itself for a meaningful comparison that leads
to a fair evaluation of the parsers. To evaluate two
parsers across frameworks, two kinds of methods
suggest themselves:
1. Converting an analysis given in a certain for-
mat native to one framework to another na-
tive to a differernt framework (e.g. converting
from a CCG (Steedman, 2000) derivation tree
to an HPSG (Pollard and Sag, 1994) phrase
structure tree with AVM)
2. Converting analyses given in different
framework-specific formats to some simpler
format proposed as a framework-independent
evaluation schema (e.g. converting from
29
HPSG phrase structure tree with AVM to GR
(Briscoe et al, 2006))
However, the feasibility of either solution is
questionable. Even conversion between two eval-
uation schemata which make use of the simplest
representation of syntactic information in the form
of dependencies is reported to be problematic by
(Miyao et al, 2007).
In this paper, therefore, we propose a different
method of parser evaluation that makes no attempt
at any conversion of syntactic structures and se-
mantic representations. We remove the need for
such conversion by abstracting away from com-
parison of syntactic structures and semantic rep-
resentations. The basic idea is to generate a list
of names of phenomena with each parse. These
names of phenomena are matched against the phe-
nomena given in the gold standard for the same
sentence. The number of matches found is used
for evaluating the parser that produces the parse.
2 Research Problem
Grammar formalisms differ in many aspects. In
syntax, they differ in POS label assignment, phrase
structure (if any), syntactic head assignment (if
any) and so on, while in semantics, they differ
from each other in semantic head assignment, role
assignment, number of arguments taken by pred-
icates, etc. Finding a common denominator be-
tween grammar formalisms in full and complex
representation of syntactic information and seman-
tic information has been generally considered by
the NLP community to be an unrealistic task, al-
though some serious attempts have been made re-
cently to offer simpler representation of syntactic
information (Briscoe et al, 2006; de Marneffe et
al., 2006).
Briscoe et al(2006)?s Grammatical Rela-
tion (GR) scheme is proposed as a framework-
independent metric for parsing accuracy. The
promise of GR lies actually in its dependence on
a framework that makes use of simple representa-
tion of syntactic information. The assumption be-
hind the usefulness of GR for evaluating the out-
put of parsers is that most conflicts between gram-
mar formalisms would be removed by discarding
less useful information carried by complex syn-
tactic or semantic representations used in gram-
mar formalisms during conversion to GRs. But
is this assumption true? The answer is not clear.
A GR represents syntactic information in the form
of a binary relation between a token assigned as
the head of the relation and other tokens assigned
as its dependents. Notice however that grammar
frameworks considerably disagree in the way they
assign heads and non-heads. This would raise the
doubt that, no matter how much information is re-
moved, there could still remain disagreements be-
tween grammar formalisms in what is left.
The simplicity of GR, or other dependency-
based metrics, may give the impression that con-
version from a more complex representation into
it is easier than conversion between two complex
representations. In other words, GRs or a sim-
ilar dependency relation looks like a promising
candidate for lingua franca of grammar frame-
works. However the experiment results given by
Miyao et al(2007) show that even conversion into
GRs of predicate-argument structures, which is not
much more complex than GRs, is not a trivial task.
Miyao et al(2007) manage to convert 80% of the
predicate-argument structures outputted by their
deep parser, ENJU, to GRs correctly. However the
parser, with an over 90% accuracy, is too good for
the 80% conversion rate. The lesson here is that
simplicity of a representation is a different thing
from simplicity in converting into that representa-
tion.
3 Outline of our Solution
The problem of finding a common denominator for
grammar formalisms and the problem of conver-
sion to a common denominator may be best ad-
dressed by evaluating parsers without making any
attempt to find a common denominator or conduct
any conversion. Let us describe briefly in this sec-
tion how such evaluation can be realised.
3.1 Creating the Gold Standard
The first step of our evaluation method is to con-
struct or find a number of sentences and get an an-
notator to mark each sentence for the phenomena
illustrated by each sentence. After annotating all
the sentences in a test suite, we get a list of pairs,
whose first element is a sentence ID and second is
again a list, one of the corresponding phenomena.
This list of pairs is our gold standard. To illustrate,
suppose we only get sentence 1 and sentence 2 in
our test suite.
(1) John gives a flower to Mary
(2) John gives Mary a flower
30
Sentence 1 is assigned the phenomena: proper
noun, unshifted ditransitive, preposition. Sentence
2 is assigned the phenomena: proper noun, dative-
shifted ditransitive. Our gold standard is thus the
following list of pairs:
?1, ?proper noun, unshifted ditransitive, preposition? ?,
?2, ?proper noun,dative-shifted ditransitive? ?
3.2 Phenomena Recognition
The second step of our evaluation method requires
a small program that recognises what phenomena
are illustrated by an input sentence taken from the
test suite based on the output resulted from pars-
ing the sentence. The recogniser provides a set
of conditions that assign names of phenomena to
an output, based on which the output is matched
with some framework-specific regular expressions.
It looks for hints like the rule being applied at a
node, the POS label being assigned to a node, the
phrase structure and the role assigned to a refer-
ence marker. The names of phenomena assigned
to a sentence are stored in a list. The list of phe-
nomena forms a pair with the ID of the sentence,
and running the recogniser on multiple outputs ob-
tained by batch parsing (with the parser to be eval-
uated) will produce a list of such pairs, in exactly
the same format as our gold standard. Let us illus-
trate this with a parser that:
1. assigns a monotransitive verb analysis to
?give? and an adjunct analysis to ?to Mary? in
1
2. assigns a ditransitive verb analysis to ?give? in
2
The list of pairs we obtain from running the
recogniser on the results produced by batch pars-
ing the test suite with the parser to be evaluated is
the following:
?1,?proper noun,monotransitive,preposition,adjunct??,
?2, ?proper noun,dative-shifted ditransitive? ?
3.3 Performance Measure Calculation
Comparing the two list of pairs generated from the
previous steps, we can calculate the precision and
recall of a parser using the following formulae:
Precision = (
n
?
i=1
| R
i
?A
i
|
| R
i
|
)? n (1)
Recall = (
n
?
i=1
| R
i
?A
i
|
| A
i
|
)? n (2)
where list R
i
is the list generated by the recogniser
for sentence i, list A
i
is the list produced by anno-
tators for sentence i, and n the number of sentences
in the test suite.
In our example, the parser that does a good job
with dative-shifted ditransitives but does a poor job
with unshifted ditranstives would have a precision
of:
(
2
4
+
2
2
)? 2 = 0.75
and a recall of:
(
2
3
+
2
2
)? 2 = 0.83
4 Refining our Solution
In order for the precision and recall given above to
be a fair measure, it is necessary for both the recog-
niser and the annotators to produce an exhaustive
list of the phenomena illustrated by a sentence.
But we foresee that annotation errors are likely
to be a problem of exhaustive annotation, as is re-
ported in Miyao et al(2007) for the gold standard
described in Briscoe et al(2006). Exhaustive an-
notation procedures require annotators to repeat-
edly parse a sentence in search for a number of
phenomena, which is not the way language is nor-
mally processed by humans. Forcing annotators to
do this, particularly for a long and complex sen-
tence, is a probable reason for the annotation er-
rors in the gold standard described in (Briscoe et
al., 2006).
To avoid the same problem in our creation of a
gold standard, we propose to allow non-exhaustive
annotation. In fact, our proposal is to limit the
number of phenomena assigned to a sentence to
one. This decision on which phenomenon to be as-
signed is made, when the test suite is constructed,
for each of the sentences contained in it. Follow-
ing the traditional approach, we include every sen-
tence in the test suite, along with the core phe-
nomenon we intend to test it on (Lehmann and
Oepen, 1996). Thus, Sentence 1 would be as-
signed the phenomenon of unshifted ditransitive.
Sentence 2 would be assigned the phenomenon of
31
dative-shifted ditransitive. This revision of anno-
tation policy removes the need for exhaustive an-
notation. Instead, annotators are given a new task.
They are asked to assign to each sentence the most
common error that a parser is likely to make. Thus
Sentence 1 would be assigned adjunct for such an
error. Sentence 2 would be assigned the error of
noun-noun compound. Note that these errors are
also names of phenomena.
This change in annotation policy calls for a
change in the calculation of precision and recall.
We leave the recogniser as it is, i.e. to produce an
exhaustive list of phenomena, since it is far beyond
our remit to render it intelligent enough to select a
single, intended, phenomenon. Therefore, an in-
correctly low precision would result from a mis-
match between the exhaustive list generated by the
recogniser and the singleton list produced by an-
notators for a sentence. For example, suppose we
only have sentence 2 in our test suite and the parser
correctly analyses the sentence. Our recogniser as-
signs two phenomena (proper noun, dative-shifted
ditransitive) to this sentence as before. This would
result in a precision of 0.5.
Thus we need to revise our definition of preci-
sion, but before we give our new definition, let us
define a truth function t:
t(A ? B) =
{
1 A ? B
0 A ?B = ?
t(A ?B = ?) =
{
0 A ?B 6= ?
1 A ?B = ?
Now, our new definition of precision and recall
is as follows:
Precision (3)
=
(
?
n
i=1
t(R
i
?AP
i
)+t(R
i
?AN
i
=?)
2
)
n
Recall (4)
=
(
?
n
i=1
|R
i
?AP
i
|
|AP
i
|
)
n
where list AP
i
is the list of phenomena produced
by annotators for sentence i, and list AN
i
is the list
of errors produced by annotators for sentence i.
While the change in the definition of recall is
trivial, the new definition of precision requires
some explanation. The exhaustive list of phenom-
ena generated by our recogniser for each sentence
is taken as a combination of two answers to two
questions on the two lists produced by annotators
for each sentence. The correct answer to the ques-
tion on the one-item-list of phenomenon produced
by annotators for a sentence is a superset-subset re-
lation between the list generated by our recogniser
and the one-item-list of phenomenon produced by
annotators. The correct answer to the question on
the one-item-list of error produced by annotators
for a sentence is the non-existence of any common
member between the list generated by our recog-
niser and the one-item-list of error produced by an-
notators.
To illustrate, let us try a parser that does a good
job with dative-shifted ditransitives but does a poor
job with unshifted ditranstives on both 2 and 1.
The precision of such a parser would be:
(
0
2
+
2
2
)? 2 = 0.5
and its recall would be:
(
0
1
+
1
1
)? 2 = 0.5
5 Experiment
For this abstract, we evaluate ENJU (Miyao,
2006), a released deep parser based on the HPSG
formalism and a parser based on the Dynamic Syn-
tax formalism (Kempson et al, 2001) under devel-
opment against the gold standard given in table 1.
The precision and recall of the two parsers
(ENJU and DSPD, which stands for ?Dynamic
Syntax Parser under Development?) are given in
table 3:
The experiment that we report here is intended
to be an experiment with the evaluation method de-
scribed in the last section, rather than a very seri-
ous attempt to evaluate the two parsers in question.
The sentences in table 1 are carefully selected to
include both sentences that illustrate core phenom-
ena and sentences that illustrate rarer but more in-
teresting (to linguists) phenomena. But there are
too few of them. In fact, the most important num-
ber that we have obtained from our experiment is
the 100% success rate in recognizing the phenom-
ena given in table 1.
32
ID Phenomenon Error
1 unshifted ditransi-
tive
adjunct
2 dative-shifted di-
transitive
noun-noun com-
pound
3 passive adjunct
4 nominal gerund verb that takes
verbal comple-
ment
5 verbal gerund imperative
6 preposition particle
7 particle preposition
8 adjective with ex-
trapolated senten-
tial complement
relative clause
9 inversion question
10 raising control
Figure 1: Gold Standard for Parser Evaluation
ID Sentence
1 John gives a flower to Mary
2 John give Mary a flower
3 John is dumped by Mary
4 Your walking me pleases me
5 Abandoning children increased
6 He talks to Mary
7 John makes up the story
8 It is obvious that John is a fool
9 Hardly does anyone know Mary
10 John continues to please Mary
Figure 2: Sentences Used in the Gold Standard
Measure ENJU DSPD
Precision 0.8 0.7
Recall 0.7 0.5
Figure 3: Performance of Two Parsers
6 Discussion
6.1 Recognition Rate
The 100% success rate is not as surprising as it
may look. We made use of two recognisers, one
for each parser. Each of them is written by the
one of us who is somehow involved in the devel-
opment of the parser whose output is being recog-
nised and familiar with the formalism on which the
output is based. This is a clear advantage to for-
mat conversion used in other evaluation methods,
which is usually done by someone familiar with ei-
ther the source or the target of conversion, but not
both, as such a recogniser only requires knowledge
of one formalism and one parser. For someone
who is involved in the development of the gram-
mar and of the parser that runs it, it is straight-
forward to write a recogniser that can make use
of the code built into the parser or rules included
in the grammar. We can imagine that the 100%
recognition rate would drop a little if we needed
to recognise a large number of sentences but were
not allowed sufficient time to write detailed regular
expressions. Even in such a situation, we are con-
fident that the success rate of recognition would be
higher than the conversion method.
Note that the effectiveness of our evaluation
method depends on the success rate of recognition
to the same extent that the conversion method em-
ployed in Briscoe et al (2006) and de Marneff et
al. (2006) depends on the conversion rate. Given
the high success rate of recognition, we argue that
our evaluation method is more effective than any
evaluation method which makes use of a format
claimed to be framework independent and involves
conversion of output based on a different formal-
ism to the proposed format.
6.2 Strictness of Recognition and Precision
There are some precautions regarding the use of
our evaluation method. The redefined precision 4
is affected by the strictness of the recogniser. To
illustrate, let us take Sentence 8 in Table 1 as an
example. ENJU provides the correct phrase struc-
ture analysis using the desired rules for this sen-
tence but makes some mistakes in assigning roles
to the adjective and the copular verb. The recog-
niser we write for ENJU is very strict and refuses
to assign the phenomenon ?adjective with extrap-
olated sentential complement? based on the output
given by ENJU. So ENJU gets 0 point for its an-
swer to the question on the singleton list of phe-
33
nomenon in the gold standard. But it gets 1 point
for its answer to the question on the singleton list
of error in the gold standard because it does not
go to the other extreme: a relative clause analysis,
yielding a 0.5 precision. In this case, this value is
fair for ENJU, which produces a partially correct
analysis. However, a parser that does not accept
the sentence at all, a parser that fails to produce
any output or one that erroneously produces an un-
expected phenomenon would get the same result:
for Sentence 8, such a parser would still get a pre-
cision of 0.5, simply because its output does not
show that it assigns a relative clause analysis.
We can however rectify this situation. For the
lack of parse output, we can add an exception
clause to make the parser automatically get a 0 pre-
cision (for that sentence). Parsers that make unex-
pected mistakes are more problematic. An obvi-
ous solution to deal with these parsers is to come
up with an exhaustive list of mistakes but this is an
unrealistic task. For the moment, a temporary but
realistic solution would be to expand the list of er-
rors assigned to each sentence in the gold standard
and ask annotators to make more intelligent guess
of the mistakes that can be made by parsers by con-
sidering factors such as similarities in phrase struc-
tures or the sharing of sub-trees.
6.3 Combining Evaluation Methods
For all measures, some distortion is unavoidable
when applied to exceptional cases. This is true for
the classical precision and recall, and our redefined
precision and recall is no exception. In the case of
the classical precision and recall, the distortion is
countered by the inverse relation between them so
that even if one is distorted, we can tell from the
other that how well (poorly) the object of evalua-
tion performs. Our redefined precision and recall
works pretty much the same way.
What motivates us to derive measures so closely
related to the classical precision and recall is the
ease to combine the redefined precision and recall
obtained from our evaluation method with the clas-
sical precision and recall obtained from other eval-
uation methods, so as to obtain a full picture of
the performance of the object of evaluation. For
example, our redefined precision and recall figures
given in Table 3 (or figures obtained from running
the same experiment on a larger test set) for ENJU
can be combined with the precision and recall fig-
ures given in Miyao et al (2006) for ENJU, which
is based on a evaluation method that compares its
predicate-argument structures those given in Penn
Treebank. Here the precision and recall figures are
calculated by assigning an equal weight to every
sentence in Section 23 of Penn Treebank. This
means that different weights are assigned to dif-
ferent phenomena depending on their frequency in
the Penn Treebank. Such assignment of weights
may not be desirable for linguists or developers
of NLP systems who are targeting a corpus with a
very different distribution of phenomena from this
particular section of the Penn Treebank. For exam-
ple, a linguist may wish to assign an equal weight
across phenomena or more weights to ?interesting?
phenomena. A developer of a question-answering
system may wish to give more weights to question-
related phenomena than other phenomena of less
interest which are nevertheless attested more fre-
quently in the Penn Treebank.
In sum, the classical precision and recall fig-
ures calculated by assigning equal weight to ev-
ery sentence could be considered skewed from the
perspective of phenomena, whereas our redefined
precision and recall figures may be seen as skewed
from the frequency perspective. Frequency is rela-
tive to domains: less common phenomena in some
domains could occur more often in others. Our re-
defined precision and recall are not only useful for
those who want a performance measure skewed the
way they want, but also useful for those who want
a performance measure as ?unskewed? as possible.
This may be obtained by combining our redefined
precision and recall with the classical precision
and recall yielded from other evaluation methods.
7 Conclusion
We have presented a parser evaluation method
that addresses the problem of conversion between
frameworks by totally removing the need for that
kind of conversion. We do some conversion but
it is a different sort. We convert the output of a
parser to a list of names of phenomena by drawing
only on the framework that the parser is based on.
It may be inevitable for some loss or inaccuracy
to occur during this kind of intra-framework con-
version if we try our method on a much larger test
set with a much larger variety of longer sentences.
But we are confident that the loss would still be
far less than any inter-framework conversion work
done in other proposals of cross-framework evalu-
ation methods. What we believe to be a more prob-
34
lematic area is the annotation methods we have
suggested. At the time we write this paper based
on a small-scale experiment, we get slightly bet-
ter result by asking our annotator to give one phe-
nomenon and one common mistake for each sen-
tence. This may be attributed to the fact that he
is a member of the NLP community and hence he
gets the knowledge to identify the core phenom-
ena we want to test and the common error that
parsers tend to make. If we expand our test set
and includes longer sentences, annotators would
make more mistakes whether they attempt exhaus-
tive annotation or non-exhaustive annotation. It
is difficult to tell whether exhaustive annotation
or non-exhaustive annotation would be better for
large scale experiments. As future work, we intend
to try our evaluation method on more test data to
determine which one is better and find ways to im-
prove the one we believe to be better for large scale
evaluation.
References
Briscoe, Ted, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of COLING/ACL 2006.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Kempson, Ruth, Wilfried Meyer-Viol, and Dov Gab-
bay. 2001. Dynamic Syntax: The Flow of Language
Understanding. Blackwell.
Lehmann, Sabine and Stephan Oepen. 1996. TSNLP
test suites for natural language processing. In Pro-
ceedings of COLING 1996.
Miyao, Yusuke, Kenji Sagae, and Junichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of GEAF 2007.
Miyao, Yusuke. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. thesis, Uni-
versity of Tokyo.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press and CSLI Publications.
Steedman, Mark. 2000. Syntactic Process. MIT Press.
35
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 123?126,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Deep Re-annotation in a Chinese Scientific Treebank  
Kun Yu1 Xiangli Wang1 Yusuke Miyao2 Takuya Matsuzaki1 Junichi Tsujii1,3 1. The University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {kunyu, xiangli, matuzaki, tsujii}@is.s.u-tokyo.ac.jp 2. National Institute of Informatics, Hitotsubashi 2-1-2, Chiyoda-ku, Tokyo, 101-8430, Japan yusuke@nii.ac.jp 3. The University of Manchester, Oxford Road, Manchester, M13 9PL, UK  
Abstract 
In this paper, we introduce our recent work on re-annotating the deep information, which includes both the grammatical functional tags and the traces, in a Chinese scientific tree-bank. The issues with regard to re-annotation and its corresponding solutions are discussed. Furthermore, the process of the re-annotation work is described. 1 Introduction A Chinese scientific Treebank (called the NICT Chinese Treebank) has been developed by the National Institute of Information and Communi-cations Technology of Japan (NICT). This tree-bank annotates the word segmentation, pos-tags, and bracketing structures according to the anno-tation guideline of the Penn Chinese Treebank (Xia, 2000(a); Xia, 2000(b); Xue and Xia, 2000). Contrary to the Penn Chinese Treebank in news domain, the NICT Chinese Treebank includes sentences that are manually translated from Japanese scientific papers. Currently, the NICT Chinese Treebank includes around 8,000 Chinese sentences. The annotation of more sen-tences in the science domain is ongoing.  The current annotation of the NICT Chinese Treebank is informative for some language analysis tasks, such as syntactic parsing and word segmentation. However, the deep informa-tion, which includes both the grammatical func-tional tags and the traces, are omitted in the an-notation. Without grammatical functions, the simple bracketing structure is not informative enough to represent the semantics for Chinese. Furthermore, the traces are critical elements in detecting long-distance dependencies.  Gabbard et al (2006) and Blaheta and Charniak (2000) applied machine learning mod-els to automatically assign the empty categories and functional tags to an English treebank.  
However, considering about the different do-mains that the Penn Chinese Treebank and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chi-nese Treebank. With the deep re-annotation, the NICT Chinese Treebank could be used not only for the shallow natural language processing tasks, but also as a resource for deep applica-tions, such as the lexicalized grammar develop-ment from treebanks (Miyao 2006; Guo 2009; Xia 1999; Hockenmaier and Steedman 2002).  Considering that the translation quality of the sentences in the NICT Chinese Treebank may affect the quality of re-annotation, in the current phase, we only selected 2,363 sentences that are of good translation quality, for re-annotation. In the future, with the expansion of the NICT Chi-nese Treebank, we will continue this re-annotation work on large-scale sentences.  2 Content of Re-annotation Because the NICT Chinese Treebank follows the annotation guideline of the Penn Chinese Treebank, our re-annotation uses similar annota-tion criteria in the Penn Chinese Treebank.  Figure 1 exemplifies our re-annotation to a sentence in the NICT Chinese Treebank. In this example, we first re-annotate the trace (as indi-cated by the italicized part in Figure 1(b)) for the extracted head noun ??/word?. Furthermore, we re-annotate the functional tag of the trace (as indicated by the dashed-box in Figure 1(b)), to indicate that the extracted head noun should be restored into the relative clause as a topic. There are 26 functional tags in the Penn Chi-nese Treebank (Xue and Xia, 2000), in which seven functional tags describe the grammatical 
123
roles and one functional tag (i.e. LGS) indicates a logical subject. Since the eight functional tags are crucial for obtaining the grammatical func-tion of constituents, we re-annotate the eight functional tags (refer to Table 1) to the NICT Chinese Treebank. (NP (CP (IP (NP (NN ??)                             (NN ???))                               (VP (VA ?)))                                  (DEC ?))                           (NP (NN ?))) (the word of which the word cohesion is high) (a) A relative clause in the NICT Chinese Treebank          (NP (CP (WHNP-1 (-NONE- *OP*)      (CP (IP (NP-TPC (-NONE- *T*-1))                   (NP (NN ??)                           (NN ???))                   (VP (VA ?)))                                (DEC ?)))                (NP (NN ?))) (b) The relative clause after re-annotation Figure 1.  Our re-annotation to a relative clause. Functional Tag Description IO indirect object OBJ direct object EXT post-verbal complement that describes the extent, frequency, or quantity FOC object fronted to a pre-verbal but post-subject position PRD non-verbal predicate SBJ surface subject TPC topic LGS logical subject Table 1. Functional tags that we re-annotate.                 (IP (NP-TPC-1 (NN ??))                 (VP (ADVP (AD ??))                        (VP (ADVP (AD ??))                               (VP (VV ??)                                      (NP-OBJ (-NONE- *T*-1))))))                            (It is easier to obtain information.) (a) A topic construction with long-distance dependency after re-annotation of functional tag and trace          (IP (NP-TPC (DP (DT ?))                                (NP (NN ??)))                (NP-SBJ (NP (PN ?))                               (NP (NN ???)))                (VP (ADVP (AD ?))                        (VP (VV ??)                               (VV ??))))                (The rationality of this algorithm has been verified.)  (b) A topic construction without long-distance dependency after re-annotation of functional tag Figure 2. Our re-annotation to topic constructions. In addition, in the annotation guideline of the Penn Chinese Treebank, four constructions are annotated with traces: BA-construction, BEI-construction, topic construction and relative clause. The BEI-construction and relative 
clause introduce long-distance dependency. Therefore, we re-annotate the traces for the two constructions. The topic construction introduces the topic phrase. For the topic constructions that contain long-distance dependency, we re-annotate both the traces and the functional tags (refer to the italicized part in Figure 2(a)). Some topic constructions, however, do not include long-distance dependency. In such cases, we only re-annotate the functional tag to indicate that it is a topic (refer to the italicized part in Figure 2(b)). In addition, the BA-construction moves the object to a pre-verbal position. Al-though the BA-construction does not contain long-distance dependency, we still re-annotate the trace to acquire the original position of the moved object in the sentence. 3 Issues and Solutions 3.1 Trace re-annotation in the BA/BEI construction The NICT Chinese Treebank follows the word segmentation and pos-tag annotation guideline of the Penn Chinese Treebank. Therefore, there are some BA-constructions and BEI-constructions that cannot be re-annotated with traces. The principle reason for this is that the moved object has semantic relations with only part of the verb. For example, in the sentence shown in Figure 3(a), the moved head noun ???/hometown? is the object of ??/construct?, but not for ???/construct to be?.  (VP (BA ?)                (IP (NP (NN ??))                      (VP (VV ??)                             (NP (NN ??))))) (construct the hometown to be a garden) (a) The annotation in the NICT Chinese Treebank  (VP (BA ?)                            (IP (NP-SBJ-1 (NN ??))                                  (VP (VV ?)                                         (NP-OBJ (-NONE- *-1))                                         (AM ?)                                         (NP (NN ??))))) (b) Our proposed re-annotation of functional tag and trace Figure 3.  Our re-annotation to a BA construction with split verb. Our analysis of the Penn Chinese Treebank shows that only a closed list of characters (such as ??/to be?) can be attached to verbs in such a case. Therefore, we solve the problem by fol-lowing four steps (for an example, refer to Fig-ure 3(b)): 
124
(1) A linguist manually collects the characters that can be attached to verbs in such a case from the Penn Chinese Treebank and assigns them a new pos-tag ?AM (argument marker)?.  (2) The annotators use the character list as a reference during the re-annotation. When the verb in a BA/BEI construction ends with a char-acter in the list, and the annotators think the verb should be split, the annotators record the sentence ID without performing any re-annotation.  (3) The linguist collects all of the recorded sentences, and defines pattern rules to automati-cally split the verbs in the BA/BEI construc-tions. (4) The annotators annotate trace for the sen-tences with the split verbs. This step will be fin-ished in our future work. 3.2 Topic detection In the annotation guideline of the Penn Chinese Treebank, a topic is defined as ?the element that appears before the subject in a declarative sen-tence?. However, the NICT Chinese Treebank does not annotate the omitted subject. Therefore, we could not use the position of the subject as a criterion for topic detection.  In order to resolve this issue, we define some heuristic rules based on both the meaning and the bracketing structure of phrases, to help de-tect the topic phrase. Only the phrase that satis-fies all the rules will be re-annotated as a topic. The following exemplifies some rules: (1) If there is a phrase before a subject, the phrase is probably a topic. (2) A topic phrase must be parallel to the fol-lowing verb phrase. (3) The preposition phrase and localization phrase describing the location or time are not topics. 3.3 Inconsistent annotation in the NICT Chinese Treebank There are some inconsistent annotations in the NICT Chinese Treebank, which makes our re-annotation work difficult.  These inconsistencies include: (1) Inconsistent word segmentation, such as segmenting the word ???? /corresponding? into two words ???/opposite? and ??/ought?. (2) Inconsistent pos-tag annotation. For ex-ample, when the word  ???  exists between two noun phrases, it should be tagged as an associa-tive marker (i.e. DEG), according to the guide-
line of the Penn Chinese Treebank. However, in the NICT Chinese Treebank, sometimes it is tagged as a nominalizer (i.e. DEC).  (3) Inconsistent bracketing annotation. Fig-ure 4(a) shows the annotation of a relative clause in the NICT Chinese Treebank. In this annotation, the noun phrase ???/Osaka ??/subway? is incorrectly treated as the extracted head; furthermore, the adverb ???/by hand? that modifies the verb ???/make? is incor-rectly annotated as an adjective that modifies the noun ????/deformation graph?. After cor-recting these inconsistencies, the relative clause should be annotated as shown in Figure 4(b). (NP (QP (CD ??))              (ADJP (JJ ??))              (DNP (NP (CP (IP (VP (VV ??)))                                       (DEC ?))                                (NP (NR ??)                                       (NN ??)))                         (DEG ?))              (NP (NN ???))) (many deformation graphs of Osaka subway that are made by hand)  (a) The inconsistent annotation of a relative clause (NP (QP (CD ??))        (NP (CP (IP (VP (ADVP (AD ??))                                     (VP (VV ??))))                       (DEC ?))                (NP (DNP (NP (NR ??)                                         (NN ??))                                  (DEG ?))                       (NP (NN ???)))))  (b) The annotation after correcting the inconsistencies Figure 4. An inconsistent annotation in the NICT Chinese Treebank and its correction. In our re-annotation, these inconsistently an-notated sentences in the NICT Chinese Tree-bank were recorded by the annotators. We then sent them back to NICT for further verification. 4 Process of Re-annotation 4.1 Annotation Guideline  During the re-annotation, we basically follow the annotation guideline of the Penn Chinese Treebank (Xue and Xia, 2000). However, in order to fit with the characteristics of scientific sentences in the NICT Chinese Treebank, some constraints are added to the guideline.  For example, in the science domain, the rela-tive clause is often used to describe a phenome-non, in which the extracted head noun is usually an abstract noun, and the relative clause is an appositive of the extracted head noun. Figure 5 shows an example in which the relative clause ???/system ??/stop ??/working? is a de-
125
scription of the extracted head noun ???/phenomenon?. In such a case, the head noun cannot be restored into the clause. Therefore, we add the following restriction in our re-annotation guideline: Do not re-annotate the trace when the head noun of a relative clause is an abstract noun and it is an appositive of the relative clause.         (NP (CP (IP (NP (NN ??))                              (VP (VV ??)                                      (NP (NN ??))))                        (DEC ?))                 (NP (NN ??))) (the phenomenon that the system stops working) Figure 5. A relative clause in the NICT Chinese Treebank. 4.2 Quality Control Several processes were undertaken to guarantee the quality of our re-annotation:  (1) We chose graduate students who major in Chinese for all of the annotators.  (2) A visualization tool - XConc Suite (Kim et al, 2008) was used as assistance during the re-annotation.  (3) Only 2,363 sentences with good transla-tion quality in the NICT Chinese Treebank were chosen for re-annotation in the current phase.   (4) Before starting the re-annotation, a lin-guist selected 200 representative sentences, which contain all the linguistic phenomena that we want to re-annotate, from among the 2,363 sentences in the NICT Chinese Treebank. The selected 200 sentences were manually re-annotated by the linguist, and were split into two sets for training the annotators sequentially. We evaluated the annotation quality of the anno-tators during training. The average annotation quality of all the annotators after training is shown in Table 2. Annotation Quality Inter-annotator Consistency Precision Recall Precision Recall 70.71% 70.75% 61.59% 61.59% Table 2. The average annotation quality of the annotators after training.      (5) After training, the remaining sentences were split into several parts and assigned to the annotators for re-annotation. In each part, there were around 20% sentences that were shared by all of the annotators. These shared sentences were used to check and guarantee inter-annotator consistency during the re-annotation.  5 Conclusion and Future Work  We re-annotated the deep information, which includes eight types of grammatical functional 
tags and the traces in four constructions, to a Chinese scientific treebank, i.e. the NICT Chi-nese Treebank. Since the NICT Chinese Tree-bank is based on manually translated sentences, only 2,363 sentences with good translation qual-ity were re-annotated in the current phase to guarantee the re-annotation quality.  In the future, we will finish the trace annota-tion for the BA and BEI constructions with split verbs. Furthermore, we will continue our re-annotation on more sentences in the NICT Chi-nese Treebank. Acknowledgments We would like to thank Dr. Kiyotaka Uchimoto and Dr. Junichi Kazama for providing the NICT Chinese Treebank. References  Don Blaheta and Eugene Charniak. 2000. Assigning Func-tion Tags to Parsed Text. Proceedings of NAACL 2000. Ryan Gabbard, Seth Kulick and Mitchell Marcus. 2006. Fully Parsing the Penn Treebank. Proceedings of HLT-NAACL 2006. Yuqing Guo. 2009. Treebank-based acquisition of Chinese LFG Resources for Parsing and Generation. Ph.D. Thesis. Dublin City University. Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Tree-bank. Proceedings of the 3rd LREC. Jindong Kim, Tomoko Ohta, and Junichi Tsujii. 2008. Corpus Annotation for Mining Biomedical Events from Literature. BMC Bioinformatics, 9(10).  Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-oriented Grammar Development and Feature Forest Model. Ph.D Thesis. The University of Tokyo. Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. Proceedings of the 5th NLPRS. Fei Xia. 2000 (a). The Segmentation Guidelines for the Penn Chinese Treebank (3.0). Fei Xia. 2000 (b). The Part-of-speech Tagging Guidelines for the Penn Chinese Treebank (3.0). Nianwen Xue, Fudong Chiou, and Martha Palmer. 2002. Building a Large-Scale Annotated Chinese Corpus. Proceedings of COLING 2002. Nianwen Xue and Fei Xia. 2000. The Bracketing Guide-lines for the Penn Chinese Treebank. Shiwen Yu et al 2002. The Basic Processing of Contempo-rary Chinese Corpus at Peking University Specification. Journal of Chinese Information Processing, 16 (5). Qiang Zhou. 2004. Annotation Scheme for Chinese Tree-bank. Journal of Chinese Information Processing, 18 (4). 
126
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 164?173,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Parsing Natural Language Queries for Life Science Knowledge
Tadayoshi Hara
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
harasan@nii.ac.jp
Yuka Tateisi
Faculty of Informatics, Kogakuin University
1-24-2 Nishi-shinjuku, Shinjuku-ku,
Tokyo 163-8677, JAPAN
yucca@cc.kogakuin.ac.jp
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku,
Tokyo 113-0032, JAPAN
jdkim@dbcls.rois.ac.jp
Yusuke Miyao
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
yusuke@nii.ac.jp
Abstract
This paper presents our preliminary work on
adaptation of parsing technology toward natu-
ral language query processing for biomedical
domain. We built a small treebank of natu-
ral language queries, and tested a state-of-the-
art parser, the results of which revealed that
a parser trained on Wall-Street-Journal arti-
cles and Medline abstracts did not work well
on query sentences. We then experimented
an adaptive learning technique, to seek the
chance to improve the parsing performance on
query sentences. Despite the small scale of the
experiments, the results are encouraging, en-
lightening the direction for effective improve-
ment.
1 Introduction
Recent rapid progress of life science resulted in a
greatly increased amount of life science knowledge,
e.g. genomics, proteomics, pathology, therapeutics,
diagnostics, etc. The knowledge is however scat-
tered in pieces in diverse forms over a large number
of databases (DBs), e.g. PubMed, Drugs.com, Ther-
apy database, etc. As more and more knowledge is
discovered and accumulated in DBs, the need for
their integration is growing, and corresponding ef-
forts are emerging (BioMoby1, BioRDF2, etc.).
Meanwhile, the need for a query language with
high expressive power is also growing, to cope with
1http://www.biomoby.org/
2http://esw.w3.org/HCLSIG BioRDF Subgroup
the complexity of accumulated knowledge. For ex-
ample, SPARQL3 is becoming an important query
language, as RDF4 is recognized as a standard in-
teroperable encoding of information in databases.
SPARQL queries are however not easy for human
users to compose, due to its complex vocabulary,
syntax and semantics. We propose natural language
(NL) query as a potential solution to the problem.
Natural language, e.g. English, is the most straight-
forward language for human beings. Extra training
is not required for it, yet the expressive power is
very high. If NL queries can be automatically trans-
lated into SPARQL queries, human users can access
their desired knowledge without learning the com-
plex query language of SPARQL.
This paper presents our preliminary work for
NL query processing, with focus on syntactic pars-
ing. We first build a small treebank of natural
language queries, which are from Genomics track
(Hersh et al, 2004; Hersh et al, 2005; Hersh et al,
2006; Hersh et al, 2007) topics (Section 2 and 3).
The small treebank is then used to test the perfor-
mance of a state-of-the-art parser, Enju (Ninomiya
et al, 2007; Hara et al, 2007) (Section 4). The
results show that a parser trained on Wall-Street-
Journal (WSJ) articles and Medline abstracts will
not work well on query sentences. Next, we ex-
periment an adaptive learning technique, to seek the
chance to improve the parsing performance on query
sentences. Despite the small scale of the experi-
ments, the results enlighten directions for effective
3http://www.w3.org/TR/rdf-sparql-query/
4http://www.w3.org/RDF/
164
GTREC
04 05 06 07
Declarative 1 0 0 0
Imperative 22 60 0 0
Infinitive 1 0 0 0
Interrogative
- WP/WRB/WDT 3 / 1 / 11 0 / 0 / 0 6 / 22 / 0 0 / 0 / 50
- Non-wh 5 0 0 0
NP 14 0 0 0
Total 58 60 28 50
Table 1: Distribution of sentence constructions
improvement (Section 5).
2 Syntactic Features of Query Sentences
While it is reported that the state-of-art NLP tech-
nology shows reasonable performance for IR or
IE applications (Ohta et al, 2006), NLP technol-
ogy has long been developed mostly for declara-
tive sentences. On the other hand, NL queries in-
clude wide variety of sentence constructions such
as interrogative sentences, imperative sentences, and
noun phrases. Table 1 shows the distribution of the
constructions of the 196 query sentences from the
topics of the ad hoc task of Genomics track 2004
(GTREC04) and 2005 (GTREC05) in their narra-
tive forms, and the queries for the passage retrieval
task of Genomics track 2006 (GTREC06) and 2007
(GTREC07).
GTREC04 set has a variety of sentence construc-
tions, including noun phrases and infinitives, which
are not usually considered as full sentences. In the
2004 track, the queries were derived from interviews
eliciting information needs of real biologists, with-
out any control on the sentence constructions.
GTREC05 consists only of imperative sentences.
In the 2005 track, a set of templates were derived
from an analysis of the 2004 track and other known
biologist information needs. The derived templates
were used as the commands to find articles describ-
ing biological interests such as methods or roles of
genes. Although the templates were in the form
?Find articles describing ...?, actual obtained imper-
atives begin with ?Describe the procedure or method
for? (12 sentences), ?Provide information about?
(36 sentences) or ?Provide information on? (12 sen-
tences).
GTREC06 consists only of wh-questions where a
wh-word constitutes a noun phrase by itself (i.e. its
 
S  
 
  
 
VP  
 
  
 
NP  
 
  
 
PP  
 
  
 
NP  
 
  
 
PP  
 
  
NP NP NP NP 
        VB NNS IN NN IN NN 
[ ] Find articles abut function of FancD2 
 
Figure 1: The tree structure for an imperative sentence
part-of-speech is the WP in Penn Treebank (Marcus
et al, 1994) POS tag set) or is an adverb (WRB). In
the 2006 track, the templates for the 2005 track were
reformulated into the constructions of questions and
were then utilized for deriving the questions. For ex-
ample, the templates to find articles describing the
role of a gene involved in a given disease is refor-
mulated into the question ?What is the role of gene
in disease??
GTREC07 consists only of wh-questions where a
wh-word serves as a pre-nominal modifier (WDT).
In the 2007 track, unlike in those of last two years,
questions were not categorized by the templates, but
were based on biologists? information needs where
the answers were lists of named entities of a given
type. The obtained questions begin with ?what +
entity type? (45 sentences), ?which + entity type? (4
sentences), or ?In what + entity type? (1 sentence).
In contrast, the GENIA Treebank Corpus (Tateisi
et al, 2005)5 is estimated to have no imperative sen-
tences and only seven interrogative sentences (see
Section 5.2.2). Thus, the sentence constructions in
GTREC04?07 are very different from those in the
GENIA treebank.
3 Treebanking GTREC query sentences
We built a treebank (with POS) on 196 query sen-
tences following the guidelines of the GENIA Tree-
bank (Tateisi and Tsujii, 2006). The queries were
first parsed using the Stanford Parser (Klein and
Manning, 2003), and manual correction was made
5http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/
wiki.cgi?page=GENIA+Treebank
165
 SBARQ  
 
  
 
SQ  
 
   
 VP    
WHNP[i168]   NP[i169?i168]  NP[?i169] PP               
WDT NNS VBP   VBN   IN NN 
What toxicities are [ ] associated [ ] with cytarabine 
 
Figure 2: The tree structure for an interrogative sentence
by the second author. We tried to follow the guide-
line of the GENIA Treebank as closely as possible,
but for the constructions that are rare in GENIA, we
used the ATIS corpus in Penn Treebank (Bies et al,
1995), which is also a collection of query sentences,
for reference.
Figure 1 shows the tree for an imperative sen-
tence. A leaf node with [ ] corresponds to a null
constituent. Figure 2 shows the tree for an inter-
rogative sentence. Coindexing is represented by
assigning an ID to a node and a reference to the
ID to the node which is coindexed. In Figure 2,
WHNP[i168] means that the WHNP node is indexed
as i168, NP[i169?i168] means that the NP node is
indexed as i169 and coindexed to the i168 node, and
NP[?i169] means that the node is coindexed to the
i169 node. In this sentence, which is a passive wh-
question, it is assumed that the logical object (what
toxicities) of the verb (associate) is moved to the
subject position (the place of i169) and then moved
to the sentence-initial position (the place of i168).
As most of the query sentences are either impera-
tive or interrogative, there are more null constituents
compared to the GENIA Corpus. In the GTREC
query treebank, 184 / 196 (93.9%) sentences con-
tained one or more null constituents, whereas in GE-
NIA, 12,222 / 18,541 (65.9%) sentences did. We ex-
pected there are more sentences with multiple null
constituents in GTREC compared to GENIA, due to
the frequency of passive interrogative sentences, but
on the contrary the number of sentences containing
more than one null constituents are 65 (33.1%) in
GTREC, and 6,367 (34.5%) in GENIA. This may be
due to the frequency of relative clauses in GENIA.
4 Parsing system and extraction of
imperative and question sentences
We introduce the parser and the POS tagger whose
performances are examined, and the extraction of
imperative or question sentences from GTREC tree-
bank on which the performances are measured.
4.1 HPSG parser
The Enju parser (Ninomiya et al, 2007)6 is a deep
parser based on the HPSG formalism. It produces
an analysis of a sentence that includes the syntac-
tic structure (i.e., parse tree) and the semantic struc-
ture represented as a set of predicate-argument de-
pendencies. The grammar is based on the standard
HPSG analysis of English (Pollard and Sag, 1994).
The parser finds a best parse tree scored by a max-
ent disambiguation model using a Cocke-Kasami-
Younger (CKY) style algorithm.
We used a toolkit distributed with the Enju parser
for training the parser with a Penn Treebank style
(PTB-style) treebank. The toolkit initially converts
the PTB-style treebank into an HPSG treebank and
then trains the parser on it. We used a toolkit dis-
tributed with the Enju parser for extracting a HPSG
lexicon from a PTB-style treebank. The toolkit ini-
tially converts the PTB-style treebank into an HPSG
treebank and then extracts the lexicon from it.
The HPSG treebank converted from the test sec-
tion was used as the gold-standard in the evaluation.
As the evaluation metrics of the Enju parser, we used
labeled and unlabeled precision/recall/F-score of the
predicate-argument dependencies produced by the
parser. A predicate-argument dependency is repre-
sented as a tuple of ?wp, wa, r?, where wp is the
predicate word, wa is the argument word, and r is
the label of the predicate-argument relation, such
as verb-ARG1 (semantic subject of a verb) and
prep-ARG1 (modifiee of a prepositional phrase).
4.2 POS tagger
The Enju parser assumes that the input is already
POS-tagged. We use a tagger in (Tsuruoka et al,
2005). It has been shown to give a state-of-the-art
accuracy on the standard Penn WSJ data set and also
on a different text genre (biomedical literature) when
trained on the combined data set of the WSJ data and
6http://www-tsujii.is.s.u-tokyo.ac.jp/enju
166
the target genre (Tsuruoka et al, 2005). Since our
target is biomedical domain, we utilize the tagger
adapted to the domain as a baseline, which we call
?the GENIA tagger?.
4.3 Extracting imperative and question
sentences from GTREC treebank
In GTREC sentences, two major constructions of
sentences can be observed: imperative and question
sentences. These two types of sentences have differ-
ent sentence constructions and we will observe the
impact of each or both of these constructions on the
performances of parsing or POS-tagging. In order
to do so, we collected imperative and question sen-
tences from our GTREC treebank as follows:
? GTREC imperatives - Most of the impera-
tive sentences in GTREC treebank begin with
empty subjects ?(NP-SBJ */-NONE-)?. We ex-
tracted such 82 imperative sentences.
? GTREC questions - Interrogative sentences
are annotated with the phrase label ?SBARQ?
or ?SQ?, where ?SBARQ? and ?SQ? respec-
tively denote a wh-question and an yes/no ques-
tion. We extracted 98 interrogative sentences
whose top phrase labels were either of them.
5 Experiments
We examine the POS-tagger and the parser for the
sentences in the GTREC corpus. They are adapted
to each of GTREC overall, imperatives, and ques-
tions. We then observe how the parsing or POS-
tagging accuracies are improved and analyze what
is critical for parsing query sentences.
5.1 Experimental settings
5.1.1 Dividing corpora
We prepared experimental datasets for the follow-
ing four domains:
? GENIA Corpus (GENIA) (18,541 sentences)
Divided into three parts for training (14,849
sentences), development test (1,850 sentences),
and final test (1,842 sentences).
? GTREC overall (196 sentences)
Divided into two parts: one for ten-folds cross
validation test (17-18 ? 10 sentences) and the
other for error analysis (17 sentences)
Target GENIA tagger Adapted tagger
GENIA 99.04% -
GTREC (overall) 89.98% 96.54%
GTREC (imperatives) 90.32% 97.30%
GRREC (questions) 89.25% 94.77%
Table 2: Accuracy of the POS tagger for each domain
? GTREC imperatives (82 sentences)
Divided into two parts: one for ten-folds cross
validation test (7-8 ? 10 sentences) and the
other for error analysis (7 sentences)
? GTREC questions (98 sentences)
Divided into two parts: one for ten-folds cross
validation test (9 ? 10 sentences) and the other
for error analysis (8 sentences)
5.1.2 Adaptation of POS tagger and parser
In order to adapt the POS tagger and the parser to
a target domain, we took the following methods.
? POS tagger - For the GTREC overall / impera-
tives / questions, we replicated the training data
for 100,000 times and utilized the concatenated
replicas and GENIA training data in (Tsuruoka
et al, 2005) for training. For POS tagger, the
number of replicas of training data was deter-
mined among 10n(n = 0, . . . , 5) by testing
these numbers on development test sets in three
of ten datasets of cross validation.
? Enju parser - We used a toolkit in the Enju
parser (Hara et al, 2007). As a baseline model,
we utilized the model adapted to the GENIA
Corpus. We then attempted to further adapt the
model to each domain. In this paper, the base-
line model is called ?the GENIA parser?.
5.2 POS tagger and parser performances
Table 2 and 3 respectively show the POS tagging and
the parsing accuracies for the target domains, and
Figure 3 and 4 respectively show the POS tagging
and the parsing accuracies for the target domains
given by changing the size of the target training data.
The POS tagger could output for each word either
of one-best POS or POS candidates with probabili-
ties, and the Enju parser could take either of the two
output types. The bracketed numbers in Table 3 and
167
Parser GENIA Adapted
POS Gold GENIA tagger Adapted tagger Gold GENIA tagger Adapted tagger
For GENIA 88.54 88.07 (88.00) - - - -
For GTREC overall 84.37 76.81 (72.43) 83.46 (81.96) 89.00 76.98 (74.44) 86.98 (85.42)
For GTREC imperatives 85.19 78.54 (77.75) 85.71 (85.48) 89.42 74.40 (74.84) 88.97 (88.67)
For GTREC questions 85.45 76.25 (67.27) 83.55 (80.46) 87.33 81.41 (71.90) 84.87 (82.70)
[ using POS candidates with probabilities (using only one best POS) ]
Table 3: Accuracy of the Enju parser for GTREC
70
75
80
85
90
0 20 40 60 80 100 120 140
F
-
s
c
o
r
e
Corpus size (sentences)
70
75
80
85
90
0 20 40 60
F
-
s
c
o
r
e
Corpus size (sentences)
65
70
75
80
85
90
95
0 20 40 60 80
F
-
s
c
o
r
e
Corpus size (sentences)
Adapted parser, gold POS
Adapted parser, adapted tagger (prob.)
GENIA parser, adapted tagger (prob.)
Adapted parser, GENIA tagger (prob.)
Adapted parser, adapted tagger (1best)
GENIA parser, adapted tagger (1best)
Adapted parser, GENIA tagger (1best)
For GTREC imperatives For GTREC questionsFor GTREC overall
Figure 4: Parsing accuracy vs. corpus size
88
90
92
94
96
98
0 50 100 150
A
c
c
u
r
a
c
y
 (
%
)
Corpus size (sentences)
GTREC overall
GTREC imperatives
GTREC questions
Figure 3: POS tagging accuracy vs. corpus size
the dashed lines in Figure 4 show the parsing accu-
racies when we utilized one-best POS given by the
POS tagger, and the other numbers and lines show
the accuracies given by POS candidates with proba-
bilities. In the rest of this section, when we just say
?POS tagger?, the tagger?s output is POS candidates
with probabilities.
Table 4 and 5 respectively compare the types of
POS tagging and parsing errors for each domain
between before and after adapting the POS tagger,
and Table 6 compares the types of parsing errors for
Correct ? Error GENIA tagger Adapted tagger
For GTREC overall (17 sentences)
NN ? NNP 4 0.6
VB ? NN 4 0
WDT ? WP 4 0
NN ? JJ 1 1.9
For GTREC imperative (seven sentences)
FW ? NNP / NN / JJ 7 4
VB ? NN 4 0
NN ? NNP 2 0
For GTREC question (eight sentences)
WDT ? WP 3 0
VB ? VBP 2 1
NNS ? VBZ 2 0
(The table shows only error types observed more than
once for either of the taggers)
Table 4: Tagging errors for each of the GTREC corpora
each domain between before and after adapting the
parser. The numbers of errors for the rightmost col-
umn in each of the tables were given by the average
of the ten-folds cross validation results.
In the following sections, we examine the im-
pact of the performances of the POS taggers or the
parsers on parsing the GTREC documents.
168
GENIA parserError types GENIA tagger Adapted tagger
For GTREC overall (17 sentences)
Failure in detecting verb 12 0.2
Root selection 6 0
Range of NP 5 5
PP-attachment 4 3
Determiner / pronoun 4 1
Range of verb subject 4 4
Range of verb object 3 3
Adjective / modifier noun 2 3
For GTREC imperatives (seven sentences)
Failure in detecting verb 8 0
Root selection 4 0
Range of NP 3 4
PP-attachment 3 1.8
Range of PP 2 2
For GTREC questions (eight sentences)
Range of coordination 5 3
Determiner / pronoun 3 0
PP-attachment 3 1
Range of PP 2 2
Subject for verb 2 1
(The table shows only the types of parsing errors observed more
than once for either of the parsers)
Table 5: Impact of adapting POS tagger on parsing errors
5.2.1 Impact of POS tagger on parsing
In Table 2, for each of the GTREC corpora,
the GENIA tagger dropped its tagging accuracy by
around nine points, and then recovered five to seven
points by the adaptation. According to this behav-
ior of the tagger, Table 3 shows that the GENIA and
the adapted parsers with the GENIA tagger dropped
their parsing accuracies by 6?15 points in F-score
from the accuracies with the gold POS, and then re-
covered the accuracies within two points below the
accuracies with the gold POS. The performance of
the POS tagger would thus critically affect the pars-
ing accuracies.
In Figure 3, we can observe that the POS tagging
accuracy for each corpus rapidly increased only for
first 20?30 sentences, and after that the improvement
speed drastically declined. Accordingly, in Figure 4,
the line for the adapted parser with the adapted tag-
ger (the line with triangle plots) rose rapidly for the
first 20?30 sentences, and after that slowed down.
We explored the tagging and parsing errors, and
analyze the cause of the initial accuracy jump and
the successive improvement depression.
Gold POSError types GENIA parser Adapted parser
For GTREC overall (17 sentences)
Range of NP 5 1.3
Range of verb subject 3 2.6
PP-attachment 3 2.7
Whether verb takes
object & complement 3 2.9
Range of verb object 2 1
For GTREC imperatives (seven sentences)
Range of NP 4 1.1
PP-attachment 2 1.6
Range of PP 2 0.3
Preposition / modifier 2 2
For GTREC questions (eight sentences)
Coordination / conjunction 2 2.2
Auxiliary / normal verb 2 2.6
Failure in detecting verb 2 2.6
(The table shows only the types of parsing errors observed more
than once for either of the parsers)
Table 6: Impact of adapting parser on parsing errors
Cause of initial accuracy jump
In Table 4, ?VB ? NN? tagging errors were
observed only in imperative sentences and drasti-
cally decreased by the adaptation. In a impera-
tive sentence, a verb (VB) usually appears as the
first word. On the other hand, the GENIA tagger
was trained mainly on the declarative sentences and
therefore would often take the first word in a sen-
tence as the subject of the sentence, that is, noun
(NN). When the parser received a wrong NN-tag for
a verb, the parser would attempt to believe the infor-
mation (?failure in detecting verb? in Table 6) and
could then hardly choose the NN-tagged word as a
main verb (?root selection? in Table 6). By adapting
the tagger, the correct tag was given to the verb and
the parser could choose the verb as a main verb.
?WDT ? WP? tagging errors were observed only
in the question sentences and also drastically de-
creased. For example, in the sentence ?What toxici-
ties are associated with cytarabine??, ?What? works
as a determiner (WDT) which takes ?toxicities?,
while the GENIA tagger often took this ?What? as a
pronoun (WP) making a phrase by itself. This would
be because the training data for the GENIA tagger
would contain 682 WP ?what? and only 27 WDT
?what?. WP ?what? could not make a noun phrase
by taking a next noun, and then the parsing of the
parsing would corrupt (?determiner / pronoun? in
Table 5). By adapting the tagger, ?WDT? tag was
169
given to ?What?, and the parser correctly made a
phrase ?What toxicities?.
Since the variation of main verbs in GTREC im-
peratives is very small (see Section 2) and that of
interrogatives is also very small, in order to cor-
rect the above two types of errors, we would require
only small training data. In addition, these types of
errors widely occurred among imperatives or ques-
tions, the accuracy improvement by correcting the
errors was very large. The initial rapid improvement
would thus occur.
Cause of improvement depression
?NN ? NNP? tagging errors would come from
the description style of words. In the GTREC
queries, technical terms, such as the names of dis-
eases or proteins, sometimes begin with capital char-
acters. The GENIA tagger would take the capi-
talized words not as a normal noun (NN) but as a
proper noun (NNP). By adaptation, the tagger would
have learned the capital usage for terms and the er-
rors then decreased.
However, in order to achieve such improvement,
we would have to wait until a target capitalized term
is added to the training corpus. ?FW ? NNP / NN
/ JJ?, ?NN ? JJ?, and several other errors would be
similar to this type of errors in the point that, they
would be caused by the difference in annotation pol-
icy or description style between the training data for
the GENIA tagger and the GTREC queries.
?VB ? VBP? errors were found in questions. For
example, ?affect? in the question ?How do muta-
tions in Sonic Hedgehog genes affect developmen-
tal disorders?? was base form (VB), while the GE-
NIA tagger took it as a present tense (VBP) since
the GENIA tagger would be unfamiliar with such
verb behavior in questions. By adaptation, the tag-
ger would learn that verbs in the domain tend to take
base forms and the errors then decreased.
However, the tagger model based on local context
features could not substantially solve the problem.
VBP of course could appear in question sentences.
We observed that a verb to be VBP was tagged with
VB by the adapted tagger. In order to distinguish
VB from VBP, we should capture longer distance
dependencies between auxiliary and main verbs.
In tagging, the fact that the above two types of
errors occupied most of the errors other than the er-
rors involved in the initial jump, would be related
to why the accuracy improvement got so slowly,
which would lead to the improvement depression of
the parsing performances. With the POS candidates
with probabilities, the possibilities of correct POSs
would increase, and therefore the parser would give
higher parsing performances than using only one-
best POSs (see Table 3 and Figure 4).
Anyway, the problems were not substantially
solved. For these tagging problems, just adding the
training data would not work. We might need re-
construct the tagging system or re-consider the fea-
ture designs of the model.
5.2.2 Impact of parser itself on parsing
For the GTREC corpora, the GENIA parser with
gold POSs lowered the parsing accuracy by more
than three points than for the GENIA Corpus, while
the adaptation of the parser recovered a few points
for each domain (second and fifth column in Table
3). Figure 4 would also show that we could improve
the parser?s performance with more training data for
each domain. For GTREC questions, the parsing ac-
curacy dropped given the maximum size of the train-
ing data. Our training data is small and therefore
small irregular might easily make accuracies drop or
rise. 7 We might have to prepare more corpora for
confirming our observation.
Table 6 would imply that the major errors for all
of these three corpora seem not straightforwardly as-
sociated with the properties specific to imperative or
question sentences. Actually, when we explored the
parse results, errors on the sentence constructions
specific to the two types of sentences would hardly
be observed. (?Failure in detecting verb? errors in
GTREC questions came from other causes.) This
would mean that the GENIA parser itself has poten-
tial to parse the imperative or question sentences.
The training data of the GENIA parser consists
of the WSJ Penn Treebank and the GENIA Corpus.
As long as we searched with our extraction method
in Section 4.3, the WSJ and GENIA Corpus seem
respectively contain 115 and 0 imperative, and 432
7This time we could not analyze which training data affected
the decrease, because through the cross validation experiments
each sentence was forced to be once final test data. However,
we would like to find the reason for this accuracy decrease in
some way.
170
and seven question sentences. Unlike the POS tag-
ger, the parser could convey more global sentence
constructions from these sentences.
Although the GENIA parser might understand the
basic constructions of imperative or question sen-
tences, by adaptation of the parser to the GTREC
corpora, we could further learn more local construc-
tion features specific to GTREC, such as word se-
quence constructing a noun phrase, attachment pref-
erence of prepositions or other modifiers. The error
reduction in Table 6 would thus be observed.
However, we also observed that several types of
errors were still mostly unsolved after the adapta-
tion. Choosing whether to add complements for
verbs or not, and distinguishing coordinations from
conjunctions seems to be difficult for the parser. If
two question sentences were concatenated by con-
junctions into one sentence, the parser would tend to
fail to analyze the sentence construction for the lat-
ter sentence. The remaining errors in Table 6 would
imply that we should also re-consider the model de-
signs or the framework itself for the parser in addi-
tion to just increasing the training data.
6 Related work
Since domain adaptation has been an extensive re-
search area in parsing research (Nivre et al, 2007),
a lot of ideas have been proposed, including un-
/semi-supervised approaches (Roark and Bacchiani,
2003; Blitzer et al, 2006; Steedman et al, 2003;
McClosky et al, 2006; Clegg and Shepherd, 2005;
McClosky et al, 2010) and supervised approaches
(Titov and Henderson, 2006; Hara et al, 2007).
Their main focus was on adapting parsing models
trained with a specific genre of text (in most cases
PTB-WSJ) to other genres of text, such as biomed-
ical research papers. A major problem tackled in
such a task setting is the handling of unknown words
and domain-specific ways of expressions. However,
as we explored, parsing NL queries involves a sig-
nificantly different problem; even when all words in
a sentence are known, the sentence has a very differ-
ent construction from declarative sentences.
Although sentence constructions have gained lit-
tle attention, a notable exception is (Judge et al,
2006). They pointed out low accuracy of state-of-
the-art parsers on questions, and proposed super-
vised parser adaptation by manually creating a tree-
bank of questions. The question sentences are anno-
tated with phrase structure trees in the PTB scheme,
although function tags and empty categories are
omitted. An LFG parser trained on the treebank then
achieved a significant improvement in parsing ac-
curacy. (Rimell and Clark, 2008) also worked on
question parsing. They collected question sentences
from TREC 9-12, and annotated the sentences with
POSs and CCG (Steedman, 2000) lexical categories.
They reported a significant improvement in CCG
parsing without phrase structure annotations.
On the other hand, (Judge et al, 2006) also im-
plied that just increasing the training data would not
be enough. We went further from their work, built
a small but complete treebank for NL queries, and
explored what really occurred in HPSG parsing.
7 Conclusion
In this paper, we explored the problem in parsing
queries. We first attempted to build a treebank on
queries for biological knowledge and successfully
obtained 196 annotated GTREC queries. We next
examined the performances of the POS tagger and
the HPSG parser on the treebank. In the experi-
ments, we focused on the two dominant sentence
constructions in our corpus: imperatives and ques-
tions, extracted them from our corpus, and then also
examined the parser and tagger for them.
The experimental results showed that the POS
tagger?s mis-tagging to main verbs in imperatives
and wh-interrogatives in questions critically de-
creased the parsing performances, and that our
small corpus could drastically decrease such mis-
tagging and consequently improve the parsing per-
formances. The experimental results also showed
that the parser itself could improve its own perfor-
mance by increasing the training data. On the other
hand, the experimental results suggested that the
POS tagger or the parser performance would stag-
nate just by increasing the training data.
In our future research, on the basis of our findings,
we would like both to build more training data for
queries and to reconstruct the model or reconsider
the feature design for the POS tagger and the parser.
We would then incorporate the optimized parser and
tagger into NL query processing applications.
171
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
style ? Penn Treebank project. Technical report, De-
partment of Linguistics, University of Pennsylvania.
John Blitzer, Ryan Mcdonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia.
A. B. Clegg and A. Shepherd. 2005. Evaluating and in-
tegrating treebank parsers on a biomedical corpus. In
Proceedings of the ACL 2005 Workshop on Software,
Ann Arbor, Michigan.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of 10th International Confer-
ence on Parsing Technologies (IWPT 2007), pages 11?
22.
William R. Hersh, Ravi Teja Bhupatiraju, L. Ross,
Aaron M. Cohen, Dale Kraemer, and Phoebe Johnson.
2004. TREC 2004 Genomics Track Overview. In Pro-
ceedings of the Thirteenth Text REtrieval Conference,
TREC 2004.
William R. Hersh, Aaron M. Cohen, Jianji Yang,
Ravi Teja Bhupatiraju, Phoebe M. Roberts, and
Marti A. Hearst. 2005. TREC 2005 Genomics Track
Overview. In Proceedings of the Fourteenth Text RE-
trieval Conference, TREC 2005.
William R. Hersh, Aaron M. Cohen, Phoebe M. Roberts,
and Hari Krishna Rekapalli. 2006. TREC 2006 Ge-
nomics Track Overview. In Proceedings of the Fif-
teenth Text REtrieval Conference, TREC 2006.
William R. Hersh, Aaron M. Cohen, Lynn Ruslen, and
Phoebe M. Roberts. 2007. TREC 2007 Genomics
Track Overview. In Proceedings of The Sixteenth Text
REtrieval Conference, TREC 2007.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a Corpus of Parsing-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 497?504.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of ARPA Human Language Technology
Workshop.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 337?344, Sydney, Australia.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic Domain Adaptation for Parsing. In
Proceedings of the 2010 Annual Conference of the
North American Chapter of the ACL, pages 28?36, Los
Angeles, California.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,
and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate hpsg pars-
ing. In Proceedings of 10th International Conference
on Parsing Technologies (IWPT 2007), pages 60?68.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Laura Rimell and Stephen Clark. 2008. Adapting a
Lexicalized-Grammar Parser to Contrasting Domains.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 475?
584.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptation to novel domains.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 126?133, Edmonton, Canada.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics,
pages 331?338, Budapest, Hungary.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
Yuka Tateisi and Jun?ichi Tsujii. 2006. GENIA Anno-
tation Guidelines for Treebanking. Technical Report
TR-NLP-UT-2006-5, Tsujii Laboratory, University of
Tokyo.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the Second Interna-
tional Joint Conference on Natural Language Process-
172
ing (IJCNLP 2005), Companion volume, pages 222?
227.
Ivan Titov and James Henderson. 2006. Porting statis-
tical parsers with data-defined kernels. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 6?13, New York City.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics - 10th Panhellenic Conference on Infor-
matics, volume LNCS 3746, pages 382?392, Volos,
Greece, November. ISSN 0302-9743.
173
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 238?246,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Learning with Lookahead:
Can History-Based Models Rival Globally Optimized Models?
Yoshimasa Tsuruoka?? Yusuke Miyao?? Jun?ichi Kazama?
? Japan Advanced Institute of Science and Technology (JAIST), Japan
? National Institute of Informatics (NII), Japan
? National Institute of Information and Communications Technology (NICT), Japan
tsuruoka@jaist.ac.jp yusuke@nii.ac.jp kazama@nict.go.jp
Abstract
This paper shows that the performance of
history-based models can be significantly im-
proved by performing lookahead in the state
space when making each classification deci-
sion. Instead of simply using the best ac-
tion output by the classifier, we determine
the best action by looking into possible se-
quences of future actions and evaluating the
final states realized by those action sequences.
We present a perceptron-based parameter op-
timization method for this learning frame-
work and show its convergence properties.
The proposed framework is evaluated on part-
of-speech tagging, chunking, named entity
recognition and dependency parsing, using
standard data sets and features. Experimental
results demonstrate that history-based models
with lookahead are as competitive as globally
optimized models including conditional ran-
dom fields (CRFs) and structured perceptrons.
1 Introduction
History-based models have been a popular ap-
proach in a variety of natural language process-
ing (NLP) tasks including part-of-speech (POS) tag-
ging, named entity recognition, and syntactic pars-
ing (Ratnaparkhi, 1996; McCallum et al, 2000; Ya-
mada and Matsumoto, 2003; Nivre et al, 2004).
The idea is to decompose the complex structured
prediction problem into a series of simple classifi-
cation problems and use a machine learning-based
classifier to make each decision using the informa-
tion about the past decisions and partially completed
structures as features.
Although history-based models have many prac-
tical merits, their accuracy is often surpassed by
globally optimized models such as CRFs (Lafferty
et al, 2001) and structured perceptrons (Collins,
2002), mainly due to the label bias problem. To-
day, vanilla history-based models such as maximum
entropy Markov models (MEMMs) are probably not
the first choice for those who are looking for a ma-
chine learning model that can deliver the state-of-
the-art accuracy for their NLP task. Globally opti-
mized models, by contrast, are gaining popularity in
the community despite their relatively high compu-
tational cost.
In this paper, we argue that history-based mod-
els are not something that should be left behind
in research history, by demonstrating that their ac-
curacy can be significantly improved by incorpo-
rating a lookahead mechanism into their decision-
making process. It should be emphasized that we
use the word ?lookahead? differently from some lit-
erature on syntactic parsing in which lookahead sim-
ply means looking at the succeeding words to choose
the right parsing actions. In this paper, we use the
word to refer to the process of choosing the best ac-
tion by considering different sequences of future ac-
tions and evaluating the structures realized by those
sequences. In other words, we introduce a looka-
head mechanism that performs a search in the space
of future actions.
We present a perceptron-based training algorithm
that can work with the lookahead process, together
with a proof of convergence. The algorithm enables
us to tune the weight of the perceptron in such a way
that we can correctly choose the right action for the
238
State Operation Stack Queue
0 I saw a dog with eyebrows
1 shift I saw a dog with eyebrows
2 shift I saw a dog with eyebrows
3 reduceL saw(I) a dog with eyebrows
. . .
4 saw(I) dog(a) with eyebrows
5 shift saw(I) dog(a) with eyebrows
6 shift saw(I) dog(a) with eyebrows
7 reduceR saw(I) dog(a) with(eyebrows)
8 reduceR saw(I) dog(a, with(eyebrows))
5? reduceR saw(I, dog(a)) with eyebrows
6? shift saw(I, dog(a)) with eyebrows
7? shift saw(I, dog(a)) with eyebrows
8? reduce R saw(I, dog(a)) with(eyebrows)
9? reduce R saw(I, dog(a), with(eyebrows))
Figure 1: Shift-reduce dependency parsing
current state at each decision point, given the infor-
mation obtained from a search.
To answer the question of whether the history-
based models enhanced with lookahead can actually
compete with globally optimized models, we eval-
uate the proposed framework with a range of stan-
dard NLP tasks, namely, POS tagging, text chunking
(a.k.a. shallow parsing), named entity recognition,
and dependency parsing.
This paper is organized as follows. Section 2
presents the idea of lookahead with a motivating
example from dependency parsing. Section 3 de-
scribes our search algorithm for lookahead and a
perceptron-based training algorithm. Experimen-
tal results on POS tagging, chunking, named entity
recognition, and dependency parsing are presented
in Section 4. We discuss relationships between our
approach and some related work in Section 5. Sec-
tion 6 offers concluding remarks with some potential
research directions.
2 Motivation
This section describes an example of dependency
parsing that motivates the introduction of lookahead
in history-based models.
A well-known history-based approach to depen-
dency parsing is shift-reduce parsing. This al-
gorithm maintains two data structures, stack and
queue: A stack stores intermediate parsing results,
and a queue stores words to read. Two operations
(actions), shift and reduce, on these data structures
construct dependency relations one by one.
For example, assume that we are given the follow-
ing sentence.
I saw a dog with eyebrows.
In the beginning, we have an empty stack, and a
queue filled with a list of input words (State 0 in Fig-
ure 1). The shift operation moves the left-most ele-
ment of the queue to the stack. In this example, State
1 is obtained by applying shift to State 0. After the
two shift operations, we reach State 2, in which the
stack has two elements. When we have two or more
elements in the stack, we can apply the other opera-
tion, reduce, which merges the two stack elements
by creating a dependency relation between them.
When we apply reduceL, which means to have the
left element as a dependent of the right element, we
reach State 3: The word ?I? has disappeared from
the stack and instead it is attached to its head word
?saw?.1 In this way, the shift-reduce parsing con-
structs a dependency tree by reading words from the
queue and constructing dependency relations on the
stack.
1In Figure 1, H(D1, D2, . . .) indicates that D1, D2, . . . are
the dependents of the head H .
239
Let?s say we have now arrived at State 4 after sev-
eral operations. At this state, we cannot simply de-
termine whether we should shift or reduce. In such
cases, conventional methods rely on a multi-class
classifier to determine the next operation. That is,
a classifier is used to select the most plausible oper-
ation, by referring to the features about the current
state, such as surface forms and POSs of words in
the stack and the queue.
In the lookahead strategy, we make this decision
by referring to future states. For example, if we ap-
ply shift to State 4, we will reach State 8 in the end,
which indicates that ?with? attaches to ?dog?. The
other way, i.e., applying reduceR to State 4, eventu-
ally arrives at State 9?, indicating ?with? attaches to
?saw?. These future states indicate that we were im-
plicitly resolving PP-attachment ambiguity at State
4. While conventional methods attempt to resolve
such ambiguity using surrounding features at State
4, the lookahead approach resolves the same ambi-
guity by referring to the future states, for example,
State 8 and 9?. Because future states can provide ad-
ditional and valuable information for ambiguity res-
olution, improved accuracy is expected.
It should be noted that Figure 1 only shows one
sequence of operations for each choice of operation
at State 4. In general, however, the number of poten-
tial sequences grows exponentially with the looka-
head depth, so the lookahead approach requires us to
pay the price as the increase of computational cost.
The primary goal of this paper is to demonstrate that
the cost is actually worth it.
3 Learning with Lookahead
This section presents our framework for incorporat-
ing lookahead in history-based models. In this pa-
per, we focus on deterministic history-based models
although our method could be generalized to non-
deterministic cases.
We use the word ?state? to refer to a partially
completed analysis as well as the collection of his-
torical information available at each decision point
in deterministic history-based analysis. State transi-
tions are made by ?actions? that are defined at each
state. In the example of dependency parsing pre-
sented in Section 2, a state contains all the infor-
mation about past operations, stacks, and queues as
1: Input
2: d: remaining depth of search
3: S0: current state
4: Output
5: S: state of highest score
6: v: highest score
7:
8: function SEARCH(d, S0)
9: if d = 0 then
10: return (S0,w ? ?(S0))
11: (S, v)? (null,??)
12: for each a ? POSSIBLEACTIONS(S0)
13: S1 ? UPDATESTATE(S0, a)
14: (S?, v?)? SEARCH(d? 1, S1)
15: if v? > v then
16: (S, v)? (S?, v?)
17: return (S, v)
Figure 2: Search algorithm.
well as the observation (i.e. the words in the sen-
tence). The possible actions are shift, reduceR, and
reduceL. In the case of POS tagging, for example, a
state is the words and the POS tags assigned to the
words on the left side of the current target word (if
the tagging is conducted in the left-to-right manner),
and the possible actions are simply defined by the
POS tags in the annotation tag set.
3.1 Search
With lookahead, we choose the best action at each
decision point by considering possible sequences of
future actions and the states realized by those se-
quences. In other words, we need to perform a
search for each possible action.
Figure 2 describes our search algorithm in pseudo
code. The algorithm performs a depth-first search to
find the state of the highest score among the states in
its search space, which is determined by the search
depth d. This search process is implemented with
a recursive function, which receives the remaining
search depth and the current state as its input and
returns the state of the highest score together with
its score.
We assume a linear scoring model, i.e., the score
of each state S can be computed by taking the dot
product of the current weight vector w and ?(S),
the feature vector representation of the state. The
240
1: Input
2: C: perceptron margin
3: D: depth of lookahead search
4: S0: current state
5: ac: correct action
6:
7: procedure UPDATEWEIGHT(C,D, S0, ac)
8: (a?, S?, v)? (null, null,??)
9: for each a ? POSSIBLEACTIONS(S0)
10: S1 ? UPDATESTATE(S0, a)
11: (S?, v?)?SEARCH(D,S1)
12: if a = ac then
13: v? ? v? ? C
14: S?c ? S?
15: if v? > v then
16: (a?, S?, v)? (a, S?, v?)
17: if a? 6= ac then
18: w ? w + ?(S?c )? ?(S?)
Figure 3: Perceptron weight update
scores are computed at each leaf node of the search
tree and backed up to the root.2
Clearly, the time complexity of determinis-
tic tagging/parsing with this search algorithm is
O(nmD+1), where n is the number of actions
needed to process the sentence, m is the (average)
number of possible actions at each state, and D is
the search depth. It should be noted that the time
complexity of k-th order CRFs is O(nmk+1), so
a history-based model with k-depth lookahead is
comparable to k-th order CRFs in terms of train-
ing/testing time.
Unlike CRFs, our framework does not require the
locality of features since it is history-based, i.e., the
decisions can be conditioned on arbitrary features.
One interpretation of our learning framework is that
it trades off the global optimality of the learned pa-
rameters against the flexibility of features.
3.2 Training a margin perceptron
We adapt a learning algorithm for margin percep-
trons (Krauth and Mezard, 1987) to our purpose of
2In actual implementation, it is not efficient to compute the
score of a state from scratch at each leaf node. For most of
the standard features used in tagging and parsing, it is usually
straight-forward to compute the scores incrementally every time
the state is updated with an action.
optimizing the weight parameters for the lookahead
search. Like other large margin approaches such
as support vector machines, margin perceptrons are
known to produce accurate models compared to per-
ceptrons without a margin (Li et al, 2002).
Figure 3 shows our learning algorithm in pseudo
code. The algorithm is very similar to the standard
training algorithm for margin perceptrons, i.e., we
update the weight parameters with the difference of
two feature vectors (one corresponding to the cor-
rect action, and the other the action of the highest
score) when the perceptron makes a mistake. The
feature vector for the second best action is also used
when the margin is not large enough. Notice that the
feature vector for the second best action is automat-
ically selected by using a simple trick of subtracting
the margin parameter from the score for the correct
action (Line 13 in Figure 3).
The only difference between our algorithm and
the standard algorithm for margin perceptrons is that
we use the states and their scores obtained from
lookahead searches (Line 11 in Figure 3), which are
backed up from the leaves of the search trees. In Ap-
pendix A, we provide a proof of the convergence of
our training algorithm and show that the margin will
approach at least half the true margin (assuming that
the training data are linearly separable).
As in many studies using perceptrons, we average
the weight vector over the whole training iterations
at the end of the training (Collins, 2002).
4 Experiments
This section presents four sets of experimental re-
sults to show how the lookahead process improves
the accuracy of history-based models in common
NLP tasks.
4.1 Sequence prediction tasks
First, we evaluate our framework with three se-
quence prediction tasks: POS tagging, chunking,
and named entity recognition. We compare our
method with the CRF model, which is one of the de
facto standard machine learning models for such se-
quence prediction tasks. We trained L1-regularized
first-order CRF models using the efficient stochastic
gradient descent (SGD)-based training method pre-
sented in Tsuruoka et al (2009). Since our main in-
241
terest is not in achieving the state-of-the-art results
for those tasks, we did not conduct feature engineer-
ing to come up with elaborate features?we sim-
ply adopted the feature sets described in their paper
(with an exception being tag trigram features tested
in the POS tagging experiments). The experiments
for these sequence prediction tasks were carried out
using one core of a 3.33GHz Intel Xeon W5590 pro-
cessor.
The first set of experiments is about POS tagging.
The training and test data were created from theWall
Street Journal corpus of the Penn Treebank (Marcus
et al, 1994). Sections 0-18 were used as the training
data. Sections 19-21 were used for tuning the meta
parameters for learning (the number of iterations and
the margin C). Sections 22-24 were used for the
final accuracy reports.
The experimental results are shown in Table 1.
Note that the models in the top four rows use exactly
the same feature set. It is clearly seen that the looka-
head improves tagging accuracy, and our history-
based models with lookahead is as accurate as the
CRF model. We also created another set of models
by simply adding tag trigram features, which can-
not be employed by first-order CRF models. These
features have slightly improved the tagging accu-
racy, and the final accuracy achieved by a search
depth of 3 was comparable to some of the best re-
sults achieved by pure supervised learning in this
task (Shen et al, 2007; Lavergne et al, 2010).
The second set of experiments is about chunking.
We used the data set for the CoNLL 2000 shared
task, which contains 8,936 sentences where each to-
ken is annotated with the ?IOB? tags representing
text chunks. The experimental results are shown
in Table 2. Again, our history-based models with
lookahead were slightly more accurate than the CRF
model using exactly the same set of features. The
accuracy achieved by the lookahead model with a
search depth of 2 was comparable to the accuracy
achieved by a computationally heavy combination
of max-margin classifiers (Kudo and Matsumoto,
2001). We also tested the effectiveness of additional
features of tag trigrams using the development data,
but there was no improvement in the accuracy.
The third set of experiments is about named en-
tity recognition. We used the data provided for
the BioNLP/NLPBA 2004 shared task (Kim et al,
2004), which contains 18,546 sentences where each
token is annotated with the ?IOB? tags representing
biomedical named entities. We performed the tag-
ging in the right-to-left fashion because it is known
that backward tagging is more accurate than forward
tagging on this data set (Yoshida and Tsujii, 2007).
Table 3 shows the experimental results, together
with some previous performance reports achieved
by pure machine leaning methods (i.e. without rule-
based post processing or external resources such as
gazetteers). Our history-based model with no looka-
head was considerably worse than the CRF model
using the same set of features, but it was signifi-
cantly improved by the introduction of lookahead
and resulted in accuracy figures better than that of
the CRF model.
4.2 Dependency parsing
We also evaluate our method in dependency parsing.
We follow the most standard experimental setting
for English dependency parsing: The Wall Street
Journal portion of Penn Treebank is converted to de-
pendency trees by using the head rules of Yamada
and Matsumoto (2003).3 The data is split into train-
ing (section 02-21), development (section 22), and
test (section 23) sets. The parsing accuracy was eval-
uated with auto-POS data, i.e., we used our looka-
head POS tagger (depth = 2) presented in the previ-
ous subsection to assign the POS tags for the devel-
opment and test data. Unlabeled attachment scores
for all words excluding punctuations are reported.
The development set is used for tuning the meta pa-
rameters, while the test set is used for evaluating the
final accuracy.
The parsing algorithm is the ?arc-standard?
method (Nivre, 2004), which is briefly described in
Section 2. With this algorithm, state S corresponds
to a parser configuration, i.e., the stack and the
queue, and action a corresponds to shift, reduceL,
and reduceR. In this experiment, we use the same
set of feature templates as Huang and Sagae (2010).
Table 4 shows training time, test time, and parsing
accuracy. In this table, ?No lookahead (depth = 0)?
corresponds to a conventional shift-reduce parsing
method without any lookahead search. The results
3Penn2Malt is applied for this conversion, while depen-
dency labels are removed.
242
Training Time (sec) Test Time (sec) Accuracy
CRF (L1 regularization & SGD training) 847 3 97.11 %
No lookahead (depth = 0) 85 5 97.00 %
Lookahead (depth = 1) 294 9 97.19 %
Lookahead (depth = 2) 8,688 173 97.19 %
No lookahead (depth = 0) + tag trigram features 88 5 97.11 %
Lookahead (depth = 1) + tag trigram features 313 10 97.22 %
Lookahead (depth = 2) + tag trigram features 10,034 209 97.28 %
Structured perceptron (Collins, 2002) n/a n/a 97.11 %
Guided learning (Shen et al, 2007) n/a n/a 97.33 %
CRF with 4 billion features (Lavergne et al, 2010) n/a n/a 97.22 %
Table 1: Performance of English POS tagging (training times and accuracy scores on test data)
Training time (sec) Test time (sec) F-measure
CRF (L1 regularization & SGD training) 74 1 93.66
No lookahead (depth = 0) 22 1 93.53
Lookahead (depth = 1) 73 1 93.77
Lookahead (depth = 2) 1,113 9 93.81
Voting of 8 SVMs (Kudo and Matsumoto, 2001) n/a n/a 93.91
Table 2: Performance of text chunking (training times and accuracy scores on test data).
clearly demonstrate that the lookahead search boosts
parsing accuracy. As expected, training and test
speed decreases, almost by a factor of three, which
is the branching factor of the dependency parser.
The table also lists accuracy figures reported in
the literature on shift-reduce dependency parsing.
Most of the latest studies on shift-reduce depen-
dency parsing employ dynamic programing or beam
search, which implies that deterministic methods
were not as competitive as those methods. It should
also be noted that all of the listed studies learn struc-
tured perceptrons (Collins and Roark, 2004), while
our parser learns locally optimized perceptrons. In
this table, our parser without lookahead search (i.e.
depth = 0) resulted in significantly lower accuracy
than the previous studies. In fact, it is worse than the
deterministic parser of Huang et al (2009), which
uses (almost) the same set of features. This is pre-
sumably due to the difference between locally opti-
mized perceptrons and globally optimized structured
perceptrons. However, our parser with lookahead
search is significantly better than their determinis-
tic parser, and its accuracy is close to the levels of
the parsers with beam search.
5 Discussion
The reason why we introduced a lookahead mech-
anism into history-based models is that we wanted
the model to be able to avoid making such mistakes
that can be detected only in later stages. Probabilis-
tic history-based models such as MEMMs should be
able to avoid (at least some of) such mistakes by per-
forming a Viterbi search to find the highest proba-
bility path of the actions. However, as pointed out
by Lafferty et al (2001), the per-state normaliza-
tion of probabilities makes it difficult to give enough
penalty to such incorrect sequences of actions, and
that is primarily why MEMMs are outperformed by
CRFs.
Perhaps the most relevant to our work in terms
of learning is the general framework for search and
learning problems in history-based models proposed
by Daume? III and Marcu (2005). This framework,
called LaSO (Learning as Search Optimization), can
include many variations of search strategies such as
beam search and A* search as a special case. In-
deed, our lookahead framework could be regarded
as a special case in which each search node con-
243
Training time (sec) Test time (sec) F-measure
CRF (L1 regularization & SGD training) 235 4 71.63
No lookahead (depth = 0) 66 4 70.17
Lookahead (depth = 1) 91 4 72.28
Lookahead (depth = 2) 302 7 72.00
Lookahead (depth = 3) 2,419 33 72.21
Semi-Markov CRF (Okanohara et al, 2006) n/a n/a 71.48
Reranking (Yoshida and Tsujii, 2007) n/a n/a 72.65
Table 3: Performance of biomedical named entity recognition (training times and accuracy scores on test data).
Training time (sec) Test time (sec) Accuracy
No lookahead (depth = 0) 1,937 4 89.73
Lookahead (depth = 1) 4,907 13 91.00
Lookahead (depth = 2) 12,800 31 91.10
Lookahead (depth = 3) 31,684 79 91.24
Beam search (k = 64) (Zhang and Clark, 2008) n/a n/a 91.4
Deterministic (Huang et al, 2009) n/a n/a 90.2
Beam search (k = 16) (Huang et al, 2009) n/a n/a 91.3
Dynamic programming (Huang and Sagae, 2010) n/a n/a 92.1
Table 4: Performance of English dependency parsing (training times and accuracy scores on test data).
sists of the next and lookahead actions4, although
the weight updating procedure differs in several mi-
nor points. Daume? III and Marcu (2005) did not try
a lookahead search strategy, and to the best of our
knowledge, this paper is the first that demonstrates
that lookahead actually works well for various NLP
tasks.
Performing lookahead is a very common tech-
nique for a variety of decision-making problems
in the field of artificial intelligence. In computer
chess, for example, programs usually need to per-
form a very deep search in the game tree to find a
good move. Our decision-making problem is sim-
ilar to that of computer Chess in many ways, al-
though chess programs perform min-max searches
rather than the ?max? searches performed in our al-
gorithm. Automatic learning of evaluation functions
for chess programs can be seen as the training of
a machine learning model. In particular, our learn-
ing algorithm is similar to the supervised approach
4In addition, the size of the search queue is always truncated
to one for the deterministic decisions presented in this paper.
Note, however, that our lookahead framework can also be com-
bined with other search strategies such as beam search. In that
case, the search queue is not necessarily truncated.
(Tesauro, 2001; Hoki, 2006) in that the parameters
are optimized based on the differences of the feature
vectors realized by the correct and incorrect actions.
In history-based models, the order of actions is of-
ten very important. For example, backward tagging
is considerably more accurate than forward tagging
in biomedical named entity recognition. Our looka-
head method is orthogonal to more elaborate tech-
niques for determining the order of actions such as
easy-first tagging/parsing strategies (Tsuruoka and
Tsujii, 2005; Elhadad, 2010). We expect that incor-
porating such elaborate techniques in our framework
will lead to improved accuracy, but we leave it for
future work.
6 Conclusion
We have presented a simple and general framework
for incorporating a lookahead process in history-
based models and a perceptron-based training algo-
rithm for the framework. We have conducted ex-
periments using standard data sets for POS tagging,
chunking, named entity recognition and dependency
parsing, and obtained very promising results?the
accuracy achieved by the history-based models en-
244
hanced with lookahead was as competitive as glob-
ally optimized models including CRFs.
In most of the experimental results, steady im-
provement in accuracy has been observed as the
depth of the search is increased. Although it is
not very practical to perform deeper searches with
our current implementation?we naively explored
all possible sequences of actions, future work should
encompass extending the depths of search space
by introducing elaborate pruning/search extension
techniques.
In this work, we did not conduct extensive feature
engineering for improving the accuracy of individ-
ual tasks because our primary goal with this paper is
to present the learning framework itself. However,
one of the major merits of using history-based mod-
els is that we are allowed to define arbitrary features
on the partially completed structure. Another inter-
esting direction of future work is to see how much
we could improve the accuracy by performing ex-
tensive feature engineering in this particular learning
framework.
Appendix A: Convergence of the Learning
Procedure
Let {xi, aic}Ki=1 be the training examples where aic
is the correct first action for decision point xi, and
let Si be the set of all the states at the leaves of
the search trees for xi generated by the lookahead
searches and Sic be the set of all the states at the
leaves of the search tree for the correct action aic.
We also define Si = Si \ Sic. We write the weight
vector before the k-th update as wk. We define
S?c = argmax
S?Sic
w??(S) and S? = argmax
S?Si
w??(S)5.
Then the update rule can be interpreted as wk+1 =
wk +(?(S?c )??(S?)). Note that this update is per-
formed only when ?(Sc) ?wk?C < ?(S?) ?wk for
all Sc ? Sc since otherwise S? in the learning algo-
rithm cannot be a state with an incorrect first action.
In other words, ?(Sc) ?w ? ?(S?) ?w ? C for all
Sc ? Sc after convergence.
Given these definitions, we prove the convergence
for the separable case. That is, we assume the exis-
tence of a weight vector u (with ||u|| = 1), ? (> 0),
5S?c and S? depend on the weight vector at each point, but
we omit it from the notation for brevity.
and R (> 0) that satisfy:
?i,?Sc ? Sic,?S ? Si ?(Sc) ? u? ?(S) ? u ? ?,
?i,?Sc ? Sic,?S ? Si ||?(Sc)? ?(S)|| ? R.
The proof is basically an adaptation of the proofs
in Collins (2002) and Li et al (2002). First, we ob-
tain the following relation:
wk+1 ? u = wk ? u+ (?(S?c ) ? u? ?(S?) ? u)
= wk ? u+ ? ? w1 ? u+ k? = k?.
Therefore, ||wk+1 ? u||2 = ||wk+1||2 ? (k?)2 ?
(1). We assumed w1 = 0 but this is not an essential
assumption.
Next, we also obtain:
||wk+1||2 ? ||wk||2 + 2(?(S?c )? ?(S?)) ?wk
+||?(S?c )? ?(S?)||2
? ||wk||2 + 2C +R2
? ||w1||2 + k(R2 + 2C) = k(R2 + 2C)? (2)
Combining (1) and (2), we obtain k ? (R2 +
2C)/?2. That is, the number of updates is bounded
from above, meaning that the learning procedure
converges after a finite number of updates. Substi-
tuting this into (2) gives ||wk+1|| ? (R2 + 2C)/?
? (3).
Finally, we analyze the margin achieved by the
learning procedure after convergence. The margin,
?(w), is defined as follows in this case.
?(w) = min
xi
min
Sc?Sic,S?Si
?(Sc) ?w ? ?(S) ?w
||w||
= min
xi
min
Sc?Sic
?(Sc) ?w ? ?(S?) ?w
||w||
After convergence (i.e., w = wk+1), ?(Sc) ? w ?
?(S?)?w ? C for all Sc ? Sc as we noted. Together
with (3), we obtain the following bound:
?(w) ? min
xi
?C
2C +R2
= ?C
2C +R2
=
(
?
2
)(
1? R
2
2C +R2
)
As can be seen, the margin approaches at least half
the true margin, ?/2 as C ? ? (at the cost of infi-
nite number of updates).
245
References
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of ICML,
pages 169?176.
Yoav Goldbergand Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL-HLT, pages
742?750.
Kunihito Hoki. 2006. Optimal control of minimax
search results to learn positional evaluation. In Pro-
ceedings of the 11th Game Programming Workshop
(GPW), pages 78?83 (in Japanese).
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL, pages 1077?1086.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP, pages 1222?1231.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recogni-
tion task at JNLPBA. In Proceedings of the Interna-
tional Joint Workshop on Natural Language Process-
ing in Biomedicine and its Applications (JNLPBA),
pages 70?75.
W Krauth and M Mezard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Phisics A, 20(11):L745?L752.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282?289.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL, pages 504?513.
Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-
Taylor, and Jaz S. Kandola. 2002. The perceptron
algorithm with uneven margins. In Proceedings of
ICML, pages 379?386.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In Proceed-
ings of ICML, pages 591?598.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In ACL 2004 Workshop on Incre-
mental Parsing: Bringing Engineering and Cognition
Together, pages 50?57.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In Proceedings of COL-
ING/ACL, pages 465?472.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of ACL, pages 760?767.
Gerald Tesauro, 2001. Comparison training of chess
evaluation functions, pages 117?130. Nova Science
Publishers, Inc.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proceedings of HLT/EMNLP
2005, pages 467?474.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of ACL-IJCNLP, pages 477?
485.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In Proceed-
ings of ACL Workshop on BioNLP, pages 209?216.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graphbased
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP, pages 562?571.
246
Proceedings of the Fifth Law Workshop (LAW V), pages 56?64,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Collaborative Annotation between Human Annotators and a Statistical
Parser
Shun?ya Iwasawa Hiroki Hanaoka Takuya Matsuzaki
University of Tokyo
Tokyo, Japan
{iwasawa,hkhana,matuzaki}@is.s.u-tokyo.ac.jp
Yusuke Miyao
National Institute of Informatics
Tokyo, Japan
yusuke@nii.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
Beijing, P.R.China
jtsujii@microsoft.com
Abstract
We describe a new interactive annotation
scheme between a human annotator who
carries out simplified annotations on CFG
trees, and a statistical parser that converts
the human annotations automatically into a
richly annotated HPSG treebank. In order
to check the proposed scheme?s effectiveness,
we performed automatic pseudo-annotations
that emulate the system?s idealized behavior
and measured the performance of the parser
trained on those annotations. In addition,
we implemented a prototype system and con-
ducted manual annotation experiments on a
small test set.
1 Introduction
On the basis of the success of the research on the
corpus-based development in NLP, the demand for
a variety of corpora has increased, for use as both a
training resource and an evaluation data-set. How-
ever, the development of a richly annotated cor-
pus such as an HPSG treebank is not an easy task,
since the traditional two-step annotation, in which
a parser first generates the candidates and then an
annotator checks each candidate, needs intensive ef-
forts even for well-trained annotators (Marcus et al,
1994; Kurohashi and Nagao, 1998). Among many
NLP problems, adapting a parser for out-domain
texts, which is usually referred to as domain adap-
tation problem, is one of the most remarkable prob-
lems. The main cause of this problem is the lack
of corpora in that domain. Because it is difficult to
prepare a sufficient corpus for each domain without
reducing the annotation cost, research on annotation
methodologies has been intensively studied.
There has been a number of research projects
to efficiently develop richly annotated corpora with
the help of parsers, one of which is called a
discriminant-based treebanking (Carter, 1997). In
discriminant-based treebanking, the annotation pro-
cess consists of two steps: a parser first generates
the parse trees, which are annotation candidates,
and then a human annotator selects the most plau-
sible one. One of the most important characteristics
of this methodology is to use easily-understandable
questions called discriminants for picking up the fi-
nal annotation results. Human annotators can per-
form annotations simply by answering those ques-
tions without closely examining the whole tree. Al-
though this approach has been successful in break-
ing down the difficult annotations into a set of easy
questions, specific knowledge about the grammar,
especially in the case of a deep grammar, is still re-
quired for an annotator. This would be the bottle-
neck to reduce the cost of annotator training and can
restrict the size of annotations.
Interactive predictive parsing (Sa?nchez-Sa?ez et
al., 2009; Sa?nchez-Sa?ez et al, 2010) is another ap-
proach of annotations, which focuses on CFG trees.
In this system, an annotator revises the currently
proposed CFG tree until he or she gets the correct
tree by using a simple graphical user interface. Al-
though our target product is a more richly anno-
tated treebanks, the interface of CFG can be useful
to develop deep annotations such as HPSG features
by cooperating with a statistical deep parser. Since
CFG is easier to understand than HPSG, it can re-
56
duce the cost of annotator training; non-experts can
perform annotations without decent training. As a
result, crowd-sourcing or similar approach can be
adopted and the annotation process would be accel-
erated.
Before conducting manual annotation, we sim-
ulated the annotation procedure for validating our
system. In order to check whether the CFG-based
annotations can lead to sufficiently accurate HPSG
annotations, several HPSG treebanks were created
with various qualities of CFG and evaluated by their
HPSG qualities.
We further conducted manual annotation experi-
ments by two human annotators to evaluate the ef-
ficiency of the annotation system and the accuracy
of the resulting annotations. The causes of annota-
tion errors were analyzed and future direction of the
further development is discussed.
2 Statistical Deep Parser
2.1 HPSG
Head-Driven Phrase Structure Grammar (HPSG)
is one of the lexicalized grammatical formalisms,
which consists of lexical entries and a collection of
schemata. The lexical entries represent the syntac-
tic and semantic characteristics of words, and the
schemata are the rules that construct larger phrases
from smaller phrases. Figure 1 shows the mecha-
nism of the bottom-up HPSG parsing for the sen-
tence ?Dogs run.? First, a lexical entry is as-
signed to each word, and then, the lexical signs
for ?Dogs? and ?run? are combined by Subject-
Head schema. In this way, lexical signs and phrasal
signs are combined until the whole sentence be-
comes one sign. Compared to Context Free Gram-
mar (CFG), since each sign of HPSG has rich infor-
mation about the phrase, such as subcategorization
frame or predicate-argument structure, a corpus an-
notated in an HPSG manner is more difficult to build
than CFG corpus. In our system, we aim at building
HPSG treebanks with low-cost in which even non-
experts can perform annotations.
2.2 HPSG Deep Parser
The Enju parser (Ninomiya et al, 2007) is a statis-
tical deep parser based on the HPSG formalism. It
produces an analysis of a sentence that includes the
2
64
HEAD noun
SUBJ <>
COMPS <>
3
75
Dogs
2
64
HEAD verb
SUBJ < noun >
COMPS <>
3
75
Drung
?
2
64
HEAD verb
SUBJ <>
COMPS <>
3
75
Subject
1
2
64
HEAD noun
SUBJ <>
COMPS <>
3
75
Headj
2
664
HEAD verb
SUBJ < 1 >
COMPS <>
3
775
Figure 1: Example of HPSG parsing for ?Dogs run.?
syntactic structure (i.e., parse tree) and the semantic
structure represented as a set of predicate-argument
dependencies. The grammar design is based on
the standard HPSG analysis of English (Pollard and
Sag, 1994). The parser finds a best parse tree
scored by a maxent disambiguation model using a
CKY-style algorithm and beam search. We used
a toolkit distributed with the Enju parser for ex-
tracting a HPSG lexicon from a PTB-style treebank.
The toolkit initially converts the PTB-style treebank
into an HPSG treebank and then extracts the lexi-
con from it. The HPSG treebank converted from the
test section is also used as the gold standard in the
evaluation.
2.3 Evaluation Metrics
In the experiments shown below, we evaluate the ac-
curacy of an annotation result (i.e., an HPSG deriva-
tion on a sentence) by evaluating the accuracy of
the semantic description produced by the deriva-
tion, as well as a more traditional metrics such
as labeled bracketing accuracy of the tree struc-
ture. Specifically, we used labeled and unlabeled
precision/recall/F-score of the predicate-argument
dependencies and the labeled brackets compared
against a gold-standard annotation obtained by using
the Enju?s treebank conversion tool. A predicate-
argument dependency is represented as a tuple of
?wp, wa, r?, where wp is the predicate word, wa
is the argument word, and r is the label of the
predicate-argument relation, such as verb-ARG1
(semantic subject of a verb) and prep-MOD (modi-
57
fiee of a prepositional phrase). As for the bracketing
accuracies, the label of a bracket is obtained by pro-
jecting the sign corresponding to the phrase into a
simple phrasal labels such as S, NP, and VP.
3 Proposed Annotation System
In our system, a human annotator and a statistical
deep parser cooperate to build a treebank. Our sys-
tem uses CFG as user interface and bridges a gap be-
tween CFG and HPSG with a statistical CKY parser.
Following the idea of the discriminant-based tree-
banking model, the parser first generates candidate
trees and then an annotator selects the correct tree in
the form of a packed forest. For selecting the correct
tree, the annotator only edits a CFG tree projected
from an HPSG tree through pre-defined set of oper-
ations, to eventually give the constraints onto HPSG
trees. This is why annotators can annotate HPSG
trees without HPSG knowledge. The current system
is implemented based on the following client-server
model.
3.1 Client: Annotator Interface
The client-side is an annotator?s interface imple-
mented with Ajax technique, on which annotator?s
revision is carried out through Web-Browser. When
the client-side receives the data of the current best
tree from the server-side, it shows an annotator the
CFG representation of the tree. Then, an annotator
adds revisions to the CFG tree using the same GUI,
until the current best tree has the CFG structure that
exactly matches the annotators? interpretation of the
sentence. Finally, the client-side sends the annota-
tor?s revision as a CGI query to the server. Based
on interactive predicative parsing system, two kinds
of operations are implemented in our system: ?span
modification? and ?label substitution?, here abbrevi-
ated as ?S? and ?L? operations:
?S? operation modify span(left, right)
An annotator can specify that a constituent in
the tree after user?s revision must match a spec-
ified span, by sequentially clicking the leaf
nodes at the left and right boundaries.
?L? operation modify label(pos, label)
An annotator can specify that a constituent in
the tree after user?s revision must match a spec-
ified label, by inputting a label and clicking the
node position.
In addition to ?S? and ?L? operations, one more
operation, ?tree fixation?, abbreviated ?F?, is imple-
mented for making annotation more efficient. Our
system computes the best tree under the current con-
straints, which are specified by the ?S? and ?L? op-
erations that the annotator has given so far. It means
other parts of the tree that are not constrained may
change after a new operation by the annotator. This
change may lead to a structure that the annotator
does not want. To avoid such unexpected changes,
an annotator can specify a subtree which he or she
does not want to change by ?tree fixation? operation:
?F? operation fix tree(pos = i)
An annotator can specify a subtree as correct
and not to be changed. The specified subtree
does not change and always appears in the best
tree.
3.2 Server: Parsing Constraints
In our annotation system, the server-side carries out
the conversion of annotator?s constraints into HPSG
grammatical constraints on CKY chart and the re-
computation of the current best tree under the con-
straints added so far. The server-side works in the
following two steps. The first step is the conversion
of the annotator?s revision into a collection of dead
edges or dead cells; a dead edge means the edge
must not be a part of the correct tree, and a dead cell
means all edges in the cell are dead. As mentioned
in the background section, Enju creates a CKY chart
during the parsing where all the terminal and non-
terminal nodes are stored with the information of its
sign and links to daughter edges. In our annotation
system, to change the best tree according to the an-
notator?s revision, we determine whether each edge
in the chart is either alive or dead. The server-side
re-constructs the best tree under the constraints that
all the edges used in the tree are alive. The sec-
ond step is the computation of the best tree by re-
constructing the tree from the chart, under the con-
straint that the best tree contains only the alive edges
as its subconstituents. Re-construction includes the
following recursive process:
1. Start from the root edge.
58
2. Choose the link which has the highest probabil-
ity among the links and whose daughter edges
are all alive.
3. If there is such a link, recursively carry out the
process for the daughter edge.
4. If all the links from the edge are dead, go back
to the previous edge.
Note that our system parses a sentence only once,
the first time, instead of re-parsing the sentence after
each revision. Now, we are going to list the revision
operations again and explain how the operations are
interpreted as the constraints in the CKY chart. In
the description below, label(x) means the CFG-
symbol that corresponds to edge x. Note that there
is in principle an infinite variety of possible HPSG
signs. The label function maps this multitude of
signs onto a small set of simple CFG nonterminal
symbols.
?S? operation span(left = i, right = j)
When the revision type is ?S? and the left and
right boundary of the specified span is i and j
in the CGI query, we add the cells which satisfy
the following formula to the list of dead edges.
Suppose the sentence length is L, then the set
of new dead cells is defined as:
{cell(a, b) | 0 ? a < i,i ? b < j }
? {cell(c, d) | i+ 1 ? c ? j,j + 1 ? d ? n },
where the first set means the inhibition of the
edges that span across the left boundary of the
specified span. The second set means a similar
conditions for the right span.
?L? operation fix label(position = i, label = l)
When the revision type is ?L?, the node posi-
tion is i and the label is l in the CGI query, we
determine the set of new dead edges and dead
cells as follows:
1. let cell(a, b) = the cell including i
2. mark those cells that are generated by
span(a, b) as defined above to be dead,
and
3. for each edge e? in cell(a, b), mark e?
to be dead if label(e?) 6= l
?F? operation fix tree(position = i)
(a) prob = 0.4 (b) prob = 0.3 (c) prob = 0.2
NP
NX
NP
Time
NX
flies
PP
PX
like
NP
DP
an
NX
arrow
S
NP
NX
Time
VP
VP
flies
PP
PX
like
NP
DP
an
NX
arrow
S
NP
NX
NP
Time
NX
flies
VP
VX
like
NP
DP
an
NX
arrow
Figure 2: Three parse tree candidates of ?Time flies like
an arrow.?
When the revision type is ?F? and the target
node position is i in the CGI query, we carry
out the following process to determine the new
dead edges and cells:
1. for each edge e in the subtree rooted at
node i,
2. let cell(a, b) = the cell including e
3. mark those cells that are generated by
span(a, b) as defined above to be dead
4. for each edge e? in cell(a, b), mark e?
to be dead if label(e?) 6= label(e)
The above procedure adds the constraints so
that the correct tree includes a subtree that has
the same CFG-tree representation as the sub-
tree rooted at i in the current tree.
Finally we show how the best tree for the sentence
?Time flies like an arrow.? changes with the anno-
tator?s operations. Let us assume that the chart in-
cludes the three trees shown (in the CFG representa-
tion) in (Figure 2), and that there are no dead edges.
Let us further assume that the probability of each
tree is as shown in the figure and hence the current
best tree is (a). If the annotator wants to select (b)
as the best tree, s/he can apply ?L? operation on the
root node. The operation makes some of the edges
dead, which include the root edge of tree (a) (see
Figure 3). Accordingly, the best tree is now selected
from (b), (c), etc., and tree (b) will be selected as the
next best tree.
4 Validation of CFG-based Annotation
Because our system does not present HPSG anno-
tations to the annotators, there is a risk that HPSG
annotations are wrong even when their projections
to CFG trees are completely correct. Our expecta-
59
NP
Time
NX
VP
flies
PX
VX
like
DP
IanI
NX
IarrowI
NP
NX NP
PP
VP
VP
NP
S
NP
Time
NX
VP
flies
PX
VX
like
DP
IanI
NX
IarrowI
NP
NX NP
PP
VP
VP
NP
S
fix label
(root,S)
?
Figure 3: Chart constraints by ?L? operation. Solid lines
represent the link of the current best tree and dashed lines
represent the second best one. Dotted lines stand for an
unavailable link due to the death of the source edge.
tion is that the stochastic model of the HPSG parser
properly resolves the remaining ambiguities in the
HPSG annotation within the constraints given by a
part of the CFG trees. In order to check the validity
of this expectation and to measure to what extent the
CFG-based annotations can achieve correct HPSG
annotations, we performed a pseudo-annotation ex-
periment.
In this experiment, we used bracketed sentences
in the Brown Corpus (Kuc?era and Francis, 1967),
and a court transcript portion of the Manually An-
notated Sub-Corpus (MASC) (Ide et al, 2010). We
automatically created HPSG annotations that mimic
the annotation results by an ideal annotator in the
following four steps. First, HPSG treebanks for
these sentences are created by the treebank conver-
sion program distributed with the Enju parser. This
program converts a syntactic tree annotated by Penn
Treebank style into an HPSG tree. Since this pro-
gram cannot convert the sentences that are not cov-
ered by the basic design of the grammar, we used
only those that are successfully converted by the
program throughout the experiments and considered
this converted treebank as the gold-standard tree-
bank for evaluation. Second, the same sentences are
parsed by the Enju parser and the results are com-
pared with the gold-standard treebank. Then, CFG-
level differences between the Enju parser?s outputs
and the gold-standard trees are translated into oper-
ation sequences of the annotation system. For ex-
ample, ?L? operation of NX ? VP at the root node
is obtained in the case of Figure 4. Finally, those
operation sequences are executed on the annotation
system and HPSG annotations are produced.
total size ave. s. l. convertible
Brown 24,243 18.94 22,214
MASC 1,656 14.81 1,353
Table 1: Corpus and experimental data information (s. l.
means ?sentence length.?)
(a) NX
NX PP
PX NP
(b) VP
VP PP
PX NP
Figure 4: CFG representation of parser output (a) and
gold-standard tree (b)
4.1 Relationship between CFG and HPSG
Correctness
We evaluated the automatically produced annota-
tions in terms of three measures: the labeled brack-
eting accuracies of their projections to CFG trees,
the accuracy of the HPSG lexical entry assignments
to the words, and the accuracy of the semantic de-
pendencies extracted from the annotations. The
CFG-labeled bracketing accuracies are defined in
the same way as the traditional PARSEVAL mea-
sures. The HPSG lexical assignment accuracy is
the ratio of words to which the correct HPSG lex-
ical entry is assigned, and the semantic dependency
accuracy is defined as explained in Section 2.3. In
this experiment, we cut off sentences longer than 40
words for time reasons. We split the Brown Cor-
pus into three parts: training, development test and
evaluation, and evaluated the automatic annotation
results only for the training portion.
We created three sets of automatic annotations as
follows:
Baseline No operation; default parsing results are
considered as the annotation results.
S-full Only ?S? operations are used; the tree struc-
tures of the resulting annotations should thus be
identical to the gold-standard annotations.
SL-full ?S? and ?L? operations are used; the la-
beled tree structures of the resulting anno-
tations should thus be identical to the gold-
standard annotations.
Before showing the evaluation results, splitting of
the data should be described here. Our system as-
sumes that the correct tree is included in the parser?s
60
CKY chart; however, because of the beam-search
limitation and the incomplete grammar coverage, it
does not always hold true. In this paper, such sit-
uations are called ?out-chart?. Conversely, the sit-
uations in which the parser does include the cor-
rect tree in the CKY chart are ?in-chart?. The re-
sults of ?in-chart? are here considered to be the re-
sults in the ideal situation of the perfect parser. In
our experimental setting, the training portion of the
Brown Corpus has 10,576 ?in-chart? and 7,208 ?out-
chart? sentences, while the MASC portion has 864
?in-chart? and 489 ?out-chart? sentences (Table 2).
Under ?out-chart? situations, we applied the opera-
tions greedily for calculating S-full and SL-full; that
is, all operations are sequentially applied and an op-
eration is skipped when there are no HPSG trees in
the CKY chart after applying that operation.
Table 3 shows the results of our three measures:
the CFG tree bracketing accuracy, the accuracy of
HPSG lexical entry assignment and that of the se-
mantic dependency. In both of S-full and SL-full,
the improvement from the baseline is significant.
Especially, SL-full for ?in-chart? data has almost
complete agreement with the gold-standard HPSG
annotations. The detailed figures are shown in Ta-
ble 4. Therefore, we can therefore conclude that
high quality CFG annotations lead to high quality
HPSG annotations when the are combined with a
good statistical HPSG parser.
4.2 Domain Adaptation
We evaluated the parser accuracy adapted with the
automatically created treebank on the Brown Cor-
pus. In this experiment, we used the adaptation al-
gorithm by (Hara et al, 2007), with the same hyper-
parameters used there. Table 5 shows the result of
the adapted parser. Each line of this table stands for
the parser adapted with different data. ?Gold? is the
result adapted on the gold-standard annotations, and
?Gold (only covered)? is that adapted on the gold
data which is covered by the original Enju HPSG
grammar that was extracted from the WSJ portion
of the Penn Treebank. ?SL-full? is the result adapted
on our automatically created data. ?Baseline? is the
result by the original Enju parser, which is trained
only on the WSJ-PTB and whose grammar was ex-
tracted from the WSJ-PTB. The table shows SL-full
slightly improves the baseline results, which indi-
#operations
S L F Avg. Time
Brown A. 1 122 1 0 1.19 43.32A. 2 91 4 1 0.94 41.77
MASC A. 1 275 2 5 2.76 33.33A. 2 52 2 0 0.51 35.13
Table 6: The number of operations and annotation time
by human annotators. ?Annotator? is abbreviated as A.
Avg. is the average number of operations per sentence
and Time is annotation time per sentence [sec.].
cates our annotation system can be useful for do-
main adaptation. Because we used mixed data of
?in-chart? and ?out-chart? in this experiment, there
still is much room for improvement by increasing
the ratio of the ?in-chart? sentences using a larger
beam-width.
5 Interactive Annotation on a
Prototype-system
We developed an initial version of the annotation
system described in Section 3, and annotated 200
sentences in total on the system. Half of the sen-
tences were taken from the Brown corpus and the
other half were taken from a court-debate section of
the MASC corpus. All of the sentences were an-
notated twice by two annotators. Both of the anno-
tators has background in computer science and lin-
guistics.
Table 6 shows the statistics of the annotation pro-
cedures. This table indicates that human annotators
strongly prefer ?S? operation to others, and that the
manual annotation on the prototype system is at least
comparable to the recent discriminant-based annota-
tion system by (Zhang and Kordoni, 2010), although
the comparison is not strict because of the difference
of the text.
Table 7 shows the automatic evaluation results.
We can see that the interactive annotation gave slight
improvements in all accuracy metrics. The improve-
ments were however not as much as we desired.
By classifying the remaining errors in the anno-
tation results, we identified several classes of major
errors:
1. Truly ambiguous structures, which require the
context or world-knowledge to correctly re-
solve them.
61
in out in+out
Brown (train.) 10,576 / 10,394 7,190 / 6,464 17,766 / 16,858
MASC 864 / 857 489 / 449 1,353 / 1,306
Table 2: The number of ?in-chart? and ?out-chart? sentences (total / 1-40 length)
in out in+out
Brown
SL-full 100.00 / 99.31 / 99.60 88.67 / 83.95 / 82.00 94.91 / 92.21 / 92.24
S-full 98.46 / 96.64 / 96.83 89.60 / 82.02 / 81.20 94.48 / 89.88 / 90.29
Baseline 92.39 / 92.69 / 90.54 82.10 / 78.38 / 73.80 87.78 / 86.07 / 83.54
MASC
SL-full 100.00 / 99.13 / 99.30 85.91 / 80.75 / 78.80 93.38 / 90.55 / 91.02
S-full 98.71 / 96.88 / 96.73 86.95 / 79.14 / 77.43 93.18 / 88.60 / 88.93
Baseline 93.98 / 93.51 / 91.56 80.00 / 75.89 / 72.22 87.43 / 85.30 / 83.75
Table 3: Evaluation of the automatic annotation sets. Each cell has the score of CFG F1 / Lex. Acc. / Dep. F1.
CFG tree accuracy
Brown MASC
A. 1 90.55 / 90.83 / 90.69 90.62 / 90.80 / 90.71
A. 2 91.01 / 91.09 / 91.05 91.01 / 91.09 / 91.05
Enju 89.70 / 89.74 / 89.72 90.02 / 90.20 / 90.11
PAS dependency accuracy
Brown MASC
A. 1 87.48 / 87.55 / 87.52 86.02 / 86.02 / 86.02
A. 2 88.42 / 88.27 / 88.34 85.28 / 91.01 / 85.32
Enju 87.12 / 86.91 / 87.01 84.81 / 84.26 / 84.53
Table 7: Automatic evaluation of the annotation results
(LP / LR / F1)
CFG tree accuracy
in-chart out-chart
A. 1 94.52 / 94.65 / 94.58 83.95 / 84.44 / 84.19
A. 2 95.07 / 95.14 / 95.10 84.22 / 84.32 / 84.27
Enju 94.44 / 94.37 / 94.40 81.81 / 82.00 / 81.90
PAS dependency accuracy
in-chart out-chart
A. 1 92.85 / 92.85 / 92.85 77.47 / 77.65 / 77.56
A. 2 93.34 / 93.34 / 93.34 79.17 / 78.80 / 78.98
Enju 92.73 / 92.73 / 92.73 76.57 / 76.04 / 76.30
Table 8: Automatic evaluation of the annotation results
(LP/LR/F1); in-chart sentences (left-column) and out-
chart sentences (right column) both from Brown
2. Purely grammar-dependent analyses, which re-
quire in-depth knowledge of the specific HPSG
grammar behind the simplified CFG-tree repre-
sentation given to the annotators.
3. Discrepancy between human intuition and the
convention in the HPSG grammar introduced
by the automatic conversion.
4. Apparently wrong analysis left untouched due
to the limitation of the annotation system.
We suspect some of the errors of type 1 have been
caused by the experimental setting of the annotation;
we gave the test sentences randomly drawn from
the corpus in a randomized order. This would have
made it difficult for the annotators to interpret the
sentences correctly. We thus expect this kind of er-
rors would be reduced by doing the annotation on a
larger chunk of text.
The second type of the errors are due to the fact
that the annotators are not familiar with the details
of the Enju English HPSG grammar. For example,
one of the annotators systematically chose a struc-
ture like (NP (NP a cat) (PP on the mat)). This struc-
ture is however always analysed as (NP a (NP? cat
(PP on the mat))) by the Enju grammar. The style of
the analysis implemented in the grammar thus some-
times conflicts with the annotators? intuition and it
introduces errors in the annotation results.
Our intention behind the design of the annotation
system was to make the annotation system more ac-
cessible to non-experts and reduce the cost of the
annotation. To reduce the type 2 errors, rather than
the training of the annotators for a specific gram-
mar, we plan to introduce another representation
system in which the grammar-specific conventions
become invisible to the annotators. For example, the
above-shown difference in the bracketing structures
of a determiner-noun-PP sequence can be hidden by
showing the noun phrase as a ternary branch on the
three children: (NP a cat (PP on the mat)).
The third type of the errors are mainly due to the
rather arbitrary choice of the HPSG analysis intro-
duced through the semi-automatic treebank conver-
sion used to extract the HPSG grammar. For in-
stance, the Penn Treebank annotates a structure in-
cluding an adverb that intervenes an auxiliary verb
62
Lex-Acc Dep-LP Dep-LR Dep-UP Dep-UR Dep-F1 Dep-EM
Brown 99.26 99.61 99.59 99.69 99.67 99.60 95.80
MASC 99.13 99.26 99.33 99.42 99.49 99.30 95.68
Table 4: HPSG agreement of SL-full for ?in-chart? data (EM means ?Exact Match.?)
LP LR UP UR F1 EM
Gold 85.62 85.41 89.70 69.47 85.51 45.07
Gold (only covered) 84.32 84.01 88.72 88.40 84.17 42.52
SL-full 83.27 82.88 87.93 87.52 83.08 40.19
Baseline 82.64 82.20 87.50 87.03 82.42 37.63
Table 5: Domain Adaptation Results
and a following verb as in (VP is (ADVP already)
installed). The attachment direction of the adverb is
thus left unspecified. Such structures are however
indistinguishably transformed to a binary structure
like (VP (VP? is already) installed) in the course of
the conversion to HPSG analysis since there is no
way to choose the proper direction only with the
information given in the source corpus. This de-
sign could be considered as a best-effort, systematic
choice under the insufficient information, but it con-
flicts with the annotators? intuition in some cases.
We found in the annotation results that the anno-
tators have left apparently wrong analyses on some
sentences, either those remaining from the initial
output proposed by the parser or a wrong structure
appeared after some operations by the annotators
(error type 4). Such errors are mainly due to the
fact that for some sentences a correct analysis cannot
be found in the parser?s CKY chart. This can hap-
pen either when the correct analysis is not covered
by the HPSG grammar, or the correct analysis has
been pruned by the beam-search mechanism in the
parser. To correct a wrong analysis from the insuffi-
cient grammar coverage, an expansion of the gram-
mar is necessary, either in the form of the expan-
sion of the lexicon, or an introduction of a new lex-
ical type. For the other errors from the beam-search
limitation, there is a chance to get a correct analysis
from the parser by enlarging the beam size as nec-
essary. The introduction of a new lexical type def-
initely requires a deep knowledge on the grammar
and thus out of the scope of our annotation frame-
work. The other cases can in principle be handled in
the current framework, e.g., by a dynamic expansion
of the lexicon (i.e., an introduction of a new associ-
ation between a word and known lexical type), and
by a dynamic tuning of the beam size.
To see the significance of the last type of the er-
ror, we re-evaluated the annotation results on the
Brown sentences after classifying them into: (1)
those for which the correct analyses were included
in the parser?s chart (in-chart, 65 sentences) and (2)
those for which the correct analyses were not in the
chart (out-chart, 35 sentences), either because of the
pruning effect or the insufficient grammar coverage.
The results shown in Table 8 clearly show that there
is a large difference in the accuracy of the annota-
tion results between these two cases. Actually, on
the in-chart sentences, the parser has returned the
correct analysis as the initial solution for over 50%
of the sentences, and the annotators saved it without
any operations. Thus, we believe it is quite effective
to add the above-mentioned functionalities to reduce
this type of errors.
6 Conclusion and Future Work
We proposed a new annotation framework for deep
grammars by using statistical parsers. From the the-
oretical point of view, we can achieve significantly
high quality HPSG annotations only by CFG annota-
tions, and the products can be useful for the domain
adaptation task. On the other hand, preliminary ex-
periments of a manual annotation show some diffi-
culties about CFG annotations for non-experts, es-
pecially grammar-specific ones. We hence need to
develop some bridging functions reducing such dif-
ficulties. One possible strategy is to introduce an-
other representation such as flat CFG than binary
CFG. While we adopted CFG interface in our first
prototype system, our scheme can be applied to an-
other interface such as dependency as long as there
exist some relatedness over syntax or semantics.
63
References
David Carter. 1997. The treebanker: a tool for super-
vised training of parsed corpora. In Workshop On
Computational Environments For Grammar Develop-
ment And Linguistic Engineering, pages 9?15.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of the 10th International Con-
ference on Parsing Technologies, pages 11?22, Prague,
Czech Republic.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the peo-
ple. In Proceedings of the ACL 2010 Conference Short
Papers, pages 68?73, Uppsala, Sweden, July.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a japanese parsed corpus while improving the parsing
system. In Proceedings of the NLPRS, pages 719?724.
Henry Kuc?era and W. Nelson Francis. 1967. Compu-
tational Analysis of Present Day American English.
Brown University Press, June.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on Human Language
Technology, pages 114?119.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,
and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate hpsg pars-
ing. In Proceedings of the 10th International Confer-
ence on Parsing Technologies, pages 60?68.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ricardo Sa?nchez-Sa?ez, Joan-Andreu Sa?nchez, and Jose?-
Miguel Bened??. 2009. Interactive predictive parsing.
In Proceedings of the 11th International Conference
on Parsing Technologies, pages 222?225.
Ricardo Sa?nchez-Sa?ez, Luis A. Leiva, Joan-Andreu
Sa?nchez, and Jose?-Miguel Bened??. 2010. Interactive
predictive parsing using a web-based architecture. In
Proceedings of the NAACL HLT 2010 Demonstration
Session, pages 37?40.
Yi Zhang and Valia Kordoni. 2010. Discriminant rank-
ing for efficient treebanking. In Coling 2010: Posters,
pages 1453?1461, Beijing, China, August. Coling
2010 Organizing Committee.
64
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 19?27,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Utilizing State-of-the-art Parsers to Diagnose Problems in Treebank
Annotation for a Less Resourced Language
Quy T. Nguyen
University of Information
Technology, Ho Chi Minh City
quynt@uit.edu.vn
Ngan L.T. Nguyen
National Institute
of Informatics, Tokyo
ngan@nii.ac.jp
Yusuke Miyao
National Institute
of Informatics, Tokyo
yusuke@nii.ac.jp
Abstract
The recent success of statistical pars-
ing methods has made treebanks become
important resources for building good
parsers. However, constructing high-
quality annotated treebanks is a challeng-
ing task. We utilized two publicly avail-
able parsers, Berkeley and MST parsers,
for feedback on improving the quality of
part-of-speech tagging for the Vietnamese
Treebank. Analysis of the treebank and
parsing errors revealed how problems with
the Vietnamese Treebank influenced the
parsing results and real difficulties of Viet-
namese parsing that required further im-
provements to existing parsing technolo-
gies.
1 Introduction
Treebanks, corpora annotated with syntactic struc-
tures, have become more and more important
for language processing. The Vietnamese Tree-
bank (VTB) has been built as part of the national
project ?Vietnamese language and speech process-
ing (VLSP)? to strengthen automatic processing of
the Vietnamese language (Nguyen et al, 2009).
However, when we trained the Berkeley parser
(Petrov et al, 2006) in our preliminary experiment
with VTB and evaluated it using the corpus, the
parser only achieved an F-score of 72.1%. This
percentage was far lower than the state-of-the-art
performance reported for the Berkeley parser on
the English Penn Treebank of 90.2% (Petrov et
al., 2006). There are two possible reasons for this.
First, the quality of VTB is not good enough to
construct a good parser that included the quality of
the annotation scheme, the annotation guidelines,
and the annotation process. Second, parsing Viet-
namese is a difficult problem on its own, and we
need to seek new solutions to this.
Nguyen et al (2012) proposed methods of
improving the annotations of word segmentation
(WS) for VTB. They also evaluated different WS
criteria in two applications, i.e., machine trans-
lation and text classification. This paper focuses
on improving the quality of parts-of-speech (POS)
annotations by using state-of-the-art parsers to
provide feedback for this process.
The difficulties with Vietnamese POS tag-
ging have been recognized by many researchers
(Nghiem et al, 2008; Le et al, 2010). There is lit-
tle consensus as to the methodology for classifying
words. Polysemous words, words with the same
surface form but having different meanings and
grammar functions, are very popular in the Viet-
namese language. For example, the word ?c??
can be a noun that means neck/she, or an adjec-
tive that means ancient depending on the context.
This characteristic makes it difficult to tag POSs
for Vietnamese, both manually and automatically.
The rest of this paper is organized as follows:
a brief introduction to VTB and its annotation
schemes are provided in Section 2. Then, previ-
ous work is summarized in Section 3. Section 4
describes our methods of detecting and correcting
inconsistencies in POSs in the VTB corpus. Eval-
uations of these methods are described in Section
5. Finally, Section 6 explains our evaluations of
the Berkeley parser and Minimum-Spanning Tree
(MST) parser on different versions of the VTB
corpus, which were created by using detected in-
consistencies. These results from evaluations are
considered to be a way of measuring the effect
of automatically detected and corrected inconsis-
tencies. We could observe difficulties with Viet-
namese that affected the quality of parsers by ana-
lyzing the results from parsing.
Our experiences in using state-of-the-art parsers
for treebank annotation, which are presented in
this paper, should not only benefit the Vietnamese
language, but also other languages with similar
19
Label Name Example
N Common noun nh?n d?n {people}
Np Proper noun Vi?t Nam {Vietnam}
Nc Classifer noun con, c?i, b?c {*}
Nu Unit noun m?t {meter}
V Verb ng?i {sit}
A Adjective t?t {good}
P Pronoun t?i {I}, h?n {he}
L Determiner m?i {every}, nh?ng {*}
M Number m?t {one}
R Adverb ??, s?, ?ang {*}
E Preposition tr?n {on}
C Conjunction tuy nhi?n {however}
I Exclamation ?i, chao, a ha {*}
T Particle ?, ?y, ch?ng {*}
B Foreign word internet, email
Y Abbreviation APEC, WTO, HIV
S Affix b?t, v?, ?a {*}
X Other
Table 1: VTB part-of-speech tag set
characteristics.
2 Brief introduction to VTB
The VTB corpus contains 10.433 sentences
(274.266 tokens), semi-manually annotated with
three layers of WS, POS tagging, and bracketing.
The first annotation is produced for each annota-
tion layer by using automatic tools. Then, the an-
notators revise these data. The WS and POS an-
notation schemes were introduced by Nguyen et
al. (2012). This section briefly introduces POS tag
set and a bracketing annotation scheme.
VTB specifies the 18 different POS tags sum-
marized in Table 1 (Nguyen et al, 2010a). Each
unit in this table goes with several example words.
English translations of these words are included in
braces. However, as we could not find any appro-
priate English translations for some words, these
empty translations have been denoted by asterisks
(*).
The VTB corpus is annotated with three syn-
tactic tag types: constituency tags, functional
tags, and null-element tags. There are 18 con-
stituency tags in VTB. The functional tags are
used to enrich information for syntactic trees, such
as where functional tag ?SUB? is combined with
constituency tag ?NP?, which is presented as ?NP-
SUB? to indicate this noun phrase is a subject.
There are 17 functional tags in VTB. The head
word of a phrase is annotated with functional tag
?H?.
The phrase structures of Vietnamese include
three positions: <pre-head>, <head>, and <post-
head> (Vietnamese grammar, 1983; Nguyen et al,
2010c). The head word of the phrase is in the
<head> position. The words that are in the <pre-
head> and <post-head> positions are modifiers of
the head word.
There is a special type of noun in Vietnamese
that we have called Nc-noun in this paper. Nc-
nouns can be classifier nouns or common nouns
depending on their modifiers. For example, the
Nc-noun ?con? is a classifier noun if its modifier
is the word ?c? {fish}? (?con c??, which means
a specific fish, similar to ?the fish? in English).
However, the Nc-noun ?con {child}? is a common
noun if its modifier is the word ?gh?? (?con gh??,
which means ?stepchild? in English). We found
that Nc-nouns always appeared in the head posi-
tions of noun phrases by investigating the VTB
corpus. There is currently little consensus as to
the methodology for annotating Nc-nouns (Hoang,
1998; Nguyen et al, 2010b; Nguyen et al, 2010a).
3 Summarization of previous work
Nguyen et al (2012) described methods of detect-
ing and correcting WS inconsistencies in the VTB
corpus. These methods focused on two types of
WS inconsistency, variation and structural incon-
sistency, which are defined below.
Variation inconsistency: is a sequence of tokens
that has more than one way of being segmented in
the corpus.
Structural inconsistency: occurs when different
sequences have similar structures, and thus should
be split in the same way, but are segmented in dif-
ferent ways in the corpus. Nguyen et al (2012)
pointed out three typical cases of structural in-
consistency that were analyzed as classifier nouns
(Nc), affixes (S), and special characters.
Nguyen et al (2012) analyzed N-gram se-
quences and phrase structures to detect WS in-
consistencies. Then, the detected WS inconsis-
tencies were classified into several patterns of in-
consistencies, parts of which were manually fixed
to improve the quality of the corpus. The rest
were used to create different versions of the VTB
corpus. These data sets were evaluated on auto-
matic WS and its applications to text classification
and English-Vietnamese statistical machine trans-
lations to find appropriate criteria for automatic
WS and its applications.
Their experiments revealed that the
VAR_FREQ data set achieved excellent re-
sults in these applications. The VAR_FREQ data
20
set was the original VTB corpus with manually
corrected structural inconsistencies in special
characters and selected segmentations with higher
frequencies in all detected variations. There-
fore, we used the VAR_FREQ data set in our
experiments.
4 Methods of detecting and correcting
inconsistencies in POS annotations
We propose two kinds of methods of detecting
and correcting inconsistencies. They correspond
to two different types of POS inconsistency that
we call multi-POS inconsistency (MI) and Nc in-
consistency (NcI), which are defined as follows.
Multi-POS inconsistency: is a word that is not
Nc-noun and has more than one POS tag at each
position in each phrase category.
Nc inconsistency: is a sequence of Nc-noun and
modifier, in which Nc-noun has more than one
way of POS annotation in the VTB corpus.
We separated the POS inconsistencies into these
two types of inconsistencies because Nc-nouns
are special types of words in Vietnamese. The
methods of detecting and correcting NcIs were
language-specific methods developed based on the
characteristics of Vietnamese. However, as the
methods for MIs are rather general, they can be
applied to other languages.
4.1 General method for multi-POS
inconsistencies
Detection method (MI_DM)
Our main problem was to distinguish MIs
from polysemous words, since polysemous words
should not be considered inconsistent annotations.
Our method was based on the position of words in
phrases and phrase categories. This idea resulted
from the observation that polysemous words have
many POS tags; however, each word usually has
only one true POS tag at each position in each
phrase category. For example, when a phrase cat-
egory is a verb phrase, the word ?can? in the pre-
head position of the verb phrase ?(VP (MD can)
(VB can))? should be a modal, but the word ?can?
in the head position should be a verb. Further, the
word ?cut? in the head position of a noun phrase
?(NP (DT a) (JJ further) (NN cut))? should be a
noun, but the word ?cut? in the head position of
the verb phrase ?(VP (VB cut) (NP (NNS costs)))?
should be a verb. This may be more frequent in
Vietnamese because it is not an inflectional lan-
guage i.e., the word form does not change accord-
ing to tenses, word categories (e.g., nouns, verbs,
and adjectives), or number (singular and plural).
The method involved three steps. First, we
extracted words in the same position for each
phrase category. Second, we counted the num-
ber of different POS tags of each word. Words
that had more than one POS tag were determined
to be multi-POS inconsistencies. For example, in
the following two preposition phrases, ?(PP (E-
H c?a) (P ch?ng_t?i1)) {of us}? and ?(PP (C-H
c?a) (P h?i_ngh?)) {of conference}?, the words
?c?a {of}? appear at the head positions of both
phrases, but they are annotated with different POS
tags, preposition (E) and conjunction (C). There-
fore, they are MIs according to our method.
It should be noted that this method was applied
to words that were direct children of a phrase.
Embedded phrases, such as ?(PP (E c?a) (P
ch?ng_t?i))? in ?(NP (M hai) (Nc-H con) (N m?o)
(PP (E c?a) (P ch?ng_t?i))) {our two cats}?, were
considered separately.
Correction method (MI_CM)
A multi-POS inconsistency detected with the
MI_DM method is denoted by ?w|P1-f1|P2-
f2|...|Pn-fn AC?, where Pi (i = 1, 2, ..., n) is a POS
tag of word w, fi is the frequency of POS tag Pi,
and AC is applying condition of w. Our method
of correcting the POS tag for POS inconsistency
?w|P1-f1|P2-f2|...|Pn-fn AC? involves two steps.
First, we select the POS tag with the highest fre-
quency of all POS tags of ?w|P1-f1|P2-f2|...|Pn-fn
AC? (Pmax). Second, we replace POS tags Pi of
all instances (w|Pi) satisfying condition AC with
POS tag Pmax. For MIs, the AC of word w is its
phrase category and position in the phrase.
For example, ?to?n b?|L-27|P-2? is a multi-
POS inconsistency in the pre-head position of a
noun phrase. The frequency of POS tag ?L? is 27
and the frequency of POS tag ?P? is 2. There-
fore, ?L? is the POS tag that was selected by the
MI_CM method. We replace all POS tags Pi of
instances ?to?n b?|Pi? in the pre-head positions
of noun phrases with POS tag ?L?.
4.2 Language-specific method for classifier
nouns
Detection method
As mentioned in Section 2, an Nc-noun can be
1We used underscore ?_? to link syllables of Vietnamese
compound words.
21
annotated with POS tag ?Nc? or ?N? depending
on the modifier that follows that Nc-noun. Ana-
lyzing the VTB corpus revealed that Nc-nouns had
two characteristics. First, an Nc-noun that is fol-
lowed by the same word at each occurrence is usu-
ally annotated with the same POS tag. Second, an
Nc-noun that is followed by a phrase or nothing at
each occurrence is annotated with the same POS
tag. Based on these two cases, we propose two
methods of detecting NcIs, which we have called
NcI_DM1 and NcI_DM2. They are described be-
low.
NcI_DM1: We counted Nc-nouns in VTB that
had two or more ways of POS annotation, satis-
fying the condition that Nc-nouns are followed by
a phrase or nothing. For example, the Nc-noun
?con? in ?(NP (M 2) (N-H con)) {2 children}? is
followed by nothing or it is followed by a prepo-
sitional phrase as in ?(NP (L c?c) (N-H con) (PP
(E-H c?a) (P t?i))) {my children}?.
NcI_DM2: We counted two-gram sequences
beginning with an Nc-noun in VTB that had two
or more ways of POS annotation of the Nc-noun,
satisfying the conditions that two tokens were all
in the same phrase and and they all had the same
depth in a phrase. For example, the Nc-noun
?con? in the two-gram ?con g?i {daughter}? was
sometimes annotated ?Nc?, and sometimes anno-
tated ?N? in VTB; in addition, as ?con? and ?g?i?
in the structure ?(NP (Nc-H con) (N g?i) (PP (E-
H c?a) (P t?i))) {my daughter}? were in the same
phrase and have the same depth, ?con? was an
NcI.
Correction method
We denoted NcIs with ?w|P1-f1|P2-f2|...|Pn-fn
AC? similarly to MIs. We also replaced the POS
tag of Nc-nouns with the highest frequency tag.
The only differences were the applying conditions
that varied according to the previous two cases of
NcIs.
? For Nc inconsistencies detected by the
NcI_DM1 method, AC is defined as follows:
w is an Nc-noun that is followed by nothing
or a phrase.
? For Nc inconsistencies detected by the
NcI_DM2 method, AC is defined as follows:
w is an Nc-noun that must be followed by a
word, m.
5 Results and evaluation
We detected and corrected MIs and NcIs based
on the two data sets, ORG and VAR_FREQ. The
ORG data set was the original VTB corpus and
VAR_FREQ was the original corpus with modifi-
cations to WS annotation. This setting was made
similar to that used by Nguyen et al (2012) to
enable comparison.
There are a total of 128,871 phrases in the VTB
corpus. The top five types of phrases are noun
phrases (NPs) (representing 49.6% of the total
number of phrases), verb phrases (VPs), preposi-
tional phrases (PPs), adjectival phrases (ADJPs),
and quantity phrases (QPs), representing 99.1% of
the total number of phrases in the VTB corpus. We
analyzed the VTB corpus based on these five types
of phrases.
5.1 Results for detected POS inconsistencies
Tables 2 and 3 show the overall statistics for
MIs and NcIs for each phrase category. The sec-
ond and third columns in these tables indicate the
numbers of inconsistencies and their instances that
were detected in the ORG data set. The fourth and
fifth columns indicate the numbers of inconsisten-
cies and their instances that were detected in the
VAR_FREQ data set. The rows in Table 3 indicate
the number of NcIs and the number of instances
detected with the NcI_DM1 and NcI_DM2 meth-
ods.
According to Table 2, most of the MIs occurred
in noun phrases, representing more than 72% of
the total number of MIs. All NcIs in Table 3 are
also in noun phrases. There are two possible rea-
sons for this. First, noun phrases represent the ma-
jority of phrases in VTB (represent 49.6% of the
total number of phrases in the VTB corpus). Sec-
ond, nouns are sub-divided into many other types
(common noun (N), classifier noun (Nc), proper
noun (Np), and unit noun (Nu)) (mentioned in Sec-
tion 2), which may confuse annotators in anno-
tating POS tags for nouns. In addition, the high
number of NcIs in Table 3 indicate that it is diffi-
cult to distinguish between Nc and other types of
nouns. Therefore, we need to have clearer annota-
tion guidelines for this.
5.2 Evaluation of methods to detect and
correct inconsistencies
We estimated the accuracy of our methods which
detected and corrected inconsistencies in POS tag-
22
Phrase
ORG VAR_FREQ
Inc Ins Inc Ins
NP 792 28,423 752 27,067
VP 221 10,158 139 10,110
ADJP 64 1,302 61 1,257
QP 4 22 4 22
PP 14 5,649 13 5,628
Total 1,095 45,554 969 44,084
Table 2: Statistics for multi-POS inconsistencies
for each phrase category in VTB. Number of In-
consistencies (Inc) and Number of Instances (Ins).
Detection method
ORG VAR_FREQ
Inc Ins Inc Ins
NcI_DM1 52 3,801 51 3,792
NcI_DM2 338 2,468 326 2,412
Total 390 6,269 377 6,204
Table 3: Statistics for Nc inconsistencies in head
positions of noun phrases in VTB.
ging by manually inspecting inconsistent annota-
tions. We manually inspected the two data sets
of ORG_EVAL and ORG_POS_EVAL. To cre-
ate ORG_EVAL, we randomly selected 100 sen-
tences which contained instances of POS incon-
sistencies in the ORG data set. ORG_EVAL con-
tained 459 instances of 157 POS inconsistencies.
ORG_POS_EVAL was the ORG_EVAL data set
with corrections made to multi-POS inconsisten-
cies and Nc inconsistencies with our methods of
correction above.
Detection: We manually checked POS incon-
sistencies and found that 153 cases out of 157 POS
inconsistencies (97.5%) were actual inconsisten-
cies. There were four cases that our method de-
tected as multi-POS inconsistencies, but they were
actually ambiguities in Vietnamese POS tagging.
They were polysemous words whose meanings
and POS tags depended on surrounding words, but
did not depend on their positions in phrases. For
example, the word ?s?ng? in the post-head posi-
tions of the verb phrases VP1 and VP2 below, can
be a noun that means morning in English, or it can
be an adjective that means bright, depending on
the preceding verb.
VP1: (VP (V-H th?p) (A s?ng) {lighten bright}
VP2: (VP (V-H ?i) (N s?ng) {go in the morning}
Correction: Table 4 shows results of com-
parison of the POS tags for 459 instances in
ORG_EVAL and those in ORG_POS_EVAL.
These results indicate that there are instances
whose POS tags are incorrect in ORG_EVAL
but correct in ORG_POS_EVAL (the third row
ORG_EVAL ORG_POS_EVAL No. of Instances
correct correct 404
incorrect correct 41
correct incorrect 11
incorrect incorrect 3
Total 459
Table 4: Comparison of POS tags for 459
instances in ORG_EVAL with those in
ORG_POS_EVAL.
PoPOS Counts Examples
Nc-N 385 ng??i {the, person}
N-V 186 m?t m?t {loss}
N-Np 176 H?i {association}
N-A 144 kh? kh?n {difficult}
V-A 92 ph?i {must, right}
Table 5: Top five pairs of confusing POS tags.
in Table 4), and there are instances whose POS
tags are correct in ORG_EVAL but incorrect in
ORG_POS_EVAL (the fourth row in Table 4).
The results in Table 4 indicate that, the number
of correct POS tags in ORG_POS_EVAL (445 in-
stances, representing 96.9% of the total number of
instances) is higher than that in ORG_EVAL (415
instances, representing 90.4% of the total number
of instances). This means our methods of correct-
ing inconsistencies in POS tagging improved the
quality of treebank annotations.
5.3 Analysis of detected inconsistencies
We analyzed the detected POS inconsistencies to
find the reasons for inconsistent POS annotations.
We classified the detected POS inconsistencies ac-
cording to pairs of their POS tags. There were
a total of 85 patterns of pairs of POS tags. Ta-
ble 5 lists the top five confusing patterns (PoPOS),
their counts of inconsistencies (Counts), and ex-
amples. It also seemed to be extremely confus-
ing for the annotators to distinguish types of nouns
(Nc and N, and N and Np) and distinguish nouns
from other types of words (such as verbs, adjec-
tives, and pronouns).
We investigated POS inconsistencies and the
annotation guidelines (Nguyen et al, 2010b;
Nguyen et al, 2010a; Nguyen et al, 2010c) to
find why common nouns were sometimes tagged
as classifier nouns and vice versa, and verbs were
sometimes tagged as common nouns and vice
versa, and so on. We found that these POS in-
consistencies belonged to polysemous words that
were difficult to tag.
The difficulties with tagging polysemous words
23
were due to four main reasons: (1) The POS of a
polysemous word changes according to the func-
tion of that polysemous word in each phrase cate-
gory or changes according to the meaning of sur-
rounding words. Although polysemous words are
annotated with different POS tags, they do not
change their word form. (2) The way polysemous
words are tagged according to their context is not
completely clear in the POS tagging guidelines.
(3) Annotators referred to a dictionary that had
been built as part of the VLSP project (Nguyen et
al., 2009) (VLSP dictionary) to annotate the VTB
corpus. However, this dictionary lacked various
words and did not cover all contexts for the words.
For example, ?h?n {more than}? in Vietnamese is
an adjective when it is the head word of an adjec-
tival phrase, but ?h?n {over}? is an adverb when it
is the modifier of a quantifier noun (such as ?h?n
200 sinh vi?n {over 200 students}?). However, the
VLSP dictionary only considered ?h?n? to be an
adjective (?t?i h?n n? hai tu?i {I am more than
him two years old}?). No cases where ?h?n? was
an adverb were mentioned in this dictionary. (4)
There are several overlapping but conflicting in-
structions across the annotation guidelines for dif-
ferent layers of the treebank. For example, the
combinations of affixes and words they modify to
create compound words are clear in the WS guide-
lines, but POS tagging guidelines treat affixes as
words and they are annotated as POS tags ?S?.
For words modifying quantifier nouns, such as
?h?n and g?n {over and about}?, the POS tagging
guidelines treat them as adjectives, but the brack-
eting guidelines treat them as adverbs. Therefore,
our method detected multi-POS inconsistencies as
?h?n|A-135|R-51?, ?g?n|A-102|R-5? at the pre-
head positions of noun phrases. Since the frequen-
cies of the adjective tags were greater than those of
adverb tags (fA > fR), these words were automati-
cally assigned to adjective POS tags (A) according
to our method of correction. These were POS in-
consistencies that our method of correction could
not be applied to, because the frequency of incor-
rect POS tags was higher than that of actual POS
tags.
6 Evaluation of state-of-the-art parsers
on VTB
We carried out experiments to evaluate two pop-
ular parsers, a syntactic parser and a dependency
parser, on different versions of the VTB corpus.
Some of these data sets were made the same as the
data settings for WS in Nguyen et al (2012). The
other data sets contained changes in POS annota-
tions following our methods of correcting incon-
sistencies presented in Section 4. We could ob-
serve how the problems with WS and POS tag-
ging influenced the quality of Vietnamese parsing
by analyzing the parsing results.
6.1 Experimental settings
Data. Nine configurations of the VTB corpus
were created as follows:
? ORG: The original VTB corpus.
? BASE, STRUCT_AFFIX, STRUCT_NC,
VAR_SPLIT, VAR_COMB, and
VAR_FREQ correspond to different set-
tings for WS described in Nguyen et
al. (2012).
? ORG_POS: The ORG data set with correc-
tions for multi-POS inconsistencies and Nc
inconsistencies by using the methods in Sec-
tion 4.1 and 4.2.
? VAR_FREQ_POS: The VAR_FREQ data set
with corrections for multi-POS inconsisten-
cies and Nc inconsistencies by using the
methods in Section 4.1 and 4.2.
Each of the nine data sets was randomly split
into two subsets for training and testing our parser
models. The training set contained 9,443 sen-
tences, and the testing set contained 1,000 sen-
tences.
Tools
We used the Berkeley parser (Petrov et al,
2006) to evaluate the syntactic parser on VTB.
This parser has been used in experiments in En-
glish, German, and Chinese and achieved an F1 of
90.2% on the English Penn Treebank.
We used the conversion tool built by Johans-
son et al (2007) to convert VTB into dependency
trees.
We used the MST parser to evaluate the depen-
dency parsing on VTB. This parser was evaluated
on the English Penn Treebank (Mcdonald et al,
2006a) and 13 other languages (Mcdonald et al,
2006b). Its accuracy achieved 90.7% on the En-
glish Penn Treebank.
We made use of the bracket scoring program
EVALB, which was built by Sekine et al (1997),
24
Data sets Bracketing F-measures
ORG 72.10
BASE 72.20
STRUCT_AFFIX 72.60
STRUCT_NC 71.92
VAR_SPLIT 72.03
VAR_COMB 72.46
VAR_FREQ 72.34
ORG_POS 72.72
VAR_FREQ_POS 73.21
Table 6: Bracketing F-measures of Berkeley
parser on nine configurations of VTB corpus.
Data set UA LA
ORG 50.51 46.14
BASE 53.90 50.14
STRUCT_AFFIX 54.00 50.25
STRUCT_NC 53.88 49.96
VAR_SPLIT 53.95 50.14
VAR_COMB 53.93 50.27
VAR_FREQ 54.21 50.41
ORG_POS 54.20 50.37
VAR_FREQ_POS 57.87 53.19
Table 7: Dependency accuracy of MSTParser on
nine configurations of VTB corpus. Unlabeled
Accuracy (UA), Labeled Accuracy (LA).
to evaluate the performance of the Berkeley parser.
As an evaluation tool was included in the MST
parser tool, we used it to evaluate the MST parser.
6.2 Experimental results
The bracketing F-measures of the Berkeley parser
on nine configurations of the VTB corpus are
listed in Table 6. The dependency accuracies of
the MST parser on nine configurations of the VTB
corpus are shown in Table 7. These results indicate
that the quality of the treebank strongly affected
the quality of the parsers.
According to Table 6, all modifications to WS
inconsistencies improved the performance of the
Berkeley parser except for STRUCT_NC and
VAR_SPLIT. More importantly, the ORG_POS
model achieved better results than the ORG
model, and the VAR_FREQ_POS model achieved
better results than the VAR_FREQ model, which
indicates that the modifications to POS inconsis-
tencies improved the performance of the Berkeley
parser. The VAR_FREQ_POS model scored 1.11
point higher than ORG, which is a significant im-
provement.
Dependency accuracies of the MST parser
in Table 7 indicate that all modifications to
POS inconsistencies improved the performance
of the MST parser. All modifications to WS
APSs CCTs and Freq
A M N NP-79|ADJP-27
A V VP-56|ADJP-78|NP-2
Table 8: Examples of ambiguous POS sequences
(APSs), their CCTs, and frequency of each CCT
(Freq)
inconsistencies also improved the performance
of the MST parser except for STRUCT_NC.
The VAR_FREQ_POS model scored 7.36 points
higher than ORG, which is a significant improve-
ment.
6.3 Analysis of parsing results
The results for the Berkeley parser and MST
parser trained on the POS-modified versions of
VTB were better than those trained on the origi-
nal VTB corpus, but they were still much lower
than the performance of the same parsers on
the English language. We analyzed error based
on the output data of the best parsing results
(VAR_FREQ_POS) for the Berkeley parser, and
found that the unmatched annotations between
gold and test data were caused by ambiguous POS
sequences in the VTB corpus.
An ambiguous POS sequence is a sequence of
POS tags that has two or more constituency tags.
For example, there are the verb phrase ?(VP (R
?ang) (A c?m_c?i) (V l?m)) {* (be) painstak-
ingly doing}? and the adjectival phrase ?(ADJP (R
r?t) (A d?) (V th?c_hi?n)) {very easy (to) imple-
ment}? in the training data of VAR_FREQ_POS.
As these two phrases have the same POS sequence
?R A V?, ?R A V? is an ambiguous POS se-
quence, and VP and ADJP are confusing con-
stituency tags (CCTs). We found 42,373 occur-
rences of 213 ambiguous POS sequences (repre-
senting 37.02% of all phrases) in the training data
of VAR_FREQ_POS. We also found 1,065 oc-
currences of 13 ambiguous POS sequences in the
parsing results for VAR_FREQ_POS. Some ex-
amples of ambiguous POS sequences, their CCTs,
and the number of occurrences of each CCT in the
training data of VAR_FREQ_POS are listed in Ta-
ble 8.
We classified the detected ambiguous POS se-
quences according to pairs of different CCTs to
find the reasons for ambiguity in each pair. There
were a total of 42 pairs of CCTs, whose top three
pairs, along with their counts of types of am-
biguous POS sequences, and examples of ambigu-
25
Pairs of CCTs Counts Examples
NP-VP 61 P V N, ...
VP-ADJP 54 R A V, A V N, ...
ADJP-NP 52 A M N, ...
Table 9: Top three pairs of confusing constituency
tags
Pairs of CCTs 1 2
NP-VP M, L ,R ,V N, R, M, P, A
VP-ADJP A, R N, R
ADJP-NP N, R R, M, A, L
Table 10: Statistics for POS tags at pre-head posi-
tion of each phrase category.
ous POS sequences are listed in Table 9. We
extracted different POS tags at each position of
each phrase category for each pair of CCTs, based
on the ambiguous POS sequences. For example,
the third row in Table 9 has ?R A V? and ?A V
N?, which are two ambiguous POS sequences that
were sometimes annotated as VP and sometimes
annotated as ADJP. The different POS tags that
were extracted from the pre-head positions of VPs
based on these two POS sequences were ?R, A?
and ?R? was the POS tag that was extracted from
the pre-head positions of ADJPs based on these
two POS sequences. These POS tags are important
clues to finding reasons for ambiguities in POS se-
quences.
Table 10 summarizes the extracted POS tags at
pre-head positions for the top three pairs of CCTs.
For example, the POS tags in row NP-VP and col-
umn 1 are in the pre-head positions of NP and the
POS tags in row NP-VP and column 2 are in the
pre-head positions of VP. By comparing these re-
sults with the structures of the pre-head positions
of phrase categories in VTB bracketing guidelines
(Nguyen et al, 2010c), we found many cases that
were not annotated according to instructions in the
VTB bracketing guidelines, such as those accord-
ing to Table 10, where an adjective (A) is in the
pre-head position of VP, but according to the VTB
bracketing guidelines, the structure of the pre-head
position of VB only includes adverb (R).
We investigated cases that had not been anno-
tated according to the guidelines, and found two
possible reasons that caused ambiguous POS se-
quences. First, although our methods improved
the quality of the VTB corpus, some POS anno-
tation errors remained in the VTB corpus. These
POS annotation errors were cases to which our
methods could not be applied (mentioned in Sec-
tion 5). Second, there were ambiguities in POS
sequences caused by Vietnamese characteristics,
such as the adjectival phrase ?(ADJP (R ?ang)
(N ng?y_??m) (A ?au_??n)) {* day-and-night
painful}? and the noun phrase ?(NP (R c?ng) (N
sinh_vi?n) (A gi?i)) {also good student}? that had
the same POS sequence of ?R N A?.
Therefore, POS annotation errors need to be
eliminated from the VTB corpus to further im-
prove its quality and that of the Vietnamese parser.
We not only need to eliminate overlapping but
conflicting instructions, which were mentioned in
Section 5.3, from the guidelines, but we also have
to complete annotation instructions for cases that
have not been treated (or not been clearly treated)
in the guidelines. We may also need to improve
POS tag set because adverbs modifying adjectives,
verbs and nouns are all presently tagged as ?R?,
which caused ambiguous POS sequences, such as
the ambiguous POS sequence ?R N A? mentioned
above. If we use different POS tags for the adverb
??ang?, which modifies the adjective ??au ??n
{painful}?, and the adverb ?c?ng?, which modi-
fies the noun ?sinh vi?n {student}?, we can elimi-
nate ambiguous POS sequences in these cases.
7 Conclusion
We proposed several methods of improving the
quality of the VTB corpus. Our manual evalua-
tion revealed that our methods improved the qual-
ity of the VTB corpus by 6.5% with correct POS
tags. Analysis of inconsistencies and the annota-
tion guidelines suggested that: (1) better instruc-
tions should be added to the VTB guidelines to
help annotators to distinguish difficult POS tags,
(2) overlapping but conflicting instructions should
be eliminated from the VTB guidelines, and (3)
annotations that referred to dictionaries should be
avoided.
To the best of our knowledge, this paper is the
first report on evaluating state-of-the-art parsers
used on the Vietnamese language. The results ob-
tained from evaluating these two parsers were used
as feedback to improve the quality of treebank an-
notations. We also thoroughly analyzed the pars-
ing output, which revealed challenging issues in
treebank annotations and in the Vietnamese pars-
ing problem itself.
26
References
Anna M. D. Sciullo and Edwin Williams. 1987. On
the definition of word. The MIT Press.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0).
Minh Nghiem, Dien Dinh and Mai Nguyen. 2008. Im-
proving Vietnamese POS tagging by integrating a
rich feature set and Support Vector Machines. Pro-
ceedings of RIVF 2008, pages: 128?133.
Phe Hoang. 1998. Vietnamese Dictionary. Scientific
& Technical Publishing.
Phuong H. Le, Azim Roussanaly, Huyen T. M. Nguyen
and Mathias Rossignol. 2010. An empirical study of
maximum entropy approach for part-of-speech tag-
ging of Vietnamese texts. Proceedings of TALN
2010 Conference. Montreal, Canada.
Quy T. Nguyen, Ngan L.T. Nguyen and Yusuke Miyao.
2012. Comparing Different Criteria for Vietnamese
Word Segmentation. Proceedings of 3rd Workshop
on South and Southeast Asian Natural Language
Processing (SANLP), pages: 53?68.
Richard Johansson and Pierre Nugues. 2007. Extended
Constituent-to-dependency Conversion for English.
Proceedings of NODALIDA, Tartu, Estonia, pages:
105?112.
Ryan Mcdonald and Fernando Pereira. 2006a. On-
line Learning of Approximate Dependency Parsing
Algorithms. Proceedings of 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: EACL 2006, pages: 81?88.
Ryan Mcdonald, Kevin Lerman and Fernando Pereira.
2006b. Multilingual Dependency Analysis with
a Two-Stage Discriminative Parser. Proceedings
of Tenth Conference on Computational Natural
Language Learning (CoNLL-X), Bergan, Norway,
pages: 216?220.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. Proceedings of 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages: 433?440.
Thai P. Nguyen, Luong X. Vu and Huyen T.M. Nguyen.
2010a. VTB part-of-speech tagging guidelines.
Thai P. Nguyen, Luong X. Vu and Huyen T.M. Nguyen.
2010b. VTB word segmentation guidelines.
Thai P. Nguyen, Luong X. Vu, Huyen T.M. Nguyen,
Hiep V. Nguyen and Phuong H. Le. 2009. Build-
ing a large syntactically-annotated corpus of Viet-
namese. Proceedings of Third Linguistic Annota-
tion Workshop, pages: 182?185.
Thai P. Nguyen, Luong X. Vu, Huyen T.M. Nguyen,
Thu M. Dao, Ngoc T.M. Dao and Ngan K. Le.
2010c. VTB bracketing guidelines.
Vietnamese grammar. 1983. Social Sciences Publish-
ers.
27
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140?148,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Relation Annotation for Understanding Research Papers
Yuka Tateisi? Yo Shidahara? Yusuke Miyao? Akiko Aizawa?
?National Institute of Informatics, Tokyo, Japan
{yucca,yusuke,aizawa}@nii.ac.jp
?Freelance Annotator
yo.shidahara@gmail.com
Abstract
We describe a new annotation scheme for
formalizing relation structures in research
papers. The scheme has been developed
through the investigation of computer sci-
ence papers. Using the scheme, we are
building a Japanese corpus to help develop
information extraction systems for digital
libraries. We report on the outline of the
annotation scheme and on annotation ex-
periments conducted on research abstracts
from the IPSJ Journal.
1 Introduction
Present day researchers need services for search-
ing research papers. Search engines and pub-
lishing companies provide specialized search ser-
vices, such as Google Scholar, Microsoft Aca-
demic Search, and Science Direct. Academic so-
cieties provide archives of journal articles and/or
conference proceedings such as the ACL Anthol-
ogy. These services focus on simple keyword-
based searches as well as extralinguistic relations
among research papers, authors, and research top-
ics. However, because contemporary research is
becoming increasingly complicated and interre-
lated, intelligent content-based search systems are
desired (Banchs, 2012). A typical query in compu-
tational linguistics could be what tasks have CRFs
been used for?, which includes the elements of
a typical schema for searching research papers;
researchers want to find relationships between a
technique and its applications (Gupta and Man-
ning, 2011). Answers to this query can be found
in various forms in published papers, for example,
(1) CRF-based POS tagging has achieved state-of-
the-art accuracy.
(2) CRFs have been successfully applied to se-
quence labeling problems including POS tagging
and named entity recognition.
(3) We apply feature reduction to CRFs and show
its effectiveness in POS tagging.
(4) This study proposes a new method for the ef-
ficient training of CRFs. The proposed method is
evaluated for POS tagging tasks.
Note that the same semantic relation, i.e., the
use of CRFs for POS tagging, is expressed by var-
ious syntactic constructs: internal structures of the
phrase in (1), clause-level structures in (2), inter-
clause structures in (3), and discourse-level struc-
tures in (4). This implies that an integrated frame-
work is required to represent semantic relations for
phrase-level, clause-level, inter-clause level, and
discourse-level structures. Another interesting fact
is that we can recognize various fragments of in-
formation from single texts. For example, from
sentence (1), we can identify CRF is applied to
POS tagging, state-of-the-art accuracy is achieved
for POS tagging, and CRFs achieve high POS tag-
ging accuracy, all of which is valuable content for
different search requests. This indicates that we
need a framework that can cover (almost) all con-
tent in a text.
In this paper we describe a new annotation
scheme for formalizing typical schemas for repre-
senting relations among concepts in research pa-
pers, such as techniques, resources, and effects.
Our study aims to establish a framework for rep-
resenting the semantics of research papers to help
construct intelligent search systems. In particular,
we focus on the formalization of typical schemas
that we believe exemplify common query charac-
teristics.
From the above observations, we have de-
veloped the following criteria for our proposed
framework: use the same scheme for annotating
contents in all levels of linguistic structures, an-
notate (almost) all contents presented in texts, and
capture relations necessary for surveying research
papers. We investigated 71 computer science ab-
stracts (498 sentences) and defined an annotation
140
scheme comprising 16 types of semantic relations.
Computer science is particularly suitable for our
purpose because it is primarily concerned with ab-
stract concepts rather than concrete entities, which
are typically the primary focus of empirical sci-
ences such as physics and biology. In addition,
computer and computational methods can be ap-
plied to an extraordinarily wide range of top-
ics; computer science papers might discuss a bus
timetable (for automatic optimization), a person?s
palm (as a device for projecting images), or look-
ing over another person!Gs shoulder (to obtain pass-
words). Therefore, to annotate all computer sci-
ence papers, we cannot develop predefined entity
ontologies, which is the typical approach taken in
biomedical text mining (Kim et al, 2011).
However, most computer science papers have
characteristic schemata: the papers describe a
problem, postulate a method, apply the method to
the problem using particular data or devices, and
perform experiments to evaluate the method. The
typical schemata clearly represent the structure of
interests in this research field. Therefore, we can
focus on typical schemata, such as application of
a method to a problem and evaluation of a method
for a task. As we will demonstrate in this paper,
the proposed annotation scheme can cover almost
all content, from phrase levels to discourse levels,
in computer science papers.
Note that this does not necessarily mean that our
framework can only be applied to computer sci-
ence literature. The characteristics of the schemata
described above are universal in contemporary sci-
ence and engineering, and many other activities in
human society. Thus, the framework presented in
this study can be viewed as a starting point for re-
search focusing on representative schemata of hu-
man activities.
2 Related Work
Traditionally, research on searching research pa-
pers has focused more on the social aspects of
papers and their authors, such as citation links
and co-authorship analysis implemented in the
aforementioned services. Recently, research on
content-based analysis of research papers has been
emerging.
For example, methods of document zoning have
been proposed for research papers in biomedicine
(Mizuta et al, 2006; Agarwal and Yu, 2009; Li-
akata et al, 2010; Guo et al, 2011; Varga et
al., 2012), and chemistry and computational lin-
guistics (Teufel et al, 2009). Zoning provides
a sentence-based information structure of papers
to help identify the components such as the pro-
posed method and the results obtained in the study.
As such, zoning can narrow down the sections of
a paper in which the answer to a query can be
found. However, zoning alone cannot always cap-
ture the relation between the concepts described in
the sections as it focuses on relation at a sentence
level. For example, the examples (1), (2), (3) in the
previous section require intra-sentence analysis to
capture the relation between CRF and POS tag-
ging. Our annotation scheme, which can be seen
as conplementary to zoning, attempts to provide
a structure for capturing the relationship between
concepts at a finer-grained level than a sentence.
Establishing semantic relations among scien-
tific papers has also been studied. For example,
the ACL Anthology Searchbench (Scha?fer et al,
2011) provides querying by predicate-argument
relations. The system accepts specifications of
subject, predicate, and object, and searches for
texts that semantically match the query using the
results from an HPSG parser. It can also search
by topics automatically extracted from the papers.
Gupta and Manning (2011) proposed a method for
extracting Focus, Domain, and Technique from pa-
pers in the ACL anthology: Focus is a research
article?s main contribution, Domain is an applica-
tion domain, and Technique is a method or a tool
used to achieve the Focus. The change in these as-
pects over time is traced to measure the influence
of research communities on each other. Fukuda et
al. (2012) developed a method of technical trend
analysis that can be applied to both patent appli-
cations and academic papers, using the distribu-
tion of named entities. However, as processes and
functions are key concepts in computer science,
elements are often described in a unit with its own
internal structures which include data, systems,
and other entities as substructures. Thus, tech-
nical concepts such as technique cannot be cap-
tured fully by extracting named entities. Gupta
and Manning (2011) analyzed the internal struc-
tures of concepts syntactically using a dependency
parser, but did not further investigate the structure
semantically.
In addition to the methodological aspects of re-
search, i.e., what techniques are applied to what
domain, a research paper can include other infor-
141
mation that we also want to capture, such as how
the author evaluates current systems and methods
or the previous efforts of others. An attempt to
identify the evaluation and other meta-aspects of
scientific papers was made by Thompson et al
(2011), which, on top of the biomedical events
annotated in the GENIA event corpus (Kim et
al., 2008), annotated meta-knowledge such as the
certainty level of the author, polarity (positive?
negative), and manner (strong?weak) of events, as
well as source (whether the event is attributed to
the current study or previous studies), along with
the clue mentioned in the text. For in-domain
relations within and between the events, they re-
lied on the underlying GENIA annotation, which
maps events and their participants to a subset of
Gene Ontology (The Gene Ontology Consortium,
2000), a standard ontology in genome science.
We cannot assume the existence of standard do-
main ontology in the variety of domains to which
computer systems are applied, as was mentioned
in Section 1. On the other hand, using domain-
general linguistic frameworks, such as FrameNet
(Ruppenhofer et al, 2006) or the Lexical Concep-
tual Structure (Jackendoff, 1990) is also not sat-
isfactory for our purpose. These frameworks at-
tempt to identify the relations lexicalized by verbs
and their case arguments; however, they do not
consider discourse or other levels of linguistic rep-
resentation. In addition, relying on a linguistic the-
ory requires that annotators understand linguistics.
Most computer scientists, the best candidates for
performing the annotation task, would not have the
necessary knowledge of linguistics and would re-
quire training, which would increase costs for cor-
pus annotation.
3 Annotation Scheme
The principle is to employ a uniform structure to
represent semantic relations in scientific papers
in phrase-level, clause-level, inter-clause level,
and discourse-level structures. For this purpose,
a bottom-up strategy that identifies relations be-
tween the entities mentioned is used. This strat-
egy is similar to dependency parsing/annotation,
which identifies the relations between constituents
to find the overall structure of sentences.
We did not want the relations to be uncondi-
tionally concrete and domain-specific, because, as
mentioned in the previous section, new concepts
and relations that may not be expressed by pre-
In this paper, we propose a novel strategy for
parallel preconditioning of large scale linear
systems by means of a two-level approximate
inverse technique with AISM method. Accord-
ing to the numerical results on an origin 2400 by
using MPI, the proposed parallel technique of
computing the approximate inverse makes the
speedup of about 136.72 times with 16 proces-
sors.
Figure 1: Sample Abstract
defined (concrete, domain-specific) concepts and
relations may be created. For the same reason,
we did not set specific entity types on the basis of
domain ontology. We simply classified entities as
?general object,? ?specific object,? and ?measure-
ment.?
To illustrate our scheme, consider the two-
sentence abstract1 shown in Figure 12.
In the first sentence, we can read that a method
called two-level approximate inverse is used for
parallel preconditioning (1), the preconditioning
is applied to large-scale linear systems, the AISM
method is a subcomponent or a substage of the
two-level technique, and the author claims that the
use of two-level approximate inverse is a novel
strategy.
In the second sentence, we can read that the
author has conducted a numerical experiment,
the experiment was conducted on an origin 2400
(a computer system), message Passing Interface
(MPI, a standardized method for message passing)
was used in the experiment, the proposed parallel
technique was 136.72 times quicker than existing
methods, and the speedup was achieved using 16
processors.
In addition, by comparing the two sentences, we
can determine that the proposed parallel technique
in the second sentence refers to the parallel pre-
conditioning using two-level approximate inverse
mentioned in the first sentence. Consequently, we
can infer the author?s claim that the parallel pre-
conditioning using two-level approximate inverse
achieved 136.72 times speedup.
We define binary relations including
APPLY TO(A, B) (A method A is applied
to achieve the purpose B or used for do-
ing B), EVALUATE(A, B) (A is evaluated as
1Linjie Zhang, Kentaro Moriya and Takashi Nodera.
2008. Two-level Parallel Computation for Approximate In-
verse with AISM Method. IPSJ Journal, 48 (6): 2164-2168.
2Although the annotation was done for abstracts in
Japanese, we present examples in English except where we
discuss issues that we believe are specific to Japanese.
142
APPLY TO(two-level approximate inverse, parallel preconditioning)
APPLY TO(parallel preconditioning, large scale linear systems)
SUBCONCEPT(AISM method, two-level approximate inverse)
EVALUATE(two-level approximate inverse, novel)
RESULT(numerical results, 136.72 times speedup)
CONDITION(origin 2400, 136.72 times speedup)
APPLY TO(MPI, numerical results)
EVALUATE(the proposed parallel technique, 136.72 times speedup)
CONDITION(16 processors, 136.72 times speedup)
EQUIVALENCE(the proposed parallel technique, two-level approximate inverse)
Figure 2: Relations Found in the Sentences in Figure 1
B), SUBCONCEPT(A, B) (A is a part of B),
RESULT(A, B) (The result of experiment A is B),
CONDITION(A, B) (The condition A holds in
situation B), and EQUIVALENCE(A, B) (A and
B refer to the same entity), with which we can
express the relations mentioned in the example, as
shown in Figure 2.
Note that it is the use of two-level approximate
inverse for parallel preconditioning(A) that the au-
thor claims to be novel. However, the relation in A
is already represented by the first APPLY TO rela-
tion. Consequently, it is sufficient to annotate the
EVALUATE relation between two-level approxi-
mate inverse and novel. This is approximately
equivalent to paraphrasing the use of two-level ap-
proximate inverse for parallel preconditioning is
novel as two-level approximate inverse used for
parallel preconditioning is novel. The same holds
for the equivalence relation involving the proposed
method.
Expressing the content as the set of relations fa-
cilitates discovery of a concept that plays a par-
ticular role in the work. For example, if a reader
wants to know the method for achieving paral-
lel preconditioning, X, which satisfies the relation
APPLY TO(X, parallel preconditioning) must be
searched for. By using the APPLY TO relations
mentioned in Figure 2 and inference on an is-a re-
lation expressed by the SUBCONCEPT, we can ob-
tain the result that AISM method is used for paral-
lel preconditioning.
After a series of trial annotations on 71 abstracts
from the IPSJ Journal (a monthly peer-reviewed
journal published by the Information Processing
Society of Japan), the following tag set was fixed.
The annotation was conducted by the two of the
authors of this paper.
3.1 Entity and Relation Types
The current tag set has 16 relation types and three
entity types. An entity is whatever can be an argu-
Type Definition Example
OBJECT the name of concrete entities such as
a system, a person, and a company
Origin
2400, SGI
MEASURE value, measurement, necessity, obli-
gation, expectation, and possibility
novel,
136.72
TERM any other
Table 1: Entity Tags
ment or a participant in a relation. Entity types
are OBJECT, MEASURE, or TERM, as shown in
Table 1. Note that, unlike most schemes where
the term entity refers to a nominal (named entity),
in our scheme, almost all syntactic types of con-
tent words can be an entity, including numbers,
verbs, adjectives, adverbs, and even some auxil-
iaries. The 16 types of relations are shown in Ta-
ble 2. They are binary relations are directed from
A to B.
All relations except EVALUATE COMPARE, and
ATTRIBUTE can hold between any types of en-
tity. EVALUATE and COMPARE relations hold
between an entity (of any type) and an entity
of the MEASURE type. The entities involved
in an ATTRIBUTE relation must not be of the
MEASURE type.
The INPUT and OUTPUT relations were intro-
duced to deal with the distinction between the data
and method used in computer systems. We ex-
tend the use of the scheme to annotate the in-
ner structure of sentences and predicates, by es-
tablishing the relations between verbs and their
case elements. For example, in automatically
generated test data, obviously test data is an
output of the action of generate, and automati-
cally is the manner of generation. We annotate
the test data as an OUTPUT and automatically
as an ATTRIBUTE of generate. In another ex-
ample, a protocol that combines biometrics and
zero-knowledge proof, the protocol is the product
of an action of combining biometrics and zero-
143
Type Definition Example
APPLY TO(A, B) A method A is applied to achieve the purpose B or used for
conducting B
CRFA-based taggerB
RESULT(A, B) A results in B in the sense that B is either an experimental
result, a logical conclusion, or a side effect of A
experimentA shows the increaseB in F-
score compared to the baseline
PERFORM(A, B) A is the agent of an intentional action B a frustrated playerA of a gameB
INPUT(A, B) A is the input of a system or a process B, A is something
obtained for B
corpusA for trainingB
OUTPUT(A, B) A is the output of a system or a processB, A is something
generated from B
an imagea displayedB on a palm
TARGET(A, B) Ais the target of an action B, which does not suffer alteration to driveB a busA
ORIGIN(A, B) A is the starting point of action B to driveB from ShinjukuA
DESTINATION(A, B) A is the ending point of action B an image displayedB on a palmA
CONDITION(A, B) The condition A holds in situation B, e.g, time, location, ex-
perimental condition
a surveyB conducted in Indiaa
ATTRIBUTE(A, B) A is an attribute or a characteristic of B accuracyA of the taggerB
STATE(A, B) A is the sentiment of a person B other than the author, e.g. a
user of a computer system or a player of a game
a frustratedA playerB of a game
EVALUATE(A, B) A is evaluated as B in comparison to C experiment shows an increaseB
COMPARE(C, B) in F?scoreA compared to the baselineC
SUBCONCEPT(A, B) A is-a, or is a part-of B a corpusA such as PTBa
EQUIVALENCE(A, B) terms A and B refer to the same entity: definition, abbrevia-
tion, or coreference
DoSB (denial ? of ? serviceA) attack
SPLIT(A, B) a term is split by parenthesical expressions into A and B DoSB (denial-of-service) attackA
Table 2: Relation Tags
knowledge proof. Therefore, both biometrics and
zero-knowledge proof are annotated as INPUTs of
combines, and protocol is annotated as OUTPUT
of combines. This scheme is not only used for
computer-related verbs, but is further extended
to any verb phrases or phrases with nominalized
verbs. In change in a situation, situation is an-
notated as both INPUT and OUTPUT of change.
It is as if we regard change as a machine that
changes something, and when we input a situa-
tion, the change-machine processes it and output
a different situation. Similarly, in evolution of mo-
bile phones, mobile phones is annotated as both
INPUT and OUTPUT of evolution. Here we re-
gard evolution as a machine, and when we input
(old-style) mobile phones, the evolution-machine
processes them and outputs (new-style) mobile
phones. We have found that a wide variety of pred-
icates can be interpreted using these relations.
3.2 Other Features
Although we aim to annotate all possible relations
mentioned, some conventions are introduced to re-
duce the workload.
First, we do not annotate the structure within
entities. No nested entities are allowed, and com-
pound words are treated as a single word. In ad-
dition, polarity (negation) is not expressed as a re-
lation but as a part of an entity. We assume that
the internal structure of entities can be analyzed
by mechanisms such as technical term recognition.
On the other hand, nested and crossed relations are
allowed.
Second, we do not annotate words that indicate
the existence of relations. This is because the re-
lations are usually indicated by case markers and
punctuation 3 and marking them up was found to
be a considerable mental workload. In addition,
words and phrases that directly represent the re-
lations themselves are not annotated as entities.
For example, in CG iteration was applied to the
problem, we directly CG relation and the problem
directly with APPLY TO and skip the phrase was
applied to.
Third, relations other than EQUIVALENCE and
SUBCONCEPT are annotated within a sentence.
We assume that the discourse-level relation can be
inferred by the composition of relations.
In addition, the annotation of frequent verbs and
their case elements was examined in the trial pro-
cess. Verbs were classified, according to the pat-
tern of the annotated relation with the case ele-
ments. For example, verbs semantically similar to
assemble and compile form a class. The semantic
role of the direct object of these verbs varies by
context. For example, the materials in phrases like
compile source codes or the product in phrases like
3This is in the case with Japanese. In languages such as
English, there may be no trigger words, as the semantic rela-
tions are often expressed by the structure of sentences.
144
compile the driver from the source codes. In our
scheme, the former is the INPUT of the verb, and
the latter is the OUTPUT of the verb. Another ex-
ample is the class of verbs that includes learn and
obtain. The direct object (what is learned) is the
INPUT to the system but is also the result or an
output of the learning process. In such cases, we
decided that both INPUT and OUTPUT should be
annotated between the verb and its object.
Other details of annotation fixed in the process
of trial annotation include:
1) The span of entities, which is determined to be
the longest possible sequences delimited by case
suffix (-ga,-wo, etc.) in the case of nominals and to
separate the -suru suffix of verbs and the -da suffix
of adjectives but retain other conjugation suffixes;
2) How to annotate evaluation sentences involv-
ing nouns derived from adjectives that imply eval-
uation and measurement, such as necessity, diffi-
culty, and length. The initial agreement was that
we would consider that they lose MEASURE-ness
when nominalized; however, with the similarity of
Japanese expressions hitsuyou/mondai de aru (is
necessary/problematic) and hitsuyou/mondai ga
aru(there is a necessity/problem), there was con-
fusion about which word should be the MEASURE
argument necessary for the EVALUATE relation.
It was determined that, for example, in hit-
suyou/mondai de aru, de aru, a copula, is ig-
nored and hitsuyou/mondai is the MEASURE. In
hitsuyou/mondai ga aru, aru is the MEASURE;
3) How to annotate phrases like the tagger was
better in precision, where it can be understood that
the system is evaluated as being better in precision.
While what is actually measured in the evaluation
process described in the paper is the precision (an
attribute) of the tagger and the sentence has almost
the same meaning as the tagger?s precision was
better, the surface (syntactic) subject of is better
is the tagger. This can lead to two possibilities
for the target of the EVALUATE relation. We de-
cided that the EVALUATE relation holds between
precision and better, and the ATTRIBUTE relation
holds between precision and tagger, as illustrated
in Figure 3.
A set of annotation guidelines was compiled as
the result of the trial annotation, including the clas-
sifications and the pattern of annotation on fre-
quent verbs and their arguments.
Figure 3: Annotation of the tagger was better in
precision
Entity Relation
Conunt % Conunt %
Total 1895 100.0 Total 2269 100.0
OK 1658 87.5 OK 1110 48.9
Type 56 3.0 Type 250 11.0
Span 67 3.5 Direction 6 0.3
Direction+Type 106 4.7
None 114 6.0 None 797 35.1
Table 3: Tag Counts
4 Annotation Experiment
We conducted an experiment on another 30 ab-
stracts (197 sentences) from the IPSJ Journal. The
two annotators who participated in the develop-
ment of the guidelines annotated the abstracts in-
dependently, and inter-annotator discrepancy was
checked. The annotation was performed man-
ually using the brat annotation tool(Stenetorp et
al., 2012). No automatic preprocessing was per-
formed. Figure 4 shows the annotation results for
the abstract shown in Figure 1. The 30 pairs of an-
notation results were aligned automatically; The
results are shown in Tables 3, 4, and 5.
Table 3 shows the matches between the two
annotators. ?Total? denotes the count of enti-
ties/relations that at least one annotator found,
?OK? denotes complete matches, ?Type? denotes
cases where two annotations on the same span
have different entity/relation types, ?Span? de-
notes entities where two annotations partially
overlap, ?Direction? denotes the count of relations
where (only) the direction is different, and ?Direc-
tion+Type?denotes relations where the same pair
of entities were in different types of relation and
in opposite directions, and ?None? denotes cases
where no counterpart was found in the other re-
sult.
Tables 4 and 5 are the confusion matrices for
entity type and relation type, respectively. The
differences in the span and direction are ignored.
Agreement in F-score calculated in the same man-
ner as in Brants (2000) for each relation is shown
in column F, with the overall (micro-average) F-
score shown in the bottom row of column F.
If we assume the number of cases that none of
145
Figure 4: Annotation Results with brat
TERM OBJECT MEASURE NONE Total F(%)
TERM 1458 2 38 14 1512 94.9
OBJECT 0 17 0 0 17 94.4
MEASURE 28 0 238 18 284 83.8
None 74 0 8 X 82
Total 1560 19 284 32 93.0
Table 4: Confusion Matrix for Entity
the annotators recognized (the value of the cell
X in the tables) to be zero, the observed agree-
ment and Cohen?s ? coefficient are 90.3% and
70.0% for entities, and 49.3% and 43.5% for re-
lations, respectively. If we ignore the count for the
cases where one annotator did not recognize the
entity/relation (?None? rows and columns in the
tables), the observed agreement and ? are 96.1%
and 89.3% for entities, and 76.1% and 74.3% for
relations, respectively. The latter statistics indi-
cate the agreement on types for entities/relations
that both annotators recognized.
These results show that entity annotation was
consistent between the annotators but the agree-
ment for relation annotation varied, depending on
the relation type. Table 5 shows that agreement
for DESTINATION, ORIGIN, EVALUATE, and
SPLIT was reasonably high, but was low for
CONDITION and TARGET. The rise in agreement
(simple and ?) by excluding cases where only one
annotator recognized the relation indicate that the
problem is recognition, rather than classification,
of relations4.
From the investigation of the annotated text, the
following was found:
(1) ATTRIBUTE/CONDITION decision was in-
consistent in phrases involving EVALUATE rela-
tion, such as the disk space is smaller for the im-
age (Figure 5). The EVALUATE relation between
the disk space and smaller was agreed; however,
the two annotators recognized different relations
between the image and other words. One annota-
4The same observation was true for entities
tor recognized the ATTRIBUTE relation between
the disk space and the image (?the disk space as a
feature of the image is smaller?). The other recog-
nized the CONDITION relation between the image
and smaller (?the disk space is smaller in the case
of the image?).
(2) We were not in complete agreement about
skipping phrases that directly represent a relation.
The expressions to be skipped in the 71 trial ab-
stracts were listed in the guidelines; however, it is
difficult to exhaust all such expressions.
(3) In the case of some verbs, an argument can
be INPUT and OUTPUT simultaneously (Section
3.1). We agreed that an object that undergoes alter-
ation in a process should be tagged as both INPUT
and OUTPUT but one that does not undergo al-
teration or which is just moved is the TARGET.
Conflicts occurred for verbs that denote preven-
tion of some situations such as prevent, avoid, and
suppress, as illustrated in Figure 6. One annota-
tor claimed that the possibility of DoS attacks is
reduced to zero; hence the argument of the verb
should be annotated with INPUT and OUTPUT.
The other claims that since the DoS attack itself
does not change, it is a TARGET.
(4) In a coordination expression, logical inference
may be implicitly stated. For example, in it re-
quires the linguistic knowledge and is costly, the
reason for costly is likely to be the need for lin-
guistic knowledge, i.e., employment of an expert
linguist. However, the relation is not readily ap-
parent. We wanted to capture the relation in such
cases, but the disagreement shows that it is diffi-
cult to judge such a relation consistently.
(5) The decision on whether to split expressions
like XX dekiru and XX kanou (can/able to XX) was
also problematic. The guideline was to split them.
This contradicts the decision for the compound
words in general that we do not split them; how-
ever, we determined that dekiru/kanou cases had
146
APP ATT COMP COND DEST EQU EVAL IN ORIG OUT PER RES SPL STA SUB TAR None Total F(%)
APPLY TO 136 9 0 2 1 1 2 10 1 0 0 3 0 0 1 0 65 231 53.0
ATTRIBUTE 14 154 0 19 6 0 9 5 1 0 7 1 0 0 3 0 28 247 59.7
COMPARE 0 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 4 11 54.5
CONDITION 4 11 1 77 0 0 1 4 0 0 0 5 0 0 0 0 49 152 48.7
DESTINATION 6 0 0 0 39 0 0 0 0 1 0 0 0 0 0 0 4 50 77.2
EQUIVALENCE 4 1 0 1 0 54 0 0 0 0 0 0 0 0 4 0 23 87 60.0
EVALUATE 0 11 0 0 0 0 215 3 0 9 0 0 0 0 0 1 41 280 76.1
INPUT 12 2 0 0 0 1 4 96 0 11 0 0 0 0 0 9 15 150 58.7
ORIGIN 0 0 0 0 0 0 0 0 16 0 0 0 0 0 0 0 2 18 78.0
OUTPUT 2 1 0 3 0 0 4 23 0 141 0 0 0 0 0 18 37 229 56.5
PERFORM 1 0 0 0 0 0 0 0 0 0 19 0 0 0 0 0 2 22 74.5
RESULT 8 1 0 0 0 0 1 1 0 0 0 38 0 0 0 0 22 71 54.3
SPLIT 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 80.0
STATE 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
SUBCONCEPT 14 10 0 3 0 4 5 0 0 2 0 0 0 0 81 0 34 153 58.1
TARGET 6 2 1 3 2 0 7 12 0 14 1 0 0 0 0 42 6 96 47.7
None 75 67 3 55 3 33 37 23 5 92 2 22 1 0 37 10 X 465
Total 282 269 11 164 51 93 285 177 23 270 29 69 3 0 126 80 332 59.8
Table 5: Confusion Matrix for Relation
Figure 5: ATTRIBUTE/CONDITION Disagreement
Figure 6: INPUT/OUTPUT/TARGET Disagreement
to be exceptions because the possibility of XX is
expressed by dekiru/kanou and it seemed natural
to relate XX and dekiru/kanou with EVALUATE.
Unfortunately, confusion about splitting them re-
mains.
5 Conclusions
We set up a scheme to annotate the content of re-
search papers comprehensively. Sixteen semantic
relations were defined, and guidelines for anno-
tating semantic relations between concepts using
the relations were established. The experimen-
tal results on 30 abstracts show that fairly good
agreement was achieved, and that while entity-
and relation-type determination can be performed
consistently, determining whether a relation exists
between particular pairs of entities remains prob-
lematic. We also found several discrepancy pat-
terns that should be resolved and included in a fu-
ture revision of the guidelines.
Traditionally, in semantic annotation of texts
in the science/engineering domains, corpus cre-
ators focus on specific types of entities or events
in which they are interested. On the other hand,
we did not assume such specific types of entities
or events, and we attempted to design a scheme
that annotates more general relations in computer
science/engineering domain.
Although the annotation is conducted for com-
puter science abstracts in Japanese, we believe the
scheme can be used for other languages, or for
the broader science/engineering domains. The an-
notated corpus can provide data for constructing
comprehensive semantic relation extraction sys-
tems. This would be challenging but worthwhile
since such systems are in great demand. Such
relation extraction systems will be the basis for
content-based retrieval and other applications, in-
cluding paraphrasing and translation.
The abstracts annotated in the course of the ex-
periment have been cleaned up and are available
on request. We are planning to increase the vol-
ume and make the corpus widely available.
In the future, we will assess machine-learning
performance and incorporate the relation extrac-
tion mechanisms into search systems. Comparison
of the annotated structure and the structures that
can be given by existing semantic theories could
be an interesting theoretical subject for future re-
search.
Acknowledgments
This study was partially supported by the Japan
Ministry of Education, Culture, Sports, Science
and Technology Grant-in-Aid for Scientific Re-
search (B) No. 22300031.
147
References
Shashank Agarwal and Hong Yu. 2009. Automatically
classifying sentences in full-text biomedical articles
into introduction, methods, results and discussion.
Bioinformatics, 25(23):3174?3180.
Rafael E. Banchs, editor. 2012. Proceedings of the
ACL-2012 Special Workshop on Rediscovering 50
Years of Discoveries. Association for Computational
Linguistics.
Thorsten Brants. 2000. Inter-annotator agreement for
a German newspaper corpus. In Proceedings of the
Second International Conference on Language Re-
sources and Evaluation.
Satoshi Fukuda, Hidetsugu Nanba, and Toshiyuki
Takezawa. 2012. Extraction and visualization of
technical trend information from research papers
and patents. In Proceedings of the 1st International
Workshop on Mining Scientific Publications.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 273?283.
Sonal Gupta and Christopher D Manning. 2011. An-
alyzing the dynamics of research by extracting key
aspects of scientific papers. In Proceedings of 5th
IJCNLP.
Ray Jackendoff. 1990. Semantic Structures. The MIT
Press.
Jin-Dong Kim, Tomoko Ohta, and Jun ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In Proceed-
ings of BioNLP Shared Task 2011 Workshop, pages
1?6.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC 2010.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468?487.
Josef Ruppenhofer, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Ulrich Scha?fer, Bernd Kiefer, Christian Spurk, Jo?rg
Steffen, and Rui Wang. 2011. The ACL anthology
searchbench. In Proceedings of the ACL-HLT 2011
System Demonstrations, pages 7?13.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 1493?1502.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature Ge-
netics, 25(1):25?29.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12.
Andrea Varga, Daniel Preotiuc-Pietro, and Fabio
Ciravegna. 2012. Unsupervised document zone
identification using probabilistic graphical models.
In Proceedings of LREC 2012, pages 1610?1617.
148
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 25?33,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using Unlabeled Dependency Parsing for Pre-reordering
for Chinese-to-Japanese Statistical Machine Translation
Dan Han1,2 Pascual Mart??nez-Go?mez2,3 Yusuke Miyao1,2
Katsuhito Sudoh4 Masaaki Nagata4
1The Graduate University For Advanced Studies
2National Institute of Informatics, 3The University of Tokyo
4NTT Communication Science Laboratories, NTT Corporation
{handan,pascual,yusuke}@nii.ac.jp
{sudoh.katsuhito,nagata.masaaki}@lab.ntt.co.jp
Abstract
Chinese and Japanese have a different sen-
tence structure. Reordering methods are
effective, but need reliable parsers to ex-
tract the syntactic structure of the source
sentences. However, Chinese has a loose
word order, and Chinese parsers that ex-
tract the phrase structure do not perform
well. We propose a framework where only
POS tags and unlabeled dependency parse
trees are necessary, and linguistic knowl-
edge on structural difference can be en-
coded in the form of reordering rules. We
show significant improvements in transla-
tion quality of sentences from news do-
main, when compared to state-of-the-art
reordering methods.
1 Introduction
Translation between Chinese and Japanese lan-
guages gains interest as their economic and polit-
ical relationship intensifies. Despite their linguis-
tic influences, these languages have different syn-
tactic structures and phrase-based statistical ma-
chine translation (SMT) systems do not perform
well. Current word alignment models (Och and
Ney, 2003) account for local differences in word
order between bilingual sentences, but fail at cap-
turing long distance word alignments. One of
the main problems in the search of the best word
alignment is the combinatorial explosion of word
orders, but linguistically-motivated heuristics can
help to guide the search.
This work explores syntax-informed pre-
reordering for Chinese; that is, we obtain syntactic
structures of Chinese sentences, reorder the words
to resemble the Japanese word order, and then
translate the reordered sentences using a phrase-
based SMT system. However, Chinese parsers
have difficulties in extracting reliable syntactic in-
formation, mainly because Chinese has a loose
word order and few syntactic clues such as inflec-
tion and function words.
On one hand, parsers implementing head-driven
phrase structure grammars infer a detailed con-
stituent structure, and such a rich syntactic struc-
ture can be exploited to design well informed re-
ordering methods. However, inferring abundant
syntactic information often implies introducing er-
rors, and reordering methods that heavily rely on
detailed information are sensitive to those parsing
errors (Han et al, 2012).
On the other hand, dependency parsers are com-
mitted to the simpler task of finding dependency
relations and dependency labels, which can also be
useful to guide reordering (Xu et al, 2009). How-
ever, reordering methods that rely on those depen-
dency labels will also be prone to errors, specially
in the case of Chinese since it has a richer set of
dependency labels when compared to other lan-
guages. Since improving parsers for Chinese is
challenging, we thus aim at reducing the influence
of parsing errors in the reordering procedure.
We present a hybrid approach that boosts the
performance of phrase-based SMT systems by
pre-reordering the source language using unla-
beled parse trees augmented with constituent
information derived from Part-of-Speech tags.
Specifically, we propose a framework to pre-
reorder a Subject-Verb-Object (SVO) language,
in order to improve its translation to a Subject-
Object-Verb (SOV) language, where the only re-
quired syntactic information are POS tags and un-
labeled dependency parse trees. We test the per-
formance of our pre-reordering method and com-
pare it to state-of-the-art reordering methods in the
news domain for Chinese.
In the next section, we describe similar work on
pre-reordering methods for language pairs that in-
25
volve either Chinese or Japanese, and explain how
our method builds upon them. From a linguis-
tic perspective, we describe in section 3 our ob-
servations of reordering issues between Chinese
and Japanese and detail how our framework solves
those issues. In section 4 we assess to what extent
our pre-reordering method succeeds in reordering
words in Chinese sentences to resemble the order
of Japanese sentences, and measure its impact on
translation quality. The last section is dedicated to
discuss our findings and point to future directions.
2 Related Work
Although there are many works on pre-reordering
methods for other languages to English translation
or inverse (Xia and McCord, 2004; Xu et al, 2009;
Habash, 2007; Wang et al, 2007; Li et al, 2007;
Wu et al, 2011), reordering method for Chinese-
to-Japanese translation, which is a representative
of long distance language pairs, has received little
attention.
The most related work to ours is in (Han et al,
2012), in which the authors introduced a refined
reordering approach by importing an existing re-
ordering method for English proposed in (Isozaki
et al, 2010b). These reordering strategies are
based on Head-driven phrase structure grammars
(HPSG) (Pollard and Sag, 1994), in that the re-
ordering decisions are made based on the head of
phrases. Specifically, HPSG parsers (Miyao and
Tsujii, 2008; Yu et al, 2011) are used to extract the
structure of sentences in the form of binary trees,
and head branches are swapped with their depen-
dents according to certain heuristics to resemble
the word order of the target language. However,
those strategies are sensitive to parsing errors, and
the binary structure of their parse trees impose
hard constraints in sentences with loose word or-
der. Moreover, as Han et al (2012) noted, reorder-
ing strategies that are derived from the HPSG the-
ory may not perform well when the head definition
is inconsistent in the language pair under study. A
typical example for the language pair of Chinese
and Japanese that illustrates this phenomenon is
the adverb ?bu4?, which is the dependent of its
verb in Chinese but the head in Japanese.
The work in (Xu et al, 2009) used an English
dependency parser and formulated handcrafted re-
ordering rules with dependency labels, POS tags
and weights as triplets and implemented them re-
cursively into sentences. This design, however,
limited the extensibility of their method. Our ap-
proach follows the idea of using dependency tree
structures and POS tags, but we discard the infor-
mation on dependency labels since we did not find
them informative to guide our reordering strate-
gies in our preliminary experiments, partly due to
Chinese showing less dependencies and a larger
label variability (Chang et al, 2009).
3 Methodology
In Subject-Verb-Object (SVO) languages, objects
usually follow their verbs, while in Subject-
Object-Verb (SOV) languages, objects precede
them. Our objective is to reorder words in Chinese
sentences (SVO) to resemble the word order of
Japanese sentences (SOV). For that purpose, our
method consists in moving verbs to the right-hand
side of their objects. However, it is challenging
to correctly identify the appropriate verbs and ob-
jects that trigger a reordering, and this section will
be dedicated to that end.
More specifically, the first step of our method
consists in identifying the appropriate verb (and
certain words close to it) that need to be moved to
the right-hand side of its object argument. Verbs
(and those accompanying words) will move as a
block, preserving the relative order among them.
We will refer to them as verbal blocks (Vbs). The
second step will consist in identifying the right-
most argument object of the verb under considera-
tion, and moving the verbal block to the right-hand
side of it. Finally, certain invariable grammatical
particles in the original vicinity of the verb will
also be reordered, but their positions will be de-
cided relative to their verb.
In what follows, we describe in detail how to
identify verbal blocks, their objects and the invari-
able grammatical particles that will play a role in
our reordering method. As mentioned earlier, the
only information that will be used to perform this
task will be the POS tags of the words and their
unlabeled dependency structures.
3.1 Identifying verbal blocks (Vbs)
Verbal blocks are composed of a head (Vb-H)
and possibly accompanying dependents (Vb-D).
In the Chinese sentence ?wo3 (I) chi1 le5 (ate) li2
(pear).?1, ?chi1? refers to the English verb ?eat?
1In this paper, we represent a Chinese character by using
Pinyin plus a tone number (there are 5 tones in Chinese). In
the example, ?chi1(eat)? is a verb and ?le5(-ed)? is an aspect
particle that adds preterit tense to the verb.
26
Vb-H VV VE VC VA P
Vb-D AD AS SP MSP CC VV VE VC VA
BEI LB SB
RM-D NN NR NT PN OD CD M FW CC
ETC LC DEV DT JJ SP IJ ON
Oth-DEP LB SB CS
Table 1: Lists of POS tags in Chinese used to iden-
tify blocks of words to reorder (Vb-H, Vb-D, BEI
lists), the POS tags of their dependents (RM-D
lists) which indicate the reordering position, and
invariable grammatical particles (Oth-DEP) that
need to be reordered.
and the aspect particle ?le5? adds a preterit tense
to the verb. The words ?chi1 le5? are an example
of verbal block that should be reordered as a block
without altering its inner word order, i.e. ?wo3
(I) li2 (pear) chi1 le5 (ate).?, which matches the
Japanese SOV order.
Possible heads of verbal blocks (Vb-H) are
verbs (words with POS tags VV, VE, VC and VA),
or prepositions (words with POS tag P). The Vb-H
entry of Table 1 contains the list of POS tags for
heads of verbal blocks. We use prepositions for
Vb-H identification since they behave similarly to
verbs in Chinese and should be moved to the right-
most position in a prepositional phrase to resemble
the Japanese word order. There are three condi-
tions that a word should meet to be considered as
a Vb-H:
i) Its POS tag is in the set of Vb-H in Table 1.
ii) It is a dependency head, which indicates that
it may have an object as a dependent.
iii) It has no dependent whose POS tag is in the
set of BEI in Table 1. BEI particles indicate
that the verb is in passive voice and should
not be reordered since it already resembles
the Japanese order.
Chinese language does not have inflection, con-
jugation, or case markers (Li and Thompson,
1989). For that reason, some adverbs (AD), as-
pect particles (AS) or sentence-final particles (SP)
are used to signal modality, indicate grammati-
cal tense or add aspectual value to verbs. Words
in this category preserve the order when translat-
ing to Japanese, and they will be candidates to be
part of the verbal block (Vb-D) and accompany
the verb when it is reordered. Other words in this
category are coordinating conjunctions (CC) that
connect multiple verbs, and both resultative ?de5?
(DER) and manner ?de5? (DEV). The full list of
POS tags used to identify Vb-Ds can be found in
Table 1. To be a Vb-D, there are three necessary
conditions as well:
i) Its POS tag is in the Vb-D entry in Table 1.
ii) It is a dependent of a word that is already in
the Vb.
iii) It is next to its dependency head or only a
coordination conjunction is in between.
To summarize, to build verbal blocks (Vbs) we
first find the words that meet the three Vb-H con-
ditions. Then, we test the Vb-D conditions on the
words adjacent to the Vb-Hs and extend the verbal
blocks to them if they meet the conditions. This
process is iteratively applied to the adjacent words
of a block until no more words can be added to the
verbal block, possibly nesting other verbal blocks
if necessary.
Figure 1a 2 shows an example of a dependency
tree of a Chinese sentence that will be used to il-
lustrate Vb identification. By observing the POS
tags of the words in the sentence, only the words
?bian1 ji4 (edit)? and ?chu1 ban3 (publish)? have
a POS tag (i.e. VV) in the Vb-H entry of Table 1.
Moreover, both words are dependency heads and
do not have any dependent whose POS tag is in
the BEI entry of Table 1. Thus, ?bian1 ji4 (edit)?
and ?chu1 ban3 (publish)? will be selected as Vb-
Hs and form, by themselves, two separate incipi-
ent Vbs. We arbitrarily start building the Vb from
the word ?chu1 ban3 (publish)?, by analyzing its
adjacent words that are its dependents.
We observe that only ?le5 (-ed)? is adjacent to
?chu1 ban3 (publish)?, it is its dependent, and its
POS tag is in the Vb-D list. Since ?le5 (-ed)?
meets all three conditions stated above, ?le5 (-ed)?
will be included in the Vb originated by ?chu1
ban3 (publish)?. The current Vb thus consists of
the sequence of tokens ?chu1 ban3 (publish)? and
?le5 (-ed)?, and the three conditions for Vb-D are
tested on the adjacent words of this block. Since
the adjacent words (or words separated by a coor-
dinating conjunction) do not meet the conditions,
the block is not further extended. Figure 1b shows
the dependency tree where the Vb block that con-
sists of the words ?chu1 ban3 (publish)? and ?le5
(-ed)? is represented by a rectangular box.
By checking in the same way, there are three
dependents that meet the requirements of being
2For all the dependency parsing trees in this paper, arrows
are pointing from heads to their dependents.
27
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .?? .?? .? .?? .? .? .? .? .?.School .has already .edit (-ed) .and .publish .-ed .a . .book.NN .AD .VV .CC .VV .AS .CD .M .NN .PU
.ROOT.o .o
.o.o .o .o .o .o .o
(a) Original dependency tree
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .?? .?? .? .?? .? .? .? .? .?.School .has already .edit (-ed) .and .publish .-ed .a . .book
.NN .AD .VV .CC .VV .AS .CD .M .NN .PU
.ROOT .o.o .o .o.o
(b) Vbs in rectangular boxes
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .? .? .? .?? .?? .? .?? .? .?.School .a . .book .has already .edit (-ed) .and .publish .-ed
(c) Merged and reordered Vb
Figure 1: An example that shows how to de-
tect and reorder a Verbal block (Vb) in a sen-
tence. In the first two figures 1a and 1b, Chi-
nese Pinyin, Chinese tokens, word-to-word En-
glish translations, and POS tags of each Chinese
token are listed in four lines. In Figure 1c, there
are Chinese Pinyin, reordered Chinese sentence
and its word-to-word English counterpart.
Vb-Ds for ?bian1 ji4 (edit)?: ?yi3 jing1 (has al-
ready)?, ?he2 (and)? and ?chu1 ban4 (publish)?
and hence this Vb consists of three tokens and one
Vb. The outer rectangular box in Figure 1b shows
that the Vb ?bian1 ji4 (edit)? as the Vb-H. Fig-
ure 1c shows an image of how this Vb will be
reordered while the inner orders are kept. Note
that the order of building Vbs from which Vb-Hs,
?chu1 ban3 (publish)? or ?bian1 ji4 (edit)? will not
affect any change of the final result.
3.2 Identifying objects
In the most general form, objects are dependents
of verbal blocks3 that act as their arguments.
While the simplest objects are nouns (N) or pro-
nouns (PN), they can also be comprised of noun
phrases or clauses (Downing and Locke, 2006)
such as nominal groups, finite clauses (e.g. that
clauses, wh-clauses) or non-finite clauses (e.g. -
ing clauses), among others.
For every Vb in a verb phrase, clause, or sen-
tence, we define the right-most object dependent
(RM-D) as the word that:
3Dependents of verbal blocks are dependents of any word
within the verbal block.
..ta1 .chi1 .le5 .wu3 fan4 . .qu4 .xue2 xiao4 ..? .? .? .?? .? .? .?? .?.he .eat .-ed .lunch .(and) .go(to) .school ..PN .VV .AS .NN .PU .VV .NN .PU
.ROOT.o .o .o.o
.o.o .o
? ?? ? ? ? ?? ? ?he lunch eat -ed school go(to)
V ?????? O V ???? OS
O ????? V O ???? VS
English Translation: He ate lunch, and went to school.
Figure 2: An example of a Chinese sentence with
a coordination of verb phrases as predicate. Sub-
ject(S), verbs(V), and objects(O) are displayed for
both verb phrases. Lines between the original Chi-
nese sentence and the reordered Chinese sentence
indicate the reordering trace of Verbal blocks(Vb).
i) its POS tag is in the RM-D entry of Table 1,
ii) its dependency head is inside of the verbal
block, and
iii) is the right-most object among all objects of
the verbal block.
All verbal blocks in the phrase, clause, or sen-
tence will move to the right-hand side of their cor-
respondent RM-Ds recursively. Figure 1b and Fig-
ure 1c show a basic example of object identifica-
tion. The Chinese word corresponding to ?shu1
(book)? is a dependent of a word within the verbal
block and its POS tag is within the RM-D entry
list of Table 1 (i.e. NN). For this reason, ?shu1
(book)? is identified as the right-most dependent
of the verbal block (Vb), and the Vb will move to
the right-hand side of it to resemble the Japanese
word order.
A slightly more complex example can be found
in Figure 2. In this example, there is a coordina-
tion structure of verb phrases, and the dependency
tree shows that the first verb, ?chi1 (eat)?, ap-
pears as the dependency head of the second verb,
?qu4 (go)?. The direct right-most object depen-
dent (RM-D) of the first verb, ?chi1 (eat)?, is the
word ?wu3 fan4 (lunch)?, and the verb ?chi1 (eat)?
will be moved to the right-hand side of its object
dependent.
There are cases, however, where there is no co-
ordination structure of verb phrases but a simi-
lar dependency relation occurs between two verbs.
Figure 3 illustrates one of these cases, where the
main verb ?gu3 li4 (encourage)? has no direct de-
28
..xue2 xiao4 .gu3 li4 .xue2 sheng1 .can1 yu3 .she4 hui4 .shi2 jian4 ..?? .?? .?? .?? .?? .?? .?.school .encourage .student .participate .social .practice.NN .VV .NN .VV .NN .NN .PU
.o .ROOT
.o
.o.o .o .o
?? ?? ?? ?? ?? ?? ?school student social practice participate encourage
S ???? V ?????? O
S ???? V ?????????? O
S ?????? O ??????? V
S ???????????? O ??????????? VEnglish Translation: School encourages student to participate in social practice.
Figure 3: An example of a Chinese sentence in
which an embedded clause appears as the object
of the main verb. Subjects (S), verbs (V), and ob-
jects (O) are displayed for both the sentence and
the clause. Lines between the original Chinese
sentence and the reordered Chinese sentence in-
dicate the reordering trace of Verbal blocks (Vb).
pendent that can be considered as an object since
no direct dependent has a POS tag in the RM-D en-
try of Table 1. Instead, an embedded clause (SVO)
appears as the object argument of the main verb,
and the main verb ?gu3 li4 (encourage)? appears
as the dependency head of the verb ?can1 yu2 (par-
ticipate)?.
In the news domain, reported speech is a fre-
quent example that follows this pattern. In our
method, if the main verb of the sentence (labeled
as ROOT) has dependents but none of them is a
direct object, we move the main verb to the end of
the sentence. As for the embedded clause ?xue2
sheng1 (student) can1 yu2 (participate) she4 hui4
(social) shi2 jian4 (practice)?, the verbal block of
the clause is the word ?can1 yu2 (participate)?
and its object is ?shi2 jian4 (practice)?. Apply-
ing our reordering method, the clause order results
in ?xue2 sheng1 (student) she4 hui4 (social) shi2
jian4 (practice) can1 yu2 (participate)?. The result
is an SOV sentence with an SOV clause, which
resembles the Japanese word order.
3.3 Identifying invariable grammatical
particles
In Chinese, certain invariable grammatical parti-
cles that accompany verbal heads have a different
word order relative to their heads, when compared
to Japanese. Those particles are typically ?bei4?
particle (POS tags LB and SB) and subordinating
conjunctions (POS tag CS). Those particles appear
on the left-hand side of their dependency heads in
Chinese, and they should be moved to the right-
hand side of their dependency heads for them to
resemble the Japanese word order. Reordering in-
variable grammatical particles in our framework
can be summarized as:
i) Find dependents of a verbal head (Vb-H)
whose POS tags are in the Oth-DEP entry of
Table 1.
ii) Move those particles to the right-hand side of
their (possibly reordered) heads.
iii) If there is more than one such particle, move
them keeping the relative order among them.
3.4 Summary of the reordering framework
Based on the definitions above, our dependency
parsing based pre-reordering framework can be
summarized in the following steps:
1. Obtain POS tags and an unlabeled depen-
dency tree of a Chinese sentence.
2. Obtain reordering candidates: Vbs.
3. Obtain the object (RM-D) of each Vb.
4. Reorder each Vb in two exclusive cases by
following the order:
(a) If RM-D exists, reorder Vb to be the
right-hand side of RM-D.
(b) If Vb-H is ROOT and its RM-D does not
exist, reorder Vb to the end of the sen-
tence.
(c) If none of above two conditions is met,
no reordering happens.
5. Reorder grammatical particles (Oth-DEPs) to
the right-hand side of their corresponding
Vbs.
Note that, unlike other works in reordering dis-
tant languages (Isozaki et al, 2010b; Han et al,
2012; Xu et al, 2009), we do not prevent chunks
from crossing punctuations or coordination struc-
tures. Thus, our method allows to achieve an
authentic global reordering in reported speech,
which is an important reordering issue in news do-
mains.
In order to illustrate our method, a more compli-
cated Chinese sentence example is given in Fig-
ure 4, which includes the unlabeled dependency
29
..xin1wen2 .bao2dao3 . .sui2zhe5 .jing1ji4 .de5 .fa1zhan3 . .sheng4dan4jie2 .zhu2jian4 .jin4ru4 .le5 .zhong1guo2 . .cheng2wei2 .shang1jia1 .jia1qiang2 .li4cu4 .mai3qi4 .de5 .yi1 .ge4 .ji2ri4 ..?? .?? .? .?? .?? .? .?? .? .??? .?? .?? .? .?? .? .?? .?? .?? .?? .?? .? .? .? .?? .?.news .report . .with .economic .?s .development . .Christmas .gradually .enter .-ed .China . .become .businesses .strengthen .urge .purchase .?s .one .kind .festival ..NN .VV .PU .P .NN .DEG .NN .PU .NN .AD .VV .AS .NR .PU .VV .NN .VV .VV .NN .DEC .CD .M .NN .PU
.ROOT
.o.o .o .o .o.o.o
.o .o .o .o.o .o
.o
.o
.o .o.o .o .o .o.o.o
?? ? ?? ? ?? ?? ? ??? ?? ?? ?? ? ? ?? ?? ?? ?? ? ? ? ???? ?? ?
???? ?? ?? ?? ??? ?????? ? ??? ?? ? ??? ? ?? ? ? ?? ?? ? ?? ? ?? ? ?????? ???Entire English translation: News reports, with the economic development, Christmas has gradually entered into China, and becomes one of the festivals that businesses use to promote commerce.
Figure 4: Dependency parse tree of a complex Chinese sentence example, and word alignments for
reordered sentence with its Japanese counterpart. The first four lines are Chinese Pinyin, tokens, word-
to-word English translations, and the POS tags of each Chinese token. The fifth line shows the reordered
Chinese sentence while the sixth line is the segmented Japanese translation. The entire English transla-
tion for the sentence is showed in the last line.
parsing tree of the original Chinese sentence, and
the word alignment between reordered Chinese
sentence and its Japanese counterpart, etc.
Based on both POS tags and the unlabeled de-
pendency tree, first step of our method is to obtain
all Vbs. For all heads in the tree, according to the
definition of Vb introduced in Section 3.1, there
are six tokens which will be recognized as the can-
didates of Vb-Hs, that is ?bao4 dao3 (report)?,
?sui2 zhe5 (with)?, ?jin4 ru4 (enter)?, ?cheng2
wei2 (become)?, ?jia1 qiang2 (strengthen)?, and
?li4 cu4 (urge)?. Then, for each of the candidate,
its direct dependents will be checked if they are
Vb-Ds. For instance, for the verb of ?jin4 ru4 (en-
ter)?, its dependents of ?zhu2 jian4 (gradually)?
and ?le5 (-ed)? will be considered as the Vb-Ds.
For the case of ?jia1 qiang2 (strengthen)?, instead
of being a Vb-H, it will be recognized as Vb-D
of the Vb ?li4 cu4 (urge)? since it is one of the
direct dependents of ?li4 cu4 (urge)? with a qual-
ified POS tag for Vb-D. Therefore, there are five
Vbs in total, which are ?bao4 dao3 (report)?, ?sui2
zhe5 (with)?, ?zhu2 jian4 (gradually) jin4 ru4 (en-
ter) le5 (-ed)?, ?cheng2 wei2 (become)?, and ?jia1
qiang2 (strengthen) li4 cu4 (urge)?.
The next step is to identify RM-D for each
Vb, if there is one. By checking all conditions,
four Vbs have their RM-Ds: ?fa1 zhan3 (develop-
ment)? is the RM-D of the Vb ?sui2 zhe5 (with)?;
?zhong1 guo2 (China)? is the RM-D of the Vb
?zhu2 jian4 (gradually) jin4 ru4 (enter) le5 (-ed)?;
?jie2 ri4 (festival)? is the RM-D of the Vb ?cheng2
wei2 (become)?; ?mai3 qi4 (purchase)? is the RM-
D of the Vb ?jia1 qiang2 (strengthen) li4 cu4
(urge)?.
After obtaining all RM-Ds, we find those Vbs
that have RM-Ds and move them to right of their
RM-Ds. As for the case of ?bao4 dao3 (report)?,
since it is the root and does not have any matched
RM-D, it will be moved to the end of the sen-
tence, before any final punctuation. Finally, since
there is no any invariable grammatical particle in
the sentence that need to be reordered, reordering
has been finished. From the alignments between
the reordered Chinese and its Japanese translation
showed in the figure, an almost monotonic word
alignment has been achieved.
For comparison purposes, particle seed words
had been inserted into the reordered sentences in
the same way as the Refined-HFC method, which
is using the information of predicate argument
structure output by Chinese Enju (Yu et al, 2011).
We therefore can not entirely disclaim the use
of the HPSG parser at the present stage in our
method. However, we believe that dependency
parser can provide enough information for insert-
ing particles.
4 Experiments
We conducted experiments to assess how our pro-
posed dependency-based pre-reordering for Chi-
nese (DPC) impacts on translation quality, and
compared it to a baseline phrase-based system
and a Refined-HFC pre-reordering for Chinese to
Japanese translation.
We used two Chinese-Japanese training data
30
News CWMT+News
BLEU RIBES BLEU RIBES
Baseline 39.26 84.83 38.96 85.01
Ref-HFC 39.22 84.88 39.26 84.68
DPC 39.93 85.23 39.94 85.22
Table 3: Evaluation of translation quality of two
test sets when CWMT, News and the combination
of both corpora were used for training.
sets of parallel sentences, namely an in-house-
collected Chinese-Japanese news corpus (News),
and the News corpus augmented with the
CWMT (Zhao et al, 2011) corpus. We extracted
disjoint development and test sets from News cor-
pus, containing 1, 000 and 2, 000 sentences re-
spectively. Table 2 shows the corpora statistics.
We used MeCab 4 (Kudo and Matsumoto, 2000)
and the Stanford Chinese segmenter 5 (Chang et
al., 2008) to segment Japanese and Chinese sen-
tences. POS tags of Chinese sentences were ob-
tained using the Berkeley parser 6 (Petrov et al,
2006), while dependency trees were extracted us-
ing Corbit 7 (Hatori et al, 2011). Following the
work in (Han et al, 2012), we re-implemented
the Refined-HFC using the Chinese Enju to ob-
tain HPSG parsing trees. For comparison purposes
with the work in (Isozaki et al, 2010b), particle
seed words were inserted at a preprocessing stage
for Refined-HFC and our DPC method.
DPC and Refined-HFC pre-reordering strate-
gies were followed in the pipeline by a standard
Moses-based baseline system (Koehn et al, 2007),
using a default distance reordering model and a
lexicalized reordering model ?msd-bidirectional-
fe?. A 5-gram language model was built using
SRILM (Stolcke, 2002) on the target side of the
corresponding training corpus. Word alignments
were extracted using MGIZA++ (Gao and Vogel,
2008) and the parameters of the log-linear combi-
nation were tuned using MERT (Och, 2003).
Table 3 summarizes the results of the Baseline
system (no pre-reordering nor particle word inser-
tion), the Refined-HFC (Ref-HFC) and our DPC
method, using the well-known BLEU score (Pap-
ineni et al, 2002) and a word order sensitive met-
ric named RIBES (Isozaki et al, 2010a).
4http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html
5http://nlp.stanford.edu/software/segmenter.shtml
6http://nlp.cs.berkeley.edu/Software.shtml
7http://triplet.cc/software/corbit
As it can be observed, our DPC method obtains
around 0.7 BLEU points of improvement when
compared to the second best system in both cor-
pora. When measuring the translation quality in
terms of RIBES, our method obtains an improve-
ment of 0.3 and 0.2 points when compared to the
second best system in News and CWMT + News
corpora, respectively. We suspect that corpus di-
versity might be one of the reasons for Refined-
HFC not to show any advantage in this setting.
We tested the significance of BLEU improve-
ment for Refined-HFC and DPC when compared
to the baseline phrase-based system. Refined-HFC
tests obtained p-values 0.355 and 0.135 on News
and CWMT + News corpora, while our proposed
DPC method obtained p-values 0.002 and 0.0,
which indicates significant improvements over the
phrase-based system.
5 Conclusions
In the present paper, we have analyzed the dif-
ferences in word order between Chinese and
Japanese sentences. We captured the regulari-
ties of ordering differences between Chinese and
Japanese sentences, and proposed a framework to
reorder Chinese sentences to resemble the word
order of Japanese.
Our framework consists in three steps. First,
we identify verbal blocks, which consist of Chi-
nese words that will move all together as a block
without altering their relative inner order. Sec-
ond, we identify the right-most object of the verbal
block, and move the verbal block to the right of it.
Finally, we identify invariable grammatical parti-
cles in the original vicinity of the verbal block and
move them relative to their dependency heads.
Our framework only uses the unlabeled depen-
dency structure of sentences and POS tag informa-
tion of words. We compared our system to a base-
line phrase-based SMT system and a refined head-
finalization system. Our method obtained a Chi-
nese word order that is more similar to Japanese
word order, and we showed its positive impact on
translation quality.
6 Discussion and future work
In the literature, there are mainly two types of
parsers that have been used to extract sentence
structure and guide reordering. The first type cor-
responds to parsers that extract phrase structures
(i.e. HPSG parsers). These parsers infer a rich
31
News CWMT+News
Chinese Japanese Chinese Japanese
Training
Sentences 342, 050 621, 610
Running words 7,414,749 9,361,867 9,822,535 12,499,112
Vocabulary 145,133 73,909 214,085 98,333
News Devel.
Sentences 1, 000 ?
Running words 46,042 56,748 ? ?
Out of Vocab. 255 54 ? ?
News Test
Sentences 2, 000 ?
Running words 51,534 65,721 ? ?
Out of Vocab. 529 286 ? ?
Table 2: Basic statistics of our corpora. News Devel. and News Test were used to tune and test the
systems trained with both training corpora. Data statistics were collected after tokenizing and filtering
out sentences longer than 64 tokens.
annotation of the sentence in terms of semantic
structure or phrase heads. Other reordering strate-
gies use a different type of parsers, namely depen-
dency parsers. These parsers extract dependency
information among words in the sentence, often
consisting in the dependency relation between two
words and the type of relation (dependency label).
Reordering strategies that use syntactic infor-
mation have proved successful, but they are likely
to magnify parsing errors if their reordering rules
heavily rely on abundant parse information. This
is aggravated when reordering Chinese sentences,
due to its loose word order and large variety of
possible dependency labels.
In this work, we based our study of ordering
differences between Chinese and Japanese solely
on dependency relations and POS tags. This con-
trasts with the work in (Han et al, 2012) that re-
quires phrase structures, phrase-head information
and POS tags, and the work in (Xu et al, 2009)
that requires dependency relations, dependency la-
bels and POS tags.
In spite of the fact that our method uses less syn-
tactic information, it succeeds at reordering sen-
tences with reported speech even in presence of
punctuation symbols. It is worth saying that re-
ported speech is very common in the news domain,
which might be one of the reasons of the supe-
rior translation quality achieved by our reordering
method. Our method also accounted for ordering
differences in serial verb constructions, comple-
mentizers and adverbial modifiers, which would
have required an increase in the complexity of the
reordering logic in other methods.
To the best of our knowledge, dependency
parsers are more common than HPSG parsers
across languages, and our method can potentially
be applied to translate under-resourced languages
into other languages with a very different sentence
structure, as long as they count with dependency
parsers and reliable POS taggers.
Implementing our method for other languages
would first require a linguistic study on the re-
ordering differences between the two distant lan-
guage pairs. However, some word ordering differ-
ences might be consistent across SVO and SOV
language pairs (such as verbs going before or after
their objects), but other ordering differences may
need special treatment for the language pair under
consideration (i.e. Chinese ?bei? particles).
There are two possible directions to extend the
present work. The first one would be to refine the
current method to reduce its sensitivity to POS tag-
ging or dependency parse errors, and to extend our
linguistic study on ordering differences between
Chinese and Japanese languages. The second di-
rection would be to manually or automatically find
common patterns of ordering differences between
SVO and SOV languages. The objective would be
then to create a one-for-all reordering method that
induces monotonic word alignments between sen-
tences from distant language pairs, and that could
also be easily extended to account for the unique
characteristics of the source language of interest.
Acknowledgments
We would like to thank Dr. Takuya Matsuzaki for
his precious advice on this work and Dr. Jun Ha-
tori for his support on using Corbit.
32
References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
of the 3rd Workshop on SMT, pages 224?232.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations fea-
tures. In Proc. of the Third Workshop on Syntax and
Structure in Statistical Translation, pages 51?59.
Angela Downing and Philip Locke. 2006. English
grammar: a university course. Routledge.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proc. of Machine
Translation Summit XI, pages 215?222.
Dan Han, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proc. of the Sixth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 57?66.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Ju-
nichi Tsujii. 2011. Incremental joint POS tagging
and dependency parsing in Chinese. In Proc. of
5th International Joint Conference on Natural Lan-
guage Processing, pages 1216?1224.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. of EMNNLP.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for SOV languages. In Proc. of WMT-
MetricsMATR, pages 244?251.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proc. of ACL ?07, Demonstration Sessions, pages
177?180.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proc. of the EMNLP/VLC-2000, pages
18?25.
Charles N Li and Sandra Annear Thompson. 1989.
Mandarin Chinese: A functional reference gram-
mar. Univ of California Press.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. of ACL, page 720.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34:35?80.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51.
Franz J. Och. 2003. Minimum error rate training
for statistical machine translation. In Proc. of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st COL-
ING and the 44th ACL, pages 433?440.
Carl Jesse Pollard and Ivan A. Sag. 1994. Head-
driven phrase structure grammar. The University
of Chicago Press and CSLI Publications.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the 7th interna-
tional conference on Spoken Language Processing,
2002, pages 901?904.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. of the 2007 Joint Con-
ference on EMNLP-CoNLL, pages 737?745.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proc. of 5th International Joint Conference
on Natural Language Processing, pages 29?37.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. of the 20th international
conference on Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proc. of
HLT: NA-ACL 2009, pages 245?253.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the
difficulties in Chinese deep parsing. In Proc. of the
12th International Conference on Parsing Technolo-
gies, pages 48?57.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th China workshop on machine translation
(CWMT2011). The 7th China Workshop on Ma-
chine Translation (CWMT2011).
33
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 71?75,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Efficient Logical Inference for Semantic Processing
Ran Tian
?
Yusuke Miyao Takuya Matsuzaki
National Institute of Informatics, Japan
{tianran,yusuke,takuya-matsuzaki}@nii.ac.jp
Abstract
Dependency-based Compositional Se-
mantics (DCS) provides a precise and
expressive way to model semantics of
natural language queries on relational
databases, by simple dependency-like
trees. Recently abstract denotation is pro-
posed to enable generic logical inference
on DCS. In this paper, we discuss some
other possibilities to equip DCS with
logical inference, and we discuss further
on how logical inference can help textual
entailment recognition, or other semantic
precessing tasks.
1 Introduction
Dependency-based Compositional Semantics
(DCS) was proposed as an interface for querying
relational databases by natural language. It
features DCS trees as semantic representation,
with a structure similar to dependency trees. In
its basic version, a node of a DCS tree indicates
a table in the database, and an edge indicates a
join relation. Both ends of an edge are labeled by
a field of the corresponding table (Liang et al.,
2011). However, when DCS is applied to logical
inference on unrestricted texts, it is unrealistic to
assume an explicit database, because we cannot
prepare a database for everything in the world.
For this reason, DCS trees are detached from any
specific relational database, in a way that each
node of a DCS tree indicates a content word in a
sentence (thus no fixed set of possible word labels
for a DCS tree node), and each edge indicates
?
Current affiliation of the first author: Graduate School
of Information Sciences, Tohoku University, Japan. Email
address: tianran@ecei.tohoku.ac.jp
a semantic relation between two words. Labels
on the two ends of an edge, initially indicating
fields of tables in a database, are considered
as semantic roles of the corresponding words.
Abstract denotation is proposed to capture the
meaning of this abstract version of DCS tree,
and a textual inference system based on abstract
denotation is built (Tian et al., 2014).
It is quite natural to apply DCS trees, a simple
and expressive semantic representation, to textual
inference; however the use of abstract denotations
to convey logical inference is somehow unusual.
There are two seemingly obvious way to equip
DCS with logical inference: (i) at the tree level, by
defining a set of logically sound transformations
of DCS trees; or (ii) at the logic level, by convert-
ing DCS trees to first order predicate logic (FOL)
formulas and then utilizing a theorem prover. For
(i), it may not be easy to enumerate all types of
logically sound transformations, but tree transfor-
mations can be seen as an approximation of logical
inference. For (ii), abstract denotation is more ef-
ficient than FOL formula, because abstract deno-
tation eliminates quantifiers and meanings of nat-
ural language texts can be represented by atomic
sentences.
To elaborate the above discussion and to pro-
vide more topics to the literature, in this paper we
discuss the following four questions: (?2) How
well can tree transformation approximate logical
inference? (?3) With rigorous inference on DCS
trees, where does logic contribute in the system
of Tian et al. (2014)? (?4) Does logical inference
have further potentials in Recognizing Textual En-
tailment (RTE) task? and (?5) How efficient is ab-
stract denotation compared to FOL formula? We
provide examples or experimental results to the
above questions.
71
  
stormT?: H?: ARGblame deathDebbyARG ARGOBJstormARG ARG
IOBJ
tropicalARG MOD
cause losslife
ARG
SBJ
ARG
MOD
OBJ
Figure 1: DCS trees of T: Tropical storm Debby is
blamed for death and H: A storm has caused loss
of life
2 Tree transformation vs. logical
inference
In the tree transformation based approach to RTE,
it has been realized that some gaps between T and
H cannot be filled even by a large number of tree
transformation rules extracted from corpus (Bar-
Haim et al., 2007a). For example in Figure 1, it
is possible to extract the rule blamed for death?
cause loss of life, but not easy to extract tropical
storm Debby? storm, because ?Debby? could be
an arbitrary name which may not even appear in
the corpus.
This kind of gaps was typically addressed by
approximate matching methods, for example by
counting common sub-graphs of T and H, or by
computing a cost of tree edits that convert T to
H. In the example of Figure 1, we would expect
that T is ?similar enough? (i.e. has many common
sub-graphs) with H, or the cost to convert T into H
(e.g. by deleting the node Debby and then add the
node storm) is low. As for how similar is enough,
or how the cost is evaluated, we will need a statis-
tical model to train on RTE development set.
It was neglected that some combinations of tree
edits are logical (while some are not). The entail-
ment pair in Figure 1 can be easily treated by log-
ical inference, as long as the apposition tropical
storm = Debby is appropriately handled. In con-
trast to graph matching or tree edit models which
theoretically admit arbitrary tree transformation,
logical inference clearly discriminate sound trans-
formations from unsound ones. In this sense, there
would be no need to train on RTE data.
When coreference is considered, logically
sound tree transformations can be quite compli-
cated. The following is a modified example from
RTE2-dev:
T: Hurricane Isabel, which caused significant
damage, was a tropical storm when she entered
Virginia.
  
stoormblaedbhDypi
mbtdD
drclrurmblf??bcD
fo?rmby
df??
?rocrlrbd?pi
ARG
DlfDo
DlfDo mbtdD
?b?cD
df??T?: H?:ARG TIMEARG MOD TIME
ARG
SBJ
OBJ
ARG
ARG
MOD
OBJSBJ
ARG ARG
ARGARG SBJ
SBJ
SBJ
OBJ
ARG
OBJ
?rocrlrb
Figure 2: DCS trees with coreference
H: A storm entered Virginia, causing damage.
The corresponding DCS trees are shown in Fig-
ure 2. Though the DCS trees of T and H are
quite different, H can actually be proven from T.
Note the coreference between Hurricane Isabel
and she, suggesting us to copy the subtree of Hur-
ricane Isabel to she, in a tree edit approach. This
is not enough yet, because the head storm in T is
not placed at the subject of cause. The issue is in-
deed very logical: from ?Hurricane Isabel = she?,
?Hurricane Isabel = storm?, ?she = subject of en-
ter? and ?Hurricane Isabel = subject of cause?,
we can imply that ?storm = subject of enter = sub-
ject of cause?.
3 Alignment with logical clues
Tian et al. (2014) proposed a way to generate on-
the-fly knowledge to fill knowledge gaps: if H is
not proven, compare DCS trees of T and H to
generate path alignments (e.g. blamed for death
? cause loss of life, as underscored in Figure 1);
evaluate the path alignments by a similarity score
function; and path alignments with a score greater
than a threshold (0.4) are accepted and converted
to inference rules.
The word vectors Tian et al. (2014) use to
calculate similarities are reported able to cap-
ture semantic compositions by simple additions
and subtractions (Mikolov et al., 2013). This is
also the case when used as knowledge resource
for RTE, for example the similarities between
blamed+death and cause+loss+life, or between
found+shot+dead and killed, are computed >
0.4.
However, generally such kind of similarity is
very noisy. Tian et al. (2014) used some logical
clues to filter out irrelevant path alignments, which
helps to keep a high precision. To evaluate the
effect of such logical filters, we compare it with
some other alignment strategies, the performance
of which on RTE5-test data is shown in Table 1.
Each strategy is described in the following.
72
Strategy Prec. Rec. Acc.
LogicClue + Inference 69.9 55.0 65.7
LexNoun + Inference 64.2 57.3 62.7
LexNoun + Coverage 57.1 75.0 59.3
NoFilter + Coverage 54.2 87.7 56.8
Table 1: Comparison of different alignment strate-
gies
LogicClue + Inference This is the system of
Tian et al. (2014)
1
, which use logical clues to filter
out irrelevant path alignments, and apply accepted
path alignments as inference rules.
LexNoun + Inference The same system as
above, except that we only align paths between
lexically aligned nouns. Two nouns are aligned
if and only if they are synonyms, hyponyms or
derivatively related in WordNet.
LexNoun + Coverage As above, paths between
lexically aligned nouns are aligned, and aligned
paths with similarity score > 0.4 are accepted. If
all nodes in H can be covered by some accepted
path alignments, then output ?Y?. This is very
similar to the system described in Bar-Haim et al.
(2007b).
NoFilter + Coverage Same as above, but all
paths alignments with similarity score > 0.4 are
accepted.
4 How can logical inference help RTE?
Logical inference is shown to be useful for RTE,
as Tian et al. (2014) demonstrates a system with
competitive results. However, despite the expec-
tation that all entailment matters can be explained
logically, our observation is that currently logical
inference only fills very limited short gaps from T
to H. The logical phenomena easily addressed by
Tian et al. (2014)?s framework, namely universal
quantifiers and negations, seems rare in PASCAL
RTE data. Most heavy lifting is done by distribu-
tional similarities between phrases, which may fail
in complicated sentences. An especially complex
example is:
T: Wal-Mart Stores Inc. said Tuesday that a Mas-
sachusetts judge had granted its motion to decer-
tify a class action lawsuit accusing the world?s
largest retailer of denying employees breaks.
H: Employee breaks had been denied by a motion
granted by a Massachusetts judge.
1
http://kmcs.nii.ac.jp/tianran/tifmo/
  100 1000 10000 100000 10000001
2
3
4
5
6
R? = 0.24
?? ? ? ? ?? ??? ? ? ? ? ? ?
???? ??????
Figure 3: Time of forward-chaining (seconds) in
our system, plotted on weights of statements (log-
arithmic scale).
Orig. 3 Sec. Orig. 5 Min. Red. 5 Min.
Proof found 8 16 82
Too many variables 5 24 3
Failed to find proof 0 1 3
Memory limit 0 2 0
Time out 86 57 13
Table 2: Proportion (%) of exit status of Prover9
The system of Tian et al. (2014) generated on-
the-fly knowledge to join several fragments in T
and wrongly proved H. In examples of such com-
plexity, distributional similarity is no longer reli-
able. However, it may be possible to build a pri-
ori logical models at the meta level, such as on
epistemic, intentional and reportive attitudes. The
models then can provide signals for semantic pars-
ing to connect the logic to natural language, such
as the words ?grant?, ?decertify?, and ?accuse? in
the above example. We hope this approach can
bring new progress to RTE and other semantic pro-
cessing tasks.
5 Efficiency of abstract denotations
To evaluate the efficiency of logical inference on
abstract denotations, we took 110 true entailment
pairs from RTE5 development set, which are also
pairs that can be proven with on-the-fly knowl-
edge. We plot the running time of Tian et al.
(2014)?s inference engine (single-threaded) on a
2.27GHz Xeon CPU, with respect to the weighted
sum of all statements
2
, as shown in Figure 3. The
graph shows all pairs can be proven in 6 seconds,
and proof time scales logarithmically on weight of
statements.
On the other hand, we converted statements on
abstract denotations into FOL formulas, and tried
to prove the same pairs using Prover9,
3
a popu-
2
If a statement is translated to FOL formula, the weight of
this statement equals to the weighted sum of all predicates in
the FOL formula, where an n-ary predicate is weighted as n.
3
www.cs.unm.edu/
?
mccune/prover9/
73
lar FOL theorem prover. As the result turns out
(Table 2), only 8% of the pairs can be proven in
3 seconds (the ?Orig. 3 Sec.? column), and only
16% pairs can be proven in 5 minutes (the ?Orig.
5 Min.? column), showing severe difficulties for
an FOL prover to handle textual inferences with
many (usually hundreds of) on-the-fly rules. As
such, we use Tian et al. (2014)?s inference engine
to pin down statements that are actually needed for
proving H (usually just 2 or 3 statements), and try
to prove H by Prover9 again, using only necessary
statements. Proven pairs in 5 minutes then jump
to 82% (the ?Red. 5 Min.? column), showing that
a large number of on-the-fly rules may drastically
increase computation cost. Still, nearly 20% pairs
cannot be proven even in this setting, suggesting
that traditional FOL prover is not suited for tex-
tual inference.
6 Conclusion and future work
We have discussed the role that logical infer-
ence could play in RTE task, and the efficiency
of performing inference on abstract denotations.
Though currently logical inference contributes at
places that are somehow inconspicuous, there is
the possibility that with some meta level logical
models and the methodology of semantic parsing,
we can build systems that understand natural lan-
guage texts deeply: logic implies (in)consistency,
which is in turn used as signals to produce more
accurate semantic interpretation. And after all, as
there may be many possible variations of seman-
tic representations, it is good to have an efficient
inference framework that has the potential to con-
nect them. It would be exciting if we can combine
different types of structured data with natural lan-
guage in semantic processing tasks. Directions of
our future work are described below.
Improvement of similarity score To calculate
phrase similarities, Tian et al. (2014) use the co-
sine similarity of sums of word vectors, which ig-
nores syntactic information. We plan to add syn-
tactic information to words by some supertags,
and learn a vector space embedding for this struc-
ture.
Integration of FreeBase to RTE It would be
exciting if we can utilize the huge amount of Free-
Base data in RTE task. Using the framework of
abstract denotation, meanings of sentences can be
explained as relational database queries; to convert
it to FreeBase data queries is like relational to on-
tology schema matching. In order to make effec-
tive use of FreeBase data, we also need to recog-
nize entities and relations in natural language sen-
tences. Previous research on semantic parsing will
be very helpful for learning such mapping.
Winograd Schema Challenge (WSC) As the
RTE task, WSC (Levesque et al., 2012) also pro-
vides a test bed for textual inference systems. A
Winograd schema is a pair of similar sentences but
contain an ambiguity of pronouns that is resolved
in opposite ways. A complicated partial example
is:
Michael decided to freeze himself in
cryo-stasis even though his father was
against it, because he hopes to be un-
frozen in the future when there is a cure
available.
The logical interplay among decided, hopes,
even though, because, and the realization that he
is coreferent to Michael (but not his father) is in-
triguing. By working on the task, we hope to gain
further understanding on how knowledge can be
gathered and applied in natural language reason-
ing.
Acknowledgments This research was supported
by the Todai Robot Project at National Institute of
Informatics.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007a. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI 2007.
Roy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpek-
tor, and Moshe Friedman. 2007b. Semantic in-
ference at the lexical-syntactic level for textual en-
tailment recognition. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Hector Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Knowledge Representation and Reasoning Confer-
ence.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL 2011.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL
2013.
74
Ran Tian, Yusuke Miyao, and Matsuzaki Takuya.
2014. Logical inference on dependency-based com-
positional semantics. In Proceedings of ACL 2014.
75
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 44?52,
Dublin, Ireland, August 23rd 2014.
Significance of Bridging Real-world Documents and NLP Technologies
Tadayoshi Hara Goran Topic? Yusuke Miyao Akiko Aizawa
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan
{harasan, goran topic, yusuke, aizawa}@nii.ac.jp
Abstract
Most conventional natural language processing (NLP) tools assume plain text as their input,
whereas real-world documents display text more expressively, using a variety of layouts, sentence
structures, and inline objects, among others. When NLP tools are applied to such text, users
must first convert the text into the input/output formats of the tools. Moreover, this awkwardly
obtained input typically does not allow the expected maximum performance of the NLP tools to
be achieved. This work attempts to raise awareness of this issue using XML documents, where
textual composition beyond plain text is given by tags. We propose a general framework for
data conversion between XML-tagged text and plain text used as input/output for NLP tools and
show that text sequences obtained by our framework can be much more thoroughly and efficiently
processed by parsers than naively tag-removed text. These results highlight the significance of
bridging real-world documents and NLP technologies.
1 Introduction
Recent advances in natural language processing (NLP) technologies have allowed us to dream about
applying these technologies to large-scale text, and then extracting a wealth of information from the text
or enriching the text itself with various additional information. When actually considering the realization
of this dream, however, we are faced with an inevitable problem. Conventional NLP tools usually assume
an ideal situation where each input text consists of a plain word sequence, whereas real-world documents
display text more expressively using a variety of layouts, sentence structures, and inline objects, among
others. This means that obtaining valid input for a target NLP tool is left completely to the users, who
have to program pre- and postprocessors for each application to convert their target text into the required
format and integrate the output results into their original text. This additional effort reduces the viability
of technologies, while the awkwardly obtained input does not allow the expected maximum benefit of
the NLP technologies to be realized.
In this research, we raise awareness of this issue by developing a framework that simplifies this con-
version and integration process. We assume that any textual composition beyond plain text is captured by
tags in XML documents, and focus on the data conversion between XML-tagged text and the input/output
formats of NLP tools. According to our observations, the data conversion process is determined by the
textual functions of the XML-tags utilized in the target text, of which there seem to be only four types.
We therefore devise a conversion strategy for each of the four types. After all tags in the XML tagset of
the target text have been classified by the user into the four types, data conversion and integration can be
executed automatically using our strategies, regardless of the size of the text (see Figure 1).
In the experiments, we apply our framework to several types of XML documents, and the results show
that our framework can extract plain text sequences from the target XML documents by classifying only
20% or fewer of the total number of tag types. Furthermore, with the obtained sequences, two typical
parsers succeed in processing entire documents with a much greater coverage rate and using much less
parsing time than with naively tag-removed text.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/44
Formatted Input(e.g. plain text)
Output analysis(e.g. parse trees)
<p> A user task is a scenario of use of the UI, ? the UML 
notation.  In our case, we use the CTT (Concur Task Tree) 
<cite>[ <bibref bibrefs="paterno-ctte-2001,paterno-ctt-2001" separator="," show="Number" yyseparator=","/>]</cite></p>
A user task is a scenario of use of the UI, ? the UML notation.In our case, we use the CTT (Concur Task Tree) [1]
<sentence id=?s0?><cons>?</cons></sentence>
<sentence id=?s1?>?<cons><tok>[</tok>
</cons><cons><cons><tok>1</tok></cons> ? 
<cons><tok>]</tok></cons>?</sentence>
Independent
Decoration
Object
Meta-info
Tag1
Tag2
?
(Classifyinto 4 types)
TextstructuredbyXML
TextstructuredbyXML
Textstructuredby XML
Tagset
Dataconversion NLP tools(Parser)
Figure 1: Proposed data conversion framework for applying NLP tools to text structured as XML
Tag type Criteria for classification Strategies for data conversion
Independent To represent a region syntactically Remove the tag and tagged region
independent from the surrounding text ? (apply tools to the tagged region independently)
? recover the (analyzed) tagged region after applying the tools
Decoration To set the display style of Remove only the tag
the tagged region at the same level as ? recover the tag after applying the tools
the surrounding text
Object To represent the minimal object Replace the tag (and the tagged region) with a plain word
unit that should be handled in the ? (do not process the tagged region further)
the same level as the surrounding text ? recover the tag (and region) after applying the tools
Meta-info To describe the display style Remove the tag and tagged region
setting or additional information ? (do not process the tagged region further)
? recover the tag and region after applying the tools
Table 1: Four types of tags and the data conversion strategy for each type
The contribution of this work is to demonstrate the significance of bridging real-world documents and
NLP technologies in practice. We show that, if supported by a proper framework, conventional NLP tools
already have the ability to process real-world text without significant loss of performance. We expect the
demonstration to promote further discussion on real-world document processing.
In Section 2, some related research attempts are introduced. In Section 3, the four types of textual
functions for XML tags and our data conversion strategies for each of these are described and imple-
mented. In Section 4, the efficiency of our framework and the adequacy of the obtained text sequences
for use in NLP tools are examined using several types of documents.
2 Related Work
To the best of our knowledge, no significant work on a unified methodology for data conversion between
target text and the input/output formats of NLP tools has been published. Some NLP tools provide
scripts for extracting valid input text for the tools from real-world documents; however, even these scripts
assume specific formats for the documents. For example, deep syntactic parsers such as the C&C Parser
(Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, and
therefore the distributed packages for the parsers1 contain POS-taggers together with the parsers. The
POS-taggers assume plain text sentences as their input.
As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotations
in an integrated framework. However, in this work, the authors merely proposed the framework and did
1[C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki / [Enju]: http://kmcs.nii.ac.jp/enju/45
New UI is shown. The UI is more useful  than XYZ          , and ... .
text indexmark
Cite1
Notice that ? . notecite
<text>New UI</text> is shown. The UI is more useful than XYZ <indexmark>
? </indexmark> in <cite>[ ? ]</cite><note>Notice that ? .</note>, and ? .
sentence sentence
New UI is shown. The UI is more useful  than XYZ          , and ... .Cite1 sentence
<sentence><text>New UI</text> is shown.</sentence> <sentence>The UI is more useful  than XYZ<indexmark> 
? </indexmark> in <cite>[?]</cite><note><sentence>Notice that ? . </sentence></note>, and ? .</sentence> 
(a)
(c)
(d)
(b)
text indexmark notecite Notice that ? . 
Meta-infoDecoration
IndependentObject
Figure 2: Example of executing our strategy
not explain how the given text can be used in a target annotation process such as parsing. Some projects
based on the UIMA framework, such as RASP4UIMA (Andersen et al., 2008), U-compare (Kano et
al., 2011), and Kachako (Kano, 2012)2, have developed systems where the connections between various
documents and various tools are already established. Users, however, can utilize only the text and tool
pairs that have already been integrated into the systems. GATE (Cunningham et al., 2013) is based on
a similar concept to UIMA; it supports XML documents as its input, while the framework also requires
integration of tools into the systems.
In our framework, although availability of XML documents is assumed, a user can apply NLP tools to
the documents without modifying the tools; instead, this is achieved by merely classifying the XML-tags
in the documents into a small number of functional types.
3 Data Conversion Framework
We designed a framework for data conversion between tagged text and the input/output formats of NLP
tools based on the four types of textual functions of tags. First, we introduce the four tag types and
the data conversion strategy for each. Then, we introduce the procedure for managing the entire data
conversion process using the strategies.
3.1 Strategies for the Four Tag Types
The functions of the tags are classified into only four types, namely, Independent, Decoration, Object,
and Meta-info, and for each of these types, a strategy for data conversion is described, as given in Table
1. This section explains the types and their strategies using a simple example where we attempt to apply
a sentence splitter to the text given in Figure 2(a). The target text has four tags, ?<note>?, ?<text>?,
?<cite>?, and ?<indexmark>?, denoting, respectively, Independent, Decoration, Object, and Meta-
info tags. We now describe each of the types.
Regions enclosed by Independent tags contain syntactically independent text, such as titles, sections,
and so on. In some cases, a region of this type is inserted into the middle of another sentence, like
the ?<note>? tags in the example, which represent footnote text. The data conversion strategy for text
containing these tags is to split the enclosed region into multiple subregions and apply the NLP tools
separately to each subregion.
2[U-compare]: http://u-compare.org/ / [Kachako]: http://kachako.org/kano/46
<?xml ?><document ?><title>Formal approaches ? </title><creator> ? </creator><abstract>This research ? </abstract><section><title>Introduction</title><para><p><text>New UI</text> is shown. The UI is more useful than XYZ<indexmark> ? </indexmark> in <cite>[ ? ]</cite><note>Notice that ? </note> and ? .</p></para></section><bibliography> ? </bibliography></document>
(a) XML document
?xml document
creator section bibliographytitle abstractThis research ? Formal ?
paratitleIntroduction
p
textNew UI indexmark? cite[?] noteNotice ? . and ? .inis ? XYZ
(b) Structure of XML document
Figure 3: Example XML document
Decoration tags, on the other hand, do not necessarily guarantee the independence of the enclosed
text regions, and are utilized mainly for embossing the regions visually, such as changing the font or
color of the text (?<text>? in the example), paragraphing sections3, and so on. The data conversion
strategy for text containing these tags is to remove the tags before inputting the text into the NLP tools,
and then to recover the tags afterwards4.
Regions enclosed by Object tags contain special descriptions for representing objects treated as single
syntactic components in the surrounding text. The regions do not consist of natural language text, and
therefore cannot be analyzed by NLP tools5. The data conversion strategy for text containing this tag is
to replace the enclosed region with some proper character sequence before inputting the text into NLP
tools, and then to recover the replaced region afterwards.
Regions enclosed by Meta-info tags are not targets of NLP tools, mainly because the regions are not
displayed, but utilized for other purposes, such as creating index pages (like this ?<indexmark>?)6. The
data conversion strategy for text containing these tags is to delete the tagged region before inputting the
text into NLP tools, and then to recover the region afterwards.
According to the above strategies, which are summarized in Table 1, conversion of the example text
in Figure 2(a) is carried out as follows. In the first step, tags are removed from the text, whilst retaining
their offsets in the resulting tag-less sequence shown in (b). ?Cite1? in the sequence is a plain word
utilized to replace the ?<cite>? tag region. For the ?<note>? tag, we recursively apply our strategies
to its inner region, with two plain text regions ?New UI ...? and ?Notice that ...? consequently input into
the sentence splitter. Thereafter, sentence boundary information is returned as shown in (c), and finally,
using the retained offset information of the tags, the obtained analysis and original tag information are
integrated to produce the XML-tagged sequence shown in (d).
3.2 Procedure for Efficient Tag Classification and Data Conversion
In actual XML documents as shown in Figure 3(a), a number of tags are introduced and tagged regions
are multi-layered as illustrated in Figure 3(b) (where black and white boxes represent, respectively, XML
tags and plain text regions, and regions enclosed by tags are placed below the tags in the order they
appear.). We implemented a complete data conversion procedure for efficiently classifying tags in text
documents into the four types and simultaneously obtaining plain text sequences from such documents,
3In some types of scientific articles, one sentence can be split into two paragraph regions. It depends on the target text
whether a paragraph tag is classified as Independent or Decoration.
4The tags may imply that the enclosed regions constitute chunks of text, which may be suitable for use in NLP tools.
5In some cases, natural language text is used for parts of the descriptions, for example, itemization or tables in scientific
articles. How the inner textual parts are generally associated with the surrounding text would be discussed in our future work.
For the treatment of list structures, we can learn more from A??t-Mokhtar et al. (2003).
6If the tagged region contains analyzable text, it depends on the user policy whether NLP tools should be applied to the
region, that is, whether to classify the tag as Independent.47
@plain_text_sequences = ();   # plain text sequences input to NLP tools@recovery_info = ();                 # information for recovering original document after applying NLP tools@unknown = ();                        # unknown tags
function data_convert ($target_sequence, $seq_ID) {
if ($target_sequence contains any tags) {   # process one instance of tag usage in a target sequence$usage = (pick one instance of top-level tag usage in $target_sequence);$tag = (name of the top-level tag in $usage);@attributes = (attributes and their values for the top-level tag in $usage);$region = (region in $target_sequence enclosed by tag $tag in $usage); $tag_and_region =  (region in $target_sequence consisting of $region & tag $tag enclosing it);
if ($tag ?@independent)             { remove $tag_and_region from $target_sequence;add [?independent?, $tag, @attributes,  $seq_ID, $seq_ID + 1,(offset in $target_sequence where $tag_and_region should be inserted) ] to @recovery_info;data_convert($region,  $seq_ID + 1);  } # process the tagged region separatelyelse if ($tag ?@decoration)         { remove only tag $tag enclosing $region from $target_sequence; add [?decoration?, $tag, @attributes,(offsets in $target_sequence where $region begans and ends)] to @recovery_info; }else if ($tag ?@object)                 { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?object?, $uniq, $tag_and_region] to @recovery_info; }else if ($tag ?@meta_info)          { remove $tag_and_region from $target_sequence;add [?meta_info?, $tag_and_region,(offset in $target_sequence where $tag_and_region should be inserted)] to @recovery_info; }else                                                     { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?unknown?, $uniq, $tag_and_region] to @recovery_info;if ($tag ?@unknown) { add $tag to @unknown; }  }
data_convert($target_sequence, $seq_ID);  # process the remaining tags}else {  # a plain text sequence is obtainedadd [$seq_id,  $target_sequence] to @plain_text_sequences;} }
function main ($XML_document) {data_convert ($XML_document, 0);return @plain_text_sequences, @recovery_info, @unknown;}
Figure 4: Pseudo-code algorithm for data conversion from XML text to plain text for use in NLP tools
as given by the pseudo-code algorithm in Figure 4. In the remainder of this section, we explain how the
algorithm works.
Our data conversion procedure applies the strategies for the four types of tags recursively from the
top-level tags to the lower tags. The @independent, @decoration, @object and @meta info lists
contain, respectively, Independent, Decoration, Object, and Meta-info tags, which have already been
classified by the user. When applied to a target document, the algorithm uses the four lists and strategies
given in the previous section in its first attempt at converting the document into plain text sequences,
storing unknown (and therefore unprocessed) tags, if any, in @unknown. After the entire document has
been processed for the first time, the user classifies any reported unknown tags. This process is repeated
until no further unknown tags are encountered.
In the first iteration of processing the document in Figure 3(a) the algorithm is applied to the target
document with the four tag lists empty. In the function ?data convert?, top-level tags in the document,
?<?xml>? and ?<document>?, are detected as yet-to-be classified tags and added to @unknown.
The tags and their enclosed regions in the target document are replaced with unique plain text such as
?UN1? and ?UN2?, and the input text thus becomes a sequence consisting of only plain words like ?UN1
UN2?. The algorithm then adds the sequence to @plain text sequences and terminates. The user then
classifies the reported yet-to-be classified tags in @unknown into the four tag lists, and the algorithm48
Article # ar- # total tags # classified tags (# types) #obtain-
type ticles (# types) I D O M Total ed seq.
PMC 1,000 1,357,229( 421) 32,109(12) 62,414( 8) 48205( 9) 33,953(56) 176,681( 85) 25,679
ArX. 300 1,969,359(210?) 5,888(15) 46,962(12) 60,194( 8) 7,960(17) 121,004( 52) 4,167
ACL 67 130,861( 66?) 3,240(24) 14,064(29) 4,589(15) 2,304(19) 24,197( 87) 2,293
Wiki. 300 223,514( 60?) 3,530(12) 11,197( 8) 1,470(28) 11,360(67) 27,557(115) 2,286
(ArX.: arXiv.org, Wiki.: Wikipedia, I: Independent, D: Decoration, O: Object, M: Meta-info)
Table 2: Classified tags and obtained sequences for each type of article
Treat- Parsing with Enju parser Parsing with Stanford parserArticle ed tag * # sen- ** Time Avg. # failures * # sen- ** Time Avg. # failures
type classes tences (s) (**/*) (rate) tences (s) (**/*) (rate)
None 159,327 209,783 1.32 4,721 ( 2.96%) 170,999 58,865 0.39 18,621 (10.89%)
PMC O/M 112,285 135,752 1.21 810 ( 0.72%) 126,176 50,741 0.44 11,881 ( 9.42%)
All 126,215 132,250 1.05 699 ( 0.55%) 139,805 63,295 0.49 11,338 ( 8.11%)
None 74,762 108,831 1.46 2,047 ( 2.74%) 75,672 27,970 0.43 10,590 (13.99%)
ArX. O/M 41,265 89,200 2.16 411 ( 1.00%) 48,666 24,630 0.57 5,457 (11.21%)
All 43,208 87,952 2.04 348 ( 0.81%) 50,504 26,360 0.58 5,345 (10.58%)
None 19,571 15,142 0.77 115 ( 0.59%) 17,166 5,047 0.29 1,095 ( 6.38%)
ACL O/M 9,819 9,481 0.97 63 ( 0.64%) 11,182 4,157 0.37 616 ( 5.51%)
All 11,136 8,482 0.76 39 ( 0.35%) 12,402 4,871 0.39 587 ( 4.73%)
None 10,561 14,704 1.39 1,161 (10.99%) 14,883 3,114 0.24 1,651 (11.09%)
Wiki. O/M 5,026 6,743 1.34 67 ( 1.33%) 6,173 2,248 0.38 282 ( 4.57%)
All 6,893 6,058 0.88 61 ( 0.88%) 8,049 2,451 0.31 258 ( 3.21%)
(ArX.: arXiv, Wiki.: Wikipedia, O/M: Object and Meta-info)
Table 3: Impact on parsing performance of plain text sequences extracted using classified tags
starts its second iteration7.
In the case of Independent/Decoration tags, the algorithm splits the regions enclosed by the
tags/removes only the tags from the target text, and recursively processes the obtained text sequence(s)
according to our strategies. In the splitting/removal operation, the algorithm stores in @recovery info,
the locations (offsets) in the obtained text where the tags should be inserted in order to recover the tags
and textual structures after applying the NLP tools. In the case of Object/Meta-info tags, regions en-
closed by these tags are replaced with unique plain text/omitted from the target text, which means that the
inner regions are not unpacked and processed (with relevant information about the replacement/omitting
process also stored in @recovery info). This avoids unnecessary classification tasks for tags that are
utilized only in the regions, and therefore minimizes user effort.
When no further unknown tags are reported, sufficient tag classification has been done to obtain plain
text sequences for input into NLP tools, with the sequences already stored in @plain text sequences.
After applying NLP tools to the obtained sequences, @recovery info is used to integrate the anno-
tated output from the tools into the original XML document by merging the offset information8, and
consequently to recover the structure of the original document.
4 Experiments
We investigated whether the algorithm introduced in Section 3.2 is robustly applicable to different types
of XML documents and whether the obtained text sequences are adequate for input into NLP tools. The
results of this investigation highlight the significance of bridging real-world text and NLP technologies.
4.1 Target Documents
Our algorithm was applied to four types of XML documents: three types of scientific ar-
ticles, examples of which were, respectively, downloaded from PubMed Central (PMC)
(http://www.ncbi.nlm.nih.gov/pmc/tools/ftp/), arXiv.org (http://arxiv.org/) and ACL Anthology
7The user can delay the classification for some tags to later iterations.
8When crossover of tag regions occur, the region in the annotated output is divided into subregions at the crossover point.49
(http://anthology.aclweb.org/)9, and a web page type, examples of which were downloaded from
Wikipedia (http://www.wikipedia.org/). The articles obtained from PMC were originally given in an
XML format, while those from arXiv.org and ACL Anthology were given in XHTML (based on XML),
and those from Wikipedia were given in HTML, with the HTML articles generated via intermediate
XML files. These four types of articles were therefore more or less based on valid XML (or XML-like)
formats. For our experiments, we randomly selected 1,000 PMC articles, randomly selected 300
arXiv.org articles, collected 67 (31 long and 36 short) ACL 2014 conference papers without any
conversion errors (see Footnote 9), and randomly downloaded 300 Wikipedia articles.
Each of the documents contained a variety of textual parts; we decided to apply the NLP tools to the
titles of the articles and sections, abstracts, and body text of the main sections in the scientific articles,
and to the titles of the articles, body text headings, and the actual body text of the Wikipedia articles.
According to these policies, we classified the tags appearing in all articles of each type.
4.2 Efficiency of Tag Classification
Table 2 summarizes the classified tags and obtained sequences for each type of document. The second
to ninth columns give the numbers of utilized articles, tags (in tokens and types) in the documents, each
type of tag actually classified and processed, and obtained text sequences, respectively10. Using simple
regular-expression matching, we found no remaining tagged regions in the obtained sequences. From
this we concluded that our framework at least succeeded in converting XML-tagged text into plain text.
For the PMC articles, we obtained plain text sequences by classifying only a fifth or less of the total
number of tag types, that is, focusing on less than 15% of the total tag occurrences in the documents
(comparing the third and eighth columns). This is because the tags within the regions enclosed by
Object and Meta-info tags were not considered by our procedure. For each of the arXiv.org, ACL and
Wikipedia articles, a similar effect was implied by the fact that the number of classified tags was less
than 20% of the total occurrences of all tags.
4.3 Adequacy of Obtained Sequences for Use in NLP Tools
We randomly selected several articles from each article type, and confirmed that the obtained text se-
quences consisted of valid sentences, which could be directly input into NLP tools and which thoroughly
covered the content of the original articles. Then, to evaluate the impact of this adequacy in a more prac-
tical situation, we input the obtained sequences (listed in Table 2) into two typical parsers, namely, the
Enju parser for deep syntactic/semantic analysis, and the Stanford parser (de Marneffe et al., 2006)11 for
phrase structure and dependency analysis12. Table 3 compares the parsing performance on three types
of plain text sequences obtained by different strategies: simply removing all tags, processing Object
and Meta-info tags using our framework and removing the remaining tags, and processing all the tags
using our framework. For each combination of parser and article type, we give the number of detected
sentences13, the total parsing time, the average parsing time per sentence14, and the number/ratio of
sentences that could not be parsed15.
For all article types, the parsers, especially the Enju parser, succeeded in processing the entire article
with much higher coverage (see the fourth column for each parser) and in much less time (see the third
9The XHTML version of 178 ACL 2014 conference papers were available at ACL Anthology. Each of the XHTML files
was generated by automatic conversion of the original article using LaTeXML (http://dlmf.nist.gov/LaTeXML/).
10For Wikipedia, arXiv.org and ACL articles, since HTML/XHTML tag names represent more abstract textual functions,
the number of different tag types was much smaller than for PMC articles (see ? in the table). To better capture the textual
functions of the tagged regions, we used the combination of the tag name and its selected attributes as a single tag. The number
of classified tags for Wikipedia, arXiv.org and ACL given in the table reflects this decision.
11http://nlp.stanford.edu/software/lex-parser.shtml
12The annotated output from the parsers was integrated into the original XML documents by merging the offset information,
and the structures of the original documents were consequently recovered. The recovered structures were input to xmllint, a
UNIX tool for parsing XML documents, and the tool succeeded in parsing the structures without detecting any error.
13For the Enju parser, we split each text sequence into sentences using GeniaSS [http://www.nactem.ac.uk/y-matsu/geniass/].
14For the Enju parser, the time spent parsing failed sentences was also considered.
15For the Stanford parser, the maximum sentence length was limited to 50 words using the option settings because several
sentences caused parsing failures, even after increasing the memory size from 150 MB to 2 GB, which terminated the whole
process. 50
column for each parser) using the text sequences obtained by treating some (Object and Meta-info)
or all tags with our framework than with those sequences obtained by merely removing the tags. This
is mainly because the text sequences obtained by merely removing the tags contained some embedded
inserted sentences (specified by Independent tags), bare expressions consisting of non natural language
(non-NL) principles (specified by Object tags), and sequences not directly related to the displayed text
(specified by Meta-info tags), which confused the parsers. In particular, treating Object and Meta-info
tags drastically improved parsing performance, since non-NL tokens were excluded from the analysis.
Compared with treating Object/Meta-info tags, treating all tags, that is, additionally treating Inde-
pendent tags and removing the remaining tags as Decoration tags, increased the number of detected
sentences. This is because Independent tags provide solid information for separating text sequences
into shorter sequences and thus prompting the splitting of sequences into shorter sentences, which de-
creased parsing failure by preventing a lack of search space for the Enju parser and by increasing target
(? 50 word) sentences for the Stanford parser. Treating all tags increased the total time for the Stanford
parser since a decrease in failed (> 50 word) sentences directly implied an increase in processing cost,
whereas, for the Enju parser, the total time decreased since the shortened sentences drastically narrowed
the required search space.
4.4 Significance of Bridging Real-world Documents and NLP Technologies
As demonstrated above, the parsers succeeded in processing the entire article with much higher coverage
and in much less time with the text sequences obtained by our framework than with those sequences
obtained by merely removing the tags. Then, what does such thorough and efficient processing bring
about? If our target is shallow analysis of documents which can be achieved by simple approaches such
as counting words, removing tags will suffice; embedded sentences do not affect word count, and non-NL
sequences can be canceled by a large amount of valid sequences in the target documents.
Such shallow approaches, however, cannot satisfy the demands on more detailed or precise analysis
of documents: discourse analysis, translation, grammar extraction, and so on. In order to be sensitive to
subtle signs from the documents, information uttered even in small parts of text cannot be overlooked,
under the condition that sequences other than body text are excluded.
This process of plain text extraction is a well-established procedure in NLP research; in order to
concentrate on precise analysis of natural language phenomena, datasets have been arranged in the format
of plain text sequences, and, using those datasets, plenty of remarkable achievements have been reported
in various NLP tasks while brand-new tasks have been found and tackled.
But what is the ultimate goal of these challenges? Is it to just parse carefully arranged datasets? We all
know this to be just a stepping stone to the real goal: to parse real-world, richly-formatted documents. As
we demonstrated, if supported by a proper framework, conventional NLP tools already have the ability
to process real-world text without significant loss of performance. Adequately bridging target real-world
documents and NLP technologies is thus a crucial task for taking advantage of full benefit brought by
NLP technologies in ubiquitous application of NLP.
5 Conclusion
We proposed a framework for data conversion between XML-tagged text and input/output formats of
NLP tools. In our framework, once each tag utilized in the XML-tagged text has been classified as one
of the four types of textual functions, the conversion is automatically done according to the classification.
In the experiments, we applied our framework to several types of documents, and succeeded in obtaining
plain text sequences from these documents by classifying only a fifth of the total number of tag types
in the documents. We also observed that with the obtained sequences, the target documents were much
more thoroughly and efficiently processed by parsers than with naively tag-removed text. These results
emphasize the significance of bridging real-world documents and NLP technologies.
We are now ready for public release of a tool for conversion of XML documents into plain text se-
quences utilizing our framework. We would like to share further discussion on applying NLP tools to
various real-world documents for increased benefits from NLP.51
Acknowledgements
This research was partially supported by ?Data Centric Science: Research Commons? at the Research
Organization of Information and Systems (ROIS), Japan.
References
Salah A??t-Mokhtar, Veronika Lux, and ?Eva Ba?nik. 2003. Linguistic parsing of lists in structured documents. In
Proceedings of Language Technology and the Semantic Web: 3rd Workshop on NLP and XML (NLPXML-2003),
Budapest, Hungary, April.
?. Andersen, J. Nioche, E.J. Briscoe, and J. Carroll. 2008. The BNC parsed with RASP4UIMA. In Proceedings
of the 6th Language Resources and Evaluation Conference (LREC 2008), pages 865?869, Marrakech, Morocco,
May.
Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?552.
H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva. 2013. Getting more out of biomedical documents with
GATE?s full lifecycle open source text analytics. PLoS Comput Biol, 9(2).
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the 5th Language Resources and Evaluation Conference
(LREC 2006), pages 449?454, Genoa, Italy, May.
David Ferruci, Adam Lally, Daniel Gruhl, Edward Epstein, Marshall Schor, J. William Murdock, Andy Frenkiel,
Eric W. Brown, Thomas Hampp, Yurdaer Doganata, Christopher Welty, Lisa Amini, Galina Kofman, Lev Koza-
kov, and Yosi Mass. 2006. Towards an interoperability standard for text and multi-modal analytics. Technical
Report RC24122, IBM Research Report.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen, Larry Hunter, Sophia Ananiadou, and Jun?ichi Tsujii. 2011.
U-Compare: a modular NLP workflow construction and evaluation system. IBM Journal of Research and
Development, 55(3):11:1?11:10.
Yoshinobu Kano. 2012. Kachako: a hybrid-cloud unstructured information platform for full automation of service
composition, scalable deployment and evaluation. In Proceedings in the 1st International Workshop on Analyt-
ics Services on the Cloud (ASC), the 10th International Conference on Services Oriented Computing (ICSOC
2012), Shanghai, China, November.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate HPSG parsing. In Proceedings of the 10th International Conference
on Parsing Technologies (IWPT?07), Prague, Czech Republic, June.
52
