Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612?620,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Analysing Wikipedia and Gold-Standard Corpora for NER Training
Joel Nothman and Tara Murphy and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{jnot4610,tm,james}@it.usyd.edu.au
Abstract
Named entity recognition (NER) for En-
glish typically involves one of three gold
standards: MUC, CoNLL, or BBN, all created
by costly manual annotation. Recent work
has used Wikipedia to automatically cre-
ate a massive corpus of named entity an-
notated text.
We present the first comprehensive cross-
corpus evaluation of NER. We identify
the causes of poor cross-corpus perfor-
mance and demonstrate ways of making
them more compatible. Using our process,
we develop a Wikipedia corpus which out-
performs gold standard corpora on cross-
corpus evaluation by up to 11%.
1 Introduction
Named Entity Recognition (NER), the task of iden-
tifying and classifying the names of people, organ-
isations and other entities within text, is central
to many NLP systems. NER developed from in-
formation extraction in the Message Understand-
ing Conferences (MUC) of the 1990s. By MUC 6
and 7, NER had become a distinct task: tagging
proper names, and temporal and numerical expres-
sions (Chinchor, 1998).
Statistical machine learning systems have
proven successful for NER. These learn patterns
associated with individual entity classes, mak-
ing use of many contextual, orthographic, linguis-
tic and external knowledge features. However,
they rely heavily on large annotated training cor-
pora. This need for costly expert annotation hin-
ders the creation of more task-adaptable, high-
performance named entity recognisers.
In acquiring new sources for annotated corpora,
we require an analysis of training data as a variable
in NER. This paper compares the three main gold-
standard corpora. We found that tagging mod-
els built on each corpus perform relatively poorly
when tested on the others. We therefore present
three methods for analysing internal and inter-
corpus inconsistencies. Our analysis demonstrates
that seemingly minor variations in the text itself,
starting right from tokenisation can have a huge
impact on practical NER performance.
We take this experience and apply it to a corpus
created automatically using Wikipedia. This cor-
pus was created following the method of Nothman
et al (2008). By training the C&C tagger (Curran
and Clark, 2003) on the gold-standard corpora and
our new Wikipedia-derived training data, we eval-
uate the usefulness of the latter and explore the
nature of the training corpus as a variable in NER.
Our Wikipedia-derived corpora exceed the per-
formance of non-corresponding training and test
sets by up to 11% F -score, and can be engineered
to automatically produce models consistent with
various NE-annotation schema. We show that it is
possible to automatically create large, free, named
entity-annotated corpora for general or domain
specific tasks.
2 NER and annotated corpora
Research into NER has rarely considered the im-
pact of training corpora. The CoNLL evalua-
tions focused on machine learning methods (Tjong
Kim Sang, 2002; Tjong Kim Sang and De Meul-
der, 2003) while more recent work has often in-
volved the use of external knowledge. Since many
tagging systems utilise gazetteers of known enti-
ties, some research has focused on their automatic
extraction from the web (Etzioni et al, 2005) or
Wikipedia (Toral et al, 2008), although Mikheev
et al (1999) and others have shown that larger
NE lists do not necessarily correspond to increased
NER performance. Nadeau et al (2006) use such
lists in an unsupervised NE recogniser, outper-
forming some entrants of the MUC Named Entity
Task. Unlike statistical approaches which learn
612
patterns associated with a particular type of entity,
these unsupervised approaches are limited to iden-
tifying common entities present in lists or those
caught by hand-built rules.
External knowledge has also been used to aug-
ment supervised NER approaches. Kazama and
Torisawa (2007) improve their F -score by 3% by
including a Wikipedia-based feature in their ma-
chine learner. Such approaches are limited by the
gold-standard data already available.
Less common is the automatic creation of train-
ing data. An et al (2003) extracted sentences con-
taining listed entities from the web, and produced
a 1.8 million word Korean corpus that gave sim-
ilar results to manually-annotated training data.
Richman and Schone (2008) used a method sim-
ilar to Nothman et al (2008) in order to derive
NE-annotated corpora in languages other than En-
glish. They classify Wikipedia articles in foreign
languages by transferring knowledge from English
Wikipedia via inter-language links. With these
classifications they automatically annotate entire
articles for NER training, and suggest that their re-
sults with a 340k-word Spanish corpus are compa-
rable to 20k-40k words of gold-standard training
data when using MUC-style evaluation metrics.
2.1 Gold-standard corpora
We evaluate our Wikipedia-derived corpora
against three sets of manually-annotated data from
(a) the MUC-7 Named Entity Task (MUC, 2001);
(b) the English CoNLL-03 Shared Task (Tjong
Kim Sang and De Meulder, 2003); (c) the
BBN Pronoun Coreference and Entity Type
Corpus (Weischedel and Brunstein, 2005). We
consider only the generic newswire NER task,
although domain-specific annotated corpora have
been developed for applications such as bio-text
mining (Kim et al, 2003).
Stylistic and genre differences between the
source texts affect compatibility for NER evalua-
tion e.g., the CoNLL corpus formats headlines in
all-caps, and includes non-sentential data such as
tables of sports scores.
Each corpus uses a different set of entity labels.
MUC marks locations (LOC), organisations (ORG)
and personal names (PER), in addition to numeri-
cal and time information. The CoNLL NER shared
tasks (Tjong Kim Sang, 2002; Tjong Kim Sang
and De Meulder, 2003) mark PER, ORG and LOC
entities, as well as a broad miscellaneous class
Corpus # tags
Number of tokens
TRAIN DEV TEST
MUC-7 3 83601 18655 60436
CoNLL-03 4 203621 51362 46435
BBN 54 901894 142218 129654
Table 1: Gold-standard NE-annotated corpora
(MISC; e.g. events, artworks and nationalities).
BBN annotates the entire Penn Treebank corpus
with 105 fine-grained tags (Brunstein, 2002): 54
corresponding to CoNLL entities; 21 for numeri-
cal and time data; and 30 for other classes. For
our evaluation, BBN?s tags were reduced to the
equivalent CoNLL tags, with extra tags in the BBN
and MUC data removed. Since no MISC tags are
marked in MUC, they need to be removed from
CoNLL, BBN and Wikipedia data for comparison.
We transformed all three corpora into a com-
mon format and annotated them with part-of-
speech tags using the Penn Treebank-trained
C&C POS tagger. We altered the default MUC
tokenisation to attach periods to abbreviations
when sentence-internal. While standard training
(TRAIN), development (DEV) and final test (TEST)
set divisions were available for CoNLL and MUC,
the BBN corpus was split at our discretion: sec-
tions 03?21 for TRAIN, 00?02 for DEV and 22-24
for TEST. Corpus sizes are compared in Table 1.
2.2 Evaluating NER performance
One challenge for NER research is establishing an
appropriate evaluation metric (Nadeau and Sekine,
2007). In particular, entities may be correctly
delimited but mis-classified, or entity boundaries
may be mismatched.
MUC (Chinchor, 1998) awarded equal score for
matching type, where an entity?s class is identi-
fied with at least one boundary matching, and text,
where an entity?s boundaries are precisely delim-
ited, irrespective of the classification. This equal
weighting is unrealistic, as some boundary errors
are highly significant, while others are arbitrary.
CoNLL awarded exact (type and text) phrasal
matches, ignoring boundary issues entirely and
providing a lower-bound measure of NER per-
formance. Manning (2006) argues that CoNLL-
style evaluation is biased towards systems which
leave entities with ambiguous boundaries un-
tagged, since boundary errors amount simultane-
ously to false positives and false negatives. In both
MUC and CoNLL, micro-averaged precision, recall
and F1 score summarise the results.
613
Tsai et al (2006) compares a number of meth-
ods for relaxing boundary requirements: matching
only the left or right boundary, any tag overlap,
per-token measures, or more semantically-driven
matching. ACE evaluations instead use a customiz-
able evaluation metric with weights specified for
different types of error (NIST-ACE, 2008).
3 Corpus and error analysis approaches
To evaluate the performance impact of a corpus
we may analyse (a) the annotations themselves; or
(b) the model built on those annotations and its
performance. A corpus can be considered in isola-
tion or by comparison with other corpora. We use
three methods to explore intra- and inter-corpus
consistency in MUC, CoNLL, and BBN in Section 4.
3.1 N-gram tag variation
Dickinson and Meurers (2003) present a clever
method for finding inconsistencies within POS an-
notated corpora, which we apply to NER corpora.
Their approach finds all n-grams in a corpus which
appear multiple times, albeit with variant tags for
some sub-sequence, the nucleus (see e.g. Table
3). To remove valid ambiguity, they suggest us-
ing (a) a minimum n-gram length; (b) a minimum
margin of invariant terms around the nucleus.
For example, the BBN TRAIN corpus includes
eight occurrences of the 6-gram the San Francisco
Bay area ,. Six instances of area are tagged as non-
entities, but two instances are tagged as part of the
LOC that precedes it. The other five tokens in this
n-gram are consistently labelled.
3.2 Entity type frequency
An intuitive approach to finding discrepancies be-
tween corpora is to compare the distribution of en-
tities within each corpus. To make this manage-
able, instances need to be grouped by more than
their class labels. We used the following groups:
POS sequences: Types of candidate entities may
often be distinguished by their POS tags, e.g.
nationalities are often JJ or NNPS.
Wordtypes: Collins (2002) proposed wordtypes
where all uppercase characters map to A, low-
ercase to a, and digits to 0. Adjacent charac-
ters in the same orthographic class were col-
lapsed. However, we distinguish single from
multiple characters by duplication. e.g. USS
Nimitz (CVN-68) has wordtype AA Aaa (AA-00).
Wordtype with functions: We also map content
words to wordtypes only?function words
are retained, e.g. Bank of New England Corp.
maps to Aaa of Aaa Aaa Aaa..
No approach provides sufficient discrimination
alone: wordtype patterns are able to distinguish
within common POS tags and vice versa. Each
method can be further simplified by merging re-
peated tokens, NNP NNP becoming NNP.
By calculating the distribution of entities over
these groupings, we can find anomalies between
corpora. For instance, 4% of MUC?s and 5.9%
of BBN?s PER entities have wordtype Aaa A. Aaa,
e.g. David S. Black, while CoNLL has only 0.05% of
PERs like this. Instead, CoNLL has many names of
form A. Aaa, e.g. S. Waugh, while BBN and MUC
have none. We can therefore predict incompatibil-
ities between systems trained on BBN and evalu-
ated on CoNLL or vice-versa.
3.3 Tag sequence confusion
A confusion matrix between predicted and correct
classes is an effective method of error analysis.
For phrasal sequence tagging, this can be applied
to either exact boundary matches or on a per-token
basis, ignoring entity bounds. We instead compile
two matrices: C/P comparing correct entity classes
against predicted tag sequences; and P/C compar-
ing predicted classes to correct tag sequences.
C/P equates oversized boundaries to correct
matches, and tabulates cases of undersized bound-
aries. For example, if [ORG Johnson and Johnson] is
tagged [PER Johnson] and [PER Johnson], it is marked
in matrix coordinates (ORG, PER O PER). P/C em-
phasises oversized boundaries: if gold-standard
Mr. [PER Ross] is tagged PER, it is counted as con-
fusion between PER and O PER. To further dis-
tinguish classes of error, the entity type groupings
from Section 3.2 are also used.
This analysis is useful for both tagger evalua-
tion and cross-corpus evaluation, e.g. BBN versus
CoNLL on a BBN test set. This involves finding
confusion matrix entries where BBN and CoNLL?s
performance differs significantly, identifying com-
mon errors related to difficult instances in the test
corpus as well as errors in the NER model.
4 Comparing gold-standard corpora
We trained the C&C NER tagger (Curran and Clark,
2003) to build separate models for each gold-
standard corpus. The C&C tagger utilises a number
614
TRAIN
With MISC Without MISC
CoNLL BBN MUC CoNLL BBN
MUC ? ? 73.5 55.5 67.5
CoNLL 81.2 62.3 65.9 82.1 62.4
BBN 54.7 86.7 77.9 53.9 88.4
Table 2: Gold standard F -scores (exact-match)
of orthographic, contextual and in-document fea-
tures, as well as gazetteers for personal names. Ta-
ble 2 shows that each training set performs much
better on corresponding (same corpus) test sets
(italics) than on test sets from other sources, also
identified by (Ciaramita and Altun, 2005). NER
research typically deals with small improvements
(?1% F -score). The 12-32% mismatch between
training and test corpora suggests that an appropri-
ate training corpus is a much greater concern. The
exception is BBN on MUC, due to differing TEST
and DEV subject matter. Here we analyse the vari-
ation within and between the gold standards.
Table 3 lists some n-gram tag variations for BBN
and CoNLL (TRAIN + DEV). These include cases of
schematic variations (e.g. the period in Co .) and
tagging errors. Some n-grams have three variants,
e.g. the Standard & Poor ?s 500 which appears un-
tagged, as the [ORG Standard & Poor] ?s 500, or the
[ORG Standard & Poor ?s] 500. MUC is too small for
this method. CoNLL only provides only a few ex-
amples, echoing BBN in the ambiguities of trailing
periods and leading determiners or modifiers.
Wordtype distributions were also used to com-
pare the three gold standards. We investigated all
wordtypes which occur with at least twice the fre-
quency in one corpus as in another, if that word-
type was sufficiently frequent. Among the differ-
ences recovered from this analysis are:
? CoNLL has an over-representation of uppercase words
due to all-caps headlines.
? Since BBN also annotates common nouns, some have
been mistakenly labelled as proper-noun entities.
? BBN tags text like Munich-based as LOC; CoNLL
tags it as MISC; MUC separates the hyphen as a token.
? CoNLL is biased to sports and has many event names
in the form of 1990 World Cup.
? BBN separates organisation names from their products
as in [ORG Commodore] [MISC 64].
? CoNLL has few references to abbreviated US states.
? CoNLL marks conjunctions of people (e.g. Ruth and
Edwin Brooks) as a single PER entity.
? CoNLL text has Co Ltd instead of Co. Ltd.
We analysed the tag sequence confusion when
training with each corpus and testing on BBN DEV.
While full confusion matrices are too large for this
paper, Table 4 shows some examples where the
Figure 1: Deriving training data from Wikipedia
NER models disagree. MUC fails to correctly tag
U.K. and U.S.. U.K. only appears once in MUC, and
U.S. appears 22 times as ORG and 77 times as LOC.
CoNLL has only three instances of Mr., so it often
mis-labels Mr. as part of a PER entity. The MUC
model also has trouble recognising ORG names
ending with corporate abbreviations, and may fail
to identify abbreviated US state names.
Our analysis demonstrates that seemingly mi-
nor orthographic variations in the text, tokenisa-
tion and annotation schemes can have a huge im-
pact on practical NER performance.
5 From Wikipedia to NE-annotated text
Wikipedia is a collaborative, multilingual, online
encyclopedia which includes over 2.3 million arti-
cles in English alone. Our baseline approach de-
tailed in Nothman et al (2008) exploits the hyper-
linking between articles to derive a NE corpus.
Since ?74% of Wikipedia articles describe top-
ics covering entity classes, many of Wikipedia?s
links correspond to entity annotations in gold-
standard NE corpora. We derive a NE-annotated
corpus by the following steps:
1. Classify all articles into entity classes
2. Split Wikipedia articles into sentences
3. Label NEs according to link targets
4. Select sentences for inclusion in a corpus
615
N-gram Tag # Tag #
Co . - 52 ORG 111
Smith Barney , Harris Upham & Co. - 1 ORG 9
the Contra rebels MISC 1 ORG 2
in the West is - 1 LOC 1
that the Constitution MISC 2 - 1
Chancellor of the Exchequer Nigel Lawson - 11 ORG 2
the world ?s - 80 LOC 1
1993 BellSouth Classic - 1 MISC 1
Atlanta Games LOC 1 MISC 1
Justice Minister - 1 ORG 1
GOLF - GERMAN OPEN - 2 LOC 1
Table 3: Examples of n-gram tag variations in BBN (top) and CoNLL (bottom). Nucleus is in bold.
Tag sequence
Grouping
# if trained on
Example
Correct Pred. MUC CoNLL BBN
LOC LOC A.A. 101 349 343 U.K.
- PER PER Aa. Aaa 9 242 0 Mr. Watson
- LOC Aa. 16 109 0 Mr.
ORG ORG Aaa Aaa. 118 214 218 Campeau Corp.
LOC - Aaa. 20 0 3 Calif.
Table 4: Tag sequence confusion on BBN DEV when training on gold-standard corpora (no MISC)
In Figure 1, a sentence introducing Holden as an
Australian car maker based in Port Melbourne has
links to separate articles about each entity. Cues
in the linked article about Holden indicate that it is
an organisation, and the article on Port Melbourne
is likewise classified as a location. The original
sentence can then be automatically annotated with
these facts. We thus extract millions of sentences
from Wikipedia to form a new NER corpus.
We classify each article in a bootstrapping pro-
cess using its category head nouns, definitional
nouns from opening sentences, and title capital-
isation. Each article is classified as one of: un-
known; a member of a NE category (LOC, ORG,
PER, MISC, as per CoNLL); a disambiguation page
(these list possible referent articles for a given ti-
tle); or a non-entity (NON). This classifier classi-
fier achieves 89% F -score.
A sentence is selected for our corpus when all
of its capitalised words are linked to articles with a
known class. Exceptions are made for common ti-
tlecase words, e.g. I, Mr., June, and sentence-initial
words. We also infer additional links ? variant ti-
tles are collected for each Wikipedia topic and are
marked up in articles which link to them ? which
Nothman et al (2008) found increases coverage.
Transforming links into annotations that con-
form to a gold standard is far from trivial. Link
boundaries need to be adjusted, e.g. to remove ex-
cess punctuation. Adjectival forms of entities (e.g.
American, Islamic) generally link to nominal arti-
cles. However, they are treated by CoNLL and our
N-gram Tag # Tag #
of Batman ?s MISC 2 PER 5
in the Netherlands - 58 LOC 4
Chicago , Illinois - 8 LOC 3
the American and LOC 1 MISC 2
Table 5: N-gram variations in the Wiki baseline
BBN mapping as MISC. POS tagging the corpus and
relabelling entities ending with JJ as MISC solves
this heuristically. Although they are capitalised in
English, personal titles (e.g. Prime Minister) are not
typically considered entities. Initially we assume
that all links immediately preceding PER entities
are titles and delete their entity classification.
6 Improving Wikipedia performance
The baseline system described above achieves
only 58.9% and 62.3% on the CoNLL and
BBN TEST sets (exact-match scoring) with 3.5-
million training tokens. We apply methods pro-
posed in Section 3 to to identify and minimise
Wikipedia errors on the BBN DEV corpus.
We begin by considering Wikipedia?s internal
consistency using n-gram tag variation (Table 5).
The breadth of Wikipedia leads to greater genuine
ambiguity, e.g. Batman (a character or a comic
strip). It also shares gold-standard inconsistencies
like leading modifiers. Variations in American and
Chicago, Illinois indicate errors in adjectival entity
labels and in correcting link boundaries.
Some errors identified with tag sequence confu-
sion are listed in Table 6. These correspond to re-
616
Tag sequence
Grouping
# if trained on
Example
Correct Pred. BBN Wiki
LOC LOC Aaa. 103 14 Calif.
LOC - LOC ORG Aaa , Aaa. 0 15 Norwalk , Conn.
LOC LOC Aaa-aa 23 0 Texas-based
- PER PER Aa. Aaa 4 208 Mr. Yamamoto
- PER PER Aaa Aaa 1 49 Judge Keenan
- PER Aaa 7 58 President
MISC MISC A. 25 1 R.
MISC LOC NNPS 0 39 Soviets
Table 6: Tag sequence confusion on BBN DEV with training on BBN and the Wikipedia baseline
sults of an entity type frequency analysis and mo-
tivate many of our Wikipedia extensions presented
below. In particular, personal titles are tagged as
PER rather than unlabelled; plural nationalities are
tagged LOC, not MISC; LOCs hyphenated to fol-
lowing words are not identified; nor are abbrevi-
ated US state names. Using R. to abbreviate Re-
publican in BBN is also a high-frequency error.
6.1 Inference from disambiguation pages
Our baseline system infers extra links using a set
of alternative titles identified for each article. We
extract the alternatives from the article and redirect
titles, the text of all links to the article, and the first
and last word of the article title if it is labelled PER.
Our extension is to extract additional inferred ti-
tles fromWikipedia?s disambiguation pages. Most
disambiguation pages are structured as lists of ar-
ticles that are often referred to by the titleD being
disambiguated. For each link with target A that
appears at the start of a list item on D?s page, D
and its redirect aliases are added to the list of al-
ternative titles for A.
Our new source of alternative titles includes
acronyms and abbreviations (AMP links to AMP
Limited and Ampere), and given or family names
(Howard links to Howard Dean and John Howard).
6.2 Personal titles
Personal titles (e.g. Brig. Gen., Prime Minister-
elect) are capitalised in English. Titles are some-
times linked in Wikipedia, but the target articles,
e.g. U.S. President, are in Wikipedia categories like
Presidents of the United States, causing their incor-
rect classification as PER.
Our initial implementation assumed that links
immediately preceding PER entity links are titles.
While this feature improved performance, it only
captured one context for personal titles and failed
to handle instances where the title was only a
portion of the link text, such as Australian Prime
Minister-elect or Prime Minister of Australia.
To handle titles more comprehensively, we
compiled a list of the terms most frequently linked
immediately prior to PER links. These were man-
ually filtered, removing LOC or ORG mentions and
complemented with abbreviated titles extracted
from BBN, producing a list of 384 base title forms,
11 prefixes (e.g. Vice) and 3 suffixes (e.g. -elect).
Using these gazetteers, titles are stripped of erro-
neous NE tags.
6.3 Adjectival forms
In English, capitalisation is retained in adjectival
entity forms, such as American or Islamic. While
these are not exactly entities, both CoNLL and BBN
annotate them as MISC. Our baseline approach
POS tagged the corpus and marked all adjectival
entities as MISC. This missed instances where na-
tionalities are used nominally, e.g. five Italians.
We extracted 339 frequent LOC and ORG ref-
erences with POS tag JJ. Words from this list
(e.g. Italian) are relabelled MISC, irrespective of
POS tag or pluralisation (e.g. Italian/JJ, Italian/NNP,
Italian/NNPS). This unfiltered list includes some er-
rors from POS tagging, e.g. First, Emmy; and others
where MISC is rarely the appropriate tag, e.g. the
Democrats (an ORG).
6.4 Miscellaneous changes
Entity-word aliases Longest-string matching for
inferred links often adds redundant words, e.g.
both Australian and Australian people are redirects to
Australia. We therefore exclude from inference ti-
tles of form X Y where X is an alias of the same
article and Y is lowercase.
State abbreviations A gold standard may use
stylistic forms which are rare in Wikipedia. For
instance, the Wall Street Journal (BBN) uses US
state abbreviations, while Wikipedia nearly al-
ways refers to states in full. We boosted perfor-
mance by substituting a random selection of US
state names in Wikipedia with their abbreviations.
617
TRAIN
With MISC No MISC
CoN. BBN MUC CoN. BBN
MUC ? ? 82.3 54.9 69.3
CoNLL 85.9 61.9 69.9 86.9 60.2
BBN 59.4 86.5 80.2 59.0 88.0
WP0 ? no inf. 62.8 69.7 69.7 64.7 70.0
WP1 67.2 73.4 75.3 67.7 73.6
WP2 69.0 74.0 76.6 69.4 75.1
WP3 68.9 73.5 77.2 69.5 73.7
WP4 ? all inf. 66.2 72.3 75.6 67.3 73.3
Table 7: Exact-match DEV F -scores
Removing rare cases We explicitly removed
sentences containing title abbreviations (e.g. Mr.)
appearing in non-PER entities such as movie titles.
Compared to newswire, these forms as personal
titles are rare in Wikipedia, so their appearance in
entities causes tagging errors. We used a similar
approach to personal names including of, which
also act as noise.
Fixing tokenization Hyphenation is a problem
in tokenisation: should London-based be one token,
two, or three? Both BBN and CoNLL treat it as one
token, but BBN labels it a LOC and CoNLL a MISC.
Our baseline had split hyphenated portions from
entities. Fixing this to match the BBN approach
improved performance significantly.
7 Experiments
We evaluated our annotation process by build-
ing separate NER models learned from Wikipedia-
derived and gold-standard data. Our results are
given as micro-averaged precision, recall and F -
scores both in terms of MUC-style and CoNLL-style
(exact-match) scoring. We evaluated all experi-
ments with and without the MISC category.
Wikipedia?s articles are freely available for
download.1 We have used data from the 2008
May 22 dump of English Wikipedia which in-
cludes 2.3 million articles. Splitting this into sen-
tences and tokenising produced 32 million sen-
tences each containing an average of 24 tokens.
Our experiments were performed with a
Wikipedia corpus of 3.5 million tokens. Although
we had up to 294 million tokens available, we
were limited by the RAM required by the C&C tag-
ger training software.
8 Results
Tables 7 and 8 show F -scores on the MUC, CoNLL,
and BBN development sets for CoNLL-style exact
1http://download.wikimedia.org/
TRAIN
With MISC No MISC
CoN. BBN MUC CoN. BBN
MUC ? ? 89.0 68.2 79.2
CoNLL 91.0 75.1 81.4 90.9 72.6
BBN 72.7 91.1 87.6 71.8 91.5
WP0 ? no inf. 71.0 79.3 76.3 71.1 78.7
WP1 74.9 82.3 81.4 73.1 81.0
WP2 76.1 82.7 81.6 74.5 81.9
WP3 76.3 82.2 81.9 74.7 80.7
WP4 ? all inf. 74.3 81.4 80.9 73.1 80.7
Table 8: MUC-style DEV F -scores
Training corpus
DEV (MUC-style F )
MUC CoNLL BBN
Corresponding TRAIN 89.0 91.0 91.1
TRAIN + WP2 90.6 91.7 91.2
Table 9: Wikipedia as additional training data
TRAIN
With MISC No MISC
CoN. BBN MUC CoN. BBN
MUC ? ? 73.5 55.5 67.5
CoNLL 81.2 62.3 65.9 82.1 62.4
BBN 54.7 86.7 77.9 53.9 88.4
WP2 60.9 69.3 76.9 61.5 69.9
Table 10: Exact-match TEST results for WP2
TRAIN
With MISC No MISC
CoN. BBN MUC CoN. BBN
MUC ? ? 81.0 68.5 77.6
CoNLL 87.8 75.0 76.2 87.9 74.1
BBN 69.3 91.1 83.6 68.5 91.9
WP2 70.2 79.1 81.3 68.6 77.3
Table 11: MUC-eval TEST results for WP2
match and MUC-style evaluations (which are typi-
cally a few percent higher). The cross-corpus gold
standard experiments on the DEV sets are shown
first in both tables. As in Table 2, the performance
drops significantly when the training and test cor-
pus are from different sources. The corresponding
TEST set scores are given in Tables 9 and 10.
The second group of experiments in these ta-
bles show the performance of Wikipedia corpora
with increasing levels of link inference (described
in Section 6.1). Links inferred upon match-
ing article titles (WP1) and disambiguation ti-
tles (WP2) consistently increase F -score by ?5%,
while surnames for PER entities (WP3) and all link
texts (WP4) tend to introduce error. A key re-
sult of our work is that the performance of non-
corresponding gold standards is often significantly
exceeded by our Wikipedia training data.
Our third group of experiments combined our
Wikipedia corpora with gold-standard data to im-
prove performance beyond traditional train-test
pairs. Table 9 shows that this approach may lead
618
Token Corr. Pred. Count Why?
. ORG - 90 Inconsistencies in BBN
House ORG LOC 56 Article White House is a LOC due to classification bootstrapping
Wall - LOC 33 Wall Street is ambiguously a location and a concept
Gulf ORG LOC 29 Georgia Gulf is common in BBN, but Gulf indicates LOC
, ORG - 26 A difficult NER ambiguity in e.g. Robertson , Stephens & Co.
?s ORG - 25 Unusually high frequency of ORGs ending ?s in BBN
Senate ORG LOC 20 Classification bootstrapping identifies Senate as a house, i.e. LOC
S&P - MISC 20 Rare in Wikipedia, and inconsistently labelled in BBN
D. MISC PER 14 BBN uses D. to abbreviate Democrat
Table 12: Tokens in BBN DEV that our Wikipedia model frequently mis-tagged
Class
By exact phrase By token
P R F P R F
LOC 66.7 87.9 75.9 64.4 89.8 75.0
MISC 48.8 58.7 53.3 46.5 61.6 53.0
ORG 76.9 56.5 65.1 88.9 68.1 77.1
PER 67.3 91.4 77.5 70.5 93.6 80.5
All 68.6 69.9 69.3 80.9 75.3 78.0
Table 13: Category results for WP2 on BBN TEST
to small F -score increases.
Our per-class Wikipedia results are shown in
Table 13. LOC and PER entities are relatively easy
to identify, although a low precision for PER sug-
gests that many other entities have been marked
erroneously as people, unlike the high precision
and low recall of ORG. As an ill-defined category,
with uncertain mapping between BBN and CoNLL
classes, MISC precision is unsurprisingly low. We
also show results evaluating the correct labelling
of each token, where Nothman et al (2008) had
reported results 13% higher than phrasal match-
ing, reflecting a failure to correctly identify entity
boundaries. We have reduced this difference to
9%. A BBN-trained model gives only 5% differ-
ence between phrasal and token F -score.
Among common tagging errors, we identified:
tags continuing over additional words as in New
York-based Loews Corp. all being marked as a sin-
gle ORG; nationalities marked as LOC rather than
MISC; White House a LOC rather than ORG, as
with many sports teams; single-word ORG entities
marked as PER; titles such as Dr. included in PER
tags; mis-labelling un-tagged title-case terms and
tagged lowercase terms in the gold-standard.
The corpus analysis methods described in
Section 3 show greater similarity between our
Wikipedia-derived corpus and BBN after imple-
menting our extensions. There is nonetheless
much scope for further analysis and improvement.
Notably, the most commonly mis-tagged tokens in
BBN (see Table 12) relate more often to individual
entities and stylistic differences than to a general-
isable class of errors.
9 Conclusion
We have demonstrated the enormous variability in
performance between using NER models trained
and tested on the same corpus versus tested on
other gold standards. This variability arises from
not only mismatched annotation schemes but also
stylistic conventions, tokenisation, and missing
frequent lexical items. Therefore, NER corpora
must be carefully matched to the target text for rea-
sonable performance. We demonstrate three ap-
proaches for gauging corpus and annotation mis-
match, and apply them to MUC, CoNLL and BBN,
and our automatically-derived Wikipedia corpora.
There is much room for improving the results of
our Wikipedia-based NE annotations. In particu-
lar, a more careful approach to link inference may
further reduce incorrect boundaries of tagged en-
tities. We plan to increase the largest training set
the C&C tagger can support so that we can fully
exploit the enormous Wikipedia corpus.
However, we have shown that Wikipedia can
be used a source of free annotated data for train-
ing NER systems. Although such corpora need
to be engineered specifically to a desired appli-
cation, Wikipedia?s breadth may permit the pro-
duction of large corpora even within specific do-
mains. Our results indicate that Wikipedia data
can perform better (up to 11% for CoNLL on MUC)
than training data that is not matched to the eval-
uation, and hence is widely applicable. Trans-
formingWikipedia into training data thus provides
a free and high-yield alternative to the laborious
manual annotation required for NER.
Acknowledgments
We would like to thank the Language Technol-
ogy Research Group and the anonymous review-
ers for their feedback. This project was sup-
ported by Australian Research Council Discovery
Project DP0665973 and Nothman was supported
by a University of Sydney Honours Scholarship.
619
References
Joohui An, Seungwoo Lee, and Gary Geunbae Lee.
2003. Automatic acquisition of named entity tagged
corpus from world wide web. In The Companion
Volume to the Proceedings of 41st Annual Meeting
of the Association for Computational Linguistics,
pages 165?168.
Ada Brunstein. 2002. Annotation guidelines for an-
swer types. LDC2005T33.
Nancy Chinchor. 1998. Overview of MUC-7. In Proc.
of the 7th Message Understanding Conference.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Proceedings of the
NIPS Workshop on Advances in Structured Learning
for Text and Speech Processing.
Michael Collins. 2002. Ranking algorithms for
named-entity extraction: boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
489?496, Morristown, NJ, USA.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of the 7th Conference on Natural
Language Learning, pages 164?167.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 107?114, Budapest, Hungary.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91?134.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 698?707.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus?a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1):i180?i182.
Christopher Manning. 2006. Doing named entity
recognition? Don?t optimize for F1. In NLPers
Blog, 25 August. http://nlpers.blogspot.
com.
Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of the 9th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 1?8, Bergen, Norway.
2001. Message Understanding Conference (MUC) 7.
Linguistic Data Consortium, Philadelphia.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30:3?26.
David Nadeau, Peter D. Turney, and Stan Matwin.
2006. Unsupervised named-entity recognition:
Generating gazetteers and resolving ambiguity. In
Proceedings of the 19th Canadian Conference on
Artificial Intelligence, volume 4013 of LNCS, pages
266?277.
NIST-ACE. 2008. Automatic content extraction 2008
evaluation plan (ACE08). NIST, April 7.
Joel Nothman, James R. Curran, and Tara Murphy.
2008. Transforming Wikipedia into named entity
training data. In Proceedings of the Australian Lan-
guage Technology Workshop, pages 124?132, Ho-
bart.
Alexander E. Richman and Patrick Schone. 2008.
Mining wiki resources for multilingual named entity
recognition. In 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1?9, Columbus, Ohio.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the 7th Conference on Natural Lan-
guage Learning, pages 142?147.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning, pages
1?4.
Antonio Toral, Rafael Mun?oz, and Monica Monachini.
2008. Named entityWordNet. In Proceedings of the
6th International Language Resources and Evalua-
tion Conference.
Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi
Chou, Yu-Chun Lin, Ding He, Jieh Hsiang, Ting-
Yi Sung, and Wen-Lian Hsu. 2006. Various criteria
in the evaluation of biomedical named entity recog-
nition. BMC Bioinformatics, 7:96?100.
RalphWeischedel and Ada Brunstein. 2005. BBN Pro-
noun Coreference and Entity Type Corpus. Linguis-
tic Data Consortium, Philadelphia.
620
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Named Entity Recognition in Wikipedia
Dominic Balasuriya Nicky Ringland Joel Nothman Tara Murphy James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{dbal7610,nicky,joel,tm,james}@it.usyd.edu.au
Abstract
Named entity recognition (NER) is used in
many domains beyond the newswire text
that comprises current gold-standard cor-
pora. Recent work has used Wikipedia?s
link structure to automatically generate
near gold-standard annotations. Until now,
these resources have only been evaluated
on newswire corpora or themselves.
We present the first NER evaluation on
a Wikipedia gold standard (WG) corpus.
Our analysis of cross-corpus performance
on WG shows that Wikipedia text may
be a harder NER domain than newswire.
We find that an automatic annotation of
Wikipedia has high agreement with WG
and, when used as training data, outper-
forms newswire models by up to 7.7%.
1 Introduction
Named Entity Recognition (NER) is the task of
identifying and classifying people, organisations
and other named entities (NE) within text. NER is
central to many NLP systems, especially informa-
tion extraction and question answering.
Machine learning approaches now dominate
NER, learning patterns associated with individual
entity classes from annotated training data. This
training data, including English newswire from
the MUC-6, MUC-7 (Chinchor, 1998), and CONLL-
03 (Tjong Kim Sang and De Meulder, 2003) com-
petitive evaluation tasks, and the BBN Pronoun
Coreference and Entity Type Corpus (Weischedel
and Brunstein, 2005), is critical to the success of
these approaches.
This data dependence has impeded the adapta-
tion or porting of existing NER systems to new
domains, such as scientific or biomedical text,
e.g. Nobata et al (2000). Similar domain sensi-
tivity is exhibited by most tasks across NLP, e.g.
parsing (Gildea, 2001), and the adaptation penalty
is still apparent even when the same set of named
entity classes is used in text from similar domains
(Ciaramita and Altun, 2005).
Wikipedia is an important corpus for informa-
tion extraction, e.g. Bunescu and Pas?ca (2006)
and Wu et al (2008) because of its size, cur-
rency, rich semi-structured content, and its closer
resemblance to web text than newswire. Recently,
Wikipedia?s markup has been exploited to auto-
matically derive NE annotated text for training sta-
tistical models (Richman and Schone, 2008; Mika
et al, 2008; Nothman et al, 2008).
However, without a gold standard, existing eval-
uations of these models were forced to compare
against mismatched newswire corpora or the noisy
Wikipedia-derived annotations themselves. Fur-
ther, it was not possible to directly ascertain the
accuracy of these automatic extraction methods.
We have manually annotated 39,007 tokens of
Wikipedia with coarse-grained named entity tags
(WG). We present the first evaluation of Wiki-
pedia-trained models on Wikipedia: the C&C NER
tagger (Curran and Clark, 2003b) trained on (a)
automatically annotated Wikipedia text (WP2) ex-
tracted by Nothman et al (2009); and (b) tradi-
tional newswire NER corpora (MUC, CONLL and
BBN). The WP2 model, though trained on noisy
annotations, outperforms newswire models on WG
by 7.7%. However, every model, including WP2,
performs far worse on WG than on the newswire.
We examined the quality of WG, and found that
our annotation strategy produced a high-quality,
consistent corpus. Our analysis suggests that it is
the form and distribution of NEs in Wikipedia that
make it a difficult target domain.
Finally, we compared WG with the annotations
extracted by Nothman et al (2009), and found
agreement comparable to our inter-annotator
agreement, demonstrating that NE corpora can be
derived very accurately from Wikipedia.
10
2 Background
Traditional evaluations of NER have considered
the performance of a tagger on test data from the
same source as its training data. Although the
majority of annotated corpora available consist of
newswire text, recent practical applications cover
a far wider range of genres, including Wikipedia,
blogs, RSS feeds, and other data sources. Cia-
ramita and Altun (2005) showed that even when
moving a short distance, e.g. annotating WSJ text
with the same scheme as CONLL?s Reuters, the per-
formance was 26% worse than on the original text.
Similar differences are reported by Nothman
et al (2009) who compared MUC, CONLL and
BBN annotations reduced to a common tag-set.
They found poor cross-corpus performance to be
due to tokenisation and annotation scheme mis-
match, missing frequent lexical items, and naming
conventions. They then compared automatically-
annotated Wikipedia text as training data and
found it also differs in otherwise inconsequen-
tial ways from the newswire corpora, in particular
lacking abbreviations necessary to tag news text.
2.1 Automatic Wikipedia annotation
Wikipedia, a collaboratively-written online ency-
clopedia, is readily exploited in NLP, because it is
large, semi-structured and multilingual. Its arti-
cles often correspond to NEs, so it has been used
for NE recognition (Kazama and Torisawa, 2007)
and disambiguation (Bunescu and Pas?ca, 2006;
Cucerzan, 2007). Wikipedia links often span NEs,
which may be exploited to automatically create
annotated NER training data by determining the
entity class of the linked article and then labelling
the link text with it.
Richman and Schone (2008) use article clas-
sification knowledge from English Wikipedia to
produce NE-annotated corpora in other languages
(evaluated against NE gold standards for French,
Spanish, and Ukrainian). Mika et al (2008) ex-
plored the use of tags from a CONLL-trained tag-
ger to seed the labelling of entities and evaluate
the performance of a Wikipedia-trained model by
hand.
We make use of an approach described by Noth-
man et al (2009) which is engineered to perform
well on BBN data with a reduced tag-set (LOC,
MISC, ORG, PER). They derive an annotated cor-
pus with the following steps:
1. Classify Wikipedia articles into entity classes
2. Split the articles into tokenised sentences
3. Label expanded links according to target NEs
4. Select sentences for inclusion in a corpus
To prepare the text, they use mwlib (Pedi-
aPress, 2007) to parse Wikipedia?s native markup
retaining only paragraph text with links, ap-
ply Punkt (Kiss and Strunk, 2006) estimated on
Wikipedia text to perform sentence boundary de-
tection, and tokenise the resulting text using regu-
lar expressions.
Nothman et al (2009) infer additional NEs not
provided by existing links, and apply rules to ad-
just link boundaries and classifications to closer
match BBN annotations.
2.2 NER evaluation
Meaningful automatic evaluation of NER is dif-
ficult and a number of metrics have been pro-
posed (Nadeau and Sekine, 2007). Ambiguity
leads to entities correctly delimited but misclas-
sified, or boundaries mismatched despite correct
classification.
Although the MUC-7 evaluation (Chinchor,
1998) defined a metric which was less sensitive
to often-meaningless boundary errors, we consider
only exact entity matches as correct, following
the standard CONLL evaluation (Tjong Kim Sang,
2002). We report precision, recall and F -score for
each entity type.
3 Creating the Wikipedia gold standard
We created a corpus by manually annotating the
text of 149 articles from the May 22, 2008 dump
of English Wikipedia. The articles were selected
at random from all articles describing named en-
tities, with a roughly equal proportion of arti-
cle topics from each of the four CONLL-03 classes
(LOC, MISC, ORG, PER). We adopted Nothman et
al.?s (2008) preprocessing described above to pro-
duce tokenised sentences for annotation.
Only body text was extracted from the chosen
articles for inclusion in the corpus. Four articles
were found not to have any usable text, consisting
solely of tables, lists, templates and section head-
ings, which we remove. Their exclusion leaves a
corpus of 145 articles.
3.1 Annotation
Annotation was initially carried out using a fine-
grained tag-set which was expanded by the an-
11
[COMPANY Aero Gare] was a kitplane manufacturer
founded by [PERSON Gary LeGare] in [CITY Mojave] ,
[STATE California] to marketed the [PLANE Sea Hawker]
amphibious aircraft .
[ORG Aero Gare] was a kitplane manufacturer founded
by [PER Gary LeGare] in [LOC Mojave] , [LOC Califor-
nia] to marketed the [MISC Sea Hawker] amphibious
aircraft .
(a) Fine-grained annotation (b) Coarse-grained annotation
Figure 1: An example of coarse and fine-grained annotation of Wikipedia text.
notators as annotation progressed, and eventually
contained 96 tags.
We created a mapping from these fine-grained
tags to the four coarse-grained tags used in the
CONLL-03 data: PER, LOC, MISC and ORG. This
enables evaluation with existing NER models. We
believe this two-phase approach allowed anno-
tators to defer difficult mapping decisions, (e.g.
should an airport be classified as a LOC, ORG, or
MISC?) which can then be made after discussion.
The mapping could also be modified to suit a par-
ticular evaluation task.
Figure 1 shows an example of the use of fine and
coarse-grained tags to annotate a sentence. Tags
such as PERSON correspond directly to coarse-
grained tags, while most map to a more general
tag, such as STATE and CITY mapping to LOC.
PLANE is an example of a fine-grained tag that
cannot be mapped to LOC, ORG, or PER. These
tags may be mapped to MISC; some are not con-
sidered entities under the CONLL scheme and are
left unlabelled in the coarse-grained annotation.
Three independent annotators were involved in
the annotation process. Annotator 1 annotated all
145 articles using the fine-grained tags. Annota-
tors 2 and 3 then re-annotated 19 of these articles
(316 sentences or 8030 tokens), amounting to 21%
of the corpus. Annotator 2 used the fine-grained
tags described above, while Annotator 3 used the
four coarse-grained CONLL tags. To measure vari-
ation, all three annotations of this common portion
were mapped down to the CONLL tag-set and inter-
annotator agreement was calculated.
We found that 202 tokens were disagreed upon
by at least one annotator (2.5% of all tokens
annotated), and these discrepancies were then
discussed by the three annotators. The inter-
annotator agreement will be analysed in more de-
tail in Section 5.
Sentences containing grammatical and typo-
graphical errors were not corrected, so that the cor-
pus would be as close as possible to the source
text. Web text often contains errors, such as to
Train Test P R F
WP2 WG 66.5 67.4 66.9
BBN WG 59.2 59.1 59.2
CONLL WG 54.3 57.2 55.7
WP2 * WG * 75.1 67.7 71.2
BBN * WG * 57.2 64.1 60.4
CONLL * WG * 53.1 62.7 57.5
MUC * WG * 52.3 57.2 54.6
WP2 BBN 73.4 74.6 74.0
WP2 CONLL 73.6 64.9 69.0
WP2 * MUC * 86.2 68.9 76.6
BBN BBN 85.7 87.3 86.5
CONLL CONLL 85.3 86.5 85.9
MUC MUC 81.0 83.6 82.3
Table 2: Tagger performance on various corpora.
Asterisks indicate that MISC tags are ignored.
marketed the Sea Hawker from the example in Fig-
ure 1, so any NER system must deal with these er-
rors. Sentences with poor tokenisation or sentence
boundary detection were identified and corrected
manually, since these errors are introduced by our
processing and annotation, and do not exist in the
source text.
The final corpus was created by correcting an-
notation mistakes, with annotators 2 and 3 each
correcting 50% of the corpus. The fine-grained
tags were mapped to the four CONLL tags before
the final corrections were made. The final WG cor-
pus consists of the body text of 145 Wikipedia ar-
ticles tagged with the four CONLL-03 tags.
4 NER on the Wikipedia gold-standard
Nothman et al (2009) have previously shown that
that an NER system trained on automatically anno-
tated Wikipedia corpora performs reasonably well
on non-Wikipedia text. Having created our WG
corpus of gold-standard annotations, we are able
to evaluate the performance of these models on
Wikipedia text.
We compare the C&C NE maximum-entropy
tagger (Curran and Clark, 2003b) trained on
gold-standard newswire corpora (MUC-7, BBN and
CONLL-03) with the same tagger trained on auto-
matically annotated Wikipedia text, WP2. WG is
12
WG WP2 BBN CONLL-03 MUC-7
Test Train Train Test Train Test Train Test
Tokens 39 007 3 500 032 901 849 129 654 203 621 46 435 83 601 60 436
Sentences 1 696 146 543 37 843 5 462 14 987 3 453 3 485 2 419
Articles 145 ? 1775 238 946 231 102 99
NEs 3 558 288 545 49 999 7 307 23 498 5 648 4 315 3 540
Table 1: Corpus sizes.
too small to train a reasonable NER model on gold-
standard Wikipedia annotations. Part-of-speech
tags are added to all corpora using the C&C POS
tagger (Curran and Clark, 2003a) before training
and testing.
1
We evaluate each model on tradi-
tional newswire evaluation corpora as well as WG.
Table 1 gives the size of each corpus.
The results are shown in Table 2. The WP2 tag-
ger performed substantially better on WG than tag-
gers trained on newswire text, with a 7?11% in-
crease in F -score compared to BBN and CONLL-
03, and a 16% increase compared to MUC-7, when
miscellaneous NEs in the corpus are not consid-
ered in the evaluation. The Wikipedia trained
model thus outperforms newswire models on our
new WG corpus even though the training annota-
tions were automatically extracted.
The WP2 tagger performed worse on WG than
on gold-standard news corpora (BBN and CONLL),
with a 2?7% reduction in F -score. Further, the
performance of WP2 on WG is 11?20% F -score
lower than same-source evaluation results, e.g.
BBN on BBN, CONLL on CONLL. Therefore, de-
spite WP2 showing an advantage in tagging WG
due to their common source domain, we find that
WG?s annotations are harder to predict than the
newswire test data commonly used for evaluation.
One possible explanation is that our WG corpus
has been inconsistently annotated. When NEs of
miscellaneous type are not considered in the eval-
uation (asterisks in Table 2), the performance of all
taggers on WG improves, with WP2 demonstrating
a 4% increase. This result suggests another par-
tial explanation: that MISC NEs in Wikipedia are
more difficult to annotate correctly, due to their
poor definition and broad coverage. A third ex-
planation is that the automatic conversion process
proposed by Nothman et al (2008) produces much
lower quality training data than manual annota-
tion. We explore these three possibilities below.
1
Both taggers are available from http://svn.ask.
it.usyd.edu.au/trac/candc.
Token Exact NE only
A1 and A2 0.95 0.99 0.88
A1 and A3 0.91 0.95 0.81
A2 and A3 0.91 0.96 0.79
Fleiss? Kappa 0.92 0.97 0.83
Table 3: Initial human inter-annotator agreement.
5 Quality of the Wikipedia gold standard
The low performance observed on WG may be
due to the poor quality of its annotation. We en-
sure that this is not the case by measuring inter-
annotator agreement. The WG annotation process
produced three independent annotations of a sub-
set of WG. These annotations were compared us-
ing Cohen?s ? (Fleiss and Cohen, 1973) between
pairs of annotators, and Fleiss? ? (Fleiss, 1971),
which generalises Cohen?s ? to more than two
concurrent annotations.
Table 3 shows the three types of ? values cal-
culated. Token is calculated on a per token basis,
comparing the agreement of annotators on each
token in the corpus; NE only, is calculated on
the agreement between entities alone, excluding
agreement in cases where all annotators agreed
that a token was not a NE; Exact refers to the
agreement between annotators where all annota-
tors have agreed on the boundaries of a NE, but
disagree on the type of NE.
Annotator 1 originally annotated the entire cor-
pus, and Annotators 2 and 3 then corrected exactly
half of the corpus each after a discussion between
the three annotators to resolve ambiguities. Landis
and Koch (1977) determine that a ? value greater
than 0.81 indicates almost perfect agreement. By
this standard, our three annotators were in strong
agreement prior to discussion, with our Fleiss? ?
values all greater than 0.81. Inconsistencies in the
corpus due to annotation mistakes by Annotator 1
were corrected by Annotators 2 and 3.
Inter-annotator agreement for cases where the
annotators agreed on NE boundaries was higher
than agreement on each token, which suggests
that many discrepancies resulted from NE bound-
13
LOC MISC ORG PER H(C): With O Without O Total NEs % NE tokens
WG 28.5 20.0 25.2 26.3 0.98 2.0 3 558 17.1
BBN 22.4 9.8 46.4 21.3 0.61 1.7 49 999 9.6
MUC 33.3 ? 40.7 26.1 0.52 1.5 4 315 8.1
CONLL 30.4 14.6 26.9 28.1 0.98 1.9 23 498 17.1
Table 4: NE class distribution, tag entropy and NE density statistics for gold-standard corpora and WG.
ary ambiguities, or disagreement as to whether
a phrase constituted a NE at all. Higher inter-
annotator agreement between Annotators 1 and 2
leads us to believe that the two-phase annotation
strategy, where an initially fine-grained tag-set is
reduced, results in more consistent annotation.
Our analysis demonstrates that WG is annotated
in a consistent and accurate manner and the small
number of errors cannot alone explain the reduced
performance figures.
6 Comparing gold-standard corpora
6.1 NE class distribution
Table 4 compares the distribution of different
classes of NEs across different corpora on the four
CONLL categories. WG has a higher proportion of
PER and MISC NEs and a lower proportion of ORG
NEs than the BBN corpus. This is also found in the
MUC corpus, although comparisons to MUC are af-
fected by its lack of a MISC category. The CONLL-
03 corpus is most similar to WG in terms of the
distribution of the NE classes, although CONLL-03
has a smaller proportion of MISC NEs than WG.
An analysis of the lengths of NEs in CONLL shows,
however, that they are very different to those in
WG (see Table 8), perhaps explaining the differ-
ence in performance observed.
Tag entropy H(C) was calculated for each cor-
pus with respect to the 5 possible classes (4 NE
classes, and the O tag, indicating non-entities).
H(C) is a measure of the amount of information
required to represent the classification of each to-
ken in the corpus. Two calculations are made, in-
cluding and excluding the frequent O tag. Our re-
sults (Table 4) suggest that WG?s tags are least pre-
dictable, with a tag entropy of 2.0 bits (without the
O class) compared to 1.7 and 1.9 bits for BBN and
CONLL respectively.
6.2 Fine-grained class distribution
While the CONLL-03 and MUC evaluation corpora
are marked up with only very coarse tags, the BBN
corpus uses 29 coarse tags, many with specific
subtypes, including NEs, descriptors of NEs and
Mapped BBN tag WG BBN
PERSON 25.9 19.3
ORGANIZATION:OTHER 13.0 2.8
ORGANIZATION:CORPORATION 9.2 43.1
GPE:CITY 8.0 6.7
WORK OF ART:SONG 4.7 0.1
NORP 4.3 3.1
WORK OF ART:OTHER 4.1 1.3
GPE:COUNTRY 3.5 5.1
ORGANIZATION:EDUCATIONAL 3.0 0.9
GPE:STATE PROVINCE 2.8 2.8
ORGANIZATION:POLITICAL 2.6 0.6
EVENT:OTHER 2.5 0.4
ORGANIZATION:GOVERNMENT 2.0 7.5
WORK OF ART:BOOK 1.6 0.4
EVENT:WAR 1.6 0.1
FAC:OTHER 1.4 0.2
LOCATION:REGION 1.3 0.8
FAC:ATTRACTION 1.2 0.0
Table 5: Distribution of some fine-grained tags
non-NEs, intended as answer types for question
answering (Brunstein, 2002). Non-NE types in-
clude MONEY and TIME, which are also tagged in
the MUC corpus, and others such as ANIMAL. When
evaluating the performance of the taggers, each of
BBN?s 150 fine-grained tags was mapped to one of
four coarse-grained classes or none, using a map-
ping described in Nothman (2008).
However, since the WG corpus was initially an-
notated using 96 distinct classes, we map these
tags to the corresponding fine-grained BBN NE
classes. In some cases, the tags map exactly
(e.g. COUNTRY mapped to LOCATION:COUNTRY);
in other cases, classes have to be merged or not
mapped at all, where the BBN and WG annotations
differ in granularity. Where possible, we map to
fine-grained BBN categories.
We create mappings to a total of 36 BBN entity
types, and apply them across the WG corpus. Table
5 shows the distribution of the most common tags,
calculated as a percentage of all counts of the 36
selected tags across each corpus. Tags for which
there is at least a two-fold difference in proportion
between BBN and WG are marked in bold.
The comparison is dominated by the
presence of a disproportionate number of
ORG:CORPORATIONS in the BBN corpus com-
14
1 2 3 4 5 6 7+ # NEs
WG 53.0 77.0 88.9 94.8 96.6 98.2 100 712
BBN (train) 75.0 91.0 95.4 97.2 98.2 98.7 100 4 913
CONLL (train) 75.0 93.8 98.1 99.5 99.9 99.9 100 3 437
Table 6: Comparing MISC NE lengths (cumulative).
Feature group WG BBN CONLL
Current token 0.88 0.89 0.93
Current POS 0.43 0.57 0.48
Current word-type 0.42 0.49 0.48
Previous token 0.46 0.43 0.47
Previous POS 0.12 0.19 0.14
Previous word-type 0.07 0.14 0.12
Table 7: Feature-tag gain ratios.
pared to WG. It also mentions many more
governmental organisations. Prominent cases of
tags found in higher proportions in WG are works
of art, organisations of type OTHER (e.g. bands,
sports teams, clubs), events and attractions.
This comparison demonstrates that there are ob-
servable differences in NE types between the news
and Wikipedia domains. These differences are re-
flected in the distribution of both coarse and fine-
grained types of NEs. The more complex entity
distribution in Wikipedia is a likely cause for re-
duced NER performance on WG.
6.3 Feature-tag gain
Nobata et al (2000) use gain ratio as an infor-
mation-theoretic measure of corpus difficulty:
GR(C;F ) =
I(C;F )
H(C)
where I(C;F ) = H(C) ? H(C|F ) is the infor-
mation gain of the NE tag distribution (C) with re-
spect to a feature set F .
This gain ratio normalises the information gain
over the tag entropy, which Nobata et al (2000)
suggest allows us to compare gain ratios between
corpora. It also makes the impact of including the
?O? tag negligible for our calculations.
We apply this approach to measure the relative
difficulty of tagging NEs in the WG corpus. Ta-
ble 7 shows that WG tags seem generally harder
to predict than those in newswire, on the basis of
words, POS tags or orthographic word-types (like
those used in the Curran and Clark (2003b) tagger
as proposed by Collins (2002)).
In particular, POS tags are less indicative than
in BBN and CONLL, suggesting a wider variety of
1 2 3 4 5 6 7+
WG 49.9 81.7 93.1 97.4 98.6 99.4 100
BBN (train) 57.4 83.3 92.9 97.4 99.1 99.6 100
CONLL (train) 63.1 94.5 98.4 99.4 99.8 99.9 100
MUC (train) 62.0 89.1 96.1 99.1 99.7 99.8 100
Table 8: Comparing all NE lengths (cumulative).
grammatical functions in NE names in Wikipedia
? this might be expected with more band names,
and song and movie titles. Alternatively, it may be
an indication that the POS tagging is less reliable
on Wikipedia using newswire-trained models.
The previous word?s orthographic form also
provides less information, which may relate to ti-
tles like Mr. and Mrs., strong indicators of PER en-
tities, which are frequent in BBN and to a lesser
extent CONLL, but are almost absent in Wikipedia.
6.4 Lengths of named entities
The number of tokens in NEs is substantially dif-
ferent between WG and other gold-standard cor-
pora. When compared with WG, other gold-
standard corpora have a larger proportion of
single-word NEs (between 7 and 13% more), as
shown in Table 8. The distribution of NE lengths
in BBN is most similar to WG, but it still differs
significantly in the proportion of single-word NEs.
Additionally, WG has a larger number of long
multi-word NEs than the other gold-standard cor-
pora. Longer entities are more difficult to clas-
sify, since boundary resolution is more error prone
and they typically contain lowercase words with
a wider range of syntactic roles. This adds to the
difficulty of correctly identifying NEs in WG.
The difference in entity lengths is most pro-
nounced MISC NEs (Table 6), with Wikipedia hav-
ing a substantially smaller number of single-word
MISC NEs. The presence of a large number of long
miscellaneous NEs, including song, film and book
titles, and other works of art are a feature that char-
acterises the nature of Wikipedia text in contrast
to newswire text. Typically, longer MISC NEs in
newswire text are laws and NORPs, which also ap-
pear in Wikipedia text.
15
1 2 3 4 5 6 7+ # NEs
WG 49.2 82.9 94.2 98.0 99.2 99.8 100 2 846
BBN (train) 55.4 82.4 92.6 97.4 99.2 99.7 100 45 086
CONLL (train) 61.1 94.7 98.4 99.4 99.8 99.9 100 20 061
MUC (train) 62.0 89.1 96.1 99.1 99.7 99.8 100 4 315
Table 9: Comparing non-MISC NE lengths (cumulative).
# Sents # with NEs # NEs
WG 1 696 1 341 3 558
WG WP2-style 571 298 569
WG WP4-style 698 425 831
Table 10: Size of WG and auto-annotated subsets.
7 Evaluation of automatic annotation
We compared the gold-standard annotations in our
WG corpus to those sentences that were automati-
cally annotated by Nothman et al (2009). Their
automatic annotation process does not retain all
Wikipedia sentences. Rather, it selects sentences
where, on the basis of capitalisation heuristics,
it seems all named entities in the sentence have
been tagged by the automatic process. We adopt
this confidence criterion to produce automatically-
annotated subsets of the WG corpus.
Two variants of their automatic annotation pro-
cedure were used: WP2 uses a few rules to infer
tags for non-linked NEs in Wikipedia; WP4 has
looser criteria for inferring additional links, and its
over-generation typically reduced its performance
as training data (Nothman et al, 2009).
A large proportion of sentences in our WG cor-
pus cannot be automatically tagged with confi-
dence. Sentence selection leaves 571 sentences
(33.7%) after the WP2 process and 698 (41.2%)
after the WP4 process (see Table 10). The use of
the more permissive WP4 process may lead to the
labelling of more NEs, but many may be spurious.
We use three approaches to compare automatic
and manual annotations of WG text: (a) treat each
corpus as test data and evaluate NER performance
on each; (b) treat WP2 and WP4-style subsets as
NER predictions on the WG corpus to calculate an
F -score; and (c) treat the automatic annotations
like human annotators and calculate ? values.
We first evaluate the WP2 model on each
corpus and find that performance is higher on
automatically-annotated subsets of WG (Table 11).
This is unsurprising given the common automatic
annotation process and the effects of the selection
criterion. However, Nothman (2008) provides an
TRAIN TEST P R F
WP2 WG manual 66.5 67.4 66.9
WP2 WG WP2-style 76.0 72.9 74.4
WP2 WG WP4-style 75.5 71.4 73.4
WP2 WP2 ten folds ? ? 83.6
WP2 * WG manual * 75.1 67.7 71.2
WP2 * WG WP2-style * 81.5 74.4 77.8
WP2 * WG WP4-style * 81.9 74.6 78.1
WP2 * WP2 ten folds * ? ? 86.1
Table 11: NER performance of the WP2-trained
model on auto-annotated subsets of WG.
? NE ? P R F
WP2-style 0.94 0.84 89.0 89.0 89.0
WP4-style 0.93 0.83 86.8 87.6 87.2
Table 12: Comparing WP2-style WG and WP4-
style WG on WG. The automatically annotated data
was treated as predicted annotations on WG.
F -score for the WP2 model when evaluated on 10
folds of automatically-annotated (WP2-style) test
data. This F -score is 8?10% higher than WP2?s
performance on the WP2-style subset of WG, sug-
gesting that WG?s text is somewhat more difficult
to annotate than typical portions of WP2-style text.
We compare the annotations of WG text more
directly by treating the automatic annotations as
if they are the output from a tagger run on the 698
and 571 sentences that were confidently chosen. A
reasonable agreement between the gold standard
and automatic annotation is observed (Table 12),
with F -scores of 87.2% and 89.0% achieved by
WP2 and WP4.
Table 12 also shows inter-annotator agreement
calculated between the automatically annotated
subsets and the gold-standard annotations in WG,
using Cohen?s ? in the same way as for human an-
notators. The agreement was very high: equal or
better than the agreement between human annota-
tors prior to discussion and correction.
8 Conclusion
We have presented the first evaluation of named
entity recognition (NER) on a gold-standard eval-
uation of Wikipedia, a resource of increasing
16
importance in Computational Linguistics. We
annotated a corpus of Wikipedia articles (WG)
with gold-standard NE tags. Using this new re-
source as test data we have evaluated models
trained on three gold-standard newswire corpora
for NER, and compared them to a model trained on
Wikipedia-derived NER annotations (Nothman et
al., 2009). We found that this WP2 model outper-
formed models trained on MUC, CONLL, and BBN
data by more than 7.7% F -score.
However, we found that all four models per-
formed significantly worse on the WG corpus than
they did on news text, suggesting that Wikipedia
as a textual domain is more difficult for NER. We
initially suspected that annotation quality was re-
sponsible, but found that we had very high inter-
annotator agreement even before further discus-
sion and correction of the corpus. This also val-
idates our approach of creating many fine-grained
categories and then reducing them down to the
four CONLL types.
To further examine the difficulty of tagging WG,
we compared the distribution of fine-grained entity
types in WG and BBN, finding a more even dis-
tribution over a larger range of types in WG. We
found that the standard NER features such as cur-
rent and previous POS tags and words had lower
predictive power on WG. We also compared the
distribution of NEs lengths and showed that WG
entities are longer on average (for instance song
and book titles). This all suggests that Wikipedia
is genuinely more difficult to automatically anno-
tate with named entities than newswire.
Finally, we compared the common sentences
between Nothman et al?s (2009) automatic NE an-
notation of Wikipedia and WG, directly measuring
the quality of automatically deriving NE annota-
tions from Wikipedia.
We found that WP2 agreed with our final
WG corpus to a high degree, demonstrating that
Wikipedia is a viable source of automatically an-
notated NE annotated data, reducing our depen-
dence on expensive manual annotation for training
NER systems.
Acknowledgements
We would like to thank the anonymous review-
ers for their helpful feedback. This work was
supported by the Australian Research Council un-
der Discovery Project DP0665973. Dominic Bal-
asuriya was supported by a University of Syd-
ney Outstanding Achievement Scholarship. Nicky
Ringland was supported by a Capital Markets
CRC High Achievers Scholarship. Joel Noth-
man was supported by a Capital Markets CRC
PhD Scholarship and a University of Sydney Vice-
Chancellor?s Research Scholarship.
References
Ada Brunstein. 2002. Annotation guidelines for an-
swer types. LDC2005T33, Linguistic Data Consor-
tium, Philadelphia.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of the 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 9?16.
Nancy Chinchor. 1998. Overview of MUC-7. In Pro-
ceedings of the 7th Message Understanding Confer-
ence.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Proceedings of the
NIPS Workshop on Advances in Structured Learning
for Text and Speech Processing.
Michael Collins. 2002. Ranking algorithms for
named-entity extraction: boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 489?496.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 708?
716.
James R. Curran and Stephen Clark. 2003a. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proceedings of the 10th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 91?98, Budapest, Hungary,
12?17 April.
James R. Curran and Stephen Clark. 2003b. Language
independent NER using a maximum entropy tagger.
In Proceedings of the 7th Conference on Natural
Language Learning, pages 164?167.
Joseph L. Fleiss and Jacob Cohen. 1973. The equiv-
alence of weighted kappa and the intraclass corre-
lation coefficient as measures of reliability. Educa-
tional and Psychological Measurement, 33(3):613.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
17
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In 2001 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
Pittsburgh, PA.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 698?707.
Tibor Kiss and Jan Strunk. 2006. Unsupervised mul-
tilingual sentence boundary detection. Computa-
tional Linguistics, 32(4):485?525.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza,
and Jordi Atserias. 2008. Learning to tag and tag-
ging to learn: A case study on wikipedia. IEEE In-
telligent Systems, 23(5, Sep./Oct.):26?33.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30:3?26.
Chikashi Nobata, Nigel Collier, and Jun?ichi Tsuji.
2000. Comparison between tagged corpora for the
named entity task. In Proceedings of the Workshop
on Comparing Corpora, pages 20?27.
Joel Nothman, James R Curran, and Tara Murphy.
2008. Transforming Wikipedia into named entity
training data. In Proceedings of the Australasian
Language Technology Association Workshop 2008,
pages 124?132, Hobart, Australia, December.
Joel Nothman, Tara Murphy, and James R. Curran.
2009. Analysing Wikipedia and gold-standard cor-
pora for NER training. In Proceedings of the
12th Conference of the European Chapter of the
ACL (EACL 2009), pages 612?620, Athens, Greece,
March.
Joel Nothman. 2008. Learning Named Entity Recogni-
tion from Wikipedia. Honours Thesis. School of IT,
University of Sydney.
PediaPress. 2007. mwlib MediaWiki parsing library.
http://code.pediapress.com.
Alexander E. Richman and Patrick Schone. 2008.
Mining wiki resources for multilingual named en-
tity recognition. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1?
9, Columbus, Ohio.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recogni-
tion. In Proceedings of the 7th Conference on Natu-
ral Language Learning, pages 142?147, Edmonton,
Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning, pages
1?4, Taipei, Taiwan.
Ralph Weischedel and Ada Brunstein. 2005.
BBN Pronoun Coreference and Entity Type Cor-
pus. LDC2005T33, Linguistic Data Consortium,
Philadelphia.
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008.
Information extraction from Wikipedia: Moving
down the long tail. In Proceedings of the 14th In-
ternational Conference on Knowledge Discovery &
Data Mining, Las Vegas, USA, August.
18
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 38?41,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Evaluating a Statistical CCG Parser on Wikipedia
Matthew Honnibal Joel Nothman
School of Information Technologies
University of Sydney
NSW 2006, Australia
{mhonn,joel,james}@it.usyd.edu.au
James R. Curran
Abstract
The vast majority of parser evaluation is
conducted on the 1984 Wall Street Journal
(WSJ). In-domain evaluation of this kind
is important for system development, but
gives little indication about how the parser
will perform on many practical problems.
Wikipedia is an interesting domain for
parsing that has so far been under-
explored. We present statistical parsing re-
sults that for the first time provide infor-
mation about what sort of performance a
user parsing Wikipedia text can expect.
We find that the C&C parser?s standard
model is 4.3% less accurate on Wikipedia
text, but that a simple self-training ex-
ercise reduces the gap to 3.8%. The
self-training also speeds up the parser on
newswire text by 20%.
1 Introduction
Modern statistical parsers are able to retrieve accu-
rate syntactic analyses for sentences that closely
match the domain of the parser?s training data.
Breaking this domain dependence is now one
of the main challenges for increasing the indus-
trial viability of statistical parsers. Substantial
progress has been made in adapting parsers from
newswire domains to scientific domains, espe-
cially for biomedical literature (Nivre et al, 2007).
However, there is also substantial interest in pars-
ing encyclopedia text, particularly Wikipedia.
Wikipedia has become an influential resource
for NLP for many reasons. In addition to its va-
riety of interesting metadata, it is massive, con-
stantly updated, and multilingual. Wikipedia is
now given its own submission keyword in general
CL conferences, and there are workshops largely
centred around exploiting it and other collabora-
tive semantic resources.
Despite this interest, there have been few in-
vestigations into how accurately existing NLP pro-
cessing tools work on Wikipedia text. If it is found
that Wikipedia text poses new challenges for our
processing tools, then our results will constitute
a baseline for future development. On the other
hand, if we find that models trained on newswire
text perform well, we will have discovered another
interesting way Wikipedia text can be exploited.
This paper presents the first evaluation of a sta-
tistical parser on Wikipedia text. The only pre-
vious published results we are aware of were de-
scribed by Ytrest?l et al (2009), who ran the
LinGo HPSG parser over Wikipedia, and found
that the correct parse was in the top 500 returned
parses for 60% of sentences. This is an interesting
result, but one that gives little indication of how
well a user could expect a parser to actually anno-
tate Wikipedia text, or how to go about adjusting
one if its performance is inadequate.
To investigate this, we randomly selected 200
sentences from Wikipedia, and hand-labelled them
with CCG annotation in order to evaluate the C&C
parser (Clark and Curran, 2007). C&C is the fastest
deep-grammar parser, making it a likely choice for
parsing Wikipedia, given its size.
Even at the parser?s WSJ speeds, it would
take about 18 days to parse the current English
Wikipedia on a single CPU. We find that the parser
is 54% slower on Wikipedia text, so parsing a full
dump is inconvenient at best. The parser is only
4.3% less accurate, however.
We then examine how these figures might be
improved. We try a simple domain adaptation
experiment, using self-training. One of our ex-
periments, which involves self-training using the
Simple English Wikipedia, improves the accuracy
of the parser?s standard model on Wikipedia by
0.8%. The bootstrapping also makes the parser
faster. Parse speeds on newswire text improve
20%, and speeds on Wikipedia improve by 34%.
38
Corpus Sentences Mean length
WSJ 02-21 39,607 23.5
FEW 889,027 (586,724) 22.4 (16.6)
SEW 224,251 (187,321) 16.5 (14.1)
Table 1: Sentence lengths before (and after) length filter.
2 CCG Parsing
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a linguistically motivated grammar
formalism with several advantages for NLP. Like
HPSG, LFG and LTAG, a CCG parse recovers the
semantic structure of a sentence, including long-
range dependencies and complement/adjunct dis-
tinctions, providing substantially more informa-
tion than skeletal brackets.
Clark and Curran (2007) describe how a fast and
accurate CCG parser can be trained from CCGbank
(Hockenmaier and Steedman, 2007). One of the
keys to the system?s success is supertagging (Ban-
galore and Joshi, 1999). Supertagging is the as-
signment of lexical categories before parsing. The
parser is given only tags assigned a high proba-
bility, greatly restricting the search space it must
explore. We use this system, referred to as C&C,
for our parsing experiments.
3 Processing Wikipedia Data
We began by processing all articles from the
March 2009 dump of Simple English Wikipedia
(SEW) and the matching Full English Wikipedia
(FEW) articles. SEW is an online encyclopedia
written in basic English. It has stylistic guidelines
that instruct contributors to use basic vocabulary
and syntax, to improve the articles? readability.
This might make SEW text easier to parse, mak-
ing it useful for our self-training experiments.
mwlib (PediaPress, 2007) was used to parse
the MediaWiki markup. We did not expand tem-
plates, and retained only paragraph text tokenized
according to the WSJ, after it was split into sen-
tences using the NLTK (Loper and Bird, 2002) im-
plementation of Punkt (Kiss and Strunk, 2006) pa-
rameterised on Wikipedia text. Finally, we dis-
carded incorrectly parsed markup and other noise.
We also introduced a sentence length filter for
the domain adaptation data (but not the evaluation
data), discarding sentences longer than 25 words
or shorter than 3 words. The length filter was used
to gather sentences that would be easier to parse.
The effect of this filter is shown in Table 1.
4 Self-training Methodology
To investigate how the parser could be improved
on Wikipedia text, we experimented with semi-
supervised learning. We chose a simple method,
self-training. Unlabelled data is annotated by the
system, and the predictions are taken as truth and
integrated into the training system.
Steedman et al (2003) showed that the selec-
tion of sentences for semi-supervised parsing is
very important. There are two issues: the accu-
racy with which the data can be parsed, which de-
termines how noisy the new training data will be;
and the utility of the examples, which determines
how informative the examples will be.
We experimented with a novel source of data
to balance these two concerns. Simple English
Wikipedia imposes editorial guidelines on the
length and syntactic style authors can use. This
text should be easier to parse, lowering the noise,
but the syntactic restrictions might mean its exam-
ples have lower utility for adapting the parser to
the full English Wikipedia.
We train the C&C supertagger and parser (Clark
and Curran, 2007) on sections 02-21 of the Wall
Street Journal (WSJ) marked up with CCG annota-
tions (Hockenmaier and Steedman, 2007) in the
standard way. We then parse all of the Sim-
ple English Wikipedia remaining after our pre-
processing. We discard the 826 sentences the
parser could not find an analysis for, and set aside
1,486 randomly selected sentences as a future de-
velopment set, leaving a corpus of 185,000 auto-
matically parsed sentences (2.6 million words).
We retrain the supertagger on a simple concate-
nation of the 39,607 WSJ training sentences and
the Wikipedia sentences, and then use it with the
normal-form derivations and hybrid dependencies
model distributed with the parser
1
.
We repeated our experiments using text from
the full English Wikipedia (FEW) for articles
whose names match an article in SEW. We ran-
domly selected a sample of 185,000 sentences
from these, to match the size of the SEW corpus.
We also performed a set of experiments where
we re-parsed the corpus using the updated su-
pertagger and retrained on output, the logic being
that the updated model might make fewer errors,
producing higher quality training data. This itera-
tive retraining was found to have no effect.
1
http://svn.ask.it.usyd.edu.au/trac/candc
39
Model WSJ Section 23 Wiki 200 Wiki 90k
P R F speed cov P R F speed cov speed cov
WSJ derivs 85.51 84.62 85.06 545 99.58 81.20 80.51 80.86 394 99.00 239 98.81
SEW derivs 85.06 84.11 84.59 634 99.75 81.96 81.34 81.65 739 99.50 264 99.11
FEW derivs 85.24 84.32 84.78 653 99.79 81.94 81.36 81.65 776 99.50 296 99.15
WSJ hybrid 86.20 84.80 85.50 481 99.58 81.93 80.51 81.22 372 99.00 221 98.81
SEW hybrid 85.80 84.30 85.05 571 99.75 82.16 80.49 81.32 643 99.50 257 99.11
FEW hybrid 85.94 84.46 85.19 577 99.79 82.49 81.03 81.75 665 99.50 275 99.15
Table 2: Parsing results with automatic POS tags. SEW and FEW models incorporate self-training.
5 Annotating the Wikipedia Data
We manually annotated a Full English Wikipedia
evaluation set of 200 sentences. The sentences
were sampled at random from the 5000 articles
that were linked to most often by Wikipedia pages.
Articles used for self-training were excluded.
The annotation was conducted by one annota-
tor. First, we parsed the sentences using the C&C
parser. We then manually corrected the supertags,
supplied them back to the parser, and corrected
the parses using a GUI. The interface allowed the
annotator to specify bracket constraints until the
parser selected the correct analysis. The annota-
tion took about 20 hours in total.
We used the CCGbank manual (Hockenmaier
and Steedman, 2005) as the guidelines for our
annotation. There were, however, some system-
atic differences from CCGbank, due to the faulty
noun phrase bracketing and complement/adjunct
distinctions inherited from the Penn Treebank.
6 Results
The results in this section refer to precision, re-
call and F -Score over labelled CCG dependencies,
which are 5-tuples (head, child, category, slot,
range). Speed is reported as words per second, us-
ing a single core 2.6 GHz Pentium 4 Xeon.
6.1 Out-of-the-Box Performance
Our experiments were performed using two mod-
els provided with v1.02 of the C&C parser. The
derivs model is calculated using features from the
Eisner (1996) normal form derivation. This is the
model C&C recommend for general use, because
it is simpler and faster to train. The hybrid model
achieves the best published results for CCG pars-
ing (Clark and Curran, 2007), so we also experi-
mented with this model. The models? performance
is shown in the WSJ rows of Table 2. We report ac-
curacy using automatic POS tags, since we did not
correct the POS tags in the Wikipedia data.
The derivs and hybrid models show a simi-
lar drop in performance on Wikipedia, of about
4.3%. Since this is the first accuracy evalua-
tion conducted on Wikipedia, it is possible that
Wikipedia data is simply harder to parse, possi-
bly due to its wider vocabulary. It is also possible
that our manual annotation made the task slightly
harder, because we did not reproduce the CCGbank
noun phrase bracketing and complement/adjunct
distinction errors.
We also report the parser?s speed and coverage
on Wikipedia. Since these results do not require
labelled data, we used a sample of 90,000 sen-
tences to obtain more reliable figures. Speeds var-
ied enormously between this sample and the 200
annotated sentences. A length comparison reveals
that our manually annotated sentences are slightly
shorter, with a mean of 20 tokens per sentence.
Shorter sentences are often easier to parse, so this
issue may have affected our accuracy results, too.
The 54% drop in speed on Wikipedia text is ex-
plained by the way the supertagger and parser are
integrated. The supertagger supplies the parser
with a beam of categories. If parsing fails, the
chart is reinitialised with a wider beam and it tries
again. These failures occur more often when the
supertagger cannot produce a high quality tag se-
quence, particularly if the problem is in the tag
dictionary, which constrains the supertagger?s se-
lections for frequent words. This is why we fo-
cused on the supertagger in our domain adaptation
experiments.
6.2 Domain Adaptation Experiments
The inclusion of parsed data from Wikipedia ar-
ticles in the supertagger?s training data improves
its accuracy on Wikipedia data, with the FEW en-
hanced model achieving 89.86% accuracy, com-
pared with the original accuracy of 88.77%. The
SEW enhanced supertagger achieved 89.45% ac-
curacy. The derivs model parser improves in ac-
curacy by 0.8%, the hybrid model by 0.5%.
40
The out-of-domain training data had little im-
pact on the models? accuracy on the WSJ, but
did improve parse speed by 20%, as it did on
Wikipedia. The speed increases because the su-
pertagger?s beam width is decided by its confi-
dence scores, which are more narrowly distributed
after the model has been trained with more data.
After self-training, the derivs and hybrid mod-
els performed equally accurately. With no reason
to use the hybrid model, the total speed increase is
34%. With our pre-processing, the full Wikipedia
dump had close to 1 billion words, so speed is an
important factor.
Overall, our simple self-training experiment
was quite successful. This result may seem sur-
prising given that the CoNLL 2007 participants
generally failed to use similar resources to adapt
dependency parsers to biomedical text (Dredze
et al, 2007). However, our results confirm Rimell
and Clark?s (2009) finding that the C&C parser?s
division of labour between the supertagger and
parser make it easier to adapt to new domains.
7 Conclusion
We have presented the first investigation into sta-
tistical parsing on Wikipedia data. The parser?s
accuracy dropped 4.3%, suggesting that the sys-
tem is still useable out-of-the-box. The parser is
also 54% slower on Wikipedia text. Parsing a full
Wikipedia dump would therefore take about 52
days of CPU time using our 5-year-old architec-
ture, which is inconvenient, but manageable over
multiple processors.
Using simple domain adaptation techniques,
we are able to increase the parser?s accuracy on
Wikipedia, with the fastest model improving in ac-
curacy by 0.8%. This closed the gap in accuracy
between the two parser models, removing the need
to use the slower hybrid model. This allowed us to
achieve an overall speed improvement of 34%.
Our results reflect the general trend that
NLP systems perform worse on foreign domains
(Gildea, 2001). Our results also support Rimell
and Clark?s (2009) conclusion that because C&C
is highly lexicalised, domain adaptation is largely
a process of adapting the supertagger.
A particularly promising aspect of these results
is that the parse speeds on the Wall Street Journal
improved, by 15%. This improvement came with
no loss in accuracy, and suggests that further boot-
strapping experiments are likely to be successful.
8 Acknowledgements
We would like to thank Stephen Clark and the
anonymous reviewers for their helpful feedback.
Joel was supported by a Capital Markets CRC
PhD scholarship and a University of Sydney Vice-
Chancellor?s Research Scholarship.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguis-
tics, 25(2):237?265.
Stephen Clark and James R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4):493?552.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman
Ganchev, Jo?ao Graca, and Fernando Pereira. 2007. Frus-
tratingly hard domain adaptation for dependency pars-
ing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 1051?1055. ACL, Prague,
Czech Republic.
Jason Eisner. 1996. Efficient normal-form parsing for Com-
binatory Categorial Grammar. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 79?86. Santa
Cruz, CA, USA.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the EMNLP Conference, pages
167?202. Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank
manual. Technical Report MS-CIS-05-09, Department of
Computer Science, University of Pennsylvania.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a
corpus of CCG derivations and dependency structures ex-
tracted from the Penn Treebank. Computational Linguis-
tics, 33(3):355?396.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual
sentence boundary detection. Computational Linguistics,
32(4):485?525.
Edward Loper and Steven Bird. 2002. NLTK: The natural
language toolkit.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session, pages
915?932. Prague, Czech Republic.
PediaPress. 2007. mwlib MediaWiki parsing library.
http://code.pediapress.com.
Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-
grammar parser to the biomedical domain. Journal of
Biomedical Informatics. (in press).
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Os-
borne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Example selec-
tion for bootstrapping statistical parsers. In Proceedings
of HLT-NAACL 2003. Edmonton, Alberta.
Gisle Ytrest?l, Stephan Oepen, and Daniel Flickinger. 2009.
Extracting and annotating Wikipedia sub-domains. In
Proceedings of the 7th International Workshop on Tree-
banks and Linguistic Theories, pages 185?197. Gronin-
gen, Netherlands.
41
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 820?830,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Analysing recall loss in named entity slot filling
Glen Pink Joel Nothman James R. Curran
e
-lab, School of Information Technologies
University of Sydney
NSW 2006, Australia
{glen.pink,joel.nothman,james.r.curran}@sydney.edu.au
Abstract
State-of-the-art fact extraction is heavily
constrained by recall, as demonstrated by
recent performance in TAC Slot Filling.
We isolate this recall loss for NE slots by
systematically analysing each stage of the
slot filling pipeline as a filter over correct
answers. Recall is critical as candidates
never generated can never be recovered,
whereas precision can always be increased
in downstream processing.
We provide precise, empirical confirma-
tion of previously hypothesised sources of
recall loss in slot filling. While NE type
constraints substantially reduce the search
space with only a minor recall penalty, we
find that 10% to 39% of slot fills will be
entirely ignored by most systems. One in
six correct answers are lost if coreference
is not used, but this can be mostly retained
by simple name matching rules.
1 Introduction
The TAC Knowledge Base Population (KBP) Slot
Filling (SF) consists of extracting named attributes
from text. Given a query, e.g. John Kerry, a system
searches a corpus for documents which contain the
entity. It then fills a list of slots, named attributes
such as (per:spouse, Teresa Heinz).
The top TAC SF 2013 (TAC13) system scored
37.3% F-score (Roth et al., 2013), and the median
F-score was 16.9% (Surdeanu, 2013). Recall for
SF systems is especially low, with many systems
using precise extractors with low recall. Precision
ranges from 9% to 40% greater than recall for the
top 5 systems in TAC13, and unsurprisingly, Roth
et al. (2013) has the highest recall at 33%. Closing
the recall gap without substantially increasing the
search space is critical to improving SF results.
Ji and Grishman (2011) and Min and Grishman
(2012) identify many of the challenges of SF, and
suggest that inference, coreference and named en-
tity recognition (NER) are key sources of error.
Min and Grishman categorise the slot fills found
by human annotators but not found in the aggre-
gated output of all systems. However, this ap-
proach only allows them to hypothesise the likely
source of recall loss. For instance, it is impossible
to distinguish candidate generation errors from an-
swer merging errors. Roth et al. (2014) categorise
these errors at a high level, without specific anal-
ysis of candidate generation pipeline components
such as coreference.
In this paper, we take this analysis further by
performing a systematic recall analysis that al-
lows us to pinpoint the cause of every recall er-
ror (candidates lost that can never be recovered)
and estimate upper bounds on recall in existing ap-
proaches. We implement a collection of na??ve SF
systems utilizing a set of increasingly restrictive
filters over documents and named entities (NEs).
TAC has three slot types: NE, string and value slots.
We consider only those slots filled by NEs as there
are widely-used, high accuracy tools available for
NER, and focusing on NEs only allows us to pre-
cisely gauge performance of filters. String slots do
not have reliable classifiers, and value slots require
more normalisation than directly returning a token
span. Otherwise, this evaluation is not specifically
dependent on the nature of NEs, and we expect
similar results for other slot types.
We focus on systems which first generate can-
didates and then process them, the approach of the
majority of TAC systems. Our filters apply hard
constraints over NEs commonly used in the litera-
ture, accounting for a typical SF candidate genera-
tion pipeline?matching the query term, the form
of candidate fills and the distance between the
query and the candidate?but not performing any
further scoring or thresholding. We compare sev-
820
eral forms of coreference as filters, motivated by
the need for efficient coreference resolution when
processing large corpora. Complementing these
unsupervised experiments, we implement a max-
imum recall bootstrap to identify which fills are
reachable from training data.
We find ?10% of recall is ignored by most sys-
tems due to NER bounds errors, and despite state-
of-the-art coreference, 8% is lost when queries
and fills occur in different sentences. Using NE
type constraints is very effective, reducing recall
by only 2% for a search space reduction of 81%.
Without any coreference, 16% of typed fills are
lost, but 12% of this recall can be recovered us-
ing fast na??ve name matching rules, reducing the
search space to 59% that of full coreference. 15%
of recall is lost if a SF approach, such as a boot-
strapping, requires that dependency paths be non-
unique in a corpus. We show that most remaining
candidates are reachable via bootstrapping from
a small number of seeds. Our results provide
systematic confirmation that effective coreference
and NER are critical to high recall slot filling.
2 Why focus on recall?
In this work, we determine the recall loss caused
by candidate generation constraints in SF systems.
SF pipelines are typically implemented using a
coarse-to-fine approach, where all possible candi-
dates are generated and then filtered by hard con-
straints and more sophisticated downstream pro-
cesses. Following this, we maximally generate
candidates and assume a high-precision but rela-
tively costly downstream process selects the final
extractions. While ultimately any system makes
precision-recall trade-offs, the recall of a system?s
coarse candidate generation process sets a hard
upper bound on performance, as candidates that
are not generated at all can never be recovered by
downstream processes. SF systems could gener-
ate every noun phrase in a corpus as potential can-
didates, but they apply hard candidate generation
constraints for efficiency and precision.
We implement these hard constraints as a se-
ries of filters, and return every candidate which
passes a filter without further ranking or threshold-
ing. These filters are comprised of generic com-
ponents, such as NER, which are representative of
SF pipelines. We are only interested in precision
in so much as it corresponds to the size of the
search space (the candidates generated), assum-
ing a small, fixed number of answers. The search
space determines the workload of later stages re-
sponsible for extraction, merging and ranking.
Precision can be improved by this post-processing
of the candidate set, but recall cannot.
3 Background
Slot filling (SF) is a query-oriented relation ex-
traction (RE) task in the Knowledge Base Popu-
lation (KBP) track of the Text Analysis Confer-
ences (TAC) (McNamee and Dang, 2009). A SF
system is queried with a name and a predefined
relation schema, or slots, and must seek instances
of any relations involving the query entity, and the
corresponding slot fills, from a corpus.
Systems typically consist of several pipelined
stages (Ji et al., 2011), providing many potential
locations for error. The basic pipeline, in Fig-
ure 1, consists of four stages (Ji and Grishman,
2011): document retrieval, candidate generation,
answer extraction, and answer merging and rank-
ing. The output of the second stage is a set of can-
didates which are then usually ranked using RE
techniques,
1
to precisely pinpoint answers. TAC
penalises redundant responses, requiring a final
answer merging and ranking stage. The first two
stages are the focus of this work, as they inad-
vertently filter correct answers that cannot be re-
covered, and they determine the size of the search
space for later stages.
Min and Grishman (2012) conducted an analy-
sis of the 140 TAC 2010 SF fills that were found by
human annotators but not any system, and manu-
ally look for evidence in the reference document
and categorise the hypothetical sources of error.
They find inference, coreference and NER to be
the top sources of error, and that the most studied
component (sentence-level RE) is not the domi-
nant problem, contributing only 10% of recall loss.
We precisely characterise the contribution of these
sources of error.
We follow the SF literature in adopting RE tech-
niques for filtering candidates. RE focuses on
identifying relations between entities (or attributes
of entities) as mentioned in text. Both relation
schema and training data are often provided, and
extraction is done using learnt classifiers (Mintz
et al., 2009; Surdeanu et al., 2012; Riedel et al.,
1
We note that question answering techniques have been
used directly by SF systems (Byrne and Dunnion, 2011) but
RE techniques are the primary method for answer extraction.
821
answer extraction
answer merging and ranking
document 
retrieval
alias 
match
exact 
match
oracle 
docs
candidate generation
NEs
sentence filter
coref
NNP 
coref
NNP 
naive
dependency filters
length
types
non-
unique
no
coref
NP 
n-grams
Figure 1: Candidate filters within the standard SF pipeline. Arrows indicate a sequence of filters.
2013; Zhang et al., 2013) or semi-supervised tech-
niques (Agichtein and Gravano, 2000; Wang et al.,
2011; Carlson et al., 2010).
Relation phrases or patterns may be identified
without labels (Fader et al., 2011; Mausam et al.,
2012) or clustered (Yao et al., 2012) into types.
Generating candidate entity pairs and using the
syntactic or surface path between them to decide
whether a relation exists are common threads in
RE that also form part of the SF pipeline. In some
RE tasks, entities mentioned may already be iden-
tified in a document and provided to a RE sys-
tem; in general, automatic NER is required. Some
tasks are defined more generally to include com-
mon noun phrases (Fader et al., 2011; Carlson et
al., 2010). SF specifically includes slots that can
be filled by arbitrary strings such as per:cause
of death, which make up a large number of
slot fills but may require the use of different tech-
niques for extraction, separate from names. NER
may be further enhanced by resolving names to
a KB (Mintz et al., 2009; Hoffmann et al., 2011;
Surdeanu et al., 2012; Wang et al., 2011), reduc-
ing noise in learning and extraction processes, but
we do not take this step in this work.
Typically, a RE system will only consider enti-
ties mentioned together in a sentence. When seek-
ing all instances of a given relation between known
entities, coreference resolution is necessary to sub-
stantially expand the set of candidate pairs (Gab-
bard et al., 2011). Coreference resolution may
not be necessary where each relation is redun-
dantly mentioned in a large corpus, as in SF; in
this vein, ?Open? approaches prefer precision and
avoid automatic coreference resolution (Banko et
al., 2007). Moreover, previous analysis attributed
substantial SF error to these tools (Ji and Grish-
man, 2011). Our work evaluates NER, locality
heuristics and coreference within a SF context.
Classification features for RE typically encode:
attributes of the entities; the surface form, depen-
dency path, or phrase structure subtree between
them; and surrounding context (Zhou et al., 2005;
Mintz et al., 2009; Zhang et al., 2013). We eval-
uate the length of dependency path between enti-
ties as a variable affecting SF candidate recall, and
apply na??ve entity pair bootstrapping (Brin, 1998;
Agichtein and Gravano, 2000) to assess the gener-
alisation over dependency paths from examples.
4 Experimental setup
We begin with a set of queries (a query being a
NE entity grounded in a mention in a document)
and, for each query q, the documents D
q
known to
contain any slot fill for q, as determined by oracle
information retrieval (IR) from human annotation
and judged system output. Filling every slot in q
with every n-gram in D
q
constitutes a system with
nearly perfect recall. We apply a series of increas-
ingly restrictive filters over this set. As in Figure 1,
SF systems in practice must retrieve relevant docu-
ments and generate candidates. We propose filters
that allow for analysis of recall lost during these
stages. We ignore the remaining stages and evalu-
ate the set of candidates directly.
Filters define what documents or NEs are al-
lowed to pass through, based on constraints im-
posed by query matching, entity form, and sen-
tence and syntactic context. We combine these fil-
ters in series in a number of configurations. The
use or absence of coreference varies across our
configurations, as the need to identify the query
mention and terms that refer to the query mention
is critical. Finally, we experiment with a boot-
822
strapping training process, to reflect constraints
implicitly applied by a training approach.
The SF typical system pipeline presented in Sec-
tion 3 applies to most, but not all SF approaches.
The following filters directly apply only to sys-
tems that use NER as the method of candidate gen-
eration, and where candidate generation is distinct
from answer extraction. Fourteen of the eighteen
teams participating in TAC13 submitted system re-
ports (Surdeanu, 2013). Eleven of these systems
identify NEs with NER and pass these to an answer
extraction process. The remaining three systems
either do not document whether they rely on or do
not rely on NER for candidate generation for name
slots. We include a high recall baseline based on
noun phrases (NPs) to cover these systems.
4.1 Filters
The first step in the SF pipeline is to find a relevant
document and the query entity mentioned within
that document. We use oracle IR to find docu-
ments D
q
(ORACLE DOCS in Figure 1) but need to
find a reference to q in these documents for other
filters and downstream stages (ALIAS MATCH in
Figure 1). An exact match to the query name is
trivial, but some documents may not contain the
query verbatim. This primarily occurs in cases
where an alias is used, e.g. where the query Fyffes
PLC is only mentioned as Fyffes in a document.
SF systems typically implement a query expan-
sion step prior to searching for relevant docu-
ments, generating and extracting aliases based on
the corpus and external sources (Ji et al., 2011).
For documents that do not mention the query ver-
batim, we manually annotate the longest token
span which refers to the query. All of our filters
are applied to this base setup. To measure the ef-
fect of our manual aliases on recall, we implement
a na??ve EXACT MATCH filter, which allows a doc-
ument only if a NE matches the query verbatim.
Entity form filters are based on the form of the
entities extracted from documents. We initially
consider all substrings of all NPs for a high-recall,
yet tractable, baseline. The NP N-GRAMS filter al-
lows every n-gram of every NP. NES allows NEs
only; and for TYPES, fill NEs must be of a NER
type defined by the slot, e.g. for per:city of
birth only LOC NEs are allowed.
Sentence filters require the query mention and
fill to be in the same sentence, or to have mentions
in the same sentence. Sentence filters are COREF:
the query and the fill must be mentioned in the
same sentence; COREF NNP: as for COREF, but
the query and the fill must have coreferent proper
noun mentions in the same sentence; NA
?
IVE NNP:
as for COREF NNP, but instead of using a full
coreference system and identifying proper noun
mentions, we use a na??ve proper noun coreference
process; and NOCOREF: the verbatim query and
the fill must be named in the same sentence.
As dependency paths are often a key fea-
ture for extracting relations, we apply further
syntactic filters based on dependency paths be-
tween NEs and mentions in sentences. Where
we use dependencies, we use the Stanford col-
lapsed and propagated representation (de Marn-
effe and Manning, 2008), e.g. in Alice is an em-
ployee of Bob and Charlie the collapsed and prop-
agated dependency path between Alice and Charlie
is?nsubj?employee?prep of?.
Syntactic filters roughly capture the complex-
ity of the syntactic configuration between query
and filler: LENGTH ? N requires that the query
and fill are separated by a dependency path of at
most N arcs, e.g. the above dependency path is
two arcs; VERB requires a verb to be present in the
dependency path between the query and fill men-
tions or names; and NON-UNIQUE requires the de-
pendency path between the query and fill to occur
more than once in a corpus, modelling a hard con-
straint on bootstrapping and other learning pro-
cesses that require a shared dependency context
between training and test examples.
4.2 Bootstrapping reachability
In addition to the upper bound set by these explicit
hard constraints, we want to reflect constraints that
are implicitly applied by an extraction process?
are there fills that are never learnable given a set of
features and a set of training data? We extend our
evaluation to include a training process in a semi-
supervised setting. We treat this as a bootstrap-
ping task (Agichtein and Gravano, 2000): given
training pairs of NEs in text (each pair effectively
a query entity and a candidate slot fill, or vice-
versa), extract the context of each pair, and find
other pairs in the corpus that share that context.
A pair is reachable, and hence learnable, if it can
be found by iterating this process. We continue to
evaluate maximum recall and do not apply thresh-
olding or ranking that would typically be utilised
in a bootstrapping process. We simply output all
823
Jim Senn (PER)
Center for Global Business (ORG)
Herb Gibson (PER)
OSHA (ORG)
Leslie Walker (PER)
Massachusetts Correctional Legal Services (ORG)
per:employee_of
<-prep_of<-director<-appos<-
<-prep_for<-director<-appos<-
Figure 2: Bootstrapping. The rightmost vertex is labelled with per:employee of after two iterations.
possible candidates in order to measure recall loss:
as with hard constraints applied by filters, if recall
is lost it can never be recovered.
Given a set of training data, we identify if we
can reach a test instance by bootstrapping, no mat-
ter how remotely it is connected to training in-
stances. We use lemmatised dependency paths as
the context for this process as they are relatively
precise and discriminative, compared to other fea-
tures used for SF. In order to simplify process-
ing, we construct a graph of all pairs and paths
in the corpus first, and then bootstrap from train-
ing instances over this graph. Bootstrapping more
general features (e.g. bag-of-words) results in the
graph becoming too large to process on our com-
puting resources.
The graph is constructed as follows. Each ver-
tex represents a typed pair of NEs that occur in the
same sentence in the TAC KBP Source Data (LDC,
2010), collapsing vertices that have equal names
and types into a single vertex. An edge exists
between pairs that are connected at least once by
the same dependency path. The constructed graph
is equivalent to the EXACT MATCH + NOCOREF
+ NON-UNIQUE filter. Constructing a graph for
COREF (which requires many more edges than
NOCOREF) was impractical.
Initially, pairs in training data are labelled with
their corresponding slots (see Figure 2). In each
bootstrap iteration, the labels of each vertex are
added to its neighbouring vertices. There is no fil-
tering or competition between labels on a vertex,
they are all added. We analyse performance after
each iteration, evaluating by mapping the labelled
graph back to the equivalent SF queries. This en-
ables us to determine what fills are recoverable
from the bootstrapping process.
5 Evaluation
We evaluate our filters on the TAC KBP English
Slot Filling 2011 corpus, queries and task spec-
ification. As we aim to determine recall upper
bounds and recall loss, we use only the documents
D from the TAC KBP Source Data (LDC, 2010)
that are known to contain at least one correct slot
fill in the TAC KBP 2011 English Slot Filling As-
sessment Results (LDC, 2011).
We restrict the assessment results and the eval-
uation process to all slot types that are filled by
name content types as opposed to value or
string. We also do not evaluate the per:alt-
ernate names or org:alternate names
slots, as extraction of fills for these slots typically
falls outside the RE task: while X also known as Y
or similar may appear in text, X and Y are typically
mentioned independently across documents.
There are 100 TAC11 queries, 50 PER and 50
ORG. There are 535 fills in our reduced evalua-
tion, 1,171 correct responses over these fills: 56%
of the original evaluation slots. The distribution of
fills per slot is listed in Table 1. The number of fills
per query ranges from 0 (one query has no name
fills) to 71, with a median of 17. D is comprised
of 1,351 documents. The number of documents
per query ranges from 0 to 63, with a median of
15.5. We use TAC 2009 and 2010 results and an-
notations as training data for bootstrapping, with
4,647 relevant training examples.
We evaluate ignoring case and without requir-
ing a specific source document: nocase and
anydoc in SF evaluation. Note that each slot
fill is an equivalence class of responses: e.g. for
org:founded by the correct fills Clifford S. As-
ness and Clifford Asness are equivalent. Consis-
tent with SF evaluation, we identify at what con-
straint an entire equivalence class no longer has
any member proposed as a fill.
We process documents with Stanford CoreNLP:
tokenisation, POS tagging (Toutanova et al.,
2003), NER (Finkel et al., 2005), parsing (Klein
and Manning, 2003), and coreference resolution
(Lee et al., 2011), and these annotations form the
relevant components of our filters. Where we use
dependency paths, we lemmatise tokens on the
824
slot #
org:top members,employees 118
per:employee of 71
per:member of 47
org:subsidiaries 32
org:parents 24
per:origin 23
org:country of headquarters 22
per:countries of residence 20
org:city of headquarters 19
org:shareholders 18
slot #
per:cities of residence 17
per:children 17
org:stateorprovince of headquarters 17
per:schools attended 16
per:stateorprovinces of residence 11
org:member of 11
per:spouse 8
org:members 8
org:founded by 7
per:siblings 6
slot #
per:other family 6
per:city of birth 6
per:parents 3
per:country of birth 3
org:political,religious affiliation 2
per:stateorprovince of birth 1
per:country of death 1
per:city of death 1
Table 1: Number of fills for slots in the evaluation.
path to increase generality and recall in further
analysis. For example, for Alice employs Bob we
extract the path?nsubj?employ?dobj?.
The COREF NNP filter uses CoreNLP corefer-
ence, limited to mentions which are headed by
NNPs. For NA
?
IVE NNP we use a na??ve rule-based
coreference process (Pink et al., 2013), motivated
by efficiency reasons, as the full CoreNLP requires
parsing and a more complex model. The rules do
not require deep processing and can run quickly
over large volumes of text. All NEs from a doc-
ument are matched by processing in decreasing
length order. Two names are marked coreferent
where, ignoring titles and case: they match ex-
actly; they have a matching final word; they have
a matching initial word; or one is an acronym of
the other. If multiple conditions are matched, the
earliest (the most strict match) is used.
The NON-UNIQUE filter requires that a depen-
dency path occurs more than once between NEs
in the full TAC KBP Source Data (LDC, 2010),
comprised of 1.8M documents and 318M NE pairs.
There are 38.6M distinct lemmatised dependency
paths, 5M of which occur more than once.
6 Results
We now analyse where the filters lose recall. Re-
sults for non-syntactic filters are listed in Table 2.
Figure 3 illustrates our main pipeline which con-
tains filters that would typically be implemented.
NP n-grams We choose all n-grams of NPs
(from the CoreNLP constituency parser) to be our
highest recall filter, and so our highest baseline
has 3% recall loss. We identify the reasons for
loss at this filter. There are four errors due to
the fill not existing verbatim in text, e.g. Pinellas
and Pasco counties does not contain Pinellas County
verbatim. Four errors occur where an NP is not
correctly identified, which occurs in two differ-
ent cases: where there is genuine error or where
the sentence being parsed is actually a list or other
semi-structured data as opposed to an actual sen-
tence. four errors are where a correct answer has
not been annotated as correct, we refer to this as
ANNOTATION error below, and one case where an
incorrect response has been annotated as correct.
While 97% recall is an excellent starting point,
53M candidates is a huge, likely intractable search
space for any downstream process. Hence NER is
commonly used as the starting point for SF.
NEs Most errors here are due to NER errors, and
these errors result in nearly a 10% recall loss. 25
errors are caused where no token in the fill has
been tagged as part of a NE (NO NER); and 13
where some tokens were missed (NER BOUNDS).
There are two additional cases of ANNOTATION
due to determiners not being included in an NE,
where they perhaps should have also been anno-
tated. Hence, in agreement with previous analy-
ses, NER error has a large impact on SF.
On this data set we have 10% recall loss that
most SF or RE approaches would never be able to
extract. However, it is still fairly unconstrained
and a high recall bound in comparison to the fol-
lowing filters. Recall errors could be substan-
tially reduced if SF approaches were to take into
consideration all NEs in documents as a set of
candidates, and take a more document-based ap-
proach to RE as opposed to sentence-based. While
there has been some work in extracting relations
across sentences without coreference (Swampillai
and Stevenson, 2011), RE across sentence bound-
aries is effectively limited to coreference chains
between sentences. Currently whole document
extraction is not a research focus for SF, and
the implementation of whole document techniques
throughout SF pipelines would likely be beneficial.
825
NP
 
n
g
r
a
m
s
97%
53 M
N
E
s
90%
562 K
88%
109 K
t
y
p
e
s
49
 K
76%
64%
coref
NNP naive
NNP coref
nocoref
80%
78%
43
 K
29 K
18 K
Figure 3: Results for NP N-GRAMS + NES + TYPES, followed by sentence filters with a range of corefer-
ence configurations. Grey fill and % indicates recall after each filter, and the number in the arrow is the
size of the result set passed to the next filter or to the downstream process.
experiment R (%) |search space|
NP N-GRAMS 97 53966773
. . . + NEs 90 562318
. . . + TYPES (1) 88 109241
. . . + EXACT MATCH (2) 85 105764
(1) + COREF 80 49170
(1) + NNP COREF 78 43476
(1) + NNP NA
?
IVE 76 29171
(1) + NOCOREF 64 18331
(2) + COREF 77 47439
(2) + NNP COREF 73 30089
(2) + NNP NA
?
IVE 73 27770
(2) + NOCOREF 61 16978
(1) + COREF + NON-UNIQUE 65 19958
(1) + NNP COREF + NON-UNIQUE 62 17692
(1) + NNP NA
?
IVE + NON-UNIQUE 61 13960
(1) + NOCOREF + NON-UNIQUE 48 8084
(2) + COREF + NON-UNIQUE 63 18953
(2) + NNP COREF + NON-UNIQUE 60 16712
(2) + NNP NA
?
IVE + NON-UNIQUE 56 13064
(2) + NOCOREF + NON-UNIQUE 43 7236
Table 2: Results on D given sets of filters config-
urations. The ellipses indicate the previous line.
Exact match Requiring that the query name is
exactly matched (EXACT MATCH) loses a further
2% recall. Effectively this is the recall error cre-
ated by the IR component of SF. Five error cases
occur when an alias is required, e.g. Quds Force
for IRGC-QF; Chris Bentley for Christopher Bentley.
Eight errors occur where the query term is a refer-
ence to an entity but not its name, all pertaining to
the query GMAC?s Residential Capital LLC.
Types All errors created by the TYPES filter are
due to incorrect NER types on mentions proposed
by CoreNLP. We do not aggregate the NE type over
the coreference chain. Applying this filter cuts
down the search space substantially, with minimal
loss to recall. Adding TYPES results in a recall loss
of 2%, but cuts down the search space by 80%.
Coref This filter is the starting point for many
recent SF approaches: we consider entities that are
either named or mentioned in the same sentence.
Table 3 shows that coreference is the largest cat-
egory of recall error created by the COREF filter.
NN COREF, NNP COREF and PRP COREF indicate
failure to resolve common noun, proper noun and
pronoun coreference.
The remainder of the errors are cases where
mentions of the fills do not occur in the same sen-
tence. ROLE INF indicates that an individual?s role
is mentioned, e.g. Gene Roberts, the executive editor,
where The Inquirer is mentioned in a previous sen-
tence. LOC INF where additional location knowl-
edge is required: a French company is headquar-
tered in France. The search space has been sub-
stantially reduced, by a further 55% to 0.1% of the
original space. However, the recall upper bound
has dropped to 80% of all fills.
Coref NNP and naive NNP While coreference
is important for high recall, more difficult coref-
erence cases (common noun and pronoun coref-
erence) may generate a large number of spurious
cases. Using COREF NNP as the sentence filter
loses 2% recall, to an upper bound of 78%, for
a 12% reduction in the search space. However,
using a full coreference system generates may
more candidates than using simple NNP corefer-
ence. NA
?
IVE NNP has an upper bound of 76%.
This is only 4% lower recall than COREF, but
for a 41% reduction in search space. In addi-
tion, CoreNLP coreference is much more expen-
sive than our na??ve approach as it requires parsing.
No coref Errors for NOCOREF are listed in Ta-
ble 3. INF indicates that inference or more sophis-
ticated analysis is required to find the fill, such as
correctly identifying the relation between entities
826
Experiment NN COREF NNP COREF PRP COREF ROLE INF LOC INF INF NO NER ANNOTATION
COREF 9 6 13 4 3 0 8 1
NOCOREF 16 52 20 4 3 2 14 3
Table 3: Error types for COREF and NOCOREF.
0 20 40 60 80 100
0
2
4
1
3
Recall %
P
r
e
c
i
s
i
o
n
%
COREF
NOCOREF
Figure 4: Effect of COREF.
0 20 40 60 80 100
0
2
4
1
3
Recall %
P
r
e
c
i
s
i
o
n
%
NOCOREF + n ? 1-3
NOCOREF + n ? 4-7
Figure 5: Effect of short dependency paths, taking
the NOCOREF points from Figure 4.
0 20 40 60 80 100
0
1
2
3
4
Recall %
P
r
e
c
i
s
i
o
n
%
COREF
COREF + VERB
Figure 6: Effect of the VERB filter.
referred to in an interview. NOCOREF results in a
recall upper bound of 64%. While this gives us a
small search space, we are now losing a substan-
tial proportion of the correct fills.
Precision-recall curves for the dependency path
filters are given in Figures 4, 5 and 6. We choose
to report precision for simplicity, and note that the
downstream search space is the inverse of preci-
sion multiplied by the number of correct fills. Dots
from low recall to high recall indicate maximum
dependency path length from n = 1 to n = 7. De-
pendency paths of length 7 give maximum recall
in our experiments. Results for the addition of the
NON-UNIQUE constraint are given in Table 2.
Use of coreference While critical for recall, use
of coreference generates a large number of candi-
dates and presents a key trade-off for SF, as indi-
cated by Figure 4. At maximum dependency path
length, coreference gives 16% greater recall at a
cost of 1.1% precision, roughly half the precision
of no coreference.
Higher precision indicates that fewer candidates
are generated. Fewer candidates allows for SF ap-
proaches to be scaled to larger amounts of data,
and enables techniques that take advantage of re-
dundancy or clustering to be used. Hence the
higher precision no coreference approach may al-
low for more precise learning methods to be used,
which may provide better results overall than an
approach using coreference.
Short dependency paths In all of our filter con-
figurations, a short dependency path length is suf-
ficient for extracting the majority of slot fills for
that particular configuration. Improving precision
of fills found on short dependency paths may be a
more effective and scalable approach to improving
F-score rather than focusing on long paths.
In Figure 5 we consider NOCOREF. Limiting the
dependency path length to three loses 11% recall,
but gains 0.7% precision. While this loss of re-
call is high, the reduction in unique dependency
paths is substantial. For maximum path length
three there are 10,732 paths (1,551 unique); for all
paths there are 17,394 paths (2,863 unique).
Verb Figure 6 shows the VERB filters has less
impact or recall or precision than some other de-
pendency filters. For COREF with all paths, adding
the VERB filter loses 6% recall for a 0.1% gain in
precision. Some slots not included in this anal-
ysis, such as per:title, tend to be described
827
by shorter paths that often do not include verbs.
These slots are also frequent in the TAC11 dataset.
Non-unique The frequency of a dependency
path may be a critical feature for learning, as paths
that occur only once will not been seen by a boot-
strapping process or may not be considered by
other machine learning approaches. Applying the
NON-UNIQUE filter (Table 2) has a large effect on
recall: COREF loses 15% recall for a 41% reduc-
tion in the size of the search space; NOCOREF
loses 15% recall for a 44% reduction in search
space. To recover this recall, the strictness of this
filter could be relaxed by further generalising de-
pendency paths or using a different similarity met-
ric to direct match of paths. However, this is the
upper bound for approaches which consider only
exact dependency paths as a feature.
Bootstrapping A small amount of training data
quickly finds slot fills via bootstrapping. One it-
eration has a recall of 24%, with 7,665 candidates
generated. Two to four iterations have recall of
37%?39% (maximum recall), with 31,702?37,797
candidates. The recall upper bound for these con-
figurations is 43%?more training data will allow
for better precision, but will only minimally im-
prove recall in this setup. We note that limit-
ing bootstrap to one or two iterations is ideal for
the best trade-off between recall and search space.
However, closer analysis of discriminative paths is
required for a full SF system.
Note that even when bootstrapping through ev-
ery dependency path in the corpus, there is an up-
per bound on recall of 39%. Even if we used
the test data as additional training data the recall
would still be limited to 43%. This demonstrates
that systems need distributional features, depen-
dency tree kernels or other similarity comparison
as opposed to exact feature matching if depen-
dency paths are to be a useful feature for SF.
7 Discussion
We present an analysis of SF recall bounds given
hard constraints applied by standard system com-
ponents. Pipeline error is common across all NLP
tasks. Our analysis suggests that high-precision
na??ve tools, e.g. na??ve coreference, can lead to
state-of-the-art performance.
However, the SF task is not strictly an exhaus-
tive evaluation for each query, as the evaluation
data is comprised of the time-limited human anno-
tation plus aggregated system output only. There
may be fills that are missed in the evaluation re-
sults but are correct and returned by our high recall
filters?affecting our reported precisions.
We manually evaluate a small sample of the
queries, the first five person and the first five
organization queries, to identify missed fills in
the COREF output (2,903 of 49,170 total fills, or
5.9%). For these fills, there were 29 fills in the as-
sessment data. Of these fills, 21 are returned by
COREF, however there are two correct fills found
by COREF that are not in the assessment data. One
of these two errors would be identified with cor-
rect coreference, and the other requires complex
long range inference. These additional correct fills
that are identified will not have a large impact on
the absolute precision, as there are two of 2,903
more fills. However, the relative difference in true
positives, 21 to 23, results in some uncertainty in
results when comparing them relatively.
8 Conclusion
Recent TAC KBP Slot Filling results have shown
that state-of-the-art systems are substantially lim-
ited by low recall. In this work, we perform a
maximum recall analysis of slot filling, providing
a comprehensive analysis of recall error created
in the document retrieval and candidate generation
stages. We focus on recall error in candidate gen-
eration as a performance limitation, as candidates
that are lost in the pipeline cannot be recovered by
downstream processes.
We find ?10% of recall is ignored by most slot
filling systems due to NER error, and while state-
of-the-art coreference provides a substantial recall
gain over no coreference, 8% of recall is still lost
when queries and fills occur in different sentences.
Using NE type constraints is very effective, reduc-
ing recall by only 2% for a search space reduc-
tion of 81%. Without coreference, a further 16%
of fills are lost, but 12% of this recall can be re-
gained using efficient na??ve name matching rules,
while still reducing the search space by 41%, mak-
ing such an approach possibly preferable over full
coreference. We confirm that coreference and ac-
curate NER are critical to high recall slot filling.
We find that using maximum recall bootstrap-
ping, 39% of test slots fills are reachable from the
TAC09 and TAC10 training data, limited by an up-
per bound on non-unique paths of 43%.
In the future, we intend to assess how specific
828
slots are affected by recall and search space trade-
off, and perform evaluation over all slot types:
names, values and strings. In addition, we in-
tend to expand the bootstrapping experiments with
variations over the training data.
This work highlights NER, coreference and typ-
ing as the areas that have the most impact on
slot filling recall, enabling researchers to focus on
problems that will most improve performance.
Acknowledgements
We would like to thank the anonymous review-
ers for their useful feedback. This work was sup-
ported by an Australian Postgraduate Award, the
Capital Markets CRC Computable News project
and Australian Research Council Discovery grant
DP1097291.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting Relations from Large Plain-text Collec-
tions. In Proceedings of the Fifth ACM Confer-
ence on Digital Libraries, pages 85?94, San Anto-
nio, Texas, USA.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open Information Extraction from the Web. In Pro-
ceedings of IJCAI, pages 2670?2676, Hyderabad,
India.
Sergey Brin. 1998. Extracting patterns and rela-
tions from the World Wide Web. In Proceedings
of the 1998 International Workshop on the Web and
Databases, Valencia, Spain.
Lorna Byrne and John Dunnion. 2011. UCD IIRG at
TAC 2011. In Proceedings of TAC, Gaithersburg,
Maryland, USA.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an Architecture for Never-
Ending Language Learning. In Proceedings of
AAAI, pages 1306?1313, Atlanta, Georgia, USA.
Linguistic Data Consortium. 2010. TAC KBP Source
Data. LDC2010E12.
Linguistic Data Consortium. 2011. TAC KBP
2011 English Slot Filling Assessment Results.
LDC2011E88.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford Typed Dependencies
Representation. In Proceedings of the Workshop on
Cross-Framework and Cross-Domain Parser Evalu-
ation, pages 1?8, Manchester, United Kingdom.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of EMNLP, pages 1535?
1545, Edinburgh, United Kingdom.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of ACL, pages 363?370,
Ann Arbor, Michigan, USA.
Ryan Gabbard, Marjorie Freedman, and Ralph
Weischedel. 2011. Coreference for Learning to Ex-
tract Relations: Yes Virginia, Coreference Matters.
In Proceedings of ACL, pages 288?293, Portland,
Oregon, USA.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceed-
ings of ACL-HLT, pages 541?550, Portland, Oregon,
USA.
Heng Ji and Ralph Grishman. 2011. Knowledge
Base Population: Successful Approaches and Chal-
lenges. In Proceedings of ACL-HLT, pages 1148?
1158, Portland, Oregon.
Heng Ji, Ralph Grishman, and Hoa Dang. 2011.
Overview of the TAC2011 Knowledge Base Popu-
lation Track. In Proceedings of TAC, Gaithersburg,
Maryland, USA.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL,
pages 423?430, Sapporo, Japan.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In Proceedings of CONLL: Shared Task, pages
28?34, Portland, Oregan, USA.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open lan-
guage learning for information extraction. In Pro-
ceedings of EMNLP-CONLL, pages 523?534, Jeju
Island, Korea.
Paul McNamee and Hoa Dang. 2009. Overview of the
TAC 2009 Knowledge Base Population Track. In
Proceedings of TAC, Gaithersburg, Maryland, USA.
Bonan Min and Ralph Grishman. 2012. Challenges
in the Knowledge Base Population Slot Filling Task.
In Proceedings of LREC, pages 1148?1158, Istan-
bul, Turkey.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003?1011, Singapore.
829
Glen Pink, Will Radford, Will Cannings, Andrew
Naoum, Joel Nothman, Daniel Tse, and James R.
Curran. 2013. SYDNEY CMCRC at TAC 2013. In
Proceedings of TAC, Gaithersburg, Maryland, USA.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin,
and Andrew McCallum. 2013. Relation Extraction
with Matrix Factorization and Universal Schemas.
In Proceedings of HLT-NAACL, Atlanta, Georgia,
USA.
Benjamin Roth, Tassilo Barth, Michael Wiegand, Mit-
tul Singh, and Dietrich Klakow. 2013. Effective
Slot Filling Based on Shallow Distant Supervision
Methods. In Proceedings of TAC, Gaithersburg,
Maryland, USA.
Benjamin Roth, Tassilo Barth, Grzegorz Chrupa?a,
Martin Gropp, and Dietrich Klakow. 2014. Re-
lationFactory: A Fast, Modular and Effective Sys-
tem for Knowledge Base Population. Proceedings
of EACL, pages 89?92.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP-CONLL, pages 455?465, Jeju
Island, Korea.
Mihai Surdeanu. 2013. Overview of the TAC2013
Knowledge Base Population Evaluation: English
Slot Filling and Temporal Slot Filling. In Proceed-
ings of TAC, Gaithersburg, Maryland, USA.
Kumutha Swampillai and Mark Stevenson. 2011. Ex-
tracting Relations Within and Across Sentences. In
Proceedings of RANLP, pages 25?32, Hissar, Bul-
garia.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich Part-of-
speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL, pages 173?180, Ed-
monton, Canada.
Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2011. Harvesting Facts from
Textual Web Sources by Constrained Label Propa-
gation. In Proceedings of CIKM, pages 837?846,
Glasgow, Scotland, UK.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised Relation Discovery with Sense
Disambiguation. In Proceedings of ACL, pages
712?720, Jeju Island, Korea.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
Accurate Distant Supervision for Relational Facts
Extraction. In Proceedings of ACL-HLT, pages 810?
815, Jeju Island, Korea.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring Various Knowledge in Relation
Extraction. In Proceedings of ACL, pages 427?434,
Ann Arbor, Michigan, USA.
830
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 228?232,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Event Linking: Grounding Event Reference in a News Archive
Joel Nothman? and Matthew Honnibal+ and Ben Hachey# and James R. Curran?
? e-lab, School of IT
University of Sydney
NSW, Australia
?Capital Markets CRC
55 Harrington St
Sydney
NSW, Australia
{joel,james}@it.usyd.edu.au
+Department of
Computing
Macquarie University
NSW, Australia
#R&D, Thomson
Reuters Corporation
St. Paul
MN, USA
{honnibal,ben.hachey}@gmail.com
Abstract
Interpreting news requires identifying its con-
stituent events. Events are complex linguis-
tically and ontologically, so disambiguating
their reference is challenging. We introduce
event linking, which canonically labels an
event reference with the article where it was
first reported. This implicitly relaxes corefer-
ence to co-reporting, and will practically en-
able augmenting news archives with semantic
hyperlinks. We annotate and analyse a corpus
of 150 documents, extracting 501 links to a
news archive with reasonable inter-annotator
agreement.
1 Introduction
Interpreting news requires identifying its constituent
events. Information extraction (IE) makes this feasi-
ble by considering only events of a specified type,
such as personnel succession or arrest (Grishman
and Sundheim, 1996; LDC, 2005), an approach not
extensible to novel events, or the same event types
in sub-domains, e.g. sport. On the other hand, topic
detection and tracking (TDT; Allan, 2002) disregards
individual event mentions, clustering together arti-
cles that share a topic.
Between these fine and coarse-grained ap-
proaches, event identification requires grouping ref-
erences to the same event. However, strict corefer-
ence is hampered by the complexity of event seman-
tics: poison, murder and die may indicate the same
effective event. The solution is to tag mentions with
a canonical identifier for each news-triggering event.
This paper introduces event linking: given a past
event reference in context, find the article in a news
archive that first reports that the event happened.
The task has an immediate practical application:
some online newspapers link past event mentions to
relevant news stories, but currently do so with low
coverage and consistency; an event linker can add
referentially-precise hyperlinks to news.
The event linking task parallels entity link-
ing (NEL; Ji and Grishman, 2011), considering a
news archive as a knowledge base (KB) of events,
where each article exclusively represents the zero or
more events that it first reports. Coupled with an ap-
propriate event extractor, event linking may be per-
formed for all events mentioned in a document, like
the named entity disambiguation task (Bunescu and
Pas?ca, 2006; Cucerzan, 2007).
We have annotated and analysed 150 news and
opinion articles, marking references to past, news-
worthy events, and linking where possible to canon-
ical articles in a 13-year news archive.
2 The events in a news story
Approaches to news event processing are subsumed
within broader notions of topics, scenario templates,
or temporal entities, among others. We illustrate key
challenges in processing news events and motivate
event linking through the example story in Figure 1.
Salience Our story highlights carjackings and a
police warning as newsworthy, alongside events like
feeding, drove and told which carry less individual
weight. Orthogonally, parts of the story are new
events, while others are previously reported events
that the reader may be aware of (illustrated in Fig-
ure 1). Online, the two background carjackings and
the police warning are hyperlinked to other SMH arti-
cles where they were reported. Event schemas tend
not to directly address salience: MUC-style IE (Gr-
228
N Sydney man carjacked at knifepoint
There has been another carjacking in Sydney,
B two weeks after two people were stabbed in their cars in
separate incidents.
N A 32-year-old driver was walking to his station wagon on
Hickson Road, Millers Point, after feeding his parking me-
ter about 4.30pm yesterday when a man armed with a
knife grabbed him and told him to hand over his car keys
and mobile phone, police said. The carjacker then drove
the black 2008 Holden Commodore. . . He was described
as a 175-centimetre-tall Caucasian. . .
B Police warned Sydney drivers to keep their car doors
locked after two stabbings this month. On September 4,
a 40-year-old man was stabbed when three men tried to
steal his car on Rawson Street, Auburn, about 1.20am.
The next day, a 25-year-old woman was stabbed in her
lower back as she got into her car on Liverpool Road. . .
Figure 1: Possible event mentions marked in an ar-
ticle from SMH, segmented into news (N) and back-
ground (B) event portions.
ishman and Sundheim, 1996) selects an event type
of which all instances are salient; TDT (Allan, 2002)
operates at the document level, which avoids differ-
entiating event mentions; and TimeML (Pustejovsky
et al, 2003) marks the main event in each sentence.
Critiquing ACE05 event detection for not addressing
salience, Ji et al (2009) harness cross-document fre-
quencies for event ranking. Similarly, reference to a
previously-reported event implies it is newsworthy.
Diversity IE traditionally targets a selected event
type (Grishman and Sundheim, 1996). ACE05 con-
siders a broader event typology, dividing eight
thematic event types (business, justice, etc.) into
33 subtypes such as attack, die and declare
bankruptcy (LDC, 2005). Most subtypes suffer from
few annotated instances, while others are impracti-
cally broad: sexual abuse, gunfire and the Holocaust
each constitute attack instances (is told considered
an attack in Figure 1?). Inter-annotator agreement
is low for most types.1 While ACE05 would mark
the various attack events in our story, police warned
would be unrecognised. Despite template adapta-
tion (Yangarber et al, 2000; Filatova et al, 2006;
Li et al, 2010; Chambers and Jurafsky, 2011), event
types are brittle to particular tasks and domains, such
as bio-text mining (e.g. Kim et al, 2009); they can-
not reasonably handle novel events.
1For binary sentence classification, we calculate an inter-
quartile range of ? ? [0.46, 0.64] over the 33 sub-types. Coarse
event type classification ranges from ? = 0.47 for business to
? = 0.69 for conflict.
Identity Event coreference is complicated by par-
titive (sub-event) and logical (e.g. causation) re-
lationships between events, in addition to lexical-
semantic and syntactic issues. When consider-
ing the relationship between another carjacking and
grabbed, drove or stabbed, ACE05 would apply the
policy: ?When in doubt, do not mark any corefer-
ence? (LDC, 2005). Bejan and Harabagiu (2008)
consider event coreference across documents, mark-
ing the ?most important events? (Bejan, 2010), al-
beit within Google News clusters, where multiple
articles reporting the same event are likely to use
similar language. Similar challenges apply to iden-
tifying event causality and other relations: Bejan
and Harabagiu (2008) suggest arcs such as feeding
precedes
?????
walking enables???? grabbed ? akin to instantia-
tions of FrameNet?s frame relations (Fillmore et al,
2003). However, these too are semantically subtle.
Explicit reference By considering events through
topical document clusters, TDT avoids some chal-
lenges of precise identity. It prescribes rules of in-
terpretation for which stories pertain to a seminal
event. However, the carjackings in our story are
neither preconditions nor consequences of a semi-
nal event and so would not constitute a TDT clus-
ter. TDT fails to account for these explicit event ref-
erences. Though Feng and Allan (2009) and Yang
et al (2009) consider event dependency as directed
arcs between documents or paragraphs, they gener-
ally retain a broad sense of topic with little attention
to explicit reference.
3 The event linking task
Given an explicit reference to a past event, event
linking grounds it in a given news archive. This ap-
plies to all events worthy of having been reported,
and harnesses explicit reference rather than more
general notions of relevance. Though analogous to
NEL, our task differs in the types of expressions that
may be linked, and the manner of determining the
correct KB node to link to, if any.
3.1 Event-referring expressions
We consider a subset of newsworthy events ? things
that happen and directly trigger news ? as candidate
referents. In TimeML?s event classification (Puste-
jovsky et al, 2003), newsworthy events would gen-
229
erally be occurrence (e.g. die, build, sell) or aspec-
tual (e.g. begin, discontinue), as opposed to percep-
tion (e.g. hear), intentional state (e.g. believe), etc.
Still, we are not confined to these types when other
classes of event are newsworthy. All references must
be explicit, reporting the event as factual and com-
pleted or ongoing.
Not all event references meeting these criteria are
reasonably LINKABLE to a single article:
MULTIPLE many distinct events, or an event type,
e.g. world wars, demand;
AGGREGATE emerges from other events over time,
e.g. grew 15%, scored 100 goals;
COMPLEX an event reported over multiple articles
in terms of its sub-events, e.g. 2012 election,
World Cup, scandal.
3.2 A news archive as a KB
We define a canonical link target for each event: the
earliest article in the archive that reports the given
event happened or is happening. Each archival arti-
cle implicitly represents zero or more related events,
just as Wikipedia entries represent zero or one entity
in NEL. Links target the story as a whole: closely
related, co-reported events link to the same article,
avoiding a problematically strict approach to event
identity. An archive reports only selected events, so
a valid target may not exist (NEL?s NIL).
4 An annotated corpus
We link to a digital archive of the Sydney Morn-
ing Herald: Australian and international news from
1986 to 2009, published daily, Monday to Saturday.2
We annotate a randomly sampled corpus of 150 arti-
cles from its 2009 News and Features and Business
sections including news reports, op-eds and letters.
For this whole-document annotation, a single
word of each past/ongoing, newsworthy event men-
tion is marked.3 If LINKABLE, the annotator
searches the archive by keyword and date, selecting
a target, reported here (a self-referential link) or NIL.
An annotation of our example story (Figure 1) would
produce five groups of event references (Table 1).
2The archive may be searched at http://newsstore.
smh.com.au/apps/newsSearch.ac
3We couple marking and linking since annotators must learn
to judge newsworthiness relative to the target archive.
Mentions Annotation category / link
carjacking; LINKABLE, reported here
grabbed [him]
[were] stabbed; MULTIPLE
incidents; stabbings
[Police] warned LINKABLE, linked: Sydney drivers
told: lock your doors
[man] stabbed LINKABLE, linked: Driver stabbed
after Sydney carjacking
[woman] stabbed LINKABLE, linked: Car attack:
Driver stabbed in the back
Table 1: Event linking annotations for Figure 1
Agreement unit AB AC JA JB JC
Token has a link 27 21 61 42 34
Link target on agreed token 48 73 84 83 74
Set of link targets per document 31 40 69 51 45
Link date on agreed token 61 80 87 93 89
Set of link dates per document 36 44 71 54 56
Table 2: Inter-annotator and adjudicator F1 scores
All documents were annotated by external anno-
tator A; external annotators B and C annotated 72
and 24 respectively; and all were adjudicated by the
first author (J). Pairwise inter-annotator agreement
in Table 2 shows that annotators infrequently select
the same words to link, but that reasonable agree-
ment on the link target can be achieved for agreed
tokens.4 Adjudicator-annotator agreements are gen-
erally much higher than inter-annotator agreements:
in many cases, an annotator fails to find a target
or selects one that does not first report the event;
J accepts most annotations as valid. In other cases,
there may be multiple articles published on the same
day that describe the event in question from differ-
ent angles; agreement increases substantially when
relaxed to accept date agreement. Our adjudicated
corpus of 150 documents is summarised in Table 3.
Where a definitive link target is not available, an
annotator may erroneously select another candidate:
an opinion article describing the event, an article
where the event is mentioned as background, or an
article anticipating the event.
The task is complicated by changed perspective
between an event?s first report and its later reference.
4? ? F1 for the binary token task (F1 accounts for the ma-
jority class) and for the sparse link targets/date selection.
230
Category Mentions Types Docs
Any markable 2136 655 149
LINKABLE 1399 417 144
linked 501 229 99
reported here 667 111 111
nil 231 77 77
COMPLEX 220 79 79
MULTIPLE 328 102 102
AGGREGATE 189 57 57
Table 3: Annotation frequencies: no. of mentions,
distinct per document, and document frequency
Can overpayed link to what had been acquired? Can
10 died be linked to an article where only nine are
confirmed dead? For the application of adding hy-
perlinks to news, such a link might be beneficial, but
it may be better considered an AGGREGATE.
The schema underspecifies definitions of ?event?
and ?newsworthiness?, accounting for much of the
token-level disagreement, but not directly affecting
the task of linking a specified mention to the archive.
Adjectival mentions such as Apple?s new CEO are
easy to miss and questionably explicit. Events are
also confused with facts and abstract entities, such
as bans, plans, reports and laws. Unlike many other
facts, events can be grounded to a particular time of
occurrence, often stated in text.
5 Analysis and discussion
To assess task feasibility, we present bag-of-words
(BoW) and oracle results (Figure 2). Using the whole
document as a query5 retrieves 30% of gold targets
at rank 10, but only 60% by rank 150. Term win-
dows around each event mention perform close to
our oracle consisting of successful search keywords
collected during annotation, with over 80% recall at
150. No system recalls over 30% of targets at 1-best,
suggesting a reranking approach may be required.
Constraining search result dates is essential; an-
notators? constraints improve recall by 20% at rank
50. These constraints may draw on temporal expres-
sions in the source article or external knowledge.
Successful automated linking will therefore require
extensive use of semantic and temporal information.
Our corpus also highlights distinctions between
5Using Apache Solr defaults: TFIDF-weighted cosine simi-
larity over stemmed and stopped tokens.
0 25 50 75 100 125 150 175 200Rank (number of documents returned)0
1020
3040
5060
7080
90100
Link ta
rgets f
ound (
%)
Annotator terms + date constraintAnnotator termsMention 31-word windowWhole document
Figure 2: Recall for BoW and oracle systems
explicit event reference and broader relationships.
Yang et al (2009) makes the reasonable assumption
that news events generally build on others that re-
cently precede them. We find that the likelihood
a linked article occurred fewer than d days ago re-
duces exponentially with respect to d, yet the rate
of decay is surprisingly slow: half of all link targets
precede their source by over 3 months.
The effect of coreporting rather than coreference
is also clear: like {carjacking, grabbed} in our ex-
ample, mention chains include {return, decide, re-
contest}, {winner, Cup} as well as more familiar in-
stances like {acquired, acquisition}.
6 Conclusion
We have introduced event linking, which takes a
novel approach to news event reference, associating
each newsworthy past event with a canonical arti-
cle in a news archive. We demonstrate task?s fea-
sibility, with reasonable inter-annotator agreement
over a 150 document corpus. The corpus highlights
features of the retrieval task and its dependence on
temporal knowledge. As well as using event link-
ing to add referentially precise hyperlinks to a news
archive, further characteristics of news will emerge
by analysing the graph of event references.
7 Acknowledgements
We are grateful to the reviewers for their comments.
The work was supported by Capital Markets CRC
post-doctoral fellowships (BH; MH) and PhD Schol-
arship (JN); a University of Sydney VCRS (JN); and
ARC Discovery Grant DP1097291 (JRC).
231
References
James Allan, editor. 2002. Topic Detection and Track-
ing: Event-based Information Organization. Kluwer
Academic Publishers, Boston, MA.
Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A
linguistic resource for discovering event structures and
resolving event coreference. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation, Marrakech, Morocco.
Cosmin Adrian Bejan. 2010. Private correspondence,
November.
Razvan Bunescu and Marius Pas?ca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 9?16.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 976?986, Portland, Ore-
gon, USA, June.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 708?716.
Ao Feng and James Allan. 2009. Incident threading
for news passages. In CIKM ?09: Proceedings of
the 18th ACM international conference on Information
and knowledge management, pages 1307?1316, Hong
Kong, November.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 207?
214, Sydney, Australia, July.
Charles J. Fillmore, Christopher R. Johnson, and Miriam
R. L. Petruck. 2003. Background to FrameNet. Inter-
national Journal of Lexicography, 16(3):235?250.
Ralph Grishman and Beth Sundheim. 1996. Message un-
derstanding conference ? 6: A brief history. In COL-
ING 1996 Volume 1: The 16th International Confer-
ence on Computational Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1148?1158, Portland, Ore-
gon, June.
Heng Ji, Ralph Grishman, Zheng Chen, and Prashant
Gupta. 2009. Cross-document event extraction and
tracking: Task, evaluation, techniques and challenges.
In Proceedings of Recent Advances in Natural Lan-
guage Processing, September.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado, June.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish annotation guidelines for events. Linguistic Data
Consortium, July. Version 5.4.3.
Hao Li, Xiang Li, Heng Ji, and Yuval Marton. 2010.
Domain-independent novel event discovery and semi-
automatic event annotation. In Proceedings of the
24th Pacific Asia Conference on Language, Informa-
tion and Computation, Sendai, Japan, November.
James Pustejovsky, Jos? Casta no, Robert Ingria, Roser
Saur?, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. TimeML: Robust specification of
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics.
Christopher C. Yang, Xiaodong Shi, and Chih-Ping Wei.
2009. Discovering event evolution graphs from news
corpora. IEEE Transactions on Systems, Man and Cy-
bernetics, Part A: Systems and Humans, 34(4):850?
863, July.
Roman Yangarber, Ralph Grishman, and Pasi
Tapanainen. 2000. Automatic acquisition of do-
main knowledge for information extraction. In In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 940?946.
232
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 464?469,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Cheap and easy entity evaluation
Ben Hachey Joel Nothman Will Radford
School of Information Technologies
University of Sydney
NSW 2006, Australia
ben.hachey@sydney.edu.au
{joel,wradford}@it.usyd.edu.au
Abstract
The AIDA-YAGO dataset is a popular tar-
get for whole-document entity recogni-
tion and disambiguation, despite lacking a
shared evaluation tool. We review eval-
uation regimens in the literature while
comparing the output of three approaches,
and identify research opportunities. This
utilises our open, accessible evaluation
tool. We exemplify a new paradigm of
distributed, shared evaluation, in which
evaluation software and standardised, ver-
sioned system outputs are provided online.
1 Introduction
Modern entity annotation systems detect mentions
in text and disambiguate them to a knowledge base
(KB). Disambiguation typically returns the corre-
sponding Wikipedia page or NIL if none exists.
Named entity linking (NEL) work is driven by
the TAC shared tasks on query-driven knowledge
base population (Ji and Grishman, 2011). Eval-
uation focuses on disambiguating queried names
and clustering NIL mentions, but most systems
internally perform whole-document named en-
tity recognition, coreference, and disambiguation
(Cucerzan and Sil, 2013; Pink et al, 2013; Cheng
et al, 2013; Fahrni et al, 2013). Wikification
work generally evaluates end-to-end entity anno-
tation including KB-driven mention spotting and
disambiguation (Milne and Witten, 2008b; Kulka-
rni et al, 2009; Ratinov et al, 2011; Ferragina and
Scaiella, 2010). Despite important differences in
mention handling, NEL and wikification work have
followed a similar trajectory. Yet to our knowl-
edge, there are no comparative whole-document
evaluations of NEL and wikification systems.
Public data sets have also driven research
in whole-document entity disambiguation
(Cucerzan, 2007; Milne and Witten, 2008b;
Kulkarni et al, 2009; Bentivogli et al, 2010; Hof-
fart et al, 2011; Meij et al, 2012). However, with
many task variants and evaluation methodologies
proposed, it is very difficult to synthesise a clear
picture of the state of the art.
We present an evaluation suite for named entity
linking, leveraging and advocating for the AIDA
disambiguation annotations (Hoffart et al, 2011)
over the large and widely used CoNLL NER data
(Tjong Kim Sang and Meulder, 2003). This builds
on recent rationalisation and benchmarking work
(Cornolti et al, 2013), adding an isolated evalua-
tion of disambiguation. Contributions include:
? a simple, open-source evaluation suite for
end-to-end, whole-document NEL;
? disambiguation evaluation facilitated by
gold-standard mentions;
? reference outputs from state-of-the-art NEL
and wikification systems published with the
suite for easy comparison;
? implementation of statistical significance and
error sub-type analysis, which are often lack-
ing in entity linking evaluation;
? a venue for publishing benchmark results
continuously, complementing the annual cy-
cle of shared tasks;
? a repository for versioned corrections to
ground truth annotation.
We see this repository, at https://github.
com/wikilinks/conll03_nel_eval, as a
model for the future of informal shared evaluation.
We survey entity annotation tasks and evalua-
tion, proposing a core suite of metrics for end-to-
end linking and tagging, and settings that isolate
mention detection and disambiguation. A compar-
ison of state-of-the-art NEL and wikification sys-
tems illustrates how key differences in mention
handling affect performance. Analysis suggests
that focusing evaluation too tightly on subtasks
like candidate ranking can lead to results that do
not reflect end-to-end performance.
464
2 Tasks and metrics
The literature includes many variants of the en-
tity annotation task and even more evaluation ap-
proaches. Systems can be invoked under two set-
tings: given text with expressions to be linked
(gold mentions); or given plain text only (system
mentions). The former enables a diagnostic evalu-
ation of disambiguation, while the latter simulates
a realistic end-to-end application setting.
Within each setting, metrics may consider
different subsets of the gold (G) and system (S)
annotations. Given sets of (doc, token span, kbid)
tuples, we define precision, recall and F
1
score
with respect to some annotation filter f :
P
f
=
|f(G) ? f(S)|
|f(S)|
, R
f
=
|f(G) ? f(S)|
|f(G)|
We advocate two core metrics, corresponding to
the major whole-document entity annotation tasks.
Link annotation measures performance over every
linked mention. Its filter f
L
matches spans and
link targets, disregarding NILs. This is particularly
apt when entity annotation is a step in an informa-
tion extraction pipeline. Tag annotation measures
performance over document-level entity sets: f
T
disregards span information and NILs. This is ap-
propriate when entity annotation is used, e.g., for
document indexing or social media mining (Mi-
halcea and Csomai, 2007; Meij et al, 2012). We
proceed to ground these metrics and diagnostic
variants in the literature.
2.1 End-to-end evaluation
We follow Cornolti et al (2013) in evaluating end-
to-end entity annotation, including both mention
detection and disambiguation. In this context,
f
L
equates to Cornolti et al?s strong annotation
match; f
T
measures what they call entity match.
2.2 Mention evaluation
Mention detection performance may be evaluated
regardless of linking decisions. A filter f
M
dis-
cards the link target (kbid). Of the present metrics,
only this considers NIL-linked system mentions as
different from non-mentions. For comparability
with wikification, we consider an additional filter
f
M
KB
to NEL output that retains only linked men-
tions. f
M
and f
M
KB
are equivalent to Cucerzan?s
(2007) mention evaluation and Cornolti et al?s
strong mention match respectively. f
M
is com-
parable to the NER evaluation from the CoNLL
2003 shared task (Tjong Kim Sang and Meulder,
2003): span equivalence is handled the same way,
but metrics here ignore mention types.
2.3 Disambiguation evaluation
Most NEL and wikification literature focuses on
disambiguation, evaluating the quality of link tar-
get annotations in isolation from NER error. Pro-
viding systems with ground truth mentions makes
f
L
equivalent to Mihalcea and Csomai?s (2007)
sense disambiguation evaluation and Milne and
Witten?s (2008b) disambiguation evaluation. It
differs from Kulkarni et al?s (2009) metric in be-
ing micro-averaged (equal weight to each men-
tion), rather than macro-averaged across docu-
ments. f
L
recall is comparable to TAC?s KB recall
(Ji and Grishman, 2011). It differs in that all men-
tions are evaluated rather than specific queries.
Related evaluations have also isolated disam-
biguation performance by: considering the links
of only correctly identified mentions (Cucerzan,
2007); or only true mentions where the correct
entity appears among top candidates before dis-
ambiguation (Ratinov et al, 2011; Hoffart et al,
2011; Pilz and Paass, 2012). We do not prefer this
approach as it makes system comparison difficult.
For comparability, we implement a filter f
L
HOF
that retains only Hoffart-linkable mentions having
a YAGO means relation to the correct entity.
Tag annotation (f
T
) with ground truth men-
tions is equivalent to Milne and Witten?s (2008b)
link evaluation, Mihalcea and Csomai?s (2007)
keyword extraction evaluation and Ratinov et
al.?s (2011) bag-of-titles evaluation. It is compara-
ble to Pilz and Paass?s (2012) bag-of-titles evalua-
tion, but does not account for sequential order and
keeps all gold-standard links regardless of whether
they are found by candidate generation.
2.4 Further diagnostics and rank evaluation
Several evaluations in the literature are beyond the
scope of this paper but planned for future versions
of the code. This includes further diagnostic sub-
task evaluation, particularly candidate set recall
(Hachey et al, 2013), NIL accuracy (Ji and Grish-
man, 2011) and weak mention matching (Cornolti
et al, 2013). With a score for each prediction, fur-
ther metrics are possible: rank evaluation of tag
annotation with r-precision, mean reciprocal rank
and mean average precision (Meij et al, 2012);
and rank evaluation of mentions for comparison to
Hoffart et al (2011) and Pilz and Paass (2012).
465
3 Data
The CoNLL-YAGO dataset (Hoffart et al, 2011)
is an excellent target for end-to-end, whole-
document entity annotation. It is public, free and
much larger than most entity annotation data sets.
It is based on the widely used NER data from
the CoNLL 2003 shared task (Tjong Kim Sang
and Meulder, 2003), building disambiguation on
ground truth mentions. It has standard training
and development splits that are representative of
the held-out test data, all being sourced from the
Reuters text categorisation corpus (Lewis et al,
2004), which is provided free for research pur-
poses. Training and development comprise 1,162
stories from 22-31 August 1996 and held-out test
comprises 231 stories from 6-7 December 1996.
The layered annotation provides useful informa-
tion for analysis including categorisation topics
(e.g., general news, markets, sport) and NE type
markup (PER, ORG, LOC, MISC).
The primary drawback is that KB annotations
are currently present only if there is a YAGO
means relation between the mention string and
the correct entity. This means that there are a
number of CoNLL entity mentions referring to
entities that exist in Wikipedia that are nonethe-
less marked NIL in the ground truth (e.g. ?DSE?
for ?Dhaka Stock Exchange?). This may be ad-
dressed by using a shared repository to adopt ver-
sioned improvements to the ground truth. Anno-
tation over CoNLL tokenisation sometimes results
in strange mentions (e.g., ?Washington-based? in-
stead of ?Washington?). However, prescribed to-
kenisation simplifies comparison and analysis.
Another concern is that link annotation goes
stale, since Wikipedia titles are only canonical
with respect to a particular point in time. This is
because pages may be renamed or reorganised:
? to improve editorial structure, such as down-
grading an entity from having a page of its
own, to a mere section in another page;
? to account for newly notable entities, such as
creating a disambiguation page for a title that
formerly had a single known referent; or
? because of changes in fact, such as corporate
mergers and name changes.
All systems compared provide Wikipedia titles as
labels, which are mapped to current titles for com-
parison: for each entity title t linked in the gold
data, we query the Wikipedia API to find t?s canon-
ical form t
c
and retrieve titles of all redirects to t
c
.
4 Reference systems
Even on public data sets, comparison to published
results can be very difficult and extremely costly
(Fokkens et al, 2013). We include reference sys-
tem output in our repository for simple compar-
ison. Other researchers are welcome to add ref-
erence output, providing a continuous benchmark
that complements the annual cycle of large shared
tasks like TAC KBP.
4.1 TagMe
TagMe (Ferragina and Scaiella, 2010) is an end-
to-end wikification system specialising in short
texts. TagMe performs best among publicly avail-
able wikification systems (Cornolti et al, 2013).
Mention detection uses a dictionary of anchor
text from links between Wikipedia pages. Candi-
date ranking is based on entity relatedness (Milne
and Witten, 2008a), followed by mention prun-
ing. We use thresholds on annotation scores sup-
plied by Marco Cornolti (personal communica-
tion) of 0.289 and 0.336 respectively for men-
tion/link and tag evaluation. TagMe annota-
tions may not align with CoNLL token bound-
aries, e.g., <annot title=?Oakland, New Jer-
sey?>OAKLAND, N.J</annot>. Before evalua-
tion, we extend annotations to overlapping tokens.
4.2 AIDA
AIDA (Hoffart et al, 2011) is the system pre-
sented with the CoNLL-YAGO dataset and places
emphasis on state-of-the-art ranking of candi-
date entity sets. Mentions are ground truth from
the CoNLL data to isolate ranking performance,
equivalent to applying the f
L
HOF
filter. Ranking is
informed by a graph model of entity compatibility.
4.3 Schwa
Schwa (Radford et al, 2012) is a heuristic NEL
system based on a TAC 2012 shared task en-
trant. Mention detection uses a NER model trained
on news text followed by rule-based corefer-
ence. Disambiguation uses an unweighted com-
bination of KB statistics, document compatibil-
ity (Cucerzan, 2007), graph similarity and targeted
textual similarity. Candidates that score below
a threshold learned from TAC data are linked to
NIL. The system is very competitive, performing
at 93% and 97% respectively of the best accuracy
numbers we know of on 2011 and 2012 TAC eval-
uation data (Cucerzan and Sil, 2013).
466
System Mentions Filter P R F
1
Cucerzan System f
M
82.2 84.8 83.5
Schwa System f
M
86.9 76.7 81.5
TagMe System f
M
KB
75.2 60.4 67.0
Schwa System f
M
KB
82.5 74.5 78.3
Table 1: Mention detection results. Cucerzan re-
sults as reported (Cucerzan, 2007).
5 Results
We briefly report results over the reference sys-
tems to highlight characteristics of the evaluation
metrics and task settings. Results hinge upon
Schwa since we have obtained only its output in all
settings. Except where noted, all differences are
significant (p < 0.05) according to approximate
randomisation (Noreen, 1989), permuting annota-
tions over whole documents.
5.1 Mention evaluation
Table 1 evaluates mentions with and without NILs.
None of the systems reported use a CoNLL-
trained NER tagger, for which top shared task par-
ticipants approached 90% F
1
in a stricter evalu-
ation than f
M
. We note the impressive numbers
reported by Cucerzan (2007) using a novel ap-
proach to mention detection based on capitalisa-
tion and corpus co-occurrence statistics, and the
similar performance
1
to Schwa, whose NER com-
ponent is trained on another news corpus.
In wikification, NIL-linked mentions may not
be relevant, and it may suffice to identify only
the most canonical forms of names, rather than
all mentions in a coreference chain. With f
M
KB
,
Schwa has much higher recall than TagMe, though
TagMe?s precision is understated because it gener-
ates non-NE annotations that are not present in the
CoNLL-YAGO ground truth (e.g., linking ?striker?
to Forward (association football)).
5.2 Disambiguation evaluation
Table 2 contains results isolating disambiguation
performance. AIDA ranking outperforms Schwa
according to both the link (f
L
HOF
) and tag metrics
(f
T
HOF
). If we remove the Hoffart et al (2011)
linkable constraint, we observe that Schwa disam-
biguation performance loses about 8 points in pre-
cision on the link metric (f
L
) and 2 points on the
tag metric (f
T
). This suggests that disambiguation
1
Significance cannot be tested since we do not have the
Cucerzan (2007) output.
System Mentions Filter P R F
1
Schwa Gold f
L
67.5 78.3 72.5
Schwa Gold f
L
HOF
79.7 78.3 79.0
AIDA Gold f
L
HOF
83.2 83.2 83.2
Schwa Gold f
T
77.8 77.7 77.7
Schwa Gold f
T
HOF
80.1 77.6 78.8
AIDA Gold f
T
HOF
87.7 84.2 85.9
Table 2: Disambiguation results for mention-level
linking and document-level tagging.
System Mentions Filter P R F
1
TagMe System f
L
63.2 50.7 56.3
Schwa System f
L
67.6 61.0 64.2
TagMe System f
T
65.0 65.4 65.2
Schwa System f
T
71.2 62.6 66.6
Table 3: End-to-end results for mention-level link-
ing and document-level tagging.
evaluation without the linkable constraint is im-
portant, especially if the application requires de-
tecting and disambiguating all mentions.
The comparison here highlights a notable eval-
uation intricacy. The Schwa system disambiguates
all gold mentions rather than those with KB links,
and the document compatibility approach means
that evidence from a NIL mention may offer con-
founding evidence when linking linkable men-
tions. Further, although using the same mentions,
systems use search resources with different recall
characteristics, so the Schwa system may not re-
trieve the correct candidate to disambiguate.
5.3 End-to-end evaluation
Finally, Table 3 contains end-to-end entity anno-
tation results. Again, these results highlight key
differences in mention handling between NEL and
wikification. Coreference modelling helps NEL
detect and link ambiguous names (e.g., ?Presi-
dent Bush?) that refer to the same entity as unam-
biguous names in the same text (e.g., ?George W.
Bush?). And restricting the the universe to named
entities is appropriate for the CoNLL-YAGO data.
The advantage is marked in the mention-level link
evaluation (f
L
). However, the systems are statis-
tically indistinguishable in the document-level tag
evaluation (f
T
). Thus the extra NER and corefer-
ence machinery may not be justified if the applica-
tion is document indexing or social media mining
(Meij et al, 2012), wherein a KB-driven mention
detector may be favourable for other reasons.
467
Error
f
L
HOF
f
L
AIDA Schwa TagMe Schwa
wrong link 752 896 429 605
link as nil - 79 - 111
nil as link - - 183 337
missing - - 1,780 1,031
extra - - 1,663 927
Table 4: f
L
HOF
and f
L
error profiles.
0 20 40 60 80 100
LOC
ORG
PER
MISC
Figure 1: Schwa f
L
and f
L
HOF
F
1
for NE types
6 Analysis
We analyse the types of error that a system makes.
We also harness the multi-layered annotation to
quantify the effect of NE type and document topic.
By error type Table 4 shows error counts based
on the disambiguation link evaluation with the
linkable constraint (f
L
HOF
) and the end-to-end
link evaluation (f
L
). Errors are divided as follows:
wrong link: mention linked to wrong KB entry
link as nil: KB-entity mention linked to NIL
nil as link: NIL mention linked to the KB
missing: true mention not detected
extra: mention detected spuriously
AIDA outperforms Schwa under the linkable eval-
uation, making fewer wrong link errors. Schwa
also overgenerates NIL, which may reflect candi-
date recall errors or a conservative disambiguation
threshold. On the end-to-end evaluation, Schwa
makes more linking errors (wrong link, link as nil,
nil as link) than TagMe, but fewer in mention de-
tection, leading to higher overall performance.
By entity type Figure 1 evaluates only men-
tions where the CoNLL 2003 corpus (Tjong Kim
Sang and Meulder, 2003) marks a NE mention of
each type. This is based on the link evaluation
of Schwa. The left and right bars correspond to
end-to-end (f
L
) and disambiguation (f
L
HOF
) F
1
respectively. In accord with TAC results (Ji and
Grishman, 2011), high accuracy can be achieved
on PER when a full name is given, while ORG is
substantially more challenging. MISC entities are
somewhat difficult to disambiguate, with identifi-
cation errors hampering end-to-end performance.
0 20 40 60 80 100
Sports
Domestic Politics
Corporate / Industrial
Internat?l Relations
Markets
War / Civil War
Crime / Law Enforc?t
Figure 2: Schwa f
L
and f
L
HOF
F
1
for top topics
By topical category The underlying Reuters
Corpus documents are labelled with topic, country
and industry codes (Lewis et al, 2004). Figure 2
reports F
1
on test documents from each frequent
topic. It highlights that much ambiguity remains
unresolved in Sports, while very high performance
linking is attainable in categories such as Markets
and Domestic Politics, only when given ground
truth linkable mentions.
7 Conclusion
We surveyed entity annotation tasks and advocated
a core set of metrics for mention, disambiguation
and end-to-end evaluation. This enabled a direct
comparison of state-of-the-art NEL and wikifica-
tion systems, highlighting the effect of key differ-
ences. In particular, NER and coreference mod-
ules make NEL approaches suitable for applica-
tions that require all mentions, including ambigu-
ous names and entities that are not in the KB. For
applications where document-level entity tags are
appropriate, the NEL and wikification approaches
we evaluate have similar performance.
The big picture we wish to convey is a new
approach to community evaluation that makes
benchmarking and qualitative comparison cheap
and easy. In addition to the code being open
source, we use the repository to store reference
system output, and ? we hope ? emendations to
the ground truth. We encourage other researchers
to contribute reference output and hope that this
will provide a continuous benchmark to comple-
ment the current cycle of shared tasks.
Acknowledgements
Many thanks to Johannes Hoffart, Marco Cornolti,
Xiao Ling and Edgar Meij for reference outputs
and guidance. Ben Hachey is the recipient of
an Australian Research Council Discovery Early
Career Researcher Award (DE120102900). The
other authors were supported by the Capital Mar-
kets CRC Computable News project.
468
References
Luisa Bentivogli, Pamela Forner, Claudio Giu-
liano, Alessandro Marchetti, Emanuele Pianta, and
Kateryna Tymoshenko. 2010. Extending English
ACE 2005 corpus annotation with ground-truth links
to Wikipedia. In COLING Workshop on The Peo-
ple?s Web Meets NLP: Collaboratively Constructed
Semantic Resources, pages 19?27.
Xiao Cheng, Bingling Chen, Rajhans Samdani, Kai-
Wei Chang, Zhiye Fei, Mark Sammons, John Wi-
eting, Subhro Roy, Chizheng Wang, and Dan Roth.
2013. Illinois cognitive computation group UI-CCG
TAC 2013 entity linking and slot filler validation
systems. In Text Analysis Conference.
Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmark-
ing entity-annotation systems. In 22nd International
Conference on the World Wide Web, pages 249?260.
Silviu Cucerzan and Avirup Sil. 2013. The MSR sys-
tems for entity linking and temporal slot filling at
TAC 2013. In Text Analysis Conference.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 708?716.
Angela Fahrni, Benjamin Heinzerling, Thierry G?ockel,
and Michael Strube. 2013. HITS? monolingual and
cross-lingual entity linking system at TAC 2013. In
Text Analysis Conference.
Paolo Ferragina and Ugo Scaiella. 2010. TAGME:
On-the-fly annotation of short text fragments (by
Wikipedia entities). In 19th International Confer-
ence on Information and Knowledge Management,
pages 1625?1628.
Antske Fokkens, Marieke van Erp, Marten Postma, Ted
Pedersen, Piek Vossen, and Nuno Freire. 2013. Off-
spring from reproduction problems: What replica-
tion failure teaches us. In 51st Annual Meeting
of the Association for Computational Linguistics,
pages 1691?1701.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2013. Evaluat-
ing entity linking with Wikipedia. Artificial Intel-
ligence, 194:130?150.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Conference on Empirical Methods
in Natural Language Processing, pages 782?792.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In 49th Annual Meeting of the Association for Com-
putational Linguistics, pages 1148?1158.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of Wikipedia entities in web text. In 15th Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 457?466.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A new benchmark collection
for text categorization research. Journal of Machine
Learning Research, 5:361?397.
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In 5th
International Conference on Web Search and Data
Mining, pages 563?572.
Rada Mihalcea and Andras Csomai. 2007. Wik-
ify! Linking documents to encyclopedic knowledge.
In 16th Conference on Information and Knowledge
Management, pages 233?242.
David Milne and Ian H. Witten. 2008a. An effec-
tive, low-cost measure of semantic relatedness ob-
tained from Wikipedia links. In AAAI Workshop on
Wikipedia and Articial Intelligence, pages 25?30.
David Milne and Ian H. Witten. 2008b. Learning
to link with Wikipedia. In 17th Conference on In-
formation and Knowledge Management, pages 509?
518.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. John Wiley & Sons.
Anja Pilz and Gerhard Paass. 2012. Collective
search for concept disambiguation. In 24th Inter-
national Conference on Computational Linguistics,
pages 2243?2258.
Glen Pink, Will Radford, Will Cannings, Andrew
Naoum, Joel Nothman, Daniel Tse, and James R.
Curran. 2013. SYDNEY CMCRC at TAC 2013. In
Text Analysis Conference.
Will Radford, Will Cannings, Andrew Naoum, Joel
Nothman, Glen Pink, Daniel Tse, and James R.
Curran. 2012. (Almost) Total Recall ? SYD-
NEY CMCRC at TAC 2012. In Text Analysis Con-
ference.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1375?1384.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Conference On Computational Natural Language
Learning, pages 142?147.
469
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 60?65,
Dublin, Ireland, August 23rd 2014.
Command-line utilities for managing and exploring annotated corpora
Joel Nothman and Tim Dawborn and James R. Curran
e
-lab, School of Information Technologies
University of Sydney
NSW 2006, Australia
{joel.nothman,tim.dawborn,james.r.curran}@sydney.edu.au
Abstract
Users of annotated corpora frequently perform basic operations such as inspecting the available
annotations, filtering documents, formatting data, and aggregating basic statistics over a corpus.
While these may be easily performed over flat text files with stream-processing UNIX tools,
similar tools for structured annotation require custom design. Dawborn and Curran (2014) have
developed a declarative description and storage for structured annotation, on top of which we
have built generic command-line utilities. We describe the most useful utilities ? some for quick
data exploration, others for high-level corpus management ? with reference to comparable UNIX
utilities. We suggest that such tools are universally valuable for working with structured corpora;
in turn, their utility promotes common storage and distribution formats for annotated text.
1 Introduction
Annotated corpora are a mainstay of language technology, but are often stored or transmitted in a variety
of representations. This lack of standardisation means that data is often manipulated in an ad hoc manner,
and custom software may be needed for even basic exploration of the data, which creates a bottleneck
in the engineer or researcher?s workflow. Using a consistent storage representation avoids this problem,
since generic utilities for rapid, high-level data manipulation can be developed and reused.
Annotated text processing frameworks such as GATE (Cunningham et al., 2002) and UIMA (Lally
et al., 2009) provide a means of implementing and combining processors over collections of annotated
documents, for which each framework defines a serialisation format. Developers using these frameworks
are aided by utilities for basic tasks such as searching among annotated documents, profiling processing
costs, and generic processing like splitting each document into many. Such utilities provide a means of
quality assurance and corpus management, as well as enabling rapid prototyping of complex processors.
The present work describes a suite of command-line utilities ? summarised in Table 1 ? designed for
similar ends in a recently released document representation and processing framework, DOCREP (Daw-
born and Curran, 2014). DOCREP represents annotation layers in a binary, streaming format, such that
process pipelining can be achieved through UNIX pipes. The utilities have been developed organically
as needed over the past two years, and are akin to UNIX utilities (grep, head, wc, etc.) which instead
operate over flat file formats. The framework and utilities are free and open source (MIT Licence) and
are available at https://github.com/schwa-lab/libschwa.
Some of our tools are comparable to utilities in UIMA and GATE, while others are novel. A number of
our tools exploit the evaluation of user-supplied Python functions over each document, providing great
expressiveness while avoiding engineering overhead when exploring data or prototyping.
Our utilities make DOCREP a good choice for UNIX-style developers who would prefer a quick script-
ing language over an IDE, but such modalities should also be on offer in other frameworks. We believe
that a number of these utilities are applicable across frameworks and would be valuable to researchers
and engineers working with manually and automatically annotated corpora. Moreover, we argue, the
availability of tools for rapid corpus management and exploration is an important factor in encouraging
users to adopt common document representation and processing frameworks.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
60
2 Utilities for managing structured data
Data-oriented computing tends to couple data primitives with a declarative language or generic tools
that operate over those primitives. UNIX provides tools for operating over paths, processes, streams and
textual data. Among them are wc to count lines and words, and grep to extract passages matched by
a regular expression; piped together, such utilities accomplish diverse tasks with minimal development
cost. Windows PowerShell extends these notions to structured .NET objects (Oakley, 2006); Yahoo!
Pipes (Yahoo!, 2007) provides equivalent operations over RSS feeds; SQL transforms relational data; and
XSLT (Clark, 1999) or XQuery (Chamberlin, 2002) make XML more than mere markup.
Textual data with multiple layers of structured annotation, and processors over these, are primitives
of natural language processing. Such nested and networked structures are not well represented as flat
text files, limiting the utility of familiar UNIX utilities. By standardising formats for these primitives,
and providing means to operate over them, frameworks such as GATE and UIMA promise savings in
development and data management costs. These frameworks store annotated corpora with XML, so users
may exploit standard infrastructure (e.g. XQuery) for basic transformation and aggregation over the data.
Generic XML tools are limited in their ability to exploit the semantics of a particular XML language,
such that expressing queries over annotations (which include pointers, spatial relations, etc.) can be
cumbersome. LT-XML (Thompson et al., 1997) implements annotators using standard XML tools, while
Rehm et al. (2008) present extensions to an XQuery implementation specialised to annotated text.
Beyond generic XML transformation, UIMA and GATE and their users provide utilities with broad
application for data inspection and management, ultimately leading to quality assurance and rapid de-
velopment within those frameworks. Both GATE and UIMA provide sophisticated graphical tools for
viewing and modifying annotations; for comparing parallel annotations; and for displaying a concor-
dance of contexts for a term across a document collection (Cunningham et al., 2002; Lally et al., 2009).
Both also provide means of profiling the efficiency of processing pipelines. The Eclipse IDE serves as
a platform for tool delivery and is comparable to the UNIX command-line, albeit more graphical, while
providing further opportunities for integration. For example, UIMA employs Java Logical Structures to
yield corpus inspection within the Eclipse debugger (Lally et al., 2009).
Generic processors in these frameworks include those for combining or splitting documents, or copy-
ing annotations from one document to another. The community has further built tools to export corpora
to familiar query environments, such as a relational database or Lucene search engine (Hahn et al., 2008).
The uimaFIT library (Ogren and Bethard, 2009) simplifies the creation and deployment of UIMA proces-
sors, but to produce and execute a processor for mere data exploration still has some overhead.
Other related work includes utilities for querying or editing treebanks (e.g. Kloosterman, 2009), among
specialised annotation formats; and for working with binary-encoded structured data such as Protocol
Buffers (e.g. protostuff
1
). Like these tools and those within UIMA and GATE, the utilities presented in
this work reduce development effort, with an orientation towards data management and evaluation of
arbitrary functions from the command-line.
3 Common utility structure
For users familiar with UNIX tools to process textual streams, implementing similar tools for working
with structured DOCREP files seemed natural. Core utilities are developed in C
++
, while others are able
to exploit the expressiveness of interpreted evaluation of Python. We describe a number of the tools
according to their application in the next section; here we first outline common features of their design.
Invocation and API Command-line invocation of utilities is managed by a dispatcher program, dr,
whose operation may be familiar from the git versioning tool: an invocation of dr cmd delegates to a
command named dr-cmd where found on the user?s PATH. Together with utility development APIs in
both C
++
and Python, this makes it easy to extend the base set of commands.
2
1
https://code.google.com/p/protostuff/
2
Note that DOCREP processing is not limited in general to these languages. As detailed in Dawborn and Curran (2014) APIs
are currently available in C
++
, Java and Python.
61
Command Description Required input Output Similar in UNIX
dr count Aggregate stream or many tabular wc
dr dump View raw annotations stream JSON-like less / hexdump
dr format Excerpt stream + expression text printf / awk
dr grep Select documents stream + expression stream grep
dr head Select prefix stream stream head
dr sample Random documents stream + proportion stream shuf -n
dr shell Interactive exploration stream + commands mixed python
dr sort Reorder documents stream + expression stream sort
dr split Partition stream + expression files split
dr tail Select suffix stream stream tail
Table 1: Some useful commands and comparable UNIX tools, including required input and output types.
Streaming I/O As shown in Table 1, most of our DOCREP utility commands take a single stream of
documents as input (defaulting to stdin), and will generally output either plain text or another DOCREP
stream (to stdout by default). This parallels the UNIX workflow, and intends to exploit its constructs
such as pipes, together with their familiarity to a UNIX user. This paradigm harnesses a fundamental
design decision in DOCREP: the utilisation of a document stream, rather than storing a corpus across
multiple files.
Self-description for inspection Generic command-line tools require access to the schema as well as
the data of an annotated corpus. DOCREP includes a description of the data schema along with the data
itself, making such tools possible with minimal user input. Thus by reading from a stream, fields and
annotation layers can be referenced by name, and pointers across annotation layers can be dereferenced.
3
Extensions to this basic schema may also be useful. For many tools, a Python class (on file) can be
referenced that provides decorations over the document: in-memory fields that are not transmitted on
the stream, but are derived at runtime. For example, a document with pointers from dependent to gover-
nor may be decorated with a list of dependents on each governor. Arbitrary Python accessor functions
(descriptors) may similarly be added. Still, for many purposes, the self-description is sufficient.
Custom expression evaluation A few of our utilities rely on the ability to evaluate a function given a
document. The suite currently supports evaluating such functions in Python, providing great flexibility
and power.
4
Their input is an object representing the document, and its offset (0 for the first document
in a stream, 1 for the second, etc.). Its output depends on the purpose of the expression, which may be
for displaying, filtering, splitting or sorting a corpus, depending on the utility in use, of which examples
appear below. Often it is convenient to specify an anonymous function on the command-line, a simple
Python expression such as len(doc.tokens) > 1000, into which local variables doc (document
object) and ind (offset index) are injected as well as built-in names like len. (Python code is typeset in
blue.) In some cases, the user may want to predefine a library of such functions in a Python module, and
may then specify the path to that function on the command-line instead of an expression.
4 Working with DOCREP on the command-line
Having described their shared structure, this section presents examples of the utilities available for work-
ing with DOCREP streams. We consider three broad application areas: quality assurance, managing
corpora, and more specialised operations.
3
This should be compared to UIMA?s Type System Descriptions, and uimaFIT?s automatic detection thereof.
4
dr grep was recently ported to C
++
with a custom recursive descent parser and evaluator, which limits expressiveness
but promises faster evaluation. In terms of efficiency, we note that some of the Python utilities have performed more than twice
as fast using the JIT compilation of PyPy, rather than the standard CPython interpreter.
62
4.1 Debugging and quality assurance
Validating the input and output of a process is an essential part of pipeline development in terms of
quality assurance and as part of a debugging methodology. Basic quality assurance may require viewing
the raw content contained on a stream: schema, data, or both. This could ensure, for instance, that a user
has received the correct version of a corpus from a correspondent, or that a particular field was used as
expected. Since DOCREP centres on a binary wire format, dr dump (cf. UNIX?s hexdump) provides a
decoded textual view of the raw content on a stream. Optionally, it can provide minimal interpretation of
schema semantics to improve readability (e.g. labelling fields by name rather than number), or can show
schema details to the exclusion of data.
For an aggregate summary of the contents of a stream, dr count is a versatile tool. It mirrors UNIX?s
wc in providing the basic statistics over a stream (or multiple files) at different granularities. Without
any arguments, dr count outputs the total number of documents on standard input, but the number of
annotations in each store (total number of tokens, sentences, named entities, etc.) can be printed with flag
-a, or specific stores with -s. The same tool can produce per-document (as distinct from per-stream)
statistics with -e, allowing for quick detection of anomalies, such as an empty store where annotations
were expected. dr count also doubles as a progress meter, where its input is the output of a concurrent
corpus processor, as in the following example:
my-proc < /path/to/input | tee /path/to/output | dr count -tacv 1000
Here, the options -acvn output cumulative totals over all stores every n documents, while -t prefixes
each row of output with a timestamp.
Problems can often be identified from only a small sample of documents. dr head (cf. UNIX?s
head) extracts a specified number of documents from the beginning of a stream, defaulting to 1.
dr sample provides a stochastic alternative, employing reservoir sampling (Vitter, 1985) to efficiently
draw a specified fraction of the entire stream. Its output can be piped to processing software for smoke
testing, for instance. Such tools are obviously useful for a binary format; yet it is not trivial to split on
document boundaries even for simple text representations like the CONLL shared task format.
4.2 Corpus management
Corpora often need to be restructured: they may be heterogeneous, or need to be divided or sampled
from, such as when apportioning documents to manual annotators. In other cases, corpora or annotations
from many sources should be combined.
As with the UNIX utility of the same name, dr grep has many uses. Here it might be used to extract
a particular document, or to remove problematic documents. The user provides a function that evaluates
to a Boolean value; where true, an input document is reproduced on the output stream. Thus it might
extract a particular document by its identifier, all documents with a minimum number of words, or those
referring to a particular entity. Note, however, that like its namesake, it performs a linear search, while
a non-streaming data structure could provide fast indexed access; each traversed document is (at least
partially) deserialised, adding to the computational overhead.
5
dr grep is often piped into dr count,
to print the number of documents (or sub-document annotations) in a subcorpus.
dr split moves beyond such binary filtering. Like UNIX?s split, it partitions a file into multiple,
using a templated filename. In dr split, an arbitrary function may determine the particular output
paths, such as to split a corpus whose documents have one or more category label into a separate file
for each category label. Thus dr split -t /path/to/{key}.dr py 'doc.categories'
evaluates each document?s categories field via a Python expression, and for each key in the returned
list will write to a path built from that key. In a more common usage, dr split k 10 will assign
documents by round robin to files named fold000.dr through fold009.dr, which is useful to
derive a k-fold cross-validation strategy for machine learning. In order to stratify a particular field across
5
Two commands that are not featured in this paper may allow for faster access: dr subset extends upon dr head to
extract any number of documents with minimal deserialisation overhead, given their offset indices in a stream. dr offsets
outputs the byte offset b of each document in a stream, such that C?s fseek(b) or UNIX?s tail -c+(b+1) can be used to
skip to a particular document. An index may thus be compiled in conjunction with dr format described below. Making fast
random access more user friendly is among future work.
63
the partitions for cross-validation, it is sufficient to first sort the corpus by that field using dr split k.
This is one motivation for dr sort, which may similarly accept a Python expression as the sort key, e.g.
dr sort py doc.label. As a special case, dr sort random will shuffle the input documents,
which may be useful before manual annotation or order-sensitive machine learning algorithms.
The inverse of the partitioning performed by dr split is to concatenate multiple streams. Given
DOCREP?s streaming design, UNIX?s cat suffices; for other corpus representations, a specialised tool
may be necessary to merge corpora.
While the above management tools partition over documents, one may also operate on portions of the
annotation on each document. Deleting annotation layers, merging annotations from different streams
(cf. UNIX?s cut and paste), or renaming fields would thus be useful operations, but are not currently
available as a DOCREP command-line utility. Related tasks may be more diagnostic, such as identifying
annotation layers that consume undue space on disk; dr count --bytes shows the number of bytes
consumed by each store (cf. UNIX?s du), rather than its cardinality.
4.3 Exploration and transformation
Other tools allow for more arbitrary querying of a corpus, such as summarising each document.
dr format facilitates this by printing a string evaluated for each document. The tool could be used
to extract a concordance, or enumerate features for machine learning. The following would print each
document?s id field and its first thirty tokens, given a stream on standard input:
dr format py '"{}\t{}".format(doc.id, " ".join(tok.norm for tok in
doc.tokens[:30]))'
We have also experimented with specialised tools for more particular, but common, formats, such as
the tabular format employed in CoNLL shared tasks. dr conll makes assumptions about the schema
to print one token per line with sentences separated by a blank line, and documents by a specified delim-
iter. Additional fields of each token (e.g. part of speech), or fields derived from annotations over tokens
(e.g. IOB-encoded named entity recognition tags) can be added as additional columns using command-
line flags. However, the specification of such details on the command-line becomes verbose and may not
easily express all required fields, such that developing an ad hoc script to undertake this transformation
may often prove a more maintainable solution.
Our most versatile utility makes it easy to explore or modify a corpus in an interactive Python shell.
This functionality is inspired by server development frameworks (such as Django) that provide a shell
specially populated with data accessors and other application-specific objects. dr shell reads docu-
ments from an input stream and provides a Python iterator over them named docs. If an output path is
specified on the command-line, a function write_doc is also provided, which serialises its argument
to disk. The user would otherwise have the overhead of opening input and output streams and initialising
the de/serialisation process; the time saved is small but very useful in practice, since it makes interactive
corpus exploration cheap. This in turn lowers costs when developing complex processors, as interactive
exploration may validate a particular technique. The following shows an interactive session in which the
user prints the 100 lemmas with highest document frequency.
$ dr shell /path/to/input.dr
>>> from collections import Counter
>>> df = Counter()
>>> for doc in docs:
... doc_lemmas = {tok.lemma for tok in doc.tokens}
... df.update(doc_lemmas)
...
>>> for lemma, count in df.most_common(100):
... print("{:5d}\t{}".format(count, lemma))
Finally, dr shell -c can execute arbitrary Python code specified on the command-line, rather than
interactively. This enables rapid development of ad hoc tools employing the common read-process-write
paradigm (whether writing serialised documents or plain text), in the vein of awk or sed.
64
5 Discussion
DOCREP?s streaming model allows the reuse of existing UNIX components such as cat, tee and pipes.
This is similar to the way in which choosing an XML data representation means users can exploit standard
XML tools. The specialised tools described above are designed to mirror the functionality if not names
of familiar UNIX counterparts, making it simple for UNIX users to adopt the tool suite.
No doubt, many users of Java/XML/Eclipse find command-line tools unappealing, just as a ?UNIX
hacker? might be put off by monolithic graphical interfaces and unfamiliar XML processing tools. Ideally
a framework should appeal to a broad user base; providing tools in many modalities may increase user
adoption of a document processing framework, without which it may seem cumbersome and confining.
In this vein, a substantial area for future work within DOCREP is to provide more graphical tools, and
utilities such as concordancing or database export that are popular within other frameworks. Further
utilities might remove existing fields or layers from annotations; select sub-documents; set attributes on
the basis of evaluated expressions; merge annotations; or compare annotations.
We have described utilities developed in response to a new document representation and processing
framework, DOCREP. Similar suites deserve attention as an essential adjunct to representation frame-
works, and should encompass expressive tools and various paradigms of user interaction. When per-
forming basic corpus manipulation, these utilities save substantial user effort, particularly by adopting a
familiar interface. Such boons highlight the benefit of using a consistent annotated corpus representation.
References
Donald D. Chamberlin. 2002. XQuery: An XML query language. IBM Systems Journal, 41(4):597?615.
James Clark. 1999. XSL transformations (XSLT). W3C Recommendation, 16 November.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A frame-
work and graphical development environment for robust NLP tools and applications. In Proceedings
of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 168?175.
Tim Dawborn and James R. Curran. 2014. docrep: A lightweight and efficient document representation
framework. In Proceedings of the 25th International Conference on Computational Linguistics.
Udo Hahn, Ekaterina Buyko, Rico Landefeld, Matthias M?hlhausen, Michael Poprat, Katrin Tomanek,
and Joachim Wermter. 2008. An overview of JCoRe, the JULIE lab UIMA component repository.
In Proceedings of LREC?08 Workshop ?Towards Enhanced Interoperability for Large HLT Systems:
UIMA for NLP?, pages 1?7.
Geert Kloosterman. 2009. An overview of the Alpino Treebank tools. http://odur.let.rug.nl/
vannoord/alp/Alpino/TreebankTools.html. Last updated 19 December.
Adam Lally, Karin Verspoor, and Eoric Nyberg. 2009. Unstructured Information Management Architec-
ture (UIMA) Version 1.0. OASIS Standard, 2 March.
Andy Oakley. 2006. Monad (AKA PowerShell): Introducing the MSH Command Shell and Language.
O?Reilly Media.
Philip Ogren and Steven Bethard. 2009. Building test suites for UIMA components. In Proceedings of the
Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing
(SETQA-NLP 2009), pages 1?4.
Georg Rehm, Richard Eckart, Christian Chiarcos, and Johannes Dellert. 2008. Ontology-based
XQuery?ing of XML-encoded language resources on multiple annotation layers. In Proceedings of
the Sixth International Conference on Language Resources and Evaluation (LREC?08).
Henry S. Thompson, Richard Tobin, David McKelvie, and Chris Brew. 1997. LT XML: Software API
and toolkit for XML processing. http://www.ltg.ed.ac.uk/software/.
Jeffrey S. Vitter. 1985. Random sampling with a reservoir. ACM Transactions on Mathematical Software
(TOMS), 11(1):37?57.
Yahoo! 2007. Yahoo! Pipes. http://pipes.yahoo.com/pipes/. Launched 7 February.
65
