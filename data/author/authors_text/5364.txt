Online Learning of Approximate Dependency Parsing Algorithms
Ryan McDonald Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{ryantm,pereira}@cis.upenn.edu
Abstract
In this paper we extend the maximum
spanning tree (MST) dependency parsing
framework of McDonald et al (2005c)
to incorporate higher-order feature rep-
resentations and allow dependency struc-
tures with multiple parents per word.
We show that those extensions can make
the MST framework computationally in-
tractable, but that the intractability can be
circumvented with new approximate pars-
ing algorithms. We conclude with ex-
periments showing that discriminative on-
line learning using those approximate al-
gorithms achieves the best reported pars-
ing accuracy for Czech and Danish.
1 Introduction
Dependency representations of sentences (Hud-
son, 1984; Me?lc?uk, 1988) model head-dependent
syntactic relations as edges in a directed graph.
Figure 1 displays a dependency representation for
the sentence John hit the ball with the bat. This
sentence is an example of a projective (or nested)
tree representation, in which all edges can be
drawn in the plane with none crossing. Sometimes
a non-projective representations are preferred, as
in the sentence in Figure 2.1 In particular, for
freer-word order languages, non-projectivity is a
common phenomenon since the relative positional
constraints on dependents is much less rigid. The
dependency structures in Figures 1 and 2 satisfy
the tree constraint: they are weakly connected
graphs with a unique root node, and each non-root
node has a exactly one parent. Though trees are
1Examples are drawn from McDonald et al (2005c).
more common, some formalisms allow for words
to modify multiple parents (Hudson, 1984).
Recently, McDonald et al (2005c) have shown
that treating dependency parsing as the search
for the highest scoring maximum spanning tree
(MST) in a graph yields efficient algorithms for
both projective and non-projective trees. When
combined with a discriminative online learning al-
gorithm and a rich feature set, these models pro-
vide state-of-the-art performance across multiple
languages. However, the parsing algorithms re-
quire that the score of a dependency tree factors
as a sum of the scores of its edges. This first-order
factorization is very restrictive since it only allows
for features to be defined over single attachment
decisions. Previous work has shown that condi-
tioning on neighboring decisions can lead to sig-
nificant improvements in accuracy (Yamada and
Matsumoto, 2003; Charniak, 2000).
In this paper we extend the MST parsing frame-
work to incorporate higher-order feature represen-
tations of bounded-size connected subgraphs. We
also present an algorithm for acyclic dependency
graphs, that is, dependency graphs in which a
word may depend on multiple heads. In both cases
parsing is in general intractable and we provide
novel approximate algorithms to make these cases
tractable. We evaluate these algorithms within
an online learning framework, which has been
shown to be robust with respect approximate in-
ference, and describe experiments displaying that
these new models lead to state-of-the-art accuracy
for English and the best accuracy we know of for
Czech and Danish.
2 Maximum Spanning Tree Parsing
Dependency-tree parsing as the search for the
maximum spanning tree (MST) in a graph was
81
root John saw a dog yesterday which was a Yorkshire Terrier
Figure 2: An example non-projective dependency structure.
root
hit
John ball with
the bat
the
root0 John1 hit2 the3 ball4 with5 the6 bat7
Figure 1: An example dependency structure.
proposed byMcDonald et al (2005c). This formu-
lation leads to efficient parsing algorithms for both
projective and non-projective dependency trees
with the Eisner algorithm (Eisner, 1996) and the
Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;
Edmonds, 1967) respectively. The formulation
works by defining the score of a dependency tree
to be the sum of edge scores,
s(x,y) =
?
(i,j)?y
s(i, j)
where x = x1 ? ? ? xn is an input sentence and y
a dependency tree for x. We can view y as a set
of tree edges and write (i, j) ? y to indicate an
edge in y from word xi to word xj . Consider the
example from Figure 1, where the subscripts index
the nodes of the tree. The score of this tree would
then be,
s(0, 2) + s(2, 1) + s(2, 4) + s(2, 5)
+ s(4, 3) + s(5, 7) + s(7, 6)
We call this first-order dependency parsing since
scores are restricted to a single edge in the depen-
dency tree. The score of an edge is in turn com-
puted as the inner product of a high-dimensional
feature representation of the edge with a corre-
sponding weight vector,
s(i, j) = w ? f(i, j)
This is a standard linear classifier in which the
weight vector w are the parameters to be learned
during training. We should note that f(i, j) can be
based on arbitrary features of the edge and the in-
put sequence x.
Given a directed graph G = (V,E), the maxi-
mum spanning tree (MST) problem is to find the
highest scoring subgraph of G that satisfies the
tree constraint over the vertices V . By defining
a graph in which the words in a sentence are the
vertices and there is a directed edge between all
words with a score as calculated above, McDon-
ald et al (2005c) showed that dependency pars-
ing is equivalent to finding the MST in this graph.
Furthermore, it was shown that this formulation
can lead to state-of-the-art results when combined
with discriminative learning algorithms.
Although the MST formulation applies to any
directed graph, our feature representations and one
of the parsing algorithms (Eisner?s) rely on a linear
ordering of the vertices, namely the order of the
words in the sentence.
2.1 Second-Order MST Parsing
Restricting scores to a single edge in a depen-
dency tree gives a very impoverished view of de-
pendency parsing. Yamada and Matsumoto (2003)
showed that keeping a small amount of parsing
history was crucial to improving parsing perfor-
mance for their locally-trained shift-reduce SVM
parser. It is reasonable to assume that other pars-
ing models might benefit from features over previ-
ous decisions.
Here we will focus on methods for parsing
second-order spanning trees. These models fac-
tor the score of the tree into the sum of adjacent
edge pair scores. To quantify this, consider again
the example from Figure 1. In the second-order
spanning tree model, the score would be,
s(0,?, 2) + s(2,?, 1) + s(2,?, 4) + s(2, 4, 5)
+ s(4,?, 3) + s(5,?, 7) + s(7,?, 6)
Here we use the second-order score function
s(i, k, j), which is the score of creating a pair of
adjacent edges, from word xi to words xk and xj .
For instance, s(2, 4, 5) is the score of creating the
edges from hit to with and from hit to ball. The
score functions are relative to the left or right of
the parent and we never score adjacent edges that
are on different sides of the parent (for instance,
82
there is no s(2, 1, 4) for the adjacent edges from
hit to John and ball). This independence between
left and right descendants allow us to use a O(n3)
second-order projective parsing algorithm, as we
will see later. We write s(xi,?, xj) when xj is
the first left or first right dependent of word xi.
For example, s(2,?, 4) is the score of creating a
dependency from hit to ball, since ball is the first
child to the right of hit. More formally, if the word
xi0 has the children shown in this picture,
xi0
xi1 . . . xij xij+1 . . . xim
the score factors as follows:
?j?1
k=1 s(i0, ik+1, ik) + s(i0,?, ij)
+ s(i0,?, ij+1) +
?m?1
k=j+1 s(i0, ik, ik+1)
This second-order factorization subsumes the
first-order factorization, since the score function
could just ignore the middle argument to simulate
first-order scoring. The score of a tree for second-
order parsing is now
s(x,y) =
?
(i,k,j)?y
s(i, k, j)
where k and j are adjacent, same-side children of
i in the tree y.
The second-order model allows us to condition
on the most recent parsing decision, that is, the last
dependent picked up by a particular word, which
is analogous to the the Markov conditioning of in
the Charniak parser (Charniak, 2000).
2.2 Exact Projective Parsing
For projective MST parsing, the first-order algo-
rithm can be extended to the second-order case, as
was noted by Eisner (1996). The intuition behind
the algorithm is shown graphically in Figure 3,
which displays both the first-order and second-
order algorithms. In the first-order algorithm, a
word will gather its left and right dependents in-
dependently by gathering each half of the subtree
rooted by its dependent in separate stages. By
splitting up chart items into left and right com-
ponents, the Eisner algorithm only requires 3 in-
dices to be maintained at each step, as discussed in
detail elsewhere (Eisner, 1996; McDonald et al,
2005b). For the second-order algorithm, the key
insight is to delay the scoring of edges until pairs
2-order-non-proj-approx(x, s)
Sentence x = x0 . . . xn, x0 = root
Weight function s : (i, k, j) ? R
1. Let y = 2-order-proj(x, s)
2. while true
3. m = ??, c = ?1, p = ?1
4. for j : 1 ? ? ?n
5. for i : 0 ? ? ?n
6. y? = y[i ? j]
7. if ?tree(y?) or ?k : (i, k, j) ? y continue
8. ? = s(x,y?) ? s(x,y)
9. if ? > m
10. m = ?, c = j, p = i
11. end for
12. end for
13. if m > 0
14. y = y[p ? c]
15. else return y
16. end while
Figure 4: Approximate second-order non-
projective parsing algorithm.
of dependents have been gathered. This allows for
the collection of pairs of adjacent dependents in
a single stage, which allows for the incorporation
of second-order scores, while maintaining cubic-
time parsing.
The Eisner algorithm can be extended to an
arbitrary mth-order model with a complexity of
O(nm+1), for m > 1. An mth-order parsing al-
gorithm will work similarly to the second-order al-
gorithm, except that we collect m pairs of adjacent
dependents in succession before attaching them to
their parent.
2.3 Approximate Non-projective Parsing
Unfortunately, second-order non-projective MST
parsing is NP-hard, as shown in appendix A. To
circumvent this, we designed an approximate al-
gorithm based on the exact O(n3) second-order
projective Eisner algorithm. The approximation
works by first finding the highest scoring projec-
tive parse. It then rearranges edges in the tree,
one at a time, as long as such rearrangements in-
crease the overall score and do not violate the tree
constraint. We can easily motivate this approxi-
mation by observing that even in non-projective
languages like Czech and Danish, most trees are
primarily projective with just a few non-projective
edges (Nivre and Nilsson, 2005). Thus, by start-
ing with the highest scoring projective tree, we are
typically only a small number of transformations
away from the highest scoring non-projective tree.
The algorithm is shown in Figure 4. The ex-
pression y[i ? j] denotes the dependency graph
identical to y except that xi?s parent is xi instead
83
FIRST-ORDER
h1
h3
?
h1 r r+1 h3
(A)
h1
h3
h1 h3
(B)
SECOND-ORDER
h1
h2 h2 h3
?
h1 h2 h2 r r+1 h3
(A)
h1
h2 h2 h3
?
h1 h2 h2 h3
(B)
h1
h3
h1 h3
(C)
Figure 3: A O(n3) extension of the Eisner algorithm to second-order dependency parsing. This figure
shows how h1 creates a dependency to h3 with the second-order knowledge that the last dependent of
h1 was h2. This is done through the creation of a sibling item in part (B). In the first-order model, the
dependency to h3 is created after the algorithm has forgotten that h2 was the last dependent.
of what it was in y. The test tree(y) is true iff the
dependency graph y satisfies the tree constraint.
In more detail, line 1 of the algorithm sets y to
the highest scoring second-order projective tree.
The loop of lines 2?16 exits only when no fur-
ther score improvement is possible. Each iteration
seeks the single highest-scoring parent change to
y that does not break the tree constraint. To that
effect, the nested loops starting in lines 4 and 5
enumerate all (i, j) pairs. Line 6 sets y ? to the de-
pendency graph obtained from y by changing xj?s
parent to xi. Line 7 checks that the move from y
to y? is valid by testing that xj?s parent was not al-
ready xi and that y? is a tree. Line 8 computes the
score change from y to y?. If this change is larger
than the previous best change, we record how this
new tree was created (lines 9-10). After consid-
ering all possible valid edge changes to the tree,
the algorithm checks to see that the best new tree
does have a higher score. If that is the case, we
change the tree permanently and re-enter the loop.
Otherwise we exit since there are no single edge
switches that can improve the score.
This algorithm allows for the introduction of
non-projective edges because we do not restrict
any of the edge changes except to maintain the
tree property. In fact, if any edge change is ever
made, the resulting tree is guaranteed to be non-
projective, otherwise there would have been a
higher scoring projective tree that would have al-
ready been found by the exact projective parsing
algorithm. It is not difficult to find examples for
which this approximation will terminate without
returning the highest-scoring non-projective parse.
It is clear that this approximation will always
terminate ? there are only a finite number of de-
pendency trees for any given sentence and each it-
eration of the loop requires an increase in score
to continue. However, the loop could potentially
take exponential time, so we will bound the num-
ber of edge transformations to a fixed value M .
It is easy to argue that this will not hurt perfor-
mance. Even in freer-word order languages such
as Czech, almost all non-projective dependency
trees are primarily projective, modulo a few non-
projective edges. Thus, if our inference algorithm
starts with the highest scoring projective parse, the
best non-projective parse only differs by a small
number of edge transformations. Furthermore, it
is easy to show that each iteration of the loop takes
O(n2) time, resulting in a O(n3 + Mn2) runtime
algorithm. In practice, the approximation termi-
nates after a small number of transformations and
we do not need to bound the number of iterations
in our experiments.
We should note that this is one of many possible
approximations we could have made. Another rea-
sonable approach would be to first find the highest
scoring first-order non-projective parse, and then
re-arrange edges based on second order scores in
a similar manner to the algorithm we described.
We implemented this method and found that the
results were slightly worse.
3 Danish: Parsing Secondary Parents
Kromann (2001) argued for a dependency formal-
ism called Discontinuous Grammar and annotated
a large set of Danish sentences using this formal-
ism to create the Danish Dependency Treebank
(Kromann, 2003). The formalism allows for a
84
root Han spejder efter og ser elefanterne
He looks for and sees elephants
Figure 5: An example dependency tree from
the Danish Dependency Treebank (from Kromann
(2003)).
word to have multiple parents. Examples include
verb coordination in which the subject or object is
an argument of several verbs, and relative clauses
in which words must satisfy dependencies both in-
side and outside the clause. An example is shown
in Figure 5 for the sentence He looks for and sees
elephants. Here, the pronoun He is the subject for
both verbs in the sentence, and the noun elephants
the corresponding object. In the Danish Depen-
dency Treebank, roughly 5% of words have more
than one parent, which breaks the single parent
(or tree) constraint we have previously required
on dependency structures. Kromann also allows
for cyclic dependencies, though we deal only with
acyclic dependency graphs here. Though less
common than trees, dependency graphs involving
multiple parents are well established in the litera-
ture (Hudson, 1984). Unfortunately, the problem
of finding the dependency structure with highest
score in this setting is intractable (Chickering et
al., 1994).
To create an approximate parsing algorithm
for dependency structures with multiple parents,
we start with our approximate second-order non-
projective algorithm outlined in Figure 4. We use
the non-projective algorithm since the Danish De-
pendency Treebank contains a small number of
non-projective arcs. We then modify lines 7-10
of this algorithm so that it looks for the change in
parent or the addition of a new parent that causes
the highest change in overall score and does not
create a cycle2. Like before, we make one change
per iteration and that change will depend on the
resulting score of the new tree. Using this sim-
ple new approximate parsing algorithm, we train a
new parser that can produce multiple parents.
4 Online Learning and Approximate
Inference
In this section, we review the work of McDonald
et al (2005b) for online large-margin dependency
2We are not concerned with violating the tree constraint.
parsing. As usual for supervised learning, we as-
sume a training set T = {(xt,yt)}Tt=1, consist-
ing of pairs of a sentence xt and its correct depen-
dency representation yt.
The algorithm is an extension of the Margin In-
fused Relaxed Algorithm (MIRA) (Crammer and
Singer, 2003) to learning with structured outputs,
in the present case dependency structures. Fig-
ure 6 gives pseudo-code for the algorithm. An on-
line learning algorithm considers a single training
instance for each update to the weight vector w.
We use the common method of setting the final
weight vector as the average of the weight vec-
tors after each iteration (Collins, 2002), which has
been shown to alleviate overfitting.
On each iteration, the algorithm considers a
single training instance. We parse this instance
to obtain a predicted dependency graph, and find
the smallest-norm update to the weight vector w
that ensures that the training graph outscores the
predicted graph by a margin proportional to the
loss of the predicted graph relative to the training
graph, which is the number of words with incor-
rect parents in the predicted tree (McDonald et al,
2005b). Note that we only impose margin con-
straints between the single highest-scoring graph
and the correct graph relative to the current weight
setting. Past work on tree-structured outputs has
used constraints for the k-best scoring tree (Mc-
Donald et al, 2005b) or even all possible trees by
using factored representations (Taskar et al, 2004;
McDonald et al, 2005c). However, we have found
that a single margin constraint per example leads
to much faster training with a negligible degrada-
tion in performance. Furthermore, this formula-
tion relates learning directly to inference, which is
important, since we want the model to set weights
relative to the errors made by an approximate in-
ference algorithm. This algorithm can thus be
viewed as a large-margin version of the perceptron
algorithm for structured outputs Collins (2002).
Online learning algorithms have been shown
to be robust even with approximate rather than
exact inference in problems such as word align-
ment (Moore, 2005), sequence analysis (Daume?
and Marcu, 2005; McDonald et al, 2005a)
and phrase-structure parsing (Collins and Roark,
2004). This robustness to approximations comes
from the fact that the online framework sets
weights with respect to inference. In other words,
the learning method sees common errors due to
85
Training data: T = {(xt,yt)}Tt=1
1. w(0) = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. min
?
?
?
w(i+1) ? w(i)
?
?
?
s.t. s(xt,yt; w(i+1))
?s(xt,y?; w(i+1)) ? L(yt,y?)
where y? = arg maxy? s(xt,y?; w(i))
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 6: MIRA learning algorithm. We write
s(x,y; w(i)) to mean the score of tree y using
weight vector w(i).
approximate inference and adjusts weights to cor-
rect for them. The work of Daume? and Marcu
(2005) formalizes this intuition by presenting an
online learning framework in which parameter up-
dates are made directly with respect to errors in the
inference algorithm. We show in the next section
that this robustness extends to approximate depen-
dency parsing.
5 Experiments
The score of adjacent edges relies on the defini-
tion of a feature representation f(i, k, j). As noted
earlier, this representation subsumes the first-order
representation of McDonald et al (2005b), so we
can incorporate all of their features as well as the
new second-order features we now describe. The
old first-order features are built from the parent
and child words, their POS tags, and the POS tags
of surrounding words and those of words between
the child and the parent, as well as the direction
and distance from the parent to the child. The
second-order features are built from the following
conjunctions of word and POS identity predicates
xi-pos, xk-pos, xj-pos
xk-pos, xj-pos
xk-word, xj-word
xk-word, xj-pos
xk-pos, xj-word
where xi-pos is the part-of-speech of the ith word
in the sentence. We also include conjunctions be-
tween these features and the direction and distance
from sibling j to sibling k. We determined the use-
fulness of these features on the development set,
which also helped us find out that features such as
the POS tags of words between the two siblings
would not improve accuracy. We also ignored fea-
English
Accuracy Complete
1st-order-projective 90.7 36.7
2nd-order-projective 91.5 42.1
Table 1: Dependency parsing results for English.
Czech
Accuracy Complete
1st-order-projective 83.0 30.6
2nd-order-projective 84.2 33.1
1st-order-non-projective 84.1 32.2
2nd-order-non-projective 85.2 35.9
Table 2: Dependency parsing results for Czech.
tures over triples of words since this would ex-
plode the size of the feature space.
We evaluate dependencies on per word accu-
racy, which is the percentage of words in the sen-
tence with the correct parent in the tree, and on
complete dependency analysis. In our evaluation
we exclude punctuation for English and include it
for Czech and Danish, which is the standard.
5.1 English Results
To create data sets for English, we used the Ya-
mada and Matsumoto (2003) head rules to ex-
tract dependency trees from the WSJ, setting sec-
tions 2-21 as training, section 22 for development
and section 23 for evaluation. The models rely
on part-of-speech tags as input and we used the
Ratnaparkhi (1996) tagger to provide these for
the development and evaluation set. These data
sets are exclusively projective so we only com-
pare the projective parsers using the exact projec-
tive parsing algorithms. The purpose of these ex-
periments is to gauge the overall benefit from in-
cluding second-order features with exact parsing
algorithms, which can be attained in the projective
setting. Results are shown in Table 1. We can see
that there is clearly an advantage in introducing
second-order features. In particular, the complete
tree metric is improved considerably.
5.2 Czech Results
For the Czech data, we used the predefined train-
ing, development and testing split of the Prague
Dependency Treebank (Hajic? et al, 2001), and the
automatically generated POS tags supplied with
the data, which we reduce to the POS tag set
from Collins et al (1999). On average, 23% of
the sentences in the training, development and
test sets have at least one non-projective depen-
dency, though, less than 2% of total edges are ac-
86
Danish
Precision Recall F-measure
2nd-order-projective 86.4 81.7 83.9
2nd-order-non-projective 86.9 82.2 84.4
2nd-order-non-projective w/ multiple parents 86.2 84.9 85.6
Table 3: Dependency parsing results for Danish.
tually non-projective. Results are shown in Ta-
ble 2. McDonald et al (2005c) showed a substan-
tial improvement in accuracy by modeling non-
projective edges in Czech, shown by the difference
between two first-order models. Table 2 shows
that a second-order model provides a compara-
ble accuracy boost, even using an approximate
non-projective algorithm. The second-order non-
projective model accuracy of 85.2% is the highest
reported accuracy for a single parser for these data.
Similar results were obtained by Hall and No?va?k
(2005) (85.1% accuracy) who take the best out-
put of the Charniak parser extended to Czech and
rerank slight variations on this output that intro-
duce non-projective edges. However, this system
relies on a much slower phrase-structure parser
as its base model as well as an auxiliary rerank-
ing module. Indeed, our second-order projective
parser analyzes the test set in 16m32s, and the
non-projective approximate parser needs 17m03s
to parse the entire evaluation set, showing that run-
time for the approximation is completely domi-
nated by the initial call to the second-order pro-
jective algorithm and that the post-process edge
transformation loop typically only iterates a few
times per sentence.
5.3 Danish Results
For our experiments we used the Danish Depen-
dency Treebank v1.0. The treebank contains a
small number of inter-sentence and cyclic depen-
dencies and we removed all sentences that con-
tained such structures. The resulting data set con-
tained 5384 sentences. We partitioned the data
into contiguous 80/20 training/testing splits. We
held out a subset of the training data for develop-
ment purposes.
We compared three systems, the standard
second-order projective and non-projective pars-
ing models, as well as our modified second-order
non-projective model that allows for the introduc-
tion of multiple parents (Section 3). All systems
use gold-standard part-of-speech since no trained
tagger is readily available for Danish. Results are
shown in Figure 3. As might be expected, the non-
projective parser does slightly better than the pro-
jective parser because around 1% of the edges are
non-projective. Since each word may have an ar-
bitrary number of parents, we must use precision
and recall rather than accuracy to measure perfor-
mance. This also means that the correct training
loss is no longer the Hamming loss. Instead, we
use false positives plus false negatives over edge
decisions, which balances precision and recall as
our ultimate performance metric.
As expected, for the basic projective and non-
projective parsers, recall is roughly 5% lower than
precision since these models can only pick up at
most one parent per word. For the parser that can
introduce multiple parents, we see an increase in
recall of nearly 3% absolute with a slight drop in
precision. These results are very promising and
further show the robustness of discriminative on-
line learning with approximate parsing algorithms.
6 Discussion
We described approximate dependency parsing al-
gorithms that support higher-order features and
multiple parents. We showed that these approxi-
mations can be combined with online learning to
achieve fast parsing with competitive parsing ac-
curacy. These results show that the gain from al-
lowing richer representations outweighs the loss
from approximate parsing and further shows the
robustness of online learning algorithms with ap-
proximate inference.
The approximations we have presented are very
simple. They start with a reasonably good baseline
and make small transformations until the score
of the structure converges. These approximations
work because freer-word order languages we stud-
ied are still primarily projective, making the ap-
proximate starting point close to the goal parse.
However, we would like to investigate the benefits
for parsing of more principled approaches to ap-
proximate learning and inference techniques such
as the learning as search optimization framework
of (Daume? and Marcu, 2005). This framework
will possibly allow us to include effectively more
global features over the dependency structure than
87
those in our current second-order model.
Acknowledgments
This work was supported by NSF ITR grants
0205448.
References
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL.
D.M. Chickering, D. Geiger, and D. Heckerman. 1994.
Learning bayesian networks: The combination of
knowledge and statistical data. Technical Report
MSR-TR-94-09, Microsoft Research.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proc. ACL.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann.
1999. A statistical parser for Czech. In Proc. ACL.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. JMLR.
H. Daum?e and D. Marcu. 2005. Learning as search op-
timization: Approximate large margin methods for
structured prediction. In Proc. ICML.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. COL-
ING.
J. Hajic?, E. Hajicova, P. Pajas, J. Panevova, P. Sgall, and
B. Vidova Hladka. 2001. The Prague Dependency
Treebank 1.0 CDROM. Linguistics Data Consor-
tium Cat. No. LDC2001T10.
K. Hall and V. N?ov?ak. 2005. Corrective modeling for
non-projective dependency parsing. In Proc. IWPT.
R. Hudson. 1984. Word Grammar. Blackwell.
M. T. Kromann. 2001. Optimaility parsing and local
cost functions in discontinuous grammars. In Proc.
FG-MOL.
M. T. Kromann. 2003. The danish dependency tree-
bank and the dtag treebank tool. In Proc. TLT.
R. McDonald, K. Crammer, and F. Pereira. 2005a.
Flexible text segmentation with structured multil-
abel classifi cation. In Proc. HLT-EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005c. Non-projective dependency parsing using
spanning tree algorithms. In Proc. HLT-EMNLP.
I.A. Me?lc?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
R. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proc. HLT-EMNLP.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proc. ACL.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proc. IWPT.
A 2nd-Order Non-projective MST
Parsing is NP-hard
Proof by a reduction from 3-D matching (3DM).
3DM: Disjoint sets X,Y,Z each withm distinct elements
and a set T ? X?Y ?Z. Question: is there a subset S ? T
such that |S| = m and each v ? X?Y ?Z occurs in exactly
one element of S.
Reduction: Given an instance of 3DM we defi ne a graph
in which the vertices are the elements from X ? Y ? Z as
well as an artifi cial root node. We insert edges from root to
all xi ? X as well as edges from all xi ? X to all yi ? Y
and zi ? Z. We order the words s.t. the root is on the left
followed by all elements of X, then Y , and fi nally Z. We
then defi ne the second-order score function as follows,
s(root, xi, xj) = 0, ?xi, xj ? X
s(xi,?, yj) = 0, ?xi ? X, yj ? Y
s(xi, yj , zk) = 1, ?(xi, yj , zk) ? T
All other scores are defi ned to be ??, including for edges
pairs that were not defi ned in the original graph.
Theorem: There is a 3D matching iff the second-order
MST has a score of m. Proof: First we observe that no tree
can have a score greater thanm since that would require more
than m pairs of edges of the form (xi, yj , zk). This can only
happen when some xi has multiple yj ? Y children or mul-
tiple zk ? Z children. But if this were true then we would
introduce a?? scored edge pair (e.g. s(xi, yj , y?j)). Now, if
the highest scoring second-order MST has a score of m, that
means that every xi must have found a unique pair of chil-
dren yj and zk which represents the 3D matching, since there
would be m such triples. Furthermore, yj and zk could not
match with any other x?i since they can only have one incom-
ing edge in the tree. On the other hand, if there is a 3DM, then
there must be a tree of weight m consisting of second-order
edges (xi, yj , zk) for each element of the matching S. Since
no tree can have a weight greater than m, this must be the
highest scoring second-order MST. Thus if we can fi nd the
highest scoring second-order MST in polynomial time, then
3DM would also be solvable. 
88
Discriminative Sentence Compression with Soft Syntactic Evidence
Ryan McDonald
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
ryantm@cis.upenn.edu
Abstract
We present a model for sentence com-
pression that uses a discriminative large-
margin learning framework coupled with
a novel feature set defined on compressed
bigrams as well as deep syntactic repre-
sentations provided by auxiliary depen-
dency and phrase-structure parsers. The
parsers are trained out-of-domain and con-
tain a significant amount of noise. We ar-
gue that the discriminative nature of the
learning algorithm allows the model to
learn weights relative to any noise in the
feature set to optimize compression ac-
curacy directly. This differs from cur-
rent state-of-the-art models (Knight and
Marcu, 2000) that treat noisy parse trees,
for both compressed and uncompressed
sentences, as gold standard when calculat-
ing model parameters.
1 Introduction
The ability to compress sentences grammatically
with minimal information loss is an important
problem in text summarization. Most summariza-
tion systems are evaluated on the amount of rele-
vant information retained as well as their compres-
sion rate. Thus, returning highly compressed, yet
informative, sentences allows summarization sys-
tems to return larger sets of sentences and increase
the overall amount of information extracted.
We focus on the particular instantiation of sen-
tence compression when the goal is to produce the
compressed version solely by removing words or
phrases from the original, which is the most com-
mon setting in the literature (Knight and Marcu,
2000; Riezler et al, 2003; Turner and Charniak,
2005). In this framework, the goal is to find the
shortest substring of the original sentence that con-
veys the most important aspects of the meaning.
We will work in a supervised learning setting and
assume as input a training set T =(xt,yt)|T |t=1 of
original sentences xt and their compressions yt.
We use the Ziff-Davis corpus, which is a set of
1087 pairs of sentence/compression pairs. Fur-
thermore, we use the same 32 testing examples
from Knight and Marcu (2000) and the rest for
training, except that we hold out 20 sentences for
the purpose of development. A handful of sen-
tences occur twice but with different compres-
sions. We randomly select a single compression
for each unique sentence in order to create an un-
ambiguous training set. Examples from this data
set are given in Figure 1.
Formally, sentence compression aims to shorten
a sentence x = x1 . . . xn into a substring y =
y1 . . . ym, where yi ? {x1, . . . , xn}. We define
the function I(yi) ? {1, . . . , n} that maps word
yi in the compression to the index of the word in
the original sentence. Finally we include the con-
straint I(yi) < I(yi+1), which forces each word
in x to occur at most once in the compression y.
Compressions are evaluated on three criteria,
1. Grammaticality: Compressed sentences
should be grammatical.
2. Importance: How much of the important in-
formation is retained from the original.
3. Compression rate: How much compression
took place. A compression rate of 65%
means the compressed sentence is 65% the
length of the original.
Typically grammaticality and importance are
traded off with compression rate. The longer our
297
The Reverse Engineer Tool is priced from $8,000 for a single user to $90,000 for a multiuser project site .
The Reverse Engineer Tool is available now and is priced on a site-licensing basis , ranging from $8,000 for a single user to $90,000 for a multiuser project site .
Design recovery tools read existing code and translate it into defi nitions and structured diagrams .
Essentially , design recovery tools read existing code and translate it into the language in which CASE is conversant ? defi nitions and structured diagrams .
Figure 1: Two examples of compressed sentences from the Ziff-Davis corpus. The compressed version
and the original sentence are given.
compressions, the less likely we are to remove im-
portant words or phrases crucial to maintaining
grammaticality and the intended meaning.
The paper is organized as follows: Section 2
discusses previous approaches to sentence com-
pression. In particular, we discuss the advantages
and disadvantages of the models of Knight and
Marcu (2000). In Section 3 we present our dis-
criminative large-margin model for sentence com-
pression, including the learning framework and
an efficient decoding algorithm for searching the
space of compressions. We also show how to
extract a rich feature set that includes surface-
level bigram features of the compressed sentence,
dropped words and phrases from the original sen-
tence, and features over noisy dependency and
phrase-structure trees for the original sentence.
We argue that this rich feature set alows the
model to learn which words and phrases should
be dropped and which should remain in the com-
pression. Section 4 presents an experimental eval-
uation of our model compared to the models of
Knight and Marcu (2000) and finally Section 5
discusses some areas of future work.
2 Previous Work
Knight and Marcu (2000) first tackled this prob-
lem by presenting a generative noisy-channel
model and a discriminative tree-to-tree decision
tree model. The noisy-channel model defines the
problem as finding the compressed sentence with
maximum conditional probability
y = arg max
y
P (y|x) = arg max
y
P (x|y)P (y)
P (y) is the source model, which is a PCFG plus
bigram language model. P (x|y) is the channel
model, the probability that the long sentence is an
expansion of the compressed sentence. To calcu-
late the channel model, both the original and com-
pressed versions of every sentence in the training
set are assigned a phrase-structure tree. Given a
tree for a long sentence x and compressed sen-
tence y, the channel probability is the product of
the probability for each transformation required if
the tree for y is to expand to the tree for x.
The tree-to-tree decision tree model looks to
rewrite the tree for x into a tree for y. The model
uses a shift-reduce-drop parsing algorithm that
starts with the sequence of words in x and the cor-
responding tree. The algorithm then either shifts
(considers new words and subtrees for x), reduces
(combines subtrees from x into possibly new tree
constructions) or drops (drops words and subtrees
from x) on each step of the algorithm. A decision
tree model is trained on a set of indicative features
for each type of action in the parser. These mod-
els are then combined in a greedy global search
algorithm to find a single compression.
Though both models of Knight and Marcu per-
form quite well, they do have their shortcomings.
The noisy-channel model uses a source model
that is trained on uncompressed sentences, even
though the source model is meant to represent the
probability of compressed sentences. The channel
model requires aligned parse trees for both com-
pressed and uncompressed sentences in the train-
ing set in order to calculate probability estimates.
These parses are provided from a parsing model
trained on out-of-domain data (the WSJ), which
can result in parse trees with many mistakes for
both the original and compressed versions. This
makes alignment difficult and the channel proba-
bility estimates unreliable as a result. On the other
hand, the decision tree model does not rely on the
trees to align and instead simply learns a tree-to-
tree transformation model to compress sentences.
The primary problem with this model is that most
of the model features encode properties related to
including or dropping constituents from the tree
with no encoding of bigram or trigram surface fea-
tures to promote grammaticality. As a result, the
model will sometimes return very short and un-
grammatical compressions.
Both models rely heavily on the output of a
noisy parser to calculate probability estimates for
the compression. We argue in the next section that
298
ideally, parse trees should be treated solely as a
source of evidence when making compression de-
cisions to be balanced with other evidence such as
that provided by the words themselves.
Recently Turner and Charniak (2005) presented
supervised and semi-supervised versions of the
Knight and Marcu noisy-channel model. The re-
sulting systems typically return informative and
grammatical sentences, however, they do so at the
cost of compression rate. Riezler et al (2003)
present a discriminative sentence compressor over
the output of an LFG parser that is a packed rep-
resentation of possible compressions. Though this
model is highly likely to return grammatical com-
pressions, it required the training data be human
annotated with syntactic trees.
3 Discriminative Sentence Compression
For the rest of the paper we use x = x1 . . . xn
to indicate an uncompressed sentence and y =
y1 . . . ym a compressed version of x, i.e., each yj
indicates the position in x of the jth word in the
compression. We always pad the sentence with
dummy start and end words, x1 = -START- and
xn = -END-, which are always included in the
compressed version (i.e. y1 = x1 and ym = xn).
In this section we described a discriminative on-
line learning approach to sentence compression,
the core of which is a decoding algorithm that
searches the entire space of compressions. Let the
score of a compression y for a sentence x as
s(x,y)
In particular, we are going to factor this score us-
ing a first-order Markov assumption on the words
in the compressed sentence
s(x,y) =
|y|
?
j=2
s(x, I(yj?1), I(yj))
Finally, we define the score function to be the dot
product between a high dimensional feature repre-
sentation and a corresponding weight vector
s(x,y) =
|y|
?
j=2
w ? f(x, I(yj?1), I(yj))
Note that this factorization will allow us to define
features over two adjacent words in the compres-
sion as well as the words in-between that were
dropped from the original sentence to create the
compression. We will show in Section 3.2 how
this factorization also allows us to include features
on dropped phrases and subtrees from both a de-
pendency and a phrase-structure parse of the orig-
inal sentence. Note that these features are meant
to capture the same information in both the source
and channel models of Knight and Marcu (2000).
However, here they are merely treated as evidence
for the discriminative learner, which will set the
weight of each feature relative to the other (pos-
sibly overlapping) features to optimize the models
accuracy on the observed data.
3.1 Decoding
We define a dynamic programming table C[i]
which represents the highest score for any com-
pression that ends at word xi for sentence x. We
define a recurrence as follows
C[1] = 0.0
C[i] = maxj<i C[j] + s(x, j, i) for i > 1
It is easy to show that C[n] represents the score of
the best compression for sentence x (whose length
is n) under the first-order score factorization we
made. We can show this by induction. If we as-
sume that C[j] is the highest scoring compression
that ends at word xj , for all j < i, then C[i] must
also be the highest scoring compression ending at
word xi since it represents the max combination
over all high scoring shorter compressions plus
the score of extending the compression to the cur-
rent word. Thus, since xn is by definition in every
compressed version of x (see above), then it must
be the case that C[n] stores the score of the best
compression. This table can be filled in O(n2).
This algorithm is really an extension of Viterbi
to the case when scores factor over dynamic sub-
strings of the text (Sarawagi and Cohen, 2004;
McDonald et al, 2005a). As such, we can use
back-pointers to reconstruct the highest scoring
compression as well as k-best decoding algo-
rithms.
This decoding algorithm is dynamic with re-
spect to compression rate. That is, the algorithm
will return the highest scoring compression re-
gardless of length. This may seem problematic
since longer compressions might contribute more
to the score (since they contain more bigrams) and
thus be preferred. However, in Section 3.2 we de-
fine a rich feature set, including features on words
dropped from the compression that will help disfa-
vor compressions that drop very few words since
299
this is rarely seen in the training data. In fact,
it turns out that our learned compressions have a
compression rate very similar to the gold standard.
That said, there are some instances when a static
compression rate is preferred. A user may specif-
ically want a 25% compression rate for all sen-
tences. This is not a problem for our decoding
algorithm. We simply augment the dynamic pro-
gramming table and calculate C[i][r], which is the
score of the best compression of length r that ends
at word xi. This table can be filled in as follows
C[1][1] = 0.0
C[1][r] = ?? for r > 1
C[i][r] = maxj<i C[j][r ? 1] + s(x, j, i) for i > 1
Thus, if we require a specific compression rate, we
simple determine the number of words r that sat-
isfy this rate and calculate C[n][r]. The new com-
plexity is O(n2r).
3.2 Features
So far we have defined the score of a compres-
sion as well as a decoding algorithm that searches
the entire space of compressions to find the one
with highest score. This all relies on a score fac-
torization over adjacent words in the compression,
s(x, I(yj?1), I(yj)) = w ? f(x, I(yj?1), I(yj)).
In Section 3.3 we describe an online large-margin
method for learning w. Here we present the fea-
ture representation f(x, I(yj?1), I(yj)) for a pair
of adjacent words in the compression. These fea-
tures were tuned on a development data set.
3.2.1 Word/POS Features
The first set of features are over adjacent words
yj?1 and yj in the compression. These include
the part-of-speech (POS) bigrams for the pair, the
POS of each word individually, and the POS con-
text (bigram and trigram) of the most recent word
being added to the compression, yj . These fea-
tures are meant to indicate likely words to in-
clude in the compression as well as some level
of grammaticality, e.g., the adjacent POS features
?JJ&VB? would get a low weight since we rarely
see an adjective followed by a verb. We also add a
feature indicating if yj?1 and yj were actually ad-
jacent in the original sentence or not and we con-
join this feature with the above POS features. Note
that we have not included any lexical features. We
found during experiments on the development data
that lexical information was too sparse and led to
overfitting, so we rarely include such features. In-
stead we rely on the accuracy of POS tags to pro-
vide enough evidence.
Next we added features over every dropped
word in the original sentence between yj?1 and yj ,
if there were any. These include the POS of each
dropped word, the POS of the dropped words con-
joined with the POS of yj?1 and yj . If the dropped
word is a verb, we add a feature indicating the ac-
tual verb (this is for common verbs like ?is?, which
are typically in compressions). Finally we add the
POS context (bigram and trigram) of each dropped
word. These features represent common charac-
teristics of words that can or should be dropped
from the original sentence in the compressed ver-
sion (e.g. adjectives and adverbs). We also add a
feature indicating whether the dropped word is a
negation (e.g., not, never, etc.).
We also have a set of features to represent
brackets in the text, which are common in the data
set. The first measures if all the dropped words
between yj?1 and yj have a mismatched or incon-
sistent bracketing. The second measures if the left
and right-most dropped words are themselves both
brackets. These features come in handy for ex-
amples like, The Associated Press ( AP ) reported
the story, where the compressed version is The
Associated Press reported the story. Information
within brackets is often redundant.
3.2.2 Deep Syntactic Features
The previous set of features are meant to en-
code common POS contexts that are commonly re-
tained or dropped from the original sentence dur-
ing compression. However, they do so without a
larger picture of the function of each word in the
sentence. For instance, dropping verbs is not that
uncommon - a relative clause for instance may be
dropped during compression. However, dropping
the main verb in the sentence is uncommon, since
that verb and its arguments typically encode most
of the information being conveyed.
An obvious solution to this problem is to in-
clude features over a deep syntactic analysis of
the sentence. To do this we parse every sentence
twice, once with a dependency parser (McDon-
ald et al, 2005b) and once with a phrase-structure
parser (Charniak, 2000). These parsers have been
trained out-of-domain on the Penn WSJ Treebank
and as a result contain noise. However, we are
merely going to use them as an additional source
of features. We call this soft syntactic evidence
since the deep trees are not used as a strict gold-
standard in our model but just as more evidence for
300
root0
saw2
on4 after6
Mary1 Ralph3 Tuesday5 lunch7
S
VP
PP PP
NP NP NP NP
NNP VBD NNP IN NNP IN NN
Mary1 saw2 Ralph3 on4 Tuesday5 after6 lunch7
Figure 2: An example dependency tree from the McDonald et al (2005b) parser and phrase structure
tree from the Charniak (2000) parser. In this example we want to add features from the trees for the case
when Ralph and after become adjacent in the compression, i.e., we are dropping the phrase on Tuesday.
or against particular compressions. The learning
algorithm will set the feature weight accordingly
depending on each features discriminative power.
It is not unique to use soft syntactic features in
this way, as it has been done for many problems
in language processing. However, we stress this
aspect of our model due to the history of compres-
sion systems using syntax to provide hard struc-
tural constraints on the output.
Lets consider the sentence x = Mary saw Ralph
on Tuesday after lunch, with corresponding parses
given in Figure 2. In particular, lets consider the
feature representation f(x,3,6). That is, the fea-
ture representation of making Ralph and after ad-
jacent in the compression and dropping the prepo-
sitional phrase on Tuesday. The first set of features
we consider are over dependency trees. For every
dropped word we add a feature indicating the POS
of the words parent in the tree. For example, if
the dropped words parent is root, then it typically
means it is the main verb of the sentence and un-
likely to be dropped. We also add a conjunction
feature of the POS tag of the word being dropped
and the POS of its parent as well as a feature in-
dicating for each word being dropped whether it
is a leaf node in the tree. We also add the same
features for the two adjacent words, but indicating
that they are part of the compression.
For the phrase-structure features we find every
node in the tree that subsumes a piece of dropped
text and is not a child of a similar node. In this case
the PP governing on Tuesday. We then add fea-
tures indicating the context from which this node
was dropped. For example we add a feature spec-
ifying that a PP was dropped which was the child
of a VP. We also add a feature indicating that a PP
was dropped which was the left sibling of another
PP, etc. Ideally, for each production in the tree we
would like to add a feature indicating every node
that was dropped, e.g. ?VP?VBD NP PP PP ?
VP?VBD NP PP?. However, we cannot neces-
sarily calculate this feature since the extent of the
production might be well beyond the local context
of first-order feature factorization. Furthermore,
since the training set is so small, these features are
likely to be observed very few times.
3.2.3 Feature Set Summary
In this section we have described a rich feature
set over adjacent words in the compressed sen-
tence, dropped words and phrases from the origi-
nal sentence, and properties of deep syntactic trees
of the original sentence. Note that these features in
many ways mimic the information already present
in the noisy-channel and decision-tree models of
Knight and Marcu (2000). Our bigram features
encode properties that indicate both good and bad
words to be adjacent in the compressed sentence.
This is similar in purpose to the source model from
the noisy-channel system. However, in that sys-
tem, the source model is trained on uncompressed
sentences and thus is not as representative of likely
bigram features for compressed sentences, which
is really what we desire.
Our feature set alo encodes dropped words
and phrases through the properties of the words
themselves and through properties of their syntac-
tic relation to the rest of the sentence in a parse
tree. These features represent likely phrases to be
dropped in the compression and are thus similar in
nature to the channel model in the noisy-channel
system as well as the features in the tree-to-tree de-
cision tree system. However, we use these syntac-
tic constraints as soft evidence in our model. That
is, they represent just another layer of evidence to
be considered during training when setting param-
eters. Thus, if the parses have too much noise,
the learning algorithm can lower the weight of the
parse features since they are unlikely to be use-
ful discriminators on the training data. This dif-
fers from the models of Knight and Marcu (2000),
which treat the noisy parses as gold-standard when
301
calculating probability estimates.
An important distinction we should make is the
notion of supported versus unsupported features
(Sha and Pereira, 2003). Supported features are
those that are on for the gold standard compres-
sions in the training. For instance, the bigram fea-
ture ?NN&VB? will be supported since there is
most likely a compression that contains a adjacent
noun and verb. However, the feature ?JJ&VB?
will not be supported since an adjacent adjective
and verb most likely will not be observed in any
valid compression. Our model includes all fea-
tures, including those that are unsupported. The
advantage of this is that the model can learn nega-
tive weights for features that are indicative of bad
compressions. This is not difficult to do since most
features are POS based and the feature set size
even with all these features is only 78,923.
3.3 Learning
Having defined a feature encoding and decod-
ing algorithm, the last step is to learn the fea-
ture weights w. We do this using the Margin
Infused Relaxed Algorithm (MIRA), which is a
discriminative large-margin online learning tech-
nique shown in Figure 3 (Crammer and Singer,
2003). On each iteration, MIRA considers a single
instance from the training set (xt,yt) and updates
the weights so that the score of the correct com-
pression, yt, is greater than the score of all other
compressions by a margin proportional to their
loss. Many weight vectors will satisfy these con-
straints so we pick the one with minimum change
from the previous setting. We define the loss to be
the number of words falsely retained or dropped
in the incorrect compression relative to the correct
one. For instance, if the correct compression of the
sentence in Figure 2 is Mary saw Ralph, then the
compression Mary saw after lunch would have a
loss of 3 since it incorrectly left out one word and
included two others.
Of course, for a sentence there are exponentially
many possible compressions, which means that
this optimization will have exponentially many
constraints. We follow the method of McDon-
ald et al (2005b) and create constraints only on
the k compressions that currently have the high-
est score, bestk(x; w). This can easily be calcu-
lated by extending the decoding algorithm with
standard Viterbi k-best techniques. On the devel-
opment data, we found that k = 10 provided the
Training data: T = {(xt, yt)}Tt=1
1. w0 = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. min
?
?
?
w(i+1) ? w(i)
?
?
?
s.t. s(xt, yt) ? s(xt, y?) ? L(yt, y?)
where y? ? bestk(x; w(i))
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 3: MIRA learning algorithm as presented
by McDonald et al (2005b).
best performance, though varying k did not have a
major impact overall. Furthermore we found that
after only 3-5 training epochs performance on the
development data was maximized.
The final weight vector is the average of all
weight vectors throughout training. Averaging has
been shown to reduce overfitting (Collins, 2002)
as well as reliance on the order of the examples
during training. We found it to be particularly im-
portant for this data set.
4 Experiments
We use the same experimental methodology as
Knight and Marcu (2000). We provide every com-
pression to four judges and ask them to evaluate
each one for grammaticality and importance on a
scale from 1 to 5. For each of the 32 sentences in
our test set we ask the judges to evaluate three sys-
tems: human annotated, the decision tree model
of Knight and Marcu (2000) and our system. The
judges were told all three compressions were au-
tomatically generated and the order in which they
were presented was randomly chosen for each sen-
tence. We compared our system to the decision
tree model of Knight and Marcu instead of the
noisy-channel model since both performed nearly
as well in their evaluation, and the compression
rate of the decision tree model is nearer to our sys-
tem (around 57-58%). The noisy-channel model
typically returned longer compressions.
Results are shown in Table 1. We present the av-
erage score over all judges as well as the standard
deviation. The evaluation for the decision tree sys-
tem of Knight and Marcu is strikingly similar to
the original evaluation in their work. This provides
strong evidence that the evaluation criteria in both
cases were very similar.
Table 1 shows that all models had similar com-
302
Compression Rate Grammaticality Importance
Human 53.3% 4.96 ? 0.2 3.91 ? 1.0
Decision-Tree (K&M2000) 57.2% 4.30 ? 1.4 3.60 ? 1.3
This work 58.1% 4.61 ? 0.8 4.03 ? 1.0
Table 1: Compression results.
pressions rates, with humans preferring to com-
press a little more aggressively. Not surprisingly,
the human compressions are practically all gram-
matical. A quick scan of the evaluations shows
that the few ungrammatical human compressions
were for sentences that were not really gram-
matical in the first place. Of greater interest is
that the compressions of our system are typically
more grammatical than the decision tree model of
Knight and Marcu.
When looking at importance, we see that our
system actually does the best ? even better than
humans. The most likely reason for this is that
our model returns longer sentences and is thus less
likely to prune away important information. For
example, consider the sentence
The chemical etching process used for glare protection is
effective and will help if your office has the fluorescent-light
overkill that?s typical in offices
The human compression was Glare protection is
effective, whereas our model compressed the sen-
tence to The chemical etching process used for
glare protection is effective.
A primary reason that our model does better
than the decision tree model of Knight and Marcu
is that on a handful of sentences, the decision tree
compressions were a single word or noun-phrase.
For such sentences the evaluators typically rated
the compression a 1 for both grammaticality and
importance. In contrast, our model never failed
in such drastic ways and always output something
reasonable. This is quantified in the standard de-
viation of the two systems.
Though these results are promising, more large
scale experiments are required to really ascer-
tain the significance of the performance increase.
Ideally we could sample multiple training/testing
splits and use all sentences in the data set to eval-
uate the systems. However, since these systems
require human evaluation we did not have the time
or the resources to conduct these experiments.
4.1 Some Examples
Here we aim to give the reader a flavor of some
common outputs from the different models. Three
examples are given in Table 4.1. The first shows
two properties. First of all, the decision tree
model completely breaks and just returns a sin-
gle noun-phrase. Our system performs well, how-
ever it leaves out the complementizer of the rela-
tive clause. This actually occurred in a few exam-
ples and appears to be the most common problem
of our model. A post-processing rule should elim-
inate this.
The second example displays a case in which
our system and the human system are grammati-
cal, but the removal of a prepositional phrase hurts
the resulting meaning of the sentence. In fact,
without the knowledge that the sentence is refer-
ring to broadband, the compressions are mean-
ingless. This appears to be a harder problem ?
determining which prepositional phrases can be
dropped and which cannot.
The final, and more interesting, example
presents two very different compressions by the
human and our automatic system. Here, the hu-
man kept the relative clause relating what lan-
guages the source code is available in, but dropped
the main verb phrase of the sentence. Our model
preferred to retain the main verb phrase and drop
the relative clause. This is most likely due to the
fact that dropping the main verb phrase of a sen-
tence is much less likely in the training data than
dropping a relative clause. Two out of four eval-
uators preferred the compression returned by our
system and the other two rated them equal.
5 Discussion
In this paper we have described a new system for
sentence compression. This system uses discrim-
inative large-margin learning techniques coupled
with a decoding algorithm that searches the space
of all compressions. In addition we defined a
rich feature set of bigrams in the compression and
dropped words and phrases from the original sen-
tence. The model also incorporates soft syntactic
evidence in the form of features over dependency
and phrase-structure trees for each sentence.
This system has many advantages over previous
approaches. First of all its discriminative nature
allows us to use a rich dependent feature set and
to optimize a function directly related to compres-
303
Full Sentence The fi rst new product , ATF Protype , is a line of digital postscript typefaces that will be sold in packages of up to six fonts .
Human ATF Protype is a line of digital postscript typefaces that will be sold in packages of up to six fonts .
Decision Tree The fi rst new product .
This work ATF Protype is a line of digital postscript typefaces will be sold in packages of up to six fonts .
Full Sentence Finally , another advantage of broadband is distance .
Human Another advantage is distance .
Decision Tree Another advantage of broadband is distance .
This work Another advantage is distance .
Full Sentence The source code , which is available for C , Fortran , ADA and VHDL , can be compiled and executed on the same system or ported to other
target platforms .
Human The source code is available for C , Fortran , ADA and VHDL .
Decision Tree The source code is available for C .
This work The source code can be compiled and executed on the same system or ported to other target platforms .
Table 2: Example compressions for the evaluation data.
sion accuracy during training, both of which have
been shown to be beneficial for other problems.
Furthermore, the system does not rely on the syn-
tactic parses of the sentences to calculate probabil-
ity estimates. Instead, this information is incorpo-
rated as just another form of evidence to be consid-
ered during training. This is advantageous because
these parses are trained on out-of-domain data and
often contain a significant amount of noise.
A fundamental flaw with all sentence compres-
sion systems is that model parameters are set with
the assumption that there is a single correct answer
for each sentence. Of course, like most compres-
sion and translation tasks, this is not true, consider,
TapeWare , which supports DOS and NetWare 286 , is a
value-added process that lets you directly connect the
QA150-EXAT to a file server and issue a command from any
workstation to back up the server
The human annotated compression is, TapeWare
supports DOS and NetWare 286. However, an-
other completely valid compression might be,
TapeWare lets you connect the QA150-EXAT to a
fi le server. These two compressions overlap by a
single word.
Our learning algorithm may unnecessarily
lower the score of some perfectly valid compres-
sions just because they were not the exact com-
pression chosen by the human annotator. A pos-
sible direction of research is to investigate multi-
label learning techniques for structured data (Mc-
Donald et al, 2005a) that learn a scoring function
separating a set of valid answers from all invalid
answers. Thus if a sentence has multiple valid
compressions we can learn to score each valid one
higher than all invalid compressions during train-
ing to avoid this problem.
Acknowledgments
The author would like to thank Daniel Marcu for
providing the data as well as the output of his
and Kevin Knight?s systems. Thanks also to Hal
Daume? and Fernando Pereira for useful discus-
sions. Finally, the author thanks the four review-
ers for evaluating the compressed sentences. This
work was supported by NSF ITR grants 0205448
and 0428193.
References
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL.
M. Collins. 2002. Discriminative training methods
for hiddenMarkov models: Theory and experiments
with perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. JMLR.
K. Knight and D. Marcu. 2000. Statistical-based sum-
marization - step one: Sentence compression. In
Proc. AAAI 2000.
R. McDonald, K. Crammer, and F. Pereira. 2005a.
Flexible text segmentation with structured multil-
abel classifi cation. In Proc. HLT-EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In
Proc. ACL.
S. Riezler, T. H. King, R. Crouch, and A. Zaenen.
2003. Statistical sentence condensation using ambi-
guity packing and stochastic disambiguation meth-
ods for lexical-functional grammar. In Proc. HLT-
NAACL.
S. Sarawagi and W. Cohen. 2004. Semi-Markov con-
ditional random fi elds for information extraction. In
Proc. NIPS.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fi elds. In Proc. HLT-NAACL, pages
213?220.
J. Turner and E. Charniak. 2005. Supervised and un-
supervised learning for sentence compression. In
Proc. ACL.
304
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 523?530, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Non-projective Dependency Parsing using Spanning Tree Algorithms
Ryan McDonald Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
{ryantm,pereira}@cis.upenn.edu
Kiril Ribarov Jan Hajic?
Institute of Formal and Applied Linguistics
Charles University
{ribarov,hajic}@ufal.ms.mff.cuni.cz
Abstract
We formalize weighted dependency pars-
ing as searching for maximum spanning
trees (MSTs) in directed graphs. Using
this representation, the parsing algorithm
of Eisner (1996) is sufficient for search-
ing over all projective trees in O(n3) time.
More surprisingly, the representation is
extended naturally to non-projective pars-
ing using Chu-Liu-Edmonds (Chu and
Liu, 1965; Edmonds, 1967) MST al-
gorithm, yielding an O(n2) parsing al-
gorithm. We evaluate these methods
on the Prague Dependency Treebank us-
ing online large-margin learning tech-
niques (Crammer et al, 2003; McDonald
et al, 2005) and show that MST parsing
increases efficiency and accuracy for lan-
guages with non-projective dependencies.
1 Introduction
Dependency parsing has seen a surge of inter-
est lately for applications such as relation extrac-
tion (Culotta and Sorensen, 2004), machine trans-
lation (Ding and Palmer, 2005), synonym genera-
tion (Shinyama et al, 2002), and lexical resource
augmentation (Snow et al, 2004). The primary
reasons for using dependency structures instead of
more informative lexicalized phrase structures is
that they are more efficient to learn and parse while
still encoding much of the predicate-argument infor-
mation needed in applications.
root John hit the ball with the bat
Figure 1: An example dependency tree.
Dependency representations, which link words to
their arguments, have a long history (Hudson, 1984).
Figure 1 shows a dependency tree for the sentence
John hit the ball with the bat. We restrict ourselves
to dependency tree analyses, in which each word de-
pends on exactly one parent, either another word or a
dummy root symbol as shown in the figure. The tree
in Figure 1 is projective, meaning that if we put the
words in their linear order, preceded by the root, the
edges can be drawn above the words without cross-
ings, or, equivalently, a word and its descendants
form a contiguous substring of the sentence.
In English, projective trees are sufficient to ana-
lyze most sentence types. In fact, the largest source
of English dependency trees is automatically gener-
ated from the Penn Treebank (Marcus et al, 1993)
and is by convention exclusively projective. How-
ever, there are certain examples in which a non-
projective tree is preferable. Consider the sentence
John saw a dog yesterday which was a Yorkshire Ter-
rier. Here the relative clause which was a Yorkshire
Terrier and the object it modifies (the dog) are sep-
arated by an adverb. There is no way to draw the
dependency tree for this sentence in the plane with
no crossing edges, as illustrated in Figure 2. In lan-
guages with more flexible word order than English,
such as German, Dutch and Czech, non-projective
dependencies are more frequent. Rich inflection
systems reduce reliance on word order to express
523
root John saw a dog yesterday which was a Yorkshire Terrier
root O to nove? ve?ts?inou nema? ani za?jem a taky na to ve?ts?inou nema? pen??ze
He is mostly not even interested in the new things and in most cases, he has no money for it either.
Figure 2: Non-projective dependency trees in English and Czech.
grammatical relations, allowing non-projective de-
pendencies that we need to represent and parse ef-
ficiently. A non-projective example from the Czech
Prague Dependency Treebank (Hajic? et al, 2001) is
also shown in Figure 2.
Most previous dependency parsing models have
focused on projective trees, including the work
of Eisner (1996), Collins et al (1999), Yamada and
Matsumoto (2003), Nivre and Scholz (2004), and
McDonald et al (2005). These systems have shown
that accurate projective dependency parsers can be
automatically learned from parsed data. However,
non-projective analyses have recently attracted some
interest, not only for languages with freer word order
but also for English. In particular, Wang and Harper
(2004) describe a broad coverage non-projective
parser for English based on a hand-constructed con-
straint dependency grammar rich in lexical and syn-
tactic information. Nivre and Nilsson (2005) pre-
sented a parsing model that allows for the introduc-
tion of non-projective edges into dependency trees
through learned edge transformations within their
memory-based parser. They test this system on
Czech and show improved accuracy relative to a pro-
jective parser. Our approach differs from those ear-
lier efforts in searching optimally and efficiently the
full space of non-projective trees.
The main idea of our method is that dependency
parsing can be formalized as the search for a maxi-
mum spanning tree in a directed graph. This formal-
ization generalizes standard projective parsing mod-
els based on the Eisner algorithm (Eisner, 1996) to
yield efficient O(n2) exact parsing methods for non-
projective languages like Czech. Using this span-
ning tree representation, we extend the work of Mc-
Donald et al (2005) on online large-margin discrim-
inative training methods to non-projective depen-
dencies.
The present work is related to that of Hirakawa
(2001) who, like us, reduces the problem of depen-
dency parsing to spanning tree search. However, his
parsing method uses a branch and bound algorithm
that is exponential in the worst case, even though
it appears to perform reasonably in limited experi-
ments. Furthermore, his work does not adequately
address learning or measure parsing accuracy on
held-out data.
Section 2 describes an edge-based factorization
of dependency trees and uses it to equate depen-
dency parsing to the problem of finding maximum
spanning trees in directed graphs. Section 3 out-
lines the online large-margin learning framework
used to train our dependency parsers. Finally, in
Section 4 we present parsing results for Czech. The
trees in Figure 1 and Figure 2 are untyped, that
is, edges are not partitioned into types representing
additional syntactic information such as grammati-
cal function. We study untyped dependency trees
mainly, but edge types can be added with simple ex-
tensions to the methods discussed here.
2 Dependency Parsing and Spanning Trees
2.1 Edge Based Factorization
In what follows, x = x1 ? ? ? xn represents a generic
input sentence, and y represents a generic depen-
dency tree for sentence x. Seeing y as the set of tree
edges, we write (i, j) ? y if there is a dependency
in y from word xi to word xj .
In this paper we follow a common method of fac-
toring the score of a dependency tree as the sum of
the scores of all edges in the tree. In particular, we
define the score of an edge to be the dot product be-
524
tween a high dimensional feature representation of
the edge and a weight vector,
s(i, j) = w ? f(i, j)
Thus the score of a dependency tree y for sentence
x is,
s(x,y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? f(i, j)
Assuming an appropriate feature representation as
well as a weight vector w, dependency parsing is the
task of finding the dependency tree y with highest
score for a given sentence x.
For the rest of this section we assume that the
weight vector w is known and thus we know the
score s(i, j) of each possible edge. In Section 3 we
present a method for learning the weight vector.
2.2 Maximum Spanning Trees
We represent the generic directed graph G = (V,E)
by its vertex set V = {v1, . . . , vn} and set E ? [1 :
n]? [1 : n] of pairs (i, j) of directed edges vi ? vj .
Each such edge has a score s(i, j). Since G is di-
rected, s(i, j) does not necessarily equal s(j, i). A
maximum spanning tree (MST) of G is a tree y ? E
that maximizes the value
?
(i,j)?y s(i, j) such that
every vertex in V appears in y. The maximum pro-
jective spanning tree of G is constructed similarly
except that it can only contain projective edges rel-
ative to some total order on the vertices of G. The
MST problem for directed graphs is also known as
the maximum arborescence problem.
For each sentence x we define the directed graph
Gx = (Vx, Ex) given by
Vx = {x0 = root, x1, . . . , xn}
Ex = {(i, j) : i 6= j, (i, j) ? [0 : n] ? [1 : n]}
That is, Gx is a graph with the sentence words and
the dummy root symbol as vertices and a directed
edge between every pair of distinct words and from
the root symbol to every word. It is clear that de-
pendency trees for x and spanning trees for Gx co-
incide, since both kinds of trees are required to be
rooted at the dummy root and reach all the words
in the sentence. Hence, finding a (projective) depen-
dency tree with highest score is equivalent to finding
a maximum (projective) spanning tree in Gx.
Chu-Liu-Edmonds(G, s)
Graph G = (V, E)
Edge weight function s : E ? R
1. Let M = {(x?, x) : x ? V, x? = arg maxx? s(x?, x)}
2. Let GM = (V, M)
3. If GM has no cycles, then it is an MST: return GM
4. Otherwise, find a cycle C in GM
5. Let GC = contract(G, C, s)
6. Let y = Chu-Liu-Edmonds(GC , s)
7. Find a vertex x ? C s. t. (x?, x) ? y, (x??, x) ? C
8. return y ? C ? {(x??, x)}
contract(G = (V, E), C, s)
1. Let GC be the subgraph of G excluding nodes in C
2. Add a node c to GC representing cycle C
3. For x ? V ? C : ?x??C(x?, x) ? E
Add edge (c, x) to GC with
s(c, x) = maxx??C s(x?, x)
4. For x ? V ? C : ?x??C(x, x?) ? E
Add edge (x, c) to GC with
s(x, c) = maxx??C [s(x, x?) ? s(a(x?), x?) + s(C)]
where a(v) is the predecessor of v in C
and s(C) = Pv?C s(a(v), v)
5. return GC
Figure 3: Chu-Liu-Edmonds algorithm for finding
maximum spanning trees in directed graphs.
2.2.1 Non-projective Trees
To find the highest scoring non-projective tree we
simply search the entire space of spanning trees with
no restrictions. Well-known algorithms exist for the
less general case of finding spanning trees in undi-
rected graphs (Cormen et al, 1990).
Efficient algorithms for the directed case are less
well known, but they exist. We will use here the
Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;
Edmonds, 1967), sketched in Figure 3 follow-
ing Leonidas (2003). Informally, the algorithm has
each vertex in the graph greedily select the incoming
edge with highest weight. If a tree results, it must be
the maximum spanning tree. If not, there must be a
cycle. The procedure identifies a cycle and contracts
it into a single vertex and recalculates edge weights
going into and out of the cycle. It can be shown that
a maximum spanning tree on the contracted graph is
equivalent to a maximum spanning tree in the orig-
inal graph (Leonidas, 2003). Hence the algorithm
can recursively call itself on the new graph. Naively,
this algorithm runs in O(n3) time since each recur-
sive call takes O(n2) to find the highest incoming
edge for each word and to contract the graph. There
are at most O(n) recursive calls since we cannot
contract the graph more then n times. However,
525
Tarjan (1977) gives an efficient implementation of
the algorithm with O(n2) time complexity for dense
graphs, which is what we need here.
To find the highest scoring non-projective tree for
a sentence, x, we simply construct the graph Gx
and run it through the Chu-Liu-Edmonds algorithm.
The resulting spanning tree is the best non-projective
dependency tree. We illustrate here the application
of the Chu-Liu-Edmonds algorithm to dependency
parsing on the simple example x = John saw Mary,
with directed graph representation Gx,
root
saw
John Mary
10
9
9
30
3020
3
0
11
The first step of the algorithm is to find, for each
word, the highest scoring incoming edge
root
saw
John Mary30
3020
If the result were a tree, it would have to be the
maximum spanning tree. However, in this case we
have a cycle, so we will contract it into a single node
and recalculate edge weights according to Figure 3.
root
saw
John Mary
40
9
30
31
wjs
The new vertex wjs represents the contraction of
vertices John and saw. The edge from wjs to Mary
is 30 since that is the highest scoring edge from any
vertex in wjs. The edge from root into wjs is set to
40 since this represents the score of the best span-
ning tree originating from root and including only
the vertices in wjs. The same leads to the edge
from Mary to wjs. The fundamental property of the
Chu-Liu-Edmonds algorithm is that an MST in this
graph can be transformed into an MST in the orig-
inal graph (Leonidas, 2003). Thus, we recursively
call the algorithm on this graph. Note that we need
to keep track of the real endpoints of the edges into
and out of wjs for reconstruction later. Running the
algorithm, we must find the best incoming edge to
all words
root
saw
John Mary
40
30
wjs
This is a tree and thus the MST of this graph. We
now need to go up a level and reconstruct the graph.
The edge from wjs to Mary originally was from the
word saw, so we include that edge. Furthermore, the
edge from root to wjs represented a tree from root to
saw to John, so we include all those edges to get the
final (and correct) MST,
root
saw
John Mary
10
3030
A possible concern with searching the entire space
of spanning trees is that we have not used any syn-
tactic constraints to guide the search. Many lan-
guages that allow non-projectivity are still primarily
projective. By searching all possible non-projective
trees, we run the risk of finding extremely bad trees.
We address this concern in Section 4.
2.2.2 Projective Trees
It is well known that projective dependency pars-
ing using edge based factorization can be handled
with the Eisner algorithm (Eisner, 1996). This al-
gorithm has a runtime of O(n3) and has been em-
ployed successfully in both generative and discrimi-
native parsing models (Eisner, 1996; McDonald et
al., 2005). Furthermore, it is trivial to show that
the Eisner algorithm solves the maximum projective
spanning tree problem.
The Eisner algorithm differs significantly from
the Chu-Liu-Edmonds algorithm. First of all, it is a
bottom-up dynamic programming algorithm as op-
posed to a greedy recursive one. A bottom-up al-
gorithm is necessary for the projective case since it
must maintain the nested structural constraint, which
is unnecessary for the non-projective case.
2.3 Dependency Trees as MSTs: Summary
In the preceding discussion, we have shown that nat-
ural language dependency parsing can be reduced to
finding maximum spanning trees in directed graphs.
This reduction results from edge-based factoriza-
tion and can be applied to projective languages with
526
the Eisner parsing algorithm and non-projective lan-
guages with the Chu-Liu-Edmonds maximum span-
ning tree algorithm. The only remaining problem is
how to learn the weight vector w.
A major advantage of our approach over other
dependency parsing models is its uniformity and
simplicity. By viewing dependency structures as
spanning trees, we have provided a general frame-
work for parsing trees for both projective and non-
projective languages. Furthermore, the resulting
parsing algorithms are more efficient than lexi-
calized phrase structure approaches to dependency
parsing, allowing us to search the entire space with-
out any pruning. In particular the non-projective
parsing algorithm based on the Chu-Liu-Edmonds
MST algorithm provides true non-projective pars-
ing. This is in contrast to other non-projective meth-
ods, such as that of Nivre and Nilsson (2005), who
implement non-projectivity in a pseudo-projective
parser with edge transformations. This formulation
also dispels the notion that non-projective parsing is
?harder? than projective parsing. In fact, it is eas-
ier since non-projective parsing does not need to en-
force the non-crossing constraint of projective trees.
As a result, non-projective parsing complexity is just
O(n2), against the O(n3) complexity of the Eis-
ner dynamic programming algorithm, which by con-
struction enforces the non-crossing constraint.
3 Online Large Margin Learning
In this section, we review the work of McDonald et
al. (2005) for online large-margin dependency pars-
ing. As usual for supervised learning, we assume a
training set T = {(xt,yt)}Tt=1, consisting of pairs
of a sentence xt and its correct dependency tree yt.
In what follows, dt(x) denotes the set of possible
dependency trees for sentence x.
The basic idea is to extend the Margin Infused
Relaxed Algorithm (MIRA) (Crammer and Singer,
2003; Crammer et al, 2003) to learning with struc-
tured outputs, in the present case dependency trees.
Figure 4 gives pseudo-code for the MIRA algorithm
as presented by McDonald et al (2005). An on-
line learning algorithm considers a single training
instance at each update to w. The auxiliary vector
v accumulates the successive values of w, so that the
final weight vector is the average of the weight vec-
Training data: T = {(xt, yt)}Tt=1
1. w0 = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. min
?
?
?
w(i+1) ? w(i)
?
?
?
s.t. s(xt, yt) ? s(xt, y?) ? L(yt, y?), ?y? ? dt(xt)
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 4: MIRA learning algorithm.
tors after each iteration. This averaging effect has
been shown to help overfitting (Collins, 2002).
On each update, MIRA attempts to keep the new
weight vector as close as possible to the old weight
vector, subject to correctly classifying the instance
under consideration with a margin given by the loss
of the incorrect classifications. For dependency
trees, the loss of a tree is defined to be the number
of words with incorrect parents relative to the correct
tree. This is closely related to the Hamming loss that
is often used for sequences (Taskar et al, 2003).
For arbitrary inputs, there are typically exponen-
tially many possible parses and thus exponentially
many margin constraints in line 4 of Figure 4.
3.1 Single-best MIRA
One solution for the exponential blow-up in number
of trees is to relax the optimization by using only the
single margin constraint for the tree with the highest
score, s(x,y). The resulting online update (to be
inserted in Figure 4, line 4) would then be:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
where y? = arg maxy? s(xt,y?)
McDonald et al (2005) used a similar update with
k constraints for the k highest-scoring trees, and
showed that small values of k are sufficient to
achieve the best accuracy for these methods. How-
ever, here we stay with a single best tree because k-
best extensions to the Chu-Liu-Edmonds algorithm
are too inefficient (Hou, 1996).
This model is related to the averaged perceptron
algorithm of Collins (2002). In that algorithm, the
single highest scoring tree (or structure) is used to
update the weight vector. However, MIRA aggres-
sively updates w to maximize the margin between
527
the correct tree and the highest scoring tree, which
has been shown to lead to increased accuracy.
3.2 Factored MIRA
It is also possible to exploit the structure of the out-
put space and factor the exponential number of mar-
gin constraints into a polynomial number of local
constraints (Taskar et al, 2003; Taskar et al, 2004).
For the directed maximum spanning tree problem,
we can factor the output by edges to obtain the fol-
lowing constraints:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(l, j) ? s(k, j) ? 1
?(l, j) ? yt, (k, j) /? yt
This states that the weight of the correct incoming
edge to the word xj and the weight of all other in-
coming edges must be separated by a margin of 1.
It is easy to show that when all these constraints
are satisfied, the correct spanning tree and all incor-
rect spanning trees are separated by a score at least
as large as the number of incorrect incoming edges.
This is because the scores for all the correct arcs can-
cel out, leaving only the scores for the errors causing
the difference in overall score. Since each single er-
ror results in a score increase of at least 1, the entire
score difference must be at least the number of er-
rors. For sequences, this form of factorization has
been called local lattice preference (Crammer et al,
2004). Let n be the number of nodes in graph Gx.
Then the number of constraints is O(n2), since for
each node we must maintain n ? 1 constraints.
The factored constraints are in general more re-
strictive than the original constraints, so they may
rule out the optimal solution to the original prob-
lem. McDonald et al (2005) examines briefly fac-
tored MIRA for projective English dependency pars-
ing, but for that application, k-best MIRA performs
as well or better, and is much faster to train.
4 Experiments
We performed experiments on the Czech Prague De-
pendency Treebank (PDT) (Hajic?, 1998; Hajic? et al,
2001). We used the predefined training, develop-
ment and testing split of this data set. Furthermore,
we used the automatically generated POS tags that
are provided with the data. Czech POS tags are very
complex, consisting of a series of slots that may or
may not be filled with some value. These slots rep-
resent lexical and grammatical properties such as
standard POS, case, gender, and tense. The result
is that Czech POS tags are rich in information, but
quite sparse when viewed as a whole. To reduce
sparseness, our features rely only on the reduced
POS tag set from Collins et al (1999). The num-
ber of features extracted from the PDT training set
was 13, 450, 672, using the feature set outlined by
McDonald et al (2005).
Czech has more flexible word order than English
and as a result the PDT contains non-projective de-
pendencies. On average, 23% of the sentences in
the training, development and test sets have at least
one non-projective dependency. However, less than
2% of total edges are actually non-projective. There-
fore, handling non-projective edges correctly has a
relatively small effect on overall accuracy. To show
the effect more clearly, we created two Czech data
sets. The first, Czech-A, consists of the entire PDT.
The second, Czech-B, includes only the 23% of sen-
tences with at least one non-projective dependency.
This second set will allow us to analyze the effec-
tiveness of the algorithms on non-projective mate-
rial. We compared the following systems:
1. COLL1999: The projective lexicalized phrase-structure
parser of Collins et al (1999).
2. N&N2005: The pseudo-projective parser of Nivre and
Nilsson (2005).
3. McD2005: The projective parser of McDonald et al
(2005) that uses the Eisner algorithm for both training and
testing. This system uses k-best MIRA with k=5.
4. Single-best MIRA: In this system we use the Chu-Liu-
Edmonds algorithm to find the best dependency tree for
Single-best MIRA training and testing.
5. Factored MIRA: Uses the quadratic set of constraints
based on edge factorization as described in Section 3.2.
We use the Chu-Liu-Edmonds algorithm to find the best
tree for the test data.
4.1 Results
Results are shown in Table 1. There are two main
metrics. The first and most widely recognized is Ac-
curacy, which measures the number of words that
correctly identified their parent in the tree. Complete
measures the number of sentences in which the re-
sulting tree was completely correct.
Clearly, there is an advantage in using the Chu-
Liu-Edmonds algorithm for Czech dependency pars-
528
Czech-A Czech-B
Accuracy Complete Accuracy Complete
COLL1999 82.8 - - -
N&N2005 80.0 31.8 - -
McD2005 83.3 31.3 74.8 0.0
Single-best MIRA 84.1 32.2 81.0 14.9
Factored MIRA 84.4 32.3 81.5 14.3
Table 1: Dependency parsing results for Czech. Czech-B is the subset of Czech-A containing only sentences
with at least one non-projective dependency.
ing. Even though less than 2% of all dependencies
are non-projective, we still see an absolute improve-
ment of up to 1.1% in overall accuracy over the
projective model. Furthermore, when we focus on
the subset of data that only contains sentences with
at least one non-projective dependency, the effect
is amplified. Another major improvement here is
that the Chu-Liu-Edmonds non-projective MST al-
gorithm has a parsing complexity of O(n2), versus
the O(n3) complexity of the projective Eisner algo-
rithm, which in practice leads to improvements in
parsing time. The results also show that in terms
of Accuracy, factored MIRA performs better than
single-best MIRA. However, for the factored model,
we do have O(n2) margin constraints, which re-
sults in a significant increase in training time over
single-best MIRA. Furthermore, we can also see that
the MST parsers perform favorably compared to the
more powerful lexicalized phrase-structure parsers,
such as those presented by Collins et al (1999) and
Zeman (2004) that use expensive O(n5) parsing al-
gorithms. We should note that the results in Collins
et al (1999) are different then reported here due to
different training and testing data sets.
One concern raised in Section 2.2.1 is that search-
ing the entire space of non-projective trees could
cause problems for languages that are primarily pro-
jective. However, as we can see, this is not a prob-
lem. This is because the model sets its weights with
respect to the parsing algorithm and will disfavor
features over unlikely non-projective edges.
Since the space of projective trees is a subset of
the space of non-projective trees, it is natural to won-
der how the Chu-Liu-Edmonds parsing algorithm
performs on projective data since it is asymptotically
better than the Eisner algorithm. Table 2 shows the
results for English projective dependency trees ex-
tracted from the Penn Treebank (Marcus et al, 1993)
using the rules of Yamada and Matsumoto (2003).
English
Accuracy Complete
McD2005 90.9 37.5
Single-best MIRA 90.2 33.2
Factored MIRA 90.2 32.3
Table 2: Dependency parsing results for English us-
ing spanning tree algorithms.
This shows that for projective data sets, training
and testing with the Chu-Liu-Edmonds algorithm is
worse than using the Eisner algorithm. This is not
surprising since the Eisner algorithm uses the a pri-
ori knowledge that all trees are projective.
5 Discussion
We presented a general framework for parsing de-
pendency trees based on an equivalence to maxi-
mum spanning trees in directed graphs. This frame-
work provides natural and efficient mechanisms
for parsing both projective and non-projective lan-
guages through the use of the Eisner and Chu-Liu-
Edmonds algorithms. To learn these structures we
used online large-margin learning (McDonald et al,
2005) that empirically provides state-of-the-art per-
formance for Czech.
A major advantage of our models is the abil-
ity to naturally model non-projective parses. Non-
projective parsing is commonly considered more
difficult than projective parsing. However, under
our framework, we show that the opposite is actually
true that non-projective parsing has a lower asymp-
totic complexity. Using this framework, we pre-
sented results showing that the non-projective model
outperforms the projective model on the Prague De-
pendency Treebank, which contains a small number
of non-projective edges.
Our method requires a tree score that decomposes
according to the edges of the dependency tree. One
might hope that the method would generalize to
529
include features of larger substructures. Unfortu-
nately, that would make the search for the best tree
intractable (Ho?ffgen, 1993).
Acknowledgments
We thank Lillian Lee for bringing an important
missed connection to our attention, and Koby Cram-
mer for his help with learning algorithms. This work
has been supported by NSF ITR grants 0205448 and
0428193.
References
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. In-
troduction to Algorithms. MIT Press/McGraw-Hill.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Proc.
NIPS.
K. Crammer, R. McDonald, and F. Pereira. 2004. New
large margin algorithms for structured prediction. In
Learning with Structured Outputs Workshop (NIPS).
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proc. ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
J. Hajic?, E. Hajicova, P. Pajas, J. Panevova, P. Sgall, and
B. Vidova Hladka. 2001. The Prague Dependency
Treebank 1.0 CDROM. Linguistics Data Consortium
Cat. No. LDC2001T10.
J. Hajic?. 1998. Building a syntactically annotated cor-
pus: The Prague dependency treebank. Issues of Va-
lency and Meaning, pages 106?132.
H. Hirakawa. 2001. Semantic dependency analysis
method for Japanese based on optimum tree search al-
gorithm. In Proc. of PACLING.
Klaus-U. Ho?ffgen. 1993. Learning and robust learning
of product distributions. In Proceedings of COLT?93,
pages 77?83.
W. Hou. 1996. Algorithm for finding the first k shortest
arborescences of a digraph. Mathematica Applicata,
9(1):1?4.
R. Hudson. 1984. Word Grammar. Blackwell.
G. Leonidas. 2003. Arborescence optimization problems
solvable by Edmonds? algorithm. Theoretical Com-
puter Science, 301:427 ? 437.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
ACL.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. ACL.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proc. HLT.
R. Snow, D. Jurafsky, and A. Y. Ng. 2004. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS 2004.
R.E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7:25?35.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP.
W. Wang and M. P. Harper. 2004. A statistical constraint
dependency grammar (CDG) parser. In Workshop on
Incremental Parsing: Bringing Engineering and Cog-
nition Together (ACL).
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
D. Zeman. 2004. Parsing with a Statistical Dependency
Model. Ph.D. thesis, Univerzita Karlova, Praha.
530
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 987?994, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Flexible Text Segmentation with Structured Multilabel Classification
Ryan McDonald Koby Crammer Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{ryantm,crammer,pereira}@cis.upenn.edu
Abstract
Many language processing tasks can be re-
duced to breaking the text into segments
with prescribed properties. Such tasks
include sentence splitting, tokenization,
named-entity extraction, and chunking.
We present a new model of text segmenta-
tion based on ideas from multilabel clas-
sification. Using this model, we can natu-
rally represent segmentation problems in-
volving overlapping and non-contiguous
segments. We evaluate the model on en-
tity extraction and noun-phrase chunking
and show that it is more accurate for over-
lapping and non-contiguous segments, but
it still performs well on simpler data sets
for which sequential tagging has been the
best method.
1 Introduction
Text segmentation is a basic task in language pro-
cessing, with applications such as tokenization, sen-
tence splitting, named-entity extraction, and chunk-
ing. Many parsers, translation systems, and extrac-
tion systems rely on such segmentations to accu-
rately process the data. Depending on the applica-
tion, segments may be tokens, phrases, or sentences.
However, in this paper we primarily focus on seg-
menting sentences into tokens.
The most common approach to text segmenta-
tion is to use finite-state sequence tagging mod-
els, in which each atomic text element (character
or token) is labeled with a tag representing its role
in a segmentation. Models of that form include
hidden Markov models (Rabiner, 1989; Bikel et
al., 1999) as well as discriminative tagging mod-
els based on maximum entropy classification (Rat-
naparkhi, 1996; McCallum et al, 2000), conditional
random fields (Lafferty et al, 2001; Sha and Pereira,
2003), and large-margin techniques (Kudo and Mat-
sumoto, 2001; Taskar et al, 2003). Tagging mod-
els are the best previous methods for text segmen-
tation. However, their purely sequential form limits
their ability to naturally handle overlapping or non-
contiguous segments.
We present here an alternative view of segmenta-
tion as structured multilabel classification. In this
view, a segmentation of a text is a set of segments,
each of which is defined by the set of text positions
that belong to the segment. Thus, a particular seg-
ment may not be a set of consecutive positions in
the text, and segments may overlap. Given a text
x = x1 ? ? ? xn, the set of possible segments, which
corresponds to the set of possible classification la-
bels, is seg(x) = {O,I}n; for y ? seg(x), yi = I
iff xi belongs to the segment. Then, our segmen-
tation task is to determine which labels are correct
segments in a given text. We have thus a structured
multilabel classification problem: each instance, a
text, may have multiple structured labels, represent-
ing each of its segments. These labels are structured
in that they do not come from a predefined set, but
instead are built from sets of choices associated to
the elements of arbitrarily long instances.
More generally, we may be interested in typed
segments, e.g. segments naming different types of
987
entities. In that case, the set of segment labels is
seg(x) = T ? {O,I}n, where T is the set of seg-
ment types. Since the extension is straightforward,
we frame the discussion in terms of untyped seg-
ments, and only discuss segment types as needed.
At first sight, it might appear that we have made
the segmentation problem intractably harder by turn-
ing it into a classification problem with a number
of labels exponential on the length of the instance.
However, we can bound the number of labels under
consideration and take advantage of the structure of
labels to find the k most likely labels efficiently. This
will allow us to exploit recent advances in online dis-
criminative methods for multilabel classification and
ranking (Crammer and Singer, 2002).
Though multilabel classification has been well
studied (Schapire and Singer, 1999; Elisseeff and
Weston, 2001), as far as we are aware, this is the
first study involving structured labels.
2 Segmentation as Tagging
The standard approach to text segmentation is to use
tagging techniques with a BIO tag set. Elements in
the input text are tagged with one of B for the be-
ginning of a contiguous segment, I for the inside
of a contiguous segment, or O for outside a seg-
ment. Thus, segments must be contiguous and non-
overlapping. For instance, consider the sentence Es-
timated volume was a light 2.4 million ounces. Fig-
ure 1a shows how this sentence would be labeled
using the BIO tag set for the problem of identifying
base NPs in text. Given a particular tagging for a
sentence, it is trivial to find all the segments, those
whose tag sequences are longest matches for the reg-
ular expression BI?. For typed segments, the BIO
tag set is easily augmented to indicate not only seg-
ment boundaries, but also the type of each segment.
Figure 1b exemplifies the tags for the task of finding
people and organizations in text.
Sequential tagging with the BIO tag set has
proven quite accurate for shallow parsing and named
entity extraction tasks (Kudo and Matsumoto, 2001;
Sha and Pereira, 2003; Tjong Kim Sang and
De Meulder, 2003). However, this approach
can only identify non-overlapping, contiguous seg-
ments. This is sufficient for some applications, and
in any case, most training data sets are annotated
without concern for overlapping or non-contiguous
segments. However, there are instances in which se-
quential labeling techniques using the BIO label set
will encounter problems.
Figure 2 shows two simple examples of segmen-
tations involving overlapping, non-contiguous seg-
ments. In both cases, it is difficult to see how a
sequential tagger could extract the segments cor-
rectly. It would be possible to grow the tag set to
represent a bounded number of overlapping, non-
contiguous segments by representing all possible
combinations of segment membership over k over-
lapping segments, but this would require an arbitrary
upper bound on k and would lead to models that gen-
eralize poorly and are expensive to train.
Dickinson and Meurers (2005) point out that, as
language processing begins to tackle problems in
free-word order languages and discourse analysis,
annotating and extracting non-contiguous segmen-
tations of text will become increasingly important.
Though we focus primarily on entity extraction and
NP chunking in this paper, there is no reason why
ideas presented here could not be extended to man-
aging other non-contiguous phenomena.
3 Structured Multilabel Classification
As outlined in Section 1, we represent segmentation
as multilabel classification, assigning to each text
the set of segments it contains. Figure 3 shows the
segments for the examples of Figure 2. Each seg-
ment is given by a O/I assignment to its words, in-
dicating which words belong to the segment.
By representing the segmentation problems as
multilabel classification, we have fundamentally
changed the objective of our learning and inference
algorithms. The sequential tagging formulation is
aimed to learn and find the best possible tagging of
a text. In multilabel classification, we train model
parameters so that correct labels ? that is, correct
segments ? receive higher score than all incorrect
ones. Likewise, inference becomes the problem of
finding the set of correct labels for a text, that is, the
set of correct segments.
We now describe the learning problem using the
decision-theoretic multilabel classification and rank-
ing framework of Crammer and Singer (2002) and
Crammer (2005) as our starting point. In Sec-
988
a. Estimated volume was a light 2.4 million ounces .
B I O B I I I I O
b. Bill Clinton and Microsoft founder Bill Gates met today for 20 minutes .
B-PER I-PER O B-ORG O B-PER I-PER O O O O O O
Figure 1: Sequential labeling formulation of text segmentation using the BIO label set. a) NP-chunking
tasks. b) Named-entity extraction task.
a) Today, Bill and Hilary Clinton traveled to Canada.
- Person: Bill Clinton
- Person: Hilary Clinton
b) ... purified bovine P450 11 beta / 18 / 19 - hydroxylase was ...
- Enzyme: P450 11 beta-hydroxylase
- Enzyme: P450 18-hydroxylase
- Enzyme: P450 19-hydroxilase
Figure 2: Examples of overlapping and non-contiguous text segmentations.
tion 3.2, we describe a polynomial-time inference
algorithm for finding up to k correct segments.
3.1 Training Multilabel Classifiers
Our model is based on a linear score s(x,y; w) for
each segment y of text x, defined as
s(x,y; w) = w ? f(x,y)
where f(x,y) is a feature vector representation of
the sentence-segment pair, and w is a vector of
feature weights. For a given text x, act(x) ?
seg(x) denotes the set of correct segments for x, and
bestk(x; w) denotes the set of k segments with high-
est score relative to the weight vector w. For learn-
ing, we use a training set T = {(xt, act(xt))}|T |t=1 of
texts labeled with the correct segmentation.
We will discuss later the design of f(x,y) and an
efficient algorithm for finding the k highest scoring
segments (where k is sufficiently large to include
all correct segments). In this section, we present a
method for learning a weight vector w that seeks to
score correct segments above all incorrect segments.
Crammer and Singer (2002), extended by Cram-
mer (2005), provide online learning algorithms for
multilabel classification and ranking that take one
instance at a time, construct a set of scoring con-
straints for the instance, and adjust the weight vec-
tor to satisfy the constraints. The constraints en-
force a margin between the scores of correct labels
and those of incorrect labels. The benefits of large-
margin learning are best known from SVMs (Cris-
tianini and Shawe-Taylor, 2000; Scho?lkopf and
Training data: T = {(xt, act(xt))}|T |t=1
1. w(0) = 0; i = 0
2. for n : 1..N
3. for t : 1..|T |
4. w(i+1) = arg minw
?
?
?
w ? w(i)
?
?
?
2
s.t. s(xt, y; w) ? s(xt, y?; w) + 1
?y ? act(xt), ?y? ? bestk(xt; w(i)) ? act(xt)
6. i = i + 1
7. w = w(N?|T |)
Figure 4: A simplified version of the multilabel
learning algorithm of Crammer and Singer (2002).
Smola, 2002), and are analyzed in detail by Cram-
mer (2005) for online multilabel classification.
For segmentation, the number of possible labels
(segments) is exponential on the length of the text.
We make the problem tractable by including only the
margin constraints between correct segments and at
most k highest scoring incorrect segments. Figure 4
sketches an online learning algorithm for multilabel
classification based on the work of Crammer (2005).
In the algorithm, w(i+1) is the projection of w(i) onto
the set of weight vectors such that the scores of cor-
rect segments are separated by a margin of at least
1 from the scores of incorrect segments among the
k top-scoring segments. This update is conservative
in that there is no weight change if the constraint set
is already satisfied or empty; if some constraints are
not satisfied, we make the smallest weight change
that satisfies the constraints. Since, the objective is
quadratic in w and the constraints are linear, the op-
timization problem can be solved by Hildreth?s al-
989
a) Today , Bill and Hilary Clinton traveled to Canada .
O O I O O I O O O O
O O O O I I O O O O
b) ... purified bovine P450 11 beta / 18 / 19 - hydroxylase was ...
O O I I I O O O O I I O
O O I O O O I O O I I O
O O I O O O O O I I I O
Figure 3: Correct segments for two examples.
gorithm (Censor and Zenios, 1997).
Using standard arguments for linear classifiers
(add constant feature, rescale weights) and the fact
that all the correct scores in line 4 of Figure 4 are re-
quired to be above all the incorrect scores in the top
k, that line can be replaced by
w(i+1) = arg minw
?
?w ? w(i)
?
?
2
s.t. s(xt,y; w) ? 1 and s(xt,y?; w) ? ?1
?y ? act(xt),?y? ? bestk(xt; w(i)) ? act(xt)
If v is the number of correct segments for x,
this transformation replaces O(kv) constraints with
O(k + v) constraints: segment scores are compared
to a single positive or negative threshold rather then
to each other. At test time, we find the segments
with positive score by finding the k highest scoring
segments and discarding those with a negative score.
3.2 Inference
During learning and at test time we require a method
for finding the k highest scoring segments. At test
time, we predict as correct all the segments with pos-
itive score in the top k. In this section we give an
algorithm that calculates this precisely.
For inference, tagging models typically use the
Viterbi algorithm (Rabiner, 1989). The algorithm is
given by the following standard recurrences:
S[i, t] = maxt? s(t?, t, i) + S[i ? 1, t?]
B[i, t] = arg maxt? s(t?, t, i) + S[i ? 1, t?]
with appropriate initial conditions, where s(t?, t, i)
is the score for going from tag t? at i ? 1 to tag t
at i. The dynamic programming table S[i, t] stores
the score of the best tag sequence ending at posi-
tion i with tag t, and B[i, t] is a back-pointer to the
previous tag in the best sequence ending at i with
t, which allows us to reconstruct the best sequence.
The Viterbi algorithm has easy k-best extensions.
We could find the k highest scoring segments us-
ing Viterbi. However, for the case of non-contiguous
segments, we would like to represent higher-order
dependencies that are difficult to model in Viterbi. In
particular, in Figure 3b we definitely want a feature
bridging the gap between Bill and Clinton, which
could not be captured with a standard first-order
model. But moving to higher-order models would
require adding dimensions to the dynamic program-
ming tables S and B, with corresponding multipliers
to the complexity of inference.
To represent dependencies between non-
contiguous text positions, for any given segment
y = y1 ? ? ? yn, let i(y) = 0i1 ? ? ? im(n + 1) be the
increasing sequence of indices ij such that yij = I,
padded for convenience with the dummy first index
0 and last index n + 1. Also for convenience, set
x0 = -s- and xn+1 = -e- for fixed start and
end markers. Then, we restrict ourselves to feature
functions f(x,y) that factor relative to the input as
f(x,y) =
|i(y)|
?
j=1
g(i(y)j?1, i(y)j) (1)
where i(y)j is the jth integer in i(y) and g is a fea-
ture function depending on arbitrary properties of
the input relative to the indices i(y)j?1 and i(y)j .
Applying (1) to the segment Bill Clinton in Fig-
ure 3, its score would be
w ? [g(0, 3) + g(3, 6) + g(6, 11)]
This feature representation allows us to include de-
pendencies between non-contiguous segment posi-
tions, as well as dependencies on any properties of
the input, including properties of skipped positions.
We now define the following dynamic program
S[i] = maxj<i S[j] + w ? g(j, i)
B[i] = arg maxj<i S[j] + w ? g(j, i)
990
These recurrences compute the score S[i] of the best
partial segment ending at i as the sum of the max-
imum score of a partial segment ending at position
j < i, and the score of skipping from j to i. The
back-pointer table B allows us to reconstruct the se-
quence of positions included in the segment.
Clearly, this program requires O(n2) time for a
text of length n. Furthermore we can easily augment
this algorithm in the standard fashion to find the k
best segments, and multiple segment types, result-
ing in a runtime of O(n2kT ), where T is the number
of types. O(n2kT ) is not ideal, but is still practical
since in this work we are segmenting sentences. If
we can bound the largest gap in any non-contiguous
segment by a constant g  n, then the runtime can
be improved to O(ngkT ). This runtime does not
compare favorably to the standard Viterbi algorithm
that runs in O(nT 2), especially for large k. How-
ever, we found that for even large k we could still
train large models in a matter of hours and test on
unseen data in a few minutes.
3.2.1 Restrictions
Often a segmentation task or data set will restrict
particular kinds of segments. For instance, it may be
the case that a data set does not have any overlap-
ping or non-contiguous segments. Embedded seg-
mentations ? those in which one segment?s tokens
are a subset of another?s ? is also a phenomenon that
sometimes does not occur.
It is easy to restrict the inference algorithm to dis-
allow such segments if they are unnecessary. For ex-
ample, if two segments overlap or are embedded, the
inference algorithm can just return the highest scor-
ing one. Or it can simply ignore all non-contiguous
segments if it is known that they do not occur in the
data. In Section 4 we will augment the inference
algorithm accordingly for each data set.
3.3 Feature Representation
We now discuss the design of the feature function
for two consecutive segment positions g(j, i), where
j < i. We build individual binary-valued features
from predicates over the input, for instance, the iden-
tities of words in the sentence at particular posi-
tions relative to i and j. The selection of predicates
varies by task, and we provide specific predicate sets
in Section 4 for various data sets. In this section,
we use for illustration word-pair identity predicates
such as xj = Bill & xi = Clinton.
For sequential tagging models, predicates are
combined with the set of states (or tags) to create
a feature representation. For our model, we define
the following possible states:
start ? j = 0
end ? i = n + 1
next ? j = i ? 1
skip ? j < i ? 1
For example, the following features would be on for
g(0, 3)1 and g(3, 6), respectively, in Figure 3a:
xj = -s- & xi = Bill & start
xj = Bill & xi = Clinton & skip
These features indicate a predicate?s role in the seg-
ment: at the beginning, at the end, over contiguous
segment words or skipping over some words. All
features can be augmented to indicate specific seg-
ment types for multi-type segmentation tasks. No
matter what the task, we always add predicates that
represent ranges of the distance i?j, as well as what
words or part-of-speech tags occur between the two
words. For instance, g(3, 6) might contain
word-in-between= and & skip
These features are designed to identify common
characteristics of non-contiguous segments such
as the presence of conjunctions or punctuation in
skipped portions. Although we have considered only
binary features here, the model in principle allows
arbitrary real-valued feature.
3.4 Summary
We presented a method for text segmentation that
equates the problem to structured multilabel classi-
fication where each label corresponds to a segment.
We showed that learning and inference can be man-
aged tractably in the formulation by efficiently find-
ing the k highest scoring segments through a dy-
namic programming algorithm that factors the struc-
ture of each segment. The only concern is that k
must be large enough to include all correct segments,
1Note that ?skip? is not on for g(0, 3) even though j < i?1.
Start and end states override other states.
991
which we will discuss further in Section 4. This
method naturally models all possible segmentations
including those with overlapping or non-contiguous
segments. Out approach can be seen as multilabel
variant of the work of McDonald et al (2004), which
creates a set of constraints to separate the score of
the single correct output from the k highest scoring
outputs with an appropriate large margin.
4 Experiments
We now describe a set of experiments on named en-
tity and base NP segmentation. For these experi-
ments, we set k = n, where n is the length of the
sentence. This represents a reasonable upper bound
on the number of entities or chunks in a sentence and
results in a time complexity of O(n3T ).
We compare our methods with both the averaged
perceptron (Collins, 2002) and conditional random
fields (Lafferty et al, 2001) using identical predicate
sets. Though all systems use identical predicates, the
actual features of the systems are different due to
the fundamental differences between the multilabel
classification and sequential tagging models.
4.1 Standard data sets
Our first experiments are standard named entity and
base NP data sets with no overlapping, embedded or
non-contiguous segments. These experiments will
show that, for simple segmentations, our model is
competitive with sequential tagging models.
For the named entity experiments we used the
CoNLL 2003 (Tjong Kim Sang and De Meulder,
2003) data with people, organizations, locations and
miscellaneous entities. We used standard predicates
based on word, POS and orthographic information
over a previous to next word window. For the NP
chunking experiments we used the standard CoNLL
2000 data set (Kudo and Matsumoto, 2001; Sha and
Pereira, 2003) using the predicate set defined by Sha
and Pereira (2003).
The first three rows of Table 1 compare the mul-
tilabel classification approach to standard sequen-
tial classifiers. As one might expect, the perfor-
mance of the multilabel classification method is be-
low that of the sequential tagging methods. This is
because those methods model contiguous segments
well without the need for thresholds or k-best infer-
ence. In addition, the multilabel method shows sig-
nificantly higher precision then recall. One possible
reason for this is that during the course of learning,
the model will see many segments that are nearly
correct, e.g., segments that overlap correct segments
and differ by a single token. As a result, the model
learns to score all segments containing even a small
amount of negative evidence as invalid in order to
ensure that these nearly correct segments have a suf-
ficiently low score.
One way to alleviate this problem is to restrict the
inference algorithm to not return any overlapping,
non-contiguous or embedded segmentations as dis-
cussed in Section 3.2.1, since this data set does not
contain segments of this kind. This way, the learning
stage only updates the parameters when a nearly cor-
rect segment actually out scores the correct one. The
results of this system are shown in row 4 of Table 1.
We can see that this change did lead to a more bal-
anced precision/recall, however it is clear that more
investigation is required.
4.2 Chemical substance extraction
The second set of experiments involves extract-
ing chemical substance names from MEDLINE ab-
stracts that relevant to the inhibition of the enzyme
CYP450 (PennBioIE, 2005). We focus on abstracts
that have at least one overlapping or non-contiguous
annotation. This data set contains 6164 annotated
chemical substances, including 6% that are both
overlapping and non-contiguous. Figure 3b is an
example from the corpus. We use identical predi-
cates to the named entity experiments in Section 4.1.
Though the data does contain overlapping and non-
contiguous segments, it does not contain embedded
segments. Results are shown in Table 2 using 10-
fold cross validation. The sequential tagging models
were trained using only sentences with no overlap-
ping or non-contiguous entities. We found this pro-
vided the best performance. Row 4 of Table 2 shows
the multilabel approach with the inference algorithm
restricted to not allow embedded segments.
We can see that our method does significantly bet-
ter on this data set (up to a 26% reduction in er-
ror). It is also apparent that the model is picking up
some overlapping and non-contiguous entities (see
Table 2). However, the models performance on these
kinds of entities is lower than overall performance.
992
a. Named-Entity Extraction b. NP-chunking
Precision Recall F-measure Precision Recall F-measure
Avg. Perceptron 82.46 83.14 82.80 94.22 93.88 94.05
CRFs 83.36 83.57 83.47 94.57 94.00 94.29
Multilabel 92.47 74.19 82.33 94.65 92.28 93.45
Multilabel with Restrictions 91.08 76.68 83.26 94.10 93.70 93.90
Table 1: Results for named-entity extraction and NP-chunking on data sets with only non-overlapping and
contiguous segments annotated.
Chem Substance Extraction - A Chem Substance Extraction - B
Precision Recall F-measure Precision Recall F-measure
Avg. Perceptron 82.98 79.40 81.15 1.0 0.0 0.0
CRFs 85.85 79.06 82.31 1.0 0.0 0.0
Multilabel 88.24 80.84 84.38 62.56 33.67 43.78
Multilabel with Restrictions 88.55 84.59 86.53 72.58 45.92 56.25
Table 2: Results for chemical substance extraction. Table A is for all entities in the data set and Table B is
only for those entities that are overlapping and non-contiguous.
4.3 Tuning Precision and Recall
The learning algorithm in Section 3.1 seeks a sep-
arator through the origin, though, our experimental
results suggest that this tends to favor precision at
the expense of recall. However, at test time we can
use a separation threshold different from zero. This
parameter allows us to trade off precision against re-
call, and could be tuned on held-out data.
Figure 5 plots precision, recall and f-measure
against the threshold for the basic multilabel model
on the chemical substance, NP chunking and person
entity extraction data sets. These plots clearly show
what is expected: higher thresholds give higher pre-
cision, and lower thresholds give higher recall. In
these data sets at least, a zero threshold is almost
always near optimal, though sometimes we would
benefit from a slightly lower threshold.
5 Discussion
We have presented a method for text segmentation
that is base on discriminatively learning structured
multilabel classifications. The benefits include
? Competitive performance with sequential tag-
ging models.
? Flexible modeling of complex segmentations,
including overlapping, embedded and non-
contiguous segments.
? Adjustable precision-recall trade off.
However, there is a computation cost for our models.
For a text of length n, training and testing require
O(n3T ) time, where T is the number of segment
types. Fortunately, this still results in training times
on the order of hours.
Our approach is related to the work of Bockhorst
and Craven (2004). In this work, a conditional ran-
dom field model is trained to allow for overlapping
segments with an O(n2) inference algorithm. The
model is applied to biological sequence modeling
with promising results. However, our approaches
differ in two major respects. First, their model is
probabilistic, and trained to maximize segmenta-
tion likelihood, while our model is trained to max-
imize margin. Second, our method allows for non-
contiguous segments, at the cost of a slower O(n3)
inference algorithm.
In further work, the classification threshold
should also be learned to achieve the desired balance
between precision and recall. It would also be useful
to investigate methods for combining these models
with standard sequential tagging models to get top
performance on simple segmentations as well as on
overlapping or non-contiguous ones.
A broader area of investigation are other problems
in language processing that can benefit from struc-
tured multilabel classification, e.g., ambiguities in
language often result in multiple acceptable parses
for sentences. It may be possible to extend the al-
gorithms presented here to learn to distinguish all
acceptable parses from unacceptable ones instead of
just finding a single parse when many are valid.
993
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.4
0.5
0.6
0.7
0.8
0.9
1
CHEM
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.7
0.75
0.8
0.85
0.9
0.95
1
NP
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.4
0.5
0.6
0.7
0.8
0.9
1
PER
Figure 5: Precision (squares), Recall (circles) and F-measure (line) plotted against threshold values. CHEM:
chemical substance extraction, NP: noun-phrase chunking, and PER: person name extraction.
Acknowledgments
We thank the members of the Penn BioIE project
for the development of the CYP450 corpus that we
used for our experiments. In particular, Seth Kulick
answered many questions about the data. This work
has been supported by the NSF ITR grant 0205448.
References
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning Journal Special Issue on Natural Language
Learning, 34(1/3):221?231.
J. Bockhorst and M. Craven. 2004. Markov networks for
detecting overlapping elements in sequence data. In
Proc. NIPS.
Y. Censor and S.A. Zenios. 1997. Parallel optimization :
theory, algorithms, and applications. Oxford Univer-
sity Press.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2002. A new family of online
algorithms for category ranking. In Proc SIGIR.
K. Crammer. 2005. Online Learning for Complex Cat-
egorial Problems. Ph.D. thesis, Hebrew University of
Jerusalem. to appear.
N. Cristianini and J. Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
M. Dickinson and W.D. Meurers. 2005. Detecting errors
in discontinuous structural annotation. In Proc. ACL.
A. Elisseeff and J. Weston. 2001. A kernel method for
multi-labeled classification. In Proc. NIPS.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In Proc. NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proceedings of ICML.
R. McDonald, K. Crammer, and F. Pereira. 2004. Large
margin online learning algorithms for scalable struc-
tured classication. In NIPS Workshop on Structured
Outputs.
PennBioIE. 2005. Mining The Bibliome Project.
http://bioie.ldc.upenn.edu/.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?285, February.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP.
R. E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):1?40.
B. Scho?lkopf and A. J. Smola. 2002. Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization and Beyond. MIT Press.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. HLT-NAACL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003.
http://www.cnts.ua.ac.be/conll2003/ner.
994
Proceedings of the 43rd Annual Meeting of the ACL, pages 91?98,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Online Large-Margin Training of Dependency Parsers
Ryan McDonald Koby Crammer Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA
{ryantm,crammer,pereira}@cis.upenn.edu
Abstract
We present an effective training al-
gorithm for linearly-scored dependency
parsers that implements online large-
margin multi-class training (Crammer and
Singer, 2003; Crammer et al, 2003) on
top of efficient parsing techniques for de-
pendency trees (Eisner, 1996). The trained
parsers achieve a competitive dependency
accuracy for both English and Czech with
no language specific enhancements.
1 Introduction
Research on training parsers from annotated data
has for the most part focused on models and train-
ing algorithms for phrase structure parsing. The
best phrase-structure parsing models represent gen-
eratively the joint probability P (x,y) of sentence
x having the structure y (Collins, 1999; Charniak,
2000). Generative parsing models are very conve-
nient because training consists of computing proba-
bility estimates from counts of parsing events in the
training set. However, generative models make com-
plicated and poorly justified independence assump-
tions and estimations, so we might expect better per-
formance from discriminatively trained models, as
has been shown for other tasks like document classi-
fication (Joachims, 2002) and shallow parsing (Sha
and Pereira, 2003). Ratnaparkhi?s conditional max-
imum entropy model (Ratnaparkhi, 1999), trained
to maximize conditional likelihood P (y|x) of the
training data, performed nearly as well as generative
models of the same vintage even though it scores
parsing decisions in isolation and thus may suffer
from the label bias problem (Lafferty et al, 2001).
Discriminatively trained parsers that score entire
trees for a given sentence have only recently been
investigated (Riezler et al, 2002; Clark and Curran,
2004; Collins and Roark, 2004; Taskar et al, 2004).
The most likely reason for this is that discrimina-
tive training requires repeatedly reparsing the train-
ing corpus with the current model to determine the
parameter updates that will improve the training cri-
terion. The reparsing cost is already quite high
for simple context-free models with O(n3) parsing
complexity, but it becomes prohibitive for lexical-
ized grammars with O(n5) parsing complexity.
Dependency trees are an alternative syntactic rep-
resentation with a long history (Hudson, 1984). De-
pendency trees capture important aspects of func-
tional relationships between words and have been
shown to be useful in many applications includ-
ing relation extraction (Culotta and Sorensen, 2004),
paraphrase acquisition (Shinyama et al, 2002) and
machine translation (Ding and Palmer, 2005). Yet,
they can be parsed in O(n3) time (Eisner, 1996).
Therefore, dependency parsing is a potential ?sweet
spot? that deserves investigation. We focus here on
projective dependency trees in which a word is the
parent of all of its arguments, and dependencies are
non-crossing with respect to word order (see Fig-
ure 1). However, there are cases where crossing
dependencies may occur, as is the case for Czech
(Hajic?, 1998). Edges in a dependency tree may be
typed (for instance to indicate grammatical func-
tion). Though we focus on the simpler non-typed
91
root John hit the ball with the bat
Figure 1: An example dependency tree.
case, all algorithms are easily extendible to typed
structures.
The following work on dependency parsing is
most relevant to our research. Eisner (1996) gave
a generative model with a cubic parsing algorithm
based on an edge factorization of trees. Yamada and
Matsumoto (2003) trained support vector machines
(SVM) to make parsing decisions in a shift-reduce
dependency parser. As in Ratnaparkhi?s parser, the
classifiers are trained on individual decisions rather
than on the overall quality of the parse. Nivre and
Scholz (2004) developed a history-based learning
model. Their parser uses a hybrid bottom-up/top-
down linear-time heuristic parser and the ability to
label edges with semantic types. The accuracy of
their parser is lower than that of Yamada and Mat-
sumoto (2003).
We present a new approach to training depen-
dency parsers, based on the online large-margin
learning algorithms of Crammer and Singer (2003)
and Crammer et al (2003). Unlike the SVM
parser of Yamada and Matsumoto (2003) and Ratna-
parkhi?s parser, our parsers are trained to maximize
the accuracy of the overall tree.
Our approach is related to those of Collins and
Roark (2004) and Taskar et al (2004) for phrase
structure parsing. Collins and Roark (2004) pre-
sented a linear parsing model trained with an aver-
aged perceptron algorithm. However, to use parse
features with sufficient history, their parsing algo-
rithm must prune heuristically most of the possible
parses. Taskar et al (2004) formulate the parsing
problem in the large-margin structured classification
setting (Taskar et al, 2003), but are limited to pars-
ing sentences of 15 words or less due to computation
time. Though these approaches represent good first
steps towards discriminatively-trained parsers, they
have not yet been able to display the benefits of dis-
criminative training that have been seen in named-
entity extraction and shallow parsing.
Besides simplicity, our method is efficient and ac-
curate, as we demonstrate experimentally on English
and Czech treebank data.
2 System Description
2.1 Definitions and Background
In what follows, the generic sentence is denoted by
x (possibly subscripted); the ith word of x is de-
noted by xi. The generic dependency tree is denoted
by y. If y is a dependency tree for sentence x, we
write (i, j) ? y to indicate that there is a directed
edge from word xi to word xj in the tree, that is, xi
is the parent of xj . T = {(xt,yt)}Tt=1 denotes the
training data.
We follow the edge based factorization method of
Eisner (1996) and define the score of a dependency
tree as the sum of the score of all edges in the tree,
s(x,y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? f(i, j)
where f(i, j) is a high-dimensional binary feature
representation of the edge from xi to xj . For exam-
ple, in the dependency tree of Figure 1, the following
feature would have a value of 1:
f(i, j) =
{
1 if xi=?hit? and xj=?ball?
0 otherwise.
In general, any real-valued feature may be used, but
we use binary features for simplicity. The feature
weights in the weight vector w are the parameters
that will be learned during training. Our training al-
gorithms are iterative. We denote by w(i) the weight
vector after the ith training iteration.
Finally we define dt(x) as the set of possi-
ble dependency trees for the input sentence x and
bestk(x; w) as the set of k dependency trees in dt(x)
that are given the highest scores by weight vector w,
with ties resolved by an arbitrary but fixed rule.
Three basic questions must be answered for mod-
els of this form: how to find the dependency tree y
with highest score for sentence x; how to learn an
appropriate weight vector w from the training data;
and finally, what feature representation f(i, j) should
be used. The following sections address each of
these questions.
2.2 Parsing Algorithm
Given a feature representation for edges and a
weight vector w, we seek the dependency tree or
92
h1 h1 h2 h2
?
s h1 h1 r r+1 h2 h2 t
h1
h1 h2 h2
?
s h1 h1 h2 h2 t
h1
h1
s h1 h1 t
Figure 2: O(n3) algorithm of Eisner (1996), needs to keep 3 indices at any given stage.
trees that maximize the score function, s(x,y). The
primary difficulty is that for a given sentence of
length n there are exponentially many possible de-
pendency trees. Using a slightly modified version of
a lexicalized CKY chart parsing algorithm, it is pos-
sible to generate and represent these sentences in a
forest that is O(n5) in size and takes O(n5) time to
create.
Eisner (1996) made the observation that if the
head of each chart item is on the left or right periph-
ery, then it is possible to parse in O(n3). The idea is
to parse the left and right dependents of a word inde-
pendently and combine them at a later stage. This re-
moves the need for the additional head indices of the
O(n5) algorithm and requires only two additional
binary variables that specify the direction of the item
(either gathering left dependents or gathering right
dependents) and whether an item is complete (avail-
able to gather more dependents). Figure 2 shows
the algorithm schematically. As with normal CKY
parsing, larger elements are created bottom-up from
pairs of smaller elements.
Eisner showed that his algorithm is sufficient for
both searching the space of dependency parses and,
with slight modification, finding the highest scoring
tree y for a given sentence x under the edge fac-
torization assumption. Eisner and Satta (1999) give
a cubic algorithm for lexicalized phrase structures.
However, it only works for a limited class of lan-
guages in which tree spines are regular. Further-
more, there is a large grammar constant, which is
typically in the thousands for treebank parsers.
2.3 Online Learning
Figure 3 gives pseudo-code for the generic online
learning setting. A single training instance is con-
sidered on each iteration, and parameters updated
by applying an algorithm-specific update rule to the
instance under consideration. The algorithm in Fig-
ure 3 returns an averaged weight vector: an auxil-
iary weight vector v is maintained that accumulates
Training data: T = {(xt, yt)}Tt=1
1. w0 = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. w(i+1) = update w(i) according to instance (xt, yt)
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 3: Generic online learning algorithm.
the values of w after each iteration, and the returned
weight vector is the average of all the weight vec-
tors throughout training. Averaging has been shown
to help reduce overfitting (Collins, 2002).
2.3.1 MIRA
Crammer and Singer (2001) developed a natural
method for large-margin multi-class classification,
which was later extended by Taskar et al (2003) to
structured classification:
min ?w?
s.t. s(x,y) ? s(x,y?) ? L(y,y?)
?(x,y) ? T , y? ? dt(x)
where L(y,y?) is a real-valued loss for the tree y?
relative to the correct tree y. We define the loss of
a dependency tree as the number of words that have
the incorrect parent. Thus, the largest loss a depen-
dency tree can have is the length of the sentence.
Informally, this update looks to create a margin
between the correct dependency tree and each incor-
rect dependency tree at least as large as the loss of
the incorrect tree. The more errors a tree has, the
farther away its score will be from the score of the
correct tree. In order to avoid a blow-up in the norm
of the weight vector we minimize it subject to con-
straints that enforce the desired margin between the
correct and incorrect trees1.
1The constraints may be unsatisfiable, in which case we can
relax them with slack variables as in SVM training.
93
The Margin Infused Relaxed Algorithm
(MIRA) (Crammer and Singer, 2003; Cram-
mer et al, 2003) employs this optimization directly
within the online framework. On each update,
MIRA attempts to keep the norm of the change to
the parameter vector as small as possible, subject to
correctly classifying the instance under considera-
tion with a margin at least as large as the loss of the
incorrect classifications. This can be formalized by
substituting the following update into line 4 of the
generic online algorithm,
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
?y? ? dt(xt)
(1)
This is a standard quadratic programming prob-
lem that can be easily solved using Hildreth?s al-
gorithm (Censor and Zenios, 1997). Crammer and
Singer (2003) and Crammer et al (2003) provide
an analysis of both the online generalization error
and convergence properties of MIRA. In equation
(1), s(x,y) is calculated with respect to the weight
vector after optimization, w(i+1).
To apply MIRA to dependency parsing, we can
simply see parsing as a multi-class classification
problem in which each dependency tree is one of
many possible classes for a sentence. However, that
interpretation fails computationally because a gen-
eral sentence has exponentially many possible de-
pendency trees and thus exponentially many margin
constraints.
To circumvent this problem we make the assump-
tion that the constraints that matter for large margin
optimization are those involving the incorrect trees
y? with the highest scores s(x,y?). The resulting
optimization made by MIRA (see Figure 3, line 4)
would then be:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
?y? ? bestk(xt; w(i))
reducing the number of constraints to the constant k.
We tested various values of k on a development data
set and found that small values of k are sufficient to
achieve close to best performance, justifying our as-
sumption. In fact, as k grew we began to observe a
slight degradation of performance, indicating some
overfitting to the training data. All the experiments
presented here use k = 5. The Eisner (1996) algo-
rithm can be modified to find the k-best trees while
only adding an additional O(k log k) factor to the
runtime (Huang and Chiang, 2005).
A more common approach is to factor the struc-
ture of the output space to yield a polynomial set of
local constraints (Taskar et al, 2003; Taskar et al,
2004). One such factorization for dependency trees
is
min
?
?w(i+1) ? w(i)
?
?
s.t. s(l, j) ? s(k, j) ? 1
?(l, j) ? yt, (k, j) /? yt
It is trivial to show that if these O(n2) constraints
are satisfied, then so are those in (1). We imple-
mented this model, but found that the required train-
ing time was much larger than the k-best formu-
lation and typically did not improve performance.
Furthermore, the k-best formulation is more flexi-
ble with respect to the loss function since it does not
assume the loss function can be factored into a sum
of terms for each dependency.
2.4 Feature Set
Finally, we need a suitable feature representation
f(i, j) for each dependency. The basic features in
our model are outlined in Table 1a and b. All fea-
tures are conjoined with the direction of attachment
as well as the distance between the two words being
attached. These features represent a system of back-
off from very specific features over words and part-
of-speech tags to less sparse features over just part-
of-speech tags. These features are added for both the
entire words as well as the 5-gram prefix if the word
is longer than 5 characters.
Using just features over the parent-child node
pairs in the tree was not enough for high accuracy,
because all attachment decisions were made outside
of the context in which the words occurred. To solve
this problem, we added two other types of features,
which can be seen in Table 1c. Features of the first
type look at words that occur between a child and
its parent. These features take the form of a POS
trigram: the POS of the parent, of the child, and of
a word in between, for all words linearly between
the parent and the child. This feature was particu-
larly helpful for nouns identifying their parent, since
94
a)
Basic Uni-gram Features
p-word, p-pos
p-word
p-pos
c-word, c-pos
c-word
c-pos
b)
Basic Big-ram Features
p-word, p-pos, c-word, c-pos
p-pos, c-word, c-pos
p-word, c-word, c-pos
p-word, p-pos, c-pos
p-word, p-pos, c-word
p-word, c-word
p-pos, c-pos
c)
In Between POS Features
p-pos, b-pos, c-pos
Surrounding Word POS Features
p-pos, p-pos+1, c-pos-1, c-pos
p-pos-1, p-pos, c-pos-1, c-pos
p-pos, p-pos+1, c-pos, c-pos+1
p-pos-1, p-pos, c-pos, c-pos+1
Table 1: Features used by system. p-word: word of parent node in dependency tree. c-word: word of child
node. p-pos: POS of parent node. c-pos: POS of child node. p-pos+1: POS to the right of parent in sentence.
p-pos-1: POS to the left of parent. c-pos+1: POS to the right of child. c-pos-1: POS to the left of child.
b-pos: POS of a word in between parent and child nodes.
it would typically rule out situations when a noun
attached to another noun with a verb in between,
which is a very uncommon phenomenon.
The second type of feature provides the local con-
text of the attachment, that is, the words before and
after the parent-child pair. This feature took the form
of a POS 4-gram: The POS of the parent, child,
word before/after parent and word before/after child.
The system also used back-off features to various tri-
grams where one of the local context POS tags was
removed. Adding these two features resulted in a
large improvement in performance and brought the
system to state-of-the-art accuracy.
2.5 System Summary
Besides performance (see Section 3), the approach
to dependency parsing we described has several
other advantages. The system is very general and
contains no language specific enhancements. In fact,
the results we report for English and Czech use iden-
tical features, though are obviously trained on differ-
ent data. The online learning algorithms themselves
are intuitive and easy to implement.
The efficient O(n3) parsing algorithm of Eisner
allows the system to search the entire space of de-
pendency trees while parsing thousands of sentences
in a few minutes, which is crucial for discriminative
training. We compare the speed of our model to a
standard lexicalized phrase structure parser in Sec-
tion 3.1 and show a significant improvement in pars-
ing times on the testing data.
The major limiting factor of the system is its re-
striction to features over single dependency attach-
ments. Often, when determining the next depen-
dent for a word, it would be useful to know previ-
ous attachment decisions and incorporate these into
the features. It is fairly straightforward to modify
the parsing algorithm to store previous attachments.
However, any modification would result in an as-
ymptotic increase in parsing complexity.
3 Experiments
We tested our methods experimentally on the Eng-
lish Penn Treebank (Marcus et al, 1993) and on the
Czech Prague Dependency Treebank (Hajic?, 1998).
All experiments were run on a dual 64-bit AMD
Opteron 2.4GHz processor.
To create dependency structures from the Penn
Treebank, we used the extraction rules of Yamada
and Matsumoto (2003), which are an approximation
to the lexicalization rules of Collins (1999). We split
the data into three parts: sections 02-21 for train-
ing, section 22 for development and section 23 for
evaluation. Currently the system has 6, 998, 447 fea-
tures. Each instance only uses a tiny fraction of these
features making sparse vector calculations possible.
Our system assumes POS tags as input and uses the
tagger of Ratnaparkhi (1996) to provide tags for the
development and evaluation sets.
Table 2 shows the performance of the systems
that were compared. Y&M2003 is the SVM-shift-
reduce parsing model of Yamada and Matsumoto
(2003), N&S2004 is the memory-based learner of
Nivre and Scholz (2004) and MIRA is the the sys-
tem we have described. We also implemented an av-
eraged perceptron system (Collins, 2002) (another
online learning algorithm) for comparison. This ta-
ble compares only pure dependency parsers that do
95
English Czech
Accuracy Root Complete Accuracy Root Complete
Y&M2003 90.3 91.6 38.4 - - -
N&S2004 87.3 84.3 30.4 - - -
Avg. Perceptron 90.6 94.0 36.5 82.9 88.0 30.3
MIRA 90.9 94.2 37.5 83.3 88.6 31.3
Table 2: Dependency parsing results for English and Czech. Accuracy is the number of words that correctly
identified their parent in the tree. Root is the number of trees in which the root word was correctly identified.
For Czech this is f-measure since a sentence may have multiple roots. Complete is the number of sentences
for which the entire dependency tree was correct.
not exploit phrase structure. We ensured that the
gold standard dependencies of all systems compared
were identical.
Table 2 shows that the model described here per-
forms as well or better than previous comparable
systems, including that of Yamada and Matsumoto
(2003). Their method has the potential advantage
that SVM batch training takes into account all of
the constraints from all training instances in the op-
timization, whereas online training only considers
constraints from one instance at a time. However,
they are fundamentally limited by their approximate
search algorithm. In contrast, our system searches
the entire space of dependency trees and most likely
benefits greatly from this. This difference is am-
plified when looking at the percentage of trees that
correctly identify the root word. The models that
search the entire space will not suffer from bad ap-
proximations made early in the search and thus are
more likely to identify the correct root, whereas the
approximate algorithms are prone to error propaga-
tion, which culminates with attachment decisions at
the top of the tree. When comparing the two online
learning models, it can be seen that MIRA outper-
forms the averaged perceptron method. This differ-
ence is statistically significant, p < 0.005 (McNe-
mar test on head selection accuracy).
In our Czech experiments, we used the depen-
dency trees annotated in the Prague Treebank, and
the predefined training, development and evaluation
sections of this data. The number of sentences in
this data set is nearly twice that of the English tree-
bank, leading to a very large number of features ?
13, 450, 672. But again, each instance uses just a
handful of these features. For POS tags we used the
automatically generated tags in the data set. Though
we made no language specific model changes, we
did need to make some data specific changes. In par-
ticular, we used the method of Collins et al (1999) to
simplify part-of-speech tags since the rich tags used
by Czech would have led to a large but rarely seen
set of POS features.
The model based on MIRA also performs well on
Czech, again slightly outperforming averaged per-
ceptron. Unfortunately, we do not know of any other
parsing systems tested on the same data set. The
Czech parser of Collins et al (1999) was run on a
different data set and most other dependency parsers
are evaluated using English. Learning a model from
the Czech training data is somewhat problematic
since it contains some crossing dependencies which
cannot be parsed by the Eisner algorithm. One trick
is to rearrange the words in the training set so that
all trees are nested. This at least allows the train-
ing algorithm to obtain reasonably low error on the
training set. We found that this did improve perfor-
mance slightly to 83.6% accuracy.
3.1 Lexicalized Phrase Structure Parsers
It is well known that dependency trees extracted
from lexicalized phrase structure parsers (Collins,
1999; Charniak, 2000) typically are more accurate
than those produced by pure dependency parsers
(Yamada and Matsumoto, 2003). We compared
our system to the Bikel re-implementation of the
Collins parser (Bikel, 2004; Collins, 1999) trained
with the same head rules of our system. There are
two ways to extract dependencies from lexicalized
phrase structure. The first is to use the automatically
generated dependencies that are explicit in the lex-
icalization of the trees, we call this system Collins-
auto. The second is to take just the phrase structure
output of the parser and run the automatic head rules
over it to extract the dependencies, we call this sys-
96
English
Accuracy Root Complete Complexity Time
Collins-auto 88.2 92.3 36.1 O(n5) 98m 21s
Collins-rules 91.4 95.1 42.6 O(n5) 98m 21s
MIRA-Normal 90.9 94.2 37.5 O(n3) 5m 52s
MIRA-Collins 92.2 95.8 42.9 O(n5) 105m 08s
Table 3: Results comparing our system to those based on the Collins parser. Complexity represents the
computational complexity of each parser and Time the CPU time to parse sec. 23 of the Penn Treebank.
tem Collins-rules. Table 3 shows the results compar-
ing our system, MIRA-Normal, to the Collins parser
for English. All systems are implemented in Java
and run on the same machine.
Interestingly, the dependencies that are automati-
cally produced by the Collins parser are worse than
those extracted statically using the head rules. Ar-
guably, this displays the artificialness of English de-
pendency parsing using dependencies automatically
extracted from treebank phrase-structure trees. Our
system falls in-between, better than the automati-
cally generated dependency trees and worse than the
head-rule extracted trees.
Since the dependencies returned from our system
are better than those actually learnt by the Collins
parser, one could argue that our model is actu-
ally learning to parse dependencies more accurately.
However, phrase structure parsers are built to max-
imize the accuracy of the phrase structure and use
lexicalization as just an additional source of infor-
mation. Thus it is not too surprising that the de-
pendencies output by the Collins parser are not as
accurate as our system, which is trained and built to
maximize accuracy on dependency trees. In com-
plexity and run-time, our system is a huge improve-
ment over the Collins parser.
The final system in Table 3 takes the output of
Collins-rules and adds a feature to MIRA-Normal
that indicates for given edge, whether the Collins
parser believed this dependency actually exists, we
call this system MIRA-Collins. This is a well known
discriminative training trick ? using the sugges-
tions of a generative system to influence decisions.
This system can essentially be considered a correc-
tor of the Collins parser and represents a significant
improvement over it. However, there is an added
complexity with such a model as it requires the out-
put of the O(n5) Collins parser.
k=1 k=2 k=5 k=10 k=20
Accuracy 90.73 90.82 90.88 90.92 90.91
Train Time 183m 235m 627m 1372m 2491m
Table 4: Evaluation of k-best MIRA approximation.
3.2 k-best MIRA Approximation
One question that can be asked is how justifiable is
the k-best MIRA approximation. Table 4 indicates
the accuracy on testing and the time it took to train
models with k = 1, 2, 5, 10, 20 for the English data
set. Even though the parsing algorithm is propor-
tional to O(k log k), empirically, the training times
scale linearly with k. Peak performance is achieved
very early with a slight degradation around k=20.
The most likely reason for this phenomenon is that
the model is overfitting by ensuring that even un-
likely trees are separated from the correct tree pro-
portional to their loss.
4 Summary
We described a successful new method for training
dependency parsers. We use simple linear parsing
models trained with margin-sensitive online training
algorithms, achieving state-of-the-art performance
with relatively modest training times and no need
for pruning heuristics. We evaluated the system on
both English and Czech data to display state-of-the-
art performance without any language specific en-
hancements. Furthermore, the model can be aug-
mented to include features over lexicalized phrase
structure parsing decisions to increase dependency
accuracy over those parsers.
We plan on extending our parser in two ways.
First, we would add labels to dependencies to rep-
resent grammatical roles. Those labels are very im-
portant for using parser output in tasks like infor-
mation extraction or machine translation. Second,
97
we are looking at model extensions to allow non-
projective dependencies, which occur in languages
such as Czech, German and Dutch.
Acknowledgments: We thank Jan Hajic? for an-
swering queries on the Prague treebank, and Joakim
Nivre for providing the Yamada and Matsumoto
(2003) head rules for English that allowed for a di-
rect comparison with our systems. This work was
supported by NSF ITR grants 0205456, 0205448,
and 0428193.
References
D.M. Bikel. 2004. Intricacies of Collins parsing model.
Computational Linguistics.
Y. Censor and S.A. Zenios. 1997. Parallel optimization :
theory, algorithms, and applications. Oxford Univer-
sity Press.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. NAACL.
S. Clark and J.R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In Proc. ACL.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. ACL.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2001. On the algorithmic
implementation of multiclass kernel based vector ma-
chines. JMLR.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Proc.
NIPS.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proc. ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In Proc. ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
J. Hajic?. 1998. Building a syntactically annotated cor-
pus: The Prague dependency treebank. Issues of Va-
lency and Meaning.
L. Huang and D. Chiang. 2005. Better k-best parsing.
Technical Report MS-CIS-05-08, University of Penn-
sylvania.
Richard Hudson. 1984. Word Grammar. Blackwell.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: the penn
treebank. Computational Linguistics.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning.
S. Riezler, T. King, R. Kaplan, R. Crouch, J. Maxwell,
and M. Johnson. 2002. Parsing the Wall Street Journal
using a lexical-functional grammar and discriminative
estimation techniques. In Proc. ACL.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. HLT-NAACL.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proc. HLT.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
98
Proceedings of the 43rd Annual Meeting of the ACL, pages 491?498,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Simple Algorithms for Complex Relation Extraction
with Applications to Biomedical IE
Ryan McDonald1 Fernando Pereira1 Seth Kulick2
1CIS and 2IRCS, University of Pennsylvania, Philadelphia, PA
{ryantm,pereira}@cis.upenn.edu, skulick@linc.cis.upenn.edu
Scott Winters Yang Jin Pete White
Division of Oncology, Children?s Hospital of Pennsylvania, Philadelphia, PA
{winters,jin,white}@genome.chop.edu
Abstract
A complex relation is any n-ary relation
in which some of the arguments may be
be unspecified. We present here a simple
two-stage method for extracting complex
relations between named entities in text.
The first stage creates a graph from pairs
of entities that are likely to be related, and
the second stage scores maximal cliques
in that graph as potential complex relation
instances. We evaluate the new method
against a standard baseline for extracting
genomic variation relations from biomed-
ical text.
1 Introduction
Most research on text information extraction (IE)
has focused on accurate tagging of named entities.
Successful early named-entity taggers were based
on finite-state generative models (Bikel et al, 1999).
More recently, discriminatively-trained models have
been shown to be more accurate than generative
models (McCallum et al, 2000; Lafferty et al, 2001;
Kudo and Matsumoto, 2001). Both kinds of mod-
els have been developed for tagging entities such
as people, places and organizations in news mate-
rial. However, the rapid development of bioinfor-
matics has recently generated interest on the extrac-
tion of biological entities such as genes (Collier et
al., 2000) and genomic variations (McDonald et al,
2004b) from biomedical literature.
The next logical step for IE is to begin to develop
methods for extracting meaningful relations involv-
ing named entities. Such relations would be ex-
tremely useful in applications like question answer-
ing, automatic database generation, and intelligent
document searching and indexing. Though not as
well studied as entity extraction, relation extraction
has still seen a significant amount of work. We dis-
cuss some previous approaches at greater length in
Section 2.
Most relation extraction systems focus on the spe-
cific problem of extracting binary relations, such
as the employee of relation or protein-protein in-
teraction relation. Very little work has been done
in recognizing and extracting more complex rela-
tions. We define a complex relation as any n-ary
relation among n typed entities. The relation is
defined by the schema (t1, . . . , tn) where ti ? T
are entity types. An instance (or tuple) in the rela-
tion is a list of entities (e1, . . . , en) such that either
type(ei) = ti, or ei =? indicating that the ith ele-
ment of the tuple is missing.
For example, assume that the entity types
are T = {person, job, company} and we are
interested in the ternary relation with schema
(person, job, company) that relates a person
to their job at a particular company. For
the sentence ?John Smith is the CEO at Inc.
Corp.?, the system would ideally extract the tu-
ple (John Smith, CEO, Inc. Corp.). However, for
the sentence ?Everyday John Smith goes to his
office at Inc. Corp.?, the system would extract
(John Smith,?, Inc. Corp.), since there is no men-
tion of a job title. Hence, the goal of complex re-
lation extraction is to identify all instances of the
relation of interest in some piece of text, including
491
incomplete instances.
We present here several simple methods for ex-
tracting complex relations. All the methods start by
recognized pairs of entity mentions, that is, binary
relation instances, that appear to be arguments of the
relation of interest. Those pairs can be seen as the
edges of a graph with entity mentions as nodes. The
algorithms then try to reconstruct complex relations
by making tuples from selected maximal cliques in
the graph. The methods are general and can be ap-
plied to any complex relation fitting the above def-
inition. We also assume throughout the paper that
the entities and their type are known a priori in the
text. This is a fair assumption given the current high
standard of state-of-the-art named-entity extractors.
A primary advantage of factoring complex rela-
tions into binary relations is that it allows the use of
standard classification algorithms to decide whether
particular pairs of entity mentions are related. In ad-
dition, the factoring makes training data less sparse
and reduces the computational cost of extraction.
We will discuss these benefits further in Section 4.
We evaluated the methods on a large set of anno-
tated biomedical documents to extract relations re-
lated to genomic variations, demonstrating a consid-
erable improvement over a reasonable baseline.
2 Previous work
A representative approach to relation extraction is
the system of Zelenko et al (2003), which attempts
to identify binary relations in news text. In that
system, each pair of entity mentions of the correct
types in a sentence is classified as to whether it is
a positive instance of the relation. Consider the bi-
nary relation employee of and the sentence ?John
Smith, not Jane Smith, works at IBM?. The pair
(John Smith, IBM) is a positive instance, while the
pair (Jane Smith, IBM) is a negative instance. In-
stances are represented by a pair of entities and their
position in a shallow parse tree for the containing
sentence. Classification is done by a support-vector
classifier with a specialized kernel for that shallow
parse representation.
This approach ? enumerating all possible en-
tity pairs and classifying each as positive or nega-
tive ? is the standard method in relation extraction.
The main differences among systems are the choice
of trainable classifier and the representation for in-
stances.
For binary relations, this approach is quite
tractable: if the relation schema is (t1, t2), the num-
ber of potential instances is O(|t1| |t2|), where |t| is
the number of entity mentions of type t in the text
under consideration.
One interesting system that does not belong to
the above class is that of Miller et al (2000), who
take the view that relation extraction is just a form
of probabilistic parsing where parse trees are aug-
mented to identify all relations. Once this augmen-
tation is made, any standard parser can be trained
and then run on new sentences to extract new re-
lations. Miller et al show such an approach can
yield good results. However, it can be argued that
this method will encounter problems when consid-
ering anything but binary relations. Complex re-
lations would require a large amount of tree aug-
mentation and most likely result in extremely sparse
probability estimates. Furthermore, by integrating
relation extraction with parsing, the system cannot
consider long-range dependencies due to the local
parsing constraints of current probabilistic parsers.
The higher the arity of a relation, the more likely
it is that entities will be spread out within a piece
of text, making long range dependencies especially
important.
Roth and Yih (2004) present a model in which en-
tity types and relations are classified jointly using a
set of global constraints over locally trained classi-
fiers. This joint classification is shown to improve
accuracy of both the entities and relations returned
by the system. However, the system is based on con-
straints for binary relations only.
Recently, there has also been many results from
the biomedical IE community. Rosario and Hearst
(2004) compare both generative and discriminative
models for extracting seven relationships between
treatments and diseases. Though their models are
very flexible, they assume at most one relation per
sentence, ruling out cases where entities participate
in multiple relations, which is a common occurrence
in our data. McDonald et al (2004a) use a rule-
based parser combined with a rule-based relation
identifier to extract generic binary relations between
biological entities. As in predicate-argument extrac-
tion (Gildea and Jurafsky, 2002), each relation is
492
always associated with a verb in the sentence that
specifies the relation type. Though this system is
very general, it is limited by the fact that the design
ignores relations not expressed by a verb, as the em-
ployee of relation in?John Smith, CEO of Inc. Corp.,
announced he will resign?.
Most relation extraction systems work primarily
on a sentential level and never consider relations that
cross sentences or paragraphs. Since current data
sets typically only annotate intra-sentence relations,
this has not yet proven to be a problem.
3 Definitions
3.1 Complex Relations
Recall that a complex n-ary relation is specified by
a schema (t1, . . . , tn) where ti ? T are entity types.
Instances of the relation are tuples (e1, . . . , en)
where either type(ei) = ti, or ei =? (missing ar-
gument). The only restriction this definition places
on a relation is that the arity must be known. As we
discuss it further in Section 6, this is not required by
our methods but is assumed here for simplicity. We
also assume that the system works on a single rela-
tion type at a time, although the methods described
here are easily generalizable to systems that can ex-
tract many relations at once.
3.2 Graphs and Cliques
An undirected graph G = (V,E) is specified by a
set of vertices V and a set of edges E, with each
edge an unordered pair (u, v) of vertices. G? =
(V ?, E?) is a subgraph of G if V ? ? V and E? =
{(u, v) : u, v ? V ?, (u, v) ? E}. A clique C of G is
a subgraph of G in which there is an edge between
every pair of vertices. A maximal clique of G is a
clique C = (VC , EC) such that there is no other
clique C ? = (VC? , EC?) such that VC ? VC? .
4 Methods
We describe now a simple method for extracting
complex relations. This method works by first fac-
toring all complex relations into a set of binary re-
lations. A classifier is then trained in the standard
manner to recognize all pairs of related entities. Fi-
nally a graph is constructed from the output of this
classifier and the complex relations are determined
from the cliques of this graph.
a. All possible
relation instances
(John, CEO, Inc. Corp.)
(John,?, Inc. Corp.)
(John, CEO, Biz. Corp.)
(John,?, Biz. Corp.)
(John, CEO,?)
(Jane, CEO, Inc. Corp.)
(Jane,?, Inc. Corp.)
(Jane, CEO, Biz. Corp.)
(Jane,?, Biz. Corp.)
(Jane, CEO,?)
(?, CEO, Inc. Corp.)
(?, CEO, Biz. Corp.)
b. All possible
binary relations
(John, CEO)
(John, Inc. Corp.)
(John, Biz. Corp.)
(CEO, Inc. Corp.)
(CEO, Biz. Corp.)
(Jane, CEO)
(Jane, Inc. Corp.)
(Jane, Biz. Corp.)
Figure 1: Relation factorization of the sentence:
John and Jane are CEOs at Inc. Corp. and Biz.
Corp. respectively.
4.1 Classifying Binary Relations
Consider again the motivating example of the
(person, job, company) relation and the sentence
?John and Jane are CEOs at Inc. Corp. and Biz.
Corp. respectively?. This sentence contains two
people, one job title and two companies.
One possible method for extracting the rela-
tion of interest would be to first consider all 12
possible tuples shown in Figure 1a. Using all
these tuples, it should then be possible to train
a classifier to distinguish valid instances such as
(John, CEO, Inc. Corp.) from invalid ones such as
(Jane, CEO, Inc. Corp.). This is analogous to the
approach taken by Zelenko et al (2003) for binary
relations.
There are problems with this approach. Computa-
tionally, for an n-ary relation, the number of possi-
ble instances is O(|t1| |t2| ? ? ? |tn|). Conservatively,
letting m be the smallest |ti|, the run time is O(mn),
exponential in the arity of the relation. The second
problem is how to manage incomplete but correct
instances such as (John,?, Inc. Corp.) when train-
ing the classifier. If this instance is marked as neg-
ative, then the model might incorrectly disfavor fea-
tures that correlate John to Inc. Corp.. However,
if this instance is labeled positive, then the model
may tend to prefer the shorter and more compact in-
complete relations since they will be abundant in the
positive training examples. We could always ignore
instances of this form, but then the data would be
heavily skewed towards negative instances.
493
Instead of trying to classify all possible relation
instances, in this work we first classify pairs of en-
tities as being related or not. Then, as discussed in
Section 4.2, we reconstruct the larger complex rela-
tions from a set of binary relation instances.
Factoring relations into a set of binary decisions
has several advantages. The set of possible pairs is
much smaller then the set of all possible complex
relation instances. This can be seen in Figure 1b,
which only considers pairs that are consistent with
the relation definition. More generally, the num-
ber of pairs to classify is O((?i |ti|)2) , which is
far better than the exponentially many full relation
instances. There is also no ambiguity when label-
ing pairs as positive or negative when constructing
the training data. Finally, we can rely on previous
work on classification for binary relation extraction
to identify pairs of related entities.
To train a classifier to identify pairs of related
entities, we must first create the set of all positive
and negative pairs in the data. The positive in-
stances are all pairs that occur together in a valid
tuple. For the example sentence in Figure 1, these
include the pairs (John, CEO), (John, Inc. Corp.),
(CEO, Inc. Corp.), (CEO, Biz. Corp.), (Jane, CEO)
and (Jane, Biz. Corp.). To gather negative in-
stances, we extract all pairs that never occur to-
gether in a valid relation. From the same exam-
ple these would be the pairs (John, Biz. Corp.) and
(Jane, Inc. Corp.).
This leads to a large set of positive and negative
binary relation instances. At this point we could em-
ploy any binary relation classifier and learn to iden-
tify new instances of related pairs of entities. We
use a standard maximum entropy classifier (Berger
et al, 1996) implemented as part of MALLET (Mc-
Callum, 2002). The model is trained using the fea-
tures listed in Table 1.
This is a very simple binary classification model.
No deep syntactic structure such as parse trees is
used. All features are basically over the words sepa-
rating two entities and their part-of-speech tags. Of
course, it would be possible to use more syntactic
information if available in a manner similar to that
of Zelenko et al (2003). However, the primary pur-
pose of our experiments was not to create a better
binary relation extractor, but to see if complex re-
lations could be extracted through binary factoriza-
Feature Set
entity type of e1 and e2
words in e1 and e2
word bigrams in e1 and e2
POS of e1 and e2
words between e1 and e2
word bigrams between e1 and e2
POS between e1 and e2
distance between e1 and e2
concatenations of above features
Table 1: Feature set for maximum entropy binary
relation classifier. e1 and e2 are entities.
a. Relation graph G
John Jane
CEO
Inc. Corp. Biz. Corp.
b. Tuples from G
(John, CEO,?)
(John,?, Inc. Corp.)
(John,?, Biz. Corp.)
(Jane, CEO,?)
(?, CEO, Inc. Corp.)
(?, CEO, Biz. Corp.)
(John, CEO, Inc. Corp.)
(John, CEO, Biz. Corp.)
Figure 2: Example of a relation graph and tuples
from all the cliques in the graph.
tion followed by reconstruction. In Section 5.2 we
present an empirical evaluation of the binary relation
classifier.
4.2 Reconstructing Complex Relations
4.2.1 Maximal Cliques
Having identified all pairs of related entities in the
text, the next stage is to reconstruct the complex re-
lations from these pairs. Let G = (V,E) be an undi-
rected graph where the vertices V are entity men-
tions in the text and the edges E represent binary
relations between entities. We reconstruct the com-
plex relation instances by finding maximal cliques
in the graphs.
The simplest approach is to create the graph
so that two entities in the graph have an edge
if the binary classifier believes they are related.
For example, consider the binary factoriza-
tion in Figure 1 and imagine the classifier
identified the following pairs as being related:
(John, CEO), (John, Inc. Corp.), (John, Biz. Corp.),
(CEO, Inc. Corp.), (CEO, Biz. Corp.) and
(Jane, CEO). The resulting graph can be seen
in Figure 2a.
Looking at this graph, one solution to construct-
494
ing complex relations would be to consider all the
cliques in the graph that are consistent with the def-
inition of the relation. This is equivalent to having
the system return only relations in which the binary
classifier believes that all of the entities involved are
pairwise related. All the cliques in the example are
shown in Figure 2b. We add ? fields to the tuples to
be consistent with the relation definition.
This could lead to a set of overlapping
cliques, for instance (John, CEO, Inc. Corp.) and
(John, CEO,?). Instead of having the system re-
turn all cliques, our system just returns the maximal
cliques, that is, those cliques that are not subsets of
other cliques. Hence, for the example under con-
sideration in Figure 2, the system would return the
one correct relation, (John, CEO, Inc. Corp.), and
two incorrect relations, (John, CEO, Biz. Corp.) and
(Jane, CEO,?). The second is incorrect since it
does not specify the company slot of the relation
even though that information is present in the text.
It is possible to find degenerate sentences in which
perfect binary classification followed by maximal
clique reconstruction will lead to errors. One such
sentence is, ?John is C.E.O. and C.F.O. of Inc. Corp.
and Biz. Corp. respectively and Jane vice-versa?.
However, we expect such sentences to be rare; in
fact, they never occur in our data.
The real problem with this approach is that an ar-
bitrary graph can have exponentially many cliques,
negating any efficiency advantage over enumerating
all n-tuples of entities. Fortunately, there are algo-
rithms for finding all maximal cliques that are effi-
cient in practice. We use the algorithm of Bron and
Kerbosch (1973). This is a well known branch and
bound algorithm that has been shown to empirically
run linearly in the number of maximal cliques in the
graph. In our experiments, this algorithm found all
maximal cliques in a matter of seconds.
4.2.2 Probabilistic Cliques
The above approach has a major shortcom-
ing in that it assumes the output of the bi-
nary classifier to be absolutely correct. For
instance, the classifier may have thought with
probability 0.49, 0.99 and 0.99 that the fol-
lowing pairs were related: (Jane, Biz. Corp.),
(CEO, Biz. Corp.) and (Jane, CEO) respectively.
The maximal clique method would not produce the
tuple (Jane, CEO, Biz. Corp.) since it never consid-
ers the edge between Jane and Biz. Corp. However,
given the probability of the edges, we would almost
certainly want this tuple returned.
What we would really like to model is a belief
that on average a clique represents a valid relation
instance. To do this we use the complete graph
G = (V,E) with edges between all pairs of entity
mentions. We then assign weight w(e) to edge e
equal to the probability that the two entities in e are
related, according to the classifier. We define the
weight of a clique w(C) as the mean weight of the
edges in the clique. Since edge weights represent
probabilities (or ratios), we use the geometric mean
w(C) =
?
?
?
e?EC
w(e)
?
?
1/|EC |
We decide that a clique C represents a valid tuple if
w(C) ? 0.5. Hence, the system finds all maximal
cliques as before, but considers only those where
w(C) ? 0.5, and it may select a non-maximal clique
if the weight of all larger cliques falls below the
threshold. The cutoff of 0.5 is not arbitrary, since it
ensures that the average probability of a clique rep-
resenting a relation instance is at least as large as
the average probability of it not representing a rela-
tion instance. We ran experiments with varying lev-
els of this threshold and found that, roughly, lower
thresholds result in higher precision at the expense
of recall since the system returns fewer but larger
tuples. Optimum results were obtained for a cutoff
of approximately 0.4, but we report results only for
w(C) ? 0.5.
The major problem with this approach is that
there will always be exponentially many cliques
since the graph is fully connected. However, in our
experiments we pruned all edges that would force
any containing clique C to have w(C) < 0.5. This
typically made the graphs very sparse.
Another problem with this approach is the as-
sumption that the binary relation classifier outputs
probabilities. For maximum entropy and other prob-
abilistic frameworks this is not an issue. However,
many classifiers, such as SVMs, output scores or
distances. It is possible to transform the scores from
those models through a sigmoid to yield probabili-
495
ties, but there is no guarantee that those probability
values will be well calibrated.
5 Experiments
5.1 Problem Description and Data
We test these methods on the task of extracting ge-
nomic variation events from biomedical text (Mc-
Donald et al, 2004b). Briefly, we define a varia-
tion event as an acquired genomic aberration: a spe-
cific, one-time alteration at the genomic level and
described at the nucleic acid level, amino acid level
or both. Each variation event is identified by the re-
lationship between a type of variation, its location,
and the corresponding state change from an initial-
state to an altered-state. This can be formalized as
the following complex schema
(var-type, location, initial-state, altered-state)
A simple example is the sentence
?At codons 12 and 61, the occurrence of
point mutations from G/A to T/G were observed?
which gives rise to the tuples
(point mutation, codon 12, G, T)
(point mutation, codon 61, A, G)
Our data set consists of 447 abstracts selected
from MEDLINE as being relevant to populating a
database with facts of the form: gene X with vari-
ation event Y is associated with malignancy Z. Ab-
stracts were randomly chosen from a larger corpus
identified as containing variation mentions pertain-
ing to cancer.
The current data consists of 4691 sentences that
have been annotated with 4773 entities and 1218 re-
lations. Of the 1218 relations, 760 have two ? ar-
guments, 283 have one ? argument, and 175 have
no ? arguments. Thus, 38% of the relations tagged
in this data cannot be handled using binary relation
classification alone. In addition, 4% of the relations
annotated in this data are non-sentential. Our sys-
tem currently only produces sentential relations and
is therefore bounded by a maximum recall of 96%.
Finally, we use gold standard entities in our exper-
iments. This way we can evaluate the performance
of the relation extraction system isolated from any
kind of pipelined entity extraction errors. Entities in
this domain can be found with fairly high accuracy
(McDonald et al, 2004b).
It is important to note that just the presence of two
entity types does not entail a relation between them.
In fact, 56% of entity pairs are not related, due either
to explicit disqualification in the text (e.g. ?... the
lack of G to T transversion ...?) or ambiguities that
arise from multiple entities of the same type.
5.2 Results
Because the data contains only 1218 examples of re-
lations we performed 10-fold cross-validation tests
for all results. We compared three systems:
? MC: Uses the maximum entropy binary classi-
fier coupled with the maximal clique complex
relation reconstructor.
? PC: Same as above, except it uses the proba-
bilistic clique complex relation reconstructor.
? NE: A maximum entropy classifier that naively
enumerates all possible relation instances as
described in Section 4.1.
In training system NE, all incomplete but correct
instances were marked as positive since we found
this had the best performance. We used the same
pairwise entity features in the binary classifier of
the above two systems. However, we also added
higher order versions of the pairwise features. For
this system we only take maximal relations,that is,
if (John, CEO, Inc. Corp.) and (John,?, Inc. Corp.)
are both labeled positive, the system would only re-
turn the former.
Table 2 contains the results of the maximum en-
tropy binary relation classifier (used in systems MC
and PC). The 1218 annotated complex relations pro-
duced 2577 unique binary pairs of related entities.
We can see that the maximum entropy classifier per-
forms reasonably well, although performance may
be affected by the lack of rich syntactic features,
which have been shown to help performance (Miller
et al, 2000; Zelenko et al, 2003).
Table 3 compares the three systems on the real
problem of extracting complex relations. An ex-
tracted complex relation is considered correct if and
only if all the entities in the relation are correct.
There is no partial credit. All training and clique
finding algorithms took under 5 minutes for the en-
tire data set. Naive enumeration took approximately
26 minutes to train.
496
ACT PRD COR
2577 2722 2101
Prec Rec F-Meas
0.7719 0.8153 0.7930
Table 2: Binary relation classification results for the
maximum entropy classifier. ACT: actual number of
related pairs, PRD: predicted number of related pairs
and COR: correctly identified related pairs.
System Prec Rec F-Meas
NE 0.4588 0.6995 0.5541
MC 0.5812 0.7315 0.6480
PC 0.6303 0.7726 0.6942
Table 3: Full relation classification results. For a
relation to be classified correctly, all the entities in
the relation must be correctly identified.
First we observe that the maximal clique method
combined with maximum entropy (system MC) re-
duces the relative error rate over naively enumer-
ating and classifying all instances (system NE) by
21%. This result is very positive. The system based
on binary factorization not only is more efficient
then naively enumerating all instances, but signifi-
cantly outperforms it as well. The main reason naive
enumeration does so poorly is that all correct but
incomplete instances are marked as positive. Thus,
even slight correlations between partially correct en-
tities would be enough to classify an instance as cor-
rect, which results in relatively good recall but poor
precision. We tried training only with correct and
complete positive instances, but the result was a sys-
tem that only returned few relations since negative
instances overwhelmed the training set. With fur-
ther tuning, it may be possible to improve the per-
formance of this system. However, we use it only as
a baseline and to demonstrate that binary factoriza-
tion is a feasible and accurate method for extracting
complex relations.
Furthermore, we see that using probabilistic
cliques (system PC) provides another large im-
provement, a relative error reduction of 13%
over using maximal cliques and 31% reduction
over enumeration. Table 4 shows the breakdown
of relations returned by type. There are three
types of relations, 2-ary, 3-ary and 4-ary, each
with 2, 1 and 0 ? arguments respectively, e.g.
System 2-ary 3-ary 4-ary
NE 760:1097:600 283:619:192 175:141:60
MC 760:1025:601 283:412:206 175:95:84
PC 760:870:590 283:429:223 175:194:128
Table 4: Breakdown of true positive relations by
type that were returned by each system. Each cell
contains three numbers, Actual:Predicted:Correct,
which represents for each arity the actual, predicted
and correct number of relations for each system.
(point mutation, codon 12,?,?) is a 2-ary relation.
Clearly the probabilistic clique method is much
more likely to find larger non-binary relations, veri-
fying the motivation that there are some low proba-
bility edges that can still contribute to larger cliques.
6 Conclusions and Future Work
We presented a method for complex relation extrac-
tion, the core of which was to factorize complex re-
lations into sets of binary relations, learn to identify
binary relations and then reconstruct the complex re-
lations by finding maximal cliques in graphs that
represent relations between pairs of entities. The
primary advantage of this method is that it allows
for the use of almost any binary relation classifier,
which have been well studied and are often accu-
rate. We showed that such a method can be suc-
cessful with an empirical evaluation on a large set
of biomedical data annotated with genomic varia-
tion relations. In fact, this approach is both signifi-
cantly quicker and more accurate then enumerating
and classifying all possible instances. We believe
this work provides a good starting point for contin-
ued research in this area.
A distinction may be made between the factored
system presented here and one that attempts to clas-
sify complex relations without factorization. This
is related to the distinction between methods that
learn local classifiers that are combined with global
constraints after training and methods that incorpo-
rate the global constraints into the learning process.
McCallum and Wellner (2003) showed that learning
binary co-reference relations globally improves per-
formance over learning relations in isolation. How-
ever, their model relied on the transitive property in-
herent in the co-reference relation. Our system can
be seen as an instance of a local learner. Punyakanok
497
et al (2004) argued that local learning actually out-
performs global learning in cases when local deci-
sions can easily be learnt by the classifier. Hence, it
is reasonable to assume that our binary factorization
method will perform well when binary relations can
be learnt with high accuracy.
As for future work, there are many things that we
plan to look at. The binary relation classifier we em-
ploy is quite simplistic and most likely can be im-
proved by using features over a deeper representa-
tion of the data such as parse trees. Other more pow-
erful binary classifiers should be tried such as those
based on tree kernels (Zelenko et al, 2003). We also
plan on running these algorithms on more data sets
to test if the algorithms empirically generalize to dif-
ferent domains.
Perhaps the most interesting open problem is how
to learn the complex reconstruction phase. One pos-
sibility is recent work on supervised clustering. Let-
ting the edge probabilities in the graphs represent a
distance in some space, it may be possible to learn
how to cluster vertices into relational groups. How-
ever, since a vertex/entity can participate in one or
more relation, any clustering algorithm would be re-
quired to produce non-disjoint clusters.
We mentioned earlier that the only restriction of
our complex relation definition is that the arity of
the relation must be known in advance. It turns out
that the algorithms we described can actually handle
dynamic arity relations. All that is required is to
remove the constraint that maximal cliques must be
consistent with the structure of the relation. This
represents another advantage of binary factorization
over enumeration, since it would be infeasible to
enumerate all possible instances for dynamic arity
relations.
Acknowledgments
The authors would like to thank Mark Liberman,
Mark Mandel and Eric Pancoast for useful discus-
sion, suggestions and technical support. This work
was supported in part by NSF grant ITR 0205448.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1).
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning Journal Special Issue on Natural Language
Learning, 34(1/3):221?231.
C. Bron and J. Kerbosch. 1973. Algorithm 457: finding
all cliques of an undirected graph. Communications of
the ACM, 16(9):575?577.
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting
the names of genes and gene products with a hidden
Markov model. In Proc. COLING.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proc. NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on In-
formation Integration on the Web.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. ICML.
A. K. McCallum. 2002. MALLET: A machine learning
for language toolkit.
D.M. McDonald, H. Chen, H. Su, and B.B. Marshall.
2004a. Extracting gene pathway relations using a hy-
brid grammar: the Arizona Relation Parser. Bioinfor-
matics, 20(18):3370?78.
R.T. McDonald, R.S. Winters, M. Mandel, Y. Jin, P.S.
White, and F. Pereira. 2004b. An entity tagger for
recognizing acquired genomic variations in cancer lit-
erature. Bioinformatics, 20(17):3249?3251.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proc. NAACL.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Learning via inference over structurally constrained
output. In Workshop on Learning Structured with Out-
put, NIPS.
Barbara Rosario and Marti A. Hearst. 2004. Classifying
semantic relations in bioscience texts. In ACL.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proc. CoNLL.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. JMLR.
498
Integrated Annotation for Biomedical Information Extraction
Seth Kulick and Ann Bies and Mark Liberman and Mark Mandel
and Ryan McDonald and Martha Palmer and Andrew Schein and Lyle Ungar
University of Pennsylvania
Philadelphia, PA 19104
 
skulick,bies,myl  @linc.cis.upenn.edu,
mamandel@unagi.cis.upenn.edu,
 
ryantm,mpalmer,ais,ungar  @cis.upenn.edu
Scott Winters and Pete White
Division of Oncology,
Children?s Hospital of Philadelphia
Philadelphia, Pa 19104
 
winters,white  @genome.chop.edu
Abstract
We describe an approach to two areas of
biomedical information extraction, drug devel-
opment and cancer genomics. We have devel-
oped a framework which includes corpus anno-
tation integrated at multiple levels: a Treebank
containing syntactic structure, a Propbank con-
taining predicate-argument structure, and an-
notation of entities and relations among the en-
tities. Crucial to this approach is the proper
characterization of entities as relation compo-
nents, which allows the integration of the entity
annotation with the syntactic structure while
retaining the capacity to annotate and extract
more complex events. We are training statis-
tical taggers using this annotation for such ex-
traction as well as using them for improving the
annotation process.
1 Introduction
Work over the last few years in literature data mining
for biology has progressed from linguistically unsophisti-
cated models to the adaptation of Natural Language Pro-
cessing (NLP) techniques that use full parsers (Park et
al., 2001; Yakushiji et al, 2001) and coreference to ex-
tract relations that span multiple sentences (Pustejovsky
et al, 2002; Hahn et al, 2002) (For an overview, see
(Hirschman et al, 2002)). In this work we describe an ap-
proach to two areas of biomedical information extraction,
drug development and cancer genomics, that is based on
developing a corpus that integrates different levels of se-
mantic and syntactic annotation. This corpus will be a
resource for training machine learning algorithms useful
for information extraction and retrieval and other data-
mining applications. We are currently annotating only
abstracts, although in the future we plan to expand this to
full-text articles. We also plan to make publicly available
the corpus and associated statistical taggers.
We are collaborating with researchers in the Division
of Oncology at The Children?s Hospital of Philadelphia,
with the goal of automatically mining the corpus of can-
cer literature for those associations that link specified
variations in individual genes with known malignancies.
In particular we are interested in extracting three entities
(Gene, Variation Event, and Malignancy) in the follow-
ing relationship: Gene X with genomic Variation Event
Y is correlated with Malignancy Z. For example, WT1 is
deleted in Wilms Tumor #5. Such statements found in the
literature represent individual gene-variation-malignancy
observables. A collection of such observables serves
two important functions. First, it summarizes known
relationships between genes, variation events, and ma-
lignancies in the cancer literature. As such, it can be
used to augment information available from curated pub-
lic databases, as well as serve as an independent test for
accuracy and completeness of such repositories. Second,
it allows inferences to be made about gene, variation, and
malignancy associations that may not be explicitly stated
in the literature, both at the fact and entity instance lev-
els. Such inferences provide testable hypotheses and thus
future research targets.
The other major area of focus, in collaboration with
researchers in the Knowledge Integration and Discov-
ery Systems group at GlaxoSmithKline (GSK), is the ex-
traction of information about enzymes, focusing initially
on compounds that affect the activity of the cytochrome
P450 (CYP) family of proteins. For example, the goal is
to see a phrase like
Amiodarone weakly inhibited CYP2C9,
CYP2D6, and CYP3A4-mediated activities
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 61-68.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
with Ki values of 45.1?271.6 
and extract the facts
amiodarone inhibits CYP2C9 with
Ki=45.1-271.6
amiodarone inhibits CYP2D6 with
Ki=45.1-271.6
amiodarone inhibits CYP3A4 with
Ki=45.1-271.6
Previous work at GSK has used search algorithms that
are based on pattern matching rules filling template slots.
The rules rely on identifying the relevant passages by first
identifying compound names and then associating them
with a limited number of relational terms such as inhibit
or inactivate. This is similar to other work in biomedical
extraction projects (Hirschman et al, 2002).
Creating good pattern-action rules for an IE problem is
far from simple. There are many complexities in the dif-
ferent ways that a relation can be expressed in language,
such as syntactic alternations and the heavy use of co-
ordination. While sufficiently complex patterns can deal
with these issues, it requires a good amount of time and
effort to build such hand-crafted rules, particularly since
such rules are developed for each specific problem. A
corpus that is annotated with sufficient syntactic and se-
mantic structure offers the promise of training taggers for
quicker and easier information extraction.
The corpus that we are developing for the two differ-
ent application demands consists of three levels of anno-
tation: the entities and relations among the entities for the
oncology or CYP domain, syntactic structure (Treebank),
and predicate-argument structure (Propbank). This is a
novel approach from the point-of-view of NLP since pre-
vious efforts at Treebanking and Propbanking have been
independent of the special status of any entities, and pre-
vious efforts at entity annotation have been independent
of corresponding layers of syntactic and semantic struc-
ture. The decomposition of larger entities into compo-
nents of a relation, worthwhile by itself on conceptual
grounds for entity definition, also allows the component
entities to be mapped to the syntactic structure. These
entities can be viewed as semantic types associated with
syntactic constituents, and so our expectation is that au-
tomated analyses of these related levels will interact in a
mutually reinforcing and beneficial way for development
of statistical taggers. Development of such statistical tag-
gers is proceeding in parallel with the annotation effort,
and these taggers help in the annotation process, as well
as being steps towards automatic extraction.
In this paper we focus on the aspects of this project
that have been developed and are in production, while
also trying to give enough of the overall vision to place
the work that has been done in context. Section 2 dis-
cusses some of the main issues around the development
of the guidelines for entity annotation, for both the on-
cology and inhibition domains. Section 3 first discusses
the overall plan for the different levels of annotation, and
then focuses on the integration of the two levels currently
in production, entity annotation and syntactic structure.
Section 4 describes the flow of the annotation process,
including the development of the statistical taggers men-
tioned above. Section 5 is the conclusion.
2 Guidelines for Entity Annotation
Annotation has been proceeding for both the oncology
and the inhibition domains. Here we give a summary of
the main features of the annotation guidelines that have
been developed. We have been influenced in this by pre-
vious work in annotation for biomedical information ex-
traction (Ohta et al, 2002; Gaizauskas et al, 2003). How-
ever, we differ in the domains we are annotating and the
design philosophy for the entity guidelines. For exam-
ple, we have been concentrating on explicit concepts for
entities like genes rather than developing a wide-range
ontology for the various physical instantiations.
2.1 Oncology Domain
Gene Entity For the sake of this project the defini-
tion for ?Gene Entity? has two significant characteristics.
First, ?Gene? refers to a composite entity as opposed to
the strict biological definition. As has been noted by oth-
ers, there are often ambiguities in the usage of the en-
tity names. For example, it is sometimes unclear as to
whether it is the gene or protein being referenced, or the
same name might refer to the gene or the protein at dif-
ferent locations in the same document. Our approach to
this problem is influenced by the named entity annota-
tion in the Automatic Content Extraction (ACE) project
(Consortium, 2002), in which ?geopolitical? entities can
have different roles, such as ?location? or ?organization?.
Analogously, we consider a ?gene? to be a composite en-
tity that can have different roles throughout a document.
Standardization of ?Gene? references between different
texts and between gene synonyms is handled by exter-
nally referencing each instance to a standard ontology
(Ashburner et al, 2000).
In the context of this project, ?Gene? refers to a con-
ceptual entity as opposed to the specific manifestation of
a gene (i.e. an allele or nucleotide sequence). Therefore,
we consider genes to be abstract concepts identifying ge-
nomic regions often associated with a function, such as
MYC or TrkB; we do not consider actual instances of
such genes within the gene-entity domain. Since we are
interested in the association between Gene-entities and
malignancies, for this project genes are of interest to us
when they have an associated variation event. Therefore,
the combination of Gene entities and Variation events
provides us with an evoked entity representing the spe-
cific instance of a gene.
Variation Events as Relations Variations comprise a
relationship between the following entities: Type (e.g.
point mutation, translocation, or inversion), Location
(e.g. codon 14, 1p36.1, or base pair 278), Original-State
(e.g. Alanine), and Altered-State (e.g. Thymine). These
four components represent the key elements necessary
to describe any genomic variation event. Variations are
often underspecified in the literature, frequently having
only two or three of these specifications. Characterizing
individual variations as a relation among such compo-
nents provides us with a great deal of flexibility: 1) it al-
lows us to capture the complete variation event even when
specific components are broadly spaced in the text, often
spanning multiple sentences or even paragraphs; 2) it pro-
vides us with a convenient means of tracking anaphora
between detailed descriptions (e.g. a point mutation at
codon 14 and summary references (e.g. this variation);
and 3) it provides a single structure capable of capturing
the breadth of variation specifications (e.g. A-  T point
mutation at base pair 47, A48-  G or t(11;14)(q13;32)).
Malignancy The guidelines for malignancy annotation
are under development. We are planning to define it in a
manner analogous to variation, whereby a Malignancy is
composed of various attribute types (such as developmen-
tal stage, behavior, topographic site, and morphology).
2.2 CYP Domain
In the CYP Inhibition annotation task we are tagging
three types of entities:
1. CYP450 enzymes (cyp)
2. other substances (subst)
3. quantitative measurements (quant)
Each category has its own questions and uncertain-
ties. Names like CYP2C19 and cytochrome P450 en-
zymes proclaim their membership, but there are many
aliases and synonyms that do not proclaim themselves,
such as 17,20-lyase. We are compiling a list of such
names.
Other substances is a potentially huge and vaguely-
delimited set, which in the current corpus includes grape-
fruit juice and red wine as well as more obviously bio-
chemical entities like polyunsaturated fatty acids and ery-
thromycin. The quantitative measurements we are di-
rectly interested in are those directly related to inhibition,
such as IC50 and K(i). We tag the name of the measure-
ment, the numerical value, and the unit. For example, in
the phrase ...was inhibited by troleandomycin (ED50 = 1
microM), ED50 is the name, 1 the value, and microM the
unit. We are also tagging other measurements, since it
is easy to do and may provide valuable information for
future IE work.
3 Integrated Annotation
As has been noted in the literature on biomedical IE (e.g.,
(Pustejovsky et al, 2002; Yakushiji et al, 2001)), the
same relation can take a number of syntactic forms. For
example, the family of words based on inhibit occurs
commonly in MEDLINE abstracts about CYP enzymes
(as in the example in the introduction) in patterns like A
inhibited B, A inhibited the catalytic activity of B, inhibi-
tion of B by A, etc.
Such alternations have led to the use of pattern-
matching rules (often hand-written) to match all the rele-
vant configurations and fill in template slots based on the
resulting pattern matches. As discussed in the introduc-
tion, dealing with such complications in patterns can take
much time and effort.
Our approach instead is to build an annotated corpus
in which the predicate-argument information is annotated
on top of the parsing annotations in the Treebank, the re-
sulting corpus being called a ?proposition bank? or Prop-
bank. This newly annotated corpus is then used for train-
ing processors that will automatically extract such struc-
tures from new examples.
In a Propbank for biomedical text, the types of in-
hibit examples listed above would consistently have their
compounds labeled as Arg0 and their enzymes labeled as
Arg1, for nominalized forms such as A is an inhibitor of
B, A caused inhibition of B, inhibition of B by A, as well
the standard A inhibits B. We would also be able to la-
bel adjuncts consistently, such as the with prepositional
phrase in CYP3A4 activity was decreased by L, S and F
with IC(50) values of about 200 mM. In accordance with
other Calibratable verbs such as rise, fall, decline, etc.,
this phrase would be labeled as an Arg2-EXTENT, re-
gardless of its syntactic role.
A Propbank has been built on top of the Penn Tree-
bank, and has been used to train ?semantic taggers?, for
extracting argument roles for the predicates of interest,
regardless of the particular syntactic context.1
Such semantic taggers have been developed by using
machine learning techniques trained on the Penn Prop-
bank (Surdeanu et al, 2003; Gildea and Palmer, 2002;
Kingsbury and Palmer, 2002). However, the Penn Tree-
bank and Propbank involve the annotation of Wall Street
Journal text. This text, being a financial domain, differs
in significant ways from the biomedical text, and so it is
1The Penn Propbank is complemented by NYU?s Nom-
bank project (Meyers, October 2003), which includes tagging
of nominal predicate structure. This is particular relevant for
the biomedical domain, given the heavy use of nominals such
mutation and inhibition.
necessary for this approach to have a corpus of biomed-
ical texts such as MEDLINE articles annotated for both
syntactic structure (Treebanking) and shallow semantic
structure (Propbanking).
In this project, the syntactic and semantic annotation is
being done on a corpus which is also being annotated for
entities, as described in Section 2. Since semantic tag-
gers of the sort described above result in semantic roles
assigned to syntactic tree constituents, it is desirable to
have the entities correspond to syntactic constituents so
that the semantic roles are assigned to entities. The en-
tity information can function as type information and be
taken advantage of by learning algorithms to help charac-
terize the properties of the terms filling specified roles in
a given predicate.
This integration of these three different annotation lev-
els, including the entities, is being done for the first time2,
and we discuss here three main challenges to this corre-
spondence between entities and constituents: (1) entities
that are large enough to cut across multiple constituents,
(2) entities within prenominal modifiers, and (3) coordi-
nation.3
Relations and Large Entities One major area of con-
cern is the possibility of entities that contain more than
one syntactic constituent and do not match any node in
the syntax tree. For example, as discussed in Section 2, a
variation event includes material on a variation?s type, lo-
cation, and state, and can cut not only across constituents,
but even sentences and paragraphs. A simple example is
point mutations at codon 12, containing both the nominal
(the type of mutation) and following NP (the location).
Note that while in isolation this could also be considered
one syntactic constituent, the NP and PP together, the ac-
tual context is ...point mutations at codon 12 in duode-
nal lavage fluid.... Since all PPs are attached at the same
level, at codon 12 and in duodenal lavage fluid are sis-
ters, and so there is no constituent consisting of just point
mutations at codon 12.
Casting the variation event as a relation between dif-
ferent component entities allows the component entities
to correspond to tree constituents, while retaining the ca-
pacity to annotate and search for more complex events.
In this case, one component entity point mutations cor-
2An influential precursor to this integration is the system de-
scribed in (Miller et al, 1996). Our work is in much the same
spirit, although the representation of the predicate-argument
structure via Propbank and the linkage to the entities is quite
different, as well as of course the domain of annotation.
3There are cases where the entities are so minimal that they
are contained within a NP, not including the determiner, such as
CpG site in the NP a CpG site. entities. We are not as concerned
about these cases since we expect that such entity information
properly contained within a base NP can be associated with the
full base NP.
responds to a (base) NP node, and at codon 12 is corre-
sponds to the PP node that is the NP?s sister. At the same
time, the relation annotation contains the information re-
lating these two constituents.
Similarly, while the malignancy entity definition is cur-
rently under development, as mentioned in Section 2.1, a
guiding principle is that it will also be treated as a relation
and broken down into component entities. While this also
has conceptual benefits for the annotation guidelines, it
has the fortunate effect of making such otherwise syntax-
unfriendly malignancies as colorectal adenomas contain-
ing early cancer and acute myelomonocytic leukemia in
remission amenable for mapping the component parts to
syntactic nodes.
Entities within Prenominal Modifiers While we are
for the most part following the Penn Treebank guide-
lines (Bies et al, 1995), we are modifying them in two
important aspects. One concerns the prenominal mod-
ifiers, which in the Penn Treebank were left flat, with
no structure, but in this biomedical domain contain much
of the information - e.g., cancer-associated autoimmune
antigen. Not only would this have had no annotation
for structure, but even more bizarrely, cancer-associated
would have been a single token in the Penn Treebank,
thus making it impossible to capture the information as
to what is associated with what. We have developed new
guidelines to assign structure to prenominal entities such
as breast cancer, as well as changed the tokenization
guidelines to break up tokens such as cancer-associated.
Coordination We have also modified the treebank an-
notation to account for the well-known problem of enti-
ties that are discontinuous within a coordination structure
- e.g., K- and H-ras, where the entities are K-ras and H-
ras. Our annotation tool allows for discontinuous entities,
so that both K-ras and H-ras are annotated as genes.
Under standard Penn Treebank guidelines for tokeniza-
tion and syntactic structure, this would receive the flat
structure
NP
K- and H-ras
in which there is no way to directly associate the entity
K-ras with a constituent node.
We have modified the treebank guidelines so that K-ras
and H-ras are both constituents, with the ras part of K-ras
represented with an empty category co-indexed with ras
in H-ras:4.
4This is related to the approach to coordination in the GE-
NIA project.
NP
NP
K - NX-1
*P*
and NP
H - NX-1
ras
4 Annotation Process
We are currently annotating MEDLINE abstracts for both
the oncology and CYP domains. The flowchart for the
annotation process is shown in Figure 1. Tokenization,
POS-tagging, entity annotation (both domains), and tree-
banking are in full production. Propbank annotation and
the merging of the entities and treebanking remain to be
integrated into the current workflow. The table in Fig-
ure 2 shows the number of abstracts completed for each
annotation area.
The annotation sequence begins with tokenization and
part-of-speech annotating. While both aspects are simi-
lar to those used for the Penn Treebank, there are some
differences, partly alluded to in Section 3. Tokens are
somewhat more fine-grained than in the Penn Treebank,
so that H-ras, e.g., would consist of three tokens: H, -,
and ras.
Tokenized and part-of-speech annotated files are then
sent to the entity annotators, either for oncology or CYP,
depending on which domain the abstract has been chosen
for. The entities described in Section 2 are annotated at
this step. We are using WordFreak, a Java-based linguis-
tic annotation tool5, for annotation of tokenization, POS,
and entities. Figure 3 is a screen shot of the oncology do-
main annotation, here showing a variation relation being
created out of component entities for type and location.
In parallel with the entity annotation, a file is tree-
banked - i.e., annotated for its syntactic structure. Note
that this is done independently of the entity annotation.
This is because the treebanking guidelines are relatively
stable (once they were adjusted for the biomedical do-
main as described in Section 3), while the entity defini-
tions can require a significant period of study before sta-
bilizing, and with the parallel treatment the treebanking
can proceed without waiting for the entity annotation.
However, this does mean that to produce the desired
integrated annotation, the entity and treebanking annota-
tions need to be merged into one representation. The con-
sideration of the issues described in Section 3 has been
carried out for the purpose of allowing this integration
of the treebanking and entity annotation. This has been
completed for some pilot documents, but the full merging
remains to be integrated into the workflow system.
5http://www.sf.net/projects/wordfreak
As mentioned in the introduction, statistical taggers
are being developed in parallel with the annotation effort.
While such taggers are part of the final goal of the project,
providing the building blocks for extracting entities and
relations, they are also useful in the annotation process
itself, so that the annotators only need to perform correc-
tion of automatically tagged data, instead of starting from
scratch.
Until recently (Feb. 10), the part-of-speech annotation
was done by hand-correcting the results of tagging the
data with a part-of-speech tagger trained on a modified
form of the Penn Treebank.6 The tagger is a maximum-
entropy model utilizing the opennlp package available
at http://www.sf.net/projects/opennlp . It
has now been retrained using 315 files (122 from the
oncology domain, 193 from the cyp domain). Figure 4
shows the improvement of the new vs. the old POS tag-
ger on the same 294 files that have been hand-corrected.
These results are based on testing files that have already
been tokenized, and thus are an evaluation only of the
POS tagger and not the tokenizer. While not directly
comparable to results such as (Tateisi and Tsujii, 2004),
due to the different tag sets and tokenization, they are in
the same general range.7
The oncology and cyp entity annotation, as well as the
treebanking are still being done fully manually, although
that will change in the near future. Initial results for a tag-
ger to identify the various components of a variation re-
lation are promising, although not yet integrated into an-
notation process. The tagger is based on the implementa-
tion of Conditional Random Fields (Lafferty et al, 2001)
in the Mallet toolkit (McCallum, 2002). Briefly, Condi-
tional Random Fields are log-linear models that rely on
weighted features to make predictions on the input. Fea-
tures used by our system include standard pattern match-
ing and word features as well as some expert-created reg-
ular expression features8. Using 10-fold cross-validation
on 264 labelled abstracts containing 551 types, 1064 lo-
6Roughly, Penn Treebank tokens were split at hyphens, with
the individual components then sent through a Penn Treebank-
trained POS tagger, to create training data for another POS tag-
ger. For example (JJ York-based) is treated as (NNP
York) (HYPH -) (JJ based). While this works rea-
sonably well for tokenization, the POS tagger suffered severely
from being trained on a corpus with such different properties.
7The tokenizer has also been retrained and the new tokenizer
is being used for annotation, although although we do not have
the evaluation results here.
8e.g., chr|chromosome [1-9]|1[0-9]|2[0-
2]|X|Y p|q
Merged Entity/
Treebank Annotation
Tokenization
Entity Annotation
POS Annotation
Treebank/Propbank
Annotation
Figure 1: Annotation Flow
Annotation Task Start Date Annotated Documents
Part-of-Speech Tagging 8/22/03 422
Entity Tagging 9/12/03 414
Treebanking 1/8/04 127
Figure 2: Current Annotation Production Results
Figure 3: Relation Annotation in WordFreak
Tagger Training Material Token Instances
Old Sections 00-15 Penn Treebank 773832
New 315 abstracts 103159
Tagger Overall Accuracy Number Token Instances Accuracy on Accuracy on
Unseen in Training Data Unseen Seen
Old 88.53% 14542 58.80% 95.53%
New 97.33% 4096 85.05% 98.02%
(Testing Material: 294 abstracts from the oncology domain, with 76324 token instances.)
Figure 4: Evaluation of Part-of-Speech Taggers
cations and 557 states, we obtained the following results:
Entity Precision Recall F-measure
Type 0.80 0.72 0.76
Location 0.85 0.73 0.79
State 0.90 0.80 0.85
Overall 0.86 0.75 0.80
An entity is considered correctly identified if and only
if it matches the human labeling by both category (type,
location or state) and span (from position a to position b).
At this stage we have not distinguished between initial
and final states.
While it is difficult to compare taggers that tag
different types of entities (e.g., (Friedman et al, 2001;
Gaizauskas et al, 2003)), CRFs have been utilized for
state-of-the-art results in NP-chunking and gene and
protein tagging (Sha and Pereira, 2003; McDonald
and Pereira, 2004) Currently, we are beginning to
investigate methods to identify relations over the varia-
tion components that are extracted using the entity tagger.
5 Conclusion
We have described here an integrated annotation ap-
proach for two areas of biomedical information extrac-
tion. We discussed several issues that have arisen for this
integration of annotation layers. Much effort has been
spent on the entity definitions and how they relate to the
higher-level concepts which are desired for extraction.
There are promising initial results for training taggers to
extract these entities.
Next steps in the project include: (1) continued anno-
tation of the layers we are currently doing, (2) integra-
tion of the level of predicate-argument annotation, and
(3) further development of the statistical taggers, includ-
ing taggers for identifying relations over their component
entities.
Acknowledgements
The project described in this paper is based at the In-
stitute for Research in Cognitive Science at the Uni-
versity of Pennsylvania and is supported by grant EIA-
0205448 from the National Science Foundation?s Infor-
mation Technology Research (ITR) program.
We would like to thank Aravind Joshi, Jeremy
Lacivita, Paula Matuszek, Tom Morton, and Fernando
Pereira for their comments.
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, M.A. Harris, D.P. Hill, L. Issel-Tarver,
A. Kasarskis, S. Lewis, J.C. Matese, J.E. Richardson,
M. Ringwald, G.M. Rubin, and G. Sherlock. 2000.
Gene ontology: Tool for the unification of biology.
Nature Genetics, 25(1):25?29.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
Style, Penn Treebank Project. Tech report MS-CIS-
95-06, University of Pennsylvania, Philadelphia, PA.
Linguistic Data Consortium. 2002. Entity de-
tection and tracking - phase 1 - EDT and
metonymy annotation guidelines version 2.5
20021205. http://www.ldc.upenn.edu/Projects/ACE
/PHASE2/Annotation/.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. Genies: a
natural-language processing system for the extraction
of molecular pathways from journal articles. ISMB
(Supplement of Bioinformatics), pages 74?82.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Bioinformatics applications of information
extraction from journal articles. Journal of Bioinfor-
matics, 19(1):135?143.
Daniel Gildea and Martha Palmer. 2002. The Necessity
of Syntactic Parsing for Predicate Argument Recogni-
tion. In Proc. of ACL-2002.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: The
MEDSYNDIKATE text mining system. In Proceed-
ings of the Pacific Rim Symposium on Biocomputing,
pages 338?349.
Lynette Hirschman, Jong C. Park, Junichi Tsuji, Limsoon
Wong, and Cathy H. Wu. 2002. Accomplishments and
challenges in literature data mining for biology. Bioin-
formatics Review, 18(12):1553?1561.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC2002), Las Palmas, Spain.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2004. Identify-
ing gene and protein mentions in text using conditional
random fields. In A Critical Assessment of Text Min-
ing Methods in Molecular Biology workshop. To be
presented.
Adam Meyers. October, 2003. Nombank. Talk at Auto-
matic Content Extraction (ACE) PI Meeting, Alexan-
dria, VA.
Scott Miller, David Stallard, Robert Bobrow, and Richard
Schwartz. 1996. A fully statistical approach to
natural language interfaces. In Aravind Joshi and
Martha Palmer, editors, Proceedings of the Thirty-
Fourth Annual Meeting of the Association for Compu-
tational Linguistics, pages 55?61, San Francisco. Mor-
gan Kaufmann Publishers.
Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, and Jun?ici
Tsuji. 2002. The GENIA corpus: An annotated corpus
in molecular biology domain. In Proceedings of the
10th International Conference on Intelligent Systems
for Molecular Biology.
J. Park, H. Kim, and J. Kim. 2001. Bidirectional in-
cremental parsing for automatic pathway identification
with combinatory categorial grammar. In Proceedings
of the Pacific Rim Symposium on Biocomputing, pages
396?407.
J. Pustejovsky, J. Castano, and J. Zhang. 2002. Robust
relational parsing over biomedical literature: Extract-
ing inhibit relations. In Proceedings of the Pacific Rim
Symposium on Biocomputing, pages 362?373.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceeds of Human
Language Technology-NAACL 2003.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Yuka Tateisi and Jun-ichi Tsujii. 2004. Part-of-speech
annotation of biology research abstracts. In Proceed-
ings of LREC04. To be presented.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001.
Event extraction from biomedical papers using a full
parser. In Proceedings of the Pacific Rim Symposium
on Biocomputing, pages 408?419.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 120?128,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Domain Adaptation with Structural Correspondence Learning
John Blitzer Ryan McDonald Fernando Pereira
{blitzer|ryantm|pereira}@cis.upenn.edu
Department of Computer and Information Science, University of Pennsylvania
3330 Walnut Street, Philadelphia, PA 19104, USA
Abstract
Discriminative learning methods are
widely used in natural language process-
ing. These methods work best when their
training and test data are drawn from the
same distribution. For many NLP tasks,
however, we are confronted with new
domains in which labeled data is scarce
or non-existent. In such cases, we seek
to adapt existing models from a resource-
rich source domain to a resource-poor
target domain. We introduce structural
correspondence learning to automatically
induce correspondences among features
from different domains. We test our tech-
nique on part of speech tagging and show
performance gains for varying amounts
of source and target training data, as well
as improvements in target domain parsing
accuracy using our improved tagger.
1 Introduction
Discriminative learning methods are ubiquitous in
natural language processing. Discriminative tag-
gers and chunkers have been the state-of-the-art
for more than a decade (Ratnaparkhi, 1996; Sha
and Pereira, 2003). Furthermore, end-to-end sys-
tems like speech recognizers (Roark et al, 2004)
and automatic translators (Och, 2003) use increas-
ingly sophisticated discriminative models, which
generalize well to new data that is drawn from the
same distribution as the training data.
However, in many situations we may have a
source domain with plentiful labeled training data,
but we need to process material from a target do-
main with a different distribution from the source
domain and no labeled data. In such cases, we
must take steps to adapt a model trained on the
source domain for use in the target domain (Roark
and Bacchiani, 2003; Florian et al, 2004; Chelba
and Acero, 2004; Ando, 2004; Lease and Char-
niak, 2005; Daume? III and Marcu, 2006). This
work focuses on using unlabeled data from both
the source and target domains to learn a common
feature representation that is meaningful across
both domains. We hypothesize that a discrimi-
native model trained in the source domain using
this common feature representation will general-
ize better to the target domain.
This representation is learned using a method
we call structural correspondence learning (SCL).
The key idea of SCL is to identify correspon-
dences among features from different domains by
modeling their correlations with pivot features.
Pivot features are features which behave in the
same way for discriminative learning in both do-
mains. Non-pivot features from different domains
which are correlated with many of the same pivot
features are assumed to correspond, and we treat
them similarly in a discriminative learner.
Even on the unlabeled data, the co-occurrence
statistics of pivot and non-pivot features are likely
to be sparse, and we must model them in a com-
pact way. There are many choices for modeling
co-occurrence data (Brown et al, 1992; Pereira
et al, 1993; Blei et al, 2003). In this work we
choose to use the technique of structural learn-
ing (Ando and Zhang, 2005a; Ando and Zhang,
2005b). Structural learning models the correla-
tions which are most useful for semi-supervised
learning. We demonstrate how to adapt it for trans-
fer learning, and consequently the structural part
of structural correspondence learning is borrowed
from it.1
SCL is a general technique, which one can ap-
ply to feature based classifiers for any task. Here,
1Structural learning is different from learning with struc-
tured outputs, a common paradigm for discriminative nat-
ural language processing models. To avoid terminologi-
cal confusion, we refer throughout the paper to a specific
structural learning method, alternating structural optimiza-
tion (ASO) (Ando and Zhang, 2005a).
120
(a) Wall Street Journal
DT JJ VBZ DT NN IN DT JJ NN
The clash is a sign of a new toughness
CC NN IN NNP POS JJ JJ NN .
and divisiveness in Japan ?s once-cozy financial circles .
(b) MEDLINE
DT JJ VBN NNS IN DT NN NNS VBP
The oncogenic mutated forms of the ras proteins are
RB JJ CC VBP IN JJ NN NN .
constitutively active and interfere with normal signal transduction .
Figure 1: Part of speech-tagged sentences from both corpora
we investigate its use in part of speech (PoS) tag-
ging (Ratnaparkhi, 1996; Toutanova et al, 2003).
While PoS tagging has been heavily studied, many
domains lack appropriate training corpora for PoS
tagging. Nevertheless, PoS tagging is an impor-
tant stage in pipelined language processing sys-
tems, from information extractors to speech syn-
thesizers. We show how to use SCL to transfer a
PoS tagger from the Wall Street Journal (financial
news) to MEDLINE (biomedical abstracts), which
use very different vocabularies, and we demon-
strate not only improved PoS accuracy but also
improved end-to-end parsing accuracy while using
the improved tagger.
An important but rarely-explored setting in do-
main adaptation is when we have no labeled
training data for the target domain. We first
demonstrate that in this situation SCL significantly
improves performance over both supervised and
semi-supervised taggers. In the case when some
in-domain labeled training data is available, we
show how to use SCL together with the classifier
combination techniques of Florian et al (2004) to
achieve even greater performance.
In the next section, we describe a motivating
example involving financial news and biomedical
data. Section 3 describes the structural correspon-
dence learning algorithm. Sections 6 and 7 report
results on adapting from the Wall Street Journal to
MEDLINE. We discuss related work on domain
adaptation in section 8 and conclude in section 9.
2 A Motivating Example
Figure 1 shows two PoS-tagged sentences, one
each from the Wall Street Journal (hereafter WSJ)
and MEDLINE. We chose these sentences for two
reasons. First, we wish to visually emphasize the
difference between the two domains. The vocab-
ularies differ significantly, and PoS taggers suf-
fer accordingly. Second, we want to focus on the
(a) An ambiguous instance
JJ vs. NN
with normal signal transduction
(b) MEDLINE occurrences of
signal, together with pivot
features
the signal required to
stimulatory signal from
essential signal for
(c) Corresponding WSJ
words, together with pivot
features
of investment required
of buyouts from buyers
to jail for violating
Figure 2: Correcting an incorrect biomedical tag.
Corresponding words are in bold, and pivot fea-
tures are italicized
phrase ?with normal signal transduction? from the
MEDLINE sentence, depicted in Figure 2(a). The
word ?signal? in this sentence is a noun, but a tag-
ger trained on the WSJ incorrectly classifies it as
an adjective. We introduce the notion of pivot fea-
tures. Pivot features are features which occur fre-
quently in the two domains and behave similarly
in both. Figure 2(b) shows some pivot features
that occur together with the word ?signal? in our
biomedical unlabeled data. In this case our pivot
features are all of type <the token on the
right>. Note that ?signal? is unambiguously a
noun in these contexts. Adjectives rarely precede
past tense verbs such as ?required? or prepositions
such as ?from? and ?for?.
We now search for occurrences of the pivot fea-
tures in the WSJ. Figure 2(c) shows some words
that occur together with the pivot features in the
WSJ unlabeled data. Note that ?investment?,
?buy-outs?, and ?jail? are all common nouns in the
financial domain. Furthermore, since we have la-
beled WSJ data, we expect to be able to label at
least some of these nouns correctly.
This example captures the intuition behind
structural correspondence learning. We want to
use pivot features from our unlabeled data to put
domain-specific words in correspondence. That is,
121
Input: labeled source data {(xt, yt)Tt=1},
unlabeled data from both domains {xj}
Output: predictor f : X ? Y
1. Choose m pivot features. Create m binary
prediction problems, p`(x), ` = 1 . . . m
2. For ` = 1 to m
w?` = argmin
w
?
P
j L(w ? xj , p`(xj))+
?||w||2
?
end
3. W = [w?1| . . . |w?m], [U D V T ] = SVD(W ),
? = UT[1:h,:]
4. Return f , a predictor trained
on
(
??
xt
?xi
?
, yt
?T
t=1
)
Figure 3: SCL Algorithm
we want the pivot features to model the fact that in
the biomedical domain, the word signal behaves
similarly to the words investments, buyouts and
jail in the financial news domain. In practice, we
use this technique to find correspondences among
all features, not just word features.
3 Structural Correspondence Learning
Structural correspondence learning involves a
source domain and a target domain. Both domains
have ample unlabeled data, but only the source do-
main has labeled training data. We refer to the task
for which we have labeled training data as the su-
pervised task. In our experiments, the supervised
task is part of speech tagging. We require that the
input x in both domains be a vector of binary fea-
tures from a finite feature space. The first step of
SCL is to define a set of pivot features on the unla-
beled data from both domains. We then use these
pivot features to learn a mapping ? from the orig-
inal feature spaces of both domains to a shared,
low-dimensional real-valued feature space. A high
inner product in this new space indicates a high de-
gree of correspondence.
During supervised task training, we use both
the transformed and original features from the
source domain. During supervised task testing, we
use the both the transformed and original features
from the target domain. If we learned a good map-
ping ?, then the classifier we learn on the source
domain will also be effective on the target domain.
The SCL algorithm is given in Figure 3, and the
remainder of this section describes it in detail.
3.1 Pivot Features
Pivot features should occur frequently in the un-
labeled data of both domains, since we must esti-
mate their covariance with non-pivot features ac-
curately, but they must also be diverse enough
to adequately characterize the nuances of the su-
pervised task. A good example of this tradeoff
are determiners in PoS tagging. Determiners are
good pivot features, since they occur frequently
in any domain of written English, but choosing
only determiners will not help us to discriminate
between nouns and adjectives. Pivot features cor-
respond to the auxiliary problems of Ando and
Zhang (2005a).
In section 2, we showed example pivot fea-
tures of type <the token on the right>.
We also use pivot features of type <the token
on the left> and <the token in the
middle>. In practice there are many thousands
of pivot features, corresponding to instantiations
of these three types for frequent words in both do-
mains. We choose m pivot features, which we in-
dex with `.
3.2 Pivot Predictors
From each pivot feature we create a binary clas-
sification problem of the form ?Does pivot fea-
ture ` occur in this instance??. One such ex-
ample is ?Is <the token on the right>
required?? These binary classification problems
can be trained from the unlabeled data, since they
merely represent properties of the input. If we rep-
resent our features as a binary vector x, we can
solve these problems using m linear predictors.
f`(x) = sgn(w?` ? x), ` = 1 . . . m
Note that these predictors operate on the original
feature space. This step is shown in line 2 of Fig-
ure 3. Here L(p, y) is a real-valued loss func-
tion for binary classification. We follow Ando and
Zhang (2005a) and use the modified Huber loss.
Since each instance contains features which are
totally predictive of the pivot feature (the feature
itself), we never use these features when making
the binary prediction. That is, we do not use any
feature derived from the right word when solving
a right token pivot predictor.
The pivot predictors are the key element in SCL.
The weight vectors w?` encode the covariance of
the non-pivot features with the pivot features. If
the weight given to the z?th feature by the `?th
122
pivot predictor is positive, then feature z is posi-
tively correlated with pivot feature `. Since pivot
features occur frequently in both domains, we ex-
pect non-pivot features from both domains to be
correlated with them. If two non-pivot features are
correlated in the same way with many of the same
pivot features, then they have a high degree of cor-
respondence. Finally, observe that w?` is a linear
projection of the original feature space onto R.
3.3 Singular Value Decomposition
Since each pivot predictor is a projection onto R,
we could create m new real-valued features, one
for each pivot. For both computational and statis-
tical reasons, though, we follow Ando and Zhang
(2005a) and compute a low-dimensional linear ap-
proximation to the pivot predictor space. Let W
be the matrix whose columns are the pivot pre-
dictor weight vectors. Now let W = UDV T be
the singular value decomposition of W , so that
? = UT[1:h,:] is the matrix whose rows are the top
left singular vectors of W .
The rows of ? are the principal pivot predictors,
which capture the variance of the pivot predictor
space as best as possible in h dimensions. Further-
more, ? is a projection from the original feature
space onto Rh. That is, ?x is the desired mapping
to the (low dimensional) shared feature represen-
tation. This is step 3 of Figure 3.
3.4 Supervised Training and Inference
To perform inference and learning for the super-
vised task, we simply augment the original fea-
ture vector with features obtained by applying the
mapping ?. We then use a standard discrimina-
tive learner on the augmented feature vector. For
training instance t, the augmented feature vector
will contain all the original features xt plus the
new shared features ?xt. If we have designed the
pivots well, then ? should encode correspondences
among features from different domains which are
important for the supervised task, and the classi-
fier we train using these new features on the source
domain will perform well on the target domain.
4 Model Choices
Structural correspondence learning uses the tech-
niques of alternating structural optimization
(ASO) to learn the correlations among pivot and
non-pivot features. Ando and Zhang (2005a) de-
scribe several free paramters and extensions to
ASO, and we briefly address our choices for these
here. We set h, the dimensionality of our low-rank
representation to be 25. As in Ando and Zhang
(2005a), we observed that setting h between 20
and 100 did not change results significantly, and a
lower dimensionality translated to faster run-time.
We also implemented both of the extensions de-
scribed in Ando and Zhang (2005a). The first is
to only use positive entries in the pivot predictor
weight vectors to compute the SVD. This yields
a sparse representation which saves both time and
space, and it also performs better. The second is to
compute block SVDs of the matrix W , where one
block corresponds to one feature type. We used
the same 58 feature types as Ratnaparkhi (1996).
This gave us a total of 1450 projection features for
both semisupervised ASO and SCL.
We found it necessary to make a change to the
ASO algorithm as described in Ando and Zhang
(2005a). We rescale the projection features to al-
low them to receive more weight from a regular-
ized discriminative learner. Without any rescaling,
we were not able to reproduce the original ASO
results. The rescaling parameter is a single num-
ber, and we choose it using heldout data from our
source domain. In all our experiments, we rescale
our projection features to have average L1 norm on
the training set five times that of the binary-valued
features.
Finally, we also make one more change to make
optimization faster. We select only half of the
ASO features for use in the final model. This
is done by running a few iterations of stochas-
tic gradient descent on the PoS tagging problem,
then choosing the features with the largest weight-
variance across the different labels. This cut in
half training time and marginally improved perfor-
mance in all our experiments.
5 Data Sets and Supervised Tagger
5.1 Source Domain: WSJ
We used sections 02-21 of the Penn Treebank
(Marcus et al, 1993) for training. This resulted in
39,832 training sentences. For the unlabeled data,
we used 100,000 sentences from a 1988 subset of
the WSJ.
5.2 Target Domain: Biomedical Text
For unlabeled data we used 200,000 sentences that
were chosen by searching MEDLINE for abstracts
pertaining to cancer, in particular genomic varia-
123
company
transaction
investors
officials yourpretty
short-term
political
receptors mutation
assays
lesions functional
transientneuronal
metastatic
WSJ Only
MEDLINE Only
Figure 4: An example projection of word features onto R. Words on the left (negative valued) behave
similarly to each other for classification, but differently from words on the right (positive valued). The
projection distinguishes nouns from adjectives and determiners in both domains.
tions and mutations. For labeled training and test-
ing purposes we use 1061 sentences that have been
annotated by humans as part of the Penn BioIE
project (PennBioIE, 2005). We use the same 561-
sentence test set in all our experiments. The part-
of-speech tag set for this data is a superset of
the Penn Treebank?s including the two new tags
HYPH (for hyphens) and AFX (for common post-
modifiers of biomedical entities such as genes).
These tags were introduced due to the importance
of hyphenated entities in biomedical text, and are
used for 1.8% of the words in the test set. Any
tagger trained only on WSJ text will automatically
predict wrong tags for those words.
5.3 Supervised Tagger
Since SCL is really a method for inducing a set
of cross-domain features, we are free to choose
any feature-based classifier to use them. For
our experiments we use a version of the discrim-
inative online large-margin learning algorithm
MIRA (Crammer et al, 2006). MIRA learns and
outputs a linear classification score, s(x,y;w) =
w ? f(x,y), where the feature representation f can
contain arbitrary features of the input, including
the correspondence features described earlier. In
particular, MIRA aims to learn weights so that
the score of correct output, yt, for input xt is
separated from the highest scoring incorrect out-
puts2, with a margin proportional to their Ham-
ming losses. MIRA has been used successfully for
both sequence analysis (McDonald et al, 2005a)
and dependency parsing (McDonald et al, 2005b).
As with any structured predictor, we need to
factor the output space to make inference tractable.
We use a first-order Markov factorization, allow-
ing for an efficient Viterbi inference procedure.
2We fix the number of high scoring incorrect outputs to 5.
6 Visualizing ?
In section 2 we claimed that good representations
should encode correspondences between words
like ?signal? from MEDLINE and ?investment?
from the WSJ. Recall that the rows of ? are pro-
jections from the original feature space onto the
real line. Here we examine word features under
these projections. Figure 4 shows a row from
the matrix ?. Applying this projection to a word
gives a real value on the horizontal dashed line
axis. The words below the horizontal axis occur
only in the WSJ. The words above the axis occur
only in MEDLINE. The verticle line in the mid-
dle represents the value zero. Ticks to the left or
right indicate relative positive or negative values
for a word under this projection. This projection
discriminates between nouns (negative) and adjec-
tives (positive). A tagger which gives high pos-
itive weight to the features induced by applying
this projection will be able to discriminate among
the associated classes of biomedical words, even
when it has never observed the words explicitly in
the WSJ source training set.
7 Empirical Results
All the results we present in this section use the
MIRA tagger from Section 5.3. The ASO and
structural correspondence results also use projec-
tion features learned using ASO and SCL. Sec-
tion 7.1 presents results comparing structural cor-
respondence learning with the supervised baseline
and ASO in the case where we have no labeled
data in the target domain. Section 7.2 gives results
for the case where we have some limited data in
the target domain. In this case, we use classifiers
as features as described in Florian et al (2004).
Finally, we show in Section 7.3 that our SCL PoS
124
(a)
100  500  1k 5k 40k75
80
85
90
Results for 561 MEDLINE Test Sentences
Number of WSJ Training Sentences
Ac
cu
ra
cy
supervised
semi?ASO
SCL
(b) Accuracy on 561-sentence test set
Words
Model All Unknown
Ratnaparkhi (1996) 87.2 65.2
supervised 87.9 68.4
semi-ASO 88.4 70.9
SCL 88.9 72.0
(c) Statistical Significance (McNemar?s)
for all words
Null Hypothesis p-value
semi-ASO vs. super 0.0015
SCL vs. super 2.1 ? 10?12
SCL vs. semi-ASO 0.0003
Figure 5: PoS tagging results with no target labeled training data
(a)
50 100 200 500
86
88
90
92
94
96
Number of MEDLINE Training Sentences
Ac
cu
ra
cy
Results for 561 MEDLINE Test Sentences
40k?SCL
40k?super
1k?SCL
1k?super
nosource
(b) 500 target domain training sentences
Model Testing Accuracy
nosource 94.5
1k-super 94.5
1k-SCL 95.0
40k-super 95.6
40k-SCL 96.1
(c) McNemar?s Test (500 training sentences)
Null Hypothesis p-value
1k-super vs. nosource 0.732
1k-SCL vs. 1k-super 0.0003
40k-super vs. nosource 1.9 ? 10?12
40k-SCL vs. 40k-super 6.5 ? 10?7
Figure 6: PoS tagging results with no target labeled training data
tagger improves the performance of a dependency
parser on the target domain.
7.1 No Target Labeled Training Data
For the results in this section, we trained a
structural correspondence learner with 100,000
sentences of unlabeled data from the WSJ and
100,000 sentences of unlabeled biomedical data.
We use as pivot features words that occur more
than 50 times in both domains. The supervised
baseline does not use unlabeled data. The ASO
baseline is an implementation of Ando and Zhang
(2005b). It uses 200,000 sentences of unlabeled
MEDLINE data but no unlabeled WSJ data. For
ASO we used as auxiliary problems words that oc-
cur more than 500 times in the MEDLINE unla-
beled data.
Figure 5(a) plots the accuracies of the three
models with varying amounts of WSJ training
data. With one hundred sentences of training
data, structural correspondence learning gives a
19.1% relative reduction in error over the super-
vised baseline, and it consistently outperforms
both baseline models. Figure 5(b) gives results
for 40,000 sentences, and Figure 5(c) shows cor-
responding significance tests, with p < 0.05 be-
ing significant. We use a McNemar paired test for
labeling disagreements (Gillick and Cox, 1989).
Even when we use all the WSJ training data avail-
able, the SCL model significantly improves accu-
racy over both the supervised and ASO baselines.
The second column of Figure 5(b) gives un-
known word accuracies on the biomedical data.
125
Of thirteen thousand test instances, approximately
three thousand were unknown. For unknown
words, SCL gives a relative reduction in error of
19.5% over Ratnaparkhi (1996), even with 40,000
sentences of source domain training data.
7.2 Some Target Labeled Training Data
In this section we give results for small amounts of
target domain training data. In this case, we make
use of the out-of-domain data by using features of
the source domain tagger?s predictions in training
and testing the target domain tagger (Florian et al,
2004). Though other methods for incorporating
small amounts of training data in the target domain
were available, such as those proposed by Chelba
and Acero (2004) and by Daume? III and Marcu
(2006), we chose this method for its simplicity and
consistently good performance. We use as features
the current predicted tag and all tag bigrams in a
5-token window around the current token.
Figure 6(a) plots tagging accuracy for varying
amounts of MEDLINE training data. The two
horizontal lines are the fixed accuracies of the
SCL WSJ-trained taggers using one thousand and
forty thousand sentences of training data. The five
learning curves are for taggers trained with vary-
ing amounts of target domain training data. They
use features on the outputs of taggers from sec-
tion 7.1. The legend indicates the kinds of features
used in the target domain (in addition to the stan-
dard features). For example, ?40k-SCL? means
that the tagger uses features on the outputs of an
SCL source tagger trained on forty thousand sen-
tences of WSJ data. ?nosource? indicates a tar-
get tagger that did not use any tagger trained on
the source domain. With 1000 source domain sen-
tences and 50 target domain sentences, using SCL
tagger features gives a 20.4% relative reduction
in error over using supervised tagger features and
a 39.9% relative reduction in error over using no
source features.
Figure 6(b) is a table of accuracies for 500 tar-
get domain training sentences, and Figure 6(c)
gives corresponding significance scores. With
1000 source domain sentences and 500 target do-
main sentences, using supervised tagger features
gives no improvement over using no source fea-
tures. Using SCL features still does, however.
7.3 Improving Parser Performance
We emphasize the importance of PoS tagging in a
pipelined NLP system by incorporating our SCL
100  500  1k 5k 40k
58
62
66
70
74
78
82
Dependency Parsing for 561 Test Sentences
Number of WSJ Training Sentences
Ac
cu
ra
cy
supervised
SCL
gold
Figure 7: Dependency parsing results using differ-
ent part of speech taggers
tagger into a WSJ-trained dependency parser and
and evaluate it on MEDLINE data. We use the
parser described by McDonald et al (2005b). That
parser assumes that a sentence has been PoS-
tagged before parsing. We train the parser and PoS
tagger on the same size of WSJ data.
Figure 7 shows dependency parsing accuracy on
our 561-sentence MEDLINE test set. We parsed
the sentences using the PoS tags output by our
source domain supervised tagger, the SCL tagger
from subsection 7.1, and the gold PoS tags. All
of the differences in this figure are significant ac-
cording to McNemar?s test. The SCL tags consis-
tently improve parsing performance over the tags
output by the supervised tagger. This is a rather in-
direct method of improving parsing performance
with SCL. In the future, we plan on directly incor-
porating SCL features into a discriminative parser
to improve its adaptation properties.
8 Related Work
Domain adaptation is an important and well-
studied area in natural language processing. Here
we outline a few recent advances. Roark and Bac-
chiani (2003) use a Dirichlet prior on the multi-
nomial parameters of a generative parsing model
to combine a large amount of training data from a
source corpus (WSJ), and small amount of train-
ing data from a target corpus (Brown). Aside
from Florian et al (2004), several authors have
also given techniques for adapting classification to
new domains. Chelba and Acero (2004) first train
a classifier on the source data. Then they use max-
imum a posteriori estimation of the weights of a
126
maximum entropy target domain classifier. The
prior is Gaussian with mean equal to the weights
of the source domain classifier. Daume? III and
Marcu (2006) use an empirical Bayes model to es-
timate a latent variable model grouping instances
into domain-specific or common across both do-
mains. They also jointly estimate the parameters
of the common classification model and the do-
main specific classification models. Our work fo-
cuses on finding a common representation for fea-
tures from different domains, not instances. We
believe this is an important distinction, since the
same instance can contain some features which are
common across domains and some which are do-
main specific.
The key difference between the previous four
pieces of work and our own is the use of unlabeled
data. We do not require labeled training data in
the new domain to demonstrate an improvement
over our baseline models. We believe this is essen-
tial, since many domains of application in natural
language processing have no labeled training data.
Lease and Charniak (2005) adapt a WSJ parser
to biomedical text without any biomedical tree-
banked data. However, they assume other labeled
resources in the target domain. In Section 7.3 we
give similar parsing results, but we adapt a source
domain tagger to obtain the PoS resources.
To the best of our knowledge, SCL is the first
method to use unlabeled data from both domains
for domain adaptation. By using just the unlabeled
data from the target domain, however, we can view
domain adaptation as a standard semisupervised
learning problem. There are many possible ap-
proaches for semisupservised learning in natural
language processing, and it is beyond the scope
of this paper to address them all. We chose to
compare with ASO because it consistently outper-
forms cotraining (Blum and Mitchell, 1998) and
clustering methods (Miller et al, 2004). We did
run experiments with the top-k version of ASO
(Ando and Zhang, 2005a), which is inspired by
cotraining but consistently outperforms it. This
did not outperform the supervised method for do-
main adaptation. We speculate that this is because
biomedical and financial data are quite different.
In such a situation, bootstrapping techniques are
likely to introduce too much noise from the source
domain to be useful.
Structural correspondence learning is most sim-
ilar to that of Ando (2004), who analyzed a
situation with no target domain labeled data.
Her model estimated co-occurrence counts from
source unlabeled data and then used the SVD of
this matrix to generate features for a named en-
tity recognizer. Our ASO baseline uses unlabeled
data from the target domain. Since this consis-
tently outperforms unlabeled data from only the
source domain, we report only these baseline re-
sults. To the best of our knowledge, this is the first
work to use unlabeled data from both domains to
find feature correspondences.
One important advantage that this work shares
with Ando (2004) is that an SCL model can be
easily combined with all other domain adaptation
techniques (Section 7.2). We are simply induc-
ing a feature representation that generalizes well
across domains. This feature representation can
then be used in all the techniques described above.
9 Conclusion
Structural correspondence learning is a marriage
of ideas from single domain semi-supervised
learning and domain adaptation. It uses unla-
beled data and frequently-occurring pivot features
from both source and target domains to find corre-
spondences among features from these domains.
Finding correspondences involves estimating the
correlations between pivot and non-pivot feautres,
and we adapt structural learning (ASO) (Ando and
Zhang, 2005a; Ando and Zhang, 2005b) for this
task. SCL is a general technique that can be ap-
plied to any feature-based discriminative learner.
We showed results using SCL to transfer a PoS
tagger from the Wall Street Journal to a corpus
of MEDLINE abstracts. SCL consistently out-
performed both supervised and semi-supervised
learning with no labeled target domain training
data. We also showed how to combine an SCL
tagger with target domain labeled data using the
classifier combination techniques from Florian et
al. (2004). Finally, we improved parsing perfor-
mance in the target domain when using the SCL
PoS tagger.
One of our next goals is to apply SCL directly
to parsing. We are also focusing on other po-
tential applications, including chunking (Sha and
Pereira, 2003), named entity recognition (Florian
et al, 2004; Ando and Zhang, 2005b; Daume? III
and Marcu, 2006), and speaker adaptation (Kuhn
et al, 1998). Finally, we are investigating more
direct ways of applying structural correspondence
127
learning when we have labeled data from both
source and target domains. In particular, the la-
beled data of both domains, not just the unlabeled
data, should influence the learned representations.
Acknowledgments
We thank Rie Kubota Ando and Tong Zhang
for their helpful advice on ASO, Steve Carroll
and Pete White of The Children?s Hospital of
Philadelphia for providing the MEDLINE data,
and the PennBioIE annotation team for the anno-
tated MEDLINE data used in our test sets. This
material is based upon work partially supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. NBCHD030010.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this material are those
of the author(s) and do not necessarily reflect
the views of the DARPA or the Department
of Interior-National Business Center (DOI-NBC).
Additional support was provided by NSF under
ITR grant EIA-0205448.
References
R. Ando and T. Zhang. 2005a. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. JMLR, 6:1817?1853.
R. Ando and T. Zhang. 2005b. A high-performance
semi-supervised learning method for text chunking.
In ACL.
R. Ando. 2004. Exploiting unannotated corpora for
tagging and chunking. In ACL. Short paper.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. JMLR, 3:993?1022.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Workshop
on Computational Learning Theory.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467?479.
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot.
In EMNLP.
K. Crammer, Dekel O, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
H. Daum e? III and D. Marcu. 2006. Domain adaptation
for statistical classifiers. JAIR.
R. Florian, H. Hassan, A.Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.
2004. A statistical model for multilingual entity de-
tection and tracking. In of HLT-NAACL.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
ICASSP.
R. Kuhn, P. Nguyen, J.C. Junqua, L. Goldwasser,
N. Niedzielski, S. Fincke, K. Field, and M. Con-
tolini. 1998. Eigenvoices for speaker adaptation.
In ICSLP.
M. Lease and E. Charniak. 2005. Parsing biomedical
literature. In IJCNLP.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005a.
Flexible text segmentation with structured multil-
abel classification. In HLT-EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In
ACL.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative train-
ing. In HLT-NAACL.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
PennBioIE. 2005. Mining The Bibliome Project.
http://bioie.ldc.upenn.edu/.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of english words. In ACL.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP.
B. Roark and M. Bacchiani. 2003. Supervised and
unsupervised PCFG adaptation to novel domains. In
HLT-NAACL.
B. Roark, M. Saraclar, M. Collins, and M. Johnson.
2004. Discriminative language modeling with con-
ditional random fields and the perceptron algorithm.
In ACL.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In HLT-NAACL.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL.
128
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 216?220, New York City, June 2006. c?2006 Association for Computational Linguistics
Multilingual Dependency Analysis with a Two-Stage Discriminative Parser
Ryan McDonald Kevin Lerman Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA
{ryantm,klerman,pereira}@cis.upenn.edu
Abstract
We present a two-stage multilingual de-
pendency parser and evaluate it on 13
diverse languages. The first stage is
based on the unlabeled dependency pars-
ing models described by McDonald and
Pereira (2006) augmented with morpho-
logical features for a subset of the lan-
guages. The second stage takes the out-
put from the first and labels all the edges
in the dependency graph with appropri-
ate syntactic categories using a globally
trained sequence classifier over compo-
nents of the graph. We report results on
the CoNLL-X shared task (Buchholz et
al., 2006) data sets and present an error
analysis.
1 Introduction
Often in language processing we require a deep syn-
tactic representation of a sentence in order to assist
further processing. With the availability of resources
such as the Penn WSJ Treebank, much of the fo-
cus in the parsing community had been on producing
syntactic representations based on phrase-structure.
However, recently their has been a revived interest
in parsing models that produce dependency graph
representations of sentences, which model words
and their arguments through directed edges (Hud-
son, 1984; Mel?c?uk, 1988). This interest has gener-
ally come about due to the computationally efficient
and flexible nature of dependency graphs and their
ability to easily model non-projectivity in freer-word
order languages. Nivre (2005) gives an introduction
to dependency representations of sentences and re-
cent developments in dependency parsing strategies.
Dependency graphs also encode much of the deep
syntactic information needed for further process-
ing. This has been shown through their success-
ful use in many standard natural language process-
ing tasks, including machine translation (Ding and
Palmer, 2005), sentence compression (McDonald,
2006), and textual inference (Haghighi et al, 2005).
In this paper we describe a two-stage discrimi-
native parsing approach consisting of an unlabeled
parser and a subsequent edge labeler. We evaluate
this parser on a diverse set of 13 languages using
data provided by the CoNLL-X shared-task organiz-
ers (Buchholz et al, 2006; Hajic? et al, 2004; Simov
et al, 2005; Simov and Osenova, 2003; Chen et al,
2003; Bo?hmova? et al, 2003; Kromann, 2003; van
der Beek et al, 2002; Brants et al, 2002; Kawata
and Bartels, 2000; Afonso et al, 2002; Dz?eroski et
al., 2006; Civit Torruella and Mart?? Anton??n, 2002;
Nilsson et al, 2005; Oflazer et al, 2003; Atalay et
al., 2003). The results are promising and show the
language independence of our system under the as-
sumption of a labeled dependency corpus in the tar-
get language.
For the remainder of this paper, we denote by
x = x1, . . . xn a sentence with n words and by
y a corresponding dependency graph. A depen-
dency graph is represented by a set of ordered pairs
(i, j) ? y in which xj is a dependent and xi is the
corresponding head. Each edge can be assigned a la-
bel l(i,j) from a finite set L of predefined labels. We
216
assume that all dependency graphs are trees but may
be non-projective, both of which are true in the data
sets we use.
2 Stage 1: Unlabeled Parsing
The first stage of our system creates an unlabeled
parse y for an input sentence x. This system is
primarily based on the parsing models described
by McDonald and Pereira (2006). That work ex-
tends the maximum spanning tree dependency pars-
ing framework (McDonald et al, 2005a; McDonald
et al, 2005b) to incorporate features over multiple
edges in the dependency graph. An exact projec-
tive and an approximate non-projective parsing al-
gorithm are presented, since it is shown that non-
projective dependency parsing becomes NP-hard
when features are extended beyond a single edge.
That system uses MIRA, an online large-margin
learning algorithm, to compute model parameters.
Its power lies in the ability to define a rich set of fea-
tures over parsing decisions, as well as surface level
features relative to these decisions. For instance, the
system of McDonald et al (2005a) incorporates fea-
tures over the part of speech of words occurring be-
tween and around a possible head-dependent rela-
tion. These features are highly important to over-
all accuracy since they eliminate unlikely scenarios
such as a preposition modifying a noun not directly
to its left, or a noun modifying a verb with another
verb occurring between them.
We augmented this model to incorporate morpho-
logical features derived from each token. Consider a
proposed dependency of a dependent xj on the head
xi, each with morphological features Mj and Mi re-
spectively. We then add to the representation of the
edge: Mi as head features, Mj as dependent fea-
tures, and also each conjunction of a feature from
both sets. These features play the obvious role of
explicitly modeling consistencies and commonali-
ties between a head and its dependents in terms of
attributes like gender, case, or number. Not all data
sets in our experiments include morphological fea-
tures, so we use them only when available.
3 Stage 2: Label Classification
The second stage takes the output parse y for sen-
tence x and classifies each edge (i, j) ? y with a
particular label l(i,j). Ideally one would like to make
all parsing and labeling decisions jointly so that the
shared knowledge of both decisions will help resolve
any ambiguities. However, the parser is fundamen-
tally limited by the scope of local factorizations that
make inference tractable. In our case this means
we are forced only to consider features over single
edges or pairs of edges. However, in a two stage
system we can incorporate features over the entire
output of the unlabeled parser since that structure is
fixed as input. The simplest labeler would be to take
as input an edge (i, j) ? y for sentence x and find
the label with highest score,
l(i,j) = argmax
l
s(l, (i, j),y,x)
Doing this for each edge in the tree would pro-
duce the final output. Such a model could easily be
trained using the provided training data for each lan-
guage. However, it might be advantageous to know
the labels of other nearby edges. For instance, if we
consider a head xi with dependents xj1 , . . . , xjM , it
is often the case that many of these dependencies
will have correlated labels. To model this we treat
the labeling of the edges (i, j1), . . . , (i, jM ) as a se-
quence labeling problem,
(l(i,j1), . . . , l(i,jM )) = l? = argmax
l?
s(l?, i,y,x)
We use a first-order Markov factorization of the
score
l? = argmax
l?
M
?
m=2
s(l(i,jm), l(i,jm?1), i,y,x)
in which each factor is the score of labeling the adja-
cent edges (i, jm) and (i, jm?1) in the tree y. We at-
tempted higher-order Markov factorizations but they
did not improve performance uniformly across lan-
guages and training became significantly slower.
For score functions, we use simple dot products
between high dimensional feature representations
and a weight vector
s(l(i,jm), l(i,jm?1), i,y,x) =
w ? f(l(i,jm), l(i,jm?1), i,y,x)
Assuming we have an appropriate feature repre-
sentation, we can find the highest scoring label se-
quence with Viterbi?s algorithm. We use the MIRA
217
online learner to set the weights (Crammer and
Singer, 2003; McDonald et al, 2005a) since we
found it trained quickly and provide good perfor-
mance. Furthermore, it made the system homoge-
neous in terms of learning algorithms since that is
what is used to train our unlabeled parser (McDon-
ald and Pereira, 2006). Of course, we have to define
a set of suitable features. We used the following:
? Edge Features: Word/pre-suffix/part-of-speech
(POS)/morphological feature identity of the head and the
dependent (affix lengths 2 and 3). Does the head and its
dependent share a prefix/suffix? Attachment direction.
What morphological features do head and dependent
have the same value for? Is the dependent the first/last
word in the sentence?
? Sibling Features: Word/POS/pre-suffix/morphological
feature identity of the dependent?s nearest left/right sib-
lings in the tree (siblings are words with same parent in
the tree). Do any of the dependent?s siblings share its
POS?
? Context Features: POS tag of each intervening word be-
tween head and dependent. Do any of the words between
the head and the dependent have a parent other than the
head? Are any of the words between the head and the de-
pendent not a descendant of the head (i.e. non-projective
edge)?
? Non-local: How many children does the dependent have?
What morphological features do the grandparent and the
dependent have identical values? Is this the left/right-
most dependent for the head? Is this the first dependent
to the left/right of the head?
Various conjunctions of these were included
based on performance on held-out data. Note that
many of these features are beyond the scope of the
edge based factorizations of the unlabeled parser.
Thus a joint model of parsing and labeling could not
easily include them without some form of re-ranking
or approximate parameter estimation.
4 Results
We trained models for all 13 languages provided
by the CoNLL organizers (Buchholz et al, 2006).
Based on performance from a held-out section of the
training data, we used non-projective parsing algo-
rithms for Czech, Danish, Dutch, German, Japanese,
Portuguese and Slovene, and projective parsing al-
gorithms for Arabic, Bulgarian, Chinese, Spanish,
Swedish and Turkish. Furthermore, for Arabic and
Spanish, we used lemmas instead of inflected word
DATA SET UA LA
ARABIC 79.3 66.9
BULGARIAN 92.0 87.6
CHINESE 91.1 85.9
CZECH 87.3 80.2
DANISH 90.6 84.8
DUTCH 83.6 79.2
GERMAN 90.4 87.3
JAPANESE 92.8 90.7
PORTUGUESE 91.4 86.8
SLOVENE 83.2 73.4
SPANISH 86.1 82.3
SWEDISH 88.9 82.5
TURKISH 74.7 63.2
AVERAGE 87.0 80.8
Table 1: Dependency accuracy on 13 languages.
Unlabeled (UA) and Labeled Accuracy (LA).
forms, again based on performance on held-out
data1.
Results on the test set are given in Table 1. Per-
formance is measured through unlabeled accuracy,
which is the percentage of words that modify the
correct head in the dependency graph, and labeled
accuracy, which is the percentage of words that
modify the correct head and label the dependency
edge correctly in the graph. These results show that
the discriminative spanning tree parsing framework
(McDonald et al, 2005b; McDonald and Pereira,
2006) is easily adapted across all these languages.
Only Arabic, Turkish and Slovene have parsing ac-
curacies significantly below 80%, and these lan-
guages have relatively small training sets and/or are
highly inflected with little to no word order con-
straints. Furthermore, these results show that a two-
stage system can achieve a relatively high perfor-
mance. In fact, for every language our models per-
form significantly higher than the average perfor-
mance for all the systems reported in Buchholz et
al. (2006).
For the remainder of the paper we provide a gen-
eral error analysis across a wide set of languages
plus a detailed error analysis of Spanish and Arabic.
5 General Error Analysis
Our system has several components, including the
ability to produce non-projective edges, sequential
1Using the non-projective parser for all languages does not
effect performance significantly. Similarly, using the inflected
word form instead of the lemma for all languages does not
change performance significantly.
218
SYSTEM UA LA
N+S+M 86.3 79.7
P+S+M 85.6 79.2
N+S+B 85.5 78.6
N+A+M 86.3 79.4
P+A+B 84.8 77.7
Table 2: Error analysis of parser components av-
eraged over Arabic, Bulgarian, Danish, Dutch,
Japanese, Portuguese, Slovene, Spanish, Swedish
and Turkish. N/P: Allow non-projective/Force pro-
jective, S/A: Sequential labeling/Atomic labeling,
M/B: Include morphology features/No morphology
features.
assignment of edge labels instead of individual as-
signment, and a rich feature set that incorporates
morphological properties when available. The bene-
fit of each of these is shown in Table 2. These results
report the average labeled and unlabeled precision
for the 10 languages with the smallest training sets.
This allowed us to train new models quickly.
Table 2 shows that each component of our system
does not change performance significantly (rows 2-
4 versus row 1). However, if we only allow projec-
tive parses, do not use morphological features and
label edges with a simple atomic classifier, the over-
all drop in performance becomes significant (row
5 versus row 1). Allowing non-projective parses
helped with freer word order languages like Dutch
(78.8%/74.7% to 83.6%/79.2%, unlabeled/labeled
accuracy). Including rich morphology features natu-
rally helped with highly inflected languages, in par-
ticular Spanish, Arabic, Turkish, Slovene and to a
lesser extent Dutch and Portuguese. Derived mor-
phological features improved accuracy in all these
languages by 1-3% absolute.
Sequential classification of labels had very lit-
tle effect on overall labeled accuracy (79.4% to
79.7%)2. The major contribution was in helping to
distinguish subjects, objects and other dependents
of main verbs, which is the most common label-
ing error. This is not surprising since these edge
labels typically are the most correlated (i.e., if you
already know which noun dependent is the subject,
then it should be easy to find the object). For in-
stance, sequential labeling improves the labeling of
2This difference was much larger for experiments in which
gold standard unlabeled dependencies are used.
objects from 81.7%/75.6% to 84.2%/81.3% (la-
beled precision/recall) and the labeling of subjects
from 86.8%/88.2% to 90.5%/90.4% for Swedish.
Similar improvements are common across all lan-
guages, though not as dramatic. Even with this im-
provement, the labeling of verb dependents remains
the highest source of error.
6 Detailed Analysis
6.1 Spanish
Although overall unlabeled accuracy is 86%, most
verbs and some conjunctions attach to their head
words with much lower accuracy: 69% for main
verbs, 75% for the verb ser, and 65% for coor-
dinating conjunctions. These words form 17% of
the test corpus. Other high-frequency word classes
with relatively low attachment accuracy are preposi-
tions (80%), adverbs (82%) and subordinating con-
junctions (80%), for a total of another 23% of the
test corpus. These weaknesses are not surprising,
since these decisions encode the more global as-
pects of sentence structure: arrangement of clauses
and adverbial dependents in multi-clause sentences,
and prepositional phrase attachment. In a prelimi-
nary test of this hypothesis, we looked at all of the
sentences from a development set in which a main
verb is incorrectly attached. We confirmed that the
main clause is often misidentified in multi-clause
sentences, or that one of several conjoined clauses
is incorrectly taken as the main clause. To test this
further, we added features to count the number of
commas and conjunctions between a dependent verb
and its candidate head. Unlabeled accuracy for all
verbs increases from 71% to 73% and for all con-
junctions from 71% to 74%. Unfortunately, accu-
racy for other word types decreases somewhat, re-
sulting in no significant net accuracy change. Nev-
ertheless, this very preliminary experiment suggests
that wider-range features may be useful in improv-
ing the recognition of overall sentence structure.
Another common verb attachment error is a
switch between head and dependent verb in phrasal
verb forms like dejan intrigar or qiero decir, possi-
bly because the non-finite verb in these cases is often
a main verb in training sentences. We need to look
more carefully at verb features that may be useful
here, in particular features that distinguish finite and
219
non-finite forms.
In doing this preliminary analysis, we noticed
some inconsistencies in the reference dependency
structures. For example, in the test sentence Lo
que decia Mae West de si misma podr??amos decirlo
tambie?n los hombres:..., decia?s head is given as de-
cirlo, although the main verbs of relative clauses are
normally dependent on what the relative modifies, in
this case the article Lo.
6.2 Arabic
A quick look at unlabeled attachment accuracies in-
dicate that errors in Arabic parsing are the most
common across all languages: prepositions (62%),
conjunctions (69%) and to a lesser extent verbs
(73%). Similarly, for labeled accuracy, the hard-
est edges to label are for dependents of verbs, i.e.,
subjects, objects and adverbials. Note the differ-
ence in error between the unlabeled parser and the
edge labeler: the former makes mistakes on edges
into prepositions, conjunctions and verbs, and the
latter makes mistakes on edges into nouns (sub-
ject/objects). Each stage by itself is relatively ac-
curate (unlabeled accuracy is 79% and labeling ac-
curacy3 is also 79%), but since there is very little
overlap in the kinds of errors each makes, overall la-
beled accuracy drops to 67%. This drop is not nearly
as significant for other languages.
Another source of potential error is that the aver-
age sentence length of Arabic is much higher than
other languages (around 37 words/sentence). How-
ever, if we only look at performance for sentences
of length less than 30, the labeled accuracy is still
only 71%. The fact that Arabic has only 1500 train-
ing instances might also be problematic. For exam-
ple if we train on 200, 400, 800 and the full training
set, labeled accuracies are 54%, 60%, 62% and 67%.
Clearly adding more data is improving performance.
However, when compared to the performance of
Slovene (1500 training instances) and Spanish (3300
instances), it appears that Arabic parsing is lagging.
7 Conclusions
We have presented results showing that the spanning
tree dependency parsing framework of McDonald et
3Labeling accuracy is the percentage of words that correctly
label the dependency between the head that they modify, even
if the right head was not identified.
al. (McDonald et al, 2005b; McDonald and Pereira,
2006) generalizes well to languages other than En-
glish. In the future we plan to extend these mod-
els in two ways. First, we plan on examining the
performance difference between two-staged depen-
dency parsing (as presented here) and joint parsing
plus labeling. It is our hypothesis that for languages
with fine-grained label sets, joint parsing and label-
ing will improve performance. Second, we plan on
integrating any available morphological features in
a more principled manner. The current system sim-
ply includes all morphological bi-gram features. It
is our hope that a better morphological feature set
will help with both unlabeled parsing and labeling
for highly inflected languages.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. SIGNLL.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In Proc. HTL-
EMNLP.
R. Hudson. 1984. Word Grammar. Blackwell.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT-EMNLP.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. EACL.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
J. Nivre. 2005. Dependency grammar and dependency
parsing. Technical Report MSI report 05133, Va?xjo?
University: School of Mathematics and Systems Engi-
neering.
220
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 122?131, Prague, June 2007. c?2007 Association for Computational Linguistics
Characterizing the Errors of Data-Driven Dependency Parsing Models
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Joakim Nivre
Va?xjo? University Uppsala University
35195 Va?xjo? 75126 Uppsala
Sweden Sweden
nivre@msi.vxu.se
Abstract
We present a comparative error analysis
of the two dominant approaches in data-
driven dependency parsing: global, exhaus-
tive, graph-based models, and local, greedy,
transition-based models. We show that, in
spite of similar performance overall, the two
models produce different types of errors, in
a way that can be explained by theoretical
properties of the two models. This analysis
leads to new directions for parser develop-
ment.
1 Introduction
Syntactic dependency representations have a long
history in descriptive and theoretical linguistics and
many formal models have been advanced (Hudson,
1984; Mel?c?uk, 1988; Sgall et al, 1986; Maruyama,
1990). A dependency graph of a sentence repre-
sents each word and its syntactic modifiers through
labeled directed arcs, as shown in Figure 1, taken
from the Prague Dependency Treebank (Bo?hmova? et
al., 2003). A primary advantage of dependency rep-
resentations is that they have a natural mechanism
for representing discontinuous constructions, aris-
ing from long distance dependencies or free word
order, through non-projective dependency arcs, ex-
emplified by the arc from jedna to Z in Figure 1.
Syntactic dependency graphs have recently
gained a wide interest in the computational lin-
guistics community and have been successfully em-
ployed for many problems ranging from machine
translation (Ding and Palmer, 2004) to ontology
Figure 1: Example dependency graph.
construction (Snow et al, 2004). In this work we
focus on a common parsing paradigm called data-
driven dependency parsing. Unlike grammar-based
parsing, data-driven approaches learn to produce de-
pendency graphs for sentences solely from an anno-
tated corpus. The advantage of such models is that
they are easily ported to any domain or language in
which annotated resources exist.
As evident from the CoNLL-X shared task on de-
pendency parsing (Buchholz and Marsi, 2006), there
are currently two dominant models for data-driven
dependency parsing. The first is what Buchholz and
Marsi (2006) call the ?all-pairs? approach, where ev-
ery possible arc is considered in the construction of
the optimal parse. The second is the ?stepwise? ap-
proach, where the optimal parse is built stepwise and
where the subset of possible arcs considered depend
on previous decisions. Theoretically, these models
are extremely different. The all-pairs models are
globally trained, use exact (or near exact) inference
algorithms, and define features over a limited history
of parsing decisions. The stepwise models use local
training and greedy inference algorithms, but define
features over a rich history of parse decisions. How-
ever, both models obtain similar parsing accuracies
122
McDonald Nivre
Arabic 66.91 66.71
Bulgarian 87.57 87.41
Chinese 85.90 86.92
Czech 80.18 78.42
Danish 84.79 84.77
Dutch 79.19 78.59
German 87.34 85.82
Japanese 90.71 91.65
Portuguese 86.82 87.60
Slovene 73.44 70.30
Spanish 82.25 81.29
Swedish 82.55 84.58
Turkish 63.19 65.68
Overall 80.83 80.75
Table 1: Labeled parsing accuracy for top scoring
systems at CoNLL-X (Buchholz and Marsi, 2006).
on a variety of languages, as seen in Table 1, which
shows results for the two top performing systems in
the CoNLL-X shared task, McDonald et al (2006)
(?all-pairs?) and Nivre et al (2006) (?stepwise?).
Despite the similar performance in terms of over-
all accuracy, there are indications that the two types
of models exhibit different behaviour. For example,
Sagae and Lavie (2006) displayed that combining
the predictions of both parsing models can lead to
significantly improved accuracies. In order to pave
the way for new and better methods, a much more
detailed error analysis is needed to understand the
strengths and weaknesses of different approaches.
In this work we set out to do just that, focusing on
the two top performing systems from the CoNLL-X
shared task as representatives of the two dominant
models in data-driven dependency parsing.
2 Two Models for Dependency Parsing
2.1 Preliminaries
Let L = {l1, . . . , l|L|} be a set of permissible arc
labels. Let x = w0, w1, . . . , wn be an input sen-
tence wherew0=root. Formally, a dependency graph
for an input sentence x is a labeled directed graph
G = (V,A) consisting of a set of nodes V and a
set of labeled directed arcs A ? V ? V ? L, i.e., if
(i, j, l) ? A for i, j ? V and l ? L, then there is an
arc from node i to node j with label l in the graph.
A dependency graph G for sentence x must satisfy
the following properties:
1. V = {0, 1, . . . , n}
2. If (i, j, l) ? A, then j 6= 0.
3. If (i, j, l) ? A, then for all i? ? V ? {i} and
l? ? L, (i?, j, l?) /? A.
4. For all j ? V ?{0}, there is a (possibly empty)
sequence of nodes i1, . . . , im?V and labels
l1, . . . , lm, l?L such that (0, i1, l1),(i1, i2, l2),
. . . , (im, j, l)?A.
The constraints state that the dependency graph
spans the entire input (1); that the node 0 is a root
(2); that each node has at most one incoming arc
in the graph (3); and that the graph is connected
through directed paths from the node 0 to every other
node in the graph (4). A dependency graph satisfy-
ing these constraints is a directed tree originating out
of the root node 0. We say that an arc (i, j, l) is non-
projective if not all words k occurring between i and
j in the linear order are dominated by i (where dom-
inance is the transitive closure of the arc relation).
2.2 Global, Exhaustive, Graph-Based Parsing
For an input sentence, x = w0, w1, . . . , wn consider
the dense graph Gx = (Vx, Ax) where:
1. Vx = {0, 1, . . . , n}
2. Ax = {(i, j, l) | ? i, j ? Vx and l ? L}
Let D(Gx) represent the subgraphs of graph Gx
that are valid dependency graphs for the sentence
x. Since Gx contains all possible labeled arcs, the
set D(Gx) must necessarily contain all valid depen-
dency graphs for x.
Assume that there exists a dependency arc scoring
function, s : V ? V ? L ? R. Furthermore, define
the score of a graph as the sum of its arc scores,
s(G = (V,A)) =
?
(i,j,l)?A
s(i, j, l)
The score of a dependency arc, s(i, j, l) represents
the likelihood of creating a dependency from word
wi to word wj with the label l. If the arc score func-
tion is known a priori, then the parsing problem can
be stated as,
123
G = argmax
G?D(Gx)
s(G) = argmax
G?D(Gx)
?
(i,j,l)?A
s(i, j, l)
This problem is equivalent to finding the highest
scoring directed spanning tree in the graph Gx origi-
nating out of the root node 0, which can be solved for
both the labeled and unlabeled case in O(n2) time
(McDonald et al, 2005b). In this approach, non-
projective arcs are produced naturally through the
inference algorithm that searches over all possible
directed trees, whether projective or not.
The parsing models of McDonald work primarily
in this framework. To learn arc scores, these mod-
els use large-margin structured learning algorithms
(McDonald et al, 2005a), which optimize the pa-
rameters of the model to maximize the score mar-
gin between the correct dependency graph and all
incorrect dependency graphs for every sentence in a
training set. The learning procedure is global since
model parameters are set relative to the classification
of the entire dependency graph, and not just over sin-
gle arc attachment decisions. The primary disadvan-
tage of these models is that the feature representa-
tion is restricted to a limited number of graph arcs.
This restriction is required so that both inference and
learning are tractable.
The specific model studied in this work is that
presented by McDonald et al (2006), which factors
scores over pairs of arcs (instead of just single arcs)
and uses near exhaustive search for unlabeled pars-
ing coupled with a separate classifier to label each
arc. We call this system MSTParser, which is also
the name of the freely available implementation.1
2.3 Local, Greedy, Transition-Based Parsing
A transition system for dependency parsing defines
1. a set C of parser configurations, each of which
defines a (partially built) dependency graph G
2. a set T of transitions, each a function t :C?C
3. for every sentence x = w0, w1, . . . , wn,
(a) a unique initial configuration cx
(b) a set Cx of terminal configurations
1http://mstparser.sourceforge.net
A transition sequence Cx,m = (cx, c1, . . . , cm) for a
sentence x is a sequence of configurations such that
cm ? Cx and, for every ci (ci 6= cx), there is a tran-
sition t ? T such that ci = t(ci?1). The dependency
graph assigned to x byCx,m is the graphGm defined
by the terminal configuration cm.
Assume that there exists a transition scoring func-
tion, s : C ? T ? R. The score of a transition
t in a configuration c, s(c, t), represents the likeli-
hood of taking transition t out of configuration c.
The parsing problem consists in finding a terminal
configuration cm ? Cx, starting from the initial
configuration cx and taking the optimal transition
t? = argmaxt?T s(c, t) out of every configuration
c. This can be seen as a greedy search for the optimal
dependency graph, based on a sequence of locally
optimal decisions in terms of the transition system.
Many transition systems for data-driven depen-
dency parsing are inspired by shift-reduce parsing,
where configurations contain a stack for storing par-
tially processed nodes. Transitions in such systems
add arcs to the dependency graph and/or manipu-
late the stack. One example is the transition system
defined by Nivre (2003), which parses a sentence
x = w0, w1, . . . , wn in O(n) time, producing a pro-
jective dependency graph satisfying conditions 1?4
in section 2.1, possibly after adding arcs (0, i, lr)
for every node i 6= 0 that is a root in the output
graph (where lr is a special label for root modifiers).
Nivre and Nilsson (2005) showed how the restric-
tion to projective dependency graphs could be lifted
by using graph transformation techniques to pre-
process training data and post-process parser output,
so-called pseudo-projective parsing.
To learn transition scores, these systems use dis-
criminative learning methods, e.g., memory-based
learning or support vector machines. The learning
procedure is local since only single transitions are
scored, not entire transition sequences. The primary
advantage of these models is that features are not re-
stricted to a limited number of graph arcs but can
take into account the entire dependency graph built
so far. The main disadvantage is that the greedy
parsing strategy may lead to error propagation.
The specific model studied in this work is that pre-
sented by Nivre et al (2006), which uses labeled
pseudo-projective parsing with support vector ma-
chines. We call this systemMaltParser, which is also
124
the name of the freely available implementation.2
2.4 Comparison
These models differ primarily with respect to three
important properties.
1. Inference: MaltParser uses a transition-based
inference algorithm that greedily chooses the
best parsing decision based on a trained clas-
sifier and current parser history. MSTParser
instead uses near exhaustive search over a
dense graphical representation of the sentence
to find the dependency graph that maximizes
the score.
2. Training: MaltParser trains a model to make
a single classification decision (choose the next
transition). MSTParser trains a model to maxi-
mize the global score of correct graphs.
3. Feature Representation: MaltParser can in-
troduce a rich feature history based on previ-
ous parser decisions. MSTParser is forced to
restrict the score of features to a single or pair
of nearby parsing decisions in order to make
exhaustive inference tractable.
These differences highlight an inherent trade-off be-
tween exhaustive inference algorithms plus global
learning and expressiveness of feature representa-
tions. MSTParser favors the former at the expense
of the latter and MaltParser the opposite.
3 The CoNLL-X Shared Task
The CoNLL-X shared task (Buchholz and Marsi,
2006) was a large-scale evaluation of data-driven de-
pendency parsers, with data from 13 different lan-
guages and 19 participating systems. The official
evaluation metric was the labeled attachment score
(LAS), defined as the percentage of tokens, exclud-
ing punctuation, that are assigned both the correct
head and the correct dependency label.3
The output of all systems that participated in the
shared task are available for download and consti-
tute a rich resource for comparative error analysis.
2http://w3.msi.vxu.se/users/nivre/research/MaltParser.html
3In addition, results were reported for unlabeled attachment
score (UAS) (tokens with the correct head) and label accuracy
(LA) (tokens with the correct label).
The data used in the experiments below are the out-
puts of MSTParser and MaltParser for all 13 lan-
guages, together with the corresponding gold stan-
dard graphs used in the evaluation. We constructed
the data by simply concatenating a system?s output
for every language. This resulted in a single out-
put file for each system and a corresponding single
gold standard file. This method is sound because the
data sets for each language contain approximately
the same number of tokens ? 5,000. Thus, evalu-
ating system performance over the aggregated files
can be roughly viewed as measuring system perfor-
mance through an equally weighted arithmetic mean
over the languages.
It could be argued that a language by language
comparison would be more appropriate than com-
paring system performance across all languages.
However, as table Table 1 shows, the difference in
accuracy between the two systems is typically small
for all languages, and only in a few cases is this
difference significant. Furthermore, by aggregating
over all languages we gain better statistical estimates
of parser errors, since the data set for each individual
language is very small.
4 Error Analysis
The primary purpose of this study is to characterize
the errors made by standard data-driven dependency
parsing models. To that end, we present a large set of
experiments that relate parsing errors to a set of lin-
guistic and structural properties of the input and pre-
dicted/gold standard dependency graphs. We argue
that the results can be correlated to specific theoreti-
cal aspects of each model ? in particular the trade-off
highlighted in Section 2.4.
For simplicity, all experiments report labeled
parsing accuracies. Identical experiments using un-
labeled parsing accuracies did not reveal any addi-
tional information. Furthermore, all experiments are
based on the data from all 13 languages together, as
explained in section 3.
4.1 Length Factors
It is well known that parsing systems tend to have
lower accuracies for longer sentences. Figure 2
shows the accuracy of both parsing models relative
to sentence length (in bins of size 10: 1?10, 11?20,
125
10 20 30 40 50 50+Sentence Length (bins of size 10)
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
Depe
nden
cy A
ccur
acy MSTParserMaltParser
Figure 2: Accuracy relative to sentence length.
etc.). System performance is almost indistinguish-
able. However, MaltParser tends to perform better
on shorter sentences, which require the greedy in-
ference algorithm to make less parsing decisions. As
a result, the chance of error propagation is reduced
significantly when parsing these sentences. The fact
that MaltParser has a higher accuracy (rather than
the same accuracy) when the likelihood of error
propagation is reduced comes from its richer feature
representation.
Another interesting property is accuracy relative
to dependency length. The length of a dependency
from word wi to word wj is simply equal to |i? j|.
Longer dependencies typically represent modifiers
of the root or the main verb in a sentence. Shorter
dependencies are often modifiers of nouns such as
determiners or adjectives or pronouns modifying
their direct neighbours. Figure 3 measures the pre-
cision and recall for each system relative to depen-
dency lengths in the predicted and gold standard de-
pendency graphs. Precision represents the percent-
age of predicted arcs of length d that were correct.
Recall measures the percentage of gold standard arcs
of length d that were correctly predicted.
Here we begin to see separation between the two
systems. MSTParser is far more precise for longer
dependency arcs, whereas MaltParser does better
for shorter dependency arcs. This behaviour can
be explained using the same reasoning as above:
shorter arcs are created before longer arcs in the
greedy parsing procedure of MaltParser and are less
prone to error propagation. Theoretically, MST-
Parser should not perform better or worse for edges
of any length, which appears to be the case. There
is still a slight degradation, but this can be attributed
to long dependencies occurring more frequently in
constructions with possible ambiguity. Note that
even though the area under the curve is much larger
for MSTParser, the number of dependency arcs with
a length greater than ten is much smaller than the
number with length less than ten, which is why the
overall accuracy of each system is nearly identical.
For all properties considered here, bin size generally
shrinks in size as the value on the x-axis increases.
4.2 Graph Factors
The structure of the predicted and gold standard de-
pendency graphs can also provide insight into the
differences between each model. For example, mea-
suring accuracy for arcs relative to their distance to
the artificial root node will detail errors at different
levels of the dependency graph. For a given arc, we
define this distance as the number of arcs in the re-
verse path from the modifier of the arc to the root.
Figure 4 plots the precision and recall of each sys-
tem for arcs of varying distance to the root. Preci-
sion is equal to the percentage of dependency arcs in
the predicted graph that are at a distance of d and are
correct. Recall is the percentage of dependency arcs
in the gold standard graph that are at a distance of d
and were predicted.
Figure 4 clearly shows that for arcs close to the
root, MSTParser is much more precise than Malt-
Parser, and vice-versa for arcs further away from the
root. This is probably the most compelling graph
given in this study since it reveals a clear distinction:
MSTParser?s precision degrades as the distance to
the root increases whereas MaltParser?s precision in-
creases. The plots essentially run in opposite direc-
tions crossing near the middle. Dependency arcs fur-
ther away from the root are usually constructed early
in the parsing algorithm of MaltParser. Again a re-
duced likelihood of error propagation coupled with
a rich feature representation benefits that parser sub-
stantially. Furthermore, MaltParser tends to over-
predict root modifiers, because all words that the
parser fails to attach as modifiers are automatically
connected to the root, as explained in section 2.3.
Hence, low precision for root modifiers (without a
corresponding drop in recall) is an indication that the
transition-based parser produces fragmented parses.
The behaviour of MSTParser is a little trickier to
explain. One would expect that its errors should be
distributed evenly over the graph. For the most part
this is true, with the exception of spikes at the ends
126
0 5 10 15 20 25 30Dependency Length
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Depe
nden
cy P
recis
ion MSTParserMaltParser
0 5 10 15 20 25 30Dependency Length
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Depe
nden
cy R
ecall
MSTParserMaltParser
Figure 3: Dependency arc precision/recall relative to predicted/gold dependency length.
of the plot. The high performance for root modifica-
tion (distance of 1) can be explained through the fact
that this is typically a low entropy decision ? usu-
ally the parsing algorithm has to determine the main
verb from a small set of possibilities. On the other
end of the plot there is a sharp downwards spike for
arcs of distance greater than 10. It turns out that
MSTParser over-predicts arcs near the bottom of the
graph. Whereas MaltParser pushes difficult parsing
decisions higher in the graph, MSTParser appears to
push these decisions lower.
The next graph property we will examine aims to
quantify the local neighbourhood of an arc within
a dependency graph. Two dependency arcs, (i, j, l)
and (i?, j?, l?) are classified as siblings if they repre-
sent syntactic modifications of the same word, i.e.,
i = i?. Figure 5 measures the precision and recall
of each system relative to the number of predicted
and gold standard siblings of each arc. There is
not much to distinguish between the parsers on this
metric. MSTParser is slightly more precise for arcs
that are predicted with more siblings, whereas Malt-
Parser has slightly higher recall on arcs that have
more siblings in the gold standard tree. Arcs closer
to the root tend to have more siblings, which ties this
result to the previous ones.
The final graph property we wish to look at is the
degree of non-projectivity. The degree of a depen-
dency arc from word w to word u is defined here
as the number of words occurring between w and u
that are not descendants ofw and modify a word that
does not occur between w and u (Nivre, 2006). In
the example from Figure 1, the arc from jedna to Z
has a degree of one, and all other arcs have a degree
of zero. Figure 6 plots dependency arc precision and
recall relative to arc degree in predicted and gold
standard dependency graphs. MSTParser is more
precise when predicting arcs with high degree and
MaltParser vice-versa. Again, this can be explained
by the fact that there is a tight correlation between a
high degree of non-projectivity, dependency length,
distance to root and number of siblings.
4.3 Linguistic Factors
It is important to relate each system?s accuracy to a
set of linguistic categories, such as parts of speech
and dependency types. Therefore, we have made
an attempt to distinguish a few broad categories
that are cross-linguistically identifiable, based on the
available documentation of the treebanks used in the
shared task.
For parts of speech, we distinguish verbs (includ-
ing both main verbs and auxiliaries), nouns (includ-
ing proper names), pronouns (sometimes also in-
cluding determiners), adjectives, adverbs, adposi-
tions (prepositions, postpositions), and conjunctions
(both coordinating and subordinating). For depen-
dency types, we distinguish a general root category
(for labels used on arcs from the artificial root, in-
cluding either a generic label or the label assigned
to predicates of main clauses, which are normally
verbs), a subject category, an object category (in-
cluding both direct and indirect objects), and various
categories related to coordination.
Figure 7 shows the accuracy of the two parsers
for different parts of speech. This figure measures
labeled dependency accuracy relative to the part of
speech of the modifier word in a dependency rela-
tion. We see that MaltParser has slightly better ac-
curacy for nouns and pronouns, while MSTParser
does better on all other categories, in particular con-
junctions. This pattern is consistent with previous
results insofar as verbs and conjunctions are often
involved in dependencies closer to the root that span
127
2 4 6 8 10Distance to Root
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
0.9
Depe
nden
cy P
recis
ion MSTParserMaltParser
2 4 6 8 10Distance to Root
0.76
0.78
0.8
0.82
0.84
0.86
0.88
Depe
nden
cy R
ecall
MSTParserMaltParser
Figure 4: Dependency arc precision/recall relative to predicted/gold distance to root.
0 2 4 6 8 10+Number of Modifier Siblings
0.5
0.6
0.7
0.8
0.9
Depe
nden
cy P
recis
ion MSTParserMaltParser
0 2 4 6 8 10+Number of Modifier Siblings
0.5
0.6
0.7
0.8
0.9
Depe
nden
cy R
ecall
MSTParserMaltParser
Figure 5: Dependency arc precision/recall relative to number of predicted/gold siblings.
longer distances, while nouns and pronouns are typ-
ically attached to verbs and therefore occur lower in
the graph, with shorter distances. Empirically, ad-
verbs resemble verbs and conjunctions with respect
to root distance but group with nouns and pronouns
for dependency length, so the former appears to be
more important. In addition, both conjunctions and
adverbs tend to have a high number of siblings, mak-
ing the results consistent with the graph in Figure 5.
Adpositions and especially adjectives constitute
a puzzle, having both high average root distance
and low average dependency length. Adpositions do
tend to have a high number of siblings on average,
which could explain MSTParser?s performance on
that category. However, adjectives on average occur
the furthest away from the root, have the shortest
dependency length and the fewest siblings. As such,
we do not have an explanation for this behaviour.
In the top half of Figure 8, we consider precision
and recall for dependents of the root node (mostly
verbal predicates), and for subjects and objects. As
already noted, MSTParser has considerably better
precision (and slightly better recall) for the root cat-
egory, but MaltParser has an advantage for the nomi-
nal categories, especially subjects. A possible expla-
nation for the latter result, in addition to the length-
based and graph-based factors invoked before, is that
60.0%65.0%70.0%
75.0%80.0%85.0%
90.0%95.0%
Verb Noun Pron Adj Adv Adpos ConjPart of Speech (POS)Labele
d Attachment Score (
LAS) MSTParserMaltParser
Figure 7: Accuracy for different parts of speech.
MaltParser integrates labeling into the parsing pro-
cess, so that previously assigned dependency labels
can be used as features, which may be important to
disambiguate subjects and objects.
Finally, in the bottom half of Figure 8, we dis-
play precision and recall for coordinate structures,
divided into different groups depending on the type
of analysis adopted in a particular treebank. The cat-
egory CCH (coordinating conjunction as head) con-
tains conjunctions analyzed as heads of coordinate
structures, with a special dependency label that does
not describe the function of the coordinate structure
in the larger syntactic structure, a type of category
found in the so-called Prague style analysis of coor-
dination and used in the data sets for Arabic, Czech,
128
0 1 2 3 4 5 6 7+Non-Projective Arc Degree
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Depe
nden
cy P
recis
ion MSTParserMaltParser
0 1 2 3 4 5 6 7+Non-Projective Arc Degree
0.6
0.65
0.7
0.75
0.8
0.85
Depe
nden
cy R
ecall
MSTParserMaltParser
Figure 6: Dependency arc precision/recall relative to predicted/gold degree of non-projectivity.
65.0%70.0%
75.0%80.0%
85.0%90.0%
95.0%
Root Subj ObjDependency Type (DEP)De
pendency Precision MSTParserMaltParser
72.0%74.0%76.0%
78.0%80.0%82.0%
84.0%86.0%88.0%
90.0%
Root Subj ObjDependency Type (DEP)D
ependency Recall MSTParserMaltParser
0.0%10.0%20.0%
30.0%40.0%50.0%
60.0%70.0%80.0%
90.0%
CCH CCD CJCC CJCJDependency Type (DEP)De
pendency Precision MSTParserMaltParser
0.0%10.0%20.0%
30.0%40.0%50.0%
60.0%70.0%80.0%
90.0%
CCH CCD CJCC CJCJDependency Tyle (DEP)D
ependency Recall MSTParserMaltParser
Figure 8: Precision/recall for different dependency types.
and Slovene. The category CCD (coordinating con-
junction as dependent) instead denotes conjunctions
that are attached as dependents of one of the con-
juncts with a label that only marks them as conjunc-
tions, a type of category found in the data sets for
Bulgarian, Danish, German, Portuguese, Swedish
and Turkish. The two remaining categories con-
tain conjuncts that are assigned a dependency label
that only marks them as conjuncts and that are at-
tached either to the conjunction (CJCC) or to an-
other conjunct (CJCJ). The former is found in Bul-
garian, Danish, and German; the latter only in Por-
tuguese and Swedish. For most of the coordination
categories there is little or no difference between the
two parsers, but for CCH there is a difference in both
precision and recall of almost 20 percentage points
to MSTParser?s advantage. This can be explained by
noting that, while the categories CCD, CJCC, and
CJCJ denote relations that are internal to the coor-
dinate structure and therefore tend to be local, the
CCH relations hold between the coordinate struc-
ture and its head, which is often a relation that spans
over a greater distance and is nearer the root of the
dependency graph. It is likely that the difference in
accuracy for this type of dependency accounts for a
large part of the difference in accuracy noted earlier
for conjunctions as a part of speech.
4.4 Discussion
The experiments from the previous section highlight
the fundamental trade-off between global training
and exhaustive inference on the one hand and ex-
pressive feature representations on the other. Error
propagation is an issue for MaltParser, which typi-
129
cally performs worse on long sentences, long depen-
dency arcs and arcs higher in the graphs. But this is
offset by the rich feature representation available to
these models that result in better decisions for fre-
quently occurring arc types like short dependencies
or subjects and objects. The errors for MSTParser
are spread a little more evenly. This is expected,
as the inference algorithm and feature representation
should not prefer one type of arc over another.
What has been learned? It was already known that
the two systems make different errors through the
work of Sagae and Lavie (2006). However, in that
work an arc-based voting scheme was used that took
only limited account of the properties of the words
connected by a dependency arc (more precisely, the
overall accuracy of each parser for the part of speech
of the dependent). The analysis in this work not only
shows that the errors made by each system are dif-
ferent, but that they are different in a way that can be
predicted and quantified. This is an important step
in parser development.
To get some upper bounds of the improvement
that can be obtained by combining the strengths of
each models, we have performed two oracle experi-
ments. Given the output of the two systems, we can
envision an oracle that can optimally choose which
single parse or combination of sub-parses to predict
as a final parse. For the first experiment the oracle
is provided with the single best parse from each sys-
tem, say G = (V,A) and G? = (V ?, A?). The oracle
chooses a parse that has the highest number of cor-
rectly predicted labeled dependency attachments. In
this situation, the oracle accuracy is 84.5%. In the
second experiment the oracle chooses the tree that
maximizes the number of correctly predicted depen-
dency attachments, subject to the restriction that the
tree must only contain arcs from A ? A?. This can
be computed by setting the weight of an arc to 1 if
it is in the correct parse and in the set A ? A?. All
other arc weights are set to negative infinity. One can
then simply find the tree that has maximal sum of
arc weights using directed spanning tree algorithms.
This technique is similar to the parser voting meth-
ods used by Sagae and Lavie (2006). In this situa-
tion, the oracle accuracy is 86.9%.
In both cases we see a clear increase in accuracy:
86.9% and 84.5% relative to 81% for the individual
systems. This indicates that there is still potential
for improvement, just by combining the two existing
models. More interestingly, however, we can use
the analysis to get ideas for new models. Below we
sketch some possible new directions:
1. Ensemble systems: The error analysis pre-
sented in this paper could be used as inspiration
for more refined weighting schemes for ensem-
ble systems of the kind proposed by Sagae and
Lavie (2006), making the weights depend on a
range of linguistic and graph-based factors.
2. Hybrid systems: Rather than using an ensem-
ble of several parsers, we may construct a sin-
gle system integrating the strengths of each
parser described here. This could defer to
a greedy inference strategy during the early
stages of the parse in order to benefit from a
rich feature representation, but then default to
a global exhaustive model as the likelihood for
error propagation increases.
3. Novel approaches: The two approaches inves-
tigated are each based on a particular combina-
tion of training and inference methods. Wemay
naturally ask what other combinations may
prove fruitful. For example, what about glob-
ally trained, greedy, transition-based models?
This is essentially what Daume? III et al (2006)
provide, in the form of a general search-based
structured learning framework that can be di-
rectly applied to dependency parsing. The ad-
vantage of this method is that the learning can
set model parameters relative to errors resulting
directly from the search strategy ? such as error
propagation due to greedy search. When com-
bined with MaltParser?s rich feature represen-
tation, this could lead to significant improve-
ments in performance.
5 Conclusion
We have presented a thorough study of the dif-
ference in errors made between global exhaustive
graph-based parsing systems (MSTParser) and lo-
cal greedy transition-based parsing systems (Malt-
Parser). We have shown that these differences can
be quantified and tied to theoretical expectations of
each model, which may provide insights leading to
better models in the future.
130
References
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The PDT: A 3-level annotation scenario. In
A. Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, chapter 7. Kluwer Academic Publish-
ers.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. CoNLL.
Hal Daume? III, John Langford, and Daniel Marcu. 2006.
Search-based structured prediction. In Submission.
Y. Ding and M. Palmer. 2004. Synchronous dependency
insertion grammars: A grammar formalism for syntax
based statistical MT. InWorkshop on Recent Advances
in Dependency Grammars (COLING).
R. Hudson. 1984. Word Grammar. Blackwell.
H. Maruyama. 1990. Structural disambiguation with
constraint propagation. In Proc. ACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT/EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proc. CoNLL.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. ACL.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. CoNLL.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proc. IWPT.
J. Nivre. 2006. Constraints on non-projective depen-
dency parsing. In Proc. EACL.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. HLT/NAACL.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The Mean-
ing of the Sentence in Its Pragmatic Aspects. Reidel.
R. Snow, D. Jurafsky, and A. Y. Ng. 2004. Learning
syntactic patterns for automatic hypernym discovery.
In Proc. NIPS.
131
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 915?932,
Prague, June 2007. c?2007 Association for Computational Linguistics
The CoNLL 2007 Shared Task on Dependency Parsing
Joakim Nivre?? Johan Hall? Sandra Ku?bler? Ryan McDonald??
Jens Nilsson? Sebastian Riedel?? Deniz Yuret??
?Va?xjo? University, School of Mathematics and Systems Engineering, first.last@vxu.se
?Uppsala University, Dept. of Linguistics and Philology, joakim.nivre@lingfil.uu.se
?Indiana University, Department of Linguistics, skuebler@indiana.edu
??Google Inc., ryanmcd@google.com
??University of Edinburgh, School of Informatics, S.R.Riedel@sms.ed.ac.uk
??Koc? University, Dept. of Computer Engineering, dyuret@ku.edu.tr
Abstract
The Conference on Computational Natural
Language Learning features a shared task, in
which participants train and test their learn-
ing systems on the same data sets. In 2007,
as in 2006, the shared task has been devoted
to dependency parsing, this year with both a
multilingual track and a domain adaptation
track. In this paper, we define the tasks of the
different tracks and describe how the data
sets were created from existing treebanks for
ten languages. In addition, we characterize
the different approaches of the participating
systems, report the test results, and provide
a first analysis of these results.
1 Introduction
Previous shared tasks of the Conference on Compu-
tational Natural Language Learning (CoNLL) have
been devoted to chunking (1999, 2000), clause iden-
tification (2001), named entity recognition (2002,
2003), and semantic role labeling (2004, 2005). In
2006 the shared task was multilingual dependency
parsing, where participants had to train a single
parser on data from thirteen different languages,
which enabled a comparison not only of parsing and
learning methods, but also of the performance that
can be achieved for different languages (Buchholz
and Marsi, 2006).
In dependency-based syntactic parsing, the task is
to derive a syntactic structure for an input sentence
by identifying the syntactic head of each word in the
sentence. This defines a dependency graph, where
the nodes are the words of the input sentence and the
arcs are the binary relations from head to dependent.
Often, but not always, it is assumed that all words
except one have a syntactic head, which means that
the graph will be a tree with the single independent
word as the root. In labeled dependency parsing, we
additionally require the parser to assign a specific
type (or label) to each dependency relation holding
between a head word and a dependent word.
In this year?s shared task, we continue to explore
data-driven methods for multilingual dependency
parsing, but we add a new dimension by also intro-
ducing the problem of domain adaptation. The way
this was done was by having two separate tracks: a
multilingual track using essentially the same setup
as last year, but with partly different languages, and
a domain adaptation track, where the task was to use
machine learning to adapt a parser for a single lan-
guage to a new domain. In total, test results were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). Not everyone submitted papers describ-
ing their system, and some papers describe more
than one system (or the same system in both tracks),
which explains why there are only (!) twenty-one
papers in the proceedings.
In this paper, we provide task definitions for the
two tracks (section 2), describe data sets extracted
from available treebanks (section 3), report results
for all systems in both tracks (section 4), give an
overview of approaches used (section 5), provide a
first analysis of the results (section 6), and conclude
with some future directions (section 7).
915
2 Task Definition
In this section, we provide the task definitions that
were used in the two tracks of the CoNLL 2007
Shard Task, the multilingual track and the domain
adaptation track, together with some background
and motivation for the design choices made. First
of all, we give a brief description of the data format
and evaluation metrics, which were common to the
two tracks.
2.1 Data Format and Evaluation Metrics
The data sets derived from the original treebanks
(section 3) were in the same column-based format
as for the 2006 shared task (Buchholz and Marsi,
2006). In this format, sentences are separated by a
blank line; a sentence consists of one or more to-
kens, each one starting on a new line; and a token
consists of the following ten fields, separated by a
single tab character:
1. ID: Token counter, starting at 1 for each new
sentence.
2. FORM: Word form or punctuation symbol.
3. LEMMA: Lemma or stem of word form, or an
underscore if not available.
4. CPOSTAG: Coarse-grained part-of-speech tag,
where the tagset depends on the language.
5. POSTAG: Fine-grained part-of-speech tag,
where the tagset depends on the language, or
identical to the coarse-grained part-of-speech
tag if not available.
6. FEATS: Unordered set of syntactic and/or mor-
phological features (depending on the particu-
lar language), separated by a vertical bar (|), or
an underscore if not available.
7. HEAD: Head of the current token, which is
either a value of ID or zero (0). Note that,
depending on the original treebank annotation,
there may be multiple tokens with HEAD=0.
8. DEPREL: Dependency relation to the HEAD.
The set of dependency relations depends on
the particular language. Note that, depending
on the original treebank annotation, the depen-
dency relation when HEAD=0 may be mean-
ingful or simply ROOT.
9. PHEAD: Projective head of current token,
which is either a value of ID or zero (0), or an
underscore if not available.
10. PDEPREL: Dependency relation to the
PHEAD, or an underscore if not available.
The PHEAD and PDEPREL were not used at all
in this year?s data sets (i.e., they always contained
underscores) but were maintained for compatibility
with last year?s data sets. This means that, in prac-
tice, the first six columns can be considered as input
to the parser, while the HEAD and DEPREL fields
are the output to be produced by the parser. Labeled
training sets contained all ten columns; blind test
sets only contained the first six columns; and gold
standard test sets (released only after the end of the
test period) again contained all ten columns. All data
files were encoded in UTF-8.
The official evaluation metric in both tracks was
the labeled attachment score (LAS), i.e., the per-
centage of tokens for which a system has predicted
the correct HEAD and DEPREL, but results were
also reported for unlabeled attachment score (UAS),
i.e., the percentage of tokens with correct HEAD,
and the label accuracy (LA), i.e., the percentage of
tokens with correct DEPREL. One important differ-
ence compared to the 2006 shared task is that all to-
kens were counted as ?scoring tokens?, including in
particular all punctuation tokens. The official eval-
uation script, eval07.pl, is available from the shared
task website.1
2.2 Multilingual Track
The multilingual track of the shared task was orga-
nized in the same way as the 2006 task, with an-
notated training and test data from a wide range of
languages to be processed with one and the same
parsing system. This system must therefore be able
to learn from training data, to generalize to unseen
test data, and to handle multiple languages, possi-
bly by adjusting a number of hyper-parameters. Par-
ticipants in the multilingual track were expected to
submit parsing results for all languages involved.
1http://depparse.uvt.nl/depparse-wiki/SoftwarePage
916
One of the claimed advantages of dependency
parsing, as opposed to parsing based on constituent
analysis, is that it extends naturally to languages
with free or flexible word order. This explains the
interest in recent years for multilingual evaluation
of dependency parsers. Even before the 2006 shared
task, the parsers of Collins (1997) and Charniak
(2000), originally developed for English, had been
adapted for dependency parsing of Czech, and the
parsing methodology proposed by Kudo and Mat-
sumoto (2002) and Yamada and Matsumoto (2003)
had been evaluated on both Japanese and English.
The parser of McDonald and Pereira (2006) had
been applied to English, Czech and Danish, and the
parser of Nivre et al (2007) to ten different lan-
guages. But by far the largest evaluation of mul-
tilingual dependency parsing systems so far was the
2006 shared task, where nineteen systems were eval-
uated on data from thirteen languages (Buchholz and
Marsi, 2006).
One of the conclusions from the 2006 shared task
was that parsing accuracy differed greatly between
languages and that a deeper analysis of the factors
involved in this variation was an important problem
for future research. In order to provide an extended
empirical foundation for such research, we tried to
select the languages and data sets for this year?s task
based on the following desiderata:
? The selection of languages should be typolog-
ically varied and include both new languages
and old languages (compared to 2006).
? The creation of the data sets should involve as
little conversion as possible from the original
treebank annotation, meaning that preference
should be given to treebanks with dependency
annotation.
? The training data sets should include at least
50,000 tokens and at most 500,000 tokens.2
The final selection included data from Arabic,
Basque, Catalan, Chinese, Czech, English, Greek,
Hungarian, Italian, and Turkish. The treebanks from
2The reason for having an upper bound on the training set
size was the fact that, in 2006, some participants could not train
on all the data for some languages because of time limitations.
Similar considerations also led to the decision to have a smaller
number of languages this year (ten, as opposed to thirteen).
which the data sets were extracted are described in
section 3.
2.3 Domain Adaptation Track
One well known characteristic of data-driven pars-
ing systems is that they typically perform much
worse on data that does not come from the train-
ing domain (Gildea, 2001). Due to the large over-
head in annotating text with deep syntactic parse
trees, the need to adapt parsers from domains with
plentiful resources (e.g., news) to domains with lit-
tle resources is an important problem. This prob-
lem is commonly referred to as domain adaptation,
where the goal is to adapt annotated resources from
a source domain to a target domain of interest.
Almost all prior work on domain adaptation as-
sumes one of two scenarios. In the first scenario,
there are limited annotated resources available in the
target domain, and many studies have shown that
this may lead to substantial improvements. This in-
cludes the work of Roark and Bacchiani (2003), Flo-
rian et al (2004), Chelba and Acero (2004), Daume?
and Marcu (2006), and Titov and Henderson (2006).
Of these, Roark and Bacchiani (2003) and Titov and
Henderson (2006) deal specifically with syntactic
parsing. The second scenario assumes that there are
no annotated resources in the target domain. This is
a more realistic situation and is considerably more
difficult. Recent work by McClosky et al (2006)
and Blitzer et al (2006) have shown that the exis-
tence of a large unlabeled corpus in the new domain
can be leveraged in adaptation. For this shared-task,
we are assuming the latter setting ? no annotated re-
sources in the target domain.
Obtaining adequate annotated syntactic resources
for multiple languages is already a challenging prob-
lem, which is only exacerbated when these resources
must be drawn from multiple and diverse domains.
As a result, the only language that could be feasibly
tested in the domain adaptation track was English.
The setup for the domain adaptation track was as
follows. Participants were provided with a large an-
notated corpus from the source domain, in this case
sentences from the Wall Street Journal. Participants
were also provided with data from three different
target domains: biomedical abstracts (development
data), chemical abstracts (test data 1), and parent-
child dialogues (test data 2). Additionally, a large
917
unlabeled corpus for each data set (training, devel-
opment, test) was provided. The goal of the task was
to use the annotated source data, plus any unlabeled
data, to produce a parser that is accurate for each of
the test sets from the target domains.3
Participants could submit systems in either the
?open? or ?closed? class (or both). The closed class
requires a system to use only those resources pro-
vided as part of the shared task. The open class al-
lows a system to use additional resources provided
those resources are not drawn from the same domain
as the development or test sets. An example might
be a part-of-speech tagger trained on the entire Penn
Treebank and not just the subset provided as train-
ing data, or a parser that has been hand-crafted or
trained on a different training set.
3 Treebanks
In this section, we describe the treebanks used in the
shared task and give relevant information about the
data sets created from them.
3.1 Multilingual Track
Arabic The analytical syntactic annotation
of the Prague Arabic Dependency Treebank
(PADT) (Hajic? et al, 2004) can be considered a
pure dependency annotation. The conversion, done
by Otakar Smrz, from the original format to the
column-based format described in section 2.1 was
therefore relatively straightforward, although not all
the information in the original annotation could be
transfered to the new format. PADT was one of the
treebanks used in the 2006 shared task but then only
contained about 54,000 tokens. Since then, the size
of the treebank has more than doubled, with around
112,000 tokens. In addition, the morphological
annotation has been made more informative. It
is also worth noting that the parsing units in this
treebank are in many cases larger than conventional
sentences, which partly explains the high average
number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
3Note that annotated development data for the target domain
was only provided for the development domain, biomedical ab-
stracts. For the two test domains, chemical abstracts and parent-
child dialogues, the only annotated data sets were the gold stan-
dard test sets, released only after test runs had been submitted.
Basque For Basque, we used the 3LB Basque
treebank (Aduriz et al, 2003). At present, the tree-
bank consists of approximately 3,700 sentences, 334
of which were used as test data. The treebank com-
prises literary and newspaper texts. It is annotated
in a dependency format and was converted to the
CoNLL format by a team led by Koldo Gojenola.
Catalan The Catalan section of the CESS-ECE
Syntactically and Semantically Annotated Cor-
pora (Mart?? et al, 2007) is annotated with, among
other things, constituent structure and grammatical
functions. A head percolation table was used for
automatically converting the constituent trees into
dependency trees. The original data only contains
functions related to the verb, and a function table
was used for deriving the remaining syntactic func-
tions. The conversion was performed by a team led
by Llu??s Ma`rquez and Anto`nia Mart??.
Chinese The Chinese data are taken from the
Sinica treebank (Chen et al, 2003), which con-
tains both syntactic functions and semantic func-
tions. The syntactic head was used in the conversion
to the CoNLL format, carried out by Yu-Ming Hsieh
and the organizers of the 2006 shared task, and the
syntactic functions were used wherever it was pos-
sible. The training data used is basically the same
as for the 2006 shared task, except for a few correc-
tions, but the test data is new for this year?s shared
task. It is worth noting that the parsing units in this
treebank are sometimes smaller than conventional
sentence units, which partly explains the low aver-
age number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
Czech The analytical syntactic annotation of the
Prague Dependency Treebank (PDT) (Bo?hmova? et
al., 2003) is a pure dependency annotation, just as
for PADT. It was also used in the shared task 2006,
but there are two important changes compared to
last year. First, version 2.0 of PDT was used in-
stead of version 1.0, and a conversion script was
created by Zdenek Zabokrtsky, using the new XML-
based format of PDT 2.0. Secondly, due to the upper
bound on training set size, only sections 1?3 of PDT
constitute the training data, which amounts to some
450,000 tokens. The test data is a small subset of the
development test set of PDT.
918
English For English we used the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993). In particular, we used sections 2-11 for train-
ing and a subset of section 23 for testing. As a pre-
processing stage we removed many functions tags
from the non-terminals in the phrase structure repre-
sentation to make the representations more uniform
with out-of-domain test sets for the domain adapta-
tion track (see section 3.2). The resulting data set
was then converted to dependency structures using
the procedure described in Johansson and Nugues
(2007a). This work was done by Ryan McDonald.
Greek The Greek Dependency Treebank
(GDT) (Prokopidis et al, 2005) adopts a de-
pendency structure annotation very similar to those
of PDT and PADT, which means that the conversion
by Prokopis Prokopidis was relatively straightfor-
ward. GDT is one of the smallest treebanks in
this year?s shared task (about 65,000 tokens) and
contains sentences of Modern Greek. Just like PDT
and PADT, the treebank contains more than one
level of annotation, but we only used the analytical
level of GDT.
Hungarian For the Hungarian data, the Szeged
treebank (Csendes et al, 2005) was used. The tree-
bank is based on texts from six different genres,
ranging from legal newspaper texts to fiction. The
original annotation scheme is constituent-based, fol-
lowing generative principles. It was converted into
dependencies by Zo?ltan Alexin based on heuristics.
Italian The data set used for Italian is a subset
of the balanced section of the Italian Syntactic-
Semantic Treebank (ISST) (Montemagni et al,
2003) and consists of texts from the newspaper Cor-
riere della Sera and from periodicals. A team led
by Giuseppe Attardi, Simonetta Montemagni, and
Maria Simi converted the annotation to the CoNLL
format, using information from two different anno-
tation levels, the constituent structure level and the
dependency structure level.
Turkish For Turkish we used the METU-Sabanc?
Turkish Treebank (Oflazer et al, 2003), which was
also used in the 2006 shared task. A new test set of
about 9,000 tokens was provided by Gu?ls?en Eryig?it
(Eryig?it, 2007), who also handled the conversion to
the CoNLL format, which means that we could use
all the approximately 65,000 tokens of the original
treebank for training. The rich morphology of Turk-
ish requires the basic tokens in parsing to be inflec-
tional groups (IGs) rather than words. IGs of a single
word are connected to each other deterministically
using dependency links labeled DERIV, referred to
as word-internal dependencies in the following, and
the FORM and the LEMMA fields may be empty
(they contain underscore characters in the data files).
Sentences do not necessarily have a unique root;
most internal punctuation and a few foreign words
also have HEAD=0.
3.2 Domain Adaptation Track
As mentioned previously, the source data is drawn
from a corpus of news, specifically the Wall Street
Journal section of the Penn Treebank (Marcus et al,
1993). This data set is identical to the English train-
ing set from the multilingual track (see section 3.1).
For the target domains we used three different
labeled data sets. The first two were annotated
as part of the PennBioIE project (Kulick et al,
2004) and consist of sentences drawn from either
biomedical or chemical research abstracts. Like the
source WSJ corpus, this data is annotated using the
Penn Treebank phrase structure scheme. To con-
vert these sets to dependency structures we used the
same procedure as before (Johansson and Nugues,
2007a). Additional care was taken to remove sen-
tences that contained non-WSJ part-of-speech tags
or non-terminals (e.g., HYPH part-of-speech tag in-
dicating a hyphen). Furthermore, the annotation
scheme for gaps and traces was made consistent with
the Penn Treebank wherever possible. As already
mentioned, the biomedical data set was distributed
as a development set for the training phase, while
the chemical data set was only used for final testing.
The third target data set was taken from the
CHILDES database (MacWhinney, 2000), in partic-
ular the EVE corpus (Brown, 1973), which has been
annotated with dependency structures. Unfortu-
nately the dependency labels of the CHILDES data
were inconsistent with those of the WSJ, biomedi-
cal and chemical data sets, and we therefore opted
to only evaluate unlabeled accuracy for this data
set. Furthermore, there was an inconsistency in how
main and auxiliary verbs were annotated for this data
set relative to others. As a result of this, submitting
919
Multilingual Domain adaptation
Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES
Language family Sem. Isol. Rom. Sin. Sla. Ger. Hel. F.-U. Rom. Tur. Ger.
Annotation d d c+f c+f d c+f d c+f c+f d c+f d
Training data Development data
Tokens (k) 112 51 431 337 432 447 65 132 71 65 5
Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2
Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1
LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No
No. CPOSTAG 15 25 17 13 12 31 18 16 14 14 25
No. POSTAG 21 64 54 294 59 45 38 43 28 31 37
No. FEATS 21 359 33 0 71 0 31 50 21 78 0
No. DEPREL 29 35 42 69 46 20 46 49 22 25 18
No. DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1
% HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0
% HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0
% HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0
HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0
% Non-proj. arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4
% Non-proj. sent. 10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0
Punc. attached S S A S S A S A A S A
DEPRELS for punc. 10 13 6 29 16 13 15 1 10 12 8
Test data PCHEM CHILDES
Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999
Sentences 131 334 167 690 286 214 197 390 249 300 195 666
Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9
% New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10
% New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a
Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development
set and the two test sets of the domain adaptation track.
920
results for the CHILDES data was considered op-
tional. Like the chemical data set, this data set was
only used for final testing.
Finally, a large corpus of unlabeled in-domain
data was provided for each data set and made avail-
able for training. This data was drawn from theWSJ,
PubMed.com (specific to biomedical and chemical
research literature), and the CHILDES data base.
The data was tokenized to be as consistent as pos-
sible with the WSJ training set.
3.3 Overview
Table 1 describes the characteristics of the data sets.
For the multilingual track, we provide statistics over
the training and test sets; for the domain adaptation
track, the statistics were extracted from the develop-
ment set. Following last year?s shared task practice
(Buchholz and Marsi, 2006), we use the following
definition of projectivity: An arc (i, j) is projective
iff all nodes occurring between i and j are dominated
by i (where dominates is the transitive closure of the
arc relation).
In the table, the languages are abbreviated to their
first two letters. Language families are: Semitic,
Isolate, Romance, Sino-Tibetan, Slavic, Germanic,
Hellenic, Finno-Ugric, and Turkic. The type of the
original annotation is either constituents plus (some)
functions (c+f) or dependencies (d). For the train-
ing data, the number of words and sentences are
given in multiples of thousands, and the average
length of a sentence in words (including punctua-
tion tokens). The following rows contain informa-
tion about whether lemmas are available, the num-
ber of coarse- and fine-grained part-of-speech tags,
the number of feature components, and the number
of dependency labels. Then information is given on
how many different dependency labels can co-occur
with HEAD=0, the percentage of HEAD=0 depen-
dencies, and the percentage of heads preceding (left)
or succeeding (right) a token (giving an indication of
whether a language is predominantly head-initial or
head-final). This is followed by the average number
of HEAD=0 dependencies per sentence and the per-
centage of non-projective arcs and sentences. The
last two rows show whether punctuation tokens are
attached as dependents of other tokens (A=Always,
S=Sometimes) and specify the number of depen-
dency labels that exist for punctuation tokens. Note
that punctuation is defined as any token belonging to
the UTF-8 category of punctuation. This means, for
example, that any token having an underscore in the
FORM field (which happens for word-internal IGs
in Turkish) is also counted as punctuation here.
For the test sets, the number of words and sen-
tences as well as the ratio of words per sentence are
listed, followed by the percentage of new words and
lemmas (if applicable). For the domain adaptation
sets, the percentage of new words is computed with
regard to the training set (Penn Treebank).
4 Submissions and Results
As already stated in the introduction, test runs were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). In the result tables below, systems are
identified by the last name of the teammember listed
first when test runs were uploaded for evaluation. In
general, this name is also the first author of a paper
describing the system in the proceedings, but there
are a few exceptions and complications. First of all,
for four out of twenty-seven systems, no paper was
submitted to the proceedings. This is the case for the
systems of Jia, Maes et al, Nash, and Zeman, which
is indicated by the fact that these names appear in
italics in all result tables. Secondly, two teams sub-
mitted two systems each, which are described in a
single paper by each team. Thus, the systems called
?Nilsson? and ?Hall, J.? are both described in Hall et
al. (2007a), while the systems called ?Duan (1)? and
?Duan (2)? are both described in Duan et al (2007).
Finally, please pay attention to the fact that there
are two teams, where the first author?s last name is
Hall. Therefore, we use ?Hall, J.? and ?Hall, K.?,
to disambiguate between the teams involving Johan
Hall (Hall et al, 2007a) and Keith Hall (Hall et al,
2007b), respectively.
Tables 2 and 3 give the scores for the multilingual
track in the CoNLL 2007 shared task. The Average
column contains the average score for all ten lan-
guages, which determines the ranking in this track.
Table 4 presents the results for the domain adapta-
tion track, where the ranking is determined based on
the PCHEM results only, since the CHILDES data
set was optional. Note also that there are no labeled
921
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2)
Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5)
Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1)
Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10)
Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3)
Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11)
Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7)
Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9)
Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12)
Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6)
Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15)
Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4)
Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8)
Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14)
Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17)
Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13)
Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16)
Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18)
Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19)
Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20)
Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6)
Nash 8.65(22)* 86.49(8)
Shimizu 7.20(23) 72.02(20)
Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task. Teams
are denoted by the last name of their first member, with italics indicating that there is no corresponding
paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a score
in the Average column indicates a statistically significant difference with the next lower rank.
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3)
Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2)
Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1)
Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9)
Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10)
Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5)
Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7)
Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12)
Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4)
Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13)
Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15)
Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6)
Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8)
Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14)
Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17)
Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16)
Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11)
Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18)
Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19)
Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20)
Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8)
Nash 8.77(22)* 87.71(9)
Shimizu 7.79(23) 77.91(20)
Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.
Teams are denoted by the last name of their first member, with italics indicating that there is no correspond-
ing paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a
score in the Average column indicates a statistically significant difference with the next lower rank.
922
LAS UAS
Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o
Sagae 81.06(1) 83.42(1)
Attardi 80.40(2) 83.08(3) 58.67(3)
Dredze 80.22(3) 83.38(2) 61.37(1)
Nguyen 79.50(4)* 82.04(4)*
Jia 76.48(5)* 78.92(5)* 57.43(5)
Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1)
Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)*
Zeman 50.61(8) 54.57(8) 58.89(2)
Schneider 63.01(3)* 66.53(3)* 60.27(2)
Watson 55.47(4) 62.79(4) 45.61(3)
Wu 52.89(6)
Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of
the domain adaptation track in the CoNLL 2007 shared task. Teams are denoted by the last name of their
first member, with italics indicating that there is no corresponding paper in the proceedings. The number
in parentheses next to each score gives the rank. A star next to a score in the PCHEM columns indicates a
statistically significant difference with the next lower rank.
attachment scores for the CHILDES data set, for rea-
sons explained in section 3.2. The number in paren-
theses next to each score gives the rank. A star next
to a score indicates that the difference with the next
lower rank is significant at the 5% level using a z-
test for proportions. A more complete presentation
of the results, including the significance results for
all the tasks and their p-values, can be found on the
shared task website.4
Looking first at the results in the multilingual
track, we note that there are a number of systems
performing at almost the same level at the top of the
ranking. For the average labeled attachment score,
the difference between the top score (Nilsson) and
the fifth score (Hall, J.) is no more than half a per-
centage point, and there are generally very few sig-
nificant differences among the five or six best sys-
tems, regardless of whether we consider labeled or
unlabeled attachment score. For the closed class of
the domain adaptation track, we see a very similar
pattern, with the top system (Sagae) being followed
very closely by two other systems. For the open
class, the results are more spread out, but then there
are very few results in this class. It is also worth not-
ing that the top scores in the closed class, somewhat
unexpectedly, are higher than the top scores in the
4http://nextens.uvt.nl/depparse-wiki/AllScores
open class. But before we proceed to a more detailed
analysis of the results (section 6), we will make an
attempt to characterize the approaches represented
by the different systems.
5 Approaches
In this section we give an overview of the models,
inference methods, and learning methods used in the
participating systems. For obvious reasons the dis-
cussion is limited to systems that are described by
a paper in the proceedings. But instead of describ-
ing the systems one by one, we focus on the basic
methodological building blocks that are often found
in several systems although in different combina-
tions. For descriptions of the individual systems, we
refer to the respective papers in the proceedings.
Section 5.1 is devoted to system architectures. We
then describe the two main paradigms for learning
and inference, in this year?s shared task as well as in
last year?s, which we call transition-based parsers
(section 5.2) and graph-based parsers (section 5.3),
adopting the terminology of McDonald and Nivre
(2007).5 Finally, we give an overview of the domain
adaptation methods that were used (section 5.4).
5This distinction roughly corresponds to the distinction
made by Buchholz and Marsi (2006) between ?stepwise? and
?all-pairs? approaches.
923
5.1 Architectures
Most systems perform some amount of pre- and
post-processing, making the actual parsing compo-
nent part of a sequential workflow of varying length
and complexity. For example, most transition-
based parsers can only build projective dependency
graphs. For languages with non-projective depen-
dencies, graphs therefore need to be projectivized
for training and deprojectivized for testing (Hall et
al., 2007a; Johansson and Nugues, 2007b; Titov and
Henderson, 2007).
Instead of assigning HEAD and DEPREL in a
single step, some systems use a two-stage approach
for attaching and labeling dependencies (Chen et al,
2007; Dredze et al, 2007). In the first step unlabeled
dependencies are generated, in the second step these
are labeled. This is particularly helpful for factored
parsing models, in which label decisions cannot be
easily conditioned on larger parts of the structure
due to the increased complexity of inference. One
system (Hall et al, 2007b) extends this two-stage ap-
proach to a three-stage architecture where the parser
and labeler generate an n-best list of parses which in
turn is reranked.6
In ensemble-based systems several base parsers
provide parsing decisions, which are added together
for a combined score for each potential dependency
arc. The tree that maximizes the sum of these com-
bined scores is taken as the final output parse. This
technique is used by Sagae and Tsujii (2007) and in
the Nilsson system (Hall et al, 2007a). It is worth
noting that both these systems combine transition-
based base parsers with a graph-based method for
parser combination, as first described by Sagae and
Lavie (2006).
Data-driven grammar-based parsers, such as Bick
(2007), Schneider et al (2007), and Watson and
Briscoe (2007), need pre- and post-processing in or-
der to map the dependency graphs provided as train-
ing data to a format compatible with the grammar
used, and vice versa.
5.2 Transition-Based Parsers
Transition-based parsers build dependency graphs
by performing sequences of actions, or transitions.
Both learning and inference is conceptualized in
6They also flip the order of the labeler and the reranker.
terms of predicting the correct transition based on
the current parser state and/or history. We can fur-
ther subclassify parsers with respect to the model (or
transition system) they adopt, the inference method
they use, and the learning method they employ.
5.2.1 Models
The most common model for transition-based
parsers is one inspired by shift-reduce parsing,
where a parser state contains a stack of partially
processed tokens and a queue of remaining input
tokens, and where transitions add dependency arcs
and perform stack and queue operations. This type
of model is used by the majority of transition-based
parsers (Attardi et al, 2007; Duan et al, 2007; Hall
et al, 2007a; Johansson and Nugues, 2007b; Man-
nem, 2007; Titov and Henderson, 2007; Wu et al,
2007). Sometimes it is combined with an explicit
probability model for transition sequences, which
may be conditional (Duan et al, 2007) or generative
(Titov and Henderson, 2007).
An alternative model is based on the list-based
parsing algorithm described by Covington (2001),
which iterates over the input tokens in a sequen-
tial manner and evaluates for each preceding token
whether it can be linked to the current token or not.
This model is used by Marinov (2007) and in com-
ponent parsers of the Nilsson ensemble system (Hall
et al, 2007a). Finally, two systems use models based
on LR parsing (Sagae and Tsujii, 2007; Watson and
Briscoe, 2007).
5.2.2 Inference
The most common inference technique in transition-
based dependency parsing is greedy deterministic
search, guided by a classifier for predicting the next
transition given the current parser state and history,
processing the tokens of the sentence in sequen-
tial left-to-right order7 (Hall et al, 2007a; Mannem,
2007; Marinov, 2007; Wu et al, 2007). Optionally
multiple passes over the input are conducted until no
tokens are left unattached (Attardi et al, 2007).
As an alternative to deterministic parsing, several
parsers use probabilistic models and maintain a heap
or beam of partial transition sequences in order to
pick the most probable one at the end of the sentence
7For diversity in parser ensembles, right-to-left parsers are
also used.
924
(Duan et al, 2007; Johansson and Nugues, 2007b;
Sagae and Tsujii, 2007; Titov and Henderson, 2007).
One system uses as part of their parsing pipeline a
?neighbor-parser? that attaches adjacent words and
a ?root-parser? that identifies the root word(s) of a
sentence (Wu et al, 2007). In the case of grammar-
based parsers, a classifier is used to disambiguate
in cases where the grammar leaves some ambiguity
(Schneider et al, 2007; Watson and Briscoe, 2007)
5.2.3 Learning
Transition-based parsers either maintain a classifier
that predicts the next transition or a global proba-
bilistic model that scores a complete parse. To train
these classifiers and probabilitistic models several
approaches were used: SVMs (Duan et al, 2007;
Hall et al, 2007a; Sagae and Tsujii, 2007), modified
finite Newton SVMs (Wu et al, 2007), maximum
entropy models (Sagae and Tsujii, 2007), multiclass
averaged perceptron (Attardi et al, 2007) and max-
imum likelihood estimation (Watson and Briscoe,
2007).
In order to calculate a global score or probabil-
ity for a transition sequence, two systems used a
Markov chain approach (Duan et al, 2007; Sagae
and Tsujii, 2007). Here probabilities from the output
of a classifier are multiplied over the whole sequence
of actions. This results in a locally normalized
model. Two other entries used MIRA (Mannem,
2007) or online passive-aggressive learning (Johans-
son and Nugues, 2007b) to train a globally normal-
ized model. Titov and Henderson (2007) used an in-
cremental sigmoid Bayesian network to model the
probability of a transition sequence and estimated
model parameters using neural network learning.
5.3 Graph-Based Parsers
While transition-based parsers use training data to
learn a process for deriving dependency graphs,
graph-based parsers learn a model of what it means
to be a good dependency graph given an input sen-
tence. They define a scoring or probability function
over the set of possible parses. At learning time
they estimate parameters of this function; at pars-
ing time they search for the graph that maximizes
this function. These parsers mainly differ in the
type and structure of the scoring function (model),
the search algorithm that finds the best parse (infer-
ence), and the method to estimate the function?s pa-
rameters (learning).
5.3.1 Models
The simplest type of model is based on a sum of
local attachment scores, which themselves are cal-
culated based on the dot product of a weight vector
and a feature representation of the attachment. This
type of scoring function is often referred to as a first-
order model.8 Several systems participating in this
year?s shared task used first-order models (Schiehlen
and Spranger, 2007; Nguyen et al, 2007; Shimizu
and Nakagawa, 2007; Hall et al, 2007b). Canisius
and Tjong Kim Sang (2007) cast the same type of
arc-based factorization as a weighted constraint sat-
isfaction problem.
Carreras (2007) extends the first-order model to
incorporate a sum over scores for pairs of adjacent
arcs in the tree, yielding a second-order model. In
contrast to previous work where this was constrained
to sibling relations of the dependent (McDonald and
Pereira, 2006), here head-grandchild relations can
be taken into account.
In all of the above cases the scoring function is
decomposed into functions that score local proper-
ties (arcs, pairs of adjacent arcs) of the graph. By
contrast, the model of Nakagawa (2007) considers
global properties of the graph that can take multi-
ple arcs into account, such as multiple siblings and
children of a node.
5.3.2 Inference
Searching for the highest scoring graph (usually a
tree) in a model depends on the factorization cho-
sen and whether we are looking for projective or
non-projective trees. Maximum spanning tree al-
gorithms can be used for finding the highest scor-
ing non-projective tree in a first-order model (Hall
et al, 2007b; Nguyen et al, 2007; Canisius and
Tjong Kim Sang, 2007; Shimizu and Nakagawa,
2007), while Eisner?s dynamic programming algo-
rithm solves the problem for a first-order factoriza-
tion in the projective case (Schiehlen and Spranger,
2007). Carreras (2007) employs his own exten-
sion of Eisner?s algorithm for the case of projective
trees and second-order models that include head-
grandparent relations.
8It is also known as an edge-factored model.
925
The methods presented above are mostly efficient
and always exact. However, for models that take
global properties of the tree into account, they can-
not be applied. Instead Nakagawa (2007) uses Gibbs
sampling to obtain marginal probabilities of arcs be-
ing included in the tree using his global model and
then applies a maximum spanning tree algorithm to
maximize the sum of the logs of these marginals and
return a valid cycle-free parse.
5.3.3 Learning
Most of the graph-based parsers were trained using
an online inference-based method such as passive-
aggressive learning (Nguyen et al, 2007; Schiehlen
and Spranger, 2007), averaged perceptron (Carreras,
2007), or MIRA (Shimizu and Nakagawa, 2007),
while some systems instead used methods based on
maximum conditional likelihood (Nakagawa, 2007;
Hall et al, 2007b).
5.4 Domain Adaptation
5.4.1 Feature-Based Approaches
One way of adapting a learner to a new domain with-
out using any unlabeled data is to only include fea-
tures that are expected to transfer well (Dredze et
al., 2007). In structural correspondence learning a
transformation from features in the source domain
to features of the target domain is learnt (Shimizu
and Nakagawa, 2007). The original source features
along with their transformed versions are then used
to train a discriminative parser.
5.4.2 Ensemble-Based Approaches
Dredze et al (2007) trained a diverse set of parsers
in order to improve cross-domain performance by
incorporating their predictions as features for an-
other classifier. Similarly, two parsers trained with
different learners and search directions were used
in the co-learning approach of Sagae and Tsujii
(2007). Unlabeled target data was processed with
both parsers. Sentences that both parsers agreed on
were then added to the original training data. This
combined data set served as training data for one of
the original parsers to produce the final system. In
a similar fashion, Watson and Briscoe (2007) used a
variant of self-training to make use of the unlabeled
target data.
5.4.3 Other Approaches
Attardi et al (2007) learnt tree revision rules for the
target domain by first parsing unlabeled target data
using a strong parser; this data was then combined
with labeled source data; a weak parser was applied
to this new dataset; finally tree correction rules are
collected based on the mistakes of the weak parser
with respect to the gold data and the output of the
strong parser.
Another technique used was to filter sentences of
the out-of-domain corpus based on their similarity
to the target domain, as predicted by a classifier
(Dredze et al, 2007). Only if a sentence was judged
similar to target domain sentences was it included in
the training set.
Bick (2007) used a hybrid approach, where a data-
driven parser trained on the labeled training data was
given access to the output of a Constraint Grammar
parser for English run on the same data. Finally,
Schneider et al (2007) learnt collocations and rela-
tional nouns from the unlabeled target data and used
these in their parsing algorithm.
6 Analysis
Having discussed the major approaches taken in the
two tracks of the shared task, we will now return to
the test results. For the multilingual track, we com-
pare results across data sets and across systems, and
report results from a parser combination experiment
involving all the participating systems (section 6.1).
For the domain adaptation track, we sum up the most
important findings from the test results (section 6.2).
6.1 Multilingual Track
6.1.1 Across Data Sets
The average LAS over all systems varies from 68.07
for Basque to 80.95 for English. Top scores vary
from 76.31 for Greek to 89.61 for English. In gen-
eral, there is a good correlation between the top
scores and the average scores. For Greek, Italian,
and Turkish, the top score is closer to the average
score than the average distance, while for Czech, the
distance is higher. The languages that produced the
most stable results in terms of system ranks with re-
spect to LAS are Hungarian and Italian. For UAS,
Catalan also falls into this group. The language that
926
Setup Arabic Chinese Czech Turkish
2006 without punctuation 66.9 90.0 80.2 65.7
2007 without punctuation 75.5 84.9 80.0 71.6
2006 with punctuation 67.0 90.0 80.2 73.8
2007 with punctuation 76.5 84.7 80.2 79.8
Table 5: A comparison of the LAS top scores from 2006 and 2007. Official scoring conditions in boldface.
For Turkish, scores with punctuation also include word-internal dependencies.
produced the most unstable results with respect to
LAS is Turkish.
In comparison to last year?s languages, the lan-
guages involved in the multilingual track this year
can be more easily separated into three classes with
respect to top scores:
? Low (76.31?76.94):
Arabic, Basque, Greek
? Medium (79.19?80.21):
Czech, Hungarian, Turkish
? High (84.40?89.61):
Catalan, Chinese, English, Italian
It is interesting to see that the classes are more easily
definable via language characteristics than via char-
acteristics of the data sets. The split goes across
training set size, original data format (constituent
vs. dependency), sentence length, percentage of un-
known words, number of dependency labels, and ra-
tio of (C)POSTAGS and dependency labels. The
class with the highest top scores contains languages
with a rather impoverished morphology. Medium
scores are reached by the two agglutinative lan-
guages, Hungarian and Turkish, as well as by Czech.
The most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection. Based on these characteristics, one would
expect to find Czech in the last class. However, the
Czech training set is four times the size of the train-
ing set for Arabic, which is the language with the
largest training set of the difficult languages.
However, it would be wrong to assume that train-
ing set size alone is the deciding factor. A closer
look at table 1 shows that while Basque and Greek
in fact have small training data sets, so do Turk-
ish and Italian. Another factor that may be asso-
ciated with the above classification is the percent-
age of new words (PNW) in the test set. Thus, the
expectation would be that the highly inflecting lan-
guages have a high PNW while the languages with
little morphology have a low PNW. But again, there
is no direct correspondence. Arabic, Basque, Cata-
lan, English, and Greek agree with this assumption:
Catalan and English have the smallest PNW, and
Arabic, Basque, and Greek have a high PNW. But
the PNW for Italian is higher than for Arabic and
Greek, and this is also true for the percentage of
new lemmas. Additionally, the highest PNW can be
found in Hungarian and Turkish, which reach higher
scores than Arabic, Basque, and Greek. These con-
siderations suggest that highly inflected languages
with (relatively) free word order need more training
data, a hypothesis that will have to be investigated
further.
There are four languages which were included in
the shared tasks on multilingual dependency pars-
ing both at CoNLL 2006 and at CoNLL 2007: Ara-
bic, Chinese, Czech, and Turkish. For all four lan-
guages, the same treebanks were used, which allows
a comparison of the results. However, in some cases
the size of the training set changed, and at least one
treebank, Turkish, underwent a thorough correction
phase. Table 5 shows the top scores for LAS. Since
the official scores excluded punctuation in 2006 but
includes it in 2007, we give results both with and
without punctuation for both years.
For Arabic and Turkish, we see a great improve-
ment of approximately 9 and 6 percentage points.
For Arabic, the number of tokens in the training
set doubled, and the morphological annotation was
made more informative. The combined effect of
these changes can probably account for the substan-
tial improvement in parsing accuracy. For Turkish,
the training set grew in size as well, although only by
600 sentences, but part of the improvement for Turk-
ish may also be due to continuing efforts in error cor-
927
rection and consistency checking. We see that the
choice to include punctuation or not makes a large
difference for the Turkish scores, since non-final IGs
of a word are counted as punctuation (because they
have the underscore character as their FORM value),
which means that word-internal dependency links
are included if punctuation is included.9 However,
regardless of whether we compare scores with or
without punctuation, we see a genuine improvement
of approximately 6 percentage points.
For Chinese, the same training set was used.
Therefore, the drop from last year?s top score to this
year?s is surprising. However, last year?s top scor-
ing system for Chinese (Riedel et al, 2006), which
did not participate this year, had a score that was
more than 3 percentage points higher than the sec-
ond best system for Chinese. Thus, if we compare
this year?s results to the second best system, the dif-
ference is approximately 2 percentage points. This
final difference may be attributed to the properties of
the test sets. While last year?s test set was taken from
the treebank, this year?s test set contains texts from
other sources. The selection of the textual basis also
significantly changed average sentence length: The
Chinese training set has an average sentence length
of 5.9. Last year?s test set alo had an average sen-
tence length of 5.9. However, this year, the average
sentence length is 7.5 tokens, which is a significant
increase. Longer sentences are typically harder to
parse due to the increased likelihood of ambiguous
constructions.
Finally, we note that the performance for Czech
is almost exactly the same as last year, despite the
fact that the size of the training set has been reduced
to approximately one third of last year?s training set.
It is likely that this in fact represents a relative im-
provement compared to last year?s results.
6.1.2 Across Systems
The LAS over all languages ranges from 80.32 to
54.55. The comparison of the system ranks aver-
aged over all languages with the ranks for single lan-
9The decision to include word-internal dependencies in this
way can be debated on the grounds that they can be parsed de-
terministically. On the other hand, they typically correspond to
regular dependencies captured by function words in other lan-
guages, which are often easy to parse as well. It is therefore
unclear whether scores are more inflated by including word-
internal dependencies or deflated by excluding them.
guages show considerably more variation than last
year?s systems. Buchholz and Marsi (2006) report
that ?[f]or most parsers, their ranking differs at most
a few places from their overall ranking?. This year,
for all of the ten best performing systems with re-
spect to LAS, there is at least one language for which
their rank is at least 5 places different from their
overall rank. The most extreme case is the top per-
forming Nilsson system (Hall et al, 2007a), which
reached rank 1 for five languages and rank 2 for
two more languages. Their only outlier is for Chi-
nese, where the system occupies rank 14, with a
LAS approximately 9 percentage points below the
top scoring system for Chinese (Sagae and Tsujii,
2007). However, Hall et al (2007a) point out that
the official results for Chinese contained a bug, and
the true performance of their system was actually
much higher. The greatest improvement of a sys-
tem with respect to its average rank occurs for En-
glish, for which the system by Nguyen et al (2007)
improved from the average rank 15 to rank 6. Two
more outliers can be observed in the system of Jo-
hansson and Nugues (2007b), which improves from
its average rank 12 to rank 4 for Basque and Turkish.
The authors attribute this high performance to their
parser?s good performance on small training sets.
However, this hypothesis is contradicted by their re-
sults for Greek and Italian, the other two languages
with small training sets. For these two languages,
the system?s rank is very close to its average rank.
6.1.3 An Experiment in System Combination
Having the outputs of many diverse dependency
parsers for standard data sets opens up the interest-
ing possibility of parser combination. To combine
the outputs of each parser we used the method of
Sagae and Lavie (2006). This technique assigns to
each possible labeled dependency a weight that is
equal to the number of systems that included the de-
pendency in their output. This can be viewed as
an arc-based voting scheme. Using these weights
it is possible to search the space of possible depen-
dency trees using directed maximum spanning tree
algorithms (McDonald et al, 2005). The maximum
spanning tree in this case is equal to the tree that on
average contains the labeled dependencies that most
systems voted for. It is worth noting that variants
of this scheme were used in two of the participating
928
5 10 15 20Number of Systems
80
82
84
86
88
Accu
racy
Unlabeled AccuracyLabeled Accuracy
Figure 1: System Combination
systems, the Nilsson system (Hall et al, 2007a) and
the system of Sagae and Tsujii (2007).
Figure 1 plots the labeled and unlabeled accura-
cies when combining an increasing number of sys-
tems. The data used in the plot was the output of all
competing systems for every language in the mul-
tilingual track. The plot was constructed by sort-
ing the systems based on their average labeled accu-
racy scores over all languages, and then incremen-
tally adding each system in descending order.10 We
can see that both labeled and unlabeled accuracy are
significantly increased, even when just the top three
systems are included. Accuracy begins to degrade
gracefully after about ten different parsers have been
added. Furthermore, the accuracy never falls below
the performance of the top three systems.
6.2 Domain Adaptation Track
For this task, the results are rather surprising. A look
at the LAS and UAS for the chemical research ab-
stracts shows that there are four closed systems that
outperform the best scoring open system. The best
system (Sagae and Tsujii, 2007) reaches an LAS of
81.06 (in comparison to their LAS of 89.01 for the
English data set in the multilingual track). Consider-
ing that approximately one third of the words of the
chemical test set are new, the results are noteworthy.
The next surprise is to be found in the relatively
low UAS for the CHILDES data. At a first glance,
this data set has all the characteristics of an easy
10The reason that there is no data point for two parsers is
that the simple voting scheme adopted only makes sense with at
least three parsers voting.
set; the average sentence is short (12.9 words), and
the percentage of new words is also small (6.10%).
Despite these characteristics, the top UAS reaches
62.49 and is thus more than 10 percentage points
below the top UAS for the chemical data set. One
major reason for this is that auxiliary and main
verb dependencies are annotated differently in the
CHILDES data than in the WSJ training set. As a
result of this discrepancy, participants were not re-
quired to submit results for the CHILDES data. The
best performing system on the CHILDES corpus is
an open system (Bick, 2007), but the distance to
the top closed system is approximately 1 percent-
age point. In this domain, it seems more feasible to
use general language resources than for the chemi-
cal domain. However, the results prove that the extra
effort may be unnecessary.
7 Conclusion
Two years of dependency parsing in the CoNLL
shared task has brought an enormous boost to the
development of dependency parsers for multiple lan-
guages (and to some extent for multiple domains).
But even though nineteen languages have been cov-
ered by almost as many different parsing and learn-
ing approaches, we still have only vague ideas about
the strengths and weaknesses of different methods
for languages with different typological characteris-
tics. Increasing our knowledge of the multi-causal
relationship between language structure, annotation
scheme, and parsing and learning methods probably
remains the most important direction for future re-
search in this area. The outputs of all systems for all
data sets from the two shared tasks are freely avail-
able for research and constitute a potential gold mine
for comparative error analysis across languages and
systems.
For domain adaptation we have barely scratched
the surface so far. But overcoming the bottleneck
of limited annotated resources for specialized do-
mains will be as important for the deployment of
human language technology as being able to handle
multiple languages in the future. One result from
the domain adaptation track that may seem surpris-
ing at first is the fact that closed class systems out-
performed open class systems on the chemical ab-
stracts. However, it seems that the major problem in
929
adapting pre-existing parsers to the new domain was
not the domain as such but the mapping from the
native output of the parser to the kind of annotation
provided in the shared task data sets. Thus, find-
ing ways of reusing already invested development
efforts by adapting the outputs of existing systems
to new requirements, without substantial loss in ac-
curacy, seems to be another line of research that may
be worth pursuing.
Acknowledgments
First and foremost, we want to thank all the peo-
ple and organizations that generously provided us
with treebank data and helped us prepare the data
sets and without whom the shared task would have
been literally impossible: Otakar Smrz, Charles
University, and the LDC (Arabic); Maxux Aranz-
abe, Kepa Bengoetxea, Larraitz Uria, Koldo Go-
jenola, and the University of the Basque Coun-
try (Basque); Ma. Anto`nia Mart?? Anton??n, Llu??s
Ma`rquez, Manuel Bertran, Mariona Taule?, Difda
Monterde, Eli Comelles, and CLiC-UB (Cata-
lan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming
Hsieh, and Academia Sinica (Chinese); Jan Hajic?,
Zdenek Zabokrtsky, Charles University, and the
LDC (Czech); Brian MacWhinney, Eric Davis, the
CHILDES project, the Penn BioIE project, and
the LDC (English); Prokopis Prokopidis and ILSP
(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun-
garian); Giuseppe Attardi, Simonetta Montemagni,
Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril
Ribarov, Alessandro Lenci, Nicoletta Calzolari,
ILC, and ELRA (Italian); Gu?ls?en Eryig?it, Kemal
Oflazer, and Ruket C?ak?c? (Turkish).
Secondly, we want to thank the organizers of last
year?s shared task, Sabine Buchholz, Amit Dubey,
Erwin Marsi, and Yuval Krymolowski, who solved
all the really hard problems for us and answered all
our questions, as well as our colleagues who helped
review papers: Jason Baldridge, Sabine Buchholz,
James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju-
lia Hockenmaier, Yuval Krymolowski, Erwin Marsi,
Bea?ta Megyesi, Yannick Versley, and Alexander
Yeh. Special thanks to Bertjan Busser and Erwin
Marsi for help with the CoNLL shared task website
and many other things, and to Richard Johansson for
letting us use his conversion tool for English.
Thirdly, we want to thank the program chairs
for EMNLP-CoNLL 2007, Jason Eisner and Taku
Kudo, the publications chair, Eric Ringger, the
SIGNLL officers, Antal van den Bosch, Hwee Tou
Ng, and Erik Tjong Kim Sang, and members of the
LDC staff, Tony Castelletto and Ilya Ahtaridis, for
great cooperation and support.
Finally, we want to thank the following people,
who in different ways assisted us in the organi-
zation of the CoNLL 2007 shared task: Giuseppe
Attardi, Eckhard Bick, Matthias Buch-Kromann,
Xavier Carreras, Tomaz Erjavec, Svetoslav Mari-
nov, Wolfgang Menzel, Xue Nianwen, Gertjan van
Noord, Petya Osenova, Florian Schiel, Kiril Simov,
Zdenka Uresova, and Heike Zinsmeister.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT).
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
E. Bick. 2007. Hybrid ways to improve domain inde-
pendence in an ML dependency parser. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
the Tenth Conf. on Computational Natural Language
Learning (CoNLL).
S. Canisius and E. Tjong Kim Sang. 2007. A constraint
satisfaction approach to dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
930
X. Carreras. 2007. Experiments with a high-order pro-
jective dependency parser. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of the First Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
W. Chen, Y. Zhang, and H. Isahara. 2007. A two-stage
parser for multilingual dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
H. Daume? and D. Marcu. 2006. Domain adaptation for
statistical classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
M. Dredze, J. Blitzer, P. P. Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly hard do-
main adaptation for dependency parsing. In Proc. of
the CoNLL 2007 Shared Task. EMNLP-CoNLL.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
G. Eryig?it. 2007. ITU validation set for Metu-Sabanc?
Turkish Treebank. URL: http://www3.itu.edu.tr/
?gulsenc/papers/validationset.pdf.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, A. Luo, N. Nicolov, and S. Roukos. 2004. A
statisical model for multilingual entity detection and
tracking. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP).
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools.
J. Hall, J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007a. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
K. Hall, J. Havelka, and D. Smith. 2007b. Log-linear
models of non-projective trees, k-best MST parsing
and tree-ranking. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Johansson and P. Nugues. 2007a. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conf. on Computational Lin-
guistics (NODALIDA).
R. Johansson and P. Nugues. 2007b. Incremental depen-
dency parsing using online learning. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of the Sixth
Conf. on Computational Language Learning (CoNLL).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
P. R. Mannem. 2007. Online learning for determinis-
tic dependency parsing. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
S. Marinov. 2007. Covington variations. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
931
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of the 11th Conf. of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Methods
in Natural Language Processing (HLT/EMNLP).
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
T. Nakagawa. 2007. Multilingual dependency parsing
using Gibbs sampling. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
L.-M. Nguyen, T.-P. Nguyen, and A. Shimazu. 2007. A
multilingual dependency analysis system using online
passive-aggressive learning. In Proc. of the CoNLL
2007 Shared Task. EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13:95?135.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT).
S. Riedel, R. C?ak?c?, and I. Meza-Ruiz. 2006. Multi-
lingual dependency parsing with incremental integer
linear programming. In Proc. of the Tenth Conf. on
Computational Natural Language Learning (CoNLL).
B. Roark and M. Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In
Proc. of the Human Language Technology Conf. and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conf. of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT/NAACL).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
M. Schiehlen and Kristina Spranger. 2007. Global learn-
ing of labelled dependency trees. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
G. Schneider, K. Kaljurand, F. Rinaldi, and T. Kuhn.
2007. Pro3Gres parser in the CoNLL domain adap-
tation shared task. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
N. Shimizu and H. Nakagawa. 2007. Structural corre-
spondence learning for dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL).
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Watson and T. Briscoe. 2007. Adapting the RASP
system for the CoNLL07 domain-adaptation task. In
Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
Y.-C. Wu, J.-C. Yang, and Y.-S. Lee. 2007. Multi-
lingual deterministic dependency parsing framework
using modified finite Newton method support vector
machines. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT).
932
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 514?522,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Sentiment Summarization: Evaluating and Learning User Preferences
Kevin Lerman
Columbia University
New York, NY
klerman@cs.columbia.edu
Sasha Blair-Goldensohn
Google, Inc.
New York, NY
sasha@google.com
Ryan McDonald
Google, Inc.
New York, NY
ryanmcd@google.com
Abstract
We present the results of a large-scale,
end-to-end human evaluation of various
sentiment summarization models. The
evaluation shows that users have a strong
preference for summarizers that model
sentiment over non-sentiment baselines,
but have no broad overall preference be-
tween any of the sentiment-based models.
However, an analysis of the human judg-
ments suggests that there are identifiable
situations where one summarizer is gener-
ally preferred over the others. We exploit
this fact to build a new summarizer by
training a ranking SVM model over the set
of human preference judgments that were
collected during the evaluation, which re-
sults in a 30% relative reduction in error
over the previous best summarizer.
1 Introduction
The growth of the Internet as a commerce
medium, and particularly the Web 2.0 phe-
nomenon of user-generated content, have resulted
in the proliferation of massive numbers of product,
service and merchant reviews. While this means
that users have plenty of information on which to
base their purchasing decisions, in practice this is
often too much information for a user to absorb.
To alleviate this information overload, research on
systems that automatically aggregate and summa-
rize opinions have been gaining interest (Hu and
Liu, 2004a; Hu and Liu, 2004b; Gamon et al,
2005; Popescu and Etzioni, 2005; Carenini et al,
2005; Carenini et al, 2006; Zhuang et al, 2006;
Blair-Goldensohn et al, 2008).
Evaluating these systems has been a challenge,
however, due to the number of human judgments
required to draw meaningful conclusions. Of-
ten systems are evaluated piecemeal, selecting
pieces that can be evaluated easily and automati-
cally (Blair-Goldensohn et al, 2008). While this
technique produces meaningful evaluations of the
selected components, other components remain
untested, and the overall effectiveness of the entire
system as a whole remains unknown. When sys-
tems are evaluated end-to-end by human judges,
the studies are often small, consisting of only a
handful of judges and data points (Carenini et
al., 2006). Furthermore, automated summariza-
tion metrics like ROUGE (Lin and Hovy, 2003)
are non-trivial to adapt to this domain as they re-
quire human curated outputs.
We present the results of a large-scale, end-to-
end human evaluation of three sentiment summa-
rization models applied to user reviews of con-
sumer products. The evaluation shows that there
is no significant difference in rater preference be-
tween any of the sentiment summarizers, but that
raters do prefer sentiment summarizers over non-
sentiment baselines. This indicates that even sim-
ple sentiment summarizers provide users utility.
An analysis of the rater judgments also indicates
that there are identifiable situations where one sen-
timent summarizer is generally preferred over the
others. We attempt to learn these preferences by
training a ranking SVM that exploits the set of
preference judgments collected during the evalu-
ation. Experiments show that the ranking SVM
summarizer?s cross-validation error decreases by
as much as 30% over the previous best model.
Human evaluations of text summarization have
been undertaken in the past. McKeown et al
(2005) presented a task-driven evaluation in the
news domain in order to understand the utility of
different systems. Also in the news domain, the
Document Understanding Conference1 has run a
number of multi-document and query-driven sum-
marization shared-tasks that have used a wide
1http://duc.nist.gov/
514
iPod Shuffle: 4/5 stars
?In final analysis the iPod Shuffle is a decent player that offers a sleek
compact form factor an excessively simple user interface and a low
price? ... ?It?s not good for carrying a lot of music but for a little bit of
music you can quickly grab and go with this nice little toy? ... ?Mine came
in a nice bright orange color that makes it easy to locate.?
Figure 1: An example summary.
range of automatic and human-based evaluation
criteria. This year, the new Text Analysis Con-
ference2 is running a shared-task that contains an
opinion component. The goal of that evaluation is
to summarize answers to opinion questions about
entities mentioned in blogs.
Our work most closely resembles the evalua-
tions in Carenini et al (2006, 2008). Carenini et
al. (2006) had raters evaluate extractive and ab-
stractive summarization systems. Mirroring our
results, they show that both extractive and abstrac-
tive summarization outperform a baseline, but that
overall, humans have no preference between the
two. Again mirroring our results, their analysis in-
dicates that even though there is no overall differ-
ence, there are situations where one system gener-
ally outperforms the other. In particular, Carenini
and Cheung (2008) show that an entity?s contro-
versiality, e.g., mid-range star rating, is correlated
with which summary has highest value.
The study presented here differs from Carenini
et al in many respects: First, our evaluation is
over different extractive summarization systems in
an attempt to understand what model properties
are correlated with human preference irrespective
of presentation; Secondly, our evaluation is on a
larger scale including hundreds of judgments by
hundreds of raters; Finally, we take a major next
step and show that it is possible to automatically
learn significantly improved models by leveraging
data collected in a large-scale evaluation.
2 Sentiment Summarization
A standard setting for sentiment summarization
assumes a set of documents D = {d1, . . . , dm}
that contain opinions about some entity of interest.
The goal of the system is to generate a summary S
of that entity that is representative of the average
opinion and speaks to its important aspects. An
example summary is given in figure 1. For sim-
plicity we assume that all opinions in D are about
the entity being summarized. When this assump-
tion fails, one can parse opinions at a finer-level
2http://www.nist.gov/tac/
(Jindal and Liu, 2006; Stoyanov and Cardie, 2008)
In this study, we look at an extractive summa-
rization setting where S is built by extracting rep-
resentative bits of text from the set D, subject to
pre-specified length constraints. Specifically, as-
sume each document di is segmented into can-
didate text excerpts. For ease of discussion we
will assume all excerpts are sentences, but in prac-
tice they can be phrases or multi-sentence groups.
Viewed this way, D is a set of candidate sentences
for our summary, D = {s1, . . . , sn}, and summa-
rization becomes the following optimization:
argmax
S?D
L(S) s.t.: LENGTH(S) ? K (1)
where L is some score over possible summaries,
LENGTH(S) is the length of the summary and K
is the pre-specified length constraint. The defini-
tion of L will be the subject of much of this sec-
tion and it is precisely different forms of L that
will be compared in our evaluation. The nature of
LENGTH is specific to the particular use case.
Solving equation 1 is typically NP-hard, even
under relatively strong independence assumptions
between the sentences selected for the summary
(McDonald, 2007). In cases where solving L is
non-trivial we use an approximate hill climbing
technique. First we randomly initialize the sum-
mary S to length ?K. Then we greedily in-
sert/delete/swap sentences in and out of the sum-
mary to maximize L(S) while maintaining the
bound on length. We run this procedure until no
operation leads to a higher scoring summary. In
all our experiments convergence was quick, even
when employing random restarts.
Alternate formulations of sentiment summa-
rization are possible, including aspect-based sum-
marization (Hu and Liu, 2004a), abstractive sum-
marization (Carenini et al, 2006) or related tasks
such as opinion attribution (Choi et al, 2005). We
choose a purely extractive formulation as it makes
it easier to develop baselines and allows raters to
compare summaries with a simple, consistent pre-
sentation format.
2.1 Definitions
Before delving into the details of the summariza-
tion models we must first define some useful func-
tions. The first is the sentiment polarity func-
tion that maps a lexical item t, e.g., word or short
phrase, to a real-valued score,
LEX-SENT(t) ? [?1, 1]
515
The LEX-SENT function maps items with positive
polarity to higher values and items with negative
polarity to lower values. To build this function we
constructed large sentiment lexicons by seeding a
semantic word graph induced from WordNet with
positive and negative examples and then propagat-
ing this score out across the graph with a decaying
confidence. This method is common among sen-
timent analysis systems (Hu and Liu, 2004a; Kim
and Hovy, 2004; Blair-Goldensohn et al, 2008).
In particular, we use the lexicons that were created
and evaluated by Blair-Goldensohn et al (2008).
Next we define sentiment intensity,
INTENSITY(s) =
?
t?s
|LEX-SENT(t)|
which simply measures the magnitude of senti-
ment in a sentence. INTENSITY can be viewed as a
measure of subjectiveness irrespective of polarity.
A central function in all our systems is a sen-
tences normalized sentiment,
SENT(s) =
?
t?s LEX-SENT(t)
?+ INTENSITY(s)
This function measures the (signed) ratio of lexical
sentiment to intensity in a sentence. Sentences that
only contain lexical items of the same polarity will
have high absolute normalized sentiment, whereas
sentences with mixed polarity items or no polar-
ity items will have a normalized sentiment near
zero. We include the constant ? in the denomi-
nator so that SENT gives higher absolute scores to
sentences containing many strong sentiment items
of the same polarity over sentences with a small
number of weak items of the same polarity.
Most sentiment summarizers assume that as in-
put, a system is given an overall rating of the en-
tity it is attempting to summarize, R ? [?1, 1],
where a higher rating indicates a more favorable
opinion. This rating may be obtained directly from
user provided information (e.g., star ratings) or au-
tomatically derived by averaging the SENT func-
tion over all sentences in D. Using R, we can de-
fine a mismatch function between the sentiment of
a summary and the known sentiment of the entity,
MISMATCH(S) = (R?
1
|S|
?
si?S
SENT(si))
2
Summaries with a higher mismatch are those
whose sentiment disagrees most with R.
Another key input many sentiment summarizers
assume is a list of salient entity aspects, which are
specific properties of an entity that people tend to
rate when expressing their opinion. For example,
aspects of a digital camera could include picture
quality, battery life, size, color, value, etc. Find-
ing such aspects is a challenging research problem
that has been addressed in a number of ways (Hu
and Liu, 2004b; Gamon et al, 2005; Carenini et
al., 2005; Zhuang et al, 2006; Branavan et al,
2008; Blair-Goldensohn et al, 2008; Titov and
McDonald, 2008b; Titov and McDonald, 2008a).
We denote the set of aspects for an entity as A and
each aspect as a ? A. Furthermore, we assume
that given A it is possible to determine whether
some sentence s ? D mentions an aspect in A.
For our experiments we use a hybrid supervised-
unsupervised method for finding aspects as de-
scribed and evaluated in Blair-Goldensohn et al
(2008).
Having defined what an aspect is, we next de-
fine a summary diversity function over aspects,
DIVERSITY(S) =
?
a?A
COVERAGE(a)
where COVERAGE(a) ? R is a function that
weights how well the aspect is covered in the
summary and is proportional to the importance of
the aspect as some aspects are more important to
cover than others, e.g., ?picture quality? versus
?strap? for digital cameras. The diversity func-
tion rewards summaries that cover many important
aspects and plays the redundancy reducing role
that is common in most extractive summarization
frameworks (Goldstein et al, 2000).
2.2 Systems
For our evaluation we developed three extractive
sentiment summarization systems. Each system
models increasingly complex objectives.
2.2.1 Sentiment Match (SM)
The first system that we look at attempts to ex-
tract sentences so that the average sentiment of the
summary is as close as possible to the entity level
sentiment R, which was previously defined in sec-
tion 2.1. In this case L can be simply defined as,
L(S) = ?MISMATCH(S)
Thus, the model prefers summaries with average
sentiment as close as possible to the average sen-
timent across all the reviews.
516
There is an obvious problem with this model.
For entities that have a mediocre rating, i.e., R ?
0, the model could prefer a summary that only
contains sentences with no opinion whatsoever.
There are two ways to alleviate this problem. The
first is to include the INTENSITY function into L,
L(S) = ? ? INTENSITY(S)? ? ? MISMATCH(S)
Where the coefficients allow one to trade-off sen-
timent intensity versus sentiment mismatch.
The second method, and the one we chose based
on initial experiments, was to address the problem
at inference time. This is done by prohibiting the
algorithm from including a given positive or nega-
tive sentence in the summary if another more pos-
itive/negative sentence is not included. Thus the
summary is forced to consist of only the most pos-
itive and most negative sentences, the exact mix
being dependent upon the overall star rating.
2.2.2 Sentiment Match + Aspect Coverage
(SMAC)
The SM model extracts sentences for the summary
without regard to the content of each sentence rel-
ative to the others in the summary. This is in con-
trast to standard summarization models that look
to promote sentence diversity in order to cover as
many important topics as possible (Goldstein et
al., 2000). The sentiment match + aspect cov-
erage system (SMAC) attempts to model diver-
sity by building a summary that trades-off max-
imally covering important aspects with matching
the overall sentiment of the entity. The model does
this through the following linear score,
L(S) = ? ? INTENSITY(S)? ? ? MISMATCH(S)
+? ? DIVERSITY(S)
This score function rewards summaries for be-
ing highly subjective (INTENSITY), reflecting the
overall product rating (MISMATCH), and covering
a variety of product aspects (DIVERSITY). The co-
efficients were set by inspection.
This system has its roots in event-based summa-
rization (Filatova and Hatzivassiloglou, 2004) for
the news domain. In that work an optimization
problem was developed that attempted to maxi-
mize summary informativeness while covering as
many (weighted) sub-events as possible.
2.2.3 Sentiment-Aspect Match (SAM)
Because the SMAC model only utilizes an entity?s
overall sentiment when calculating MISMATCH, it
is susceptible to degenerate solutions. Consider a
product with aspects A and B, where reviewers
overwhelmingly like A and dislike B, resulting in
an overall SENT close to zero. If the SMAC model
finds a very negative sentence describing A and
a very positive sentence describing B, it will as-
sign that summary a high score, as the summary
has high intensity, has little overall mismatch, and
covers both aspects. However, in actuality, the
summary is entirely misleading.
To address this issue, we constructed the
sentiment-aspect match model (SAM), which not
only attempts to cover important aspects, but cover
them with appropriate sentiment. There are many
ways one might design a model to do this, includ-
ing linear combinations of functions similar to the
SMAC model. However, we decided to employ a
probabilistic approach as it provided performance
benefits based on development data experiments.
Under the SAM model, each sentence is treated as
a bag of aspects and their corresponding mentions?
sentiments. For a given sentence s, we define As
as the set of aspects mentioned within it. For a
given aspect a ? As, we denote SENT(as) as the
sentiment associated with the textual mention of a
in s. The probability of a sentence is defined as,
p(s) = p(a1, . . . , an, SENT(a1s), . . . , SENT(a
n
s ))
which can be re-written as,
?
a?As
p(a, SENT(as)) =
?
a?As
p(a)p(SENT(as)|a)
if we assume aspect mentions are generated inde-
pendently of one another. Thus we need to esti-
mate both p(a) and p(SENT(as)|a). The probabil-
ity of seeing an aspect, p(a), is simply set to the
maximum likelihood estimates over the data set
D. Furthermore, we assume that p(SENT(as)|a)
is normal about the mean sentiment for the as-
pect ?a with a constant standard deviation, ?a.
The mean and standard deviation are estimated
straight-forwardly using the data set D. Note that
the number of parameters our system must es-
timate is very small. For every possible aspect
a ? A we need three values: p(a), ?a, and ?a.
Since |A| is typically small ? on the order of 5-10
? it is not difficult to estimate these models even
from small sets of data.
Having constructed this model, one logical ap-
proach to summarization would be to select sen-
tences for the summary that have highest proba-
bility under the model trained on D. We found,
517
however, that this produced very redundant sum-
maries ? if one aspect is particularly prevalent in
a product?s reviews, this approach will select all
sentences about that aspect, and discuss nothing
else. To combat this we developed a technique that
scores the summary as a whole, rather than by in-
dividual components. First, denote SAM(D) as the
previously described model learned over the set of
entity documents D. Next, denote SAM(S) as an
identical model, but learned over a candidate sum-
mary S, i.e., given a summary S, compute p(a),
ma, and ?a for all a ? A using only the sentences
from S. We can then measure the difference be-
tween these models using KL-divergence:
L(S) = ?KL(SAM(D), SAM(S))
In our case we have 1 + |A| distributions ? p(a),
and p(?|a) for all a ? A ? so we just sum the KL-
divergence of each. The key property of the SAM
system is that it naturally builds summaries where
important aspects are discussed with appropriate
sentiment, since it is precisely these aspects that
will contribute the most to the KL-divergence. It
is important to note that the short length of a can-
didate summary S can make estimates in SAM(S)
rather crude. But we only care about finding the
?best? of a set of crude models, not about finding
one that is ?good? in absolute terms. Between the
few parameters we must learn and the specific way
we use these models, we generally get models use-
ful for our purposes.
Alternatively we could have simply incorpo-
rated the DIVERSITY measure into the objec-
tive function or used an inference algorithm that
specifically accounts for redundancy, e.g., maxi-
mal marginal relevance (Goldstein et al, 2000).
However, we found that this solution was well
grounded and required no tuning of coefficients.
Initial experiments indicated that the SAM sys-
tem, as described above, frequently returned sen-
tences with low intensity when important aspects
had luke-warm sentiment. To combat this we re-
moved low intensity sentences from consideration,
which had the effect of encouraging important
luke-warm aspects to mentioned multiple times in
order to balance the overall sentiment.
Though the particulars of this model are unique,
fundamentally it is closest to the work of Hu and
Liu (2004a) and Carenini et al (2006).
3 Experiments
We evaluated summary performance for reviews
of consumer electronics. In this setting an entity
to be summarized is one particular product, D is
a set of user reviews about that product, and R is
the normalized aggregate star ratings left by users.
We gathered reviews for 165 electronics products
from several online review aggregators. The prod-
ucts covered a variety of electronics, such as MP3
players, digital cameras, printers, wireless routers,
and video game systems. Each product had a min-
imum of four reviews and up to a maximum of
nearly 3000. The mean number of reviews per
product was 148, and the median was 70. We
ran each of our algorithms over the review corpus
and generated summaries for each product with
K = 650. All summaries were roughly equal
length to avoid length-based rater bias3. In total
we ran four experiments for a combined number of
1980 rater judgments (plus additional judgments
during the development phase of this study).
Our initial set of experiments were over the
three opinion-based summarization systems: SM,
SMAC, and SAM. We ran three experiments com-
paring SMAC to SM, SAM to SM, and SAM to
SMAC. In each experiment two summaries of the
same product were placed side-by-side in a ran-
dom order. Raters were also shown an overall rat-
ing, R, for each product (these ratings are often
provided in a form such as ?3.5 of 5 stars?). The
two summaries on either side were shown below
this information with links to the full text of the
reviews for the raters to explore.
Raters were asked to express their preference
for one summary over the other. For two sum-
maries SA and SB they could answer,
1. No preference
2. Strongly preferred SA (or SB)
3. Preferred SA (or SB)
4. Slightly preferred SA (or SB)
Raters were free to choose any rating, but were
specifically instructed that their rating should ac-
count for a summaries representativeness of the
overall set of reviews. Raters were also asked
to provide a brief comment justifying their rat-
ing. Over 100 raters participated in each study,
and each comparison was evaluated by three raters
with no rater making more than five judgments.
3In particular our systems each extracted four text ex-
cerpts of roughly 160-165 characters.
518
Comparison (A v B) Agreement (%) No Preference (%) Preferred A (%) Preferred B (%) Mean Numeric
SM v SMAC 65.4 6.0 52.0 42.0 0.01
SAM v SM 69.3 16.8 46.0 37.2 0.01
SAM v SMAC? 73.9 11.5 51.6 36.9 0.08
SMAC v LT? 64.1 4.1 70.4 25.5 0.24
Table 1: Results of side-by-side experiments. Agreement is the percentage of items for which all raters
agreed on a positive/negative/no-preference rating. No Preference is the percentage of agreement items
in which the raters had no preference. Preferred A/B is the percentage of agreement items in which the
raters preferred either A or B respectively. Mean Numeric is the average of the numeric ratings (converted
from discreet preference decisions) indicating on average the raters preferred system A over B on a scale
of -1 to 1. Positive scores indicate a preference for system A. ? significant at a 95% confidence interval
for the mean numeric score.
We chose to have raters leave pairwise prefer-
ences, rather than evaluate each candidate sum-
mary in isolation, because raters can make a pref-
erence decisions more quickly than a valuation
judgment, which allowed for collection of more
data points. Furthermore, there is evidence that
rater agreement is much higher in preference deci-
sions than in value judgments (Ariely et al, 2008).
Results are shown in the first three rows of ta-
ble 1. The first column of the table indicates the
experiment that was run. The second column indi-
cates the percentage of judgments for which the
raters were in agreement. Agreement here is a
weak agreement, where three raters are defined to
be in agreement if they all gave a no preference rat-
ing, or if there was a preference rating, but no two
preferences conflicted. The next three columns in-
dicate the percentage of judgments for each pref-
erence category, grouped here into three coarse as-
signments. The final column indicates a numeric
average for the experiment. This was calculated
by converting users ratings to a scale of 1 (strongly
preferred SA) to -1 (strongly preferred SB) at 0.33
intervals. Table 1 shows only results for items in
which the raters had agreement in order to draw
reliable conclusions, though the results change lit-
tle when all items are taken into account.
Ultimately, the results indicate that none of the
sentiment summarizers are strongly preferred over
any other. Only the SAM v SMAC model has a
difference that can be considered statistically sig-
nificant. In terms of order we might conclude that
SAM is the most preferred, followed by SM, fol-
lowed by SMAC. However, the slight differences
make any such conclusions tenuous at best. This
leads one to wonder whether raters even require
any complex modeling when summarizing opin-
ions. To test this we took the lowest scoring model
overall, SMAC, and compared it to a leading text
baseline (LT) that simply selects the first sentence
from a ranked list of reviews until the length con-
straint is violated. The results are given in the last
row of 1. Here there is a clear distinction as raters
preferred SMAC to LT, indicating that they did
find usefulness in systems that modeled aspects
and sentiment. However, there are still 25.5%
of agreement items where the raters did choose a
simple leading text baseline.
4 Analysis
Looking more closely at the results we observed
that, even though raters did not strongly prefer
any one sentiment-aware summarizer over another
overall, they mostly did express preferences be-
tween systems on individual pairs of comparisons.
For example, in the SAM vs SM experiment, only
16.8% of the comparisons yielded a ?no prefer-
ence? judgment from all three raters ? by far the
highest percentage of any experiment. This left
83.2% ?slight preference? or higher judgments.
With this in mind we began examining the com-
ments left by raters throughout all our experi-
ments, including a set of additional experiments
used during development of the systems. We ob-
served several trends: 1) Raters tended to pre-
fer summaries with lists, e.g., pros-cons lists; 2)
Raters often did not like text without sentiment,
hence the dislike of the leading text system where
there is no guarantee that the first sentence will
have any sentiment; 3) Raters disliked overly gen-
eral comments, e.g., ?The product was good?.
These statements carry no additional information
over a product?s overall star rating; 4) Raters did
recognize (and strongly disliked) when the overall
sentiment of the summary was inconsistent with
the star rating; 5) Raters tended to prefer different
519
systems depending on what the star rating was. In
particular, the SMAC system was generally pre-
ferred for products with neutral overall ratings,
whereas the SAM system is preferred for products
with ratings at the extremes. We hypothesize that
SAM?s low performance on neutral rated products
is because the system suffers from the dual imper-
atives of selecting high intensity snippets and of
selecting snippets that individually reflect partic-
ular sentiment polarities. When the desired senti-
ment polarity is neutral, it is difficult to find a snip-
pet with lots of sentiment, whose overall polarity
is still neutral, thus SAM may either ignore that
aspect or include multiple mentions of that aspect
at the expense of others; 6) Raters also preferred
summaries with grammatically fluent text, which
benefitted the leading text baseline.
These observations suggest that we could build
a new system that takes into account all these
factors (weighted accordingly) or we could build
a rule-based meta-classifier that selects a single
summary from the four systems described in this
paper based on the global characteristics of each.
The problem with the former is that it will require
hand-tuning of coefficients for many different sig-
nals that are all, for the most part, weakly corre-
lated to summary quality. The problem with the
latter is inefficiency, i.e., it will require the main-
tenance and output of all four systems. In the next
section we explore an alternate method that lever-
ages the data gathered in the evaluation to auto-
matically learn a new model. This approach is
beneficial as it will allow any coefficients to be au-
tomatically tuned and will result in a single model
that can be used to build new summaries.
5 Summarization with Ranking SVMs
Besides allowing us to assess the relative perfor-
mance of our summarizers, our evaluation pro-
duced several hundred points of empirical data in-
dicating which among two summaries raters pre-
fer. In this section we explore how to build im-
proved summarizers with this data by learning
preference ranking SVMs, which are designed to
learn relative to a set of preference judgments
(Joachims, 2002).
A ranking SVM typically assumes as input a set
of queries and associated partial ordering on the
items returned by the query. The training data is
defined as pairs of points, T = {(xki , x
k
j )t}
|T |
t=1,
where each pair indicates that the ith item is pre-
ferred over the jth item for the kth query. Each
input point xki ? R
m is a feature vector repre-
senting the properties of that particular item rel-
ative to the query. The goal is to learn a scoring
function s(xki ) ? R such that s(x
k
i ) > s(x
k
j ) if
(xki , x
k
j ) ? T . In other words, a ranking SVM
learns a scoring function whose induced ranking
over data points respects all preferences in the
training data. The most straight-forward scoring
function, and the one used here, is a linear classi-
fier, s(xki ) = w ? x
k
i , making the goal of learning
to find an appropriate weight vector w ? Rm.
In its simplest form, the ranking SVM opti-
mization problem can be written as the following
quadratic programming problem,
min
1
2
||w||2 s.t.: ?(xki , x
k
j ) ? T ,
s(xki )? s(x
k
j ) ? PREF(x
k
i , x
k
j )
where PREF(xki , x
k
j ) ? R is a function indicating
to what degree item xki is preferred over x
k
j (and
serves as the margin of the classifier). This opti-
mization is well studied and can be solved with a
wide variety of techniques. In our experiments we
used the SVM-light software package4.
Our summarization evaluation provides us with
precisely a large collection of preference points
over different summaries for different product
queries. Thus, we naturally have a training set T
where each query is analogous to a specific prod-
uct of interest and training points are two possi-
ble summarizations produced by two different sys-
tems with corresponding rater preferences. As-
suming an appropriate choice of feature represen-
tation it is straight-forward to then train the model
on our data using standard techniques for SVMs.
To train and test the model we compiled 1906
pairs of summary comparisons, each judged by
three different raters. These pairs were extracted
from the four experiments described in section 3
as well as the additional experiments we ran dur-
ing development. For each pair of summaries
(Ski , S
k
j ) (for some product query indexed by k),
we recorded how many raters preferred each of the
items as vki and v
k
j respectively, i.e., v
k
i is the num-
ber of the three raters who preferred summary Si
over Sj for product k. Note that vki + v
k
j does not
necessarily equal 3 since some raters expressed no
preference between them. We set the loss function
PREF(Ski , S
k
j ) = v
k
i ? v
k
j , which in some cases
4http://svmlight.joachims.org/
520
could be zero, but never negative since the pairs
are ordered. Note that this training set includes all
data points, even those in which raters disagreed.
This is important as the model can still learn from
these points as the margin function PREF encodes
the fact that these judgments are less certain.
We used a variety of features for a candidate
summary: how much capitalization, punctuation,
pros-cons, and (unique) aspects a summary had;
the overall intensity, sentiment, min sentence sen-
timent, and max sentence sentiment in the sum-
mary; the overall ratingR of the product; and con-
junctions of these. Note that none of these fea-
tures encode which system produced the summary
or which experiment it was drawn from. This is
important, as it allows the model to be used as
standalone scoring function, i.e., we can set L to
the learned linear classifier s(S). Alternatively
we could have included features like what system
was the summary produced from. This would have
helped the model learn things like the SMAC sys-
tem is typically preferred for products with mid-
range overall ratings. Such a model could only be
used to rank the outputs of other summarizers and
cannot be used standalone.
We evaluated the trained model by measuring
its accuracy on predicting a single preference pre-
diction, i.e., given pairs of summaries (Ski , S
k
j ),
how accurate is the model at predicting that Si is
preferred to Sj for product query k? We measured
10-fold cross-validation accuracy on the subset of
the data for which the raters were in agreement.
We measure accuracy for both weak agreement
cases (at least one rater indicated a preference and
the other two raters were in agreement or had no
preference) and strong agreement cases (all three
raters indicated the same preference). We ignored
pairs in which all three raters made a no preference
judgment as both summaries can can be consid-
ered equally valid. Furthermore, we ignored pairs
in which two raters indicated conflicting prefer-
ences as there is no gold standard for such cases.
Results are given in table 2. We compare the
ranking SVM summarizer to a baseline system
that always selects the overall-better-performing
summarization system from the experiment that
the given datapoint was drawn from, e.g., for all
the data points drawn from the SAM versus SMAC
experiment, the baseline always chooses the SAM
summary as its preference. Note that in most ex-
periments the two systems emerged in a statistical
Preference Prediction Accuracy
Weak Agr. Strong Agr.
Baseline 54.3% 56.9%
Ranking SVM 61.8% 69.9%
Table 2: Accuracies for learned summarizers.
tie, so this baseline performs only slightly better
than chance. Table 2 clearly shows that the rank-
ing SVM can predict preference accuracy much
better than chance, and much better than that ob-
tained by using only one summarizer (a reduction
in error of 30% for strong agreement cases).
We can thus conclude that the data gathered
in human preference evaluation experiments, such
as the one presented here, have a beneficial sec-
ondary use as training data for constructing a new
and more accurate summarizer. This raises an
interesting line of future research: can we iter-
ate this process to build even better summariz-
ers? That is, can we use this trained summarizer
(and variants of it) to generate more examples for
raters to judge, and then use that data to learn even
more powerful summarizers, which in turn could
be used to generate even more training judgments,
etc. This could be accomplished using Mechani-
cal Turk5 or another framework for gathering large
quantities of cheap annotations.
6 Conclusions
We have presented the results of a large-scale eval-
uation of different sentiment summarization algo-
rithms. In doing so, we explored different ways
of using sentiment and aspect information. Our
results indicated that humans prefer sentiment in-
formed summaries over a simple baseline. This
shows the usefulness of modeling sentiment and
aspects when summarizing opinions. However,
the evaluations also show no strong preference be-
tween different sentiment summarizers. A detailed
analysis of the results led us to take the next step
in this line of research ? leveraging preference
data gathered in human evaluations to automati-
cally learn new summarization models. These new
learned models show large improvements in pref-
erence prediction accuracy over the previous sin-
gle best model.
Acknowledgements: The authors would like to
thank Kerry Hannan, Raj Krishnan, Kristen Parton
and Leo Velikovich for insightful discussions.
5http://www.mturk.com
521
References
D. Ariely, G. Loewenstein, and D. Prelec. 2008. Co-
herent arbitrariness: Stable demand curves without
stable preferences. The Quarterly Journal of Eco-
nomics, 118:73105.
S. Blair-Goldensohn, K. Hannan, R. McDonald,
T. Neylon, G.A. Reis, and J. Reynar. 2008. Building
a sentiment summarizer for local service reviews. In
WWW Workshop on NLP in the Information Explo-
sion Era.
S.R.K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic prop-
erties from free-text annotations. In Proceedings of
the Annual Conference of the Association for Com-
putational Linguistics (ACL).
G. Carenini and J. Cheung. 2008. Extractive vs. nlg-
based abstractive summarization of evaluative text:
The effect of corpus controversiality. In Interna-
tional Conference on Natural Language Generation
(INLG).
G. Carenini, R.T. Ng, and E. Zwart. 2005. Extract-
ing knowledge from evaluative text. In Proceedings
of the International Conference on Knowledge Cap-
ture.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-
document summarization of evaluative text. In Pro-
ceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL).
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005.
Identifying sources of opinions with conditional ran-
dom fields and extraction patterns. In Proceedings
the Joint Conference on Human Language Technol-
ogy and Empirical Methods in Natural Language
Processing (HLT-EMNLP).
E. Filatova and V. Hatzivassiloglou. 2004. A formal
model for information selection in multi-sentence
text extraction. In Proceedings of the International
Conference on Computational Linguistics (COL-
ING).
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free
text. In Proceedings of the 6th International Sympo-
sium on Intelligent Data Analysis (IDA).
J. Goldstein, V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Proceedings
of the ANLP/NAACL Workshop on Automatic
Summarization.
M. Hu and B. Liu. 2004a. Mining and summariz-
ing customer reviews. In Proceedings of the Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD).
M. Hu and B. Liu. 2004b. Mining opinion features in
customer reviews. In Proceedings of National Con-
ference on Artificial Intelligence (AAAI).
N. Jindal and B. Liu. 2006. Mining comprative sen-
tences and relations. In Proceedings of 21st Na-
tional Conference on Artificial Intelligence (AAAI).
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
S.M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of Conference on
Computational Linguistics (COLING).
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram cooccurrence statistics.
In Proceedings of the Conference on Human Lan-
guage Technologies and the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL).
R. McDonald. 2007. A Study of Global Inference
Algorithms in Multi-document Summarization. In
Proceedings of the European Conference on Infor-
mation Retrieval (ECIR).
K. McKeown, R.J. Passonneau, D.K. Elson,
A. Nenkova, and J. Hirschberg. 2005. Do
Summaries Help? A Task-Based Evaluation of
Multi-Document Summarization. In Proceedings
of the ACM SIGIR Conference on Research and
Development in Information Retrieval.
A.M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings of
the Conference on Computational Linguistics (COL-
ING).
I. Titov and R. McDonald. 2008a. A joint model of
text and aspect ratings. In Proceedings of the An-
nual Conference of the Association for Computa-
tional Linguistics (ACL).
I. Titov and R. McDonald. 2008b. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the Annual World Wide Web Conference
(WWW).
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings
of the International Conference on Information and
Knowledge Management (CIKM).
522
Proceedings of NAACL HLT 2009: Short Papers, pages 113?116,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Contrastive Summarization: An Experiment with Consumer Reviews
Kevin Lerman
Columbia University
New York, NY
klerman@cs.columbia.edu
Ryan McDonald
Google Inc.
New York, NY
ryanmcd@google.com
Abstract
Contrastive summarization is the problem of
jointly generating summaries for two entities
in order to highlight their differences. In this
paper we present an investigation into con-
trastive summarization through an implemen-
tation and evaluation of a contrastive opinion
summarizer in the consumer reviews domain.
1 Introduction
Automatic summarization has historically focused
on summarizing events, a task embodied in the
series of Document Understanding Conferences1.
However, there has also been work on entity-centric
summarization, which aims to produce summaries
from text collections that are relevant to a particu-
lar entity of interest, e.g., product, person, company,
etc. A well-known example of this is from the opin-
ion mining community where there has been a num-
ber of studies on summarizing the expressed senti-
ment towards entities (cf. Hu and Liu (2006)). An-
other recent example of entity-centric summariza-
tion is the work of Filippova et al (2009) to produce
company-specific financial report summaries.
In this study we investigate a variation of entity-
centric summarization where the goal is not to sum-
marize information about a single entity, but pairs
of entities. Specifically, our aim is to jointly gen-
erate two summaries that highlight differences be-
tween the entities ? a task we call contrastive sum-
marization. An obvious application comes from the
consumer reviews domain, where a person consider-
ing a purchase wishes to see the differences in opin-
ion about the top candidates without reading all the
reviews for each product. Other applications include
1http://duc.nist.gov/
contrasting financial news about related companies
or comparing platforms of political candidates.
Contrastive summarization has many points of
comparison in the NLP, IR and Data-Mining liter-
ature. Jindal and Liu (2006) introduce techniques
to find and analyze explicit comparison sentences,
but this assumes that such sentences exist. In con-
trastive summarization, there is no assumption that
two entities have been explicitly compared. The
goal is to automatically generate the comparisons
based on the data. In the IR community, Sun et
al. (2006) explores retrieval systems that align query
results to highlight points of commonality and dif-
ference. In contrast, we attempt to identify con-
trasts from the data, and then generate summaries
that highlight them. The novelty detection task of
determining whether a new text in a collection con-
tains information distinct from that already gathered
is also related (Soboroff and Harman, 2005). The
primary difference here is that contrastive summa-
rization aims to extract information from one col-
lection not present in the other in addition to infor-
mation present in both collections that highlights a
difference between the entities.
This paper describes a contrastive summarization
experiment where the goal is to generate contrasting
opinion summaries of two products based on con-
sumer reviews of each. We look at model design
choices, describe an implementation of a contrastive
summarizer, and provide an evaluation demonstrat-
ing a significant improvement in the usefulness of
contrastive summaries versus summaries generated
by single-product opinion summarizers.
2 Single-Product Opinion Summarization
As input we assume a set of relevant text excerpts
(typically sentences), T = {t1, . . . , tm}, which con-
113
tain opinions about some product of interest. The
goal of opinion summarization2 is to select some
number of text excerpts to form a summary S of
the product so that S is representative of the aver-
age opinion and speaks to its important aspects (also
proportional to opinion), which we can formalize as:
S = argmax
S?T
L(S) s.t. LENGTH(S) ? K
where L is some score over possible summaries that
embodies what a user might desire in an opinion
summary, LENGTH(S) is the length of the summary
and K is a pre-specified length constraint.
We assume the existence of standard sentiment
analysis tools to provide the information used in the
scoring function L. First, we assume the tools can
assign a sentiment score from -1 (negative) to 1 (pos-
itive) to an arbitrary span of text. Second, we as-
sume that we can extract a set of aspects that the text
is discussing (e.g, ?The sound was crystal clear? is
about the aspect sound quality). We refer the reader
to abundance of literature on sentiment analysis for
more details on how such tools can be constructed
(cf. Pang and Lee (2008)). For this study, we use
the tools described and evaluated in Lerman et al
(2009). We note however, that the subject of this
discussion is not the tools themselves, but their use.
The single product opinion summarizer we con-
sider is the Sentiment Aspect Match model (SAM)
described and evaluated in (Lerman et al, 2009).
Underlying SAM is the assumption that opinions
can be described by a bag-of-aspects generative pro-
cess where each aspect is generated independently
and the sentiment associated with the aspect is gen-
erated conditioned on its identity,
p(t) = ?
a?At
p(a)p(SENT(at)|a)
where At is a set of aspects that are mentioned in
text excerpt t, p(a) is the probability of seeing aspect
a, and SENT(at) ? [?1, 1] is the sentiment associ-
ated with aspect a in t. The SAM model sets p(a)
through the maximum likelihood estimates over T
and assumes p(SENT(at)|a) is normally distributed
with a mean and variance also estimated from T . We
2We focus on text-only opinion summaries as opposed to
those based on numeric ratings (Hu and Liu, 2006).
denote SAM(T ) as the model learned using the entire
set of candidate text excerpts T .
The SAM summarizer scores each potential sum-
mary, S, by learning another model SAM(S) based
on the text excerpts used to construct S. We can then
measure the distance between a model learned over
the full set T and a summary S by summing the KL-
divergence between their learned probability distri-
butions. In our case we have 1 + |AT | distributions
? p(a), and p(?|a) for all a ? AT . We then define L:
L(S) = ?KL(SAM(T ), SAM(S))
That is, the SAM summarizer prefers summaries
whose induced model is close to the model induced
for all the opinions about the product of interest.
Thus, a good summary should (1) mention aspects in
roughly the same proportion that they are mentioned
in the full set of opinions and (2) mention aspects
with sentiment also in proportion to what is observed
in the full opinion set. A high scoring summary is
found by initializing a summary with random sen-
tences and hill-climbing by replacing sentences one
at a time until convergence.
We chose to use the SAM model for our exper-
iment for two reasons. First, Lerman et al (2009)
showed that among a set of different opinion sum-
marizers, SAM was rated highest in a user study.
Secondly, as we will show in the next section, the
SAM summarization model can be naturally ex-
tended to produce contrastive summaries.
3 Constrastive Summarization
When jointly generating pairs of summaries, we at-
tempt to highlight differences between two products.
These differences can take multiple forms. Clearly,
two products can have different prevailing sentiment
scores with respect to an aspect (e.g. ?Product X has
great image quality? vs ?Product Y?s image quality
is terrible?). Reviews of different products can also
emphasize different aspects. Perhaps one product?s
screen is particularly good or bad, but another?s is
not particularly noteworthy ? or perhaps the other
product simply doesn?t have a screen. Regardless of
sentiment, reviews of the first product will empha-
size the screen quality aspect more than those of the
second, indicating that our summary should as well.
114
Tx Ty
Sx Sy
Tx Ty
Sx Sy
Tx Ty
Sx Sy
(a) (b) (c)
Figure 1: (a) Non-joint model: Generates summaries for
two products independently. (b) Joint model: Summaries
attempt to look like text they are drawn from, but contrast
each-other. (c) Joint model: Like (b), except summaries
contrast text that the other summary is drawn from.
As input to our contrastive summarizer we assume
two products, call them x and y as well as two corre-
sponding candidate sets of opinions, Tx and Ty, re-
spectively. As output, a contrastive summarizer will
produce two summaries ? Sx for product x and Sy
for product y ? so that the summaries highlight the
differences in opinion between the two products.
What might a contrastive summarizer look like on
a high-level? Figure 1 presents some options. The
first example (1a) shows a system where each sum-
mary is generated independently, i.e., running the
SAM model on each product separately without re-
gard to the other. This procedure may provide some
useful contrastive information, but any such infor-
mation will be present incidentally. To make the
summaries specifically contrast each other, we can
modify our system by explicitly modeling the fact
that we want summaries Sx and Sy to contrast. In
the SAM model this is trivial as we can simply add a
term to the scoring function L that attempts to maxi-
mize the KL-divergence between the two summaries
induced models SAM(Sx) and SAM(Sy).
This approach is graphically depicted in figure 1b,
where the system attempts to produce summaries
that are maximally similar to the opinion set they are
drawn from and minimally similar from each other.
However, some obvious degenerate solutions arise
if we chose to model our system this way. Consider
two products, x and y, for which all opinions dis-
cuss two aspects a and b with identical frequency
and sentiment polarity. Furthermore, several opin-
ions of x and y discuss an aspect c, but with oppo-
site sentiment polarity. Suppose we have to build
contrastive summaries and only have enough space
to cover a single aspect. The highest scoring con-
trastive pair of summaries would consist of one for x
that mentions a exclusively, and one for y that men-
tions b exclusively ? these summaries each mention
a promiment aspect of their product, and have no
overlap with each other. However, they provide a
false contrast because they each attempt to contrast
the other summary, rather than the other product.
Better would be for both to cover aspect c.
To remedy this, we reward summaries that in-
stead have a high KL-divergence with respect to the
other product?s full model SAM(T ) as depicted in
Figure 1c. Under this setup, the degenerate solution
described above is no longer appealing, as both sum-
maries have the same KL-divergence with respect to
the other product as they do to their own product.
The fact that the summaries themselves are dissim-
ilar is irrelevant. Comparing the summaries only to
the products? full language models prevents us from
rewarding summaries that convey a false contrast be-
tween the products under comparison. Specifically,
we now optimize the following joint summary score:
L(Sx, Sy) = ?KL(SAM(Tx), SAM(Sx))
?KL(SAM(Ty), SAM(Sy))
+KL(SAM(Tx), SAM(Sy))
+KL(SAM(Ty), SAM(Sx))
Note that we could additionally model divergence
between the two summaries (i.e., merging models in
figures 1b and c), but such modeling is redundant.
Furthermore, by not explicitly modeling divergence
between the two summaries we simplify the search
space as each summary can be constructed without
knowledge of the content of the second summary.
4 The Experiment
Our experiments focused on consumer electronics.
In this setting an entity to be summarized is one spe-
cific product and T is a set of segmented user re-
views about that product. We gathered reviews for
56 electronics products from several sources such as
CNet, Epinions, and PriceGrabber. The products
covered 15 categories of electronics products, in-
cluding MP3 players, digital cameras, laptops, GPS
systems, and more. Each had at least four reviews,
and the mean number of reviews per product was 70.
We manually grouped the products into cate-
gories (MP3 players, cameras, printers, GPS sys-
115
System As Received Consolidated
SAM 1.85 ? 0.05 1.82 ? 0.05
SAM + contrastive 1.76 ? 0.05 1.68 ? 0.05
Table 1: Mean rater scores for contrastive summaries by
system. Scores range from 0-3 and lower is better.
tems, headphones, computers, and others), and gen-
erated contrastive summaries for each pair of prod-
ucts in the same category using 2 different algo-
rithms: (1) The SAM algorithm for each product in-
dividually (figure 1a) and (2) The SAM algorithm
with our adaptation for contrastive summarization
(figure 1c). Summaries were generated using K =
650, which typically consisted of 4 text excerpts of
roughly 160 characters. This allowed us to compare
different summaries without worrying about the ef-
fects of summary length on the ratings. In all, we
gathered 178 contrastive summaries (89 per system)
to be evaluated by raters and each summary was
evaluated by 3 random raters resulting in 534 rat-
ings. The raters were 55 everyday internet users
that signed-up for the experiment and were assigned
roughly 10 random ratings each. Raters were shown
two products and their contrastive summaries, and
were asked to list 1-3 differences between the prod-
ucts as seen in the two summaries. They were also
asked to read the products? reviews to help ensure
that the differences observed were not simply arti-
facts of the summarizer but in fact are reflected in
actual opinions. Finally, raters were asked to rate
the helpfulness of the summaries in identifying these
distinctions, rating each with an integer score from
0 (?extremely useful?) to 3 (?not useful?).
Upon examining the results, we found that raters
had a hard time finding a meaningful distinction be-
tween the two middle ratings of 1 and 2 (?useful?
and ?somewhat useful?). We therefore present two
sets of results: one with the scores as received from
raters, and another with all 1 and 2 votes consol-
idated into a single class of votes with numerical
score 1.5. Table 1 gives the average scores per sys-
tem, lower scores indicating superior performance.
5 Analysis and Conclusions
The scores indicate that the addition of the con-
trastive term to the SAM model improves helpful-
ness, however both models roughly have average
System 2+ raters All 3 raters
SAM 0.8 0.2
SAM + contrastive 2.0 0.6
Table 2: Average number of points of contrast per com-
parison observed by multiple raters, by system. Raters
were asked to list up to 3. Higher is better.
scores in the somewhat-useful to useful range. The
difference becomes more pronounced when look-
ing at the consolidated scores. The natural question
arises: does the relatively small increase in helpful-
ness reflect that the contrastive summarizer is doing
a poor job? Or does it indicate that users only find
slightly more utility in contrastive information in
this domain? We inspected comments left by raters
in an attempt to answer this. Roughly 80% of raters
were able to find at least two points of contrast in
summaries generated by the SAM+contrastive ver-
sus 40% for summaries generated by the simple
SAM model. We then examined the consistency
of rater comments, i.e., to what degree did differ-
ent raters identify the same points of contrast from a
specific comparison? We report the results in table 2.
Note that by this metric in particular, the contrastive
summarizer outperforms its the single-product sum-
marizer by significant margins and provides a strong
argument that the contrastive model is doing its job.
Acknowledgements: The Google sentiment analy-
sis team for insightful discussions and suggestions.
References
K. Filippova, M. Surdeanu, M. Ciaramita, and
H. Zaragoza. 2009. Company-oriented extractive
summarization of financial news. In Proc. EACL.
M. Hu and B. Liu. 2006. Opinion extraction and sum-
marization on the web. In Proc. AAAI.
N. Jindal and B. Liu. 2006. Mining comparative sen-
tences and relations. In Proc. AAAI.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: Evaluating
and learning user preferences. In Proc. EACL.
B. Pang and L. Lee. 2008. Opinion mining and sentiment
analysis. Now Publishers.
I. Soboroff and D. Harman. 2005. Novelty detection:
The TREC experience. In Proc. HLT/EMNLP.
Sun, Wang, Shen, Zeng, and Chen. 2006. CWS: A Com-
parative Web search System. In Proc. WWW.
116
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432?439,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Structured Models for Fine-to-Coarse Sentiment Analysis
Ryan McDonald? Kerry Hannan Tyler Neylon Mike Wells Jeff Reynar
Google, Inc.
76 Ninth Avenue
New York, NY 10011
?Contact email: ryanmcd@google.com
Abstract
In this paper we investigate a structured
model for jointly classifying the sentiment
of text at varying levels of granularity. Infer-
ence in the model is based on standard se-
quence classification techniques using con-
strained Viterbi to ensure consistent solu-
tions. The primary advantage of such a
model is that it allows classification deci-
sions from one level in the text to influence
decisions at another. Experiments show that
this method can significantly reduce classifi-
cation error relative to models trained in iso-
lation.
1 Introduction
Extracting sentiment from text is a challenging prob-
lem with applications throughout Natural Language
Processing and Information Retrieval. Previous
work on sentiment analysis has covered a wide range
of tasks, including polarity classification (Pang et
al., 2002; Turney, 2002), opinion extraction (Pang
and Lee, 2004), and opinion source assignment
(Choi et al, 2005; Choi et al, 2006). Furthermore,
these systems have tackled the problem at differ-
ent levels of granularity, from the document level
(Pang et al, 2002), sentence level (Pang and Lee,
2004; Mao and Lebanon, 2006), phrase level (Tur-
ney, 2002; Choi et al, 2005), as well as the speaker
level in debates (Thomas et al, 2006). The abil-
ity to classify sentiment on multiple levels is impor-
tant since different applications have different needs.
For example, a summarization system for product
reviews might require polarity classification at the
sentence or phrase level; a question answering sys-
tem would most likely require the sentiment of para-
graphs; and a system that determines which articles
from an online news source are editorial in nature
would require a document level analysis.
This work focuses on models that jointly classify
sentiment on multiple levels of granularity. Consider
the following example,
This is the first Mp3 player that I have used ... I
thought it sounded great ... After only a few weeks,
it started having trouble with the earphone connec-
tion ... I won?t be buying another.
Mp3 player review from Amazon.com
This excerpt expresses an overall negative opinion of
the product being reviewed. However, not all parts
of the review are negative. The first sentence merely
provides some context on the reviewer?s experience
with such devices and the second sentence indicates
that, at least in one regard, the product performed
well. We call the problem of identifying the senti-
ment of the document and of all its subcomponents,
whether at the paragraph, sentence, phrase or word
level, fine-to-coarse sentiment analysis.
The simplest approach to fine-to-coarse sentiment
analysis would be to create a separate system for
each level of granularity. There are, however, obvi-
ous advantages to building a single model that clas-
sifies each level in tandem. Consider the sentence,
My 11 year old daughter has also been using it and
it is a lot harder than it looks.
In isolation, this sentence appears to convey negative
sentiment. However, it is part of a favorable review
432
for a piece of fitness equipment, where hard essen-
tially means good workout. In this domain, hard?s
sentiment can only be determined in context (i.e.,
hard to assemble versus a hard workout). If the clas-
sifier knew the overall sentiment of a document, then
disambiguating such cases would be easier.
Conversely, document level analysis can benefit
from finer level classification by taking advantage
of common discourse cues, such as the last sentence
being a reliable indicator for overall sentiment in re-
views. Furthermore, during training, the model will
not need to modify its parameters to explain phe-
nomena like the typically positive word great ap-
pearing in a negative text (as is the case above). The
model can also avoid overfitting to features derived
from neutral or objective sentences. In fact, it has al-
ready been established that sentence level classifica-
tion can improve document level analysis (Pang and
Lee, 2004). This line of reasoning suggests that a
cascaded approach would also be insufficient. Valu-
able information is passed in both directions, which
means any model of fine-to-coarse analysis should
account for this.
In Section 2 we describe a simple structured
model that jointly learns and infers sentiment on dif-
ferent levels of granularity. In particular, we reduce
the problem of joint sentence and document level
analysis to a sequential classification problem us-
ing constrained Viterbi inference. Extensions to the
model that move beyond just two-levels of analysis
are also presented. In Section 3 an empirical eval-
uation of the model is given that shows significant
gains in accuracy over both single level classifiers
and cascaded systems.
1.1 Related Work
The models in this work fall into the broad class of
global structured models, which are typically trained
with structured learning algorithms. Hidden Markov
models (Rabiner, 1989) are one of the earliest struc-
tured learning algorithms, which have recently been
followed by discriminative learning approaches such
as conditional random fields (CRFs) (Lafferty et al,
2001; Sutton and McCallum, 2006), the structured
perceptron (Collins, 2002) and its large-margin vari-
ants (Taskar et al, 2003; Tsochantaridis et al, 2004;
McDonald et al, 2005; Daume? III et al, 2006).
These algorithms are usually applied to sequential
labeling or chunking, but have also been applied to
parsing (Taskar et al, 2004; McDonald et al, 2005),
machine translation (Liang et al, 2006) and summa-
rization (Daume? III et al, 2006).
Structured models have previously been used for
sentiment analysis. Choi et al (2005, 2006) use
CRFs to learn a global sequence model to classify
and assign sources to opinions. Mao and Lebanon
(2006) used a sequential CRF regression model to
measure polarity on the sentence level in order to
determine the sentiment flow of authors in reviews.
Here we show that fine-to-coarse models of senti-
ment can often be reduced to the sequential case.
Cascaded models for fine-to-coarse sentiment
analysis were studied by Pang and Lee (2004). In
that work an initial model classified each sentence
as being subjective or objective using a global min-
cut inference algorithm that considered local label-
ing consistencies. The top subjective sentences are
then input into a standard document level polarity
classifier with improved results. The current work
differs from that in Pang and Lee through the use of
a single joint structured model for both sentence and
document level analysis.
Many problems in natural language processing
can be improved by learning and/or predicting mul-
tiple outputs jointly. This includes parsing and rela-
tion extraction (Miller et al, 2000), entity labeling
and relation extraction (Roth and Yih, 2004), and
part-of-speech tagging and chunking (Sutton et al,
2004). One interesting work on sentiment analysis
is that of Popescu and Etzioni (2005) which attempts
to classify the sentiment of phrases with respect to
possible product features. To do this an iterative al-
gorithm is used that attempts to globally maximize
the classification of all phrases while satisfying local
consistency constraints.
2 Structured Model
In this section we present a structured model for
fine-to-coarse sentiment analysis. We start by exam-
ining the simple case with two-levels of granularity
? the sentence and document ? and show that the
problem can be reduced to sequential classification
with constrained inference. We then discuss the fea-
ture space and give an algorithm for learning the pa-
rameters based on large-margin structured learning.
433
Extensions to the model are also examined.
2.1 A Sentence-Document Model
Let Y(d) be a discrete set of sentiment labels at
the document level and Y(s) be a discrete set of
sentiment labels at the sentence level. As input a
system is given a document containing sentences
s = s1, . . . , sn and must produce sentiment labels
for the document, yd ? Y(d), and each individ-
ual sentence, ys = ys1, . . . , y
s
n, where y
s
i ? Y(s) ?
1 ? i ? n. Define y = (yd,ys) = (yd, ys1, . . . , y
s
n)
as the joint labeling of the document and sentences.
For instance, in Pang and Lee (2004), yd would be
the polarity of the document and ysi would indicate
whether sentence si is subjective or objective. The
models presented here are compatible with arbitrary
sets of discrete output labels.
Figure 1 presents a model for jointly classifying
the sentiment of both the sentences and the docu-
ment. In this undirected graphical model, the label
of each sentence is dependent on the labels of its
neighbouring sentences plus the label of the docu-
ment. The label of the document is dependent on
the label of every sentence. Note that the edges
between the input (each sentence) and the output
labels are not solid, indicating that they are given
as input and are not being modeled. The fact that
the sentiment of sentences is dependent not only on
the local sentiment of other sentences, but also the
global document sentiment ? and vice versa ? al-
lows the model to directly capture the importance
of classification decisions across levels in fine-to-
coarse sentiment analysis. The local dependencies
between sentiment labels on sentences is similar to
the work of Pang and Lee (2004) where soft local
consistency constraints were created between every
sentence in a document and inference was solved us-
ing a min-cut algorithm. However, jointly modeling
the document label and allowing for non-binary la-
bels complicates min-cut style solutions as inference
becomes intractable.
Learning and inference in undirected graphical
models is a well studied problem in machine learn-
ing and NLP. For example, CRFs define the prob-
ability over the labels conditioned on the input us-
ing the property that the joint probability distribu-
tion over the labels factors over clique potentials in
undirected graphical models (Lafferty et al, 2001).
Figure 1: Sentence and document level model.
In this work we will use structured linear classi-
fiers (Collins, 2002). We denote the score of a la-
beling y for an input s as score(y, s) and define this
score as the sum of scores over each clique,
score(y, s) = score((yd,ys), s)
= score((yd, ys1, . . . , y
s
n), s)
=
n?
i=2
score(yd, ysi?1, y
s
i , s)
where each clique score is a linear combination of
features and their weights,
score(yd, ysi?1, y
s
i , s) = w ? f(y
d, ysi?1, y
s
i , s) (1)
and f is a high dimensional feature representation
of the clique and w a corresponding weight vector.
Note that s is included in each score since it is given
as input and can always be conditioned on.
In general, inference in undirected graphical mod-
els is intractable. However, for the common case of
sequences (a.k.a. linear-chain models) the Viterbi al-
gorithm can be used (Rabiner, 1989; Lafferty et al,
2001). Fortunately there is a simple technique that
reduces inference in the above model to sequence
classification with a constrained version of Viterbi.
2.1.1 Inference as Sequential Labeling
The inference problem is to find the highest scor-
ing labeling y for an input s, i.e.,
argmax
y
score(y, s)
If the document label yd is fixed, then inference
in the model from Figure 1 reduces to the sequen-
tial case. This is because the search space is only
over the sentence labels ysi , whose graphical struc-
ture forms a chain. Thus the problem of finding the
434
Input: s = s1, . . . , sn
1. y = null
2. for each yd ? Y(d)
3. ys = argmaxys score((y
d,ys), s)
4. y? = (yd,ys)
5. if score(y?, s) > score(y, s) or y = null
6. y = y?
7. return y
Figure 2: Inference algorithm for model in Figure 1.
The argmax in line 3 can be solved using Viterbi?s
algorithm since yd is fixed.
highest scoring sentiment labels for all sentences,
given a particular document label yd, can be solved
efficiently using Viterbi?s algorithm.
The general inference problem can then be solved
by iterating over each possible yd, finding ys max-
imizing score((yd,ys), s) and keeping the single
best y = (yd,ys). This algorithm is outlined in Fig-
ure 2 and has a runtime of O(|Y(d)||Y(s)|2n), due
to running Viterbi |Y(d)| times over a label space of
size |Y(s)|. The algorithm can be extended to pro-
duce exact k-best lists. This is achieved by using
k-best Viterbi techniques to return the k-best global
labelings for each document label in line 3. Merging
these sets will produce the final k-best list.
It is possible to view the inference algorithm in
Figure 2 as a constrained Viterbi search since it is
equivalent to flattening the model in Figure 1 to a
sequential model with sentence labels from the set
Y(s) ? Y(d). The resulting Viterbi search would
then need to be constrained to ensure consistent
solutions, i.e., the label assignments agree on the
document label over all sentences. If viewed this
way, it is also possible to run a constrained forward-
backward algorithm and learn the parameters for
CRFs as well.
2.1.2 Feature Space
In this section we define the feature representa-
tion for each clique, f(yd, ysi?1, y
s
i , s). Assume that
each sentence si is represented by a set of binary
predicates P(si). This set can contain any predicate
over the input s, but for the present purposes it will
include all the unigram, bigram and trigrams in
the sentence si conjoined with their part-of-speech
(obtained from an automatic classifier). Back-offs
of each predicate are also included where one or
more word is discarded. For instance, if P(si) con-
tains the predicate a:DT great:JJ product:NN,
then it would also have the predicates
a:DT great:JJ *:NN, a:DT *:JJ product:NN,
*:DT great:JJ product:NN, a:DT *:JJ *:NN, etc.
Each predicate, p, is then conjoined with the label
information to construct a binary feature. For exam-
ple, if the sentence label set is Y(s) = {subj, obj}
and the document set is Y(d) = {pos, neg}, then
the system might contain the following feature,
f(j)(y
d, ysi?1, y
s
i , s) =
?
?????
?????
1 if p ? P(si)
and ysi?1 = obj
and ysi = subj
and yd = neg
0 otherwise
Where f(j) is the jth dimension of the feature space.
For each feature, a set of back-off features are in-
cluded that only consider the document label yd, the
current sentence label ysi , the current sentence and
document label ysi and y
d, and the current and pre-
vious sentence labels ysi and y
s
i?1. Note that through
these back-off features the joint models feature set
will subsume the feature set of any individual level
model. Only features observed in the training data
were considered. Depending on the data set, the di-
mension of the feature vector f ranged from 350K to
500K. Though the feature vectors can be sparse, the
feature weights will be learned using large-margin
techniques that are well known to be robust to large
and sparse feature representations.
2.1.3 Training the Model
Let Y = Y(d) ? Y(s)n be the set of all valid
sentence-document labelings for an input s. The
weights, w, are set using the MIRA learning al-
gorithm, which is an inference based online large-
margin learning technique (Crammer and Singer,
2003; McDonald et al, 2005). An advantage of this
algorithm is that it relies only on inference to learn
the weight vector (see Section 2.1.1). MIRA has
been shown to provide state-of-the-art accuracy for
many language processing tasks including parsing,
chunking and entity extraction (McDonald, 2006).
The basic algorithm is outlined in Figure 3. The
algorithm works by considering a single training in-
stance during each iteration. The weight vector w is
updated in line 4 through a quadratic programming
problem. This update modifies the weight vector so
435
Training data: T = {(yt, st)}Tt=1
1. w(0) = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. w(i+1) = argminw*
?
?
?w*? w(i)
?
?
?
s.t. score(yt, st)? score(y?, s) ? L(yt,y?)
relative to w*
?y? ? C ? Y , where |C| = k
5. i = i + 1
6. return w(N?T )
Figure 3: MIRA learning algorithm.
that the score of the correct labeling is larger than
the score of every labeling in a constraint set C with
a margin proportional to the loss. The constraint set
C can be chosen arbitrarily, but it is usually taken to
be the k labelings that have the highest score under
the old weight vector w(i) (McDonald et al, 2005).
In this manner, the learning algorithm can update its
parameters relative to those labelings closest to the
decision boundary. Of all the weight vectors that sat-
isfy these constraints, MIRA chooses the one that is
as close as possible to the previous weight vector in
order to retain information about previous updates.
The loss function L(y,y?) is a positive real val-
ued function and is equal to zero when y = y?. This
function is task specific and is usually the hamming
loss for sequence classification problems (Taskar et
al., 2003). Experiments with different loss functions
for the joint sentence-document model on a develop-
ment data set indicated that the hamming loss over
sentence labels multiplied by the 0-1 loss over doc-
ument labels worked best.
An important modification that was made to the
learning algorithm deals with how the k constraints
are chosen for the optimization. Typically these con-
straints are the k highest scoring labelings under the
current weight vector. However, early experiments
showed that the model quickly learned to discard
any labeling with an incorrect document label for
the instances in the training set. As a result, the con-
straints were dominated by labelings that only dif-
fered over sentence labels. This did not allow the al-
gorithm adequate opportunity to set parameters rel-
ative to incorrect document labeling decisions. To
combat this, k was divided by the number of doc-
ument labels, to get a new value k?. For each doc-
ument label, the k? highest scoring labelings were
Figure 4: An extension to the model from Figure 1
incorporating paragraph level analysis.
extracted. Each of these sets were then combined to
produce the final constraint set. This allowed con-
straints to be equally distributed amongst different
document labels.
Based on performance on the development data
set the number of training iterations was set to N =
5 and the number of constraints to k = 10. Weight
averaging was also employed (Collins, 2002), which
helped improve performance.
2.2 Beyond Two-Level Models
To this point, we have focused solely on a model for
two-level fine-to-coarse sentiment analysis not only
for simplicity, but because the experiments in Sec-
tion 3 deal exclusively with this scenario. In this
section, we briefly discuss possible extensions for
more complex situations. For example, longer doc-
uments might benefit from an analysis on the para-
graph level as well as the sentence and document
levels. One possible model for this case is given
in Figure 4, which essentially inserts an additional
layer between the sentence and document level from
the original model. Sentence level analysis is de-
pendent on neighbouring sentences as well as the
paragraph level analysis, and the paragraph anal-
ysis is dependent on each of the sentences within
it, the neighbouring paragraphs, and the document
level analysis. This can be extended to an arbitrary
level of fine-to-coarse sentiment analysis by simply
inserting new layers in this fashion to create more
complex hierarchical models.
The advantage of using hierarchical models of
this form is that they are nested, which keeps in-
ference tractable. Observe that each pair of adja-
cent levels in the model is equivalent to the origi-
nal model from Figure 1. As a result, the scores
of the every label at each node in the graph can
be calculated with a straight-forward bottom-up dy-
namic programming algorithm. Details are omitted
436
Sentence Stats Document Stats
Pos Neg Neu Tot Pos Neg Tot
Car 472 443 264 1179 98 80 178
Fit 568 635 371 1574 92 97 189
Mp3 485 464 214 1163 98 89 187
Tot 1525 1542 849 3916 288 266 554
Table 1: Data statistics for corpus. Pos = positive
polarity, Neg = negative polarity, Neu = no polarity.
for space reasons.
Other models are possible where dependencies
occur across non-neighbouring levels, e.g., by in-
serting edges between the sentence level nodes and
the document level node. In the general case, infer-
ence is exponential in the size of each clique. Both
the models in Figure 1 and Figure 4 have maximum
clique sizes of three.
3 Experiments
3.1 Data
To test the model we compiled a corpus of 600 on-
line product reviews from three domains: car seats
for children, fitness equipment, and Mp3 players. Of
the original 600 reviews that were gathered, we dis-
carded duplicate reviews, reviews with insufficient
text, and spam. All reviews were labeled by on-
line customers as having a positive or negative polar-
ity on the document level, i.e., Y(d) = {pos, neg}.
Each review was then split into sentences and ev-
ery sentence annotated by a single annotator as ei-
ther being positive, negative or neutral, i.e., Y(s) =
{pos, neg, neu}. Data statistics for the corpus are
given in Table 1.
All sentences were annotated based on their con-
text within the document. Sentences were anno-
tated as neutral if they conveyed no sentiment or had
indeterminate sentiment from their context. Many
neutral sentences pertain to the circumstances un-
der which the product was purchased. A common
class of sentences were those containing product
features. These sentences were annotated as having
positive or negative polarity if the context supported
it. This could include punctuation such as excla-
mation points, smiley/frowny faces, question marks,
etc. The supporting evidence could also come from
another sentence, e.g., ?I love it. It has 64Mb of
memory and comes with a set of earphones?.
3.2 Results
Three baseline systems were created,
? Document-Classifier is a classifier that learns
to predict the document label only.
? Sentence-Classifier is a classifier that learns
to predict sentence labels in isolation of one
another, i.e., without consideration for either
the document or neighbouring sentences sen-
timent.
? Sentence-Structured is another sentence clas-
sifier, but this classifier uses a sequential chain
model to learn and classify sentences. The
third baseline is essentially the model from Fig-
ure 1 without the top level document node. This
baseline will help to gage the empirical gains of
the different components of the joint structured
model on sentence level classification.
The model described in Section 2 will be called
Joint-Structured. All models use the same ba-
sic predicate space: unigram, bigram, trigram con-
joined with part-of-speech, plus back-offs of these
(see Section 2.1.2 for more). However, due to the
structure of the model and its label space, the feature
space of each might be different, e.g., the document
classifier will only conjoin predicates with the doc-
ument label to create the feature set. All models are
trained using the MIRA learning algorithm.
Results for each model are given in the first four
rows of Table 2. These results were gathered using
10-fold cross validation with one fold for develop-
ment and the other nine folds for evaluation. This
table shows that classifying sentences in isolation
from one another is inferior to accounting for a more
global context. A significant increase in perfor-
mance can be obtained when labeling decisions be-
tween sentences are modeled (Sentence-Structured).
More interestingly, even further gains can be had
when document level decisions are modeled (Joint-
Structured). In many cases, these improvements are
highly statistically significant.
On the document level, performance can also be
improved by incorporating sentence level decisions
? though these improvements are not consistent.
This inconsistency may be a result of the model
overfitting on the small set of training data. We
437
suspect this because the document level error rate
on the Mp3 training set converges to zero much
more rapidly for the Joint-Structured model than the
Document-Classifier. This suggests that the Joint-
Structured model might be relying too much on
the sentence level sentiment features ? in order to
minimize its error rate ? instead of distributing the
weights across all features more evenly.
One interesting application of sentence level sen-
timent analysis is summarizing product reviews on
retail websites like Amazon.com or review aggrega-
tors like Yelp.com. In this setting the correct polar-
ity of a document is often known, but we wish to
label sentiment on the sentence or phrase level to
aid in generating a cohesive and informative sum-
mary. The joint model can be used to classify sen-
tences in this setting by constraining inference to the
known fixed document label for a review. If this is
done, then sentiment accuracy on the sentence level
increases substantially from 62.6% to 70.3%.
Finally we should note that experiments using
CRFs to train the structured models and logistic re-
gression to train the local models yielded similar re-
sults to those in Table 2.
3.2.1 Cascaded Models
Another approach to fine-to-coarse sentiment
analysis is to use a cascaded system. In such a sys-
tem, a sentence level classifier might first be run
on the data, and then the results input into a docu-
ment level classifier ? or vice-versa.1 Two cascaded
systems were built. The first uses the Sentence-
Structured classifier to classify all the sentences
from a review, then passes this information to the
document classifier as input. In particular, for ev-
ery predicate in the original document classifier, an
additional predicate that specifies the polarity of the
sentence in which this predicate occurred was cre-
ated. The second cascaded system uses the docu-
ment classifier to determine the global polarity, then
passes this information as input into the Sentence-
Structured model, constructing predicates in a simi-
lar manner.
The results for these two systems can be seen in
the last two rows of Table 2. In both cases there
1Alternatively, decisions from the sentence classifier can
guide which input is seen by the document level classifier (Pang
and Lee, 2004).
is a slight improvement in performance suggesting
that an iterative approach might be beneficial. That
is, a system could start by classifying documents,
use the document information to classify sentences,
use the sentence information to classify documents,
and repeat until convergence. However, experiments
showed that this did not improve accuracy over a sin-
gle iteration and often hurt performance.
Improvements from the cascaded models are far
less consistent than those given from the joint struc-
ture model. This is because decisions in the cas-
caded system are passed to the next layer as the
?gold? standard at test time, which results in errors
from the first classifier propagating to errors in the
second. This could be improved by passing a lattice
of possibilities from the first classifier to the second
with corresponding confidences. However, solutions
such as these are really just approximations of the
joint structured model that was presented here.
4 Future Work
One important extension to this work is to augment
the models for partially labeled data. It is realistic
to imagine a training set where many examples do
not have every level of sentiment annotated. For
example, there are thousands of online product re-
views with labeled document sentiment, but a much
smaller amount where sentences are also labeled.
Work on learning with hidden variables can be used
for both CRFs (Quattoni et al, 2004) and for in-
ference based learning algorithms like those used in
this work (Liang et al, 2006).
Another area of future work is to empirically in-
vestigate the use of these models on longer docu-
ments that require more levels of sentiment anal-
ysis than product reviews. In particular, the rela-
tive position of a phrase to a contrastive discourse
connective or a cue phrase like ?in conclusion? or
?to summarize? may lead to improved performance
since higher level classifications can learn to weigh
information passed from these lower level compo-
nents more heavily.
5 Discussion
In this paper we have investigated the use of a global
structured model that learns to predict sentiment on
different levels of granularity for a text. We de-
438
Sentence Accuracy Document Accuracy
Car Fit Mp3 Total Car Fit Mp3 Total
Document-Classifier - - - - 72.8 80.1 87.2 80.3
Sentence-Classifier 54.8 56.8 49.4 53.1 - - - -
Sentence-Structured 60.5 61.4 55.7 58.8 - - - -
Joint-Structured 63.5? 65.2?? 60.1?? 62.6?? 81.5? 81.9 85.0 82.8
Cascaded Sentence ? Document 60.5 61.4 55.7 58.8 75.9 80.7 86.1 81.1
Cascaded Document ? Sentence 59.7 61.0 58.3 59.5 72.8 80.1 87.2 80.3
Table 2: Fine-to-coarse sentiment accuracy. Significance calculated using McNemar?s test between top two
performing systems. ?Statistically significant p < 0.05. ??Statistically significant p < 0.005.
scribed a simple model for sentence-document anal-
ysis and showed that inference in it is tractable. Ex-
periments show that this model obtains higher ac-
curacy than classifiers trained in isolation as well
as cascaded systems that pass information from one
level to another at test time. Furthermore, extensions
to the sentence-document model were discussed and
it was argued that a nested hierarchical structure
would be beneficial since it would allow for efficient
inference algorithms.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In Proc. HLT/EMNLP.
Y. Choi, E. Breck, and C. Cardie. 2006. Joint extraction of enti-
ties and relations for opinion recognition. In Proc. EMNLP.
M. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative online
algorithms for multiclass problems. JMLR.
Hal Daume? III, John Langford, and Daniel Marcu. 2006.
Search-based structured prediction. In Submission.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar. 2006. An
end-to-end discriminative approach to machine translation.
In Proc. ACL.
Y. Mao and G. Lebanon. 2006. Isotonic conditional random
fields and local sentiment flow. In Proc. NIPS.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-
margin training of dependency parsers. In Proc. ACL.
R. McDonald. 2006. Discriminative Training and Spanning
Tree Algorithms for Dependency Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel. 2000.
A novel use of statistical parsing to extract information from
text. In Proc NAACL, pages 226?233.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proc. ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine learning techniques.
In EMNLP.
A. Popescu and O. Etzioni. 2005. Extracting product features
and opinions from reviews. In Proc. HLT/EMNLP.
A. Quattoni, M. Collins, and T. Darrell. 2004. Conditional
random fields for object recognition. In Proc. NIPS.
L. R. Rabiner. 1989. A tutorial on hidden Markov models and
selected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?285, February.
D. Roth and W. Yih. 2004. A linear programming formula-
tion for global inference in natural language tasks. In Proc.
CoNLL.
C. Sutton and A. McCallum. 2006. An introduction to con-
ditional random fields for relational learning. In L. Getoor
and B. Taskar, editors, Introduction to Statistical Relational
Learning. MIT Press.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dy-
namic conditional random fields: Factorized probabilistic
models for labeling and segmenting sequence data. In Proc.
ICML.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In Proc. EMNLP.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote:
Determining support or opposition from congressional floor-
debate transcripts. In Proc. EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004.
Support vector learning for interdependent and structured
output spaces. In Proc. ICML.
P. Turney. 2002. Thumbs up or thumbs down? Sentiment ori-
entation applied to unsupervised classification of reviews. In
EMNLP.
439
Proceedings of ACL-08: HLT, pages 308?316,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Joint Model of Text and Aspect Ratings for Sentiment Summarization
Ivan Titov
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
titov@uiuc.edu
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Abstract
Online reviews are often accompanied with
numerical ratings provided by users for a set
of service or product aspects. We propose
a statistical model which is able to discover
corresponding topics in text and extract tex-
tual evidence from reviews supporting each of
these aspect ratings ? a fundamental problem
in aspect-based sentiment summarization (Hu
and Liu, 2004a). Our model achieves high ac-
curacy, without any explicitly labeled data ex-
cept the user provided opinion ratings. The
proposed approach is general and can be used
for segmentation in other applications where
sequential data is accompanied with corre-
lated signals.
1 Introduction
User generated content represents a unique source of
information in which user interface tools have facil-
itated the creation of an abundance of labeled con-
tent, e.g., topics in blogs, numerical product and ser-
vice ratings in user reviews, and helpfulness rank-
ings in online discussion forums. Many previous
studies on user generated content have attempted to
predict these labels automatically from the associ-
ated text. However, these labels are often present
in the data already, which opens another interesting
line of research: designing models leveraging these
labelings to improve a wide variety of applications.
In this study, we look at the problem of aspect-
based sentiment summarization (Hu and Liu, 2004a;
Popescu and Etzioni, 2005; Gamon et al, 2005;
Nikos? Fine Dining
Food 4/5 ?Best fish in the city?, ?Excellent appetizers?
Decor 3/5 ?Cozy with an old world feel?, ?Too dark?
Service 1/5 ?Our waitress was rude?, ?Awful service?
Value 5/5 ?Good Greek food for the $?, ?Great price!?
Figure 1: An example aspect-based summary.
Carenini et al, 2006; Zhuang et al, 2006).1 An
aspect-based summarization system takes as input
a set of user reviews for a specific product or ser-
vice and produces a set of relevant aspects, the ag-
gregated sentiment for each aspect, and supporting
textual evidence. For example, figure 1 summarizes
a restaurant using aspects food, decor, service, and
value plus a numeric rating out of 5.
Standard aspect-based summarization consists of
two problems. The first is aspect identification and
mention extraction. Here the goal is to find the set
of relevant aspects for a rated entity and extract all
textual mentions that are associated with each. As-
pects can be fine-grained, e.g., fish, lamb, calamari,
or coarse-grained, e.g., food, decor, service. Sim-
ilarly, extracted text can range from a single word
to phrases and sentences. The second problem is
sentiment classification. Once all the relevant as-
pects and associated pieces of texts are extracted,
the system should aggregate sentiment over each as-
pect to provide the user with an average numeric or
symbolic rating. Sentiment classification is a well
studied problem (Wiebe, 2000; Pang et al, 2002;
Turney, 2002) and in many domains users explicitly
1We use the term aspect to denote properties of an object
that can be rated by a user as in Snyder and Barzilay (2007).
Other studies use the term feature (Hu and Liu, 2004b).
308
Food: 5; Decor: 5; Service: 5; Value: 5
The chicken was great. On top of that our service was
excellent and the price was right. Can?t wait to go back!
Food: 2; Decor: 1; Service: 3; Value: 2
We went there for our anniversary. My soup was cold and
expensive plus it felt like they hadn?t painted since 1980.
Food: 3; Decor: 5; Service: 4; Value: 5
The food is only mediocre, but well worth the cost.
Wait staff was friendly. Lot?s of fun decorations.
?
Food ?The chicken was great?, ?My soup wascold?, ?The food is only mediocre?
Decor ?it felt like they hadn?t painted since1980?, ?Lots of fun decorations?
Service ?service was excellent?,?Wait staff was friendly?
Value ?the price was right?, ?My soup was coldand expensive?, ?well worth the cost?
Figure 2: Extraction problem: Produce aspect mentions from a corpus of aspect rated reviews.
provide ratings for each aspect making automated
means unnecessary.2 Aspect identification has also
been thoroughly studied (Hu and Liu, 2004b; Ga-
mon et al, 2005; Titov and McDonald, 2008), but
again, ontologies and users often provide this infor-
mation negating the need for automation.
Though it may be reasonable to expect a user to
provide a rating for each aspect, it is unlikely that
a user will annotate every sentence and phrase in a
review as being relevant to some aspect. Thus, it
can be argued that the most pressing challenge in
an aspect-based summarization system is to extract
all relevant mentions for each aspect, as illustrated
in figure 2. When labeled data exists, this prob-
lem can be solved effectively using a wide variety
of methods available for text classification and in-
formation extraction (Manning and Schutze, 1999).
However, labeled data is often hard to come by, es-
pecially when one considers all possible domains of
products and services. Instead, we propose an un-
supervised model that leverages aspect ratings that
frequently accompany an online review.
In order to construct such model, we make two
assumptions. First, ratable aspects normally repre-
sent coherent topics which can be potentially dis-
covered from co-occurrence information in the text.
Second, we hypothesize that the most predictive fea-
tures of an aspect rating are features derived from
the text segments discussing the corresponding as-
pect. Motivated by these observations, we construct
a joint statistical model of text and sentiment ratings.
The model is at heart a topic model in that it as-
signs words to a set of induced topics, each of which
may represent one particular aspect. The model is
extended through a set of maximum entropy classi-
fiers, one per each rated aspect, that are used to pre-
2E.g., http://zagat.com and http://tripadvisor.com.
dict the sentiment rating towards each of the aspects.
However, only the words assigned to an aspects cor-
responding topic are used in predicting the rating
for that aspect. As a result, the model enforces that
words assigned to an aspects? topic are predictive of
the associated rating. Our approach is more general
than the particular statistical model we consider in
this paper. For example, other topic models can be
used as a part of our model and the proposed class of
models can be employed in other tasks beyond senti-
ment summarization, e.g., segmentation of blogs on
the basis of topic labels provided by users, or topic
discovery on the basis of tags given by users on so-
cial bookmarking sites.3
The rest of the paper is structured as follows. Sec-
tion 2 begins with a discussion of the joint text-
sentiment model approach. In Section 3 we provide
both a qualitative and quantitative evaluation of the
proposed method. We conclude in Section 4 with an
examination of related work.
2 The Model
In this section we describe a new statistical model
called the Multi-Aspect Sentiment model (MAS),
which consists of two parts. The first part is based on
Multi-Grain Latent Dirichlet Allocation (Titov and
McDonald, 2008), which has been previously shown
to build topics that are representative of ratable as-
pects. The second part is a set of sentiment pre-
dictors per aspect that are designed to force specific
topics in the model to be directly correlated with a
particular aspect.
2.1 Multi-Grain LDA
The Multi-Grain Latent Dirichlet Allocation model
(MG-LDA) is an extension of Latent Dirichlet Allo-
cation (LDA) (Blei et al, 2003). As was demon-
3See e.g. del.ico.us (http://del.ico.us).
309
strated in Titov and McDonald (2008), the topics
produced by LDA do not correspond to ratable as-
pects of entities. In particular, these models tend to
build topics that globally classify terms into product
instances (e.g., Creative Labs Mp3 players versus
iPods, or New York versus Paris Hotels). To com-
bat this, MG-LDA models two distinct types of top-
ics: global topics and local topics. As in LDA, the
distribution of global topics is fixed for a document
(a user review). However, the distribution of local
topics is allowed to vary across the document.
A word in the document is sampled either from
the mixture of global topics or from the mixture of
local topics specific to the local context of the word.
It was demonstrated in Titov and McDonald (2008)
that ratable aspects will be captured by local topics
and global topics will capture properties of reviewed
items. For example, consider an extract from a re-
view of a London hotel: ?. . . public transport in Lon-
don is straightforward, the tube station is about an 8
minute walk . . . or you can get a bus for ?1.50?. It
can be viewed as a mixture of topic London shared
by the entire review (words: ?London?, ?tube?, ???),
and the ratable aspect location, specific for the local
context of the sentence (words: ?transport?, ?walk?,
?bus?). Local topics are reused between very differ-
ent types of items, whereas global topics correspond
only to particular types of items.
In MG-LDA a document is represented as a set
of sliding windows, each covering T adjacent sen-
tences within a document.4 Each window v in docu-
ment d has an associated distribution over local top-
ics ?locd,v and a distribution defining preference for lo-
cal topics versus global topics pid,v. A word can be
sampled using any window covering its sentence s,
where the window is chosen according to a categor-
ical distribution ?d,s. Importantly, the fact that win-
dows overlap permits the model to exploit a larger
co-occurrence domain. These simple techniques are
capable of modeling local topics without more ex-
pensive modeling of topic transitions used in (Grif-
fiths et al, 2004; Wang and McCallum, 2005; Wal-
lach, 2006; Gruber et al, 2007). Introduction of a
symmetrical Dirichlet prior Dir(?) for the distribu-
tion ?d,s can control the smoothness of transitions.
4Our particular implementation is over sentences, but sliding
windows in theory can be over any sized fragment of text.
(a) (b)
Figure 3: (a) MG-LDA model. (b) An extension of MG-
LDA to obtain MAS.
The formal definition of the model with Kgl
global and K loc local topics is as follows: First,
draw Kgl word distributions for global topics ?glz
from a Dirichlet prior Dir(?gl) and K loc word dis-
tributions for local topics ?locz? - from Dir(?loc).
Then, for each document d:
? Choose a distribution of global topics ?gld ? Dir(?gl).
? For each sentence s choose a distribution over sliding
windows ?d,s(v) ? Dir(?).
? For each sliding window v
? choose ?locd,v ? Dir(?loc),
? choose pid,v ? Beta(?mix).
? For each word i in sentence s of document d
? choose window vd,i ? ?d,s,
? choose rd,i ? pid,vd,i ,
? if rd,i = gl choose global topic zd,i ? ?gld ,
? if rd,i= loc choose local topic zd,i??locd,vd,i ,
? choose word wd,i from the word distribution ?
rd,i
zd,i .
Beta(?mix) is a prior Beta distribution for choos-
ing between local and global topics. In Figure 3a the
corresponding graphical model is presented.
2.2 Multi-Aspect Sentiment Model
MG-LDA constructs a set of topics that ideally cor-
respond to ratable aspects of an entity (often in a
many-to-one relationship of topics to aspects). A
major shortcoming of this model ? and all other un-
supervised models ? is that this correspondence is
not explicit, i.e., how does one say that topic X is re-
ally about aspect Y? However, we can observe that
numeric aspect ratings are often included in our data
by users who left the reviews. We then make the
assumption that the text of the review discussing an
aspect is predictive of its rating. Thus, if we model
the prediction of aspect ratings jointly with the con-
struction of explicitly associated topics, then such a
310
model should benefit from both higher quality topics
and a direct assignment from topics to aspects. This
is the basic idea behind the Multi-Aspect Sentiment
model (MAS).
In its simplest form, MAS introduces a classifier
for each aspect, which is used to predict its rating.
Each classifier is explicitly associated to a single
topic in the model and only words assigned to that
topic can participate in the prediction of the senti-
ment rating for the aspect. However, it has been ob-
served that ratings for different aspects can be cor-
related (Snyder and Barzilay, 2007), e.g., very neg-
ative opinion about room cleanliness is likely to re-
sult not only in a low rating for the aspect rooms,
but also is very predictive of low ratings for the as-
pects service and dining. This complicates discovery
of the corresponding topics, as in many reviews the
most predictive features for an aspect rating might
correspond to another aspect. Another problem with
this overly simplistic model is the presence of opin-
ions about an item in general without referring to
any particular aspect. For example, ?this product is
the worst I have ever purchased? is a good predic-
tor of low ratings for every aspect. In such cases,
non-aspect ?background? words will appear to be the
most predictive. Therefore, the use of the aspect sen-
timent classifiers based only on the words assigned
to the corresponding topics is problematic. Such a
model will not be able to discover coherent topics
associated with each aspect, because in many cases
the most predictive fragments for each aspect rating
will not be the ones where this aspect is discussed.
Our proposal is to estimate the distribution of pos-
sible values of an aspect rating on the basis of the
overall sentiment rating and to use the words as-
signed to the corresponding topic to compute cor-
rections for this aspect. An aspect rating is typically
correlated to the overall sentiment rating5 and the
fragments discussing this particular aspect will help
to correct the overall sentiment in the appropriate di-
rection. For example, if a review of a hotel is gen-
erally positive, but it includes a sentence ?the neigh-
borhood is somewhat seedy? then this sentence is
predictive of rating for an aspect location being be-
low other ratings. This rectifies the aforementioned
5In the dataset used in our experiments all three aspect rat-
ings are equivalent for 5,250 reviews out of 10,000.
problems. First, aspect sentiment ratings can often
be regarded as conditionally independent given the
overall rating, therefore the model will not be forced
to include in an aspect topic any words from other
aspect topics. Secondly, the fragments discussing
overall opinion will influence the aspect rating only
through the overall sentiment rating. The overall
sentiment is almost always present in the real data
along with the aspect ratings, but it can be coarsely
discretized and we preferred to use a latent overall
sentiment.
The MAS model is presented in Figure 3b. Note
that for simplicity we decided to omit in the figure
the components of the MG-LDA model other than
variables r, z and w, though they are present in the
statistical model. MAS also allows for extra unasso-
ciated local topics in order to capture aspects not ex-
plicitly rated by the user. As in MG-LDA, MAS has
global topics which are expected to capture topics
corresponding to particular types of items, such Lon-
don hotels or seaside resorts for the hotel domain. In
figure 3b we shaded the aspect ratings ya, assuming
that every aspect rating is present in the data (though
in practice they might be available only for some re-
views). In this model the distribution of the overall
sentiment rating yov is based on all the n-gram fea-
tures of a review text. Then the distribution of ya, for
every rated aspect a, can be computed from the dis-
tribution of yov and from any n-gram feature where
at least one word in the n-gram is assigned to the
associated aspect topic (r = loc, z = a).
Instead of having a latent variable yov,6 we use a
similar model which does not have an explicit no-
tion of yov. The distribution of a sentiment rating ya
for each rated aspect a is computed from two scores.
The first score is computed on the basis of all the n-
grams, but using a common set of weights indepen-
dent of the aspect a. Another score is computed only
using n-grams associated with the related topic, but
an aspect-specific set of weights is used in this com-
putation. More formally, we consider the log-linear
distribution:
P (ya = y|w, r, z)?exp(bay+
?
f?w
Jf,y+paf,r,zJaf,y), (1)
where w, r, z are vectors of all the words in a docu-
6Preliminary experiments suggested that this is also a feasi-
ble approach, but somewhat more computationally expensive.
311
ment, assignments of context (global or local) and
topics for all the words in the document, respec-
tively. bay is the bias term which regulates the prior
distribution P (ya = y), f iterates through all the
n-grams, Jy,f and Jay,f are common weights and
aspect-specific weights for n-gram feature f . paf,r,z
is equal to a fraction of words in n-gram feature f
assigned to the aspect topic (r = loc, z = a).
2.3 Inference in MAS
Exact inference in the MAS model is intractable.
Following Titov and McDonald (2008) we use a col-
lapsed Gibbs sampling algorithm that was derived
for the MG-LDA model based on the Gibbs sam-
pling method proposed for LDA in (Griffiths and
Steyvers, 2004). Gibbs sampling is an example of a
Markov Chain Monte Carlo algorithm (Geman and
Geman, 1984). It is used to produce a sample from
a joint distribution when only conditional distribu-
tions of each variable can be efficiently computed.
In Gibbs sampling, variables are sequentially sam-
pled from their distributions conditioned on all other
variables in the model. Such a chain of model states
converges to a sample from the joint distribution. A
naive application of this technique to LDA would
imply that both assignments of topics to words z
and distributions ? and ? should be sampled. How-
ever, (Griffiths and Steyvers, 2004) demonstrated
that an efficient collapsed Gibbs sampler can be con-
structed, where only assignments z need to be sam-
pled, whereas the dependency on distributions ? and
? can be integrated out analytically.
In the case of MAS we also use maximum a-
posteriori estimates of the sentiment predictor pa-
rameters bay, Jy,f and Jay,f . The MAP estimates for
parameters bay , Jy,f and Jay,f are obtained by us-
ing stochastic gradient ascent. The direction of the
gradient is computed simultaneously with running a
chain by generating several assignments at each step
and averaging over the corresponding gradient esti-
mates. For details on computing gradients for log-
linear graphical models with Gibbs sampling we re-
fer the reader to (Neal, 1992).
Space constraints do not allow us to present either
the derivation or a detailed description of the sam-
pling algorithm. However, note that the conditional
distribution used in sampling decomposes into two
parts:
P (vd,i = v, rd,i = r, zd,i = z|v?, r?, z?,w, y) ?
?d,iv,r,z ? ?d,ir,z, (2)
where v?, r? and z? are vectors of assignments of
sliding windows, context (global or local) and top-
ics for all the words in the collection except for the
considered word at position i in document d; y is the
vector of sentiment ratings. The first factor ?d,iv,r,z is
responsible for modeling co-occurrences on the win-
dow and document level and coherence of the topics.
This factor is proportional to the conditional distri-
bution used in the Gibbs sampler of the MG-LDA
model (Titov and McDonald, 2008). The last fac-
tor quantifies the influence of the assignment of the
word (d, i) on the probability of the sentiment rat-
ings. It appears only if ratings are known (observ-
able) and equals:
?d,ir,z =
?
a
P (yda|w, r?, rd,i = r, z?, zd,i = z)
P (yda|w, r?, z?, rd,i = gl)
,
where the probability distribution is computed as de-
fined in expression (1), yda is the rating for the ath
aspect of review d.
3 Experiments
In this section we present qualitative and quantita-
tive experiments. For the qualitative analysis we
show that topics inferred by the MAS model cor-
respond directly to the associated aspects. For the
quantitative analysis we show that the MAS model
induces a distribution over the rated aspects which
can be used to accurately predict whether a text frag-
ment is relevant to an aspect or not.
3.1 Qualitative Evaluation
To perform qualitative experiments we used a set
of reviews of hotels taken from TripAdvisor.com7
that contained 10,000 reviews (109,024 sentences,
2,145,313 words in total). Every review was
rated with at least three aspects: service, location
and rooms. Each rating is an integer from 1 to 5.
The dataset was tokenized and sentence split auto-
matically.
7(c) 2005-06, TripAdvisor, LLC All rights reserved
312
rated aspect top words
service staff friendly helpful service desk concierge excellent extremely hotel great reception english pleasant help
location hotel walk location station metro walking away right minutes close bus city located just easy restaurants
local rooms room bathroom shower bed tv small water clean comfortable towels bath nice large pillows space beds tub
topics - breakfast free coffee internet morning access buffet day wine nice lobby complimentary included good fruit
- $ night parking rate price paid day euros got cost pay hotel worth euro expensive car extra deal booked
- room noise night street air did door floor rooms open noisy window windows hear outside problem quiet sleep
global - moscow st russian petersburg nevsky russia palace hermitage kremlin prospect river prospekt kempinski
topics - paris tower french eiffel dame notre rue st louvre rer champs opera elysee george parisian du pantheon cafes
Table 1: Top words from MAS for hotel reviews.
Krooms top words
2 rooms clean hotel room small nice comfortable modern good quite large lobby old decor spacious decorated bathroom size
room noise night street did air rooms door open noisy window floor hear windows problem outside quiet sleep bit light
3 room clean bed comfortable rooms bathroom small beds nice large size tv spacious good double big space huge king
room floor view rooms suite got views given quiet building small balcony upgraded nice high booked asked overlooking
room bathroom shower air water did like hot small towels door old window toilet conditioning open bath dirty wall tub
4 room clean rooms comfortable bed small beds nice bathroom size large modern spacious good double big quiet decorated
check arrived time day airport early room luggage took late morning got long flight ready minutes did taxi bags went
room noise night street did air rooms noisy open door hear windows window outside quiet sleep problem floor conditioning
bathroom room shower tv bed small water towels bath tub large nice toilet clean space toiletries flat wall sink screen
Table 2: Top words for aspect rooms with different number of topicsKrooms.
We ran the sampling chain for 700 iterations to
produce a sample. Distributions of words in each
topic were estimated as the proportion of words as-
signed to each topic, taking into account topic model
priors ?gl and ?loc. The sliding windows were cho-
sen to cover 3 sentences for all the experiments. All
the priors were chosen to be equal to 0.1. We used
15 local topics and 30 global topics. In the model,
the first three local topics were associated to the
rating classifiers for each aspects. As a result, we
would expect these topics to correspond to the ser-
vice, location, and rooms aspects respectively. Un-
igram and bigram features were used in the senti-
ment predictors in the MAS model. Before apply-
ing the topic models we removed punctuation and
also removed stop words using the standard list of
stop words,8 however, all the words and punctuation
were used in the sentiment predictors.
It does not take many chain iterations to discover
initial topics. This happens considerably faster than
the appropriate weights of the sentiment predictor
being learned. This poses a problem, because, in the
beginning, the sentiment predictors are not accurate
enough to force the model to discover appropriate
topics associated with each of the rated aspects. And
as soon as topic are formed, aspect sentiment predic-
tors cannot affect them anymore because they do not
8http://www.dcs.gla.ac.uk/idom/ir resources/linguistic utils/
stop words
have access to the true words associated with their
aspects. To combat this problem we first train the
sentiment classifiers by assuming that paf,r,z is equal
for all the local topics, which effectively ignores the
topic model. Then we use the estimated parame-
ters within the topic model.9 Secondly, we mod-
ify the sampling algorithm. The conditional prob-
ability used in sampling, expression (2), is propor-
tional to the product of two factors. The first factor,
?d,iv,r,z , expresses a preference for topics likely from
the co-occurrence information, whereas the second
one, ?d,ir,z , favors the choice of topics which are pre-
dictive of the observable sentiment ratings. We used
(?d,ir,z)1+0.95
tq in the sampling distribution instead of
?d,ir,z , where t is the iteration number. q was chosen
to be 4, though the quality of the topics seemed to
be indistinguishable with any q between 3 and 10.
This can be thought of as having 1 + 0.95tq ratings
instead of a single vector assigned to each review,
i.e., focusing the model on prediction of the ratings
rather than finding the topic labels which are good at
explaining co-occurrences of words. These heuris-
tics influence sampling only during the first itera-
tions of the chain.
Top words for some of discovered local topics, in-
9Initial experiments suggested that instead of doing this
?pre-training? we could start with very large priors ?loc and
?mix, and then reduce them through the course of training.
However, this is significantly more computationally expensive.
313
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Recall
Pr
ec
isi
on
topic model
max?ent classifier
topic model
max?ent classifier
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Recall
Pr
ec
isi
on
max?ent classifier
1 topic
2 topics
3 topics
4 topics
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Recall
Pr
ec
isi
on
(a) (b) (c)
Figure 4: (a) Aspect service. (b) Aspect location. (c) Aspect rooms.
cluding the first 3 topics associated with the rated as-
pects, and also top words for some of global topics
are presented in Table 1. We can see that the model
discovered as its first three topics the correct associ-
ated aspects: service, location, and rooms. Other lo-
cal topics, as for the MG-LDA model, correspond to
other aspects discussed in reviews (breakfast, prices,
noise), and as it was previously shown in Titov and
McDonald (2008), aspects for global topics corre-
spond to the types of reviewed items (hotels in Rus-
sia, Paris hotels) or background words.
Notice though, that the 3rd local topic induced for
the rating rooms is slightly narrow. This can be ex-
plained by the fact that the aspect rooms is a central
aspect of hotel reviews. A very significant fraction
of text in every review can be thought of as a part of
the aspect rooms. These portions of reviews discuss
different coherent sub-aspects related to the aspect
rooms, e.g., the previously discovered topic noise.
Therefore, it is natural to associate several topics to
such central aspects. To test this we varied the num-
ber of topics associated with the sentiment predictor
for the aspect rooms. Top words for resulting top-
ics are presented in Table 2. It can be observed that
the topic model discovered appropriate topics while
the number of topics was below 4. With 4 topics
a semantically unrelated topic (check-in/arrival) is
induced. Manual selection of the number of topics
is undesirable, but this problem can be potentially
tackled with Dirichlet Process priors or a topic split
criterion based on the accuracy of the sentiment pre-
dictor in the MAS model. We found that both ser-
vice and location did not benefit by the assignment
of additional topics to their sentiment rating models.
The experimental results suggest that the MAS
model is reliable in the discovery of topics corre-
sponding to the rated aspects. In the next section
we will show that the induced topics can be used to
accurately extract fragments for each aspect.
3.2 Sentence Labeling
A primary advantage of MAS over unsupervised
models, such as MG-LDA or clustering, is that top-
ics are linked to a rated aspect, i.e., we know ex-
actly which topics model which aspects. As a re-
sult, these topics can be directly used to extract tex-
tual mentions that are relevant for an aspect. To test
this, we hand labeled 779 random sentences from
the dataset considered in the previous set of experi-
ments. The sentences were labeled with one or more
aspects. Among them, 164, 176 and 263 sentences
were labeled as related to aspects service, location
and rooms, respectively. The remaining sentences
were not relevant to any of the rated aspects.
We compared two models. The first model uses
the first three topics of MAS to extract relevant men-
tions based on the probability of that topic/aspect be-
ing present in the sentence. To obtain these probabil-
ities we used estimators based on the proportion of
words in the sentence assigned to an aspects? topic
and normalized within local topics. To improve the
reliability of the estimator we produced 100 sam-
ples for each document while keeping assignments
of the topics to all other words in the collection fixed.
The probability estimates were then obtained by av-
eraging over these samples. We did not perform
any model selection on the basis of the hand-labeled
data, and tested only a single model of each type.
314
For the second model we trained a maximum en-
tropy classifier, one per each aspect, using 10-fold
cross validation and unigram/bigram features. Note
that this is a supervised system and as such repre-
sents an upper-bound in performance one might ex-
pect when comparing an unsupervised model such
as MAS. We chose this comparison to demonstrate
that our model can find relevant text mentions with
high accuracy relative to a supervised model. It is
difficult to compare our model to other unsupervised
systems such as MG-LDA or LDA. Again, this is
because those systems have no mechanism for di-
rectly correlating topics or clusters to corresponding
aspects, highlighting the benefit of MAS.
The resulting precision-recall curves for the as-
pects service, location and rooms are presented
in Figure 4. In Figure 4c, we varied the number
of topics associated with the aspect rooms.10 The
average precision we obtained (the standard mea-
sure proportional to the area under the curve) is
75.8%, 85.5% for aspects service and location, re-
spectively. For the aspect rooms these scores are
equal to 75.0%, 74.5%, 87.6%, 79.8% with 1?4 top-
ics per aspect, respectively. The logistic regression
models achieve 80.8%, 94.0% and 88.3% for the as-
pects service, location and rooms. We can observe
that the topic model, which does not use any explic-
itly aspect-labeled text, achieves accuracies lower
than, but comparable to a supervised model.
4 Related Work
There is a growing body of work on summariz-
ing sentiment by extracting and aggregating senti-
ment over ratable aspects and providing correspond-
ing textual evidence. Text excerpts are usually ex-
tracted through string matching (Hu and Liu, 2004a;
Popescu and Etzioni, 2005), sentence clustering
(Gamon et al, 2005), or through topic models (Mei
et al, 2007; Titov and McDonald, 2008). String ex-
traction methods are limited to fine-grained aspects
whereas clustering and topic model approaches must
resort to ad-hoc means of labeling clusters or topics.
However, this is the first work we are aware of that
uses a pre-defined set of aspects plus an associated
signal to learn a mapping from text to an aspect for
10To improve readability we smoothed the curve for the as-
pect rooms.
the purpose of extraction.
A closely related model to ours is that of Mei et
al. (2007) which performs joint topic and sentiment
modeling of collections. Our model differs from
theirs in many respects: Mei et al only model senti-
ment predictions for the entire document and not on
the aspect level; They treat sentiment predictions as
unobserved variables, whereas we treat them as ob-
served signals that help to guide the creation of top-
ics; They model co-occurrences solely on the docu-
ment level, whereas our model is based onMG-LDA
and models both local and global contexts.
Recently, Blei and McAuliffe (2008) proposed an
approach for joint sentiment and topic modeling that
can be viewed as a supervised LDA (sLDA) model
that tries to infer topics appropriate for use in a
given classification or regression problem. MAS and
sLDA are similar in that both use sentiment predic-
tions as an observed signal that is predicted by the
model. However, Blei et al do not consider multi-
aspect ranking or look at co-occurrences beyond the
document level, both of which are central to our
model. Parallel to this study Branavan et al (2008)
also showed that joint models of text and user anno-
tations benefit extractive summarization. In partic-
ular, they used signals from pros-cons lists whereas
our models use aspect rating signals.
5 Conclusions
In this paper we presented a joint model of text and
aspect ratings for extracting text to be displayed in
sentiment summaries. The model uses aspect ratings
to discover the corresponding topics and can thus ex-
tract fragments of text discussing these aspects with-
out the need of annotated data. We demonstrated
that the model indeed discovers corresponding co-
herent topics and achieves accuracy in sentence la-
beling comparable to a standard supervised model.
The primary area of future work is to incorporate the
model into an end-to-end sentiment summarization
system in order to evaluate it at that level.
Acknowledgments
This work benefited from discussions with Sasha
Blair-Goldensohn and Fernando Pereira.
315
References
David M. Blei and Jon D. McAuliffe. 2008. Supervised
topic models. In Advances in Neural Information Pro-
cessing Systems (NIPS).
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning Re-
search, 3(5):993?1022.
S.R.K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic proper-
ties from free-text annotations. In Proceedings of the
Annual Conference of the Association for Computa-
tional Linguistics.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-Document
Summarization of Evaluative Text. In Proceedings of
the Conference of the European Chapter of the Asso-
ciation for Computational Linguistics.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free text.
In Proc. of the 6th International Symposium on Intelli-
gent Data Analysis, pages 121?132.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the Natural Academy of
Sciences, 101 Suppl 1:5228?5235.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B. Tenen-
baum. 2004. Integrating topics and syntax. In Ad-
vances in Neural Information Processing Systems.
A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden
Topic Markov Models. In Proceedings of the Confer-
ence on Artificial Intelligence and Statistics.
M. Hu and B. Liu. 2004a. Mining and summarizing
customer reviews. In Proceedings of the 2004 ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177. ACM Press
New York, NY, USA.
M. Hu and B. Liu. 2004b. Mining Opinion Features
in Customer Reviews. In Proceedings of Nineteenth
National Conference on Artificial Intellgience.
C. Manning and M. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
Q. Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the 16th Interna-
tional Conference on World Wide Web, pages 171?180.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71?113.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
A.M. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
B. Snyder and R. Barzilay. 2007. Multiple Aspect Rank-
ing using the Good Grief Algorithm. In Proceedings
of the Joint Conference of the North American Chapter
of the Association for Computational Linguistics and
Human Language Technologies, pages 300?307.
I. Titov and R. McDonald. 2008. Modeling online re-
views with multi-grain topic models. In Proceedings
of the 17h International Conference on World Wide
Web.
P. Turney. 2002. Thumbs up or thumbs down? Senti-
ment orientation applied to unsupervised classification
of reviews. In Proceedings of the Annual Conference
of the Association for Computational Linguistics.
Hanna M. Wallach. 2006. Topic modeling; beyond bag
of words. In International Conference on Machine
Learning.
Xuerui Wang and Andrew McCallum. 2005. A note on
topical n-grams. Technical Report UM-CS-2005-071,
University of Massachusetts.
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In Proceedings of the National Conference on
Artificial Intelligence.
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management (CIKM), pages 43?50.
316
Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Integrating Graph-Based and Transition-Based Dependency Parsers
Joakim Nivre
Va?xjo? University Uppsala University
Computer Science Linguistics and Philology
SE-35195 Va?xjo? SE-75126 Uppsala
nivre@msi.vxu.se
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Abstract
Previous studies of data-driven dependency
parsing have shown that the distribution of
parsing errors are correlated with theoretical
properties of the models used for learning and
inference. In this paper, we show how these
results can be exploited to improve parsing
accuracy by integrating a graph-based and a
transition-based model. By letting one model
generate features for the other, we consistently
improve accuracy for both models, resulting
in a significant improvement of the state of
the art when evaluated on data sets from the
CoNLL-X shared task.
1 Introduction
Syntactic dependency graphs have recently gained
a wide interest in the natural language processing
community and have been used for many problems
ranging from machine translation (Ding and Palmer,
2004) to ontology construction (Snow et al, 2005).
A dependency graph for a sentence represents each
word and its syntactic dependents through labeled
directed arcs, as shown in figure 1. One advantage
of this representation is that it extends naturally to
discontinuous constructions, which arise due to long
distance dependencies or in languages where syntac-
tic structure is encoded in morphology rather than in
word order. This is undoubtedly one of the reasons
for the emergence of dependency parsers for a wide
range of languages. Many of these parsers are based
on data-driven parsing models, which learn to pro-
duce dependency graphs for sentences solely from
an annotated corpus and can be easily ported to any
Figure 1: Dependency graph for an English sentence.
language or domain in which annotated resources
exist.
Practically all data-driven models that have been
proposed for dependency parsing in recent years can
be described as either graph-based or transition-
based (McDonald and Nivre, 2007). In graph-based
parsing, we learn a model for scoring possible de-
pendency graphs for a given sentence, typically by
factoring the graphs into their component arcs, and
perform parsing by searching for the highest-scoring
graph. This type of model has been used by, among
others, Eisner (1996), McDonald et al (2005a), and
Nakagawa (2007). In transition-based parsing, we
instead learn a model for scoring transitions from
one parser state to the next, conditioned on the parse
history, and perform parsing by greedily taking the
highest-scoring transition out of every parser state
until we have derived a complete dependency graph.
This approach is represented, for example, by the
models of Yamada and Matsumoto (2003), Nivre et
al. (2004), and Attardi (2006).
Theoretically, these approaches are very different.
The graph-based models are globally trained and use
exact inference algorithms, but define features over a
limited history of parsing decisions. The transition-
based models are essentially the opposite. They use
local training and greedy inference algorithms, but
950
define features over a rich history of parsing deci-
sions. This is a fundamental trade-off that is hard
to overcome by tractable means. Both models have
been used to achieve state-of-the-art accuracy for a
wide range of languages, as shown in the CoNLL
shared tasks on dependency parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007), but McDonald and
Nivre (2007) showed that a detailed error analysis
reveals important differences in the distribution of
errors associated with the two models.
In this paper, we consider a simple way of inte-
grating graph-based and transition-based models in
order to exploit their complementary strengths and
thereby improve parsing accuracy beyond what is
possible by either model in isolation. The method
integrates the two models by allowing the output
of one model to define features for the other. This
method is simple ? requiring only the definition of
new features ? and robust by allowing a model to
learn relative to the predictions of the other.
2 Two Models for Dependency Parsing
2.1 Preliminaries
Given a set L = {l1, . . . , l|L|} of arc labels (depen-
dency relations), a dependency graph for an input
sentence x = w0, w1, . . . , wn (where w0 = ROOT) is
a labeled directed graph G = (V,A) consisting of a
set of nodes V = {0, 1, . . . , n}1 and a set of labeled
directed arcs A ? V ?V ?L, i.e., if (i, j, l) ? A for
i, j ? V and l ? L, then there is an arc from node
i to node j with label l in the graph. A dependency
graphG for a sentence xmust be a directed tree orig-
inating out of the root node 0 and spanning all nodes
in V , as exemplified by the graph in figure 1. This
is a common constraint in many dependency parsing
theories and their implementations.
2.2 Graph-Based Models
Graph-based dependency parsers parameterize a
model over smaller substructures in order to search
the space of valid dependency graphs and produce
the most likely one. The simplest parameterization
is the arc-factored model that defines a real-valued
score function for arcs s(i, j, l) and further defines
the score of a dependency graph as the sum of the
1We use the common convention of representing words by
their index in the sentence.
score of all the arcs it contains. As a result, the de-
pendency parsing problem is written:
G = argmax
G=(V,A)
?
(i,j,l)?A
s(i, j, l)
This problem is equivalent to finding the highest
scoring directed spanning tree in the complete graph
over the input sentence, which can be solved in
O(n2) time (McDonald et al, 2005b). Additional
parameterizations are possible that take more than
one arc into account, but have varying effects on
complexity (McDonald and Satta, 2007). An advan-
tage of graph-based methods is that tractable infer-
ence enables the use of standard structured learning
techniques that globally set parameters to maximize
parsing performance on the training set (McDonald
et al, 2005a). The primary disadvantage of these
models is that scores ? and as a result any feature
representations ? are restricted to a single arc or a
small number of arcs in the graph.
The specific graph-based model studied in this
work is that presented by McDonald et al (2006),
which factors scores over pairs of arcs (instead of
just single arcs) and uses near exhaustive search for
unlabeled parsing coupled with a separate classifier
to label each arc. We call this system MSTParser, or
simply MST for short, which is also the name of the
freely available implementation.2
2.3 Transition-Based Models
Transition-based dependency parsing systems use a
model parameterized over transitions of an abstract
machine for deriving dependency graphs, such that
every transition sequence from the designated initial
configuration to some terminal configuration derives
a valid dependency graph. Given a real-valued score
function s(c, t) (for transition t out of configuration
c), parsing can be performed by starting from the ini-
tial configuration and taking the optimal transition
t? = argmaxt?T s(c, t) out of every configuration
c until a terminal configuration is reached. This can
be seen as a greedy search for the optimal depen-
dency graph, based on a sequence of locally optimal
decisions in terms of the transition system.
Many transition systems for data-driven depen-
dency parsing are inspired by shift-reduce parsing,
2http://mstparser.sourceforge.net
951
where each configuration c contains a stack ?c for
storing partially processed nodes and a buffer ?c
containing the remaining input. Transitions in such a
system add arcs to the dependency graph and mani-
pulate the stack and buffer. One example is the tran-
sition system defined by Nivre (2003), which parses
a sentence x = w0, w1, . . . , wn in O(n) time.
To learn a scoring function on transitions, these
systems rely on discriminative learning methods,
such as memory-based learning or support vector
machines, using a strictly local learning procedure
where only single transitions are scored (not com-
plete transition sequences). The main advantage of
these models is that features are not restricted to a
limited number of graph arcs but can take into ac-
count the entire dependency graph built so far. The
major disadvantage is that the greedy parsing strat-
egy may lead to error propagation.
The specific transition-based model studied in
this work is that presented by Nivre et al (2006),
which uses support vector machines to learn transi-
tion scores. We call this system MaltParser, or Malt
for short, which is also the name of the freely avail-
able implementation.3
2.4 Comparison and Analysis
These models differ primarily with respect to three
properties: inference, learning, and feature repre-
sentation. MaltParser uses an inference algorithm
that greedily chooses the best parsing decision based
on the current parser history whereas MSTParser
uses exhaustive search algorithms over the space of
all valid dependency graphs to find the graph that
maximizes the score. MaltParser trains a model
to make a single classification decision (choose the
next transition) whereas MSTParser trains a model
to maximize the global score of correct graphs.
MaltParser can introduce a rich feature history based
on previous parser decisions, whereas MSTParser is
forced to restrict features to a single decision or a
pair of nearby decisions in order to retain efficiency.
These differences highlight an inherent trade-off
between global inference/learning and expressive-
ness of feature representations. MSTParser favors
the former at the expense of the latter andMaltParser
the opposite. This difference was highlighted in the
3http://w3.msi.vxu.se/?jha/maltparser/
study of McDonald and Nivre (2007), which showed
that the difference is reflected directly in the error
distributions of the parsers. Thus, MaltParser is less
accurate than MSTParser for long dependencies and
those closer to the root of the graph, but more accu-
rate for short dependencies and those farthest away
from the root. Furthermore, MaltParser is more ac-
curate for dependents that are nouns and pronouns,
whereas MSTParser is more accurate for verbs, ad-
jectives, adverbs, adpositions, and conjunctions.
Given that there is a strong negative correlation
between dependency length and tree depth, and
given that nouns and pronouns tend to be more
deeply embedded than (at least) verbs and conjunc-
tions, these patterns can all be explained by the same
underlying factors. Simply put, MaltParser has an
advantage in its richer feature representations, but
this advantage is gradually diminished by the nega-
tive effect of error propagation due to the greedy in-
ference strategy as sentences and dependencies get
longer. MSTParser has a more even distribution of
errors, which is expected given that the inference al-
gorithm and feature representation should not prefer
one type of arc over another. This naturally leads
one to ask: Is it possible to integrate the two models
in order to exploit their complementary strengths?
This is the topic of the remainder of this paper.
3 Integrated Models
There are many conceivable ways of combining the
two parsers, including more or less complex en-
semble systems and voting schemes, which only
perform the integration at parsing time. However,
given that we are dealing with data-driven models,
it should be possible to integrate at learning time, so
that the two complementary models can learn from
one another. In this paper, we propose to do this by
letting one model generate features for the other.
3.1 Feature-Based Integration
As explained in section 2, both models essentially
learn a scoring function s : X ? R, where the
domain X is different for the two models. For the
graph-based model, X is the set of possible depen-
dency arcs (i, j, l); for the transition-based model,
X is the set of possible configuration-transition pairs
(c, t). But in both cases, the input is represented
952
MSTMalt ? defined over (i, j, l) (? = any label/node)
Is (i, j, ?) in GMaltx ?
Is (i, j, l) in GMaltx ?
Is (i, j, ?) not in GMaltx ?
Is (i, j, l) not in GMaltx ?
Identity of l? such that (?, j, l?) is in GMaltx ?
Identity of l? such that (i, j, l?) is in GMaltx ?
MaltMST ? defined over (c, t) (? = any label/node)
Is (?0c , ?
0
c , ?) in G
MST
x ?
Is (?0c , ?
0
c , ?) in G
MST
x ?
Head direction for ?0c in G
MST
x (left/right/ROOT)
Head direction for ?0c in G
MST
x (left/right/ROOT)
Identity of l such that (?, ?0c , l) is in G
MST
x ?
Identity of l such that (?, ?0c , l) is in G
MST
x ?
Table 1: Guide features for MSTMalt and MaltMST.
by a k-dimensional feature vector f : X ? Rk.
In the feature-based integration we simply extend
the feature vector for one model, called the base
model, with a certain number of features generated
by the other model, which we call the guide model
in this context. The additional features will be re-
ferred to as guide features, and the version of the
base model trained with the extended feature vector
will be called the guided model. The idea is that the
guided model should be able to learn in which situ-
ations to trust the guide features, in order to exploit
the complementary strength of the guide model, so
that performance can be improved with respect to
the base parser. This method of combining classi-
fiers is sometimes referred to as classifier stacking.
The exact form of the guide features depend on
properties of the base model and will be discussed
in sections 3.2?3.3 below, but the overall scheme for
the feature-based integration can be described as fol-
lows. To train a guided version BC of base model B
with guide model C and training set T , the guided
model is trained, not on the original training set T ,
but on a version of T that has been parsed with the
guide model C under a cross-validation scheme (to
avoid overlap with training data for C). This means
that, for every sentence x ? T , BC has access at
training time to both the gold standard dependency
graph Gx and the graph GCx predicted by C, and it is
the latter that forms the basis for the additional guide
features. When parsing a new sentence x? with BC ,
x? is first parsed with model C (this time trained on
the entire training set T ) to derive GCx? , so that the
guide features can be extracted also at parsing time.
3.2 The Guided Graph-Based Model
The graph-based model, MSTParser, learns a scor-
ing function s(i, j, l) ? R over labeled dependen-
cies. More precisely, dependency arcs (or pairs of
arcs) are first represented by a high dimensional fea-
ture vector f(i, j, l) ? Rk, where f is typically a bi-
nary feature vector over properties of the arc as well
as the surrounding input (McDonald et al, 2005a;
McDonald et al, 2006). The score of an arc is de-
fined as a linear classifier s(i, j, l) = w ? f(i, j, l),
where w is a vector of feature weights to be learned
by the model.
For the guided graph-based model, which we call
MSTMalt, this feature representation is modified to
include an additional argument GMaltx , which is the
dependency graph predicted by MaltParser on the
input sentence x. Thus, the new feature represen-
tation will map an arc and the entire predicted Malt-
Parser graph to a high dimensional feature repre-
sentation, f(i, j, l, GMaltx ) ? R
k+m. These m ad-
ditional features account for the guide features over
the MaltParser output. The specific features used by
MSTMalt are given in table 1. All features are con-
joined with the part-of-speech tags of the words in-
volved in the dependency to allow the guided parser
to learn weights relative to different surface syntac-
tic environments. Though MSTParser is capable of
defining features over pairs of arcs, we restrict the
guide features over single arcs as this resulted in
higher accuracies during preliminary experiments.
3.3 The Guided Transition-Based Model
The transition-based model, MaltParser, learns a
scoring function s(c, t) ? R over configurations and
transitions. The set of training instances for this
learning problem is the set of pairs (c, t) such that
t is the correct transition out of c in the transition
sequence that derives the correct dependency graph
Gx for some sentence x in the training set T . Each
training instance (c, t) is represented by a feature
vector f(c, t) ? Rk, where features are defined in
terms of arbitrary properties of the configuration c,
including the state of the stack ?c, the input buffer
?c, and the partially built dependency graph Gc. In
particular, many features involve properties of the
two target tokens, the token on top of the stack ?c
(?0c ) and the first token in the input buffer ?c (?
0
c ),
953
which are the two tokens that may become con-
nected by a dependency arc through the transition
out of c. The full set of features used by the base
model MaltParser is described in Nivre et al (2006).
For the guided transition-based model, which we
call MaltMST, training instances are extended to
triples (c, t, GMSTx ), where G
MST
x is the dependency
graph predicted by the graph-based MSTParser for
the sentence x to which the configuration c belongs.
We define m additional guide features, based on
properties of GMSTx , and extend the feature vector
accordingly to f(c, t, GMSTx ) ? R
k+m. The specific
features used by MaltMST are given in table 1. Un-
like MSTParser, features are not explicitly defined
to conjoin guide features with part-of-speech fea-
tures. These features are implicitly added through
the polynomial kernel used to train the SVM.
4 Experiments
In this section, we present an experimental evalua-
tion of the two guided models based on data from
the CoNLL-X shared task, followed by a compar-
ative error analysis including both the base models
and the guided models. The data for the experiments
are training and test sets for all thirteen languages
from the CoNLL-X shared task on multilingual de-
pendency parsing with training sets ranging in size
from from 29,000 tokens (Slovene) to 1,249,000 to-
kens (Czech). The test sets are all standardized to
about 5,000 tokens each. For more information on
the data sets, see Buchholz and Marsi (2006).
The guided models were trained according to the
scheme explained in section 3, with two-fold cross-
validation when parsing the training data with the
guide parsers. Preliminary experiments suggested
that cross-validation with more folds had a negli-
gible impact on the results. Models are evaluated
by their labeled attachment score (LAS) on the test
set, i.e., the percentage of tokens that are assigned
both the correct head and the correct label, using
the evaluation software from the CoNLL-X shared
task with default settings.4 Statistical significance
was assessed using Dan Bikel?s randomized pars-
ing evaluation comparator with the default setting of
10,000 iterations.5
4http://nextens.uvt.nl/?conll/software.html
5http://www.cis.upenn.edu/?dbikel/software.html
Language MST MSTMalt Malt MaltMST
Arabic 66.91 68.64 (+1.73) 66.71 67.80 (+1.09)
Bulgarian 87.57 89.05 (+1.48) 87.41 88.59 (+1.18)
Chinese 85.90 88.43 (+2.53) 86.92 87.44 (+0.52)
Czech 80.18 82.26 (+2.08) 78.42 81.18 (+2.76)
Danish 84.79 86.67 (+1.88) 84.77 85.43 (+0.66)
Dutch 79.19 81.63 (+2.44) 78.59 79.91 (+1.32)
German 87.34 88.46 (+1.12) 85.82 87.66 (+1.84)
Japanese 90.71 91.43 (+0.72) 91.65 92.20 (+0.55)
Portuguese 86.82 87.50 (+0.68) 87.60 88.64 (+1.04)
Slovene 73.44 75.94 (+2.50) 70.30 74.24 (+3.94)
Spanish 82.25 83.99 (+1.74) 81.29 82.41 (+1.12)
Swedish 82.55 84.66 (+2.11) 84.58 84.31 (?0.27)
Turkish 63.19 64.29 (+1.10) 65.58 66.28 (+0.70)
Average 80.83 82.53 (+1.70) 80.74 82.01 (+1.27)
Table 2: Labeled attachment scores for base parsers and
guided parsers (improvement in percentage points).
10 20 30 40 50 60Sentence Length
0.7
0.75
0.8
0.85
0.9
Accu
racy
MaltMSTMalt+MSTMST+Malt
Figure 2: Accuracy relative to sentence length.
4.1 Results
Table 2 shows the results, for each language and on
average, for the two base models (MST, Malt) and
for the two guided models (MSTMalt, MaltMST).
First of all, we see that both guided models show
a very consistent increase in accuracy compared to
their base model, even though the extent of the im-
provement varies across languages from about half
a percentage point (MaltMST on Chinese) up to al-
most four percentage points (MaltMST on Slovene).6
It is thus quite clear that both models have the capa-
city to learn from features generated by the other
model. However, it is also clear that the graph-based
MST model shows a somewhat larger improvement,
both on average and for all languages except Czech,
6The only exception to this pattern is the result for MaltMST
on Swedish, where we see an unexpected drop in accuracy com-
pared to the base model.
954
2 4 6 8 10 12             14      15+Dependency Length
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Recal
l
MaltMSTMalt+MSTMST+Malt
2 4 6 8 10 12             14      15+Dependency Length0.55
0.6
0.65
0.7
0.75
0.8
0.85
Precis
ion
MaltMSTMalt+MSTMST+Malt
1 2 3 4 5 6 7+Distance to Root
0.8
0.82
0.84
0.86
0.88
0.9
Recall
MaltMSTMalt+MSTMST+Malt
1 2 3 4 5 6 7+Distance to Root
0.78
0.8
0.82
0.84
0.86
0.88
0.9
0.92
Precis
ion
MaltMSTMalt+MSTMST+Malt
(a) (b)
Figure 3: Dependency arc precision/recall relative to predicted/gold for (a) dependency length and (b) distance to root.
German, Portuguese and Slovene. Finally, given
that the two base models had the previously best
performance for these data sets, the guided models
achieve a substantial improvement of the state of the
art. While there is no statistically significant differ-
ence between the two base models, they are both
outperformed by MaltMST (p < 0.0001), which in
turn has significantly lower accuracy than MSTMalt
(p < 0.0005).
An extension to the models described so far would
be to iteratively integrate the two parsers in the
spirit of pipeline iteration (Hollingshead and Roark,
2007). For example, one could start with a Malt
model, use it to train a guided MSTMalt model, then
use that as the guide to train a MaltMSTMalt model,
etc. We ran such experiments, but found that accu-
racy did not increase significantly and in some cases
decreased slightly. This was true regardless of which
parser began the iterative process. In retrospect, this
result is not surprising. Since the initial integration
effectively incorporates knowledge from both pars-
ing systems, there is little to be gained by adding
additional parsers in the chain.
4.2 Analysis
The experimental results presented so far show that
feature-based integration is a viable approach for
improving the accuracy of both graph-based and
transition-based models for dependency parsing, but
they say very little about how the integration benefits
the two models and what aspects of the parsing pro-
cess are improved as a result. In order to get a better
understanding of these matters, we replicate parts of
the error analysis presented by McDonald and Nivre
(2007), where parsing errors are related to different
structural properties of sentences and their depen-
dency graphs. For each of the four models evalu-
ated, we compute error statistics for labeled attach-
ment over all twelve languages together.
Figure 2 shows accuracy in relation to sentence
length, binned into ten-word intervals (1?10, 11-20,
etc.). As expected, Malt and MST have very simi-
lar accuracy for short sentences but Malt degrades
more rapidly with increasing sentence length be-
cause of error propagation (McDonald and Nivre,
2007). The guided models, MaltMST and MSTMalt,
behave in a very similar fashion with respect to each
other but both outperform their base parser over the
entire range of sentence lengths. However, except
for the two extreme data points (0?10 and 51?60)
there is also a slight tendency for MaltMST to im-
prove more for longer sentences and for MSTMalt to
improve more for short sentences, which indicates
that the feature-based integration allows one parser
to exploit the strength of the other.
Figure 3(a) plots precision (top) and recall (bot-
tom) for dependency arcs of different lengths (pre-
dicted arcs for precision, gold standard arcs for re-
call). With respect to recall, the guided models ap-
pear to have a slight advantage over the base mod-
955
Part of Speech MST MSTMalt Malt MaltMST
Verb 82.6 85.1 (2.5) 81.9 84.3 (2.4)
Noun 80.0 81.7 (1.7) 80.7 81.9 (1.2)
Pronoun 88.4 89.4 (1.0) 89.2 89.3 (0.1)
Adjective 89.1 89.6 (0.5) 87.9 89.0 (1.1)
Adverb 78.3 79.6 (1.3) 77.4 78.1 (0.7)
Adposition 69.9 71.5 (1.6) 68.8 70.7 (1.9)
Conjunction 73.1 74.9 (1.8) 69.8 72.5 (2.7)
Table 3: Accuracy relative to dependent part of speech
(improvement in percentage points).
els for short and medium distance arcs. With re-
spect to precision, however, there are two clear pat-
terns. First, the graph-based models have better pre-
cision than the transition-based models when pre-
dicting long arcs, which is compatible with the re-
sults of McDonald and Nivre (2007). Secondly, both
the guided models have better precision than their
base model and, for the most part, also their guide
model. In particular MSTMalt outperformsMST and
is comparable to Malt for short arcs. More inter-
estingly, MaltMST outperforms both Malt and MST
for arcs up to length 9, which provides evidence that
MaltMST has learned specifically to trust the guide
features from MST for longer dependencies. The
reason that accuracy does not improve for dependen-
cies of length greater than 9 is probably that these
dependencies are too rare for MaltMST to learn from
the guide parser in these situations.
Figure 3(b) shows precision (top) and recall (bot-
tom) for dependency arcs at different distances from
the root (predicted arcs for precision, gold standard
arcs for recall). Again, we find the clearest pat-
terns in the graphs for precision, where Malt has
very low precision near the root but improves with
increasing depth, while MST shows the opposite
trend (McDonald and Nivre, 2007). Considering
the guided models, it is clear that MaltMST im-
proves in the direction of its guide model, with a
5-point increase in precision for dependents of the
root and smaller improvements for longer distances.
Similarly, MSTMalt improves precision in the range
where its base parser is inferior to Malt and for dis-
tances up to 4 has an accuracy comparable to or
higher than its guide parser Malt. This again pro-
vides evidence that the guided parsers are learning
from their guide models.
Table 3 gives the accuracy for arcs relative to de-
pendent part-of-speech. As expected, we see that
MST does better than Malt for all categories except
nouns and pronouns (McDonald and Nivre, 2007).
But we also see that the guided models in all cases
improve over their base parser and, in most cases,
also over their guide parser. The general trend is that
MST improves more thanMalt, except for adjectives
and conjunctions, where Malt has a greater disad-
vantage from the start and therefore benefits more
from the guide features.
Considering the results for parts of speech, as well
as those for dependency length and root distance, it
is interesting to note that the guided models often
improve even in situations where their base parsers
are more accurate than their guide models. This sug-
gests that the improvement is not a simple function
of the raw accuracy of the guide model but depends
on the fact that labeled dependency decisions inter-
act in inference algorithms for both graph-based and
transition-based parsing systems. Thus, if a parser
can improve its accuracy on one class of dependen-
cies, e.g., longer ones, then we can expect to see im-
provements on all types of dependencies ? as we do.
The interaction between different decisions may
also be part of the explanation why MST benefits
more from the feature-based integration than Malt,
with significantly higher accuracy for MSTMalt than
for MaltMST as a result. Since inference is global
(or practically global) in the graph-based model,
an improvement in one type of dependency has a
good chance of influencing the accuracy of other de-
pendencies, whereas in the transition-based model,
where inference is greedy, some of these additional
benefits will be lost because of error propagation.
This is reflected in the error analysis in the following
recurrent pattern: Where Malt does well, MaltMST
does only slightly better. But where MST is good,
MSTMalt is often significantly better.
Another part of the explanation may have to do
with the learning algorithms used by the systems.
Although both Malt and MST use discriminative
algorithms, Malt uses a batch learning algorithm
(SVM) and MST uses an online learning algorithm
(MIRA). If the original rich feature representation
of Malt is sufficient to separate the training data,
regularization may force the weights of the guided
features to be small (since they are not needed at
training time). On the other hand, an online learn-
956
ing algorithm will recognize the guided features as
strong indicators early in training and give them a
high weight as a result. Features with high weight
early in training tend to have the most impact on the
final classifier due to both weight regularization and
averaging. This is in fact observed when inspecting
the weights of MSTMalt.
5 Related Work
Combinations of graph-based and transition-based
models for data-driven dependency parsing have
previously been explored by Sagae and Lavie
(2006), who report improvements of up to 1.7 per-
centage points over the best single parser when
combining three transition-based models and one
graph-based model for unlabeled dependency pars-
ing, evaluated on data from the Penn Treebank. The
combined parsing model is essentially an instance of
the graph-based model, where arc scores are derived
from the output of the different component parsers.
Unlike the models presented here, integration takes
place only at parsing time, not at learning time, and
requires at least three different base parsers. The
same technique was used by Hall et al (2007) to
combine six transition-based parsers in the best per-
forming system in the CoNLL 2007 shared task.
Feature-based integration in the sense of letting a
subset of the features for one model be derived from
the output of a different model has been exploited
for dependency parsing by McDonald (2006), who
trained an instance of MSTParser using features
generated by the parsers of Collins (1999) and Char-
niak (2000), which improved unlabeled accuracy by
1.7 percentage points, again on data from the Penn
Treebank. In addition, feature-based integration has
been used by Taskar et al (2005), who trained a
discriminative word alignment model using features
derived from the IBM models, and by Florian et al
(2004), who trained classifiers on auxiliary data to
guide named entity classifiers.
Feature-based integration also has points in com-
mon with co-training, which have been applied to
syntactic parsing by Sarkar (2001) and Steedman et
al. (2003), among others. The difference, of course,
is that standard co-training is a weakly supervised
method, where guide features replace, rather than
complement, the gold standard annotation during
training. Feature-based integration is also similar to
parse re-ranking (Collins, 2000), where one parser
produces a set of candidate parses and a second-
stage classifier chooses the most likely one. How-
ever, feature-based integration is not explicitly con-
strained to any parse decisions that the guide model
might make and only the single most likely parse is
used from the guide model, making it significantly
more efficient than re-ranking.
Finally, there are several recent developments in
data-driven dependency parsing, which can be seen
as targeting the specific weaknesses of graph-based
and transition-based models, respectively, though
without integrating the two models. Thus, Naka-
gawa (2007) and Hall (2007) both try to overcome
the limited feature scope of graph-based models by
adding global features, in the former case using
Gibbs sampling to deal with the intractable infer-
ence problem, in the latter case using a re-ranking
scheme. For transition-based models, the trend is
to alleviate error propagation by abandoning greedy,
deterministic inference in favor of beam search with
globally normalized models for scoring transition
sequences, either generative (Titov and Henderson,
2007a; Titov and Henderson, 2007b) or conditional
(Duan et al, 2007; Johansson and Nugues, 2007).
6 Conclusion
In this paper, we have demonstrated how the two
dominant approaches to data-driven dependency
parsing, graph-based models and transition-based
models, can be integrated by letting one model learn
from features generated by the other. Our experi-
mental results show that both models consistently
improve their accuracy when given access to fea-
tures generated by the other model, which leads to
a significant advancement of the state of the art in
data-driven dependency parsing. Moreover, a com-
parative error analysis reveals that the improvements
are largely predictable from theoretical properties of
the two models, in particular the tradeoff between
global learning and inference, on the one hand, and
rich feature representations, on the other. Directions
for future research include a more detailed analysis
of the effect of feature-based integration, as well as
the exploration of other strategies for integrating dif-
ferent parsing models.
957
References
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of CoNLL, pages 166?170.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML, pages
175?182.
Yuan Ding and Martha Palmer. 2004. Synchronous de-
pendency insertion grammars: A grammar formalism
for syntax based statistical MT. In Proceedings of the
Workshop on Recent Advances in Dependency Gram-
mar, pages 90?97.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic parsing action models for multi-lingual dependency
parsing. In Proceedings of EMNLP-CoNLL, pages
940?946.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of COLING, pages 340?345.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A statisti-
cal model for multilingual entity detection and track-
ing. In Proceedings of NAACL/HLT.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilin-
gual parser optimization. In Proceedings of EMNLP-
CoNLL.
Keith Hall. 2007. K-best spanning tree parsing. In Pro-
ceedings of ACL, pages 392?399.
Kristy Hollingshead and Brian Roark. 2007. Pipeline
iteration. In Proceedings of ACL, pages 952?959.
Richard Johansson and Pierre Nugues. 2007. Incremen-
tal dependency parsing using online learning. In Pro-
ceedings of EMNLP-CoNLL, pages 1134?1138.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, pages 122?
131.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Proceedings of IWPT, pages 122?131.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT/EMNLP, pages 523?530.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL,
pages 216?220.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proceedings of EMNLP-
CoNLL, pages 952?956.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL, pages 221?225.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL, pages
915?932.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of IWPT,
pages 149?160.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL: Short Papers,
pages 129?132.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL, pages
175?182.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of NIPS.
Mark Steedman, Rebecca Hwa, Miles Osborne, and
Anoop Sarkar. 2003. Corrected co-training for statis-
tical parsers. In Proceedings of ICML, pages 95?102.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In Proceedings of HLT/EMNLP, pages 73?80.
Ivan Titov and James Henderson. 2007a. Fast and ro-
bust multilingual dependency parsing with a genera-
tive latent variable model. In Proceedings of EMNLP-
CoNLL, pages 947?951.
Ivan Titov and James Henderson. 2007b. A latent vari-
able model for generative dependency parsing. In Pro-
ceedings of IWPT, pages 144?155.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
958
Proceedings of the 10th Conference on Parsing Technologies, pages 121?132,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
On the Complexity of Non-Projective Data-Driven Dependency Parsing
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10028
ryanmcd@google.com
Giorgio Satta
University of Padua
via Gradenigo 6/A
I-35131 Padova, Italy
satta@dei.unipd.it
Abstract
In this paper we investigate several non-
projective parsing algorithms for depen-
dency parsing, providing novel polynomial
time solutions under the assumption that
each dependency decision is independent of
all the others, called here the edge-factored
model. We also investigate algorithms for
non-projective parsing that account for non-
local information, and present several hard-
ness results. This suggests that it is unlikely
that exact non-projective dependency pars-
ing is tractable for any model richer than the
edge-factored model.
1 Introduction
Dependency representations of natural language are
a simple yet flexible mechanism for encoding words
and their syntactic dependencies through directed
graphs. These representations have been thoroughly
studied in descriptive linguistics (Tesnie`re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988) and
have been applied in numerous language process-
ing tasks. Figure 1 gives an example dependency
graph for the sentence Mr. Tomash will remain as a
director emeritus, which has been extracted from the
Penn Treebank (Marcus et al, 1993). Each edge in
this graph represents a single syntactic dependency
directed from a word to its modifier. In this rep-
resentation all edges are labeled with the specific
syntactic function of the dependency, e.g., SBJ for
subject and NMOD for modifier of a noun. To sim-
plify computation and some important definitions,
an artificial token is inserted into the sentence as the
left most word and will always represent the root of
the dependency graph. We assume all dependency
graphs are directed trees originating out of a single
node, which is a common constraint (Nivre, 2005).
The dependency graph in Figure 1 is an exam-
ple of a nested or projective graph. Under the as-
sumption that the root of the graph is the left most
word of the sentence, a projective graph is one where
the edges can be drawn in the plane above the sen-
tence with no two edges crossing. Conversely, a
non-projective dependency graph does not satisfy
this property. Figure 2 gives an example of a non-
projective graph for a sentence that has also been
extracted from the Penn Treebank. Non-projectivity
arises due to long distance dependencies or in lan-
guages with flexible word order. For many lan-
guages, a significant portion of sentences require
a non-projective dependency analysis (Buchholz et
al., 2006). Thus, the ability to learn and infer non-
projective dependency graphs is an important prob-
lem in multilingual language processing.
Syntactic dependency parsing has seen a num-
ber of new learning and inference algorithms which
have raised state-of-the-art parsing accuracies for
many languages. In this work we focus on data-
drivenmodels of dependency parsing. These models
are not driven by any underlying grammar, but in-
stead learn to predict dependency graphs based on
a set of parameters learned solely from a labeled
corpus. The advantage of these models is that they
negate the need for the development of grammars
when adapting the model to new languages.
One interesting class of data-driven models are
121
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
122
mal dependency grammar is hard. In addition, the
work of Kahane et al (1998) provides a polynomial
parsing algorithm for a constrained class of non-
projective structures. Non-projective dependency
parsing can be related to certain parsing problems
defined for phrase structure representations, as for
instance immediate dominance CFG parsing (Barton
et al, 1987) and shake-and-bake translation (Brew,
1992).
Independently of this work, Koo et al (2007) and
Smith and Smith (2007) showed that the Matrix-
Tree Theorem can be used to train edge-factored
log-linear models of dependency parsing. Both stud-
ies constructed implementations that compare favor-
ably with the state-of-the-art. The work of Meila?
and Jaakkola (2000) is also of note. In that study
they use the Matrix Tree Theorem to develop a
tractable bayesian learning algorithms for tree belief
networks, which in many ways are closely related
to probabilistic dependency parsing formalisms and
the problems we address here.
2 Preliminaries
Let L = {l1, . . . , l|L|} be a set of permissible syn-
tactic edge labels and x = x0x1 ? ? ?xn be a sen-
tence such that x0=root. From this sentence we con-
struct a complete labeled directed graph (digraph)
Gx = (Vx, Ex) such that,
? Vx = {0, 1, . . . , n}
? Ex = {(i, j)k | ? i, j ? Vx and 1 ? k ? |L|}
Gx is a graph where each word in the sentence is a
node, and there is a directed edge between every pair
of nodes for every possible label. By its definition,
Gx is a multi-digraph, which is a digraph that may
have more than one edge between any two nodes.
Let (i, j)k represent the kth edge from i to j. Gx en-
codes all possible labeled dependencies between the
words of x. Thus every possible dependency graph
of x must be a subgraph of Gx.
Let i ?+ j be a relation that is true if and only
if there is a non-empty directed path from node i to
node j in some graph under consideration. A di-
rected spanning tree1 of a graph G, that originates
1A directed spanning tree is commonly referred to as a ar-
borescence in the graph theory literature.
out of node 0, is any subgraph T = (VT , ET ) such
that,
? VT = Vx and ET ? Ex
? ?j ? VT , 0 ?+ j if and only if j 6= 0
? If (i, j)k ? ET , then (i?, j)k
?
/? ET , ?i? 6= i
and/or k? 6= k.
Define T (G) as the set of all directed spanning trees
for a graph G. As McDonald et al (2005b) noted,
there is a one-to-one correspondence between span-
ning trees of Gx and labeled dependency graphs
of x, i.e., T (Gx) is exactly the set of all possible
projective and non-projective dependency graphs for
sentence x. Throughout the rest of this paper, we
will refer to any T ? T (Gx) as a valid dependency
graph for a sentence x. Thus, by definition, every
valid dependency graph must be a tree.
3 Edge-factored Models
In this section we examine the class of models that
assume each dependency decision is independent.
Within this setting, every edge in an induced graph
Gx for a sentence x will have an associated weight
wkij ? 0 that maps the k
th directed edge from node
i to node j to a real valued numerical weight. These
weights represents the likelihood of a dependency
occurring from word wi to word wj with label lk.
Define the weight of a spanning tree T = (VT , ET )
as the product of the edge weights
w(T ) =
?
(i,j)k?ET
wkij
It is easily shown that this formulation includes
the projective model of Paskin (2001) and the non-
projective model of McDonald et al (2005b).
The definition of wkij depends on the context in
which it is being used. For example, in the work of
McDonald et al (2005b) it is simply a linear classi-
fier that is a function of the words in the dependency,
the label of the dependency, and any contextual fea-
tures of the words in the sentence. In a generative
probabilistic model (such as Paskin (2001)) it could
represent the conditional probability of a word wj
being generated with a label lk given that the word
being modified is wi (possibly with some other in-
formation such as the orientation of the dependency
123
or the number of words betweenwi andwj). We will
attempt to make any assumptions about the formwkij
clear when necessary.
For the remainder of this section we discuss three
crucial problems for learning and inference while
showing that each can be computed tractably for the
non-projective case.
3.1 Finding the Argmax
The first problem of interest is finding the highest
weighted tree for a given input sentence x
T = argmax
T?T (Gx)
?
(i,j)k?ET
wkij
McDonald et al (2005b) showed that this can be
solved in O(n2) for unlabeled parsing using the
Chu-Liu-Edmonds algorithm for standard digraphs
(Chu and Liu, 1965; Edmonds, 1967). Unlike most
exact projective parsing algorithms, which use effi-
cient bottom-up chart parsing algorithms, the Chu-
Liu-Edmonds algorithm is greedy in nature. It be-
gins by selecting the single best incoming depen-
dency edge for each node j. It then post-processes
the resulting graph to eliminate cycles and then con-
tinues recursively until a spanning tree (or valid
dependency graph) results (see McDonald et al
(2005b) for details).
The algorithm is trivially extended to the multi-
digraph case for use in labeled dependency parsing.
First we note that if the maximum directed spanning
tree of a multi-digraph Gx contains any edge (i, j)k,
then we must have k = k? = argmaxk w
k
ij . Oth-
erwise we could simply substitute (i, j)k
?
in place
of (i, j)k and obtain a higher weighted tree. There-
fore, without effecting the solution to the argmax
problem, we can delete all edges in Gx that do not
satisfy this property. The resulting digraph is no
longer a multi-digraph and the Chu-Liu-Edmonds
algorithm can be applied directly. The new runtime
is O(|L|n2).
As a side note, the k-best argmax problem for di-
graphs can be solved in O(kn2) (Camerini et al,
1980). This can also be easily extended to the multi-
digraph case for labeled parsing.
3.2 Partition Function
A common step in many learning algorithms is to
compute the sum over the weight of all the possi-
ble outputs for a given input x. This value is often
referred to as the partition function due to its sim-
ilarity with a value by the same name in statistical
mechanics. We denote this value as Zx,
Zx =
?
T?T (Gx)
w(T ) =
?
T?T (Gx)
?
(i,j)k?ET
wki,j
To compute this sum it is possible to use the Matrix
Tree Theorem for multi-digraphs,
Matrix Tree Theorem (Tutte, 1984): Let G be a
multi-digraph with nodes V = {0, 1, . . . , n} and
edges E. Define (Laplacian) matrix Q as a (n +
1)?(n+1) matrix indexed from 0 to n. For all i and
j, define:
Qjj =
?
i6=j,(i,j)k?E
wkij & Qij =
?
i6=j,(i,j)k?E
?wkij
If the ith row and column are removed from Q to
produce the matrixQi, then the sum of the weights of
all directed spanning trees rooted at node i is equal
to |Qi| (the determinant of Qi).
Thus, if we construct Q for a graph Gx, then the de-
terminant of the matrix Q0 is equivalent to Zx. The
determinant of an n?n matrix can be calculated in
numerous ways, most of which takeO(n3) (Cormen
et al, 1990). The most efficient algorithms for cal-
culating the determinant of a matrix use the fact that
the problem is no harder than matrix multiplication
(Cormen et al, 1990). Matrix multiplication cur-
rently has known O(n2.38) implementations and it
has been widely conjectured that it can be solved in
O(n2) (Robinson, 2005). However, most algorithms
with sub-O(n3) running times require constants that
are large enough to negate any asymptotic advantage
for the case of dependency parsing. As a result, in
this work we use O(n3) as the runtime for comput-
ing Zx.
Since it takes O(|L|n2) to construct the matrix Q,
the entire runtime to compute Zx is O(n3 + |L|n2).
3.3 Edge Expectations
Another important problem for various learning
paradigms is to calculate the expected value of each
edge for an input sentence x,
?(i, j)k?x =
?
T?T (Gx)
w(T )? I((i, j)k, T )
124
Input: x = x0x1 ? ? ?xn
1. Construct Q O(|L|n2)
2. for j : 1 .. n O(n)
3. Q?jj = Qjj and Q
?
ij = Qij , 0 ? ?i ? n O(n)
4. Qjj = 1 and Qij = 0, 0 ? ?i ? n O(n)
5. for i : 0 .. n & i 6= j O(n)
6. Qij = ?1 O(1)
7. Zx = |Q0| O(n3)
8. ?(i, j)k?x = wkijZx, ?1 ? k ? |L| O(|L|)
9. end for
10. Qjj = Q?jj and Qij = Q
?
ij , 0 ? ?i ? n O(n)
11. end for
Figure 3: Algorithm to calculate ?(i, j)k?x in
O(n5 + |L|n2).
where I((i, j)k, T ) is an indicator function that is
one when the edge (i, j)k is in the tree T .
To calculate the expectation for the edge (i, j)k,
we can simply eliminate all edges (i?, j)k
?
6= (i, j)k
from Gx and calculate Zx. Zx will now be equal
to the sum of the weights of all trees that con-
tain (i, j)k. A naive implementation to compute
the expectation of all |L|n2 edges takes O(|L|n5 +
|L|2n4), since calculating Zx takes O(n3 + |L|n2)
for a single edge. However, we can reduce this con-
siderably by constructing Q a single time and only
making modifications to it when necessary. An al-
gorithm is given in Figure 3.3 that has a runtime of
O(n5 + |L|n2). This algorithm works by first con-
structing Q. It then considers edges from the node i
to the node j. Now, assume that there is only a single
edge from i to j and that that edge has a weight of 1.
Furthermore assume that this edge is the only edge
directed into the node j. In this case Q should be
modified so that Qjj = 1, Qij = ?1, and Qi?j = 0,
?i? 6= i, j (by the Matrix Tree Theorem). The value
of Zx under this new Q will be equivalent to the
weight of all trees containing the single edge from i
to j with a weight of 1. For a specific edge (i, j)k its
expectation is simplywkijZx, since we can factor out
the weight 1 edge from i to j in all the trees that con-
tribute to Zx and multiply through the actual weight
for the edge. The algorithm then reconstructs Q and
continues.
Following the work of Koo et al (2007) and Smith
and Smith (2007), it is possible to compute all ex-
pectations in O(n3 + |L|n2) through matrix inver-
sion. To make this paper self contained, we report
here their algorithm adapted to our notation. First,
consider the equivalence,
? logZx
?wkij
=
? logZx
?Zx
?Zx
?wkij
=
1
Zx
?
T?T (Gx)
w(T )
wkij
? I((i, j)k, T )
As a result, we can re-write the edge expectations as,
?(i, j)k? = Zxw
k
ij
? logZx
?wkij
= Zxw
k
ij
? log |Q0|
?wkij
Using the chain rule, we get,
? log |Q0|
?wkij
=
?
i?,j??1
? log |Q0|
?(Q0)i?j?
?(Q0)i?j?
?wkij
We assume the rows and columns of Q0 are in-
dexed from 1 so that the indexes of Q and Q0 co-
incide. To calculate ?(i, j)k? when i, j > 0, we can
use the fact that ? log |X|/Xij = (X?1)ji and that
?(Q0)i?j?/?wkij is non zero only when i
? = i and
j? = j or i? = j? = j to get,
?(i, j)k? = Zxw
k
ij [((Q
0)?1)jj ? ((Q
0)?1)ji]
When i = 0 and j > 0 the only non zero term of
this sum is when i? = j? = j and so
?(0, j)k? = Zxw
k
0j((Q
0)?1)jj
Zx and (Q0)?1 can both be calculated a single time,
each taking O(n3). Using these values, each expec-
tation is computed in O(1). Coupled with with the
fact that we need to construct Q and compute the
expectation for all |L|n2 possible edges, in total it
takes O(n3 + |L|n2) time to compute all edge ex-
pectations.
3.4 Comparison with Projective Parsing
Projective dependency parsing algorithms are well
understood due to their close connection to phrase-
based chart parsing algorithms. The work of Eis-
ner (1996) showed that the argmax problem for di-
graphs could be solved in O(n3) using a bottom-
up dynamic programming algorithm similar to CKY.
Paskin (2001) presented an O(n3) inside-outside al-
gorithm for projective dependency parsing using the
Eisner algorithm as its backbone. Using this al-
gorithm it is trivial to calculate both Zx and each
125
Projective Non-Projective
argmax O(n3 + |L|n2) O(|L|n2)
Zx O(n3 + |L|n2) O(n3 + |L|n2)
?(i, j)k?x O(n3 + |L|n2) O(n3 + |L|n2)
Table 1: Comparison of runtime for non-projective
and projective algorithms.
edge expectation. Crucially, the nested property of
projective structures allows edge expectations to be
computed in O(n3) from the inside-outside values.
It is straight-forward to extend the algorithms of Eis-
ner (1996) and Paskin (2001) to the labeled case
adding only a factor of O(|L|n2).
Table 1 gives an overview of the computational
complexity for the three problems considered here
for both the projective and non-projective case. We
see that the non-projective case compares favorably
for all three problems.
4 Applications
To motivate the algorithms from Section 3, we
present some important situations where each cal-
culation is required.
4.1 Inference Based Learning
Many learning paradigms can be defined as
inference-based learning. These include the per-
ceptron (Collins, 2002) and its large-margin vari-
ants (Crammer and Singer, 2003; McDonald et al,
2005a). In these settings, a models parameters are
iteratively updated based on the argmax calculation
for a single or set of training instances under the
current parameter settings. The work of McDon-
ald et al (2005b) showed that it is possible to learn
a highly accurate non-projective dependency parser
for multiple languages using the Chu-Liu-Edmonds
algorithm for unlabeled parsing.
4.2 Non-Projective Min-Risk Decoding
In min-risk decoding the goal is to find the depen-
dency graph for an input sentence x, that on average
has the lowest expected risk,
T = argmin
T?T (Gx)
?
T ??T (Gx)
w(T ?)R(T, T ?)
where R is a risk function measuring the error be-
tween two graphs. Min-risk decoding has been
studied for both phrase-structure parsing and depen-
dency parsing (Titov and Henderson, 2006). In that
work, as is common with many min-risk decoding
schemes, T (Gx) is not the entire space of parse
structures. Instead, this set is usually restricted to
a small number of possible trees that have been pre-
selected by some baseline system. In this subsection
we show that when the risk function is of a specific
form, this restriction can be dropped. The result is
an exact min-risk decoding procedure.
Let R(T, T ?) be the Hamming distance between
two dependency graphs for an input sentence x =
x0x1 ? ? ?xn,
R(T, T ?) = n ?
?
(i,j)k?ET
I((i, j)k, T ?)
This is a common definition of risk between two
graphs as it corresponds directly to labeled depen-
dency parsing accuracy (McDonald et al, 2005a;
Buchholz et al, 2006). Some algebra reveals,
T = argmin
T?T (Gx)
X
T ??T (Gx)
w(T ?)R(T, T ?)
= argmin
T?T (Gx)
X
T ??T (Gx)
w(T ?)[n ?
X
(i,j)k?ET
I((i, j)k, T ?)]
= argmin
T?T (Gx)
?
X
T ??T (Gx)
w(T ?)
X
(i,j)k?ET
I((i, j)k, T ?)
= argmin
T?T (Gx)
?
X
(i,j)k?ET
X
T ??T (Gx)
w(T ?)I((i, j)k, T ?)
= argmax
T?T (Gx)
X
(i,j)k?ET
X
T ??T (Gx)
w(T ?)I((i, j)k, T ?)
= argmax
T?T (Gx)
Y
(i,j)k?ET
e
P
T ??T (Gx)
w(T ?)I((i,j)k,T ?)
= argmax
T?T (Gx)
Y
(i,j)k?ET
e?(i,j)
k?x
By setting the edge weights to wkij = e
?(i,j)k?x we
can directly solve this problem using the edge ex-
pectation algorithm described in Section 3.3 and the
argmax algorithm described in Section 3.1.
4.3 Non-Projective Log-Linear Models
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are global discriminative learning algorithms
for problems with structured output spaces, such as
dependency parsing. For dependency parsing, CRFs
would define the conditional probability of a depen-
dency graph T for a sentence x as a globally nor-
126
malized log-linear model,
p(T |x) =
?
(i,j)k?ET
ew?f(i,j,k)
?
T ??T (Gx)
?
(i,j)k?ET ?
ew?f(i,j,k)
=
?
(i,j)k?ET
wkij
?
T ??T (Gx)
?
(i,j)k?ET ?
wkij
=
w(T )
Zx
Here, the weights wkij are potential functions over
each edge defined as an exponentiated linear classi-
fier with weight vector w ? RN and feature vector
f(i, j, k) ? RN , where fu(i, j, k) ? R represents a
single dimension of the vector f. The denominator,
which is exactly the sum over all graph weights, is a
normalization constant forcing the conditional prob-
ability distribution to sum to one.
CRFs set the parameters w to maximize the log-
likelihood of the conditional probability over a train-
ing set of examples T = {(x?, T?)}
|T |
?=1,
w = argmax
w
?
?
log p(T?|x?)
This optimization can be solved through a vari-
ety of iterative gradient based techniques. Many
of these require the calculation of feature expecta-
tions over the training set under model parameters
for the previous iteration. First, we note that the
feature functions factor over edges, i.e., fu(T ) =?
(i,j)k?ET
fu(i, j, k). Because of this, we can use
edge expectations to compute the expectation of ev-
ery feature fu. Let ?fu?x? represent the expectation
of feature fu for the training instance x?,
?fu?x? =
X
T?T (Gx? )
p(T |x?)fu(T )
=
X
T?T (Gx? )
p(T |x?)
X
(i,j)k?ET
fu(i, j, k)
=
X
T?T (Gx? )
w(T )
Zx
X
(i,j)k?ET
fu(i, j, k)
=
1
Zx
X
(i,j)k?Ex?
X
T?T (Gx)
w(T )I((i, j)k, T )fu(i, j, k)
=
1
Zx
X
(i,j)k?Ex?
?(i, j)k?x?fu(i, j, k)
Thus, we can calculate the feature expectation per
training instance using the algorithms for comput-
ing Zx and edge expectations. Using this, we can
calculate feature expectations over the entire train-
ing set,
?fu?T =
?
?
p(x?)?fu?x?
where p(x?) is typically set to 1/|T |.
4.4 Non-projective Generative Parsing Models
A generative probabilistic dependency model over
some alphabet ? consists of parameters pkx,y asso-
ciated with each dependency from word x ? ? to
word y ? ? with label lk ? L. In addition, we im-
pose 0 ? pkx,y ? 1 and the normalization conditions?
y,k p
k
x,y = 1 for each x ? ?. We define a gen-
erative probability model p over trees T ? T (Gx)
and a sentence x = x0x1 ? ? ?xn conditioned on the
sentence length, which is always known,
p(x, T |n) = p(x|T, n)p(T |n)
=
?
(i,j)k?ET
pkxi,xj p(T |n)
We assume that p(T |n) = ? is uniform. This model
is studied specifically by Paskin (2001). In this
model, one can view the sentence as being generated
recursively in a top-down process. First, a tree is
generated from the distribution p(T |n). Then start-
ing at the root of the tree, every word generates all of
its modifiers independently in a recursive breadth-
first manner. Thus, pkx,y represents the probability
of the word x generating its modifier y with label
lk. This distribution is usually smoothed and is of-
ten conditioned on more information including the
orientation of x relative to y (i.e., to the left/right)
and distance between the two words. In the super-
vised setting this model can be trained with maxi-
mum likelihood estimation, which amounts to sim-
ple counts over the data. Learning in the unsuper-
vised setting requires EM and is discussed in Sec-
tion 4.4.2.
Another generative dependency model of interest
is that given by Klein and Manning (2004). In this
model the sentence and tree are generated jointly,
which allows one to drop the assumption that p(T |n)
is uniform. This requires the addition to the model
of parameters px,STOP for each x ? ?, with the nor-
malization condition px,STOP +
?
y,k p
k
x,y = 1. It is
possible to extend the model of Klein and Manning
127
(2004) to the non-projective case. However, the re-
sulting distribution will be over multisets of words
from the alphabet instead of strings. The discus-
sion in this section is stated for the model in Paskin
(2001); a similar treatment can be developed for the
model in Klein and Manning (2004).
4.4.1 Language Modeling
A generative model of dependency structure
might be used to determine the probability of a sen-
tence x by marginalizing out all possible depen-
dency trees,
p(x|n) =
?
T?T (Gx)
p(x, T |n)
=
?
T?T (Gx)
p(x|T, n)p(T |n)
= ?
?
T?T (Gx)
?
(i,j)k?ET
pkxi,xj = ?Zx
This probability can be used directly as a non-
projective syntactic language model (Chelba et al,
1997) or possibly interpolated with a separate n-
gram model.
4.4.2 Unsupervised Learning
In unsupervised learning we train our model on
a sample of unannotated sentences X = {x?}
|X |
?=1.
Let |x?| = n? and p(T |n?) = ??. We choose the
parameters that maximize the log-likelihood
|X |?
?=1
log(p(x?|n?)) =
=
|X |?
?=1
log(
?
T?T (Gx? )
p(x?|T, n?)) +
|X |?
?=1
log(??),
viewed as a function of the parameters and subject
to the normalization conditions, i.e.,
?
y,k p
k
x,y = 1
and pkx,y ? 0.
Let x?i be the ith word of x?. By solving the
above constrained optimization problem with the
usual Lagrange multipliers method one gets
pkx,y =
=
?|X |
?=1
1
Zx?
?
i : x?i = x,
j : x?j = y
?(i, j)k?x?
?|X |
?=1
1
Zx?
?
y?,k?
?
i : x?i = x,
j? : x?j? = y
?
?(i, j?)k??x?
,
where for each x? the expectation ?(i, j)k?x? is de-
fined as in Section 3, but with the weight w(T ) re-
placed by the probability distribution p(x?|T, n?).
The above |L| ? |?|2 relations represent a non-
linear system of equations. There is no closed form
solution in the general case, and one adopts the ex-
pectation maximization (EM) method, which is a
specialization of the standard fixed-point iteration
method for the solution of non-linear systems. We
start with some initial assignment of the parameters
and at each iteration we use the induced distribu-
tion p(x?|T, n?) to compute a refined value for the
parameters themselves. We are always guaranteed
that the Kullback-Liebler divergence between two
approximated distributions computed at successive
iterations does not increase, which implies the con-
vergence of the method to some local maxima (with
the exception of saddle points).
Observe that at each iteration we can compute
quantities ?(i, j)k?x? and Zx? in polynomial time
using the algorithms from Section 3 with pkx?i,x?j
in place of wki,j . Furthermore, under some standard
conditions the fixed-point iteration method guaran-
tees a constant number of bits of precision gain for
the parameters at each iteration, resulting in overall
polynomial time computation in the size of the input
and in the required number of bits for the precision.
As far as we know, this is the first EM learning algo-
rithm for the model in Paskin (2001) working in the
non-projective case. The projective case has been
investigated in Paskin (2001).
5 Beyond Edge-factored Models
We have shown that several computational problems
related to parsing can be solved in polynomial time
for the class of non-projective dependency models
with the assumption that dependency relations are
mutually independent. These independence assump-
tions are unwarranted, as it has already been estab-
lished that modeling non-local information such as
arity and nearby parsing decisions improves the ac-
curacy of dependency models (Klein and Manning,
2002; McDonald and Pereira, 2006).
In the spirit of our effort to understand the nature
of exact non-projective algorithms, we examine de-
pendency models that introduce arity constraints as
well as permit edge decisions to be dependent on a
128
limited neighbourhood of other edges in the graph.
Both kinds of models can no longer be considered
edge-factored, since the likelihood of a dependency
occurring in a particular analysis is now dependent
on properties beyond the edge itself.
5.1 Arity
One feature of the edge-factored models is that no
restriction is imposed on the arity of the nodes in the
dependency trees. As a consequence, these models
can generate dependency trees of unbounded arity.
We show below that this is a crucial feature in the
development of the complexity results we have ob-
tained in the previous sections.
Let us assume a graph G(?)x = (Vx, Ex) defined
as before, but with the additional condition that each
node i ? Vx is associated with an integer value
?(i) ? 0. T (G(?)x ) is now defined as the set of all
directed spanning trees for G(?)x rooted in node 0,
such that every node i ? Vx has arity smaller than or
equal to ?(i). We now introduce a construction that
will be used to establish several hardness results for
the computational problems discussed in this paper.
Recall that a Hamiltonian path in a directed graph
G is a directed path that visits all of the nodes of G
exactly once.
Let G be some directed graph with set of nodes
V = {1, 2, . . . , n}. We construct a target graph
G(?)x = (Vx, Ex) with Vx = V ? {0} (0 the root
node) and |L| = 1. For each i, j ? Vx with i 6= j,
we add an edge (i, j)1 to Ex. We set w1i,j = 1 if
there is an edge from i to j in G, or else if i or j
is the root node 0, and w1i,j = 0 otherwise. Fur-
thermore, we set ?(i) = 1 for each i ? Vx. This
construction can be clearly carried out in log-space.
Note that each T ? T (G(?)x ) must be a monadic
tree with weight equal to either 0 or 1. It is not dif-
ficult to see that if w(T ) = 1, then when we remove
the root node 0 from T we obtain a Hamiltonian path
in G. Conversely, each Hamiltonian path in G can
be extended to a spanning tree T ? T (G(?)x ) with
w(T ) = 1, by adding the root node 0.
Using the above observations, it can be shown that
the solution of the argmax problem for G(?)x pro-
vides some Hamiltonian directed path in G. The lat-
ter search problem is FNP-hard, and is unlikely to
be solved in polynomial time. Furthermore, quan-
tity Zx provides the count of the Hamiltonian di-
rected paths in G, and for each i ? V , the expecta-
tion ?(0, i)1?x provides the count of the Hamiltonian
directed paths in G starting from node i. Both these
counting problems are #P-hard, and very unlikely to
have polynomial time solutions.
This result helps to relate the hardness of data-
driven models to the commonly known hardness
results in the grammar-driven literature given by
Neuhaus and Bo?ker (1997). In that work, an arity
constraint is included in their minimal grammar.
5.2 Vertical and Horizontal Markovization
In general, we would like to say that every depen-
dency decision is dependent on every other edge in
a graph. However, modeling dependency parsing in
such a manner would be a computational nightmare.
Instead, we would like to make a Markov assump-
tion over the edges of the tree, in a similar way that
a Markov assumption can be made for sequential
classification problems in order to ensure tractable
learning and inference.
Klein and Manning (2003) distinguish between
two kinds of Markovization for unlexicalized CFG
parsing. The first is vertical Markovization, which
makes the generation of a non-terminal dependent
on other non-terminals that have been generated at
different levels in the phrase-structure tree. The
second is horizontal Markovization, which makes
the generation of a non-terminal dependent on other
non-terminals that have been generated at the same
level in the tree.
For dependency parsing there are analogous no-
tions of vertical and horizontal Markovization for a
given edge (i, j)k. First, let us define the vertical and
horizontal neighbourhoods of (i, j)k. The vertical
neighbourhood includes all edges in any path from
the root to a leaf that passes through (i, j)k. The hor-
izontal neighbourhood contains all edges (i, j?)k
?
.
Figure 4 graphically displays the vertical and hor-
izontal neighbourhoods for an edge in the depen-
dency graph from Figure 1.
Vertical and horizontal Markovization essentially
allow the score of the graph to factor over a larger
scope of edges, provided those edges are in the same
vertical or horizontal neighbourhood. A dth order
factorization is one in which the score factors only
over the d nearest edges in the neighbourhoods. In
129
Figure 4: Vertical and Horizontal neighbourhood for
the edge from will to remain.
McDonald and Pereira (2006), it was shown that
non-projective dependency parsing with horizontal
Markovization is FNP-hard. In this study we com-
plete the picture and show that vertical Markoviza-
tion is also FNP-hard.
Consider a first-order vertical Markovization in
which the score for a dependency graph factors over
pairs of vertically adjacent edges2,
w(T ) =
?
(h,i)k,(i,j)k??ET
k
hiw
k?
ij
where khiw
k?
ij is the weight of including both edges
(h, i)k and (i, j)k
?
in the dependency graph. Note
that this formulation does not include any contribu-
tions from dependencies that have no vertically adja-
cent neighbours, i.e., any edge (0, i)k such that there
is no edge (i, j)k
?
in the graph. We can easily rec-
tify this by inserting a second root node, say 0?, and
including the weights k0?0w
k?
0i . To ensure that only
valid dependency graphs get a weight greater than
zero, we can set khiw
k?
ij = 0 if i = 0
? and k0?iw
k?
ij = 0
if i 6= 0.
Now, consider the NP-complete 3D-matching
problem (3DM). As input we are given three sets
of size m, call them A, B and C, and a set S ?
A?B ?C. The 3DM problem asks if there is a set
S? ? S such that |S?| = m and for any two tuples
(a, b, c), (a?, b?, c?) ? S? it is the case that a 6= a?,
b 6= b?, and c 6= c?.
2McDonald and Pereira (2006) define this as a second-order
Markov assumption. This is simply a difference in terminology
and does not represent any meaningful distinction.
We can reduce the 3D-matching problem to the
first-order vertical Markov parsing problem by con-
structing a graph G = (V,E), such that L =
A ? B ? C, V = {0?, 0} ? A ? B ? C and E =
{(i, j)k | i, j ? V, k ? L}. The set E contains mul-
tiple edges between ever pair of nodes, each edge
taking on a label representing a single element of
the set A ? B ? C. Now, define k0?0w
k?
0a = 1, for all
a ? A and k, k? ? A ? B ? C, and b0aw
c
ab = 1, for
all a ? A and b ? B and c ? C, and cabw
c
bc = 1, for
all (a, b, c) ? S. All other weights are set to zero.
We show below that there exists a bijection be-
tween the set of valid 3DMs for S and the set of non-
zero weighted dependency graphs in T (G). First, it
is easy to show that for any 3DM S?, there is a rep-
resentative dependency graph that has a weight of
1. This graph simply consists of the edges (0, a)b,
(a, b)c, and (b, c)c, for all (a, b, c) ? S?, plus an ar-
bitrarily labeled edge from 0? to 0.
To prove the reverse, consider a graph with weight
1. This graph must have a weight 1 edge into the
node a of the form (0, a)b since the graph must be
spanning. By the definition of the weight function,
in any non-zero weighted tree, a must have a sin-
gle outgoing edge, and that edge must be directed
into the node b. Let?s say that this edge is (a, b)c.
Then again by the weighting function, in any non-
zero weighted graph, b must have a single outgoing
edge that is directed into c, in particular the edge
(b, c)c. Thus, for any node a, there is a single path
directed out of it to a single leaf c ? C. We can
then state that the only non-zero weighted depen-
dency graph is one where each a ? A, b ? B and
c ? C occurs in exactly one ofm disjoint paths from
the root of the form 0 ? a ? b ? c. This is be-
cause the label of the single edge going into node a
will determine exactly the node b that the one outgo-
ing edge from a must go into. The label of that edge
determines exactly the single outgoing edge from b
into some node c. Now, since the weighting func-
tion ensures that the only non-zero weighted paths
into any leaf node c correspond directly to elements
of S, each of the m disjoint paths represent a single
tuple in a 3DM. Thus, if there is a non-zero weighted
graph in T (G), then it must directly correspond to a
valid 3DM, which concludes the proof.
Note that any dth order Markovization can be em-
bedded into a d + 1th Markovization. Thus, this re-
130
sult also holds for any arbitrary Markovization.
6 Discussion
In this paper we have shown that many important
learning and inference problems can be solved effi-
ciently for non-projective edge-factored dependency
models by appealing to the Matrix Tree Theorem
for multi-digraphs. These results extend the work
of McDonald et al (2005b) and help to further our
understanding of when exact non-projective algo-
rithms can be employed. When this analysis is cou-
pled with the projective parsing algorithms of Eisner
(1996) and Paskin (2001) we begin to get a clear pic-
ture of the complexity for data-driven dependency
parsing within an edge-factored framework. To fur-
ther justify the algorithms presented here, we out-
lined a few novel learning and inference settings in
which they are required.
However, for the non-projective case, moving
beyond edge-factored models will almost certainly
lead to intractable parsing problems. We have pro-
vided further evidence for this by proving the hard-
ness of incorporating arity constraints and hori-
zontal/vertical edge Markovization, both of which
incorporate information unavailable to an edge-
factored model. The hardness results provided
here are also of interest since both arity constraints
and Markovization can be incorporated efficiently
in the projective case through the straight-forward
augmentation of the underlying chart parsing algo-
rithms used in the projective edge-factored models.
This highlights a fundamental difference between
the nature of projective parsing algorithms and non-
projective parsing algorithms. On the projective
side, all algorithms use a bottom-up chart parsing
framework to search the space of nested construc-
tions. On the non-projective side, algorithms are
either greedy-recursive in nature (i.e., the Chu-Liu-
Edmonds algorithm) or based on the calculation of
the determinant of a matrix (i.e., the partition func-
tion and edge expectations).
Thus, the existence of bottom-up chart parsing
algorithms for projective dependency parsing pro-
vides many advantages. As mentioned above, it
permits simple augmentation techniques to incorpo-
rate non-local information such as arity constraints
and Markovization. It also ensures the compatibility
of projective parsing algorithms with many impor-
tant natural language processing methods that work
within a bottom-up chart parsing framework, includ-
ing information extraction (Miller et al, 2000) and
syntax-based machine translation (Wu, 1996).
The complexity results given here suggest that
polynomial chart-parsing algorithms do not exist
for the non-projective case. Otherwise we should
be able to augment them and move beyond edge-
factored models without encountering intractability
? just like the projective case. An interesting line
of research is to investigate classes of non-projective
structures that can be parsed with chart-parsing algo-
rithms and how these classes relate to the languages
parsable by other syntactic formalisms.
Acknowledgments
Thanks to Ben Taskar for pointing out the work of
Meila? and Jaakkola (2000). Thanks to David Smith,
Noah Smith and Michael Collins for making drafts
of their EMNLP papers available.
References
G. E. Barton, R. C. Berwick, and E. S. Ristad. 1987.
Computational Complexity and Natural Language.
MIT Press, Cambridge, MA.
C. Brew. 1992. Letting the cat out of the bag: Generation
for Shake-and-Bake MT. In Proc. COLING.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. In Proc. CoNLL.
P. M. Camerini, L. Fratta, and F. Maffioli. 1980. The k
best spanning arborescences of a network. Networks,
10(2):91?110.
C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudan-
pur, L. Mangu, H. Printz, E.S. Ristad, R. Rosenfeld,
A. Stolcke, and D. Wu. 1997. Structure and per-
formance of a dependency language model. In Eu-
rospeech.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. In-
troduction to Algorithms. MIT Press/McGraw-Hill.
131
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
K. Hall and V. No?va?k. 2005. Corrective modeling for
non-projective dependency parsing. In Proc. IWPT.
H. Hirakawa. 2006. Graph branch algorithm: An opti-
mum tree search method for scored dependency graph
with arc co-occurrence constraints. In Proc. ACL.
R. Hudson. 1984. Word Grammar. Blackwell.
S. Kahane, A. Nasr, and O Rambow. 1998. Pseudo-
projectivity: A polynomially parsable non-projective
dependency grammar. In Proc. ACL.
D. Klein and C.D. Manning. 2002. Fast exact natu-
ral language parsing with a factored model. In Proc.
NIPS.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proc. ACL.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. ACL.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proc. EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc
EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT/EMNLP.
M. Meila? and T. Jaakkola. 2000. Tractable Bayesian
learning of tree belief networks. In Proc. UAI.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proc NAACL, pages 226?233.
P. Neuhaus and N. Bo?ker. 1997. The complexity
of recognition of linguistically adequate dependency
grammars. In Proc. ACL.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. ACL.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
J. Nivre. 2005. Dependency grammar and dependency
parsing. Technical Report MSI report 05133, Va?xjo?
University: School of Mathematics and Systems Engi-
neering.
M.A. Paskin. 2001. Cubic-time parsing and learning al-
gorithms for grammatical bigram models. Technical
Report UCB/CSD-01-1148, Computer Science Divi-
sion, University of California Berkeley.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. EMNLP.
S. Robinson. 2005. Toward an optimal algorithm for
matrix multiplication. News Journal of the Society for
Industrial and Applied Mathematics, 38(9).
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The Mean-
ing of the Sentence in Its Pragmatic Aspects. Reidel.
D.A. Smith and N.A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In Proc. EMNLP.
L. Tesnie`re. 1959. E?le?ments de syntaxe structurale. Edi-
tions Klincksieck.
I. Titov and J. Henderson. 2006. Bayes risk minimiza-
tion in natural language parsing. University of Geneva
technical report.
W.T. Tutte. 1984. Graph Theory. Cambridge University
Press.
W. Wang and M. P. Harper. 2004. A statistical constraint
dependency grammar (CDG) parser. In Workshop on
Incremental Parsing: Bringing Engineering and Cog-
nition Together (ACL).
D. Wu. 1996. A polynomial-time algorithm for statisti-
cal machine translation. In Proc. ACL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
132
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 833?841,
Beijing, August 2010
Evaluation of Dependency Parsers on Unbounded Dependencies
Joakim Nivre Laura Rimell Ryan McDonald Carlos Go?mez-Rodr??guez
Uppsala University Univ. of Cambridge Google Inc. Universidade da Corun?a
joakim.nivre@lingfil.uu.se laura.rimell@cl.cam.ac.uk ryanmcd@google.com cgomezr@udc.es
Abstract
We evaluate two dependency parsers,
MSTParser and MaltParser, with respect
to their capacity to recover unbounded de-
pendencies in English, a type of evalu-
ation that has been applied to grammar-
based parsers and statistical phrase struc-
ture parsers but not to dependency parsers.
The evaluation shows that when combined
with simple post-processing heuristics,
the parsers correctly recall unbounded
dependencies roughly 50% of the time,
which is only slightly worse than two
grammar-based parsers specifically de-
signed to cope with such dependencies.
1 Introduction
Though syntactic parsers for English are re-
ported to have accuracies over 90% on the Wall
Street Journal (WSJ) section of the Penn Tree-
bank (PTB) (McDonald et al, 2005; Sagae and
Lavie, 2006; Huang, 2008; Carreras et al, 2008),
broad-coverage parsing is still far from being a
solved problem. In particular, metrics like attach-
ment score for dependency parsers (Buchholz and
Marsi, 2006) and Parseval for constituency parsers
(Black et al, 1991) suffer from being an aver-
age over a highly skewed distribution of differ-
ent grammatical constructions. As a result, in-
frequent yet semantically important construction
types could be parsed with accuracies far below
what one might expect.
This shortcoming of aggregate parsing met-
rics was highlighted in a recent study by Rimell
et al (2009), introducing a new parser evalua-
tion corpus containing around 700 sentences an-
notated with unbounded dependencies in seven
different grammatical constructions. This corpus
was used to evaluate five state-of-the-art parsers
for English, focusing on grammar-based and sta-
tistical phrase structure parsers. For example, in
the sentence By Monday, they hope to have a
sheaf of documents both sides can trust., parsers
should recognize that there is a dependency be-
tween trust and documents, an instance of object
extraction out of a (reduced) relative clause. In the
evaluation, the recall of state-of-the-art parsers on
this kind of dependency varies from a high of 65%
to a low of 1%. When averaging over the seven
constructions in the corpus, none of the parsers
had an accuracy higher than 61%.
In this paper, we extend the evaluation of
Rimell et al (2009) to two dependency parsers,
MSTParser (McDonald, 2006) and MaltParser
(Nivre et al, 2006a), trained on data from the
PTB, converted to Stanford typed dependencies
(de Marneffe et al, 2006), and combined with a
simple post-processor to extract unbounded de-
pendencies from the basic dependency tree. Ex-
tending the evaluation to dependency parsers is of
interest because it sheds light on whether highly
tuned grammars or computationally expensive
parsing formalisms are necessary for extracting
complex linguistic phenomena in practice. Unlike
the best performing grammar-based parsers stud-
ied in Rimell et al (2009), neither MSTParser nor
MaltParser was developed specifically as a parser
for English, and neither has any special mecha-
nism for dealing with unbounded dependencies.
Dependency parsers are also often asymptotically
faster than grammar-based or constituent parsers,
e.g., MaltParser parses sentences in linear time.
Our evaluation ultimately shows that the re-
call of MSTParser and MaltParser on unbounded
dependencies is much lower than the average
(un)labeled attachment score for each system.
Nevertheless, the two dependency parsers are
found to perform only slightly worse than the best
grammar-based parsers evaluated in Rimell et al
833
Each must match Wisman 's "pie" with the fragment that he carries with him
nsubj dobj
prepaux pobjposs've
prep rcmoddobj
nsubjdet
pobj
poss
dobj
a: ObRC
Five things you can do for $ 15,000  or less
pobjnsubjaux
rcmod
num num
prep cc conj
dobj
b: ObRed
They will remain on a lower-priority list that includes 17 other countries
pobjnsubj
aux
rcmod
nsubj
num
prep amod
det
nsubj
c: SbRC
amod
dobj
What you see are self-help projects
nsubj
csubj cop
amod
dobj
dobj d: Free
What effect does a prism have on light
pobjnsubj
aux
det
det prep
dobj
dobj
e: ObQ
Now he felt ready for the many actions he saw spreading out before him
pobj rcmod
prtmod
detprepacompnsubj
nsubj
amod xcompnsubj
prep
pobj
g: SbEmadv
The men were at first puzz led, then angered by the aimless tacking
pobj
cop conj
advmod
detadvmod
prep
det
prep
amod
pobjnsubjpass
f : RNR
Figure 1: Examples of seven unbounded dependency constructions (a?g). Arcs drawn below each sentence represent the
dependencies scored in the evaluation, while the tree above each sentence is the Stanford basic dependency representation,
with solid arcs indicating crucial dependencies (cf. Section 4). All examples are from the development sets.
(2009) and considerably better than the other sta-
tistical parsers in that evaluation. Interestingly,
though the two systems have similar accuracies
overall, there is a clear distinction between the
kinds of errors each system makes, which we ar-
gue is consistent with observations by McDonald
and Nivre (2007).
2 Unbounded Dependency Evaluation
An unbounded dependency involves a word or
phrase interpreted at a distance from its surface
position, where an unlimited number of clause
boundaries may in principle intervene. The
unbounded dependency corpus of Rimell et al
(2009) includes seven grammatical constructions:
object extraction from a relative clause (ObRC),
object extraction from a reduced relative clause
(ObRed), subject extraction from a relative clause
(SbRC), free relatives (Free), object questions
(ObQ), right node raising (RNR), and subject ex-
traction from an embedded clause (SbEm), all
chosen for being relatively frequent and easy to
identify in PTB trees. Examples of the con-
structions can be seen in Figure 1. The evalu-
ation set contains 80 sentences per construction
(which may translate into more than 80 depen-
dencies, since sentences containing coordinations
may have more than one gold-standard depen-
dency), while the development set contains be-
tween 13 and 37 sentences per construction. The
data for ObQ sentences was obtained from various
years of TREC, and for the rest of the construc-
tions from the WSJ (0-1 and 22-24) and Brown
sections of the PTB.
Each sentence is annotated with one or more
gold-standard dependency relations representing
the relevant unbounded dependency. The gold-
standard dependencies are shown as arcs below
the sentences in Figure 1. The format of the de-
pendencies in the corpus is loosely based on the
Stanford typed dependency scheme, although the
evaluation procedure permits alternative represen-
tations and does not require that the parser out-
put match the gold-standard exactly, as long as the
?spirit? of the construction is correct.
The ability to recover unbounded dependencies
is important because they frequently form part of
the basic predicate-argument structure of a sen-
tence. Subject and object dependencies in par-
ticular are crucial for a number of tasks, includ-
ing information extraction and question answer-
ing. Moreover, Rimell et al (2009) show that,
although individual types of unbounded depen-
dencies may be rare, the unbounded dependency
types in the corpus, considered as a class, occur in
as many as 10% of sentences in the PTB.
In Rimell et al (2009), five state-of-the-art
parsers were evaluated for their recall on the gold-
standard dependencies. Three of the parsers were
based on grammars automatically extracted from
the PTB: the C&C CCG parser (Clark and Curran,
2007), the Enju HPSG parser (Miyao and Tsujii,
2005), and the Stanford parser (Klein and Man-
ning, 2003). The two remaining systems were the
834
RASP parser (Briscoe et al, 2006), using a man-
ually constructed grammar and a statistical parse
selection component, and the DCU post-processor
of PTB parsers (Cahill et al, 2004) using the out-
put of the Charniak and Johnson reranking parser
(Charniak and Johnson, 2005). Because of the
wide variation in parser output representations, a
mostly manual evaluation was performed to en-
sure that each parser got credit for the construc-
tions it recovered correctly. The parsers were run
essentially ?out of the box?, meaning that the de-
velopment set was used to confirm input and out-
put formats, but no real tuning was performed. In
addition, since a separate question model is avail-
able for C&C, this was also evaluated on ObQ
sentences. The best overall performers were C&C
and Enju, which is unsurprising since they are
deep parsers based on grammar formalisms de-
signed to recover just such dependencies. The
DCU post-processor performed somewhat worse
than expected, often identifying the existence of
an unbounded dependency but failing to iden-
tify the grammatical class (subject, object, etc.).
RASP and Stanford, although not designed to re-
cover such dependencies, nevertheless recovered
a subset of them. Performance of the parsers also
varied widely across the different constructions.
3 Dependency Parsers
In this paper we repeat the study of Rimell et al
(2009) for two dependency parsers, with the goal
of evaluating how parsers based on dependency
grammars perform on unbounded dependencies.
MSTParser1 is a freely available implementa-
tion of the parsing models described in McDon-
ald (2006). According to the categorization of
parsers in Ku?bler et al (2008) it is a graph-based
parsing system in that core parsing algorithms can
be equated to finding directed maximum span-
ning trees (either projective or non-projective)
from a dense graph representation of the sentence.
Graph-based parsers typically rely on global train-
ing and inference algorithms, where the goal is to
learn models in which the weight/probability of
correct trees is higher than that of incorrect trees.
At inference time a global search is run to find the
1http://mstparser.sourceforge.net
highest weighted dependency tree. Unfortunately,
global inference and learning for graph-based de-
pendency parsing is typically NP-hard (McDonald
and Satta, 2007). As a result, graph-based parsers
(including MSTParser) often limit the scope of
their features to a small number of adjacent arcs
(usually two) and/or resort to approximate infer-
ence (McDonald and Pereira, 2006).
MaltParser2 is a freely available implementa-
tion of the parsing models described in Nivre et
al. (2006a) and Nivre et al (2006b). MaltParser is
categorized as a transition-based parsing system,
characterized by parsing algorithms that produce
dependency trees by transitioning through abstract
state machines (Ku?bler et al, 2008). Transition-
based parsers learn models that predict the next
state given the current state of the system as well
as features over the history of parsing decisions
and the input sentence. At inference time, the
parser starts in an initial state, then greedily moves
to subsequent states ? based on the predictions of
the model ? until a termination state is reached.
Transition-based parsing is highly efficient, with
run-times often linear in sentence length. Further-
more, transition-based parsers can easily incorpo-
rate arbitrary non-local features, since the current
parse structure is fixed by the state. However, the
greedy nature of these systems can lead to error
propagation if early predictions place the parser
in incorrect states.
McDonald and Nivre (2007) compared the ac-
curacy of MSTParser and MaltParser along a
number of structural and linguistic dimensions.
They observed that, though the two parsers ex-
hibit indistinguishable accuracies overall, MST-
Parser tends to outperform MaltParser on longer
dependencies as well as those dependencies closer
to the root of the tree (e.g., verb, conjunction and
preposition dependencies), whereas MaltParser
performs better on short dependencies and those
further from the root (e.g., pronouns and noun de-
pendencies). Since long dependencies and those
near to the root are typically the last constructed
in transition-based parsing systems, it was con-
cluded that MaltParser does suffer from some
form of error propagation. On the other hand, the
2http://www.maltparser.org
835
richer feature representations of MaltParser led to
improved performance in cases where error prop-
agation has not occurred. However, that study did
not investigate unbounded dependencies.
4 Methodology
In this section, we describe the methodological
setup for the evaluation, including parser training,
post-processing, and evaluation.3
4.1 Parser Training
One important difference between MSTParser and
MaltParser, on the one hand, and the best perform-
ing parsers evaluated in Rimell et al (2009), on
the other, is that the former were never developed
specifically as parsers for English. Instead, they
are best understood as data-driven parser gener-
ators, that is, tools for generating a parser given
a training set of sentences annotated with de-
pendency structures. Over the years, both sys-
tems have been applied to a wide range of lan-
guages (see, e.g., McDonald et al (2006), Mc-
Donald (2006), Nivre et al (2006b), Hall et al
(2007), Nivre et al (2007)), but they come with
no language-specific enhancements and are not
equipped specifically to deal with unbounded de-
pendencies.
Since the dependency representation used in
the evaluation corpus is based on the Stanford
typed dependency scheme (de Marneffe et al,
2006), we opted for using the WSJ section of
the PTB, converted to Stanford dependencies, as
our primary source of training data. Thus, both
parsers were trained on section 2?21 of the WSJ
data, which we converted to Stanford dependen-
cies using the Stanford parser (Klein and Man-
ning, 2003). The Stanford scheme comes in sev-
eral varieties, but because both parsers require the
dependency structure for each sentence to be a
tree, we had to use the so-called basic variety (de
Marneffe et al, 2006).
It is well known that questions are very rare
in the WSJ data, and Rimell et al (2009) found
that parsers trained only on WSJ data generally
performed badly on the questions included in the
3To ensure replicability, we provide all experimental
settings, post-processing scripts and additional information
about the evaluation at http://stp.ling.uu.se/?nivre/exp/.
evaluation corpus, while the C&C parser equipped
with a model trained on a combination of WSJ
and question data had much better performance.
To investigate whether the performance of MST-
Parser and MaltParser on questions could also be
improved by adding more questions to the train-
ing data, we trained one variant of each parser
using data that was extended with 3924 ques-
tions taken from QuestionBank (QB) (Judge et al,
2006).4 Since the QB sentences are annotated in
PTB style, it was possible to use the same conver-
sion procedure as for the WSJ data. However, it is
clear that the conversion did not always produce
adequate dependency structures for the questions,
an observation that we will return to in the error
analysis below.
In comparison to the five parsers evaluated in
Rimell et al (2009), it is worth noting that MST-
Parser and MaltParser were trained on the same
basic data as four of the five, but with a differ-
ent kind of syntactic representation ? dependency
trees instead of phrase structure trees or theory-
specific representations from CCG and HPSG. It
is especially interesting to compare MSTParser
and MaltParser to the Stanford parser, which es-
sentially produces the same kind of dependency
structures as output but uses the original phrase
structure trees from the PTB as input to training.
For our experiments we used MSTParser with
the same parsing algorithms and features as re-
ported in McDonald et al (2006). However, un-
like that work we used an atomic maximum en-
tropy model as the second stage arc predictor as
opposed to the more time consuming sequence la-
beler. McDonald et al (2006) showed that there is
negligible accuracy loss when using atomic rather
than structured labeling. For MaltParser we used
the projective Stack algorithm (Nivre, 2009) with
default settings and a slightly enriched feature
model. All parsing was projective because the
Stanford dependency trees are strictly projective.
4QB contains 4000 questions, but we removed all ques-
tions that also occurred in the test or development set of
Rimell et al (2009), who sampled their questions from the
same TREC QA test sets.
836
4.2 Post-Processing
All the development and test sets in the corpus
of Rimell et al (2009) were parsed using MST-
Parser and MaltParser after part-of-speech tagging
the input using SVMTool (Gime?nez and Ma`rquez,
2004) trained on section 2?21 of the WSJ data in
Stanford basic dependency format. The Stanford
parser has an internal module that converts the
basic dependency representation to the collapsed
representation, which explicitly represents addi-
tional dependencies, including unbounded depen-
dencies, that can be inferred from the basic rep-
resentation (de Marneffe et al, 2006). We per-
formed a similar conversion using our own tool.
Broadly speaking, there are three ways in which
unbounded dependencies can be inferred from the
Stanford basic dependency trees, which we will
refer to as simple, complex, and indirect. In the
simple case, the dependency coincides with a sin-
gle, direct dependency relation in the tree. This
is the case, for example, in Figure 1d?e, where
all that is required is that the parser identifies
the dependency relation from a governor to an
argument (dobj(see, What), dobj(have,
effect)), which we call the Arg relation; no
post-processing is needed.
In the complex case, the dependency is repre-
sented by a path of direct dependencies in the tree,
as exemplified in Figure 1a. In this case, it is
not enough that the parser correctly identifies the
Arg relation dobj(carries, that); it must
also find the dependency rcmod(fragment,
carries). We call this the Link relation, be-
cause it links the argument role inside the relative
clause to an element outside the clause. Other ex-
amples of the complex case are found in Figure 1c
and in Figure 1f.
In the indirect case, finally, the dependency
cannot be defined by a path of labeled depen-
dencies, whether simple or complex, but must
be inferred from a larger context of the tree us-
ing heuristics. Consider Figure 1b, where there
is a Link relation (rcmod(things, do)), but
no corresponding Arg relation inside the relative
clause (because there is no overt relative pro-
noun). However, given the other dependencies,
we can infer with high probability that the im-
plicit relation is dobj. Another example of the
indirect case is in Figure 1g. Our post-processing
tool performs more heuristic inference for the in-
direct case than the Stanford parser does (cf. Sec-
tion 4.3).
In order to handle the complex and indirect
cases, our post-processor is triggered by the oc-
currence of a Link relation (rcmod or conj) and
first tries to add dependencies that are directly im-
plied by a single Arg relation (relations involving
relative pronouns for rcmod, shared heads and
dependents for conj). If there is no overt rela-
tive pronoun, or the function of the relative pro-
noun is underspecified, the post-processor relies
on the obliqueness hierarchy subj < dobj <
pobj and simply picks the first ?missing func-
tion?, unless it finds a clausal complement (indi-
cated by the labels ccomp and xcomp), in which
case it descends to the lower clause and restarts
the search there.
4.3 Parser Evaluation
The evaluation was performed using the same cri-
teria as in Rimell et al (2009). A dependency
was considered correctly recovered if the gold-
standard head and dependent were correct and
the label was an ?acceptable match? to the gold-
standard label, indicating the grammatical func-
tion of the extracted element at least to the level
of subject, passive subject, object, or adjunct.
The evaluation in Rimell et al (2009) took
into account a wide variety of parser output for-
mats, some of which differed significantly from
the gold-standard. Since MSTParser and Malt-
Parser produced Stanford dependencies for this
experiment, evaluation required less manual ex-
amination than for some of the other parsers, as
was also the case for the output of the Stanford
parser in the original evaluation. However, a man-
ual evaluation was still performed in order to re-
solve questionable cases.
5 Results
The results are shown in Table 1, where the ac-
curacy for each construction is the percentage of
gold-standard dependencies recovered correctly.
The Avg column represents a macroaverage, i.e.
the average of the individual scores on the seven
constructions, while the WAvg column represents
837
Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvg
MST 34.1 47.3 78.9 65.5 13.8 45.4 37.6 46.1 63.4
Malt 40.7 50.5 84.2 70.2 16.2 39.7 23.5 46.4 66.9
MST-Q 41.2 50.0
Malt-Q 31.2 48.5
Table 1: Parser accuracy on the unbounded dependency corpus.
Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvg
C&C 59.3 62.6 80.0 72.6 81.2 49.4 22.4 61.1 69.9
Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.9 70.9
MST 34.1 47.3 78.9 65.5 41.2 45.4 37.6 50.0 63.4
Malt 40.7 50.5 84.2 70.2 31.2 39.7 23.5 48.5 66.9
DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 34.6 47.0
RASP 16.5 1.1 53.7 17.9 27.5 34.5 15.3 23.8 34.1
Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 37.0 50.3
Table 2: Parser accuracy on the unbounded dependency corpus. The ObQ score for C&C, MSTParser, and MaltParser is for
a model trained with additional questions (without this C&C scored 27.5; MSTParser and MaltParser as in Table 1).
a weighted macroaverage, where the construc-
tions are weighted proportionally to their relative
frequency in the PTB. WAvg excludes ObQ sen-
tences, since frequency statistics were not avail-
able for this construction in Rimell et al (2009).
Our first observation is that the accuracies for
both systems are considerably below the ?90%
unlabeled and ?88% labeled attachment scores
for English that have been reported previously
(McDonald and Pereira, 2006; Hall et al, 2006).
Comparing the two parsers, we see that Malt-
Parser is more accurate on dependencies in rela-
tive clause constructions (ObRC, ObRed, SbRC,
and Free), where argument relations tend to be
relatively local, while MSTParser is more accu-
rate on dependencies in RNR and SbEm, which
involve more distant relations. Without the ad-
ditional QB training data, the average scores for
the two parsers are indistinguishable, but MST-
Parser appears to have been better able to take
advantage of the question training, since MST-Q
performs better than Malt-Q on ObQ sentences.
On the weighted average MaltParser scores 3.5
points higher, because the constructions on which
it outperforms MSTParser are more frequent in
the PTB, and because WAvg excludes ObQ, where
MSTParser is more accurate.
Table 2 shows the results for MSTParser and
MaltParser in the context of the other parsers eval-
uated in Rimell et al (2009).5 For the parsers
5The average scores reported differ slightly from those in
which have a model trained on questions, namely
C&C, MSTParser, and MaltParser, the figure
shown for ObQ sentences is that of the question
model. It can be seen that MSTParser and Malt-
Parser perform below C&C and Enju, but above
the other parsers, and that MSTParser achieves the
highest score on SbEm sentences and MaltParser
on SbRC sentences. It should be noted, however,
that Table 2 does not represent a direct compar-
ison across all parsers, since most of the other
parsers would have benefited from heuristic post-
processing of the kind implemented here for MST-
Parser and MaltParser. This is especially true for
RASP, where the grammar explicitly leaves some
types of attachment decisions for post-processing.
For DCU, improved labeling heuristics would sig-
nificantly improve performance. It is instructive to
compare the dependency parsers to the Stanford
parser, which uses the same output representation
and has been used to prepare the training data for
our experiments. Stanford has very low recall on
ObRed and SbEm, the categories where heuristic
inference plays the largest role, but mirrors MST-
Parser for most other categories.
6 Error Analysis
We now proceed to a more detailed error analy-
sis, based on the development sets, and classify
Rimell et al (2009), where a microaverage (i.e., average over
all dependencies in the corpus, regardless of construction)
was reported.
838
the errors made by the parsers into three cate-
gories: A global error is one where the parser
completely fails to build the relevant clausal struc-
ture ? the relative clause in ObRC, ObRed, SbRC,
Free, SbEmb; the interrogative clause in ObQ; and
the clause headed by the higher conjunct in RNR
? often as a result of surrounding parsing errors.
When a global error occurs, it is usually mean-
ingless to further classify the error, which means
that this category excludes the other two. An Arg
error is one where the parser has constructed the
relevant clausal structure but fails to find the Arg
relation ? in the simple and complex cases ? or the
set of surrounding Arg relations needed to infer
an implicit Arg relation ? in the indirect case (cf.
Section 4.2). A Link error is one where the parser
fails to find the crucial Link relation ? rcmod
in ObRC, ObRed, SbRC, SbEmb; conj in RNR
(cf. Section 4.2). Link errors are not relevant for
Free and ObQ, where all the crucial relations are
clause-internal.
Table 3 shows the frequency of different error
types for MSTParser (first) and MaltParser (sec-
ond) in the seven development sets. First of all,
we can see that the overall error distribution is
very similar for the two parsers, which is proba-
bly due to the fact that they have been trained on
exactly the same data with exactly the same an-
notation (unlike the five parsers previously eval-
uated). However, there is a tendency for MST-
Parser to make fewer Link errors, especially in
the relative clause categories ObRC, ObRed and
SbRC, which is compatible with the observation
from the test results that MSTParser does better
on more global dependencies, while MaltParser
has an advantage on more local dependencies, al-
though this is not evident from the statistics from
the relatively small development set.
Comparing the different grammatical construc-
tions, we see that Link errors dominate for the rel-
ative clause categories ObRC, ObRed and SbRC,
where the parsers make very few errors with
respect to the internal structure of the relative
clauses (in fact, no errors at all for MaltParser
on SbRC). This is different for SbEm, where the
analysis of the argument structure is more com-
plex, both because there are (at least) two clauses
involved and because the unbounded dependency
Type Glo
bal
Arg Lin
k
A+
L
Err
ors
# D
eps
ObRC 0/1 1/1 7/11 5/3 13/16 20
ObRed 0/1 0/1 6/7 3/4 9/13 23
SbRC 2/1 1/0 7/13 0/0 10/14 43
Free 2/1 3/5 ? ? 5/6 22
ObQ 4/7 13/13 ? ? 17/20 25
RNR 6/4 4/6 0/0 4/5 14/15 28
SbEm 3/4 3/2 0/0 3/3 9/9 13
Table 3: Distribution of error types in the development
sets; frequencies for MSTParser listed first and MaltParser
second. The columns Arg and Link give frequencies for
Arg/Link errors occurring without the other error type, while
A+L give frequencies for joint Arg and Link errors.
can only be inferred indirectly from the basic de-
pendency representation (cf. Section 4.2). An-
other category where Arg errors are frequent is
RNR, where all such errors consist in attaching
the relevant dependent to the second conjunct in-
stead of to the first.6 Thus, in the example in Fig-
ure 1f, both parsers found the conj relation be-
tween puzzled and angered but attached by to the
second verb.
Global errors are most frequent for RNR, prob-
ably indicating that coordinate structures are diffi-
cult to parse in general, and for ObQ (especially
for MaltParser), probably indicating that ques-
tions are not well represented in the training set
even after the addition of QB data.7 As noted
in Section 4.1, this may be partly due to the fact
that conversion to Stanford dependencies did not
seem to work as well for QB as for the WSJ data.
Another problem is that the part-of-speech tagger
used was trained on WSJ data only and did not
perform as well on the ObQ data. Uses of What as
a determiner were consistently mistagged as pro-
nouns, which led to errors in parsing. Thus, for
the example in Figure 1e, both parsers produced
the correct analysis except that, because of the tag-
ging error, they treated What rather than effect as
the head of the wh-phrase, which counts as an er-
ror in the evaluation.
In order to get a closer look specifically at the
Arg errors, Table 4 gives the confusion matrix
6In the Stanford scheme, an argument or adjunct must be
attached to the first conjunct in a coordination to indicate that
it belongs to both conjuncts.
7Parsers trained without QB had twice as many global
errors.
839
Sb Ob POb EmSb EmOb Other Total
Sb ? 0/0 0/0 0/0 0/0 2/1 2/1
Ob 2/3 ? 0/0 0/1 0/0 4/2 6/6
POb 2/0 7/5 ? 0/0 0/0 5/8 14/13
EmSb 1/1 4/2 0/0 ? 0/0 1/2 6/5
EmOb 0/0 3/1 0/0 0/0 ? 1/6 4/7
Total 5/4 14/8 0/0 0/1 0/0 13/19 32/32
Table 4: Confusion matrix for Arg errors (excluding RNR
and using parsers trained on QB for ObQ); frequencies for
MSTParser listed first and MaltParser second. The column
Other covers errors where the function is left unspecified or
the argument is attached to the wrong head.
for such errors, showing which grammatical func-
tions are mistaken for each other, with an extra
category Other for cases where the function is left
unspecified by the parser or the error is an attach-
ment error rather than a labeling error (and ex-
cluding the RNR category because of the special
nature of the Arg errors in this category). The
results again confirm that the two parsers make
very few errors on subjects and objects clause-
internally. The few cases where an object is
mistaken as a subject occur in ObQ, where both
parsers perform rather poorly in general. By con-
trast, there are many more errors on prepositional
objects and on embedded subjects and objects. We
believe an important part of the explanation for
this pattern is to be found in the Stanford depen-
dency representation, where subjects and objects
are marked as such but all other functions real-
ized by wh elements are left unspecified (using the
generic rel dependency), which means that the re-
covery of these functions currently has to rely on
heuristic rules as described in Section 4.2. Finally,
we think it is possible to observe the tendency for
MaltParser to be more accurate at local labeling
decisions ? reflected in fewer cross-label confu-
sions ? and for MSTParser to perform better on
more distant attachment decisions ? reflected in
fewer errors in the Other category (and in fewer
Link errors).
7 Conclusion
In conclusion, the capacity of MSTParser and
MaltParser to recover unbounded dependencies is
very similar on the macro and weighted macro
level, but there is a clear distinction in their
strengths ? constructions involving more distant
dependencies such as ObQ, RNR and SbEm for
MSTParser and constructions with more locally
defined configurations such as ObRC, ObRed,
SbRC and Free for MaltParser. This is a pattern
that has been observed in previous evaluations of
the parsers and can be explained by the global
learning and inference strategy of MSTParser and
the richer feature space of MaltParser (McDonald
and Nivre, 2007).
Perhaps more interestingly, the accuracies of
MSTParser and MaltParser are only slightly be-
low the best performing systems in Rimell et al
(2009) ? C&C and Enju. This is true even though
MSTParser and MaltParser have not been engi-
neered specifically for English and lack special
mechanisms for handling unbounded dependen-
cies, beyond the simple post-processing heuristic
used to extract them from the output trees. Thus,
it is reasonable to speculate that the addition of
such mechanisms could lead to computationally
lightweight parsers with the ability to extract un-
bounded dependencies with high accuracy.
Acknowledgments
We thank Marie-Catherine de Marneffe for great
help with the Stanford parser and dependency
scheme, Llu??s Ma`rquez and Jesu?s Gime?nez for
great support with SVMTool, Josef van Gen-
abith for sharing the QuestionBank data, and
Stephen Clark and Mark Steedman for helpful
comments on the evaluation process and the pa-
per. Laura Rimell was supported by EPSRC grant
EP/E035698/1 and Carlos Go?mez-Rodr??guez
by MEC/FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus,
Bolsas Estadas INCITE/FSE cofinanced).
References
Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quanti-
tatively comparing the syntactic coverage of English
grammars. In Proceedings of 4th DARPAWorkshop,
306?311.
Briscoe, T., J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings
840
of the COLING/ACL 2006 Interactive Presentation
Sessions, 77?80.
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of CoNLL, 149?164.
Cahill, A., M. Burke, R. O?Donovan, J. Van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of ACL, 320?327.
Carreras, X., M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of
CoNLL, 9?16.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL, 173?180.
Clark, S. and J. R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of
LREC.
Gime?nez, J. and L. Ma`rquez. 2004. SVMTool: A gen-
eral POS tagger generator based on support vector
machines. In Proceedings of LREC.
Hall, J., J. Nivre, and J. Nilsson. 2006. Discriminative
classifiers for deterministic dependency parsing. In
Proceedings of the COLING/ACL 2006 Main Con-
ference Poster Sessions, 316?323.
Hall, J., J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task.
Huang, L. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, 586?594.
Judge, J., A. Cahill, and J. van Genabith. 2006. Ques-
tionBank: Creating a corpus of parse-annotated
questions. In Proceedings of COLING-ACL, 497?
504.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, 423?430.
Ku?bler, S., R. McDonald, and J. Nivre. 2008. Depen-
dency Parsing. Morgan and Claypool.
McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, 122?131.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of EACL, 81?88.
McDonald, R. and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing.
In Proceedings of IWPT, 122?131.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, 91?98.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL, 216?
220.
McDonald, R.. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Miyao, Y. and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proceedings of ACL, 83?90.
Nivre, J., J. Hall, and J. Nilsson. 2006a. MaltParser:
A data-driven parser-generator for dependency pars-
ing. In Proceedings of LREC, 2216?2219.
Nivre, J., J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006b. Labeled pseudo-projective dependency
parsing with support vector machines. In Proceed-
ings of CoNLL, 221?225.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13:95?135.
Nivre, J. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of ACL-
IJCNLP, 351?359.
Rimell, L., S. Clark, and M. Steedman. 2009. Un-
bounded dependency recovery for parser evaluation.
In Proceedings EMNLP, 813?821.
Sagae, K. and A. Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL HLT: Short
Papers, 129?132.
841
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1783?1792, Dublin, Ireland, August 23-29 2014.
Adapting taggers to Twitter with not-so-distant supervision
Barbara Plank
1
, Dirk Hovy
1
, Ryan McDonald
2
and Anders S?gaard
1
Center for Language Technology, University of Copenhagen
1
Google Inc.
2
{bplank,dirkh}@cst.dk,ryanmcd@google.com,soegaard@hum.ku.dk
Abstract
We experiment with using different sources of distant supervision to guide unsupervised and
semi-supervised adaptation of part-of-speech (POS) and named entity taggers (NER) to Twitter.
We show that a particularly good source of not-so-distant supervision is linked websites. Specif-
ically, with this source of supervision we are able to improve over the state-of-the-art for Twitter
POS tagging (89.76% accuracy, 8% error reduction) and NER (F1=79.4%, 10% error reduction).
1 Introduction
Twitter contains a vast amount of information, including first stories and breaking news (Petrovic et al.,
2010), fingerprints of public opinions (Jiang et al., 2011) and recommendations of relevance to poten-
tially very small target groups (Benson et al., 2011). In order to automatically extract this information,
we need to be able to analyze tweets, e.g., determine the part-of-speech (POS) of words and recognize
named entities. Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013;
Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions
for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, frag-
mented or mixed language, etc.
Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample
Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts
of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski
et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective
samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing
even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due
to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems
perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and
suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by learning
from additional unlabeled tweets. This is the hypothesis we explore in this paper.
We present a semi-supervised learning method that does not require additional labeled in-domain data
to correct sample bias, but rather leverages pools of unlabeled Twitter data. However, since taggers
trained on newswire perform poorly on Twitter data, we need additional guidance when utilizing the
unlabeled data. This paper proposes distant supervision to help our models learn from unlabeled data.
Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploited
to gather (possible noisy) training instances (Mintz et al., 2009). Our basic idea is to can use linguistic
analysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets. We
explore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also propose
to use the linked websites of tweets with URLs as supervision. The intuition is that we can use websites
to provide a richer linguistic context for our tagging decisions. We exploit the fact that tweets with URLs
provide a one-to-one map between an unlabeled instance and the source of supervision, making this
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1783
1: X = {?x
i
, y
i
?}
N
i=1
labeled tweets
2: U = {?x
i
, w
i
?}
M
i=1
unlabeled tweet-website pairs
3: I iterations
4: k = 1000 pool size
5: v=train(X) base model
6: for i ? I do
7: for ?x, w? ? pool
k
(U) do
8: y?=predict(?x, w?;v)
9: X ? X ? {?y?,x?}
10: end for
11: v=train(X)
12: end for
13: return v
Figure 1: Semi-supervised learning with not-so-distant supervision, i.e. tweet-website pairs {?x
i
, w
i
?}.
SELF-TRAINING, WEB, DICT, DICT?WEB and WEB?DICT differ only in how predict() (line 8) is
implemented (cf. Section 2).
less distant supervision. Note that we use linked websites only for semi-supervised learning, but do not
require them at test time.
Our semi-supervised learning method enables us to learn POS tagging and NER models that perform
more robustly across different samples of tweets than existing approaches. We consider both the scenario
where a small sample of labeled Twitter data is available, and the scenario where only newswire data is
available. Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unla-
beled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our
tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd
2 Tagging with not-so-distant supervision
We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), popula-
tion drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use
unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training
sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper
presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and
not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging deci-
sions in tweets on a richer linguistic context than what is available in the tweets. This semi-supervised
approach gives state-of-the-art performance across available Twitter POS and NER data sets.
The overall semi-supervised learning algorithm is presented in Figure 1. The aim is to correct model
bias by predicting tag sequences on small pools of unlabeled tweets, and re-training the model across
several iterations to gradually correct model bias. Since information from hyperlinks will be important,
the unlabeled data U is a corpus of tweets containing URLs. We present a baseline and four system
proposals that only differ in their treatment of the predict() function.
In the SELF-TRAINING baseline, predict() corresponds to standard Viterbi inference on the unlabeled
Twitter data. This means, the current model v is applied to the tweets by disregarding the websites in
the tweet-website pairs, i.e., tagging x without considering w. Then the automatically tagged tweets are
added to the current pool of labeled data and the procedure is iterated (line 7-11 in Figure 1).
In the WEB method, we additionally use the information from the websites. The current model v
is used to predict tags for the pooled tweets and the website they linked to. For all the words that
occur both in a tweet and on the corresponding website, we then project the tag most frequently
assigned to those words on the website to their occurrences in the tweet. This enables us to basically
condition the tag decision for each such word on its accumulated context on the website. The assumption
of course being that the word in the tweet has the part-of-speech it most often has on the website linked to.
1784
Example Here is an example of a tweet that contains a URL:
(1) #Localization #job: Supplier / Project Manager - Localisation Vendor - NY, NY, United States
http://bit.ly/16KigBg #nlppeople
The words in the tweet are all common words, but they occur without linguistic context that could
help a tagging model to infer whether these words are nouns, verbs, named entities, etc. However, on the
website that the tweet refers to, all of these words occur in context:
(2) The Supplier/Project Manager performs the selection and maintenance . . .
For illustration, the Urbana-Champaign POS tagger
1
incorrectly tags Supplier in (1) as an adjective.
In (2), however, it gets the same word right and tags it as a noun. The tagging of (2) could potentially
help us infer that Supplier is also a noun in (1).
Obviously, the superimposition of tags in the WEB method may change the tag of a tweet word such
that it results in an unlikely tag sequence, as we will discuss later. Therefore we also implemented
type-constrained decoding (T?ackstr?om et al., 2013), i.e., prune the lattice such that the tweet words ob-
served on the website have one of the tags they were labeled with on the website (soft constraints), or,
alternatively, were forced during decoding to have the most frequent tags they were labeled with (hard
constraint decoding), thereby focusing on licensed sequences. However, none of these approaches per-
formed significantly better than the simple WEB approach on held-out data. This suggests that sequential
dependencies are less important for tagging Twitter data, which is of rather fragmented nature. Also, the
WEB approach allows us to override transitional probabilities that are biased by the observations we
made about the distribution of tags in our out-of-domain data.
Furthermore, we combine the not-so-distant supervision from linked websites (WEB) with supervision
from dictionaries (DICT). The idea here is to exploit the fact that many word types in a dictionary are
actually unambiguous, i.e., contain only a single tag. In particular, 93% of the word types in Wiktionary
2
are unambiguous. Wiktionary is a crowdsourced tag dictionary that has previously been used for mini-
mally supervised POS tagging (Li et al., 2012; T?ackstr?om et al., 2013). In the case of NER, we use a
gazetteer that combines information on PER, LOC and ORG from the KnownLists of the Illinois tagger.
3
For this gazetteer, 79% of the word types contained only a single named entity tag.
We experiment with a model that uses the dictionary only (DICT) and two ways to combine the two
sources. In the former setup, the current model is first applied to tag the tweets, then any token that
appears in the dictionary and is unambiguous is projected back to the tweet. The next two methods are
combinations of WEB and DICT: either first project the predicted tags from the website and then, in case
of conflicts, overrule predictions by the dictionary (WEB?DICT), or the other way around (DICT?WEB).
The intuition behind the idea of using linked websites as not-so-distant supervision is that while tweets
are hard to analyze (even for humans) because of the limited context available in 140 character messages,
tweets relate to real-world events, and Twitter users often use hyperlinks to websites to indicate what
real-world events their comments address. In fact, we observed that about 20% of tweets contain URLs.
The websites they link to are often newswire sites that provide more context and are written in a more
canonical language, and are therefore easier to process. Our analysis of the websites can then potentially
inform our analysis of the tweets. The tweets with the improved analyses can then be used to bootstrap
our tagging models using a self-training mechanism. Note that our method does not require tweets to
contain URLs at test time, but rather uses unlabeled tweets with URLs during training to build better
tagging models for tweets in general. At test time, these models can be applied to any tweet.
1
http://cogcomp.cs.illinois.edu/demo/pos/
2
http://en.wiktionary.org/ - We used the Wiktionary version derived by Li et al. (2012).
3
http://cogcomp.cs.illinois.edu/page/software_view/NETagger
1785
3 Experiments
3.1 Model
In our experiments we use a publicly available implementation of conditional random fields (CRF) (Laf-
ferty et al., 2001).
4
We use the features proposed by Gimpel et al. (2011), in particular features for word
tokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase,
3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2
i
for
i ? 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is pub-
licly available.
5
We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000}
showing similar performance. The number of iterations i is set on the development data.
For NER on websites, we use the Stanford NER system (Finkel et al., 2005)
6
with POS tags from the
LAPOS tagger (Tsuruoka et al., 2011).
7
For POS we found it to be superior to use the current POS model
for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line
NER tagging rather than retagging the websites in every iteration.
3.2 Data
In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semi-
supervised domain adaptation (DA), respectively (Daum?e et al., 2010; Plank, 2011). In unsupervised
DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from both
domains, besides unlabeled target data, but the amount of labeled target data is much smaller than the
labeled source data. Most annotated corpora for English are newswire corpora. Some annotated Twitter
data sets have been made available recently, described next.
POS NER
train
WSJ (700k) REUTER-CONLL (Tjong Kim Sang and De Meulder, 2003) (200k)
GIMPEL-TRAIN (Owoputi et al., 2013) (14k) FININ-TRAIN (Finin et al., 2010) (170k)
dev
FOSTER-DEV (Foster et al., 2011) (3k) n/a
RITTER-DEV (Ritter et al., 2011) (2k) n/a
test
FOSTER-TEST (Foster et al., 2011) (2.8k) RITTER-TEST (Ritter et al., 2011) (46k)
GIMPEL-TEST (Gimpel et al., 2011) (7k) FININ-TEST (Finin et al., 2010) (51k)
HOVY-TEST (Hovy et al., 2014) FROMREIDE-TEST (Fromreide et al., 2014) (20k)
Table 1: Overview of data sets. Number in parenthesis: size in number of tokens.
Training data. An overview of the different data sets is given in Table 3.2. In our experiments, we
use the SANCL shared task
8
splits of the OntoNotes 4.0 distribution of the WSJ newswire annotations
as newswire training data for POS tagging.
9
For NER, we use the CoNLL 2003 data sets of annotated
newswire from the Reuters corpus.
10
The in-domain training POS data comes from Gimpel et al. (2011),
and the in-domain NER data comes from Finin et al. (2010) (FININ-TRAIN). These data sets are added
to the newswire sets when doing semi-supervised DA. Note that for NER, we thus do not rely on expert-
annotated Twitter data, but rely on crowdsourced annotations. We use MACE
11
(Hovy et al., 2013) to
resolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold). We
believe relying on crowdsourced annotations makes our set-up more robust across different samples of
Twitter data.
Development and test data. We use several evaluation sets for both tasks to prevent overfitting to a
specific sample. We use the (out-of-sample) development data sets from Ritter et al. (2011) and Foster
4
http://www.chokkan.org/software/crfsuite/
5
http://www.ark.cs.cmu.edu/TweetNLP/
6
http://http://nlp.stanford.edu/software/CRF-NER.shtml
7
http://www.logos.ic.i.u-tokyo.ac.jp/
?
tsuruoka/lapos/
8
https://sites.google.com/site/sancl2012/home/shared-task
9
LDC2011T03.
10
http://www.clips.ua.ac.be/conll2003/ner/
11
http://www.isi.edu/publications/licensed-sw/mace/
1786
et al. (2011). For NER, we simply use the parameters from our POS tagging experiments and thus do
not assume to have access to further development data. For both POS tagging and NER, we have three
test sets. For POS tagging, the ones used in Foster et al. (2011) (FOSTER-TEST) and Ritter et al. (2011)
(RITTER-TEST),
12
as well as the one presented in Hovy et al. (2014) (HOVY-TEST). For NER, we use
the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets.
One is a manual correction of a held-out portion of FININ-TRAIN, named FININ-TEST; the other one
is referred to as FROMREIDE-TEST. Since the different POS corpora use different tag sets, we map all
of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a
few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire
tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong.
Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a
few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The
major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We
follow Finin et al. (2010) in doing so.
Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a period
of one week in August 2013 by searching for tweets that contain the string http and downloading the
content of the websites they linked to. We filter out duplicate tweets and restrict ourselves to websites
that contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).
13
We also
require website and tweet to have at least one matching word that is not a stopword (as defined by the
NLTK stopword list).
14
Finally we restrict ourselves to pairs where the website is a subsite, because
website head pages tend to contain mixed content that is constantly updated. The resulting files are all
tokenized using the Twokenize tool.
15
Tweets were treated as one sentence, similar to the approaches in
Gimpel et al. (2011) and Owoputi et al. (2013); websites were processed by applying the Moses sentence
splitter.
16
The out-of-vocabulary (OOV) rates in Figure 2 show that in-domain training data reduces the number
of unseen words considerably, especially in the NER data sets. They also suggest that some evaluation
data sets share more vocabulary with our training data than others. In particular, we would expect better
performance on FOSTER-TEST than on RITTER-TEST and HOVY-TEST in POS tagging, as well as better
performance on FININ-TEST than on the other two NER test sets. In POS tagging, we actually do see
better results with FOSTER-TEST across the board, but in NER, FININ-TEST actually turns out to be the
hardest data set.
4 Results
4.1 POS results
Baselines We use three supervised CRF models as baselines (cf. the first part of Table 2). The first
supervised model is trained only on WSJ. This model does very well on FOSTER-DEV and FOSTER-
TEST, presumably because of the low OOV rates (Figure 2). The second supervised model is trained
only on GIMPEL-TRAIN; the third on the concatenation of WSJ and GIMPEL-TRAIN. While the second
baseline performs well on held-out data from its own sample (90.3% on GIMPEL-DEV), it performs
poorly across our out-of-sample test and development sets. Thus, it seems to overfit the sample of
tweets described in Gimpel et al. (2011). The third model trained on the concatenation of WSJ and
GIMPEL-TRAIN achieves the overall best baseline performance (88.4% macro-average accuracy). We
note that this is around one percentage point better than the best available off-the-shelf system for Twitter
(Owoputi et al., 2013) with an average accuracy of 87.5%.
12
Actually (Ritter et al., 2011) do cross-validation over this data, but we use the splits of Derczynski et al. (2013) for POS.
13
Using https://github.com/miso-belica/jusText
14
ftp://ftp.cs.cornell.edu/pub/smart/english.stop
15
https://github.com/brendano/ark-tweet-nlp
16
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/
split-sentences.perl
1787
Figure 2: Test set (type-level) OOV rates for POS (left) and NER (right).
l l
l
l l l
l l l l l l l
l l l l l l l l l l l l l l l l l l l l l l
0 5 10 15 20 25 30
88.5
89.0
89.5
90.0
DEV?avg wsj
iteration
accu
racy
l self?trainingWebDictWeb<DictDict<Web
l l
l l l
l l l l l l l
l l l l l l l l l l l l
l l
l l l l
0 5 10 15 20 25 30
88.8
89.0
89.2
89.4
89.6
89.8
90.0
90.2
DEV?avg wsj+gimpel
iteration
accu
racy
Figure 3: Learning curves on DEV-avg for systems trained on WSJ (left) and WSJ+GIMPEL (right) used
to set the hyperparameter i.
Learning with URLs The results of our approaches are presented in Table 2. The hyperparameter i
was set on the development data (cf. Figure 3). Note, again, that they do not require the test data to
contain URLs. First of all, naive self-training does not work: accuracy declines or is just around baseline
performance (Table 2 and Figure 3). In contrast, our augmented self-training methods with WEB or
DICT reach large improvements. In case we assume no target training data (train on WSJ only, i.e.
unsupervised DA), we obtain improvements of up to 9.1% error reduction. Overall the system improves
from 88.42% to 89.07%. This also holds for the second scenario, i.e. training on WSJ+GIMPEL-TRAIN
(semi-supervised DA, i.e., the case where we have some labeled target data, besides the pool of unlabeled
tweets) where we reach error reductions of up to 10%. Our technique, in other words, improves the
robustness of taggers, leading to much better performance on new samples of tweets.
4.2 NER results
For our NER results, cf. Table 3, we used the same feature models and parameter settings as those used for
POS tagging, except conditioning also on POS information. It is conceivable that other parameter settings
would have led to better results, but we did not want to assume the existence of in-domain development
data for this task. Our baselines are again supervised systems, as well as off-the-shelf systems. Our in-
1788
DEV-avg TEST TEST-avg
FOSTER HOVY RITTER
Baselines trained on
WSJ 88.82 91.87 87.01 86.38 88.42
GIMPEL-TRAIN 83.32 84.86 86.03 81.67 84.19
WSJ+GIMPEL-TRAIN 89.07 91.59 87.50 87.39 88.83
Systems trained on WSJ
SELF-TRAINING i = 25 85.52 91.80 86.72 85.90 88.14
DICT i = 25 85.61 92.08 87.63 85.68 88.46
WEB i = 25 85.27 92.47 87.30 86.60 88.79
DICT?WEB i = 25 86.11 92.61 87.70 86.69 89.00
WEB?DICT i = 25 86.15 92.57 88.12 86.51 89.07
max err.red 4.7% 9.1% 8.6% 2.3% 4.2%
Systems trained on WSJ+GIMPEL-TRAIN
SELF-TRAINING i = 27 89.12 91.83 86.88 87.43 88.71
DICT i = 27 89.43 92.22 88.38 87.69 89.43
WEB i = 27 89.82 92.43 87.43 88.21 89.36
DICT?WEB i = 27 90.04 92.43 88.38 88.48 89.76
WEB?DICT i = 27 90.04 92.40 87.99 88.39 89.59
max err.red 8.9% 10% 7.1% 8.6% 8.4%
Table 2: POS results.
house supervised baselines perform better than the available off-the-shelf systems, including the system
provided by Ritter et al. (2011) (TEST-avg of 54.2%). We report micro-average F
1
-scores over entity
types, computed using the publicly available evaluation script.
17
Our approaches again lead to substantial
error reductions of 8?13% across our NER evaluation data sets.
TEST TEST-avg
RITTER FROMREIDE FININ
Baseline trained on
CONLL+FININ-TRAIN 77.44 82.13 74.02 77.86
Systems trained on CONLL+FININ-TRAIN
SELF-TRAINING i = 27 78.63 82.88 74.89 78.80
DICT i = 27 65.24 69.1 65.45 66.60
WEB i = 27 78.29 83.82 74.99 79.03
DICT?WEB i = 27 78.53 83.91 75.83 79.42
WEB?DICT i = 27 65.97 69.92 65.86 67.25
err.red 9.1% 13.3% 8.0% 9.8%
Table 3: NER results.
5 Error analysis
The majority of cases where our taggers improve on the ARK tagger (Owoputi et al., 2013) seem to
relate to richer linguistic context. The ARK tagger incorrectly tags the sequence Man Utd as PRT-
NOUN, whereas our taggers correctly predict NOUN-NOUN. In a similar vein, our taggers correctly
predict the tag sequence NOUN-NOUN for Radio Edit, while the ARK tagger predicts NOUN-VERB.
However, some differences seem arbitrary. For example, the ARK tagger tags the sequence Nokia
17
http://www.cnts.ua.ac.be/conll2000/chunking/
1789
D5000 in FOSTER-TEST as NOUN-NUM. Our systems correctly predict NOUN-NOUN, but it is not
clear which analysis is better in linguistic terms. Our systems predict a sequence such as Love his version
to be VERB-PRON-NOUN, whereas the ARK tagger predicts VERB-DET-NOUN. Both choices seem
linguistically motivated.
Finally, some errors are made by all systems. For example, the word please in please, do that, for
example, is tagged as VERB by all systems. In FOSTER-TEST, this is annotated as X (which in the PTB
style was tagged as interjection UH). Obviously, please often acts as a verb, and while its part-of-speech
in this case may be debatable, we see please annotated as a verb in similar contexts in the PTB, e.g.:
(3) Please/VERB make/VERB me/PRON . . .
It is interesting to look at the tags that are projected from the websites to the tweets. Several of the
observed projections support the intuition that coupling tweets and the websites they link to enables us
to condition our tagging decisions on a richer linguistic context. Consider, for example Salmon-Safe,
initially predicted to be a NOUN, but after projection correctly analyzed as an ADJ:
Word Context Initial tag Projected tag
Salmon-Safe . . . parks NOUN ADJ
Snohomish . . . Bakery ADJ NOUN
toxic ppl r . . . NOUN ADJ
One of the most frequent projections is analyzing you?re, correctly, as a VERB rather than an ADV (if
the string is not split by tokenization).
One obvious limitation of the WEB-based models is that the projections apply to all occurrences of a
word. In rare cases, some words occur with different parts of speech in a single tweet, e.g., wish in:
(4) If I gave you one wish that will become true . What?s your wish ?... ? i wish i?ll get <num> wishes
from you :p <url>
In this case, our models enforce all occurrences of wish to, incorrectly, be verbs.
6 Related work
Previous work on tagging tweets has assumed labeled training data (Ritter et al., 2011; Gimpel et al.,
2011; Owoputi et al., 2013; Derczynski et al., 2013). Strictly supervised approaches to analyzing Twitter
has the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter.
This paper presents results using no in-domain labeled data that are significantly better than several off-
the-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data
to reach new highs across several data sets.
Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings
(Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T?ackstr?om et al., 2013).
Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea
of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et
al. (2012) for search query tagging.
Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and
the associated snippets provided by the search engine, projecting tags from the snippets to the queries,
guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more
advanced matching of snippets and search queries, giving priority to n-gram matches with larger n.
Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less
spelling variation than tweets.
In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and
Torisawa, 2007; Cucerzan, 2007). R?ud et al. (2011) consider using search engines for distant supervision
of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use
click-through data. They use the search engine snippets to generate feature representations rather than
projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts,
1790
applying their model to Twitter data, but their results are considerably below the state of the art. Also,
their source of supervision is not linked to the individual tweets in the way mentioned websites are.
In sum, our method is the first successful application of distant supervision to POS tagging and NER
for Twitter. Moreover, it is, to the best of our knowledge, the first paper that addresses both problems
using the same technique. Finally, our results are significantly better than state-of-the-art results in both
POS tagging and NER.
7 Conclusion
We presented a semi-supervised approach to POS tagging and NER for Twitter data that uses dictionaries
and linked websites as a source of not-so-distant (or linked) supervision to guide the bootstrapping. Our
approach outperforms off-the-shelf taggers when evaluated across various data sets, achieving average
error reductions across data sets of 5% on POS tagging and 10% on NER over state-of-the-art baselines.
References
Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text,
how diffrnt social media sources? In IJCNLP.
Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In ACL.
Silvia Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In EMNLP-CoNLL.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In ACL.
Hal Daum?e, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation.
In ACL Workshop on Domain Adaptation for NLP.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all:
overcoming sparse and noisy data. In RANLP.
Jacob Eisenstein. 2013. What to do about bad language on the internet. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Anno-
tating named entities in Twitter data with crowdsourcing. In NAACL-HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Jenny Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into infor-
mation extraction systems by Gibbs sampling. In ACL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van
Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In
IJCNLP.
Hege Fromreide, Dirk Hovy, and Anders S?gaard. 2014. Crowdsourcing and annotating ner for twitter #drift. In
LREC.
Kuzman Ganchev, Keith Hall, Ryan McDonald, and Slav Petrov. 2012. Using search-logs to improve query
tagging. In ACL.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-Speech Tagging for Twitter:
Annotation, Features, and Experiments. In ACL.
David Hand. 2006. Classifier technology and illusion of progress. Statistical Science, 21(1):1?15.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014. When pos datasets don?t add up: Combatting sample bias.
In LREC.
1791
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2009. Self-training with products of latent variable grammars.
In EMNLP.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.
Long Jiang, Mo Yo, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter sentiment
classification. In ACL.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploiting wikipedia as external knowledge for named entity
recognition. In EMNLP-CoNLL.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In HLT-NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In ACL.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conversational text with word clusters. In NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to
Twitter. In NAACL.
Barbara Plank. 2011. Domain Adaptation for Parsing. Ph.D. thesis, University of Groningen.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In
EMNLP.
Stefan R?ud, Massimiliano Ciaramita, Jens M?uller, and Hinrich Sch?utze. 2011. Piggyback: Using search engines
for robust cross-domain named entity recognition. In ACL.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. TACL, 1:1?12.
Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In In CoNLL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Kazama. 2011. Learning with lookahead: can history-based
models rival globally optimized models? In CoNLL.
Chun-Kai Wang, Bo-June Hsu, Ming-Wei Chang, and Emre Kiciman. 2013. Simple and knowledge-intensive
generative model for named entity recognition. Technical report, Microsoft Research.
1792
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62?72,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Multi-Source Transfer of Delexicalized Dependency Parsers
Ryan McDonald
Google
New York, NY
ryanmcd@google.com
Slav Petrov
Google
New York, NY
slav@google.com
Keith Hall
Google
Zu?rich
kbhall@google.com
Abstract
We present a simple method for transferring
dependency parsers from source languages
with labeled training data to target languages
without labeled training data. We first demon-
strate that delexicalized parsers can be di-
rectly transferred between languages, produc-
ing significantly higher accuracies than unsu-
pervised parsers. We then use a constraint
driven learning algorithm where constraints
are drawn from parallel corpora to project the
final parser. Unlike previous work on project-
ing syntactic resources, we show that simple
methods for introducing multiple source lan-
guages can significantly improve the overall
quality of the resulting parsers. The projected
parsers from our system result in state-of-the-
art performance when compared to previously
studied unsupervised and projected parsing
systems across eight different languages.
1 Introduction
Statistical parsing has been one of the most active ar-
eas of research in the computational linguistics com-
munity since the construction of the Penn Treebank
(Marcus et al, 1993). This includes work on phrase-
structure parsing (Collins, 1997; Charniak, 2000;
Petrov et al, 2006), dependency parsing (McDonald
et al, 2005; Nivre et al, 2006) as well as a num-
ber of other formalisms (Clark and Curran, 2004;
Wang and Harper, 2004; Shen and Joshi, 2008).
As underlying modeling techniques have improved,
these parsers have begun to converge to high lev-
els of accuracy for English newswire text. Subse-
quently, researchers have begun to look at both port-
ing these parsers to new domains (Gildea, 2001; Mc-
Closky et al, 2006; Petrov et al, 2010) and con-
structing parsers for new languages (Collins et al,
1999; Buchholz and Marsi, 2006; Nivre et al, 2007).
One major obstacle in building statistical parsers
for new languages is that they often lack the manu-
ally annotated resources available for English. This
observation has led to a vast amount of research
on unsupervised grammar induction (Carroll and
Charniak, 1992; Klein and Manning, 2004; Smith
and Eisner, 2005; Cohen and Smith, 2009; Berg-
Kirkpatrick and Klein, 2010; Naseem et al, 2010;
Spitkovsky et al, 2010; Blunsom and Cohn, 2010).
Grammar induction systems have seen large ad-
vances in quality, but parsing accuracies still signif-
icantly lag behind those of supervised systems. Fur-
thermore, they are often trained and evaluated under
idealized conditions, e.g., only on short sentences
or assuming the existence of gold-standard part-of-
speech (POS) tags.1 The reason for these assump-
tions is clear. Unsupervised grammar induction is
difficult given the complexity of the analysis space.
These assumptions help to give the model traction.
The study of unsupervised grammar induction has
many merits. Most notably, it increases our under-
standing of how computers (and possibly humans)
learn in the absence of any explicit feedback. How-
ever, the gold POS tag assumption weakens any con-
clusions that can be drawn, as part-of-speech are
also a form of syntactic analysis, only shallower.
Furthermore, from a practical standpoint, it is rarely
the case that we are completely devoid of resources
for most languages. This point has been made by
1A notable exception is the work of Seginer (2007).
62
studies that transfer parsers to new languages by
projecting syntax across word alignments extracted
from parallel corpora (Hwa et al, 2005; Ganchev et
al., 2009; Smith and Eisner, 2009). Although again,
most of these studies also assume the existence of
POS tags.
In this work we present a method for creating de-
pendency parsers for languages for which no labeled
training data is available. First, we train a source
side English parser that, crucially, is delexicalized so
that its predictions rely soley on the part-of-speech
tags of the input sentence, in the same vein as Ze-
man and Resnik (2008). We empirically show that
directly transferring delexicalized models (i.e. pars-
ing a foreign language POS sequence with an En-
glish parser) already outperforms state-of-the-art un-
supervised parsers by a significant margin. This re-
sult holds in the presence of both gold POS tags as
well as automatic tags projected from English. This
emphasizes that even for languages with no syntac-
tic resources ? or possibly even parallel data ? sim-
ple transfer methods can already be more powerful
than grammar induction systems.
Next, we use this delexicalized English parser to
seed a perceptron learner for the target language.
The model is trained to update towards parses that
are in high agreement with a source side English
parse based on constraints drawn from alignments in
the parallel data. We use the augmented-loss learn-
ing procedure (Hall et al, 2011) which is closely
related to constraint driven learning (Chang et al,
2007; Chang et al, 2010). The resulting parser con-
sistently improves on the directly transferred delex-
icalized parser, reducing relative errors by 8% on
average, and as much as 18% on some languages.
Finally, we show that by transferring parsers from
multiple source languages we can further reduce er-
rors by 16% over the directly transferred English
baseline. This is consistent with previous work on
multilingual part-of-speech (Snyder et al, 2009) and
grammar (Berg-Kirkpatrick and Klein, 2010; Cohen
and Smith, 2009) induction, that shows that adding
languages leads to improvements.
We present a comprehensive set of experiments
on eight Indo-European languages for which a sig-
nificant amount of parallel data exists. We make
no language specific enhancements in our experi-
ments. We report results for sentences of all lengths,
??????????????????????????????????????????????????????????????
Figure 1: An example (unlabeled) dependency tree.
as well as with gold and automatically induced
part-of-speech tags. We also report results on sen-
tences of length 10 or less with gold part-of-speech
tags to compare with previous work. Our results
consistently outperform the previous state-of-the-art
across all languages and training configurations.
2 Preliminaries
In this paper we focus on transferring dependency
parsers between languages. A dependency parser
takes a tokenized input sentence (optionally part-of-
speech tagged) and produces a connected tree where
directed arcs represent a syntactic head-modifier re-
lationship. An example of such a tree is given in
Figure 1. Dependency tree arcs are often labeled
with the role of the syntactic relationship, e.g., is to
hearing might be labeled as SUBJECT. However, we
focus on unlabeled parsing in order to reduce prob-
lems that arise due to different treebank annotation
schemes. Of course, even for unlabeled dependen-
cies, significant variations in the annotation schemes
remain. For example, in the Danish treebank deter-
miners govern adjectives and nouns in noun phrases,
while in most other treebanks the noun is the head of
the noun phrase. Unlike previous work (Zeman and
Resnik, 2008; Smith and Eisner, 2009), we do not
apply any transformations to the treebanks, which
makes our results easier to reproduce, but systemat-
ically underestimates accuracy.
2.1 Data Sets
The treebank data in our experiments are from the
CoNLL shared-tasks on dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al, 2007). We use
English (en) only as a source language throughout
the paper. Additionally, we use the following eight
languages as both source and target languages: Dan-
ish (da), Dutch (nl), German (de), Greek (el), Italian
(it), Portuguese (pt), Spanish (es) and Swedish (sv).
For languages that were included in both the 2006
and 2007 tasks, we used the treebank from the lat-
63
ter. We focused on this subset of languages because
they are Indo-European and a significant amount of
parallel data exists for each language. By present-
ing results on eight languages our study is already
more comprehensive than most previous work in this
area. However, the restriction to Indo-European lan-
guages does make the results less conclusive when
one wishes to transfer a parser from English to Chi-
nese, for example. To account for this, we report
additional results in the discussion for non-Indo-
European languages. For all data sets we used the
predefined training and testing splits.
Our approach relies on a consistent set of part-
of-speech tags across languages and treebanks. For
this we used the universal tagset from Petrov et
al. (2011), which includes: NOUN (nouns), VERB
(verbs), ADJ (adjectives), ADV (adverbs), PRON
(pronouns), DET (determiners), ADP (prepositions
or postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), PUNC (punctuation marks)
and X (a catch-all tag). Similar tagsets are used by
other studies on grammar induction and projection
(Naseem et al, 2010; Zeman and Resnik, 2008). For
all our experiments we replaced the language spe-
cific part-of-speech tags in the treebanks with these
universal tags.
Like all treebank projection studies we require a
corpus of parallel text for each pair of languages we
study. For this we used the Europarl corpus version
5 (Koehn, 2005). The corpus was preprocessed in
standard ways and word aligned by running six it-
erations of IBM Model 1 (Brown et al, 1993), fol-
lowed by six iterations of the HMM model (Vogel et
al., 1996) in both directions. We then intersect word
alignments to generate one-to-one alignments.
2.2 Parsing Model
All of our parsing models are based on the
transition-based dependency parsing paradigm
(Nivre, 2008). Specifically, all models use an
arc-eager transition strategy and are trained using
the averaged perceptron algorithm as in Zhang and
Clark (2008) with a beam size of 8. The features
used by all models are: the part-of-speech tags of
the first four words on the buffer and of the top two
words on the stack; the word identities of the first
two words on the buffer and of the top word on the
stack; the word identity of the syntactic head of
the top word on the stack (if available). All feature
conjunctions are included. For treebanks with
non-projective trees we use the pseudo-projective
parsing technique to transform the treebank into
projective structures (Nivre and Nilsson, 2005).
We focus on using this parsing system for two
reasons. First, the parser is near state-of-the-art on
English parsing benchmarks and second, and more
importantly, the parser is extremely fast to train and
run, making it easy to run a large number of exper-
iments. Preliminary experiments using a different
dependency parser ? MSTParser (McDonald et al,
2005) ? resulted in similar empirical observations.
2.3 Evaluation
All systems are evaluated using unlabeled attach-
ment score (UAS), which is the percentage of words
(ignoring punctuation tokens) in a corpus that mod-
ify the correct head (Buchholz and Marsi, 2006).
Furthermore, we evaluate with both gold-standard
part-of-speech tags, as well as predicted part-of-
speech tags from the projected part-of-speech tagger
of Das and Petrov (2011).2 This tagger relies only on
labeled training data for English, and achieves accu-
racies around 85% on the languages that we con-
sider. We evaluate in the former setting to compare
to previous studies that make this assumption. We
evaluate in the latter setting to measure performance
in a more realistic scenario ? when no target lan-
guage resources are available.
3 Transferring from English
To simplify discussion, we first focus on the most
common instantiation of parser transfer in the liter-
ature: transferring from English to other languages.
In the next section we expand our system to allow
for the inclusion of multiple source languages.
3.1 Direct Transfer
We start with the observation that discriminatively
trained dependency parsers rely heavily on part-of-
speech tagging features. For example, when train-
ing and testing a parser on our English data, a parser
with all features obtains an UAS of 89.3%3 whereas
2Available at http://code.google.com/p/pos-projection/
3The best system at CoNLL 2007 achieved 90.1% and used
a richer part-of-speech tagset (Nivre et al, 2007).
64
a delexicalized parser ? a parser that only has non-
lexical features ? obtains an UAS of 82.5%. The
key observation is that part-of-speech tags contain a
significant amount of information for unlabeled de-
pendency parsing.
This observation combined with our universal
part-of-speech tagset, leads to the idea of direct
transfer, i.e., directly parsing the target language
with the source language parser without relying on
parallel corpora. This idea has been previously ex-
plored by Zeman and Resnik (2008) and recently by
S?gaard (2011). Because we use a mapping of the
treebank specific part-of-speech tags to a common
tagset, the performance of a such a system is easy to
measure ? simply parse the target language data set
with a delexicalized parser trained on the source lan-
guage data. We conducted two experiments. In the
first, we assumed that the test set for each target lan-
guage had gold part-of-speech tags, and in the sec-
ond we used predicted part-of-speech tags from the
projection tagger of Das and Petrov (2011), which
also uses English as the source language.
UAS for all sentence lengths without punctuation
are given in Table 1. We report results for both the
English direct transfer parser (en-dir.) as well as a
baseline unsupervised grammar induction system ?
the dependency model with valence (DMV) of Klein
and Manning (2004), as obtained by the implemen-
tation of Ganchev et al (2010). We trained on sen-
tences of length 10 or less and evaluated on all sen-
tences from the test set.4 For DMV, we reversed the
direction of all dependencies if this led to higher per-
formance. From this table we can see that direct
transfer is a very strong baseline and is over 20%
absolute better than the DMV model for both gold
and predicted POS tags. Table 4, which we will dis-
cuss in more detail later, further shows that the direct
transfer parser also significantly outperforms state-
of-the-art unsupervised grammar induction models,
but in a more limited setting of sentences of length
less than 10.
Direct transfer works for a couple of reasons.
First, part-of-speech tags contain a significant
amount of information for parsing unlabeled depen-
dencies. Second, this information can be transferred,
4Training on all sentences results in slightly lower accura-
cies on average.
to some degree, across languages and treebank stan-
dards. This is because, at least for Indo-European
languages, there is some regularity in how syntax
is expressed, e.g., primarily SVO, prepositional, etc.
Even though there are some differences with respect
to relative location of certain word classes, strong
head-modifier POS tag preferences can still help re-
solve these, especially when no other viable alter-
natives are available. Consider for example an arti-
ficial sentence with a tag sequence: ?VERB NOUN
ADJ DET PUNC?. The English parser still predicts
that the NOUN and PUNC modify the VERB and the
ADJ and DET modify the NOUN, even though in the
English data such noun phrases are unlikely.5
3.2 Projected Transfer
Unlike most language transfer systems for parsers,
the direct transfer approach does not rely on project-
ing syntax across aligned parallel corpora (modulo
the fact that non-gold tags come from a system that
uses parallel corpora). In this section we describe
a simple mechanism for projecting from the direct
transfer system using large amounts of parallel data
in a similar vein to Hwa et al (2005), Ganchev et
al. (2009), Smith and Eisner (2009) inter alia. The
algorithm is based on the work of Hall et al (2011)
for training extrinsic parser objective functions and
borrows heavily from ideas in learning with weak
supervision including work on learning with con-
straints (Chang et al, 2007) and posterior regular-
ization (Ganchev et al, 2010). In our case, the
weak signals come from aligned source and target
sentences, and the agreement in their corresponding
parses, which is similar to posterior regularization
or the bilingual view of Smith and Smith (2004) and
Burkett et al (2010).
The algorithm is given in Figure 2. It starts by
labeling a set of target language sentences with a
parser, which in our case is the direct transfer parser
from the previous section (line 1). Next, it uses
these parsed target sentences to ?seed? a new parser
by training a parameter vector using the predicted
parses as a gold standard via standard perceptron
updates for J rounds (lines 3-6). This generates a
parser that emulates the direct transfer parser, but
5This requires a transition-based parser with a beam greater
than 1 to allow for ambiguity to be resolved at later stages.
65
Notation:
x: input sentence
y: dependency tree
a: alignment
w: parameter vector
?(x, y): feature vector
DP : dependency parser, i.e., DP : x? y
Input:
X = {xi}ni=1: target language sentences
P = {(xsi , xti, ai)}mi=1: aligned source-target sentences
DPdelex: delexicalized source parser
DPlex: lexicalized source parser
Algorithm:
1. Let X ? = {(xi, yi)}ni=1 where yi = DPdelex(xi)
2. w = 0
see
d-s
tag
e 3. for j : 1 . . . J
4. for xi : x1 . . . xn
5. Let y = argmaxy w ? ?(xi, y)
6. w = w + ?(xt, yi)? ?(xi, y)
pro
jec
tio
n-s
tag
e 7. for (xsi , xti, ai) : (xs1, xt1, a1) . . . (xsm, xsm, am)8. Let ys = DPlex(xsi )
9. Let Yt = {y1i , . . . , yki }, where:
yki = argmaxy/?{y1i ,...,yk?1i } w ? ?(x
t
i, y)
10. Let yt = argmaxyt?Yt ALIGN(ys, yt, ai)11. w = w + ?(xi, yt)? ?(xi, y1i )
return DP ? such that DP ?(x) = argmaxy w ? ?(x, y)
Figure 2: Perceptron-based learning algorithm for train-
ing a parser by seeding the model with a direct transfer
parser and projecting constraints across parallel corpora.
has now been lexicalized and is working in the space
of target language sentences. Next, the algorithm it-
erates over the sentences in the parallel corpus. It
parses the English sentence with an English parser
(line 8, again a lexicalized parser). It then uses the
current target language parameter vector to create
a k-best parse list for the target sentence (line 9).
From this list, it selects the parse whose dependen-
cies align most closely with the English parse via the
pre-specified alignment (line 10, also see below for
the definition of the ALIGN function). It then uses
this selected parse as a proxy to the gold standard
parse to update the parameters (line 11).
The intuition is simple. The parser starts with
non-random accuracies by emulating the direct
transfer model and slowly tries to induce better pa-
rameters by selecting parses from its k-best list
that are considered ?good? by some external met-
ric. The algorithm then updates towards that out-
put. In this case ?goodness? is determined through
the pre-specified sentence alignment and how well
the target language parse aligns with the English
parse. As a result, the model will, ideally, converge
to a state where it predicts target parses that align as
closely as possible with the corresponding English
parses. However, since we seed the learner with the
direct transfer parser, we bias the parameters to se-
lect parses that both align well and also have high
scores under the direct transfer model. This helps
to not only constrain the search space at the start
of learning, but also helps to bias dependencies be-
tween words that are not part of the alignment.
So far we have not defined the ALIGN function
that is used to score potential parses. Let a =
{(s(1), t(1)), . . . , (s(n), t(n))} be an alignment where
s(i) is a word in the source sentence xs (not nec-
essarily the ith word) and t(i) is similarly a word
in the target sentence xt (again, not necessarily the
ith word). The notation (s(i), t(i)) ? a indicates
two words are the ith aligned pair in a. We define
the ALIGN function to encode the Direct Correspon-
dence Assumption (DCA) from Hwa et al (2005):
ALIGN(ys, yt, a)
=
?
(s(i),t(i))?a
(s(j),t(j))?a
SCORE(ys, yt, (s(i), s(j)), (t(i), t(j)))
SCORE(ys, yt, (s(i), s(j)), (t(i), t(j)))
=
?
???
???
+1 if (s(i), s(j)) ? ys and (t(i), t(j)) ? yt
?1 if (s(i), s(j)) ? ys and (t(i), t(j)) /? yt
?1 if (s(i), s(j)) /? ys and (t(i), t(j)) ? yt
0 otherwise
The notation (i, j) ? y indicates that a dependency
from head i to modifier j is in tree y. The ALIGN
function rewards aligned head-modifier pairs and
penalizes unaligned pairs when a possible alignment
exists. For all other cases it is agnostic, i.e., when
one or both of the modifier or head are not aligned.
Figure 3 shows an example of aligned English-
Greek sentences, the English parse and a potential
Greek parse. In this case the ALIGN function re-
turns a value of 2. This is because there are three
aligned dependencies: took?book, book?the and
66
?????????????????????????????????????
?????????????????????????????????????
Figure 3: A Greek and English sentence pair. Word
alignments are shown as dashed lines, dependency arcs
as solid lines.
from?John. These add 3 to the score. There is
one incorrectly aligned dependency: the preposi-
tion mistakenly modifies the noun on the Greek side.
This subtracts 1. Finally, there are two dependencies
that do not align: the subject on the English side
and a determiner to a proper noun on the Greek side.
These do not effect the result.
The learning algorithm in Figure 2 is an instance
of augmented-loss training (Hall et al, 2011) which
is closely related to the constraint driven learning al-
gorithms of Chang et al (2007). In that work, ex-
ternal constraints on output structures are used to
help guide the learner to good parameter regions.
In our model, we use constraints drawn from paral-
lel data exactly in the same manner. Since posterior
regularization is closely related to constraint driven
learning, this makes our algorithm also similar to the
parser projection approach of Ganchev et al (2009).
There are a couple of differences. First, we bias our
model towards the direct transfer model, which is
already quite powerful. Second, our alignment con-
straints are used to select parses from a k-best list,
whereas in posterior regularization they are used as
soft constraints on full model expectations during
training. The latter is beneficial as the use of k-best
lists does not limit the class of parsers to those whose
parameters and search space decompose neatly with
the DCA loss function. An empirical comparison to
Ganchev et al (2009) is given in Section 5.
Results are given in Table 1 under the column en-
proj. For all experiments we train the seed-stage
perceptron for 5 iterations (J = 5) and we use one
hundred times as much parallel data as seed stage
non-parallel data (m = 100n). The seed-stage non-
parallel data is the training portion of each treebank,
stripped of all dependency annotations. After train-
ing the projected parser we average the parameters
gold-POS pred-POS
DMV en-dir. en-proj. DMV en-dir. en-proj.
da 33.4 45.9 48.2 18.4 44.0 45.5
de 18.0 47.2 50.9 30.3 44.7 47.4
el 39.9 63.9 66.8 21.2 63.0 65.2
es 28.5 53.3 55.8 19.9 50.2 52.4
it 43.1 57.7 60.8 37.7 53.7 56.3
nl 38.5 60.8 67.8 19.9 62.1 66.5
pt 20.1 69.2 71.3 21.0 66.2 67.7
sv 44.0 58.3 61.3 33.8 56.5 59.7
avg 33.2 57.0 60.4 25.3 55.0 57.6
Table 1: UAS for the unsupervised DMV model (DMV),
a delexicalized English direct transfer parser (en-dir.)
and a English projected parser (en-proj.). Measured on
all sentence lengths for both gold and predicted part-of-
speech tags as input.
of the model (Collins, 2002). The parsers evaluated
using predicted part-of-speech tags use the predicted
tags at both training and testing time and are thus
free of any target language specific resources.
When compared with the direct transfer model
(en-dir. in Table 1), we can see that there is an im-
provement for every single language, reducing rela-
tive error by 8% on average (57.0% to 60.4%) and
up to 18% for Dutch (60.8 to 67.8%). One could
wonder whether the true power of the projection
model comes from the re-lexicalization step ? lines
3-6 of the algorithm. However, if just this step is run,
then the average UAS only increases from 57.0%
to 57.4%, showing that most of the improvement
comes from the projection stage. Note that the re-
sults in Table 1 indicate that parsers using predicted
part-of-speech tags are only slightly worse than the
parsers using gold tags (about 2-3% absolute), show-
ing that these methods are robust to tagging errors.
4 Multi-Source Transfer
The previous section focused on transferring an En-
glish parser to a new target language. However,
there are over 20 treebanks available for a variety
of language groups including Indo-European, Altaic
(including Japanese), Semitic, and Sino-Tibetan.
Many of these are even in standardized formats
(Buchholz and Marsi, 2006; Nivre et al, 2007). Past
studies have shown that for both part-of-speech tag-
ging and grammar induction, learning with multiple
comparable languages leads to improvements (Co-
hen and Smith, 2009; Snyder et al, 2009; Berg-
Kirkpatrick and Klein, 2010). In this section we ex-
67
Source Training Language
da de el en es it nl pt sv
Ta
rge
tT
est
La
ng
ua
ge
da 79.2 45.2 44.0 45.9 45.0 48.6 46.1 48.1 47.8
de 34.3 83.9 53.2 47.2 45.8 53.4 55.8 55.5 46.2
el 33.3 52.5 77.5 63.9 41.6 59.3 57.3 58.6 47.5
en 34.4 37.9 45.7 82.5 28.5 38.6 43.7 42.3 43.7
es 38.1 49.4 57.3 53.3 79.7 68.4 51.2 66.7 41.4
it 44.8 56.7 66.8 57.7 64.7 79.3 57.6 69.1 50.9
nl 38.7 43.7 62.1 60.8 40.9 50.4 73.6 58.5 44.2
pt 42.5 52.0 66.6 69.2 68.5 74.7 67.1 84.6 52.1
sv 44.5 57.0 57.8 58.3 46.3 53.4 54.5 66.8 84.8
Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a
delexicalized parser and each row represents which target language test data was used. Bold numbers are when source
equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths
without punctuation.
amine whether this is also true for parser transfer.
Table 2 shows the matrix of source-target lan-
guage UAS for all nine languages we consider (the
original eight target languages plus English). We
can see that there is a wide range from 33.3% to
74.7%. There is also a wide range of values depend-
ing on the source training data and/or target testing
data, e.g., Portuguese as a source tends to parse tar-
get languages much better than Danish, and is also
more amenable as a target testing language. Some
of these variations are expected, e.g., the Romance
languages (Spanish, Italian and Portuguese) tend to
transfer well to one another. However, some are
unexpected, e.g., Greek being the best source lan-
guage for Dutch, as well as German being one of the
worst. This is almost certainly due to different an-
notation schemes across treebanks. Overall, Table 2
does indicate that there are possible gains in accu-
racy through the inclusion of additional languages.
In order to take advantage of treebanks in multi-
ple languages, our multi-source system simply con-
catenates the training data from all non-target lan-
guages. In other words, the multi-source direct
transfer parser for Danish will be trained by first
concatenating the training corpora of the remain-
ing eight languages, training a delexicalized parser
on this data and then directly using this parser to
analyze the Danish test data. For the multi-source
projected parser, the procedure is identical to that
in Section 3.2 except that we use the multi-source
direct transfer model to seed the algorithm instead
of the English-only direct transfer model. For these
experiments we still only use English-target parallel
data because that is the format of the readily avail-
able data in the Europarl corpus.
Table 3 presents four sets of results. The first
(best-source) is the direct transfer results for the ora-
cle single-best source language per target language.
The second (avg-source) is the mean UAS over all
source languages per target language. The third
(multi-dir.) is the multi-source direct transfer sys-
tem. The fourth and final result set (multi-proj.)
is the multi-source projected system. The resulting
parsers are typically much more accurate than the
English direct transfer system (Table 1). On aver-
age, the multi-source direct transfer system reduces
errors by 10% relative over the English-only direct
transfer system. These improvements are not consis-
tent. For Greek and Dutch we see significant losses
relative to the English-only system. An inspection of
Table 2 shows that for these two languages English
is a particularly good source training language.
For the multi-source projected system the results
are mixed. Some languages see basically no change
relative the multi-source direct transfer model, while
some languages see modest to significant increases.
But again, there is an overall trend to better mod-
els. In particular, starting with an English-only di-
rect transfer parser with 57.0% UAS on average,
by adding parallel corpora and multiple source lan-
guages we finish with parser having 63.8% UAS
on average, which is a relative reduction in error
of roughly 16% and more than doubles the perfor-
mance of a DMV model (Table 1).
Interestingly, the multi-source systems provide,
on average, accuracies near that of the single-best
source language and significantly better than the av-
erage source UAS. Thus, even this simple method of
68
best-source avg-source gold-POS pred-POS
source gold-POS gold-POS multi-dir. multi-proj. multi-dir. multi-proj.
da it 48.6 46.3 48.9 49.5 46.2 47.5
de nl 55.8 48.9 56.7 56.6 51.7 52.0
el en 63.9 51.7 60.1 65.1 58.5 63.0
es it 68.4 53.2 64.2 64.5 55.6 56.5
it pt 69.1 58.5 64.1 65.0 56.8 58.9
nl el 62.1 49.9 55.8 65.7 54.3 64.4
pt it 74.8 61.6 74.0 75.6 67.7 70.3
sv pt 66.8 54.8 65.3 68.0 58.3 62.1
avg 63.7 51.6 61.1 63.8 56.1 59.3
Table 3: UAS for multi-source direct (multi-dir.) and projected (multi-proj.) transfer systems. best-source is the best
source model from the languages in Table 2 (excluding the target language). avg-source is the mean UAS over the
source models for the target (excluding target language).
multi-source transfer already provides strong perfor-
mance gains. We expect that more principled tech-
niques will lead to further improvements. For exam-
ple, recent work by S?gaard (2011) explores data set
sub-sampling methods. Unlike our work, S?gaard
found that simply concatenating all the data led to
degradation in performance. Cohen et al (2011) ex-
plores the idea learning language specific mixture
coefficients for models trained independently on the
target language treebanks. However, their results
show that this method often did not significantly out-
perform uniform mixing.
5 Comparison
Comparing unsupervised and parser projection sys-
tems is difficult as many publications use non-
overlapping sets of languages or different evaluation
criteria. We compare to the following three systems
that do not augment the treebanks and report results
for some of the languages that we considered:
? USR: The weakly supervised system of
Naseem et al (2010), in which manually de-
fined universal syntactic rules (USR) are used
to constrain a probabilistic Bayesian model. In
addition to their original results, we also report
results using the same part-of-speech tagset as
the systems described in this paper (USR?).
This is useful for two reasons. First, it makes
the comparison more direct. Second, we can
generate USR results for all eight languages
and not just for the languages that they report.
? PGI: The phylogenetic grammar induction
(PGI) model of Berg-Kirkpatrick and Klein
(2010), in which the parameters of completely
unsupervised DMV models for multiple lan-
guages are coupled via a phylogenetic prior.
? PR: The posterior regularization (PR) approach
of Ganchev et al (2009), in which a supervised
English parser is used to generate constraints
that are projected using a parallel corpus and
used to regularize a target language parser. We
report results without treebank specific rules.
Table 4 gives results comparing the models pre-
sented in this work to those three systems. For this
comparison we use sentences of length 10 or less
after punctuation has been removed in order to be
consistent with reported results. The overall trends
carry over from the full treebank setting to this re-
duced sentence length setup: the projected mod-
els outperform the direct transfer models and multi-
source transfer gives higher accuracy than transfer-
ring only from English. Most previous work has as-
sumed gold part-of-speech tags, but as the code for
USR is publicly available we were able to train it
using the same projected part-of-speech tags used
in our models. These results are also given in Ta-
ble 4 under USR?. Again, we can see that the multi-
source systems (both direct and projected) signifi-
cantly outperform the unsupervised models.
It is not surprising that a parser transferred from
annotated resources does significantly better than
unsupervised systems since it has much more in-
formation from which to learn. The PR system of
Ganchev et al (2009) is similar to ours as it also
projects syntax across parallel corpora. For Span-
ish we can see that the multi-source direct trans-
fer parser is better (75.1% versus 70.6%), and this
is also true for the multi-source projected parser
69
?? gold-POS ?? ? pred-POS?
en-dir. en-proj. multi-dir. multi-proj. USR? USR PGI PR multi-dir. multi-proj. USR?
da 53.2 57.4 58.4 58.8 55.1 51.9 41.6 54.9 54.6 41.7
de 65.9 67.0 74.9 72.0 60.0 63.7 63.4 55.1
el 73.9 73.9 73.5 78.7 60.3 65.2 74.3 53.4
es 58.0 62.3 75.1 73.2 68.3 67.2 58.4 70.6 59.1 56.8 43.3
it 65.5 69.9 75.5 75.5 47.9 65.5 70.2 41.4
nl 67.6 72.2 58.8 70.7 44.0 45.1 56.3 67.2 38.8
pt 77.9 80.6 81.1 86.2 70.9 71.5 63.0 74.0 79.2 66.4
sv 70.4 71.3 76.0 77.6 52.6 58.3 72.0 73.9 59.4
avg 66.6 69.4 71.7 74.1 57.4 63.9 67.5 49.9
Table 4: UAS on sentences of length 10 or less without punctuation, comparing the systems presented in this work
to three representative systems from related work. en-dir./en-proj. are the direct/projected English parsers and multi-
dir./multi-proj. are the multi-source direct/projected parsers. Section 5 contains a description of the baseline systems.
(73.2%). Ganchev et al also report results for
Bulgarian. We trained a multi-source direct trans-
fer parser for Bulgarian which obtained a score of
72.8% versus 67.8% for the PR system. If we only
use English as a source language, as in Ganchev et
al., the English direct transfer model achieves 66.1%
on Bulgarian and 69.3% on Spanish versus 67.8%
and 70.6% for PR. In this setting the English pro-
jected model gets 72.0% on Spanish. Thus, under
identical conditions the direct transfer model obtains
accuracies comparable to PR.6
Another projection based system is that of Smith
and Eisner (2009), who report results for German
(68.5%) and Spanish (64.8%) on sentences of length
15 and less inclusive of punctuation. Smith and Eis-
ner use custom splits of the data and modify a sub-
set of the dependencies. The multi-source projected
parser obtains 71.9% for German and 67.8% for
Spanish on this setup.7 If we cherry-pick the source
language the results can improve, e.g., for Spanish
we can obtain 71.7% and 70.8% by directly transfer-
ring parsers form Italian or Portuguese respectively.
6 Discussion
One fundamental point the above experiments il-
lustrate is that even for languages for which no
resources exist, simple methods for transferring
parsers work remarkably well. In particular, if
6Note that the last set of results was obtained by using the
same English training data as Ganchev et al Using the CoNLL
2007 English data set for training, the English direct transfer
model is 63.2% for Bulgarian and 58.0% for Spanish versus
67.8% and 70.6% for PR, highlighting the large impact that dif-
ference treebank annotation standards can have.
7Data sets and evaluation criteria obtained via communica-
tions with David Smith and Jason Eisner.
one can transfer part-of-speech tags, then a large
part of transferring unlabeled dependencies has been
solved. This observation should lead to a new base-
line in unsupervised and projected grammar induc-
tion ? the UAS of a delexicalized English parser.
Of course, our experiments focus strictly on Indo-
European languages. Preliminary experiments for
Arabic (ar), Chinese (zh), and Japanese (ja) suggest
similar direct transfer methods are applicable. For
example, on the CoNLL test sets, a DMV model
obtains UAS of 28.7/41.8/34.6% for ar/zh/ja re-
spectively, whereas an English direct transfer parser
obtains 32.1/53.8/32.2% and a multi-source direct
transfer parser obtains 39.9/41.7/43.3%. In this
setting only Indo-European languages are used as
source data. Thus, even across language groups di-
rect transfer is a reasonable baseline. However, this
is not necessary as treebanks are available for a num-
ber of language groups, e.g., Indo-European, Altaic,
Semitic, and Sino-Tibetan.
The second fundamental observation is that when
available, multiple sources should be used. Even
through naive multi-source methods (concatenating
data), it is possible to build a system that has compa-
rable accuracy to the single-best source for all lan-
guages. This advantage does not come simply from
having more data. In fact, if we randomly sam-
pled from the multi-source data until the training set
size was equivalent to the size of the English data,
then the results still hold (and in fact go up slightly
for some languages). This suggests that even bet-
ter transfer models can be produced by separately
weighting each of the sources depending on the tar-
get language ? either weighting by hand, if we know
the language group of the target language, or auto-
70
matically, if we do not. As previously mentioned,
the latter has been explored in both S?gaard (2011)
and Cohen et al (2011).
7 Conclusions
We presented a simple, yet effective approach
for projecting parsers from languages with labeled
training data to languages without any labeled train-
ing data. Central to our approach is the idea of
delexicalizing the models, which combined with a
standardized part-of-speech tagset alows us to di-
rectly transfer models between languages. We then
use a constraint driven learning algorithm to adapt
the transferred parsers to the respective target lan-
guage, obtaining an additional 16% error reduc-
tion on average in a multi-source setting. Our final
parsers achieve state-of-the-art accuracies on eight
Indo-European languages, significantly outperform-
ing previous unsupervised and projected systems.
Acknowledgements: We would like to thank Kuz-
man Ganchev, Valentin Spitkovsky and Dipanjan
Das for numerous discussions on this topic and com-
ments on earlier drafts of this paper. We would
also like to thank Shay Cohen, Dipanjan Das, Noah
Smith and Anders S?gaard for sharing early drafts
of their recent related work.
References
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic
grammar induction. In Proc. of ACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
Proc. of EMNLP.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In Proc. of CoNLL.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. In Proc. of the Working Notes of the Workshop
Statistically-Based NLP Techniques.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
Proc. of ACL.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In Proc. of ICML.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In Proc. of ACL.
S.B. Cohen and N.A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. In Proc. of NAACL.
S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proc. of EMNLP.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. of ACL.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. of ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proc. of ACL-HLT.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection con-
straints. In Proc. of ACL-IJCNLP.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc of EMNLP.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In Proc. of EMNLP.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Natural Language Engineer-
ing, 11(03):311?325.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: models of dependency and
constituency. In Proc. of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
M. P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn treebank. Computational Linguis-
tics, 19.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL.
71
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proc. of EMNLP.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.
S. Petrov, P. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In EMNLP ?10.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. In ArXiv:1104.2086.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Proc. of ACL.
L. Shen and A.K. Joshi. 2008. Ltag dependency parsing
with bidirectional incremental construction. In Proc.
of EMNLP.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
D.A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous grammar features.
In Proc. of EMNLP.
D.A. Smith and N.A. Smith. 2004. Bilingual parsing
with factored estimation: Using english to parse ko-
rean. In Proc. of EMNLP.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2009. Adding more languages improves unsupervised
multilingual part-of-speech tagging: A Bayesian non-
parametric approach. In Proc. of NAACL.
A. S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Proc.
ACL.
V.I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From baby steps to leapfrog: How ?less is more? in un-
supervised dependency parsing. In Proc. of NAACL-
HLT.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
W. Wang and M. P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proc. of
the Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together.
D. Zeman and P. Resnik. 2008. Cross-language parser
adaptation between related languages. In NLP for Less
Privileged Languages.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
72
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183?192,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training a Parser for Machine Translation Reordering
Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och
David Talbot Hiroshi Ichikawa Masakazu Seno Hideto Kazawa
Google
{jasonkb|slav|ryanmcd|och|talbot|ichikawa|seno|kazawa}@google.com
Abstract
We propose a simple training regime that can
improve the extrinsic performance of a parser,
given only a corpus of sentences and a way
to automatically evaluate the extrinsic quality
of a candidate parse. We apply our method
to train parsers that excel when used as part
of a reordering component in a statistical ma-
chine translation system. We use a corpus of
weakly-labeled reference reorderings to guide
parser training. Our best parsers contribute
significant improvements in subjective trans-
lation quality while their intrinsic attachment
scores typically regress.
1 Introduction
The field of syntactic parsing has received a great
deal of attention and progress since the creation of
the Penn Treebank (Marcus et al, 1993; Collins,
1997; Charniak, 2000; McDonald et al, 2005;
Petrov et al, 2006; Nivre, 2008). A common?
and valid?criticism, however, is that parsers typi-
cally get evaluated only on Section 23 of the Wall
Street Journal portion of the Penn Treebank. This
is problematic for many reasons. As previously ob-
served, this test set comes from a very narrow do-
main that does not necessarily reflect parser perfor-
mance on text coming from more varied domains
(Gildea, 2001), especially web text (Foster, 2010).
There is also evidence that after so much repeated
testing, parsers are indirectly over-fitting to this set
(Petrov and Klein, 2007). Furthermore, parsing was
never meant as a stand-alone task, but is rather a
means to an end, towards the goal of building sys-
tems that can process natural language input.
This is not to say that parsers are not used in larger
systems. All to the contrary, as parsing technology
has become more mature, parsers have become ef-
ficient and accurate enough to be useful in many
natural language processing systems, most notably
in machine translation (Yamada and Knight, 2001;
Galley et al, 2004; Xu et al, 2009). While it has
been repeatedly shown that using a parser can bring
net gains on downstream application quality, it is of-
ten unclear how much intrinsic parsing accuracy ac-
tually matters.
In this paper we try to shed some light on this is-
sue by comparing different parsers in the context of
machine translation (MT). We present experiments
on translation from English to three Subject-Object-
Verb (SOV) languages,1 because those require ex-
tensive syntactic reordering to produce grammatical
translations. We evaluate parse quality on a num-
ber of extrinsic metrics, including word reordering
accuracy, BLEU score and a human evaluation of fi-
nal translation quality. We show that while there is
a good correlation between those extrinsic metrics,
parsing quality as measured on the Penn Treebank
is not a good indicator of the final downstream ap-
plication quality. Since the word reordering metric
can be computed efficiently offline (i.e. without the
use of the final MT system), we then propose to tune
parsers specifically for that metric, with the goal of
improving the performance of the overall system.
To this end we propose a simple training regime
1We experiment with Japanese, Korean and Turkish, but
there is nothing language specific in our approach.
183
which we refer to as targeted self-training (Sec-
tion 2). Similar to self-training, a baseline model
is used to produce predictions on an unlabeled data
set. However, rather than directly training on the
output of the baseline model, we generate a list of
hypotheses and use an external signal to select the
best candidate. The selected parse trees are added
to the training data and the model is then retrained.
The experiments in Section 5 show that this simple
procedure noticeably improves our parsers for the
task at hand, resulting in significant improvements
in downstream translation quality, as measured in a
human evaluation on web text.
This idea is similar in vein to McClosky. et al
(2006) and Petrov et al (2010), except that we use an
extrinsic quality metric instead of a second parsing
model for making the selection. It is also similar to
Burkett and Klein (2008) and Burkett et al (2010),
but again avoiding the added complexity introduced
by the use of additional (bilingual) models for can-
didate selection.
It should be noted that our extrinsic metric is com-
puted from data that has been manually annotated
with reference word reorderings. Details of the re-
ordering metric and the annotated data we used are
given in Sections 3 and 4. While this annotation re-
quires some effort, such annotations are much easier
to obtain than full parse trees. In our experiments
in Section 6 we show that we can obtain similar
improvements on downstream translation quality by
targeted self-training with weakly labeled data (in
form of word reorderings), as with training on the
fully labeled data (with full syntactic parse trees).
2 Targeted Self-Training
Our technique for retraining a baseline parser is an
extension of self-training. In standard parser self-
training, one uses the baseline parsing model to
parse a corpus of sentences, and then adds the 1-best
output of the baseline parser to the training data. To
target the self-training, we introduce an additional
step, given as Algorithm 1. Instead of taking the 1-
best parse, we produce a ranked n-best list of predic-
tions and select the parser which gives the best score
according to an external evaluation function. That
is, instead of relying on the intrinsic model score,
we use an extrinsic score to select the parse towards
Algorithm 1 Select parse that maximizes an extrin-
sic metric.
Input: baseline parser B
Input: sentence S
Input: function COMPUTEEXTRINSIC(parse P )
Output: a parse for the input sentence
Pn = {P1, . . . , Pn} ? n-best parses of S by B
maxScore = 0
bestParse = ?
for k = 1 to n do
extrinsicScore = COMPUTEEXTRINSIC(Pk)
if extrinsicScore > maxScore then
maxScore = extrinsicScore
bestParse = Pk
end if
end for
return bestParse
which to update. In the case of a tie, we prefer the
parse ranked most highly in the n-best list.
The motivation of this selection step is that good
performance on the downstream external task, mea-
sured by the extrinsic metric, should be predictive
of an intrinsically good parse. At the very least,
even if the selected parse is not syntactically cor-
rect, or even if it goes against the original treebank-
ing guidelines, it results in a higher extrinsic score
and should therefore be preferred.
One could imagine extending this framework by
repeatedly running self-training on successively im-
proving parsers in an EM-style algorithm. A recent
work by Hall et al (2011) on training a parser with
multiple objective functions investigates a similar
idea in the context of online learning.
In this paper we focus our attention on machine
translation as the final application, but one could en-
vision applying our techniques to other applications
such as information extraction or question answer-
ing. In particular, we explore one application of
targeted self-training, where computing the extrin-
sic metric involves plugging the parse into an MT
system?s reordering component and computing the
accuracy of the reordering compared to a reference
word order. We now direct our attention to the de-
tails of this application.
184
3 The MT Reordering Task
Determining appropriate target language word or-
der for a translation is a fundamental problem in
MT. When translating between languages with sig-
nificantly different word order such as English and
Japanese, it has been shown that metrics which ex-
plicitly account for word-order are much better cor-
related with human judgments of translation qual-
ity than those that give more weight to word choice,
like BLEU (Lavie and Denkowski, 2009; Isozaki et
al., 2010a; Birch and Osborne, 2010). This demon-
strates the importance of getting reordering right.
3.1 Reordering as a separately evaluable
component
One way to break down the problem of translat-
ing between languages with different word order
is to handle reordering and translation separately:
first reorder source-language sentences into target-
language word order in a preprocessing step, and
then translate the reordered sentences. It has been
shown that good results can be achieved by reorder-
ing each input sentence using a series of tree trans-
formations on its parse tree. The rules for tree
transformation can be manually written (Collins et
al., 2005; Wang, 2007; Xu et al, 2009) or auto-
matically learned (Xia and McCord, 2004; Habash,
2007; Genzel, 2010).
Doing reordering as a preprocessing step, sepa-
rately from translation, makes it easy to evaluate re-
ordering performance independently from the MT
system. Accordingly, Talbot et al (2011) present a
framework for evaluating the quality of reordering
separately from the lexical choice involved in trans-
lation. They propose a simple reordering metric
based on METEOR?s reordering penalty (Lavie and
Denkowski, 2009). This metric is computed solely
on the source language side. To compute it, one
takes the candidate reordering of the input sentence
and partitions it into a set C of contiguous spans
whose content appears contiguously in the same or-
der in the reference. The reordering score is then
computed as
?(esys, eref) = 1?
|C| ? 1
|e| ? 1 .
This metric assigns a score between 0 and 1 where 1
indicates that the candidate reordering is identical to
the reference and 0 indicates that no two words that
are contiguous in the candidate reordering are con-
tiguous in the reference. For example, if a reference
reordering is A B C D E, candidate reordering A
B E C D would get score 1?(3?1)/(5?1) = 0.5.
Talbot et al (2011) show that this reordering score
is strongly correlated with human judgment of trans-
lation quality. Furthermore, they propose to evalu-
ate the reordering quality of an MT system by com-
puting its reordering score on a test set consisting
of source language sentences and their reference re-
orderings. In this paper, we take the same approach
for evaluation, and in addition, we use corpora of
source language sentences and their reference re-
orderings for training the system, not just testing
it. We describe in more detail how the reference re-
ordering data was prepared in Section 4.1.
3.2 Reordering quality as predictor of parse
quality
Figure 1 gives concrete examples of good and bad
reorderings of an English sentence into Japanese
word order. It shows that a bad parse leads to a bad
reordering (lacking inversion of verb ?wear? and ob-
ject ?sunscreen?) and a low reordering score. Could
we flip this causality around, and perhaps try to iden-
tify a good parse tree based on its reordering score?
With the experiments in this paper, we show that in-
deed a high reordering score is predictive of the un-
derlying parse tree that was used to generate the re-
ordering being a good parse (or, at least, being good
enough for our purpose).
In the case of translating English to Japanese or
another SOV language, there is a large amount of
reordering required, but with a relatively small num-
ber of reordering rules one can cover a large pro-
portion of reordering phenomena. Isozaki et al
(2010b), for instance, were able to get impressive
English?Japanese results with only a single re-
ordering rule, given a suitable definition of a head.
Hence, the reordering task depends crucially on a
correct syntactic analysis and is extremely sensitive
to parser errors.
185
4 Experimental Setup
4.1 Treebank data
In our experiments the baseline training corpus is
the Wall Street Journal (WSJ) section of the Penn
Treebank (Marcus et al, 1993) using standard train-
ing/development/testing splits. We converted the
treebank to match the tokenization expected by our
MT system. In particular, we split tokens containing
hyphens into multiple tokens and, somewhat sim-
plistically, gave the original token?s part-of-speech
tag to all newly created tokens. In Section 6 we
make also use of the Question Treebank (QTB)
(Judge et al, 2006), as a source of syntactically an-
notated out-of-domain data. Though we experiment
with both dependency parsers and phrase structure
parsers, our MT system assumes dependency parses
as input. We use the Stanford converter (de Marneffe
et al, 2006) to convert phrase structure parse trees to
dependency parse trees (for both treebank trees and
predicted trees).
4.2 Reference reordering data
We aim to build an MT system that can accurately
translate typical English text that one finds on the
Internet to SOV langauges. To this end, we ran-
domly sampled 13595 English sentences from the
web and created Japanese-word-order reference re-
orderings for them. We split the sentences arbitrarily
into a 6268-sentence Web-Train corpus and a 7327-
sentence Web-Test corpus.
To make the reference alignments we used the
technique suggested by Talbot et al (2011): ask
annotators to translate each English sentence to
Japanese extremely literally and annotate which En-
glish words align to which Japanese words. Golden
reference reorderings can be made programmati-
cally from these annotations. Creating a large set
of reference reorderings is straightforward because
annotators need little special background or train-
ing, as long as they can speak both the source and
target languages. We chose Japanese as the target
language through which to create the English refer-
ence reorderings because we had access to bilingual
annotators fluent in English and Japanese.
Good parse
Reordered:
15 or greater of an SPF has that sunscreen Wear
Reordering score: 1.0 (matches reference)
Bad parse
Reordered:
15 or greater of an SPF has that Wear sunscreen
Reordering score: 0.78 (?Wear? is out of place)
Figure 1: Examples of good and bad parses and cor-
responding reorderings for translation from English to
Japanese. The good parse correctly identifies ?Wear? as
the main verb and moves it to the end of the sentence; the
bad parse analyses ?Wear sunscreen? as a noun phrase
and does not reorder it. This example was one of the
wins in the human evaluation of Section 5.2.
4.3 Parsers
The core dependency parser we use is an implemen-
tation of a transition-based dependency parser using
an arc-eager transition strategy (Nivre, 2008). The
parser is trained using the averaged perceptron algo-
rithm with an early update strategy as described in
Zhang and Clark (2008). The parser uses the fol-
lowing features: word identity of the first two words
on the buffer, the top word on the stack and the head
of the top word on the stack (if available); part-of-
speech identities of the first four words on the buffer
and top two words on the stack; dependency arc la-
bel identities for the top word on the stack, the left
and rightmost modifier of the top word on the stack,
and the leftmost modifier of the first word in the
buffer. We also include conjunctions over all non-
lexical features.
We also give results for the latent variable parser
(a.k.a. BerkeleyParser) of Petrov et al (2006). We
convert the constituency trees output by the Berke-
leyParser to labeled dependency trees using the same
procedure that is applied to the treebanks.
While the BerkeleyParser views part-of-speech
(POS) tagging as an integral part of parsing, our
dependency parser requires the input to be tagged
186
with a separate POS tagger. We use the TnT tag-
ger (Brants, 2000) in our experiments, because of
its efficiency and ease of use. Tagger and parser are
always trained on the same data.
For all parsers, we lowercase the input at train and
test time. We found that this improves performance
in parsing web text. In addition to general upper-
case/lowercase noisiness of the web text negatively
impacting scores, we found that the baseline case-
sensitive parsers are especially bad at parsing imper-
ative sentences, as discussed in Section 5.3.2.
4.4 Reordering rules
In this paper we focus on English to Japanese, Ko-
rean, and Turkish translation. We use a superset of
the reordering rules proposed by Xu et al (2009),
which flatten a dependency tree into SOV word or-
der that is suitable for all three languages. The rules
define a precedence order for the dependents of each
part of speech. For example, a slightly simplified
version of the precedence order of child labels for
a verbal head HEADVERB is: advcl, nsubj, prep,
[other children], dobj, prt, aux, neg, HEADVERB,
mark, ref, compl.
Alternatively, we could have used an automatic
reordering-rule learning framework like that of Gen-
zel (2010). Because the reordering accuracy met-
ric can be computed for any source/target language
pair, this would have made our approach language
completely independent and applicable to any lan-
guage pair. We chose to use manually written rules
to eliminate the variance induced by the automatic
reordering-rule learning framework.
4.5 MT system
We carried out all our translation experiments on a
state-of-the-art phrase-based statistical MT system.
During both training and testing, the system reorders
source-language sentences in a preprocessing step
using the above-mentioned rules. During decoding,
we used an allowed jump width of 4 words. In ad-
dition to the regular distance distortion model, we
incorporate a maximum entropy based lexicalized
phrase reordering model (Zens and Ney, 2006) as
a feature used in decoding.
Overall for decoding, we use between 20 to
30 features, whose weights are optimized using
MERT (Och, 2003). All experiments for a given lan-
guage pair use the same set of MERT weights tuned
on a system using a separate parser (that is neither
the baseline nor the experiment parser). This po-
tentially underestimates the improvements that can
be obtained, but also eliminates MERT as a pos-
sible source of improvement, allowing us to trace
back improvements in translation quality directly to
parser changes.2
For parallel training data, we use a custom collec-
tion of parallel documents. They come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. For all language pairs, we
trained on approximately 300 million source words
each.
5 Experiments Reordering Web Text
We experimented with parsers trained in three dif-
ferent ways:
1. Baseline: trained only on WSJ-Train.
2. Standard self-training: trained on WSJ-Train
and 1-best parse of the Web-Train set by base-
line parser.
3. Targeted self-training: trained on WSJ-Train
and, for each sentence in Web-Train, the parse
from the baseline parser?s 512-best list that
when reordered gives the highest reordering
score.3
5.1 Standard self-training vs targeted
self-training
Table 1 shows that targeted self-training on Web-
Train significantly improves Web-Test reordering
score more than standard self-training for both the
shift-reduce parser and for the BerkeleyParser. The
reordering score is generally divorced from the at-
tachment scores measured on the WSJ-Test tree-
bank: for the shift-reduce parser, Web-Test reorder-
ing score and WSJ-Test labeled attachment score
2We also ran MERT on all systems and the pattern of im-
provement is consistent, but sometimes the improvement is big-
ger or smaller after MERT. For instance, the BLEU delta for
Japanese is +0.0030 with MERT on both sides as opposed to
+0.0025 with no MERT.
3We saw consistent but diminishing improvements as we in-
creased the size of the n-best list.
187
Parser Web-Test reordering WSJ-Test LAS
Shift-reduce WSJ baseline 0.757 85.31%
+ self-training 1x 0.760 85.26%
+ self-training 10x 0.756 84.14%
+ targeted self-training 1x 0.770 85.19%
+ targeted self-training 10x 0.777 84.48%
Berkeley WSJ baseline 0.780 88.66%
+ self-training 1x 0.785 89.21%
+ targeted self-training 1x 0.790 89.32%
Table 1: English?Japanese reordering scores on Web-Test for standard self-training and targeted self-training on
Web-Train. Label ?10x? indicates that the self-training data was weighted 10x relative to the WSJ training data.
Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly different
from each other within the same group.
English to BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.1777 0.1802 2.56 2.69 yes (at 95% level)
Korean 0.3229 0.3259 2.61 2.70 yes (at 90% level)
Turkish 0.1344 0.1370 2.10 2.20 yes (at 95% level)
Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only the
parser between systems. ?WSJ-only? corresponds to the baseline WSJ-only shift-reduce parser; ?Targeted? corre-
sponds to the Web-Train targeted self-training 10x shift-reduce parser.
(LAS) are anti-correlated, but for BerkeleyParser
they are correlated. Interestingly, weighting the self-
training data more seems to have a negative effect on
both metrics.4
One explanation for the drops in LAS is that some
parts of the parse tree are important for downstream
reordering quality while others are not (or only to
a lesser extent). Some distinctions between labels
become less important; for example, arcs labeled
?amod? and ?advmod? are transformed identically
by the reordering rules. Some semantic distinctions
also become less important; for example, any sane
interpretation of ?red hot car? would be reordered
the same, that is, not at all.
5.2 Translation quality improvement
To put the improvement of the MT system in terms
of BLEU score (Papineni et al, 2002), a widely used
metric for automatic MT evaluation, we took 5000
sentences from Web-Test and had humans gener-
ate reference translations into Japanese, Korean, and
4We did not attempt this experiment for the BerkeleyParser
since training was too slow.
Turkish. We then trained MT systems varying only
the parser used for reordering in training and decod-
ing. Table 2 shows that targeted self-training data
increases BLEU score for translation into all three
languages.
In addition to BLEU increase, a side-by-side hu-
man evaluation on 500 sentences (sampled from
the 5000 used to compute BLEU scores) showed
a statistically significant improvement for all three
languages (see again Table 2). For each sen-
tence, we asked annotators to simultaneously score
both translations from 0 to 6, with guidelines
that 6=?Perfect?, 4=?Most Meaning/Grammar?,
2=?Some Meaning/Grammar?, 0=?Nonsense?. We
computed confidence intervals for the average score
difference using bootstrap resampling; a difference
is significant if the two-sided confidence interval
does not include 0.
5.3 Analysis
As the divergence between the labeled attachment
score on the WSJ-Test data and the reordering score
on the WSJ-Test data indicates, parsing web text
188
Parser Click as N Click as V Imperative rate
case-sensitive shift-reduce WSJ-only 74 0 6.3%
case-sensitive shift-reduce + Web-Train targeted self-training 75 0 10.5%
case-insensitive shift-reduce WSJ-only 75 0 10.3%
case-insensitive shift-reduce + Web-Train targeted self-training 75 0 11.6%
Berkeley WSJ-only 35 35 11.9%
Berkeley + Web-Train targeted self-training 13 58 12.5%
(WSJ-Train) 1 0 0.7%
Table 3: Counts on Web-Test of ?click? tagged as a noun and verb and percentage of sentences parsed imperatively.
poses very different challenges compared to parsing
newswire. We show how our method improves pars-
ing performance and reordering performance on two
examples: the trendy word ?click? and imperative
sentences.
5.3.1 Click
The word ?click? appears only once in the train-
ing portion of the WSJ (as a noun), but appears many
times in our Web test data. Table 3 shows the distri-
bution of part-of-speech tags that different parsers
assign to ?click?. The WSJ-only parsers tag ?click?
as a noun far too frequently. The WSJ-only shift-
reduce parser refuses to tag ?click? as a verb even
with targeted self-training, but BerkeleyParser does
learn to tag ?click? more often as a verb.
It turns out that the shift-reduce parser?s stub-
bornness is not due to a fundamental problem of
the parser, but due to an artifact in TnT. To in-
crease speed, TnT restricts the choices of tags for
known words to previously-seen tags. This causes
the parser?s n-best lists to never hypothesize ?click?
as a verb, and self-training doesn?t click no matter
how targeted it is. This shows that the targeted self-
training approach heavily relies on the diversity of
the baseline parser?s n-best lists.
It should be noted here that it would be easy to
combine our approach with the uptraining approach
of Petrov et al (2010). The idea would be to use the
BerkeleyParser to generate the n-best lists; perhaps
we could call this targeted uptraining. This way, the
shift-reduce parser could benefit both from the gen-
erally higher quality of the parse trees produced by
the BerkeleyParser, as well as from the information
provided by the extrinsic scoring function.
5.3.2 Imperatives
As Table 3 shows, the WSJ training set contains
only 0.7% imperative sentences.5 In contrast, our
test sentences from the web contain approximately
10% imperatives. As a result, parsers trained exclu-
sively on the WSJ underproduce imperative parses,
especially a case-sensitive version of the baseline.
Targeted self-training helps the parsers to predict im-
perative parses more often.
Targeted self-training works well for generating
training data with correctly-annotated imperative
constructions because the reordering of main sub-
jects and verbs in an SOV language like Japanese
is very distinct: main subjects stay at the begin-
ning of the sentence, and main verbs are reordered
to the end of the sentence. It is thus especially easy
to know whether an imperative parse is correct or
not by looking at the reference reordering. Figure 1
gives an example: the bad (WSJ-only) parse doesn?t
catch on to the imperativeness and gets a low re-
ordering score.
6 Targeted Self-Training vs Training on
Treebanks for Domain Adaptation
If task-specific annotation is cheap, then it is rea-
sonable to consider whether we could use targeted
self-training to adapt a parser to a new domain as
a cheaper alternative to making new treebanks. For
example, if we want to build a parser that can reorder
question sentences better than our baseline WSJ-
only parser, we have these two options:
1. Manually construct PTB-style trees for 2000
5As an approximation, we count every parse that begins with
a root verb as an imperative.
189
questions and train on the resulting treebank.
2. Create reference reorderings for 2000 questions
and then do targeted self-training.
To compare these approaches, we created reference
reordering data for our train (2000 sentences) and
test (1000 sentences) splits of the Question Tree-
bank (Judge et al, 2006). Table 4 shows that both
ways of training on QTB-Train sentences give sim-
ilarly large improvements in reordering score on
QTB-Test. Table 5 confirms that this corresponds
to very large increases in English?Japanese BLEU
score and subjective translation quality. In the hu-
man side-by-side comparison, the baseline transla-
tions achieved an average score of 2.12, while the
targeted self-training translations received a score of
2.94, where a score of 2 corresponds to ?some mean-
ing/grammar? and ?4? corresponds to ?most mean-
ing/grammar?.
But which of the two approaches is better? In
the shift-reduce parser, targeted self-training gives
higher reordering scores than training on the tree-
bank, and in BerkeleyParser, the opposite is true.
Thus both approaches produce similarly good re-
sults. From a practical perspective, the advantage of
targeted self-training depends on whether the extrin-
sic metric is cheaper to calculate than treebanking.
For MT reordering, making reference reorderings is
cheap, so targeted self-training is relatively advanta-
geous.
As before, we can examine whether labeled at-
tachment score measured on the test set of the
QTB is predictive of reordering quality. Table 4
shows that targeted self-training raises LAS from
64.78?69.17%. But adding the treebank leads
to much larger increases, resulting in an LAS of
84.75%, without giving higher reordering score. We
can conclude that high LAS is not necessary to
achieve top reordering scores.
Perhaps our reordering rules are somehow defi-
cient when it comes to reordering correctly-parsed
questions, and as a result the targeted self-training
process steers the parser towards producing patho-
logical trees with little intrinsic meaning. To explore
this possibility, we computed reordering scores after
reordering the QTB-Test treebank trees directly. Ta-
ble 4 shows that this gives reordering scores similar
to those of our best parsers. Therefore it is at least
possible that the targeted self-training process could
have resulted in a parser that achieves high reorder-
ing score by producing parses that look like those in
the QuestionBank.
7 Related Work
Our approach to training parsers for reordering is
closely related to self/up-training (McClosky. et al,
2006; Petrov et al, 2010). However, unlike uptrain-
ing, our method does not use only the 1-best output
of the first-stage parser, but has access to the n-best
list. This makes it similar to the work of McClosky.
et al (2006), except that we use an extrinsic metric
(MT reordering score) to select a high quality parse
tree, rather than a second, reranking model that has
access to additional features.
Targeted self-training is also similar to the re-
training of Burkett et al (2010) in which they
jointly parse unannotated bilingual text using a mul-
tiview learning objective, then retrain the monolin-
gual parser models to include each side of the jointly
parsed bitext as monolingual training data. Our ap-
proach is different in that it doesn?t use a second
parser and bitext to guide the creation of new train-
ing data, and instead relies on n-best lists and an
extrinsic metric.
Our method can be considered an instance of
weakly or distantly supervised structured prediction
(Chang et al, 2007; Chang et al, 2010; Clarke et al,
2010; Ganchev et al, 2010). Those methods attempt
to learn structure models from related external sig-
nals or aggregate data statistics. This work differs
in two respects. First, we use the external signals
not as explicit constraints, but to compute an ora-
cle score used to re-rank a set of parses. As such,
there are no requirements that it factor by the struc-
ture of the parse tree and can in fact be any arbitrary
metric. Second, our final objective is different. In
weakly/distantly supervised learning, the objective
is to use external knowledge to build better struc-
tured predictors. In our case this would mean using
the reordering metric as a means to train better de-
pendency parsers. Our objective, on the other hand,
is to use the extrinsic metric to train parsers that are
specifically better at the reordering task, and, as a re-
sult, better suited for MT. This makes our work more
in the spirit of Liang et al (2006), who train a per-
190
Parser QTB-Test reordering QTB-Test LAS
Shift-reduce WSJ baseline 0.663 64.78%
+ treebank 1x 0.704 77.12%
+ treebank 10x 0.768 84.75%
+ targeted self-training 1x 0.746 67.84%
+ targeted self-training 10x 0.779 69.17%
Berkeley WSJ baseline 0.733 76.50%
+ treebank 1x 0.800 87.79%
+ targeted self-training 1x 0.775 80.64%
(using treebank trees directly) 0.788 100%
Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training on
QTB-Train.
English to QTB-Test BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level)
Table 5: BLEU scores and human evaluation results for English?Japanese translation of the QTB-Test corpus, varying
only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training
10x shift-reduce parser.
ceptron model for an end-to-end MT system where
the alignment parameters are updated based on se-
lecting an alignment from a n-best list that leads to
highest BLEU score. As mentioned earlier, this also
makes our work similar to Hall et al (2011) who
train a perceptron algorithm on multiple objective
functions with the goal of producing parsers that are
optimized for extrinsic metrics.
It has previously been observed that parsers of-
ten perform differently for downstream applications.
Miyao et al (2008) compared parser quality in the
biomedical domain using a protein-protein interac-
tion (PPI) identification accuracy metric. This al-
lowed them to compare the utility of extant depen-
dency parsers, phrase structure parsers, and deep
structure parsers for the PPI identification task. One
could apply the targeted self-training technique we
describe to optimize any of these parsers for the PPI
task, similar to how we have optimized our parser
for the MT reordering task.
8 Conclusion
We introduced a variant of self-training that targets
parser training towards an extrinsic evaluation met-
ric. We use this targeted self-training approach to
train parsers that improve the accuracy of the word
reordering component of a machine translation sys-
tem. This significantly improves the subjective qual-
ity of the system?s translations from English into
three SOV languages. While the new parsers give
improvements in these external evaluations, their in-
trinsic attachment scores go down overall compared
to baseline parsers trained only on treebanks. We
conclude that when using a parser as a component
of a larger external system, it can be advantageous
to incorporate an extrinsic metric into parser train-
ing and evaluation, and that targeted self-training is
an effective technique for incorporating an extrinsic
metric into parser training.
References
A. Birch and M. Osborne. 2010. LRscore for evaluating
lexical and reordering quality in MT. In ACL-2010
WMT.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In EMNLP ?08.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In CoNLL ?10.
191
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL
?07.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In ICML ?10.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL ?10.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In ACL
?05.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL ?97.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In HLT-NAACL ?04.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
COLING ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In MTS ?07.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In EMNLP ?11.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010a. Automatic evaluation of translation quality for
distant language pairs. In EMNLP ?10.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b.
Head finalization: A simple reordering rule for SOV
languages. In ACL-2010 WMT.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
Bank: creating a corpus of parse-annotated questions.
In ACL ?06.
A. Lavie and M. Denkowski. 2009. The Meteor metric
for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL ?06.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
D. McClosky., E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ?06.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsu-
jii. 2008. Task-oriented evaluation of syntactic parsers
and their representations. In ACL ?08.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL ?02.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov, P. Chang, and M. Ringgaard H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing.
In EMNLP ?10.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalua-
tion framework for machine translation reordering. In
EMNLP-2011 WMT.
C. Wang. 2007. Chinese syntactic reordering for statisti-
cal machine translation. In EMNLP ?07.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Coling ?04.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In NAACL-HLT ?09.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL ?01.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In NAACL-
06 WMT.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In EMNLP ?08.
192
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1489?1499,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training dependency parsers by jointly optimizing multiple objectives
Keith Hall Ryan McDonald Jason Katz-Brown Michael Ringgaard
Google Research
{kbhall|ryanmcd|jasonkb|ringgaard}@google.com
Abstract
We present an online learning algorithm for
training parsers which allows for the inclusion
of multiple objective functions. The primary
example is the extension of a standard su-
pervised parsing objective function with addi-
tional loss-functions, either based on intrinsic
parsing quality or task-specific extrinsic mea-
sures of quality. Our empirical results show
how this approach performs for two depen-
dency parsing algorithms (graph-based and
transition-based parsing) and how it achieves
increased performance on multiple target tasks
including reordering for machine translation
and parser adaptation.
1 Introduction
The accuracy and speed of state-of-the-art depen-
dency parsers has motivated a resumed interest in
utilizing the output of parsing as an input to many
downstream natural language processing tasks. This
includes work on question answering (Wang et al,
2007), sentiment analysis (Nakagawa et al, 2010),
MT reordering (Xu et al, 2009), and many other
tasks. In most cases, the accuracy of parsers de-
grades when run on out-of-domain data (Gildea,
2001; McClosky et al, 2006; Blitzer et al, 2006;
Petrov et al, 2010). But these accuracies are mea-
sured with respect to gold-standard out-of-domain
parse trees. There are few tasks that actually depend
on the complete parse tree. Furthermore, when eval-
uated on a downstream task, often the optimal parse
output has a model score lower than the best parse
as predicted by the parsing model. While this means
that we are not properly modeling the downstream
task in the parsers, it also means that there is some
information from small task or domain-specific data
sets which could help direct our search for optimal
parameters during parser training. The goal being
not necessarily to obtain better parse performance,
but to exploit the structure induced from human la-
beled treebank data while targeting specific extrinsic
metrics of quality, which can include task specific
metrics or external weak constraints on the parse
structure.
One obvious approach to this problem is to em-
ploy parser reranking (Collins, 2000). In such a
setting, an auxiliary reranker is added in a pipeline
following the parser. The standard setting involves
training the base parser and applying it to a devel-
opment set (this is often done in a cross-validated
jack-knife training framework). The reranker can
then be trained to optimize for the downstream or
extrinsic objective. While this will bias the reranker
towards the target task, it is limited by the oracle
performance of the original base parser.
In this paper, we propose a training algorithm for
statistical dependency parsers (Ku?bler et al, 2009)
in which a single model is jointly optimized for a
regular supervised training objective over the tree-
bank data as well as a task-specific objective ? or
more generally an extrinsic objective ? on an ad-
ditional data set. The case where there are both
gold-standard trees and a task-specific objective for
the entire training set is a specific instance of the
larger problem that we address here. Specifically,
the algorithm takes the form of an online learner
where a training instance is selected and the param-
1489
eters are optimized based on the objective function
associated with the instance (either intrinsic or ex-
trinsic), thus jointly optimizing multiple objectives.
An update schedule trades-off the relative impor-
tance of each objective function. We call our algo-
rithm augmented-loss training as it optimizes mul-
tiple losses to augment the traditional supervised
parser loss.
There have been a number of efforts to exploit
weak or external signals of quality to train better pre-
diction models. This includes work on generalized
expectation (Mann and McCallum, 2010), posterior
regularization (Ganchev et al, 2010) and constraint
driven learning (Chang et al, 2007; Chang et al,
2010). The work of Chang et al (2007) on constraint
driven learning is perhaps the closest to our frame-
work and we draw connections to it in Section 5.
In these studies the typical goal is to use the weak
signal to improve the structured prediction models
on the intrinsic evaluation metrics. For our setting
this would mean using weak application specific sig-
nals to improve dependency parsing. Though we
explore such ideas in our experiments, in particular
for semi-supervised domain adaptation, we are pri-
marily interested in the case where the weak signal
is precisely what we wish to optimize, but also de-
sire the benefit from using both data with annotated
parse structures and data specific to the task at hand
to guide parser training.
In Section 2 we outline the augmented-loss algo-
rithm and provide a convergence analysis. In Sec-
tion 3 and 4 we present a set of experiments defin-
ing diffent augmented losses covering a task-specific
extrinsic loss (MT reordering), a domain adapta-
tion loss, and an alternate intrinsic parser loss. In
all cases we show the augmented-loss framework
can lead to significant gains in performance. In
Section 5 we tie our augmented-loss algorithm to
other frameworks for encoding auxiliary informa-
tion and/or joint objective optimization.
2 Methodology
We present the augmented-loss algorithm in the con-
text of the structured perceptron. The structured
perceptron (Algorithm 1) is an on-line learning al-
gorithm which takes as input: 1) a set of training
examples di = (xi, yi) consisting of an input sen-
Algorithm 1 Structured Perceptron
{Input data sets: D = {d1 = (x1, y1) . . . dN = (xN , yN )}}
{Input 0/1 loss: L(F?(x), y) = [F?(x) 6= y ? 1 : 0]}
{Let: F?(x) = arg maxy?Y ? ? ?(y)}
{Initialize model parameters: ? = ~0}
repeat
for i = 1 . . . N do
{Compute structured loss}
y?i = F?(xi)
if L(y?i, yi) > 0 then
{Update model Parameters}
? = ? + ?(yi)? ?(y?i)
end if
end for
until converged
{Return model ?}
tence xi and an output yi; and 2) a loss-function,
L(y?, y), that measures the cost of predicting out-
put y? relative to the gold standard y and is usu-
ally the 0/1 loss (Collins, 2002). For dependency
parser training, this set-up consists of input sen-
tences x and the corresponding gold dependency
tree y ? Yx, where Yx is the space of possible
parse trees for sentence x. In the perceptron setting,
F?(x) = arg maxy?Yx ? ??(y) where ? is mappingfrom a parse tree y for sentence x to a high dimen-
sional feature space. Learning proceeds by predict-
ing a structured output given the current model, and
if that structure is incorrect, updating the model: re-
warding features that fire in the gold-standard ?(yi),
and discounting features that fire in the predicted
output, ?(y?i).
The structured perceptron, as given in Algo-
rithm 1, only updates when there is a positive loss,
meaning that there was a prediction mistake. For
the moment we will abstract away from details such
as the precise definition of F (x) and ?(y). We
will show in the next section that our augmented-
loss method is general and can be applied to any de-
pendency parsing framework that can be trained by
the perceptron algorithm, such as transition-based
parsers (Nivre, 2008; Zhang and Clark, 2008) and
graph-based parsers (McDonald et al, 2005).
2.1 Augmented-Loss Training
The augmented-loss training algorithm that we pro-
pose is based on the structured perceptron; however,
the augmented-loss training framework is a general
1490
mechanism to incorporate multiple loss functions in
online learner training. Algorithm 2 is the pseudo-
code for the augmented-loss structured perceptron
algorithm. The algorithm is an extension to Algo-
rithm 1 where there are 1) multiple loss functions
being evaluated L1, . . . , LM ; 2) there are multiple
datasets associated with each of these loss functions
D1, . . . ,DM ; and 3) there is a schedule for pro-
cessing examples from each of these datasets, where
Sched(j, i) is true if the jth loss function should be
updated on the ith iteration of training. Note that
for data point dji = (x, y), which is the ith training
instance of the jth data set, that y does not neces-
sarily have to be a dependency tree. It can either
be a task-specific output of interest, a partial tree, or
even null, in the case where learning will be guided
strictly by the loss Lj . The training algorithm is ef-
fectively the same as the perceptron, the primary dif-
ference is that if Lj is an extrinsic loss, we cannot
compute the standard updates since we do not nec-
essarily know the correct parse (the line indicated by
?). Section 2.2 shows one method for updating the
parser parameters for extrinsic losses.
In the experiments in this paper, we only consider
the case where there are two loss functions: a super-
vised dependency parsing labeled-attachment loss;
and an additional loss, examples of which are pre-
sented in Section 3.
2.2 Inline Ranker Training
In order to make Algorithm 2 more concrete, we
need a way of defining the loss and resulting pa-
rameter updates for the case when Lj is not a stan-
dard supervised parsing loss (? from Algorithm 2).
Assume that we have a cost function C(xi, y?, yi)
which, given a training example (xi, yi) will give a
score for a parse y? ? Yxi relative to some output
yi. While we can compute the score for any parse,
we are unable to determine the features associated
with the optimal parse, as yi need not be a parse
tree. For example, consider a machine translation re-
ordering system which uses the parse y? to reorder the
words of xi, the optimal reordering being yi. Then
C(xi, y?, yi) is a reordering cost which is large if the
predicted parse induces a poor reordering of xi.
We propose a general purpose loss function which
is based on parser k-best lists. The inline reranker
uses the currently trained parser model ? to parse
Algorithm 2 Augmented-Loss Perceptron
{Input data sets}:
D1 = {d11 = (x11, y11) . . . d1N1 = (x1N1 , y1N1)},
. . .
DM = {dM1 = (xM1 , yM1 ) . . . dMNM = (xMNM , yMNM )}
{Input loss functions: L1 . . . LM}
{Initialize indexes: c1 . . . cM = ~0}
{Initialize model parameters: ? = ~0}
i = 0
repeat
for j = 1 . . .M do
{Check whether to update Lj on iteration i}
if Sched(j, i) then
{Compute index of instance ? reset if cj ? N j}
cj = [(cj ? N j) ? 0 : cj + 1]
{Compute structured loss for instance}
if Lj is intrinsic loss then
y? = F?(xjcj )
if Lj(y?, yjcj ) > 0 then
? = ? + ?(yjcj )? ?(y?) {yjcj is a tree}end if
else if Lj is an extrinsic loss then
{See Section 2.2}?
end if
end if
end for
i = i+ 1
until converged
{Return model ?}
the external input, producing a k-best set of parses:
Fk-best? (xi) = {y?1, . . . , y?k}. We can compute the
cost function C(xi, y?, yi) for all y? ? Fk-best? (xi). If
the 1-best parse, y?1, has the lowest cost, then there is
no lower cost parse in this k-best list. Otherwise, the
lowest-cost parse in Fk-best? (xi) is taken to be the
correct output structure yi, and the 1-best parse is
taken to be an incorrect prediction. We can achieve
this by substituting the following into Algorithm 2
at line ?.
Algorithm 3 Reranker Loss
{y?1, . . . , y?k} = Fk-best? (xi)
? = min? C(xjcj , y?? , yjcj ) {? is min const index}
Lj(y?1, yjcj ) = C(xjcj , y?1, yjcj )? C(xjcj , y?? , yjcj )
if Lj(y?1, yjcj ) > 0 then
? = ? + ?(y?? )? ?(y?1)
end if
Again the algorithm only updates when there is
an error ? when the 1-best output has a higher cost
than any other output in the k-best list ? resulting
1491
in positive Lj . The intuition behind this method is
that in the presence of only a cost function and a
k-best list, the parameters will be updated towards
the parse structure that has the lowest cost, which
over time will move the parameters of the model to
a place with low extrinsic loss.
We exploit this formulation of the general-
purpose augmented-loss function as it allows one to
include any extrinsic cost function which is depen-
dent of parses. The scoring function used does not
need to be factored, requiring no internal knowledge
of the function itself. Furthermore, we can apply this
to any parsing algorithm which can generate k-best
lists. For each parse, we must retain the features
associated with the parse (e.g., for transition-based
parsing, the features associated with the transition
sequence resulting in the parse).
There are two significant differences from the in-
line reranker loss function and standard reranker
training. First, we are performing this decision per
example as each data item is processed (this is done
in the inner loop of the Algorithm 2). Second, the
feedback function for selecting a parse is based on
an external objective function. The second point is
actually true for many minimum-error-rate training
scenarios, but in those settings the model is updated
as a post-processing stage (after the base-model is
trained).
2.3 Convergence of Inline Ranker Training
A training setD is loss-separable with margin ? > 0
if there exists a vector u with ?u? = 1 such that
for all y?, y?? ? Yx and (x, y) ? D, if L(y?, y) <
L(y??, y), then u??(y?)?u??(y??) ? ?. Furthermore,
let R ? ||?(y)? ?(y?)||, for all y, y?.
Assumption 1. Assume training set D is loss-
separable with margin ?.
Theorem 1. Given Assumption 1. Letm be the num-
ber of mistakes made when training the perceptron
(Algorithm 2) with inline ranker loss (Algorithm 3)
on D, where a mistake occurs for (x, y) ? D with
parameter vector ? when ?y?j ? F k-best? (x) where
y?j 6= y?1 and L(y?j , y) < L(y?1, y). If training is run
indefinitely, then m ? R2?2 .
Proof. Identical to the standard perceptron proof,
e.g., Collins (2002), by inserting in loss-separability
for normal separability.
Like the original perceptron theorem, this implies
that the algorithm will converge. However, unlike
the original theorem, it does not imply that it will
converge to a parameter vector ? such that for all
(x, y) ? D, if y? = arg maxy? ? ??(y?) then L(y?, y) =
0. Even if we assume for every x there exists an out-
put with zero loss, Theorem 1 still makes no guar-
antees. Consider a training set with one instance
(x, y). Now, set k = 2 for the k-best output list and
let y?1, y?2, and y?3 be the top-3 scoring outputs and
let L(y?1, y) = 1, L(y?2, y) = 2 and L(y?3, y) = 0.
In this case, no updates will ever be made and y?1
will remain unchanged even though it doesn?t have
minimal loss. Consider the following assumption:
Assumption 2. For any parameter vector ? that ex-
ists during training, either 1) for all (x, y) ? D,
L(y?1, y) = 0 (or some optimal minimum loss),
or 2) there exists at least one (x, y) ? D where
?y?j ? F k-best? (x) such that L(y?j , y) < L(y?1, y).
Assumption 2 states that for any ? that exists
during training, but before convergence, there is at
least one example in the training data where k is
large enough to include one output with a lower loss
when y?1 does not have the optimal minimal loss. If
k = ?, then this is the standard perceptron as it
guarantees the optimal loss output to be in the k-best
list. But we are assuming something much weaker
here, i.e., not that the k-best list will include the min-
imal loss output, only a single output with a lower
loss than the current best guess. However, it is strong
enough to show the following:
Theorem 2. Given Assumption 1 and Assumption 2.
Training the perceptron (Algorithm 2) with inline
ranker loss (Algorithm 3) on D 1) converges in fi-
nite time, and 2) produces parameters ? such that
for all (x, y) ? D, if y? = arg maxy? ? ? ?(y?) then
L(y?, y) = 0 (or equivalent minimal loss).
Proof. It must be the case for all (x, y) ? D that
L(y?1, y) = 0 (and y?1 is the argmax) after a finite
amount of time. Otherwise, by Assumption 2, there
exists some x, such that when it is next processed,
there would exist an output in the k-best list that
had a lower loss, which will result in an additional
mistake. Theorem 1 guarantees that this can not
continue indefinitely as the number of mistakes is
bounded.
1492
Thus, the perceptron algorithm will converge to
optimal minimal loss under the assumption that k
is large enough so that the model can keep improv-
ing. Note that this does not mean k must be large
enough to include a zero or minimum loss output,
just large enough to include a better output than
the current best hypothesis. Theorem 2, when cou-
pled with Theorem 1, implies that augmented-loss
learning will make at most R2/?2 mistakes at train-
ing, but does not guarantee the rate at which these
mistakes will be made, only that convergence is fi-
nite, providing that the scheduling time (defined by
Sched()) between seeing the same instance is always
finite, which is always true in our experiments.
This analysis does not assume anything about the
loss L. Every instance (x, y) can use a different loss.
It is only required that the loss for a specific input-
output pair is fixed throughout training. Thus, the
above analysis covers the case where some training
instances use an extrinsic loss and others an intrin-
sic parsing loss. This also suggests more efficient
training methods when extracting the k-best list is
prohibitive. One can parse with k = 2, 4, 8, 16, . . .
until an k is reached that includes a lower loss parse.
It may be the case that for most instances a small
k is required, but the algorithm is doing more work
unnecessarily if k is large.
3 Experimental Set-up
3.1 Dependency Parsers
The augmented-loss framework we present is gen-
eral in the sense that it can be combined with any
loss function and any parser, provided the parser can
be parameterized as a linear classifier, trained with
the perceptron and is capable of producing a k-best
list of trees. For our experiments we focus on two
dependency parsers.
? Transition-based: An implementation of the
transition-based dependency parsing frame-
work (Nivre, 2008) using an arc-eager transi-
tion strategy and are trained using the percep-
tron algorithm as in Zhang and Clark (2008)
with a beam size of 8. Beams with varying
sizes can be used to produce k-best lists. The
features used by all models are: the part-of-
speech tags of the first four words on the buffer
and of the top two words on the stack; the word
identities of the first two words on the buffer
and of the top word on the stack; the word iden-
tity of the syntactic head of the top word on the
stack (if available); dependency arc label iden-
tities for the top word on the stack, the left and
rightmost modifier of the top word on the stack,
and the left most modifier of the first word in
the buffer (if available). All feature conjunc-
tions are included.
? Graph-based: An implementation of graph-
based parsing algorithms with an arc-factored
parameterization (McDonald et al, 2005). We
use the non-projective k-best MST algorithm to
generate k-best lists (Hall, 2007), where k = 8
for the experiments in this paper. The graph-
based parser features used in the experiments
in this paper are defined over a word, wi at po-
sition i; the head of this word w?(i) where ?(i)
provides the index of the head word; and part-
of-speech tags of these words ti. We use the
following set of features similar to McDonald
et al (2005):
isolated features: wi, ti, w?(i), t?(i)
word-tag pairs: (wi, ti); (w?(i), t?(i))
word-head pairs: (wi, w?(i)), (ti, t?(i))
word-head-tag triples: (t?(i), wi, ti)
(w?(i), wi, ti)
(w?(i), t?(i), ti)
(w?(i), t?(i), wi)
tag-neighbourhood: (t?(i), t?(i)+1, ti?1, ti)
(t?(i), t?(i)+1, ti+1, ti)
(t?(i), t?(i)?1, ti?1, ti)
(t?(i), t?(i)?1, ti+1, ti)
between features: ?j i < j < ?(i) || ?(i) < j < i
(t?(i), tj , ti)
arc-direction/length : (i? ?(i) > 0, |i? ?(i)|)
3.2 Data and Tasks
In the next section, we present a set of scoring func-
tions that can be used in the inline reranker loss
framework, resulting in a new augmented-loss for
each one. Augmented-loss learning is then applied
to target a downstream task using the loss functions
to measure gains. We show empirical results for two
extrinsic loss-functions (optimizing for the down-
stream task): machine translation and domain adap-
tation; and for one intrinsic loss-function: an arc-
length parsing score. For some experiments we also
1493
measure the standard intrinsic parser metrics unla-
beled attachment score (UAS) and labeled attach-
ment score (LAS) (Buchholz and Marsi, 2006).
In terms of treebank data, the primary training
corpus is the Penn Wall Street Journal Treebank
(PTB) (Marcus et al, 1993). We also make use
of the Brown corpus, and the Question Treebank
(QTB) (Judge et al, 2006). For PTB and Brown
we use standard training/development/testing splits
of the data. For the QTB we split the data into
three sections: 2000 training, 1000 development,
and 1000 test. All treebanks are converted to de-
pendency format using the Stanford converter v1.6
(de Marneffe et al, 2006).
4 Experiments
4.1 Machine Translation Reordering Score
As alluded to in Section 2.2, we use a reordering-
based loss function to improve word order in a ma-
chine translation system. In particular, we use a sys-
tem of source-side reordering rules which, given a
parse of the source sentence, will reorder the sen-
tence into a target-side order (Collins et al, 2005).
In our experiments we work with a set of English-
Japanese reordering rules1 and gold reorderings
based on human generated correct reordering of an
aligned target sentences. We use a reordering score
based on the reordering penalty from the METEOR
scoring metric. Though we could have used a fur-
ther downstream measure like BLEU, METEOR has
also been shown to directly correlate with translation
quality (Banerjee and Lavie, 2005) and is simpler to
measure.
reorder-score = 1? # chunks? 1# unigrams matched? 1
reorder-cost = 1? reorder-score
All reordering augmented-loss experiments are
run with the same treebank data as the baseline
(the training portions of PTB, Brown, and QTB).
The extrinsic reordering training data consists of
10930 examples of English sentences and their cor-
rect Japanese word-order. We evaluate our results on
an evaluation set of 6338 examples of similarly cre-
ated reordering data. The reordering cost, evaluation
1Our rules are similar to those from Xu et al (2009).
Exact Reorder
trans?PTB + Brown + QTB 35.29 76.49
trans?0.5?aug.-loss 38.71 78.19
trans?1.0?aug.-loss 39.02 78.39
trans?2.0?aug.-loss 39.58 78.67
graph?PTB + Brown + QTB 25.71 69.84
graph?0.5? aug.-loss 28.99 72.23
graph?1.0?aug.-loss 29.99 72.88
graph?2.0?aug.-loss 30.03 73.15
Table 1: Reordering scores for parser-based reordering
(English-to-Japanese). Exact is the number of correctly
reordered sentences. All models use the same treebank-
data (PTB, QTB, and the Brown corpus). Results for
three augmented-loss schedules are shown: 0.5 where for
every two treebank updates we make one augmented-loss
update, 1 is a 1-to-1 mix, and 2 is where we make twice
as many augmented-loss updates as treebank updates.
criteria and data used in our experiments are based
on the work of Talbot et al (2011).
Table 1 shows the results of using the reordering
cost as an augmented-loss to the standard treebank
objective function. Results are presented as mea-
sured by the reordering score as well as a coarse
exact-match score (the number of sentences which
would have correct word-order given the parse and
the fixed reordering rules). We see continued im-
provements as we adjust the schedule to process the
extrinsic loss more frequently, the best result being
when we make two augmented-loss updates for ev-
ery one treebank-based loss update.
4.2 Semi-supervised domain adaptation
Another application of the augmented-loss frame-
work is to improve parser domain portability in the
presence of partially labeled data. Consider, for ex-
ample, the case of questions. Petrov et al (2010)
observed that dependency parsers tend to do quite
poorly when parsing questions due to their lim-
ited exposure to them in the news corpora from
the PennTreebank. Table 2 shows the accuracy
of two parsers (LAS, UAS and the F1 of the root
dependency attachment) on the QuestionBank test
data. The first is a parser trained on the standard
training sections of the PennTreebank (PTB) and
the second is a parser trained on the training por-
tion of the QuestionBank (QTB). Results for both
1494
LAS UAS Root-F1
trans?PTB 67.97 73.52 47.60
trans?QTB 84.59 89.59 91.06
trans?aug.-loss 76.27 86.42 83.41
graph?PTB 65.27 72.72 43.10
graph?QTB 82.73 87.44 91.58
graph?aug.-loss 72.82 80.68 86.26
Table 2: Domain adaptation results. Table shows (for
both transition and graph-based parsers) the labeled ac-
curacy score (LAS), unlabeled accuracy score (UAS)
and Root-F1 for parsers trained on the PTB and QTB
and tested on the QTB. The augmented-loss parsers are
trained on the PTB but with a partial tree loss on QTB
that considers only root dependencies.
transition-based parsers and graph-based parsers are
given. Clearly there is significant drop in accu-
racy for a parser trained on the PTB. For example,
the transition-based PTB parser achieves a LAS of
67.97% relative to 84.59% for the parser trained on
the QTB.
We consider the situation where it is possible to
ask annotators a single question about the target do-
main that is relatively easy to answer. The question
should be posed so that the resulting answer pro-
duces a partially labeled dependency tree. Root-F1
scores from Table 2 suggest that one simple ques-
tion is ?what is the main verb of this sentence?? for
sentences that are questions. In most cases this task
is straight-forward and will result in a single depen-
dency, that from the root to the main verb of the sen-
tence. We feel this is a realistic partial labeled train-
ing setting where it would be possible to quickly col-
lect a significant amount of data.
To test whether such weak information can signif-
icantly improve the parsing of questions, we trained
an augmented-loss parser using the training set of
the QTB stripped of all dependencies except the de-
pendency from the root to the main verb of the sen-
tence. In other words, for each sentence, the parser
may only observe a single dependency at training
from the QTB ? the dependency to the main verb.
Our augmented-loss function in this case is a simple
binary function: 0 if a parse has the correct root de-
pendency and 1 if it does not. Thus, the algorithm
will select the first parse in the k-best list that has the
correct root as the proxy to a gold standard parse.2
The last row in each section of Table 2 shows the
results for this augmented-loss system when weight-
ing both losses equally during training. By simply
having the main verb annotated in each sentence ?
the sentences from the training portion of the QTB
? the parser can eliminate half of the errors of the
original parser. This is reflected by both the Root-
F1 as well as LAS/UAS. It is important to point out
that these improvements are not limited to simply
better root predictions. Due to the fact that parsing
algorithms make many parsing decisions jointly at
test time, all such decisions influence each other and
improvements are seen across the board. For exam-
ple, the transition-based PTB parser has an F1 score
of 41.22% for verb subjects (nsubj), whereas the
augmented-loss parser has an F1 of 73.52%. Clearly
improving just a single (and simple to annotate) de-
pendency leads to general parser improvements.
4.3 Average Arc Length Score
The augmented-loss framework can be used to in-
corporate multiple treebank-based loss functions as
well. Labeled attachment score is used as our base
model loss function. In this set of experiments we
consider adding an additional loss function which
weights the lengths of correct and incorrect arcs, the
average (labeled) arc-length score:
ALS =
?
i ?(??i, ?i)(i? ?i)?
i(i? ?i)
For each word of the sentence we compute the dis-
tance between the word?s position i and the posi-
tion of the words head ?i. The arc-length score is
the summed length of all those with correct head as-
signments (?(??i, ?i) is 1 if the predicted head and
the correct head match, 0 otherwise). The score is
normalized by the summed arc lengths for the sen-
tence. The labeled version of this score requires that
the labels of the arc are also correct. Optimizing
for dependency arc length is particularly important
as parsers tend to do worse on longer dependencies
(McDonald and Nivre, 2007) and these dependen-
cies are typically the most meaningful for down-
stream tasks, e.g., main verb dependencies for tasks
2For the graph-based parser one can also find the higest scor-
ing tree with correct root by setting the score of all competing
arcs to ??.
1495
LAS UAS ALS
trans?PTB 88.64 91.64 82.96
trans?unlabeled aug.-loss 88.74 91.91 83.65
trans?labeled aug.-loss 88.84 91.91 83.46
graph?PTB 85.75 88.70 73.88
graph?unlabeled aug.-loss 85.80 88.81 74.26
graph?labeled aug.-loss 85.85 88.93 74.40
Table 3: Results for both parsers on the development set
of the PTB. When training with ALS (labeled and unla-
beled), we see an improvement in UAS, LAS, and ALS.
Furthermore, if we use a labeled-ALS as the metric for
augmented-loss training, we also see a considerable in-
crease in LAS.
like information extraction (Yates and Etzioni, 2009)
and textual entailment (Berant et al, 2010).
In Table 3 we show results for parsing with the
ALS augmented-loss objective. For each parser, we
consider two different ALS objective functions; one
based on unlabeled-ALS and the other on labeled-
ALS. The arc-length score penalizes incorrect long-
distance dependencies more than local dependen-
cies; long-distance dependencies are often more de-
structive in preserving sentence meaning and can be
more difficult to predict correctly due to the larger
context on which they depend. Combining this with
the standard attachment scores biases training to fo-
cus on the difficult head dependencies.
For both experiments we see that by adding the
ALS augmented-loss we achieve an improvement in
LAS and UAS in addition to ALS. The augmented-
loss not only helps us improve on the longer depen-
dencies (as reflected in the increased ALS), but also
in the main parser objective function of LAS and
UAS. Using the labeled loss function provides better
reinforcement as can be seen in the improvements
over the unlabeled loss-function. As with all experi-
ments in this paper, the graph-based parser baselines
are much lower than the transition-based parser due
to the use of arc-factored features. In these experi-
ments we used an inline-ranker loss with 8 parses.
We experimented with larger sizes (16 and 64) and
found very similar improvements: for example, the
transition parser?s LAS for the labeled loss is 88.68
and 88.84, respectively).
We note that ALS can be decomposed locally and
could be used as the primary objective function for
parsing. A parse with perfect scores under ALS
and LAS will match the gold-standard training tree.
However, if we were to order incorrect parses of a
sentence, ALS and LAS will suggest different order-
ings. Our results show that by optimizing for losses
based on a combination of these metrics we train a
more robust parsing model.
5 Related Work
A recent study by Katz-Brown et al (2011) also in-
vestigates the task of training parsers to improve MT
reordering. In that work, a parser is used to first
parse a set of manually reordered sentences to pro-
duce k-best lists. The parse with the best reordering
score is then fixed and added back to the training set
and a new parser is trained on resulting data. The
method is called targeted self-training as it is simi-
lar in vein to self-training (McClosky et al, 2006),
with the exception that the new parse data is targeted
to produce accurate word reorderings. Our method
differs as it does not statically fix a new parse, but
dynamically updates the parameters and parse selec-
tion by incorporating the additional loss in the inner
loop of online learning. This allows us to give guar-
antees of convergence. Furthermore, we also evalu-
ate the method on alternate extrinsic loss functions.
Liang et al (2006) presented a perceptron-based
algorithm for learning the phrase-translation param-
eters in a statistical machine translation system.
Similar to the inline-ranker loss function presented
here, they use a k-best lists of hypotheses in order to
identify parameters which can improve a global ob-
jective function: BLEU score. In their work, they
are interested in learning a parameterization over
translation phrases (including the underlying word-
alignment) which optimizes the BLEU score. Their
goal is considerably different; they want to incor-
porate additional features into their model and de-
fine an objective function which allows them to do
so; whereas, we are interested in allowing for mul-
tiple objective functions in order to adapt the parser
model parameters to downstream tasks or alternative
intrinsic (parsing) objectives.
The work that is most similar to ours is that
of Chang et al (2007), who introduced the Con-
straint Driven Learning algorithm (CODL). Their al-
gorithm specifically optimizes a loss function with
1496
the addition of constraints based on unlabeled data
(what we call extrinsic datasets). For each unla-
beled example, they use the current model along
with their set of constraints to select a set of k au-
tomatically labeled examples which best meet the
constraints. These induced examples are then added
to their training set and, after processing each unla-
beled dataset, they perform full model optimization
with the concatenation of training data and newly
generated training items. The augmented-loss al-
gorithm can be viewed as an online version of this
algorithm which performs model updates based on
the augmented-loss functions directly (rather than
adding a set of examples to the training set). Un-
like the CODL approach, we do not perform com-
plete optimization on each iteration over the unla-
beled dataset; rather, we incorporate the updates in
our online learning algorithm. As mentioned earlier,
CODL is one example of learning algorithms that
use weak supervision, others include Mann and Mc-
Callum (2010) and Ganchev et al (2010). Again,
these works are typically interested in using the ex-
trinsic metric ? or, in general, extrinsic information
? to optimize the intrinsic metric in the absence of
any labeled intrinsic data. Our goal is to optimize
both simultaneously.
The idea of jointly training parsers to optimize
multiple objectives is related to joint learning and in-
ference for tasks like information extraction (Finkel
and Manning, 2009) and machine translation (Bur-
kett et al, 2010). In such works, a large search space
that covers both the space of parse structures and
the space of task-specific structures is defined and
parameterized so that standard learning and infer-
ence algorithms can be applied. What sets our work
apart is that there is still just a single parameter set
that is being optimized ? the parser parameters. Our
method only uses feedback from task specific objec-
tives in order to update the parser parameters, guid-
ing it towards better downstream performance. This
is advantageous for two reasons. First, it decouples
the tasks, making inference and learning more effi-
cient. Second, it does not force arbitrary paraemter
factorizations in order to define a joint search space
that can be searched efficiently.
Finally, augmented-loss training can be viewed
as multi-task learning (Caruana, 1997) as the model
optimizes multiple objectives over multiple data sets
with a shared underlying parameter space.
6 Discussion
The empirical results show that incorporating an
augmented-loss using the inline-ranker loss frame-
work achieves better performance under metrics as-
sociated with the external loss function. For the in-
trinsic loss, we see that the augmented-loss frame-
work can also result in an improvement in parsing
performance; however, in the case of ALS, this is
due to the fact that the loss function is very closely
related to the standard evaluation metrics of UAS
and LAS.
Although our analysis suggests that this algorithm
is guaranteed to converge only for the separable
case, it makes a further assumption that if there is
a better parse under the augmented-loss, then there
must be a lower cost parse in the k-best list. The em-
pirical evaluation presented here is based on a very
conservative approximation by choosing lists with
at most 8 parses. However, in our experiments, we
found that increasing the size of the lists did not sig-
nificantly increase our accuracy under the external
metrics. If we do have at least one improvement
in our k-best lists, the analysis suggests that this is
enough to move in the correct direction for updating
the model. The assumption that there will always
be an improvement in the k-best list if there is some
better parse breaks down as training continues. We
suspect that an increasing k, as suggested in Sec-
tion 2.3, will allow for continued improvements.
Dependency parsing, as presented in this pa-
per, is performed over (k-best) part-of-speech tags
and is therefore dependent on the quality of the
tagger. The experiments presented in this paper
made use of a tagger trained on the source treebank
data which severely limits the variation in parses.
The augmented-loss perceptron algorithm presented
here can be applied to any online learning prob-
lem, including part-of-speech tagger training. To
build a dependency parser which is better adapted
to a downstream task, one would want to perform
augmented-loss training on the tagger as well.
7 Conclusion
We introduced the augmented-loss training algo-
rithm and show that the algorithm can incorporate
1497
additional loss functions to adapt the model towards
extrinsic evaluation metrics. Analytical results are
presented that show that the algorithm can opti-
mize multiple objective functions simultaneously.
We present an empirical analysis for training depen-
dency parsers for multiple parsing algorithms and
multiple loss functions.
The augmented-loss framework supports both in-
trinsic and extrinsic losses, allowing for both com-
binations of objectives as well as multiple sources
of data for which the results of a parser can be eval-
uated. This flexibility makes it possible to tune a
model for a downstream task. The only requirement
is a metric which can be defined over parses of the
downstream data. Our dependency parsing results
show that we are not limited to increasing parser
performance via more data or external domain adap-
tation techniques, but that we can incorporate the
downstream task into parser training.
Acknowledgements: We would like to thank Kuz-
man Ganchev for feedback on an earlier draft of this
paper as well as Slav Petrov for frequent discussions
on this topic.
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion.
J. Berant, I. Dagan, and J. Goldberger. 2010. Global
learning of focused entailment graphs. In Proc. of
ACL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of EMNLP.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
D. Burkett, J. Blitzer, and D. Klein. 2010. Joint parsing
and alignment with weakly synchronized grammars.
In Proc. of NAACL.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
Proc. of ACL.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In Proc. of ICML.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
M.C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC, Genoa,
Italy.
J.R. Finkel and C.D. Manning. 2009. Joint parsing and
named entity recognition. In Proc. of NAACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
K. Hall. 2007. k-best spanning tree parsing. In Proc. of
ACL, June.
J. Judge, A. Cahill, and J. Van Genabith. 2006. Question-
bank: Creating a corpus of parse-annotated questions.
In Proc. of ACL, pages 497?504.
J. Katz-Brown, S. Petrov, R. McDonald, D. Talbot,
F. Och, H. Ichikawa, M. Seno, and H. Kazawa. 2011.
Training a parser for machine translation reordering.
In Proc. of EMNLP.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.
P. Liang, A. Bouchard-Ct, D. Klein, and B. Taskar. 2006.
An end-to-end discriminative approach to machine
translation. In Proc. of COLING/ACL.
G.S. Mann and A. McCallum. 2010. Generalized Ex-
pectation Criteria for Semi-Supervised Learning with
Weakly Labeled Data. The Journal of Machine Learn-
ing Research, 11:955?984.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19:313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CoNLL.
1498
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. De-
pendency tree-based sentiment classification using crfs
with hidden variables. In Proc. of NAACL.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
S. Petrov, P.C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In Proc. of EMNLP, pages 705?713.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalu-
ation framework for machine translation reordering.
In Proc. of the Sixth Workshop on Statistical Machine
Translation.
M. Wang, N.A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proc. of EMNLP-CoNLL.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Us-
ing a dependency parser to improve SMT for Subject-
Object-Verb languages. In Proc. of NAACL.
A. Yates and O. Etzioni. 2009. Unsupervised meth-
ods for determining object and relation synonyms on
the web. Journal of Artificial Intelligence Research,
34(1):255?296.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP, pages 562?571.
1499
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 320?331, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generalized Higher-Order Dependency Parsing with Cube Pruning
Hao Zhang Ryan McDonald
Google, Inc.
{haozhang,ryanmcd}@google.com
Abstract
State-of-the-art graph-based parsers use fea-
tures over higher-order dependencies that rely
on decoding algorithms that are slow and
difficult to generalize. On the other hand,
transition-based dependency parsers can eas-
ily utilize such features without increasing the
linear complexity of the shift-reduce system
beyond a constant. In this paper, we attempt to
address this imbalance for graph-based pars-
ing by generalizing the Eisner (1996) algo-
rithm to handle arbitrary features over higher-
order dependencies. The generalization is at
the cost of asymptotic efficiency. To account
for this, cube pruning for decoding is utilized
(Chiang, 2007). For the first time, label tuple
and structural features such as valencies can
be scored efficiently with third-order features
in a graph-based parser. Our parser achieves
the state-of-art unlabeled accuracy of 93.06%
and labeled accuracy of 91.86% on the stan-
dard test set for English, at a faster speed than
a reimplementation of the third-order model of
Koo et al2010).
1 Introduction
The trade-off between rich features and exact de-
coding in dependency parsing has been well docu-
mented (McDonald and Nivre, 2007; Nivre and Mc-
Donald, 2008). Graph-based parsers typically trade-
off rich feature scope for exact (or near exact) de-
coding, whereas transition-based parsers make the
opposite trade-off. Recent research on both parsing
paradigms has attempted to address this.
In the transition-based parsing literature, the fo-
cus has been on increasing the search space of the
system at decoding time, as expanding the feature
scope is often trivial and in most cases only leads to
a constant-time increase in parser complexity. The
most common approach is to use beam search (Duan
et al2007; Johansson and Nugues, 2007; Titov and
Henderson, 2007; Zhang and Clark, 2008; Zhang
and Nivre, 2011), but more principled dynamic pro-
gramming solutions have been proposed (Huang and
Sagae, 2010). In all cases inference remains approx-
imate, though a larger search space is explored.
In the graph-based parsing literature, the main
thrust of research has been on extending the Eisner
chart-parsing algorithm (Eisner, 1996) to incorpo-
rate higher-order features (McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010). A
similar line of research investigated the use of inte-
ger linear programming (ILP) formulations of pars-
ing (Riedel and Clarke, 2006; Martins et al2009;
Martins et al2010). Both solutions allow for exact
inference with higher-order features, but typically at
a high cost in terms of efficiency. Furthermore, spe-
cialized algorithms are required that deeply exploit
the structural properties of the given model. Upgrad-
ing a parser to score new types of higher-order de-
pendencies thus requires significant changes to the
underlying decoding algorithm. This is in stark con-
trast to transition-based systems, which simply re-
quire the definition of new feature extractors.
In this paper, we abandon exact search in graph-
based parsing in favor of freedom in feature scope.
We propose a parsing algorithm that keeps the back-
bone Eisner chart-parsing algorithm for first-order
parsing unchanged. Incorporating higher-order fea-
tures only involves changing the scoring function of
320
potential parses in each chart cell by expanding the
signature of each chart item to include all the non-
local context required to compute features. The core
chart-parsing algorithm remains the same regardless
of which features are incorporated. To control com-
plexity we use cube pruning (Chiang, 2007) with the
beam size k in each cell. Furthermore, dynamic pro-
gramming in the style of Huang and Sagae (2010)
can be done by merging k-best items that are equiv-
alent in scoring. Thus, our method is an applica-
tion of integrated decoding with a language model
in MT (Chiang, 2007) to dependency parsing, which
has previously been applied to constituent parsing
(Huang, 2008). However, unlike Huang, we only
have one decoding pass and a single trained model,
while Huang?s constituent parser maintains a sep-
arate generative base model from a following dis-
criminative re-ranking model. We draw connections
to related work in Section 6.
Our chart-based approximate search algorithm al-
lows for features on dependencies of an arbitrary or-
der ? as well as over non-local structural proper-
ties of the parse trees ? to be scored at will. In
this paper, we use first to third-order features of
greater varieties than Koo and Collins (2010). Ad-
ditionally, we look at higher-order dependency arc-
label features, which is novel to graph-based pars-
ing, though commonly exploited in transition-based
parsing (Zhang and Nivre, 2011). This is because
adding label tuple features would introduce a large
constant factor of O(|L|3), where |L| is the size of
the label set L, into the complexity for exact third-
order parsing. In our formulation, only the top-
ranked labelled arcs would survive in each cell. As
a result, label features can be scored without combi-
natorial explosion. In addition, we explore the use
of valency features counting how many modifiers a
word can have on its left and right side. In the past,
only re-rankers on k-best lists of parses produced by
a simpler model use such features due to the diffi-
culty of incorporating them into search (Hall, 2007).
The final parser with all these features is both ac-
curate and fast. In standard experiments for English,
the unlabeled attachment score (UAS) is 93.06%,
and the labeled attachment score (LAS) is 91.86%.
The UAS score is state-of-art. The speed of our
parser is 220 tokens per second, which is over 4
times faster than an exact third-order parser that at-
Figure 1: Example Sentence.
tains UAS of 92.81% and comparable to the state-of-
the-art transition-based system of Zhang and Nivre
(2011) that employs beam search.
2 Graph-based Dependency Parsing
Dependency parsers produce directed relationships
between head words and their syntactic modifiers.
Each word modifies exactly one head, but can have
any number of modifiers itself. The root of a sen-
tence is a designated special symbol which all words
in the sentence directly or indirectly modify. Thus,
the dependency graph for a sentence is constrained
to be a directed tree. The directed syntactic rela-
tionships, aka dependency arcs or dependencies for
short, can often be labeled to indicate their syntactic
role. Figure 1 gives an example dependency tree.
For a sentence x = x1 . . . xn, dependency pars-
ing is the search for the set of head-modifier depen-
dency arcs y? such that y? = argmaxy?Y(x) f(x, y),
where f is a scoring function. As mentioned before,
y? must represent a directed tree. |Y(x)| is then the
set of valid dependency trees for x and grows ex-
ponentially with respect to its length |x|. We fur-
ther define L as the set of possible arc labels and use
the notation (i l?? j) ? y to indicate that there is a
dependency from head word xi to modifier xj with
label l in dependency tree y.
In practice, f(x, y) is factorized into scoring func-
tions on parts of (x, y). For example, in first-
order dependency parsing (McDonald et al2005),
f(x, y) is factored by the individual arcs:
y? = argmax
y?Y(x)
f(x, y) = argmax
y?Y(x)
?
(i l??j)?y
f(i l?? j)
The factorization of dependency structures into arcs
enables an efficient dynamic programming algo-
rithm with running time O(|x|3) (Eisner, 1996), for
the large family of projective dependency structures.
Figure 2 shows the parsing logic for the Eisner
algorithm. It has two types of dynamic program-
ming states: complete items and incomplete items.
321
Complete items correspond to half-constituents, and
are represented as triangles graphically. Incomplete
items correspond to dependency arcs, and are repre-
sented as trapezoids. The Eisner algorithm is the ba-
sis for the more specialized variants of higher-order
projective dependency parsing.
Second-order sibling models (McDonald and
Pereira, 2006) score adjacent arcs with a common
head. In order to score them efficiently, a new state
corresponding to modifier pairs was introduced to
the chart-parsing algorithm. Due to the careful fac-
torization, the asymptotic complexity of the revised
algorithm remains O(|x|3). The resulting scoring
function is:
y? = argmax
y?Y(x)
?
(i l??j,i l???k)?y
f(i l?? j, i l
?
?? k)
where (i l?? j, i l
?
?? k) ? y indicates two adja-
cent head-modifier relationships in dependency tree
y, one from xi to xj with label l and another from
xi to xk with label l?. Words xj and xk are com-
monly referred to as siblings. In order to maintain
cubic parsing complexity, adjacent dependencies are
scored only if the modifiers occur on the same side
in the sentence relative to the head.
Second-order grandchild models (Carreras, 2007)
score adjacent arcs in length-two head-modifier
chains. For example, if word xi modifies word xj
with label l, but itself has a dependency to modi-
fier xk with label l?, then we would add a scoring
function f(j l?? i l
?
?? k). These are called grand-
child models as they can score dependencies be-
tween a word and its modifier?s modifiers, i.e., xk
is the grandchild of xj in the above example. The
states in the Eisner algorithm need to be augmented
with the indices to the outermost modifiers in order
to score the outermost grandchildren. The resulting
algorithm becomes O(|x|4).
Finally, third-order models (Koo and Collins,
2010) score arc triples such as three adjacent sib-
ling modifiers, called tri-siblings, or structures look-
ing at both horizontal contexts and vertical contexts,
e.g., grand-siblings that score a word, its modifier
and its adjacent grandchildren. To accommodate
the scorers for these sub-graphs, even more special-
ized dynamic programming states were introduced.
The Koo and Collins (2010) factorization enables
(a) = +
(b) = +
Figure 2: Structures and rules for parsing first-order mod-
els with the (Eisner, 1996) algorithm. This shows only
the construction of right-pointing dependencies and not
the symmetric case of left-pointing dependencies.
the scoring of certain types of third-order dependen-
cies with O(|x|4) decoder run-time complexity.
Each of these higher-order parsing algorithms
makes a clever factorization for the specific model
in consideration to keep complexity as low as possi-
ble. However, this results in a loss of generality.
3 Generalizing Eisner?s Algorithm
In this section, we generalize the Eisner algorithm
without introducing new parsing rules. The general-
ization is straight-forward: expand the dynamic pro-
gramming state to incorporate feature histories. This
is done on top of the two distinct chart items in the
O(|x|3) Eisner chart-parsing algorithm (Figure 2).
The advantage of this approach is that it maintains
the simplicity of the original Eisner algorithm. Un-
fortunately, it can increase the run-time complex-
ity of the algorithm substantially, but we will em-
ploy cube pruning to regain tractability. Because our
higher-order dependency parsing algorithm is based
the Eisner algorithm, it is currently limited to pro-
duce projective trees only.
3.1 Arbitrary n-th-order dependency parsing
We start with the simplest case of sibling models. If
we want to score sibling arcs, at rule (b) in Figure 2,
we can see that the complete item lying between
the head and the modifier (the middle of the three
items) does not contain information about the out-
ermost modifier of the head, which is the previous
dependency constructed and the sibling to the mod-
ifier of the dependency currently being constructed.
This fact suggests that, in order to score modifier
bigrams, the complete item states should be aug-
mented by the outermost modifier. We can aug-
ment the chart items with such information, which
322
(a) = +
(b) = +
Figure 3: Structures and rules for parsing models based
on modifier bigrams, with a generalized (Eisner, 1996)
algorithm. Here the dashed arrows indicate additional in-
formation stored in each chart-cell. Specifically the pre-
vious modifier in complete chart items.
is shown in Figure 3. It refines the complete items
by storing the previously constructed dependency to
the outermost modifiers. Note that now the signature
of the complete items is not simply the end-point in-
dexes, but contains the index of the outer modifier.
Using this chart item augmentation it is now pos-
sible to score both first-order arcs as well as second-
order sibling arcs. In fact, by symmetry, the new
dynamic program can also score the leftmost and
rightmost grandchildren of a head-modifier pair, in
rule (a) and rule (b) respectively. By counting the
number of free variables in each parsing rule, we
see that the parsing complexity is O(|x|5), which is
higher than both McDonald and Pereira (2006) and
Carreras (2007). The added complexity comes from
the fact that it is now possible to score a third-order
dependency consisting of the head, the modifier, the
sibling, and the outermost grandchild jointly.
We can go further to augment the complete and
incomplete states with more parsing history. Fig-
ure 4 shows one possible next step of generaliza-
tion. We generalize the states to keep track of the
latest two modifiers of the head. As a result, it be-
comes possible to score tri-siblings involving three
adjacent modifiers and grand-siblings involving two
outermost grandchildren ? both of which comprise
the third-order Model 2 of Koo and Collins (2010) ?
plus potentially any additional interactions of these
roles. Figure 5 shows another possible generaliza-
tion. We keep modifier chains up to length two in
the complete states. The added history enables the
computation of features for great-grandchildren re-
lationships: (h l?? m l
?
?? gc l
??
?? ggc).
In general, we can augment the complete and in-
complete states with n variables representing the
(a) = +
(b) = +
Figure 4: Structures and rules for parsing models based
on modifier trigrams in horizontal contexts, with a gener-
alized (Eisner, 1996) algorithm. Here the dashed arrows
indicate the previous two modifiers to the head in each
chart item.
(a) = +
(b) = +
Figure 5: Structures and rules for parsing models based
on modifier trigrams in vertical contexts, with a gener-
alized (Eisner, 1996) algorithm. Here the dashed arrows
indicate the modifier to the head and the modifier?s mod-
ifier, forming a modifier chain of length two.
possible parsing histories and loop over the cross
product of the histories in the innermost loop of Eis-
ner algorithm. The cardinality of the cross product
is |x|n ? |x|n. Thus, the complexity of the algo-
rithm augmented by n variables is O(|x|3 ? |x|2n) =
O(|x|3+2n), where n ? 0. Note that this complexity
is for unlabeled parsing. A factor of |L| for all or
a subset of the encoded arcs must be multiplied in
when predicting labeled parse structures.
3.2 History-based dependency parsing
The previous n modifiers, either horizontal or ver-
tical, is a potential signature of parsing history. We
can put arbitrary signatures of parsing history into
the chart items so that when we score a new item,
we can draw the distinguishing power of features
based on an arbitrarily deep history. For example,
consider the position of a modifier, which is the po-
sition in which it occurs amongst its siblings relative
to the location of the head. We can store the position
of the last modifier into both chart states. In com-
plete states, this signature tells us the position of the
outermost modifier, which is the valency of the head
in the left or right half-constituent.
323
In the extreme case, we can use full subtrees as
histories, although the cardinality of the set of his-
tories would quickly become exponential, especially
when one considers label ambiguity. Regardless, the
high complexity associated with this generalization,
even for second or third-order models, requires us to
appeal to approximate search algorithms.
3.3 Advantage of the generalization
The complexity analysis earlier in this section re-
veals the advantage of such a generalization scheme.
It factorizes a dynamic programming state for de-
pendency parsing into two parts: 1) the structural
state, which consists of the boundaries of incom-
plete and complete chart items, and accounts for the
O(|x|3) term in the analysis, and 2) the feature his-
tory, which is a signature of the internal content of a
sub-parse and accounts for the O(|x|2n) term. The
rules of the deductive parsing system ? the Eisner al-
gorithm ? stay the same as long as the structural rep-
resentation is unchanged. To generalize the parser
to handle richer features, one can simply enrich the
feature signature and the scoring function without
changing the structural state. A natural grouping of
states follows where all sub-parses sharing the same
chart boundaries are grouped together. This group-
ing will enable the cube pruning in Section 4 for ap-
proximate search.
There is another advantage of keeping the Eis-
ner parsing logic unchanged: derivations one-to-one
correspond to dependency parse trees. Augmenting
the complete and incomplete states does not intro-
duce spurious ambiguity. This grouping view is use-
ful for proving this point. Introducing higher order
features in each chart item will cause sub-derivations
to be re-ranked only. As a result, the final Viterbi
parse can differ from the one from the standard Eis-
ners algorithm. But the one-to-one correspondence
still holds.
4 Approximate Search with Cube Pruning
In machine translation decoding, an n-gram lan-
guage model can be incorporated into a translation
model by augmenting the dynamic programming
states for the translation model with the boundary
n ? 1 words on the target side. The complexity
for exact search involves a factor of |x|4n?4 in the
hierarchical phrase-based model of Chiang (2007),
where |x| is the input sentence length. The standard
technique is to force a beam size k on each transla-
tion state so that the possible combinations of lan-
guage model histories is bounded by k2. Further-
more, if the list of k language model states are sorted
from the lowest cost to the highest cost, we can as-
sume the best combinations will still be among the
combinations of the top items from each list, al-
though the incorporation of n-gram features breaks
the monotonic property of the underlying semi-ring.
Cube pruning is based on this approximation
(Chiang, 2007). It starts with the combination of the
top items in the lists to be combined. At each step, it
puts the neighbors of the current best combination,
which consists of going one position down in one of
the k-best lists, into a priority queue. The algorithm
stops when k items have been popped off from the
queue. At the final step, it sorts the popped items
since they can be out-of-order. It reduces the combi-
nation complexity from O(k2) to O(k ? log(k)).
Our history-augmented parsing is analogous to
MT decoding. The possible higher-order histories
can similarly be limited to at most k in each com-
plete or incomplete item. The core loop of the gener-
alized algorithm which has a complexity of O(|x|2n)
can similarly be reduced to O(k ?log(k)). Therefore,
the whole parsing algorithm remains O(|x|3) re-
gardless how deep we look into parsing history. Fig-
ure 6 illustrates the computation. We apply rule (b)
to combine two lists of augmented complete items
and keep the combinations with the highest model
scores. With cube pruning, we only explore cells at
(0, 0), (0, 1), (1, 0), (2, 0), and (1, 1), without the
need to evaluate scoring functions for the remaining
cells in the table. Similar computation happens with
rule (a).
In this example cube pruning does find the high-
est scoring combination, i.e., cell (1, 1). However,
note that the scores are not monotonic in the order in
which we search these cells as non-local features are
used to score the combinations. Thus, cube pruning
may not find the highest scoring combination. This
approximation is at the heart of cube pruning.
4.1 Recombination
The significance of using feature signatures is that
when two combinations result in a state with the
324
identical feature signature the one with the highest
score survives. This is the core principle of dynamic
programming. We call it recombination. It denotes
the same meaning as state-merging in Huang and
Sagae (2010) for transition-based parsers.
In cube pruning, with recombination, the k-best
items in each chart cell are locally optimal (in the
pruned search space) over all sub-trees with an
equivalent state for future combinations. The cube
pruning algorithm without recombination degener-
ates to a recursive k-best re-scoring algorithm since
each of the k-best items would be unique by itself
as a sub-tree. It should be noted that by working
on a chart (or a forest, equivalently) the algorithm is
already applying recombination at a coarser level.
In machine translation, due to its large search
space and the abstract nature of an n-gram language
model, it is more common to see many sub-trees
with the same language model feature signature,
making recombination crucial (Chiang, 2007). In
constituent parser reranking (Huang, 2008), recom-
bination is less likely to happen since the rerank-
ing features capture peculiarities of local tree struc-
tures. For dependency parsing, we hypothesize that
the higher-order features are more similar to the n-
gram language model features in MT as they tend to
be common features among many sub-trees. But as
the feature set becomes richer, recombination tends
to have a smaller effect. We will discuss the empiri-
cal results on recombination in Section 5.4.
5 Experiments
We define the scoring function f(x, y) as a linear
classifier between a vector of features and a corre-
sponding weight vector, i.e., f(x, y) = w ? ?(x, y).
The feature function ? decomposes with respect to
scoring function f . We train the weights to optimize
the first-best structure. We use the max-loss vari-
ant of the margin infused relaxed algorithm (MIRA)
(Crammer et al2006) with a hamming-loss margin
as is common in the dependency parsing literature
(Martins et al2009; Martins et al2010). MIRA
only requires a first-best decoding algorithm, which
in our case is the approximate chart-based parsing
algorithms defined in Sections 3 and 4. Because our
decoding algorithm is approximate, this may lead to
invalid updates given to the optimizer (Huang and
=
0 : 1 : 2 :
+
0 : f = 2.5 f = 1 f = 2
1 : f = 1.5 f = 3.2 f = 0.5
2 : f = 2.3 f = 3 f = 1.8
...
Figure 6: Combining two lists of complete items with
cube pruning.
Fayong, 2012). However, we found that ignoring or
modifying such updates led to negligible differences
in practice. In all our experiments, we train MIRA
for 8 epochs and use a beam of k = 5 during de-
coding. Both these values were determined on the
English development data.
5.1 Features
The feature templates we use are drawn from the
past work on graph-based parsing and transition-
based parsing. The base templates for the higher-
order dependencies are close to Koo and Collins
(2010), with the major exception that our features
include label-tuple information. The basic features
include identities, part of speech tags, and labels of
the words in dependency structures. These atomic
features are conjoined with the directions of arcs to
create composite n-gram features. The higher-order
dependency features can be categorized into the fol-
lowing sub-groups, where we use h to indicate the
head, m the modifier, s the modifier?s sibling and gc
a grandchild word in a dependency part.
? (labeled) modifier features: (h l?? m)
? (labeled) sibling features: (h l?? m,h l??? s)
? (labeled) outermost grandchild features:
(h l?? m l
?
?? gc)
? (labeled) tri-sibling features:
(h l?? m,h l
?
?? s, h l
??
?? s2)
? (labeled) grand-sibling features:
(h l?? m l
?
?? gc, h l?? m l
??
?? gc2),
325
? (labeled) sibling and grandchild conjoined features:
(h l?? m,h l
?
?? s,m l
??
?? gc)
The general history features include valencies of
words conjoined with the directions of the dominat-
ing arcs. The positions of the modifiers are also con-
joined with the higher-order dependency features in
the previous list.
The features that are new compared to Koo and
Collins (2010) are the label tuple features, the sib-
ling and grandchild conjoined features, and the va-
lency features. We determine this feature set based
on experiments on the development data for English.
In Section 5.3 we examine the impact of these new
features on parser performance.
5.2 Main Results
Our first set of results are on English dependen-
cies. We used the Penn WSJ Treebank converted to
dependencies with Penn2Malt1 conversion software
specifying Yamada and Matsumoto head rules and
Malt label set. We used the standard splits of this
data: sections 2-21 for training; section 22 for vali-
dation; and section 23 for evaluation. We evaluated
our parsers using standard labeled accuracy scores
(LAS) and unlabeled accuracy scores (UAS) exclud-
ing punctuation. We report run-times in tokens per
second. Part-of-speech tags are predicted as input
using a linear-chain CRF.
Results are given in Table 1. We compare our
method to a state-of-the-art graph-based parser (Koo
and Collins, 2010) as well as a state-of-the-art
transition-based parser that uses a beam (Zhang
and Nivre, 2011) and the dynamic programming
transition-based parser of Huang and Sagae (2010).
Additionally, we compare to our own implementa-
tion of exact first to third-order graph-based parsing
and the transition-based system of Zhang and Nivre
(2011) with varying beam sizes.
There are a number of points to make. First,
approximate decoding with rich features and cube
pruning gives state-of-the-art labeled and unlabeled
parsing accuracies relative to previously reported re-
sults. This includes the best graph-based parsing
results of Koo and Collins (2010), which has near
identical performance, as well as the best beam-
based and dynamic-programming-based transition
1http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
Parser UAS LAS Toks/Sec
Huang and Sagae (2010) 92.1- - -
Zhang and Nivre (2011) 92.9- 91.8- -
Zhang and Nivre (reimpl.) (beam=64) 92.73 91.67 760
Zhang and Nivre (reimpl.) (beam=256) 92.75 91.71 190
Koo and Collins (2010) 93.04 - -
1st-order exact (reimpl.) 91.80 90.50 2070
2nd-order exact (reimpl.) 92.40 91.12 1110
3rd-order exact (reimpl.) 92.81 -? 50
this paper 93.06 91.86 220
Table 1: Comparing this work in terms of parsing accu-
racy compared to state-of-the-art baselines on the English
test data. We also report results for a re-implementation
of exact first to third-order graph-based parsing and a re-
implementation of Zhang and Nivre (2011) in order to
compare parser speed. ?Our exact third-order implemen-
tation currently only supports unlabeled parsing.
parsers (Huang and Sagae, 2010; Zhang and Nivre,
2011). Second, at a similar toks/sec parser speed,
our method achieves better performance than the
transition-based model of Zhang and Nivre (2011)
with a beam of 256. Finally, compared to an im-
plementation of an exact third-order parser ? which
provides us with an apples-to-apples comparison in
terms of features and runtime ? approximate decod-
ing with cube pruning is both more accurate and
while being 4-5 times as fast. It is more accurate as
it can easily incorporate more complex features and
it is faster since its asymptotic complexity is lower.
We should point out that our third-order reimple-
mentation is a purely unlabeled parser as we do not
have an implementation of an exact labeled third-
order parser. This likely under estimates its accu-
racy, but also significantly overestimates its speed.
Next, we looked at the impact of our system
on non-English treebanks. Specifically we fo-
cused on two sets of data. The first is the Chi-
nese Treebank converted to dependencies. Here
we use the identical training/validation/evaluation
splits and experimental set-up as Zhang and Nivre
(2011). Additionally, we evaluate our system on
eight other languages from the CoNLL 2006/2007
shared-task (Buchholz and Marsi, 2006; Nivre et
al., 2007). We selected the following four data
sets since they are primarily projective treebanks
(<1.0% non-projective arcs): Bulgarian and Span-
ish from CoNLL 2006 as well as Catalan and Ital-
ian from CoNLL 2007. Currently our method is
restricted to predicting strictly projective trees as it
326
uses the Eisner chart parsing algorithm as its back-
bone. We also report results from four additional
CoNLL data sets reported in Rush and Petrov (2012)
in order to directly compare accuracy. These are
German, Japanese, Portuguese and Swedish. For all
data sets we measure UAS and LAS excluding punc-
tuation and use gold tags as input to the parser as is
standard for these data sets.
Results are given in Table 2. Here we compare to
our re-implementations of Zhang and Nivre (2011),
exact first to third-order parsing and Rush and Petrov
(2012) for the data sets in which they reported re-
sults. We again see that approximate decoding with
rich features and cube pruning has higher accu-
racy than transition-based parsing with a large beam.
In particular, for the ZH-CTB data set, our system
is currently the best reported result. Furthermore,
our system returns comparable accuracy with exact
third-order parsing, while being significantly faster
and more flexible.
5.3 Ablation studies
In this section, we analyze the contributions from
each of the feature groups. Each row in Table 3 uses
a super set of features than the previous row. All
systems use our proposed generalized higher-order
parser with cube-pruning. I.e., they are all using the
Eisner chart-parsing algorithm with expanded fea-
ture signatures. The only difference between sys-
tems is the set of features used. This allows us to see
the improvement from additional features.
The first row uses no higher-order features. It
is equivalent to the first-order model from Table 1.
The only difference is that it uses the k-best algo-
rithm to find the first-best, so it has additional over-
head compared to the standard Viterbi algorithm.
Each of the following rows gets a higher accuracy
than its previous row by adding more higher or-
der features. Putting in the sibling and grandchild
conjoined features and the valency features yields
a further improvement over the approximation of
Koo and Collins (2010). Thus, the addition of new
higher-order features, including valency, extra third-
order, and label tuple features, results in increased
accuracy. However, this is not without cost as the
run-time in terms of tokens/sec decreases (300 to
220). But this decrease is not asymptotic, as it would
be if one were to exactly search over our final model
Higher-order Features UAS LAS Toks/Sec
none 91.74 90.46 1510
McDonald (2006) features + labels 92.48 91.25 860
Carreras (2007) features + labels 92.85 91.66 540
Koo (2010) features + labels 92.92 91.75 300
all features 93.06 91.86 220
Table 3: Generalized higher-order parsing with cube
pruning using different feature sets.
Beam Recombination UAS LAS Toks/Sec
2 no 92.86 91.63 280
2 yes 92.89 91.65 260
5 no 93.05 91.85 240
5 yes 93.06 91.86 230
10 yes 93.05 91.85 140
Table 4: Showing the effect of better search on accuracy
and speed on the English test data with a fixed model.
with these additional features, e.g., valency would at
least multiply an additional O(n) factor.
5.4 Impact of Search Errors
Since our decoding algorithm is not exact, it could
return sub-optimal outputs under the current model.
We analyze the effect of search errors on accuracies
in Table 4. We vary the beam size at each cell and
switch the option for signature-based recombination
to make search better or worse to see how much im-
pact it has on the final accuracy.
The results indicate that a relatively small per-cell
beam is good enough. Going from a beam of 2 to
5 increases accuracy notably, but going to a larger
beam size has little effect but at a cost in terms of
efficiency. This suggests that most of the parser am-
biguity is represented in the top-5 feature signatures
at each chart cell. Furthermore, recombination does
help slightly, but more so at smaller beam sizes.
If we keep the beam size constant but enlarge
the feature scope from second-order to third-order,
one would expect more search errors to occur. We
measured this empirically by computing the num-
ber of sentences where the gold tree had a higher
model score than the predicted tree in the English
evaluation data. Indeed, larger feature scopes do
lead to more search errors, but the absolute num-
ber of search errors is usually quite small ? there
are only 19 search errors using second-order features
and 32 search errors using third-order plus valency
features out of 2416 English test sentences. Part
of the reason for this is that there are only 12 la-
327
Zhang and Nivre Zhang and Nivre Rush 1st-order 2nd-order 3rd-order
(reimpl.) (reimpl.) and exact exact exact
Language (beam=64) (beam=256) Petrov? (reimpl.) (reimpl.) (reimpl.) this paper
BG-CONLL 92.22 / 87.87 92.28 / 87.91 91.9- / - 91.98 / 87.13 93.02 / 88.13 92.96 / - 93.08 / 88.23
CA-CONLL 93.76 / 87.74 93.83 / 87.85 92.83 / 86.22 93.45 / 87.19 94.07 / - 94.00 / 88.08
DE-CONLL 89.18 / 86.50 88.94 / 86.58 90.8- / - 89.28 / 86.06 90.87 / 87.72 91.29 / - 91.35 / 88.42
ES-CONLL 86.64 / 83.25 86.62 / 83.11 85.35 / 81.53 86.80 / 82.91 87.26 / - 87.48 / 84.05
IT-CONLL 85.51 / 81.12 85.45 / 81.10 84.98 / 80.23 85.46 / 80.66 86.49 / - 86.54 / 82.15
JA-CONLL 92.70 / 91.03 92.76 / 91.09 92.3- / - 93.00 / 91.03 93.20 / 91.25 93.36 / - 93.24 / 91.45
PT-CONLL 91.32 / 86.98 91.28 / 86.88 91.5- / - 90.36 / 85.77 91.36 / 87.22 91.66 / - 91.69 / 87.70
SV-CONLL 90.84 / 85.30 91.00 / 85.42 90.1- / - 89.32 / 82.06 90.50 / 83.01 90.32 / - 91.44 / 84.58
ZH-CTB 86.04 / 84.48? 86.14 / 84.57 84.38 / 82.62 86.63 / 84.95 86.77 / - 86.87 / 85.19
AVG 89.80 / 86.03 89.81 / 86.06 89.05 / 84.74 90.14 / 85.89 90.46 / - 90.63 / 86.65
Table 2: UAS/LAS for experiments on non-English treebanks. Numbers in bold are the highest scoring system. Zhang
and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. Rush and Petrov are
the UAS results reported in Rush and Petrov (2012). Nth-order exact are implementations of exact 1st-3rd order
dependency parsing. ?For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result
reported on this data set. ?It should be noted that Rush and Petrov (2012) do not jointly optimize labeled and unlabeled
dependency structure, which we found to often help. This, plus extra features, accounts for the differences in UAS.
bels in the Penn2Malt label set, which results in lit-
tle non-structural ambiguity. In contrast, Stanford-
style dependencies contain a much larger set of la-
bels (50) with more fine-grained syntactic distinc-
tions (De Marneffe et al2006). Training and test-
ing a model using this dependency representation2
increases the number of search errors of the full
model to 126 out 2416 sentences. But that is still
only 5% of all sentences and significantly smaller
when measured per dependency.
6 Related Work
As mentioned in the introduction, there has been
numerous studies on trying to reconcile the rich-
features versus exact decoding trade-off in depen-
dency parsing. In the transition-based parsing lit-
erature this has included the use of beam search to
increase the search space (Duan et al2007; Johans-
son and Nugues, 2007; Titov and Henderson, 2007;
Zhang and Clark, 2008; Zhang and Nivre, 2011).
Huang and Sagae (2010) took a more principled ap-
proach proposing a method combining shift-reduce
parsing with dynamic programming. They showed
how feature signatures can be compiled into dy-
2This model gets 90.4/92.8 LAS/UAS which is comparable
to the UAS of 92.7 reported by Rush and Petrov (2012).
namic programming states and how best-first search
can be used to find the optimal transition sequence.
However, when the feature scope becomes large,
then the state-space and resulting search space can
be either intractable or simply non-practical to ex-
plore. Thus, they resort to an approximate beam
search that still exploring an exponentially-larger
space than greedy or beam-search transition-based
systems. One can view the contribution in this pa-
per as being the complement of the work of Huang
and Sagae (2010) for graph-based systems. Our ap-
proach also uses approximate decoding in order to
exploit arbitrary feature scope, while still exploring
an exponentially-large search space. The primary
difference is how the system is parameterized, over
dependency sub-graphs or transitions. Another criti-
cal difference is that a chart-based algorithm, though
still subject to search errors, is less likely to be hin-
dered by an error made at one word position be-
cause it searches over many parallel alternatives in a
bottom-up search as opposed to a left-to-right pass.
In the graph-based parsing literature, exact pars-
ing algorithms for higher-order features have been
studied extensively (McDonald and Pereira, 2006;
Carreras, 2007; Koo and Collins, 2010), but at a
high computational cost as increasing the order of a
model typically results in an asymptotic increase in
328
running time. ILP formulations of parsing (Riedel
and Clarke, 2006; Martins et al2009; Martins et
al., 2010) also allow for exact inference with higher-
order features, but again at a high computational
cost as ILP?s have, in the worst-case, exponential
run-time with respect to the sentence length. Stud-
ies that have abandoned exact inference have fo-
cused on sampling (Nakagawa, 2007), belief prop-
agation (Smith and Eisner, 2008), Lagrangian re-
laxation (Koo et al2010; Martins et al2011),
and more recently structured prediction cascades
(Weiss and Taskar, 2010; Rush and Petrov, 2012).
However, these approximations themselves are often
computationally expensive, requiring multiple de-
coding/sampling stages in order to produce an out-
put. All the methods above, both exact and approx-
imate, require specialized algorithms for every new
feature that is beyond the scope of the previous fac-
torization. In our method, the same parsing algo-
rithm can be utilized (Eisner?s + cube pruning) just
with slight different feature signatures.
Our proposed parsing model draws heavily on the
work of Huang (2008). Huang introduced the idea
of ?forest rescoring?, which uses cube pruning to
enable the incorporation of non-local features into
a constituency parsing model providing state-of-the
art performance. This paper is the extension of such
ideas to dependency parsing, also giving state-of-
the-art results. An important difference between our
formulation and forest rescoring is that we only have
one decoding pass and a single trained model, while
forest rescoring, as formulated by Huang (2008),
separates a generative base model from a follow-
ing discriminative re-ranking model. Hence, our
formulation is more akin to the one pass decoding
algorithm of Chiang (2007) for integrated decoding
with a language model in machine translation. This
also distinguishes it from previous work on depen-
dency parse re-ranking (Hall, 2007) as we are not
re-ranking/re-scoring the output of a base model but
using a single decoding algorithm and learned model
at training and testing.
This work is largely orthogonal to other attempts
to speed up chart parsing algorithms. This in-
cludes work on coarse-to-fine parsing (Charniak and
Johnson, 2005; Petrov and Klein, 2007; Rush and
Petrov, 2012), chart-cell closing and pruning (Roark
and Hollingshead, 2008; Roark and Hollingshead,
2009), and dynamic beam-width prediction (Boden-
stab et al2011). Of particular note, Rush and
Petrov (2012) report run-times far better than our
cube pruning system. At the heart of their system is
a linear time vine-parsing stage that prunes most of
the search space before higher-order parsing. This
effectively makes their final system linear time in
practice as the higher order models have far fewer
parts to consider. One could easily use the same
first-pass pruner in our cube-pruning framework.
In our study we use cube pruning only for de-
coding and rely on inference-based learning algo-
rithms to train model parameters. Gimpel and Smith
(2009) extended cube pruning concepts to partition-
function and marginal calculations, which would en-
able the training of probabilistic graphical models.
Finally, due to its use of the Eisner chart-parsing
algorithm as a backbone, our model is fundamen-
tally limited to predicting projective dependency
structures. Investigating extensions of this work to
the non-projective case is an area of future study.
Work on defining bottom-up chart-parsing algo-
rithms for non-projective dependency trees could
potentially serve as a mechanism to solving this
problem (Go?mez-Rodr??guez et al2009; Kuhlmann
and Satta, 2009; Go?mez-Rodr??guez et al2010).
7 Conclusion
In this paper we presented a method for general-
ized higher-order dependency parsing. The method
works by augmenting the dynamic programming
signatures of the Eisner chart-parsing algorithm and
then controlling complexity via cube pruning. The
resulting system has the flexibility to incorporate ar-
bitrary feature history while still exploring an ex-
ponential search space efficiently. Empirical results
show that the system gives state-of-the-art accura-
cies across numerous data sets while still maintain-
ing practical parsing speeds ? as much as 4-5 times
faster than exact third-order decoding.
Acknowledgments: We would like to thank Sasha Rush
and Slav Petrov for help modifying their hypergraph pars-
ing code. We would also like to thank the parsing team
at Google for providing interesting discussions and new
ideas while we conducted this work, as well as comments
on earlier drafts of the paper.
329
References
N. Bodenstab, A. Dunlop, K. Hall, and B. Roark. 2011.
Beam-width prediction for efficient context-free pars-
ing. In Proc. ACL.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. ACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
M. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of EMNLP-CoNLL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. of COL-
ING.
K. Gimpel and N.A. Smith. 2009. Cube summing,
approximate inference with non-local features, and
dynamic programming without semirings. In Proc.
EACL.
C. Go?mez-Rodr??guez, M. Kuhlmann, G. Satta, and
D. Weir. 2009. Optimal reduction of rule length in lin-
ear context-free rewriting systems. In Proc. NAACL.
C. Go?mez-Rodr??guez, M. Kuhlmann, and G. Satta. 2010.
Efficient parsing of well-nested linear context-free
rewriting systems. In Proc. NAACL.
K. Hall. 2007. K-best spanning tree parsing. In Proc. of
ACL.
L. Huang and S. Fayong. 2012. Structured perceptron
with inexact search. In Proc. of NAACL.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of ACL.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
R. Johansson and P. Nugues. 2007. Incremental de-
pendency parsing using online learning. In Proc. of
EMNLP-CoNLL.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
M. Kuhlmann and G. Satta. 2009. Treebank grammar
techniques for non-projective dependency parsing. In
Proc. EACL.
A. F. T. Martins, N. Smith, and E. P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proc. of ACL.
A. F. T. Martins, N. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, N. Smith, M. A. T. Figueiredo, and
P. M. Q. Aguiar. 2011. Dual decomposition with
many overlapping components. In Proc of EMNLP.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CoNLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Nakagawa. 2007. Multilingual dependency parsing
using global features. In Proc. of EMNLP-CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. NAACL.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. of EMNLP.
B. Roark and K. Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference.
In Proc. COLING.
B. Roark and K. Hollingshead. 2009. Linear complexity
context-free parsing pipelines via chart constraints. In
Proce. NAACL.
A. Rush and S. Petrov. 2012. Efficient multi-pass depen-
dency pruning with vine parsing. In Proc. of NAACL.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of EMNLP-CoNLL.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS.
330
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL-HLT, volume 2.
331
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 908?913,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Online Learning for Inexact Hypergraph Search
Hao Zhang
Google
haozhang@google.com
Liang Huang Kai Zhao
City University of New York
{lhuang@cs.qc,kzhao@gc}.cuny.edu
Ryan McDonald
Google
ryanmcd@google.com
Abstract
Online learning algorithms like the percep-
tron are widely used for structured predic-
tion tasks. For sequential search problems,
like left-to-right tagging and parsing, beam
search has been successfully combined with
perceptron variants that accommodate search
errors (Collins and Roark, 2004; Huang et
al., 2012). However, perceptron training with
inexact search is less studied for bottom-up
parsing and, more generally, inference over
hypergraphs. In this paper, we generalize
the violation-fixing perceptron of Huang et
al. (2012) to hypergraphs and apply it to the
cube-pruning parser of Zhang and McDonald
(2012). This results in the highest reported
scores on WSJ evaluation set (UAS 93.50%
and LAS 92.41% respectively) without the aid
of additional resources.
1 Introduction
Structured prediction problems generally deal with
exponentially many outputs, often making exact
search infeasible. For sequential search problems,
such as tagging and incremental parsing, beam
search coupled with perceptron algorithms that ac-
count for potential search errors have been shown
to be a powerful combination (Collins and Roark,
2004; Daume? and Marcu, 2005; Zhang and Clark,
2008; Huang et al, 2012). However, sequen-
tial search algorithms, and in particular left-to-right
beam search (Collins and Roark, 2004; Zhang and
Clark, 2008), squeeze inference into a very narrow
space. To address this, Huang (2008) formulated
constituency parsing as approximate bottom-up in-
ference in order to compactly represent an exponen-
tial number of outputs while scoring features of ar-
bitrary scope. This idea was adapted to graph-based
dependency parsers by Zhang and McDonald (2012)
and shown to outperform left-to-right beam search.
Both these examples, bottom-up approximate de-
pendency and constituency parsing, can be viewed
as specific instances of inexact hypergraph search.
Typically, the approximation is accomplished by
cube-pruning throughout the hypergraph (Chiang,
2007). Unfortunately, as the scope of features at
each node increases, the inexactness of search and
its negative impact on learning can potentially be ex-
acerbated. Unlike sequential search, the impact on
learning of approximate hypergraph search ? as well
as methods to mitigate any ill effects ? has not been
studied. Motivated by this, we develop online learn-
ing algorithms for inexact hypergraph search by gen-
eralizing the violation-fixing percepron of Huang et
al. (2012). We empirically validate the benefit of
this approach within the cube-pruning dependency
parser of Zhang and McDonald (2012).
2 Structured Perceptron for Inexact
Hypergraph Search
The structured perceptron algorithm (Collins, 2002)
is a general learning algorithm. Given training in-
stances (x, y?), the algorithm first solves the decod-
ing problem y? = argmaxy?Y(x)w ? f(x, y) given
the weight vector w for the high-dimensional fea-
ture representation f of the mapping (x, y), where
y? is the prediction under the current model, y? is the
gold output and Y(x) is the space of all valid outputs
for input x. The perceptron update rule is simply:
w? = w + f(x, y?) ? f(x, y?).
The convergence of original perceptron algorithm
relies on the argmax function being exact so that
the conditionw ?f(x, y?) > w ?f(x, y?) (modulo ties)
always holds. This condition is called a violation
because the prediction y? scores higher than the cor-
rect label y?. Each perceptron update moves weights
908
A B C D E F
G H I J
K L
M
N
Figure 1: A hypergraph showing the union of the gold
and Viterbi subtrees. The hyperedges in bold and dashed
are from the gold and Viterbi trees, respectively.
away from y? and towards y? to fix such violations.
But when search is inexact, y? could be suboptimal
so that sometimes w ? f(x, y?) < w ? f(x, y?). Huang
et al (2012) named such instances non-violations
and showed that perceptron model updates for non-
violations nullify guarantees of convergence. To ac-
count for this, they generalized the original update
rule to select an output y? within the pruned search
space that scores higher than y?, but is not necessar-
ily the highest among all possibilities, which repre-
sents a true violation of the model on that training
instance. This violation fixing perceptron thus re-
laxes the argmax function to accommodate inexact
search and becomes provably convergent as a result.
In the sequential cases where y? has a linear struc-
ture such as tagging and incremental parsing, the
violation fixing perceptron boils down to finding
and updating along a certain prefix of y?. Collins
and Roark (2004) locate the earliest position in a
chain structure where y?pref is worse than y?pref by
a margin large enough to cause y? to be dropped
from the beam. Huang et al (2012) locate the po-
sition where the violation is largest among all pre-
fixes of y?, where size of a violation is defined as
w ? f(x, y?pref) ? w ? f(x, y?pref).
For hypergraphs, the notion of prefix must be gen-
eralized to subtrees. Figure 1 shows the packed-
forest representation of the union of gold subtrees
and highest-scoring (Viterbi) subtrees at every gold
node for an input. At each gold node, there are
two incoming hyperedges: one for the gold subtree
and the other for the Viterbi subtree. After bottom-
up parsing, we can compute the scores for the gold
subtrees as well as extract the corresponding Viterbi
subtrees by following backpointers. These Viterbi
subtrees need not necessarily to belong to the full
Viterbi path (i.e., the Viterbi tree rooted at node N ).
An update strategy must choose a subtree or a set of
subtrees at gold nodes. This is to ensure that the
model is updating its weights relative to the inter-
section of the search space and the gold path.
Our first update strategy is called single-node
max-violation (s-max). Given a gold tree y?, it tra-
verses the gold tree and finds the node n on which
the violation between the Viterbi subtree and the
gold subtree is the largest over all gold nodes. The
violation is guaranteed to be greater than or equal to
zero because the lower bound for the max-violation
on any hypergraph is 0 which happens at the leaf
nodes. Then we choose the subtree pair (y?n, y?n) and
do the update similar to the prefix update for the se-
quential case. For example, in Figure 1, suppose the
max-violation happens at node K , which covers the
left half of the input x, then the perceptron update
would move parameters to the subtree represented
by nodes B , C , H and K and away from A ,
B , G and K .
Our second update strategy is called parallel max-
violation (p-max). It is based on the observation that
violations on non-overlapping nodes can be fixed
in parallel. We define a set of frontiers as a set
of nodes that are non-overlapping and the union of
which covers the entire input string x. The frontier
set can include up to |x| nodes, in the case where the
frontier is equivalent to the set of leaves. We traverse
y? bottom-up to compute the set of frontiers such
that each has the max-violation in the span it cov-
ers. Concretely, for each node n, the max-violation
frontier set can be defined recursively,
ft(n) =
{
n, if n = maxv(n)
?
ni?children(n) ft(ni), otherwise
where maxv(n) is the function that returns the node
with the absolute maximum violation in the subtree
rooted at n and can easily be computed recursively
over the hypergraph. To make a perceptron update,
we generate the max-violation frontier set for the en-
tire hypergraph and use it to choose subtree pairs
?
n?ft(root(x))(y?n, y
?
n), where root(x) is the root of
the hypergraph for input x. For example, in Figure 1,
if the union of K and L satisfies the definition of
ft, then the perceptron update would move feature
909
weights away from the union of the two Viterbi sub-
trees and towards their gold counterparts.
In our experiments, we compare the performance
of the two violation-fixing update strategies against
two baselines. The first baseline is the standard up-
date, where updates always happen at the root node
of a gold tree, even if the Viterbi tree at the root node
leads to a non-violation update. The second baseline
is the skip update, which also always updates at the
root nodes but skips any non-violations. This is the
strategy used by Zhang and McDonald (2012).
3 Experiments
We ran a number of experiments on the cube-
pruning dependency parser of Zhang and McDonald
(2012), whose search space can be represented as a
hypergraph in which the nodes are the complete and
incomplete states and the hyperedges are the instan-
tiations of the two parsing rules in the Eisner algo-
rithm (Eisner, 1996).
The feature templates we used are a superset of
Zhang and McDonald (2012). These features in-
clude first-, second-, and third-order features and
their labeled counterparts, as well as valency fea-
tures. In addition, we also included a feature tem-
plate from Bohnet and Kuhn (2012). This tem-
plate examines the leftmost child and the rightmost
child of a modifier simultaneously. All other high-
order features of Zhang and McDonald (2012) only
look at arcs on the same side of their head. We
trained the parser with hamming-loss-augmented
MIRA (Crammer et al, 2006), following Martins et
al. (2010). Based on results on the English valida-
tion data, in all the experiments, we trained MIRA
with 8 epochs and used a beam of size 6 per node.
To speed up the parser, we used an unlabeled
first-order model to prune unlikely dependency arcs
at both training and testing time (Koo and Collins,
2010; Martins et al, 2013). We followed Rush and
Petrov (2012) to train the first-order model to min-
imize filter loss with respect to max-marginal filter-
ing. On the English validation corpus, the filtering
model pruned 80% of arcs while keeping the oracle
unlabeled attachment score above 99.50%. During
training only, we insert the gold tree into the hy-
pergraph if it was mistakenly pruned. This ensures
that the gold nodes are always available, which is
required for model updates.
3.1 English and Chinese Results
We report dependency parsing results on the Penn
WSJ Treebank and the Chinese CTB-5 Treebank.
Both treebanks are constituency treebanks. We gen-
erated two versions of dependency treebanks by ap-
plying commonly-used conversion procedures. For
the first English version (PTB-YM), we used the
Penn2Malt1 software to apply the head rules of Ya-
mada and Matsumoto and the Malt label set. For
the second English version (PTB-S), we used the
Stanford dependency framework (De Marneffe et
al., 2006) by applying version 2.0.5 of the Stan-
ford parser. We split the data in the standard way:
sections 2-21 for training; section 22 for validation;
and section 23 for evaluation. We utilized a linear
chain CRF tagger which has an accuracy of 96.9%
on the validation data and 97.3% on the evaluation
data2. For Chinese, we use the Chinese Penn Tree-
bank converted to dependencies and split into train/-
validation/evaluation according to Zhang and Nivre
(2011). We report both unlabeled attachment scores
(UAS) and labeled attachment scores (LAS), ignor-
ing punctuations (Buchholz and Marsi, 2006).
Table 1 displays the results. Our improved
cube-pruned parser represents a significant improve-
ment over the feature-rich transition-based parser of
Zhang and Nivre (2011) with a large beam size. It
also improves over the baseline cube-pruning parser
without max-violation update strategies (Zhang and
McDonald, 2012), showing the importance of up-
date strategies in inexact hypergraph search. The
UAS score on Penn-YM is slightly higher than the
best result known in the literature which was re-
ported by the fourth-order unlabeled dependency
parser of Ma and Zhao (2012), although we did
not utilize fourth-order features. The LAS score on
Penn-YM is on par with the best reported by Bohnet
and Kuhn (2012). On Penn-S, there are not many
existing results to compare with, due to the tradition
of reporting results on Penn-YM in the past. Never-
theless, our result is higher than the second best by
a large margin. Our Chinese parsing scores are the
highest reported results.
1http://stp.lingfil.uu.se//?nivre/research/Penn2Malt.html
2The data was prepared by Andre? F. T. Martins as was done
in Martins et al (2013).
910
Penn-YM Penn-S CTB-5
Parser UAS LAS Toks/Sec UAS LAS Toks/Sec UAS LAS Toks/Sec
Zhang and Nivre (2011) 92.9- 91.8- ?680 - - - 86.0- 84.4- -
Zhang and Nivre (reimpl.) (beam=64) 93.00 91.98 800 92.96 90.74 500 85.93 84.42 700
Zhang and Nivre (reimpl.) (beam=128) 92.94 91.91 400 93.11 90.84 250 86.05 84.50 360
Koo and Collins (2010) 93.04 - - - - - - - -
Zhang and McDonald (2012) 93.06 91.86 220 - - - 86.87 85.19 -
Rush and Petrov (2012) - - - 92.7- - 4460 - - -
Martins et al (2013) 93.07 - 740 92.82 - 600 - - -
Qian and Liu (2013) 93.17 - 180 - - - 87.25 - 100
Bohnet and Kuhn (2012) 93.39 92.38 ?120 - - - 87.5- 85.9- -
Ma and Zhao (2012) 93.4- - - - - - 87.4- - -
cube-pruning w/ skip 93.21 92.07 300 92.92 90.35 200 86.95 85.23 200
w/ s-max 93.50 92.41 300 93.59 91.17 200 87.78 86.13 200
w/ p-max 93.44 92.33 300 93.64 91.28 200 87.87 86.24 200
Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except
punctuations. We also include the tokens per second numbers for different parsers whenever available, although the
numbers from other papers were obtained on different machines. Speed numbers marked with ? were converted from
sentences per second.
The speed of our parser is around 200-300 tokens
per second for English. This is faster than the parser
of Bohnet and Kuhn (2012) which has roughly the
same level of accuracy, but is slower than the parser
of Martins et al (2013) and Rush and Petrov (2012),
both of which only do unlabeled dependency pars-
ing and are less accurate. Given that predicting la-
bels on arcs can slow down a parser by a constant
factor proportional to the size of the label set, the
speed of our parser is competitive. We also tried to
prune away arc labels based on observed labels for
each POS tag pair in the training data. By doing so,
we could speed up our parser to 500-600 tokens per
second with less than a 0.2% drop in both UAS and
LAS.
3.2 Importance of Update Strategies
The lower portion of Table 1 compares cube-pruning
parsing with different online update strategies in or-
der to show the importance of choosing an update
strategy that accommodates search errors. The max-
violation update strategies (s-max and p-max) im-
proved results on both versions of the Penn Treebank
as well as the CTB-5 Chinese treebank. It made
a larger difference on Penn-S relative to Penn-YM,
improving as much as 0.93% in LAS against the skip
update strategy. Additionally, we measured the per-
centage of non-violation updates at root nodes. In
the last epoch of training, on Penn-YM, there was
24% non-violations if we used the skip update strat-
egy; on Penn-S, there was 36% non-violations. The
portion of non-violations indicates the inexactness
 92
 92.2
 92.4
 92.6
 92.8
 93
 93.2
 93.4
 93.6
 93.8
 94
 1  2  3  4  5  6  7  8
UA
S
epochs
UAS on Penn-YM dev
s-max
p-max
skip
standard
Figure 2: Constrast of different update strategies on the
validation data set of Penn-YM. The x-axis is the number
of training epochs. The y-axis is the UAS score. s-max
stands for single-node max-violation. p-max stands for
parallel max-violation.
of the underlying search. Search is harder on Penn-S
due to the larger label set. Thus, as expected, max-
violation update strategies improve most where the
search is the hardest and least exact.
Figure 2 shows accuracy per training epoch on the
validation data. It can be seen that bad update strate-
gies are not simply slow learners. More iterations
of training cannot close the gap between strategies.
Forcing invalid updates on non-violations (standard
update) or simply ignoring them (skip update) pro-
duces less accurate models overall.
911
ZN 2011 (reimpl.) skip s-max p-max Best Published?
Language UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS
SPANISH 86.76 83.81 87.34 84.15 87.96 84.95 87.68 84.75 87.48 84.05
CATALAN 94.00 88.65 94.54 89.14 94.58 89.05 94.98 89.56 94.07 89.09
JAPANESE 93.10 91.57 93.40 91.65 93.26 91.67 93.20 91.49 93.72 91.7-
BULGARIAN 93.08 89.23 93.52 89.25 94.02 89.87 93.80 89.65 93.50 88.23
ITALIAN 87.31 82.88 87.75 83.41 87.57 83.22 87.79 83.59 87.47 83.50
SWEDISH 90.98 85.66 90.64 83.89 91.62 85.08 91.62 85.00 91.44 85.42
ARABIC 78.26 67.09 80.42 69.46 80.48 69.68 80.60 70.12 81.12 66.9-
TURKISH 76.62 66.00 76.18 65.90 76.94 66.80 76.86 66.56 77.55 65.7-
DANISH 90.84 86.65 91.40 86.59 91.88 86.95 92.00 87.07 91.86 84.8-
PORTUGUESE 91.18 87.66 91.69 88.04 92.07 88.30 92.19 88.40 93.03 87.70
GREEK 85.63 78.41 86.37 78.29 86.14 78.20 86.46 78.55 86.05 77.87
SLOVENE 84.63 76.06 85.01 75.92 86.01 77.14 85.77 76.62 86.95 73.4-
CZECH 87.78 82.38 86.92 80.36 88.36 82.16 88.48 82.38 90.32 80.2-
BASQUE 79.65 71.03 79.57 71.43 79.59 71.52 79.61 71.65 80.23 73.18
HUNGARIAN 84.71 80.16 85.67 80.84 85.85 81.02 86.49 81.67 86.81 81.86
GERMAN 91.57 89.48 91.23 88.34 92.03 89.44 91.79 89.28 92.41 88.42
DUTCH 82.49 79.71 83.01 79.79 83.57 80.29 83.35 80.09 86.19 79.2-
AVG 86.98 81.55 87.33 81.56 87.76 82.08 87.80 82.14
Table 2: Parsing Results for languages from CoNLL 2006/2007 shared tasks. When a language is in both years,
we use the 2006 data set. The best results with ? are the maximum in the following papers: Buchholz and Marsi
(2006), Nivre et al (2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al (2013), For
consistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script. ZN 2011 (reimpl.) is
our reimplementation of Zhang and Nivre (2011), with a beam of 64. Results in bold are the best among ZN 2011
reimplementation and different update strategies from this paper.
3.3 CoNLL Results
We also report parsing results for 17 languages from
the CoNLL 2006/2007 shared-task (Buchholz and
Marsi, 2006; Nivre et al, 2007). The parser in
our experiments can only produce projective depen-
dency trees as it uses an Eisner algorithm backbone
to generate the hypergraph (Eisner, 1996). So, at
training time, we convert non-projective trees ? of
which there are many in the CoNLL data ? to projec-
tive ones through flattening, i.e., attaching words to
the lowest ancestor that results in projective trees. At
testing time, our parser can only predict projective
trees, though we evaluate on the true non-projective
trees.
Table 2 shows the full results. We sort the
languages according to the percentage of non-
projective trees in increasing order. The Spanish
treebank is 98% projective, while the Dutch tree-
bank is only 64% projective. With respect to the
Zhang and Nivre (2011) baseline, we improved UAS
in 16 languages and LAS in 15 languages. The im-
provements are stronger for the projective languages
in the top rows. We achieved the best published
UAS results for 7 languages: Spanish, Catalan, Bul-
garain, Italian, Swedish, Danish, and Greek. As
these languages are typically from the more projec-
tive data sets, we speculate that extending the parser
used in this study to handle non-projectivity will
lead to state-of-the-art models for the majority of
languages.
4 Conclusions
We proposed perceptron update strategies for in-
exact hypergraph search and experimented with
a cube-pruning dependency parser. Both single-
node max-violation and parallel max-violation up-
date strategies signficantly improved parsing results
over the strategy that ignores any invalid udpates
caused by inexactness of search. The update strate-
gies are applicable to any bottom-up parsing prob-
lems such as constituent parsing (Huang, 2008) and
syntax-based machine translation with online learn-
ing (Chiang et al, 2008).
Acknowledgments: We thank Andre? F. T. Martins
for the dependency converted Penn Treebank with
automatic POS tags from his experiments; the re-
viewers for their useful suggestions; the NLP team
at Google for numerous discussions and comments;
Liang Huang and Kai Zhao are supported in part by
DARPA FA8750-13-2-0041 (DEFT), PSC-CUNY,
and a Google Faculty Research Award.
912
References
B. Bohnet and J. Kuhn. 2012. The best of bothworlds
- a graph-based completion model for transition-based
parsers. In Proc. of EACL.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. of ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
H. Daume? and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proc. of ICML.
M. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. of COL-
ING.
L. Huang, S. Fayong, and G. Yang. 2012. Structured
perceptron with inexact search. In Proc. of NAACL.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
X. Ma and H. Zhao. 2012. Fourth-order dependency
parsing. In Proc. of COLING.
A. F. T. Martins, N. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, M. B. Almeida, and N. A. Smith. 2013.
Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
X. Qian and Y. Liu. 2013. Branch and bound algo-
rithm for dependency parsing with non-local features.
TACL, Vol 1.
A. Rush and S. Petrov. 2012. Efficient multi-pass depen-
dency pruning with vine parsing. In Proc. of NAACL.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Proc.
of EMNLP.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL-HLT, volume 2.
913
Analyzing and Integrating
Dependency Parsers
Ryan McDonald?
Google Inc.
Joakim Nivre??
Uppsala University
There has been a rapid increase in the volume of research on data-driven dependency parsers in
the past five years. This increase has been driven by the availability of treebanks in a wide variety
of languages?due in large part to the CoNLL shared tasks?as well as the straightforward
mechanisms by which dependency theories of syntax can encode complex phenomena in free word
order languages. In this article, our aim is to take a step back and analyze the progress that has
been made through an analysis of the two predominant paradigms for data-driven dependency
parsing, which are often called graph-based and transition-based dependency parsing. Our
analysis covers both theoretical and empirical aspects and sheds light on the kinds of errors each
type of parser makes and how they relate to theoretical expectations. Using these observations,
we present an integrated system based on a stacking learning framework and show that such a
system can learn to overcome the shortcomings of each non-integrated system.
1. Introduction
Syntactic dependency representations have a long history in descriptive and theoretical
linguistics and many formal models have been advanced, most notably Word Gram-
mar (Hudson 1984), Meaning-Text Theory (Mel?c?uk 1988), Functional Generative De-
scription (Sgall, Hajic?ova?, and Panevova? 1986), and Constraint Dependency Grammar
(Maruyama 1990). Common to all theories is the notion of directed syntactic depen-
dencies between the words of a sentence, an example of which is given in Figure 1 for
the sentence A hearing is scheduled on the issue today, which has been extracted from the
Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). A dependency graph of
a sentence represents each word and its syntactic modifiers through labeled directed
arcs, where each arc label comes from some finite set representing possible syntactic
roles. Returning to our example in Figure 1, we can see multiple instances of labeled
dependency relations such as the one from the finite verb is to hearing labeled SBJ
indicating that hearing is the head of the syntactic subject of the finite verb. An artificial
word has been inserted at the beginning of the sentence that will always serve as the
single root of the graph and is primarily a means to simplify computation.
? 76 Ninth Ave., New York, NY 10011. E-mail: ryanmcd@google.com.
?? Department of Linguistics and Philology, Box 635, SE-75126 Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
Submission received: 25 August 2009; revised submission received: 20 August 2010; accepted for publication:
7 October 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
Figure 1
Dependency graph for an English sentence.
Syntactic dependency graphs have recently gained a wide interest in the computa-
tional linguistics community and have been successfully employed for many problems
ranging from machine translation (Ding and Palmer 2004) to ontology construction
(Snow, Jurafsky, and Ng 2005). A primary advantage of dependency representations
is that they have a natural mechanism for representing discontinuous constructions,
which arise due to long-distance dependencies or in languages where grammatical
relations are often signaled by morphology instead of word order. This is undoubt-
edly one of the reasons for the emergence of dependency parsers for a wide range of
languages (Buchholz and Marsi 2006; Nivre et al 2007). Thus, the example in Figure 1
contains an instance of a discontinuous construction through the subgraph rooted at
the word hearing. Specifically, the dependency arc from hearing to on spans the words
is and scheduled, which are not nodes in this subgraph. An arc of this kind is said to be
non-projective.
In this article we focus on a common paradigm called data-driven dependency
parsing, which encompasses parsing systems that learn to produce dependency graphs
for sentences from a corpus of sentences annotated with dependency graphs. The
advantage of such models is that they are easily ported to any domain or language
in which annotated resources exist. Many data-driven parsing systems are grammar-
less, in that they do not assume the existence of a grammar that defines permissible
sentences of the language. Instead, the goal of most data-driven parsing systems is to
discriminate good parses from bad for a given sentence, regardless of its grammaticality.
Alternatively, one can view such systems as parsers for a grammar that induces the
language of all strings.
The rise of statistical methods in natural language processing coupled with the
availability of dependency annotated corpora for multiple languages?most notably
from the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi 2006; Nivre et al
2007)?has led to a boom in research on data-driven dependency parsing. Making sense
of this work is a challenging problem, but an important one if the field is to continue to
make advances. Of the many important questions to be asked, three are perhaps most
crucial at this stage in the development of parsers:
1. How can we formally categorize the different approaches to data-driven
dependency parsing?
2. Can we characterize the kinds of errors each category of parser makes
through an empirical analysis?
3. Can we benefit from such an error analysis and build improved parsers?
The organizers of the CoNLL-X shared task on dependency parsing (Buchholz and
Marsi 2006) point out that there are currently two dominant approaches for data-driven
198
McDonald and Nivre Analyzing and Integrating Dependency Parsers
dependency parsing. The first category parameterizes models over dependency sub-
graphs and learns these parameters to globally score correct graphs above incorrect
ones. Inference is also global, in that systems attempt to find the highest scoring graph
among the set of all graphs. We call such systems graph-based parsing models to
reflect the fact that parameterization is over the graph. Graph-based models are mainly
associated with the pioneering work of Eisner (Eisner 1996), as well as McDonald and
colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al 2005; McDonald
and Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C?ak?c?, and
Meza-Ruiz 2006; Carreras 2007; Koo et al 2007; Nakagawa 2007; Smith and Smith 2007).
The second category of parsing systems parameterizes models over transitions from one
state to another in an abstract state-machine. Parameters in these models are typically
learned using standard classification techniques that learn to predict one transition from
a set of permissible transitions given a state history. Inference is local, in that systems
start in a fixed initial state and greedily construct the graph by taking the highest
scoring transitions at each state entered until a termination condition is met. We call
such systems transition-based parsing models to reflect the fact that parameterization
is over possible state transitions. Transition-based models have been promoted by
the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003;
Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre and
Nilsson 2005; Nivre et al 2006), and others (Attardi 2006; Attardi and Ciaramita 2007;
Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a,
2007b).
It is important to note that there is no a priori reason why a graph-based pa-
rameterization should require global learning and inference, and a transition-based
parameterization would necessitate local learning and greedy inference. Nevertheless,
as observed by Buchholz and Marsi (2006), it is striking that recent work on data-driven
dependency parsing has been dominated by global, exhaustive, graph-based models, on
the one hand, and local, greedy, transition-based models, on the other. Therefore, a careful
comparative analysis of these model types appears highly relevant, and this is what we
will try to provide in this article. For convenience, we will use the shorthand terms
?graph-based? and ?transition-based? for these models, although both graph-based
and transition-based parameterizations can be (and have been) combined with different
types of learning and inference. For example, the system described by Zhang and Clark
(2008) could be characterized as a transition-based model with global learning, and the
ensemble system of Zeman and Z?abokrtsky` (2005) as a graph-based model with greedy
inference.
Perhaps the most interesting reason to study the canonical graph-based and
transition-based models is that even though they appear to be quite different theoret-
ically (see Section 2), recent empirical studies show that both obtain similar parsing
accuracies on a variety of languages. For example, Table 1 shows the results of the two
top performing systems in the CoNLL-X shared task, those of McDonald, Lerman, and
Pereira (2006) (graph-based) and Nivre et al (2006) (transition-based), which exhibit
no statistically significant difference in accuracy when averaged across all languages.
This naturally leads us to our Question 2, that is, can we empirically characterize the
errors of these systems to understand whether, in practice, these errors are the same
or distinct? Towards this end, Section 2 describes in detail the theoretical properties
and expectations of these two parsing systems and Section 4 provides a fine-grained
error analysis of each system on the CoNLL-X shared task data sets (Buchholz and
Marsi 2006). The result of this analysis strongly suggests that (1) the two systems do
make different, yet complementary, errors, which lends support to the categorization of
199
Computational Linguistics Volume 37, Number 1
Table 1
Labeled parsing accuracy for top-scoring systems at CoNLL-X (Buchholz and Marsi 2006).
Language Graph-based Transition-based
(McDonald, Lerman, and Pereira 2006) (Nivre et al 2006)
Arabic 66.91 66.71
Bulgarian 87.57 87.41
Chinese 85.90 86.92
Czech 80.18 78.42
Danish 84.79 84.77
Dutch 79.19 78.59
German 87.34 85.82
Japanese 90.71 91.65
Portuguese 86.82 87.60
Slovene 73.44 70.30
Spanish 82.25 81.29
Swedish 82.55 84.58
Turkish 63.19 65.68
Average 80.83 80.75
parsers as graph-based and transition-based, and (2) the errors made by each system
are directly correlated with our expectations, based on their theoretical underpinnings.
This leads to our Question 3: Can we use these insights to integrate parsers and
achieve improved accuracies? In Section 5 we consider a simple way of integrating
graph-based and transition-based models in order to exploit their complementary
strengths and thereby improve parsing accuracy beyond what is possible by either
model in isolation. The method integrates the two models by allowing the output of one
model to define features for the other, which is commonly called ?classifier stacking.?
This method is simple?requiring only the definition of new features?and robust by
allowing a model to learn relative to the predictions of the other. More importantly, we
rerun the error analysis and show that the integrated models do indeed take advantage
of the complementary strengths of both the graph-based and transition-based parsing
systems.
Combining the strengths of different machine learning systems, and even parsing
systems, is by no means new as there are a number of previous studies that have looked
at combining phrase-structure parsers (Henderson and Brill 1999), dependency parsers
(Zeman and Z?abokrtsky` 2005), or both (McDonald 2006). Of particular note is past work
on combining graph-based and transition-based dependency parsers. Sagae and Lavie
(2006) present a system that combines multiple transition-based parsers with a single
graph-based parser by weighting each potential dependency relation by the number of
parsers that predicted it. A final dependency graph is predicted by using spanning tree
inference algorithms from the graph-based parsing literature (McDonald et al 2005).
Sagae and Lavie report improvements of up to 1.7 percentage points over the best single
parser when combining three transition-based models and one graph-based model for
unlabeled dependency parsing, evaluated on data from the Penn Treebank. The same
technique was used by Hall et al (2007) to combine six transition-based parsers in the
best performing system in the CoNLL 2007 shared task.
Zhang and Clark (2008) propose a parsing system that uses global learning coupled
with beam search over a transition-based backbone incorporating both graph-based
200
McDonald and Nivre Analyzing and Integrating Dependency Parsers
and transition-based features, that is, features over both sub-graphs and transitions.
Huang and Sagae (2010) go even further and show how transition-based parsing can be
tabularized to allow for dynamic programming, which in turn permits an exponentially
larger search space. Martins et al (2008) present a method for integrating graph-based
and transition-based parsers based on stacking, which is similar to the approach taken
in this work. Other studies have tried to overcome the weaknesses of parsing models
by changing the underlying model structure directly. For example, Hall (2007), Riedel,
C?ak?c?, and Meza-Ruiz (2006), Nakagawa (2007), Smith and Eisner (2008), and Martins,
Smith, and Xing (2009) attempt to overcome local restrictions in feature scope for graph-
based parsers through both approximations and exact solutions with integer linear
programming.
Our work differs from past studies in that we attempt to quantify exactly the
types of errors these parsers make, tie them to their theoretical expectations, and show
that integrating graph-based and transition-based parsers not only increases overall
accuracy, but does so directly exploiting the strengths of each system. Thus, this is the
first large-scale error analysis of modern data-driven dependency parsers.1 The rest
of the article is structured as follows: Section 2 describes canonical graph-based and
transition-based parsing systems and discusses their theoretical benefits and limitations
with respect to one another; Section 3 introduces the experimental setup based on the
CoNLL-X shared task data sets that incorporate dependency treebanks from 13 diverse
languages; Section 4 gives a fine-grained error analysis for the two parsers in this setup;
Section 5 describes a stacking-based dependency parser combination framework;
Section 6 evaluates the stacking-based parsers in comparison to the original systems
with a detailed error analysis; we conclude in Section 7.
2. Two Models for Dependency Parsing
In this section we introduce central notation and define canonical graph-based and
transition-based dependency parsing at an abstract level. We further compare and
contrast their theoretical underpinnings with an eye to understanding the kinds of
errors each system is likely to make in practice.
2.1 Preliminaries
Let L = {l1, . . . , l|L|} be a set of permissible arc labels. Let x = w0, w1, . . . , wn be an input
sentence where w0 = ROOT. Formally, a dependency graph for an input sentence x is a
labeled directed graph G = (V, A) consisting of a set of nodes V and a set of labeled
directed arcs A ? V ? V ? L; that is, if (i, j, l) ? A for i, j ? V and l ? L, then there is
an arc from node i to node j with label l in the graph. In terms of standard linguistic
dependency theory nomenclature, we say that (i, j, l) ? A if there is a dependency with
head wi, dependent wj, and syntactic role l.
A dependency graph G for sentence x must satisfy the following properties:
1. V = {0, 1, . . . , n}.
2. If (i, j, l) ? A, then j = 0.
1 This work has previously been published partially in McDonald and Nivre (2007) and Nivre and
McDonald (2008).
201
Computational Linguistics Volume 37, Number 1
3. If (i, j, l) ? A, then for all arcs (i?, j, l?) ? A, i = i? and l = l?.
4. For all j ? V ? {0}, either (0, j, l) for some l ? L or there is a non-empty
sequence of nodes i1, . . . , im ? V and labels l1, . . . , lm+1 ? L such that
(0, i1, l1),(i1, i2, l2), . . . , (im, j, lm+1)?A.
The first constraint states that the dependency graph spans the entire input. The sec-
ond constraint states that the node 0 is a root. The third constraint states that each
node has at most one incoming arc in the graph. The final constraint states that the
graph is connected through directed paths from the node 0 to every other node in
the graph. It is not difficult to show that a dependency graph satisfying these con-
straints is in fact a directed tree originating out of the root node 0 and we will use
the term dependency tree to refer to any valid dependency graph. The characterization
of syntactic dependency graphs as trees is consistent with most formal theories (e.g.,
Sgall, Hajic?ova?, and Panevova? 1986; Mel?c?uk 1988). Exceptions include Word Grammar
(Hudson 1984), which allows a word to modify multiple other words in the sentence,
which results in directed acyclic graphs with nodes possibly having multiple incoming
arcs.
We define an arc (i, j, l) connecting words wi and wj as non-projective if at least
one word occurring between wi and wj in the input sentence is not a descendant of
wi (where ?descendant? is the transitive closure of the arc relation). Alternatively, we
can view non-projectivity in trees as breaking the nested property, which can be seen
through the arcs that cross in the example in Figure 1. Non-projective dependencies
are typically difficult to represent or parse in phrase-based models of syntax. This can
either be due to nested restrictions arising in context-free formalisms or computationally
expensive operations in mildly context-sensitive formalisms (e.g., adjunction in TAG
frameworks).
2.2 Global, Exhaustive, Graph-Based Parsing
For an input sentence, x = w0, w1, . . . , wn consider the dense graph Gx = (Vx, Ax) de-
fined as:
1. Vx = {0, 1, . . . , n}.
2. Ax = {(i, j, l) | i, j ? Vx and l ? L}.
Let D(Gx) represent the subgraphs of graph Gx that are valid dependency graphs for the
sentence x, that is, dependency trees. Because Gx contains all possible labeled arcs, the
set D(Gx) must necessarily contain all dependency trees for x.
Assume that there exists a dependency arc scoring function, s : V ? V ? L ? R.
Furthermore, define the score of a graph as the sum of its arc scores,
s(G = (V, A)) =
?
(i,j,l)?A
s(i, j, l)
The score of an arc, s(i, j, l) represents the likelihood of creating a dependency from
head wi to modifier wj with the label l in a dependency tree. This score is commonly
defined to be the product of a high dimensional feature representation of the arc and a
202
McDonald and Nivre Analyzing and Integrating Dependency Parsers
learned parameter vector, s(i, j, l) = w ? f(i, j, l). If the arc score function is known, then
the parsing problem can be stated as
G? = arg max
G?D(Gx )
s(G) = arg max
G?D(Gx )
?
(i,j,l)?A
s(i, j, l) (1)
An example graph Gx and the dependency tree maximizing the scoring function are
given in Figure 2 for the sentence John saw Mary. We omit arcs into the root node for
simplicity.
McDonald et al (2005) showed that this problem is equivalent to finding the highest
scoring directed spanning tree for the graph Gx originating out of the root node 0. It is
not difficult to see this, because both dependency trees and spanning trees must contain
all nodes of the graph and must have a tree structure with root 0. The directed spanning
tree problem (also known as the r-arborescence problem) can be solved for both the
labeled and unlabeled case using the Chu-Liu-Edmonds algorithm (Chu and Liu 1965;
Edmonds 1967), a variant of which can be shown to have an O(n2) runtime (Tarjan
1977). Non-projective arcs are produced naturally through the inference algorithm that
searches over all possible directed trees, whether projective or not.
Graph-based parsers are typically trained using structured learning algorithms
(McDonald, Crammer, and Pereira 2005; Koo et al 2007; Smith and Smith 2007), which
optimize the parameters of the model to maximize the difference in score/probability
between the correct dependency graph and all incorrect dependency graphs for every
sentence in a training set. Such a learning procedure is global because model parameters
are set relative to the classification of the entire dependency graph, and not just over
single arc attachment decisions. Although a learning procedure that only optimizes the
score of individual arcs is conceivable, it would not be likely to produce competitive
results.
Going beyond arc-factored models, McDonald and Pereira (2006) presented a
system where scores are increased in scope to include pairs of adjacent arcs in the
dependency graph. In the case of projective dependency trees, polynomial time
parsing algorithms were shown to exist, but non-projective trees required approximate
inference that used an exhaustive projective algorithm followed by transformations
to the graph that incrementally introduce non-projectivity. In general, inference and
learning for graph-based dependency parsing is NP-hard when the score is factored
Figure 2
A graph-based parsing example. A dense graph Gx is shown on the left (arcs into the root are
omitted) with corresponding arc scores. On the right is the predicted dependency tree based on
Equation (1).
203
Computational Linguistics Volume 37, Number 1
over anything larger than arcs (McDonald and Satta 2007). Thus, graph-based parsing
systems cannot easily condition on any extended scope of the dependency graph
beyond a single arc, which is their primary shortcoming relative to transition-based
systems. McDonald, Crammer, and Pereira (2005) show that a rich feature set over
the input space, including lexical and surface syntactic features of neighboring words,
can partially alleviate this problem, and both Carreras (2007) and Koo et al (2010)
explore higher-order models for projective trees. Additionally, work has been done on
approximate non-factored parsing systems (McDonald and Pereira 2006; Hall 2007;
Nakagawa 2007; Smith and Eisner 2008) as well as exact solutions through integer linear
programming (Riedel, C?ak?c?, and Meza-Ruiz 2006; Martins, Smith, and Xing 2009).
The specific graph-based system studied in this work is that presented by
McDonald, Lerman, and Pereira (2006), which uses pairwise arc scoring and approx-
imate exhaustive search for unlabeled parsing. A separate arc label classifier is then
used to label each arc. This two-stage process was adopted primarily for computational
reasons and often does not affect performance significantly (see McDonald [2006] for
more). Throughout the rest of this study we will refer to this system as MSTParser (or
MST for short), which is also the name of the freely available implementation.2
2.3 Local, Greedy, Transition-Based Parsing
A transition system for dependency parsing defines
1. a set C of parser configurations, each of which defines a (partially built)
dependency graph G,
2. a set T of transitions, each of which is a partial function t : C ? C,
3. for every sentence x = w0, w1, . . . , wn,
(a) a unique initial configuration cx,
(b) a set Cx of terminal configurations.
A transition sequence C0,m = (c0, c1, . . . , cm) for a sentence x = w0, w1, . . . , wn is a se-
quence of configurations such that c0 = cx, cm ? Cx, and, for every ci (1 ? i ? m), there
is a transition t ? T such that ci = t(ci?1). The dependency graph assigned to a sentence
x = w0, w1, . . . , wn by a sequence C0,m = (c0, c1, . . . , cm) is the graph Gm defined by the
terminal configuration cm.
Assume that there exists a transition scoring function, s : C ? T ? R. The score of a
transition t in a configuration c, s(c, t), represents the likelihood of taking transition t out
of configuration c in a transition sequence leading to the optimal dependency graph for
the given sentence. This score is usually defined by a classifier g taking as input a high
dimensional feature representation of the configuration, s(c, t) = g(f(c), t).
Given a transition scoring function, the parsing problem consists in finding a ter-
minal configuration cm ? Cx, starting from the initial configuration cx and taking the
optimal transition t? out of every configuration c:
t? = arg max
t?T
s(c, t)
2 http://mstparser.sourceforge.net.
204
McDonald and Nivre Analyzing and Integrating Dependency Parsers
This can be seen as a greedy search for the optimal dependency graph, based on a
sequence of locally optimal decisions in terms of the transition system.
By way of example, we consider the transition system first presented in Nivre
(2003), where a parser configuration is a triple c = (?,?, A), consisting of a stack ?
of partially processed nodes, a buffer ? of remaining input nodes, and a set A of
labeled dependency arcs. The initial configuration for a sentence x = w0, w1, . . . , wn is
cx = ([0], [1, . . . , n], ?) and the set of terminal configurations Cx contains all configu-
rations of the form c = (?, [ ], A) (that is, all configurations with an empty buffer and
with arbitrary ? and A). The set T of transitions for this system is specified in Figure 3.
The transitions LEFT-ARCl and RIGHT-ARCl extend the arc set A with an arc (labeled l)
connecting the top node i on the stack and the first node j of the buffer. In the case of
LEFT-ARCl, the node i becomes the dependent and is also popped from the stack; in
the case of RIGHT-ARCl, the node j becomes the dependent and is also pushed onto the
stack. The REDUCE transition pops the stack (and presupposes that the top node has
already been attached to its head in a previous RIGHT-ARCl transition), and the SHIFT
transition extracts the first node of the buffer and pushes it onto the stack.
This system can derive any projective dependency tree G for an input sentence
x and in doing so always adds arcs as early as possible. For this reason, the system
is often referred to as arc-eager. When coupled with the greedy deterministic parsing
strategy, the system guarantees termination after at most 2n transitions (for a sentence
of length n), which means that the time complexity is O(n) given that transitions can be
performed in constant time. The dependency graph given at termination is guaranteed
to be acyclic and projective and to satisfy dependency graph conditions 1?3, which
means that it can always be turned into a well-formed dependency graph by adding
arcs (0, i, lr) for every node i = 0 that is a root in the output graph (where lr is a spe-
cial label for root modifiers). Whereas the initial formulation in Nivre (2003) was lim-
ited to unlabeled dependency graphs, the system was extended to labeled graphs in
Nivre, Hall, and Nilsson (2004), and Nivre and Nilsson (2005) showed how the restric-
tion to projective dependency graphs could be lifted by using graph transformation
Figure 3
Transitions for dependency parsing (with preconditions).
205
Computational Linguistics Volume 37, Number 1
techniques to pre-process training data and post-process parser output, a technique
called pseudo-projective parsing. Transition systems that derive non-projective trees
directly have been explored by Attardi (2006) and Nivre (2007, 2009), among others.
To learn a scoring function, transition-based parsers use discriminative learning
methods, such as memory-based learning or support vector machines. The training
data are obtained by constructing transition sequences corresponding to gold standard
parses from a treebank. The typical learning procedure is local because only single
transitions are scored?not entire transition sequences?but more global optimization
methods have also been proposed. The primary advantage of these models is that the
feature representation is not restricted to a limited number of graph arcs but can take
into account the entire dependency graph built so far, including previously assigned
labels, and still support efficient inference and learning. The main disadvantage is that
the greedy parsing strategy may lead to error propagation as false early predictions can
eliminate valid trees due to structural constraints and are also used to create features
when making future predictions. Using beam search instead of strictly deterministic
parsing can to some extent alleviate this problem but does not eliminate it.
The specific transition-based system studied in this work is that presented by Nivre
et al (2006), which uses the projective, arc-eager system described here in combina-
tion with pseudo-projective parsing, which uses support vector machines to learn the
scoring function for transitions and which uses greedy, deterministic one-best search at
parsing time. We will refer to this system as MaltParser (or Malt for short), which is also
the name of the freely available implementation.3
2.4 Comparison
In the previous two sections we have outlined the theoretical characteristics of canonical
graph-based and transition-based dependency parsing systems. From now on, our
experiments will rely on two standard implementations: MSTParser, a graph-based
system, and MaltParser, a transition-based system. Here we contrast the two parsing
systems with respect to how they are trained, how they produce dependency trees for
new sentences, and what kinds of features they use.
Training Algorithms. Both systems use large-margin learning for linear classifiers. MST-
Parser uses on-line algorithms (McDonald, Crammer, and Pereira 2005; Crammer et al
2006) and MaltParser uses support vector machines (Cortes and Vapnik 1995). The
primary difference is that MaltParser trains the model to make a single classification
decision (create arc, shift, reduce, etc.), whereas MSTParser trains the model to maxi-
mize the global score of correct graphs relative to incorrect graphs. It has been argued
that locally trained algorithms can suffer from label bias issues (Lafferty, McCallum,
and Pereira 2001). However, it is expensive to train global models since the complexity
of learning is typically proportional to inference. In addition, MaltParser makes use of
kernel functions, which eliminates the need for explicit conjunctions of features.
Inference. MaltParser uses a transition-based inference algorithm that greedily chooses
the best parsing decision based on a trained classifier and current parser history. MST-
Parser instead uses exhaustive search over a dense graphical representation of the
3 http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
206
McDonald and Nivre Analyzing and Integrating Dependency Parsers
sentence to find the dependency graph that maximizes the score. On the one hand,
the greedy algorithm is far quicker computationally (O(n) vs. O(n2) for the Chu-Liu-
Edmonds algorithm and O(n3) for Eisner?s algorithm). On the other hand, it may be
prone to error propagation when early incorrect decisions negatively influence the
parser at later stages. In particular, MaltParser uses the projective arc-eager transition
system first described in Nivre (2003), which has consequences for the form of error
propagation we may expect to see because the system determines the order in which
arcs must be added to the graph. On the one hand, if an arc (i, m, l) covers another
arc ( j, k, l?) (i.e., i ? j and k ? m), then the smaller arc ( j, k, l?) has to be added to the
graph first (because of projectivity). On the other hand, if two arcs (i, j, l) and (k, m, l?)
do not overlap, then the leftmost arc has to be added first (because of arc-eagerness).
Therefore, we can expect error propagation from shorter to longer overlapping arcs and
from preceding to succeeding arcs.
Feature Representation. Due to the nature of their inference and training algorithms, the
feature representations of the two systems differ substantially. MaltParser can introduce
a rich feature space based on the history of previous parser decisions. This is because
the greedy nature of the algorithm allows it to fix the structure of the graph and
use this structure to help improve future parsing decisions. By contrast, MSTParser is
forced to restrict the scope of features to a single or pair of nearby parsing decisions
in order to make exhaustive inference tractable. As a result, the feature representation
available to the locally trained greedy models is much richer than the globally trained
exhaustive models. Concisely, we can characterize MSTParser as using global training
and inference with local features and MaltParser as using local training and inference
with global features. (For more information about the features used in the two systems,
see Sections 3.2 and 3.3.)
These differences highlight an inherent trade-off between exhaustive inference algo-
rithms plus global learning and expressiveness of feature representations. MSTParser
favors the former at the expense of the latter and MaltParser the opposite. When
analyzing, and ultimately explaining, the empirical difference between the systems,
understanding this trade-off will be of central importance.
3. Experimental Setup
The experiments presented in this article are all based on data from the CoNLL-X
shared task (Buchholz and Marsi 2006). In this section we first describe the task and the
resources created there and then describe how MSTParser and MaltParser were trained
for the task, including feature representations and learning algorithms.
3.1 The CoNLL-X Shared Task
The CoNLL-X shared task (Buchholz and Marsi 2006) was a large-scale evaluation
of data-driven dependency parsers, with data from 13 different languages and 19 par-
ticipating systems. The data sets were quite heterogeneous, both with respect to size
and with respect to linguistic annotation principles, and the best reported parsing
accuracy varied from 65.7% for Turkish to 91.7% for Japanese. The official evaluation
metric was the labeled attachment score (LAS), defined as the percentage of tokens,
207
Computational Linguistics Volume 37, Number 1
Table 2
Data sets. Tok = number of tokens (?1000); Sen = number of sentences (?1000); T/S = tokens per
sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained part-of-speech
tags; PoS = number of (fine-grained) part-of-speech tags; MSF = number of morphosyntactic
features (split into atoms); Dep = number of dependency types; NPT = proportion of
non-projective dependencies/tokens (%); NPS = proportion of non-projective dependency
graphs/sentences (%).
Language Tok Sen T/S Lem CPoS PoS MSF Dep NPT NPS
Arabic 54 1.5 37.2 yes 14 19 19 27 0.4 11.2
Bulgarian 190 14.4 14.8 no 11 53 50 18 0.4 5.4
Chinese 337 57.0 5.9 no 22 303 0 82 0.0 0.0
Czech 1,249 72.7 17.2 yes 12 63 61 78 1.9 23.2
Danish 94 5.2 18.2 no 10 24 47 52 1.0 15.6
Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4
German 700 39.2 17.8 no 52 52 0 46 2.3 27.8
Japanese 151 17.0 8.9 no 20 77 0 7 1.1 5.3
Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9
Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2
Spanish 89 3.3 27.0 yes 15 38 33 21 0.1 1.7
Swedish 191 11.0 17.3 no 37 37 0 56 1.0 9.8
Turkish 58 5.0 11.5 yes 14 30 82 25 1.5 11.6
excluding punctuation, that are assigned both the correct head and the correct depen-
dency label.4
The outputs of all systems that participated in the shared task are available for
download and constitute a rich resource for comparative error analysis. In Section 4,
we will use the outputs of MSTParser and MaltParser for all 13 languages, together
with the corresponding gold standard graphs used in the evaluation, as the basis for an
in-depth error analysis designed to answer Question 2 from Section 1. In Section 6, we
will then evaluate our stacking-based parsers on the same data sets and repeat the error
analysis. This will allow us to compare the error profiles of the new and old systems at
a much finer level of detail than in standard evaluation campaigns.
Table 2 gives an overview of the training sets available for the 13 languages. First
of all, we see that training set size varies from over 1.2 million words and close to
73,000 sentences for Czech to only 29,000 words and 1,500 sentences for Slovene. We
also see that the average sentence length varies from close to 40 words for Arabic, using
slightly different principles for sentence segmentation than the other languages, to less
than 10 words for Japanese, where the data consist of transcribed spoken dialogues.
Differences such as these can be expected to have a large impact on the parsing accuracy
obtained for different languages, and it is probably significant that Japanese has the
highest top score of all languages, whereas Arabic has the second lowest. We also see
that the amount of information available in the input, in the form of lemmas (Lem),
coarse and fine part-of-speech tags (CPoS, PoS), and morphosyntactic features (MSF)
varies considerably, as does the granularity of the dependency label sets (Dep). Fi-
nally, the proportion of non-projective structures, whether measured on the token level
(NPT) or on the sentence level (NPS), is another important source of variation.
4 In addition, results were reported for unlabeled attachment score (UAS) (tokens with the correct head)
and label accuracy (LA) (tokens with the correct label).
208
McDonald and Nivre Analyzing and Integrating Dependency Parsers
The final test set for each language has been standardized in size to about 5,000
words, which makes it possible to evaluate the performance of a system over all lan-
guages by simply concatenating the system?s output for all test sets and comparing this
to the concatenation of the gold standard test sets. Thus, most of the statistics used in the
subsequent error analysis are based on the concatenation of all test sets. Because some of
the phenomena under study are relatively rare, this allows us to get more reliable esti-
mates, even though these estimates inevitably hide important inter-language variation.
Analyzing individual languages in more detail would be an interesting complementary
study but is beyond the scope of this article.
3.2 Training MSTParser
MSTParser operates primarily over arc-scores, s(i, j, l), which are parameterized by a
linear combination of a parameter vector, w, and a corresponding feature vector for the
arc, f(i, j, l). We use a two-stage approach to training. The first-stage learns a model to
predict unlabeled dependency trees for a sentence. Thus, arc scores do not condition
on possible labels and are parameterized by features only over the head-modifier pair,
s(i, j) = w ? f(i, j). As a result, for the first stage of training the parser, we must define
the feature representation f(i, j), which is outlined in Table 3(a) and Table 3(b) for a
potential unlabeled arc (i, j). These features represent both information about the head
and modifier in the dependency relation as well as the context of the dependency via
local part-of-speech information. We include context part-of-speech features for both
the fine-grained and coarse-grained tags (when available).
As mentioned in Section 2.2, the implementation of MSTParser used in our exper-
iments also contains features over adjacent arcs, (i, j) and (i, k), which we will denote
compactly as (i, j  k). Scores for adjacent arcs are also defined as a linear combination
between weights and a feature vector s(i, j  k) = w ? f(i, j  k), thus requiring us to de-
fine the feature representation f(i, j  k), which is outlined in Table 3(c). These features
Table 3
Features for MSTParser. ? indicates a conjunction of features. ? indicates that all back-off
versions of a conjunction feature are included as well. A back-off version of a conjunction feature
is one where one or more base features are disregarded. ? indicates that all back-off versions are
included where a single base feature is disregarded.
Base features for sentence: x = w0, w1, . . . , wn
Lexical features: Identity of wi, wi ? x
Affix features: 3-gram lexical prefix/suffix identity of Pref(wi)/Suff(wi), wi ? x
Part-of-speech features: Identity of PoS(wi), wi ? x
Morphosyntactic features: For all morphosyntactic features MSFk for a word wi, identity of MSFk(wi), wi ? x
Label features: Identity of l in some labeled arc (i, j, l)
(a) Head-modifier features for unlabeled arc (i, j)
wi ? PoS(wi ) ? wj ? PoS(wj ) ?
Pref(wi ) ? PoS(wi ) ? Pref(wj ) ? PoS(wj ) ?
Suff(wi ) ? PoS(wi ) ? Suff(wj ) ? PoS(wj ) ?
?k, k? : MSFk(wi ) ? PoS(wi ) ? MSFk? (wj ) ? PoS(wj ) ?
(b) PoS-context features for unlabeled arc (i, j)
?k, i < k < j : PoS(wi ) ? PoS(wk ) ? PoS(wj )
PoS(wi?1) ? PoS(wi ) ? PoS(wj?1) ? PoS(wj ) ?
PoS(wi?1) ? PoS(wi ) ? PoS(wj ) ? PoS(wj+1 ) ?
PoS(wi ) ? PoS(wi+1 ) ? PoS(wj?1) ? PoS(wj ) ?
PoS(wi ) ? PoS(wi+1 ) ? PoS(wj ) ? PoS(wj+1) ?
(c) Head-modifier features for unlabeled arc pair (i, j 
 k)
wj ? wk
wj ? PoS(wk )
PoS(wj ) ? wk
PoS(wj ) ? PoS(wk )
PoS(wi ) ? PoS(wj ) ? PoS(wk )
(d) Arc-label features for labeled arc (i, j, l)
wi ? PoS(wi ) ? wj ? PoS(wj ) ? l ?
?k, i < k < j : PoS(wi ) ? PoS(wk ) ? PoS(wj ) ? l
PoS(wj?1) ? PoS(wj ) ? PoS(wj+1 ) ? l ?
PoS(wi?1) ? PoS(wi ) ? PoS(wi+1 ) ? l ?
209
Computational Linguistics Volume 37, Number 1
attempt to capture likely properties about adjacent arcs in the tree via their lexical and
part-of-speech information. Finally, all features in Table 3(a)?(c) contain two versions.
The first is the standard feature outlined in the table, and the second is the feature
conjoined with both the direction of dependency attachment (left or right) as well as the
bucketed distance between the head and modifier in buckets of 0 (adjacent), 1, 2, 3, 4,
5?9, and 10+.
For the second stage label classifier we use a log-linear classifier, which is again
parameterized by a vector of weights and a corresponding feature vector s(l|i, j) =
w ? f(i, j, l), where the score of a label l is now conditioned on a fixed dependency arc
(i, j) produced from the first-stage unlabeled parser. The feature representation f(i, j, l) is
defined in Table 3(d). These features provide the lexical and part-of-speech context for
determining whether a given arc label is suitable for a given head and modifier. Each
feature in Table 3(d) again has two versions, except this time the second version is only
conjoined with attachment direction.
These feature representations were used to train an on-line large-margin unlabeled
dependency parser (McDonald, Crammer, and Pereira 2005; McDonald and Pereira
2006) and a log-linear arc-labeler (Berger, Pietra, and Pietra 1996) regularized with a
zero mean Gaussian prior with the variance hyper-parameter set to 1.0. The unlabeled
dependency parser was trained for 10 iterations and the log-linear arc-labeler was
trained for 100 iterations. The feature sets and model hyper-parameters were fixed for
all languages. The only exception is that features containing coarse part-of-speech or
morphosyntactic information were ignored if this information was not available in a
corresponding treebank.
3.3 Training MaltParser
Training MaltParser amounts to estimating a function for scoring configuration-
transition pairs (c, t), represented by a feature vector f(c, t) ? Rk. Features are defined
in terms of arbitrary properties of the configuration c, including the state of the stack
?c, the input buffer ?c, and the partially built dependency graph Gc (represented in the
configuration by the arc set Ac). In particular, many features involve properties of the
two target tokens, the token on top of the stack ?c (denoted ?
0
c ) and the first token in
the input buffer ?c (denoted ?
0
c ), which are the two tokens that may become connected
by a dependency arc through the transition out of c. The basic feature representation
used for all languages in the CoNLL-X shared task included three groups of features:5
 Part-of-speech features: Identity of PoS(w), w ? {?0c ,?
1
c ,?
0
c ,?
1
c ,?
2
c ,?
3
c}.
 Lexical features: Identity of w, w ? {?0c ,?
0
c ,?
1
c} or (w,?
0
c , l) ? Gc.
 Arc features: Identity of l, (w, w?, l) ? Gc and w ? {?0c ,?
0
c} or w
? ? {?0c}.
Note in particular that features can be defined with respect to the partially built de-
pendency graph Gc. This is most obvious for the arc features, which extract the labels
of particular arcs in the graph, but it is also true of the last lexical feature, which picks
out the word form of the syntactic head of the word on top of the stack. This is pre-
cisely what gives transition-based parsers a richer feature space than their graph-based
5 We use the notation ?ic and ?
i
c to denote the ith element from the top/head of the stack/buffer (with
index 0 for the first element).
210
McDonald and Nivre Analyzing and Integrating Dependency Parsers
counterparts, even though graph-defined features are usually limited to a fairly small
region of the graph around ?0c and ?
0
c , such as their leftmost and rightmost dependents
and their syntactic head (if available).
Over and above the basic features described here, additional features were added
for some languages depending on availability in the training data. This included the
following:
 Coarse part-of-speech features: Identity of CPoS(w), w ? {?0c ,?
0
c ,?
1
c}.
 Lemma features: Identity of Lem(w), w ? {?0c ,?
0
c ,?
1
c}.
 Morphosyntactic features: Identity of MSF(w), w ? {?0c ,?
0
c ,?
1
c}.
Additional feature selection experiments were carried out for each language to the
extent that time permitted. Complete information about feature representations can be
found in Nivre et al (2006) and on the companion web site.6
The feature representations described here were used to train support vector ma-
chines as implemented in the LIBSVM library (Chang and Lin 2001), with a quadratic
kernel K(xi, xj) = (?x
T
i xj + r)
2 and LIBSVM?s built-in one-versus-one strategy for multi-
class classification, converting symbolic features to numerical ones using the standard
technique of binarization.7 One thing to note is that the quadratic kernel implicitly adds
features corresponding to pairs of explicit features, thus obviating the need for explicit
feature conjunctions as seen in the feature representations of MSTParser.
4. Error Analysis
A primary goal of this study is to characterize the errors made by standard data-driven
dependency parsing models. To that end, this section presents a number of experiments
that relate parsing errors to a set of linguistic and structural properties of the input and
predicted/gold standard dependency trees. We argue throughout that the results of this
analysis can be correlated to specific theoretical aspects of each model?in particular the
trade-off previously highlighted in Section 2.4.
For simplicity, all experiments report labeled parsing metrics (either accuracy, preci-
sion, or recall). Identical experiments using unlabeled parsing accuracies did not reveal
any additional information. Statistical significance was measured?for each metric at
each point along the operating curve?by employing randomized stratified shuffling at
the instance level using 10,000 iterations.8 Furthermore, all experiments report aggre-
gate statistics over the data from all 13 languages together, as explained in Section 3.
Finally, in all figures and tables, MSTParser and MaltParser are referred to as MST and
Malt, respectively, for short.
4.1 Length Factors
It is well known that parsing systems tend to have lower accuracies for longer sentences.
This is primarily due to the increased presence of complex syntactic constructions
6 http://maltparser.org/conll/conllx.
7 For details about parameter settings, we again refer to Nivre et al (2006) and the companion web site
http://maltparser.org/conll/conllx/.
8 This is the method used by the CoNLL-X shared task on dependency parsing.
211
Computational Linguistics Volume 37, Number 1
Figure 4
Accuracy relative to sentence length. Differences statistically significant (p < 0.05) at no
positions.
involving prepositions, conjunctions, and multi-clause sentences. Figure 4 shows the
accuracy of both parsing models relative to sentence length (in bins of size 10: 1?10,
11?20, etc.). System performance is almost indistinguishable, but MaltParser tends to
perform better on shorter sentences, which require the greedy inference algorithm to
make fewer parsing decisions. As a result, the chance of error propagation is reduced
significantly when parsing these sentences. However, if this was the only difference
between the two systems, we would expect them to have equal accuracy for shorter
sentences. The fact that MaltParser actually has higher accuracy when the likelihood
of error propagation is reduced is probably due to its richer feature space relative to
MSTParser.
Another interesting property is accuracy relative to dependency length as opposed
to sentence length. We define the length of a dependency from word wi to word wj
as equal to |i ? j|. Longer dependencies typically represent modifiers of the root or
the main verb in a sentence. Shorter dependencies are often modifiers of nouns such
as determiners or adjectives or pronouns modifying their direct neighbors. Figure 5
measures the precision and recall for each system relative to dependency lengths in the
predicted and gold standard dependency graphs. Precision represents the percentage
of predicted arcs of length d that were correct. Recall measures the percentage of gold
standard arcs of length d that were predicted.
Here we begin to see separation between the two systems. MSTParser is far more
precise for longer dependency arcs, whereas MaltParser does better for shorter depen-
dency arcs. This behavior can be explained using the same reasoning as above: Shorter
dependency arcs are usually created first in the greedy parsing procedure of MaltParser
and are less prone to error propagation. In contrast, longer dependencies are typically
constructed at the later stages of the parsing algorithm and are affected more by error
propagation. Theoretically, MSTParser should not perform better or worse for arcs of
any length. However, due to the fact that longer dependencies are typically harder
to parse, there is still a degradation in performance for MSTParser?up to 20% in the
212
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Figure 5
Dependency arc precision/recall relative to predicted/gold dependency length. Precision
statistically significant (p < 0.05) at 1, 2, 4, 7, 8, 10 through >14. Recall statistically significant
(p < 0.05) at >14.
extreme. However, the precision curve for MSTParser is much flatter than MaltParser,
which sees a drop of up to 40% in the extreme. Note that even though the area under the
curve is much larger for MSTParser, the number of dependency arcs with a length >10
is much smaller than the number with length <10, which is why the overall accuracy of
the two systems is nearly identical.
4.2 Graph Factors
The structure of the predicted and gold standard dependency graphs can also provide
insight into the differences between each model. For example, measuring accuracy for
arcs relative to their distance to the artificial root node will detail errors at different
levels of the dependency graph. For a given arc, we define this distance as the number
of arcs in the reverse path from the modifier of the arc to the root. For example, the
dependency arc from ROOT to is in Figure 1 would have a distance of 1 and the arc from
hearing to A a distance of 3. Figure 6 plots the precision and recall of each system for arcs
of varying distance to the root. Precision is equal to the percentage of dependency arcs
Figure 6
Dependency arc precision/recall relative to the predicted/gold distance to root. Precision
statistically significant (p < 0.05) at 1, 2, 4 through >6. Recall statistically significant (p < 0.05) at
1, 2, 3.
213
Computational Linguistics Volume 37, Number 1
in the predicted graph that are at a distance of d and are correct. Recall is the percentage
of dependency arcs in the gold standard graph that are at a distance of d and were
predicted.
Figure 6 clearly shows that for arcs close to the root, MSTParser is much more pre-
cise than MaltParser, and vice versa for arcs further away from the root. This is probably
the most compelling graph given in this study because it reveals a clear distinction:
MSTParser?s precision degrades as the distance to the root increases whereas Malt-
Parser?s precision increases. The plots essentially run in opposite directions crossing
near the middle. Dependency arcs further away from the root are usually constructed
early in the parsing algorithm of MaltParser. Again a reduced likelihood of error propa-
gation coupled with a rich feature representation benefits that parser substantially. Fur-
thermore, MaltParser tends to over-predict root modifiers, which comes at the expense
of its precision. This is because all words that the parser fails to attach as modifiers are
automatically connected to the root, as explained in Section 2.3. Hence, low precision
for root modifiers (without a corresponding drop in recall) is an indication that the
transition-based parser produces fragmented parses.
The behavior of MSTParser is a little trickier to explain. One would expect that
its errors should be distributed evenly over the graph. For the most part this is true,
with the exception of spikes at the ends of the plot. The high performance for root
modification (distance of 1) can be explained through the fact that this is typically a
low-entropy decision?usually the parsing algorithm has to determine the main verb
from a small set of possibilities. On the other end of the plot there is a slight downward
trend for arcs of distance greater than 3 from the root. An examination of dependency
length for predicted arcs shows that MSTParser predicts many more arcs of length 1
than MaltParser, which naturally leads to over-predicting more arcs at larger distances
from the root due to the presence of chains, which in turn will lower precision for these
arcs. In ambiguous situations, it is not surprising that MSTParser predicts many length-
1 dependencies, as this is the most common dependency length across treebanks. Thus,
whereas MaltParser pushes difficult parsing decisions higher in the graph, MSTParser
appears to push these decisions lower.
The final graph property we will examine aims to quantify the local neighborhood
of an arc within a dependency graph. Two dependency arcs, (i, j, l) and (i?, j?, l?), are clas-
sified as siblings if they represent syntactic modifications of the same word (i.e., i = i?).
In Figure 1 the arcs from the word is to the words hearing, scheduled, and the period are
all considered siblings under this definition. Figure 7 measures the precision and recall
of each system relative to the number of predicted and gold standard siblings of each
arc. There is not much to distinguish between the parsers on this metric. MSTParser is
slightly more precise for arcs that are predicted with more siblings, whereas MaltParser
has slightly higher recall on arcs that have more siblings in the gold standard tree. Arcs
closer to the root tend to have more siblings, which ties this result to the previous ones.
4.3 Linguistic Factors
It is important to relate each system?s accuracy to a set of linguistic categories, such
as parts of speech and dependency types. However, given the important typological
differences that exist between languages, as well as the diversity of annotation schemes
used in different treebanks, it is far from straightforward to compare these categories
across languages. Nevertheless, we have made an attempt to distinguish a few broad
categories that are cross-linguistically identifiable, based on the available documenta-
tion of the treebanks used in the shared task.
214
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Figure 7
Dependency arc precision/recall relative to the number of predicted/gold siblings. Precision
statistically significant (p < 0.05) at 0, 4, 6, 7, 9. Recall statistically significant (p < 0.05) at 5, >9.
For parts of speech, we distinguish verbs (including both main verbs and auxil-
iaries), nouns (including proper names), pronouns (sometimes also including deter-
miners), adjectives, adverbs, adpositions (prepositions, postpositions), and conjunctions
(both coordinating and subordinating). For dependency types, we have only managed
to distinguish a general root category (for labels used on arcs from the artificial root,
including either a generic label or the label assigned to predicates of main clauses, which
are normally verbs), a subject category, and an object category (including both direct
and indirect objects). Unfortunately, we had to exclude many interesting types that
could not be identified with high enough precision across languages, such as adverbials,
which cannot be clearly distinguished in annotation schemes that subsume them under
a general modifier category, and coordinate structures, which are sometimes annotated
with special dependency types, sometimes with ordinary dependency types found also
in non-coordinated structures.
Table 4(a) shows the accuracy of the two parsers for different parts of speech.
This figure measures labeled dependency accuracy relative to the part of speech of
the modifier word in a dependency relation. We see that MaltParser has slightly better
accuracy for nouns and pronouns, and MSTParser does better on all other categories, in
particular conjunctions. This pattern is consistent with previous results insofar as verbs
Table 4
(a) Accuracy relative to dependent part of speech. (b) Precision/recall for different dependency
types.
Part of Speech MST Malt
Verb 82.6 81.9
Noun 80.0 80.7
Pronoun 88.4 89.2
Adjective 89.1 87.9
Adverb 78.3 77.4
Adposition 69.9 68.8
Conjunction 73.1 69.8
(a)
Dependency MST Malt
Type Precision/Recall Precision/Recall
Root 89.9 / 88.7 84.7 / 87.5
Subject 79.9 / 78.9 80.3 / 80.7
Object 76.5 / 77.7 77.2 / 77.6
(b)
215
Computational Linguistics Volume 37, Number 1
and conjunctions are often involved in dependencies closer to the root that span longer
distances, whereas nouns and pronouns are typically attached to verbs and therefore
occur lower in the graph and with shorter distances. Thus, the average distance to the
root is 3.1 for verbs and 3.8 for conjunctions, but 4.7 for nouns and 4.9 for pronouns;
the average dependency length is 4.2 for verbs, 4.8 for conjunctions, 2.3 for nouns,
and 1.6 for pronouns. Adverbs resemble verbs and conjunctions with respect to root
distance (3.7) but group with nouns and pronouns for dependency length (2.3), so it
appears that the former is more important here. Furthermore, adverb modifiers have
2.4 siblings on average, which is greater than the sibling average for conjunctions (2.1),
adpositions (1.9), pronouns (1.7), verbs (1.3), nouns (1.3), and adjectives (1.2). This
would be consistent with the graph in Figure 7.
Adpositions and especially adjectives constitute a puzzle. With a root distance of 4.4
and 5.2, respectively, a dependency length of 2.5/1.5 and a sibling average of 1.9/1.2, we
would expect MaltParser to do better than MSTParser for these categories. Adpositions
do tend to have a high number of siblings on average, which could explain MSTParser?s
performance on that category. However, adjectives on average occur the furthest away
from the root, have the shortest dependency length, and the fewest siblings. At present,
we do not have an explanation for this behavior.
Finally, in Table 4(b), we consider precision and recall for dependents of the root
node (mostly verbal predicates), and for subjects and objects. As already noted, MST-
Parser has considerably better precision (and slightly better recall) for the root category,
but MaltParser has an advantage for the nominal categories, especially subjects. A pos-
sible explanation for the latter result, in addition to the length-based and graph-based
factors invoked before, is that MaltParser integrates labeling into the parsing process,
which means that previously assigned dependency labels can be used as features.
This may sometimes be important to disambiguate subjects and objects, especially in
free-word order languages where a dependent?s position relative to the verb does not
determine its syntactic role.
4.4 Discussion
The experiments in this section highlight the fundamental trade-off between global
training and exhaustive inference on the one hand and expressive feature representa-
tions on the other. Error propagation is an issue for MaltParser, which typically performs
worse on long sentences, long dependency arcs, and arcs higher in the graphs. But this
is offset by the rich feature representation available to these models that result in better
decisions for frequently occurring classes of arcs like short dependencies or subject
and object modifiers. The errors for MSTParser are spread a little more evenly. This
is expected, as the inference algorithm and feature representation should not prefer one
type of arc over another.
What has been learned? It was already known that the two systems make different
errors through the work of Sagae and Lavie (2006). However, in that work an arc-based
majority voting scheme was used that took only limited account of the properties of
the words connected by a dependency arc (more precisely, the overall accuracy of each
parser for the part of speech of the dependent). The analysis in this work not only shows
that the errors made by each system are different, but that they are different in a way
that can be predicted and quantified. This is an important step in parser development.
By understanding the strengths and weaknesses of each model we have gained insights
towards new and better models for dependency parsing.
216
McDonald and Nivre Analyzing and Integrating Dependency Parsers
To get some upper bounds on the improvement that can be obtained by combining
the strengths of each model, we can perform two oracle experiments. Given the output
of the two systems, we can envision an oracle that can optimally choose which single
parse or combination of sub-parses to predict as a final parse. For the first experiment
the oracle is provided with the single best parse from each system, say G = (V, A) and
G? = (V?, A?). The oracle chooses a parse that has the highest number of correctly pre-
dicted labeled dependency attachments. In this situation, the oracle labeled attachment
score is 84.5%. In the second experiment the oracle chooses the tree that maximizes the
number of correctly predicted dependency attachments, subject to the restriction that
the tree must only contain arcs from A ? A?. This can be computed by setting the weight
of an arc to 1 if it is in the correct parse and in the set A ? A?. All other arc weights are
set to negative infinity. One can then simply find the tree that has maximal sum of arc
weights using directed spanning tree algorithms. This technique is similar to the parser
voting methods used by Sagae and Lavie (2006). In this situation, the oracle accuracy
is 86.9%.
In both cases we see a clear increase in accuracy: 86.9% and 84.5% relative to 81%
for the individual systems. This indicates that there is still potential for improvement,
just by combining the two existing models. More interestingly, however, we can use
the analysis from this section to generate ideas for new models. Below we sketch some
possible new directions:
1. Ensemble systems: The error analysis presented in this article could be used
as inspiration for more refined weighting schemes for ensemble systems of
the kind proposed by Sagae and Lavie (2006), making the weights depend
on a range of linguistic and graph-based factors.
2. Integrated/Hybrid systems: Rather than using an ensemble of several
independent parsers, we may construct systems that trust different parsers
in different situations, possibly based on the characteristics of the input
and predicted dependency trees. The oracle results reported here show
that such an approach could potentially result in substantial
improvements.
3. Novel approaches: The theoretical analysis presented in this article reveals
that the two dominant approaches are each based on a particular
combination of training and inference methods, which raises the question
of which other combinations can fruitfully be explored. For example, can
we construct globally trained, greedy, transition-based parsers? Or
graph-based parsers with global features? To some extent the former
characterization fits the approach of Zhang and Clark (2008) and Huang
and Sagae (2010), and the latter that of Riedel and Clarke (2006),
Nakagawa (2007), and others. The analysis presented in this section
explains the relative success of such approaches.
In the next two sections we explore a model that falls into category 2. The system we
propose uses a two-stage stacking framework, where a second-stage parser conditions
on the predictions of a first-stage parser during inference. The second-stage parser is
also learned with access to the first-stage parser?s decisions and thus learns when to
trust the first-stage parser?s predictions and when to trust its own. The method is not a
traditional ensemble, because the parsers are not learned independently of one another.
217
Computational Linguistics Volume 37, Number 1
5. Integrated Models
As just discussed, there are many conceivable ways of combining the two parsers,
including more or less complex ensemble systems and voting schemes, which only
perform the integration at parsing time. However, given that we are dealing with data-
driven models, it should be possible to integrate at learning time, so that the two
complementary models can learn from one another. In this article, we propose to do this
by letting one model generate features for the other in a stacked learning framework.
Feature-based integration in this sense has previously been exploited for depen-
dency parsing by McDonald (2006), who trained an instance of MSTParser using fea-
tures generated by the parsers of Collins (1999) and Charniak (2000), which improved
unlabeled accuracy by 1.7 percentage points on data from the Penn Treebank. In other
NLP domains, feature-based integration has been used by Taskar, Lacoste-Julien, and
Klein (2005), who trained a discriminative word alignment model using features de-
rived from the IBM models, by Florian et al (2004), who trained classifiers on auxiliary
data to guide named entity classifiers, and by others.
Feature-based integration also has points in common with co-training, which has
been applied to syntactic parsing by Sarkar (2001) and Steedman et al (2003), among
others. The difference, of course, is that standard co-training is a weakly supervised
method, where the first-stage parser?s predictions replace, rather than complement, the
gold standard annotation during training. Feature-based integration is also similar to
parse reranking (Collins 2000), where one parser produces a set of candidate parses
and a second-stage classifier chooses the most likely one. However, feature-based in-
tegration is not explicitly constrained to any parse decisions that the first-stage parser
might make. Furthermore, as only the single most likely parse is used from the first-
stage model, it is significantly more efficient than reranking, which requires both com-
putationally and conceptually more complex parsing algorithms (Huang and Chiang
2005).
5.1 Parser Stacking with Rich Features
As explained in Section 2, both models essentially learn a scoring function s : X ? R,
where the domain X is different for the two models. For the graph-based model, X is
the set of possible dependency arcs (i, j, l); for the transition-based model, X is the set of
possible configuration-transition pairs (c, t). But in both cases, the input is represented
by a k-dimensional feature vector f : X ? Rk. In a stacked parsing system we simply
extend the feature vector for one model, called the base model, with a certain number
of features generated by the other model, which we call the guide model in this context.
The additional features will be referred to as guide features, and the version of the base
model trained with the extended feature vector will be called the guided model. The
idea is that the guided model should be able to learn in which situations to trust the
guide features, in order to exploit the complementary strength of the guide model, so
that performance can be improved with respect to the base model.
The exact form of the guide features depends on properties of the base model and
will be discussed in Sections 5.2?5.3, but the overall scheme for the stacked parsing
model can be described as follows. Assume as input a training set T = {(xt, Gxt )}
|T|
t=1 of
input sentences xt and corresponding gold standard dependency trees Gxt . In order to
train the guide model we use a cross-validation scheme and divide T into n different
disjoint subsets Ti (i.e., T =
?n
i=1 Ti). Let M[T] be the result of training the model M on T
218
McDonald and Nivre Analyzing and Integrating Dependency Parsers
and let M[T](x) be the result of parsing a new input sentence x with M[T]. Now, consider
a guide model C, base model B, and guided model BC. For each x in T, define
GCx = C[T ? Ti](x) if x ? Ti
GCx is the prediction of model C on training input x when C is trained on all the subsets
of T, except the one containing x. The reason for using this cross-validation scheme is
that if C had been trained on all of T, then GCx would not be representative of the types of
errors that C might make when parsing sentence x. Using cross-validation in this way is
similar to how it is used in parse reranking (Collins 2000). Now, define a new training set
of the form T? = {(?xt, GCxt?, Gxt )}
|T|
t=1. That is, T
? is identical to T, except that each training
input x is augmented with the cross-validation prediction of model C. Finally, let
BC = B[T
?]
This means that, for every sentence x ? T, BC has access at training time to both the
gold standard dependency graph Gx and the graph G
C
x predicted by C. Thus, BC is able
to define guide features over GCx , which can prove beneficial if features over G
C
x can be
used to discern when parsing model C outperforms or underperforms parsing model
B. When parsing a new sentence x with BC, x is first parsed with model C[T] (this time
trained on the entire training set) to derive an input ?x, GCx ?, so that the guide features
can be extracted also at parsing time. This input is then passed through model BC.
5.2 The Guided Graph-Based Model
The graph-based model, MSTParser, learns a scoring function s(i, j, l) ? R over labeled
dependencies. As described in Section 3.2, dependency arcs (or pairs of arcs) are repre-
sented by a high dimensional feature vector f(i, j, l) ? Rk, where f is typically a binary
feature vector over properties of the arc as well as the surrounding input (McDonald,
Crammer, and Pereira 2005; McDonald, Lerman, and Pereira 2006). For the guided
graph-based model, which we call MSTMalt, this feature representation is modified to
include an additional argument GMaltx , which is the dependency graph predicted by
MaltParser on the input sentence x. Thus, the new feature representation will map an arc
and the entire predicted MaltParser graph to a high dimensional feature representation,
f(i, j, l, GMaltx ) ? R
k+m. These m additional features account for the guide features over
the MaltParser output. The specific features used by MSTMalt are given in Table 5.
All features are conjoined with the part-of-speech tags of the words involved in the
Table 5
Guide features for MSTMalt and MaltMST.
MSTMalt ? defined over (i, j, l) MaltMST ? defined over (c, t)
(? = any label/node) (? = any label/node)
Is (i, j, ?) in GMaltx ? Is (?
0
c ,?
0
c , ?) in G
MST
x ?
Is (i, j, l) in GMaltx ? Is (?
0
c ,?
0
c , ?) in G
MST
x ?
Is (i, j, ?) not in GMaltx ? Head direction for ?
0
c in G
MST
x (left/right/ROOT)
Is (i, j, l) not in GMaltx ? Head direction for ?
0
c in G
MST
x (left/right/ROOT)
Identity of l? such that (?, j, l?) is in GMaltx ? Identity of l such that (?,?
0
c , l) is in G
MST
x ?
Identity of l? such that (i, j, l?) is in GMaltx ? Identity of l such that (?,?
0
c , l) is in G
MST
x ?
219
Computational Linguistics Volume 37, Number 1
dependency to allow the guided model to learn weights relative to different surface
syntactic environments. Features that include the arc label l are only included in the
second-stage arc-labeler. Though MSTParser is capable of defining features over pairs
of arcs, we restrict the guide features to single arcs as this resulted in higher accuracies
during preliminary experiments.
5.3 The Guided Transition-Based Model
The transition-based model, MaltParser, learns a scoring function s(c, t) ? R over con-
figurations and transitions. The set of training instances for this learning problem is the
set of pairs (c, t) such that t is the correct transition out of c in the transition sequence
that derives the correct dependency graph Gx for some sentence x in the training set
T. As described in Section 3.3, each training instance (c, t) is represented by a feature
vector f(c, t) ? Rk, where features are defined in terms of arbitrary properties of the
configuration c.
For the guided transition-based model, which we call MaltMST, training instances
are extended to triples (c, t, GMSTx ), where G
MST
x is the dependency graph predicted by
the graph-based MSTParser for the sentence x to which the configuration c belongs.
We define m additional guide features, based on properties of GMSTx , and extend the
feature vector accordingly to f(c, t, GMSTx ) ? R
k+m. The specific features used by MaltMST
are given in Table 5. Unlike MSTParser, features are not explicitly defined to conjoin
guide features with part-of-speech features. These features are implicitly added through
the polynomial kernel used to train the SVM.
6. Integrated Parsing Experiments
In this section, we present an experimental evaluation of the two guided models fol-
lowed by a comparative error analysis including both the base models and the guided
models. The data sets used in these experiments are identical to those used in Section 4.
The guided models were trained according to the scheme explained in Section 5, with
two-fold cross-validation when parsing the training data with the guide parsers. Pre-
liminary experiments suggested that cross-validation with more folds had a negligible
impact on the results. Models are evaluated by their labeled attachment score on the test
set using the evaluation software from the CoNLL-X shared task with default settings.9
Statistical significance was assessed using Dan Bikel?s randomized parsing evaluation
comparator with the default setting of 10,000 iterations.10
6.1 Results
Table 6 shows the results, for each language and on average, for the two base models
(MST, Malt) and for the two guided models (MSTMalt, MaltMST). We also give oracle
combination scores based on both by taking the best graph or the best set of arcs
relative to the gold standard, as discussed in Section 4.4. First of all, we see that both
guided models show a consistent increase in accuracy compared to their base model,
even though the extent of the improvement varies across languages from about half a
percentage point (MaltMST on Chinese) up to almost four percentage points (MaltMST on
9 http://nextens.uvt.nl/?conll/software.html.
10 http://www.cis.upenn.edu/?dbikel/software.html.
220
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Table 6
Labeled attachment scores for base parsers and guided parsers (improvement in percentage
points).
Oracle
Language MST MSTMalt Malt MaltMST graph arc
Arabic 66.91 68.64 (+1.73) 66.71 67.80 (+1.09) 70.3 75.8
Bulgarian 87.57 89.05 (+1.48) 87.41 88.59 (+1.18) 90.7 92.4
Chinese 85.90 88.43 (+2.53) 86.92 87.44 (+0.52) 90.8 91.5
Czech 80.18 82.26 (+2.08) 78.42 81.18 (+2.76) 84.2 86.6
Danish 84.79 86.67 (+1.88) 84.77 85.43 (+0.66) 87.9 89.6
Dutch 79.19 81.63 (+2.44) 78.59 79.91 (+1.32) 83.5 86.4
German 87.34 88.46 (+1.12) 85.82 87.66 (+1.84) 89.9 92.0
Japanese 90.71 91.43 (+0.72) 91.65 92.20 (+0.55) 93.2 94.1
Portuguese 86.82 87.50 (+0.68) 87.60 88.64 (+1.04) 90.0 91.6
Slovene 73.44 75.94 (+2.50) 70.30 74.24 (+3.94) 77.2 80.7
Spanish 82.25 83.99 (+1.74) 81.29 82.41 (+1.12) 85.4 88.2
Swedish 82.55 84.66 (+2.11) 84.58 84.31 (?0.27) 86.8 88.8
Turkish 63.19 64.29 (+1.10) 65.58 66.28 (+0.70) 69.3 72.6
Average 80.83 82.53 (+1.70) 80.75 82.01 (+1.27) 84.5 86.9
Slovene).11 It is thus quite clear that both models have the capacity to learn from features
generated by the other model. However, it is also clear that the graph-based MST model
shows a somewhat larger improvement, both on average and for all languages except
Czech, German, Portuguese, and Slovene. Finally, given that the two base models had
the best performance for these data sets at the CoNLL-X shared task, the guided models
achieve a substantial improvement of the state of the art.12 Although there is no statis-
tically significant difference between the two base models, they are both outperformed
by MaltMST (p < 0.0001), which in turn has significantly lower accuracy than MSTMalt
(p < 0.0005).
An extension to the models described so far would be to iteratively integrate the two
parsers in the spirit of pipeline iteration (Hollingshead and Roark 2007). For example,
one could start with a Malt model, use it to train a guided MSTMalt model, then use that
as the guide to train a MaltMSTMalt model, and so forth. We ran such experiments, but
found that accuracy did not increase significantly and in some cases decreased slightly.
This was true regardless of which parser began the iterative process. In retrospect, this
result is not surprising. Because the initial integration effectively incorporates knowl-
edge from both parsing systems, there is little to be gained by adding additional parsers
in the chain.
6.2 Error Analysis
The experimental results presented so far show that feature-based integration (stacking)
is a viable approach for improving the accuracy of both graph-based and transition-
based models for dependency parsing, but they say very little about how the integration
11 The only exception to this pattern is the result for MaltMST on Swedish, where we see an unexpected drop
in accuracy compared to the base model.
12 Martins et al (2008) and Martins, Smith, and Xing (2009) report additional improvements.
221
Computational Linguistics Volume 37, Number 1
benefits the two models and what aspects of the parsing process are improved as a
result. In order to get a better understanding of these matters, we replicate parts of
the error analysis presented in Section 4, but include both integrated models into the
analysis. As in Section 4, for each of the four models evaluated, we aggregate error
statistics for labeled attachment over all 13 languages together.
Figure 8 shows accuracy in relation to sentence length, binned into 10-word
intervals (1?10, 11-20, etc.). As mentioned earlier, Malt and MST have very similar
accuracy for short sentences but Malt degrades more rapidly with increasing sentence
length because of error propagation. The guided models, MaltMST and MSTMalt, behave
in a very similar fashion with respect to each other but both outperform their base
parser over the entire range of sentence lengths. However, except for the two extreme
data points (0?10 and 51?60) there is also a slight tendency for MaltMST to improve more
for longer sentences (relative to its base model) and for MSTMalt to improve more for
short sentences (relative to its base model). Thus, whereas most of the improvement for
the guided parsers seems to come from a higher accuracy in predicting arcs in general,
there is also some evidence that the feature-based integration allows one parser to
exploit the strength of the other.
Figure 9 plots precision (left) and recall (right) for dependency arcs of different
lengths (predicted arcs for precision, gold standard arcs for recall). With respect to recall,
the guided models appear to have a slight advantage over the base models for short and
medium distance arcs. With respect to precision, however, there are two clear patterns.
First, the graph-based models have better precision than the transition-based models
when predicting long arcs, as discussed earlier. Secondly, both the guided models have
better precision than their base model and, for the most part, also their guide model.
In particular MSTMalt outperforms MST for all dependency lengths and is comparable
to Malt for short arcs. More interestingly, MaltMST outperforms both Malt and MST for
arcs up to length 9, which provides evidence that MaltMST has learned specifically to
Figure 8
Accuracy relative to sentence length. Differences between MST+Malt and MST statistically
significant (p < 0.05) at all positions. Differences between Malt+MST and Malt statistically
significant (p < 0.05) at all positions. Differences between MST+Malt and Malt+MST
statistically significant (p < 0.05) at 11?20, 21?30, and 31?40.
222
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Figure 9
Dependency arc precision/recall relative to predicted/gold for dependency length. Precision
between MST+Malt and MST statistically significant (p < 0.05) at 1?7, 9?12, 14, and >14.
Recall between MST+Malt and MST statistically significant (p < 0.05) at 1?7, 9, 14, and >14.
Precision between Malt+MST and Malt statistically significant (p < 0.05) at 1?8, 10?13,
and > 14. Recall between Malt+MST and Malt statistically significant (p < 0.05) at 1?12, 14,
and >14. Precision between MST+Malt and Malt+MST statistically significant (p < 0.05) at 1 and
9?>14. Recall between MST+Malt and Malt+MST statistically significant (p < 0.05) at 1, 2, 3, 14,
and >14.
trust the guide features from MST for longer dependencies (those greater than length 4)
and its own base features for shorter dependencies (those less than or equal to length 4).
However, for dependencies of length greater than 9, the performance of MaltMST begins
to degrade. Because the absolute number of dependencies of length greater than 9 in
the training sets is relatively small, it might be difficult for MaltMST to learn from the
guide parser in these situations. Interestingly, both models seem to improve most in
the medium range (roughly 8?12 words), although this pattern is clearer for MSTParser
than for MaltParser.
Figure 10 shows precision (left) and recall (right) for dependency arcs at different
distances from the root (predicted arcs for precision, gold standard arcs for recall).
Figure 10
Dependency arc precision/recall relative to predicted/gold for distance to root. Precision
between MST+Malt and MST statistically significant (p < 0.05) at 1?6. Recall between MST+Malt
and MST statistically significant (p < 0.05) at all positions. Precision between Malt+MST and
Malt statistically significant (p < 0.05) at 1?4. Recall between Malt+MST and Malt statistically
significant (p < 0.05) at all positions. Precision between MST+Malt and Malt+MST statistically
significant (p < 0.05) at 1, 2, 3, 6, and >6. Recall between MST+Malt and Malt+MST
statistically significant (p < 0.05) at 4 and >6.
223
Computational Linguistics Volume 37, Number 1
Again, we find the clearest patterns in the graphs for precision, where Malt has very
low precision near the root but improves with increasing depth, whereas MST shows
the opposite trend, as observed earlier. Considering the guided models, it is clear that
MaltMST improves in the direction of its guide model, with a five-point increase in
precision for dependents of the root and smaller improvements for longer distances
(where its base model is most accurate). Similarly, MSTMalt improves precision the
largest in the range where its base model is inferior to Malt (roughly distances of 2?
6) and is always superior to its base model. This again indicates that the guided models
are learning from their guide models as they improve the most in situations where the
base model has inferior accuracy.
Table 7 gives the accuracy for arcs relative to dependent part of speech. As observed
earlier, we see that MST does better than Malt for all categories except nouns and
pronouns. But we also see that the guided models in all cases improve over their base
model and, in most cases, also over their guide model. The general trend is that MST
improves more than Malt, except for adjectives and conjunctions, where Malt has a
greater disadvantage from the start and therefore benefits more from the guide features.
The general trend is that the parser with worse performance for a particular part-of-
speech tag improves the most in terms of absolute accuracy (5 out of 7 cases), again
suggesting that the guided models are learning when to trust their guide features. The
exception here is verbs and adverbs, where MST has superior performance to Malt, but
MSTMalt has a larger increase in accuracy than MaltMST.
Considering the results for parts of speech, as well as those for dependency length
and root distance, it is interesting to note that the guided models often improve even
in situations where their base models are more accurate than their guide models. This
suggests that the improvement is not a simple function of the raw accuracy of the guide
model but depends on the fact that labeled dependency decisions interact in inference
algorithms for both graph-based and transition-based parsing systems. Thus, if a parser
can improve its accuracy on one class of dependencies (for example, longer ones), then
we can expect to see improvements on all types of dependencies?as we do.
6.3 Discussion
In summation, it is clear that both guided models benefit from a higher accuracy in
predicting arcs in general, which results in better performance regardless of sentence
length, dependency length, or dependency depth. However, there is strong evidence
that MSTMalt improves in the direction of Malt, with a slightly larger improvement
compared to its base model for short sentences and short dependencies (but not for deep
Table 7
Accuracy relative to dependent part of speech (improvement in percentage points).
Part of Speech MST MSTMalt Malt MaltMST
Verb 82.6 85.1 (2.5) 81.9 84.3 (2.4)
Noun 80.0 81.7 (1.7) 80.7 81.9 (1.2)
Pronoun 88.4 89.4 (1.0) 89.2 89.3 (0.1)
Adjective 89.1 89.6 (0.5) 87.9 89.0 (1.1)
Adverb 78.3 79.6 (1.3) 77.4 78.1 (0.7)
Adposition 69.9 71.5 (1.6) 68.8 70.7 (1.9)
Conjunction 73.1 74.9 (1.8) 69.8 72.5 (2.7)
224
McDonald and Nivre Analyzing and Integrating Dependency Parsers
dependencies). Conversely, MaltMST improves in the direction of MST, with a larger
improvement for long sentences and for dependents of the root.
The question remains why MST generally benefits more from the feature-based
integration. The likely explanation is the previously mentioned interaction between
different dependency decisions at inference time. Because inference in MST is exact
(or nearly exact), an improvement in one type of dependency has a good chance of
influencing the accuracy of other dependencies, whereas in the transition-based model,
where inference is greedy, some of these additional benefits will be lost because of error
propagation. This is reflected in the error analysis in the following recurrent pattern:
Where Malt does well, MaltMST does only slightly better. But where MST is good,
MSTMalt is often significantly better. Furthermore, this observation easily explains the
limited increases in accuracy of words with verb and adverb modifiers that is observed
in MaltMST relative to MSTMalt (Table 7) as these dependencies occur close to the root
and have increased likelihood of being affected by error propagation.
Another part of the explanation may have to do with the learning algorithms used
by the systems. Although both Malt and MST use discriminative algorithms, Malt uses
a batch learning algorithm (SVM) and MST uses an on-line learning algorithm (MIRA).
If the original rich feature representation of Malt is sufficient to separate the training
data, regularization may force the weights of the guided features to be small (as they
are not needed at training time). On the other hand, an on-line learning algorithm will
recognize the guided features as strong indicators early in training and give them a high
weight as a result. Frequent features with high weight early in training tend to have the
most impact on the final classifier due to both weight regularization and averaging. This
is in fact observed when inspecting the weights of MSTMalt.
Finally, comparing the results of the guided models to the oracle results discussed
in Section 4.4, we see that there should be room for further improvement, as the best
guided parser (MSTMalt) does not quite reach the level of the graph selection oracle,
let alne that of the arc selection oracle. Further exploration of the space of possible
systems, as outlined in Section 6.3, will undoubtedly be necessary to close this gap.
As already noted, there are several recent developments in data-driven dependency
parsing, which can be seen as targeting the specific weaknesses of traditional graph-
based and transition-based models, respectively. For graph-based parsers, McDonald
and Pereira (2006), Hall (2007), Nakagawa (2007), and Smith and Eisner (2008) attempt
to overcome the limited feature scope of graph-based models by adding global features
in conjunction with approximate inference. Additionally, Riedel and Clarke (2006) and
Martins, Smith, and Xing (2009) integrate global features and maintain exact inference
through integer linear programming solutions. For transition-based models, the trend
is to alleviate error propagation by abandoning greedy, deterministic inference in fa-
vor of beam search with globally normalized models for scoring transition sequences,
either generative (Titov and Henderson 2007a, 2007b) or conditional (Duan, Zhao,
and Xu 2007; Johansson and Nugues 2007). In addition, Zhang and Clark (2008) has
proposed a learning method for transition-based parsers based on global optimization
similar to that traditionally used for graph-based parsers, albeit only with approxi-
mate inference through beam search, and Huang and Sagae (2010) has shown how a
subclass of transition-based parsers can be tabularized to permit the use of dynamic
programming.
One question that can be asked, given the correlation provided here between ob-
served errors and algorithmic expectations, is whether it is possible to characterize the
errors of a new parsing system simply by analyzing its theoretical properties. This is a
difficult question to answer. Consider a parsing system that uses greedy inference. One
225
Computational Linguistics Volume 37, Number 1
can speculate that it will result in error propagation and, as a result, a large number
of parsing errors on long dependencies as well as those close to the root. However,
if the algorithm is run on data that contains only deterministic local decisions and
complex global decisions, such a system might not suffer from error propagation. This
is because the early local decisions are made correctly. Furthermore, saying something
about specific linguistic constructions is also difficult, due to the wide spectrum of
difficulty when parsing certain phenomena across languages. Ultimately, this is an
empirical question. What we have shown here is that, on a number of data sets, our
algorithmic expectations about two widely used dependency parsing paradigms are
confirmed.
7. Conclusion
In this article, we have shown that the two dominant approaches to data-driven depen-
dency parsing?global, exhaustive, graph-based models and local, greedy, transition-
based models?have distinctive error distributions despite often having very similar
parsing accuracy overall. We have demonstrated that these error distributions can be
explained by theoretical properties of the two models, in particular related to the funda-
mental tradeoff between global learning and inference, traditionally favored by graph-
based parsers, and a rich feature space, typically found in transition-based parsers.
Based on this analysis, we have proposed new directions of research on data-driven
dependency parsing, some of which are already beginning to be explored.
We have also demonstrated how graph-based and transition-based models can be
integrated by letting one model learn from features generated by the other, using the
technique known as stacking in the machine learning community. Our experimental
results show that both models consistently improve their accuracy when given access
to features generated by the other model, which leads to a significant advancement
of the state of the art in data-driven dependency parsing. Moreover, a comparative
error analysis reveals that the improvements are predictable from the same theoretical
properties identified in the initial error analysis, such as the tradeoff between global
learning and inference, on the one hand, and rich feature representations, on the other.
On a more general note, we believe that this shows the importance of careful error
analysis, informed by theoretical predictions, for the further advancement of data-
driven methods in natural language processing.
Acknowledgments
We want to thank our collaborators for great
support in developing the parsing
technology, the organizers of the CoNLL-X
shared task for creating the data, and three
anonymous reviewers for their feedback that
substantially improved the article.
References
Attardi, Giuseppe. 2006. Experiments with a
multilanguage non-projective dependency
parser. In Proceedings of the 10th Conference
on Computational Natural Language
Learning (CoNLL), pages 166?170,
New York, NY.
Attardi, Giuseppe and Massimiliano
Ciaramita. 2007. Tree revision learning for
dependency parsing. In Proceedings of
Human Language Technologies: The Annual
Conference of the North American Chapter of
the Association for Computational Linguistics
(NAACL HLT), pages 388?395,
Rochester, NY.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22:39?71.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
226
McDonald and Nivre Analyzing and Integrating Dependency Parsers
10th Conference on Computational Natural
Language Learning (CoNLL), pages 149?164,
New York, NY.
Carreras, Xavier. 2007. Experiments with a
higher-order projective dependency
parser. In Proceedings of the CoNLL Shared
Task of EMNLP-CoNLL 2007,
pages 957?961, Prague.
Chang, Chih-Chung and Chih-Jen Lin. 2001.
LIBSVM: A Library for Support Vector
Machines. Software available at www.csie.
ntu.edu.tw/?cjlin/libsvm.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the First Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL), pages 132?139,
Seattle, WA.
Cheng, Yuchang, Masayuki Asahara, and
Yuji Matsumoto. 2006. Multi-lingual
dependency parsing at NAIST. In
Proceedings of the 10th Conference on
Computational Natural Language
Learning (CoNLL), pages 191?195,
New York, NY.
Chu, Yoeng-Jin and Tseng-Hong Liu. 1965.
On the shortest arborescence of a
directed graph. Scientia Sinica,
14:1396?1400.
Collins, Michael. 1999. Head-Driven
Statistical Models for Natural Language
Parsing. Ph.D. thesis, University of
Pennsylvania.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing. In
Proceedings of the International Conference on
Machine Learning (ICML), pages 175?182,
Stanford, CA.
Cortes, Corinna and Vladimir Vapnik. 1995.
Support-vector networks. Machine
Learning, 20(3):273?297.
Crammer, Koby, Ofer Dekel, Joseph Keshet,
Shai Shalev-Shwartz, and Yoram Singer.
2006. Online passive-aggressive
algorithms. The Journal of Machine Learning
Research, 7:551?585.
Ding, Yuan and Martha Palmer. 2004.
Synchronous dependency insertion
grammars: A grammar formalism for
syntax based statistical MT. In Workshop
on Recent Advances in Dependency
Grammars (COLING), pages 90?97,
Geneva.
Duan, Xiangyu, Jun Zhao, and Bo Xu. 2007.
Probabilistic parsing action models for
multi-lingual dependency parsing. In
Proceedings of the CoNLL Shared Task
of EMNLP-CoNLL 2007, pages 940?946,
Prague.
Edmonds, Jack. 1967. Optimum branchings.
Journal of Research of the National Bureau of
Standards, 71B:233?240.
Eisner, Jason M. 1996. Three new
probabilistic models for dependency
parsing: An exploration. In Proceedings of
the 16th International Conference on
Computational Linguistics (COLING),
pages 340?345, Copenhagen.
Florian, Radu, Hany Hassan, Abraham
Ittycheriah, Hongyan Jing, Nanda
Kambhatla, Xiaoqiang Luo, Nicolas
Nicolov, and Salim Roukos. 2004. A
statistical model for multilingual entity
detection and tracking. In Proceedings of
Human Language Technology and the
Conference of the North American Chapter
of the Association for Computational
Linguistics (HLT-NAACL), pages 1?8,
Boston, MA.
Hall, Keith. 2007. K-best spanning tree
parsing. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 392?399, Prague.
Hall, Johan, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias
Nilsson, and Markus Saers. 2007. Single
malt or blended? A study in multilingual
parser optimization. In Proceedings of the
CoNLL Shared Task of EMNLP-CoNLL 2007,
pages 933?939, Prague.
Henderson, John C. and Eric Brill. 1999.
Exploiting diversity in natural language
processing: Combining parsers. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 188?194, College
Park, MD.
Hollingshead, Kristy and Brian Roark. 2007.
Pipeline iteration. In Proceedings of the
45th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 952?959, Prague.
Huang, Liang and David Chiang. 2005.
Better k-best parsing. In Proceedings of the
9th International Workshop on Parsing
Technologies (IWPT), pages 53?64,
Vancouver.
Huang, Liang and Kenjie Sagae. 2010.
Dynamic programming for linear-time
incremental parsing. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 1077?1086, Uppsala.
Hudson, Richard A. 1984. Word Grammar.
Blackwell, Oxford.
Johansson, Richard and Pierre Nugues. 2007.
Incremental dependency parsing using
227
Computational Linguistics Volume 37, Number 1
online learning. In Proceedings of the
CoNLL Shared Task of EMNLP-CoNLL 2007,
pages 1134?1138, Prague.
Koo, Terry, Amir Globerson, Xavier
Carreras, and Michael Collins. 2010.
Efficient third-order dependency
parsers. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 1?11, Uppsala.
Koo, Terry, Amir Globerson, Xavier Carreras,
and Michael Collins. 2007. Structured
prediction models via the matrix-tree
theorem. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning (EMNLP-
CoNLL), pages 141?150, Prague.
Kudo, Taku and Yuji Matsumoto. 2002.
Japanese dependency analysis using
cascaded chunking. In Proceedings of the
Sixth Workshop on Computational Language
Learning (CoNLL), pages 63?69, Edmonton.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data. In
Proceedings of the International Conference on
Machine Learning (ICML), pages 282?289,
Williamstown, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313?330.
Martins, Andre F. T., Dipanjan Das,
Noah A. Smith, and Eric P. Xing. 2008.
Stacking dependency parsers. In
Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 157?166,
Honolulu, HI.
Martins, Andre F. T., Noah A. Smith, and
Eric P. Xing. 2009. Concise integer linear
programming formulations for
dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP), pages 342?350,
Singapore.
Maruyama, Hiroshi. 1990. Structural
disambiguation with constraint
propagation. In Proceedings of the 28th
Meeting of the Association for Computational
Linguistics (ACL), pages 31?38,
Pittsburgh, PA.
McDonald, Ryan. 2006. Discriminative
Learning and Spanning Tree Algorithms for
Dependency Parsing. Ph.D. thesis,
University of Pennsylvania.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 91?98,
Ann Arbor, MI.
McDonald, Ryan, Kevin Lerman, and
Fernando Pereira. 2006. Multilingual
dependency analysis with a two-stage
discriminative parser. In Proceedings of the
10th Conference on Computational Natural
Language Learning (CoNLL), pages 216?220,
New York, NY.
McDonald, Ryan and Joakim Nivre. 2007.
Characterizing the errors of data-driven
dependency parsing models. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131,
Prague.
McDonald, Ryan and Fernando Pereira. 2006.
Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 81?88, Trento.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing
using spanning tree algorithms. In
Proceedings of the Human Language
Technology Conference and the Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 523?530,
Vancouver.
McDonald, Ryan and Giorgio Satta. 2007.
On the complexity of non-projective
data-driven dependency parsing. In
Proceedings of the 10th International
Conference on Parsing Technologies (IWPT),
pages 122?131, Prague.
Mel?c?uk, Igor. 1988. Dependency Syntax:
Theory and Practice. State University of
New York Press.
Nakagawa, Tetsuji. 2007. Multilingual
dependency parsing using global features.
In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 952?956,
Prague.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT),
pages 149?160, Nancy.
228
McDonald and Nivre Analyzing and Integrating Dependency Parsers
Nivre, Joakim. 2007. Incremental
non-projective dependency parsing.
In Proceedings of Human Language
Technologies: The Annual Conference
of the North American Chapter of the
Association for Computational Linguistics
(NAACL HLT), pages 396?403,
Rochester, NY.
Nivre, Joakim. 2009. Non-projective
dependency parsing in expected linear
time. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on
Natural Language Processing of the
AFNLP (ACL-IJCNLP), pages 351?359,
Singapore.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007. The CoNLL
2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 915?932,
Prague.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 49?56, Boston, MA.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gu?lsen Eryig?it, and Svetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support vector
machines. In Proceedings of the 10th
Conference on Computational Natural
Language Learning (CoNLL), pages 221?225,
New York, NY.
Nivre, Joakim and Ryan McDonald.
2008. Integrating graph-based and
transition-based dependency parsers.
In Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 950?958,
Columbus, OH.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 99?106, Ann Arbor, MI.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 129?137, Sydney.
Riedel, Sebastian, Ruket C?ak?c?, and Ivan
Meza-Ruiz. 2006. Multi-lingual
dependency parsing with incremental
integer linear programming. In Proceedings
of the 10th Conference on Computational
Natural Language Learning (CoNLL),
pages 226?230, New York, NY.
Sagae, Kenji and Alon Lavie. 2006. Parser
combination by reparsing. In Proceedings
of NAACL: Short Papers, pages 129?132,
New York, NY.
Sarkar, Anoop. 2001. Applying co-training
methods to statistical parsing. In
Proceedings of the Second Meeting of the
North American Chapter of the Association for
Computational Linguistics (NAACL),
pages 175?182, Pittsburgh, PA.
Sgall, Petr, Eva Hajic?ova?, and Jarmila
Panevova?. 1986. The Meaning of the
Sentence in Its Pragmatic Aspects. Reidel,
Dordrecht.
Smith, David A. and Jason Eisner. 2008.
Dependency parsing by belief
propagation. Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 145?156,
Honolulu, HI.
Smith, David A. and Noah A. Smith. 2007.
Probabilistic models of nonprojective
dependency trees. In Proceedings of the 2007
Joint Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 132?140, Prague.
Snow, Rion, Dan Jurafsky, and Andrew Y.
Ng. 2005. Learning syntactic patterns for
automatic hypernym discovery. In
Advances in Neural Information Processing
Systems (NIPS), pages 1297?1304,
Vancouver.
Steedman, Mark, Rebecca Hwa, Miles
Osborne, and Anoop Sarkar. 2003.
Corrected co-training for statistical
parsers. In Proceedings of the International
Conference on Machine Learning (ICML),
pages 95?102, Washington, DC.
Tarjan, Robert E. 1977. Finding optimum
branchings. Networks, 7:25?35.
Taskar, Ben, Simon Lacoste-Julien, and Dan
Klein. 2005. A discriminative matching
approach to word alignment. In
Proceedings of the Human Language
Technology Conference and the Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 73?80,
Vancouver.
Titov, Ivan and James Henderson. 2007a.
Fast and robust multilingual dependency
parsing with a generative latent variable
model. In Proceedings of the CoNLL
Shared Task of EMNLP-CoNLL 2007,
pages 947?951, Prague.
Titov, Ivan and James Henderson. 2007b. A
latent variable model for generative
229
Computational Linguistics Volume 37, Number 1
dependency parsing. In Proceedings of the
10th International Conference on Parsing
Technologies (IWPT), pages 144?155,
Prague.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis with
support vector machines. In Proceedings of
the 8th International Workshop on Parsing
Technologies (IWPT), pages 195?206,
Nancy.
Zeman, Daniel and Zdene?k Z?abokrtsky`.
2005. Improving parsing accuracy by
combining diverse dependency parsers.
Proceedings of the International Workshop
on Parsing Technologies, pages 171?178,
Vancouver.
Zhang, Yue and Stephen Clark. 2008.
A tale of two parsers: Investigating
and combining graph-based and
transition-based dependency parsing.
Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 562?571,
Honolulu, HI.
230
Constrained Arc-Eager Dependency Parsing
Joakim Nivre?
Uppsala University
Yoav Goldberg??
Bar-Ilan University
Ryan McDonald?
Google
Arc-eager dependency parsers process sentences in a single left-to-right pass over the input
and have linear time complexity with greedy decoding or beam search. We show how such
parsers can be constrained to respect two different types of conditions on the output dependency
graph: span constraints, which require certain spans to correspond to subtrees of the graph,
and arc constraints, which require certain arcs to be present in the graph. The constraints are
incorporated into the arc-eager transition system as a set of preconditions for each transition and
preserve the linear time complexity of the parser.
1. Introduction
Data-driven dependency parsers in general achieve high parsing accuracy without re-
lying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamada
and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira
2005; Zhang and Clark 2008; Koo and Collins 2010). Nevertheless, there are situations
where additional information sources, not available at the time of training the parser,
may be used to derive hard constraints at parsing time. For example, Figure 1 shows
the parse of a greedy arc-eager dependency parser trained on the Wall Street Journal
section of the Penn Treebank before (left) and after (right) being constrained to build a
single subtree over the span corresponding to the named entity ?Cat on a Hot Tin Roof,?
which does not occur in the training set but can easily be found in on-line databases. In
this case, adding the span constraint fixes both prepositional phrase attachment errors.
Similar constraints can also be derived from dates, times, or other measurements that
can often be identified with high precision using regular expressions (Karttunen et al.
1996), but are under-represented in treebanks.
? Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126, Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
?? Bar-Ilan University, Department of Computer Science, Ramat-Gan, 5290002, Israel.
E-mail: yoav.goldberg@gmail.com.
? Google, 76 Buckingham Palace Road, London SW1W9TQ, United Kingdom.
E-mail: ryanmcd@google.com.
Submission received: 26 June 2013; accepted for publication: 10 October 2013.
doi:10.1162/COLI a 00184
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
Figure 1
Span constraint derived from a title assisting parsing. Left: unconstrained. Right: constrained.
In this article, we examine the problem of constraining transition-based dependency
parsers based on the arc-eager transition system (Nivre 2003, 2008), which perform a
single left-to-right pass over the input, eagerly adding dependency arcs at the earliest
possible opportunity, resulting in linear time parsing. We consider two types of con-
straints: span constraints, exemplified earlier, require the output graph to have a single
subtree over one or more (non-overlapping) spans of the input; arc constraints instead
require specific arcs to be present in the output dependency graph. The main contri-
bution of the article is to show that both span and arc constraints can be implemented
as efficiently computed preconditions on parser transitions, thus maintaining the linear
runtime complexity of the parser.1
Demonstrating accuracy improvements due to hard constraints is challenging, be-
cause the phenomena we wish to integrate as hard constraints are by definition not
available in the parser?s training and test data. Moreover, adding hard constraints may
be desirable even if it does not improve parsing accuracy. For example, many organi-
zations have domain-specific gazetteers and want the parser output to be consistent
with these even if the output disagrees with gold treebank annotations, sometimes
because of expectations of downstream modules in a pipeline. In this article, we con-
centrate on the theoretical side of constrained parsing, but we nevertheless provide
some experimental evidence illustrating how hard constraints can improve parsing
accuracy.
2. Preliminaries and Notation
Dependency Graphs. Given a set L of dependency labels, we define a dependency graph
for a sentence x = w1, . . . , wn as a labeled directed graph G = (Vx, A), consisting of a
set of nodes Vx = {1, . . . , n}, where each node i corresponds to the linear position of a
word wi in the sentence, and a set of labeled arcs A ? Vx ? L ? Vx, where each arc (i, l, j)
represents a dependency with head wi, dependent wj, and label l. We assume that the
final word wn is always a dummy word ROOT and that the corresponding node n is a
designated root node.
Given a dependency graph G for sentence x, we say that a subgraph G[i,j] =
(V[i,j], A[i,j]) of G is a projective spanning tree over the interval [i, j] (1 ? i ? j ? n) iff
(i) G[i,j] contains all nodes corresponding to words between wi and wj inclusive, (ii) G[i,j]
is a directed tree, and (iii) it holds for every arc (i, l, j) ? G[i,j] that there is a directed path
1 Although span and arc constraints can easily be added to other dependency parsing frameworks, this
often affects parsing complexity. For example, in graph-based parsing (McDonald, Crammer, and Pereira
2005) arc constraints can be enforced within the O(n3) Eisner algorithm (Eisner 1996) by pruning out
inconsistent chart cells, but span constraints require the parser to keep track of full subtree end points,
which would necessitate the use of O(n4) algorithms (Eisner and Satta 1999).
250
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
from i to every node k such that min(i, j) < k < max(i, j) (projectivity). We now define
two constraints on a dependency graph G for a sentence x:
r G is a projective dependency tree (PDT) if and only if it is a projective
spanning tree over the interval [1, n] rooted at node n.
r G is a projective dependency graph (PDG) if and only if it can be extended
to a projective dependency tree simply by adding arcs.
It is clear from the definitions that every PDT is also a PDG, but not the other way around.
Every PDG can be created by starting with a PDT and removing some arcs.
Arc-Eager Transition-Based Parsing. In the arc-eager transition system of Nivre (2003), a
parser configuration is a triple c = (?|i, j|?, A) such that ? and B are disjoint sublists of
the nodes Vx of some sentence x, and A is a set of dependency arcs over Vx (and some
label set L). Following Ballesteros and Nivre (2013), we take the initial configuration
for a sentence x = w1, . . . , wn to be cs(x) = ([ ], [1, . . . , n], { }), where n is the designated
root node, and we take a terminal configuration to be any configuration of the form
c = ([ ], [n], A) (for any arc set A). We will refer to the list ? as the stack and the list B
as the buffer, and we will use the variables ? and ? for arbitrary sublists of ? and B,
respectively. For reasons of perspicuity, we will write ? with its head (top) to the right
and B with its head to the left. Thus, c = (?|i, j|?, A) is a configuration with the node i on
top of the stack ? and the node j as the first node in the buffer B.
There are four types of transitions for going from one configuration to the next,
defined formally in Figure 2 (disregarding for now the Added Preconditions column):
r LEFT-ARCl adds the arc (j, l, i) to A, where i is the node on top of the stack
and j is the first node in the buffer, and pops the stack. It has as a
precondition that the token i does not already have a head.
r RIGHT-ARCl adds the arc (i, l, j) to A, where i is the node on top of the stack
and j is the first node in the buffer, and pushes j onto the stack. It has as a
precondition that j 6= n.
r REDUCE pops the stack and requires that the top token has a head.
r SHIFT removes the first node in the buffer and pushes it onto the stack,
with the precondition that j 6= n.
A transition sequence for a sentence x is a sequence C0,m = (c0, c1, . . . , cm) of configu-
rations, such that c0 is the initial configuration cs(x), cm is a terminal configuration, and
there is a legal transition t such that ci = t(ci?1) for every i, 1 ? i ? m. The dependency
graph derived by C0,m is Gcm = (Vx, Acm ), where Acm is the set of arcs in cm.
Complexity and Correctness. For a sentence of length n, the number of transitions in
the arc-eager system is bounded by 2n (Nivre 2008). This means that a parser using
greedy inference (or constant width beam search) will run in O(n) time provided that
transitions plus required precondition checks can be performed in O(1) time. This holds
for the arc-eager system and, as we will demonstrate, its constrained variants as well.
The arc-eager transition system as presented here is sound and complete for the set
of PDTs (Nivre 2008). For a specific sentence x = w1, . . . , wn, this means that any transi-
tion sequence for x produces a PDT (soundness), and that any PDT for x is generated by
251
Computational Linguistics Volume 40, Number 2
Transition Added Preconditions
LEFT-ARCl (?|i, j|?, A) ? (?, j|?, A?{(j, l, i)}) ARC CONSTRAINTS
??a ? AC : da = i ? [ha ? ? ? la 6= l]
??a ? A : da = i ??a ? AC : ha = i ? da ? j|?
SPAN CONSTRAINTS
?[IN-SPAN(i) ? s(i) = s(j) ? i = r(s(i))]
?[IN-SPAN(i) ? s(i) 6= s(j) ? i 6= r(s(i))]
?[NONE? IN-SPAN(j) ? s(i) 6= s(j)]
?[ROOT ? IN-SPAN(j) ? s(i) 6= s(j) ? j 6= r(s(j))]
RIGHT-ARCl (?|i, j|?, A) ? (?|i|j,?, A?{(i, l, j)}) ARC CONSTRAINTS
??a ? AC : da = j ? [ha ? ? ? la 6= l]
j 6= n ??a ? AC : ha = j ? da ? i|?
SPAN CONSTRAINTS
?[ENDS-SPAN(j) ? #CC > 1]
?[IN-SPAN(j) ? s(i) = s(j) ? j = r(s(j))]
?[IN-SPAN(j) ? s(i) 6= s(j) ? j 6= r(s(j))]
?[NONE? IN-SPAN(i) ? s(i) 6= s(j)]
?[ROOT ? IN-SPAN(i) ? s(i) 6= s(j) ? i 6= r(s(i))]
REDUCE (?|i, j|?, A) ? (?, j|?, A) ARC CONSTRAINTS
??a ? AC : ha = i ? da ? j|?
?a ? A : da = i SPAN CONSTRAINTS
?[IN-SPAN(i) ? s(i) = s(j) ? i = r(s(i))]
SHIFT (?, i|?, A) ? (?|i,?, A) ARC CONSTRAINTS
??a ? AC : da = j ? ha ? i|?
i 6= n ??a ? AC : ha = j ? da ? i|?
SPAN CONSTRAINTS
?[ENDS-SPAN(j) ? #CC > 0]
Figure 2
Transitions for the arc-eager transition system with preconditions for different constraints. The
symbols ha, la, and da are used to denote the head node, label, and dependent node, respectively,
of an arc a (that is, a = (ha, la, da )); IN-SPAN(i) is true if i is contained in a span in SC; END-SPAN(i)
is true if i is the last word in a span in SC; s(i) denotes the span containing i (with a dummy span
for all words that are not contained in any span); r(s) denotes the designated root of span s
(if any); #CC records the number of connected components in the current span up to and
including the last word that was pushed onto the stack; NONE and ROOT are true if we allow no
outgoing arcs from spans and if we allow outgoing arcs only from the span root, respectively.
some transition sequence (completeness).2 In constrained parsing, we want to restrict
the system so that, when applied to a sentence x, it is sound and complete for the subset
of PDTs that satisfy all constraints.
3. Parsing with Arc Constraints
Arc Constraints. Given a sentence x = w1, . . . , wn and a label set L, an arc constraint
set is a set AC of dependency arcs (i, l, j) (1 ? i, j ? n, i 6= j 6= n, l ? L), where each arc
is required to be included in the parser output. Because the arc-eager system can only
derive PDTs, the arc constraint set has to be such that the constraint graph GC = (Vx, AC)
can be extended to a PDT, which is equivalent to requiring that GC is a PDG. Thus, the
task of arc-constrained parsing can be defined as the task of deriving a PDT G such
2 Although the transition system in Nivre (2008) is complete but not sound, it is trivial to show that the
system as presented here (with the root node at the end of the buffer) is both sound and complete.
252
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
that GC ? G. An arc-constrained transition system is sound if it only derives proper
extensions of the constraint graph and complete if it derives all such extensions.
Added Preconditions. We know that the unconstrained arc-eager system can derive any
PDT for the input sentence x, which means that any arc in Vx ? L ? Vx is reachable
from the initial configuration, including any arc in the arc constraint set AC. Hence, in
order to make the parser respect the arc constraints, we only need to add preconditions
that block transitions that would make an arc in AC unreachable.3 We achieve this
through the following preconditions, defined formally in Figure 2 under the heading
ARC CONSTRAINTS for each transition:
r LEFT-ARCl in a configuration (?|i, j|?, A) adds the arc (j, l, i) and makes
unreachable any arc that involves i and a node in the buffer (other than
(j, l, i)). Hence, we permit LEFT-ARCl only if no such arc is in AC.
r RIGHT-ARCl in a configuration (?|i, j|?, A) adds the arc (i, l, j) and makes
unreachable any arc that involves j and a node on the stack (other than
(i, l, j)). Hence, we permit RIGHT-ARCl only if no such arc is in AC.
r REDUCE in a configuration (?|i, j|?, A) pops i from the stack and makes
unreachable any arc that involves i and a node in the buffer. Hence, we
permit REDUCE only if no such arc is in AC.
r SHIFT in a configuration (?, i|?, A) moves i to the stack and makes
unreachable any arc that involves j and a node on the stack. Hence,
we permit SHIFTl only if no such arc is in AC.
Complexity and Correctness. Because the transitions remain the same, the arc-constrained
parser will terminate after at most 2n transitions, just like the unconstrained system.
However, in order to guarantee termination, we must also show that at least one
transition is applicable in every non-terminal configuration. This is trivial in the un-
constrained system, where the SHIFT transition can apply to any configuration that
has a non-empty buffer. In the arc-constrained system, SHIFT will be blocked if there
is an arc a ? AC involving the node i to be shifted and some node on the stack, and
we need to show that one of the three remaining transitions is then permissible. If a
involves i and the node on top of the stack, then either LEFT-ARCl and RIGHT-ARCl
is permissible (in fact, required). Otherwise, either LEFT-ARCl or REDUCE must be
permissible, because their preconditions are implied by the fact that AC is a PDG.
In order to obtain linear parsing complexity, we must also be able to check all pre-
conditions in constant time. This can be achieved by preprocessing the sentence x and
arc constraint set AC and recording for each node i ? Vx its constrained head (if any),
its leftmost constrained dependent (if any), and its rightmost constrained dependent (if
any), so that we can evaluate the preconditions in each configuration without having
to scan the stack and buffer linearly. Because there are at most O(n) arcs in the arc
constraint set, the preprocessing will not take more than O(n) time but guarantees that
all permissibility checks can be performed in O(1) time.
Finally, we note that the arc-constrained system is sound and complete in the sense
that it derives all and only PDTs compatible with a given arc constraint set AC for a sen-
tence x. Soundness follows from the fact that, for every arc (i, l, j) ? AC, the preconditions
3 For further discussion of reachability in the arc-eager system, see Goldberg and Nivre (2012, 2013).
253
Computational Linguistics Volume 40, Number 2
force the system to reach a configuration of the form (?|min(i, j),max(i, j)|?, A) in which
either LEFT-ARCl (i > j) or RIGHT-ARCl (i < j) will be the only permissible transition.
Completeness follows from the observation that every PDT G compatible with AC is also
a PDG and can therefore be viewed as a larger constraint set for which every transition
sequence (given soundness) derives G exactly.
Empirical Case Study: Imperatives. Consider the problem of parsing commands to
personal assistants such as Siri or Google Now. In this setting, the distribution of
utterances is highly skewed towards imperatives making them easy to identify.
Unfortunately, parsers trained on treebanks like the Penn Treebank (PTB) typically
do a poor job of parsing such utterances (Hara et al. 2011). However, we know that
if the first word of a command is a verb, it is likely the root of the sentence. If we
take an arc-eager beam search parser (Zhang and Nivre 2011) trained on the PTB, it
gets 82.14 labeled attachment score on a set of commands.4 However, if we constrain
the same parser so that the first word of the sentence must be the root, accuracy
jumps dramatically to 85.56. This is independent of simply knowing that the first
word of the sentence is a verb, as both parsers in this experiment had access to gold
part-of-speech tags.
4. Parsing with Span Constraints
Span Constraints. Given a sentence x = w1, . . . , wn, we take a span constraint set to be
a set SC of non-overlapping spans [i, j] (1 ? i < j ? n). The task of span-constrained
parsing can then be defined as the task of deriving a PDT G such that, for every span
[i, j] ? SC, G[i,j] is a (projective) spanning tree over the interval [i, j]. A span-constrained
transition system is sound if it only derives dependency graphs compatible with the
span constraint set and complete if it derives all such graphs. In addition, we may add
the requirement that no word inside a span may have dependents outside the span
(NONE), or that only the root of the span may have such dependents (ROOT).
Added Preconditions. Unlike the case of arc constraints, parsing with span constraints
cannot be reduced to simply enforcing (and blocking) specific dependency arcs. In
this sense, span constraints are more global than arc constraints as they require en-
tire subgraphs of the dependency graph to have a certain property. Nevertheless,
we can use the same basic technique as before and enforce span constraints by
adding new preconditions to transitions, but these preconditions need to refer to vari-
ables that are updated dynamically during parsing. We need to keep track of two
things:
r Which word is the designated root of a span? A word becomes the
designated root r(s) of its span s if it acquires a head outside the span or
if it acquires a dependent outside the span under the ROOT condition.
r How many connected components are in the subgraph over the current
span up to and including the last word pushed onto the stack? A variable
#CC is set to 1 when the first span word enters the stack, incremented by
1 for every SHIFT and decremented by 1 for every LEFT-ARCl.
4 Data and splits from the Web Treebank of Petrov and McDonald (2012). Commands used for evaluation
were sentences from the test set that had a sentence initial verb root.
254
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
Given this information, we need to add preconditions that guarantee the following:
r The designated root must not acquire a head inside the span.
r No word except the designated root may acquire a head outside the span.
r The designated root must not be popped from the stack before the last
word of the span has been pushed onto the stack.
r The last word of a span must not be pushed onto the stack in a
RIGHT-ARCl transition if #CC > 1.
r The last word of a span must not be pushed onto the stack in a SHIFT
transition if #CC > 0.
In addition, we must block outside dependents of all words in a span under the NONE
condition, and of all words in a span other than the designated root under the ROOT
condition. All the necessary preconditions are given in Figure 2 under the heading
SPAN CONSTRAINTS.
Complexity and Correctness. To show that the span-constrained parser always terminates
after at most 2n transitions, it is again sufficient to show that there is at least one
permissible transition for every non-terminal configuration. Here, SHIFT is blocked if
the word i to be shifted is the last word of a span and #CC > 0. But in this case, one of the
other three transitions must be permissible. If #CC = 1, then RIGHT-ARCl is permissible;
if #CC > 1 and the word on top of the stack does not have a head, then LEFT-ARCl is
permissible; and if #CC > 1 and the word on top of the stack already has a head, then
REDUCE is permissible (as #CC > 1 rules out the possibility that the word on top of the
stack has its head outside the span). In order to obtain linear parsing complexity, all
preconditions should be verifiable in constant time. This can be achieved during initial
sentence construction by recording the span s(i) for every word i (with a dummy span
for words that are not inside a span) and by updating r(s) (for every span s) and #CC as
described herein.
Finally, we note that the span-constrained system is sound and complete in the
sense that it derives all and only PDTs compatible with a given span constraint set SC for
a sentence x. Soundness follows from the observation that failure to have a connected
subgraph G[i,j] for some span [i, j] ? SC can only arise from pushing j onto the stack in
a SHIFT with #CC > 0 or a RIGHT-ARCl with #CC > 1, which is explicitly ruled out by
the added preconditions. Completeness can be established by showing that a transition
sequence that derives a PDT G compatible with SC in the unconstrained system cannot
violate any of the added preconditions, which is straightforward but tedious.
Empirical Case Study: Korean Parsing. In Korean, white-space-separated tokens corre-
spond to phrasal units (similar to Japanese bunsetsus) and not to basic syntactic cat-
egories like nouns, adjectives, or verbs. For this reason, a further segmentation step is
needed in order to transform the space-delimited tokens to units that are a suitable input
for a parser and that will appear as the leaves of a syntactic tree. Here, the white-space
boundaries are good candidates for posing hard constraints on the allowed sentence
structure, as only a single dependency link is allowed between different phrasal units,
and all the other links are phrase-internal. An illustration of the process is given in
Figure 3. Experiments on the Korean Treebank from McDonald et al. (2013) show that
adding span constraints based on white space indeed improves parsing accuracy for
an arc-eager beam search parser (Zhang and Nivre 2011). Unlabeled attachment score
255
Computational Linguistics Volume 40, Number 2
Figure 3
Parsing a Korean sentence (the man writes the policy decisions) using span constraints derived from
original white space cues indicating phrasal chunks.
increases from an already high 94.10 without constraints to 94.92, and labeled attach-
ment score increases from 89.91 to 90.75.
Combining Constraints. What happens if we want to add arc constraints on top of
the span constraints? In principle, we can simply take the conjunction of the added
preconditions from the arc constraint case and the span constraint case, but some
care is required to enforce correctness. First of all, we have to check that the arc
constraints are consistent with the span constraints and do not require, for example,
that there are two words with outside heads inside the the same span. In addition, we
need to update the variables r(s) already in the preprocessing phase in case the arc
constraints by themselves fix the designated root because they require a word inside
the span to have an outside head or (under the ROOT condition) to have an outside
dependent.
5. Conclusion
We have shown how the arc-eager transition system for dependency parsing can
be modified to take into account both arc constraints and span constraints, without
affecting the linear runtime and while preserving natural notions of soundness and
completeness. Besides the practical applications discussed in the introduction and case
studies, constraints can also be used as partial oracles for parser training.
References
Ballesteros, Miguel and Joakim Nivre.
2013. Getting to the roots of dependency
parsing. Computational Linguistics,
39:5?13.
Eisner, Jason and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton grammars.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics,
pages 457?464, Santa Cruz, CA.
Eisner, Jason M. 1996. Three new
probabilistic models for dependency
parsing: An exploration. In Proceedings
of the 16th International Conference on
Computational Linguistics (COLING),
pages 340?345, Copenhagen.
Goldberg, Yoav and Joakim Nivre.
2012. A dynamic oracle for arc-eager
dependency parsing. In Proceedings
of the 24th International Conference on
Computational Linguistics, pages 959?976,
Shanghai.
Goldberg, Yoav and Joakim Nivre. 2013.
Training deterministic parsers with
non-deterministic oracles. Transactions
of the Association for Computational
Linguistics, 1:403?414.
Hara, Tadayoshi, Takuya Matsuzaki, Yusuke
Miyao, and Jun?ichi Tsujii. 2011. Exploring
difficulties in parsing imperatives and
questions. In Proceedings of the 5th
International Joint Conference on Natural
Language Processing, pages 749?757,
Chiang Mai.
Karttunen, Lauri, Jean-Pierre Chanod,
Gregory Grefenstette, and Anne Schiller.
1996. Regular expressions for language
engineering. Natural Language Engineering,
2(4):305?328.
256
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
Koo, Terry and Michael Collins. 2010.
Efficient third-order dependency parsers.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 1?11, Uppsala.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 91?98, Ann Arbor, MI.
McDonald, Ryan, Joakim Nivre, Yvonne
Quirmbach-Brundage, Yoav Goldberg,
Dipanjan Das, Kuzman Ganchev, Keith
Hall, Slav Petrov, Hao Zhang, Oscar
Ta?ckstro?m, Claudia Bedini, Nu?ria
Bertomeu Castello?, and Jungmee Lee.
2013. Universal dependency annotation
for multilingual parsing. In Proceedings of
the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2:
Short Papers), pages 92?97, Sofia.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International Workshop
on Parsing Technologies, pages 149?160,
Nancy.
Nivre, Joakim. 2008. Algorithms for
deterministic incremental dependency
parsing. Computational Linguistics,
34:513?553.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 49?56, Boston, MA.
Petrov, Slav and Ryan McDonald. 2012.
Overview of the 2012 shared task on
parsing the web. In Notes of the First
Workshop on Syntactic Analysis of
Non-Canonical Language (SANCL),
Montreal.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis
with support vector machines.
In Proceedings of the 8th International
Workshop on Parsing Technologies,
pages 195?206, Nancy.
Zhang, Yue and Stephen Clark. 2008.
A tale of two parsers: Investigating
and combining graph-based and
transition-based dependency parsing.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 562?571,
Honolulu, HI.
Zhang, Yue and Joakim Nivre. 2011.
Transition-based parsing with rich
non-local features. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics, pages 188?193,
Portland, OR.
257

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 456?464,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Distributed Training Strategies for the Structured Perceptron
Ryan McDonald Keith Hall Gideon Mann
Google, Inc., New York / Zurich
{ryanmcd|kbhall|gmann}@google.com
Abstract
Perceptron training is widely applied in the
natural language processing community for
learning complex structured models. Like all
structured prediction learning frameworks, the
structured perceptron can be costly to train
as training complexity is proportional to in-
ference, which is frequently non-linear in ex-
ample sequence length. In this paper we
investigate distributed training strategies for
the structured perceptron as a means to re-
duce training times when computing clusters
are available. We look at two strategies and
provide convergence bounds for a particu-
lar mode of distributed structured perceptron
training based on iterative parameter mixing
(or averaging). We present experiments on
two structured prediction problems ? named-
entity recognition and dependency parsing ?
to highlight the efficiency of this method.
1 Introduction
One of the most popular training algorithms for
structured prediction problems in natural language
processing is the perceptron (Rosenblatt, 1958;
Collins, 2002). The structured perceptron has many
desirable properties, most notably that there is no
need to calculate a partition function, which is
necessary for other structured prediction paradigms
such as CRFs (Lafferty et al, 2001). Furthermore,
it is robust to approximate inference, which is of-
ten required for problems where the search space
is too large and where strong structural indepen-
dence assumptions are insufficient, such as parsing
(Collins and Roark, 2004; McDonald and Pereira,
2006; Zhang and Clark, 2008) and machine trans-
lation (Liang et al, 2006). However, like all struc-
tured prediction learning frameworks, the structure
perceptron can still be cumbersome to train. This
is both due to the increasing size of available train-
ing sets as well as the fact that training complexity
is proportional to inference, which is frequently non-
linear in sequence length, even with strong structural
independence assumptions.
In this paper we investigate distributed training
strategies for the structured perceptron as a means
of reducing training times when large computing
clusters are available. Traditional machine learning
algorithms are typically designed for a single ma-
chine, and designing an efficient training mechanism
for analogous algorithms on a computing cluster ?
often via a map-reduce framework (Dean and Ghe-
mawat, 2004) ? is an active area of research (Chu
et al, 2007). However, unlike many batch learning
algorithms that can easily be distributed through the
gradient calculation, a distributed training analog for
the perceptron is less clear cut. It employs online up-
dates and its loss function is technically non-convex.
A recent study by Mann et al (2009) has shown
that distributed training through parameter mixing
(or averaging) for maximum entropy models can
be empirically powerful and has strong theoretical
guarantees. A parameter mixing strategy, which can
be applied to any parameterized learning algorithm,
trains separate models in parallel, each on a disjoint
subset of the training data, and then takes an average
of all the parameters as the final model. In this paper,
we provide results which suggest that the percep-
tron is ill-suited for straight-forward parameter mix-
ing, even though it is commonly used for large-scale
structured learning, e.g., Whitelaw et al (2008) for
named-entity recognition. However, a slight mod-
456
ification we call iterative parameter mixing can be
shown to: 1) have similar convergence properties to
the standard perceptron algorithm, 2) find a sepa-
rating hyperplane if the training set is separable, 3)
reduce training times significantly, and 4) produce
models with comparable (or superior) accuracies to
those trained serially on all the data.
2 Related Work
Distributed cluster computation for many batch
training algorithms has previously been examined
by Chu et al (2007), among others. Much of the
relevant prior work on online (or sub-gradient) dis-
tributed training has been focused on asynchronous
optimization via gradient descent. In this sce-
nario, multiple machines run stochastic gradient de-
scent simultaneously as they update and read from
a shared parameter vector asynchronously. Early
work by Tsitsiklis et al (1986) demonstrated that
if the delay between model updates and reads is
bounded, then asynchronous optimization is guaran-
teed to converge. Recently, Zinkevich et al (2009)
performed a similar type of analysis for online learn-
ers with asynchronous updates via stochastic gra-
dient descent. The asynchronous algorithms in
these studies require shared memory between the
distributed computations and are less suitable to
the more common cluster computing environment,
which is what we study here.
While we focus on the perceptron algorithm, there
is a large body of work on training structured pre-
diction classifiers. For batch training the most com-
mon is conditional random fields (CRFs) (Lafferty
et al, 2001), which is the structured analog of maxi-
mum entropy. As such, its training can easily be dis-
tributed through the gradient or sub-gradient com-
putations (Finkel et al, 2008). However, unlike per-
ceptron, CRFs require the computation of a partition
function, which is often expensive and sometimes
intractable. Other batch learning algorithms include
M3Ns (Taskar et al, 2004) and Structured SVMs
(Tsochantaridis et al, 2004). Due to their efficiency,
online learning algorithms have gained attention, es-
pecially for structured prediction tasks in NLP. In
addition to the perceptron (Collins, 2002), others
have looked at stochastic gradient descent (Zhang,
2004), passive aggressive algorithms (McDonald et
Perceptron(T = {(xt,yt)}
|T |
t=1)
1. w(0) = 0; k = 0
2. for n : 1..N
3. for t : 1..T
4. Let y? = argmaxy? w
(k) ? f(xt,y?)
5. if y? 6= yt
6. w(k+1) = w(k) + f(xt,yt)? f(xt,y?)
7. k = k + 1
8. return w(k)
Figure 1: The perceptron algorithm.
al., 2005; Crammer et al, 2006), the recently intro-
duced confidence weighted learning (Dredze et al,
2008) and coordinate descent algorithms (Duchi and
Singer, 2009).
3 Structured Perceptron
The structured perceptron was introduced by Collins
(2002) and we adopt much of the notation and pre-
sentation of that study. The structured percetron al-
gorithm ? which is identical to the multi-class per-
ceptron ? is shown in Figure 1. The perceptron is an
online learning algorithm and processes training in-
stances one at a time during each epoch of training.
Lines 4-6 are the core of the algorithm. For a input-
output training instance pair (xt,yt) ? T , the algo-
rithm predicts a structured output y? ? Yt, where Yt
is the space of permissible structured outputs for in-
put xt, e.g., parse trees for an input sentence. This
prediction is determined by a linear classifier based
on the dot product between a high-dimensional fea-
ture representation of a candidate input-output pair
f(x,y) ? RM and a corresponding weight vector
w ? RM , which are the parameters of the model1.
If this prediction is incorrect, then the parameters
are updated to add weight to features for the cor-
responding correct output yt and take weight away
from features for the incorrect output y?. For struc-
tured prediction, the inference step in line 4 is prob-
lem dependent, e.g., CKY for context-free parsing.
A training set T is separable with margin ? >
0 if there exists a vector u ? RM with ?u? = 1
such that u ? f(xt,yt) ? u ? f(xt,y?) ? ?, for all
(xt,yt) ? T , and for all y? ? Yt such that y? 6= yt.
Furthermore, letR ? ||f(xt,yt)?f(xt,y?)||, for all
(xt,yt) ? T and y? ? Yt. A fundamental theorem
1The perceptron can be kernalized for non-linearity.
457
of the perceptron is as follows:
Theorem 1 (Novikoff (1962)). Assume training set
T is separable by margin ?. Let k be the number of
mistakes made training the perceptron (Figure 1) on
T . If training is run indefinitely, then k ? R
2
?2 .
Proof. See Collins (2002) Theorem 1.
Theorem 1 implies that if T is separable then 1) the
perceptron will converge in a finite amount of time,
and 2) will produce a w that separates T . Collins
also proposed a variant of the structured perceptron
where the final weight vector is a weighted average
of all parameters that occur during training, which
he called the averaged perceptron and can be viewed
as an approximation to the voted perceptron algo-
rithm (Freund and Schapire, 1999).
4 Distributed Structured Perceptron
In this section we examine two distributed training
strategies for the perceptron algorithm based on pa-
rameter mixing.
4.1 Parameter Mixing
Distributed training through parameter mixing is a
straight-forward way of training classifiers in paral-
lel. The algorithm is given in Figure 2. The idea is
simple: divide the training data T into S disjoint
shards such that T = {T1, . . . , TS}. Next, train
perceptron models (or any learning algorithm) on
each shard in parallel. After training, set the final
parameters to a weighted mixture of the parameters
of each model using mixture coefficients ?. Note
that we call this strategy parameter mixing as op-
posed to parameter averaging to distinguish it from
the averaged perceptron (see previous section). It is
easy to see how this can be implemented on a cluster
through a map-reduce framework, i.e., the map step
trains the individual models in parallel and the re-
duce step mixes their parameters. The advantages of
parameter mixing are: 1) that it is parallel, making
it possibly to scale to extremely large data sets, and
2) it is resource efficient, in particular with respect
to network usage as parameters are not repeatedly
passed across the network as is often the case for
exact distributed training strategies.
For maximum entropy models, Mann et al (2009)
show it is possible to bound the norm of the dif-
PerceptronParamMix(T = {(xt,yt)}
|T |
t=1)
1. Shard T into S pieces T = {T1, . . . , TS}
2. w(i) = Perceptron(Ti) ?
3. w =
?
i ?iw
(i) ?
4. return w
Figure 2: Distributed perceptron using a parameter mix-
ing strategy. ? Each w(i) is computed in parallel. ? ? =
{?1, . . . , ?S}, ??i ? ? : ?i ? 0 and
?
i ?i = 1.
ference between parameters trained on all the data
serially versus parameters trained with parameter
mixing. However, their analysis requires a stabil-
ity bound on the parameters of a regularized max-
imum entropy model, which is not known to hold
for the perceptron. In Section 5, we present empir-
ical results showing that parameter mixing for dis-
tributed perceptron can be sub-optimal. Addition-
ally, Dredze et al (2008) present negative parame-
ter mixing results for confidence weighted learning,
which is another online learning algorithm. The fol-
lowing theorem may help explain this behavior.
Theorem 2. For a any training set T separable by
margin ?, the perceptron algorithm trained through
a parameter mixing strategy (Figure 2) does not nec-
essarily return a separating weight vector w.
Proof. Consider a binary classification setting
where Y = {0, 1} and T has 4 instances.
We distribute the training set into two shards,
T1 = {(x1,1,y1,1), (x1,2,y1,2)} and T2 =
{(x2,1,y2,1), (x2,2,y2,2)}. Let y1,1 = y2,1 = 0 and
y1,2 = y2,2 = 1. Now, let w, f ? R6 and using
block features, define the feature space as,
f(x1,1, 0) = [1 1 0 0 0 0] f(x1,1, 1) = [0 0 0 1 1 0]
f(x1,2, 0) = [0 0 1 0 0 0] f(x1,2, 1) = [0 0 0 0 0 1]
f(x2,1, 0) = [0 1 1 0 0 0] f(x2,1, 1) = [0 0 0 0 1 1]
f(x2,2, 0) = [1 0 0 0 0 0] f(x2,2, 1) = [0 0 0 1 0 0]
Assuming label 1 tie-breaking, parameter mixing re-
turns w1=[1 1 0 -1 -1 0] and w2=[0 1 1 0 -1 -1]. For
any ?, the mixed weight vector w will not separate
all the points. If both ?1/?2 are non-zero, then all
examples will be classified 0. If ?1=1 and ?2=0,
then (x2,2,y2,2) will be incorrectly classified as 0
and (x1,2,y1,2) when ?1=0 and ?2=1. But there is a
separating weight vector w = [-1 2 -1 1 -2 1].
This counter example does not say that a parameter
mixing strategy will not converge. On the contrary,
458
if T is separable, then each of its subsets is separa-
ble and converge via Theorem 1. What it does say
is that, independent of ?, the mixed weight vector
produced after convergence will not necessarily sep-
arate the entire data, even when T is separable.
4.2 Iterative Parameter Mixing
Consider a slight augmentation to the parameter
mixing strategy. Previously, each parallel percep-
tron was trained to convergence before the parame-
ter mixing step. Instead, shard the data as before, but
train a single epoch of the perceptron algorithm for
each shard (in parallel) and mix the model weights.
This mixed weight vector is then re-sent to each
shard and the perceptrons on those shards reset their
weights to the new mixed weights. Another single
epoch of training is then run (again in parallel over
the shards) and the process repeats. This iterative
parameter mixing algorithm is given in Figure 3.
Again, it is easy to see how this can be imple-
mented as map-reduce, where the map computes the
parameters for each shard for one epoch and the re-
duce mixes and re-sends them. This is analogous
to batch distributed gradient descent methods where
the gradient for each shard is computed in parallel in
the map step and the reduce step sums the gradients
and updates the weight vector. The disadvantage of
iterative parameter mixing, relative to simple param-
eter mixing, is that the amount of information sent
across the network will increase. Thus, if network
latency is a bottleneck, this can become problematic.
However, for many parallel computing frameworks,
including both multi-core computing as well as clus-
ter computing with high rates of connectivity, this is
less of an issue.
Theorem 3. Assume a training set T is separable
by margin ?. Let ki,n be the number of mistakes that
occurred on shard i during the nth epoch of train-
ing. For any N , when training the perceptron with
iterative parameter mixing (Figure 3),
N?
n=1
S?
i=1
?i,nki,n ?
R2
?2
Proof. Let w(i,n) to be the weight vector for the
ith shard after the nth epoch of the main loop and
let w([i,n]?k) be the weight vector that existed on
shard i in the nth epoch k errors before w(i,n). Let
PerceptronIterParamMix(T = {(xt,yt)}
|T |
t=1)
1. Shard T into S pieces T = {T1, . . . , TS}
2. w = 0
3. for n : 1..N
4. w(i,n) = OneEpochPerceptron(Ti,w) ?
5. w =
?
i ?i,nw
(i,n) ?
6. return w
OneEpochPerceptron(T , w?)
1. w(0) = w?; k = 0
2. for t : 1..T
3. Let y? = argmaxy? w
(k) ? f(xt,y?)
4. if y? 6= yt
5. w(k+1) = w(k) + f(xt,yt)? f(xt,y?)
6. k = k + 1
7. return w(k)
Figure 3: Distributed perceptron using an iterative param-
eter mixing strategy. ? Each w(i,n) is computed in paral-
lel. ? ?n = {?1,n, . . . , ?S,n}, ??i,n ? ?n: ?i,n ? 0 and
?n:
?
i ?i,n = 1.
w(avg,n) be the mixed vector from the weight vec-
tors returned after the nth epoch, i.e.,
w(avg,n) =
S?
i=1
?i,nw(i,n)
Following the analysis from Collins (2002) Theorem
1, by examining line 5 of OneEpochPerceptron in
Figure 3 and the fact that u separates the data by ?:
u ?w(i,n) = u ?w([i,n]?1)
+ u ? (f(xt,yt)? f(xt,y?))
? u ?w([i,n]?1) + ?
? u ?w([i,n]?2) + 2?
. . . ? u ?w(avg,n?1) + ki,n? (A1)
That is, u ? w(i,n) is bounded below by the average
weight vector for the n-1st epoch plus the number
of mistakes made on shard i during the nth epoch
times the margin ?. Next, by OneEpochPerceptron
line 5, the definition ofR, and w([i,n]?1)(f(xt,yt)?
f(xt,y?)) ? 0 when line 5 is called:
?w(i,n)?2 = ?w([i,n]?1)?2
+?f(xt,yt)? f(xt,y?)?2
+ 2w([i,n]?1)(f(xt,yt)? f(xt,y?))
? ?w([i,n]?1)?2 +R2
? ?w([i,n]?2)?2 + 2R2
. . . ? ?w(avg,n?1)?2 + ki,nR2 (A2)
459
That is, the squared L2-norm of a shards weight vec-
tor is bounded above by the same value for the aver-
age weight vector of the n-1st epoch and the number
of mistakes made on that shard during the nth epoch
times R2.
Using A1/A2 we prove two inductive hypotheses:
u ?w(avg,N) ?
N?
n=1
S?
i=1
?i,nki,n? (IH1)
?w(avg,N)?2 ?
N?
n=1
S?
i=1
?i,nki,nR
2 (IH2)
IH1 implies ?w(avg,N)? ?
?N
n=1
?S
i=1 ?i,nki,n?
since u ?w ? ?u??w? and ?u? = 1.
The base case is w(avg,1), where we can observe:
u ?wavg,1 =
S?
i=1
?i,1u ?w(i,1) ?
S?
i=1
?i,1ki,1?
using A1 and the fact that w(avg,0) = 0 for the sec-
ond step. For the IH2 base case we can write:
?w(avg,1)?2 =
?
?
?
?
?
S?
i=1
?i,1w(i,1)
?
?
?
?
?
2
?
S?
i=1
?i,1?w(i,1)?2 ?
S?
i=1
?i,1ki,1R
2
The first inequality is Jensen?s inequality, and the
second is true by A2 and ?w(avg,0)?2 = 0.
Proceeding to the general case, w(avg,N):
u ?w(avg,N) =
S?
i=1
?i,N (u ?w(i,N))
?
S?
i=1
?i,N (u ?w(avg,N?1) + ki,N?)
= u ?w(avg,N?1) +
S?
i=1
?i,Nki,N?
?
[
N?1?
n=1
S?
i=1
?i,nki,n?
]
+
S?
i=1
?i,Nki,N
=
N?
n=1
S?
i=1
?i,nki,n?
The first inequality uses A1, the second step
?
i ?i,N = 1 and the second inequality the induc-
tive hypothesis IH1. For IH2, in the general case,
we can write:
?w(avg,N)?2 ?
S?
i=1
?i,N?w(i,N)?2
?
S?
i=1
?i,N (?w(avg,N?1)?2 + ki,NR2)
= ?w(avg,N?1)?2 +
S?
i=1
?i,Nki,NR
2
?
[
N?1?
n=1
S?
i=1
?i,nki,nR
2
]
+
S?
i=1
?i,Nki,NR
2
=
N?
n=1
S?
i=1
?i,nki,nR
2
The first inequality is Jensen?s, the second A2, and
the third the inductive hypothesis IH2. Putting to-
gether IH1, IH2 and ?w(avg,N)? ? u ?w(avg,N):
[
N?
n=1
S?
i=1
?i,nki,n
]2
?2 ?
[
N?
n=1
S?
i=1
?i,nki,n
]
R2
which yields:
?N
n=1
?S
i=1 ?i,nki,n ?
R2
?2
4.3 Analysis
If we set each ?n to be the uniform mixture, ?i,n =
1/S, then Theorem 3 guarantees convergence to
a separating hyperplane. If
?S
i=1 ?i,nki,n = 0,
then the previous weight vector already separated
the data. Otherwise,
?N
n=1
?S
i=1 ?i,nki,n is still in-
creasing, but is bounded and cannot increase indefi-
nitely. Also note that if S = 1, then ?1,n must equal
1 for all n and this bound is identical to Theorem 1.
However, we are mainly concerned with how fast
convergence occurs, which is directly related to the
number of training epochs each algorithm must run,
i.e., N in Figure 1 and Figure 3. For the non-
distributed variant of the perceptron we can say that
Nnon dist ? R2/?2 since in the worst case a single
mistake happens on each epoch.2 For the distributed
case, consider setting ?i,n = ki,n/kn, where kn =?
i ki,n. That is, we mix parameters proportional to
the number of errors each made during the previous
epoch. Theorem 3 still implies convergence to a sep-
arating hyperplane with this choice. Further, we can
2It is not hard to derive such degenerate cases.
460
bound the required number of epochs Ndist:
Ndist ?
Ndist?
n=1
S?
i=1
[ki,n]
ki,n
kn ?
Ndist?
n=1
S?
i=1
ki,n
kn
ki,n ?
R2
?2
Ignoring when all ki,n are zero (since the algorithm
will have converged), the first inequality is true since
either ki,n ? 1, implying that [ki,n]ki,n/kn ? 1, or
ki,n = 0 and [ki,n]ki,n/kn = 1. The second inequal-
ity is true by the generalized arithmetic-geometric
mean inequality and the final inequality is Theo-
rem 3. Thus, the worst-case number of epochs is
identical for both the regular and distributed percep-
tron ? but the distributed perceptron can theoreti-
cally process each epoch S times faster. This ob-
servation holds only for cases where ?i,n > 0 when
ki,n ? 1 and ?i,n = 0 when ki,n = 0, which does
not include uniform mixing.
5 Experiments
To investigate the distributed perceptron strategies
discussed in Section 4 we look at two structured pre-
diction tasks ? named entity recognition and depen-
dency parsing. We compare up to four systems:
1. Serial (All Data): This is the classifier returned
if trained serially on all the available data.
2. Serial (Sub Sampling): Shard the data, select
one shard randomly and train serially.
3. Parallel (Parameter Mix): Parallel strategy
discussed in Section 4.1 with uniform mixing.
4. Parallel (Iterative Parameter Mix): Parallel
strategy discussed in Section 4.2 with uniform
mixing (Section 5.1 looks at mixing strategies).
For all four systems we compare results for both the
standard perceptron algorithm as well as the aver-
aged perceptron algorithm (Collins, 2002).
We report the final test set metrics of the con-
verged classifiers to determine whether any loss in
accuracy is observed as a consequence of distributed
training strategies. We define convergence as ei-
ther: 1) the training set is separated, or 2) the train-
ing set performance measure (accuracy, f-measure,
etc.) does not change by more than some pre-defined
threshold on three consecutive epochs. As with most
real world data sets, convergence by training set sep-
aration was rarely observed, though in both cases
training set accuracies approached 100%. For both
tasks we also plot test set metrics relative to the user
wall-clock taken to obtain the classifier. The results
were computed by collecting the metrics at the end
of each epoch for every classifier. All experiments
used 10 shards (Section 5.1 looks at convergence rel-
ative to different shard size).
Our first experiment is a named-entity recogni-
tion task using the English data from the CoNLL
2003 shared-task (Tjong Kim Sang and De Meul-
der, 2003). The task is to detect entities in sentences
and label them as one of four types: people, organi-
zations, locations or miscellaneous. For our exper-
iments we used the entire training set (14041 sen-
tences) and evaluated on the official development
set (3250 sentences). We used a straight-forward
IOB label encoding with a 1st order Markov fac-
torization. Our feature set consisted of predicates
extracted over word identities, word affixes, orthog-
raphy, part-of-speech tags and corresponding con-
catenations. The evaluation metric used was micro
f-measure over the four entity class types.
Results are given in Figure 4. There are a num-
ber of things to observe here: 1) training on a single
shard clearly provides inferior performance to train-
ing on all data, 2) the simple parameter mixing strat-
egy improves upon a single shard, but does not meet
the performance of training on all data, 3) iterative
parameter mixing achieves performance as good as
or better than training serially on all the data, and
4) the distributed algorithms return better classifiers
much quicker than training serially on all the data.
This is true regardless of whether the underlying al-
gorithm is the regular or the averaged perceptron.
Point 3 deserves more discussion. In particular, the
iterative parameter mixing strategy has a higher final
f-measure than training on all the data serially than
the standard perceptron (f-measure of 87.9 vs. 85.8).
We suspect this happens for two reasons. First, the
parameter mixing has a bagging like effect which
helps to reduce the variance of the per-shard classi-
fiers (Breiman, 1996). Second, the fact that parame-
ter mixing is just a form of parameter averaging per-
haps has the same effect as the averaged perceptron.
Our second set of experiments looked at the much
more computationally intensive task of dependency
parsing. We used the Prague Dependency Tree-
bank (PDT) (Hajic? et al, 2001), which is a Czech
461
Wall Clock
0.65
0.7
0.75
0.8
0.85
Test 
Data
 F-m
easu
re
Perceptron -- Serial (All Data)Perceptron -- Serial (Sub Sampling)Perceptron -- Parallel (Parameter Mix)Perceptron -- Parallel (Iterative Parameter Mix)
Wall Clock
0.7
0.75
0.8
0.85
Test 
Data
 F-m
easu
re
Averaged Perceptron -- Serial (All Data)Averaged Perceptron -- Serial (Sub Sampling)Averaged Perceptron -- Parallel (Parameter Mix)Averaged Perceptron -- Parallel (Iterative Parameter Mix)
Reg. Perceptron Avg. Perceptron
F-measure F-measure
Serial (All Data) 85.8 88.2
Serial (Sub Sampling) 75.3 76.6
Parallel (Parameter Mix) 81.5 81.6
Parallel (Iterative Parameter Mix) 87.9 88.1
Figure 4: NER experiments. Upper figures plot test data f-measure versus wall clock for both regular perceptron (left)
and averaged perceptron (right). Lower table is f-measure for converged models.
language treebank and currently one of the largest
dependency treebanks in existence. We used the
CoNLL-X training (72703 sentences) and testing
splits (365 sentences) of this data (Buchholz and
Marsi, 2006) and dependency parsing models based
on McDonald and Pereira (2006) which factors fea-
tures over pairs of dependency arcs in a tree. To
parse all the sentences in the PDT, one must use a
non-projective parsing algorithm, which is a known
NP-complete inference problem when not assuming
strong independence assumptions. Thus, the use of
approximate inference techniques is common in or-
der to find the highest weighted tree for a sentence.
We use the approximate parsing algorithm given in
McDonald and Pereira (2006), which runs in time
roughly cubic in sentence length. To train such a
model is computationally expensive and can take on
the order of days to train on a single machine.
Unlabeled attachment scores (Buchholz and
Marsi, 2006) are given in Figure 5. The same trends
are seen for dependency parsing that are seen for
named-entity recognition. That is, iterative param-
eter mixing learns classifiers faster and has a final
accuracy as good as or better than training serially
on all data. Again we see that the iterative parame-
ter mixing model returns a more accurate classifier
than the regular perceptron, but at about the same
level as the averaged perceptron.
5.1 Convergence Properties
Section 4.3 suggests that different weighting strate-
gies can lead to different convergence properties,
in particular with respect to the number of epochs.
For the named-entity recognition task we ran four
experiments comparing two different mixing strate-
gies ? uniform mixing (?i,n=1/S) and error mix-
ing (?i,n=ki,n/kn) ? each with two shard sizes ?
S = 10 and S = 100. Figure 6 plots the number
of training errors per epoch for each strategy.
We can make a couple observations. First, the
mixing strategy makes little difference. The rea-
son being that the number of observed errors per
epoch is roughly uniform across shards, making
both strategies ultimately equivalent. The other ob-
servation is that increasing the number of shards
can slow down convergence when viewed relative to
epochs3. Again, this appears in contradiction to the
analysis in Section 4.3, which, at least for the case
of error weighted mixtures, implied that the num-
ber of epochs to convergence was independent of
the number of shards. But that analysis was based
on worst-case scenarios where a single error occurs
on a single shard at each epoch, which is unlikely to
occur in real world data. Instead, consider the uni-
3As opposed to raw wall-clock/CPU time, which benefits
from faster epochs the more shards there are.
462
Wall Clock
0.74
0.76
0.78
0.8
0.82
0.84
Unla
beled
 Atta
chme
nt Sc
ore
Perceptron -- Serial (All Data)Perceptron -- Serial (Sub Sampling)Perceptron -- Parallel (Iterative Parameter Mix)
Wall Clock0.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
Unla
beled
 Atta
chme
nt Sc
ore
Averaged Perceptron -- Serial (All Data)Averaged Perceptron -- Serial (Sub Sampling)Averaged Perceptron -- (Iterative Parameter Mix) 
Reg. Perceptron Avg. Perceptron
Unlabeled Attachment Score Unlabeled Attachment Score
Serial (All Data) 81.3 84.7
Serial (Sub Sampling) 77.2 80.1
Parallel (Iterative Parameter Mix) 83.5 84.5
Figure 5: Dependency Parsing experiments. Upper figures plot test data unlabeled attachment score versus wall clock
for both regular perceptron (left) and averaged perceptron (right). Lower table is unlabeled attachment score for
converged models.
0 10 20 30 40 50Training Epochs
0
2000
4000
6000
8000
10000
# Tra
ining
 Mist
akes
Error mixing (10 shards)Uniform mixing (10 shards)Error mixing (100 shards)Uniform mixing (100 shards)
Figure 6: Training errors per epoch for different shard
size and parameter mixing strategies.
form mixture case. Theorem 3 implies:
N?
n=1
S?
i=1
ki,n
S
?
R2
?2
=?
N?
n=1
S?
i=1
ki,n ? S ?
R2
?2
Thus, for cases where training errors are uniformly
distributed across shards, it is possible that, in the
worst-case, convergence may slow proportional the
the number of shards. This implies a trade-off be-
tween slower convergence and quicker epochs when
selecting a large number of shards. In fact, we ob-
served a tipping point for our experiments in which
increasing the number of shards began to have an ad-
verse effect on training times, which for the named-
entity experiments occurred around 25-50 shards.
This is both due to reasons described in this section
as well as the added overhead of maintaining and
summing multiple high-dimensional weight vectors
after each distributed epoch.
It is worth pointing out that a linear term S in
the convergence bound above is similar to conver-
gence/regret bounds for asynchronous distributed
online learning, which typically have bounds lin-
ear in the asynchronous delay (Mesterharm, 2005;
Zinkevich et al, 2009). This delay will be on aver-
age roughly equal to the number of shards S.
6 Conclusions
In this paper we have investigated distributing the
structured perceptron via simple parameter mixing
strategies. Our analysis shows that an iterative pa-
rameter mixing strategy is both guaranteed to sepa-
rate the data (if possible) and significantly reduces
the time required to train high accuracy classifiers.
However, there is a trade-off between increasing
training times through distributed computation and
slower convergence relative to the number of shards.
Finally, we note that using similar proofs to those
given in this paper, it is possible to provide theoreti-
cal guarantees for distributed online passive aggres-
sive learning (Crammer et al, 2006), which is a form
of large-margin perceptron learning. Unfortunately
space limitations prevent exploration here.
Acknowledgements: We thank Mehryar Mohri, Fer-
nando Periera, Mark Dredze and the three anonymous re-
views for their helpful comments on this work.
463
References
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning.
C.T. Chu, S.K. Kim, Y.A. Lin, Y.Y. Yu, G. Bradski, A.Y.
Ng, and K. Olukotun. 2007. Map-Reduce for ma-
chine learning on multicore. In Advances in Neural
Information Processing Systems.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithm. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551?585.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In Sixth Sym-
posium on Operating System Design and Implementa-
tion.
M. Dredze, K. Crammer, and F. Pereira. 2008.
Confidence-weighted linear classification. In Pro-
ceedings of the International Conference on Machine
learning.
J. Duchi and Y. Singer. 2009. Efficient learning using
forward-backward splitting. In Advances in Neural In-
formation Processing Systems.
J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In Proceedings of the Conference of the Association
for Computational Linguistics.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
J. Hajic?, B. Vidova Hladka, J. Panevova?, E. Hajic?ova?,
P. Sgall, and P. Pajas. 2001. Prague Dependency Tree-
bank 1.0. LDC, 2001T10.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proceedings of the Conference of
the Association for Computational Linguistics.
G. Mann, R. McDonald, M. Mohri, N. Silberman, and
D. Walker. 2009. Efficient large-scale distributed
training of conditional maximum entropy models. In
Advances in Neural Information Processing Systems.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of the Conference of the European Chapter
of the Association for Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the Conference of the Association for
Computational Linguistics.
C. Mesterharm. 2005. Online learning with delayed la-
bel feedback. In Proceedings of Algorithmic Learning
Theory.
A.B. Novikoff. 1962. On convergence proofs on percep-
trons. In Symposium on the Mathematical Theory of
Automata.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning.
J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans. 1986.
Distributed asynchronous deterministic and stochastic
gradient optimization algorithms. IEEE Transactions
on Automatic Control, 31(9):803?812.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the International Conference on Machine learning.
C. Whitelaw, A. Kehlenbeck, N. Petrovic, and L. Ungar.
2008. Web-scale named entity recognition. In Pro-
ceedings of the International Conference on Informa-
tion and Knowledge Management.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
T. Zhang. 2004. Solving large scale linear prediction
problems using stochastic gradient descent algorithms.
In Proceedings of the International Conference on Ma-
chine Learning.
M. Zinkevich, A. Smola, and J. Langford. 2009. Slow
learners are fast. In Advances in Neural Information
Processing Systems.
464
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 777?785,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The viability of web-derived polarity lexicons
Leonid Velikovich Sasha Blair-Goldensohn Kerry Hannan Ryan McDonald
Google Inc., New York, NY
{leonidv|sasha|khannan|ryanmcd}@google.com
Abstract
We examine the viability of building large
polarity lexicons semi-automatically from the
web. We begin by describing a graph propa-
gation framework inspired by previous work
on constructing polarity lexicons from lexi-
cal graphs (Kim and Hovy, 2004; Hu and
Liu, 2004; Esuli and Sabastiani, 2009; Blair-
Goldensohn et al, 2008; Rao and Ravichan-
dran, 2009). We then apply this technique
to build an English lexicon that is signifi-
cantly larger than those previously studied.
Crucially, this web-derived lexicon does not
require WordNet, part-of-speech taggers, or
other language-dependent resources typical of
sentiment analysis systems. As a result, the
lexicon is not limited to specific word classes
? e.g., adjectives that occur in WordNet ?
and in fact contains slang, misspellings, multi-
word expressions, etc. We evaluate a lexicon
derived from English documents, both qual-
itatively and quantitatively, and show that it
provides superior performance to previously
studied lexicons, including one derived from
WordNet.
1 Introduction
Polarity lexicons are large lists of phrases that en-
code the polarity of each phrase within it ? either
positive or negative ? often with some score rep-
resenting the magnitude of the polarity (Hatzivas-
siloglou and McKeown, 1997; Wiebe, 2000; Turney,
2002). Though classifiers built with machine learn-
ing algorithms have become commonplace in the
sentiment analysis literature, e.g., Pang et al (2002),
the core of many academic and commercial senti-
ment analysis systems remains the polarity lexicon,
which can be constructed manually (Das and Chen,
2007), through heuristics (Kim and Hovy, 2004;
Esuli and Sabastiani, 2009) or using machine learn-
ing (Turney, 2002; Rao and Ravichandran, 2009).
Often lexicons are combined with machine learning
for improved results (Wilson et al, 2005). The per-
vasiveness and sustained use of lexicons can be as-
cribed to a number of reasons, including their inter-
pretability in large-scale systems as well as the gran-
ularity of their analysis.
In this work we investigate the viability of polar-
ity lexicons that are derived solely from unlabeled
web documents. We propose a method based on
graph propagation algorithms inspired by previous
work on constructing polarity lexicons from lexical
graphs (Kim and Hovy, 2004; Hu and Liu, 2004;
Esuli and Sabastiani, 2009; Blair-Goldensohn et al,
2008; Rao and Ravichandran, 2009). Whereas past
efforts have used linguistic resources ? e.g., Word-
Net ? to construct the lexical graph over which prop-
agation runs, our lexicons are constructed using a
graph built from co-occurrence statistics from the
entire web. Thus, the method we investigate can
be seen as a combination of methods for propagat-
ing sentiment across lexical graphs and methods for
building sentiment lexicons based on distributional
characteristics of phrases in raw data (Turney, 2002).
The advantage of breaking the dependence on Word-
Net (or related resources like thesauri (Mohammad
et al, 2009)) is that it allows the lexicons to include
non-standard entries, most notably spelling mistakes
and variations, slang, and multiword expressions.
The primary goal of our study is to understand the
characteristics and practical usefulness of such a lex-
icon. Towards this end, we provide both a qualitative
and quantitative analysis for a web-derived English
777
lexicon relative to two previously published lexicons
? the lexicon used in Wilson et al (2005) and the
lexicon used in Blair-Goldensohn et al (2008). Our
experiments show that a web-derived lexicon is not
only significantly larger, but has improved accuracy
on a sentence polarity classification task, which is
an important problem in many sentiment analysis
applications, including sentiment aggregation and
summarization (Hu and Liu, 2004; Carenini et al,
2006; Lerman et al, 2009). These results hold true
both when the lexicons are used in conjunction with
string matching to classify sentences, and when they
are included within a contextual classifier frame-
work (Wilson et al, 2005).
Extracting polarity lexicons from the web has
been investigated previously by Kaji and Kitsure-
gawa (2007), who study the problem exclusively for
Japanese. In that work a set of positive/negative sen-
tences are first extracted from the web using cues
from a syntactic parser as well as the document
structure. Adjectives phrases are then extracted from
these sentences based on different statistics of their
occurrence in the positive or negative set. Our work,
on the other hand, does not rely on syntactic parsers
or restrict the set of candidate lexicon entries to spe-
cific syntactic classes, i.e., adjective phrases. As a
result, the lexicon built in our study is on a different
scale than that examined in Kaji and Kitsuregawa
(2007). Though this hypothesis is not tested here, it
also makes our techniques more amenable to adap-
tation for other languages.
2 Constructing the Lexicon
In this section we describe a method to construct po-
larity lexicons using graph propagation over a phrase
similarity graph constructed from the web.
2.1 Graph Propagation Algorithm
We construct our lexicon using graph propagation
techniques, which have previously been investigated
in the construction of polarity lexicons (Kim and
Hovy, 2004; Hu and Liu, 2004; Esuli and Sabas-
tiani, 2009; Blair-Goldensohn et al, 2008; Rao and
Ravichandran, 2009). We assume as input an undi-
rected edge weighted graph G = (V,E), where
wij ? [0, 1] is the weight of edge (vi, vj) ? E. The
node set V is the set of candidate phrases for inclu-
sion in a sentiment lexicon. In practice,G should en-
code semantic similarities between two nodes, e.g.,
for sentiment analysis one would hope that wij >
wik if vi=good, vj=great and vk=bad. We also as-
sume as input two sets of seed phrases, denoted P
for the positive seed set and N for the negative seed
set. The common property among all graph propaga-
tion algorithms is that they attempt to propagate in-
formation from the seed sets to the rest of the graph
through its edges. This can be done using machine
learning, graph algorithms or more heuristic means.
The specific algorithm used in this study is given
in Figure 1, which is distinct from common graph
propagation algorithms, e.g., label propagation (see
Section 2.3). The output is a polarity vector pol ?
R|V | such that poli is the polarity score for the i
th
candidate phrase (or the ith node inG). In particular,
we desire pol to have the following semantics:
poli =
?
??
??
> 0 ith phrase has positive polarity
< 0 ith phrase has negative polarity
= 0 ith phrase has no sentiment
Intuitively, the algorithm works by computing both
a positive and a negative polarity magnitude for
each node in the graph, call them pol+i and pol
-
i.
These values are equal to the sum over the max
weighted path from every seed word (either posi-
tive or negative) to node vi. Phrases that are con-
nected to multiple positive seed words through short
yet highly weighted paths will receive high positive
values. The final polarity of a phrase is then set to
poli = pol
+
i ? ?pol
-
i, where ? a constant meant to
account for the difference in overall mass of positive
and negative flow in the graph. Thus, after the al-
gorithm is run, if a phrase has a higher positive than
negative polarity score, then its final polarity will be
positive, and negative otherwise.
There are some implementation details worth
pointing out. First, the algorithm in Figure 1 is writ-
ten in an iterative framework, where on each itera-
tion, paths of increasing lengths are considered. The
input variable T controls the max path length con-
sidered by the algorithm. This can be set to be a
small value in practice, since the multiplicative path
weights result in long paths rarely contributing to
polarity scores. Second, the parameter ? is a thresh-
old that defines the minimum polarity magnitude a
778
Input: G = (V,E), wij ? [0, 1],
P , N , ? ? R, T ? N
Output: pol ? R|V |
Initialize: poli,pol
+
i ,pol
-
i = 0, for all i
pol+i = 1.0 for all vi ? P and
pol-i = 1.0 for all vi ? N
1. set ?ij = 0 for all i, j
2. for vi ? P
3. F = {vi}
4. for t : 1 . . . T
5. for (vk, vj) ? E such that vk ? F
6. ?ij = max{?ij , ?ik ? wkj}
F = F ? {vj}
7. for vj ? V
8. pol+j =
?
vi?P
?ij
9. Repeat steps 1-8 using N to compute pol-
10. ? =
?
i pol
+
i /
?
i pol
-
i
11. poli = pol
+
i ? ?pol
-
i, for all i
12. if |poli| < ? then poli = 0.0, for all i
Figure 1: Graph Propagation Algorithm.
phrase must have to be included in the lexicon. Both
T and ? were tuned on held-out data.
To construct the final lexicon, the remaining
nodes ? those with polarity scores above ? ? are ex-
tracted and assigned their corresponding polarity.
2.2 Building a Phrase Graph from the Web
Graph propagation algorithms rely on the existence
of graphs that encode meaningful relationships be-
tween candidate nodes. Past studies on building po-
larity lexicons have used linguistic resources like
WordNet to define the graph through synonym and
antonym relations (Kim and Hovy, 2004; Esuli and
Sabastiani, 2009; Blair-Goldensohn et al, 2008;
Rao and Ravichandran, 2009). The goal of this study
is to examine the size and quality of polarity lexi-
cons when the graph is induced automatically from
documents on the web.
Constructing a graph from web-computed lexi-
cal co-occurrence statistics is a difficult challenge
in and of itself and the research and implementa-
tion hurdles that arise are beyond the scope of this
work (Alfonseca et al, 2009; Pantel et al, 2009).
For this study, we used an English graph where the
node set V was based on all n-grams up to length
10 extracted from 4 billion web pages. This list was
filtered to 20 million candidate phrases using a num-
ber of heuristics including frequency and mutual in-
formation of word boundaries. A context vector for
each candidate phrase was then constructed based
on a window of size six aggregated over all men-
tions of the phrase in the 4 billion documents. The
edge set E was constructed by first, for each po-
tential edge (vi, vj), computing the cosine similar-
ity value between context vectors. All edges (vi, vj)
were then discarded if they were not one of the 25
highest weighted edges adjacent to either node vi or
vj . This serves to both reduce the size of the graph
and to eliminate many spurious edges for frequently
occurring phrases, while still keeping the graph rela-
tively connected. The weight of the remaining edges
was set to the corresponding cosine similarity value.
Since this graph encodes co-occurrences over a
large, but local context window, it can be noisy for
our purposes. In particular, we might see a number
of edges between positive and negative sentiment
words as well as sentiment words and non-sentiment
words, e.g., sentiment adjectives and all other adjec-
tives that are distributionally similar. Larger win-
dows theoretically alleviate this problem as they en-
code semantic as opposed to syntactic similarities.
We note, however, that the graph propagation al-
gorithm described above calculates the sentiment of
each phrase as the aggregate of all the best paths to
seed words. Thus, even if some local edges are erro-
neous in the graph, one hopes that, globally, positive
phrases will be influenced more by paths from pos-
itive seed words as opposed to negative seed words.
Section 3, and indeed this paper, aims to measure
whether this is true or not.
2.3 Why Not Label Propagation?
Previous studies on constructing polarity lexicons
from lexical graphs, e.g., Rao and Ravichandran
(2009), have used the label propagation algorithm,
which takes the form in Figure 2 (Zhu and Ghahra-
mani, 2002). Label propagation is an iterative algo-
rithm where each node takes on the weighted aver-
age of its neighbour?s values from the previous iter-
ation. The result is that nodes with many paths to
seeds get high polarities due to the influence from
their neighbours. The label propagation algorithm
is known to have many desirable properties includ-
ing convergence, a well defined objective function
779
Input: G = (V,E), wij ? [0, 1], P , N
Output: pol ? R|V |
Initialize: poli = 1.0 for all vi ? P and
poli = ?1.0 for all vi ? N and
poli = 0.0 ?vi /? P ?N
1. for : t .. T
2. poli =
P
(vi,vj)?E
wij?polj
P
(vi,vj)
wij
, ?vi ? V
3. reset poli = 1.0 ?vi ? P
reset poli = ?1.0 ?vi ? N
Figure 2: The label propagation algorithm (Zhu and
Ghahramani, 2002).
(minimize squared error between values of adjacent
nodes), and an equivalence to computing random
walks through graphs.
The primary difference between standard label
propagation and the graph propagation algorithm
given in Section 2.1, is that a node with multiple
paths to a seed will be influenced by all these paths
in the label propagation algorithm, whereas only the
single path from a seed will influence the polarity
of a node in our proposed propagation algorithm ?
namely the path with highest weight. The intuition
behind label propagation seems justified. That is, if
a node has multiple paths to a seed, it should be re-
flected in a higher score. This is certainly true when
the graph is of high quality and all paths trustwor-
thy. However, in a graph constructed from web co-
occurrence statistics, this is rarely the case.
Our graph consisted of many dense subgraphs,
each representing some semantic entity class, such
as actors, authors, tech companies, etc. Problems
arose when polarity flowed into these dense sub-
graphs with the label propagation algorithm. Ulti-
mately, this flow would amplify since the dense sub-
graph provided exponentially many paths from each
node to the source of the flow, which caused a re-
inforcement effect. As a result, the lexicon would
consist of large groups of actor names, companies,
etc. This also led to convergence issues since the
polarity is divided proportional to the size of the
dense subgraph. Additionally, negative phrases in
the graph appeared to be in more densely connected
regions, which resulted in the final lexicons being
highly skewed towards negative entries due to the
influence of multiple paths to seed words.
For best path propagation, these problems were
less acute as each node in the dense subgraph would
only get the polarity a single time from each seed,
which is decayed by the fact that edge weights are
smaller than 1. Furthermore, the fact that edge
weights are less than 1 results in most long paths
having weights near zero, which in turn results in
fast convergence.
3 Lexicon Evaluation
We ran the best path graph propagation algorithm
over a graph constructed from the web using manu-
ally constructed positive and negative seed sets of
187 and 192 words in size, respectively. These
words were generated by a set of five humans and
many are morphological variants of the same root,
e.g., excel/excels/excelled. The algorithm produced
a lexicon that contained 178,104 entries. Depending
on the threshold ? (see Figure 1), this lexicon could
be larger or smaller. As stated earlier, our selection
of ? and all hyperparameters was based on manual
inspection of the resulting lexicons and performance
on held-out data.
In the rest of this section we investigate the prop-
erties of this lexicon to understand both its general
characteristics as well as its possible utility in sen-
timent applications. To this end we compare three
different lexicons:
1. Wilson et al: Described in Wilson et al
(2005). Lexicon constructed by combining the
lexicon built in Riloff and Wiebe (2003) with
other sources1. Entries are are coarsely rated
? strong/weak positive/negative ? which we
weighted as 1.0, 0.5, -0.5, and -1.0 for our ex-
periments.
2. WordNet LP: Described in Blair-Goldensohn
et al (2008). Constructed using label propaga-
tion over a graph derived from WordNet syn-
onym and antonym links. Note that label prop-
agation is not prone to the kinds of errors dis-
cussed in Section 2.3 since the lexical graph is
derived from a high quality source.
3. Web GP: The web-derived lexicon described
in Section 2.1 and Section 2.2.
1See http://www.cs.pitt.edu/mpqa/
780
3.1 Qualitative Evaluation
Table 1 breaks down the lexicon by the number of
positive and negative entries of each lexicon, which
clearly shows that the lexicon derived from the web
is more than an order of magnitude larger than pre-
viously constructed lexicons.2 This in and of it-
self is not much of an achievement if the additional
phrases are of poor quality. However, in Section 3.2
we present an empirical evaluation that suggests that
these terms provide both additional and useful in-
formation. Table 1 also shows the recall of the each
lexicon relative to the other. Whereas the Wilson
et al (2005) and WordNet lexicon have a recall of
only 3% relative to the web lexicon, the web lexi-
con has a recall of 48% and 70% relative to the two
other lexicons, indicating that it contains a signifi-
cant amount of information from the other lexicons.
However, this overlap is still small, suggesting that
a combination of all the lexicons could provide the
best performance. In Section 3.2 we investigate this
empirically through a meta classification system.
Table 2 shows the distribution of phrases in the
web-derived lexicon relative to the number of to-
kens in each phrase. Here a token is simply defined
by whitespace and punctuation, with punctuation
counting as a token, e.g., ?half-baked? is counted as
3 tokens. For the most part, we see what one might
expect, as the number of tokens increases, the num-
ber of corresponding phrases in the lexicon also de-
creases. Longer phrases are less frequent and thus
will have both fewer and lower weighted edges to
adjacent nodes in the graph. There is a single phrase
of length 9, which is ?motion to dismiss for failure
to state a claim?. In fact, the lexicon contains quite
a number of legal and medical phrases. This should
not be surprising, since in a graph induced from the
web, a phrase like ?cancer? (or any disease) should
be distributionally similar to phrases like ?illness?,
?sick?, and ?death?, which themselves will be simi-
lar to standard sentiment phrases like ?bad? and ?ter-
rible?. These terms are predominantly negative in
the lexicon representing the broad notion that legal
and medical events are undesirable.
2This also includes the web-derived lexicon of (Kaji and Kit-
suregawa, 2007), which has 10K entries. A recent study by
Mohammad et al (2009) generated lexicons from thesauri with
76K entries.
Phrase length 1 2 3
# of phrases 37,449 108,631 27,822
Phrase length 4 5 6 7 8 9
# of phrases 3,489 598 71 29 4 1
Table 2: Number of phrases by phrase length in lexicon
built from the web.
Perhaps the most interesting characteristic of the
lexicon is that the most frequent phrase length is 2
and not 1. The primary reason for this is an abun-
dance of adjective phrases consisting of an adverb
and an adjective, such as ?more brittle? and ?less
brittle?. Almost every adjective of length 1 is fre-
quently combined in such a way on the web, so it
not surprising that we see many of these phrases
in the lexicon. Ideally we would see an order on
such phrases, e.g., ?more brittle? has a larger neg-
ative polarity than ?brittle?, which in turn has a
larger negative polarity than ?less brittle?. However,
this is rarely the case and usually the adjective has
the highest polarity magnitude. Again, this is eas-
ily explained. These phrases are necessarily more
common and will thus have more edges with larger
weights in the graph and thus a greater chance of ac-
cumulating a high sentiment score. The prominence
of such phrases suggests that a more principled treat-
ment of them should be investigated in the future.
Finally, Table 3 presents a selection of phrases
from both the positive and negative lexicons cate-
gorized into revealing verticals. For both positive
and negative phrases we present typical examples of
phrases ? usually adjectives ? that one would expect
to be in a sentiment lexicon. These are phrases not
included in the seed sets. We also present multiword
phrases for both positive and negative cases, which
displays concretely the advantage of building lexi-
cons from the web as opposed to using restricted lin-
guistic resources such as WordNet. Finally, we show
two special cases. The first is spelling variations
(and mistakes) for positive phrases, which were far
more prominent than for negative phrases. Many of
these correspond to social media text where one ex-
presses an increased level of sentiment by repeat-
ing characters. The second is vulgarity in negative
phrases, which was far more prominent than for pos-
itive phrases. Some of these are clearly appropri-
781
Recall wrt other lexicons
All Phrases Pos. Phrases Neg. Phrases Wilson et al WordNet LP Web GP
Wilson et al 7,628 2,718 4,910 100% 37% 2%
WordNet LP 12,310 5,705 6,605 21% 100% 3%
Web GP 178,104 90,337 87,767 70% 48% 100%
Table 1: Lexicon statistics. Wilson et al is the lexicon used in Wilson et al (2005), WordNet LP is the lexicon
constructed by Blair-Goldensohn et al (2008) that uses label propagation algorithms over a graph constructed through
WordNet, and Web GP is the web-derived lexicon from this study.
POSITIVE PHRASES NEGATIVE PHRASES
Typical Multiword expressions Spelling variations Typical Multiword expressions Vulgarity
cute once in a life time loveable dirty run of the mill fucking stupid
fabulous state - of - the - art nicee repulsive out of touch fucked up
cuddly fail - safe operation niice crappy over the hill complete bullshit
plucky just what the doctor ordered cooool sucky flash in the pan shitty
ravishing out of this world coooool subpar bumps in the road half assed
spunky top of the line koool horrendous foaming at the mouth jackass
enchanting melt in your mouth kewl miserable dime a dozen piece of shit
precious snug as a bug cozy lousy pie - in - the - sky son of a bitch
charming out of the box cosy abysmal sick to my stomach sonofabitch
stupendous more good than bad sikk wretched pain in my ass sonuvabitch
Table 3: Example positive and negative phrases from web lexicon.
ate, e.g., ?shitty?, but some are clearly insults and
outbursts that are most likely included due to their
co-occurrence with angry texts. There were also a
number of derogatory terms and racial slurs in the
lexicon, again most of which received negative sen-
timent due to their typical disparaging usage.
3.2 Quantitative Evaluation
To determine the practical usefulness of a polarity
lexicon derived from the web, we measured the per-
formance of the lexicon on a sentence classifica-
tion/ranking task. The input is a set of sentences and
the output is a classification of the sentences as be-
ing either positive, negative or neutral in sentiment.
Additionally, the system outputs two rankings, the
first a ranking of the sentence by positive polarity
and the second a ranking of the sentence by negative
polarity. Classifying sentences by their sentiment is
a subtask of sentiment aggregation systems (Hu and
Liu, 2004; Gamon et al, 2005). Ranking sentences
by their polarity is a critical sub-task in extractive
sentiment summarization (Carenini et al, 2006; Ler-
man et al, 2009).
To classify sentences as being positive, negative
or neutral, we used an augmented vote-flip algo-
rithm (Choi and Cardie, 2009), which is given in
Figure 3. This intuition behind this algorithm is sim-
ple. The number of matched positive and negative
phrases from the lexicon are counted and whichever
has the most votes wins. The algorithm flips the de-
cision if the number of negations is odd. Though this
algorithm appears crude, it benefits from not relying
on threshold values for neutral classification, which
is difficult due to the fact that the polarity scores in
the three lexicons are not on the same scale.
To rank sentences we defined the purity of a sen-
tence X as the normalized sum of the sentiment
scores for each phrase x in the sentence:
purity(X) =
?
x?X polx
? +
?
x?X |polx|
This is a normalized score in the range [?1, 1]. In-
tuitively, sentences with many terms of the same po-
larity will have purity scores at the extreme points of
the range. Before calculating purity, a simple nega-
tion heuristic was implemented that reversed the
sentiment scores of terms that were within the scope
of negations. The term ? helps to favor sentences
with multiple phrase matches. Purity is a common
metric used for ranking sentences for inclusion in
sentiment summaries (Lerman et al, 2009). Purity
and negative purity were used to rank sentences as
being positive and negative sentiment, respectively.
The data used in our initial English-only experi-
782
Lexicon Classifier Contextual Classifier
Positive Negative Positive Negative
P R AP P R AP P R AP P R AP
Wilson et al 56.4 61.8 60.8 58.1 39.0 59.7 74.5 70.3 76.2 80.7 70.1 81.2
WordNet LP 50.9 61.7 62.0 54.9 36.4 59.7 72.0 72.5 75.7 78.0 69.8 79.3
Web GP 57.7 65.1? 69.6? 60.3 42.9 68.5? 74.1 75.0? 79.9? 80.5 72.6? 82.9?
Meta Classifier - - - - - - 76.6? 74.7 81.2? 81.8? 72.2 84.1?
Table 4: Positive and negative precision (P), recall (R), and average precision (AP) for three lexicons using either
lexical matching or contextual classification strategies. ?Web GP is statistically significantly better than Wilson et al
and WordNet LP (p < 0.05). ?Meta Classifier is statistically significantly better than all other systems (p < 0.05).
Input: Scored lexicon pol, negation list NG,
input sentence X
Output: sentiment ? {POS, NEG, NEU}
1. set p, n, ng = 0
2. for x ? X
3. if polx > 0 then p++
4. else if polx < 0 then n++
5. else if x ? NG then ng++
6. flip = (ng % 2 == 1) //ng is odd
7. if (p > n & ?flip) ? (n > p & flip)
return POS
8. else if (p > n & flip) ? (n > p & ?flip)
return NEG
19. return NEU
Figure 3: Vote-flip algorithm (Choi and Cardie, 2009).
ments were a set of 554 consumer reviews described
in (McDonald et al, 2007). Each review was sen-
tence split and annotated by a human as being pos-
itive, negative or neutral in sentiment. This resulted
in 3,916 sentences, with 1,525, 1,542 and 849 posi-
tive, negative and neutral sentences, respectively.
The first six columns of Table 4 shows: 1) the pos-
itive/negative precision-recall of each lexicon-based
system where sentence classes were determined us-
ing the vote-flip algorithm, and 2) the average preci-
sion for each lexicon-based system where purity (or
negative purity) was used to rank sentences. Both
the Wilson et al and WordNet LP lexicons perform
at a similar level, with the former slightly better, es-
pecially in terms of precision. The web-derived lex-
icon, Web GP, outperforms the other two lexicons
across the board, in particular when looking at av-
erage precision, where the gains are near 10% ab-
solute. If we plot the precision-recall graphs using
purity to classify sentences ? as opposed to the vote-
flip algorithm, which only provides an unweighted
classification ? we can see that at almost all recall
levels the web-derived lexicon has superior preci-
sion to the other lexicons (Figure 4). Thus, even
though the web-derived lexicon is constructed from
a lexical graph that contains noise, the graph prop-
agation algorithms appear to be fairly robust to this
noise and are capable of producing large and accu-
rate polarity lexicons.
The second six columns of Table 4 shows the per-
formance of each lexicon as the core of a contextual
classifier (Wilson et al, 2005). A contextual classi-
fier is a machine learned classifier that predicts the
polarity of a sentence using features of that sentence
and its context. For our experiments, this was a max-
imum entropy classifier trained and evaluated us-
ing 10-fold cross-validation on the evaluation data.
The features included in the classifier were the pu-
rity score, the number of positive and negative lex-
icon matches, and the number of negations in the
sentence, as well as concatenations of these features
within the sentence and with the same features de-
rived from the sentences in a window of size 1.
For each sentence, the contextual classifier pre-
dicted either a positive, negative or neutral classifi-
cation based on the label with highest probability.
Additionally, all sentences were placed in the posi-
tive and negative sentence rankings by the probabil-
ity the classifier assigned to the positive and negative
classes, respectively. Mirroring the results of Wil-
son et al (2005), we see that contextual classifiers
improve results substantially over lexical matching.
More interestingly, we see that the a contextual clas-
sifier over the web-derived lexicons maintains the
performance edge over the other lexicons, though
the gap is smaller. Figure 5 plots the precision-recall
curves for the positive and negative sentence rank-
783
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et alWordNet LPWeb GP
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et alWordNet LPWeb GP
Figure 4: Lexicon classifier precision/recall curves for positive (left) and negative (right) classes.
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et al CCWordNet LP CCWeb GP CCMeta Classifier
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et al CCWordNet LP CCWeb GP CCMeta Classifier
Figure 5: Contextual classifier precision/recall curves for positive (left) and negative (right) classes
ings, again showing that at almost every level of re-
call, the web-derived lexicon has higher precision.
For a final English experiment we built a meta-
classification system that is identical to the contex-
tual classifiers, except it is trained using features de-
rived from all lexicons. Results are shown in the
last row of Table 4 and precision-recall curves are
shown in Figure 5. Not surprisingly, this system has
the best performance in terms of average precision
as it has access to the largest amount of information,
though its performance is only slightly better than
the contextual classifier for the web-derived lexicon.
4 Conclusions
In this paper we examined the viability of senti-
ment lexicons learned semi-automatically from the
web, as opposed to those that rely on manual anno-
tation and/or resources such as WordNet. Our quali-
tative experiments indicate that the web derived lex-
icon can include a wide range of phrases that have
not been available to previous systems, most no-
tably spelling variations, slang, vulgarity, and multi-
word expressions. Quantitatively, we observed that
the web derived lexicon had superior performance
to previously published lexicons for English clas-
sification. Ultimately, a meta classifier that incor-
porates features from all lexicons provides the best
performance. In the future we plan to investigate the
construction of web-derived lexicons for languages
other than English, which is an active area of re-
search (Mihalcea et al, 2007; Jijkoun and Hofmann,
2009; Rao and Ravichandran, 2009). The advantage
of the web-derived lexicons studied here is that they
do not rely on language specific resources besides
unlabeled data and seed lists. A primary question is
whether such lexicons improve performance over a
translate-to-English strategy (Banea et al, 2008).
Acknowledgements: The authors thank Andrew
Hogue, Raj Krishnan and Deepak Ravichandran for
insightful discussions about this work.
784
References
E. Alfonseca, K. Hall, and S. Hartmann. 2009. Large-
scale computation of distributional similarities for
queries. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT).
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. 2008.
Multilingual subjectivity analysis using machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
S. Blair-Goldensohn, K. Hannan, R. McDonald, T. Ney-
lon, G.A. Reis, and J. Reynar. 2008. Building a senti-
ment summarizer for local service reviews. In NLP in
the Information Explosion Era.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-document
summarization of evaluative text. In Proceedings of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Y. Choi and C. Cardie. 2009. Adapting a polarity lexicon
using integer linear programming for domain-specific
sentiment classification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
S.R. Das and M.Y. Chen. 2007. Yahoo! for Amazon:
Sentiment extraction from small talk on the web. Man-
agement Science, 53(9):1375?1388.
A Esuli and F. Sabastiani. 2009. SentiWordNet: A pub-
licly available lexical resource for opinion mining. In
Proceedings of the Language Resource and Evaluation
Conference (LREC).
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free text.
In Proceedings of the 6th International Symposium on
Intelligent Data Analysis (IDA).
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics (EACL).
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proceedings of the International
Conference on Knowledge Discovery and Data Min-
ing (KDD).
V.B. Jijkoun and K. Hofmann. 2009. Generating a non-
english subjectivity lexicon: Relations that matter. In
Proceedings of the European Chapter of the Associa-
tion for Computational Linguistics (EACL).
N. Kaji and M. Kitsuregawa. 2007. Building lexicon for
sentiment analysis from massive collection of HTML
documents. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL).
S.M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of the International
Conference on Computational Linguistics (COLING).
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: Evaluat-
ing and learning user preferences. In Proceedings of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured models for fine-to-coarse
sentiment analysis. In Proceedings of the Annual Con-
ference of the Association for Computational Linguis-
tics (ACL).
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Annual Conference of
the Association for Computational Linguistics (ACL).
S. Mohammad, B. Dorr, and C. Dunne. 2009. Generat-
ing high-coverage semantic orientation lexicons from
overtly marked words and a thesaurus. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learn-
ing techniques. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
D. Rao and D. Ravichandran. 2009. Semi-Supervised
Polarity Lexicon Induction. In Proceedings of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL).
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
P. Turney. 2002. Thumbs up or thumbs down? Sentiment
orientation applied to unsupervised classification of re-
views. In Proceedings of the Annual Conference of the
Association for Computational Linguistics (ACL).
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In Proceedings of the National Conference on
Artificial Intelligence (AAAI).
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
report, CMU CALD tech report CMU-CALD-02.
785
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477?487,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure
Oscar Ta?ckstro?m?
SICS / Uppsala University
Kista / Uppsala, Sweden
oscar@sics.se
Ryan McDonald
Google
New York, NY
ryanmcd@google.com
Jakob Uszkoreit
Google
Mountain View, CA
uszkoreit@google.com
Abstract
It has been established that incorporating word
cluster features derived from large unlabeled
corpora can significantly improve prediction of
linguistic structure. While previous work has
focused primarily on English, we extend these
results to other languages along two dimen-
sions. First, we show that these results hold
true for a number of languages across families.
Second, and more interestingly, we provide an
algorithm for inducing cross-lingual clusters
and we show that features derived from these
clusters significantly improve the accuracy of
cross-lingual structure prediction. Specifically,
we show that by augmenting direct-transfer sys-
tems with cross-lingual cluster features, the rel-
ative error of delexicalized dependency parsers,
trained on English treebanks and transferred
to foreign languages, can be reduced by up to
13%. When applying the same method to di-
rect transfer of named-entity recognizers, we
observe relative improvements of up to 26%.
1 Introduction
The ability to predict the linguistic structure of sen-
tences or documents is central to the field of nat-
ural language processing (NLP). Structures such as
named-entity tag sequences (Bikel et al, 1999) or sen-
timent relations (Pang and Lee, 2008) are inherently
useful in data mining, information retrieval and other
user-facing technologies. More fundamental struc-
tures such as part-of-speech tag sequences (Ratna-
parkhi, 1996) or syntactic parse trees (Collins, 1997;
Ku?bler et al, 2009), on the other hand, comprise the
core linguistic analysis for many important down-
stream tasks such as machine translation (Chiang,
?The majority of this work was performed while the author
was an intern at Google, New York, NY.
2005; Collins et al, 2005). Currently, supervised
data-driven methods dominate the literature on lin-
guistic structure prediction (Smith, 2011). Regret-
tably, the majority of studies on these methods have
focused on evaluations specific to English, since it is
the language with the most annotated resources. No-
table exceptions include the CoNLL shared tasks
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003; Buchholz and Marsi, 2006; Nivre
et al, 2007) and subsequent studies on this data, as
well as a number of focused studies on one or two
specific languages, as discussed by Bender (2011).
While annotated resources for parsing and several
other tasks are available in a number of languages, we
cannot expect to have access to labeled resources for
all tasks in all languages. This fact has given rise to
a large body of research on unsupervised (Klein and
Manning, 2004), semi-supervised (Koo et al, 2008)
and transfer (Hwa et al, 2005) systems for prediction
of linguistic structure. These methods all attempt to
benefit from the plethora of unlabeled monolingual
and/or cross-lingual data that has become available
in the digital age. Unsupervised methods are ap-
pealing in that they are often inherently language
independent. This is borne out by the many recent
studies on unsupervised parsing that include evalu-
ations covering a number of languages (Cohen and
Smith, 2009; Gillenwater et al, 2010; Naseem et al,
2010; Spitkovsky et al, 2011). However, the perfor-
mance for most languages is still well below that of
supervised systems and recent work has established
that the performance is also below simple methods
of linguistic transfer (McDonald et al, 2011).
In this study we focus on semi-supervised and
linguistic-transfer methods for multilingual structure
prediction. In particular, we pursue two lines of re-
search around the use of word cluster features in
discriminative models for structure prediction:
477
1. Monolingual word cluster features induced from
large corpora of text for semi-supervised learn-
ing (SSL) of linguistic structure. Previous stud-
ies on this approach have typically focused only
on a small set of languages and tasks (Freitag,
2004; Miller et al, 2004; Koo et al, 2008;
Turian et al, 2010; Faruqui and Pado?, 2010; Haf-
fari et al, 2011; Tratz and Hovy, 2011). Here
we show that this method is robust across 13 lan-
guages for dependency parsing and 4 languages
for named-entity recognition (NER). This is the
first study with such a broad view on this subject,
in terms of language diversity.
2. Cross-lingual word cluster features for transfer-
ring linguistic structure from English to other
languages. We develop an algorithm that gener-
ates cross-lingual word clusters; that is clusters
of words that are consistent across languages.
This is achieved by means of a probabilistic
model over large amounts of monolingual data
in two languages, coupled with parallel data
through which cross-lingual word-cluster con-
straints are enforced. We show that by augment-
ing the delexicalized direct transfer system of
McDonald et al (2011) with cross-lingual clus-
ter features, we are able to reduce its error by
up to 13% relative. Further, we show that by ap-
plying the same method to direct-transfer NER,
we achieve a relative error reduction of 26%.
By incorporating cross-lingual cluster features in a
linguistic transfer system, we are for the first time
combining SSL and cross-lingual transfer.
2 Monolingual Word Cluster Features
Word cluster features have been shown to be use-
ful in various tasks in natural language processing,
including syntactic dependency parsing (Koo et al,
2008; Haffari et al, 2011; Tratz and Hovy, 2011),
syntactic chunking (Turian et al, 2010), and NER
(Freitag, 2004; Miller et al, 2004; Turian et al, 2010;
Faruqui and Pado?, 2010). Intuitively, the reason for
the effectiveness of cluster features lie in their abil-
ity to aggregate local distributional information from
large unlabeled corpora, which aid in conquering data
sparsity in supervised training regimes as well as in
mitigating cross-domain generalization issues.
In line with much previous work on word clusters
for tasks such as dependency parsing and NER, for
which local syntactic and semantic constraints are
of importance, we induce word clusters by means of
a probabilistic class-based language model (Brown
et al, 1992; Clark, 2003). However, rather than the
more commonly used model of Brown et al (1992),
we use the predictive class bigram model introduced
by Uszkoreit and Brants (2008). The two models
are very similar, but whereas the former takes class-
to-class transitions into account, the latter directly
models word-to-class transitions. By ignoring class-
to-class transitions, an approximate maximum likeli-
hood clustering can be found efficiently with the dis-
tributed exchange algorithm (Uszkoreit and Brants,
2008). This is a useful property, as we later develop
an algorithm for inducing cross-lingual word clusters
that calls this monolingual algorithm as a subroutine.
More formally, let C : V 7? 1, . . . ,K be a (hard)
clustering function that maps each word type from the
vocabulary, V , to one ofK cluster identities. With the
model of Uszkoreit and Brants (2008), the likelihood
of a sequence of word tokens, w = ?wi?
m
i=1, with
wi ? V ? {S}, where S is a designated start-of-
segment symbol, factors as
L(w; C) =
m?
i=1
p(wi|C(wi))p(C(wi)|wi?1) . (1)
Compare this to the model of Brown et al (1992):
L?(w; C) =
m?
i=1
p(wi|C(wi))p(C(wi)|C(wi?1)) .
While the use of class-to-class transitions can lead
to more compact models, which is often useful for
conquering data sparsity, when clustering large data
sets we can get reliable statistics directly on the word-
to-class transitions (Uszkoreit and Brants, 2008).
In addition to the clustering model that we make
use of in this study, a number of additional word
clustering and embedding variants have been pro-
posed. For example, Turian et al (2010) assessed
the effectiveness of the word embedding techniques
of Collobert and Weston (2008) and Mnih and Hin-
ton (2007) along with the word clustering technique
of Brown et al (1992) for syntactic chunking and
NER. Recently, Dhillon et al (2011) proposed a word
478
Single words S0c{p}, N0c{p}, N1c{p}, N2c{p}
Word pairs S0c{p}N0c{p}, S0pcN0p, S0pN0pc,
S0wN0c, S0cN0w, N0cN1c, N1cN2c
Word triples N0cN1cN2c, S0cN0cN1c, S0hcS0cN0c,
S0cS0lcN0c, S0cS0rcN0c, S0cN0cN0lc
Distance S0cd, N0cd, S0cN0cd
Valency S0cvl, S0cvr , N0cS0vl
Unigrams S0hc, S0lc, S0rc, N0lc
Third-order S0h2c, S0l2c, S0r2c, N0l2c
Label set S0cS0ll, S0cS0rl, N0cN0ll, N0cN0rl
Table 1: Additional cluster-based parser features. Si and
Ni: the ith tokens in the stack and buffer. p: the part-of-
speech tag, c: the cluster. v: the valence of the left (l) or
right (r) set of children. l: the label of the token under
consideration. d: distance between the words on the top of
the stack and buffer. Sih, Sir and Sil: the head, right-most
modifier and left-most modifier of the token at the top of
the stack. Gx{y} expands to Gxy and Gx.
embedding method based on canonical correlation
analysis that provides state-of-the art results for word-
based SSL for English NER. As an alternative to clus-
tering words, Lin and Wu (2009) proposed a phrase
clustering approach that obtained the state-of-the-art
result for English NER.
3 Monolingual Cluster Experiments
Before moving on to the multilingual setting, we
conduct a set of monolingual experiments where we
evaluate the use of the monolingual word clusters
just described as features for dependency parsing and
NER. In the parsing experiments, we study the fol-
lowing thirteen languages:1 Danish (DA), German
(DE), Greek (EL), English (EN), Spanish (ES), French
(FR), Italian (IT), Korean (KO), Dutch (NL), Portugese
(PT), Russian (RU), Swedish (SV) and Chinese (ZH)
? representing the Chinese, Germanic, Hellenic, Ro-
mance, Slavic, Altaic and Korean genera. In the NER
experiments, we study three Germanic languages:
German (DE), English (EN) and Dutch (NL); and one
Romance language: Spanish (ES).
Details of the labeled and unlabeled data sets used
are given in Appendix A. For all experiments we
fixed the number of clusters to 256 as this performed
well on held-out data. Furthermore, we only clus-
tered the 1 million most frequent word types in each
language for both efficiency and sparsity reasons. For
1The particular choice of languages was made purely based
on data availability and institution licensing.
Word & bias w?1,0,1, w?1:0, w0:1, w?1:1, b
Pre-/suffix w:1,:2,:3,:4,:5?1,0,1 , w
?5:,?4:,?3:,?2:,?1:
?1,0,1
Orthography Hyp?1,0,1, Cap?1,0,1, Cap?1:0,
Cap0:1, Cap?1:1
PoS p?1,0,1, p?1:0, p0:1, p?1:1, p?2:1, p?1:2
Cluster c?1,0,1, c?1:0, c0:1, c?1:1, c?2:1, c?1:2
Transition ?/p?1,0,1,?/c?1,0,1,?/Cap?1,0,1,?/b
Table 2: NER features. Hyp: Word contains hyphen. Cap:
First letter is capitalized. ?/f - Transition from previous
to current label conjoined with feature f . w:j : j-character
prefix of w. w?j:: j-character suffix of w. fi: Feature f
at relative position i. fi,j : Union of features at positions i
and j. fi:j : Conjoined feature sequence between relative
positions i and j (inclusive). b: Bias.
languages in which our unlabeled data did not have
at least 1 million types, we considered all types.
3.1 Cluster Augmented Feature Models
All of the parsing experiments reported in this study
are based on the transition-based dependency parsing
paradigm (Nivre, 2008). For all languages and set-
tings, we use an arc-eager decoding strategy, with a
beam of eight hypotheses, and perform ten epochs of
the averaged structured perceptron algorithm (Zhang
and Clark, 2008). We extend the state-of-the-art fea-
ture model recently introduced by Zhang and Nivre
(2011) by adding an additional word cluster based
feature template for each word based template. Ad-
ditionally, we add templates where one or more part-
of-speech feature is replaced with the corresponding
cluster feature. The resulting set of additional fea-
ture templates are shown in Table 1. The expanded
feature model includes all of the feature templates de-
fined by Zhang and Nivre (2011), which we also use
as the baseline model, whereas Table 1 only shows
our new templates due to space limitations.
For all NER experiments, we use a sequential first-
order conditional random field (CRF) with a unit
variance Normal prior, trained with L-BFGS until
-convergence ( = 0.0001, typically obtained after
less than 400 iterations). The feature model used
for the NER tagger is shown in Table 2. These are
similar to the features used by Turian et al (2010),
with the main difference that we do not use any long
range features and that we add templates that conjoin
adjacent clusters and adjacent tags as well as tem-
plates that conjoin label transitions with tags, clusters
and capitalization features.
479
DA DE EL EN ES FR IT KO NL PT RU SV ZH AVG
NO CLUSTERS 84.3 88.9 76.1 90.3 82.8 85.7 81.4 82.0 77.2 86.9 83.5 84.7 74.9 83.0
CLUSTERS 85.8 89.5 77.3 90.7 83.6 85.7 82.2 83.6 77.8 87.6 86.0 86.5 75.5 84.0
Table 3: Supervised parsing results measured with labeled attachment score (LAS) on the test set. All results are
statistically significant at p < 0.05, except FR and NL.
DE EN ES NL AVG
NO CLUSTERS 65.4 89.2 75.0 75.7 76.3
CLUSTERS 74.8 91.8 81.1 84.2 83.0
? DEVELOPMENT SET ? TEST SET
NO CLUSTERS 69.1 83.5 78.9 79.6 77.8
CLUSTERS 74.4 87.8 82.0 85.7 82.5
Table 4: Supervised NER results measured with F1-score
on the CoNLL 2002/2003 development and test sets.
3.2 Results
The results of the parsing experiments, measured
with labeled accuracy score (LAS) on all sentence
lengths, excluding punctuation, are shown in Table 3.
The baselines are all comparable to the state-of-the-
art. On average, the addition of word cluster features
yields a 6% relative reduction in error and upwards
of 15% (for RU). All languages improve except FR,
which sees neither an increase nor a decrease in LAS.
We observe an average absolute increase in LAS
of approximately 1%, which is inline with previous
observations (Koo et al, 2008). It is perhaps not
surprising that RU sees a large gain as it is a highly
inflected language, making observations of lexical
features far more sparse. Some languages, e.g., FR,
NL, and ZH see much smaller gains. One likely cul-
prit is a divergence between the tokenization schemes
used in the treebank and in our unlabeled data, which
for Indo-European languages is closely related to the
Penn Treebank tokenization. For example, the NL
treebank contains many multi-word tokens that are
typically broken apart by our automatic tokenizer.
The NER results, in terms of F1 measure, are listed
in Table 4. Introducing word cluster features for
NER reduces relative errors on the test set by 21%
(39% on the development set) on average. Broken
down per language, reductions on the test set vary
from substantial for NL (30%) and EN (26%), down
to more modest for DE (17%) and ES (12%). The
addition of cluster features most markedly improve
recognition of the PER category, with an average error
reduction on the test set of 44%, while the reductions
for ORG (19%), LOC (17%) and MISC (10%) are more
modest, but still significant. Although our results
are below the best reported results for EN and DE
(Lin and Wu, 2009; Faruqui and Pado?, 2010), the
relative improvements of adding word clusters are
inline with previous results on NER for EN (Miller
et al, 2004; Turian et al, 2010), who report error
reductions of approximately 25% from adding word
cluster features. Slightly higher reductions where
achieved for DE by Faruqui and Pado? (2010), who
report a reduction of 22%. Note that we did not tune
hyper-parameters of the supervised learning methods
and of the clustering method, such as the number
of clusters (Turian et al, 2010; Faruqui and Pado?,
2010), and that we did not apply any heuristic for data
cleaning such as that used by Turian et al (2010).
4 Cross-lingual Word Cluster Features
All results of the previous section rely on the avail-
ability of large quantities of language specific anno-
tations for each task. Cross-lingual transfer methods
and unsupervised methods have recently been shown
to hold promise as a way to at least partially sidestep
the demand for labeled data. Unsupervised methods
attempt to infer linguistic structure without using any
annotated data (Klein and Manning, 2004) or possi-
bly by using a set of linguistically motivated rules
(Naseem et al, 2010) or a linguistically informed
model structure (Berg-Kirkpatrick and Klein, 2010).
The aim of transfer methods is instead to use knowl-
edge induced from labeled resources in one or more
source languages to construct systems for target lan-
guages in which no or few such resources are avail-
able (Hwa et al, 2005). Currently, the performance
of even the most simple direct transfer systems far
exceeds that of unsupervised systems (Cohen et al,
2011; McDonald et al, 2011; S?gaard, 2011).
480
Figure 1: Cross-lingual word cluster features for parsing. Top-left: Cross-lingual (EN-ES) word clustering model.
Top-right: Samples of some of the induced cross-lingual word clusters. Bottom-left: Delexicalized cluster-augmented
source (EN) treebank for training transfer parser. Bottom-right: Parsing of target (ES) sentence using the transfer parser.
4.1 Direct Transfer of Discriminative Models
Our starting point is the delexicalized direct transfer
method proposed by McDonald et al (2011) based on
work by Zeman and Resnik (2008). This method was
shown to outperform a number of state-of-the-art un-
supervised and transfer-based baselines. The method
is simple; for a given training set, the learner ignores
all lexical identities and only observes features over
other characteristics, e.g., part-of-speech tags, ortho-
graphic features, direction of syntactic attachment,
etc. The learner builds a model from an annotated
source language data set, after which the model is
used to directly make target language predictions.
There are three basic assumptions that drive this ap-
proach. First, that high-level tasks, such as syntactic
parsing, can be learned reliably using coarse-grained
statistics, such as part-of-speech tags, in place of
fine-grained statistics such as lexical word identities.
Second, that the parameters of features over coarse-
grained statistics are in some sense language inde-
pendent, e.g., a feature that indicates that adjectives
modify their closest noun is useful in all languages.
Third, that these coarse-grained statistics are robustly
available across languages. The approach proposed
by McDonald et al (2011) relies on these three as-
sumptions. Specifically, by replacing fine-grained
language specific part-of-speech tags with universal
part-of-speech tags, generated with the method de-
scribed by Das and Petrov (2011), a universal parser
is achieved that can be applied to any language for
which universal part-of-speech tags are available.
Below, we extend this approach to universal pars-
ing by adding cross-lingual word cluster features. A
cross-lingual word clustering is a clustering of words
in two languages, in which the clusters correspond to
some meaningful cross-lingual property. For exam-
ple, prepositions from both languages should be in
the same cluster, proper names from both languages
in another cluster and so on. By adding features de-
fined over these clusters, we can, to some degree,
481
re-lexicalize the delexicalized models, while main-
taining the ?universality? of the features. This ap-
proach is outlined in Figure 1. Assuming that we
have an algorithm for generating cross-lingual word
clusters (see Section 4.2), we can augment the delex-
icalized parsing algorithm to use these word cluster
features at training and testing time.
In order to further motivate the proposed approach,
consider the accuracy of the supervised English
parser. A parser with lexical, part-of-speech and
cluster features achieves 90.7% LAS (see Table 3). If
we remove all lexical and cluster features, the same
parser achieves 83.1%. However, if we add back just
the cluster features, the accuracy jumps back up to
89.5%, which is only 1.2% below the full system.
Thus, if we can accurately learn cross-lingual clus-
ters, there is hope of regaining some of the accuracy
lost due to the delexicalization process.
4.2 Inducing Cross-lingual Word Clusters
Our first method for inducing cross-lingual clusters
has two stages. First, it clusters a source language
(S) as in the monolingual case, and then projects
these clusters to a target language (T), using word
alignments. Given two aligned word sequences
wS =
?
wSi
?mS
i=1 and w
T =
?
wTi
?mT
j=1, let A
T |S be a
set of scored alignments from the source language to
the target language, where (wTj , w
S
aj , sj,aj ) ? A
T |S
is an alignment from the aj th source word to the jth
target word, with score sj,aj ? ?.
2 We use the short-
hand j ? AT |S to denote those target words wTj that
are aligned to some source word wSaj . Provided a
clustering CS , we assign the target word t ? VT to
the cluster with which it is most often aligned:
CT (t) = argmax
k
?
j?AT |S
s.t. wTj =t
sj,aj
[
CS(wSaj ) = k
]
, (2)
where [?] is the indicator function. We refer to the
cross-lingual clusters induced in this way as PRO-
JECTED CLUSTERS.
This simple projection approach has two potential
drawbacks. First, it only provides a clustering of
those target language words that occur in the word
2In our case, the alignment score corresponds to the condi-
tional alignment probability p(wTj |w
S
aj ). All -alignments are
ignored and we use ? = 0.95 throughout.
aligned data, which is typically smaller than our
monolingual data sets. Second, the mapped cluster-
ing may not necessarily correspond to an acceptable
target language clustering in terms of monolingual
likelihood. In order to tackle these issues, we pro-
pose the following more complex model. First, to
find clusterings that are good according to both the
source and target language, and to make use of more
unlabeled data, we model word sequences in each lan-
guage by the monolingual language model with like-
lihood function defined by equation (1). Denote these
likelihood functions respectively by LS(wS ; CS) and
LT (wT ; CT ), where we have overloaded notation so
that the word sequences denoted by wS and wT in-
clude much more plentiful non-aligned data when
taken as an argument of the monolingual likelihood
functions. Second, we couple the clusterings defined
by these individual models, by introducing additional
factors based on word alignments, as proposed by
Och (1999):
LT |S(wT ;AT |S , CT , CS) =
?
j?AT |S
p(wTj |C
T (wTj ))p(C
T (wTj )|C
S(wSaj )) .
and the symmetric LS|T (wS ;AS|T , CS , CT ). Note
that the simple projection defined by equation (2)
correspond to a hard assignment variant of this prob-
abilistic formulation when the source clustering is
fixed. Combining all four factors results in the joint
monolingual and cross-lingual objective function
LS,T (wS ,wT ;AT |S ,AS|T , CS , CT ) =
LS(. . .) ? LT (. . .) ? LT |S(. . .) ? LS|T (. . .) . (3)
The intuition of this approach is that the clusterings
CS and CT are forced to jointly explain the source
and target data, treating the word alignments as a
form of soft constraints. We approximately optimize
(3) with the alternating procedure in Algorithm 1, in
which we iteratively maximize LS and LT , keeping
the other factors fixed. In this way we can generate
cross-lingual clusterings using all the monolingual
data while forcing the clusterings to obey the word
alignment constraints. We refer to the clusters in-
duced with this method as X-LINGUAL CLUSTERS.
In practice we found that each unconstrained
monolingual run of the exchange algorithm (lines
482
Algorithm 1 Cross-lingual clustering.
Randomly initialize source/target clusterings CS and CT .
for i = 1 . . . N do
1. Find C?S ? argmaxCS L
S(wS ; CS). (?)
2. Project C?S to CT using equation (2).
- keep cluster of non-projected words in CT fixed.
3. Find C?T ? argmaxCT L
T (wT ; CT ). (?)
4. Project C?T to CS using equation (2).
- keep cluster of non-projected words in CS fixed.
end for
? Optimized via the exchange algorithm keeping the cluster
of projected words fixed and only clustering additional words
not in the projection.
1 and 3) moves the clustering too far from those that
obey the word alignment constraints, which causes
the procedure to fail to converge. However, we found
that fixing the clustering of the words that are as-
signed clusters in the projection stages (lines 2 and
4) and only clustering the remaining words works
well in practice. Furthermore, we found that iterating
the procedure has little effect on performance and set
N = 1 for all subsequent experiments.
5 Cross-lingual Experiments
In our first set of experiments on using cross-lingual
cluster features, we evaluate direct transfer of our
EN parser, trained on Stanford style dependencies
(De Marneffe et al, 2006), to the the ten non-EN
Indo-European languages listed in Section 3. We ex-
clude KO and ZH as initial experiments proved direct
transfer a poor technique when transferring parsers
between such diverse languages. We study the impact
of using cross-lingual cluster features by comparing
the strong delexicalized baseline model of McDon-
ald et al (2011), which only has features derived
from universal part-of-speech tags, projected from
English with the method of Das and Petrov (2011), to
the same model when adding features derived from
cross-lingual clusters. In both cases the feature mod-
els are the same as those used in Section 3.1, except
that they are delexicalized by removing all lexical
word-identity features. We evaluate both the PRO-
JECTED CLUSTERS and the X-LINGUAL CLUSTERS.
For these experiments we train the perceptron
for only five epochs in order to prevent over-fitting,
which is an acute problem due to the divergence be-
tween the training and testing data sets in this setting.
Furthermore, in accordance to standard practices we
only evaluate unlabeled attachment score (UAS) due
to the fact that each treebank uses a different ? possi-
bly non-overlapping ? label set.
In our second set of experiments, we evaluate di-
rect transfer of a NER system trained on EN to DE,
ES and NL. We use the same feature models as in
the monolingual case, with the exception that we use
universal part-of-speech tags for all languages and
we remove the capitalization feature when transfer-
ring from EN to DE. Capitalization is both a prevalent
and highly predictive feature of named-entities in EN,
while in DE, capitalization is even more prevalent, but
has very low predictive power. Interestingly, while
delexicalization has shown to be important for di-
rect transfer of dependency-parsers (McDonald et al,
2011), we noticed in preliminary experiments that
it substantially degrades performance for NER. We
hypothesize that this is because word features are pre-
dictive of common proper names and that these are
often translated directly across languages, at least in
the case of newswire text. As for the transfer parser,
when training the source NER model, we regularize
the model more heavily by setting ? = 0.1.
Appendix A contains the details of the training,
testing, unlabeled and parallel/aligned data sets.
5.1 Results
Table 5 lists the results of the transfer experiments
for dependency parsing. The baseline results are
comparable to those in McDonald et al (2011) and
thus also significantly outperform the results of re-
cent unsupervised approaches (Berg-Kirkpatrick and
Klein, 2010; Naseem et al, 2010). Importantly, cross-
lingual cluster features are helpful across the board
and give a relative error reduction ranging from 3%
for DA to 13% for PT, with an average reduction of
6%, in terms of unlabeled attachment score (UAS).
This shows the utility of cross-lingual cluster fea-
tures for syntactic transfer. However, X-LINGUAL
CLUSTERS provides roughly the same performance
as PROJECTED CLUSTERS suggesting that even sim-
ple methods of cross-lingual clustering are sufficient
for direct transfer dependency parsing.
We would like to stress that these results are likely
to be under-estimating the parsers? actual ability to
predict Stanford-style dependencies in the target lan-
guages. This is because the target language anno-
tations that we use for evaluation differ from the
483
DA DE EL ES FR IT NL PT RU SV AVG
NO CLUSTERS 36.7 48.9 59.5 60.2 70.0 64.6 52.8 66.8 29.7 55.4 54.5
PROJECTED CLUSTERS 38.9 50.3 61.1 62.6 71.6 68.6 54.5 70.7 32.9 57.0 56.8
X-LINGUAL CLUSTERS 38.7 50.7 63.0 62.9 72.1 68.8 54.3 71.0 34.4 56.9 57.3
? ALL DEPENDENCY RELATIONS ? ONLY SUBJECT/OBJECT RELATIONS
NO CLUSTERS 44.6 56.7 67.2 60.7 77.4 64.6 59.5 53.3 29.3 57.3 57.1
PROJECTED CLUSTERS 49.8 57.1 72.2 65.9 80.4 70.5 67.0 62.6 34.6 65.0 62.5
X-LINGUAL CLUSTERS 49.2 59.0 72.5 65.9 80.9 72.7 65.7 62.5 37.2 64.4 63.0
Table 5: Direct transfer dependency parsing from English. Results measured by unlabeled attachment score (UAS).
ONLY SUBJECT/OBJECT RELATIONS ? UAS measured only over words marked as subject/object in the evaluation data.
Stanford dependency annotation. Some of these dif-
ferences are warranted in that certain target language
phenomena are better captured by the native annota-
tion. However, differences such as choice of lexical
versus functional head are more arbitrary.
To highlight this point we run two additional ex-
periments. First, we had linguists, who were also
fluent speakers of German, re-annotate the DE test set
so that unlabeled arcs are consistent with Stanford-
style dependencies. Using this data, NO CLUSTERS
obtains 60.0% UAS, PROJECTED CLUSTERS 63.6%
and X-LINGUAL CLUSTERS 64.4%. When compared
to the scores on the original data set (48.9%, 50.3%
and 50.7%, respectively) we can see that not only is
the baseline system doing much better, but that the
improvements from cross-lingual clustering are much
more pronounced. Next, we investigated the accuracy
of subject and object dependencies, as these are often
annotated in similar ways across treebanks, typically
modifying the main verb of the sentence. The bottom
half of Table 5 gives the scores when restricted to
such dependencies in the gold data. We measure the
percentage of modifiers in subject and object depen-
dencies that modify the correct word. Indeed, here
we see the difference in performance become clearer,
with the cross-lingual cluster model reducing errors
by 14% relative to the non-cross-lingual model and
upwards of 22% relative for IT.
We now turn to the results of the transfer experi-
ments for NER, listed in Table 6. While the perfor-
mance of the transfer systems is very poor when no
word clusters are used, adding cross-lingual word
clusters give substantial improvements across all lan-
guages. The simple PROJECTED CLUSTERS work
well, but the X-LINGUAL CLUSTERS provide even
larger improvements. On average the latter reduce
DE ES NL AVG
NO CLUSTERS 25.4 49.5 49.9 41.6
PROJECTED CLUSTERS 39.1 62.1 61.8 54.4
X-LINGUAL CLUSTERS 43.1 62.8 64.7 56.9
? DEVELOPMENT SET ? TEST SET
NO CLUSTERS 23.5 45.6 48.4 39.1
PROJECTED CLUSTERS 35.2 59.1 56.4 50.2
X-LINGUAL CLUSTERS 40.4 59.3 58.4 52.7
Table 6: Direct transfer NER results (from English) mea-
sured with average F1-score on the CoNLL 2002/2003
development and test sets.
errors on the test set by 22% in terms of F1 and up
to 26% for ES. We also measure how well the di-
rect transfer NER systems are able to detect entity
boundaries (ignoring the entity categories). Here, on
average, the best clusters provide a 24% relative error
reduction on the test set (75.8 vs. 68.1 F1).
To our knowledge there are no comparable results
on transfer learning of NER systems. Based on the
results of this first attempt at this scenario, we believe
that transfer learning by multilingual word clusters
could be developed into a practical way to construct
NER systems for resource poor languages.
6 Conclusion
In the first part of this study, we showed that word
clusters induced from a simple class-based language
model can be used to significantly improve on state-
of-the-art supervised dependency parsing and NER
for a wide range of languages and even across lan-
guage families. Although the improvements vary
between languages, the addition of word cluster fea-
tures never has a negative impact on performance.
484
This result has important practical consequences as
it allows practitioners to simply plug in word clus-
ter features into their current feature models. Given
previous work on word clusters for various linguistic
structure prediction tasks, these results are not too
surprising. However, to our knowledge this is the first
study to apply the same type of word cluster features
across languages and tasks.
In the second part, we provided two simple meth-
ods for inducing cross-lingual word clusters. The first
method works by projecting word clusters, induced
from monolingual data, from a source language to
a target language directly via word alignments. The
second method, on the other hand, makes use of
monolingual data in both the source and the target
language, together with word alignments that act as
constraints on the joint clustering. We then showed
that by using these cross-lingual word clusters, we
can significantly improve on direct transfer of dis-
criminative models for both parsing and NER. As
in the monolingual case, both types of cross-lingual
word cluster features yield improvements across the
board, with the more complex method providing a
significantly larger improvement for NER. Although
the performance of transfer systems is still substan-
tially below that of supervised systems, this research
provides one step towards bridging this gap. Further,
we believe that it opens up an avenue for future work
on multilingual clustering methods, cross-lingual fea-
ture projection and domain adaptation for direct trans-
fer of linguistic structure.
Acknowledgments
We thank John DeNero for help with creating the
word alignments; Reut Tsarfaty and Joakim Nivre for
rewarding discussions on evaluation; Slav Petrov and
Kuzman Ganchev for discussions on cross-lingual
clustering; and the anonymous reviewers, along with
Joakim Nivre, for valuable comments that helped
improve the paper. The first author is grateful for the
financial support of the Swedish National Graduate
School of Language Technology (GSLT).
A Data Sets
In the parsing experiments, we use the following data
sets. For DA, DE, EL, ES, IT, NL, PT and SV, we
use the predefined training and evaluation data sets
from the CoNLL 2006/2007 data sets (Buchholz and
Marsi, 2006; Nivre et al, 2007). For EN we use
sections 02-21, 22, and 23 of the Penn WSJ Tree-
bank (Marcus et al, 1993) for training, development
and evaluation. For FR we used the French Treebank
(Abeille? and Barrier, 2004) with splits defined in Can-
dito et al (2010). For KO we use the Sejong Korean
Treebank (Han et al, 2002) randomly splitting the
data into 80% training, 10% development and 10%
evaluation. For RU we use the SynTagRus Treebank
(Boguslavsky et al, 2000; Apresjan et al, 2006) ran-
domly splitting the data into 80% training, 10% devel-
opment and 10% evaluation. For ZH we use the Penn
Chinese Treebank v6 (Xue et al, 2005) using the
proposed data splits from the documentation. Both
EN and ZH were converted to dependencies using
v1.6.8 of the Stanford Converter (De Marneffe et al,
2006). FR was converted using the procedure defined
in Candito et al (2010). RU and KO are native depen-
dency treebanks. For the CoNLL data sets we use
the part-of-speech tags provided with the data. For
all other data sets, we train a part-of-speech tagger
on the training data in order to tag the development
and evaluation data.
For the NER experiments we use the training, de-
velopment and evaluation data sets from the CoNLL
2002/2003 shared tasks (Tjong Kim Sang, 2002;
Tjong Kim Sang and De Meulder, 2003) for all
four languages (DE, EN, ES and NL). The data set
for each language consists of newswire text anno-
tated with four entity categories: Location (LOC),
Miscellaneous (MISC), Organization (ORG) and Per-
son (PER). We use the part-of-speech tags supplied
with the data, except for ES where we instead use
universal part-of-speech tags (Petrov et al, 2011).
Unlabeled data for training the monolingual cluster
models was extracted from one year of newswire ar-
ticles from multiple sources from a news aggregation
website. This consists of 0.8 billion (DA) to 121.6 bil-
lion (EN) tokens per language. All word alignments
for the cross-lingual clusterings were produced with
the dual decomposition aligner described by DeNero
and Macherey (2011) using 10.5 million (DA) to 12.1
million (FR) sentences of aligned web data.
485
References
Anne Abeille? and Nicolas Barrier. 2004. Enriching a
french treebank. In Proceedings of LREC.
Juri Apresjan, Igor Boguslavsky, Boris Iomdin, Leonid
Iomdin, Andrei Sannikov, and Victor Sizov. 2006. A
syntactically and semantically tagged corpus of russian:
State of the art and prospects. In Proceedings of LREC.
Emily M. Bender. 2011. On achieving and evaluating
language-independence in NLP. Linguistic Issues in
Language Technology, 6(3):1?26.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of ACL.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34(1):211?231.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grigoriev,
Leonid Kreidlin, and Nadezhda Frid. 2000. Depen-
dency treebank for Russian: Concept, tools, types of
information. In Proceedings of COLING.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank conver-
sion and first results. In Proceedings of LREC.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induction.
In Proceedings of EACL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine trans-
lation. In Proceedings of ACL.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceed-
ings of LREC.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Paramveer Dhillon, Dean Foster, and Lyle Dean. 2011.
Multi-view learning of word embeddings via cca. In
Proceedings of NIPS.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and evaluating a german named entity recognizer with
semantic generalization. In Proceedings of KONVENS.
Dayne Freitag. 2004. Trained named entity recogni-
tion using distributional clusters. In Proceedings of
EMNLP.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity in
dependency grammar induction. In Proceedings of
ACL.
Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar.
2011. An ensemble model that combines syntactic
and semantic clustering for discriminative dependency
parsing. In Proceedings of ACL.
Chung-hye Han, Na-Rare Han, Eon-Suk Ko, and Martha
Palmer. 2002. Development and evaluation of a korean
treebank and its application to nlp. In Proceedings of
LREC.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of dependency
and constituency. In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-HLT.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre. 2009.
Dependency parsing. Morgan & Claypool Publishers.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP, pages 1030?1038.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
486
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative
training. In Proceedings of HLT-NAACL.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of ICML.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Franz Josef Och. 1999. An efficient method for determin-
ing bilingual word classes. In Proceedings of EACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Publishers Inc.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A
universal part-of-speech tagset. In ArXiv:1104.2086.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Morgan & Claypool Publishers.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of EMNLP.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL.
Erik F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named entity
recognition. In Proceedings of CoNLL.
Stephen Tratz and Eduard Hovy. 2011. A fast, effec-
tive, non-projective, semantically-enriched parser. In
Proceedings of EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.
2005. The penn chinese treebank: Phrase structure
annotation of a large corpus. Natural Language Engi-
neering, 11(02):207?238.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In IJC-
NLP Workshop: NLP for Less Privileged Languages.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of EMNLP.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
487
Proceedings of NAACL-HLT 2013, pages 1061?1071,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Target Language Adaptation of Discriminative Transfer Parsers
Oscar T?ckstr?m?
SICS | Uppsala University
Sweden
oscar@sics.se
Ryan McDonald
Google
New York
ryanmcd@google.com
Joakim Nivre?
Uppsala University
Sweden
joakim.nivre@lingfil.uu.se
Abstract
We study multi-source transfer parsing for
resource-poor target languages; specifically
methods for target language adaptation of
delexicalized discriminative graph-based de-
pendency parsers. We first show how recent
insights on selective parameter sharing, based
on typological and language-family features,
can be applied to a discriminative parser by
carefully decomposing its model features. We
then show how the parser can be relexicalized
and adapted using unlabeled target language
data and a learning method that can incorporate
diverse knowledge sources through ambiguous
labelings. In the latter scenario, we exploit
two sources of knowledge: arc marginals de-
rived from the base parser in a self-training
algorithm, and arc predictions from multiple
transfer parsers in an ensemble-training algo-
rithm. Our final model outperforms the state of
the art in multi-source transfer parsing on 15
out of 16 evaluated languages.
1 Introduction
Many languages still lack access to core NLP tools,
such as part-of-speech taggers and syntactic parsers.
This is largely due to the reliance on fully supervised
learning methods, which require large quantities of
manually annotated training data. Recently, meth-
ods for cross-lingual transfer have appeared as a
promising avenue for overcoming this hurdle for both
part-of-speech tagging (Yarowsky et al, 2001; Das
and Petrov, 2011) and syntactic dependency parsing
(Hwa et al, 2005; Zeman and Resnik, 2008; Ganchev
et al, 2009; McDonald et al, 2011; Naseem et al,
?Work primarily carried out while at Google, NY.
2012). While these methods do not yet compete with
fully supervised approaches, they can drastically out-
perform both unsupervised methods (Klein and Man-
ning, 2004) and weakly supervised methods (Naseem
et al, 2010; Berg-Kirkpatrick and Klein, 2010).
A promising approach to cross-lingual transfer
of syntactic dependency parsers is to use multiple
source languages and to tie model parameters across
related languages. This idea was first explored for
weakly supervised learning (Cohen and Smith, 2009;
Snyder et al, 2009; Berg-Kirkpatrick and Klein,
2010) and recently by Naseem et al (2012) for multi-
source cross-lingual transfer. In particular, Naseem
et al showed that by selectively sharing parameters
based on typological features of each language, sub-
stantial improvements can be achieved, compared
to using a single set of parameters for all languages.
However, these methods all employ generative mod-
els with strong independence assumptions and weak
feature representations, which upper bounds their ac-
curacy far below that of feature-rich discriminative
parsers (McDonald et al, 2005; Nivre, 2008).
In this paper, we improve upon the state of the art
in cross-lingual transfer of dependency parsers from
multiple source languages by adapting feature-rich
discriminatively trained parsers to a specific target
language. First, in ?4 we show how selective sharing
of model parameters based on typological traits can
be incorporated into a delexicalized discriminative
graph-based parsing model. This requires a careful
decomposition of features into language-generic and
language-specific sets in order to tie specific target
language parameters to their relevant source language
counterparts. The resulting parser outperforms the
method of Naseem et al (2012) on 12 out of 16 eval-
uated languages. Second, in ?5 we introduce a train-
1061
ing method that can incorporate diverse knowledge
sources through ambiguously predicted labelings of
unlabeled target language data. This permits effective
relexicalization and target language adaptation of the
transfer parser. Here, we experiment with two differ-
ent knowledge sources: arc sets, which are filtered by
marginal probabilities from the cross-lingual transfer
parser, are used in an ambiguity-aware self-training
algorithm (?5.2); these arc sets are then combined
with the predictions of a different transfer parser in an
ambiguity-aware ensemble-training algorithm (?5.3).
The resulting parser provides significant improve-
ments over a strong baseline parser and achieves a
13% relative error reduction on average with respect
to the best model of Naseem et al (2012), outper-
forming it on 15 out of the 16 evaluated languages.
2 Multi-Source Delexicalized Transfer
The methods proposed in this paper fall into the delex-
icalized transfer approach to multilingual syntactic
parsing (Zeman and Resnik, 2008; McDonald et al,
2011; Cohen et al, 2011; S?gaard, 2011). In contrast
to annotation projection approaches (Yarowsky et al,
2001; Hwa et al, 2005; Ganchev et al, 2009; Spreyer
and Kuhn, 2009), delexicalized transfer methods do
not rely on any bitext. Instead, a parser is trained
on annotations in a source language, relying solely
on features that are available in both the source
and the target language, such as ?universal? part-of-
speech tags (Zeman and Resnik, 2008; Naseem et al,
2010; Petrov et al, 2012), cross-lingual word clusters
(T?ckstr?m et al, 2012) or type-level features derived
from bilingual dictionaries (Durrett et al, 2012).1
This parser is then directly used to parse the target
language. For languages with similar typology, this
method can be quite accurate, especially when com-
pared to purely unsupervised methods. For instance,
a parser trained on English with only part-of-speech
features can correctly parse the Greek sentence in Fig-
ure 1, even without knowledge of the lexical items
since the sequence of part-of-speech tags determines
the syntactic structure almost unambiguously.
Learning with multiple languages has been shown
to benefit unsupervised learning (Cohen and Smith,
1Note that T?ckstr?m et al (2012) and Durrett et al (2012)
do require bitext or a bilingual dictionary. The same holds for
most cross-lingual representations, e.g., Klementiev et al (2012).
? ???? ????? ???? ????? ?? ?????? .
(The) (John) (gave) (to-the) (Maria) (the) (book) .
DET NOUN VERB ADP NOUN DET NOUN P
DET NSUBJ
ROOT
PREP POBJ
DOBJ
DET
PUNC
Figure 1: A Greek sentence which is correctly parsed by a
delexicalized English parser, provided that part-of-speech
tags are available in both the source and target language.
2009; Snyder et al, 2009; Berg-Kirkpatrick and
Klein, 2010). Annotations in multiple languages
can be combined in delexicalized transfer as well, as
long as the parser features are available across the in-
volved languages. This idea was explored by McDon-
ald et al (2011), who showed that target language
accuracy can be improved by simply concatenating
delexicalized treebanks in multiple languages. In
similar work, Cohen et al (2011) proposed a mixture
model in which the parameters of a generative target
language parser is expressed as a linear interpola-
tion of source language parameters, whereas S?gaard
(2011) showed that target side language models can
be used to selectively subsample training sentences
to improve accuracy. Recently, inspired by the phylo-
genetic prior of Berg-Kirkpatrick and Klein (2010),
S?gaard and Wulff (2012) proposed ? among other
ideas ? a typologically informed weighting heuristic
for linearly interpolating source language parameters.
However, this weighting did not provide significant
improvements over uniform weighting.
The aforementioned approaches work well for
transfer between similar languages. However, their
assumptions cease to hold for typologically divergent
languages; a target language can rarely be described
as a linear combination of data or model parameters
from a set of source languages, as languages tend
to share varied typological traits; this critical insight
is discussed further in ?4. To account for this issue,
Naseem et al (2012) recently introduced a novel gen-
erative model of dependency parsing, in which the
generative process is factored into separate steps for
the selection of dependents and their ordering. The
parameters used in the selection step are all language
independent, capturing only head-dependent attach-
ment preferences. In the ordering step, however, pa-
rameters are selectively shared between subsets of
1062
Feature Description
81A Order of Subject, Object and Verb
85A Order of Adposition and Noun
86A Order of Genitive and Noun
87A Order of Adjective and Noun
88A Order of Demonstrative and Noun
89A Order of Numeral and Noun
Table 1: Typological features from WALS (Dryer and
Haspelmath, 2011), proposed for selective sharing by
Naseem et al (2012). Feature 89A has the same value for
all studied languages, while 88A differs only for Basque.
These features are therefore subsequently excluded.
source languages based on typological features of
the languages extracted from WALS ? the World
Atlas of Language Structures (Dryer and Haspelmath,
2011) ? as shown in Table 1. In the transfer scenario,
where no supervision is available in the target lan-
guage, this parser achieves the hitherto best published
results across a number of languages; in particular
for target languages with a word order divergent from
the source languages.
However, the generative model of Naseem et al is
quite impoverished. In the fully supervised setting,
it obtains substantially lower accuracies compared
to a standard arc-factored graph-based parser (Mc-
Donald et al, 2005). Averaged across 16 languages,2
the generative model trained with full supervision on
the target language obtains an accuracy of 67.1%. A
comparable lexicalized discriminative arc-factored
model (McDonald et al, 2005) obtains 84.1%. Even
when delexicalized, this model reaches 78.9%. This
gap in supervised accuracy holds for all 16 languages.
Thus, while selective sharing is a powerful device for
transferring parsers across languages, the underly-
ing generative model used by Naseem et al (2012)
restricts its potential performance.
3 Basic Models and Experimental Setup
Inspired by the superiority of discriminative graph-
based parsing in the supervised scenario, we inves-
tigate whether the insights of Naseem et al (2012)
on selective parameter sharing can be incorporated
into such models in the transfer scenario. We first re-
view the basic graph-based parser framework and the
2Based on results in Naseem et al (2012), excluding English.
experimental setup that we will use throughout. We
then delve into details on how to incorporate selec-
tive sharing in this model in ?4. In ?5, we show how
learning with ambiguous labelings in this parser can
be used for further target language adaptation, both
through self-training and through ensemble-training.
3.1 Discriminative Graph-Based Parser
Let x denote an input sentence and let y ? Y(x)
denote a dependency tree, where Y(x) is the set of
well-formed dependency trees spanning x. Hence-
forth, we restrictY(x) to projective dependency trees,
but all our methods are equally applicable in the non-
projective case. Provided a vector of model parame-
ters ?, the probability of a dependency tree y ? Y(x),
conditioned on a sentence x, has the following form:
p?(y | x) =
exp
{
?>?(x, y)
}
?
y??Y(x) exp {?
>?(x, y?)}
.
Without loss of generality, we restrict ourselves to
first-order models, where the feature function ?(x, y)
factors over individual arcs (h,m) in y, such that
?(x, y) =
?
(h,m)?y
?(x, h,m) ,
where h ? [0, |x|] and m ? [1, |x|] are the indices
of the head word and the dependent word of the
arc; h = 0 represents a dummy ROOT token. The
model parameters are estimated by maximizing the
log-likelihood of the training dataD = {(xi, yi)}ni=1,
L(?;D) =
n?
i=1
log p?(yi | xi) .
We use the standard gradient-based L-BFGS algo-
rithm (Liu and Nocedal, 1989) to maximize the log-
likelihood. Eisner?s algorithm (Eisner, 1996) is used
for inference of the Viterbi parse and arc-marginals.
3.2 Data Sets and Experimental Setup
To facilitate comparison with the state of the art, we
use the same treebanks and experimental setup as
Naseem et al (2012). Notably, we use the map-
ping proposed by Naseem et al (2010) to map from
fine-grained treebank specific part-of-speech tags to
coarse-grained ?universal? tags, rather than the more
recent mapping proposed by Petrov et al (2012). For
1063
l[l]? h.p
[l]? m.p
[l]? h.p? m.p
d? w.81A? 1[h.p = VERB ? m.p = NOUN]
d? w.81A? 1[h.p = VERB ? m.p = PRON]
d? w.85A? 1[h.p = ADP ? m.p = NOUN]
d? w.85A? 1[h.p = ADP ? m.p = PRON]
d? w.86A? 1[h.p = NOUN ? m.p = NOUN]
d? w.87A? 1[h.p = NOUN ? m.p = ADJ]
d? l; [d? l]? h.p; [d? l]? m.p
[d? l]? h.p? m.p
[d? l]? h.p? h+1.p? m?1.p? m.p
[d? l]? h?1.p? h.p? m?1.p? m.p
[d? l]? h.p? h+1.p? m.p? m+1.p
[d? l]? h?1.p? h.p? m.p? m+1.p
h.p? between.p? m.p
Delexicalized MSTParser Selectively sharedBare
Figure 2: Arc-factored feature templates for graph-based parsing. Direction: d ? {LEFT, RIGHT}; dependency length:
l ? {1, 2, 3, 4, 5+}; part of speech of head / dependent / words between head and dependent: h.p / m.p / between.p ?
{NOUN, VERB, ADJ, ADV, PRON, DET, ADP, NUM, CONJ, PRT, PUNC, X}; token to the left / right of z: z?1 / z+1; WALS
features: w.X for X = 81A, 85A, 86A, 87A (see Table 1). [?] denotes an optional template, e.g., [d? l] ? h.p? m.p
expands to templates d? l? h.p? m.p and h.p? m.p, so that the template also falls back on its undirectional variant.
each target language evaluated, the treebanks of the
remaining languages are used as labeled training data,
while the target language treebank is used for testing
only (in ?5 a different portion of the target language
treebank is additionally used as unlabeled training
data). We refer the reader to Naseem et al (2012) for
detailed information on the different treebanks. Due
to divergent treebank annotation guidelines, which
makes fine-grained evaluation difficult, all results
are evaluated in terms of unlabeled attachment score
(UAS). In line with Naseem et al (2012), we use gold
part-of-speech tags and evaluate only on sentences
of length 50 or less excluding punctuation.
3.3 Baseline Models
We compare our models to two multi-source base-
line models. The first baseline, NBG, is the gener-
ative model with selective parameter sharing from
Naseem et al (2012).3 This model is trained without
target language data, but we investigate the use of
such data in ?5.4. The second baseline, Delex, is a
delexicalized projective version of the well-known
graph-based MSTParser (McDonald et al, 2005).
The feature templates used by this model are shown
to the left in Figure 2. Note that there is no selective
sharing in this model.
The second and third columns of Table 2 show the
unlabeled attachment scores of the baseline models
for each target language. We see that Delex performs
well on target languages that are related to the major-
ity of the source languages. However, for languages
3Model ?D-,To? in Table 2 from Naseem et al (2012).
that diverge from the Indo-European majority family,
the selective sharing model, NBG, achieves substan-
tially higher accuracies.
4 Feature-Based Selective Sharing
The results for the baseline models are not surpris-
ing considering the feature templates used by Delex.
There are two fundamental issues with these fea-
tures when used for direct transfer. First, all but
one template include the arc direction. Second,
some features are sensitive to local word order; e.g.,
[d? l]? h.p? h+1.p? m?1.p? m.p, which models
direction as well as word order in the local contexts
of the head and the dependent. Such features do not
transfer well across typologically different languages.
In order to verify that these issues are the cause of
the poor performance of the Delex model, we remove
all directional features and all features that model
local word order from Delex. The feature templates
of the resulting Bare model are shown in the center
of Figure 2. These features only model selectional
preferences and dependency length, analogously to
the selection component of NBG. The performance
of Bare is shown in the fourth column of Table 2.
The removal of most of the features results in a per-
formance drop on average. However, for languages
outside of the Indo-European family, Bare is often
more accurate, especially for Basque, Hungarian and
Japanese, which supports our hypothesis.
4.1 Sharing Based on Typological Features
After removing all directional features, we now care-
fully reintroduce them. Inspired by Naseem et al
1064
Graph-Based Models
Lang. NBG Delex Bare Share Similar Family
ar 57.2 43.3 43.1 52.7 52.7 52.7
bg 67.6 64.5 56.1 65.4 62.4 65.4
ca 71.9 72.0 58.1 66.1 80.2 77.6
cs 43.9 40.5 43.1 42.5 45.3 43.5
de 54.0 57.0 49.3 55.2 58.1 59.2
el 61.9 63.2 57.7 62.9 59.9 63.2
es 62.3 66.9 52.6 59.3 69.0 67.1
eu 39.7 29.5 43.3 46.8 46.8 46.8
hu 56.9 56.2 60.5 64.5 64.5 64.5
it 68.0 70.8 55.7 63.5 74.6 72.5
ja 62.3 38.9 50.6 57.1 64.6 65.9
nl 56.2 57.9 51.6 55.0 51.8 56.8
pt 76.2 77.5 63.0 72.7 78.4 78.4
sv 52.0 61.4 55.9 58.8 48.8 63.5
tr 59.1 37.4 36.0 41.7 59.5 59.4
zh 59.9 45.1 47.9 54.8 54.8 54.8
avg 59.3 55.1 51.5 57.4 60.7 62.0
Table 2: Unlabeled attachment scores of the multi-source
transfer models. Boldface numbers indicate the best result
per language. Underlined numbers indicate languages
whose group is not represented in the training data (these
default to Share under Similarity and Family). NBG is the
?D-,To? model in Table 2 from Naseem et al (2012).
(2012), we make use of the typological features from
WALS (Dryer and Haspelmath, 2011), listed in Ta-
ble 1, to selectively share directional parameters be-
tween languages. As a natural first attempt at sharing
parameters, one might consider forming the cross-
product of all features of Delex with all WALS prop-
erties, similarly to a common domain adaptation tech-
nique (Daum? III, 2007; Finkel and Manning, 2009).
However, this approach has two issues. First, it re-
sults in a huge number of features, making the model
prone to overfitting. Second, and more critically, it
ties together languages via features for which they
are not typologically similar. Consider English and
French, which are both prepositional and thus have
the same value for WALS property 85A. These lan-
guages will end up sharing a parameter for the feature
[d? l]?h.p = NOUN?m.p = ADJ?w.85A; yet they
have the exact opposite direction of attachment pref-
erence when it comes to nouns and adjectives. This
problem applies to any method for parameter mixing
that treats all the parameters as equal.
Like Naseem et al (2012), we instead share pa-
rameters more selectively. Our strategy is to use the
relevant part-of-speech tags of the head and depen-
dent to select which parameters to share, based on
very basic linguistic knowledge. The resulting fea-
tures are shown to the right in Figure 2. For example,
there is a shared directional feature that models the or-
der of Subject, Object and Verb by conjoining WALS
feature 81A with the arc direction and an indicator
feature that fires only if the head is a verb and the de-
pendent is a noun. These features would not be very
useful by themselves, so we combine them with the
Bare features. The accuracy of the resulting Share
model is shown in column five of Table 2. Although
this model still performs worse than NBG, it is an
improvement over the Delex baseline and actually
outperforms the former on 5 out of the 16 languages.
4.2 Sharing Based on Language Groups
While Share models selectional preferences and arc
directions for a subset of dependency relations, it
does not capture the rich local word order informa-
tion captured by Delex. We now consider two ways of
selectively including such information based on lan-
guage similarity. While more complex sharing could
be explored (Berg-Kirkpatrick and Klein, 2010), we
use a flat structure and consider two simple groupings
of the source and target languages.
First, the Similar model consists of the features
used by Share together with the features from Delex
in Figure 2. The latter are conjoined with an indicator
feature that fires only when the source and target
languages share values for all the WALS features in
Table 1. This is accomplished by adding the template
f? [w.81A? w.85A? w.86A? w.87A? w.88A]
for each template f in Delex. This groups: 1) Cata-
lan, Italian, Portuguese and Spanish; 2) Bulgarian,
Czech and English; 3) Dutch, German and Greek;
and 4) Japanese and Turkish. The remaining lan-
guages do not share all WALS properties with at
least one source language and thus revert to Share,
since they cannot exploit these grouped features.
Second, instead of grouping languages according
to WALS, the Family model is based on a simple
subdivision into Indo-European languages (Bulgar-
ian, Catalan, Czech, Greek, English, Spanish, Italian,
1065
Dutch, Portuguese, Swedish) and Altaic languages
(Japanese, Turkish). This is accomplished with in-
dicator features analogous to those used in Similar.
The remaining languages are again treated as isolates
and revert to Similar.
The results for these models are given in the last
two columns of Table 2. We see that by adding these
rich features back into the fold, but having them fire
only for languages in the same group, we can sig-
nificantly increase the performance ? from 57.4%
to 62.0% on average when considering Family. If
we consider our original Delex baseline, we see an
absolute improvement of 6.9% on average and a rela-
tive error reduction of 15%. Particular gains are seen
for non-Indo-European languages; e.g., Japanese in-
creases from 38.9% to 65.9%. Furthermore, Family
achieves a 7% relative error reduction over the NBG
baseline and outperforms it on 12 of the 16 languages.
This shows that a discriminative graph-based parser
can achieve higher accuracies compared to generative
models when the features are carefully constructed.
5 Target Language Adaptation
While some higher-level linguistic properties of the
target language have been incorporated through se-
lective sharing, so far no features specific to the target
language have been employed. Cohen et al (2011)
and Naseem et al (2012) have shown that using
expectation-maximization (EM) to this end can in
some cases bring substantial accuracy gains. For dis-
criminative models, self-training has been shown to
be quite effective for adapting monolingual parsers to
new domains (McClosky et al, 2006), as well as for
relexicalizing delexicalized parsers using unlabeled
target language data (Zeman and Resnik, 2008). Sim-
ilarly T?ckstr?m (2012) used self-training to adapt a
multi-source direct transfer named-entity recognizer
(T?ckstr?m et al, 2012) to different target languages,
?relexicalizing? the model with word cluster features.
However, as discussed in ?5.2, standard self-training
is not optimal for target language adaptation.
5.1 Ambiguity-Aware Training
In this section, we propose a related training method:
ambiguity-aware training. In this setting a discrimi-
native probabilistic model is induced from automat-
ically inferred ambiguous labelings over unlabeled
target language data, in place of gold-standard depen-
dency trees. The ambiguous labelings can combine
multiple sources of evidence to guide the estimation
or simply encode the underlying uncertainty from the
base parser. This uncertainty is marginalized out dur-
ing training. The structure of the output space, e.g.,
projectivity and single-headedness constraints, along
with regularities in the feature space, can together
guide the estimation, similar to what occurs with the
expectation-maximization algorithm.
Core to this method is the idea of an ambiguous
labeling y?(x) ? Y(x), which encodes a set of pos-
sible dependency trees for an input sentence x. In
subsequent sections we describe how to define such
labelings. Critically, y?(x) should be large enough to
capture the correct labeling, but on the other hand
small enough to provide concrete guidance for model
estimation. Ideally, y?(x) will capture heterogenous
knowledge that can aid the parser in target language
adaptation. In a first-order arc-factored model, we
define y?(x) in terms of a collection of ambiguous
arc setsA(x) = {A(x,m)}|x|m=1, whereA(x,m) de-
notes the set of ambiguously specified heads for the
mth token in x. Then, y?(x) is defined as the set of
all projective dependency trees spanning x that can
be assembled from the arcs in A(x).
Methods for learning with ambiguous labelings
have previously been proposed in the context of
multi-class classification (Jin and Ghahramani, 2002),
sequence-labeling (Dredze et al, 2009), log-linear
LFG parsing (Riezler et al, 2002), as well as for
discriminative reranking of generative constituency
parsers (Charniak and Johnson, 2005). In contrast to
Dredze et al, who allow for weights to be assigned
to partial labels, we assume that the ambiguous arcs
are weighted uniformly. For target language adapta-
tion, these weights would typically be derived from
unreliable sources and we do not want to train the
model to simply mimic their beliefs. Furthermore,
with this assumption, learning is simply achieved
by maximizing the marginal log-likelihood of the
ambiguous training set D? = {(xi, y?(xi)}ni=1,
L(?; D?) =
n?
i=1
log
?
?
?
?
y?y?(xi)
p?(y | xi)
?
?
?
? ? ???22 .
In maximizing the marginal log-likelihood, the model
is free to distribute probability mass among the trees
1066
in the ambiguous labeling to its liking, as long as the
marginal log-likelihood improves. The same objec-
tive function is used by Riezler et al (2002) and Char-
niak and Johnson (2005). A key difference is that in
these works, the ambiguity is constrained through a
supervised signal, while we use ambiguity as a way
to achieve self-training, using the base-parser itself,
or some other potentially noisy knowledge source as
the sole constraints. Note that we have introduced
an `2-regularizer, weighted by ?. This is important
as we are now training lexicalized target language
models which can easily overfit. In all experiments,
we optimize parameters with L-BFGS. Note also that
the marginal likelihood is non-concave, so that we
are only guaranteed to find a local maximum.
5.2 Ambiguity-Aware Self-Training
In standard self-training ? hereafter referred to as
Viterbi self-training ? a base parser is used to la-
bel each unlabeled sentence with its most probable
parse tree to create a self-labeled data set, which is
subsequently used to train a supervised parser. There
are two reasons why this simple approach may work.
First, if the base parser?s errors are not too systematic
and if the self-training model is not too expressive,
self-training can reduce the variance on the new do-
main. Second, self-training allows for features in the
new domain with low support ? or no support in the
case of lexicalized features ? in the base parser to
be ?filled in? by exploiting correlations in the feature
representation. However, a potential pitfall of this
approach is that the self-trained parser is encouraged
to blindly mimic the base parser, which leads to error
reinforcement. This may be particularly problematic
when relexicalizing a transfer parser, since the lexical
features provide the parser with increased power and
thereby an increased risk of overfitting to the noise.
To overcome this potential problem, we propose an
ambiguity-aware self-training (AAST) method that is
able to take the noise of the base parser into account.
We use the arc-marginals of the base parser to
construct the ambiguous labeling y?(x) for a sentence
x. For each token m ? [1, |x|], we first sort the set of
arcs in which m is the dependent, {(h,m)}|x|h=0, by
the marginal probabilities of the arcs:
p?(h,m | x) =
?
{y?Y(x) | (h,m)?y}
p?(y | x)
We next construct the ambiguous arc set A(x,m) by
adding arcs (h,m) in order of decreasing probability,
until their cumulative probability exceeds ?, i.e. until
?
(h,m)?A(x,m)
p?(h,m | x) ? ? .
Lower values of ? result in more aggressive pruning,
with ? = 0 corresponding to including no arc and
? = 1 corresponding to including all arcs. We always
add the highest scoring tree y? to y?(x) to ensure that
it contains at least one complete projective tree.
Figure 3 outlines an example of how (and why)
AAST works. In the Greek example, the genitive
phrase ? pi??????? ?????? (the stay of vessels) is
incorrectly analyzed as a flat noun phrase. This is not
surprising given that the base parser simply observes
this phrase as DET NOUN NOUN. However, looking
at the arc marginals we can see that the correct anal-
ysis is available during AAST, although the actual
marginal probabilities are quite misleading. Further-
more, the genitive noun ?????? also appears in other
less ambiguous contexts, where the base parser cor-
rectly predicts it to modify a noun and not a verb.
This allows the training process to add weight to the
corresponding lexical feature pairing ?????? with a
noun head and away from the feature pairing it with
a verb. The resulting parser correctly predicts the
genitive construction.
5.3 Ambiguity-Aware Ensemble-Training
While ambiguous labelings can be used as a means
to improve self-training, any information that can
be expressed as hard arc-factored constraints can be
incorporated, including linguistic expert knowledge
and annotation projected via bitext. Here we explore
another natural source of information: the predic-
tions of other transfer parsers. It is well known that
combining several diverse predictions in an ensem-
ble often leads to improved predictions. However, in
most ensemble methods there is typically no learning
involved once the base learners have been trained
(Sagae and Lavie, 2006). An exception is the method
of Sagae and Tsujii (2007), who combine the outputs
of many parsers on unlabeled data to train a parser
for a new domain. However, in that work the learner
is not exposed to the underlying ambiguity of the
base parsers; it is only given the Viterbi parse of the
combination system as the gold standard. In contrast,
1067
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
0.55
0.44
0.62
0.36
0.10
0.87
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
Figure 3: An example of ambiguity-aware self-training
(AAST) on a sentence from the Greek self-training data.
The sentence roughly translates to The stay of vessels
is permitted only for the day. Top: Arcs from the base
model?s Viterbi parse are shown above the sentence. When
only the part-of-speech tags are observed, the parser tends
to treat everything to the left of the verb as a head-final
noun phrase. The dashed arcs below the sentence are
the arcs for the true genitive construction stay of vessels.
These arcs and the corresponding incorrect arcs in the
Viterbi parse are marked with their marginal probabilities.
Middle: The ambiguous labeling y?(x), which is used
as supervision in AAST. Additional non-Viterbi arcs are
present in y?(x); for clarity, these are not shown. When
learning with AAST, probability mass will be pushed to-
wards any tree consistent with y?(x). Marginal probabili-
ties are ignored at this stage, so that all arcs in y?(x) are
treated as equals. Bottom: The Viterbi parse of the AAST
model, which has selected the correct arcs from y?(x).
we propose an ambiguity-aware ensemble-training
(AAET) method that treats the union of the ensemble
predictions for a sentence x as an ambiguous labeling
y?(x). An additional advantage of this approach is
that the ensemble is compiled into a single model
and therefore does not require multiple models to be
stored and used at runtime.
It is straightforward to construct y?(x) from multi-
ple parsers. Let Ak(x,m) be the set of arcs for the
mth token in x according to the kth parser in the en-
semble. When arc-marginals are used to construct the
ambiguity set, |Ak(x,m)| ? 1, but when the Viterbi-
parse is used, Ak(x,m) is a singleton. We next form
A(x,m) =
?
kAk(x,m) as the ensemble arc ambi-
guity set from which y?(x) is assembled. In this study,
we combine the arc sets of two base parsers: first, the
arc-marginal ambiguity set of the base parser (?5.2);
and second, the Viterbi arc set from the NBG parser
of Naseem et al (2012) in Table 2.4 Thus, the lat-
ter will have singleton arc ambiguity sets, but when
combined with the arc-marginal ambiguity sets of
our base parser, the result will encode uncertainty
derived from both parsers.
5.4 Adaptation Experiments
We now study the different approaches to target lan-
guage adaptation empirically. As in Naseem et al
(2012), we use the CoNLL training sets, stripped of
all dependency information, as the unlabeled target
language data in our experiments. We use the Family
model as the base parser, which is used to label the
unlabeled target data with the Viterbi parses as well
as with the ambiguous labelings. The final model
is then trained on this data using standard lexical-
ized features (McDonald et al, 2005). Since labeled
training data is unavailable in the target language,
we cannot tune any hyper-parameters and simply set
?= 1 and ?= 0.95 throughout. Although the latter
may suggest that y?(x) contains a high degree of am-
biguity, in reality, the marginal distributions of the
base model have low entropy and after filtering with
? = 0.95, the average number of potential heads per
dependent ranges from 1.4 to 3.2, depending on the
target language.
The ambiguity-aware training methods, that is
ambiguity-aware self-training (AAST) and ambiguity-
aware ensemble-training (AAET), are compared to
three baseline systems. First, NBG+EM is the gen-
erative model of Naseem et al (2012) trained with
expectation-maximization on additional unlabeled
target language text. Second, Family is the best dis-
criminative model from the previous section. Third,
Viterbi is the basic Viterbi self-training model. The
results of each of these models are shown in Table 3.
There are a number of things that can be observed.
First, Viterbi self-training helps slightly on average,
but the gains are not consistent and there are even
drops in accuracy for some languages. Second, AAST
outperforms the Viterbi variant on all languages and
4We do not have access to the marginals of NBG.
1068
Target Adaptation
Lang. NBG+EM Family Viterbi AAST AAET
ar 59.3 52.7 52.6 53.5 58.7
bg 67.0 65.4 66.4 67.9 73.0
ca 71.7 77.6 78.0 79.9 76.1
cs 44.3 43.5 43.6 44.4 48.3
de 54.1 59.2 59.7 62.5 61.5
el 67.9 63.2 64.5 65.5 69.6
es 62.0 67.1 68.2 68.5 66.9
eu 47.8 46.8 47.5 48.6 49.4
hu 58.6 64.5 64.6 65.6 67.5
it 65.6 72.5 71.6 72.4 73.4
ja 64.1 65.9 65.7 68.8 72.0
nl 56.6 56.8 57.9 58.1 60.2
pt 75.8 78.4 79.9 80.7 79.9
sv 61.7 63.5 63.4 65.5 65.5
tr 59.4 59.4 59.5 64.1 64.2
zh 51.0 54.8 54.8 57.9 60.7
avg 60.4 62.0 62.4 64.0 65.4
Table 3: Target language adaptation using unlabeled tar-
get data. AAST: ambiguity-aware self-training. AAET:
ambiguity-aware ensemble-training. Boldface numbers
indicate the best result per language. Underlined numbers
indicate the best result, excluding AAET. NBG+EM is the
?D+,To? model from Naseem et al (2012).
nearly always improves on the base parser, although
it sees a slight drop for Italian. AAST improves the
accuracy over the base model by 2% absolute on av-
erage and by as much as 5% absolute for Turkish.
Comparing this model to the NBG+EM baseline, we
observe an improvement by 3.6% absolute, outper-
forming it on 14 of the 16 languages. Furthermore,
ambiguity-aware self-training appears to help more
than expectation-maximization for generative (unlex-
icalized) models. Naseem et al observed an increase
from 59.3% to 60.4% on average by adding unlabeled
target language data and the gains were not consistent
across languages. AAST, on the other hand, achieves
consistent gains, rising from 62.0% to 64.0% on av-
erage. Third, as shown in the rightmost column of
Table 3, ambiguity-aware ensemble-training is indeed
a successful strategy; AAET outperforms the previ-
ous best self-trained model on 13 and NB&G+EM
on 15 out of 16 languages. The relative error reduc-
tion with respect to the base Family model is 9% on
average, while the average reduction with respect to
NBG+EM is 13%.
Before concluding, two additional points are worth
making. First, further gains may potentially be
achievable with feature-rich discriminative models.
While the best generative transfer model of Naseem
et al (2012) approaches its upper-bounding super-
vised accuracy (60.4% vs. 67.1%), our relaxed self-
training model is still far below its supervised coun-
terpart (64.0% vs. 84.1%). One promising statistic
along these lines is that the oracle accuracy for the
ambiguous labelings of AAST is 75.7%, averaged
across languages, which suggests that other training
algorithms, priors or constraints could improve the
accuracy substantially. Second, relexicalization is a
key component of self-training. If we use delexical-
ized features during self-training, we only observe a
small average improvement from 62.0% to 62.1%.
6 Conclusions
We contributed to the understanding of multi-source
syntactic transfer in several complementary ways.
First, we showed how selective parameter sharing,
based on typological features and language family
membership, can be incorporated in a discriminative
graph-based model of dependency parsing. We then
showed how ambiguous labelings can be used to in-
tegrate heterogenous knowledge sources in parser
training. Two instantiations of this framework were
explored. First, an ambiguity-aware self-training
method that can be used to effectively relexicalize
and adapt a delexicalized transfer parser using unla-
beled target language data. Second, an ambiguity-
aware ensemble-training method, in which predic-
tions from different parsers can be incorporated and
further adapted. On average, our best model provides
a relative error reduction of 13% over the state-of-
the-art model of Naseem et al (2012), outperforming
it on 15 out of 16 evaluated languages.
Acknowledgments We thank Alexander Rush for
help with the hypergraph framework used for inference.
Tahira Naseem kindly provided us with her data sets and
the predictions of her systems. This work benefited from
many discussions with Yoav Goldberg and members of the
Google parsing team. We finally thank the three anony-
mous reviewers for their valuable feedback. The work of
the first author was partly funded by the Swedish National
Graduate School of Language Technology (GSLT).
1069
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative reranking.
In Proceedings of ACL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Hal Daum? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of ACL.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multiple
labels. In Proceedings of the ECML/PKDD Workshop
on Learning from Multi-Label Data.
Matthew S. Dryer and Martin Haspelmath, editors. 2011.
The World Atlas of Language Structures Online. Mu-
nich: Max Planck Digital Library. http://wals.info/.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntac-
tic transfer using a bilingual lexicon. In Proceedings of
EMNLP-CoNLL.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: an exploration. In Proceedings of
COLING.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian domain adaptation. In Proceed-
ings of HLT-NAACL.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL-IJCNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Rong Jin and Zoubin Ghahramani. 2002. Learning with
multiple labels. In Proceedings of NIPS.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of dependency
and constituency. In Proceedings of ACL.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012. Inducing crosslingual distributed representations
of words. In Proceedings of COLING.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adaptation.
In Proceedings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of ACL.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, III, and Mark Johnson.
2002. Parsing the Wall Street Journal using a lexical-
functional grammar and discriminative estimation tech-
niques. In Proceedings of ACL.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of EMNLP-CoNLL.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2009. Adding more languages im-
proves unsupervised multilingual part-of-speech tag-
ging: A Bayesian non-parametric approach. In Pro-
ceedings of NAACL.
Anders S?gaard and Julie Wulff. 2012. An empirical
study of non-lexical extensions to delexicalized transfer.
In Proceedings of COLING.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of ACL.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using incomplete
and noisy training data. In Proceedings of CONLL.
Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Oscar T?ckstr?m. 2012. Nudging the envelope of direct
transfer methods for multilingual named entity recog-
nition. In Proceedings of the NAACL-HLT 2012 Work-
shop on Inducing Linguistic Structure (WILS 2012).
1070
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In IJC-
NLP Workshop: NLP for Less Privileged Languages.
1071
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 569?574,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-supervised latent variable models for sentence-level sentiment analysis
Oscar Ta?ckstro?m
SICS, Kista / Uppsala University, Uppsala
oscar@sics.se
Ryan McDonald
Google, Inc., New York
ryanmcd@google.com
Abstract
We derive two variants of a semi-supervised
model for fine-grained sentiment analysis.
Both models leverage abundant natural super-
vision in the form of review ratings, as well as
a small amount of manually crafted sentence
labels, to learn sentence-level sentiment clas-
sifiers. The proposed model is a fusion of a
fully supervised structured conditional model
and its partially supervised counterpart. This
allows for highly efficient estimation and infer-
ence algorithms with rich feature definitions.
We describe the two variants as well as their
component models and verify experimentally
that both variants give significantly improved
results for sentence-level sentiment analysis
compared to all baselines.
1 Sentence-level sentiment analysis
In this paper, we demonstrate how combining
coarse-grained and fine-grained supervision bene-
fits sentence-level sentiment analysis ? an important
task in the field of opinion classification and retrieval
(Pang and Lee, 2008). Typical supervised learning ap-
proaches to sentence-level sentiment analysis rely on
sentence-level supervision. While such fine-grained
supervision rarely exist naturally, and thus requires
labor intensive manual annotation effort (Wiebe et
al., 2005), coarse-grained supervision is naturally
abundant in the form of online review ratings. This
coarse-grained supervision is, of course, less infor-
mative compared to fine-grained supervision, how-
ever, by combining a small amount of sentence-level
supervision with a large amount of document-level
supervision, we are able to substantially improve on
the sentence-level classification task. Our work com-
bines two strands of research: models for sentiment
analysis that take document structure into account;
and models that use latent variables to learn unob-
served phenomena from that which can be observed.
Exploiting document structure for sentiment anal-
ysis has attracted research attention since the early
work of Pang and Lee (2004), who performed min-
imal cuts in a sentence graph to select subjective
sentences. McDonald et al (2007) later showed that
jointly learning fine-grained (sentence) and coarse-
grained (document) sentiment improves predictions
at both levels. More recently, Yessenalina et al
(2010) described how sentence-level latent variables
can be used to improve document-level prediction
and Nakagawa et al (2010) used latent variables over
syntactic dependency trees to improve sentence-level
prediction, using only labeled sentences for training.
In a similar vein, Sauper et al (2010) integrated gen-
erative content structure models with discriminative
models for multi-aspect sentiment summarization
and ranking. These approaches all rely on the avail-
ability of fine-grained annotations, but Ta?ckstro?m
and McDonald (2011) showed that latent variables
can be used to learn fine-grained sentiment using only
coarse-grained supervision. While this model was
shown to beat a set of natural baselines with quite a
wide margin, it has its shortcomings. Most notably,
due to the loose constraints provided by the coarse
supervision, it tends to only predict the two dominant
fine-grained sentiment categories well for each docu-
ment sentiment category, so that almost all sentences
in positive documents are deemed positive or neutral,
and vice versa for negative documents. As a way of
overcoming these shortcomings, we propose to fuse
a coarsely supervised model with a fully supervised
model.
Below, we describe two ways of achieving such
a combined model in the framework of structured
conditional latent variable models. Contrary to (gen-
erative) topic models (Mei et al, 2007; Titov and
569
a) yd
? ? ? ysi?1 y
s
i y
s
i+1 ? ? ?
? ? ? si?1 si si+1 ? ? ?
b) yd
? ? ? ysi?1 y
s
i y
s
i+1 ? ? ?
? ? ? si?1 si si+1 ? ? ?
Figure 1: a) Factor graph of the fully observed graphical model. b) Factor graph of the corresponding latent variable
model. During training, shaded nodes are observed, while non-shaded nodes are unobserved. The input sentences si are
always observed. Note that there are no factors connecting the document node, yd, with the input nodes, s, so that the
sentence-level variables, ys, in effect form a bottleneck between the document sentiment and the input sentences.
McDonald, 2008; Lin and He, 2009), structured con-
ditional models can handle rich and overlapping fea-
tures and allow for exact inference and simple gradi-
ent based estimation. The former models are largely
orthogonal to the one we propose in this work and
combining their merits might be fruitful. As shown
by Sauper et al (2010), it is possible to fuse gener-
ative document structure models and task specific
structured conditional models. While we do model
document structure in terms of sentiment transitions,
we do not model topical structure. An interesting
avenue for future work would be to extend the model
of Sauper et al (2010) to take coarse-grained task-
specific supervision into account, while modeling
fine-grained task-specific aspects with latent vari-
ables.
Note also that the proposed approach is orthogonal
to semi-supervised and unsupervised induction of
context independent (prior polarity) lexicons (Turney,
2002; Kim and Hovy, 2004; Esuli and Sebastiani,
2009; Rao and Ravichandran, 2009; Velikovich et al,
2010). The output of such models could readily be
incorporated as features in the proposed model.
1.1 Preliminaries
Let d be a document consisting of n sentences, s =
(si)ni=1, with a document?sentence-sequence pair de-
noted d = (d, s). Let yd = (yd,ys) denote random
variables1 ? the document level sentiment, yd, and the
sequence of sentence level sentiment, ys = (ysi )
n
i=1.
1We are abusing notation throughout by using the same sym-
bols to refer to random variables and their particular assignments.
In what follows, we assume that we have access to
two training sets: a small set of fully labeled in-
stances, DF = {(dj ,ydj )}
mf
j=1, and a large set of
coarsely labeled instances DC = {(dj , ydj )}
mf+mc
j=mf+1
.
Furthermore, we assume that yd and all ysi take val-
ues in {POS, NEG, NEU}.
We focus on structured conditional models in the
exponential family, with the standard parametrization
p?(y
d,ys|s) = exp
{
??(yd,ys, s), ?? ?A?(s)
}
,
where ? ? <n is a parameter vector, ?(?) ? <n is a
vector valued feature function that factors according
to the graph structure outlined in Figure 1, and A?
is the log-partition function. This class of models is
known as conditional random fields (CRFs) (Lafferty
et al, 2001), when all variables are observed, and as
hidden conditional random fields (HCRFs) (Quattoni
et al, 2007), when only a subset of the variables are
observed.
1.2 The fully supervised fine-to-coarse model
McDonald et al (2007) introduced a fully super-
vised model in which predictions of coarse-grained
(document) and fine-grained (sentence) sentiment are
learned and inferred jointly. They showed that learn-
ing both levels jointly improved performance at both
levels, compared to learning each level individually,
as well as to using a cascaded model in which the
predictions at one level are used as input to the other.
Figure 1a outlines the factor graph of the corre-
570
sponding conditional random field.2 The parameters,
?F , of this model can be estimated from the set of
fully labeled data, DF , by maximizing the joint con-
ditional likelihood function
LF (?F ) =
mf?
j=1
log p?F (y
d
j ,y
s
j |sj)?
??F ?
2
2?2F
,
where ?2F is the variance of the Normal(0, ?
2
F ) prior.
Note that LF is a concave function and consequently
its unique maximum can be found by gradient based
optimization techniques.
1.3 Latent variables for coarse supervision
Recently, Ta?ckstro?m and McDonald (2011) showed
that fine-grained sentiment can be learned from
coarse-grained supervision alone. Specifically, they
used a HCRF model with the same structure as that
in Figure 1a, but with sentence labels treated as la-
tent variables. The factor graph corresponding to this
model is outlined in Figure 1b.
The fully supervised model might benefit from fac-
tors that directly connect the document variable, yd,
with the inputs s. However, as argued by Ta?ckstro?m
and McDonald (2011), when only document-level
supervision is available, the document variable, yd,
should be independent of the input, s, conditioned
on the latent variables, ys. This prohibits the model
from bypassing the latent variables, which is crucial,
since we seek to improve the sentence-level predic-
tions, rather than the document-level predictions.
The parameters, ?C , of this model can be esti-
mated from the set of coarsely labeled data, DC , by
maximizing the marginalized conditional likelihood
function
LC(?C) =
mf+mc?
j=mf+1
log
?
ys
p?C (y
d
j ,y
s|sj)?
??C?
2
2?2C
,
where the marginalization is over all possible se-
quences of latent sentence label assignments ys.
Due to the introduction of latent variables, the
marginal likelihood function is non-concave and thus
there are no guarantees of global optimality, how-
ever, we can still use a gradient based optimization
technique to find a local maximum.
2Figure 1a differs slightly from the model employed by Mc-
Donald et al (2007), where they had factors connecting the
document label yd with each input si as well.
2 Combining coarse and full supervision
The fully supervised and the partially supervised
models both have their merits. The former requires
an expensive and laborious process of manual an-
notation, while the latter can be used with readily
available document labels, such as review star rat-
ings. The latter, however, has its shortcomings in
that the coarse-grained sentiment signal is less infor-
mative compared to a fine-grained signal. Thus, in
order to get the best of both worlds, we would like to
combine the merits of both of these models.
2.1 A cascaded model
A straightforward way of fusing the two models is
by means of a cascaded model in which the predic-
tions of the partially supervised model, trained by
maximizing LC(?C) are used to derive additional
features for the fully supervised model, trained by
maximizing LF (?F ).
Although more complex representations are pos-
sible, we generate meta-features for each sentence
based solely on operations on the estimated distribu-
tions, p?C (y
d, ysi |s). Specifically, we encode the fol-
lowing probability distributions as discrete features
by uniform bucketing, with bucket width 0.1: the
joint distribution, p?C (y
d, ysi |s); the marginal docu-
ment distribution, p?C (y
d|s); and the marginal sen-
tence distribution, p?C (y
s
i |s). We also encode the
argmax of these distributions, as well as the pair-
wise combinations of the derived features.
The upshot of this cascaded approach is that it is
very simple to implement and efficient to train. The
downside is that only the partially supervised model
influences the fully supervised model; there is no
reciprocal influence between the models. Given the
non-concavity of LC(?C), such influence could be
beneficial.
2.2 Interpolating likelihood functions
A more flexible way of fusing the two models is to
interpolate their likelihood functions, thereby allow-
ing for both coarse and joint supervision of the same
model. Such a combination can be achieved by con-
straining the parameters so that ?I = ?F = ?C and
taking the mean of the likelihood functions LF and
LC , appropriately weighted by a hyper-parameter ?.
571
The result is the interpolated likelihood function
LI(?I) = ?LF (?I) + (1? ?)LC(?I) .
A simple, yet efficient, way of optimizing this ob-
jective function is to use stochastic gradient ascent
with learning rate ?. At each step we select a fully
labeled instance, (dj ,ydj ) ? DF , with probability ?
and a coarsely labeled instance, (dj , ydj ) ? DC , with
probability (1? ?). We then update the parameters,
?I , according to the gradients ?LF and ?LC , respec-
tively. In principle we could use different learning
rates ?F and ?C as well as different prior variances
?2F and ?
2
C , but in what follows we set them equal.
Since we are interpolating conditional models, we
need at least partial observations of each instance.
Methods for blending discriminative and generative
models (Lasserre et al, 2006; Suzuki et al, 2007;
Agarwal and Daume?, 2009; Sauper et al, 2010),
would enable incorporation of completely unlabeled
data as well. It is straightforward to extend the pro-
posed model along these lines, however, in practice
coarsely labeled sentiment data is so abundant on
the web (e.g., rated consumer reviews) that incorpo-
rating completely unlabeled data seems superfluous.
Furthermore, using conditional models with shared
parameters throughout allows for rich overlapping
features, while maintaining simple and efficient in-
ference and estimation.
3 Experiments
For the following experiments, we used the same data
set and a comparable experimental setup to that of
Ta?ckstro?m and McDonald (2011).3 We compare the
two proposed hybrid models (Cascaded and Interpo-
lated) to the fully supervised model of McDonald et
al. (2007) (FineToCoarse) as well as to the soft vari-
ant of the coarsely supervised model of Ta?ckstro?m
and McDonald (2011) (Coarse).
The learning rate was fixed to ? = 0.001, while
we tuned the prior variances, ?2, and the number of
epochs for each model. When sampling according to
? during optimization of LI(?I), we cycle through
DF and DC deterministically, but shuffle these sets
between epochs. Due to time constraints, we fixed the
interpolation factor to ? = 0.1, but tuning this could
3The annotated test data can be downloaded from
http://www.sics.se/people/oscar/datasets.
potentially improve the results of the interpolated
model. For the same reason we allowed a maximum
of 30 epochs, for all models, while Ta?ckstro?m and
McDonald (2011) report a maximum of 75 epochs.
To assess the impact of fully labeled versus
coarsely labeled data, we took stratified samples with-
out replacement, of sizes 60, 120, and 240 reviews,
from the fully labeled folds and of sizes 15,000 and
143,580 reviews from the coarsely labeled data. On
average each review consists of ten sentences. We
performed 5-fold stratified cross-validation over the
labeled data, while using stratified samples for the
coarsely labeled data. Statistical significance was as-
sessed by a hierachical bootstrap of 95% confidence
intervals, using the technique described by Davison
and Hinkley (1997).
3.1 Results and analysis
Table 1 lists sentence-level accuracy along with 95%
confidence interval for all tested models. We first
note that the interpolated model dominates all other
models in terms of accuracy. While the cascaded
model requires both large amounts of fully labeled
and coarsely labeled data, the interpolated model
is able to take advantage of both types of data on
its own and jointly. Still, by comparing the fully
supervised and the coarsely supervised models, the
superior impact of fully labeled over coarsely labeled
data is evident. As can be seen in Figure 2, when
all data is used, the cascaded model outperforms the
interpolated model for some recall values, and vice
versa, while both models dominate the supervised
approach for the full range of recall values.
As discussed earlier, and confirmed by Table 2,
the coarse-grained model only performs well on the
predominant sentence-level categories for each docu-
ment category. The supervised model handles nega-
tive and neutral sentences well, but performs poorly
on positive sentences even in positive documents.
The interpolated model, while still better at capturing
the predominant category, does a better job overall.
These results are with a maximum of 30 training
iterations. Preliminary experiments with a maximum
of 75 iterations indicate that all models gain from
more iterations; this seems to be especially true for
the supervised model and for the cascaded model
with less amount of course-grained data.
572
|DC | = 15,000 |DC | = 143,580
|DF | = 60 |DF | = 120 |DF | = 240 |DF | = 60 |DF | = 120 |DF | = 240
FineToCoarse 49.3 (-1.3, 1.4) 53.4 (-1.8, 1.7) 54.6 (-3.6, 3.8) 49.3 (-1.3, 1.4) 53.4 (-1.8, 1.7) 54.6 (-3.6, 3.8)
Coarse 49.6 (-1.5, 1.8) 49.6 (-1.5, 1.8) 49.6 (-1.5, 1.8) 53.5 (-1.2, 1.4) 53.5 (-1.2, 1.4) 53.5 (-1.2, 1.4)
Cascaded 39.7 (-6.8, 5.7) 45.4 (-3.1, 2.9) 42.6 (-6.5, 6.5) 55.6 (-2.9, 2.7) 55.0 (-3.2, 3.4) 56.8 (-3.8, 3.6)
Interpolated 54.3 (-1.4, 1.4) 55.0 (-1.7, 1.6) 57.5 (-4.1, 5.2) 56.0 (-2.4, 2.1) 54.5 (-2.9, 2.8) 59.1 (-2.8, 3.4)
Table 1: Sentence level results for varying numbers of fully labeled (DF ) and coarsely labeled (DC) reviews. Bold:
significantly better than the FineToCoarse model according to a hierarchical bootstrapped confidence interval, p < 0.05.
0 10 20 30 40 50 60 70 80 90 100
Recall
0
10
20
30
40
50
60
70
80
90
100
Pr
ec
is
io
n
POS sentences
FineToCoarse
Cascaded
Interpolated
0 10 20 30 40 50 60 70 80 90 100
Recall
0
10
20
30
40
50
60
70
80
90
100
Pr
ec
is
io
n
NEG sentences
FineToCoarse
Cascaded
Interpolated
Figure 2: Interpolated POS / NEG sentence-level precision-recall curves with |DC | = 143,580 and |DF | = 240.
POS docs. NEG docs. NEU docs.
FineToCoarse 35 / 11 / 59 33 / 76 / 42 29 / 63 / 55
Coarse 70 / 14 / 43 11 / 71 / 34 43 / 47 / 53
Cascaded 43 / 17 / 61 0 / 75 / 49 10 / 64 / 50
Interpolated 73 / 16 / 51 42 / 72 / 48 54 / 52 / 57
Table 2: POS / NEG / NEU sentence-level F1-scores per
document category (|DC | = 143,580 and |DF | = 240).
4 Conclusions
Learning fine-grained classification tasks in a fully su-
pervised manner does not scale well due to the lack of
naturally occurring supervision. We instead proposed
to combine coarse-grained supervision, which is natu-
rally abundant but less informative, with fine-grained
supervision, which is scarce but more informative.
To this end, we introduced two simple, yet effective,
methods of combining fully labeled and coarsely
labeled data for sentence-level sentiment analysis.
First, a cascaded approach where a coarsely super-
vised model is used to generate features for a fully
supervised model. Second, an interpolated model
that directly optimizes a combination of joint and
marginal likelihood functions. Both proposed mod-
els are structured conditional models that allow for
rich overlapping features, while maintaining highly
efficient exact inference and robust estimation prop-
erties. Empirically, the interpolated model is superior
to the other investigated models, but with sufficient
amounts of coarsely labeled and fully labeled data,
the cascaded approach is competitive.
Acknowledgments
The first author acknowledges the support of the
Swedish National Graduate School of Language
Technology (GSLT). The authors would also like to
thank Fernando Pereira and Bob Carpenter for early
discussions on using HCRFs in sentiment analysis.
573
References
Arvind Agarwal and Hal Daume?. 2009. Exponential
family hybrid semi-supervised learning. In Proceed-
ings of the International Jont conference on Artifical
Intelligence (IJCAI).
Anthony C. Davison and David V. Hinkley. 1997. Boot-
strap Methods and Their Applications. Cambridge Se-
ries in Statistical and Probabilistic Mathematics. Cam-
bridge University Press, Cambridge, UK.
Andrea Esuli and Fabrizio Sebastiani. 2009. SentiWord-
Net: A publicly available lexical resource for opinion
mining. In Proceedings of the Language Resource and
Evaluation Conference (LREC).
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning (ICML).
Julia A. Lasserre, Christopher M. Bishop, and Thomas P.
Minka. 2006. Principled hybrids of generative and
discriminative models. In Proceedings of the IEEE
Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR).
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In Proceeding of the Con-
ference on Information and Knowledge Management
(CIKM).
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the Annual Conference of the Association for Computa-
tional Linguistics (ACL).
Q. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the International
Conference on World Wide Web (WWW).
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency Tree-based Sentiment Classification
using CRFs with Hidden Variables. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Associ-
ation for Computational Linguistics (ACL).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Publishers.
Ariadna Quattoni, Sybor Wang, Louis-Philippe Morency,
Michael Collins, and Trevor Darrell. 2007. Hidden
conditional random fields. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text analy-
sis applications. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structured output learning based on
a hybrid generative and discriminative approach. In
Porceedings of the Conference on Emipirical Methods
in Natural Language Processing (EMNLP).
Oscar Ta?ckstro?m and Ryan McDonald. 2011. Discov-
ering fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the Euro-
pean Conference on Information Retrieval (ECIR).
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of the Annual World Wide Web Conference (WWW).
Peter Turney. 2002. Thumbs up or thumbs down? Senti-
ment orientation applied to unsupervised classification
of reviews. In Proceedings of the Annual Conference of
the Association for Computational Linguistics (ACL).
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of
web-derived polarity lexicons. In Proceedings of the
North American Chapter of the Association for Compu-
tational Linguistics (NAACL).
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. In Language Resources and Evaluation
(LREC).
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level senti-
ment classification. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP).
574
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 238?242,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Using Search-Logs to Improve Query Tagging
Kuzman Ganchev Keith Hall Ryan McDonald Slav Petrov
Google, Inc.
{kuzman|kbhall|ryanmcd|slav}@google.com
Abstract
Syntactic analysis of search queries is im-
portant for a variety of information-retrieval
tasks; however, the lack of annotated data
makes training query analysis models diffi-
cult. We propose a simple, efficient proce-
dure in which part-of-speech tags are trans-
ferred from retrieval-result snippets to queries
at training time. Unlike previous work, our
final model does not require any additional re-
sources at run-time. Compared to a state-of-
the-art approach, we achieve more than 20%
relative error reduction. Additionally, we an-
notate a corpus of search queries with part-
of-speech tags, providing a resource for future
work on syntactic query analysis.
1 Introduction
Syntactic analysis of search queries is important for
a variety of tasks including better query refinement,
improved matching and better ad targeting (Barr
et al, 2008). However, search queries differ sub-
stantially from traditional forms of written language
(e.g., no capitalization, few function words, fairly
free word order, etc.), and are therefore difficult
to process with natural language processing tools
trained on standard corpora (Barr et al, 2008). In
this paper we focus on part-of-speech (POS) tagging
queries entered into commercial search engines and
compare different strategies for learning from search
logs. The search logs consist of user queries and
relevant search results retrieved by a search engine.
We use a supervised POS tagger to label the result
snippets and then transfer the tags to the queries,
producing a set of noisy labeled queries. These la-
beled queries are then added to the training data and
the tagger is retrained. We evaluate different strate-
gies for selecting which annotation to transfer and
find that using the result that was clicked by the user
gives comparable performance to using just the top
result or to aggregating over the top-k results.
The most closely related previous work is that of
Bendersky et al (2010, 2011). In their work, un-
igram POS tag priors generated from a large cor-
pus are blended with information from the top-50
results from a search engine at prediction time. Such
an approach has the disadvantage that it necessitates
access to a search engine at run-time and is com-
putationally very expensive. We re-implement their
method and show that our direct transfer approach is
more effective, while being simpler to instrument:
since we use information from the search engine
only during training, we can train a stand-alone POS
tagger that can be run without access to additional
resources. We also perform an error analysis and
find that most of the remaining errors are due to er-
rors in POS tagging of the snippets.
2 Direct Transfer
The main intuition behind our work, Bendersky et
al. (2010) and Ru?d et al (2011), is that standard NLP
annotation tools work better on snippets returned by
a search engine than on user supplied queries. This
is because snippets are typically well-formed En-
glish sentences, while queries are not. Our goal is to
leverage this observation and use a supervised POS
tagger trained on regular English sentences to gen-
erate annotations for a large set of queries that can
be used for training a query-specific model. Perhaps
the simplest approach ? but also a surprisingly pow-
erful one ? is to POS tag some relevant snippets for
238
a given query, and then to transfer the tags from the
snippet tokens to matching query tokens. This ?di-
rect? transfer idea is at the core of all our experi-
ments. In this work, we provide a comparison of
techniques for selecting snippets associated with the
query, as well as an evaluation of methods for align-
ing the matching words in the query to those in the
selected snippets.
Specifically, for each query1 with a corresponding
set of ?relevant snippets,? we first apply the baseline
tagger to the query and all the snippets. We match
any query terms in these snippets, and copy over the
POS tag to the matching query term. Note that this
can produce multiple labelings as the relevant snip-
pet set can be very diverse and varies even for the
same query. We choose the most frequent tagging
as the canonical one and add it to our training set.
We then train a query tagger on all our training data:
the original human annotated English sentences and
also the automatically generated query training set.
The simplest way to match query tokens to snip-
pet tokens is to allow a query token to match any
snippet token. This can be problematic when we
have queries that have a token repeated with differ-
ent parts-of-speech such as in ?tie a tie.? To make a
more precise matching we try a sequence of match-
ing rules: First, exact match of the query n-gram.
Then matching the terms in order, so the query ?tiea
a tieb? matched to the snippet ?to tie1 a neck tie2?
would match tiea:tie1 and tieb:tie2. Finally, we
match as many query terms as possible. An early
observation showed that when a query term occurs
in the result URL, e.g., searching for ?irs mileage
rate? results in the page irs.gov, the query term
matching the URL domain name is usually a proper
noun. Consequently we add this rule.
In the context of search logs, a relevant snippet
set can refer to the top k snippets (including the case
where k = 1) or the snippet(s) associated with re-
sults clicked by users that issued the query. In our
experiments we found that different strategies for se-
lecting relevant snippets, such as selecting the snip-
pets of the clicked results, using the top-10 results
or using only the top result, perform similarly (see
Table 1).
1We skip navigational queries, e.g, amazon or amazon.com,
since syntactic analysis of such queries is not useful.
Query budget/NN rent/VB a/DET car/NN Clicks
Snip 1 . . . Budget/NNP Rent/NNP 2
A/NNP Car/NNP . . .
Snip 2 . . . Go/VB to/TO Budget/NNP 1
to/TO rent/VB a/DET car/NN . . .
Snip 3 . . . Rent/VB a/DET car/NN 1
from/IN Budget/NNP . . .
Figure 1: Example query and snippets as tagged by a
baseline tagger as well as associated clicks.
By contrast Bendersky et al (2010) use a lin-
ear interpolation between a prior probability and the
snippet tagging. They define pi(t|w) as the relative
frequency of tag t given by the baseline tagger to
word w in some corpus and ?(t|w, s) as the indica-
tor function for word w in the context of snippet s
has tag t. They define the tagging of a word as
argmax
t
0.2pi(t|w) + 0.8mean
s:w?s
?(t|w, s) (1)
We illustrate the difference between the two ap-
proaches in Figure 1. The numbered rows of the
table correspond to three snippets (with non-query
terms elided). The strategy that uses the clicks to se-
lect the tagging would count two examples of ?Bud-
get/NNP Rent/NNP A/NNP Car/NNP? and one for
each of two other taggings. Note that snippet 1
and the query get different taggings primarily due
to orthographic variations. It would then add ?bud-
get/NNP rent/NNP a/NNP car/NNP? to its training
set. The interpolation approach of Bendersky et al
(2010) would tag the query as ?budget/NNP rent/VB
a/DET car/NN?. To see why this is the case, consider
the probability for rent/VB vs rent/NNP. For rent/VB
we have 0.2 + 0.8? 23 , while for rent/NNP we have
0 + 0.8? 13 assuming that pi(VB|rent) = 1.
3 Experimental Setup
We assume that we have access to labeled English
sentences from the PennTreebank (Marcus et al,
1993) and the QuestionBank (Judge et al, 2006), as
well as large amounts of unlabeled search queries.
Each query is paired with a set of relevant results
represented by snippets (sentence fragments con-
taining the search terms), as well as information
about the order in which the results were shown to
the user and possibly the result the user clicked on.
Note that different sets of results are possible for the
239
same query, because of personalization and ranking
changes over time.
3.1 Evaluation Data
We use two data sets for evaluation. The first is the
set of 251 queries from Microsoft search logs (MS-
251) used in Bendersky et al (2010, 2011). The
queries are annotated with three POS tags represent-
ing nouns, verbs and ?other? tags (MS-251 NVX).
We additionally refine the annotation to cover 14
POS tags comprising the 12 universal tags of Petrov
et al (2012), as well as proper nouns and a special
tag for search operator symbols such as ?-? (for
excluding the subsequent word). We refer to this
evaluation set as MS-251 in our experiments. We
had two annotators annotate the whole of the MS-
251 data set. Before arbitration, the inter-annotator
agreement was 90.2%. As a reference, Barr et al
(2008) report 79.3% when annotating queries with
19 POS tags. We then examined all the instances
where the annotators disagreed, and corrected
the discrepancy. Our annotations are available at
http://code.google.com/p/query-syntax/.
The second evaluation set consists of 500 so
called ?long-tail? queries. These are queries that oc-
curred rarely in the search logs, and are typically
difficult to tag because they are searching for less-
frequent information. They do not contain naviga-
tional queries.
3.2 Baseline Model
We use a linear chain tagger trained with the aver-
aged perceptron (Collins, 2002). We use the follow-
ing features for our tagger: current word, suffixes
and prefixes of length 1 to 3; additionally we use
word cluster features (Uszkoreit and Brants, 2008)
for the current word, and transition features of the
cluster of the current and previous word. When
training on Sections 1-18 of the Penn Treebank
and testing on sections 22-24, our tagger achieves
97.22% accuracy with the Penn Treebank tag set,
which is state-of-the-art for this data set. When we
evaluate only on the 14 tags used in our experiments,
the accuracy increases to 97.88%.
We experimented with 4 baseline taggers (see Ta-
ble 2). WSJ corresponds to training on only the
standard training sections of Wall Street Journal por-
tion of the Penn Treebank. WSJ+QTB adds the
Method
MS-251
NVX
MS-251 long-tail
DIRECT-CLICK 93.43 84.11 78.15
DIRECT-ALL 93.93 84.39 77.73
DIRECT-TOP-1 93.93 84.60 77.60
Table 1: Evaluation of snippet selection strategies.
QuestionBank as training data. WSJ NOCASE and
WSJ+QTB NOCASE use case-insensitive version of
the tagger (conceptually lowercasing the text before
training and before applying the tagger). As we will
see, all our baseline models are better than the base-
line reported in Bendersky et al (2010); our lower-
cased baseline model significantly outperforms even
their best model.
4 Experiments
First, we compared different strategies for selecting
relevant snippets from which to transfer the tags.
These systems are: DIRECT-CLICK, which uses
snippets clicked on by users; DIRECT-ALL, which
uses all the returned snippets seen by the user;2
and DIRECT-TOP-1, which uses just the snippet in
the top result. Table 1 compares these systems on
our three evaluation sets. While DIRECT-ALL and
DIRECT-TOP-1 perform best on the MS-251 data
sets, DIRECT-CLICK has an advantage on the long
tail queries. However, these differences are small
(<0.6%) suggesting that any strategy for selecting
relevant snippet sets will return comparable results
when aggregated over large amounts of data.
We then compared our method to the baseline
models and a re-implementation of Bendersky et al
(2010), which we denote BSC. We use the same
matching scheme for both BSC and our system, in-
cluding the URL matching described in Section 2.
The URL matching improves performance by 0.4-
3.0% across all models and evaluation settings.
Table 2 summarizes our final results. For com-
parison, Bendersky et al (2010) report 91.6% for
their final system, which is comparable to our im-
plementation of their system when the baseline tag-
ger is trained on just the WSJ corpus. Our best sys-
tem achieves a 21.2% relative reduction in error on
their annotations. Some other trends become appar-
2Usually 10 results, but more if the user viewed the second
page of results.
240
Method
MS-251
NVX
MS-251 long-tail
WSJ 90.54 75.07 53.06
BSC 91.74 77.82 57.65
DIRECT-CLICK 93.36 85.81 76.13
WSJ + QTB 90.18 74.86 53.48
BSC 91.74 77.54 57.65
DIRECT-CLICK 93.01 85.03 76.97
WSJ NOCASE 92.87 81.92 74.31
BSC 93.71 84.32 76.63
DIRECT-CLICK 93.50 84.46 77.48
WSJ + QTB NOCASE 93.08 82.70 74.65
BSC 93.57 83.90 77.27
DIRECT-CLICK 93.43 84.11 78.15
Table 2: Tagging accuracies for different baseline settings
and two transfer methods.DIRECT-CLICK is the approach
we propose (see text). Column MS-251 NVX evaluates
with tags from Bendersky et al (2010). Their baseline
is 89.3% and they report 91.6% for their method. MS-
251 and Long-tail use tags from Section 3.1. We observe
snippets for 2/500 long-tail queries and 31/251 MS-251
queries.
ent in Table 2. Firstly, a large part of the benefit of
transfer has to do with case information that is avail-
able in the snippets but is missing in the query. The
uncased tagger is insensitive to this mismatch and
achieves significantly better results than the cased
taggers. However, transferring information from the
snippets provides additional benefits, significantly
improving even the uncased baseline taggers. This
is consistent with the analysis in Barr et al (2008).
Finally, we see that the direct transfer method from
Section 2 significantly outperforms the method de-
scribed in Bendersky et al (2010). Table 3 confirms
this trend when focusing on proper nouns, which are
particularly difficult to identify in queries.
We also manually examined a set of 40 queries
with their associated snippets, for which our best
DIRECT-CLICK system made mistakes. In 32 cases,
the errors in the query tagging could be traced back
to errors in the snippet tagging. A better snippet
tagger could alleviate that problem. In the remain-
ing 8 cases there were problems with the matching
? either the mis-tagged word was not found at all,
or it was matched incorrectly. For example one of
the results for the query ?bell helmet? had a snippet
containing ?Bell cycling helmets? and we failed to
match helmet to helmets.
Method P R F
WSJ + QTB NOCASE 72.12 79.80 75.77
BSC 82.87 69.05 75.33
BSC + URL 83.01 70.80 76.42
DIRECT-CLICK 79.57 76.51 78.01
DIRECT-ALL 75.88 78.38 77.11
DIRECT-TOP-1 78.38 76.40 77.38
Table 3: Precision and recall of the NNP tag on the long-
tail data for the best baseline method and the three trans-
fer methods using that baseline.
5 Related Work
Barr et al (2008) manually annotate a corpus of
2722 queries with 19 POS tags and use it to train
and evaluate POS taggers, and also describe the lin-
guistic structures they find. Unfortunately their data
is not available so we cannot use it to compare to
their results. Ru?d et al (2011) create features based
on search engine results, that they use in an NER
system applied to queries. They report report sig-
nificant improvements when incorporating features
from the snippets. In particular, they exploit capital-
ization and query terms matching URL components;
both of which we have used in this work. Li et al
(2009) use clicks in a product data base to train a tag-
ger for product queries, but they do not use snippets
and do not annotate syntax. Li (2010) and Manshadi
and Li (2009) also work on adding tags to queries,
but do not use snippets or search logs as a source of
information.
6 Conclusions
We described a simple method for training a search-
query POS tagger from search-logs by transfer-
ring context from relevant snippet sets to query
terms. We compared our approach to previous work,
achieving an error reduction of 20%. In contrast to
the approach proposed by Bendersky et al (2010),
our approach does not require access to the search
engine or index when tagging a new query. By ex-
plicitly re-training our final model, it has the ability
to pool knowledge from several related queries and
incorporate the information into the model param-
eters. An area for future work is to transfer other
syntactic information, such as parse structures or su-
pertags using a similar transfer approach.
241
References
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search queries.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1021?1030, Honolulu, Hawaii, October. Association
for Computational Linguistics.
M. Bendersky, W.B. Croft, and D.A. Smith. 2010.
Structural annotation of search queries using pseudo-
relevance feedback. In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, pages 1537?1540. ACM.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 497?504, Sydney, Australia, July.
Association for Computational Linguistics.
X. Li, Y.Y. Wang, and A. Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 572?579. ACM.
X. Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1337?1345. Association for Com-
putational Linguistics.
M. Manshadi and X. Li. 2009. Semantic tagging of web
search queries. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2, pages
861?869. Association for Computational Linguistics.
M. P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn treebank. Computational Linguis-
tics, 19.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC.
Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, and
Hinrich Schu?tze. 2011. Piggyback: Using search en-
gines for robust cross-domain named entity recogni-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 965?975, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
J. Uszkoreit and T. Brants. 2008. Distributed word clus-
tering for large scale class-based language modeling in
machine translation. In Proc. of ACL.
242
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 656?661,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Enforcing Structural Diversity in Cube-pruned Dependency Parsing
Hao Zhang Ryan McDonald
Google, Inc.
{haozhang,ryanmcd}@google.com
Abstract
In this paper we extend the cube-pruned
dependency parsing framework of Zhang
et al (2012; 2013) by forcing inference to
maintain both label and structural ambigu-
ity. The resulting parser achieves state-of-
the-art accuracies, in particular on datasets
with a large set of dependency labels.
1 Introduction
Dependency parsers assign a syntactic depen-
dency tree to an input sentence (K?ubler et al,
2009), as exemplified in Figure 1. Graph-based
dependency parsers parameterize models directly
over substructures of the tree, including single
arcs (McDonald et al, 2005), sibling or grand-
parent arcs (McDonald and Pereira, 2006; Car-
reras, 2007) or higher-order substructures (Koo
and Collins, 2010; Ma and Zhao, 2012). As the
scope of each feature function increases so does
parsing complexity, e.g., o(n
5
) for fourth-order
dependency parsing (Ma and Zhao, 2012). This
has led to work on approximate inference, typ-
ically via pruning (Bergsma and Cherry, 2010;
Rush and Petrov, 2012; He et al, 2013)
Recently, it has been shown that cube-pruning
(Chiang, 2007) can efficiently introduce higher-
order dependencies in graph-based parsing (Zhang
and McDonald, 2012). Cube-pruned dependency
parsing runs standard bottom-up chart parsing us-
ing the lower-order algorithms. Similar to k-best
inference, each chart cell maintains a beam of k-
best partial dependency structures. Higher-order
features are scored when combining beams during
inference. Cube-pruning is an approximation, as
the highest scoring tree may fall out of the beam
before being fully scored with higher-order fea-
tures. However, Zhang et al (2013) observe state-
of-the-art results when training accounts for errors
that arise due to such approximations.
John emailed April about one month ago
NSUBJ
IOBJ
ADVMOD
Q
U
A
N
T
M
O
D
N
U
M
N
P
A
D
V
M
O
D
Figure 1: A sample dependency parse.
In this work we extend the cube-pruning frame-
work of Zhang et al by observing that dependency
parsing has two fundamental sources of ambiguity.
The first, structural ambiguity, pertains to confu-
sions about the unlabeled structure of the tree, e.g.,
the classic prepositional phrase attachment prob-
lem. The second, label ambiguity, pertains to sim-
ple label confusions, e.g., whether a verbal object
is direct or indirect.
Distinctions between arc labels are frequently
fine-grained and easily confused by parsing mod-
els. For example, in the Stanford dependency
label set (De Marneffe et al, 2006), the labels
TMOD (temporal modifier), NPADVMOD (noun-
phrase adverbial modifier), IOBJ (indirect object)
and DOBJ (direct object) can all be noun phrases
that modify verbs to their right. In the context of
cube-pruning, during inference, the system opts to
maintain a large amount of label ambiguity at the
expense of structural ambiguity. Frequently, the
beam stores only label ambiguities and the result-
ing set of trees have identical unlabeled structure.
For example, in Figure 1, the aforementioned la-
bel ambiguity around noun objects to the right of
the verb (DOBJ vs. IOBJ vs. TMP) could lead one
or more of the structural ambiguities falling out of
the beam, especially if the beam is small.
To combat this, we introduce a secondary beam
for each unique unlabeled structure. That is,
we partition the primary (entire) beam into dis-
joint groups according to the identity of unla-
beled structure. By limiting the size of the sec-
ondary beam, we restrict label ambiguity and en-
force structural diversity within the primary beam.
The resulting parser consistently improves on the
state-of-the-art parser of Zhang et al (2013). In
656
(a)
l
=
l
+
l
1
(b)
l
=
l
1
+
l
2
Figure 2: Structures and rules for parsing with the
(Eisner, 1996) algorithm. Solid lines show only
the construction of right-pointing first-order de-
pendencies. l is the predicted arc label. Dashed
lines are the additional sibling modifier signatures
in a generalized algorithm, specifically the previ-
ous modifier in complete chart items.
particular, data sets with large label sets (and thus
a large number of label confusions) typically see
the largest jumps in accuracy. Finally, we show
that the same result cannot be achieved by simply
increasing the size of the beam, but requires ex-
plicit enforcing of beam diversity.
2 Structural Diversity in Cube-Pruning
Our starting point is the cube-pruned dependency
parsing model of Zhang and McDonald (2012). In
that work, as here, inference is simply the Eis-
ner first-order parsing model (Eisner, 1996) shown
in Figure 2. In order to score higher-order fea-
tures, each chart item maintains a list of signa-
tures, which represent subtrees consistent with the
chart item. The stored signatures are the relevant
portions of the subtrees that will be part of higher-
order feature calculations. For example, to score
features over adjacent arcs, we might maintain ad-
ditional signatures, again shown in Figure 2.
The scope of the signature adds asymptotic
complexity to parsing. Even for second-order sib-
lings, there will now be O(n) possible signatures
per chart item. The result is that parsing com-
plexity increases from O(n
3
) to O(n
5
). Instead
of storing all signatures, Zhang and McDonald
(2012) store the current k-best in a beam. This re-
sults in approximate inference, as some signatures
may fall out of the beam before higher-order fea-
tures can be scored. This general trick is known as
cube-pruning and is a common approach to deal-
ing with large hypergraph search spaces in ma-
chine translation (Chiang, 2007).
Cube-pruned parsing is analogous to k-best
parsing algorithmically. But there is a fundamen-
tal difference. In k-best parsing, if two subtrees
t
a
and t
b
belong to the same chart item, with t
a
l
=
0 : 1 : 2 :
+
l
1
l
2
l
1
0 :
l
1
1 :
l
2
2 :
l
3
l
=
0 : 1 : 2 :
+
l
1
l
2
l
1
0 :
l
1
1 :
l
2
2 :
l
1
Figure 3: Merging procedure in cube pruning. The
bottom shows that enforcing diversity in the k-best
lists can give chance to a good structure at (2, 2).
ranking higher than t
b
, then an extension of t
a
through combing with a subtree t
c
from another
chart item must also score higher than that of t
b
.
This property is called the monotonicity property.
Based on it, k-best parsing merges k-best subtrees
in the following way: given two chart items with
k-best lists to be combined, it proceeds on the two
sorted lists monotonically from beginning to end
to generate combinations. Cube pruning follows
the merging procedure despite the loss of mono-
tonicity due to the addition of higher-order feature
functions over the signatures of the subtrees. The
underlying assumption of cube pruning is that the
true k-best results are likely in the cross-product
space of top-ranked component subtrees. Figure 3
shows that the space is the top-left corner of the
grid in the binary branching cases.
As mentioned earlier, the elements in chart item
k-best lists are feature signatures of subtrees. We
make a distinction between labeled signatures and
unlabeled signatures. As feature functions are de-
fined on sub-graphs of the dependency trees, a fea-
ture signature is labeled if and only if feature func-
tions draw information from both the arcs in the
sub-graph and the labels on the arcs. Every la-
beled signature projects to an unlabeled signature
657
that ignores the arc labels.
The motivation for introducing unlabeled signa-
tures for labeled parsing is to enforce structural di-
versity. Figure 3 illustrates the idea. In the top
diagram, there is only one unlabeled signature in
one of the two lists. This is likely to happen when
there is label ambiguity so that all three labels have
similar scores. In such cases, alternative tree struc-
tures further down in the list that have the poten-
tial to be scored higher when incorporating higher-
order features, lose this opportunity due to prun-
ing. By contrast, if we introduce structural diver-
sity by limiting the number of label variants, such
alternative structures can come out on top.
More formally, when the feature signatures of
the subtrees include arc labels, the cardinality of
the set of all possible signatures grows by a poly-
nomial of the size of the label set. This factor has a
diluting effect on the diversity of unlabeled signa-
tures within the beam. The larger the label set is,
the greater the chance label ambiguity will dom-
inate the beam. Therefore, we introduce a sec-
ond level of beam specifically for labeled signa-
tures. We call it the secondary beam, relative to
the primary beam, i.e., the entire beam. The sec-
ondary beam limits the number of labeled signa-
tures for each unlabeled signature, a projection of
labeled signature, while the primary beam limits
the total number of labeled signatures. To illus-
trate this, consider an original primary beam of
length b and a secondary beam length of s
b
. Let
t
j
i
represent the i
th
highest scoring labeled variant
of unlabeled structure j. The table below shows a
specific example of beam configurations for b = 4
for all possible values of s
b
. The original beam is
the pathological case where all signatures have the
same unlabeled projection. When s
b
= 1, all sig-
natures in the beam now have a different unlabeled
projection. When s
b
= 4, the beam reverts to the
original without any structural diversity. Values
between balance structural and label diversity.
beam original b = 4 b = 4 b = 4 b = 4
rank b=4 s
b
= 1 s
b
= 2 s
b
= 3 s
b
= 4
1 t
1
1
t
1
1
t
1
1
t
1
1
t
1
1
2 t
1
2
t
2
1
t
1
2
t
1
2
t
1
2
3 t
1
3
t
3
1
t
2
1
t
1
3
t
1
3
4 t
1
4
t
4
1
t
3
1
t
2
1
t
1
4
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? beam cut-off ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
5 t
2
1
. . . . . . . . . . . .
6 t
3
1
. . . . . . . . . . . .
7 t
2
2
. . . . . . . . . . . .
8 t
3
2
. . . . . . . . . . . .
9 t
4
1
. . . . . . . . . . . .
To achieve this in cube pruning, deeper explo-
ration in the merging procedure becomes neces-
sary. In this example, originally the merging pro-
cedure stops when t
1
4
has been explored. When
s
b
= 1, the exploration needs to go further from
rank 4 to 9. When s
b
= 2, it needs to go from 4
to 6. When s
b
= 3, only one more step to rank
5 is necessary. The amount of additional compu-
tation depends on the value of s
b
, the composi-
tion of the incoming k-best lists, and the feature
functions which determine feature signatures. To
account for this we also compare to baselines sys-
tems that simply increase the size of the beam to a
comparable run-time.
In our experiments we found that s
b
= b/2 is
typically a good choice. As in most parsing sys-
tems, beams are applied consistently during learn-
ing and testing because feature weights will be ad-
justed according to the diversity of the beam.
3 Experiments
We use the cube-pruned dependency parser of
Zhang et al (2013) as our baseline system. To
make an apples-to-apples comparison, we use the
same online learning algorithm and the same fea-
ture templates. The feature templates include first-
to-third-order labeled features and valency fea-
tures. More details of these features are described
in Zhang and McDonald (2012). For online learn-
ing, we apply the same violation-fixing strategy
(so-called single-node max-violation) on MIRA
and run 8 epochs of training for all experiments.
For English, we conduct experiments on
the commonly-used constituency-to-dependency-
converted Penn Treebank data sets. The first one,
Penn-YM, was created by the Penn2Malt
1
soft-
ware. The second one, Penn-S-2.0.5, used the
Stanford dependency framework (De Marneffe et
al., 2006) by applying version 2.0.5 of the Stan-
ford parser. The third one, Penn-S-3.3.0 was con-
verted by version 3.3.0 of the Stanford parser. The
train/dev/test split was standard: sections 2-21 for
training; 22 for validation; and 23 for evaluation.
Automatic POS tags for Penn-YM and Penn-S-
2.0.5 are provided by TurboTagger (Martins et al,
2013) with an accuracy of 97.3% on section 23.
For Chinese, we use the CTB-5 dependency tree-
bank which was converted from the original con-
stituent treebank by Zhang and Nivre (2011) and
use gold-standard POS tags as is standard.
1
http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html
658
Berkeley Parser TurboParser Cube-pruned w/o diversity Cube-pruned w/ diversity
UAS LAS UAS LAS UAS LAS UAS LAS
PENN-YM - - 93.07 - 93.50 92.41 93.57 92.48
PENN-S-2.0.5 - - 92.82 - 93.59 91.17 93.71 91.37
PENN-S-3.3.0 93.31 91.01 92.20 89.67 92.91 90.52 93.01 90.64
PENN-S-3.3.0-GOLD 93.65 92.05 93.56 91.99 94.32 92.90 94.40 93.02
CTB-5 - - - - 87.78 86.13 87.96 86.34
Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of
structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is
from Martins et al (2013). Following Kong and Smith (2014), we trained our models on Penn-S-3.3.0
with gold POS tags and evaluated with both non-gold (Stanford tagger) and gold tags.
Table 1 shows the main results of the paper.
Both the baseline and the new system keep a beam
of size 6 for each chart cell. The difference is
that the new system enforces structural diversity
with the introduction of a secondary beam for la-
bel variants. We choose the secondary beam that
yields the highest LAS on the development data
sets for Penn-YM, Penn-S-2.0.5 and CTB-5. In-
deed we observe larger improvements for the data
sets with larger label sets. Penn-S-2.0.5 has 49 la-
bels and observes a 0.2% absolute improvement in
LAS. Although CTB-5 has a small label set (18),
we do see similar improvements for both UAS and
LAS. There is a slight improvement for Penn-YM
despite the fact that Penn-YM has the most com-
pact label set (12). These results are the highest
known in the literature. For the Penn-S-3.3.0 re-
sults we can see that our model outperforms Tur-
boPaser and is competitive with the Berkeley con-
stituency parser (Petrov et al, 2006). In particu-
lar, if gold tags are assumed, cube-pruning signif-
icantly outperforms Berkeley. This suggests that
joint tagging and parsing should improve perfor-
mance further in the non-gold tag setting, as that
is a differentiating characteristic of constituency
parsers. Table 2 shows the results on the CoNLL
2006/2007 data sets (Buchholz and Marsi, 2006;
Nivre et al, 2007). For simplicity, we set the sec-
ondary beam to 3 for all. We can see that over-
all there is an improvement in accuracy and this is
highly correlated with the size of the label set.
In order to examine the importance of balancing
structural diversity and labeled diversity, we let the
size of the secondary beam vary from one to the
size of the primary beam. In Table 3, we show the
results of all combinations of beam settings of pri-
mary beam sizes 4 and 6 for three data sets: Penn-
YM, Penn-S-2.0.5, and CTB-5 respectively. In the
table, we highlight the best results for each beam
size and data set on the development data. For 5
of the total of 6 comparison groups ? three lan-
w/o diversity w/ diversity
Language(labels) UAS LAS UAS LAS
CZECH(82) 88.36 82.16 88.36 82.02
SWEDISH(64) 91.62 85.08 91.85 85.26
PORTUGUESE(55) 92.07 88.30 92.23 88.50
DANISH(53) 91.88 86.95 91.78 86.93
HUNGARIAN(49) 85.85 81.02 86.55 81.79
GREEK(46) 86.14 78.20 86.21 78.45
GERMAN(46) 92.03 89.44 92.01 89.52
CATALAN(42) 94.58 89.05 94.91 89.54
BASQUE(35) 79.59 71.52 80.14 71.94
ARABIC(27) 80.48 69.68 80.56 69.98
TURKISH(26) 76.94 66.80 77.14 67.00
SLOVENE(26) 86.01 77.14 86.27 77.44
DUTCH(26) 83.57 80.29 83.39 80.19
ITALIAN(22) 87.57 83.22 87.38 82.95
SPANISH(21) 87.96 84.95 87.98 84.79
BULGARIAN(19) 94.02 89.87 93.88 89.63
JAPANESE(8) 93.26 91.67 93.16 91.51
AVG 87.76 82.08 87.87 82.20
Table 2: Results for languages from CoNLL
2006/2007 shared tasks. When a language is in
both years, the 2006 set is used. Languages are
sorted by the number of unique arc labels.
guages times two primary beams ? the best result
is obtained by choosing a secondary beam size that
is close to one half the size of the primary beam.
Contrasting Table 1 and Table 3, the accuracy im-
provements are consistent across the development
set and the test set for all three data sets.
A reasonable question is whether such improve-
ments could be obtained by simply enlarging the
beam in the baseline parser. The bottom row of
Table 3 shows the parsing results for the three data
sets when the beam is enlarged to 16. On Penn-
S-2.0.5, the baseline with beam 16 is at roughly
the same speed as the highlighted best system with
primary beam 6 and secondary beam 3. On CTB-
5, the beam 16 baseline is 30% slower. Table 3
indicates that simply enlarging the beam ? rela-
tive to parsing speed ? does not recover the wins
of structural diversity on Penn-S-2.0.5 and CTB-5,
though it does reduce the gap on Penn-S-2.0.5. On
Penn-YM, the beam 16 baseline is slightly better
than the new system, but 90% slower.
659
primary secondary PENN-YM PENN-S-2.0.5 CTB-5
beam beam UAS LAS UAS LAS UAS LAS
4
1 93.67 92.64 93.65 91.04 87.53 85.85
2 93.79 92.68 93.77 91.30 87.62 85.96
3 93.80 92.66 93.69 91.23 87.48 85.91
4 93.75 92.63 93.62 91.11 87.68 86.08
6
1 93.65 92.46 93.76 91.15 87.72 86.05
2 93.80 92.69 93.80 91.35 87.61 85.96
3 93.75 92.64 93.99 91.55 87.80 86.18
4 93.82 92.74 93.84 91.40 87.91 86.28
5 93.82 92.71 93.71 91.26 87.75 86.12
6 93.74 92.61 93.70 91.21 87.66 86.05
16 16 93.87 92.75 93.77 91.35 87.59 85.86
Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with
different primary beams. When the size of the secondary beam is equal to the primary beam, the parser
degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,
there is more structural diversity and less label diversity. Results are on development sets.
To better understand the behaviour of structural
diversity pruning relative to increasing the beam,
we looked at the unlabeled attachment F-score per
dependency label in the Penn-S-2.0.5 development
set
2
. Table 4 shows the 10 labels with the largest
increase in attachment scores for structural diver-
sity pruning relative to standard pruning. Impor-
tantly, the biggest wins are primarily for labels in
which unlabeled attachment is lower than average
(93.99, 8 out of 10). Thus, diversity pruning gets
most of its wins on difficult attachment decisions.
Indeed, many of the relations represent clausal
dependencies that are frequently structurally am-
biguous. There are also cases of relatively short
dependencies that can be difficult to attach. For
instance, quantmod dependencies are typically ad-
verbs occurring after verbs that modify quantities
to their right. But these can be confused as ad-
verbial modifiers of the verb to the left. These re-
sults support our hypothesis that label ambiguity
is causing hard attachment decisions to be pruned
and that structural diversity can ameliorate this.
4 Discussion
Keeping multiple beams in approximate search
has been explored in the past. In machine transla-
tion, multiple beams are used to prune translation
hypotheses at different levels of granularity (Zens
and Ney, 2008). However, the focus is improving
the speed of translation decoder rather than im-
proving translation quality through enforcement
of hypothesis diversity. In parsing, Bohnet and
Nivre (2012) and Bohnet et al (2013) propose a
model for joint morphological analysis, part-of-
speech tagging and dependency parsing using a
2
Using eval.pl from Buchholz and Marsi (2006).
w/o diversity w/ diversity
Label large beam small beam diff
quantmod 86.65 88.06 1.41
partmod 83.63 85.02 1.39
xcomp 87.76 88.74 0.98
tmod 89.75 90.72 0.97
appos 88.89 89.84 0.95
nsubjpass 92.53 93.31 0.78
complm 94.50 95.15 0.64
advcl 81.10 81.74 0.63
ccomp 82.64 83.17 0.54
number 96.86 97.39 0.53
Table 4: Unlabeled attachment F-score per de-
pendency relation. The top 10 score increases
for structural diversity pruning (beam 6 and la-
bel beam of 3) over basic pruning (beam 16) are
shown. Only labels with more than 100 instances
in the development data are considered.
left-to-right beam. With a single beam, token level
ambiguities (morphology and tags) dominate and
dependency level ambiguity is suppressed. This is
addressed by essentially keeping two beams. The
first forces every tree to be different at the depen-
dency level and the second stores the remaining
highest scoring options, which can include outputs
that differ only at the token level.
The present work looks at beam diversity in
graph-based dependency parsing, in particular la-
bel versus structural diversity. It was shown that
by keeping a diverse beam significant improve-
ments could be achieved on standard benchmarks,
in particular with respect to difficult attachment
decisions. It is worth pointing out that other
dependency parsing frameworks (e.g., transition-
based parsing (Zhang and Clark, 2008; Zhang and
Nivre, 2011)) could also benefit from modeling
structural diversity in search.
660
References
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proc. of COL-
ING.
B. Bohnet and J. Nivre. 2012. A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proc. of
EMNLP/CoNLL.
B. Bohnet, J. Nivre, I. Boguslavsky, F. Ginter, Rich?ard
F., and J. Hajic. 2013. Joint morphological and syn-
tactic analysis for richly inflected languages. TACL,
1.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
M. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. of COL-
ING.
H. He, H. Daum?e III, and J. Eisner. 2013. Dynamic
feature selection for dependency parsing. In Proc.
of EMNLP.
L. Kong and N. A. Smith. 2014. An empirical compar-
ison of parsing methods for stanford dependencies.
In ArXiv:1404.4314.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
S. K?ubler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Morgan & Claypool Publishers.
X. Ma and H. Zhao. 2012. Fourth-order dependency
parsing. In Proc. of COLING.
A. F. T. Martins, M. B. Almeida, and N. A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proc. of ACL.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. of ACL.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of EMNLP-CoNLL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proc. of ACL.
A. Rush and S. Petrov. 2012. Efficient multi-pass de-
pendency pruning with vine parsing. In Proc. of
NAACL.
R. Zens and H. Ney. 2008. Improvements in dynamic
programming beam search for phrase-based statisti-
cal machine translation. In Proc. IWSLT.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
H. Zhang and R. McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of EMNLP.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
of ACL-HLT, volume 2.
H. Zhang, L. Huang, K.Zhao, and R. McDonald. 2013.
Online learning for inexact hypergraph search. In
Proc. of EMNLP.
661
Transactions of the Association for Computational Linguistics, 1 (2013) 1?12. Action Editor: Sharon Goldwater.
Submitted 11/2012; Revised 1/2013; Published 3/2013. c?2013 Association for Computational Linguistics.
Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging
Oscar Ta?ckstro?m?? Dipanjan Das? Slav Petrov? Ryan McDonald? Joakim Nivre??
 Swedish Institute of Computer Science
?Department of Linguistics and Philology, Uppsala University
?Google Research, New York
oscar@sics.se
{dipanjand|slav|ryanmcd}@google.com
joakim.nivre@lingfil.uu.se
Abstract
We consider the construction of part-of-speech
taggers for resource-poor languages. Recently,
manually constructed tag dictionaries from
Wiktionary and dictionaries projected via bitext
have been used as type constraints to overcome
the scarcity of annotated data in this setting.
In this paper, we show that additional token
constraints can be projected from a resource-
rich source language to a resource-poor target
language via word-aligned bitext. We present
several models to this end; in particular a par-
tially observed conditional random field model,
where coupled token and type constraints pro-
vide a partial signal for training. Averaged
across eight previously studied Indo-European
languages, our model achieves a 25% relative
error reduction over the prior state of the art.
We further present successful results on seven
additional languages from different families,
empirically demonstrating the applicability of
coupled token and type constraints across a
diverse set of languages.
1 Introduction
Supervised part-of-speech (POS) taggers are avail-
able for more than twenty languages and achieve ac-
curacies of around 95% on in-domain data (Petrov et
al., 2012). Thanks to their efficiency and robustness,
supervised taggers are routinely employed in many
natural language processing applications, such as syn-
tactic and semantic parsing, named-entity recognition
and machine translation. Unfortunately, the resources
required to train supervised taggers are expensive to
create and unlikely to exist for the majority of written
?Work primarily carried out while at Google Research.
languages. The necessity of building NLP tools for
these resource-poor languages has been part of the
motivation for research on unsupervised learning of
POS taggers (Christodoulopoulos et al, 2010).
In this paper, we instead take a weakly supervised
approach towards this problem. Recently, learning
POS taggers with type-level tag dictionary constraints
has gained popularity. Tag dictionaries, noisily pro-
jected via word-aligned bitext, have bridged the gap
between purely unsupervised and fully supervised
taggers, resulting in an average accuracy of over 83%
on a benchmark of eight Indo-European languages
(Das and Petrov, 2011). Li et al (2012) further im-
proved upon this result by employing Wiktionary1 as
a tag dictionary source, resulting in the hitherto best
published result of almost 85% on the same setup.
Although the aforementioned weakly supervised
approaches have resulted in significant improvements
over fully unsupervised approaches, they have not
exploited the benefits of token-level cross-lingual
projection methods, which are possible with word-
aligned bitext between a target language of interest
and a resource-rich source language, such as English.
This is the setting we consider in this paper (?2).
While prior work has successfully considered both
token- and type-level projection across word-aligned
bitext for estimating the model parameters of genera-
tive tagging models (Yarowsky and Ngai, 2001; Xi
and Hwa, 2005, inter alia), a key observation under-
lying the present work is that token- and type-level
information offer different and complementary sig-
nals. On the one hand, high confidence token-level
projections offer precise constraints on a tag in a
particular context. On the other hand, manually cre-
1http://www.wiktionary.org/.
1
ated type-level dictionaries can have broad coverage
and do not suffer from word-alignment errors; they
can therefore be used to filter systematic as well as
random noise in token-level projections.
In order to reap these potential benefits, we pro-
pose a partially observed conditional random field
(CRF) model (Lafferty et al, 2001) that couples to-
ken and type constraints in order to guide learning
(?3). In essence, the model is given the freedom to
push probability mass towards hypotheses consistent
with both types of information. This approach is flex-
ible: we can use either noisy projected or manually
constructed dictionaries to generate type constraints;
furthermore, we can incorporate arbitrary features
over the input. In addition to standard (contextual)
lexical features and transition features, we observe
that adding features from a monolingual word cluster-
ing (Uszkoreit and Brants, 2008) can significantly im-
prove accuracy. While most of these features can also
be used in a generative feature-based hidden Markov
model (HMM) (Berg-Kirkpatrick et al, 2010), we
achieve the best accuracy with a globally normalized
discriminative CRF model.
To evaluate our approach, we present extensive
results on standard publicly available datasets for 15
languages: the eight Indo-European languages pre-
viously studied in this context by Das and Petrov
(2011) and Li et al (2012), and seven additional lan-
guages from different families, for which no compa-
rable study exists. In ?4 we compare various features,
constraints and model types. Our best model uses
type constraints derived from Wiktionary, together
with token constraints derived from high-confidence
word alignments. When averaged across the eight
languages studied by Das and Petrov (2011) and Li
et al (2012), we achieve an accuracy of 88.8%. This
is a 25% relative error reduction over the previous
state of the art. Averaged across all 15 languages,
our model obtains an accuracy of 84.5% compared to
78.5% obtained by a strong generative baseline. Fi-
nally, we provide an in depth analysis of the relative
contributions of the two types of constraints in ?5.
2 Coupling Token and Type Constraints
Type-level information has been amply used in
weakly supervised POS induction, either via pure
manually crafted tag dictionaries (Smith and Eisner,
2005; Ravi and Knight, 2009; Garrette and Baldridge,
2012), noisily projected tag dictionaries (Das and
Petrov, 2011) or through crowdsourced lexica, such
as Wiktionary (Li et al, 2012). At the other end
of the spectrum, there have been efforts that project
token-level information across word-aligned bitext
(Yarowsky and Ngai, 2001; Xi and Hwa, 2005). How-
ever, systems that combine both sources of informa-
tion in a single model have yet to be fully explored.
The following three subsections outline our overall
approach for coupling these two types of information
to build robust POS taggers that do not require any
direct supervision in the target language.
2.1 Token Constraints
For the majority of resource-poor languages, there
is at least some bitext with a resource-rich source
language; for simplicity, we choose English as our
source language in all experiments. It is then nat-
ural to consider using a supervised part-of-speech
tagger to predict part-of-speech tags for the English
side of the bitext. These predicted tags can subse-
quently be projected to the target side via automatic
word alignments. This approach was pioneered by
Yarowsky and Ngai (2001), who used the resulting
partial target annotation to estimate the parameters
of an HMM. However, due to the automatic nature
of the word alignments and the POS tags, there will
be significant noise in the projected tags. To conquer
this noise, they used very aggressive smoothing tech-
niques when training the HMM. Fossum and Abney
(2005) used similar token-level projections, but in-
stead combined projections from multiple source lan-
guages to filter out random projection noise as well
as the systematic noise arising from different source
language annotations and syntactic divergences.
2.2 Type Constraints
It is well known that given a tag dictionary, even if
it is incomplete, it is possible to learn accurate POS
taggers (Smith and Eisner, 2005; Goldberg et al,
2008; Ravi and Knight, 2009; Naseem et al, 2009).
While widely differing in the specific model struc-
ture and learning objective, all of these approaches
achieve excellent results. Unfortunately, they rely
on tag dictionaries extracted directly from the un-
derlying treebank data. Such dictionaries provide in
depth coverage of the test domain and also list all
2
	
 
     
   

 
   	
 
  	  	  

	

	
 



	



 

	    


Figure 1: Lattice representation of the inference search space Y(x) for an authentic sentence in Swedish (?The farming
products must be pure and must not contain any additives?), after pruning with Wiktionary type constraints. The
correct parts of speech are listed underneath each word. Bold nodes show projected token constraints y?. Underlined
text indicates incorrect tags. The coupled constraints lattice Y?(x, y?) consists of the bold nodes together with nodes for
words that are lacking token constraints; in this case, the coupled constraints lattice thus defines exactly one valid path.
inflected forms ? both of which are difficult to obtain
and unrealistic to expect for resource-poor languages.
In contrast, Das and Petrov (2011) automatically
create type-level tag dictionaries by aggregating over
projected token-level information extracted from bi-
text. To handle the noise in these automatic dictionar-
ies, they use label propagation on a similarity graph
to smooth (and also expand) the label distributions.
While their approach produces good results and is
applicable to resource-poor languages, it requires a
complex multi-stage training procedure including the
construction of a large distributional similarity graph.
Recently, Li et al (2012) presented a simple and
viable alternative: crowdsourced dictionaries from
Wiktionary. While noisy and sparse in nature, Wik-
tionary dictionaries are available for 170 languages.2
Furthermore, their quality and coverage is growing
continuously (Li et al, 2012). By incorporating type
constraints from Wiktionary into the feature-based
HMM of Berg-Kirkpatrick et al (2010), Li et al were
able to obtain the best published results in this setting,
surpassing the results of Das and Petrov (2011) on
eight Indo-European languages.
2.3 Coupled Constraints
Rather than relying exclusively on either token or
type constraints, we propose to complement the one
with the other during training. For each sentence in
our training set, a partially constrained lattice of tag
sequences is constructed as follows:
2http://meta.wikimedia.org/wiki/
Wiktionary ? October 2012.
1. For each token whose type is not in the tag dic-
tionary, we allow the entire tag set.
2. For each token whose type is in the tag dictio-
nary, we prune all tags not licensed by the dictio-
nary and mark the token as dictionary-pruned.
3. For each token that has a tag projected via a
high-confidence bidirectional word alignment:
if the projected tag is still present in the lattice,
then we prune every tag but the projected tag for
that token; if the projected tag is not present in
the lattice, which can only happen for dictionary-
pruned tokens, then we ignore the projected tag.
Figure 1 provides a running example. The lattice
shows tags permitted after constraining the words
to tags licensed by the dictionary (up until Step 2
from above). There is only a single token ?Jordbruk-
sprodukterna? (?the farming products?) not in the
dictionary; in this case the lattice permits the full
set of tags. With token-level projections (Step 3;
nodes with bold border in Figure 1), the lattice can
be further pruned. In most cases, the projected tag
is both correct and is in the dictionary-pruned lattice.
We thus successfully disambiguate such tokens and
shrink the search space substantially.
There are two cases we highlight in order to show
where our model can break. First, for the token
?Jordbruksprodukterna?, the erroneously projected
tag ADJ will eliminate all other tags from the lattice,
including the correct tag NOUN. Second, the token
?na?gra? (?any?) has a single dictionary entry PRON
and is missing the correct tag DET. In the case where
3
DET is the projected tag, we will not add it to the
lattice and simply ignore it. This is because we hy-
pothesize that the tag dictionary can be trusted more
than the tags projected via noisy word alignments. As
we will see in ?4, taking the union of tags performs
worse, which supports this hypothesis.
For generative models, such as HMMs (?3.1), we
need to define only one lattice. For our best gen-
erative model this is the coupled token- and type-
constrained lattice.3 At prediction time, in both the
discriminative and the generative cases, we find the
most likely label sequence using Viterbi decoding.
For discriminative models, such as CRFs (?3.2),
we need to define two lattices: one that the model
moves probability mass towards and another one
defining the overall search space (or partition func-
tion). In traditional supervised learning without a
dictionary, the former is a trivial lattice containing
the gold standard tag sequence and the latter is the
set of all possible tag sequences spanning the tokens.
With our best model, we will move mass towards the
coupled token- and type-constrained lattice, such that
the model can freely distribute mass across all paths
consistent with these constraints. The lattice defining
the partition function will be the full set of possible
tag sequences when no dictionary is used; when a
dictionary is used it will consist of all dictionary-
pruned tag sequences (sans Step 3 above; the full set
of possibilities shown in Figure 1 for our running
example).
Figures 2 and 3 provide statistics regarding the
supervision coverage and remaining ambiguity. Fig-
ure 2 shows that more than two thirds of all tokens in
our training data are in Wiktionary. However, there is
considerable variation between languages: Spanish
has the highest coverage with over 90%, while Turk-
ish, an agglutinative language with a vast number
of word forms, has less than 50% coverage. Fig-
ure 3 shows that there is substantial uncertainty left
after pruning with Wiktionary, since tokens are rarely
fully disambiguated: 1.3 tags per token are allowed
on average for types in Wiktionary.
Figure 2 further shows that high-confidence align-
ments are available for about half of the tokens for
most languages (Japanese is a notable exception with
3Other training methods exist as well, for example, con-
trastive estimation (Smith and Eisner, 2005).
0
25
50
75
100
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Pe
rc
en
t o
f to
ke
ns
 c
ov
er
ed
Token
coverage Wiktionary Projected Projected+Filtered
Figure 2: Wiktionary and projection dictionary coverage.
Shown is the percentage of tokens in the target side of the
bitext that are covered by Wiktionary, that have a projected
tag, and that have a projected tag after intersecting the two.
0.0
0.5
1.0
1.5
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Nu
mb
er 
of 
tag
s p
er 
tok
en
Figure 3: Average number of licensed tags per token on
the target side of the bitext, for types in Wiktionary.
less than 30% of the tokens covered). Intersecting the
Wiktionary tags and the projected tags (Step 2 and 3
above) filters out some of the potentially erroneous
tags, but preserves the majority of the projected tags;
the remaining, presumably more accurate projected
tags cover almost half of all tokens, greatly reducing
the search space that the learner needs to explore.
3 Models with Coupled Constraints
We now formally present how we couple token and
type constraints and how we use these coupled con-
straints to train probabilistic tagging models. Let
x = (x1x2 . . . x|x|) ? X denote a sentence, where
each token xi ? V is an instance of a word type from
the vocabulary V and let y = (y1y2 . . . y|x|) ? Y de-
note a tag sequence, where yi ? T is the tag assigned
to token xi and T denotes the set of all possible part-
of-speech tags. We denote the lattice of all admissible
tag sequences for the sentence x by Y(x). This is the
4
inference search space in which the tagger operates.
As we shall see, it is crucial to constrain the size of
this lattice in order to simplify learning when only
incomplete supervision is available.
A tag dictionary maps a word type xj ? V to
a set of admissible tags T (xj) ? T . For word
types not in the dictionary we allow the full set of
tags T (while possible, in this paper we do not at-
tempt to distinguish closed-class versus open-class
words). When provided with a tag dictionary, the
lattice of admissible tag sequences for a sentence x
is Y(x) = T (x1) ? T (x2) ? . . . ? T (x|x|). When
no tag dictionary is available, we simply have the full
lattice Y(x) = T |x|.
Let y? = (y?1y?2 . . . y?|x|) be the projected tags for
the sentence x. Note that {y?i} = ? for tokens without
a projected tag. Next, we define a piecewise operator
_ that couples y? and Y(x) with respect to every
sentence index, which results in a token- and type-
constrained lattice. The operator behaves as follows,
coherent with the high level description in ?2.3:
T? (xi, y?i) = y?i _ T (xi) =
{
{y?i} if y?i ? T (xi)
T (xi) otherwise .
We denote the token- and type-constrained lattice as
Y?(x, y?) = T? (x1, y?1)?T? (x2, y?2)?. . .?T? (x|x|, y?|x|).
Note that when token-level projections are not used,
the dictionary-pruned lattice and the lattice with cou-
pled constraints are identical, that is Y?(x, y?) = Y(x).
3.1 HMMs with Coupled Constraints
A first-order hidden Markov model (HMM) specifies
the joint distribution of a sentence x ? X and a
tag-sequence y ? Y(x) as:
p?(x, y) =
|x|?
i=1
p?(xi | yi)? ?? ?
emission
p?(yi | yi?1)? ?? ?
transition
.
We follow the recent trend of using a log-linear
parametrization of the emission and the transition
distributions, instead of a multinomial parametriza-
tion (Chen, 2003). This allows model parameters ?
to be shared across categorical events, which has
been shown to give superior performance (Berg-
Kirkpatrick et al, 2010). The categorical emission
and transition events are represented by feature vec-
tors ?(xi, yi) and ?(yi, yi?1). Each element of the
parameter vector ? corresponds to a particular fea-
ture; the component log-linear distributions are:
p?(xi | yi) =
exp
(
?>?(xi, yi)
)
?
x?i?V exp (?
>?(x?i, yi))
,
and
p?(yi | yi?1) =
exp
(
?>?(yi, yi?1)
)
?
y?i?T exp (?
>?(y?i, yi?1))
.
In maximum-likelihood estimation of the parameters,
we seek to maximize the likelihood of the observed
parts of the data. For this we need the joint marginal
distribution p?(x, Y?(x, y?)) of a sentence x, and its
coupled constraints lattice Y?(x, y?), which is obtained
by marginalizing over all consistent outputs:
p?(x, Y?(x, y?)) =
?
y?Y?(x,y?)
p?(x, y) .
If there are no projections and no tag dictionary, then
Y?(x, y?) = T |x|, and thus p?(x, Y?(x, y?)) = p?(x),
which reduces to fully unsupervised learning. The
`2-regularized marginal joint log-likelihood of the
constrained training data D = {(x(i), y?(i))}ni=1 is:
L(?;D) =
n?
i=1
log p?(x(i), Y?(x(i), y?(i)))?? ???22 .
(1)
We follow Berg-Kirkpatrick et al (2010) and take a
direct gradient approach for optimizing Eq. 1 with
L-BFGS (Liu and Nocedal, 1989). We set ? = 1 and
run 100 iterations of L-BFGS. One could also em-
ploy the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977) to optimize this objective, al-
though the relative merits of EM versus direct gradi-
ent training for these models is still a topic of debate
(Berg-Kirkpatrick et al, 2010; Li et al, 2012).4 Note
that since the marginal likelihood is non-concave, we
are only guaranteed to find a local maximum of Eq. 1.
After estimating the model parameters ?, the tag-
sequence y? ? Y(x) for a sentence x ? X is pre-
dicted by choosing the one with maximal joint prob-
ability:
y? ? arg max
y?Y(x)
p?(x, y) .
4We trained the HMM with EM as well, but achieved better
results with direct gradient training and hence omit those results.
5
3.2 CRFs with Coupled Constraints
Whereas an HMM models the joint probability of
the input x ? X and output y ? Y(x), using locally
normalized component distributions, a conditional
random field (CRF) instead models the probability of
the output conditioned on the input as a globally nor-
malized log-linear distribution (Lafferty et al, 2001):
p?(y | x) =
exp
(
?>?(x, y)
)
?
y??Y(x) exp (?>?(x, y?))
,
where ? is a parameter vector. As for the HMM,
Y(x) is not necessarily the full space of possible
tag-sequences; specifically, for us, it is the dictionary-
pruned lattice without the token constraints.
With a first-order Markov assumption, the feature
function factors as:
?(x, y) =
|x|?
i=1
?(x, yi, yi?1) .
This model is more powerful than the HMM in that
it can use richer feature definitions, such as joint in-
put/transition features and features over a wider input
context. We model a marginal conditional probabil-
ity, given by the total probability of all tag sequences
consistent with the lattice Y?(x, y?):
p?(Y?(x, y?) | x) =
?
y?Y?(x,y?)
p?(y | x) .
The parameters of this constrained CRF are estimated
by maximizing the `2-regularized marginal condi-
tional log-likelihood of the constrained data (Riezler
et al, 2002):
L(?;D) =
n?
i=1
log p?(Y?(x(i), y?(i)) | x(i))? ????22 .
(2)
As with Eq. 1, we maximize Eq. 2 with 100 itera-
tions of L-BFGS and set ? = 1. In contrast to the
HMM, after estimating the model parameters ?, the
tag-sequence y? ? Y(x) for a sentence x ? X is
chosen as the sequence with the maximal conditional
probability:
y? ? arg max
y?Y(x)
p?(y | x) .
4 Empirical Study
We now present a detailed empirical study of the mod-
els proposed in the previous sections. In addition to
comparing with the state of the art in Das and Petrov
(2011) and Li et al (2012), we present models with
several combinations of token and type constraints,
additional features incorporating word clusters. Both
generative and discriminative models are explored.
4.1 Experimental Setup
Before delving into the experimental details, we
present our setup and datasets.
Languages. We evaluate on eight target languages
used in previous work (Das and Petrov, 2011; Li et
al., 2012) and on seven additional languages (see Ta-
ble 1). While the former eight languages all belong to
the Indo-European family, we broaden the coverage
to language families more distant from the source
language (for example, Chinese, Japanese and Turk-
ish). We use the treebanks from the CoNLL shared
tasks on dependency parsing (Buchholz and Marsi,
2006; Nivre et al, 2007) for evaluation.5 The two-
letter abbreviations from the ISO 639-1 standard are
used when referring to these languages in tables and
figures.
Tagset. In all cases, we map the language-specific
POS tags to universal POS tags using the mapping
of Petrov et al (2012).6 Since we use indirect super-
vision via projected tags or Wiktionary, the model
states induced by all models correspond directly to
POS tags, enabling us to compute tagging accuracy
without a greedy 1-to-1 or many-to-1 mapping.
Bitext. For all experiments, we use English as the
source language. Depending on availability, there
are between 1M and 5M parallel sentences for each
language. The majority of the parallel data is gath-
ered automatically from the web using the method
of Uszkoreit et al (2010). We further include data
from Europarl (Koehn, 2005) and from the UN par-
allel corpus (UN, 2006), for languages covered by
these corpora. The English side of the bitext is
POS tagged with a standard supervised CRF tagger,
trained on the Penn Treebank (Marcus et al, 1993),
with tags mapped to universal tags. The parallel sen-
5For French we use the treebank of Abeille? et al (2003).
6We use version 1.03 of the mappings available at http:
//code.google.com/p/universal-pos-tags/.
6
tences are word aligned with the aligner of DeNero
and Macherey (2011). Intersected high-confidence
alignments (confidence >0.95) are extracted and ag-
gregated into projected type-level dictionaries. For
purely practical reasons, the training data with token-
level projections is created by randomly sampling
target-side sentences with a total of 500K tokens.
Wiktionary. We use a snapshot of the Wiktionary
word definitions, and follow the heuristics of Li et
al. (2012) for creating the Wiktionary dictionary by
mapping the Wiktionary tags to universal POS tags.7
Features. For all models, we use only an identity
feature for tag-pair transitions. We use five features
that couple the current tag and the observed word
(analogous to the emission in an HMM): word iden-
tity, suffixes of up to length 3, and three indicator
features that fire when the word starts with a capital
letter, contains a hyphen or contains a digit. These are
the same features as those used by Das and Petrov
(2011). Finally, for some models we add a word
cluster feature that couples the current tag and the
word cluster identity of the word. These (monolin-
gual) word clusters are induced with the exchange
algorithm (Uszkoreit and Brants, 2008). We set the
number of clusters to 256 across all languages, as this
has previously been shown to produce robust results
for similar tasks (Turian et al, 2010; Ta?ckstro?m et
al., 2012). The clusters for each language are learned
on a large monolingual newswire corpus.
4.2 Models with Type Constraints
To examine the sole effect of type constraints, we
experiment with the HMM, drawing constraints from
three different dictionaries. Table 1 compares the per-
formance of our models with the best results of Das
and Petrov (2011, D&P) and Li et al (2012, LG&T).
As in previous work, training is done exclusively on
the training portion of each treebank, stripped of any
manual linguistic annotation.
We first use all of our parallel data to generate
projected tag dictionaries: the English POS tags are
projected across word alignments and aggregated to
tag distributions for each word type. As in Das and
Petrov (2011), the distributions are then filtered with
a threshold of 0.2 to remove noisy tags and to create
7The definitions were downloaded on August 31, 2012 from
http://toolserver.org/?enwikt/definitions/.
This snapshot is more recent than that used by Li et al
Prior work HMM with type constraints
Lang. D&P LG&T YHMMproj. YHMMwik. YHMMunion YHMMunion +C
bg ? ? 84.2 68.1 87.2 87.9
cs ? ? 75.4 70.2 75.4 79.2
da 83.2 83.3 87.7 82.0 78.4 89.5
de 82.8 85.8 86.6 85.1 80.0 88.3
el 82.5 79.2 83.3 83.8 86.0 83.2
es 84.2 86.4 83.9 83.7 88.3 87.3
fr ? ? 88.4 75.7 75.6 86.6
it 86.8 86.5 89.0 85.4 89.9 90.6
ja ? ? 45.2 76.9 74.4 73.7
nl 79.5 86.3 81.7 79.1 83.8 82.7
pt 87.9 84.5 86.7 79.0 83.8 90.4
sl ? ? 78.7 64.8 82.8 83.4
sv 80.5 86.1 80.6 85.9 85.9 86.7
tr ? ? 66.2 44.1 65.1 65.7
zh ? ? 59.2 73.9 63.2 73.0
avg (8) 83.4 84.8 84.9 83.0 84.5 87.3
avg ? ? 78.5 75.9 80.0 83.2
Table 1: Tagging accuracies for type-constrained HMM
models. D&P is the ?With LP? model in Table 2 of
Das and Petrov (2011), while LG&T is the ?SHMM-ME?
model in Table 2 of Li et al (2012). YHMMproj. , YHMMwik. and
YHMMunion are HMMs trained solely with type constraints
derived from the projected dictionary, Wiktionary and
the union of these dictionaries, respectively. YHMMunion +C is
equivalent to YHMMunion with additional cluster features. All
models are trained on the treebank of each language,
stripped of gold labels. Results are averaged over the
8 languages from Das and Petrov (2011), denoted avg (8),
as well as over the full set of 15 languages, denoted avg.
an unweighted tag dictionary. We call this model
YHMMproj. ; its average accuracy of 84.9% on the eight
languages is higher than the 83.4% of D&P and on
par with LG&T (84.8%).8 Our next model (YHMMwik. )
simply draws type constraints from Wiktionary. It
slightly underperforms LG&T (83.0%), presumably
because they used a second-order HMM. As a simple
extension to these two models, we take the union
of the projected dictionary and Wiktionary to con-
strain an HMM, which we name YHMMunion . This model
performs a little worse on the eight Indo-European
languages (84.5), but gives an improvement over the
projected dictionary when evaluated across all 15
languages (80.0% vs. 78.5%).
8Our model corresponds to the weaker, ?No LP? projection
of Das and Petrov (2011). We found that label propagation was
only beneficial when small amounts of bitext were available.
7
Token constraints HMM with coupled constraints CRF with coupled constraints
Lang. YHMMunion +C+L y?HMM+C+L y?CRF+C+L Y?HMMproj. +C+L Y?HMMwik. +C+L Y?HMMunion +C+L Y?CRFproj. +C+L Y?CRFwik. +C+L Y?CRFunion+C+L
bg 87.7 77.9 84.1 84.5 83.9 86.7 86.0 87.8 85.4
cs 78.3 65.4 74.9 74.8 81.1 76.9 74.7 80.3** 75.0
da 87.3 80.9 85.1 87.2 85.6 88.1 85.5 88.2* 86.0
de 87.7 81.4 83.3 85.0 89.3 86.7 84.4 90.5** 85.5
el 85.9 81.1 77.8 80.1 87.0 83.9 79.6 89.5** 79.7
es 89.1** 84.1 85.5 83.7 85.9 88.0 85.7 87.1 86.0
fr 88.4** 83.5 84.7 85.9 86.4 87.4 84.9 87.2 85.6
it 89.6 85.2 88.5 88.7 87.6 89.8 88.3 89.3 89.4
ja 72.8 47.6 54.2 43.2 76.1 70.5 44.9 81.0** 68.0
nl 83.1 78.4 82.4 82.3 84.2 83.2 83.1 85.9** 83.2
pt 89.1 84.7 87.0 86.6 88.7 88.0 87.9 91.0** 88.3
sl 82.4 69.8 78.2 78.5 81.8 80.1 79.7 82.3 80.0
sv 86.1 80.1 84.2 82.3 87.9 86.9 84.4 88.9** 85.5
tr 62.4 58.1 64.5 64.6 61.8 64.8 65.0 64.1** 65.2
zh 72.6 52.7 39.5 56.0 74.1 73.3 59.7 74.4** 73.4
avg (8) 87.2 82.0 84.2 84.5 87.0 86.8 84.9 88.8 85.4
avg 82.8 74.1 76.9 77.6 82.8 82.3 78.2 84.5 81.1
Table 2: Tagging accuracies for models with token constraints and coupled token and type constraints. All models use
cluster features (. . . +C) and are trained on large training sets each containing 500k tokens with (partial) token-level
projections (. . . +L). The best type-constrained model, trained on the larger datasets, YHMMunion +C+L, is included for
comparison. The remaining columns correspond to HMM and CRF models trained only with token constraints (y? . . .)
and with coupled token and type constraints (Y? . . .). The latter are trained using the projected dictionary (?proj.),
Wiktionary (?wik.) and the union of these dictionaries (?union), respectively. The search spaces of the models trained with
coupled constraints (Y? . . .) are each pruned with the respective tag dictionary used to derive the coupled constraints.
The observed difference between Y?CRFwik. +C+L and YHMMunion +C+L is statistically significant at p < 0.01 (**) and p < 0.015(*) according to a paired bootstrap test (Efron and Tibshirani, 1993). Significance was not assessed for avg or avg (8).
We next add monolingual cluster features to
the model with the union dictionary. This model,
YHMMunion +C, significantly outperforms all other type-
constrained models, demonstrating the utility of
word-cluster features.9 For further exploration, we
train the same model on the datasets containing 500K
tokens sampled from the target side of the parallel
data (YHMMunion +C+L); this is done to explore the effects
of large data during training. We find that training
on these datasets result in an average accuracy of
87.2% which is comparable to the 87.3% reported
for YHMMunion +C in Table 1. This shows that the different
source domain and amount of training data does not
influence the performance of the HMM significantly.
Finally, we train CRF models where we treat type
constraints as a partially observed lattice and use the
full unpruned lattice for computing the partition func-
9These are monolingual clusters. Bilingual clusters as intro-
duced in Ta?ckstro?m et al (2012) might bring additional benefits.
tion (?3.2). Due to space considerations, the results
of these experiments are not shown in table 1. We ob-
serve similar trends in these results, but on average,
accuracies are much lower compared to the type-
constrained HMM models; the CRF model with the
union dictionary along with cluster features achieves
an average accuracy of 79.3% when trained on same
data. This result is not unsurprising. First, the CRF?s
search space is fully unconstrained. Second, the dic-
tionary only provides a weak set of observation con-
straints, which do not provide sufficient information
to successfully train a discriminative model. How-
ever, as we will observe next, coupling the dictionary
constraints with token-level information solves this
problem.
4.3 Models with Token and Type Constraints
We now proceed to add token-level information,
focusing in particular on coupled token and type
8
constraints. Since it is not possible to generate
projected token constraints for our monolingual
treebanks, we train all models in this subsection
on the 500K-tokens datasets sampled from the bi-
text. As a baseline, we first train HMM and CRF
models that use only projected token constraints
(y?HMM+C+L and y?CRF+C+L). As shown in Table 2,
these models underperform the best type-level model
(YHMMunion +C+L),10 which confirms that projected to-
ken constraints are not reliable on their own. This
is in line with similar projection models previously
examined by Das and Petrov (2011).
We then study models with coupled token and type
constraints. These models use the same three dictio-
naries as used in ?4.2, but additionally couple the
derived type constraints with projected token con-
straints; see the caption of Table 2 for a list of these
models. Note that since we only allow projected tags
that are licensed by the dictionary (Step 3 of the trans-
fer, ?2.3), the actual token constraints used in these
models vary with the different dictionaries.
From Table 2, we see that coupled constraints are
superior to token constraints, when used both with
the HMM and the CRF. However, for the HMM, cou-
pled constraints do not provide any benefit over type
constraints alone, in particular when the projected
dictionary or the union dictionary is used to derive the
coupled constraints (Y?HMMproj. +C+L and Y?HMMunion +C+L).
We hypothesize that this is because these dictionar-
ies (in particular the former) have the same bias as
the token-level tag projections, so that the dictionary
is unable to correct the systematic errors in the pro-
jections (see ?2.1). Since the token constraints are
stronger than the type constraints in the coupled mod-
els, this bias may have a substantial impact. With
the Wiktionary dictionary, the difference between the
type-constrained and the coupled-constrained HMM
is negligible: YHMMunion +C+L and Y?HMMwik. +C+L both av-
erage at an accuracy of 82.8%.
The CRF model, on the other hand, is able to take
advantage of the complementary information in the
coupled constraints, provided that the dictionary is
able to filter out the systematic token-level errors.
With a dictionary derived from Wiktionary and pro-
jected token-level constraints, Y?CRFwik. +C+L performs
10To make the comparison fair vis-a-vis potential divergences
in training domains, we compare to the best type-constrained
model trained on the same 500K tokens training sets.
0 1 2 3
0
25
50
75
100
0 1 10 100 0 1 10 100 0 1 10 100 0 1 10 100
Number of token?level projections
Ta
gg
ing
 ac
cur
ac
y
Number of tags listed in Wiktionary
Figure 4: Relative influence of token and type constraints
on tagging accuracy in the Y?CRFwik. +C+L model. Word typesare categorized according to a) their number of Wiktionary
tags (0,1,2 or 3+ tags, with 0 representing no Wiktionary
entry; top-axis) and b) the number of times they are token-
constrained in the training set (divided into buckets of
0, 1-9, 10-99 and 100+ occurrences; x-axis). The boxes
summarize the accuracy distributions across languages
for each word type category as defined by a) and b). The
horizontal line in each box marks the median accuracy,
the top and bottom mark the first and third quantile, re-
spectively, while the whiskers mark the minimum and
maximum values of the accuracy distribution.
better than all the remaining models, with an average
accuracy of 88.8% across the eight Indo-European
languages available to D&P and LG&T. Averaged
over all 15 languages, its accuracy is 84.5%.
5 Further Analysis
In this section we provide a detailed analysis of the
impact of token versus type constraints and we study
the pruning and filtering mistakes resulting from in-
complete Wiktionary entries in detail. This analysis
is based on the training portion of each treebank.
5.1 Influence of Token and Type Constraints
The empirical success of the model trained with cou-
pled token and type constraints confirms that these
constraints indeed provide complementary signals.
Figure 4 provides a more detailed view of the rela-
tive benefits of each type of constraint. We observe
several interesting trends.
First, word types that occur with more token con-
straints during training are generally tagged more
accurately, regardless of whether these types occur
9
90.0
92.5
95.0
97.5
100.0
0 50 100 150 200 250
Number of corrected Wiktionary entries
Pr
un
ing
 ac
cur
ac
y
Figure 5: Average pruning accuracy (line) across lan-
guages (dots) as a function of the number of hypotheti-
cally corrected Wiktionary entries for the k most frequent
word types. For example, position 100 on the x-axis cor-
responds to manually correcting the entries for the 100
most frequent types, while position 0 corresponds to ex-
perimental conditions.
in Wiktionary. The most common scenario is for a
word type to have exactly one tag in Wiktionary and
to occur with this projected tag over 100 times in
the training set (facet 1, rightmost box). These com-
mon word types are typically tagged very accurately
across all languages.
Second, the word types that are ambiguous accord-
ing to Wiktionary (facets 2 and 3) are predominantly
frequent ones. The accuracy is typically lower for
these words compared to the unambiguous words.
However, as the number of projected token con-
straints is increased from zero to 100+ observations,
the ambiguous words are effectively disambiguated
by the token constraints. This shows the advantage
of intersecting token and type constraints.
Finally, projection generally helps for words that
are not in Wiktionary, although the accuracy for these
words never reach the accuracy of the words with
only one tag in Wiktionary. Interestingly, words that
occur with a projected tag constraint less than 100
times are tagged more accurately for types not in the
dictionary compared to ambiguous word types with
the same number of projected constraints. A possible
explanation for this is that the ambiguous words are
inherently more difficult to predict and that most of
the words that are not in Wiktionary are less common
words that tend to also be less ambiguous.
zh
tr
sv
sl
pt
nl
jait
fr
es
el
de
da
cs
bg
avg
0 25 50 75 100
Proportion of pruning errors
PRON
NOUN
DET
ADP
PRT
ADV
NUM
CONJ
ADJ
VERB
X
.
Figure 6: Prevalence of pruning mistakes per POS tag,
when pruning the inference search space with Wiktionary.
5.2 Wiktionary Pruning Mistakes
The error analysis by Li et al (2012) showed that the
tags licensed by Wiktionary are often valid. When
using Wiktionary to prune the search space of our
constrained models and to filter token-level projec-
tions, it is also important that correct tags are not
mistakenly pruned because they are missing from
Wiktionary. While the accuracy of filtering is more
difficult to study, due to the lack of a gold standard
tagging of the bitext, Figure 5 (position 0 on the x-
axis) shows that search space pruning errors are not
a major issue for most languages; on average the
pruning accuracy is almost 95%. However, for some
languages such as Chinese and Czech the correct tag
is pruned from the search space for nearly 10% of all
tokens. When using Wiktionary as a pruner, the upper
bound on accuracy for these languages is therefore
only around 90%. However, Figure 5 also shows that
with some manual effort we might be able to remedy
many of these errors. For example, by adding miss-
ing valid tags to the 250 most common word types in
the worst language, the minimum pruning accuracy
would rise above 95% from below 90%. If the same
was to be done for all of the studied languages, the
mean pruning accuracy would reach over 97%.
Figure 6 breaks down pruning errors resulting from
incorrect or incomplete Wiktionary entries across
the correct POS tags. From this we observe that,
for many languages, the pruning errors are highly
skewed towards specific tags. For example, for Czech
over 80% of the pruning errors are caused by mistak-
enly pruned pronouns.
10
6 Conclusions
We considered the problem of constructing multilin-
gual POS taggers for resource-poor languages. To
this end, we explored a number of different models
that combine token constraints with type constraints
from different sources. The best results were ob-
tained with a partially observed CRF model that ef-
fectively integrates these complementary constraints.
In an extensive empirical study, we showed that this
approach substantially improves on the state of the
art in this context. Our best model significantly out-
performed the second-best model on 10 out of 15
evaluated languages, when trained on identical data
sets, with an insignificant difference on 3 languages.
Compared to the prior state of the art (Li et al, 2012),
we observed a relative reduction in error by 25%,
averaged over the eight languages common to our
studies.
Acknowledgments
We thank Alexander Rush for help with the hyper-
graph framework that was used to implement our
models and Klaus Macherey for help with the bi-
text extraction. This work benefited from many dis-
cussions with Yoav Goldberg, Keith Hall, Kuzman
Ganchev and Hao Zhang. We also thank the editor
and the three anonymous reviewers for their valuable
feedback. The first author is grateful for the financial
support from the Swedish National Graduate School
of Language Technology (GSLT).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a Treebank for French. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, John
DeNero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL-HLT.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Stanley F Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.
1977. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Brad Efron and Robert J. Tibshirani. 1993. An Introduc-
tion to the Bootstrap. Chapman & Hall, New York, NY,
USA.
Victoria Fossum and Steven Abney. 2005. Automatically
inducing a part-of-speech tagger by projecting from
multiple source languages across aligned corpora. In
Proceedings of IJCNLP.
Dan Garrette and Jason Baldridge. 2012. Type-supervised
hidden markov models for part-of-speech tagging with
incomplete tag dictionaries. In Proceedings of EMNLP-
CoNLL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of ACL-HLT.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2).
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. JAIR, 36.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
11
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, III, and Mark Johnson. 2002.
Parsing the wall street journal using a lexical-functional
grammar and discriminative estimation techniques. In
Proceedings of ACL.
Noah Smith and Jason Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data. In
Proceedings of ACL.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-English languages.
In Proceedings of HLT-EMNLP.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
12
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 51?59,
Uppsala, July 2010.
What?s Great and What?s Not: Learning to Classify the Scope of Negation
for Improved Sentiment Analysis
Isaac G. Councill
Google, Inc.
76 Ninth Avenue
New York, NY 10011
icouncill@google.com
Ryan McDonald
Google, Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Leonid Velikovich
Google, Inc.
76 Ninth Avenue
New York, NY 10011
leonidv@google.com
Abstract
Automatic detection of linguistic negation
in free text is a critical need for many text
processing applications, including senti-
ment analysis. This paper presents a nega-
tion detection system based on a condi-
tional random field modeled using fea-
tures from an English dependency parser.
The scope of negation detection is limited
to explicit rather than implied negations
within single sentences. A new negation
corpus is presented that was constructed
for the domain of English product reviews
obtained from the open web, and the pro-
posed negation extraction system is eval-
uated against the reviews corpus as well
as the standard BioScope negation corpus,
achieving 80.0% and 75.5% F1 scores, re-
spectively. The impact of accurate nega-
tion detection on a state-of-the-art senti-
ment analysis system is also reported.
1 Introduction
The automatic detection of the scope of linguistic
negation is a problem encountered in wide variety
of document understanding tasks, including but
not limited to medical data mining, general fact or
relation extraction, question answering, and senti-
ment analysis. This paper describes an approach
to negation scope detection in the context of sen-
timent analysis, particularly with respect to sen-
timent expressed in online reviews. The canoni-
cal need for proper negation detection in sentiment
analysis can be expressed as the fundamental dif-
ference in semantics inherent in the phrases, ?this
is great,? versus, ?this is not great.? Unfortunately,
expressions of negation are not always so syntac-
tically simple.
Linguistic negation is a complex topic: there
are many forms of negation, ranging from the use
of explicit cues such as ?no? or ?not? to much
more subtle linguistic patterns. At the highest
structural level, negations may occur in two forms
(Givo?n, 1993): morphological negations, where
word roots are modified with a negating prefix
(e.g., ?dis-?, ?non-?, or ?un-?) or suffix (e.g., ?-
less?), and syntactic negation, where clauses are
negated using explicitly negating words or other
syntactic patterns that imply negative semantics.
For the purposes of negation scope detection, only
syntactic negations are of interest, since the scope
of any morphological negation is restricted to an
individual word. Morphological negations are
very important when constructing lexicons, which
is a separate but related research topic.
Tottie (1991) presents a comprehensive taxon-
omy of clausal English negations, where each
form represents unique challenges for a negation
scope detection system. The top-level negation
categories ? denials, rejections, imperatives, ques-
tions, supports, and repetitions ? can be described
as follows:
? Denials are the most common form and are
typically unambiguous negations of a partic-
ular clause, such as, ?There is no question
that the service at this restaurant is excellent,?
or, ?The audio system on this television is not
very good, but the picture is amazing.?
? Rejections often occur in discourse, where
one participant rejects an offer or sugges-
tion of another, e.g., ?Can I get you any-
thing else? No.? However, rejections may ap-
pear in expository text where a writer explic-
itly rejects a previous supposition or expec-
tation, for instance, ?Given the poor reputa-
tion of the manufacturer, I expected to be dis-
appointed with the device. This was not the
case.?
? Imperatives involve directing an audience
51
away from a particular action, e.g., ?Do not
neglect to order their delicious garlic bread.?
? Questions, rhetorical or otherwise, can indi-
cate negations often in the context of surprise
or bewilderment. For example, a reviewer of
a desk phone may write, ?Why couldn?t they
include a decent speaker in this phone??, im-
plying that the phone being reviewed does not
have a decent speaker.
? Supports and Repetitions are used to ex-
press agreement and add emphasis or clar-
ity, respectively, and each involve multiple
expressions of negation. For the purpose of
negation scope detection, each instance of
negation in a support or repetition can be iso-
lated and treated as an independent denial or
imperative.
Tottie also distinguishes between intersenten-
tial and sentential negation. In the case of inter-
sentential negation, the language used in one sen-
tence may explicitly negate a proposition or impli-
cation found in another sentence. Rejections and
supports are common examples of intersentential
negation. Sentential negation, or negations within
the scope of a single sentence, are much more
frequent; thus sentential denials, imperatives, and
questions are the primary focus of the work pre-
sented here.
The goal of the present work is to develop a sys-
tem that is robust to differences in the intended
scope of negation introduced by the syntactic and
lexical features in each negation category. In par-
ticular, as the larger context of this research in-
volves sentiment analysis, it is desirable to con-
struct a negation system that can correctly identify
the presence or absence of negation in spans of text
that are expressions of sentiment. It so follows that
in developing a solution for the specific case of the
negation of sentiment, the proposed system is also
effective at solving the general case of negation
scope identification.
This rest of this paper is organized as follows.
?2 presents related work on the topic of auto-
matic detection of the scope of linguistic nega-
tions. The annotated corpora used to evaluate
the proposed negation scope identification method
are presented in ?3, including a new data set de-
veloped for the purpose of identifying negation
scopes in the context of online reviews. ?4 de-
scribes the proposed negation scope detection sys-
tem. The novel system is evaluated in ?5 in
terms of raw results on the annotated negation cor-
pora as well as the performance improvement on
sentiment classification achieved by incorporating
the negation system in a state-of-the-art sentiment
analysis pipeline. Lessons learned and future di-
rections are discussed in ?6.
2 Related work
Negation and its scope in the context of senti-
ment analysis has been studied in the past (Moila-
nen and Pulman, 2007). In this work we focus
on explicit negation mentions, also called func-
tional negation by Choi and Cardie (2008). How-
ever, others have studied various forms of nega-
tion within the domain of sentiment analysis, in-
cluding work on content negators, which typi-
cally are verbs such as ?hampered?, ?lacked?, ?de-
nied?, etc. (Moilanen and Pulman, 2007; Choi
and Cardie, 2008). A recent study by Danescu-
Niculescu-Mizil et al (2009) looked at the prob-
lem of finding downward-entailing operators that
include a wider range of lexical items, includ-
ing soft negators such as the adverbs ?rarely? and
?hardly?.
With the absence of a general purpose corpus
annotating the precise scope of negation in sen-
timent corpora, many studies incorporate nega-
tion terms through heuristics or soft-constraints in
statistical models. In the work of Wilson et al
(2005), a supervised polarity classifier is trained
with a set of negation features derived from a
list of cue words and a small window around
them in the text. Choi and Cardie (2008) com-
bine different kinds of negators with lexical polar-
ity items through various compositional semantic
models, both heuristic and machine learned, to im-
prove phrasal sentiment analysis. In that work the
scope of negation was either left undefined or de-
termined through surface level syntactic patterns
similar to the syntactic patterns from Moilanen
and Pulman (2007). A recent study by Nakagawa
et al (2010) developed an semi-supervised model
for sub-sentential sentiment analysis that predicts
polarity based on the interactions between nodes
in dependency graphs, which potentially can in-
duce the scope of negation.
As mentioned earlier, the goal of this work is to
define a system that can identify exactly the scope
of negation in free text, which requires a robust-
ness to the wide variation of negation expression,
52
both syntactic and lexical. Thus, this work is com-
plimentary to those mentioned above in that we
are measuring not only whether negation detec-
tion is useful for sentiment, but to what extent we
can determine its exact scope in the text. Towards
this end in we describe both an annotated nega-
tion span corpus as well as a negation span detec-
tor that is trained on the corpus. The span detec-
tor is based on conditional random fields (CRFs)
(Lafferty, McCallum, and Pereira, 2001), which is
a structured prediction learning framework com-
mon in sub-sentential natural language process-
ing tasks, including sentiment analysis (Choi and
Cardie, 2007; McDonald et al, 2007)
The approach presented here resembles work by
Morante and Daelemans (2009), who used IGTree
to predict negation cues and a CRF metalearner
that combined input from k-nearest neighbor clas-
sification, a support vector machine, and another
underlying CRF to predict the scope of nega-
tions within the BioScope corpus. However, our
work represents a simplified approach that re-
places machine-learned cue prediction with a lex-
icon of explicit negation cues, and uses only a sin-
gle CRF to predict negation scopes, with a more
comprehensive model that includes features from
a dependency parser.
3 Data sets
One of the only freely available resources for eval-
uating negation detection performance is the Bio-
Scope corpus (Vincze et al, 2008), which consists
of annotated clinical radiology reports, biological
full papers, and biological abstracts. Annotations
in BioScope consist of labeled negation and spec-
ulation cues along with the boundary of their as-
sociated text scopes. Each cue is associated with
exactly one scope, and the cue itself is considered
to be part of its own scope. Traditionally, negation
detection systems have encountered the most dif-
ficulty in parsing the full papers subcorpus, which
contains nine papers and a total of 2670 sentences,
and so the BioScope full papers were held out as a
benchmark for the methods presented here.
The work described in this paper was part of a
larger research effort to improve the accuracy of
sentiment analysis in online reviews, and it was
determined that the intended domain of applica-
tion would likely contain language patterns that
are significantly distinct from patterns common in
the text of professional biomedical writings. Cor-
rect analysis of reviews generated by web users
requires robustness in the face of ungrammatical
sentences and misspelling, which are both exceed-
ingly rare in BioScope. Therefore, a novel cor-
pus was developed containing the text of entire
reviews, annotated according to spans of negated
text.
A sample of 268 product reviews were obtained
by randomly sampling reviews from Google Prod-
uct Search1 and checking for the presence of nega-
tion. The annotated corpus contains 2111 sen-
tences in total, with 679 sentences determined to
contain negation. Each review was manually an-
notated with the scope of negation by a single per-
son, after achieving inter-annotator agreement of
91% with a second person on a smaller subset of
20 reviews containing negation. Inter-annotator
agreement was calculated using a strict exact span
criteria where both the existence and the left/right
boundaries of a negation span were required to
match. Hereafter the reviews data set will be re-
ferred to as the Product Reviews corpus.
The Product Reviews corpus was annotated ac-
cording to the following instructions:
1. Negation cues: Negation cues (e.g., the
words ?never?, ?no?, or ?not? in it?s various
forms) are not included the negation scope.
For example, in the sentence, ?It was not X?
only ?X? is annotated as the negation span.
2. General Principles: Annotate the minimal
span of a negation covering only the portion
of the text being negated semantically. When
in doubt, prefer simplicity.
3. Noun phrases: Typically entire noun
phrases are annotated as within the scope
of negation if a noun within the phrase is
negated. For example, in the sentence, ?This
was not a review? the string ?a review? is an-
notated. This is also true for more complex
noun phrases, e.g., ?This was not a review
of a movie that I watched? should be anno-
tated with the span ?a review of a movie that
I watched?.
4. Adjectives in noun phrases: Do not anno-
tate an entire noun phrase if an adjective is all
that is being negated - consider the negation
of each term separately. For instance, ?Not
1http://www.google.com/products/
53
top-drawer cinema, but still good...?: ?top-
drawer? is negated, but ?cinema? is not, since
it is still cinema, just not ?top-drawer?.
5. Adverbs/Adjective phrases:
(a) Case 1: Adverbial comparatives like
?very,? ?really,? ?less,? ?more?, etc., an-
notate the entire adjective phrase, e.g.,
?It was not very good? should be anno-
tated with the span ?very good?.
(b) Case 2: If only the adverb is directly
negated, only annotate the adverb it-
self. E.g., ?Not only was it great?, or
?Not quite as great?: in both cases the
subject still ?is great?, so just ?only?
and ?quite? should be annotated, respec-
tively. However, there are cases where
the intended scope of adverbial negation
is greater, e.g., the adverb phrase ?just a
small part? in ?Tony was on stage for the
entire play. It was not just a small part?.
(c) Case 3: ?as good as X?. Try to identify
the intended scope, but typically the en-
tire phrase should be annotated, e.g., ?It
was not as good as I remember?. Note
that Case 2 and 3 can be intermixed,
e.g., ?Not quite as good as I remem-
ber?, in this case follow 2 and just anno-
tate the adverb ?quite?, since it was still
partly ?as good as I remember?, just not
entirely.
6. Verb Phrases: If a verb is directly negated,
annotate the entire verb phrase as negated,
e.g., ?appear to be red? would be marked in
?It did not appear to be red?.
For the case of verbs (or adverbs), we made no
special instructions on how to handle verbs that
are content negators. For example, for the sen-
tence ?I can?t deny it was good?, the entire verb
phrase ?deny it was good? would be marked as the
scope of ?can?t?. Ideally annotators would also
mark the scope of the verb ?deny?, effectively can-
celing the scope of negation entirely over the ad-
jective ?good?. As mentioned previously, there are
a wide variety of verbs and adverbs that play such
a role and recent studies have investigated meth-
ods for identifying them (Choi and Cardie, 2008;
Danescu-Niculescu-Mizil et al, 2009). We leave
the identification of the scope of such lexical items
hardly lack lacking lacks
neither nor never no
nobody none nothing nowhere
not n?t aint cant
cannot darent dont doesnt
didnt hadnt hasnt havnt
havent isnt mightnt mustnt
neednt oughtnt shant shouldnt
wasnt wouldnt without
Table 1: Lexicon of explicit negation cues.
and their interaction with explicit negation as fu-
ture work.
The Product Reviews corpus is different from
BioScope in several ways. First, BioScope ignores
direct adverb negation, such that neither the nega-
tion cue nor the negation scope in the the phrase,
?not only,? is annotated in BioScope. Second,
BioScope annotations always include entire adjec-
tive phrases as negated, where our method distin-
guishes between the negation of adjectives and ad-
jective targets. Third, BioScope includes nega-
tion cues within their negation scopes, whereas
our corpus separates the two.
4 System description
As the present work focuses on explicit negations,
the choice was made to develop a lexicon of ex-
plicit negation cues to serve as primary indicators
of the presence of negation. Klima (1964) was the
first to identify negation words using a statistics-
driven approach, by analyzing word co-occurrence
with n-grams that are cues for the presence of
negation, such as ?either? and ?at all?. Klima?s
lexicon served as a starting point for the present
work, and was further refined through the inclu-
sion of common misspellings of negation cues and
the manual addition of select cues from the ?Neg?
and ?Negate? tags of the General Inquirer (Stone
et al, 1966). The final list of cues used for the
evaluations in ?5 is presented in Table 1. The lex-
icon serves as a reliable signal to detect the pres-
ence of explicit negations, but provides no means
of inferring the scope of negation. For scope de-
tection, additional signals derived from surface
and dependency level syntactic structure are em-
ployed.
The negation scope detection system is built as
an individual annotator within a larger annotation
pipeline. The negation annotator relies on two dis-
54
tinct upstream annotators for 1) sentence boundary
annotations, derived from a rule-based sentence
boundary extractor and 2) token annotations from
a dependency parser. The dependency parser is an
implementation of the parsing systems described
in Nivre and Scholz (2004) and Nivre et al (2007).
Each annotator marks the character offsets for the
begin and end positions of individual annotation
ranges within documents, and makes the annota-
tions available to downstream processes.
The dependency annotator controls multiple
lower-level NLP routines, including tokenization
and part of speech (POS) tagging in addition to
parsing sentence level dependency structure. The
output that is kept for downstream use includes
only POS and dependency relations for each to-
ken. The tokenization performed at this stage is re-
cycled when learning to identify negation scopes.
The feature space of the learning problem ad-
heres to the dimensions presented in Table 2,
and negation scopes are modeled using a first or-
der linear-chain conditional random field (CRF)2,
with a label set of size two indicating whether a
token is within or outside of a negation span. The
features include the lowercased token string, token
POS, token-wise distance from explicit negation
cues, POS information from dependency heads,
and dependency distance from dependency heads
to explicit negation cues. Only unigram features
are employed, but each unigram feature vector is
expanded to include bigram and trigram represen-
tations derived from the current token in conjunc-
tion with the prior and subsequent tokens.
The distance measures can be explained as fol-
lows. Token-wise distance is simply the number
of tokens from one token to another, in the order
they appear in a sentence. Dependency distance is
more involved, and is calculated as the minimum
number of edges that must be traversed in a de-
pendency tree to move from one node (or token)
to another. Each edge is considered to be bidi-
rectional. The CRF implementation used in our
system employs categorical features, so both inte-
ger distances are treated as encodings rather than
continuous values. The number 0 implies that a
token is, or is part of, an explicit negation cue.
The numbers 1-4 encode step-wise distance from
a negation cue, and the number 5 is used to jointly
encode the concepts of ?far away? and ?not appli-
cable?. The maximum integer distance is 5, which
2Implemented with CRF++: http://crfpp.sourceforge.net/
Feature Description
Word The lowercased token string.
POS The part of speech of a token.
Right Dist. The linear token-wise distance to
the nearest explicit negation cue
to the right of a token.
Left Dist. The linear token-wise distance to
the nearest explicit negation cue
to the left of a token.
Dep1 POS The part of speech of the the first
order dependency of a token.
Dep1 Dist. The minimum number of depen-
dency relations that must be tra-
versed to from the first order de-
pendency head of a token to an
explicit negation cue.
Dep2 POS The part of speech of the the sec-
ond order dependency of a token.
Dep2 Dist. The minimum number of depen-
dency relations that must be tra-
versed to from the second order
dependency head of a token to an
explicit negation cue.
Table 2: Token features used in the conditional
random field model for negation.
was determined empirically.
The negation annotator vectorizes the tokens
generated in the dependency parser annotator and
can be configured to write token vectors to an out-
put stream (training mode) or load a previously
learned conditional random field model and ap-
ply it by sending the token vectors directly to the
CRF decoder (testing mode). The output annota-
tions include document-level negation span ranges
as well as sentence-level token ranges that include
the CRF output probability vector, as well as the
alpha and beta vectors.
5 Results
The negation scope detection system was evalu-
ated against the data sets described in ?3. The
negation CRF model was trained and tested
against the Product Reviews and BioScope biolog-
ical full papers corpora. Subsequently, the practi-
cal effect of robust negation detection was mea-
sured in the context of a state-of-the-art sentiment
analysis system.
55
Corpus Prec. Recall F1 PCS
Reviews 81.9 78.2 80.0 39.8
BioScope 80.8 70.8 75.5 53.7
Table 3: Results of negation scope detection.
5.1 Negation Scope Detection
To measure scope detection performance, the
automatically generated results were compared
against each set of human-annotated negation cor-
pora in a token-wise fashion. That is, precision
and recall were calculated as a function of the pre-
dicted versus actual class of each text token. To-
kens made up purely of punctuation were consid-
ered to be arbitrary artifacts of a particular tok-
enization scheme, and thus were excluded from
the results. In keeping with the evaluation pre-
sented by Morante and Daelemans (2009), the
number of perfectly identified negation scopes is
measured separately as the percentage of correct
scopes (PCS). The PCS metric is calculated as the
number of correct spans divided by the number of
true spans, making it a recall measure.
Only binary classification results were consid-
ered (whether a token is of class ?negated? or ?not
negated?) even though the probabilistic nature of
conditional random fields makes it possible to ex-
press uncertainty in terms of soft classification
scores in the range 0 to 1. Correct predictions of
the absence of negation are excluded from the re-
sults, so the reported measurements only take into
account correct prediction of negation and incor-
rect predictions of either class.
The negation scope detection results for both
the Product Reviews and BioScope corpora are
presented in Table 3. The results on the Product
Reviews corpus are based on seven-fold cross vali-
dation, and the BioScope results are based on five-
fold cross validation, since the BioScope data set
is smaller. For each fold, the number of sentences
with and without negation were balanced in both
training and test sets.
The system was designed primarily to support
the case of negation scope detection in the open
web, and no special considerations were taken to
improve performance on the BioScope corpus. In
particular, the negation cue lexicon presented in
Table 1 was not altered in any way, even though
BioScope contains additional cues such as ?rather
than? and ?instead of?. This had a noticeable ef-
fect on on recall in BioScope, although in several
Condition Prec. Recall F1 PCS
BioScope,
trained on
Reviews
72.2 42.1 53.5 52.2
Reviews,
trained on
Bioscope
58.8 68.8 63.4 45.7
Table 4: Results for cross-trained negation mod-
els. This shows the results for BioScope with
a model trained on the Product Reviews corpus,
and the results for Product Reviews with a model
trained on the BioScope corpus.
cases the CRF was still able to learn the missing
cues indirectly through lexical features.
In general, the system performed significantly
better on the Product Reviews corpus than on Bio-
Scope, although the performance on BioScope full
papers is state-of-the-art. This can be accounted
for at least partially by the differences in the nega-
tion cue lexicons. However, significantly more
negation scopes were perfectly identified in Bio-
Scope, with a 23% improvement in the PCS metric
over the Product Reviews corpus.
The best reported performance to date on the
BioScope full papers corpus was presented by
Morante and Daelemans (2009), who achieved an
F1 score of 70.9 with predicted negation signals,
and an F1 score of 84.7 by feeding the manually
annotated negation cues to their scope finding sys-
tem. The system presented here compares favor-
ably to Morante and Daelemans? fully automatic
results, achieving an F1 score of 75.5, which is
a 15.8% reduction in error, although the results
are significantly worse than what was achieved via
perfect negation cue information.
5.2 Cross training
The degree to which models trained on each
corpus generalized to each other was also mea-
sured. For this experiment, each of the two mod-
els trained using the methods described in ?5.1
was evaluated against its non-corresponding cor-
pus, such that the BioScope-trained corpus was
evaluated against all of Product Reviews, and the
model derived from Product Reviews was evalu-
ated against all of BioScope.
The cross training results are presented in Ta-
ble 4. Performance is generally much worse, as
expected. Recall drops substantially in BioScope,
56
which is almost certainly due to the fact that not
only are several of the BioScope negation cues
missing from the cue lexicon, but the CRF model
has not had the opportunity to learn from the lex-
ical features in BioScope. The precision in Bio-
Scope remains fairly high, and the percentage of
perfectly labeled scopes remains almost the same.
For Product Reviews, an opposing trend can be
seen: precision drops significantly but recall re-
mains fairly high. This seems to indicate that the
scope boundaries in the Product Reviews corpus
are generally harder to predict. The percentage
of perfectly labeled scopes actually increases for
Product Reviews, which could also indicate that
scope boundaries are less noisy in BioScope.
5.3 Effect on sentiment classification
In addition to measuring the raw performance of
the negation scope detection system, an experi-
ment was conducted to measure the effect of the
final negation system within the context of a larger
sentiment analysis system.
The negation system was built into a senti-
ment analysis pipeline consisting of the following
stages:
1. Sentence boundary detection.
2. Sentiment detection.
3. Negation scope detection, applying the sys-
tem described in ?4.
4. Sentence sentiment scoring.
The sentiment detection system in stage 2 finds
and scores mentions of n-grams found in a large
lexicon of sentiment terms and phrases. The sen-
timent lexicon is based on recent work using label
propagation over a very large distributional simi-
larity graph derived from the web (Velikovich et
al., 2010), and applies positive or negative scores
to terms such as ?good?, ?bad?, or ?just what the
doctor ordered?. The sentence scoring system in
stage 4 then determines whether any scored senti-
ment terms fall within the scope of a negation, and
flips the sign of the sentiment score for all negated
sentiment terms. The scoring system then sums all
sentiment scores within each sentence and com-
putes overall sentence sentiment scores.
A sample of English-language online reviews
was collected, containing a total of 1135 sen-
tences. Human raters were presented with consec-
utive sentences and asked to classify each sentence
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
isi
on
 
 
With Negation Detection
Without Negation Detection
Figure 1: Precision-recall curve showing the effect
of negation detection on positive sentiment predic-
tion.
as expressing one of the following types of sen-
timent: 1) positive, 2) negative, 3) neutral, or 4)
mixed positive and negative. Each sentence was
reviewed independently by five separate raters,
and final sentence classification was determined
by consensus. Of the original 1135 sentences 216,
or 19%, were found to contain negations.
The effect of the negation system on sentiment
classification was evaluated on the smaller subset
of 216 sentences in order to more precisely mea-
sure the impact of negation detection. The smaller
negation subset contained 73 sentences classified
as positive, 114 classified as negative, 12 classified
as neutral, and 17 classified as mixed. The num-
ber of sentences classified as neutral or mixed was
too small for a useful performance measurement,
so only sentences classified as positive or negative
sentences were considered.
Figures 1 and 2 show the precision-recall curves
for sentences predicted by the sentiment analysis
system to be positive and negative, respectively.
The curves indicate relatively low performance,
which is consistent with the fact that sentiment
polarity detection is notoriously difficult on sen-
tences with negations. The solid lines show per-
formance with the negation scope detection sys-
tem in place, and the dashed lines show perfor-
mance with no negation detection at all. From
the figures, a significant improvement is immedi-
ately apparent at all recall levels. It can also be
inferred from the figures that the sentiment analy-
sis system is significantly biased towards positive
predictions: even though there were significantly
more sentences classified by human raters as neg-
57
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Recall
Pr
ec
isi
on
 
 
With Negation Detection
Without Negation Detection
Figure 2: Precision-recall curve showing the ef-
fect of negation detection on negative sentiment
prediction.
Metric w/o Neg. w/ Neg. % Improv.
Positive Sentiment
Prec. 44.0 64.1 35.9
Recall 54.8 63.7 20.0
F1 48.8 63.9 29.5
Negative Sentiment
Prec. 68.6 83.3 46.8
Recall 21.1 26.3 6.6
F1 32.3 40.0 11.4
Table 5: Sentiment classification results, show-
ing the percentage improvement obtained from in-
cluding negation scope detection (w/ Neg.) over
results obtained without including negation scope
detection (w/o Neg.).
ative, the number of data points for positive pre-
dictions far exceeds the number of negative pre-
dictions, with or without negation detection.
The overall results are presented in Table 5, sep-
arated by positive and negative class predictions.
As expected, performance is improved dramati-
cally by introducing negation scope detection. The
precision of positive sentiment predictions sees the
largest improvement, largely due to the inherent
bias in the sentiment scoring algorithm. F1 scores
for positive and negative sentiment predictions im-
prove by 29.5% and 11.4%, respectively.
6 Conclusions
This paper presents a system for identifying the
scope of negation using shallow parsing, by means
of a conditional random field model informed by
a dependency parser. Results were presented on
the standard BioScope corpus that compare favor-
ably to the best results reported to date, using a
software stack that is significantly simpler than the
best-performing approach.
A new data set was presented that targets the
domain of online product reviews. The product re-
view corpus represents a departure from the stan-
dard BioScope corpus in two distinct dimensions:
the reviews corpus contains diverse common and
vernacular language patterns rather than profes-
sional prose, and also presents a divergent method
for annotating negations in text. Cross-training by
learning a model on one corpus and testing on an-
other suggests that scope boundary detection in the
product reviews corpus may be a more difficult
learning problem, although the method used to an-
notate the reviews corpus may result in a more
consistent representation of the problem.
Finally, the negation system was built into a
state-of-the-art sentiment analysis system in order
to measure the practical impact of accurate nega-
tion scope detection, with dramatic results. The
negation system improved the precision of positive
sentiment polarity detection by 35.9% and nega-
tive sentiment polarity detection by 46.8%. Error
reduction on the recall measure was less dramatic,
but still significant, showing improved recall for
positive polarity of 20.0% and improved recall for
negative polarity of 6.6%.
Future research will include treatment of im-
plicit negation cues, ideally by learning to predict
the presence of implicit negation using a prob-
abilistic model that generates meaningful confi-
dence scores. A related topic to be addressed
is the automatic detection of sarcasm, which is
an important problem for proper sentiment anal-
ysis, particularly in open web domains where lan-
guage is vernacular. Additionally, we would like
to tackle the problem of inter-sentential negations,
which could involve a natural extension of nega-
tion scope detection through co-reference resolu-
tion, such that negated pronouns trigger negations
in text surrounding their pronoun antecedents.
Acknowledgments
The authors would like to thank Andrew Hogue
and Kerry Hannan for useful discussions regarding
this work.
58
References
Yejin Choi and Claire Cardie. 2007. Structured Lo-
cal Training and Biased Potential Functions for Con-
ditional Random Fields with Application to Coref-
erence Resolution. Proceedings of The 9th Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, ACL,
Rochester, NY.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. Proceedings of
the Conference on Empirical Methods on Natural
Language Processing. ACL, Honolulu, HI.
Cristian Danescu-Niculescu-Mizil, Lillian Lee, and
Richard Ducott. 2008. Without a ?doubt?? Un-
supervised discovery of downward-entailing opera-
tors. Proceedings of The 10th Annual Conference of
the North American Chapter of the Association for
Computational Linguistics. ACL, Boulder, CO.
Talmy Givo?n. 1993. English Grammer: A Function-
Based Introduction. Benjamins, Amsterdam, NL.
Edward S. Klima. 1964. Negation in English. Read-
ings in the Philosophy of Language. Ed. J. A. Fodor
and J. J. Katz. Prentice Hall, Englewood Cliffs, NJ:
246-323.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random elds: Prob-
abilistic models for segmenting and labeling se-
quence data. Proceedings of the International Con-
ference on Machine Learning. Morgan Kaufmann,
Williamstown, MA.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured Models for
Fine-to-Coarse Sentiment Analysis. Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics. Prague, Czech Republic.
Karo Moilanen and Stephen Pulman 2007. Sentiment
Composition. Proceedings of the Recent Advances
in Natural Language Processing International Con-
ference Borovets, Bulgaria
Roser Morante and Walter Daelemans. 2009. A
metalearning approach to processing the scope of
negation. Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL). ACM, Boulder, CO.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency Tree-based Sentiment Classifi-
cation using CRFs with Hidden Variables. Proceed-
ings of The 11th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics ACL, Los Angeles, CA.
Joakim Nivre and Mario Scholz. 2004. Deterministic
Dependency Parsing of English Text. Proceedings
of the 20th International Conference on Computa-
tional Linguistics. ACM, Geneva, Switzerland.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gulsen Eryigit Sandra Kubler, Svetoslav
Marinov and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing Natural Language Engineering
13(02):95?135
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press, Cambridge, MA.
Gunnel Tottie. 1991. Negation in English Speech
and Writing: A Study in Variation Academic, San
Diego, CA.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viabil-
ity of web-derived polarity lexicons. Proceedings of
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. ACL, Los Angeles, CA.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing Van-
couver, Canada.
59
