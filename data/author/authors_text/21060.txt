Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 1?10,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
One Step Closer to Automatic Evaluation
of Text Simplification Systems
Sanja
?
Stajner
1
and Ruslan Mitkov
1
and Horacio Saggion
2
1
Research Group in Computational Linguistics, University of Wolverhampton, UK
2
TALN Research Group, Universitat Pompeu Fabra, Spain
S.Stajner@wlv.ac.uk, R.Mitkov@wlv.ac.uk, horacio.saggion@upf.edu
Abstract
This study explores the possibility of re-
placing the costly and time-consuming
human evaluation of the grammaticality
and meaning preservation of the output
of text simplification (TS) systems with
some automatic measures. The focus is on
six widely used machine translation (MT)
evaluation metrics and their correlation
with human judgements of grammatical-
ity and meaning preservation in text snip-
pets. As the results show a significant cor-
relation between them, we go further and
try to classify simplified sentences into:
(1) those which are acceptable; (2) those
which need minimal post-editing; and (3)
those which should be discarded. The pre-
liminary results, reported in this paper, are
promising.
1 Introduction
Lexically and syntactically complex sentences can
be difficult to understand for non-native speak-
ers (Petersen and Ostendorf, 2007; Alu??sio et
al., 2008b), and for people with language impair-
ments, e.g. people diagnosed with aphasia (Car-
roll et al., 1999; Devlin, 1999), autism spectrum
disorder (
?
Stajner et al., 2012; Martos et al., 2012),
dyslexia (Rello, 2012), congenital deafness (Inui
et al., 2003), and intellectual disability (Feng,
2009). At the same time, long and complex sen-
tences are also a stumbling block for many NLP
tasks and applications such as parsing, machine
translation, information retrieval, and summarisa-
tion (Chandrasekar et al., 1996). This justifies the
need for Text Simplification (TS) systems which
would convert such sentences into their simpler
and easier-to-read variants, while at the same time
preserving the original meaning.
So far, TS systems have been developed for En-
glish (Siddharthan, 2006; Zhu et al., 2010; Wood-
send and Lapata, 2011a; Coster and Kauchak,
2011; Wubben et al., 2012), Spanish (Saggion et
al., 2011), and Portuguese (Alu??sio et al., 2008a),
with recent attempts at Basque (Aranzabe et al.,
2012), Swedish (Rybing et al., 2010), Dutch
(Ruiter et al., 2010), and Italian (Barlacchi and
Tonelli, 2013).
Usually, TS systems are either evaluated for: (1)
the quality of the generated output, or (2) the effec-
tiveness/usefulness of such simplification on read-
ing speed and comprehension of the target popula-
tion. For the purpose of this study we focused only
on the former. The quality of the output generated
by TS systems is commonly evaluated by using
a combination of readability metrics (measuring
the degree of simplification) and human assess-
ment (measuring the grammaticality and meaning
preservation). Despite the noticeable similarity
between evaluation of the fluency and adequacy of
a machine translation (MT) output, and evaluation
of grammaticality and meaning preservation of a
TS system output, there have been no works ex-
ploring whether any of the MT evaluation metrics
are well correlated with the latter, and could thus
replace the time-consuming human assessment.
The contributions of the present work are the
following:
? It is the first study to explore the possibility of
replacing human assessment of the quality of
TS system output with automatic evaluation.
? It is the first study to investigate the correla-
tion of human assessment of TS system out-
put with MT evaluation metrics.
? It proposes a decision-making procedure for
the classification of simplified sentences into:
(1) those which are acceptable; (2) those
which need further post-editing; and (3) those
which should be discarded.
1
2 Related Work
The output of the TS system proposed by Sid-
dharthan (2006) was rated for grammaticality and
meaning preservation by three human evaluators.
Similarly, Drndarevic et al. (2013) evaluated the
grammaticality and the meaning preservation of
automatically simplified Spanish sentences on a
Likert scale with the help of twenty-five human
annotators. Additionally, the authors used seven
readability metrics to assess the degree of simplifi-
cation. Woodsend and Lapata (2011b), and Glava?s
and
?
Stajner (2013) used human annotators? rat-
ings for evaluating simplification, meaning preser-
vation, and grammaticality, while additionally ap-
plying several readability metrics for evaluating
complexity reduction in entire texts.
Another set of studies approached TS as an MT
task translating from ?original? to ?simplified?
language, e.g. (Specia, 2010; Woodsend and Lap-
ata, 2011a; Zhu et al., 2010). In this case, the qual-
ity of the output generated by the system was eval-
uated using several standard MT evaluation met-
rics: BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), and TERp (Snover et al., 2009).
3 Methodology
All experiments were conducted on a freely avail-
able sentence-level dataset
1
, fully described in
(Glava?s and
?
Stajner, 2013), and the two datasets
we derived from it. The original dataset and the
instructions for the human assessment are given in
the next two subsections. Section 3.3 explains how
we derived two additional datasets from the origi-
nal one, and to what end. Section 3.4 describes the
automatic MT evaluation metrics used as features
in correlation and classification experiments; Sec-
tion 3.5 presents the main goals of the study; and
Section 3.6 describes the conducted experiments.
3.1 Original dataset
The dataset contains 280 pairs of original sen-
tences and their corresponding simplified versions
annotated by humans for grammaticality, meaning
preservation, and simplicity of the simplified ver-
sion. We used all sentence pairs, focusing only on
four out of eight available features: (1) the original
text, (2) the simplified text, (3) the grammaticality
score, and (4) the score for meaning preservation.
2
1
http://takelab.fer.hr/data/evsimplify/
2
The other four features contain the pairID, groupID, the
method with which the simplification was obtained, and the
Category weighted ? Pearson MAE
Grammaticality 0.68 0.77 0.18
Meaning 0.53 0.67 0.37
Simplicity 0.54 0.60 0.28
Table 1: IAA from (Glava?s and
?
Stajner, 2013)
The simplified versions of original sentences
were obtained by using four different simplifi-
cation methods: baseline, sentence-wise, event-
wise, and pronominal anaphora. The baseline re-
tains only the main clause of a sentence, and dis-
cards all subordinate clauses, based on the out-
put of the Stanford constituency parser (Klein and
Manning, 2003). Sentence-wise simplification
eliminates all those tokens in the original sentence
that do not belong to any of the extracted factual
event mentions, while the event-wise simplifica-
tion transforms each factual event mention into a
separate sentence of the output. The last simplifi-
cation scheme (pronominal anaphora) additionally
employs pronominal anaphora resolution on top of
the event-wise simplification scheme.
3
3.2 Human Assessment
Human assessors were asked to score the given
sentence pairs (or text snippets in the case of split
sentences) on a 1?3 scale based on three crite-
ria: Grammaticality (1 ? ungrammatical, 2 ? mi-
nor problems with grammaticality, 3 ? grammati-
cal), Meaning (1 ? meaning is seriously changed
or most of the relevant information lost, 2 ? some
of the relevant information is lost but the meaning
of the remaining information is unchanged, 3 ? all
relevant information is kept without any change in
meaning), and Simplicity (1 ? a lot of irrelevant in-
formation is retained, 2 ? some of irrelevant infor-
mation is retained, 3 ? all irrelevant information is
eliminated). The inter-annotator agreement (IAA)
was calculated using weighted Kappa (weighted
?), Pearson?s correlation (Pearson), and mean av-
erage error (MAE), and the obtained results are
presented in Table 1. A few examples of assigned
scores are given in Table 2, where G, M, and S
denote human scores for grammaticality, meaning
preservation and simplicity respectively.
score for simplicity, which are not relevant here.
3
For more detailed explanation of simplification schemes
and the dataset see (Glava?s and
?
Stajner, 2013).
2
Ex. Original Simplified G M S SM
(a) ?It is understood the dead girl had
been living at her family home, in
a neighbouring housing estate, and
was visiting her older sister at the
time of the shooting.?
?The dead girl had been living at
her family home, in a neighbouring
housing estate and was visiting her
older sister.?
3 3 3 S
(b) ?On Facebook, more than 10,000
people signed up to a page an-
nouncing an opposition rally for
Saturday.?
?On Facebook, more than 10,000
people signed to a page announcing
an opposition rally for Saturday.?
2 3 3 S
(c) ?Joel Elliott, also 22, of North
Road, Brighton, was charged on
May 3 with murder. He appeared at
Lewes Crown Court on May 8 but
did not enter a plea.?
?Joel Elliott was charged on May 3
with murder. He appeared at Lewes
Crown Court on May 8.?
3 2 3 S
(d) ?For years the former Bosnia Serb
army commander Ratko Mladic had
evaded capture and was one of the
world?s most wanted men, but his
time on the run finally ended last
year when he was arrested near
Belgrade.?
?For years the former Bosnia Serb
army commander Ratko Mladic had
evaded but his time the run ended
last year he was arrested near Bel-
grade.?
1 2 3 S
(e) ?Police have examined the scene at
a house at William Court in Bel-
laghy, near Magherafelt for clues to
the incident which has stunned the
community.?
?Police have examined the scene
at William Court near Magherafelt.
The incident has stunned the com-
munity.?
3 1 3 P
(f) ?Rastan, 25 km (15 miles) north
of Homs city, has slipped in and
out of government control several
times since the uprising against As-
sad erupted in March 2011.?
?Rastan has slipped government
control several times. The uprising
erupted in March 2011.?
2 1 3 P
(g) ?But opposition parties and inter-
national observers said the vote was
marred by vote-rigging, including
alleged ballot-box stuffing and false
voter rolls.?
?But opposition parties and inter-
national observers said .?
1 1 3 B
(h) ?Foreign Affairs Secretary Albert
del Rosario was seeking a diplo-
matic solution with Chinese Ambas-
sador Ma Keqing, the TV network
said.?
?Foreign Affairs Secretary Albert
del Rosario was seeking a diplo-
matic solution with Chinese Ambas-
sador Ma Keqing, the TV network
said.?
3 3 1 B
(h) ? On Wednesday, two video jour-
nalists working for the state-owned
RIA Novosti news agency were
briefly detained outside the Election
Commission building where Putin
was handing in his application to
run.?
?On Wednesday two video journal-
ists were briefly detained outside
the Election Commission building.
Two video journalists worked for
the state-owned RIA Novosti news
agency. Putin was handing in his
application.?
3 2 2 E
Table 2: Human evaluation examples (G, M, and S correspond to the human scores for grammaticality,
meaning preservation and simplicity, and SM denotes the simplification method used: B ? baseline, S ?
sentence-wise, E ? event-wise, and P ? pronominal anaphora)
3
3.3 Derived Datasets
The original dataset (Original) contains separate
scores for grammaticality (G), meaning preserva-
tion (M), and simplicity (S), each of them on a 1?3
scale. From this dataset we derived two additional
ones: Total3 and Total2.
The Total3 dataset contains three marks (OK ?
use as it is, PE ? post-editing required, and Dis
? discard) derived from G and M in the Original
dataset. Those simplified sentences which scored
?3? for both meaning preservation (M) and gram-
maticality (G) are placed in the OK class as they
do not need any kind of post-editing. A closer
look at the remaining sentences suggests that any
simplified sentence which got a score ?2? or ?3?
for meaning preservation (M) could be easily post-
edited, i.e. it requires minimal changes which are
obvious from its comparison to the corresponding
original. For instance, in the sentence (b) in Ta-
ble 2 the only change that needs to be made is
adding the word ?up? after ?signed?. Those sen-
tences which scored ?2? for meaning need slightly
more, albeit simple modification. The simplified
text snippet (c) in Table 2 would need ?but did
not enter a plea? added at the end of the last
sentence. The next sentence (d) in the same ta-
ble needs a few more changes, but still very mi-
nor ones: adding the word ?capture? after ?had
evaded?, adding the preposition ?on? before ?the
run?, and adding ?when? after ?last year?. There-
fore, we grouped all those sentences into one class
? PE (sentences which require a minimal post-
editing effort). Those sentences which scored ?1?
for meaning need to either be left in their original
form or simplified from scratch. We thus classify
them as Dis. This newly created dataset (Total3)
allows us to investigate whether we could auto-
matically classify simplified sentences into those
three categories, taking into account both gram-
maticality and meaning preservation at the same
time.
The Total2 dataset contains only two marks (?0?
and ?1?) which correspond to the sentences which
should be discarded (?0?) and those which should
be retained (?1?), where ?0? corresponds to Dis in
Total3, and ?1? corresponds to the union of OK and
PE in Total3. The derivation procedure for both
datasets is presented in Table 3. We wanted to in-
vestigate whether the classification task would be
simpler (better performed) if there were only two
classes instead of three. In the case that such clas-
sification could be performed with satisfactory ac-
curacy, all sentences classified as ?0? would be left
in their original form or simplified with some dif-
ferent simplification strategy, while those classi-
fied as ?1? would be sent for a quick human post-
editing procedure.
Original
Total3 Total2
G M
3 3 OK 1
2 3 PE 1
1 3 PE 1
3 2 PE 1
2 2 PE 1
1 2 PE 1
3 1 Dis 0
2 1 Dis 0
1 1 Dis 0
Table 3: Datasets
Here it is important to mention that we decided
not to use human scores for simplicity (S) for sev-
eral reasons. First, simplicity was defined as the
amount of irrelevant information which was elim-
inated. Therefore, we cannot expect that any of
the six MT evaluation metrics would have a sig-
nificant correlation with this score (except maybe
TERp and, in particular, one of its parts ? ?number
of deletions?. However, none of the two demon-
strated any significant correlation with the sim-
plicity score, and those results are thus not re-
ported in this paper). Second, the output sentences
with a low simplicity score are not as detrimental
for the TS system as those with a low grammat-
icality or meaning preservation score. The sen-
tences with a low simplicity score would simply
not help the target user read faster or understand
better, but would not do any harm either. Alter-
natively, if the target ?user? is an MT or infor-
mation extraction (IE) system, or a parser for ex-
ample, such sentences would not lower the perfor-
mance of the system; they would just not improve
it. Low scores for G and M, however, would lead
to a worse performance for such NLP systems,
longer reading time, and a worse or erroneous un-
derstanding of the text. Third, the simplicity of
the output (or complexity reduction performed by
a TS system) could be evaluated separately, in a
fully automatic manner ? using some readability
measures or average sentence length as features
(as in (Drndarevi?c et al., 2013; Glava?s and
?
Stajner,
4
2013) for example).
3.4 Features: MT Evaluation Metrics
In all experiments, we focused on six commonly
used MT evaluation metrics. These are cosine
similarity (using the bag-of-words representation),
METEOR (Denkowski and Lavie, 2011), TERp
(Snover et al., 2009), TINE (Rios et al., 2011), and
two components of TINE: T-BLEU (which differs
from the standard BLEU (Papineni et al., 2002) by
using 3-grams, 2-grams, and 1-grams when there
are no 4-grams found, where the ?original? BLEU
would give score ?0?) and SRL (which is the com-
ponent of TINE based on semantic role labeling
using SENNA
4
). Although these two components
contribute equally to TINE (thus being linearly
correlated with TINE), we wanted to investigate
which one of them contributes more to the cor-
relation of TINE with human judgements. Given
their different natures, we expect T-BLEU to con-
tribute more to the correlation of TINE with hu-
man judgements of grammaticality, and SRL to
contribute more to the correlation of TINE with
human judgements of meaning preservation.
As we do not have the reference for the simpli-
fied sentence, all metrics are applied in a slightly
different way than in MT. Instead of evaluating the
translation hypothesis (output of the automatic TS
system in our case) with the corresponding ref-
erence translation (which would be a ?gold stan-
dard? simplified sentence), we apply the metrics
to the output of the automatic TS system com-
paring it with the corresponding original sentence.
Given that the simplified sentences in the used
dataset are usually shorter than the original ones
(due to the elimination of irrelevant content which
was the main focus of the TS system proposed by
Glava?s and
?
Stajner (2013)), we expect low scores
of T-BLEU and METEOR which apply a brevity
penalty. However, our dataset does not contain any
kind of lexical simplification, but rather copies all
relevant information from the original sentence
5
.
Therefore, we expect the exact matches of word
forms and semantic role labels (which are compo-
nents of the MT evaluation metrics) to have a good
correlation to human judgements of grammatical-
ity and meaning preservation.
4
http://ml.nec-labs.com/senna/
5
The exceptions being changes of gerundive forms into
past tense, and anaphoric pronoun resolution in some simpli-
fication schemes. See Section 3.1 and (Glava?s and
?
Stajner,
2013) for more details.
3.5 Goal
After we obtained the six automatic metrics (co-
sine, METEOR, TERp, TINE, T-BLEU, and
SRL), we performed two sets of experiments, try-
ing to answer two main questions:
1. Are the chosen MT evaluation metrics cor-
related with the human judgements of gram-
maticality and meaning preservation of the
TS system output?
2. Could we automatically classify the simpli-
fied sentences into those which are: (1) cor-
rect, (2) require a minimal post-editing, (3)
incorrect and need to be discarded?
A positive answer to the first question would
mean that there is a possibility of finding an au-
tomatic metric (or a combination of several au-
tomatic metrics) which could successfully replace
the time consuming human evaluation. The search
for that ?ideal? combination of automatic metrics
could be performed by using various classification
algorithms and carefully designed features. If we
manage to classify simplified sentences into the
three aforementioned categories with a satisfying
accuracy, the benefits would be two-fold. Firstly,
such a classification system could be used for an
automatic evaluation of TS systems and an easy
comparison of their performances. Secondly, it
could be used inside a TS system to mark those
sentences of low quality which need to be checked
further, or those sentences whose original mean-
ing changed significantly. The latter could then be
left in their original form or simplified using some
different technique.
3.6 Experiments
The six experiments conducted in this study are
presented in Table 4. The first two experiments
had the aim of answering the first question (Sec-
tion 3.5) as to whether the chosen MT metrics cor-
relate with the human judgements of grammatical-
ity (G) and meaning preservation (M) of the TS
system output. The results were obtained in terms
of Pearson?s, Kendall?s and Spearman?s correla-
tion coefficients. The third and the fourth exper-
iments (Table 4) could be seen as the intermediate
experiments exploring the possibility of automatic
classification of simplified sentences according to
their grammaticality, and meaning preservation.
The main experiment was the fifth experiment, try-
ing to answer the second question (Section 3.5)
5
Exp. Description
1. Correlation of the six automatic MT metrics with the human scores for Grammaticality
2. Correlation of the six automatic MT metrics with the human scores for Meaning preservation
3. Classification of the simplified sentences into 3 classes (?1? ? Bad, ?2? ? Medium, and ?3? ? Good) according to
their Grammaticality
4. Classification of the simplified sentences into 3 classes (?1? ? Bad, ?2? ? Medium, and ?3? ? Good) according to
their Meaning preservation
5. Classification of the simplified sentences into 3 classes (OK, PE, Dis) according to their Total3 score
6. Classification of the simplified sentences into 2 classes (?1? ? Retain, ?0? ? Discard) according to their Total2 score
Table 4: Experiments
as to whether we could automatically classify the
simplified sentences into those which are: (1) cor-
rect (OK), (2) require minimal post-editing (PE),
and (3) incorrect and need to be discarded (Dis).
The last experiment (Table 4) was conducted with
the aim of exploring whether the classification of
simplified sentences into only two classes ? Retain
(for further post-editing) and Discard ? would lead
to better results than the classification into three
classes (OK, PE, and Dis) in the fifth experiment.
All classification experiments were performed
in Weka workbench (Witten and Frank, 2005; Hall
et al., 2009), using seven classification algorithms
in a 10-fold cross-validation setup:
? NB ? NaiveBayes (John and Langley, 1995),
? SMO ? Weka implementation of Support
Vector Machines (Keerthi et al., 2001) with
normalisation (n) or with standardisation (s),
? Logistic (le Cessie and van Houwelingen,
1992),
? Lazy.IBk ? K-nearest neighbours (Aha and
Kibler, 1991),
? JRip ? a propositional rule learner (Cohen,
1995),
? J48 ? Weka implementation of C4.5 (Quin-
lan, 1993).
As a baseline we use the classifier which assigns
the most frequent (majority) class to all instances.
4 Results and Discussion
The results of the first two experiments (correla-
tion experiments in Table 4) are presented in Sec-
tion 4.1, while the results of the other four exper-
iments (classification experiments in Table 4) can
be found in Section 4.2. When interpreting the re-
sults of all experiments, it is important to keep in
mind that human agreements for meaning preser-
vation (M) and grammaticality (G) were accept-
able but far from perfect (Section 3.2), and thus
it would be unrealistic to expect the correlation
between the MT evaluation metrics and human
judgements or the agreement of the classification
system with human assessments to be higher than
the reported IAA agreement.
4.1 Correlation of Automatic Metrics with
Human Judgements
The correlations of automatic metrics with hu-
man judgements of grammaticality and meaning
preservation are given in Tables 5 and 6 respec-
tively. Statistically significant correlations (at a
0.01 level of significance) are presented in bold.
Metric Pearson Kendall Spearman
cosine 0.097 0.092 0.115
METEOR 0.176 0.141 0.178
T-BLEU 0.226 0.185 0.234
SRL 0.097 0.076 0.095
TINE 0.175 0.145 0.181
TERp -0.208 -0.158 -0.198
Table 5: Correlation between automatic evaluation
metrics and human scores for grammaticality
Metric Pearson Kendall Spearman
cosine 0.293 0.262 0.334
METEOR 0.386 0.322 0.405
T-BLEU 0.442 0.382 0.475
SRL 0.348 0.285 0.356
TINE 0.427 0.385 0.447
TERp -0.414 -0.336 -0.416
Table 6: Correlation between automatic evaluation
metrics and human scores for meaning preserva-
tion
It can be noted that human perception of gram-
maticality is positively correlated with three auto-
6
Algorithm
Grammaticality Meaning Total3 Total2
P R F P R F P R F P R F
NB 0.53 0.46 0.48 0.54 0.54 0.54 0.54 0.53 0.53 0.74 0.69 0.71
SMO(n) 0.39 0.63 0.48 0.52 0.49 0.45 0.43 0.53 0.44 0.55 0.74 0.63
SMO(s) 0.39 0.63 0.48 0.57 0.56 0.55 0.57 0.55 0.51 0.60 0.73 0.63
Logistic 0.45 0.61 0.49 0.57 0.57 0.56 0.61 0.60 0.59 0.75 0.77 0.74
Lazy.IBk 0.57 0.58 0.57 0.50 0.50 0.50 0.54 0.54 0.54 0.73 0.73 0.73
JRip 0.41 0.59 0.48 0.53 0.50 0.48 0.57 0.56 0.55 0.72 0.75 0.73
J48 0.45 0.61 0.49 0.48 0.47 0.47 0.59 0.57 0.54 0.68 0.71 0.69
baseline 0.39 0.63 0.48 0.17 0.41 0.24 0.21 0.46 0.29 0.55 0.74 0.63
Table 7: Classification results (the best performances are shown in bold; baseline uses the majority class)
Actual
Grammaticality Meaning
Good Med. Bad Good Med. Bad
Good 127 21 23 50 31 7
Med. 29 19 10 24 73 16
Bad 24 9 10 9 31 31
Table 8: Confusion matrices for the best classifications according to Grammaticality (Lazy.IBk) and
Meaning (Logistic). The number of ?severe? classification mistakes (classifying Good as Bad or vice
versa) are presented in bold.
matic measures ? METEOR, T-BLEU, and TINE,
while it is negatively correlated with TERp (TERp
measures the number of edits necessary to perform
on the simplified sentence to transform it into its
original one, i.e. the higher the value of TERp,
the less similar the original and its corresponding
simplified sentence are. The other five MT metrics
measure the similarity between the original and its
corresponding simplified version, i.e. the higher
their value is, the more similar are the sentences
are). All the MT metrics appear to be even bet-
ter correlated with the human scores for meaning
preservation (Table 6), demonstrating six positive
and one (TERp) negative statistically significant
correlation with M. The correlation is the highest
for T-BLEU, TINE, and TERp, though closely fol-
lowed by all others.
4.2 Sentence Classification
The results of the four classification experiments
(Section 3.6) are given in Table 7.
At first glance, the performance of the classifi-
cation algorithms seems similar for the first two
tasks (classification of the simplified sentences
according to their Grammaticality and Meaning
preservation). However, one needs to take into ac-
count that the baseline for the first task was much
much higher than for the second task (Table 7).
Furthermore, it can be noted that for the first task,
recall was significantly higher than precision for
most classification algorithms (all except NB and
Logistic), while for the second task they were very
similar in all cases. More importantly, a closer
look at the confusion matrices reveals that most of
the incorrectly classified sentences were assigned
to the nearest class (Medium into Bad or Good;
Bad into Medium; and Good into Medium
6
) in the
second task, while it was not the case in the first
task (Table 8).
Classification performed on the Total3 dataset
outperformed both previous classifications ? that
based on Grammaticality and that based on Mean-
ing ? on four different algorithms (NB, Logis-
tic, JRip, and J48). Classification conducted on
Total3 using Logistic outperformed all results of
classifications on either Grammaticality or Mean-
ing separately (Table 7). It reached a 0.61, 0.60,
and 0.59 score for the weighted precision (P), re-
call (R), and F-measure (F), respectively, thus out-
performing the baseline significantly. More im-
portantly, classification on the Total3 dataset led
to significantly fewer mis-classifications between
Good and Bad (Table 9) than the classification
based on Grammaticality, and slightly less than
6
Bad, Medium, and Good correspond to marks ?1?, ?2?,
and ?3? given by human evaluators.
7
Actual
Total3
OK PE Dis.
OK 41 32 4
PE 17 85 12
Dis. 6 31 28
Table 9: Confusion matrix for the best classifica-
tion according to Total3 (Logistic). The number of
?severe? classification mistakes (classifying Good
as Bad or vice versa) are presented in bold.
Actual
Total2
Retain Discard
Retain 21 50
Discard 12 189
Table 10: Confusion matrix for the best classifi-
cation according to Total2 (Logistic). The num-
ber of ?severe? classification mistakes (classifying
Retain as Discard or vice versa) are presented in
bold.
the classification based only on Meaning (Table 8).
Therefore, it seems that simplified sentences are
better classified into three classes giving a unique
score for both grammaticality and preservation of
meaning together.
The binary classification experiments based on
the Total2 led to results which significantly out-
performed the baseline in terms of precision and
F-measure (Table 7). However, they resulted in
a great number of sentences which should be re-
tained (Retain) being classified into those which
should be discarded (Discard) and vice versa (Ta-
ble 10). Therefore, it seems that it would be better
to opt for classification into three classes (Total3)
than for classification into two classes (Total2).
Additionally, we used CfsSubsetEval attribute
selection algorithm (Hall and Smith, 1998) in or-
der to identify the ?best? subset of features. The
?best? subsets of features for each of the four clas-
sification tasks returned by the algorithm are listed
in Table 11. However, the classification perfor-
mances achieved (P, R, and F) when using only
the ?best? features did not differ significantly from
those when using all initially selected features, and
thus are not presented in this paper.
5 Limitations
The used dataset does not contain any kind of
lexical simplification (Glava?s and
?
Stajner, 2013).
Classification ?Best? features
Meaning {TERp, T-BLEU, SRL, TINE}
Grammaticality {TERp, T-BLEU}
New3 {TERp, T-BLEU, SRL, TINE}
New2 {TERp, T-BLEU, SRL}
Table 11: The ?best? features (CfsSubsetEval)
Therefore, one should consider the limitation of
this TS system which performs only syntactic sim-
plification and content reduction. On the other
hand, the dataset used contains a significant con-
tent reduction in most of the sentences. If the same
experiments were conducted on a dataset which
performs only syntactic simplification, we would
expect much higher correlation of MT evaluation
metrics to human judgements, due to the lesser im-
pact of the brevity penalty in that case.
If we were to apply the same MT evaluation
metrics to a TS system which additionally per-
forms some kind of lexical simplification (either
a simple lexical substitution or paraphrasing), the
correlation results for T-BLEU and cosine similar-
ity would be lower (due to the lower number of
exact matches), but not for METEOR, TERp and
SRL (and thus TINE as well). As a similar prob-
lem is also present in the evaluation of MT sys-
tems where the obtained output could differ from
the reference translation (while still being equally
good), METEOR, TERp, and SRL in TINE ad-
ditionally use inexact matching. The first two use
the stem, synonym, and paraphrase matches, while
SRL uses ontologies and thesaurus.
6 Conclusions and Future Work
While the results reported are preliminary and
their universality needs to be validated on different
TS datasets, the experiments and results presented
can be regarded as a promising step towards an au-
tomatic assessment of grammaticality and mean-
ing preservation for the output of TS systems. In
addition and to the best of our knowledge, there
are no such datasets publicly available other than
the one used. Nevertheless, we hope that these re-
sults would initiate an interesting discussion in the
TS community and start a new direction of studies
towards automatic evaluation of text simplification
systems.
8
Acknowledgements
The research described in this paper was par-
tially funded by the European Commission un-
der the Seventh (FP7-2007-2013) Framework Pro-
gramme for Research and Technological Develop-
ment (FP7-ICT-2011.5.5 FIRST 287607).
References
D. Aha and D. Kibler. 1991. Instance-based learning
algorithms. Machine Learning, 6:37?66.
S. M. Alu??sio, L. Specia, T. A. S. Pardo, E. G. Maziero,
H. M. Caseli, and R. P. M. Fortes. 2008a. A cor-
pus analysis of simple account texts and the pro-
posal of simplification strategies: first steps towards
text simplification systems. In Proceedings of the
26th annual ACM international conference on De-
sign of communication, SIGDOC ?08, pages 15?22,
New York, NY, USA. ACM.
S. M. Alu??sio, L. Specia, T. A.S. Pardo, E. G. Maziero,
and R. P.M. Fortes. 2008b. Towards brazilian por-
tuguese automatic text simplification systems. In
Proceedings of the eighth ACM symposium on Doc-
ument engineering, DocEng ?08, pages 240?248,
New York, NY, USA. ACM.
M. J. Aranzabe, A. D??az De Ilarraza, and I. Gonz?alez.
2012. First Approach to Automatic Text Simplifica-
tion in Basque. In Proceedings of the first Natural
Language Processing for Improving Textual Acces-
sibility Workshop (NLP4ITA).
G. Barlacchi and S. Tonelli. 2013. ERNESTA: A sen-
tence simplification tool for childrens stories in ital-
ian. In Computational Linguistics and Intelligent
Text Processing.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings of the
9th Conference of the European Chapter of the ACL
(EACL?99), pages 269?270.
R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifi-
cation. In In Proceedings of the Sixteenth Inter-
national Conference on Computational Linguistics
(COLING ?96, pages 1041?1044.
W. Cohen. 1995. Fast Effective Rule Induction. In
Proceedings of the Twelfth International Conference
on Machine Learning, pages 115?123.
W. Coster and D. Kauchak. 2011. Learning to Sim-
plify Sentences Using Wikipedia. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics, pages 1?9.
M. Denkowski and A. Lavie. 2011. Meteor 1.3: Au-
tomatic Metric for Reliable Optimization and Evalu-
ation of Machine Translation Systems. In Proceed-
ings of the EMNLP Workshop on Statistical Machine
Translation.
S. Devlin. 1999. Simplifying natural language text for
aphasic readers. Ph.D. thesis, University of Sunder-
land, UK.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram coocurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138?145. Morgan Kaufmann Pub-
lishers Inc.
B. Drndarevi?c, S.
?
Stajner, S. Bott, S. Bautista, and
H. Saggion. 2013. Automatic Text Simplication
in Spanish: A Comparative Evaluation of Com-
plementing Components. In Proceedings of the
12th International Conference on Intelligent Text
Processing and Computational Linguistics. Lecture
Notes in Computer Science. Samos, Greece, 24-30
March, 2013., pages 488?500.
L. Feng. 2009. Automatic readability assessment for
people with intellectual disabilities. In SIGACCESS
Access. Comput., number 93, pages 84?91. ACM,
New York, NY, USA, jan.
G. Glava?s and S.
?
Stajner. 2013. Event-Centered Sim-
plication of News Stories. In Proceedings of the
Student Workshop held in conjunction with RANLP
2013, Hissar, Bulgaria, pages 71?78.
M. A. Hall and L. A. Smith. 1998. Practical feature
subset selection for machine learning. In C. Mc-
Donald, editor, Computer Science ?98 Proceedings
of the 21st Australasian Computer Science Confer-
ence ACSC?98, pages 181?191. Berlin: Springer.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The weka data min-
ing software: an update. SIGKDD Explor. Newsl.,
11:10?18, November.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the second inter-
national workshop on Paraphrasing - Volume 16,
PARAPHRASE ?03, pages 9?16, Stroudsburg, PA,
USA. Association for Computational Linguistics.
G. H. John and P. Langley. 1995. Estimating Contin-
uous Distributions in Bayesian Classifiers. In Pro-
ceedings of the Eleventh Conference on Uncertainty
in Artificial Intelligence, pages 338?345.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 2001. Improvements to Platt?s
SMO Algorithm for SVM Classifier Design. Neural
Computation, 13(3):637?649.
9
D. Klein and C.D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, volume 1, pages 423?430. Association for
Computational Linguistics.
S. le Cessie and J.C. van Houwelingen. 1992. Ridge
Estimators in Logistic Regression. Applied Statis-
tics, 41(1):191?201.
J. Martos, S. Freire, A. Gonz?alez, D. Gil, and M. Se-
bastian. 2012. D2.1: Functional requirements spec-
ifications and user preference survey. Technical re-
port, FIRST technical report.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
S. E. Petersen and M. Ostendorf. 2007. Text Sim-
plification for Language Learners: A Corpus Anal-
ysis. In Proceedings of Workshop on Speech and
Language Technology for Education.
R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
L. Rello. 2012. Dyswebxia: a model to improve ac-
cessibility of the textual web for dyslexic users. In
SIGACCESS Access. Comput., number 102, pages
41?44. ACM, New York, NY, USA, January.
M. Rios, W. Aziz, and L. Specia. 2011. TINE: A met-
ric to assess MT adequacy. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT-2011), Edinburgh, UK.
M. B. Ruiter, T. C. M. Rietveld, Cucchiarini C., Krah-
mer E. J., and H. Strik. 2010. Human Language
Technology and communicative disabilities: Re-
quirements and possibilities for the future. In Pro-
ceedings of the the seventh international conference
on Language Resources and Evaluation (LREC).
J. Rybing, C. Smithr, and A. Silvervarg. 2010. To-
wards a Rule Based System for Automatic Simpli-
fication of Texts. In The Third Swedish Language
Technology Conference.
H. Saggion, E. G?omez Mart??nez, E. Etayo, A. An-
ula, and L. Bourg. 2011. Text Simplification in
Simplext: Making Text More Accessible. Revista
de la Sociedad Espa?nola para el Procesamiento del
Lenguaje Natural, 47:341?342.
A. Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language & Computa-
tion, 4(1):77?109.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, Adequacy, or HTER? Exploring Dif-
ferent Human Judgments with a Tunable MT Metric.
In Proceedings of the Fourth Workshop on Statisti-
cal Machine Translation at the 12th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2009), Athens, Greece.
L. Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language, pages 30?39, Berlin, Hei-
delberg.
S.
?
Stajner, R. Evans, C. Orasan, and R. Mitkov. 2012.
What Can Readability Measures Really Tell Us
About Text Complexity? In Proceedings of the
LREC?12 Workshop: Natural Language Processing
for Improving Textual Accessibility (NLP4ITA), Is-
tanbul, Turkey.
I. H. Witten and E. Frank. 2005. Data mining: practi-
cal machine learning tools and techniques. Morgan
Kaufmann Publishers.
K. Woodsend and M. Lapata. 2011a. Learning to Sim-
plify Sentences with Quasi-Synchronous Grammar
and Integer Programming. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
K. Woodsend and M. Lapata. 2011b. WikiSimple:
Automatic Simplification of Wikipedia Articles. In
Proceedings of the 25th AAI Coference on Artificial
Intelligence.
S. Wubben, A. van den Bosch, and E. Krahmer. 2012.
Sentence simplification by monolingual machine
translation. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 1015?
1024, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Mono-
lingual Tree-based Translation Model for Sentence
Simplification. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 1353?1361.
10
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 30?40,
Dublin, Ireland, August 24th 2014.
The Fewer, the Better? A Contrastive Study about Ways to Simplify
Ruslan Mitkov and Sanja
?
Stajner
Research Group in Computational Linguistics
Research Institute of Information and Language Processing
University of Wolverhampton, UK
{R.Mitkov, SanjaStajner}@wlv.ac.uk
Abstract
Simplified texts play an important role in providing accessible and easy-to-understand informa-
tion for a whole range of users who, due to linguistic, developmental or social barriers, would
have difficulty in understanding materials which are not adapted and/or simplified. However, the
production of simplified texts can be a time-consuming and labour-intensive task. In this paper
we show that the employment of a short list of simple simplification rules could result in texts
of comparable readability to those written as a result of applying a long list of more fine-grained
rules. We also prove that the simplification process based on the short list of simple rules is more
time efficient and consistent.
1 Rationale
Simplified texts play an important role in providing accessible and easy-to-understand information for a
whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in
understanding materials which are not adapted and/or simplified. Such users include but are not limited to
people with insufficient knowledge of the language in which the document is written, people with specific
language disorders and people with low literacy levels. However, while the production of simplified texts
is certainly an indispensable activity, it often proves to be a time-consuming and labour-intensive task.
Various methodologies and simplification strategies have been developed which are often employed by
authors to simplify original texts. Most methods involve a high number of rules which could result not
only in the simplification task being time-consuming but also in the authors getting confused as to which
rules to apply. We hypothesise that it is possible to achieve a comparable simplification effect by using a
small set of simple rules similar to the ones used in Controlled Languages which, in addition, enhances
the productivity and reliability of the simplification process.
In order to test our hypothesis we conduct the following experiments. First, we propose six Controlled
Language-inspired rules which we believe are simple and easy enough for writers of simplified texts to
understand and apply. We then ask two writers to apply these rules to a selection of newswire texts and
also to produce simplified versions of these texts using the 28 rules used in the Simplext project (Saggion
et al., 2011). Both sets of texts are compared in terms of readability. In both simplification tasks the time
efficiency is assessed and the inter-annotator agreement is evaluated. In an additional experiment, we
seek to investigate the possible effect of familiarisation in simplification. In this experiment a third
writer simplifies a sample of the texts used in the previous experiments by applying each set of rules in
a mixed sequence pattern which does not offer any familiarisation nor the advantage of one set of rules
over the other. Using these samples, three-way inter-annotator agreement is reported.
The rest of the paper is structured as follows. Section 2 outlines related work on simplification rules.
Section 3 introduces our proposal for a small set of easy-to-understand and easy-to-apply rules and
contrasts them with the longer and more elaborate rules employed in the Simplext proposal. Section
4 details the experiments conducted in order to validate or refute our hypothesis, and outlines the data
used for the experiments. Section 5 presents and discusses the results, while the last section of the paper
summarises the main conclusions of this study.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
30
2 Related work
Since the late 1990s, several initiatives which proposed guidelines for producing plain, easy-to-read and
more accessible documents have emerged. These include the ?Federal Plain Language Guidelines?,
?Make it Simple, European Guidelines for the Production of Easy-to-Read Information for people with
Learning Disability?, and ?Am I making myself clear? Mencap?s guidelines for accessible writing?.
The Plain Language Action and Information Network (PLAIN)
1
developed the first version of the
?Federal Plain Language Guidelines? (PlainLanguage, 2011) in the mid-90s and have revised it every
few years since then. Their original idea was to help writers of governmental documents (primarily
regulations) to write in a clear and and simple manner so that the users can: ?find what they need; under-
stand what they find; and use what they find to meet their needs.? (PlainLanguage, 2011). The ?Make it
Simple? European Guidelines for the Production of Easy-to-Read Information for people with Learning
Disability (Freyhoff et al., 1998) were produced by Inclusion Europe
2
in order to assist writers in devel-
oping texts, publications and videos that are more accessible to people with intellectual disabilities and
other people who cannot read complex texts, and thus enable those people to be better protected from
discrimination and social injustice. The ?Am I making myself clear?? Mencap?s guidelines for accessi-
ble writing (Mencap, 2002) were produced by the UK?s leading organisation working with people with
a learning disability.
3
Their goal is to help in editing and writing accessible material for that specific
target population. All of these guidelines are concerned with both verbal content of documents and their
layout. As we are interested in text simplification and not in text representation, we will concentrate only
on the former. All three guidelines share similar instructions for accessible writing, some of them more
detailed than others. Table 1 allows us to have a quick overview of intersecting rules suggested by these
guidelines which were intended for slightly different purposes and target audiences.. For example, they
all advise the writer to use active voice instead of passive, use short, simple words and omit unnecessary
words, write short sentences and cover only one main idea per sentence, etc. However, the ?Federal
Plain Language Guidelines? also instruct writers to use contractions where appropriate, avoid hidden
verbs (i.e. verbs converted into a noun), and place the main idea before exceptions and conditions, while
the other two guidelines do not go into many details. Some of the instructions, e.g. to use the simplest
form of a verb (present and not conditional or future), or to avoid double negatives and exceptions to
exceptions, are not present in the Mencap?s guidelines for accessible writing, while they are at the same
time implicitly present in the ?Make it Simple? guidelines, and explicitly present in the ?Federal Plain
Language Guidelines?.
Karreman et al. (2007) investigated whether the application of the ?Make it Simple? guidelines to the
website?s content would enhance its usability for users with intellectual disabilities. Additionally, they
investigated whether the application of these guidelines would have a negative effect on users without
disabilities, as Web Accessibility Initiative (WAI) guidelines
4
state that creation of multiple versions of
the same website should be avoided whenever possible. The authors prepared two versions of a website,
the original one and the one adapted according to the ?Make it Simple? guidelines. These two versions
were then tested for efficiency (searching and reading time) and effectiveness (comprehension) by 40
participants, 20 with diagnosed intellectual disabilities and 20 without. The results demonstrated that the
adaptation of the website according to the guidelines enhanced the efficiency and effectiveness for both
groups of participants.
There has been a body of work associated with the development and use of Controlled Languages
for simplification purposes. The original idea of developing a Controlled Language arose during the
1930s when influential scholars sought to establish a ?minimal? variety of English, a variety specifically
designed to make English accessible to and usable by the largest possible number of people worldwide
(Arnold et al., 1994). This variety was called Basic English and one of the central ideas was to use
a few hundred general-purpose words only. Operator verbs were to be used with a set of nouns and
1
http://www.plainlanguage.gov/
2
http://inclusion-europe.org/
3
http://november5th.net/resources/Mencap/Making-Myself-Clear.pdf
4
http://www.w3.org/WAI/
31
Rule Simple Clear Plain
Use active tense (instead of passive) yes yes yes
Use the simplest form of a verb* (yes) yes
Avoid hidden verbs (i.e. verbs converted into a noun) yes
Use ?must? to indicate requirements yes
Use contractions where appropriate yes
Don?t turn verbs into nouns yes
Use ?you? to speak directly to readers yes yes yes
Avoid abbreviations yes yes
Use short, simple words yes yes
Omit unnecessary words yes
Avoid definitions as much as possible yes
Use the same term consistently yes yes
Avoid legal, foreign and technical jargon yes yes yes
Don?t use slashes yes
Write short sentences yes yes yes
Keep subject, verb and object close together yes
Avoid double negatives and exceptions to exceptions (yes) yes
Place the main idea before exceptions and conditions yes
Cover only one main idea per sentence yes yes
Use examples (avoid abstract concepts) yes yes
Keep the punctuation simple yes yes
Be careful with figures of speech and metaphors yes
Use the number and not the word yes yes
Avoid cross references yes yes
*Use present tense and not conditional or future
Table 1: Rules for verbal content of documents (the three columns ?Simple?, ?Clear?, and ?Plain? contain
?yes? if this rule is present in the corresponding guidelines: ?Make it Simple?, ?Am I making myself
clear?? and ?Federal Plain Language Guidelines?, respectively; value ?(yes)? is used when the rule is not
explicitly present in the corresponding guidelines, only implicitly)
adjectives to replace most of the derived verbs. The Controlled Language writing rules included various
rules such as ?Keep it short and simple? (Keep sentences short, Omit redundant words, Order the parts of
the sentence logically, Don?t change constructions in mid-sentence, Take care with the logic of and and
or) and ?Make it explicit? (Avoid elliptical constructions, Don?t omit conjunctions or relatives, Adhere to
the PACE dictionary, Avoid strings of nouns, Do not use -ing unless the word appears thus in the PACE
dictionary) (Arnold et al., 1994). The concept of controlled languages evolved and developed further and
they have been regarded as a prerequisite part of successful Machine Translation. Controlled Languages
have been also employed in a number of critical situations where ambiguity could be a problem.
5
3 Simplification strategies: contrasting two sets of rules
The Simplext guidelines were written under the Simplext project, with the aim of helping the authors to
produce texts which would be accessible to people with Down syndrome. They follow the same main
ideas as those in ?Make it Simple, European Guidelines for People with Intellectual Disability? but they
adapt the rules to their specific target population and the Spanish language. The Simplext guidelines
contain 28 main rules
6
concerned with the verbal content of documents. Those rules cover the same
main ideas as our rules (see below), e.g. to keep sentences short, use only the most frequent words,
5
The reader is referred to (Kittredge, 2003), (Cardey, 2009) and (Temnikova, 2012) for more details.
6
The Simplext guidelines actually provide even more sub-rules for most of the main rules, but in this study we use only the
28 main rules.
32
remove redundant words, use a simpler paraphrase if applicable. However, the Simplext rules are more
fine-grained, thus providing several more specific rules instead of our more general rules. For example,
they explicitly instruct the writer to use frequent words, use non-ambiguous words, and not use words
with more than six syllables whenever it is possible.
On the other hand, the six simple rules selected for our study have been inspired from the rules in
Controlled Languages
7
. We conjecture that there is a small set of simple, easy-to-understand and easy-
to-apply rules which can be equally efficient in terms of simplicity (readability) and yet their employment
is less time-consuming and less contentious in practice. The rules which we propose are as follows
(examples are presented in Table 2):
1. Use simple sentences
We have selected this rule to ensure that the simplified version of the document features sufficiently
short and simple sentences only so that the reader does not have to process longer complex sen-
tences.
2. Remove anaphors
This rules caters for replacing the anaphors such as pronouns and one-anaphors with their antecedent
to minimise the risk of anaphoric ambiguity but also makes sure that the texts does not feature any
elliptical constructions which may be more difficult to understand.
3. Use active voice only
We have included this rule as active voice is generally easier to process.
4. Use the most frequent words only
Similarly to the practice recommended in Basic English, we recommend the use of the 1,000 most
frequent words in Spanish as documented by RAE (Real Academia Espa?nola)
8
. If this is not pos-
sible, then words from the list of the 5,000 most frequent Spanish words are resorted to
9
. We have
allowed the following exception for this rule. There are cases where a specific technical word occurs
in the text and which is unlikely to be on the list of 1,000 (or 5,000) basic / most frequent words in
Spanish. By way of example, in the sentence ?Ana Juan gan?o el Premio Nacional de Ilustraci?on de
2010? (Ana Juan won the national prize for illustration in 2010) the word Ilustraci?on is considered
as technical and is not replaced with a basic word.
5. Remove redundant words
Our rules recommend the removal of redundant words or phrases which do not really contribute to
the understanding of the text.
6. Use a simpler paraphrase, if applicable
There are cases where the sentence is difficult to read or understand due among other things, to
its syntax. Our rules recommend that in such cases the original sentence or part of the sentence is
paraphrased.
4 Experiments and data
In order to test our hypothesis we conducted several experiments. We selected 10 newswire texts in
Spanish and asked two writers who are native speakers of Spanish and who have a language/linguistics
background, to apply both our six rules and the 28 Simplext rules in order to simplify these newswire
texts. The writers familiarised themselves with the rules beforehand, had an induction with the authors
7
We shall often refer to these rules throughout the paper as ?our rules?
8
http://corpus.rae.es/frec/1000 formas.TXT
9
http://corpus.rae.es/frec/5000 formas.TXT
33
Rule Version Example
1 Original Desde hace ya 10 a?nos, La Casa Encendida ha propuesto y desarrollado, den-
tro del mundo profesional de las Artes Esc?enicas, el Ciclo Artes Esc?enicas y
Discapacidad.
[It is now 10 years ago that La Casa Encendida first proposed and carried
out, within the professional field of performing arts, the performing arts and
disabilities course.]
Simplified Desde hace ya 10 a?nos, La Casa Encendida ha organizado el Ciclo Artes
Esc?enicas y Discapacidad. El Ciclo Artes Esc?enicas y Discapacidad est?a den-
tro del mundo profesional de las Artes Esc?enicas.
[It is now 10 years ago that La Casa Encendida organised the performing arts
and disabilities course. The performing arts and disabilities course is part of
the professional field of performing arts.]
2 Original Sus solos en directo son acontecimientos imprevisibles que siempre sorpren-
den a la audiencia, en ellos interpreta temas de sus ?albumes en solitario con
partes de improvisaci?on.
[His live solos are unpredictable events which always surprise the audience;
during these, he performs songs from his albums on his own while improvising
some parts.]
Simplified Los solos en directo de Marc Ribot siempre sorprenden a la audiencia. En los
solos Marc Ribot toca canciones de sus ?albumes con partes de improvisaci?on.
[Marc Ribots live solos always surprise the audience. During solos, Marc
Ribot plays songs from his albums while improvising some parts.]
3 Original Los avisos recibidos por la Gerencia de Emergencias Sanitarias fueron
canalizados a trav?es de las unidades del Servicio Murciano de Salud.
[Calls received by medical emergency services were directed by the Depart-
ment of Health Services in Murcia.]
Simplified La Gerencia de Emergencias Sanitarias recibieron los avisos. Las unidades
del Servicio Murciano de Salud se encargaron de los avisos.
[The medical emergency services received the calls. The Department of
Health Services in Murcia took charge of the calls.]
4 Original Ratificaci
?
on Experimental
[Experimental ratification]
Simplified Confirmaci
?
on Experimental
[Experimental confirmation]
5 Original Un disolvente agresivo, muy vol
?
atil y que entra
?
na riesgos para la salud.
[An aggressive solvent, very volatile and which involves health risks.]
Simplified El disolvente Percloroetileno puede ser peligroso para la salud.
[The solvent perchloroethylene can be dangerous to your health.]
6 Original L?ogicamente, al ser menos agresivo, mejora sustancialmente el tacto de las
prendas y no deja el caracter??stico olor a tintorer??a.
[Logically, due to it being less aggressive, it considerably improves how
clothes feel and does not leave them with that characteristic dry cleaners
smell.]
Simplified Otros disolventes, al ser menos agresivos, dejan la ropa m
?
as suave y no dejan
el olor a tintorer??a.
[Other solvents, due to their being less aggressive, make clothes softer and
don?t leave them smelling of dry cleaner.]
Table 2: Examples of each of our rules (sentence parts altered by applying the corresponding rule are
shown in bold)
34
of this paper and were asked to have sessions no longer than 1 hour so that potential fatigue did not com-
promise the experiments. In order to minimise potential familiarity effect (texts which already have been
simplified are expected to be simplified faster and more efficiently as they are familiar to the writers),
we allowed a few days interval between each time a specific text was simplified using different rules.
We applied the Spauldings Spanish Readability index ? SSR (Spaulding, 1956) as well as the Lexical
Complexity index ? LC (Anula, 2007) to assess the readability of the simplified texts. Both metrics have
shown a good correlation with the possible reading obstacles for various target populations (
?
Stajner and
Saggion, 2013), and were used for the evaluation of the automatic TS system in Simplext (Drndarevi?c et
al., 2013). We also asked a third writer to simplify samples from the texts used by the first two writers
which were pre-assessed to be of comparable complexity, with a view to establishing whether familiari-
sation has an effect on the output. The results of these readability experiments are presented in Tables 4
and 5 of the following section. We also recorded the time needed to simplify each text as an indication
of, among other things, ease of use of (and clarity for) each set of rules and its productivity in general;
these results are reported in Tables 6 and 7 of the following section.
Several experiments were conducted to assess the inter-annotator agreement. We believe that the inter-
annotator agreement is another good indicator as to how straightforward it is to apply a specific set of
simplification rules and how reliable the simplification process is in general. We compute the inter-
annotator agreement in terms of the BLEU score (Papineni et al., 2002). BLEU score is widely used in
MT to compare the reference translation with the output of the system (translation hypothesis). Here we
use the BLEU score to compare the simple sentences produced by one annotator with the corresponding
sentences of another annotator. We measure the inter-annotator agreement for all three pairs of annotators
(Table 8). In addition, we examined how many times each of the rules was selected by each writer which
in our view would be not only a way of accounting for agreement and but also assessing the usefulness
of every rule and how balanced a set of rules is in general. Tables 9 and 10 report the results of this study
on the texts simplified by all three annotators.
While in the above experiments (which involved only two writers) we made sure that there was at
least a few days? span between applying the different sets of rules on the same text, we felt that the risk
of familiarity effect could not be removed completely. It is expected that a text which has already been
simplified would take less time to be simplified for a second time, even if different rules are applied.
Also, as Simplext rules were always applied after our simple rules, we felt that additional experiments
were needed where (i) there would be no risk of familiarisation effect and (ii) the rules were applied in a
mixed order so that any experience gained from simplification in general cannot serve as unfair advantage
to one of the sets of rules. In an experiment seeking to investigate the possible effect of familiarisation
in simplification, a third writer simplified a selection of the texts used in the previous experiments by
applying each set of rules in a mixed sequence pattern which does not offer any familiarisation nor any
advantage of one set of rules over the other. In other words, instead of this writer simplifying the same
text twice using different rules, different texts of comparable level of simplicity, informed by the input of
the first two writers, were selected and simplified. Based on the results of the time efficiency experiment
(Table 6, next section), we chose three pairs (Pair 1, Pair 2 and Pair 3) of texts where for each pair the
texts are deemed to be of comparable complexity. By way of example, in Pair 1 which consists of Text 1
and Text 2, Annotator 1 needed the same time for both texts with Simplext rules, and similar time with
our simple rules, Annotator 2 needed the same time with our rules, and similar time with Simplext rules.
Pair 2 consists of Text 3 and Text 4 and Pair 3 is made of Text 9 and Text 10 for the same reasons as
above. The simplification performed by a third writer makes it possible to report readability indices for
the text simplified by the third writer, as well as the time taken to simplify, and three-way agreement.
The 10 texts made available by the Spanish news agency Servimedia
10
belong to one of the four
following domains: international news (Texts 2, 6, and 10), national news (Texts 4 and 8), society (Texts
3 and 7), or culture (Texts 1, 5, and 9). The sizes of these samples (in sentences and words) are listed in
Table 3.
10
http://www.servimedia.es/
35
Size Text 1 Text 2 Text 3 Text 4 Text 5 Text 6 Text 7 Text 8 Text 9 Text 10
Sentences 7 7 5 5 6 4 7 6 5 5
Words 166 183 172 193 176 167 197 180 156 169
Table 3: Size of the texts used for this study
5 Results and discussion
This section presents the results of a study on the readability of texts simplified with our rules as well as
with the Simplext rules. It also reports on a time efficiency experiment whose objective is to identify the
rules which are less time-consuming to apply. Next, interannotator agreement in terms of BLEU score
and selection of rules is discussed and finally, an interpretation of the results of an experiment seeking to
establish any familiarisation effect in simplification is provided.
5.1 Readability study
As can be observed from Table 4, simplification performed by our rules improves the readability of
texts in almost all cases (note the values in column ?original? with those in columns A-I and A-II for
both indices LC and SSR). This improvement was statistically significant in terms of both indices when
the texts were simplified by the second annotator, and in terms of the SSR index when the texts were
simplified by the first annotator (lower readability indices indicate text which is easier to read).
11
.
Text
LC SSR
original A - I A - II B - I B - II original A - I A - II B - I B - II
1 12.00 5.27 6.00 5.57 6.25 183.07 154.67 170.64 147.67 165.70
2 9.76 12.52 9.20 9.74 8.98 174.66 169.07 159.88 161.76 155.99
3 12.95 9.19 8.92 9.04 10.10 176.91 161.30 153.78 157.23 154.80
4 10.74 7.78 7.59 6.53 7.62 179.19 148.27 143.77 133.36 159.26
5 11.79 7.80 9.57 9.47 9.94 196.94 180.05 182.25 164.50 181.99
6 7.23 4.83 4.77 2.00 4.63 177.40 153.22 159.99 130.42 162.19
7 10.23 13.35 8.54 8.29 7.48 175.72 175.11 153.96 137.15 151.34
8 15.14 12.07 11.75 8.96 11.77 191.13 175.42 168.08 155.17 162.59
9 12.86 9.93 10.77 8.87 12.08 178.91 160.47 166.74 142.78 171.08
10 13.52 13.31 10.48 12.03 12.24 166.91 146.96 140.94 152.58 152.94
Table 4: Readability: two readability indices LC and SSR (lower readability indices indicate texts which
are easier to read; I and II refer to the two annotators who simplified all 10 texts; A and B refers to the
rules which are used: A ? ours, B ? Simplext)
Text
LC SSR
original A - III B - III original A - III B - III
1 12.00 4.92 183.07 170.64
2 9.76 8.00 174.66 172.58
3 12.95 6.38 176.91 153.78
4 10.74 7.82 179.19 175.80
9 12.86 10.57 178.91 166.74
10 13.52 12.15 166.91 154.12
Table 5: Readability of texts simplified by Annotator III (A and B refers to the rules which are used: A
? ours, B ? Simplext)
11
Statistical significance was measured by the paired t-test in SPSS at a 0.05 level of significance
36
The differences in readability between the texts written by employing our simplification rules (columns
A-I and A-II) and those written by following the Simplext rules (columns B-I and B-II), were not sta-
tistically significant when the simplification was performed by the second annotator, while they were
significant when the simplification was performed by the first annotator. When interpreting these results,
it is also important to bear in mind that the LC index measures only the lexical complexity of a text,
while the SSR index measures general complexity of a text, including both its lexical and its syntactic
complexity. We also benefited from the familiarity experiment in which a third annotator was involved,
to assess the readability of the simplified versions of the texts of comparable complexity, as produced by
the third additional annotator. The results, which are reported in Table 5, suggest that in fact the texts
simplified by the third annotator with our rules are easier to read. On the basis of these readability results,
it can be concluded that the application of Simplext rules does not necessarily result in a (significantly)
simpler version than the one produced by our rules and comparable results are likely to be achieved.
5.2 Time efficiency experiment
The results from the time efficiency experiment (Table 6) show that in all cases, the simplification with
our rules is done in shorter (or equal) time. This is also confirmed by the time needed by the third
annotator in the additional experiment seeking to establish any familiarity effect (Table 7), where texts
of comparable complexity simplified by our rules were simplified faster than the texts simplified with the
Simplext rules. In our view, the results of these experiments are indicative not only of the time and cost
savings when using our rules but also of our rules being simpler for writers and more straightforward to
employ.
Ann. Set Text 1 Text 2 Text 3 Text 4 Text 5 Text 6 Text 7 Text 8 Text 9 Text 10
I
A 48 41 30 39 55 29 32 43 24 24
B 60 60 40 44 44 18 29 19 15 16
II
A 15 15 10 12 30 30 20 15 10 10
B 30 20 20 15 15 10 10 10 10 10
Table 6: Time efficiency in simplification
Set Text 1 ? Text 2 Text 3 ? Text 4 Text 9 ? Text 10
A 12 15 11
B 16 16 14
Table 7: Time efficiency in simplification (Annotator III only)
5.3 Inter-annotator agreement and selection of rules
Table 8 presents the inter-annotator agreement in terms of BLEU score. This score accounts for the
agreement during the simplification process and the higher the value, the more similar the simplifications
performed by the annotators are. In both cases where the difference is significant our rules exhibited a
higher degree of agreement among the annotators than the Simplext rules.
Rules I ? II II ? III I ? III
A (Ours) 44.00 52.85 48.27
B (Simplext) 30.46 55.12 33.13
Table 8: Pair-wise inter-annotator agreement in terms of BLEU score
We also analysed how many times each rule was applied by each of the annotators (the annotators
were asked to write the numbers of all rules used during simplification of each sentence right after that
sentence). We regard the frequency of selection of rules as another indicator for the inter-annotator
37
agreement. Tables 9 and 10 report the frequency of selection of each of our simple rules as well as the
Simplext rules for all three annotators (measured only on the texts simplified by all three annotators).
Annotator Rule 1 Rule 2 Rule 3 Rule 4 Rule 5 Rule 6
I 12 12 5 33 13 9
II 17 14 6 31 10 4
III 15 22 5 16 7 8
Table 9: Frequency of selection of each of our rules (texts 1, 3, and 9)
Rule
Annotator
Rule
Annotator
Rule
Annotator
Rule
Annotator
I II III I II III I II III I II III
1 25 6 7 8 0 1 1 15 3 0 0 22 0 0 0
2 0 3 1 9 0 0 2 16 0 0 4 23 4 2 1
3 5 0 2 10 1 7 2 17 0 5 2 24 5 0 0
4 19 2 15 11 0 0 0 18 1 0 0 25 0 0 0
5 13 5 0 12 0 0 0 19 2 1 0 26 3 5 0
6 4 0 3 13 2 9 0 20 1 10 2 27 0 0 0
7 1 0 1 14 10 6 6 21 0 0 0 28 1 0 1
Table 10: Frequency of selection of each of the Simplext rules (texts 2, 4, and 10).
It can be seen that there is less difference/discrepancy in the selection of our rules as opposed to the
Simplext rules and hence the simplification process can be regarded as more consistent and reliable.
Here again, there is higher agreement on our rules as opposed to the Simplext ones. This phenomenon is
illustrated in the following example where the annotators used the Simplext rules:
Original: ?Esta reforma prev?e que todos los delitos relacionados con la seguridad vial (como
exceso de velocidad o conducir bajo los efectos del alcohol, las drogas, sin carn?e o sin puntos)
pueden conllevar el decomiso del veh??culo, si bien la decisi?on depender?a del juez.?
[This reform will envisage that all crimes related to road safety (such as speeding, driving
while under the effects of alcohol or drugs or driving without a licence or points) could result
in confiscation of the vehicle, although the decision to do so depends on the judge.]
Annotator 1: ?El cambio del C?odigo Penal dice que la decisi?on de embargar el coche o moto
depender?a del juez.? (rules used: 5,4,1,4,4)
[The change of the penal code says that the decision to confiscate the car or motorbike depends
on the judge.]
Annotator 2: ?Esta reforma prev?e que todos los delitos relacionados con la seguridad vial
como exceso de velocidad o conducir bajo los efectos del alcohol, las drogas, sin carn?e o sin
puntos. Los delitos pueden conllevar la retirada del veh??culo pero la decisi?on depender?a del
juez.? (rules used: 26,17,20,1,8)
[This reform will envisage that all crimes related to road safety such as speeding or driving
under the effects of alcohol, drugs, without a license or points. The crimes could result in
confiscation of the vehicle but the decision depends on the judge.]
Annotator 3: ?La reforma del C?odigo Penal prev?e que todos los delitos relacionados con la
seguridad vial pueden dar lugar a la p?erdida del veh??culo, aunque la decisi?on depender?a del
juez.? (rules used: 4,16,4,9)
[The penal code reform will envisage that all crimes related to road safety could result in loss
of the vehicle, although the decision depends on the judge.]
38
5.4 Familiarisation experiment
From the above results, it can be seen that the simplified texts written by the third annotator using a mixed
pattern indicate clearer preference to our simple rules in terms of better readability, time efficiency and
reliability as opposed to the simplified texts written by Annotator 1 and Annotator 2 where the Simplext
texts were applied only at the end. On the basis of this, we conjecture that this difference may be strongly
connected with the lingering familiarisation of the annotators when they simplify texts they have already
simplified.
6 Conclusions
Simplified texts play an important role in providing accessible and easy-to-understand information for a
whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in
understanding materials which are not adapted and/or simplified. However, the production of simplified
texts can be a time-consuming and labour-intensive task. The results of this study show that a small set
of six simple rules, inspired by the concept of Controlled Languages, could produce simplified texts of
comparable readability to those produced using a long list of more fine-grained rules such as the ones
used in the Simplext project. In addition, the results of this study suggest that our simple rules could be
more time-efficient and reliable.
Acknowledgements
We would like to express our gratitude to Horacio Saggion for his help with the resources.
References
A. Anula. 2007. Tipos de textos, complejidad ling?u??stica y facilicitaci?on lectora. In Actas del Sexto Congreso de
Hispanistas de Asia, pages 45?61.
D. Arnold, L. Balkan, R. Lee Humphreys, S. Meijer, and L. Sadler, 1994. Machine Translation. An Introductory
guide., chapter 8, Input, pages 139?155. Blackwell publishers.
S. Cardey. 2009. Controlled Languages for More Reliable Human Communication in Safety Critical Domains. In
Proceedings of the 11th International Symposium on Social Communication, Santiago de Cuba, Cuba, 19-23
January 2009, pages 330?336.
B. Drndarevi?c, S.
?
Stajner, S. Bott, S. Bautista, and H. Saggion. 2013. Automatic Text Simplication in Spanish:
A Comparative Evaluation of Complementing Components. In Proceedings of the 12th International Confer-
ence on Intelligent Text Processing and Computational Linguistics. Lecture Notes in Computer Science. Samos,
Greece, 24-30 March, 2013., pages 488?500.
G. Freyhoff, G. Hess, L. Kerr, B. Tronbacke, and K. Van Der Veken, 1998. Make it Simple, European Guide-
lines for the Production of Easy-toRead Information for People with Learning Disability. ILSMH European
Association, Brussels.
J. Karreman, T. van der Geest, and E. Buursink. 2007. Accessible website content guidelines for users with
intellectual disabilities. Journal of Applied Research in Intellectual Disabilities, 20:510?518.
R. I. Kittredge, 2003. Oxford Handbook of Computational Linguistics, chapter 23, Sub-languages and controlled
languages.
Mencap, 2002. Am I making myself clear? Mencap?s guidelines for accessible writing.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318.
PlainLanguage. 2011. Federal plain language guidelines.
H. Saggion, E. G?omez Mart??nez, E. Etayo, A. Anula, and L. Bourg. 2011. Text Simplification in Simplext:
Making Text More Accessible. Revista de la Sociedad Espa?nola para el Procesamiento del Lenguaje Natural,
47:341?342.
39
S. Spaulding. 1956. A Spanish Readability Formula. Modern Language Journal, 40:433?441.
I. Temnikova. 2012. Text Complexity and Text Simplification in the Crisis Management domain. Ph.D. thesis,
University of Wolverhampton, UK.
S.
?
Stajner and H. Saggion. 2013. Readability Indices for Automatic Evaluation of Text Simplification Systems: A
Feasability Study for Spanish. In Proceedings of the 6th International Joint Conference on Natural Language
Processing (IJCNLP 2013), Nagoya, Japan, 14-18 October 2013, pages 374?382.
40
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 53?63,
Dublin, Ireland, August 24th 2014.
Assessing Conformance of Manually Simplified Corpora with
User Requirements: the Case of Autistic Readers
Sanja
?
Stajner and Richard Evans and Iustin Dornescu
Research Group in Computational Linguistics
Research Institute of Information and Language Processing
University of Wolverhampton, UK
{SanjaStajner, R.J.Evans, I.Dornescu2}@wlv.ac.uk
Abstract
In the state of the art, there are scarce resources available to support development and evaluation
of automatic text simplification (TS) systems for specific target populations. These comprise
parallel corpora consisting of texts in their original form and in a form that is more accessible
for different categories of target reader, including neurotypical second language learners and
young readers. In this paper, we investigate the potential to exploit resources developed for such
readers to support the development of a text simplification system for use by people with autistic
spectrum disorders (ASD). We analysed four corpora in terms of nineteen linguistic features
which pose obstacles to reading comprehension for people with ASD. The results indicate that the
Britannica TS parallel corpus (aimed at young readers) and the Weekly Reader TS parallel corpus
(aimed at second language learners) may be suitable for training a TS system to assist people
with ASD. Two sets of classification experiments intended to discriminate between original and
simplified texts according to the nineteen features lent further support for those findings.
1 Introduction
As a fundamental human right, people with reading and comprehension difficulties are entitled to access
written information (UN, 2006). This entitlement enables better inclusion into society. However, the
vast majority of texts that such people encounter in their everyday life ? especially newswire texts ? are
lexically and syntactically very complex. Since the late nineties, several initiatives have emerged which
propose guidelines for producing plain, easy-to-read and more accessible documents. These include
the ?Federal Plain Language Guidelines?
1
, ?Make it Simple, European Guidelines for the Production of
Easy-to-Read Information for people with Learning Disability? (Freyhoff et al., 1998), ?Am I making
myself clear? Mencap?s guidelines for accessible writing?
2
, and the W3C ? Web Accessibility Initiative
guidelines
3
. However, manual adaptation of texts cannot match the speed with which new texts are pub-
lished on the web in order to provide up to date information. The aim of Automatic Text Simplification
(ATS) is to automatically (or at least semi-automatically) convert complex sentences into a more accessi-
ble form while preserving their original meaning. In the last twenty years, many ATS systems have been
proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank,
2006; Saggion et al., 2011; Inui et al., 2003; Alu??sio et al., 2008). Due to the scarcity of parallel corpora
of original and manually simplified texts, most of these systems are rule-based.
The emergence of Simple English Wikipedia (SEW)
4
, together with the existing English Wikipedia
(EW)
5
provided a large amount of parallel TS training data, which motivated a shift in English TS from
rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata,
2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf
2
http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf
3
http://www.w3.org/WAI/
4
http://simple.wikipedia.org/wiki/Main Page
5
http://wikipedia.org/wiki/Main Page
53
ever been made of the quality of the simplifications made in SEW and the usefulness of the transfor-
mations learned from EW?SEW parallel corpora for any of the specified target populations. The only
instructions given to the authors of SEW are to use Basic English vocabulary and shorter sentences. The
main page states that SEW is for everyone, including children and adults who are learning English. All
previously mentioned studies conducted on that corpus evaluated the quality of the generated output in
terms of grammaticality, meaning preservation, and simplicity, but not usefulness. Also, there have been
no comparisons of the types of transformations present in EW?SEW with any of the other TS corpora
in English which were simplified with a specific target population in mind, e.g. Encyclopedia Britan-
nica and its manually simplified versions for children ? Britannica Elementary (Barzilay and Elhadad,
2003)
6
, Guardian Weekly and its manually simplified versions for language learners (Allen, 2009), and
the FIRST corpus of various texts simplified for people with autism spectrum disorder (ASD)
7
.
In this study, we compare the original and simplified texts of the four aforementioned TS corpora in
terms of nineteen features which measure the complexity of texts for people with ASD. Although these
features were derived from user requirements for people with ASD, many of them are known to present
reading obstacles for other target populations as well (e.g. children or language learners). Given the lack
of parallel TS corpora for people with ASD, our main goal is to investigate whether the EW?SEW or the
other two corpora aimed at children and language learners could be used as training material for a TS
system to assist people with ASD and thus enable data-driven approaches (instead of the currently used
rule-based ones). In order to further support the results of this analysis, we conduct several classification
experiments in which we try to distinguish between original and simplified texts in each of the four
corpora, using the nineteen features.
2 The FIRST Project and User Requirements
Autistic Spectrum Disorders (ASD) are neurodevelopmental disorders characterised by qualitative im-
pairment in communication and stereotyped repetitive behaviour. People with ASD show a diverse range
of reading abilities: 5-10% have the capacity to read words from an early age without the need for
formal learning (hyperlexia) but many demonstrate reduced comprehension of what has been read (Volk-
mar and Wiesner, 2009). They may have difficulty inferring contextual information or may have trouble
understanding mental verbs, emotional language, and long sentences with complex syntactic structure
(Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the
FIRST project
8
to assist in the process of making texts more accessible for people with ASD. To achieve
this, three modues are exploited:
1. Structural complexity processor, which detects syntactically complex sentences and generates
alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014;
Dornescu et al., 2013).
2. Meaning disambiguator, which resolves pronominal references, performs word sense disambigua-
tion, and detects lexicalised (conventional) metaphors (Barbu et al., 2013).
3. Personalised document generator, which aggregates the output of processors 1 and 2 and gener-
ates additional elements such as glossaries, illustrative images, and document summaries.
The system, named Open Book, is deployed as an editing tool for healthcare and educational service
providers. It functions semi-automatically, exploiting the three processors and requiring the user to
authorise the application of the conversion operations. The system is required to assess the readability
of texts, not only to decide which texts should be converted, but also to assess the readability of texts
that are undergoing conversion. It is expected that people working to improve the accessibiity of a
given text will benefit from relevant feedback concerning the effects of the changes being introduced.
Automatic assessment of readability is one method by which such feedback can be delivered. In the
6
http://www.cs.columbia.edu/ noemie/alignment/
7
Available at: http://www.first-asd.eu/?q=system/files/FIRST D7.2 20130228 annex.pdf
8
www.first-asd.eu
54
context of improving the accessibility of texts, relevant feedback should indicate the extent to which
different versions of a text meet the particular requirements of intended readers.
User requirements were obtained through consulatation of 94 subjects meeting the strict DSM-IV cri-
teria for ASD and with IQ > 70. 43 user requirements were derived and assigned a reference code. The
requirements link linguistic phenomena to editing operations, such as deletion, explanation, or trans-
formation, that will convert the text to a more accessible form. The linguistic phenomena of concern
include instances of syntactic complexity such as long sentences containing more than 15 words (possi-
bly containing multiple copulative coordinated clauses (UR301), subordinate adjective clauses (UR302),
explicative clauses (UR303), non-initial adverbial clauses (UR307)), sentences containing passive verbs
(UR313), rarely used conjunctions and antithetic conjuncts (UR304, UR305, UR306), uncommon syn-
onyms of polysemic words (UR401, UR425, UR504, UR505, UR511), rarely-used symbols and punc-
tuation marks (UR311), anaphors, words containing more than 7 characters, adjectives ending with -ly,
long numerical expressions (UR417), negation (UR314), words more than 7 characters long and adverbs
with suffix -ly (UR317-319), anaphors, including pronouns (UR418-420).
Additional linguistic phenomena such as phraseological units (UR402, UR410, UR425, UR507), and
non-lexicalised metaphors (UR422, UR508), were also found to pose obstacles to reading comprehension
for people with ASD. At present, there is a scarcity of resources enabling accurate detection of these
items. For this reason, changes in the prevalence of these items in original and converted versions of
texts are not captured in this study. The full set of user requirements is detailed in Martos et al. (2013).
More generally, it is infrequent linguistic phenomena that cause the greatest difficulty.
3 Related Work
There have been several studies analysing the existing TS corpora. However, their main focus was on
determining necessary transformations in TS: for children (Bautista et al., 2011); for people with intel-
lectual disability (Drndarevi?c and Saggion, 2012); for language learners (Petersen and Ostendorf, 2007);
and for people with low literacy (Gasperin et al., 2009). Unfortunately, those studies are not directly
comparable (neither among themselves nor with our study), either because they focus on different types
of transformations (the study of Bautista et al. (2011) focuses on general transformations while the other
three studies focus on sentence transformations), or because they treat different languages (Spanish,
English, and Brazilian Portuguese).
Two previous studies most relevant to ours are those by Napoles and Dredze (2010), and by
?
Stajner
et al. (2013). Napoles and Dredze (2010) built a statistical classification system that discriminates
simple English from ordinary English, based on EW?SEW corpus. They used four different groups of
features: lexical, part-of-speech, surface, and syntactic parse features. The accuracy of the best classifier
(SVM) on the document classification task when using all features was 99.90%, while the accuracy
of the best classifier (maximum entropy) on the sentence classification task when using all features
was 80.80%. However, this study only demonstrated that it is fairly easy to discriminate sentences and
documents of EW from those of SEW. It did not investigate whether the simple English used in SEW
complies with the user requirements of any specific population with reading difficulties.
?
Stajner et al.
(2013) analysed a corpus of 37 newswire texts in Spanish and their manual simplifications aimed at
people with Down?s syndrome, compiled in the Simplext project
9
. They built a classification system that
discriminates the original texts from those which are simple with an F-measure of 1.00 using the SVM,
and only seven features: average number of punctuation marks (not counting end of sentence markers),
numerical expressions, average word length in characters, the ratio of simple and complex sentences,
sentence complexity index, lexical density and lexical richness. They reported the average sentence
length as being the feature with the best discriminative power, leading to an F-measure of 0.99 when
used on its own.
In spite of the many linguistic phenomena that pose obstacles to reading comprehension for different
target populations, there have been almost no studies investigating whether a TS system built with a
specific target population in mind could be successfully applied ? or adapted ? to a different target
9
www.simplext.es
55
Corpus Aimed at Version Code Texts SentPerText WordsPerText
Weekly Reader Language learners
Original Learn.-O 100 39.41 ? 14.43 746.83 ? 174.25
Simple Learn.-S 100 38.40?12.59 621.11 ? 157.17
Enc. Britannica Children
Original Brit.-O 20 27.10 ? 8.91 628.30 ? 198.19
Simple Brit.-S 20 26.45 ? 9.35 382.35 ? 127.69
Wikipedia Various
Original Wiki-O 110 34.55 ? 1.87 716.57 ? 117.82
Simple Wiki-S 110 34.49 ? 1.82 675.07 ? 107.03
FIRST People with ASD
Original FIRST-O 25 13.64 ? 3.95 285.68 ? 34.46
Simple FIRST-S 25 22.92 ? 4.79 311.36 ? 76.82
Table 1: Corpora characteristics
population. The only exception to this is the study by
?
Stajner and Saggion (2013), which demonstrated
that two classifiers ? one which discriminates sentences which should be split from those which should
be left unsplit, and another which discriminates sentences which should be deleted from those which
should be preserved ? can successfully be trained on one type of corpora and applied to the other. Both
corpora consisted of texts in Spanish, one containing newswire texts manually simplified for people with
Down?s syndrome, and the other various text genres manually simplified for people with ASD.
Motivated by those previous studies and the lack of parallel corpora aimed specifically to people with
ASD, in this paper, we investigate whether some of already existing corpora for TS in English could
potentially be used for building a data-driven TS system for this target population.
4 Methodology
The corpora, features, and experimental settings used in this study are described in Sections 4.1?4.3.
4.1 Corpora
Four parallel corpora of original and manually simplified texts for different target populations were used
in this study (Table 1):
1. The corpus of 100 texts from Weekly Reader and their manual simplifications provided by Macmil-
lan English Campus and Onestopenglish
10
aimed at foreign language learners. The corpus is divided
into three sub-corpora ? advanced, intermediate and elementary ? each representing a different level
of simplification. Given that the other three corpora used in this study contain original texts and only
one level of simplification, we only used the texts from the advanced (henceforth original) and ele-
mentary (henceforth simplified) levels. A more detailed description of this corpus can be found in
(Allen, 2009).
2. The corpus of 20 texts from the Encyclopedia Britannica and their manually simplified versions
aimed at children ? Britannica Elementary (Barzilay and Elhadad, 2003)
11
.
3. The corpus of 110 randomly selected corresponding articles from EW and SEW. Here, it is impor-
tant to note that, in general, articles from SEW do not represent direct simplifications of the articles
from EW, they just have a matching topic. For this reason, we did not use complete EW and SEW
articles. We only used those sentences in original and simplified versions, which existed in the
sentence-aligned parallel corpora version 2.0
12
(Kauchak, 2013).
4. The corpus of 25 texts on various topics manually simplified for people with autism, compiled in
the FIRST project
13
, for the purpose of a piloting task
14
. The texts were simplified by carers of
people with ASD in accordance with specified guidelines.
10
http://www.onestopenglish.com/
11
http://www.cs.columbia.edu/ noemie/alignment/
12
http://www.cs.middlebury.edu/ dkauchak/simplification/
13
www.first-asd.eu
14
http://www.first-asd.eu/?q=system/files/FIRST D7.2 20130228 annex.pdf
56
4.2 Text Features Relevant to User Requirements
In this paper, a set of 15 text complexity measures and 4 formulae exploiting these measures was used
to estimate the accessibility of the texts. These features quantify the occurrence of linguistic phenomena
identified as potential obstacles to reading comprehension for people with ASD. The set of features is
presented in Table 2. The set of formulae is presented in Table 3. In every case, accessible texts are
expected to have smaller values of each metric.
# Code Linguistic feature Explanation/relevance
1 Illative Illative conjunctions Indicators of syntactic complexity, linking clauses.
2 CompConj Comparative conjunctions [UR304-306]
3 AdvConj Adversative conjunctions
4 LongSent Long sentences Motivated by the assumption that deriving the propositions in
5 Semicol Semicolons/suspension
points
complex sentences is more difficult than deriving connections be-
tween related propositions expressed in simple sentences
6 Passive Passive verbs (Arya et al., 2011). [UR309-310, UR313]
7 UnPunc Unusual punctuation Indicates syntactic complexity, ellipsis, alternatives, and mathe-
matical expressions [UR311]
8 Negations Negation The sum of adverbial and morphological negations (?Make it Sim-
ple? (Freyhoff et al., 1998), though contrary to the findings of Tat-
tamanti (2008)) [UR314]
9 Senses Possible senses The sum over all tokens in the text of the total number of possible
senses of each token. [UR401, UR425, UR504-505, UR511]
10 PolyW Polysemic words Words with two or more senses listed in WordNet. [UR401,
UR425, UR504, UR505, UR511]
11 Infreq Infrequent words Words that are not among the 5000 most frequent words in English
[UR304-306, UR401, UR425, UR504-505, UR511]
12 NumExp Numerical expressions Numbers written as sequences of words rather than digits [UR417]
13 Pron Pronouns Studies have shown that people with ASD can have
14 DefDescr Definite descriptions difficulty processing anaphora (Fine et al., 1994) [UR418-420]
15 SylLongW Long words Words with more than three syllables [UR317-319]
Table 2: Complexity measures (1 ? words such as therefore and hence; 2 ? words such as equally and
correspondingly; 3 ? words such as although and conversely; 4 ? sentences more than 15 words long; 8 ?
negative adverbials and negative prefixes such as un- and dis-; 11 ? derived from Wiktionary frequency
lists for English
16
)
# Code Metric Formula Relevance
16 PolyType Polysemic type ratio
ptyp
typ
Indicates the proportion of the text vocabulary that is
polysemous. [UR401, UR425, UR504-505, UR511]
17 CommaInd Comma index
10?c
w
Indicates the average syntactic complexity of the
sentences in the text [UR301-303, UR307]
18 WordsPerSent Words per sentence
w
s
Indicates the average length of the sentences in the text
[UR309]
19 TypeTokRat Type-token ratio
typ
tok
Indicate the range of vocabulary used in the text
[UR401, UR425, UR504, UR505, UR511]
Table 3: Text complexity formulae (w ? the number of words in the text; s ? the number of sentences in
the text; ptyp ? the number of polysemic word types in the text; c ? the number of commas in the text;
typ ? the number of word types in the text; tok ? the number of word tokens in the text)
Scores for these measures, and the text complexity formulae that exploit them where obtained auto-
matically by the tokeniser, part-of-speech tagger, and lemmatiser distributed with LT TTT2 (Grover et al.,
2000). Detection of the features used to derive complexity measures also involved the use of additional
resources such as WordNet, gazetteers of rare illative, comparative, and adversative conjunctions, nega-
tives (words and prefixes) and a set of lexico-syntactic patterns used to detect passive verbs (presented in
Figure 1).
57
am/are/is/was/were w
RB
* w
{V BN |V BD}
am/are/is/was/were w
RB
* being w
RB
* w
{V BN |V BD}
have/has/had w
RB
* been w
RB
* w
{V BN |V BD}
will w
RB
* be w
RB
* w
{V BN |V BD}
am/is/are w
RB
* going w
RB
* to w
RB
* be w
RB
* w
{V BN |V BD}
w
MD
w
RB
* be w
{V BN |V BD}
w
MD
w
RB
* have w
RB
* been w
RB
* w
{V BN |V BD}
Figure 1: Lexico-syntactic patterns used to detect passive verbs (?*? indicates zero or more repetitions of
the item it is attached to, while RB, V BN , V BD, and MD are Penn treebank tags returned by the LT
TTT PoS tagger: RB ? adverb; V BN ? past participle; V BD ? past tense; and MD ? modal verb)
4.3 Experiments
Two sets of experiments were performed in this study:
1. Analysis of differences between original and simplified texts in terms of nineteen selected features
(Section 4.2) across four corpora (Section 4.1). Statistical difference was measured using the t-
test for related samples in the cases where the features were normally distributed, and using the
related samples Wilcoxon signed rank test otherwise. Normality of the data was tested using the
Shapiro-Wilk test of normality, which is preferred over the Kolmogorov-Smirnov test when the
dataset contains less than 2,000 elements. All tests were performed in SPSS. Features 1?15 were
first normalised (as an average per sentence) in order to allow a fair comparison across the four TS
corpora (text length in words and sentences differed significantly across different corpora).
2. Classification experiments with the aim of discriminating original from simplified texts using the
nineteen selected features. All experiments were conducted using the Weka Experimenter (Witten
and Frank, 2005; Hall et al., 2009) in 10-fold cross-validation setup with 10 repetitions, using four
different classification algorithms: NB ? NaiveBayes (John and Langley, 1995), SMO ? Weka im-
plementation of Support Vector Machines (Keerthi et al., 2001) with normalisation, JRip ? a propo-
sitional rule learner (Cohen, 1995), and J48 ? Weka implementation of C4.5 (Quinlan, 1993). The
statistical significance of the observed differences in F-measures obtained by different algorithms
was calculated using the corrected paired t-test provided in the Weka Experimenter.
The TS system in FIRST is not only supposed to decide which texts should be converted, but also
to assess the readability of texts that are undergoing conversion. It is expected that people working to
improve the accessibility of a given text will benefit from relevant feedback concerning the effects of the
changes being introduced. Automatic assessment of readability is one method by which such feedback
can be delivered. Deriving a subset of features which, when trained with an appropriate classification
algorithm, can categorize a given text as either ?original? or ?simplified?, would facilitate automatic eval-
uation of TS systems. The resulting classifier would be suitable for assessing whether those systems
perform an appropriate level of simplification. This could serve as a rough estimation, an efficient first
step offering a quick evaluation prior to being tested with real users.
5 Results and Discussion
The results of the two sets of experiments are presented and discussed in the next two subsections.
5.1 Analysis of the Features across the Corpora
Mean values (with standard deviations) of each of the first eight features on each sub-corpus are dis-
played in Table 4. The number of unusual punctuation marks (UnPunc) is the only feature whose value
does not differ significantly between the original and simplified versions of the texts in any of the four
corpora. This feature was thus excluded from further classification experiments. The number of com-
parative conjunctions per sentence (CompConj) significantly decreases only when simplifying texts for
58
Corpus Illative CompConj AdvConj LongSent Semicol UnPunc Passive Negations
Lear.-O 0.24?0.12 0.04?0.13 0.21?0.08 0.62?0.15 0.03?0.05 0.00?0.01 0.21?0.10 0.33?0.15
Lear.-S 0.20?0.13 0.03?0.09 0.19?0.09 0.51?0.14 *0.03?0.05 0.00?0.01 0.09?0.09 0.26?0.14
Brit.-O 0.13?0.09 0.15?0.26 0.14?0.07 0.72?0.11 0.13?0.20 0?0 0.33?0.10 0.28?0.16
Brit.-S 0.08?0.05 *0.02?0.10 0.06?0.04 0.38?0.11 0.00?0.02 0?0 0.25?0.12 0.14?0.09
Wiki-O 0.20?0.11 0.11?0.19 0.16?0.10 0.65?0.12 0.04?0.04 0.04?0.10 0.34?0.15 0.32?0.23
Wiki-S 0.18?0.11 0.11?0.20 0.14?0.09 0.62?0.12 0.03?0.04 0.03?0.10 0.33?0.15 0.29?0.24
FIRST-O 0.18?0.14 0.06?0.19 0.18?0.15 0.68?0.15 0.03?0.10 0.01?0.02 0.27?0.23 0.42?0.28
FIRST-S 0.11?0.10 0.01?0.06 0.09?0.07 0.33?0.19 0.00?0.01 0?0 0.20?0.15 0.22?0.13
Table 4: Mean values (with standard deviation) of features 1?8 across the corpora (O ? the original texts
in the corpora; S ? the simplified texts in the corpora; bold ? significantly different from the value on the
original texts at a 0.01 level of significance; *bold ? significantly different from the value on the original
texts at a 0.05 level of significance (but not at 0.01); ?0.00? ? a value different from zero which rounded
at two decimals gives 0.00; ?0? ? a value equal to zero)
Corpus Senses PolyW Infreq NumExp Pron DefDescr SylLongW
Lear.-O 73.95?12.32 9.37?1.72 5.64?1.33 0.18?0.11 0.97?0.40 1.86?0.54 1.12?0.28
Lear.-S 64.21?11.16 7.85?1.45 4.14?1.01 0.16?0.10 0.90?0.37 1.62?0.45 0.92?0.27
Brit.-O 67.51? 8.83 9.87?1.15 9.37?1.10 0.18?0.12 0.40?0.18 2.86?0.44 1.45?0.20
Brit.-S 48.68? 4.17 6.48?0.57 5.39?0.58 0.09?0.06 0.28?0.13 1.86?0.20 1.17?0.19
Wiki-O 67.70?12.96 9.13?1.61 7.86?1.63 0.18?0.16 0.67?0.43 2.08?0.58 1.24?0.38
Wiki-S 68.20?13.56 8.71?1.56 7.16?1.51 *0.17?0.16 0.68?0.44 1.97?0.54 1.10?0.42
FIRST-O 82.28?24.20 10.16?2.65 7.11?2.72 0.19?0.19 1.05?0.73 2.12?0.92 1.17?0.58
FIRST-S 57.13?15.96 6.47?1.77 3.92?1.56 0.09?0.07 *0.82?0.44 1.62?0.54 *0.92?0.43
Table 5: Mean values (with standard deviation) of features 9?15 across the corpora (O ? the original texts
in the corpora; S ? the simplified texts in the corpora; bold ? significantly different from the value on the
original texts at a 0.01 level of significance; *bold ? significantly different from the value on the original
texts at a 0.05 level of significance (but not at 0.01))
children (Brit.-S), while the average number of passive constructions per sentence (Passive) decreases
when simplifying for both children (Brit.-S) and language learners (Lear.-S). It is interesting to note that
the average number of passive constructions per sentence (Passive) does not decrease in the EW?SEW
corpus and that its value on the simplified versions of Wikipedia articles (Wiki-S) is significantly higher
than on Brit.-S and Lear.-S, although SEW claims to provide articles simplified for both those target
populations. It can also be observed that the fact that all four corpora were reported to have significant
differences between original and simplified texts in terms of features Illative, AdvConj, LongSent, and
Negations does not necessarily mean that the average number of occurrences of those features is similar
in all four simplified corpora. The values of Illative, AdvConj, and LongSent in the simplified versions
of the texts in the FIRST corpus seem to correspond best to those in the simplified versions of the texts
in the Britannica corpus (Brit.-S). The value of Negations in FIRST-S, however, seems to correspond
best to that in Lear.-S. This suggests that if we wish to build a component of our TS system (to assist
people with ASD) which would remove negations (Negations), we should train it on the sentence pairs
from the corpora with simplifications aimed at second language learners. If we wish to build a com-
ponent which would remove illative conjunctions (Illative), adversative conjuctions (AdvConj), or long
sentences (LongSent), we should probably train it on the sentence pairs from the corpora with simplifi-
cations aimed at young readers.
The number of occurrences per sentence of features 9?15 in the original versions of the texts was sig-
nificantly higher than in the simplified versions of the texts in all four corpora, with only two exceptions
? features Senses and Pron in the EW?SEW corpus (Wiki-O and Wiki-S), as can be observed in Table
5. Again, the mean values of all features in the simplified versions of the texts in the FIRST corpora
FIRST-S, seems to correspond better to the simplified versions of Encyclopedia Britannica (Brit.-S) and
59
Corpus PolyType CommaInd WordsPerSent TypeTokRat
Lear.-O 0.76?0.04 0.56?0.12 19.91?3.46 0.51?0.04
Lear.-S 0.77?0.04 0.46?0.15 16.69?2.78 0.47?0.05
Brit.-O 0.69?0.03 0.78?0.15 23.46?2.78 0.51?0.04
Brit.-S *0.71?0.02 *0.67?0.14 14.61?1.21 0.55?0.04
Wiki-O 0.71?0.05 0.65?0.15 20.73?3.16 0.48?0.05
Wiki-S 0.71?0.05 0.60?0.16 19.57?2.90 *0.48?0.05
FIRST-O 0.73?0.04 0.51?0.18 22.20?5.43 0.59?0.05
FIRST-S 0.75?0.06 0.19?0.15 13.86?3.41 0.53?0.08
Table 6: Mean values (with standard deviation) of features 16?19 across the corpora (O ? the original
texts in the corpora; S ? the simplified texts in the corpora; bold and *bold ? used in the same way as in
the previous two tables)
Weekly Readers (Lear.-S) than to those in the simplified versions of the Wikipedia articles (Wiki-S). It is
also interesting to note that many of the features (LongSent, Negations, Senses, PolyW, Infreq, DefDesc)
seem to have a significantly higher number of occurrences per sentence in the simplified versions of
the Wikipedia articles (Wiki-S) than in the simplified versions of Encyclopedia Britannica (Brit.-S) and
Weekly Reader (Lear.-S).
The comma index (CommaInd), type-token ratio (TypeTokRat), and the average number of words per
sentence (WordsPerSent) were found to be significantly higher in original texts than in their simplified
versions in all four corpora (Table 6). However, the values of those three text complexity formulae were
not similar in the simplified texts across the four corpora. In terms of the average number of words
per sentence (WordsPerSent) and the type-token ratio (TypeTokRat), the simplified versions of the texts
in the FIRST corpora (FIRST-S) seem to correspond better to the texts simplified for young readers
(Brit.-S), than to those simplified for second language learners (Lear.-S) and those aimed at various
target populations (Brit.-S). The comma index (CommaInd) obtained for simplified texts in the FIRST
corpora was several times lower than that obtained for simplified texts in the three other corpora. The
polysemic type ratio (PolyType) was not significantly different in original and in simplified texts of the
FIRST corpora (Table 6). The higher polysemic type ratio (PolyType) for simplified rather than original
versions of the texts in the other three corpora was unexpected, as it is usually assumed that polysemous
words can pose an obstacle for various target populations. However, it is important to bear in mind that
polysemous words usually pose an obstacle when conveying one of their infrequently used meanings.
Findings in cognitive psychology indicate that the words with the highest number of possible meanings
are actually understood more quickly, due to their high frequency (Jastrzembski, 1981). A common
lexical simplification strategy is to replace infrequent words with their more frequent synonyms, and
long words with their shorter synonyms. This strategy leads to a higher polysemic type ratio (PolyType)
in simplified versions of the texts as the shorter words are usually more frequent (Balota et al., 2004),
and frequent words tend to be more polysemous than infrequent ones (Glanzer and Bowles, 1976).
5.2 Classification between Original and Simplified Texts
Classification experiments were conducted using two different sets of features on each of the corpora:
1. all ? all 18 features (UnPunc was excluded as it was not reported as significant for any of the
corpora)
2. best ? 11 features which were reported as significant for all four corpora (Illative, AdvConj,
LongSent, Negations, PolyW, NumExp, DefDescr, SylLongW, CommaInd, WordsPerSent, Type-
TokRat)
As can be observed from Table 7, use of the SMO-n classification algorithm using the subset of 11
best features achieves perfect 1.00 F-measure for discriminating original from simplified versions of the
Encyclopedia Britannica. The same classification algorithm performs less well on the FIRST and Weekly
Readers corpora (though still quite well), while it performs significantly worse on the Wikipedia corpus.
60
The baseline (which chooses majority class) would be 0.50 in all cases. These results indicate that the
Encyclopedia Britannica TS parallel corpus, and possibly the Weekly Readers TS parallel corpus, may
serve as suitable training material for building a TS system (or at least some of its components) aimed at
people with ASD.
Dataset SMO-n NB JRip J48
Brit-all 0.98?0.09 0.94?0.12 0.94?0.14 0.97?0.11
Brit-best 1.00?0.00 0.99?0.05 0.94?0.13 0.97?0.11
FIRST-all 0.88?0.15 0.86?0.19 0.79?0.23 0.75?0.25
FIRST-best 0.88?0.15 0.85?0.20 0.78?0.25 0.76?0.25
Lear-all 0.81?0.08 0.74?0.10* 0.75?0.07* 0.72?0.10*
Lear-best 0.77?0.08 0.74?0.11 0.70?0.10* 0.73?0.10
Wiki-all 0.54?0.12 0.50?0.12 0.51?0.14 0.35?0.20*
Wiki-best 0.55?0.13 0.55?0.12 0.51?0.12 0.33?0.20*
Table 7: F-measure with standard deviation in a 10-fold cross-validation setup with 10 repetitions for
four classification algorithms: SMO-n, NB, JRip, and J48 (* ? statistically significant degradation in
comparison with SMO-n)
6 Conclusions
Automatic Text Simplification (ATS) aims to convert complex texts into a simpler form, which is more
accessible to a wider audience. Due to the lack of parallel corpora for TS consisting of original and
manually simplified texts, most of the ATS systems for specific target populations are still rule-based.
Our main goal was to explore whether some of the existing TS parallel corpora in English, aimed at dif-
ferent audiences (children ? Encyclopedia Britannica, language learners ? Weekly Reader, and various ?
Wikipedia) could be used as training material to build a TS system aimed at people with ASD. We anal-
ysed the four corpora (FIRST, Britannica, Weekly Reader, and Wikipedia) in terms of nineteen linguistic
features which pose obstacles to reading comprehension for people with ASD. The preliminary results
indicate that the Britannica TS parallel corpus, and possibly the Weekly Reader TS parallel corpus, could
be used to train a TS system aimed at people with ASD. Two sets of classification experiments which
tried to discriminate original from simplified texts according to the nineteen features derived from user
requirements further supported those findings. The results of the classification experiments indicated
that the SVM classifier trained on the Britannica corpus might be suitable for discriminating original
from simplified texts for people with ASD, and thus might be used as the initial evaluation of the texts
simplified by the TS system developed in the FIRST project.
Acknowledgements
The research described in this paper was partially funded by the European Commission under the Sev-
enth (FP7-2007-2013) Framework Programme for Research and Technological Development (FP7-ICT-
2011.5.5 FIRST 287607).
References
D. Allen. 2009. A Corpus-Based Study of the Role of Relative Clauses in the Simplification of News Texts for
Learners of English. System, 37(4):585?599.
S. M. Alu??sio, L. Specia, T. A.S. Pardo, E. G. Maziero, and R. P.M. Fortes. 2008. Towards brazilian portuguese
automatic text simplification systems. In Proceedings of the eighth ACM symposium on Document engineering,
DocEng ?08, pages 240?248, New York, NY, USA. ACM.
D. J. Arya, Elfrieda H. Hiebert, and P. D. Pearson. 2011. The effects of syntactic and lexical complexity on
the comprehension of elementary science texts. International Electronic Journal of Elementary Education, 4
(1):107?125.
61
D. Balota, M. J. Cortese, S. D. Sergent-Marshall, D. H. Spieler, and M. J. Yap. 2004. Visual word recognition of
single-syllabe words. Journal of Experimental Psychology: General, 133:283?316.
E. Barbu, M. Mart??n-Valdivia, L. Alfonso, and U. Lopez. 2013. Open book: a tool for helping asd users? semantic
comprehension. In Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual
Accessibility (NLP4ITA), pages 11?19, Atlanta, US. Association for Computational Linguistics.
R. Barzilay and N. Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the
2003 conference on Empirical methods in natural language processing, EMNLP ?03, pages 25?32, Stroudsburg,
PA, USA. Association for Computational Linguistics.
S. Bautista, C. Le?on, R. Herv?as, and P. Gerv?as. 2011. Empirical identification of text simplification strategies for
reading-impaired people. In European Conference for the Advancement of Assistive Technology.
O. Biran, S. Brody, and N. Elhadad. 2011. Putting it Simply: a Context-Aware Approach to Lexical Simplification.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 496?501, Portland, Oregon, USA. Association for Computational Linguistics.
J. Carroll, G. Minnen, Y. Canning, S. Devlin, and J. Tait. 1998. Practical Simplification of English Newspaper
Text to Assist Aphasic Readers. In Proceedings of AAAI-98 Workshop on Integrating Artificial Intelligence and
Assistive Technology, pages 7?10.
W. Cohen. 1995. Fast Effective Rule Induction. In Proceedings of the Twelfth International Conference on
Machine Learning, pages 115?123.
W. Coster and D. Kauchak. 2011. Learning to Simplify Sentences Using Wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics, pages 1?9.
S. Devlin and G. Unthank. 2006. Helping aphasic people process online information. In Proceedings of the 8th
international ACM SIGACCESS conference on Computers and accessibility, Assets ?06, pages 225?226, New
York, NY, USA. ACM.
I. Dornescu, R. Evans, and C. Orasan. 2013. A Tagging Approach to Identify Complex Constituents for Text
Simplification. In Proceedings of Recent Advances in Natural Language Processing, pages 221 ? 229, Hissar,
Bulgaria.
B Drndarevi?c and H. Saggion. 2012. Reducing Text Complexity through Automatic Lexical Simplification: an
Empirical Study for Spanish. SEPLN Journal, 49.
R. Evans, C. Orasan, and I. Dornescu. 2014. An evaluation of syntactic simplification rules for people with
autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader
Populations (PITR), pages 131?140, Gothenburg, Sweden, April. Association for Computational Linguistics.
J. Fine, G. Bartolucci, P. Szatmari, and G. Ginsberg. 1994. Cohesive discourse in pervasive developmental
disorders. Journal of Autism and Developmental Disorders, 24:315?329.
G. Freyhoff, G. Hess, L. Kerr, B. Tronbacke, and K. Van Der Veken, 1998. Make it Simple, European Guide-
lines for the Production of Easy-toRead Information for People with Learning Disability. ILSMH European
Association, Brussels.
C. Gasperin, L. Specia, T. Pereira, and S.M. Alu??sio. 2009. Learning When to Simplify Sentences for Natural Text
Simplification. In Proceedings of the Encontro Nacional de Inteligncia Artificial (ENIA-2009), Bento Gonalves,
Brazil., pages 809?818.
M. Glanzer and N. Bowles. 1976. Analysis of the word frequency effect in recognition memory. Journal of
Experimental Psychology: Human Learning and Memory, 2:21?31.
C. Grover, C. Matheson, A. Mikheev, and M. Moens. 2000. Lt ttt - a flexible tokenisation tool. In In Proceedings
of Second International Conference on Language Resources and Evaluation, pages 1147?1154.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The weka data mining
software: an update. SIGKDD Explor. Newsl., 11:10?18, November.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura. 2003. Text simplification for reading assistance: a project
note. In Proceedings of the second international workshop on Paraphrasing - Volume 16, PARAPHRASE ?03,
pages 9?16, Stroudsburg, PA, USA. Association for Computational Linguistics.
62
J. Jastrzembski. 1981. Multiple meaning, number or related meanings, frequency of occurrence and the lexicon.
Cognitive Psychology, 13:278?305.
G. H. John and P. Langley. 1995. Estimating Continuous Distributions in Bayesian Classifiers. In Proceedings of
the Eleventh Conference on Uncertainty in Artificial Intelligence, pages 338?345.
D. Kauchak. 2013. Improving text simplification language modeling using unsimplified text data. In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1537?1546, Sofia, Bulgaria, August. Association for Computational Linguistics.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. 2001. Improvements to Platt?s SMO
Algorithm for SVM Classifier Design. Neural Computation, 13(3):637?649.
S. T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J. Hagerman, and L. Abbeduto. 2012. Syntactic comprehension
in boys with autism spectrum disorders: Evidence from specific constructions. In Proceedings of the 2012
International Meeting for Autism Research, Athens, Greece. International Society for Autism Research.
J. Martos, S. Freire, A. Gonz?alez, D. Gil, R. Evans, V. Jordanova, A. Cerga, A. Shishkova, and C. Orasan. 2013.
FIRST Deliverable - User preferences: Updated. Technical Report D2.2, Deletrea, Madrid, Spain.
C. Napoles and M. Dredze. 2010. Learning simple wikipedia: a cogitation in ascertaining abecedarian language.
In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Pro-
cesses and Authoring Aids, CL&W ?10, pages 42?50, Stroudsburg, PA, USA. Association for Computational
Linguistics.
S. E. Petersen and M. Ostendorf. 2007. Text Simplification for Language Learners: A Corpus Analysis. In
Proceedings of Workshop on Speech and Language Technology for Education.
R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.
H. Saggion, E. G?omez Mart??nez, E. Etayo, A. Anula, and L. Bourg. 2011. Text Simplification in Simplext:
Making Text More Accessible. Revista de la Sociedad Espa?nola para el Procesamiento del Lenguaje Natural,
47:341?342.
H. Tager-Flusberg. 1981. Sentence comprehension in autistic children. Applied Psycholinguistics, 2:1:5?24.
M. Tattamanti, R. Manenti, P. A. Della Rosa, A. Falini, D. Perani, S. Cappa, and A. Moro. 2008. Negation in the
brain: Modulating action representations. NeuroImage, 43 (2008):358?367.
UN. 2006. Convention on the rigths of persons with disabilities.
F. R. Volkmar and L. Wiesner. 2009. A Practical Guide to Autism. Wiley, Hoboken, NJ, 2nd edition.
S.
?
Stajner and H. Saggion. 2013. Adapting Text Simplification Decisions to Different Text Genres and Target
Users. Procesamiento del Lenguaje Natural, 51:135?142.
S.
?
Stajner, B. Drndarevi?c, and H. Saggion. 2013. Corpus-based Sentence Deletion and Split Decisions for Spanish
Text Simplification. Computaci?on y Systemas, 17(2):251?262.
I. H. Witten and E. Frank. 2005. Data mining: practical machine learning tools and techniques. Morgan Kauf-
mann Publishers.
K. Woodsend and M. Lapata. 2011. Learning to Simplify Sentences with Quasi-Synchronous Grammar and
Integer Programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing (EMNLP).
S. Wubben, A. van den Bosch, and E. Krahmer. 2012. Sentence simplification by monolingual machine transla-
tion. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 1015?1024, Stroudsburg, PA, USA. Association for Computational Linguistics.
M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and L. Lee. 2010. For the sake of simplicity: unsupervised
extraction of lexical simplifications from wikipedia. In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, HLT ?10, pages
365?368, Stroudsburg, PA, USA. Association for Computational Linguistics.
Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Monolingual Tree-based Translation Model for Sentence Sim-
plification. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),
pages 1353?1361.
63
