Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 953?960
Manchester, August 2008
Using Syntactic Information for Improving Why-Question Answering
Suzan Verberne, Lou Boves, Nelleke Oostdijk and Peter-Arno Coppen
Department of Linguistics
Radboud University Nijmegen
s.verberne@let.ru.nl
Abstract
In this paper, we extend an existing para-
graph retrieval approach to why-question
answering. The starting-point is a system
that retrieves a relevant answer for 73%
of the test questions. However, in 41%
of these cases, the highest ranked relevant
answer is not ranked in the top-10. We
aim to improve the ranking by adding a re-
ranking module. For re-ranking we con-
sider 31 features pertaining to the syntactic
structure of the question and the candidate
answer. We find a significant improvement
over the baseline for both success@10 and
MRR@150. The most important features
for re-ranking are the baseline score, the
presence of cue words, the question?s main
verb, and the relation between question fo-
cus and document title.
1 Introduction
Recently, some research has been directed at prob-
lems involved in why-question answering (why-
QA). About 5% of all questions asked to QA
systems are why-questions (Hovy et al, 2002).
They need a different approach from factoid ques-
tions, since their answers cannot be stated in a sin-
gle phrase. Instead, a passage retrieval approach
seems more suitable. In (Verberne et al, 2008),
we proposed an approach to why-QA that is based
on paragraph retrieval. We reported mediocre per-
formance and suggested that adding linguistic in-
formation may improve ranking power.
c
?Suzan Verberne, 2008. Licensed under the Creative
Commons Attribution-Noncommercial-Share Alike 3.0 Un-
ported license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
In the present paper, we implement a simi-
lar paragraph retrieval approach and extend it by
adding a re-ranking module based on structural lin-
guistic information. Our aim is to find out whether
syntactic knowledge is relevant for discovering re-
lations between question and answer, and if so,
which type of information is the most beneficial.
In the following sections, we first discuss related
work (section 2). In sections 3 and 4, we introduce
the data that we used for development purposes
and the baseline retrieval and ranking method that
we implemented. In section 5, we present our re-
ranking method and the results obtained, followed
by a discussion in section 6, and directions for fur-
ther research in section 7.
2 Related work
A substantial amount of work has been done in
improving QA by adding syntactic information
(Tiedemann, 2005; Quarteroni et al, 2007; Hi-
gashinaka and Isozaki, 2008). All these studies
show that syntactic information gives a small but
significant improvement on top of the traditional
bag-of-words (BOW) approaches.
The work of (Higashinaka and Isozaki, 2008)
focuses on the problem of ranking candidate an-
swer paragraphs for Japanese why-questions. They
find a success@10 score of 70.3% with an MRR
of 0.328. They conclude that their system for
Japanese is the best-performing fully implemented
why-QA system. In (Tiedemann, 2005), passage
retrieval for Dutch factoid QA is enriched with
syntactic information from dependency structures.
The baseline approach, using only the BOW, re-
sulted in an MRR of 0.342. With the addition of
syntactic structure, MRR improved to 0.406.
The work by (Quarteroni et al, 2007) consid-
ers the problem of answering definition questions.
953
They use predicate-argument structures (PAS) for
improved answer ranking. Their results show that
PAS make a very small contribution compared to
BOW only (F-scores 70.7% vs. 69.3%).
The contribution of this paper is twofold: (1) we
consider the relatively new problem of why-QA for
English and (2) we not only improve a simple pas-
sage retrieval approach by adding syntactic infor-
mation but we also perform extensive feature se-
lection in order to find out which syntactic features
contribute to answer ranking and to what extent.
3 Data
As data for developing and testing our system
for why-QA, we use the Webclopedia question set
by (Hovy et al, 2002). This set contains ques-
tions that were asked to the online QA system
answers.com. 805 of these questions are why-
questions. As answer corpus, we use the off-line
Wikipedia XML corpus, which consists of 659,388
articles (Denoyer and Gallinari, 2006). We manu-
ally inspect a sample of 400 of the Webclopedia
why-questions. Of these, 93 have an answer in the
Wikipedia corpus. Manual extraction of one rele-
vant answer for each of these questions results in a
set of 93 why-questions and their reference answer.
We also save the title of the Wikipedia article in
which each of the answers is embedded, in order
to be able to evaluate document retrieval together
with answer retrieval.
4 Paragraph retrieval for why-QA
4.1 Baseline method
We index the Wikipedia XML corpus using the
Wumpus Search Engine (Buttcher, 2007). In
Wumpus, queries can be formulated in the GCL
format, which is especially geared to retrieving
XML items. Since we consider paragraphs as re-
trieval units, we let the engine retrieve text frag-
ments marked with ?p? as candidate answers.
We implement a baseline method for question
analysis in which first stop words are removed1.
Also, any punctuation is removed from the ques-
tion. What remains is a set of question content
words. Next, we automatically create a query for
each question that retrieves paragraphs containing
(a subset of) these question terms. For ranking
1To this end the stop word list is used that can be found
at http://marlodge.supanet.com/museum/ funcword.html. We
use all categories except the numbers and the word why
the paragraphs retrieved, we use the QAP algo-
rithm created by MultiText, which has been im-
plemented in Wumpus. QAP is a passage scor-
ing algorithm specifically developed for QA tasks
(Buttcher et al, 2004). For each question, we re-
trieve and rank the top 150 of highest scoring an-
swer candidates.
4.2 Evaluation method
For evaluation of the results, we perform manual
assessment of all answers retrieved, starting at the
highest-ranked answer and ending as soon as we
encounter a relevant answer2. Then we count the
proportion of questions that have at least one rele-
vant answer in the top n of the results for n = 10
and n = 150, giving us success@10 and suc-
cess@150. For the highest ranked relevant answer
per question, we determine the reciprocal rank
(RR). If there is no relevant answer retrieved by
the system at n = 150, the RR is 0. Over all ques-
tions, we calculate the Mean RR (MRR@150).
We also measure the performance of our system
for document retrieval: the proportion of questions
for which at least one of the answers in the top 10
comes from the reference document (success@10
for document retrieval) and the MRR@150 for the
highest position of the reference document3.
4.3 Results and discussion
Table 1: Baseline results for the why passage retrieval sys-
tem for answer retrieval and document retrieval in terms of
success@10, success@150 and MRR@150
S@10 S@150 MRR@150
Answer retrieval 43.0% 73.1% 0.260
Document retrieval 61.8% 82.2% 0.365
There are two possible directions for improving
our system: (1) by improving retrieval and (2) by
improving ranking. Since success@150 is 73.1%,
for 68 of the 93 questions in our set at least one
relevant answer is retrieved in the top 150. For the
other 25 questions, the reference answer was not
included in the long list of 150 results.
In the present paper we focus on improving an-
swer ranking. The results show that for 30.1% of
2We don?t need to assess the tail since we are only in-
terested in the highest-ranked relevant answer for calculating
MRR
3Note that we consider as relevant all documents in which
a relevant answer is embedded. So the relevant document with
the highest rank is either the reference document or the doc-
ument in which the relevant answer with the highest rank is
embedded.
954
the questions4, a relevant answer is retrieved but
is not placed in the top 10 by the ranking algo-
rithm. For these 28 questions in our set, re-ranking
may be an option. Since re-ranking will not im-
prove the results for the questions for which there
is no relevant answer in the top-150, the maximum
success@10 that we can achieve by re-ranking is
73.1% for answer paragraphs and 82.8% for docu-
ments.
5 Answer re-ranking
Before we can decide on our re-ranking approach,
we take a closer look at the ranking method that is
applied in the baseline system. The QAP algorithm
includes the following variables: (1) term overlap
between query and passage, (2) passage length and
(3) total corpus frequency for each term (Buttcher
et al, 2004). Let us consider three example ques-
tions from our collection to see the strengths and
weaknesses of these variables.
1. Why do people sneeze?
2. Why do women live longer than men on average?
3. Why are mountain tops cold?
In (1), the corpus frequencies of the question
terms people and sneeze ensure that the relatively
unique term sneeze is weighted heavier for ranking
than the very common noun people. This matches
the goal of the query, which is finding an explana-
tion for sneezing. However, in (2), the frequency
variables used by QAP do not reflect the impor-
tance of the terms. Thus, women, live, longer and
average are considered to be of equal importance,
while obviously the latter term is only peripheral to
the goal of the query. This cannot be derived from
its corpus frequency, but may be inferred from its
syntactic function in the question: an adverbial on
sentence level. In (3), mountain and tops are in-
terpreted as two distinct terms by the baseline sys-
tem, whereas the interpretation of mountain tops
as compound item is more appropriate.
Examples 2 and 3 above show that a question-
answer pair may contain more information than
is represented by the frequency variables imple-
mented in the QAP algorithm. Our aim is to find
out which features from a question-answer pair
constitute the information that discloses a relation
between the question and its answer. Moreover, we
aim at weighting these features in such a way that
we can optimize ranking performance.
4
73.1%? 43.0%
5.1 Features for re-ranking
As explained above, baseline ranking is based on
term overlap. The features that we propose for
re-ranking are also based on term overlap, but in-
stead of considering all question content words in-
discriminately in one overlap function, we select a
subset of question terms for each of the re-ranking
features. By defining different subsets based on
syntactic functions and categories, we can investi-
gate which syntactic features of the question, and
which parts of the answer are most important for
re-ranking.
The following subsections list the syntactic fea-
tures that we consider. Each feature consists of two
item sets: a set of question items and a set of an-
swer items. The value that is assigned to a feature
is a function of the intersection between these two
sets. For a set of question items Q and a set of
answer items A, the proportion P of their intersec-
tion is:
P =
|Q ? A|+ |A ? Q|
|Q|+ |A|
(1)
Our approach to composing the set of features is
described in subsections 5.1.1 to 5.1.4 below. We
label the features using the letter f followed by a
number so that we can back-reference to them.
5.1.1 The syntactic structure of the question
Example 2 in the previous section shows that
some syntactic functions in the question may be
more important than other functions. Since we do
not know as yet which syntactic functions are the
most important, we include both heads (f1) and
modifiers (f2) as item sets. We also include the
four main syntactic constituents for why-questions:
subject (f4), main verb (f6), nominal predicate (f8)
and direct object (f10) to be matched against the
answer terms. For these features, we add a vari-
ant where as answer items only words/phrases with
the same syntactic function are included (f5, f7, f9,
f11).
Example 3 in the previous section exemplifies
the potential relevance of noun phrases (f3).
5.1.2 The semantic structure of the question
The features f12 to f15 come from earlier data
analyses that we performed. We saw that often
there is a link between a specific part of the ques-
tion and the title of the document in which the ref-
erence answer is found. For example, the answer
to the question ?Why did B.B. King name his gui-
tar Lucille?? is in the Wikipedia article with the ti-
955
tle B.B. King. The answer document and the ques-
tion apparently share the same topic (B.B. King).
In analogy to linguistically motivated approaches
to factoid QA (Ferret et al, 2002) we introduce the
term question focus for this topic.
The focus is often the syntactic subject of the
question. From our data, we found the follow-
ing two exceptions to this general rule: (1) If the
subject is semantically poor, the question focus is
the (verbal or nominal) predicate: ?Why do peo-
ple sneeze??, and (2) in case of etymology ques-
tions (which cover about 10% of why-questions),
the focus is the subject complement of the pas-
sive sentence: ?Why are chicken wings called
Buffalo Wings?? In all other cases, the question
focus is the grammatical subject: ?Why do cats
sleep so much??
We include a feature (f13) for matching words
from the question focus to words from the docu-
ment title. We also add a feature (f12) for the re-
lation between all question words and words from
the document title, and a feature (f14) for the rela-
tion between question focus words and all answer
words.
5.1.3 Synonyms
For each of the features f1 to f15, we add an
alternative feature (f16 to f30) covering the set of
all WordNet synonyms for all items in the origi-
nal feature. Note that the original words are no
longer included for these features; we only include
the terms from their synonym sets. For synonyms,
we apply a variant of equation 1 in which |Q ? A|
is interpreted as the number of question items that
have at least one synonym in the set of answer
items and |A ? Q| as the number of answer items
that occur in at least one of the synonym sets of the
question items.
5.1.4 Cue words
Finally, we add a closed set of cue words that
often occur in answers to why-questions5 (f31).
5.2 Extracting feature values from the data
For the majority of features we need the syntactic
structure of the input question, and for some of the
features also of the answer. We experimented with
two different parsers for these tasks: a develop-
5These cue words come from earlier work that we did on
the analysis of why-answers: because, since, therefore, why,
in order to, reason, reasons, due to, cause, caused, causing,
called, named
ment version of the Pelican parser6 and the EP4IR
dependency parser (Koster, 2003).
Given a question-answer pair and the parse trees
of both question and answer, we extract values
from each parser?s output for all features in sec-
tion 5.1 by means of a Perl script.
Our script has access to the following external
components: A stop word list (see section 4.1), a
fixed set of cue words, the CELEX Lemma lexi-
con (Burnage et al, 1990), all WordNet synonym
sets, and a list of pronouns and semantically poor
nouns7.
Given one question-answer pair, the feature
extraction script performs the following actions.
Based on the question?s parse tree, it extracts the
subject, main verb, direct object (if present) and
nominal predicate (if present) from the question.
The script decides on question focus using the
rules suggested in section 5.1.2. For the answer, it
extracts the document title. From the parse trees
created for the answer paragraph, it extracts all
subjects, all verbs, all direct objects, and all nomi-
nal predicates.
For each feature, the script composes the re-
quired sets of question items and answer items. All
items are lowercased and punctuation is removed.
In multi-word items, spaces are replaced by un-
derscores before stop words are removed from the
question and the answer. Then the script calculates
the proportion of the intersection of the two sets for
each feature following equation 18.
Whether or not to lemmatize the items before
matching them is open to debate. In the litera-
ture, there is some discussion on the benefit of
lemmatization for information extraction (Bilotti
et al, 2004). Lemmatization can be problematic
in the case of proper names (which are not always
recognizable by capitalization) and noun phrases
that are fixed expressions such as sailors of old.
Noun phrases are involved not only in the NP fea-
ture (f3), but also in our features involving sub-
ject, direct object, nominal predicate and question
focus. Therefore, we decided only to lemmatize
verbs (for features f6 and f7) in the current version
of our system.
For each question-answer pair in our data set,
we extract all feature values using our script. We
6The Pelican parser is a constituency parser that is cur-
rently being developed at Nijmegen University. See also
http://lands.let.ru.nl/projects/pelican/
7These are the nouns humans and people
8A multi-word term is counted as one item
956
use three different settings for feature extraction:
(1) feature extraction from gold standard con-
stituency parse trees of the questions in accordance
with the descriptive model of the Pelican parser9;
(2) feature extraction from the constituency parse
trees of the questions generated by Pelican10; and
(3) feature extraction from automatically gener-
ated dependency parse trees from EP4IR.
Our training and testing method using the ex-
tracted feature values is explained in the next sec-
tion.
5.3 Re-ranking method
As the starting point for re-ranking we run the
baseline system on the complete set of 93 ques-
tions and retrieve 150 candidate answers per ques-
tion, ranked by the QAP algorithm. As described
in section 5.2, we use two different parsers. Of
these, Pelican has a more detailed descriptive
model and gives better accuracy (see section 6.3 on
parser evaluation) but EP4IR is at present more ro-
bust for parsing long sentences and large amounts
of text. Therefore, we parse all answers (93 times
150 paragraphs) with EP4IR only. The questions
are parsed by both Pelican and EP4IR.
As presented in section 5.1, we have 31 re-
ranking features. To these, we add the score that
was assigned by QAP, which makes 32 features
in total. We aim to weight the feature values in
such a way that their contribution to the overall
system performance is optimal. We set each fea-
ture weight as an integer between 0 and 10, which
makes the number of possible weighting configu-
rations 1132. In order to choose the optimal con-
figuration from this huge set of possible configura-
tions, we use a genetic algorithm11 (Goldberg and
Holland, 1988). The variable that we optimize dur-
ing training is MRR. We tune the feature weights
over 100 generations of 1000 individuals. For eval-
uation, we apply cross valuation on five question
9Pelican aims at producing all possible parse trees for a
given sentence. A linguist can then decide on the correct parse
tree given the context. We created the gold standard for each
question by manually selecting the correct parse tree from the
parse trees generated by the parser.
10For this setting, we run the Pelican parser with the option
of only giving one parse (the most likely according to Pelican)
per question. As opposed to the gold standard setting, we do
not perform manual selection of the correct parse.
11We chose to work with a genetic algorithm because we
are mainly interested in feature selection and ranking. We
are currently experimenting with Support Vector Machines
(SVM) to see whether the results obtained from using the ge-
netic algorithm are good enough for reliable feature selection.
folds: in five turns, we train the feature weights on
four of the five folds and evaluate them on the fifth.
We use the feature values that come from the
gold standard parse trees for training the feature
weights, because the benefit of a syntactic item
type can only be proved if the extraction of that
item from the data is correct. At the testing stage,
we re-rank the 93 questions using all three fea-
ture extraction settings: feature values extracted
from gold standard parse trees, feature values ex-
tracted with Pelican and feature values extracted
with EP4IR. We again regard the distribution of
questions over the five folds: we re-rank the ques-
tions in fold five according to the weights found by
training on folds one to four.
5.4 Results from re-ranking
Table 2 on the next page shows the results for the
three feature extraction settings.
Using the Wilcoxon Signed-Rank Test we find
that all three re-ranking conditions give signifi-
cantly better results than the baseline (Z = ?1.91,
P = 0.0281 for paired reciprocal ranks). The dif-
ferences between the three re-ranking conditions
are, however, not significant12.
5.5 Which features made the improvement?
If we plot the weights that were chosen for the fea-
tures in the five folds, we see that for some features
very different weights were chosen in the different
folds. Apparently, for these features, the weight
values do not generalize over the five folds. In or-
der to only use reliable features, we only consider
features that get similar weights over all five folds:
their weight values have a standard deviation < 2
and an average weight > 0. We find that of the
32 features, 21 are reliable according to this def-
inition. Five of these features make a substantial
contribution to the re-ranking score (table 3). Be-
hind each feature is its reference number from sec-
tion 5.1 and its average weight on a scale of 0 to
10.
Moreover, there are three other features that to a
limited extent contribute to the overall score (table
4).
Thirteen other reliable features get a weight < 1.5
assigned during training and thereby slightly con-
tribute to the re-ranking score.
12The slightly lower success and MRR scores for re-
ranking with gold standard parse trees compared to Pelican
parse trees can be explained by the absence of the gold stan-
dard for one question in our set.
957
Table 2: Re-ranking results for three different parser settings in terms of success@10, success@150 and MRR@150.
Answer/paragraph retrieval Document retrieval
Version S@10 S@150 MRR S@10 S@150 MRR
Baseline 43.0% 73.1% 0.260 61.8% 82.8% 0.365
Re-ranking w/ gold standard parse trees 54.4% 73.1% 0.370 63.1% 82.8% 0.516
Re-ranking w/ Pelican parse trees 54.8% 73.1% 0.380 64.5% 82.8% 0.518
Re-ranking w/ EP4IR parse trees 53.8% 73.1% 0.349 63.4% 82.8% 0.493
Table 3: Features that substantially contribute to the re-
ranking score, with their average weight
Question focus synonyms to doctitle (f28) 9.2
Question verb synonyms to answer verbs (f22) 9
Cue words (f31) 9
QAP 8.8
Question focus to doctitle (f13) 7.8
Table 4: Features that to a limited extent contribute to the
re-ranking score, with their average weight
Question subject to answer subjects (f5) 2.2
Question nominal predicate synonyms (f23) 1.8
Question object synonyms to answer objects (f26) 1.8
6 Discussion
Our re-ranking method scores significantly better
than the baseline, with use of a small subset of
the 32 features. It reaches a success@10 score
of 54.8% with an MRR@150 of 0.380 for answer
retrieval. This compares to the MRR of 0.328
that Higashinaka and Isozaki found for why-QA
and the MRR of 0.406 that Tiedemann reaches
for syntactically enhanced factoid-QA (see sec-
tion 2), showing that our method performs reason-
able well. However, the MRR of 0.380 also shows
that a substantial part of the problem of why-QA is
still to be solved.
6.1 Error analysis
For analysis of our results, we counted for how
many questions the ranking was improved, and for
how many the ranking deteriorated. First of all,
ranking remained equal for 35 questions (37.6%).
25 of these are the questions for which no rele-
vant answer was retrieved by the baseline system
at n = 150 (26.9% of questions). For these ques-
tions the ranking obviously remained equal (RR is
0) after re-ranking. For the other 10 questions for
which ranking did not change, RR was 1 and re-
mained 1. Apparently, re-ranking does not affect
excellent rankings.
For two third (69%) of the remaining questions,
ranking improved and for one third (31%), it dete-
riorated. There are eleven questions for which the
reference answer was ranked in the top 10 by the
baseline system but it drops out of the top 10 by
re-ranking. On the other hand, there are 22 ques-
tions for which the reference answer enters the top
10 by re-ranking the answers, leading to an overall
improvement in success@10.
If we take a look at the eleven questions for
which the reference answer drops out of the top
10 by re-ranking, we see that these are all cases
where there is no lexical overlap between the ques-
tion focus and the document title. The importance
of features 13 and 28 in the re-ranking weights
works against the reference answer for these ques-
tions. Here are three examples (question focus as
detected by the feature extraction script is under-
lined):
1. Why do neutral atoms have the same number of protons
as electrons? (answer in ?Oxidation number?)
2. Why do flies walk on food? (answer in ?Insect Habitat?)
3. Why is Wisconsin called the Badger State? (answer in
?Wisconsin?)
In example 1, the reference answer is outranked
by answer paragraphs from documents with one of
the words neutral and atoms in its title. In example
2, there is actually a semantic relation between the
question focus (flies) and the document title (in-
sect); however, this relation is not synonymy but
hyperonymy and therefore not included in our re-
ranking features. One could dispute the definition
of question focus for etymology questions (exam-
ple 3), but there are simply more cases where the
subject complement of the question leads to doc-
ument title than cases where its subject (such as
Winsconsin) does.
6.2 Feature selection analysis
We think that the outcome of the feature selection
(section 5.5) is very interesting. We are not sur-
prised that the original score assigned by QAP is
still important in the re-ranking module: the fre-
quency variables apparently do provide useful in-
formation on the relevance of a candidate answer.
We also see that the presence of cue words
(f31) gives useful information in re-ranking an-
958
swer paragraphs. In fact, incorporating the pres-
ence of cue words is a first step towards recogniz-
ing that a paragraph is potentially an answer to a
why-question. We feel that identifying a paragraph
as a potential answer is the most salient problem
of why-QA, since answers cannot be recognized
by simple semantic-syntactic units such as named
entities as is the case for factoid QA. The current
results show that surface patterns (the literal pres-
ence of items from a fixed set of cue words) are a
first step in the direction of answer selection.
More interesting than the baseline score and cue
words are the high average weights assigned to
the features f13 and f28. These two features refer
to the relation between question focus and docu-
ment title. As explained in section 5.1.2, we al-
ready had the intuition that there is some relation
between the question focus of a why-question and
the document title. The high weights that are as-
signed to the question focus features show that our
procedure for extracting question focus is reliable.
The importance of question focus for why-QA is
especially interesting because it is a question fea-
ture that is specific to why-questions and does not
similarly apply to factoids or other question types.
Moreover, the link from the question focus to the
document title shows that Wikipedia as an answer
source can provide QA systems with more infor-
mation than a collection of plain texts without doc-
ument structure does.
From the other features discussed in section 5.5,
we learn that all four main question constituents
contribute to the re-ranking score, but that syn-
onyms of the main verb make the highest contri-
bution (f22). Subject (f5), object (f26) and nomi-
nal predicate (f23) make a lower contribution. We
suspect that this may be due to our decision to only
lemmatize verbs, and not nouns (see section 5.2).
It could be that since lemmatization leads to more
matches, a feature can make a higher contribution
if its items are lemmatized.
6.3 The quality of the syntactic descriptions
We already concluded in the previous section that
our feature extraction module is very well capable
of extracting the question focus, since f13 and f28
get assigned high weights by training. However,
in the training stage, we used gold standard parse
trees. In this section we evaluate the two automatic
syntactic parsers Pelican and EP4IR, in order to
be able to come up with fruitful suggestions for
improving our system in the future.
As a measure for parser evaluation, we con-
sider constituent extraction: how well do both
parsers perform in identifying and delimiting the
four main constituents from a why-question: sub-
ject, main verb, direct object and nominal pred-
icate? As the gold standard for this experiment
we use manually verified constituents that were
extracted from the gold standard parse trees. We
adapt our feature extraction script so that it prints
each of the four constituents per question. Then we
calculate the recall score for each parser for each
constituent type.
Recall is the number of correctly identified con-
stituents of a specific type divided by the total
number of constituents of this type in the goldstan-
dard parse tree. This total number is not exactly 93
for all constituent types: only 34 questions have a
direct object in their main clause and 31 questions
have a nominal predicate. The results of this exer-
cise are in Table 5.
Table 5: Recall for constituent extraction (in %)
subjs verbs objs preds all
Pelican 79.6 94.6 64.7 71.0 82.1
EP4IR 63.4 64.5 44.1 48.4 59.4
We find that over all constituent types, Peli-
can reaches significantly better recall scores than
EP4IR (Z = 5.57; P < 0.0001 using the
Wilcoxon Signed-Rank Test).
Although Pelican gives much better results on
constituent extraction than EP4IR, the results on
the re-ranking task do not differ significantly. The
most plausible explanation for this is that the high
accuracy of the Pelican parses is undone by the
poor syntactic analysis on the answer side, which
is in all settings performed by EP4IR.
7 Future directions
In section 4.3, we mentioned two directions for im-
proving our pipeline system: improving retrieval
and improving ranking. Recently we have been
working on optimizing the retrieval module of our
pipeline system by investigating the influence of
different retrieval modules and passage segmenta-
tion strategies on the retrieval performance. This
work has resulted in a better passage retrieval mod-
ule in terms of success@150. Details on these ex-
periments are in (Khalid and Verberne, 2008).
Moreover, we have been collecting a larger data
collection in order to do make feature selection for
959
our re-ranking experiments more reliable and less
depending on specific cases in our dataset. This
work has resulted in a total set of 188 why-question
answer pairs. We are currently using this data
collection for further research into improving our
pipeline system.
In the near future, we aim to investigate what
type of information is needed for further improv-
ing our system for why-QA. With the addition of
syntactic information our system reaches an MRR
score of 0.380. This compares to the MRR scores
reached by other syntactically enhanced QA sys-
tems (see section 2). However, an MRR of 0.380
also shows that a substantial part of the problem
of why-QA is still to be solved. We are currently
investigating what type information is needed for
further system improvement.
Finally, we also plan experiments with a number
of dependency parsers to be used instead of EP4IR
for the syntactic analysis of the answer para-
graphs. Current experiments with Charniak (Char-
niak, 2000) show better constituent extraction than
with EP4IR. It is still to be seen whether this also
influences the overall performance of our system.
8 Conclusion
We added a re-ranking step to an existing para-
graph retrieval method for why-QA. For re-
ranking, we took the score assigned to a question
answer pair by the ranking algorithm QAP in the
baseline system, and weighted it with a number of
syntactic features. We experimented with 31 fea-
tures and trained the feature weights on a set of 93
why-questions with 150 answers provided by the
baseline system for each question. Feature values
for training the weights for the 31 features were
extracted from gold standard parse trees for each
question answer pair.
We evaluated the feature weights on automat-
ically parsed questions and answers, in five folds.
We found a significant improvement over the base-
line for both success@10 and MRR@150. The
most important features were the baseline score,
the presence of cue words, the question?s main
verb, and the relation between question focus and
document title.
We think that, although syntactic information
gives a significant improvement over baseline pas-
sage ranking, more improvement is still to be
gained from other types of information. Investi-
gating the type of information needed is part of our
future directions.
References
Bilotti, M.W., B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or mor-
phological query expansion. Proc. IR4QA at SIGIR
2004.
Burnage, G., R.H. Baayen, R. Piepenbrock, and H. van
Rijn. 1990. CELEX: A Guide for Users.
Buttcher, S., C.L.A. Clarke, and G.V. Cormack. 2004.
Domain-Specific Synonym Expansion and Valida-
tion for Biomedical Information Retrieval.
Buttcher, S. 2007. The Wumpus Search Engine.
http://www.wumpus-search.org/.
Charniak, E. 2000. A maximum-entropy-inspired
parser. ACM International Conference Proceeding
Series, 4:132?139.
Denoyer, L. and P. Gallinari. 2006. The Wikipedia
XML corpus. ACM SIGIR Forum, 40(1):64?69.
Ferret, O., B. Grau, M. Hurault-Plantet, G. Illouz,
L. Monceaux, I. Robba, and A. Vilnat. 2002. Find-
ing an answer based on the recognition of the ques-
tion focus. Proc. of TREC 2001, pages 500?250.
Goldberg, D.E. and J.H. Holland. 1988. Genetic Algo-
rithms and Machine Learning. Machine Learning,
3(2):95?99.
Higashinaka, R. and H. Isozaki. 2008. Corpus-based
Question Answering for why-Questions. In Proc. of
IJCNLP, vol.1, pages 418?425.
Hovy, E.H., U. Hermjakob, and D. Ravichandran.
2002. A Question/Answer Typology with Surface
Text Patterns. In Proc. of HLT 2002.
Khalid, M. and S. Verberne. 2008. Passage Retrieval
for Question Answering using Sliding Windows. In
Proc. of IR4QA at COLING 2008.
Koster, CHA. 2003. Head-modifier frames for every-
one. Proc. of SIGIR 2003, page 466.
Quarteroni, S., A. Moschitti, S. Manandhar, and
R. Basili. 2007. Advanced Structural Represen-
tations for Question Classification and Answer Re-
ranking. In Proc. of ECIR 2007, volume 4425, pages
234?245.
Tiedemann, J. 2005. Improving passage retrieval in
question answering using NLP. In Proc. of EPIA
2005.
Verberne, S., L. Boves, N. Oostdijk, and P.A. Coppen.
2008. Evaluating paragraph retrieval for why-QA.
In Proc. of ECIR 2008.
960
Developing an approach for why-question answering
Suzan Verberne
Dept. of Language and Speech
Radboud University Nijmegen
s.verberne@let.ru.nl
Abstract
In the current project, we aim at
developing an approach for automatically
answering why-questions. We created a
data collection for research, development
and evaluation of a method for
automatically answering why-questions
(why-QA) The resulting collection
comprises 395 why-questions. For each
question, the source document and one or
two user-formulated answers are
available in the data set. The resulting
data set is of importance for our research
and it will contribute to and stimulate
other research in the field of why-QA.
We developed a question analysis
method for why-questions, based on
syntactic categorization and answer type
determination. The quality of the output
of this module is promising for future
development of our method for why-QA.
1 Introduction
Until now, research in the field of automatic
question answering (QA) has focused on factoid
(closed-class) questions like who, what, where
and when questions. Results reported for the QA
track of the Text Retrieval Conference (TREC)
show that these types of wh-questions can be
handled rather successfully (Voorhees 2003).
In the current project, we aim at developing an
approach for automatically answering why-
questions. So far, why-questions have largely
been ignored by researchers in the QA field. One
reason for this is that the frequency of why-
questions in a QA context is lower than that of
other questions like who- and what-questions
(Hovy et al, 2002a). However, although why-
questions are less frequent than some types of
factoids (who, what and where), their frequency
is not negligible: in a QA context, they comprise
about 5 percent of all wh-questions (Hovy, 2001;
Jijkoun, 2005) and they do have relevance in QA
applications (Maybury, 2002). A second reason
for ignoring why-questions until now, is that it
has been suggested that the techniques that have
proven to be successful in QA for closed-class
questions are not suitable for questions that
expect a procedural answer rather than a noun
phrase (Kupiec, 1999). The current paper aims to
find out whether the suggestion is true that
factoid-QA techniques are not suitable for why-
QA. We want to investigate whether principled
syntactic parsing can make QA for why-
questions feasible.
In the present paper, we report on the work that
has been carried out until now. More
specifically, sections 2 and 3 describe the
approach taken to data collection and question
analysis and the results that were obtained. Then,
in section 4, we discuss the plans and goals for
the work that will be carried out in the remainder
of the project.
2 Data for why-QA
In research in the field of QA, data sources of
questions and answers play an important role.
Appropriate data collections are necessary for the
development and evaluation of QA systems
(Voorhees, 2000). While in the context of the
QA track of TREC data collections in support of
factoid questions have been created, so far, no
resources have been created for why-QA. For the
purpose of the present research therefore, we
have developed a data collection comprising a
set of questions and corresponding answers. In
doing so, we have extended the time tested
procedures previously developed in the TREC
context.
In this section, we describe the requirements
that a data set must meet to be appropriate for
development and we discuss a number of
existing sources of why-questions. Then we
describe the method employed for data collection
39
and the main characteristics of the resulting data
set.
The first requirement for an appropriate data set
concerns the nature of the questions. In the
context of the current research, a why-question is
defined as an interrogative sentence in which the
interrogative adverb why (or one of its
synonyms) occurs in (near) initial position. We
consider the subset of why-questions that could
be posed in a QA context and for which the
answer is known to be present in the related
document set. This means that the data set should
only comprise why-questions for which the
answer can be found in a fixed collection of
documents. Secondly, the data set should not
only contain questions, but also the
corresponding answers and source documents.
The answer to a why-question is a clause or
sentence (or a small number of coherent
sentences) that answers the question without
giving supplementary context. The answer is not
literally present in the source document, but can
be deduced from it. For example, a possible
answer to the question Why are 4300 additional
teachers required?, based on the source snippet
The school population is due to rise by 74,000,
which would require recruitment of an additional
4,300 teachers, is Because the school population
is due to rise by a further 74,000.
Finally, the size of the data set should be large
enough to cover all relevant variation that occur
in why-questions in a QA context.
There are a number of existing sources of
why-questions that we may consider for use in
our research. However, for various reasons, none
of these appear suitable.
Why-questions from corpora like the British
National Corpus (BNC, 2002), in which
questions typically occur in spoken dialogues,
are not suitable because the answers are not
structurally available with the questions, or they
are not extractable from a document that has
been linked to the question. The same holds for
the data collected for the Webclopedia project
(Hovy et al, 2002a), in which neither the
answers nor the source documents were
included. One could also consider questions and
answers from frequently asked questions (FAQ)
pages, like the large data set collected by
Valentin Jijkoun (Jijkoun, 2005). However, in
FAQ lists, there is no clear distinction between
the answer itself (a clause that answers the
question) and the source document that contains
the answer.
The questions in the test collections from the
TREC-QA track do contain links to the possible
answers and the corresponding source
documents. However, these collections contain
too few why-questions to qualify as a data set
that is appropriate for developing why-QA.
Given the lack of available data that match our
requirements, a new data set for QA research
into why-questions had to be compiled. In order
to meet the given requirements, it would be best
to collect questions posed in an operational QA
environment, like the compilers of the TREC-
QA test collections did: they extracted factoid
and definition questions from search logs
donated by Microsoft and AOL (TREC, 2003).
Since we do not have access to comparable
sources, it was decided to revert to the procedure
used in earlier TRECs, and imitate a QA
environment in an elicitation experiment. We
extended the conventional procedure by
collecting user-formulated answers in order to
investigate the range of possible answers to each
question. We also added paraphrases of collected
questions in order to extend the syntactic and
lexical variation in the data collection.
In the elicitation experiment, ten native
speakers of English were asked to read five texts
from Reuters? Textline Global News (1989) and
five texts from The Guardian on CD-ROM
(1992). The texts were around 500 words each.
The experiment was conducted over the Internet,
using a web form and some CGI scripts. In order
to have good control over the experiment, we
registered all participants and gave them a code
for logging in on the web site. Every time a
participant logged in, the first upcoming text that
he or she did not yet finish was presented. The
participant was asked to formulate one to six
why-questions for this text, and to formulate an
answer to each of these questions. The
participants were explicitly told that it was
essential that the answers to their questions could
be found in the text. After submitting the form,
the participant was presented the questions posed
by one of the other participants and he or she was
asked to formulate an answer to these questions
too. The collected data was saved in text format,
grouped per participant and per source
document, so that the source information is
available for each question. The answers have
been linked to the questions.
In this experiment, 395 questions and 769
corresponding answers were collected. The
number of answers would have been twice the
40
number of questions if all participants would
have been able to answer all questions that were
posed by another participant. However, for 21
questions (5.3%), the second participant was not
able to answer the first participant?s question.
Note that not every question in the elicitation
data set has a unique topic1: on average, 38
questions were formulated per text, covering
around twenty topics per text.
The collected questions have been formulated
by people who had constant access to the source
text. As a result of that, the chosen formulations
often resemble the original text, both in the use
of vocabulary and sentence structure. In order to
expand the dataset, a second elicitation
experiment was set up, in which five participants
from the first experiment were asked to
paraphrase some of the original why-questions.
The 166 unique questions were randomly
selected from the original data set. The
participants formulated 211 paraphrases in total
for these questions. This means that some
questions have more than one paraphrase. The
paraphrases were saved in a text file that includes
the corresponding original questions and the
corresponding source documents.
We studied the types of variation that occur
among questions covering the same topic. First,
we collected the types of variation that occur in
the original data set and then we compared these
to the variation types that occur in the set of
paraphrases.
In the original data set, the following types of
variation occur between different questions on
the same topic:
Lexical variation, e.g.
for the second year running vs.
again;
Verb tense variation, e.g.
have risen vs. have been rising;
Optional constituents variation, e.g.
class sizes vs. class sizes in
England and Wales;
Sentence structure variation, e.g.
would require recruitment vs.
need to be recruited
In the set of paraphrases, the same types of
variation occur, but as expected the differences
between the paraphrases and the source
1 The topic of a why-question is the proposition that is
questioned. A why-question has the form ?WHY P?, in
which P is the topic.
sentences are slightly bigger than the differences
between the original questions and the source
sentences. We measured the lexical overlap
between the questions and the source texts as the
number of content words that are in both the
question and the source text. The average relative
lexical overlap (the number of overlapping words
divided by the total number of words in the
question) between original questions and source
text is 0.35; the average relative lexical overlap
between paraphrases and source text is 0.31.
The size of the resulting collection (395 original
questions, 769 answers, and 211 paraphrases of
questions) is large enough to initiate serious
research into the development of why-QA.
Our collection meets the requirements that
were formulated with regard to the nature of the
questions and the presence of the answers and
source documents for every question.
3 Question analysis for why-QA
The goal of question analysis is to create a
representation of the user?s information need.
The result of question analysis is a query that
contains all information about the answer that
can be extracted from the question. So far, no
question analysis procedures have been created
for why-QA specifically. Therefore, we have
developed an approach for proper analysis of
why-questions. Our approach is based on existing
methods of analysis of factoid questions. This
will allow us to verify whether methods used in
handling factoid questions are suitable for use
with procedural questions. In this section, we
describe the components of successful methods
for the analysis of factoid questions. Then we
present the method that we used for the analysis
of why-questions and indicate the quality of our
method.
The first (and most simple) component in current
methods for question analysis is keyword
extraction. Lexical items in the question give
information on the topic of the user?s
information need. In keyword selection, several
different approaches may be followed. Moldovan
et al (2000), for instance, select as keywords all
named entities that were recognized as proper
nouns. In almost all approaches to keyword
extraction, syntax plays a role. Shallow parsing
is used for extracting noun phrases, which are
considered to be relevant key phrases in the
retrieval step. Based on the query?s keywords,
41
one or more documents or paragraphs can be
retrieved that may possibly contain the answer.
A second, very important, component in
question analysis is determination of the
question?s semantic answer type. The answer
type of a question defines the type of answer that
the system should look for. Often-cited work on
question analysis has been done by Moldovan et
al. (1999, 2000), Hovy et al (2001), and Ferret et
al. (2002). They all describe question analysis
methods that classify questions with respect to
their answer type. In their systems for factoid-
QA, the answer type is generally deduced
directly from the question word (who, when,
where, etc.): who leads to the answer type
person; where leads to the answer type place,
etc. This information helps the system in the
search for candidate answers to the question.
Hovy et al find that, of the question analysis
components used by their system, the
determination of the semantic answer type makes
by far the largest contribution to the performance
of the entire QA system.
For determining the answer type, syntactic
analysis may play a role. When implementing a
syntactic analysis module in a working QA
system, the analysis has to be performed fully
automatically. This may lead to concessions with
regard to either the degree of detail or the quality
of the analysis. Ferret et al implement a
syntactic analysis component based on shallow
parsing. Their syntactic analysis module yields a
syntactic category for each input question. In
their system, a syntactic category is a specific
syntactic pattern, such as ?WhatDoNP? (e.g.
What does a defibrillator do?) or
?WhenBePNborn? (e.g. When was Rosa Park
born?). They define 80 syntactic categories like
these. Each input question is parsed by a shallow
parser and hand-written rules are applied for
determining the syntactic category. Ferret et al
find that the syntactic pattern helps in
determining the semantic answer type (e.g.
company, person, date). They unfortunately do
not describe how they created the mapping
between syntactic categories and answer types.
As explained above, determination of the
semantic answer type is the most important task
of existing question analysis methods. Therefore,
the goal of our question analysis method is to
predict the answer type of why-questions.
In the work of Moldovan et al (2000), all
why-questions share the single answer type
reason. However, we believe that it is necessary
to split this answer type into sub-types, because a
more specific answer type helps the system
select potential answer sentences or paragraphs.
The idea behind this is that every sub-type has its
own lexical and syntactic cues in a source text.
Based on the classification of adverbial
clauses by Quirk (1985:15.45), we distinguish
the following sub-types of reason: cause,
motivation, circumstance (which combines
reason with conditionality), and purpose.
Below, an example of each of these answer
types is given.
Cause:
The flowers got dry because it
hadn?t rained in a month.
Motivation:
I water the flowers because I
don?t like to see them dry.
Circumstance:
Seeing that it is only three,
we should be able to finish
this today.
Purpose:
People have eyebrows to prevent
sweat running into their eyes.
The why-questions that correspond to the reason
clauses above are respectively Why did the
flowers get dry?, Why do you water the flowers?,
Why should we be able to finish this today?, and
Why do people have eyebrows?. It is not always
possible to assign one of the four answer sub-
types to a why-question. We will come back to
this later.
Often, the question gives information on the
expected answer type. For example, compare the
two questions below:
Why did McDonald's write Mr.
Bocuse a letter?
Why have class sizes risen?
Someone asking the former question expects as
an answer McDonald?s motivation for writing a
letter, whereas someone asking the latter
question expects the cause for rising class sizes
as answer.
The corresponding answer paragraphs do
indeed contain the equivalent answer sub-types:
McDonald's has acknowledged
that a serious mistake was
made. "We have written to
apologise and we hope to reach
42
a settlement with Mr. Bocuse
this week," said Marie-Pierre
Lahaye, a spokeswoman for
McDonald's France, which
operates 193 restaurants.
Class sizes in schools in
England and Wales have risen
for the second year running,
according to figures released
today by the Council of Local
Education Authorities. The
figures indicate that although
the number of pupils in schools
has risen in the last year by
more than 46,000, the number of
teachers fell by 3,600.
We aim at creating a question analysis module
that is able to predict the expected answer type of
an input question. In the analysis of factoid
questions, the question word often gives the
needed information on the expected answer type.
In case of why, the question word does not give
information on the answer type since all why-
questions have why as question word. This
means that other information from the question is
needed for determining the answer sub-type.
We decided to use Ferret?s approach, in which
syntactic categorization helps in determining the
expected answer type. In our question analysis
module, the TOSCA (TOols for Syntactic
Corpus Analysis) system (Oostdijk, 1996) is
explored for syntactic analysis. TOSCA?s
syntactic parser takes a sequence of
unambiguously tagged words and assigns
function and category information to all
constituents in the sentence. The parser yields
one or more possible output trees for (almost) all
input sentences. For the purpose of evaluating
the maximum contribution to a classification
method that can be obtained from a principled
syntactic analysis, the most plausible parse tree
from the parser?s output is selected manually.
For the next step of question analysis, we
created a set of hand-written rules, which are
applied to the parse tree in order to choose the
question?s syntactic category. We defined six
syntactic categories for this purpose:
Action questions, e.g.
Why did McDonald's write Mr.
Bocuse a letter?
Process questions, e.g.
Why has Dixville grown famous
since 1964?
Intensive complementation questions, e.g.
Why is Microsoft Windows a
success?
Monotransitive have questions, e.g.
Why did compilers of the OED
have an easier time?
Existential there questions, e.g.
Why is there a debate about
class sizes?
Declarative layer questions, e.g.
Why does McDonald's spokeswoman
think the mistake was made?
The choice for these categories is based the
information that is available from the parser, and
the information that is needed for determining
the answer type.
For some categories, the question analysis
module only needs fairly simple cues for
choosing a category. For example, a main verb
with the feature intens leads to the category
?intensive complementation question? and the
presence of the word there with the syntactic
category EXT leads to the category ?existential
there question?. For deciding on declarative layer
questions, action questions and process
questions, complementary lexical-semantic
information is needed. In order to decide whether
the question contains a declarative layer, the
module checks whether the main verb is in a list
that corresponds to the union of the verb classes
say and declare from Verbnet (Kipper et al,
2000), and whether it has a clausal object. The
distinction between action and process questions
is made by looking up the main verb in a list of
process verbs. This list contains the 529 verbs
from the causative/inchoative alternation class
(verbs like melt and grow) from the Levin verb
index (Levin, 1993); in an intransitive context,
these verbs are process verbs.
We have not yet developed an approach for
passive questions.
Based on the syntactic category, the question
analysis module tries to determine the answer
type. Some of the syntactic categories lead
directly to an answer type. All process questions
with non-agentive subjects get the expected
answer type cause. All action questions with
agentive subjects get the answer type motivation.
We extracted information on agentive and non-
agentive nouns from WordNet: all nouns that are
in the lexicographer file noun.person were
selected as agentive.
Other syntactic categories need further analysis.
Questions with a declarative layer, for example,
43
are ambiguous. The question Why did they say
that migration occurs? can be interpreted in two
ways: Why did they say it? or Why does
migration occur?. Before deciding on the answer
type, our question analysis module tries to find
out which of these two questions is supposed to
be answered. In other words: the module decides
which of the clauses has the question focus. This
decision is made on the basis of the semantics of
the declarative verb. If the declarative is a factive
verb ? a verb that presupposes the truth of its
complements ? like know, the module decides
that the main clause has the focus. The question
consequently gets the answer type motivation. In
case of a non-factive verb like think, the focus is
expected to be on the subordinate clause. In
order to predict the answer type of the question,
the subordinate clause is then treated the same
way as the complete question was. For example,
consider the question Why do the school councils
believe that class sizes will grow even more?.
Since the declarative (believe) is non-factive, the
question analysis module determines the answer
type for the subordinate clause (class sizes will
grow even more), which is cause, and assigns it
to the question as a whole.
Special attention is also paid to questions with a
modal auxiliary. Modal auxiliaries like can and
should, have an influence on the answer type.
For example, consider the questions below, in
which the only difference is the presence or
absence of the modal auxiliary can:
Why did McDonalds not use
actors to portray chefs in
amusing situations?
Why can McDonalds not use
actors to portray chefs in
amusing situations?
The former question expects a motivation as
answer, whereas the latter question expects a
cause. We implemented this difference in our
question analysis module: CAN (can, could) and
HAVE TO (have to, has to, had to) lead to the
answer type cause. Furthermore, the modal
auxiliary SHALL (shall, should) changes the
expected answer type to motivation.
When choosing an answer type, our question
analysis module follows a conservative policy: in
case of doubt, no answer type is assigned.
We did not yet perform a complete evaluation of
our question analysis module. For proper
evaluation of the module, we need a reference set
of questions and answers that is different from
the data set that we collected for development of
our system. Moreover, for evaluating the
relevance of our question analysis module for
answer retrieval, further development of our
approach is needed.
However, to have a general idea of the
performance of our method for answer type
determination, we compared the output of the
module to manual classifications. We performed
these reference classifications ourselves.
First, we manually classified 130 why-
questions from our development set with respect
to their syntactic category. Evaluation of the
syntactic categorization is straightforward: 95
percent of why-questions got assigned the correct
syntactic category using ?perfect? parse trees.
The erroneous classifications were due to
differences in the definitions of the specific verb
types. For example, argue is not in the list of
declarative verbs, as a result of which a question
with argue as main verb is classified as action
question instead of declarative layer question.
Also, die and cause are not in the list of process
verbs, so questions with either of these verbs as
main verb are labeled as action questions instead
of process questions.
Secondly, we performed a manual classification
into the four answer sub-types (cause,
motivation, circumstance and purpose). For this
classification, we used the same set of 130
questions as we did for the syntactic
categorization, combined with the corresponding
answers. Again, we performed this classification
ourselves.
During the manual classification, we assigned
the answer type cause to 23.3 percent of the
questions and motivation to 40.3 percent. We
were not able to assign an answer sub-type to the
remaining pairs (36.4 percent). These questions
are in the broader class reason and not in one of
the specific sub-classes None of the question-
answer pairs was classified as circumstance or
purpose. Descriptions of purpose are very rare in
news texts because of their generic character
(e.g. People have eyebrows to prevent sweat
running into their eyes). The answer type
circumstance, defined by Quirk (cf. section
15.45) as a combination of reason with
conditionality, is also rare as well as difficult to
recognize.
For evaluation of the question analysis
module, we mainly considered the questions that
44
did get assigned a sub-type (motivation or cause)
in the manual classification. Our question
analysis module succeeded in assigning the
correct answer sub-type to 62.2 percent of these
questions, the wrong sub-type to 2.4 percent, and
no sub-type to the other 35.4 percent. The set of
questions that did not get a sub-type from our
question analysis module can be divided in four
groups:
(a) Action questions for which the subject was
incorrectly not marked as agentive (mostly
because it was an agentive organization like
McDonald?s, or a proper noun that was not in
WordNet?s list of nouns denoting persons, like
Henk Draijen);
(b) questions with an action verb as main verb
but a non-agentive subject (e.g. Why will
restrictions on abortion damage women's
health?);
(c) passive questions, for which we have not
yet developed an approach (e.g. Why was the
Supreme Court reopened?);
(d) Monotransitive have questions. This
category contains too few questions to formulate
a general rule.
Group (a), which is by far the largest of these
four (covering half of the questions without sub-
type), can be reduced by expanding the list of
agentive nouns, especially with names of
organizations. For groups (c) and (d), general
rules may possibly be created in a later stage.
With this knowledge, we are confident that we
can reduce the number of questions without sub-
type in the output of our question analysis
module.
These first results predict that it is possible to
reach a relatively high precision in answer type
determination. (Only 2 percent of questions got
assigned a wrong sub-type.) A high precision
makes the question analysis output useful and
reliable in the next steps of the question
answering process. On the other hand, it seems
difficult to get a high recall. In this test, only
62.2 percent of the questions that were assigned
an answer type in the reference set, was assigned
an answer type by the system ? this is 39.6
percent of the total.
4 Conclusions and further research
We created a data collection for research into
why-questions and for development of a method
for why-QA. The collection comprises a
sufficient amount of why-questions. For each
question, the source document and one or two
user-formulated answers are available in the data
set. The resulting data set is of importance for
our research as well as other research in the field
of why-QA.
We developed a question analysis method for
why-questions, based on syntactic categorization
and answer type determination. In-depth
evaluation of this module will be performed in a
later stage, when the other parts of our QA
approach have been developed, and a test set has
been collected. We believe that the first test
results, which show a high precision and low
recall, are promising for future development of
our method for why-QA.
We think that, just as for factoid-QA, answer
type determination can play an important role in
question analysis for why-questions. Therefore,
Kupiec? suggestion that conventional question
analysis techniques are not suitable for why-QA
can be made more precise by saying that these
methods may be useful for a (potentially small)
subset of why-questions. The issue of recall, both
for human and machine processing, needs further
analysis.
In the near future, our work will focus on
development of the next part of our approach for
why-QA.
Until now we have focused on the first of four
sub-tasks in QA, viz. (1) question analysis (2)
retrieval of candidate paragraphs; (3) paragraph
analysis and selection; and (4) answer
generation. Of the remaining three sub-tasks, we
will focus on paragraph analysis (3). In order to
clarify the relevance of the paragraph analysis
step, let us briefly discuss the QA-processes that
follows question analysis.
The retrieval module, which comes directly
after the question analysis module, uses the
output of the question analysis module for
finding candidate answer paragraphs (or
documents). Paragraph retrieval can be
straightforward: in existing approaches for
factoid-QA, candidate paragraphs are selected
based on keyword matching only. For the current
research, we do not aim at creating our own
paragraph selection technique.
More interesting than paragraph retrieval is
the next step of QA: paragraph analysis. The
paragraph analysis module tries to determine
whether the candidate paragraphs contain
potential answers. In case of who-questions,
noun phrases denoting persons are potential
45
answers; in case of why-questions, reasons are
potential answers. In the paragraph analysis
stage, our answer sub-types come into play. The
question analysis module determines the answer
type for the input question, which is motivation,
cause, purpose, or circumstance. The paragraph
analysis module uses this information for
searching candidate answers in a paragraph. As
has been said before, the procedure for assigning
the correct sub-type needs further investigation
in order to increase the coverage and the
contribution that answer sub-type classification
can make to the performance of why-question
answering.
Once the system has extracted potential
answers from one or more paragraphs with the
same topic as the question, the eventual answer
has to be delimited and reformulated if
necessary.
References
British National Corpus, 2002. The BNC Sampler.
Oxford University Computing Services.
Fellbaum, C. (Ed.), 1998. WordNet: An Electronic
Lexical Database. Cambridge, Mass.: MIT Press.
Ferret O., Grau B., Hurault-Plantet M., Illouz G.,
Monceaux L., Robba I., and Vilnat A., 2002.
Finding An Answer Based on the Recognition of
the Question Focus. In Proceedings of The Tenth
Text REtrieval Conference (TREC 2001).
Gaithersburg, Maryland: NIST Special Publication
SP 500-250.
Hovy, E.H., Gerber, L., Hermjakob, U., Lin, C-J, and
Ravichandran, D., 2001. Toward Semantics-Based
Answer Pinpointing. In Proceedings of the DARPA
Human Language Technology Conference (HLT).
San Diego, CA
Hovy, E.H., Hermjakob, U., and Ravichandran, D.,
2002a. A Question/Answer Typology with Surface
Text Patterns. In Proceedings of the Human
Language Technology conference (HLT). San
Diego, CA.
Jijkoun, V. and De Rijke, M., 2005. Retrieving
Answers from Frequently Asked Questions Pages
on the Web. In: Proceedings CIKM-2005, to
appear.
Kipper, K., Trang Dang, H., and Palmer, M., 2000.
Class-Based Construction of a Verb Lexicon.
AAAI-2000 Seventeenth National Conference on
Artificial Intelligence, Austin, TX.
Kupiec, J.M., 1999. MURAX: Finding and
Organizing Answers from Text Search. In
Strzalkowski, T. (ed.) Natural Language
Information Retrieval. 311-332. Dordrecht,
Netherlands: Kluwer Academic.
Levin, B., 1993. English Verb Classes and
Alternations - A Preliminary Investigation. The
University of Chicago Press.
Litkowski, K. C., 1998. Analysis of Subordinating
Conjunctions, CL Research Technical Report 98-
01 (Draft). CL Research, Gaithersburg, MD.
Maybury , M., 2003. Toward a Question Answering
Roadmap. In New Directions in Question
Answering 2003: 8-11
Moldovan, D., S., Harabagiu, M., Pas?a, R.,
Mihalcea, R., G?rju, R., Goodrum, R., and Rus, V.
1999. Lasso: A Tool for Surfing the Answer Net.
175-184. In E. Voorhees and D. Harman (Eds.),
NIST Special Publication 500-246. The Eight Text
REtrieval Conference. Dept. of Commerce, NIST.
Moldovan, D., S., Harabagiu, M., Pas?a, R.,
Mihalcea, R., G?rju, R., Goodrum, R., and Rus, V.
2000. The Structure and Performance of an Open
Domain Question Answering System. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics (ACL-
2000): 563-570.
Oostdijk, N., 1996. Using the TOSCA analysis system
to analyse a software manual corpus. In: R.
Sutcliffe, H. Koch and A. McElligott (eds.),
Industrial Parsing of Software Manuals.
Amsterdam: Rodopi. 179-206.
Quirk, R., Greenbaum, S., Leech, G., and Svartvik, J.,
1985. A comprehensive grammar of the English
language. London: Longman.
Text Retrieval Conference (TREC) QA track, 2003.
http://trec.nist.gov/data/qamain.html
Voorhees, E. & Tice, D., 2000. Building a Question
Answering Test Collection. In Proceedings of
SIGIR-2000: 200-207
Voorhees, E., 2003. Overview of the TREC 2003
Question Answering Track. In Overview of TREC
2003: 1-13
46
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 561?569,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
The effect of domain and text type on text prediction quality
Suzan Verberne, Antal van den Bosch, Helmer Strik, Lou Boves
Centre for Language Studies
Radboud University Nijmegen
s.verberne@let.ru.nl
Abstract
Text prediction is the task of suggesting
text while the user is typing. Its main aim
is to reduce the number of keystrokes that
are needed to type a text. In this paper, we
address the influence of text type and do-
main differences on text prediction quality.
By training and testing our text predic-
tion algorithm on four different text types
(Wikipedia, Twitter, transcriptions of con-
versational speech and FAQ) with equal
corpus sizes, we found that there is a clear
effect of text type on text prediction qual-
ity: training and testing on the same text
type gave percentages of saved keystrokes
between 27 and 34%; training on a differ-
ent text type caused the scores to drop to
percentages between 16 and 28%.
In our case study, we compared a num-
ber of training corpora for a specific data
set for which training data is sparse: ques-
tions about neurological issues. We found
that both text type and topic domain play
a role in text prediction quality. The
best performing training corpus was a set
of medical pages from Wikipedia. The
second-best result was obtained by leave-
one-out experiments on the test questions,
even though this training corpus was much
smaller (2,672 words) than the other cor-
pora (1.5 Million words).
1 Introduction
Text prediction is the task of suggesting text while
the user is typing. Its main aim is to reduce the
number of keystrokes that are needed to type a
text, thereby saving time. Text prediction algo-
rithms have been implemented for mobile devices,
office software (Open Office Writer), search en-
gines (Google query completion), and in special-
needs software for writers who have difficulties
typing (Garay-Vitoria and Abascal, 2006). In most
applications, the scope of the prediction is the
completion of the current word; hence the often-
used term ?word completion?.
The most basic method for word completion is
checking after each typed character whether the
prefix typed since the last whitespace is unique
according to a lexicon. If it is, the algorithm sug-
gests to complete the prefix with the lexicon en-
try. The algorithm may also suggest to complete a
prefix even before the word?s uniqueness point is
reached, using statistical information on the pre-
vious context. Moreover, it has been shown that
significantly better prediction results can be ob-
tained if not only the prefix of the current word
is included as previous context, but also previ-
ous words (Fazly and Hirst, 2003) or characters
(Van den Bosch and Bogers, 2008).
In the current paper, we follow up on this work
by addressing the influence of text type and do-
main differences on text prediction quality. Brief
messages on mobile devices (such as text mes-
sages, Twitter and Facebook updates) are of a dif-
ferent style and lexicon than documents typed in
office software (Westman and Freund, 2010). In
addition, the topic domain of the text also influ-
ences its content. These differences may cause an
algorithm trained on one text type or domain to
perform poorly on another.
The questions that we aim to answer in this pa-
per are (1) ?What is the effect of text type dif-
ferences on the quality of a text prediction algo-
rithm?? and (2) ?What is the best choice of train-
ing data if domain- and text type-specific data is
sparse??. To answer these questions, we perform
three experiments:
1. A series of within-text type experiments on
four different types of Dutch text: Wikipedia
articles, Twitter data, transcriptions of con-
561
versational speech and web pages of Fre-
quently Asked Questions (FAQ).
2. A series of across-text type experiments in
which we train and test on different text
types;
3. A case study using texts from a specific do-
main and text type: questions about neuro-
logical issues. Training data for this combi-
nation of language (Dutch), text type (FAQ)
and domain (medical/neurological) is sparse.
Therefore, we search for the type of training
data that gives the best prediction results for
this corpus. We compare the following train-
ing corpora:
? The corpora that we compared in the
text type experiments: Wikipedia, Twit-
ter, Speech and FAQ, 1.5 Million words
per corpus.
? A 1.5 Million words training corpus that
is of the same domain as the target data:
medical pages from Wikipedia;
? The 359 questions from the neuro-QA
data themselves, evaluated in a leave-
one-out setting (359 times training on
358 questions and evaluating on the re-
maining questions).
The prospective application of the third series
of experiments is the development of a text predic-
tion algorithm in an online care platform: an on-
line community for patients seeking information
about their illness. In this specific case the target
group is patients with language disabilities due to
neurological disorders.
The remainder of this paper is organized as fol-
lows: In Section 2 we give a brief overview of text
prediction methods discussed in the literature. In
Section 3 we present our approach to text predic-
tion. Sections 4 and 5 describe the experiments
that we carried out and the results we obtained.
We phrase our conclusions in Section 6.
2 Text prediction methods
Text prediction methods have been developed for
several different purposes. The older algorithms
were built as communicative devices for people
with disabilities, such as motor and speech impair-
ments. More recently, text prediction is developed
for writing with reduced keyboards, specifically
for writing (composing messages) on mobile de-
vices (Garay-Vitoria and Abascal, 2006).
All modern methods share the general idea that
previous context (which we will call the ?buffer?)
can be used to predict the next block of charac-
ters (the ?predictive unit?). If the user gets correct
suggestions for continuation of the text then the
number of keystrokes needed to type the text is
reduced. The unit to be predicted by a text pre-
diction algorithm can be anything ranging from a
single character (which actually does not save any
keystrokes) to multiple words. Single words are
the most widely used as prediction units because
they are recognizable at a low cognitive load for
the user, and word prediction gives good results
in terms of keystroke savings (Garay-Vitoria and
Abascal, 2006).
There is some variation among methods in the
size and type of buffer used. Most methods use
character n-grams as buffer, because they are pow-
erful and can be implemented independently of the
target language (Carlberger, 1997). In many al-
gorithms the buffer is cleared at the start of each
new word (making the buffer never larger than
the length of the current word). In the paper
by (Van den Bosch and Bogers, 2008), two ex-
tensions to the basic prefix-model are compared.
They found that an algorithm that uses the previ-
ous n characters as buffer, crossing word borders
without clearing the buffer, performs better than
both a prefix character model and an algorithm
that includes the full previous word as feature. In
addition to using the previously typed characters
and/or words in the buffer, word characteristics
such as frequency and recency could also be taken
into account (Garay-Vitoria and Abascal, 2006).
Possible evaluation measures for text predic-
tion are the proportion of words that are correctly
predicted, the percentage of keystrokes that could
maximally be saved (if the user would always
make the correct decision), and the time saved by
the use of the algorithm (Garay-Vitoria and Abas-
cal, 2006). The performance that can be obtained
by text prediction algorithms depends on the lan-
guage they are evaluated on. Lower results are ob-
tained for higher-inflected languages such as Ger-
man than for low-inflected languages such as En-
glish (Matiasek et al 2002). In their overview of
text prediction systems, (Garay-Vitoria and Abas-
cal, 2006) report performance scores ranging from
29% to 56% of keystrokes saved.
An important factor that is known to influence
the quality of text prediction systems, is training
562
set size (Lesher et al 1999; Van den Bosch,
2011). The paper by (Van den Bosch, 2011) shows
log-linear learning curves for word prediction (a
constant improvement each time the training cor-
pus size is doubled), when the training set size is
increased incrementally from 102 to 3?107 words.
3 Our approach to text prediction
We implement a text prediction algorithm for
Dutch, which is a productive compounding lan-
guage like German, but has a somewhat simpler
inflectional system. We do not focus on the effect
of training set size, but on the effect of text type
and topic domain differences.
Our approach to text prediction is largely in-
spired by (Van den Bosch and Bogers, 2008). We
experiment with two different buffer types that are
based on character n-grams:
? ?Prefix of current word? contains all char-
acters of only the word currently keyed in,
where the buffer shifts by one character posi-
tion with every new character.
? ?Buffer15? buffer also includes any other
characters keyed in belonging to previously
keyed-in words.
Modeling character history beyond the current
word can naturally be done with a buffer model in
which the buffer shifts by one position per charac-
ter, while a typical left-aligned prefix model (that
never shifts and fixes letters to their positional fea-
ture) would not be able to do this.
In the buffer, all characters from the text are
kept, including whitespace and punctuation. The
predictive unit is one token (word or punctuation
symbol). In both the buffer and the prediction la-
bel, any capitalization is kept. At each point in the
typing process, our algorithm gives one sugges-
tion: the word that is the most likely continuation
of the current buffer.
We save the training data as a classification data
set: each character in the buffer fills a feature slot
and the word that is to be predicted is the classi-
fication label. Figures 1 and 2 give examples of
each of the buffer types Prefix and Buffer15 that
we created for the text fragment ?tot een niveau?
in the context ?stelselmatig bij elke verkiezing tot
een niveau van? ?(structurally with each election
to a level of ). We use the implementation of the
IGTree decision tree algorithm in TiMBL (Daele-
mans et al 1997) to train our models.
3.1 Evaluation
We evaluate our algorithms on corpus data. This
means that we have to make assumptions about
user behaviour. We assume that the user confirms
a suggested word as soon as it is suggested cor-
rectly, not typing any additional characters before
confirming. We evaluate our text prediction al-
gorithms in terms of the percentage of keystrokes
saved K:
K =
?n
i=0(Fi)?
?n
i=0(Wi)
?n
i=0(Fi)
? 100 (1)
in which n is the number of words in the test
set, Wi is the number of keystrokes that have been
typed before the word i is correctly suggested
and Fi is the number of keystrokes that would be
needed to type the complete word i. For example,
our algorithm correctly predicts the word niveau
after the context i n g t o t e e n n i
v in the test set. Assuming that the user confirms
the word niveau at this point, three keystrokes
were needed for the prefix niv. So, Wi = 3 and
Fi = 6. The number of keystrokes needed for
whitespace and punctuation are unchanged: these
have to be typed anyway, independently of the
support by a text prediction algorithm.
4 Text type experiments
In this section, we describe the first and second se-
ries of experiments. The case study on questions
from the neurological domain is described in Sec-
tion 5.
4.1 Data
In the text type experiments, we evaluate our text
prediction algorithm on four different types of
Dutch text: Wikipedia, Twitter data, transcriptions
of conversational speech, and web pages of Fre-
quently Asked Questions (FAQ). The Wikipedia
corpus that we use is part of the Lassy cor-
pus (Van Noord, 2009); we obtained a version
from the summer of 2010.1 The Twitter data
are collected continuously and automatically fil-
tered for language by Erik Tjong Kim Sang (Tjong
Kim Sang, 2011). We used the tweets from all
users that posted at least 19 tweets (excluding
retweets) during one day in June 2011. This is
a set of 1 Million Twitter messages from 30,000
1http://www.let.rug.nl/vannoord/trees/Treebank/Machine/
NLWIKI20100826/COMPACT/
563
t tot
t o tot
t o t tot
e een
e e een
e e n een
n niveau
n i niveau
n i v niveau
n i v e niveau
n i v e a niveau
n i v e a u niveau
Figure 1: Example of buffer type ?Prefix? for the text fragment ?(elke verkiezing) tot een niveau?. Un-
derscores represent whitespaces.
l k e v e r k i e z i n g tot
k e v e r k i e z i n g t tot
e v e r k i e z i n g t o tot
v e r k i e z i n g t o t tot
v e r k i e z i n g t o t een
e r k i e z i n g t o t e een
r k i e z i n g t o t e e een
k i e z i n g t o t e e n een
i e z i n g t o t e e n niveau
e z i n g t o t e e n n niveau
z i n g t o t e e n n i niveau
i n g t o t e e n n i v niveau
n g t o t e e n n i v e niveau
g t o t e e n n i v e a niveau
t o t e e n n i v e a u niveau
Figure 2: Example of buffer type ?Buffer15? for the text fragment ?(elke verkiezing) tot een niveau?.
Underscores represent whitespaces.
different users. The transcriptions of conversa-
tional speech are from the Spoken Dutch Corpus
(CGN) (Oostdijk, 2000); for our experiments, we
only use the category ?spontaneous speech?. We
obtained the FAQ data by downloading the first
1,000 pages that Google returns for the query ?faq?
with the language restriction Dutch. After clean-
ing the pages from HTML and other coding, the
resulting corpus contained approximately 1.7 Mil-
lion words of questions and answers.
4.2 Within-text type experiments
For each of the four text types, we compare the
buffer types ?Prefix? and ?Buffer15?. In each ex-
periment, we use 1.5 Million words from the cor-
pus to train the algorithm and 100,000 words to
test it. The results are in Table 1.
4.3 Across-text type experiments
We investigate the importance of text type differ-
ences for text prediction with a series of experi-
ments in which we train and test our algorithm on
texts of different text types. We keep the size of
the train and test sets the same: 1.5 Million words
and 100,000 words respectively. The results are in
Table 2.
4.4 Discussion of the results
Table 1 shows that for all text types, the buffer
of 15 characters that crosses word borders gives
better results than the prefix of the current word
only. We get a relative improvement of 35% (for
FAQ) to 62% (for Speech) of Buffer15 compared
to Prefix-only.
Table 2 shows that text type differences have
an influence on text prediction quality: all across-
text type experiments lead to lower results than
the within-text type experiments. From the re-
sults in Table 2, we can deduce that of the four
text types, speech and Twitter language resem-
ble each other more than they resemble the other
two, and Wikipedia and FAQ resemble each other
more. Twitter and Wikipedia data are the least
similar: training on Wikipedia data makes the text
prediction score for Twitter data drop from 29.2 to
16.5%.2
2Note that the results are not symmetric. For example,
564
Table 1: Results from the within-text type experiments in terms of percentages of saved keystrokes.
Prefix means: ?use the previous characters of the current word as features?. Buffer 15 means ?use a buffer
of the previous 15 characters as features?.
Prefix Buffer15
Wikipedia 22.2% 30.5%
Twitter 21.3% 29.2%
Speech 20.7% 33.4%
FAQ 20.2% 27.2%
Table 2: Results from the across-text type experiments in terms of percentages of saved keystrokes, using
the best-scoring configuration from the within-text type experiments: a buffer of 15 characters
Trained on Tested on Wikipedia Tested on Twitter Tested on Speech Tested on FAQ
Wikipedia 30.5% 16.5% 22.3% 24.9%
Twitter 17.9% 29.2% 27.9% 20.7%
Speech 19.7% 22.5% 33.4% 21.0%
FAQ 22.6% 18.2% 22.9% 27.2%
5 Case study: questions about
neurological issues
Online care platforms aim to bring together pa-
tients and experts. Through this medium, patients
can find information about their illness, and get in
contact with fellow-sufferers. Patients who suffer
from neurological damage may have communica-
tive disabilities because their speaking and writ-
ing skills are impaired. For these patients, existing
online care platforms are often not easily accessi-
ble. Aphasia, for example, hampers the exchange
of information because the patient has problems
with word finding.
In the project ?Communicatie en revalidatie
DigiPoli? (ComPoli), language and speech tech-
nologies are implemented in the infrastructure of
an existing online care platform in order to fa-
cilitate communication for patients suffering from
neurological damage. Part of the online care plat-
form is a list of frequently asked questions about
neurological diseases with answers. A user can
browse through the questions using a chat-by-click
interface (Geuze et al 2008). Besides reading the
listed questions and answers, the user has the op-
tion to submit a question that is not yet included in
training on Wikipedia, testing on Twitter gives a different re-
sult from training on Twitter, testing on Wikipedia. This is
due to the size and domain of the vocabularies in both data
sets and the richness of the contexts (in order for the algo-
rithm to predict a word, it has to have seen it in the train set).
If the test set has a larger vocabulary than the train set, a lower
proportion of words can be predicted than when it is the other
way around.
the list. The newly submitted questions are sent to
an expert who answers them and adds both ques-
tion and answer to the chat-by-click database. In
typing the question to be submitted, the user will
be supported by a text prediction application.
The aim of this section is to find the best train-
ing corpus for newly formulated questions in the
neurological domain. We realize that questions
formulated by users of a web interface are dif-
ferent from questions formulated by experts for
the purpose of a FAQ-list. Therefore, we plan to
gather real user data once we have a first version
of the user interface running online. For develop-
ing the text prediction algorithm that is behind the
initial version of the application, we aim to find
the best training corpus using the questions from
the chat-by-click data as training set.
5.1 Data
The chat-by-click data set on neurological issues
consists of 639 questions with corresponding an-
swers. A small sample of the data (translated to
English) is shown in Table 3. In order to create the
test data for our experiments, we removed dupli-
cate questions from the chat-by-click data, leaving
a set of 359 questions.3
In the previous sections, we used corpora of
100,000 words as test collections and we calcu-
lated the percentage of saved keystrokes over the
3Some questions and answers are repeated several times
in the chat-by-click data because they are located at different
places in the chat-by-click hierarchy.
565
Table 3: A sample of the neuro-QA data, translated to English.
question 0 505 Can (P)LS be cured?
answer 0 505 Unfortunately, a real cure is not possible. However, things can be done to combat the effects of the
diseases, mainly relieving symptoms such as stiffness and spasticity. The phisical therapist and reha-
bilitation specialist can play a major role in symptom relief. Moreover, there are medications that can
reduce spasticity.
question 0 508 How is (P)LS diagnosed?
answer 0 508 The diagnosis PLS is difficult to establish, especially because the symptoms strongly resemble HSP
symptoms (Strumpell?s disease). Apart from blood and muscle research, several neurological examina-
tions will be carried out.
Table 4: Results for the neuro-QA questions only in terms of percentages of saved keystrokes, using
different training sets. The text prediction configuration used in all settings is Buffer15. The test samples
are 359 questions with an average length of 7.5 words. The percentages of saved keystrokes are means
over the 359 questions.
Training corpus # words Mean % of saved keystrokes in
neuro-QA questions (stdev)
OOV-rate
Twitter 1.5 Million 13.3% (12.5) 28.5%
Speech 1.5 Million 14.1% (13.2) 26.6%
Wikipedia 1.5 Million 16.1% (13.1) 19.4%
FAQ 1.5 Million 19.4% (15.6) 20.0%
Medical Wikipedia 1.5 Million 28.1% (16.5) 7.0%
Neuro-QA questions (leave-one-out) 2,672 26.5% (19.9) 17.8%
complete test corpus. In the reality of our case
study however, users will type only brief frag-
ments of text: the length of the question they want
to submit. This means that there is potentially a
large deviation in the effectiveness of the text pre-
diction algorithm per user, depending on the con-
tent of the small text they are typing. Therefore,
we decided to evaluate our training corpora sepa-
rately on each of the 359 unique questions, so that
we can report both mean and standard deviation
of the text prediction scores on small (realistically
sized) samples. The average number of words per
question is 7.5; the total size of the neuro-QA cor-
pus is 2,672 words.
5.2 Experiments
We aim to find the training set that gives the best
text prediction result for the neuro-QA questions.
We compare the following training corpora:
? The corpora that we compared in the text type
experiments: Wikipedia, Twitter, Speech and
FAQ, 1.5 Million words per corpus.
? A 1.5 Million words training corpus that is
of the same topic domain as the target data:
Wikipedia articles from the medical domain;
? The 359 questions from the neuro-QA data
themselves, evaluated in a leave-one-out set-
ting (359 times training on 358 questions and
evaluating on the remaining questions).
In order to create the ?medical Wikipedia? cor-
pus, we consulted the category structure of the
Wikipedia corpus. The Wikipedia category ?Ge-
neeskunde? (Medicine) contains 69,898 pages and
in the deeper nodes of the hierarchy we see many
non-medical pages, such as trappist beers (or-
dered under beer, booze, alcohol, Psychoactive
drug, drug, and then medicine). If we remove all
pages that are more than five levels under the ?Ge-
neeskunde? category root, 21,071 pages are left,
which contain fairly over the 1.5 Million words
that we need. We used the first 1.5 Million words
of the corpus in our experiments.
The text prediction results for the different cor-
pora are in Table 4. For each corpus, the out-of-
vocabulary rate is given: the percentage of words
in the Neuro-QA questions that do not occur in the
corpus.4
5.3 Discussion of the results
We measured the statistical significance of the
mean differences between all text prediction
scores using a Wilcoxon Signed Rank test on
paired results for the 359 questions. We found that
4The OOV-rate for the Neuro-QA corpus itself is the av-
erage of the OOV-rate of each leave-one-out experiment: the
proportion of words that only occur in one question.
566
0 10 20 30 40 50 60
0.0
0.2
0.4
0.6
0.8
1.0
ECDFs for text prediction scores on Neuro?QA questions
 using six different training corpora
Text prediction scores
Cu
mu
lat
ive
 Pe
rce
nt 
of 
tes
t c
orp
us
Twitter
Speech
Wikipedia
FAQ
Neuro?QA (leave?one?out)
Medical Wikipedia
Figure 3: Empirical CDFs for text prediction scores on Neuro-QA data. Note that the curves that are at
the bottom-right side represent the better-performing settings.
the difference between the Twitter and Speech cor-
pora on the task is not significant (P = 0.18).
The difference between Neuro-QA and Medical
Wikipedia is significant with P = 0.02; all other
differences are significant with P < 0.01.
The Medical Wikipedia corpus and the leave-
one-out experiments on the Neuro-QA data give
better text prediction scores than the other corpora.
The Medical Wikipedia even scores slightly better
than the Neuro-QA data itself. Twitter and Speech
are the least-suited training corpora for the Neuro-
QA questions, and FAQ data gives a bit better re-
sults than a general Wikipedia corpus.
These results suggest that both text type and
topic domain play a role in text prediction qual-
ity, but the high scores for the Medical Wikipedia
corpus shows that topic domain is even more im-
portant than text type.5 The column ?OOV-rate?
shows that this is probably due to the high cover-
age of terms in the Neuro-QA data by the Medical
5We should note here that we did not control for domain
differences between the four different text types. They are
intended to be ?general domain? but Wikipedia articles will
naturally be of different topics than conversational speech.
Wikipedia corpus.
Table 4 also shows that the standard devia-
tion among the 359 samples is relatively large.
For some questions, we 0% of the keystrokes are
saved, while for other, scores of over 80% are ob-
tained (by the Neuro-QA and Medical Wikipedia
training corpora). We further analyzed the differ-
ences between the training sets by plotting the Em-
pirical Cumulative Distribution Function (ECDF)
for each experiment. An ECDF shows the devel-
opment of text prediction scores (shown on the X-
axis) by walking through the test set in 359 steps
(shown on the Y-axis).
The ECDFs for our training corpora are in Fig-
ure 3. Note that the curves that are at the bottom-
right side represent the better-performing settings
(they get to a higher maximum after having seen
a smaller portion of the samples). From Figure 3,
it is again clear that the Neuro-QA and Medical
Wikipedia corpora outperform the other training
corpora, and that of the other four, FAQ is the best-
performing corpus. Figure 3 also shows a large
difference in the sizes of the starting percentiles:
The proportion of samples with a text prediction
567
Histogram of text prediction scores for the Neuro?QA
 questions trained on Medical Wikipedia
percentage of keystrokes saved
Freq
uen
cy
0 20 40 60 80
0
20
40
60
80
Figure 4: Histogram of text prediction scores
for the Neuro-QA questions trained on Medical
Wikipedia. Each bin represents 36 questions.
score of 0% is less than 10% for the Medical
Wikipedia up to more than 30% for Speech.
We inspected the questions that get a text pre-
diction score of 0%. We see many medical terms
in these questions, and many of the utterances are
not even questions, but multi-word terms repre-
senting topical headers in the chat-by-click data.
Seven samples get a zero-score in the output of all
six training corpora, e.g.:
? glycogenose III.
? potassium-aggrevated myotonias.
26 samples get a zero-score in the output of all
training corpora except for Medical Wikipedia and
Neuro-QA itself. These are mainly short headings
with domain-specific terms such as:
? idiopatische neuralgische amyotrofie.
? Markesbery-Griggs distale myopathie.
? oculopharyngeale spierdystrofie.
Interestingly, the ECDFs show that the Med-
ical Wikipedia and Neuro-QA corpora cross at
around percentile 70 (around the point of 40%
saved keystrokes). This indicates that although the
means of the two result samples are close to each
other, the distribution the scores for the individ-
ual questions is different. The histograms of both
distributions (Figures 4 and 5) confirm this: the
algorithm trained on the Medical Wikipedia cor-
pus leads a larger number of samples with scores
Histogram of text prediction scores for leave?one?out
 experiments on Neuro?QA questions
percentage of keystrokes saved
Freq
uen
cy
0 20 40 60 80
0
20
40
60
80
Figure 5: Histogram of text prediction scores
for leave-one-out experiments on Neuro-QA ques-
tions. Each bin represents 36 questions.
around the mean, while the leave-one-out exper-
iments lead to a larger number of samples with
low prediction scores and a larger number of sam-
ples with high prediction scores. This is also re-
flected by the higher standard deviation for Neuro-
QA than for Medical Wikipedia.
Since both the leave-one-out training on the
Neuro-QA questions and the Medical Wikipedia
led to good results but behave differently for dif-
ferent portions of the test data, we also evaluated a
combination of both corpora on our test set: We
created training corpora consisting of the Medi-
cal Wikipedia corpus, complemented by 90% of
the Neuro-QA questions, testing on the remaining
10% of the Neuro-QA questions. This led to mean
percentage of saved keystrokes of 28.6%, not sig-
nificantly higher than just the Medical Wikipedia
corpus.
6 Conclusions
In Section 1, we asked two questions: (1) ?What
is the effect of text type differences on the quality
of a text prediction algorithm?? and (2) ?What is
the best choice of training data if domain- and text
type-specific data is sparse??
By training and testing our text prediction al-
gorithm on four different text types (Wikipedia,
Twitter, transcriptions of conversational speech
and FAQ) with equal corpus sizes, we found that
there is a clear effect of text type on text prediction
quality: training and testing on the same text type
568
gave percentages of saved keystrokes between 27
and 34%; training on a different text type caused
the scores to drop to percentages between 16 and
28%.
In our case study, we compared a number of
training corpora for a specific data set for which
training data is sparse: questions about neuro-
logical issues. We found significant differences
between the text prediction scores obtained with
the six training corpora: the Twitter and Speech
corpora were the least suited, followed by the
Wikipedia and FAQ corpus. The highest scores
were obtained by training the algorithm on the
medical pages from Wikipedia, immediately fol-
lowed by leave-one-out experiments on the 359
neurological questions. The large differences be-
tween the lexical coverage of the medical domain
played a central role in the scores for the different
training corpora.
Because we obtained good results by both
the Medical Wikipedia corpus and the neuro-QA
questions themselves, we opted for a combination
of both data types as training corpus in the initial
version of the online text prediction application.
Currently, a demonstration version of the appli-
cation is running for ComPoli-users. We hope to
collect questions from these users to re-train our
algorithm with more representative examples.
Acknowledgments
This work is part of the research programme
?Communicatie en revalidatie digiPoli? (Com-
Poli6), which is funded by ZonMW, the Nether-
lands organisation for health research and devel-
opment.
References
J. Carlberger. 1997. Design and Implementation of a
Probabilistic Word Prediciton Program. Master the-
sis, Royal Institute of Technology (KTH), Sweden.
W. Daelemans, A. Van Den Bosch, and T. Weijters.
1997. IGTree: Using trees for compression and clas-
sification in lazy learning algorithms. Artificial In-
telligence Review, 11(1):407?423.
A. Fazly and G. Hirst. 2003. Testing the efficacy of
part-of-speech information in word completion. In
Proceedings of the 2003 EACL Workshop on Lan-
guage Modeling for Text Entry Methods, pages 9?
16.
6http://lands.let.ru.nl/?strik/research/ComPoli/
N. Garay-Vitoria and J. Abascal. 2006. Text prediction
systems: a survey. Universal Access in the Informa-
tion Society, 4(3):188?203.
J. Geuze, P. Desain, and J. Ringelberg. 2008. Re-
phrase: chat-by-click: a fundamental new mode of
human communication over the internet. In CHI?08
extended abstracts on Human factors in computing
systems, pages 3345?3350. ACM.
G.W. Lesher, B.J. Moulton, D.J. Higginbotham, et al
1999. Effects of ngram order and training text size
on word prediction. In Proceedings of the RESNA
?99 Annual Conference, pages 52?54.
Johannes Matiasek, Marco Baroni, and Harald Trost.
2002. FASTY - A Multi-lingual Approach to Text
Prediction. In Klaus Miesenberger, Joachim Klaus,
and Wolfgang Zagler, editors, Computers Helping
People with Special Needs, volume 2398 of Lec-
ture Notes in Computer Science, pages 165?176.
Springer Berlin / Heidelberg.
N. Oostdijk. 2000. The spoken Dutch corpus:
overview and first evaluation. In Proceedings of
LREC-2000, Athens, volume 2, pages 887?894.
Erik Tjong Kim Sang. 2011. Het gebruik van Twit-
ter voor Taalkundig Onderzoek. In TABU: Bulletin
voor Taalwetenschap, volume 39, pages 62?72. In
Dutch.
A. Van den Bosch and T. Bogers. 2008. Efficient
context-sensitive word completion for mobile de-
vices. In Proceedings of the 10th international con-
ference on Human computer interaction with mobile
devices and services, pages 465?470. ACM.
A. Van den Bosch. 2011. Effects of context and re-
cency in scaled word completion. Computational
Linguistics in the Netherlands Journal, 1:79?94,
12/2011.
G. Van Noord. 2009. Huge parsed corpora in LASSY.
In Proceedings of The 7th International Workshop
on Treebanks and Linguistic Theories (TLT7).
S. Westman and L. Freund. 2010. Information Interac-
tion in 140 Characters or Less: Genres on Twitter. In
Proceedings of the third symposium on Information
Interaction in Context (IIiX), pages 323?328. ACM.
569
What Is Not in the Bag of Words forWhy-QA?
Suzan Verberne?
Radboud University Nijmegen
Lou Boves??
Radboud University Nijmegen
Nelleke Oostdijk?
Radboud University Nijmegen
Peter-Arno Coppen?
Radboud University Nijmegen
While developing an approach towhy-QA, we extended a passage retrieval system that uses off-
the-shelf retrieval technology with a re-ranking step incorporating structural information. We
get significantly higher scores in terms of MRR@150 (from 0.25 to 0.34) and success@10. The
23% improvement that we reach in terms of MRR is comparable to the improvement reached on
different QA tasks by other researchers in the field, although our re-ranking approach is based
on relatively lightweight overlap measures incorporating syntactic constituents, cue words, and
document structure.
1. Introduction
About 5% of all questions asked to QA systems are why-questions (Hovy, Hermjakob,
and Ravichandran 2002). Why-questions need a different approach than factoid ques-
tions, because their answers are explanations that usually cannot be stated in a single
phrase. Recently, research (Verberne 2006; Higashinaka and Isozaki 2008) has been
directed at QA forwhy-questions (why-QA). In earlier work on answeringwhy-questions
on the basis of Wikipedia, we found that the answers to most why-questions are pas-
sages of text that are at least one sentence and at most one paragraph in length (Verberne
et al 2007b). Therefore, we aim at developing a system that takes as input a why-
question and gives as output a ranked list of candidate answer passages.
In the current article, we propose a three-step setup for a why-QA system: (1) a
question-processing module that transforms the input question to a query; (2) an off-
the-shelf retrieval module that retrieves and ranks passages of text that share content
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: s.verberne@let.ru.nl.
?? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: l.boves@let.ru.nl.
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: n.oostdijk@let.ru.nl.
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: p.a.coppen@let.ru.nl.
Submission received: 30 July 2008; revised submission received: 18 February 2009; accepted for publication:
4 September 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
with the input query; and (3) a re-ranking module that adapts the scores of the re-
trieved passages using structural information from the input question and the retrieved
passages.
In the first part of this article, we focus on step 2, namely, passage retrieval. The
classic approach to finding passages in a text collection that share content with an
input query is retrieval using a bag-of-words (BOW) model (Salton and Buckley 1988).
BOWmodels are based on the assumption that text can be represented as an unordered
collection of words, disregarding grammatical structure. Most BOW-based models use
statistical weights based on term frequency, document frequency, passage length, and
term density (Tellex et al 2003).
Because BOW approaches disregard grammatical structure, systems that rely on
a BOW model have their limitations in solving problems where the syntactic relation
betweenwords or word groups is crucial. The importance of syntax for QA is sometimes
illustrated by the sentence Ruby killed Oswald, which is not an answer to the question
Who did Oswald kill? (Bilotti et al 2007). Therefore, a number of researchers in the field
investigated the use of structural information on top of a BOW approach for answer
retrieval and ranking (Tiedemann 2005; Quarteroni et al 2007; Surdeanu, Ciaramita, and
Zaragoza 2008). These studies show that although the BOW model makes the largest
contribution to the QA system results, adding structural (syntactic information) can give
a significant improvement.
In the current article, we hypothesize that for the relatively complex problem of
why-QA, a significant improvement?at least comparable to the improvement gained
for factoidQA?can be gained from the addition of structural information to the ranking
component of the QA system. We first evaluate a passage retrieval system for why-QA
based on standard BOW ranking (step 1 and 2 in our set-up). Then we perform an
analysis of the strengths and weaknesses of the BOW model for retrieving and ranking
candidate answers. In view of the observed weaknesses of the BOW model, we choose
our feature set to be applied to the set of candidate answer passages in the re-ranking
module (step 3 in our set-up).
The structural features that we propose are based on the idea that some parts of the
question and the answer passage are more important for relevance ranking than other
parts. Therefore, our re-ranking features are overlap-based: They tell us which parts of
a why-question and its candidate answers are the most salient for ranking the answers.
We evaluate our initial and adapted ranking strategies using a set of why-questions and
a corpus of Wikipedia documents, and we analyze the contribution of both the BOW
model and the structural features.
The main contributions of this article are: (1) we address the relatively new problem
of why-QA and (2) we analyze the contribution of overlap-based structural information
to the problem of answer ranking.
The paper is organized as follows. In Section 2, related work is discussed. Section 3
presents the BOW-based passage retrieval method forwhy-QA, followed by a discussion
of the strengths andweaknesses of the approach in Section 4. In Section 5, we extend our
system with a re-ranking component based on structural overlap features. A discussion
of the results and our conclusions are presented in Sections 6 and 7, respectively.
2. Related Work
Wedistinguish relatedwork in two directions: research into the development of systems
for why-QA (Section 2.1), and research into combining structural and BOW features for
QA (Section 2.2).
230
Verberne et al What Is Not in the Bag of Words forWhy-QA?
2.1 Research intoWhy-QA
In related work (Verberne et al 2007a), we focused on selecting and ranking explanatory
passages for why-QA with the use of rhetorical structures. We developed a system that
employs the discourse relations in a manually annotated document collection: the RST
Treebank (Carlson, Marcu, and Okurowski 2003). This system matches the input ques-
tion to a text span in the discourse tree of the document and it retrieves as answer the
text span that has a specific discourse relation to this question span. We evaluated our
method on a set of 336 why-questions formulated to seven texts from the WSJ corpus.
We concluded that discourse structure can play an important role in why-QA, but that
systems relying on these structures can only work if candidate answer passages have
been annotated with discourse structure. Automatic parsers for creating full rhetorical
structures are currently unavailable. Therefore, a more practical approach appears to
be necessary for work in why-QA, namely, one which is based on automatically created
annotations.
Higashinaka and Isozaki (2008) focus on the problem of ranking candidate answer
paragraphs for Japanese why-questions. They assume that a document retrieval module
has returned the top 20 documents for a given question. They extract features for content
similarity, causal expressions, and causal relations from two annotated corpora and a
dictionary. Higashinaka and Isozaki evaluate their ranking method using a set of 1,000
why-questions that were formulated to a newspaper corpus by a text analysis expert.
70.3% of the reference answers for these questions are ranked in the top 10 by their
system, and MRR1 was 0.328.
Although the approach of Higashinaka and Isozaki is very interesting, their eval-
uation collection has the same flaw as the one used by Verberne et al (2007a): Both
collections consist of questions formulated to a pre-selected answer text. Questions
elicited in response to newspaper texts tend to be unrepresentative of questions asked
in a real QA setting. In the current work, therefore, we work with a set of questions
formulated by users of an online QA system (see Section 3.1).
2.2 Combining Structural and Bag-of-Words Features for QA
Tiedemann (2005) investigates syntactic information from dependency structures in
passage retrieval for Dutch factoid QA. He indexes his corpus at different text layers
(BOW, part-of-speech, dependency relations) and uses the same layers for question
analysis and query creation. He optimizes the query parameters for the passage retrieval
task by having a genetic algorithm apply the weights to the query terms. Tiedemann
finds that the largest weights are assigned to the keywords from the BOW layer and
to the keywords related to the predicted answer type (such as ?person?). The baseline
approach, using only the BOW layer, gives an MRR of 0.342. Using the optimized IR
settings with additional layers, MRR improves to 0.406.
Quarteroni et al (2007) consider the problem of answering definition questions.
They use predicate?argument structures (PAS) for improved answer ranking. They find
that PAS as a stand-alone representation is inferior to parse tree representations, but
that together with the BOW it yields higher accuracy. Their results show a significant
1 The reciprocal rank (RR) for a question is 1 divided by the rank ordinal of the highest ranked relevant
answer. The Mean RR is obtained by averaging RR over all questions.
231
Computational Linguistics Volume 36, Number 2
improvement of PAS?BOW compared to parse trees (F-scores 70.7% vs. 59.6%) but PAS
makes only a very small contribution compared to BOW only (which gives an F-score
of 69.3%).
Recent work by Surdeanu, Ciaramita, and Zaragoza (2008) addresses the problem
of answer ranking for how-to-questions. From Yahoo! Answers,2 they extract a corpus
of 140,000 answers with 40,000 questions. They investigate the usefulness of a large
set of question and answer features in the ranking task. They conclude that the linguistic
features ?yield a small, yet statistically significant performance increase on top of the
traditional BOW and n-gram representation (page 726).?
All these authors conclude that the addition of structural information in QA
gives a small but significant improvement compared to using a BOW-model only. For
why-questions, we also expect to gain improvement from the addition of structural
information.
3. Passage Retrieval forWhy-QA Using a BOWModel
As explained in Section 1, our system comprises three modules: question2query, passage
retrieval, and re-ranking. In the current section, we present the first two system mod-
ules, and the re-ranking module, including a description of the structural features that
we consider, is presented in Section 5. First, however, we describe our data collection
and evaluation method.
3.1 Data and Evaluation Set-up
For our experiments, we use the Wikipedia INEX corpus (Denoyer and Gallinari 2006).
This corpus consists of all 659,388 articles from the online Wikipedia in the summer of
2006 in XML format.
For development and testing purposes, we exploit the Webclopedia question
set (Hovy, Hermjakob, and Ravichandran 2002), which contains questions asked to
the online QA system answers.com. Of these questions, 805 (5% of the total set) are
why-questions. For 700 randomly selected why-questions, we manually searched for an
answer in the Wikipedia XML corpus, saving the remaining 105 questions for future
testing purposes. 186 of these 700 questions have an answer in the corpus.3 Extraction
of one relevant answer for each of these questions resulted in a set of 186 why-questions
and their reference answers.4 Two examples illustrate the type of data we are working
with:
1. ?Why didn?t Socrates leave Athens after he was convicted?? ? ?Socrates
considered it hypocrisy to escape the prison: he had knowingly agreed to
live under the city?s laws, and this meant the possibility of being judged
guilty of crimes by a large jury.?
2 See http://answers.yahoo.com/.
3 Thus, about 25% of our questions have an answer in the Wikipedia corpus. The other questions are either
too specific (Why do ceiling fans turn counter-clockwise but table fans turn clockwise?) or too trivial (Why do
hotdogs come in packages of 10 and hotdog buns in packages of 8?) for the coverage of Wikipedia in 2006.
4 Just like factoid questions, most why-questions generally have one correct answer that can be formulated
in different ways.
232
Verberne et al What Is Not in the Bag of Words forWhy-QA?
2. ?Why do most cereals crackle when you add milk?? ? ?They are made of
a sugary rice mixture which is shaped into the form of rice kernels and
toasted. These kernels bubble and rise in a manner which forms very thin
walls. When the cereal is exposed to milk or juices, these walls tend to
collapse suddenly, creating the famous ?Snap, crackle and pop? sounds.?
To be able to do fast evaluation without elaborate manual assessments, we manually
created one answer pattern for each of the questions in our set. The answer pattern is a
regular expression that defines which of the retrieved passages are considered a relevant
answer to the input question. The first version of the answer patterns was directly
based on the corresponding reference answer, but in the course of the development
and evaluation process, we extended the patterns in order to cover as many as possible
of the Wikipedia passages that contain an answer. For example, for question 1, we
developed the following answer pattern based on two variants of the correct answer
that occur in the corpus: /(Socrates.* opportunity.* escape.* Athens.* considered.*
hypocrisy | leave.* run.* away.* community.* reputation)/.5
In fact, answer judgment is a complex task due to the presence of multiple answer
variants in the corpus. It is a time-consuming process because of the large number of
candidate answers that need to be judged when long lists of answers are retrieved per
question. In future work, we will come back to the assessment of relevant and irrelevant
answers.
After applying our answer patterns to the passages retrieved, we count the ques-
tions that have at least one relevant answer in the top n results. This number divided by
the total number of questions in a test set gives the measure success@n. In Section 3.2,
we explain the levels for n that we use for evaluation. For the highest ranked relevant
answer per question, we determine the RR. Questions for which the system did not
retrieve an answer in the list of 150 results get an RR of 0. Over all questions, we calculate
the mean reciprocal rank MRR.
3.2 Method and Results
In the question2query module of our system we convert the input question to a query
by removing stop words6 and punctuation, and simply list the remaining content words
as query terms.
The second module of our system performs passage retrieval using off-the-shelf
retrieval technology. In Khalid and Verberne (2008), we compared a number of settings
for our passage retrieval task. We considered two different retrieval engines (Lemur7
and Wumpus8), four different ranking models, and two types of passage segmentation:
disjoint and sliding passages. In each setting, 150 results were obtained by the retrieval
engine and ranked by the retrieval model. We evaluated all retrieval settings in terms of
5 Note that the vertical bar separates the two alternatives.
6 To this end we use the stop word list that can be found at http://marlodge.supanet.com/museum/
funcword.html.We use all items except the numbers and the word why.
7 Lemur is an open source toolkit for information retrieval that provides flexible support for different types
of retrieval models. See http://www.lemurproject.org.
8 Wumpus is an information retrieval system mainly geared at XML retrieval. See http://www.wumpus-
search.org/.
233
Computational Linguistics Volume 36, Number 2
MRR@n9 and success@n for levels n = 10 and n = 150. For the evaluation of the retrieval
module, we were mainly interested in the scores for success@150 because re-ranking
can only be successful if at least one relevant answer was returned by the retrieval
module.
We found that the best-scoring passage retrieval setting in terms of success@150 is
Lemur on an index of sliding passages with TF-IDF (Zhai 2001) as ranking model. We
obtained the following results with this passage retrieval setting: success@150 is 78.5%,
success@10 is 45.2%, and MRR@150 is 0.25. We do not include the results obtained with
the other retrieval settings here because the differences were small.
The results show that for 21.5% of the questions in our set, no answer was retrieved
in the top-150 results. We attempted to increase this coverage by retrieving 250 or
500 answers per question but this barely increased the success score at maximum
n. The main problems for the questions that we miss are infamous retrieval prob-
lems such as the vocabulary gap between a question and its answer. For example,
the answer to Why do chefs wear funny hats? contains none of the words from the
question.
4. The Strengths and Weaknesses of the BOWModel
In order to understand how answer ranking is executed by the passage retrieval mod-
ule, we first take a closer look at the TF-IDF algorithm as it has been implemented in
Lemur. TF-IDF is a pure BOW model: Both the query and the passages in the corpus
are represented by the term frequencies (numbers of occurrences) for each of the words
they contain. The terms are weighted using their inverse document frequency (IDF),
which puts a higher weight on terms that occur in few passages than on terms that
occur in many passages. The term frequency (TF) functions for the query and the doc-
ument, and the parameter values chosen for these functions in Lemur can be found in
Zhai (2001).
As explained in the previous section, we consider success@150 to be the most
important measure for the retrieval module of our system. However, for the system as a
whole, success@10 is a more important evaluation measure. This is because users tend
to pay much more attention to the top 10 results of a retrieval system than to results that
are ranked lower (Joachims et al 2005). Therefore, it is interesting to investigate which
questions are answered in the top 150 and not in the top 10 by our passage retrieval
module. This is the set of questions for which the BOW model is not effective enough
and additional (more specific) overlap information is needed for ranking a relevant
answer in the top 10.
We analyzed the set of questions that get a relevant answer at a rank between 10 and
150 (62 questions), which belowwewill refer to as our focus set. We compared our focus
set to the questions for which a relevant answer is in the top 10 (84 questions). Although
these numbers are too small to do a quantitative error analysis, a qualitative analysis
provides valuable insights into the strengths and weaknesses of a BOW representation
such as TF-IDF. In Sections 4.1 to 4.4 we discuss four different aspects of why-questions
that present problems for the BOWmodel.
9 Note that MRR is often used without the explicit cut-off point (n). We add it to clarify that RR is 0 for the
questions without a correct answer in the top-n.
234
Verberne et al What Is Not in the Bag of Words forWhy-QA?
4.1 Short Questions
Ten questions in our focus set contain only one or two content words. We can see the
effect of short queries if we compare three questions that contain only one semantically
rich content word.10 The rank of the highest ranked relevant answer is given between
parentheses; the last of these three questions is in our focus set.
1. Why do people hiccup? (2)
2. Why do people sneeze? (4)
3. Why do we dream? (76)
We found that the rank of the relevant answer is related to the corpus frequency of
the single semantically rich word, which is 64 for hiccup, 220 for sneeze, and 13,458 for
dream. This means that many passages are retrieved for question 3, making the chances
for the relevant answer to be ranked in the top 10 smaller. One way to overcome the
problem of long result lists for short queries is by adding words to the query that make
it more specific. In the case of why-QA, we know that we are not simply searching
for information on dreaming but for an explanation for dreaming. Thus, in the ranking
process, we can extend the query with explanatory cue words such as because.11 We
expect that the addition of explanatory cue phrases will give an improvement in ranking
performance.
4.2 The Document Context of the Answer
There are many cases where the context of the candidate answer gives useful infor-
mation. Consider, for example, the question Why does a snake flick out its tongue?, the
correct answer to which was ranked 29. A human searcher expects to find the answer
in a Wikipedia article about snakes. Within the Snake article he or she may search for
the words flick and/or tongue in order to find the answer. This suggests that in some
cases there is a direct relation between a specific part of the question and the context
(document and/or section) of the candidate answer. In cases like this, the answer
document and the question apparently share the same topic (snake). By analogy with
linguistically motivated approaches to factoid QA (Ferret et al 2002), we introduce the
term question focus for this topic.
In the example question flick is the word with the lowest corpus frequency (556),
followed by tongue (4,925) and snake (6,809). Using a BOW approach to document title
matching, candidate answers from documents with flick or tongue in their title would
be ranked higher than answers from documents with snake in their title. Thus, for
questions for which there is overlap between the question focus and the title of the
answer documents (two thirds of the questions in our set), we can improve the ranking
of candidate answers by correctly predicting the question focus. In Section 5.1.2, we
make concrete suggestions for achieving this.
10 The word people in subject position is a semantically poor content word.
11 The addition of cue words can also be considered to be applied in the retrieval step. We come back to this
in Section 6.3.
235
Computational Linguistics Volume 36, Number 2
4.3 Multi-Word Terms
A very important characteristic of the BOWmodel is that words are considered separate
terms. One of the consequences is that multi-word terms such as multi-word noun
phrases (mwNPs) are not treated as a single term. Here, three examples of questions
are shown in which the subject is realized by a mwNP (underlined in the examples; the
rank of the relevant answer is shown between brackets):
1. Why are hush puppies called hush puppies? (1)
2. Why is the coral reef disappearing? (29)
3. Why is a black hole black? (31)
We investigated the corpus frequencies for the separate parts of each mwNP. We found
that these are quite high for coral (3,316) and reef (2,597) compared to the corpus
frequency of the phrase coral reef (365). The numbers are even more extreme for black
(103,550) and hole (9,734) versus black hole (1,913). On the other hand, the answer to
the hush puppies question can more easily be ranked because the corpus frequencies
for the separate terms hush (594) and puppies (361) are relatively low. This shows that
multi-word terms do not necessarily give problems for the BOW model as long as the
document frequencies for the constituent words are relatively low. If (one of) the words
in the phrase is/are frequent, it is very difficult to rank the relevant answer high in the
result list with use of word overlap only.
In our focus set, 36 of the 62 questions contain a mwNP. For these questions, we can
expect improved ranking from the addition of NPs to our feature set.
4.4 Syntactic Structure
The BOW model does not take into account sentence structure. The potential impor-
tance of sentence structure for improved ranking can be exemplified by the following
two questions from our set. Note that both examples contain a subordinate clause (finite
or non-finite):
1. Why do baking soda and vinegar explode when you mix them together? (4)
2. Why are there 72 points to the inch when discussing fonts and printing? (36)
In both cases, the contents of the subordinate clause are less important to the goal of the
question than the contents of themain clause. In the first example, this is (coincidentally)
reflected by the corpus frequencies of the words in both clauses: mix (12,724) and
together (83,677) have high corpus frequencies compared to baking (832), soda (1,620),
vinegar (871), and explode (1,285). As a result, the reference answer containing these
terms is ranked in the top-10 by TF-IDF. In the second example, however, the corpus
frequencies do not reflect the importance of the terms. Fonts and printing have lower
corpus frequencies (1,243 and 6,978, respectively) than points (43,280) and inch (10,046).
Thus, fonts and printing are weighted heavier by TF-IDF although these terms are only
peripheral to the goal of the query, the core of which isWhy are there 72 points to the inch?
This cannot be derived from the corpus frequencies, but can only be inferred from the
syntactic function (adverbial) of when discussing fonts and printing in the question.
Thus, the lack of information about sentence structure in the BOW model does
not necessarily give rise to problems as long as the importance of the question terms
is reflected by their frequency counts. If term importance does not align with corpus
236
Verberne et al What Is Not in the Bag of Words forWhy-QA?
frequency, grammatical structure becomes potentially useful. Therefore, we expect that
syntactic structure can make a contribution to cases where the importance of the terms
is not reflected by their corpus frequencies but can be derived from their syntactic
function.
4.5 What Can We Expect from Structural Information?
In Sections 4.1 to 4.4 we discussed four aspects of why-questions that are problematic
for the BOW model. We expect contributions from the inclusion of information on cue
phrases, question focus and the document context of the answer, noun phrases, and
the syntactic structure of the question. We think that it is possible to achieve improved
ranking performance if features based on structural overlap are taken into account
instead of global overlap information.
5. Adding Overlap-Based Structural Information
From our analyses in Section 4, we found a number of question and answer aspects
that are potentially useful for improving the ranking performance of our system. In
this section, we present the re-ranking module of our system. We define a feature set
that is inspired by the findings from Section 4 and aims to find out which structural
features of a question?answer pair contribute the most to better answer ranking. We
aim to weigh these features in such a way that we can optimize ranking performance.
The input data for our re-ranking experiments is the output of the passage retrieval
module. A success@150 score of 78.5% for passage retrieval (see Section 3.2) means that
the maximum success@10 score that we can achieve by re-ranking is 78.5%.
5.1 Features for Re-ranking
The first feature in our re-ranking method is the score that was assigned to a candidate
answer by Lemur/TF-IDF in the retrieval module (f0). In the following sections we
introduce the other features that we implemented. Each feature represents the overlap
between two item bags:12 a bag of question items (for example: all the question?s noun
phrases, or the question?s main verb) and a bag of answer items (for example: all answer
words, or all verbs in the answer). The value that is assigned to a feature is a function of
the overlap between these two bags. We used the following overlap function:
S(Q,A) =
QA + AQ
Q+ A
(1)
in whichQA is the number of question items that occur at least once in the bag of answer
items, AQ is the number of answer items that occur at least once in the bag of question
items, and Q+ A is the number of items in both bags of items joined together.
5.1.1 The Syntactic Structure of the Question. In Section 4.4, we argued that some syntactic
parts of the question may be more important for answer ranking than others. Because
we have no quantitative evidence yet which syntactic parts of the question are the most
important, we created overlap features for each of the following question parts: phrase
12 Note that a ?bag? is a set in which duplicates are counted as distinct items.
237
Computational Linguistics Volume 36, Number 2
heads (f1), phrase modifiers (f2); the subject (f3), main verb (f4), nominal predicate (f5),
and direct object (f6) of the main clause; and all noun phrases (f11). For each of these
question parts, we calculated its word overlap with the bag of all answer words. For the
features f3?f6, we added a variant where as answer items only words/phrases with the
same syntactic function as the question token were included (f7, f8, f9, f10).
Consider for example question 1 from Section 3.1: Why didn?t Socrates leave Athens
after he was convicted?, and the reference answer as the candidate answer for which we
are determining the feature values: Socrates considered it hypocrisy to escape the prison: he
had knowingly agreed to live under the city?s laws, and this meant the possibility of being judged
guilty of crimes by a large jury.
From the parser output, our feature extraction script extracts Socrates as subject,
leave as main verb, and Athens as direct object. Neither leave nor Athens occur in the
answer passage, thus f4, f6, f8, and f10 are all given a value of 0. So are f5 and f9,
because the question has no nominal predicate. For the subject Socrates, our script finds
that it occurs once in the bag of answer words. The overlap count for the feature f3 is
thus calculated as 1+11+18 = 0.105.
13 For the feature f7, our script extracts the grammatical
subjects Socrates, he, and this from the parser?s representation of the answer passage.
Because the bag of answer subjects for f7 contains three items, the overlap is calculated
as 1+11+3 = 0.5.
5.1.2 The Semantic Structure of the Question. In Section 4.2, we saw that often there is a
link between the question focus and the title of the document in which the reference
answer is found. In those cases, the answer document and the question share the same
topic. For most questions, the focus is the syntactic subject: Why do cats sleep so much?
Judging from our data, there are two exceptions to this general rule: (1) If the subject
is semantically poor, the question focus is the (verbal or nominal) predicate: Why do
people sneeze?, and (2) in case of etymology questions (which cover about 10% of
why-questions), the focus is the subject complement of the passive sentence: Why are
chicken wings called Buffalo Wings?
We included a feature (f12) for matching words from the question focus to words
from the document title and a feature (f13) for the relation between question focuswords
and all answer words. We also include a feature (f14) for the other, non-focus question
words.
5.1.3 The Document Context of the Answer. Not only is the document title in relation to
the question focus potentially useful for answer ranking, but also other aspects of the
answer context. We include four answer context features in our feature set: overlap
between the question words and the title of the Wikipedia document (f15), overlap be-
tween question words and the heading of the answer section (f16), the relative position
of the answer passage in the document (f17), and overlap between a fixed set of words
that we selected as explanatory cues when they occur in a section heading and the set
of words that occur in the section heading of the passage (f18).14
13 The bag of question subjects contains one item (Socrates, the 1 in the denominator) and one item from
this bag occurs in the bag of answer words (the left 1 in the numerator). Without stopwords, the bag of
all answer words contains 18 items, one of which occurs in the bag of question subjects (the right 1 in
the numerator).
14 We found these section heading cues by extracting all section headings from the Wikipedia corpus,
sorting them by frequency, and then manually marking those section heading words that we expect
to occur with explanatory sections. The result is a small set of heading cues (history, origin, origins,
background, etymology, name, source, sources) that is independent of the test set we work with.
238
Verberne et al What Is Not in the Bag of Words forWhy-QA?
5.1.4 Synonyms. For each of the features f1 to f10 and f12 to f16 we add an alternative
feature (f19 to f34) covering the set of all WordNet synonyms for all question terms in
the original feature. For synonyms, we apply a variant of Equation (1) in which QA is
interpreted as the number of question items that have at least one synonym in the bag
of answer items and AQ as the number of answer items that occur in at least one of the
synonym sets of the question items.
5.1.5 WordNet Relatedness. Additionally, we included a feature representing the related-
ness between the question and the candidate answer using the WordNet Relatedness
tool (Pedersen, Patwardhan, and Michelizzi 2004) (f35). As a measure of relatedness,
we choose the Lesk measure, which incorporates information fromWordNet glosses.
5.1.6 Cue Phrases. Finally, as proposed in Section 4.1, we added a closed set of cue phrases
that are used to introduce an explanation (f36). We found these explanatory phrases in a
way that is commonly used for finding answer cues and that is independent of our own
set of question?answer pairs. We queried the key answer words to the most frequent
why-question on the Web Why is the sky blue? (blue sky rayleigh scattering) to the MSN
Search engine15 and crawled the first 250 answer fragments that are retrieved by the
engine. From these, we manually extracted all phrases that introduce the explanation.
This led to a set of 47 cue phrases such as because, as a result of, which explains why,
and so on.
5.2 Extracting Feature Values from the Data
For the majority of features we needed the syntactic structure of the input question,
and for some of the features also of the answer. We experimented with two different
syntactic parsers for these tasks: the Charniak parser (Charniak 2000) and a develop-
ment version of the Pelican parser.16 Of these, Pelican has a more detailed descriptive
model and gives better accuracy but Charniak is at present more robust for parsing
long sentences and large amounts of text. We parsed the questions with Pelican because
we need accurate parsings in order to correctly extract all constituents. We parsed all
answers (186 ? 150 passages) with Charniak because of its speed and robustness.
For feature extraction, we used the following external components: A stop word
list,17 the sets of cue phrases as described in Sections 5.1.3 and 5.1.6, the CELEX Lemma
lexicon (Burnage et al 1990), the WordNet synonym sets, the WordNet Similarity
tool (Pedersen, Patwardhan, and Michelizzi 2004), and a list of pronouns and semanti-
cally poor nouns.18 We used a Perl script for extracting feature values for each question?
answer pair. For each feature, the script composes the required bags of question items
and answer items. All words are lowercased and punctuation is removed. For terms
in the question set that consist of multiple words (for example, a multi-word subject),
spaces are replaced by underscores before stop words are removed from the question
and the answer. Then the script calculates the similarity between the two sets for each
feature following Equation (1).19
15 http://www.live.com.
16 See http://lands.let.ru.nl/projects/pelican/.
17 See Section 3.1.
18 Semantically poor nouns that we came across in our data set are the nouns humans and people.
19 A multi-word term from the question is counted as one item.
239
Computational Linguistics Volume 36, Number 2
Table 1
Results for the why-QA system: the complete system including re-ranking compared against
plain Lemur/TF-IDF for 187 why-questions.
Success@10 Success@150 MRR@150
Lemur/TF-IDF?sliding 45.2% 78.5% 0.25
TF-IDF + Re-ranking using 37 structural features 57.0% 78.5% 0.34
Whether or not to lemmatize the terms before matching them is open to debate.
In the literature, there is some discussion on the benefit of lemmatization for question
answering (Bilotti, Katz, and Lin 2004). Lemmatization can be especially problematic
in the case of proper names (which are not always recognizable by capitalization).
Therefore, we decided only to lemmatize verbs (for features f4 and f8) in the current
version of our system.
5.3 Re-ranking Method
Feature extraction led to a vector consisting of 37 feature values for each of the 27,900
items in the data set. We normalized the feature values over all 150 answer candidates
for the same question to a number between 0 and 1 using the L1 vector norm. Each
instance (representing one question?answer pair) was automatically labeled 1 if the
candidate answer matched the answer pattern for the question and 0 if it did not.
On average, a why-question had 1.6 correct answers among the set of 150 candidate
answers.
In the process of training our re-ranking module, we aim at combining the 37
features in a ranking function that is used for re-ordering the set of candidate answers.
The task of finding the optimal ranking function for ranking a set of items is referred to
as ?learning to rank? in the information retrieval literature (Liu et al 2007). In Verberne
et al (2009), we compared several machine learning techniques20 for our learning-
to-rank problem. We evaluated the results using 5-fold cross validation on the ques-
tion set.
5.4 Results from Re-ranking
The results for the complete system compared with passage retrieval with Lemur/
TF-IDF only are in Table 1.We show the results in terms of success@10, success@150, and
MRR@150. We only present the results obtained using the best-performing learning-
to-rank technique: logistic regression.21 A more detailed description of our machine
learning method and a discussion of the results obtained with other learning techniques
can be found in Verberne et al (2009).
20 Naive Bayes, Support Vector Classification, Support Vector Regression, Logistic regression, Ranking
SVM, and a genetic algorithm, all with several optimization functions.
21 We used the lrm function from the Design package in R (http://cran.r-project.org/web/packages/
Design) for training and evaluating models based on logistic regression.
240
Verberne et al What Is Not in the Bag of Words forWhy-QA?
After applying our re-ranking module, we found a significant improvement over
bare TF-IDF in terms of success@10 and MRR@150 (z = ?4.29, p < 0.0001 using the
Wilcoxon Signed-Rank test for paired reciprocal ranks).
5.5 Which Features Made the Improvement?
In order to evaluate the importance of our features, we rank them according to the
coefficient that was assigned to them in the logistic regression model (See Table 2). We
only consider features that are significant at the p = 0.05 level. We find that all eight
significant features are among the top nine features with the highest coefficient.
The feature ranking is discussed in Section 6.1.
6. Discussion
In the following sections, we discuss the feature ranking (Section 6.1), make a compari-
son to other re-ranking approaches (Section 6.2), and explain the attempts that we made
at solving the remaining problems (Section 6.3).
6.1 Discussion of the Feature Ranking
Table 2 shows that only a small subset (8) of our 37 features significantly contribute to
the re-ranking score. The highest ranked feature is TF-IDF (the bag of words), which is
not surprising since TF-IDF alone already reaches an MRR@150 of 0.25 (see Section 3.2).
In Section 4.5, we predicted a valuable contribution from the addition of cue phrases,
question focus, noun phrases, and the document context of the answer. This is partly
confirmed by Table 2, which shows that among the significant features are the feature
that links question focus to document title and the cue phrases feature. The noun
phrases feature (f11) is actually in the top nine features with the highest coefficient but
its contribution was not significant at the 0.05 level (p = 0.068).
The importance of question focus for why-QA is especially interesting because it
is a question feature that is specific to why-questions and does not similarly apply
Table 2
Features that significantly contribute to the re-ranking score (p < 0.05), ranked by their
coefficient in the logistic regression model (representing their importance).
Feature Coefficient
TF-IDF (f0) 0.39**
Overlap between question focus synonyms and document title (f30) 0.25**
Overlap between question object synonyms and answer words (f28) 0.22
Overlap between question object and answer objects (f10) 0.18*
Overlap between question words and document title synonyms (f33) 0.17
Overlap between question verb synonyms and answer words (f24) 0.16
WordNet Relatedness (f35) 0.16*
Cue phrases (f36) 0.15*
Asterisks on coefficients denote the level of significance for the feature: ** p < 0.001; * 0.001 <
p < 0.01; no asterisk means 0.01 < p < 0.05.
241
Computational Linguistics Volume 36, Number 2
to factoids or other question types. Moreover, the link from the question focus to the
document title shows that Wikipedia as an answer source can provide QA systems with
more information than a collection of plain texts with less discriminative document
titles does.
The significance of cue phrases is also an important finding. In fact, including cue
phrases in the why-QA process is the only feasible way of specifying which passages
are likely to contain an explanation (i.e., an answer to a why-question). In earlier work
(Verberne et al 2007a), we pointed out that higher-level annotation such as discourse
structure can give useful information in the why-answer selection process. However,
the development of systems that incorporate discourse structure suffers from the lack
of tools for automated annotation. The current results show that surface patterns (the
literal presence of items from a fixed set of cue words) are a step in the direction of
answer selection.
The significant features in Table 2 also show us which question constituents are
the most salient for answer ranking: focus, main verb, and direct object. We think that
features incorporating the question?s subject are not found to be significant because, in
a subset of the questions, the subject is semantically poor. Moreover, because for most
questions the subject is the question focus, the subject features and the focus features are
correlated. In our data, the question focus apparently is the more powerful predictor.
6.2 Comparison to Other Approaches
The 23% improvement that we reach in terms of MRR@150 (from 0.25 to 0.34) is com-
parable to that reached by Tiedemann in his work on improving factoid QA with the
use of structural information.
In order to see whether the improvement that we achieved with re-ranking is
on account of structural information or just the benefit of using word sequences, we
experimented with a set of re-ranking features based on sequences of question words
that are not syntactically defined. In this re-ranking experiment, we included TF-IDF,
word bigrams, and word trigrams as features. The resulting performance was around
baseline level (MRR = 0.25), significantly worse than re-ranking with structural overlap
features. This is still true if we add the cue word feature (which, in isolation, only gives
a small improvement to baseline performance) to the n-gram features.
6.3 Solving the Remaining Problems
Although the results in terms of success@10 and MRR@150 are satisfactory, there is still
a substantial proportion of why-questions that is not answered in the top 10 result list.
In this section, we discuss a number of attempts that we made to further improve our
system.
First, after we found that for some question parts synonym expansion leads to
improvement (especially the main verb and direct object), we experimented with the
addition of synonyms for these constituents in the retrieval step of our system (Lemur).
We found, however, that it does not improve the results due to the large synonym sets
of many verbs and nouns which add much noise and lead to very long queries. The
same holds for the addition of cue words in the retrieval step.
Second, although our re-ranking module incorporates expansion to synonym sets,
there are many question?answer pairs where the vocabulary gap between the question
242
Verberne et al What Is Not in the Bag of Words forWhy-QA?
and the answer is still a problem. There are cases where semantically related terms in
the question and the answer are of different word classes (e.g., hibernate?hibernation),
and there are cases of proper nouns that are not covered by WordNet (e.g., B.B. King).
We considered using dynamic stemming for verb?noun relations such as the hibernation
case but research has shown that stemming hurts as many queries as it helps (Bilotti,
Katz, and Lin 2004). Therefore, we experimented with a number of different semantic
resources, namely, the nominalization dictionary Nomlex (Meyers et al 1998) and the
wikiOntology by Ponzetto and Strube (2007). However, in their current state of develop-
ment these semantic resources cannot improve our system because their coverage is too
low to make a contribution to our re-ranking module. Moreover, the present version of
the wikiOntology is very noisy and requires a large amount of cleaning up and filtering.
Third, we considered that the use of cue phrases may not be sophisticated enough
for finding explanatory relations between question and answer. Therefore, we exper-
imented with the addition of cause?effect pairs from the English version of the EDR
Concept Dictionary (Yokoi 1995) ? as suggested by Higashinaka and Isozaki (2008).
Unfortunately, the list appeared to be extremely noisy, proving it not useful as a source
for answer ranking.
7. Conclusions and Directions for Future Research
In the current research, we extended a passage retrieval system for why-QA using off-
the-shelf retrieval technology (Lemur/TF-IDF) with a re-ranking step incorporating
structural information. We get significantly higher scores in terms of MRR@150 (from
0.25 to 0.34) and success@10. The 23% improvement that we reach in terms of MRR
is comparable to that reached on various other QA tasks by other researchers in the
field (see Section 6.3). This confirms our hypothesis in Section 1 that for the relatively
complex problem of why-QA, a significant improvement can be gained by the addition
of structural information to the ranking component of the QA system.
Most of the features that we implemented for answer re-ranking are based on word
overlap between part of the question and part of the answer. As a result of this set-up,
our features identify the parts of why-questions and their candidate answers that are the
most powerful/effective for ranking the answers. The question constituents that appear
to be the most important are the question focus, the main verb, and the direct object. On
the answer side, most important are the title of the document in which the candidate
answer is embedded and knowledge on the presence of cue phrases.
Because our features are overlap-based, they are relatively easy to implement. For
implementation of some of the significant features, a form of syntactic parsing is needed
that can identify subject, verb, and direct object from the question and sentences in
the candidate answers. An additional set of rules is needed for finding the question
focus. Finally, we need a fixed list for identifying cue phrases. Exploiting the title of
answer documents in the feature set is only feasible if the documents that may contain
the answers have titles and section headings similar to Wikipedia.
In conclusion, we developed a method for significantly improving a BOW-based
approach to why-QA that can be implemented without extensive semantic knowledge
sources. Our series of experiments suggest that we have reached the maximum per-
formance that can be obtained using a knowledge-poor approach. Experiments with
more complex types of information (discourse structure, cause?effect relations) show
that these information sources have not as yet developed sufficiently to be exploited in
a QA system.
243
Computational Linguistics Volume 36, Number 2
References
Bilotti, M. W., B. Katz, and J. Lin. 2004.
What works better for question
answering: Stemming or morphological
query expansion. In Proceedings of the
Workshop on Information Retrieval for
Question Answering (IR4QA) at SIGIR 2004,
Sheffield.
Bilotti, M. W., P. Ogilvie, J. Callan, and
E. Nyberg. 2007. Structured retrieval for
question answering. In Proceedings of the
30th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 351?358,
Amsterdam.
Burnage, G., R. H. Baayen, R. Piepenbrock,
and H. van Rijn. 1990. CELEX: A Guide for
Users. CELEX, University of Nijmegen,
the Netherlands.
Carlson, Lynn, Daniel Marcu, and
Mary Ellen Okurowski. 2003. Building
a discourse-tagged corpus in the
framework of Rhetorical Structure Theory.
In Jan van Kuppevelt and Ronnie Smith,
editors, Current Directions in Discourse and
Dialogue. Kluwer Academic Publishers,
Dordrecht, pages 85?112.
Charniak, E. 2000. A maximum-entropy-
inspired parser. ACM International
Conference Proceeding Series, 4:132?139.
Denoyer, L. and P. Gallinari. 2006. The
Wikipedia XML corpus. ACM SIGIR
Forum, 40(1):64?69.
Ferret, O., B. Grau, M. Hurault-Plantet,
G. Illouz, L. Monceaux, I. Robba, and
A. Vilnat. 2002. Finding an answer
based on the recognition of the question
focus. NIST Special Publication,
pages 362?370.
Higashinaka, R. and H. Isozaki. 2008.
Corpus-based question answering for
why-questions. In Proceedings of IJCNLP,
pages 418?425, Hyderabad.
Hovy, E. H., U. Hermjakob, and
D. Ravichandran. 2002. A question/
answer typology with surface text
patterns. In Proceedings of the Human
Language Technology conference (HLT),
pages 247?251, San Diego, CA.
Joachims, T., L. Granka, B. Pan,
H. Hembrooke, and G. Gay. 2005.
Accurately interpreting clickthrough data
as implicit feedback. In Proceedings of the
28th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 154?161,
Salvador, Brazil.
Khalid, M. and S. Verberne. 2008. Passage
retrieval for question answering using
Sliding Windows. In Proceedings of the
COLING 2008 Workshop IR4QA,
Manchester, UK.
Liu, T. Y., J. Xu, T. Qin, W. Xiong, and H. Li.
2007. Letor: Benchmark dataset for
research on learning to rank for
information retrieval. In Proceedings of
the Workshop on Learning to Rank for
Information Retrieval (LR4IR) at SIGIR 2007,
pages 3?10, Amsterdam.
Meyers, A., C. Macleod, R. Yangarber,
R. Grishman, L. Barrett, and R. Reeves.
1998. Using NOMLEX to produce
nominalization patterns for information
extraction. In Proceedings: The
Computational Treatment of Nominals,
volume 2, pages 25?32, Montreal.
Pedersen, T., S. Patwardhan, and
J. Michelizzi. 2004. WordNet::Similarity ?
measuring the relatedness of concepts. In
Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025,
San Jose, CA.
Ponzetto, S. P. and M. Strube. 2007. Deriving
a large scale taxonomy fromWikipedia.
In Proceedings of the National Conference on
Artificial Intelligence, pages 1440?1445,
Vancouver, BC.
Quarteroni, S., A. Moschitti, S. Manandhar,
and R. Basili. 2007. Advanced structural
representations for question classification
and answer re-ranking. In Proceedings of
ECIR 2007, pages 234?245, Rome.
Salton, G. and C. Buckley. 1988.
Term-weighting approaches in automatic
text retrieval. Information Processing and
Management, 24(5):513?523.
Surdeanu, M., M. Ciaramita, and
H. Zaragoza. 2008. Learning to rank
answers on large online QA collections. In
Proceedings of ACL 2008, pages 719?727,
Columbus, OH.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and
G. Marton. 2003. Quantitative evaluation
of passage retrieval algorithms for
question answering. In Proceedings of the
26th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 41?47,
Toronto.
Tiedemann, J. 2005. Improving passage
retrieval in question answering using
NLP. In Progress in Artificial Intelligence,
volume 3808. Springer, Berlin /
Heidelberg, pages 634?646.
Verberne, S. 2006. Developing an approach
for why-question answering. In Conference
Companion of the 11th Conference of the
European Chapter of the Association for
244
Verberne et al What Is Not in the Bag of Words forWhy-QA?
Computational Linguistics (EACL 2006),
pages 39?46, Trento.
Verberne, S., L. Boves, N. Oostdijk, and
P. A. Coppen. 2007a. Discourse-based
answering of why-questions. Traitement
Automatique des Langues (TAL), special issue
on ?Discours et document: traitements
automatiques?, 47(2):21?41.
Verberne, S., L. Boves, N. Oostdijk, and
P. A. Coppen. 2007b. Evaluating
discourse-based answer extraction for
why-question answering. In Proceedings of
the 30th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 735?736,
Amsterdam.
Verberne, S., H. Van Halteren, D. Theijssen,
S. Raaijmakers, and L. Boves. 2009.
Learning to rank QA data. In Proceedings
of the Workshop on Learning to Rank for
Information Retrieval (LR4IR) at SIGIR 2009,
pages 41?48, Boston, MA.
Yokoi, T. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
Zhai, C. 2001. Notes on the Lemur TFIDF
model. Technical report, School of
Computer Science, Carnegie Mellon
University.
245

Text Representations for Patent Classification
Eva D?hondt?
Radboud University Nijmegen
Suzan Verberne??
Radboud University Nijmegen
Cornelis Koster?
Radboud University Nijmegen
Lou Boves?
Radboud University Nijmegen
With the increasing rate of patent application filings, automated patent classification is of rising
economic importance. This article investigates how patent classification can be improved by
using different representations of the patent documents. Using the Linguistic Classification
System (LCS), we compare the impact of adding statistical phrases (in the form of bigrams)
and linguistic phrases (in two different dependency formats) to the standard bag-of-words text
representation on a subset of 532,264 English abstracts from the CLEF-IP 2010 corpus. In
contrast to previous findings on classification with phrases in the Reuters-21578 data set, for
patent classification the addition of phrases results in significant improvements over the unigram
baseline. The best results were achieved by combining all four representations, and the second
best by combining unigrams and lemmatized bigrams. This article includes extensive analyses of
the class models (a.k.a. class profiles) created by the classifiers in the LCS framework, to examine
which types of phrases are most informative for patent classification. It appears that bigrams
contribute most to improvements in classification accuracy. Similar experiments were performed
on subsets of French and German abstracts to investigate the generalizability of these findings.
1. Introduction
Around the world, the patent filing rates in the national patent offices have been in-
creasing year after year, creating an enormous volume of texts, which patent examiners
? Center for Language Studies, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: e.dhondt@let.ru.nl.
?? Center for Language Studies / Institute for Computing and Information Sciences, PO Box 9103, 6500 HD
Nijmegen, the Netherlands. E-mail: s.verberne@let.ru.nl.
? Institute for Computing and Information Sciences, PO Box 9010, 6500 HD Nijmegen, the Netherlands.
E-mail: kees@cs.ru.nl.
? Center for Language Studies, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: l.boves@let.ru.nl.
Submission received: 19 March 2012; revised submission received: 8 August 2012; accepted for publication:
19 September 2012.
doi:10.1162/COLI a 00149
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
are struggling to manage (Benzineb and Guyot 2011). To speed up the examination
process, a patent application needs to be directed to patent examiners specialized
in the subfield(s) of that particular patent as quickly as possible (Smith 2002). This
preclassification is done automatically in most patent offices, but substantial addi-
tional manual labor is still necessary. Furthermore, since 2010, the International Patent
Classification1 (IPC) is revised every year to keep track of recent developments in the
various subdomains. Such a revision is followed by a reclassification of portions of
the existing patent corpus, which is currently done mainly by hand by the national
patent offices (Held, Schellner, and Ota 2011). Both preclassification and reclassification
could be improved, and a higher consistency of the classifications of the documents
in the patent corpus could be obtained, if more reliable and precise automatic text
classification algorithms were available (Benzineb and Guyot 2011).
Most approaches to text classification use the bag-of-words (BOW) text representa-
tion, which represents each document by the words that occur in it, irrespective of their
ordering in the original document. In the last decades much research has gone into
expanding this representation with additional information, such as statistical phrases2
(n-grams) or some forms of syntactic or semantic knowledge. Even though (statistical)
phrases are more representative units for classes than single words (Caropreso, Matwin,
and Sebastiani 2001), they are so sparsely distributed that they have limited impact
during the classification process. Therefore, it is not surprising that the best scoring
multi-class, multi-label3 classification results for the well-known Reuters-21578 data set
have been obtained using a BOW representation (Bekkerman and Allan 2003). But the
limited contribution of phrases in addition to the BOW-representation does not seem
to hold for all classification tasks: O?zgu?r and Gu?ngo?r (2010) found significant differ-
ences in the impact of linguistic phrases between short newswire texts (Reuters-21578),
scientific abstracts (NSF), and informal posts in usenet groups (MiniNg): Especially the
classification of scientific abstracts could be improved by using phrases as index terms.
In a follow-up study, O?zgu?r and Gu?ngo?r (2012) found that for the three different data
sets, different types of linguistic phrases have most impact. The authors conclude that
more formal text types benefit from more complex syntactic dependencies.
In this article, we investigate if similar improvements can be found for patent
classification and, more specifically, which types of phrases are most effective for this
particular task. In this article we investigate the value of phrases for classification by
comparing the improvements that can be gained from extending the BOW representa-
tion with (1) statistical phrases (in the form of bigrams); (2) linguistic phrases originating
from the Stanford parser (see Section 3.2.2); (3) aboutness-based4 linguistic phrases from
the AEGIR parser (Section 3.2.3); and (4) a combination of all of these. Furthermore, we
will investigate the importance of different syntactic relations for the classification task,
1 The IPC is a complex hierarchical classification system comprising sections, classes, subclasses, and
groups. For example, the ?A42B 1/12? class label, which groups designs for bathing caps, falls under
section A ?Human necessities,? class 42 ?Headwear,? subclass B ?Head coverings,? group 1 ?Hats; caps;
hoods.? The latest edition of the IPC contains eight sections, 129 classes, 639 subclasses, 7,352 groups,
and 61,847 subgroups. The IPC covers inventions in all technological fields in which inventions can
be patented.
2 By a phrase we mean an index unit consisting of two or more words, generated through either syntactic
or statistical methods.
3 Multi-class classification is the problem of classifying instances into more than two classes. Multi-label
signifies that documents in this test set are associated with more than one class, and must be assigned a
set of labels during classification.
4 The notion of aboutness refers to the conceptual content expressed by a dependency triple. For a more
detailed description, see Section 3.2.3.
756
D?hondt et al Text Representations for Patent Classification
and the extent to which the words in the phrases overlap with the unigrams. We also
investigate which syntactic relations capture most information in the opinion of human
annotators. Finally, we perform experiments to investigate if our findings are language-
dependent. We will then draw some conclusions on what information is most valuable
for improving automatic patent classification.
2. Background
2.1 Text Representations in Classification
Lewis (1992) was the first to investigate the use of phrases as index terms for text
classification. He found that phrases generally suffer from data sparseness and may
actually cause classification performance to deteriorate. These findings were confirmed
by Apte?, Damerau, and Weiss (1994). With the advent of increasing computational
power and bigger data sets, however, the topic has been revisited in the last two decades
(Bekkerman and Allan 2003).
In this section we will give an overview of the major findings in previous re-
search on the use of statistical and syntactic phrases for text classification. Except when
mentioned explicitly, all the classification experiments reported here were conducted
using the Reuters-21578 data set, a well-known benchmark of 21,578 short newswire
texts for multi-class classification into 118 categories (a document has an average of
1.24 class labels).
2.1.1 Combining Unigrams with Statistical Phrases. For an excellent overview of the work
on using phrases done up to 2002, see Bekkerman and Allan (2003), and Tan, Wang, and
Lee (2002).
Because they contain more specific information, one might think that phrases are
more powerful features for text classification. There are two ways of using phrases as
index terms: either index terms only or in combination with unigrams. All experimental
results, however, show that using only phrases as index terms leads to a decrease
in classification accuracy compared with the BOW baseline (Bekkerman and Allan
2003). Both Mladenic and Grobelnik (1998) and Fu?rnkranz (1998) showed that classifiers
trained on combinations of unigrams and n-grams composed of at most three words
performed better than classifiers that only use unigrams; no improvement was obtained
when using larger n-grams. Because trigrams are sparser than bigrams, most of the
subsequent research has focused on optimizing the combination of unigrams and
bigrams using different feature selection techniques.
2.1.2 Feature Selection. Obviously, unigrams and bigrams overlap: Bigrams are pairs of
unigrams. Caropreso, Matwin, and Sebastiani (2001) evaluated the relative importance
of unigrams and bigrams in a classifier-independent study: Instead of determining the
impact of features on the classification scores, they scored all unigrams and bigrams
using conventional feature evaluation functions to find the features that are most
representative for the document classes. For the Reuters-21578 data set, they found
that many bigram features scored higher than unigram features. These (theoretical)
findings were not confirmed in subsequent classification experiments, however. When
the bigram/unigram ratio for a fixed number of features is changed to favor bigrams,
classification performance tends to go down. It appears that the information in the
bigrams does not turn the unigrams redundant.
757
Computational Linguistics Volume 39, Number 3
Braga, Monard, and Matsubara (2009) used a Multinomial Naive Bayes classifier
to investigate classification performance with unigrams and bigrams by comparing
multiview classification (the results of two independent classifiers trained with unigram
and bigram features are merged) with monoview classification (unigrams and bigrams
are combined in a single feature set).5 They found that there is little difference between
the output of the mono- and multiview classifiers. In the multiview classifiers, the
unigram and bigram classifiers make similar decisions in assigning labels, although
the latter generally yielded lower confidence values. Consequently, in the merge the
unigram and bigram classifiers affirm each other?s decisions, which does not result
in an overall improvement in classification accuracy. The authors suggest combining
unigrams only with those bigrams for which it holds that the whole provides more
information than the sum of the parts.
Tan, Wang, and Lee (2002) proposed selecting highly representative and meaningful
bigrams based on the Mutual Information scores of the words in a bigram compared
with the unigram class model. They selected only the top 2% of the bigrams as index
terms, and found a significant improvement over their unigram baseline, which was low
compared to state-of-the-art results. Bekkerman and Allan (2003) failed to improve over
their unigram baseline when using similar selection criteria based on the distributional
clustering of unigram models. Crawford, Koprinska, and Patrick (2004) were not able to
improve e-mail classification when using the selection criteria proposed by Tan, Wang,
and Lee.
2.1.3 Combining Unigrams with Syntactic Phrases. Lewis (1992) and Apte?, Damerau, and
Weiss (1994) were the first to investigate the impact of syntactic phrases6 as features for
text classification. Dumais et al (1998) and Scott and Matwin (1999) did not observe
a significant improvement in classification on the Reuters-21578 collection when noun
phrases obtained with a shallow parser were used instead of unigrams. Moschitti and
Basili (2004) found that neither words augmented with word sense information, nor
syntactic phrases (acquired through shallow parsing) in combination with unigrams
improved over the BOW baseline. Syntactic phrases appear to be even sparser than
bigrams. Therefore, it is not surprising that most papers concluded that classifiers using
only syntactic phrases perform worse than the baseline, except when the BOW baseline
is low for that particular classification task (Mitra et al 1997; Fu?rnkranz 1999).
Deep syntactic parsing is a computationally expensive process, but thanks to the
increase in computational power it is now possible to use phrases acquired through
deep syntactic parsing in classification tasks. Nastase, Sayyad, and Caropreso (2007)
used Minipar to generate dependency triples that are combined with lemmatized and
unlemmatized unigrams to classify the 10 most frequent classes in the Reuters-21578
data set. Their criterion for selecting triples as index terms is document frequency ? 2.
The small improvement over the lemmatized unigram baseline was not statistically
significant. O?zgu?r and Gu?ngo?r (2010, 2012) achieve small but significant improvements
when combining unigrams with a subset of the dependency types from the Stanford
parser on three different data sets, including the Reuters-21578 set. They find that
separate pruning levels (based on the term frequency?inverse document frequency
[TF-IDF] score of the index units) for the unigrams and syntactic phrases influence
5 The difference between multiview and monoview classification corresponds to what is called late and
early fusion in the pattern recognition literature.
6 The concept ?syntactic phrase? can be given several different interpretations, such as noun phrases,
verb phrases, predicate structures, dependency triples, and so forth.
758
D?hondt et al Text Representations for Patent Classification
classification accuracy. Which dependency relations prove most relevant for a classifi-
cation task depends greatly on the language use in the different data sets: The informal
MiniNG data set (usenet posts) benefits a little from ?simple? dependencies such as part,
denoting a phrasal verb, for example write down, while classification in the more formal
Reuters-21578 (newswire) and NSF (scientific abstracts) data sets is more improved by
using dependencies on phrase and clause level (adjectival modifier, compound noun,
prepositional attachment; and subject and object, respectively). The highest-ranking
features for the NSF data set are compound noun (nn), adjectival modifier (amod),
subject (subj), and object (obj), respectively. Furthermore, they observe that splitting up
more generic relator types (such as prep) into different, more specific, subtypes increases
the classification accuracy.
2.2 Patent Classification
It is not possible to draw far-reaching conclusions from previous research on patent
classification, because there is no tradition of using a ?standard? data set, and a standard
split of patent corpora in a training and test set. Furthermore, there are differences
between the various experiments in task definitions (mono-label versus multi-label
classification); the granularity of the classification (depth in the IPC hierarchy); and the
choices of (sub)sets of data. Fall and Benzineb (2002) give an overview of the work done
in patent classification research up to 2002 and of the commercial patent classification
systems available; see Benzineb and Guyot (2011) for a general introduction to patent
classification.
Larkey (1999) was the first to present a fully automated patent classification system,
but she did not report her overall accuracy results. Larkey (1998) used a combination
of weighted words and noun phrases as index terms to classify a subset of the USPTO
database, but found no improvement over a BOW baseline. The weights were calculated
as follows: Frequency of a word or phrase in a particular section times the manually
assigned weight (importance) given to that section. The weights for each word or phrase
were then summed across sections. Term selection was based on a threshold for these
weights.
Krier and Zacca` (2002) organized a comparative study of various academic and
commercial systems for patent classification for a common data set. In this informal
benchmark Koster, Seutter, and Beney (2001) achieved the best results, using the Bal-
anced Winnow algorithm with a word-only text representation. Classification is per-
formed for 44 or 549 categories (which correspond to different levels of depth in the
then used version of the IPC), with around 78% and 68% precision at 100% recall,
respectively.
Fall et al (2003) introduced the EPO-alpha data set, attempting to create a common
benchmark for patent classification. Using only words as index terms, they tested
different classification algorithms and found that SVM outperform Naive Bayes, k-NN,
SNoW, and decision-based classifiers. They achieved P@3-scores7 of 73% and 59% on
114 classes and 451 subclasses, respectively. They also found that when using only
the first 300 words from the abstract, claims, and description sections, classification
accuracy is increased compared with using the complete sections. The same data set
was later used by Koster and Seutter (2003), who experimented with a combined
7 Precision at rank 3 (P@3) signifies the percentage correct labels in the first three labels by the classifier to a
given document.
759
Computational Linguistics Volume 39, Number 3
representation of words and phrases consisting of head-modifier pairs.8 They found
that head-modifier pairs could not improve on the BOW-baseline: The phrases were too
sparse to have much impact on the classification process.
Starting in 2009, the IRF9 has organized CLEF-IP patent classification tracks in an
attempt to bridge the gap between academic research and the patent industry. For this
purpose the IRF has put a lot of effort into providing very large patent data sets,10 which
have enabled academic researchers to train their algorithms on real-life data. In the
CLEF-IP 2010 classification track the best results were achieved by Guyot, Benzineb,
and Falquet (2010). Using the Balanced Winnow algorithm, they achieved a P@1-score
of 83%, while classifying on subclass level. They used a combination of words and
statistical phrases (collocations of variable length extracted from the corpus) as index
terms and used all available documents (in English, French, and German) in the corpus
as training data. In the same competition, Derieux et al (2010) came second (in terms of
P@1). They also used a mixed document representation of both single words and longer
phrases, which had been extracted from the corpus by counting word co-occurrences.
Verberne, Vogel, and D?hondt (2010) and Beney (2010) experimented with a combined
representation of words and syntactic phrases derived from an English and French
syntactic parser, respectively. They both found that adding syntactic phrases to words
improves classification accuracy slightly. Beney (2010) remarks that this improvement
may be language-dependent. As a follow-up, Koster et al (2011) investigated the added
value of syntactic phrases. They found that attributive phrases, that is, combinations
of adjective or nouns with nouns, were by far the most important syntactic phrases for
patent classification. On a subset of the CLEF-IP 2010 corpus11 they also found a small,
but significant, improvement when adding dependency triples to words.
3. Experimental Set-up
In this article, we investigate the relative contributions of different types of terms to
the performance of patent classification. We use four different types of terms, namely,
lemmatized unigrams, lemmatized bigrams (see Section 3.2.1), lemmatized dependency
triples obtained with the Stanford parser (see Section 3.2.2), and lemmatized depen-
dency triples obtained with the AEGIR parser (see Section 3.2.3). We will leave term
(feature) selection to the preprocessing module of the Linguistic Classification System
(LCS) which we used for all experiments (see Section 3.3). We will analyze the rela-
tion between unigrams and phrases in the class profiles in some detail, however (see
Sections 4.2 and 4.3).
3.1 Data Selection
We conducted classification experiments on a collection of patent documents obtained
from the CLEF-IP 2010 corpus,12 which is a subset of the larger MAREC patent col-
lection. The corpus contains 2.6 million patent documents, which roughly correspond
8 Head-modifier pairs were derived from the syntactic analysis output of the EP4IR syntactic parser.
9 Information Retrieval Facility, see www.irf.com.
10 The CLEF-IP 2009, CLEF-IP 2010, and CLEF-IP 2011 data sets can be obtained through the IRF. The more
recent data sets subsume the older sets.
11 The same data set as will be used in this article. For a more detailed description, see Section 3.1.
12 This test collection is available through the IRF (http://www.ir-facility.org/collection).
760
D?hondt et al Text Representations for Patent Classification
to 1.3 million individual patents, published between 1985 and 2001.13 The documents
in the collection are encoded in a customized XML format and may include text in
English, French, and German. In addition to the standard sections of a patent document
(title, abstract, claims, and description section), the documents also include meta-
information on inventor, date of application, assignee, and so forth. Because our focus
lies on text representation, we did not include any of the meta-data in our document
representations.
The most informative sections of a patent document are generally considered to
be the title, the abstract, and the beginning of the description (Benzineb and Guyot
2011). Verberne and D?hondt (2011) showed that using both the description and the
abstract gives a small, but significant, improvement in classification results on the
CLEF-IP 2011 corpus, compared with classification on abstracts only. The effort in-
volved in parsing the descriptions is considerable, however: Because of the long
sentences and the dense word use, a parser will have much more difficulty in pro-
cessing text from the description section than from the abstracts. The titles of the
patent documents also pose a parsing problem: These are generally short noun phrases
that contain ambiguous PP-attachments that are impossible to disambiguate without
any domain knowledge. This leads to incorrect syntactic analyses and, consequently,
noisy dependency triple features. Because we are interested in comparing classification
results for different text representations, and not in comparing results for different
sections, we opted to use only the abstract sections of the patent document in the
current article.
From the corpus, we extracted all files that contain both an abstract in English and at
least one IPC class14 in the <classification-ipcr> field. We extracted the IPC classes on the
document level; this means that we did not include the documents where the IPC class
is in a separate file than the English abstract. In total, there were 121 different classes in
our data set. Most documents have been assigned one to three different IPC classes (on
class level). On average, a patent abstract in our data set has 2.12 class labels. Previous
cross-validation experiments on the same document set showed very little variation
(standard deviation < 0.3%) between the classification accuracies in different training-
test splits (Verberne, Vogel, and D?hondt 2010). We therefore decided to use only one
training and test set split.15
The final data set contained 532,264 abstracts, divided into two sets: (1) a training
set (425,811 documents) and (2) a test set (106,453 documents). The distribution of the
data over the classes is in accordance with the Pareto Principle: 20% of the classes cover
80% of the data, and 80% of the classes comprise only 20% of the data.
3.2 Data Preprocessing
Preprocessing included cleaning up character conversion errors like Expression (1)
and removing claims and images references (Expression (2)) and list references
13 Note the difference between a patent and a patent document: A patent is not a physical document itself,
but a name for a group of patent documents that have the same patent ID number.
14 For our classification experiments we use the codes on the class level in the IPC8 classification.
15 The data split was performed using a perl script that randomly shuffles the documents and puts them
into a train set and test set, while ensuring that the class distribution of the examples in the train set
approximates that of the whole corpus. It can be downloaded as part of the LCS distribution.
761
Computational Linguistics Volume 39, Number 3
(Expression (3)) from the original texts. This was done automatically, using the follow-
ing regular expressions (based on Parapatics and Dittenbach 2009):
s/;gt&/>/g (1)
s/(\([ ]*[0-9][0-9a-z,.; ]*\))//g (2)
s/(\([ ]*[A-Za-z]\))//g (3)
We then used a perl script to divide the running text into sentences, by splitting on
end-of-sentence punctuation such as question marks and full stops. In order to mini-
mize incorrect splitting, the perl script was supplied with a list of common English
abbreviations and a list containing abbreviations and acronyms that occur frequently in
technical texts, derived from the Specialist lexicon.16
3.2.1 Unigrams and Bigrams. The sentences in the abstract documents were converted
to single words by splitting on whitespaces and removing punctuation. The words
were then lemmatized using the AEGIR lexicon. Bigrams were created through a
similar procedure. We did not create bigrams that spanned sentence boundaries. This
resulted in approximately 60 million unigram and bigram tokens for the present
corpus.
3.2.2 Stanford. The Stanford parser is a broad-coverage natural language parser that is
trained on newswire text, for which it achieves state-of-the-art performance. The parser
has not been optimized/retrained for the patent domain.17 In spite of the technical
difficulties (Parapatics and Dittenbach 2009) and loss of linguistic accuracy for patent
texts reported in Mille and Wanner (2008), most patent processing systems that use
linguistic phrases use the Stanford parser because its dependency scheme has a number
of properties that are valuable for Text Mining purposes (de Marneffe and Manning
2008). The Stanford parser collapsed typed dependency model has a set of 55 differ-
ent syntactic relators to capture semantically contentful relations between words. For
example, the sentence The system will consist of four separate modules is analyzed into the
following set of dependency triples in the Stanford representation:
det(system-2, The-1)
nsubj(consist-4, system-2)
aux(consist-4, will-3)
root(ROOT-0, consist-4)
num(modules-8, four-6)
amod(modules-8, separate-7)
prep_of(consist-4, modules-8)
The Stanford parser was compiled with a maximum memory heap of 1.2 GB.
Sentences longer than 100 words were automatically skipped. Combined with failed
parses this led to a 1.2% loss of parser output on the complete data set. Parsing the
16 The lexicon can be downloaded at http://lexsrv3.nlm.nih.gov/Specialist/Summary/lexicon.html.
17 For retraining a parser, a substantial amount of annotated data (in the form of syntactically annotated
dependency trees) is needed. Creating such annotations is a very expensive task and beyond the scope of
this article.
762
D?hondt et al Text Representations for Patent Classification
Table 1
Impact of lemmatization on the different text types in the training set (80% of the corpus).
# tokens # types (terms) token/type (lem.)
raw lemmatized gain
unigram 48,898,738 160,424 142,396 1.12 343.39
bigram 48,473,756 3,836,212 3,119,422 1.23 15.54
Stanford 35,772,003 8,750,839 7,430,397 1.18 4.81
AEGIR 31,004,525 ? 5,096,918 ? 6.08
entire set of abstracts took 1.5 weeks on a computer cluster consisting of 60 2.4GHz
cores with 4 GB RAM per core. The resulting dependency triples were stripped of the
word indexes and then lemmatized using the AEGIR lexicon.
3.2.3 AEGIR. AEGIR18 is a dependency parser that was designed specifically for ro-
bust parsing of technical texts. It combines a hand-crafted grammar with an exten-
sive word-form lexicon. The parser lexicon was compiled from different technical
terminologies, such as the SPECIALIST lexicon and the UMLS.19 The AEGIR parser
aims to capture the aboutness of sentences. Rather than outputting extensive linguis-
tic detail on the syntactic structure of the input sentence as in the Stanford parser,
AEGIR returns only the bare syntactic?semantic structure of the sentence. During the
parsing process, it effectively performs normalization at various levels, such as ty-
pography (for example, upper and lower case, spacing), spelling (for example, British
and American English, hyphenation), morphology (lemmatization of word forms), and
syntax (standardization of the word order and transforming passive structures into
active ones).
The AEGIR parser uses only eight syntactic relators and returns fewer unique
triples than the Stanford parser. The parser is currently still under development; for this
article we used the version AEGIR v.1.7.5. The parser was constrained to a time limit of
maximum three seconds per sentence. This caused a loss of 0.7% of parser output on the
complete data set. Parsing the entire set of abstracts took slightly less than a week on
the computer cluster described above. The AEGIR parser has several output formats,
among which its own dependency format. The example sentence used to illustrate the
output of the Stanford parser is analyzed as follows:
[system,SUBJ,consist]
[consist,PREPof,module]
[module,ATTR,separate]
[module,QUANT,four]
3.2.4 Lemmatization. Table 1 shows the impact of lemmatization (using the AEGIR lex-
icon) on the distribution of terms for the different text representations. Lemmatization
18 AEGIR stands for Accurate English Grammar for Information Retrieval. Using the AGFL compiler (found at
http://www.agfl.cs.ru.nl/) this grammar can be compiled into an operational parser. The grammar is
not freely distributed.
19 The Unified Medical Language System contains a widely-used terminology of the biomedical domain
and can be downloaded at http://www.nlm.nih.gov/research/umls/.
763
Computational Linguistics Volume 39, Number 3
and stemming are standard approaches to decreasing the sparsity of features; stemming
is more aggressive than lemmatization. Ozgu?r and Gu?ngo?r (2009) showed that?when
using only words as index terms?stemming (with the Porter Stemmer) appears to
improve performance; stemming dependency triples did not improve performance,
however.
We opted to use a less aggressive form of generalization: Lemmatizing the word
forms. We found that the bigrams gain20 most by lemmatizing the word forms, resulting
in a higher token/type ratio. From Table 1 it can be seen that there are fewer triple
tokens than bigram tokens: Whereas all the (high-frequency) function words are kept in
the bigram representations, both dependency formats discard some function words in
their parser output. For example, the AEGIR parser does not create triples for auxiliary
verbs, and in both dependency formats, the prepositions become part of the relator.
Consequently, the parsers will output fewer but more variable tokens, which results in
lower token/type ratios and a lower impact of lemmatization.
3.3 Classification Experiments
The classification experiments were carried out within the framework of the LCS
(Koster, Seutter, and Beney 2003). The LCS has been developed for the purpose of
comparing different text representations. Currently, three classifier algorithms are avail-
able: Naive Bayes, Balanced Winnow (Dagan, Karov, and Roth 1997), and SVM-light
(Joachims 1999). Verberne, Vogel, and D?hondt (2010) found that Balanced Winnow and
SVM-light yield comparable classification accuracy scores for patent texts on a similar
data set, but that Balanced Winnow is much faster than SVM-light for classification
problems with a large number of classes. The Naive Bayes classifier yielded a lower
accuracy. We therefore only used the Balanced Winnow algorithm for our classification
experiments, which were run with the following LCS configuration, based on tuning
experiments on the same data by Koster et al (2011):
 Global term selection (GTS): Document frequency minimum is 2, term frequency
minimum is 3. Although initial term selection is necessary when dealing with such
a large corpus, we deliberately aimed at keeping as many of the sparse phrasal
terms as possible.
 Local term selection (LTS): Simple Chi Square (Galavotti, Sebastiani, and Simi
2000). We used the LCS option to automatically select the most representative
terms for every class, with a hard maximum of 10,000 terms per class.21
 After LTS the selected terms of all classes are aggregated into one combined term
vocabulary, which is used as the starting point for training the individual classes
(see Table 3).
20 By ?gain? we mean the decrease in number of types for the lemmatized forms compared to the
non-lemmatized forms, which will result in higher corresponding token/type ratios.
21 Increasing the cut-off to 100,000 terms resulted in a small increase in accuracy (F1 values) for the
combined representations, mostly for the larger classes. Because the patent domain has a large lexical
variety, a large amount of low-frequency terms in the tail of the term distribution can have a large
impact on the accuracy scores. Because we are more interested in the relative gains between different
text representations and the corresponding top terms in the class profiles than in achieving maximum
classification scores, we opted to use only 10,000 terms for efficiency reasons.
764
D?hondt et al Text Representations for Patent Classification
Table 2
Impact of global term selection (GTS) criteria on the different text types in the training set (80%
of the corpus).
total # of terms # of terms selected in GTS % of terms removed in GTS
unigram 142,396 58,42322 58.97
bigram 3,119,422 1,115,170 64.25
stanford 7,430,397 1,618,478 78.22
AEGIR 5,096,918 1,312,715 74.24
 Term strength calculation: LTC algorithm (Salton and Buckley 1988) which is an
extension of the TF?IDF measure.
 Training method: Ensemble learning based on one-versus-rest binary classifiers.
 Winnow configuration: We performed tuning experiments for the Winnow param-
eters on a development set of around 100,000 documents. We arrived at using the
same setting as Koster et al (2011), namely, ? = 1.02, ? = 0.98, ?+ = 2.0, ?? = 0.5,
with a maximum of 10 training iterations.
 For each document the LCS returns a ranked list of all possible labels and the
attendant confidence scores. If the score assigned is higher than a predetermined
threshold, the document is assigned that category. The Winnow algorithm has a
default (natural) threshold equal to one. We configured the LCS to return a min-
imum of one label (with the highest score, even if it is lower than the threshold)
and a maximum of four labels for each document.
 The classification quality was determined by calculating the Precision, Recall, and
F1 measures per document/class combination (see, e.g., Koster, Seutter, and Beney
2003), on the document level (micro-averaged scores).
Table 2 shows the impact of our global term selection criteria for the different text
representations. This first feature reduction step is category-independent: The features
are discarded on the basis of the term and document frequencies over the corpus, dis-
regarding their distributions for the specific categories. We can see that the token/type
ratio of Table 1 is mirrored in this table: The sparsest syntactic phrases lose most terms.
Although the Stanford parser output is the sparsest text representation, it has the largest
pool of terms to select from at the end of the GTS process.
The impact of the second feature reduction phase is shown in Table 3. During local
term selection, the LCS finds the most representative terms for each class by selecting
the terms whose distributions in the sets of positive and negative training examples
for that class are maximally different from the general term distribution. We can see
that in the combined runs only around 50% of the selectable unigrams (after GTS) are
22 For the BOW baseline, the GTS criteria resulted in a too small term set that could then be used as a
starting point for the local term selection process for the individual classes. In such cases, the LCS has
a back-off mechanism that automatically (re)selects terms that were initially discarded during GTS.
In other words, the baseline classifier used terms that do not comply with the criteria in the GTS as
described in the text. In the combination runs, enough terms remained after GTS and no unigrams or
phrases that did not match the GTS criteria were selected.
765
Computational Linguistics Volume 39, Number 3
Table 3
Impact of local term selection (LTS) criteria in the training set (80% of the corpus).
# of terms after GTS # of terms after LTS
baseline uni 58,423 69,476
unigrams + bigrams uni 58,423 23,753bi 1,115,170 300,826
unigrams + stanford triples uni 58,423 26,630stanford 1,618,478 424,204
unigrams + AEGIR triples uni 58,423 29,348AEGIR 1,312,715 409,851
Table 4
Classification results on CLEF-IP 2010 English abstracts, with ranges for a 95% confidence
interval. Bold figures indicate the best results obtained with the five classifiers. (P: Precision;
R: Recall, F1: F1-score).
P R F1
weighted random guessing 6.09% ? 0.14 6.04% ? 0.14 6.06% ? 0.14
unigrams 76.27% ? 0.26 66.13% ? 0.28 70.84% ? 0.27
unigrams + bigrams 79.00% ? 0.24 70.19% ? 0.27 74.34% ? 0.26
unigrams + Stanford triples 78.35% ? 0.25 69.57% ? 0.28 73.70% ? 0.26
unigrams + AEGIR triples 78.51% ? 0.25 69.18% ? 0.28 73.55% ? 0.26
all representations 79.51% ? 0.24 71.11% ? 0.27 75.08% ? 0.26
selected as features during LTS. This means that the phrases replace at least a part of the
information contained in the possible unigrams.
4. Results and Discussion
4.1 Classification Accuracy
Table 4 shows the micro-averages of Precision, Recall, and F1 for five classification ex-
periments with different document representations. To give an idea of the complexity of
the task we have included a random guessing baseline in the first row.23 We found that
extending a unigram representation with statistical and/or linguistic phrases gives a
significant improvement in classification accuracy over the unigram baseline. The best-
performing classifier is the one that combines all four text representations. When adding
only type of phrase to unigrams, the unigrams + bigrams combination is significantly
better than the combinations with syntactic phrases. Combining all four representations
boosts recall, but has less impact on precision.
23 The script used to calculate the baseline can be downloaded at http://lands.let.ru.nl/~dhondt/.
We used a weighted randomization that takes the category label distributions and label frequency
distributions into account.
766
D?hondt et al Text Representations for Patent Classification
Table 5
Penetration of the bigrams and triples in the B60 class profiles (in % of terms at given rank).
rnk10 rnk20 rnk50 rnk100 rnk1000
bigrams 3.0 4.0 48.0 45.0 70.5
stanford 0.0 1.0 24.0 26.0 48.0
AEGIR 0.0 0.5 20.0 25.0 44.9
all representations
bigrams 2.0 2.0 34.0 36.0 43.2
stanford 0.0 0.0 4.0 6.0 13.0
AEGIR 0.0 0.0 2.0 4.0 18.0
The results are similar to O?zgu?r and Gu?ngo?r?s (2012) findings for scientific
abstracts: Adding phrases to unigrams can significantly improve classification. The text
in the patent corpus is vastly different from the newswire text in the Reuters corpus.
Like scientific abstracts, patents are full of jargon and terminology, often expressed in
multi-word units, which might favor phrasal representations. Moreover, the innovative
concepts in a patent are sometimes described in generalized terms combined with some
specifier (to ensure larger legal scope). For example, a hose might be referred to as a
watering device. The term hose can be captured with a unigram representation, but the
multi-word expression cannot. The difference with the results on the Reuters-21578
data set (discussed in Section 2.1.1), however, may not completely be due to genre
differences: Bekkerman and Allan (2003) remark that the unigram baseline for the
Reuters-21578 task is difficult to improve upon, because in that data set a few keywords
are enough to distinguish between the categories.
4.2 Unigram versus Phrases
In this section we investigate whether adding phrases suppresses, complements, or
changes unigram selection. To examine the impact of phrases in the classification
process, we analyzed the class profiles24 of two large classes (H04 ? Electric Communica-
tion Technique; and H01 ? Basic electric elements) that show significant improvements
in both Precision and Recall25 for the bigram classifier compared with the unigram
baseline. We look at (1) the overlap of the single words in the class profiles of the
unigram and combined representations; and (2) the overlap of the single words and the
words that make up the phrases (hereafter referred to as parts) within the class profile
of one text representation.
4.2.1 Overlap of Unigrams. The class profiles in the baseline unigram classifier contained
far fewer terms ( < 20%) than the profiles in the classifiers that combine unigrams and
phrases. This could be expected from the data in tables 2 and 3.
Unigrams are the highest ranked26 features in the combined representation class
profiles (see Table 5). Furthermore, words that are important terms for unigram clas-
sification also rank high in the combined class profiles: On average, there is an 80%
24 A class profile is the model built by the LCS classifier for a class during training. It consists of a ranked
list of terms that contribute most to distinguishing members from a class from all other classes.
25 H04: P: + 3.09%; R: + 1.83%; H01: P: + 3.61%; R: + 5.14%.
26 The rank of a term is based on the decreasing order of mass assigned to that term in the class profile.
(See Section 4.2.2.)
767
Computational Linguistics Volume 39, Number 3
overlap of the top 1,000 most important words in unigram and combined representation
class profiles. This decreases to 75% when looking at the 5,000 most important words.
This shows that the classifier tends to select mostly the same words as important terms
for the different text representation combinations. The relative ranking of the words is
very similar in the class profiles of all the text representations. Thus, adding phrases
to unigrams does not result in replacing the most important unigrams for a particular
class and the improvements in classification accuracy must derive from the additional
information in the selected phrases.
4.2.2 Overlap of Single Words and Parts of Bigrams. Like Caropreso, Matwin, and Sebastiani
(2001), we investigated to what extent the parts of the high-ranked phrases overlap with
words in the unigrams + bigrams class profile. We first looked at the lexical overlap of
the words and the parts of the bigrams in the H01 unigrams + bigrams class profile.
Interestingly, we found a relatively low overlap between the words and the parts of
the phrases: For the 20 most important bigrams, only 11 of the 32 unique parts of the
bigrams overlap with the 100 most important single word terms; in the complete class
profile only 56% of the 10,387 parts of the bigrams overlap with the 9,064 words in
the class profile. This means that a large part of the bigrams contains complementary
information not present in the unigrams in the class profile.
To gain a deeper insight into the relationship between the bigrams and their parts,
we also looked at the mass of the different terms in the class profiles. The mass of a
term for a certain class is the product of its TF?IDF score and its Winnow weight for
that class; ?mass? provides an estimate of how much a term contributes to the score of
documents for a particular class. We can divide the terms into three main categories:
(a) mass(partA) ? mass(partB) ? mass(bigram);
(b) mass(partA) ? mass(bigram) > mass(partB);
(c) mass(bigram) > mass(partA) ? mass(partB).
We note that 50% of the top 1,000 highest ranked bigrams fall within category (b) and
typically consist of one part with high mass accompanied by a part with a low mass,
which can be a function word (for example a transmitter), or a general term (for example,
device in optical device). The highest ranked bigrams can be found in category (a) where
two highly informative words are combined to form very specific concepts, for example,
fuel cell. These are specifications of a more general concept that is typical for that class in
the corpus. The bigrams in this category are similar to those investigated by Caropreso,
Matwin, and Sebastiani (2001) and Tan, Wang, and Lee (2002). Though highly ranked,
they only make up a small subset (22%) of the important bigram features.
The bigrams in category (c) (27%) are typically made up from low-ranked single
words, such as mobile station. Interestingly, most bigram parts in this subset do not occur
as word terms in the unigram and bigram class profiles, but occur in the negative class
profiles (a selection of terms that are considered to describe anything but that particular
class). The complementary information of bigram phrases (compared to unigrams) is
contained in this set of bigrams.
4.3 Statistical versus Linguistic Phrases
Results in Section 4.1 indicate that bigrams are most important additional features, but
the experiment combining all four representations showed that dependency triples do
768
D?hondt et al Text Representations for Patent Classification
complement bigrams. In this section we examine what information is captured by the
different phrases and how this accounts for the differences in classification accuracy.
4.3.1 Class Profile Analysis. We first examined the differences between the statistical
phrases and the two types of linguistic phrases to discover what information contained
in the bigrams leads to better classification results. We performed our analysis on the
different class profiles of B60 (?Vehicles in general?), a medium-sized class, which most
clearly shows the advantage of the bigram classifier compared to the classifiers with
linguistic phrases.27
All four class profiles with phrases contain roughly the same set of unigrams
(between 78% to 91% overlap) that occur quite high in the corresponding unigram class
profile. The AEGIR class profile contains 10% more unigrams than the other combined
representation class profiles; these are mainly words that appear in the negative class
profile of the corresponding unigram classifier. As in class H01, the relative position of
the words remains the same. The absolute position of the words in the list, however,
does change: Caropreso, Matwin, and Sebastiani (2001) introduced a measure for the
effectiveness of phrases as terms, called the penetration, that is, the percentage of
phrases in the top k terms when classifying with both words and phrases.
Comparing the penetration levels at the various ranks for the different classifiers,
we can see that the classification results correspond with the tendency of a classifier to
select phrases in the top k terms. Interestingly, we see a large disparity in the phrasal
features that are selected by the combination classifier. The preference for bigrams
is mirrored by the penetration levels of the unigrams + bigrams classifier which has
selected more bigrams at higher ranks in the class profile than the classifiers with the
linguistic phrases. This is in line with the findings of Caropreso, Matwin, and Sebastiani
(2001) that penetration levels are a reasonable way to compute the contribution of
n-grams to the quality of a feature set. On average, the linguistic phrases have much
smaller weight in the class profiles than the bigrams and, consequently, are likely
to have a smaller impact during the classification process. For the combination run,
however, it seems that a long tail of small-impact features does improve classification
accuracy.
Linguistic analysis of the top 100 phrases in the profiles of class B60 shows that
all classifiers select similar types of phrases. We manually annotated the bigrams with
the correct syntactic dependencies (in the Stanford collapsed typed dependency format)
and compared these with the syntactic relations expressed in the linguistic phrases. The
results are summarized in Table 6.
It appears that noun phrases and compounds such as circuit board and electric
device are by far the most important terms in the class profiles. Interestingly, phrases
that contain a determiner relation (e.g., the device) are deemed equally important in
all four different class profiles. It is unlikely that this is a semantic effect, that is, that
the determiner relation provides additional semantic information to the nouns in the
phrases, but rather it seems an artefact of the abundance of noun phrases which occur
in patent texts. We also looked into the lexical overlap between the parts of the different
types of phrases. We found that the selected phrases encode almost exactly the same
information in all three representations: There is an 80% overlap between the parts of
27 Precision is 77.34% for unigrams+bigrams, 75.67% for unigrams+Stanford, 73.47% for unigrams+AEGIR,
and 77.38% for unigrams+bigrams+Stanford+AEGIR. The Recall scores are essentially equal for all three,
that is, 68.81%, 68.38%, 69.7%, and 70.18%, respectively.
769
Computational Linguistics Volume 39, Number 3
Table 6
Distribution of the top 100 statistical and syntactic phrases in the B60 class profiles.
grammatical relation bigrams stanford AEGIR combination
noun?noun compounds 41 48
6228 4428adjectival modifier 11 8
determiner 34 28 27 41
subject 6 4 6 9
prepositions 2 4 1 2
<other> 7 8 4 4
the top 100 most important phrases. This decreases only to 75% when looking at the
10,000 most important phrases.
Given that the class profiles select the same set of words and contain phrases with
a high lexical overlap, therefore, how do we explain the marked differences in classifi-
cation accuracy between the three different representations? These must stem from the
different combinations of the words in the phrasal features. To examine in detail how the
features created through the different text represenations differ, we conducted a feature
quality assessment experiment against a manually created reference set.
4.3.2 Human Quality Assessment Experiment. To gain more insight in the syntactic and
semantic relations that are considered most informative by humans, we conducted
an experiment in which we asked human annotators to select the five to ten most
informative phrases29 for 15 sentences taken at random from documents in the three
largest classes in the corpus. We then compiled a reference set consisting of 70 phrases
(4.6 phrases per sentence) which were considered as ?informative? by at least three
out of four annotators. Of these, 57 phrases were noun?noun compounds and 11 were
combinations of an adjectival modifier with a noun. None of the annotators selected
phrases containing determiners.
We created bigrams from the input and extracted head?modifier pairs30 from the
parser output for the sentences in the test set. We then compared the overlap of the
generated phrases with the reference phrases. We found that bigrams overlap with
53 of the 70 reference phrases; Stanford triples overlap with 62 phrases and AEGIR
triples overlap with 57 phrases. Although three data points are not enough to compute
a formal measure, it is interesting to note the correspondence with the number of terms
kept for the three text representations after Local Term Selection (see Table 3). The fact
that the text representation with the smallest number of terms after LTC and with the
smallest overlap with ?contentful? phrases in a text as indicated by human annotators
still yields the best classification performance suggests that not all ?contentful? phrases
are important or useful for the task of classifying that text. This finding is reminiscent of
the fact that the ?optimal? summary of a text is dependent on the goal with which the
summary was produced (Nenkova and McKeown 2011).
Only 15% of the phrases extracted by the human annotators contain word combi-
nations that have long-distance dependencies in the original sentences. This suggests
28 As mentioned in Section 3.2.3 the AEGIR parser uses a more condensed dependency output format.
The Stanford?s nn and amod are collapsed into the attributive (ATTR) relation.
29 ?Phrase? was defined as a combination of two words that both occur in the sentence, irrespective of
the order in which they occur in the sentence.
30 Head?modifier pairs are syntactic triples that are stripped of their grammatical relations.
770
D?hondt et al Text Representations for Patent Classification
that the most meaningful phrases are expressed in local dependencies, that is, adjacent
words. Consequently, syntactic analysis aimed at discovering meaning expressed by
long-distance dependencies can only make a small contribution. A further analysis of
the phrases showed that the smaller coverage of the bigrams is due to the fact that some
of the relevant noun?noun combinations are missed because function words, typically
determiners or prepositions, occur between the nouns. For example, the annotators
constructed the reference phrase rotation axis for the noun phrase the rotation of the
second axis. This reference phrase cannot be captured by the bigram representation.
When intervening function words are removed from the sentences, the coverage of
the resulting bigrams on the reference set rises31 to 59 phrases (more than AEGIR, and
almost as many as Stanford). Despite the fact that generating more phrases does not
necessarily lead to better classification performance, we intend to use bigrams stripped
of function words as additional terms for patent classification in future experiments.
The analysis also revealed an indication why syntactic phrases may lead to inferior
classification results: Both syntactic parsers consistently fail to find the correct structural
analysis of the long and complex noun phrases such as an implantable, inflatable dual
chamber shape retention tissue expander, which are frequent in patent texts. Phrases like
this contain many compounds in an otherwise complex syntactic structure, namely
[an [implantable, inflatable [[dual chamber] [shape retention] [tissue expander]]]].
For a parser it is impossible to parse this correctly without knowing which word se-
quences are actually compounds. That knowledge might be gleaned from the frequency
with which sequences of nouns and adjectives occur in a given domain. For the time
being, the Stanford parser (and the AEGIR parser, to a lesser extent) will parse any
noun phrase by attaching the individual words to the right-most head noun, resulting
in the following analysis:
[an [implantable, [inflatable [dual [chamber [shape [retention [tissue expander]]]]]]]].
This effectively destroys many of the noun?noun compounds, which are the most
important features for patent classification (see Table 6). Bigrams are less prone to this
type of ?error.?
These findings are confirmed when looking at the overlap of the word combi-
nations: Although there is high lexical overlap between the phrases of the different
representations (80% overlap of the parts of phrases in Section 4.3.1), the overlap of
the word combinations that make up the phrases is much lower: Only 33% of the top
1,000 phrases are common between all three representations.
4.4 Stanford versus AEGIR Triples
The performance with the unigrams + Stanford triples is not significantly different from
the combination with AEGIR triples. Because the AEGIR triples are slightly less sparse
(see Table 1), we expected that these would have an advantage over Stanford triples.
Most of the normalization processes that make the AEGIR triples less sparse concern
syntactic variation on the clause level, however. But as was shown in Section 4.3,
31 This result is language-dependent: English has a fairly rigid phrase-internal word order but for a more
synthetic language with a more variable word order, like Russian, bigram coverage might suffer from
the variation in the surface form.
771
Computational Linguistics Volume 39, Number 3
Table 7
Classification results on CLEF-IP 2010 French and German abstracts, with ranges for
95% confidence intervals.
P R F1
French unigrams 70.65% ? 0.68 61.40% ? 0.73 65.70% ? 0.70unigrams + bigrams 72.31% ? 0.67 62.58% ? 0.72 67.09% ? 0.69
German unigrams 76.44% ? 0.34 65.82% ? 0.38 70.73% ? 0.37unigrams + bigrams 76.39% ? 0.34 65.41% ? 0.38 70.47% ? 0.37
the most important terms for classification in the patent domain are found in the
noun phrase, where Stanford and AEGIR perform similar syntactic analyses. Although
Stanford?s dependency scheme is more detailed (see Table 6), the noun-phrase internal
dependencies in the Stanford parser map practically one-to-one onto AEGIR?s set of
relators, resulting in very similar dependency triple features for classification. Con-
sequently, there is no normalization gain in using the AEGIR dependency format to
describe the internal structure of the noun phrases.
4.5 Comparison with French and German Patent Classification
We found that phrases contribute to improving classification on English patent ab-
stracts. The improvement might be language-dependent, however, because compounds
are treated differently in different languages. A compounding language like German
might benefit less from using phrases than English. To estimate the generalizability of
our findings, we conducted additional experiments in which we compared the impact
of adding bigrams to unigrams for both French and German.
Using the same methods described in sections 3.1 and 3.2, we extracted and pro-
cessed all French and German abstracts from the CLEF-IP 2010 corpus, resulting in two
new data sets that contained 86,464 and 294,482 documents, respectively (Table 7). Both
data sets contained the same set of 121 labels and had label distributions similar to the
English data set. The sentencing script was updated with the most common French and
German abbreviations to minimize incorrect sentence splitting. The resulting sentences
were then tagged using the French and German versions of the TreeTagger.32 From the
tagged output, we extracted the lemmas and used these to construct unigrams and
bigrams for both languages. We ran the experiments with the LCS using the settings
reported in Section 3.3.
The results show a much smaller but still significant improvement for using bigrams
when classifying French patent abstracts and even a deterioration for German. Due to
the difference in size between the English and French data set it is difficult to draw hard
conclusions on which language benefits most from adding bigrams. It is clear, however,
that our findings are not generalizable to German (and probably other compounding
languages).
5. Conclusion
In this article we have examined the usefulness of statistical and linguistic phrases
for patent classification. Similar to O?zgu?r and Gu?ngo?r?s (2010) results for scientific
32 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/.
772
D?hondt et al Text Representations for Patent Classification
abstracts, we found that adding phrases to unigrams significantly improves classifi-
cation results for English. Of the three types of phrases examined in this article, bigrams
have the most impact, both in the experiment that combined all four text representa-
tions, and in combination with unigrams only.
The abundance of compounds in the terminology-rich language of the patent do-
main results in a relatively high importance for the phrases. The top phrases across
the different representations were mostly noun?noun compounds (for example watering
device), followed by phrases containing a determiner relation (for example the module)
and adjective modifier phrases (for example separate module).
The information in the phrases and unigrams overlaps to a large extent: Most of the
phrases consist of words that are important unigram features in the combined profile
and that also appear in the corresponding unigram class profile. When examining the
H01 class profiles, however, we found that 27% of the selected phrases contain words
that were not selected in the unigram profile (see Section 4.2.2).
When comparing the impact of features created from the output of the aboutness-
based AEGIR parser with those from the Stanford parser, we found the latter resulted in
slightly (but not significantly) better classification results. AEGIR?s normalization fea-
tures are not advantageous (compared with Stanford) in creating noun-phrase internal
triples, which are the most informative features for patent classification.
The parsers were not specifically trained for the patent domain and both experi-
enced problems with long, complex noun phrases consisting of sequences of words
that can function as adjective/adverb or noun and that are not interrupted by function
words that clarify the syntactic structure. The right-headed bias of both syntactic parsers
caused problems in analyzing those constructions, yielding erroneous and variable
data. As a consequence, parsers may miss potentially relevant noun?noun compounds
and noun phrases with adjectival modifiers. Because of the highly idiosyncratic nature
of the terminology used in the patent domain, it is not evident whether this prob-
lem can be solved by giving a parser access to information about the frequency with
which specific noun?noun, adjective?noun, and adjective/adverb?adjective pairs occur
in technical texts. Bigrams, on the other hand, are less variable (as seen in Table 1) and
therefore yield better classification results. This is the more important point because the
dependency relations marked as important for understanding a sentence by the human
annotators consist mainly of pairs of adjacent words.
We also performed additional experiments to examine the generalizability of our
findings for French and German: As could be expected, compounding languages like
German which express complex concepts in ?one word? do not gain from using
bigrams.
In line with Bekkerman and Allan (2003) we can conclude that with the large quan-
tities of text available today, the role of phrases as features in text classification must
be reconsidered. For the automated classification of English patents at least, adding
phrases and more specifically bigrams significantly improves classification accuracy.
References
Apte?, Chidanand, Fred Damerau, and
Sholom Weiss. 1994. Automated learning
of decision rules for text categorization.
ACM Transactions on Information Systems,
12(3):233?251.
Bekkerman, Ron and John Allan. 2003. Using
bigrams in text categorization. Technical
Report IR-408, Center of Intelligent
Information Retrieval, University of
Massachusetts, Amherst.
Beney, Jean. 2010. LCI-INSA linguistic
experiment for CLEF-IP classification
track. In Proceedings of the Conference on
Multilingual and Multimodal Information
Access Evaluation (CLEF 2010), Padua.
773
Computational Linguistics Volume 39, Number 3
Benzineb, Karim and Jacques Guyot. 2011.
Automated patent classification. In
Mihai Lupu, Katja Mayer, John Tait, and
Anthony J. Trippe, editors, Current
Challenges in Patent Information Retrieval,
volume 29. Springer, New York,
pages 239?261.
Braga, Igor, Maria Monard, and Edson
Matsubara. 2009. Combining unigrams
and bigrams in semi-supervised text
classification. In Proceedings of Progress
in Artificial Intelligence, 14th Portuguese
Conference on Artificial Intelligence
(EPIA 2009), pages 489?500, Aveiro.
Caropreso, Maria Fernanda, Stan Matwin,
and Fabrizio Sebastiani. 2001. A
learner-independent evaluation of the
usefulness of statistical phrases for
automated text categorization. In
A. G. Chin, editor, Text Databases &
Document Management. IGI Publishing,
Hershey, PA, pages 78?102.
Crawford, Elisabeth, Irena Koprinska,
and Jon Patrick. 2004. Phrases and
feature selection in e-mail classification.
In Proceedings of the 9th Australasian
Document Computing Symposium (ADCS),
pages 59?62, Melbourne.
Dagan, Ido, Yael Karov, and Dan Roth.
1997. Mistake-driven learning in text
categorization. In Proceedings of 2nd
Conference on Empirical Methods in NLP,
pages 55?63, Providence, RI.
de Marneffe, Marie-Catherine and
Christopher Manning. 2008. The Stanford
Typed Dependencies representation. In
Coling 2008: Proceedings of the Workshop on
Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8, Manchester.
Derieux, Franck, Mihaela Bobeica, Delphine
Pois, and Jean-Pierre Raysz. 2010.
Combining semantics and statistics for
patent classification. In Proceedings of the
Conference on Multilingual and Multimodal
Information Access Evaluation (CLEF 2010),
Padua.
Dumais, Susan, John Platt, David
Heckerman, and Mehran Sahami. 1998.
Inductive learning algorithms and
representations for text categorization.
In Proceedings of the Seventh International
Conference on Information and Knowledge
Management (CIKM ?98), pages 148?155,
Bethesda.
Fall, Caspar J. and Karim Benzineb. 2002.
Literature survey: Issues to be considered
in the automatic classification of patents.
Technical report, World Intellectual
Property Organization, Geneva.
Fall, Caspar J., Atilla To?rcsva?ri, Karim
Benzineb, and Gabor Karetka. 2003.
Automated categorization in the
international patent classification.
ACM SIGIR Forum, 37(1):10?25.
Fu?rnkranz, Johannes. 1998. A study using
n-gram features for text categorization.
Technical Report OEFAI-TR-98-30,
Austrian Research Institute for
Artificial Intelligence, Vienna.
Fu?rnkranz, Johannes. 1999. Exploiting
structural information for text
classification on the WWW. In Proceedings
of Advances in Intelligent Data Analysis
(IDA-99), pages 487?497, Amsterdam.
Galavotti, Luigi, Fabrizio Sebastiani, and
Maria Simi. 2000. Experiments on the
use of feature selection and negative
evidence in automated text categorization.
In Proceedings of Research and Advanced
Technology for Digital Libraries,
4th European Conference, pages 59?68,
Lisbon.
Guyot, Jacques, Karim Benzineb, and Gilles
Falquet. 2010. Myclass: A mature tool for
patent classification. In Proceedings of the
Conference on Multilingual and Multimodal
Information Access Evaluation (CLEF 2010),
Padua.
Held, Pierre, Irene Schellner, and Ryuichi
Ota. 2011. Understanding the world?s
major patent classification schemes. Paper
presented at the PIUG 2011 Annual
Conference Workshop, Vienna, 13 April.
Joachims, Thorsten. 1999. Making large-scale
support vector machine learning practical.
In Bernhard Scho?lkopf, Christopher J. C.
Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods. MIT Press,
Cambridge, MA, pages 169?184.
Koster, Cornelis, Jean Beney, Suzan Verberne,
and Merijn Vogel. 2011. Phrase-based
document categorization. In Mihai Lupu,
Katja Mayer, John Tait, and Anthony J.
Trippe, editors, Current Challenges in Patent
Information Retrieval, volume 29. Springer,
New York, pages 263?286.
Koster, Cornelis, Marc Seutter, and
Jean Beney. 2001. Classifying patent
applications with winnow. In Proceedings
Benelearn 2001. pages 19?26, Antwerpen.
Koster, Cornelis, Marc Seutter, and
Jean Beney. 2003. Multi-classification
of patent applications with winnow.
In Manfred Broy and Alexandre V.
Zamulin, editors, Perspectives of
Systems Informatics: 5th International
Andrei Ershov Memorial Conference,
volume 2890 of Lecture Notes in
774
D?hondt et al Text Representations for Patent Classification
Computer Science. Springer, New York,
pages 546?555.
Koster, Cornelis and Mark Seutter. 2003.
Taming wild phrases. In Proceedings
of the 25th European conference on
IR research (ECIR?03), pages 161?176, Pisa.
Krier, Marc and Francesco Zacca`. 2002.
Automatic categorization applications at
the European patent office. World Patent
Information, 24(3):187?196.
Larkey, Leah. 1998. Some issues in the
automatic classification of U.S. patents.
In Working Notes of the Workshop on
Learning for Text Categorization, 15th
National Conference on AI, pages 87?90,
Madison, WI.
Larkey, Leah S. 1999. A patent search and
classification system. In Proceedings of the
Fourth ACM Conference on Digital Libraries
(DL?99), pages 179?187, Berkeley.
Lewis, David D. 1992. An evaluation of
phrasal and clustered representations on a
text categorization task. In Proceedings of
the 15th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR ?92),
pages 37?50, Copenhagen.
Mille, Simon and Leo Wanner. 2008. Making
text resources accessible to the reader: The
case of patent claims. In Proceedings of the
6th International Conference on Language
Resources and Evaluation (LREC?08),
Marrakech.
Mitra, Mandar, Chris Buckley, Amit Singhal,
and Claire Cardie. 1997. An analysis
of statistical and syntactic phrases.
In Proceedings of RIAO?97 Computer-Assisted
Information Searching on Internet,
pages 200?214, Montreal.
Mladenic, Dunja and Marko Grobelnik.
1998. Word Sequences as Features in
Text-Learning. In Proceedings of the 17th
Electrotechnical and Computer Science
Conference (ERK98), pages 145?148,
Ljubljana.
Moschitti, Alessandro and Roberto Basili.
2004. Complex linguistic features for
text classification: A comprehensive
study. In Sharon McDonald and
John Tait, editors, Advances in Information
Retrieval, volume 2997 of Lecture Notes in
Computer Science. Springer, New York,
pages 181?196.
Nastase, Vivi, Jelber Sayyad, and
Maria Fernanda Caropreso. 2007.
Using dependency relations for text
classification. Technical Report TR-2007-12,
University of Ottawa.
Nenkova, Ani and Kathleen McKeown. 2011.
Automatic summarization. Foundations
and Trends in Information Retrieval,
5(2?3):103?233.
Ozgu?r, Levent and Tunga Gu?ngo?r.
2009. Analysis of stemming alternatives
and dependency pattern support in text
classification. In Proceedings of Tenth
International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing 2009),
pages 195?206, Mexico City.
O?zgu?r, Levent and Tunga Gu?ngo?r.
2010. Text classification with the support of
pruned dependency patterns. Pattern
Recognition Letters, 31(12):1598?1607.
O?zgu?r, Levent and Tunga Gu?ngo?r.
2012. Optimization of dependency and
pruning usage in text classification.
Pattern Analysis and Applications,
15(1):45?58.
Parapatics, Peter and Michael Dittenbach.
2009. Patent claim decomposition for
improved information extraction. In
Proceedings of the 2nd International
Workshop on Patent Information Retrieval
(PAIR?09), pages 33?36, Hong Kong.
Salton, Gerard and Christopher Buckley.
1988. Term-weighting approaches in
automatic text retrieval. Information
Processing Management, 24(5):513?523.
Scott, Sam and Stan Matwin. 1999. Feature
engineering for text classification.
In Proceedings of the Sixteenth International
Conference on Machine Learning (ICML ?99),
pages 379?388, Bled.
Smith, Harold. 2002. Automation of patent
classification. World Patent Information,
24(4):269?271.
Tan, Chade-Meng, Yuan-Fang Wang,
and Chan-Do Lee. 2002. The use of
bigrams to enhance text categorization.
Information Processing and Management,
38(4):529?546.
Verberne, Suzan and Eva D?hondt. 2011.
Patent classification experiments with
the Linguistic Classification System
LCS in CLEF-IP 2011. In Proceedings of the
Conference on Multilingual and Multimodal
Information Access Evaluation (CLEF 2011),
Amsterdam.
Verberne, Suzan, Merijn Vogel, and
Eva D?hondt. 2010. Patent classification
experiments with the Linguistic
Classification System LCS. In Proceedings
of the Conference on Multilingual and
Multimodal Information Access Evaluation
(CLEF 2010), Padua.
775
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 26?33
Manchester, UK. August 2008
Passage Retrieval for Question Answering using Sliding Windows
Mahboob Alam Khalid
ISLA, University of Amsterdam
mahboob@science.uva.nl
Suzan Verberne
Radboud University Nijmegen
s.verberne@let.ru.nl
Abstract
The information retrieval (IR) commu-
nity has investigated many different tech-
niques to retrieve passages from large col-
lections of documents for question answer-
ing (QA). In this paper, we specifically ex-
amine and quantitatively compare the im-
pact of passage retrieval for QA using slid-
ing windows and disjoint windows. We
consider two different data sets, the TREC
2002?2003 QA data set, and 93 why-
questions against INEX Wikipedia. We
discovered that, compared to disjoint win-
dows, using sliding windows results in im-
proved performance of TREC-QA in terms
of TDRR, and in improved performance of
why-QA in terms of success@n and MRR.
1 Introduction
In question answering (QA), text passages are an
important intermediary between full documents
and exact answers. They form a very natural unit
of response for QA systems (Tellex et al, 2003)
and it is known from user studies that users pre-
fer answers to be embedded in paragraph-sized
chunks (Lin et al, 2003) because they can provide
the context of an answer. Therefore, almost all
state-of-the-art QA systems implement some tech-
nique for extracting paragraph-sized fragments of
text from a large corpus.
Most QA systems have a pipeline architecture
consisting of at least three components: ques-
tion analysis, document/passage retrieval, and an-
swer extraction (Hirschman and Gaizauskas, 2001;
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Voorhees, 2001). The quality of a QA sys-
tem heavily depends on the effectiveness of the
integrated retrieval system (second step of the
pipeline): if a retrieval system fails to find any rel-
evant documents for a question, further processing
steps to extract an answer will inevitably fail too
(Monz, 2003). This motivates the need to study
passage retrieval for QA.
There are two common approaches to retriev-
ing passages from a corpus: one is to index each
passage as separate document and retrieve them as
such. The other option is to first retrieve relevant
documents for a given question and then retrieve
passages from the retrieved documents. The pas-
sages themselves can vary in size and degree of
overlap. Their size can be fixed as a number of
words or characters, or varying with the semantic
content (Hearst and Plaunt, 1993) or the structure
of the text (Callan, 1994). The overlap between
two adjacent passages can be either zero, in which
case we speak of disjoint passages, or the passages
may be overlapping, which we refer to as sliding
passages.
In this paper, we compare the effectiveness of
several passage retrieval techniques with respect to
their usefulness for QA. Our main interest is the
contribution of sliding passages as apposed to dis-
joint passages, and we will experiment with a num-
ber of retrieval models. We evaluate the retrieval
approaches on two different QA tasks: (1) factoid-
QA, as defined by the test collection provided by
TREC (Voorhees, 2002; Voorhees, 2003), and (2)
a relatively new problem in the QA field: that of
answering why-questions (why-QA).
The remainder of the paper is organized as fol-
lows. In the next section, we describe related work
on passage retrieval for QA and we motivate what
the main contribution of the current paper is. In
26
section 3 we describe our general set-up for pas-
sage retrieval in both QA tasks that we consider. In
section 4, we present the results of the experiments
on TREC-QA data, and in section 5 we present our
results on why-QA. Section 6 gives an overall con-
clusion.
2 Related work
The use of passage retrieval for QA has been stud-
ied before. For example, (Tellex et al, 2003)
performed a quantitative evaluation of passage re-
trieval algorithms for QA. They compared differ-
ent passage retrieval algorithms in the context of
their QA system. Their system first returns a
ranked list of 200 documents and then applies dif-
ferent passage retrieval algorithms to the retrieved
documents. They find that the performance of pas-
sage retrieval depends on the performance of the
pre-applied document retrieval step, and therefore
they suggest that document and passage retrieval
technology should be developed independently.
A similar message is conveyed by (Roberts and
Gaizauskas, 2004). They investigate different ap-
proaches to passage retrieval for QA. They iden-
tify each paragraph as a seperate passage. They
find that the optimal approach is to allow multiple
passages per document to be returned and to score
passages independently of their source document.
(Tiedemann, 2007) studies the impact of doc-
ument segmentation approaches on the retrieval
performance of IR for Dutch QA. He finds that
segmentation based on document structure such
as the use of paragraph markup (discourse-based
segmentation) works well with standard informa-
tion retrieval techniques. He tests various other
techniques for document segmentation and various
passage sizes. In his experimental setting, larger
text units (such as documents) produce better per-
formance in passage retrieval. Tiedemann com-
pares different sizes of discourse-based segmen-
tation: sentences, paragraphs and documents. He
finds that larger text units result in a large search
space for subsequent QA modules and hence re-
duce the overall performance of the QA system.
That is why we do not conduct experiments with
different passage sizes in this paper: it is difficult
to measure the outcome of such experiments in-
dependently of the specific answer extraction sys-
tem. We adopt Tiedemann?s best strategy of docu-
ment segmentation strategy, i.e., paragraph-based,
but with equally sized passages instead.
3 General experiment set-up
The main purpose of our experiments is to study
the contribution of sliding windows as apposed to
disjoint windows in the context of QA. Therefore,
in our experiment setup, we have kept fixed the
other segmentation variables, passage size and de-
gree of overlap. We set out to examine two differ-
ent strategies of document segmentation (disjoint
and sliding passages) with a number of retrieval
models for two different QA tasks: TREC factoid-
QA and why-QA.
3.1 Retrieval models
We use the Lemur retrieval engine1 for passage re-
trieval because it provides a flexible support for
different types of retrieval models including vec-
tor space models and language models. In this
paper we have selected two vector space mod-
els: TFIDF and Okapi BM25 (Robertson and
Walker, 1999), and one language model based on
Kullback-Leibler (KL) divergence (Lafferty and
Zhai, 2001).
The TFIDF weighting scheme is often used in
information retrieval. There are several variations
of the TFIDF weighting scheme that can effect the
performance significantly. The Lemur toolkit pro-
vides a variant of the TFIDF model based on the
Okapi TF formula (Robertson et al, 1995).
Lemur also provides the implementation of the
original Okapi BM25 model, and we have used
this model with default values of 1.2 for k1, 0.75
for b and 7 for k3 as suggested by (Robertson
and Walker, 1999). The KL-divergence retrieval
model, which implements the cross entropy of the
query model with respect to the document model,
is a standard metric for comparing distributions,
which has proven to work well in IR experiments
in the past. To address the data sparseness prob-
lem during model estimation, we use the Dirichlet
smoothing method (Zhai and Lafferty, 2004) with
default parameter values provided in the Lemur
toolkit.
Currently, however, the Lemur 2 does not sup-
port direct passage retrieval. For these experi-
ments, therefore, we first need to segment docu-
ments into passages before indexing them into the
1Lemur toolkit: http://www.lemurproject.org
2Lemur and Indri are different search engines. Indri pro-
vides the #passage operator, but it doesn?t consider para-
graph boundaries or sentence boundaries for constructing pas-
sages.
27
Lemur retrieval engine. Our segmenting strategy
is explained below.
3.2 Passage identification
For our experiments, we take into account two
different corpora: AQUAINT and the Wikipedia
XML corpus as used in INEX (Denoyer and Gal-
linari, 2006). The AQUAINT corpus consists of
news articles from the Associated Press, New York
Times, and Xinhua News Agency (English ver-
sion) from 1996 to 2000. The Wikipedia XML
collection consists of 659,388 articles as they oc-
cured in the online Wikipedia in the summer of
2006. As we have discussed in Section 2, (Tiede-
mann, 2007) discovered that discourse-based seg-
mentation into paragraphs works well with stan-
dard information retrieval techniques. They also
observe that larger retrieval units produce better re-
sults for passage retrieval, since larger units have
higher chance to cover the required information.
Therefore, we decide to segment each document
into similar sized passages while taking into ac-
count complete paragraphs only.
For document segmentation, our method first
detects sentences in the text using punctuation
marks as separators, and then paragraphs using
empty lines as separators. Sentence boundaries
are necessary because we aim at retrieving pas-
sages that do not contain any broken sentences.
The required passages are identified by aligning
over paragraph boundaries (merging paragraphs
into units until they have the required length ,i.e.
500 characters). The disjoint passages do not share
any content with each other, and the sliding pas-
sages slide with the difference of one paragraph
boundary, i.e., we start forming a new passage
from beginning of each paragraph of the docu-
ment. If paragraph boundaries are not detected,
then these sliding passages are half-overlapped
with each other.
For the Wikipedia XML corpus, we have found
that documents have already been annotated with
<p> elements. Thus we consider these elemens
as paragraph boundaries instead of empty lines as
we did for the AQAINT corpus. We observe that
some textual parts of the documents are not cov-
ered by the XML paragraph boundaries. Therefore
we have extended the existing paragraph bound-
aries such that the missing text fragments become
part of the paragraphs.
We split both corpora into disjoint and slid-
ing windows as we have discussed above. After
splitting the 1.03M documents of the AQUAINT-
1 collection we have 14.2M sliding passages, and
4.82M disjoint passages. And similarly we got
4.1M sliding passages and 2M disjoint passages
from the Wikipedia XML collection of 659,388
documents.
3.3 Evaluation metrics
For our experiments, we use the following metrics
for evaluation:
Mean reciprocal rank (MRR) at n is the mean
(calculated over all questions) of the recipro-
cal rank (which is 1 divided by the rank or-
dinal) of the highest ranked relevant (i.e. an-
swer bearing) passage. RR is zero for a ques-
tion if no relevant passage is returned by the
system at limit n.
Success at n for a question is 1 if the answer
to this question is found in top n passages
fetched up by our system. Success@n is av-
eraged over all questions.
Total document reciprocal rank (TDRR)
(Bilotti et al, 2004) is the sum of all recipro-
cal ranks of all answer bearing passages per
question (averaged over all questions). The
value of TDRR is maximum if all retrieved
passages are relevant. TDRR is an extension
of MRR that favors a system that ranks more
that one relevant passage higher than all
non-relevant passages. This way, TDRR
extends MRR with a notion of recall.
When we compare retrieval performance of two
retrieval settings (such as the use of disjoint versus
sliding windows), then we obtain a list of paired
scores. That?s why we use the Wilcoxon signed-
rank test to show the statistical significance of the
improvements.
In summary, we experiment with three retrieval
models in Lemur: TFIDF, Okapi, and a language
model based on the Kullback-Leibler divergence.
For each of these retrieval models, we evaluate the
use of both sliding and disjoint passages. This
makes a total of six retrieval settings.
4 Evaluating passage retrieval for
TREC-QA
As test collection for factoid QA, we use a standard
set of 822 question/answer pairs from the TREC
28
QA tasks of 2002-2003. For evaluation of the
passage retrieval approaches that we consider, we
compute strict scores as defined by (Tellex et al,
2003). Strict scoring means that a retrieved pas-
sage is considered relevant if the passage not only
matches one of the answer patterns provided by
NIST, but its associated document is also listed as
one of the relevant documents assessed by NIST.
(Bilotti et al, 2004) have reviewed 109 factoid
questions of the TREC-2002 task and they have
extended the existing set of relevant documents by
adding more relevant documents. We have also in-
cluded this extended list of relevant documents for
these questions in our experiment setup.
We evaluate the impact of disjoint and sliding
windows on passage retrieval for QA using three
different retrieval models, using the MRR@n, Suc-
cess@n and TDRR@n metrics as described in sec-
tion 3.3. Table 1 shows the evaluation results (best
scores for each measure in bold face). The ex-
periment results show that language model based
on Kullback-Leibler divergence shows better per-
formance than two vector space models for both
types of windows retrieval according to MRR, suc-
cess@n and TDRR evaluation metrics.
4.1 Discussion
In a pipeline QA system, the answer extraction
module depends on the performance of passage re-
trieval. If more answer bearing passages are pro-
vided in the stream, then there is a high chance
of selecting the correct answer from the stream
in later stages of QA. (Roberts and Gaizauskas,
2004) have also discussed the importance of this
aspect of passage retrieval for QA. They have mea-
sured the answer redundancy of a retrieval system
which measures how many answer bearing pas-
sages are returned per question at limit n. (Tiede-
mann, 2007) have also used this metric and argue
that high redundancy is desired to make it easier
for the answer extraction module to spot possible
answers. We consider TDRR as the most impor-
tant measure for the passage retrieval task since
it does not only measure the redundancy of a re-
trieval system but also measures how much im-
provement there is in returning the relevant pas-
sages at top ranks.
According to TDRR@n in table 1, retrieval of
sliding windows outperforms retrieval of disjoint
windows at all limits of n for all retrieval mod-
els. For n = 100, the improvement is significant
at p = 0.01 level. This high value of TDRR@n
suggests that segmenting the documents into slid-
ing windows is a better choice in order to return as
many relevant passages as possible at top ranks.
If we consider Success@n as evaluation mea-
sure instead of TDRR, retrieval of disjoint win-
dows outperforms retrieval of sliding windows.
We think that one of the reasons for this behaviour
is that since sliding windows overlap with their
neighbours, they are more pair-wise similar than
disjoint windows. Therefore, it is possible that for
some non-answered questions many irrelevant pas-
sages are returned at top ranks and that relevant
passages are surpressed down.
5 Evaluating passage retrieval for
why-QA
In the previous section, we showed that for TREC
data, the choice of the retrieval model and the type
of windows to be retrieved influence on the re-
trieval performance. We found that for the TREC
data, a language modeling approach (based on
Kullback-Leibler divergence) on sliding windows
gives the best results in terms of TDRR. In this
section, we aim to find out what the optimal pas-
sage retrieval approach is for a very different type
of QA, namely why-QA.
5.1 Background of why-QA system
development
In (Verberne et al, 2008), we present an approach
for why-QA that is based on paragraph retrieval
from the INEX Wikipedia corpus (Denoyer and
Gallinari, 2006). Our system for why-QA con-
sists of two modules: a passage retrieval mod-
ule and a re-ranking module. In earlier retrieval
experiments, we used the Wumpus retrieval sys-
tem (Buttcher, 2007), and we defined passages
simply by the XML paragraph markup <p>. Pas-
sage ranking in Wumpus is done by the QAP pas-
sage scoring algorithm (Buttcher et al, 2004).
The second module of our why-system is a re-
ranking step that uses syntactic features of the
question and the retrieved answers for adapting the
scores of the answers and changing the ranking or-
der. The weights of the re-ranking features have
been optimized by training on our question answer
data in five folds3 using a genetic algorithm. We
let Wumpus retrieve and rank 150 paragraphs per
3In five turns, we tune the feature weights on four of the
five folds and evaluate them on the fifth
29
Table 1: Results for passage retrieval for TREC-QA using disjoint windows (DW) and sliding windows
(SW). ?? indicates a significant improvements of sliding windows over disjoint windows at the p = 0.01
level.
MRR Success@n TDRR
n retrieval model DW SW DW SW DW SW
10 TFIDF 0.327 0.326 51.8% 50.1% 0.465 0.637
Okapi 0.322 0.328 51.9% 51.2% 0.459 0.649
KL 0.355 0.345 55.7% 51.3% 0.518 0.710
100 TFIDF 0.336 0.386 54.1% 53.3% 0.517 0.819??
Okapi 0.333 0.339 77.0% 76.2% 0.535 0.835??
KL 0.363 0.353 77.1% 75.2% 0.525 0.902??
question. This number of 150 answers was chosen
as a trade-off between covering as many as possi-
ble of the relevant answers retrieved by Wumpus,
and the system load that was needed for automatic
syntactic analysis of all answers in the second (re-
ranking) module of the system. For evaluation of
the results, we performed manual assessment of
all answers retrieved, starting at the highest-ranked
answer and ending as soon as we encountered a
relevant answer4.
The results for our original why-system are in
Table 2. We show the results in terms of suc-
cess@n and MRR@n. As opposed to the evalua-
tion of TREC-QA, we do not consider TDRR as
evaluation measure for experiments on why-QA.
This is because in why-QA, we are only interested
in the top-ranked answer-bearing passage. For cal-
culating TDRR, assessment of all 150 retrieved an-
swers would be necessary.
Table 2 shows that success@150 for the retrieval
module (Wumpus/QAP) is 73.1%. This means that
for 26.9% of the questions, no relevant answer is
retrieved in the first module. Re-ranking the an-
swers cannot increase MRR for these questions,
since none of the 150 answers in the result list
is relevant. We consider a success@150 score of
73.1% to be quite low. We aim to improve the
performance of our system by optimizing its first
module, passage retrieval.
We experiment with a number of passage re-
trieval approaches in order to reach better retrieval
in the first module of our system. We aim to find
out which type of retrieval model and what win-
dow type (disjoint or sliding) gives optimal results
for retrieving passages relevant to why-questions.
If the retrieval performance indeed goes up, we
4We didn?t need to assess the tail since we were only in-
terested in the highest-ranked relevant answer for calculating
MRR and success@n
will apply our re-ranking module to the newly
retrieved data to see what overall system perfor-
mance we can reach with the new retrieval ap-
proach.
5.2 Data and evaluation setup
For development and testing purposes, we use the
Webclopedia question set by (Hovy et al, 2002).
This set contains questions that were asked to the
online QA system answers.com. 805 of these
questions are why-questions. We manually in-
spect a sample of 400 of the Webclopedia why-
questions. Of these, 93 have an answer in the
Wikipedia XML corpus (see section 3). Manual
extraction of one correct answer for each of these
questions results in a set of 93 why-questions and
their reference answer.
In order to be able to do fast evaluation of the
different evaluation settings, we manually create
an answer pattern for each of the questions in our
set. These answer patterns are based on a set of 93
reference answers (one answer per question) that
we have manually extracted from the Wikipedia
corpus. An answer pattern is a regular expres-
sion that defines which of the retrieved passages
are considered a relevant answer to the input ques-
tion.
As opposed to the answer patterns provided by
NIST for the evaluation of factoid QA (see sec-
tion 4), our answer patterns for why-questions are
relatively strict. A why-answer can be formulated
in many different ways with different words, which
may not all be in the answer pattern. For a factoid
question such as ?When was John Lennon born??,
the answer is only one phrase, and the answer
pattern is short and unambiguous, i.e. /1940/.
However, if we consider the why-question ?Why
are some organ transplants unsuccessful??, the
answer pattern cannot be stated in one phrase. For
30
Table 2: Results for the original why-QA pipeline system
success@10 success@150 MRR@150
Wumpus/QAP Retrieval 43.0% 73.1% 0.260
+ Re-ranking module 54.8% 73.1% 0.380
this example, we created the following answer
pattern based on the pre-extracted reference
answer5: /.*immune system.*foreign
tissues.*destroy.*/. It is however pos-
sible that a relevant answer is formulated in a
way that does not match this regular expression.
Thus, the use of answer patterns for the evaluation
of why-QA leads to conservative results: some
relevant answers may be missed in the evaluation
procedure.
After applying the answer patterns, we count the
questions that have at least one relevant answer
in the top 10 and the top 150 of the results (suc-
cess@10, success@150). For the highest ranked
relevant answer per question, we determine the re-
ciprocal rank (RR). If there is no correct answer
retrieved by the system at n = 150, the RR is 0.
Over all questions, we calculate the MRR@150.
5.3 Passage retrieval results
We segment and index the Wikipedia corpus as de-
scribed in section 3 and run all six retrieval set-
tings on our set of 93 why-questions. For consis-
tent evaluation, we applied the answer patterns that
we created to the newly retrieved Lemur data as
well as to the original Wumpus output.
The retrieval results for all settings are in Table
3. We show both success@10 and success@150,
and MRR@150 for each setting. Success@150 is
important if we consider the current results as input
for the re-ranking module. As explained before,
re-ranking can only be successful if at least one rel-
evant answer is retrieved by the retrieval module.
For each measure (s@10, s@150 and MRR@150),
the score of the highest-scoring setting is printed in
bold face.
As expected, the evaluation of the Wumpus data
with the use of answer patterns gives somewhat
lower scores than evaluation based on manual as-
sessment of all answers (table 2). This confirms
our idea that the use of answer patterns for why-
QA leads to conservative results. Thus we can
5The pre-extracted reference answer is: ?This is because
a normal healthy human immune system can distinguish for-
eign tissues and attempts to destroy them, just as it attempts
to destroy infective organisms such as bacteria and viruses.?
state that the Lemur scores shown in table 3 are
not overestimated and therefore reliable.
Since we are using the output of the passage re-
trieval module as input for our re-ranking mod-
ule, we are mainly interested in the scores for
success@150. For the four retrieval models, we
see that TFIDF seems to score somewhat better
on retrieving sliding windows in terms of suc-
cess@150 than Okapi and the Kullback-Leibler
language model. On the other hand, Kullback-
Leibler and QAP seem to perform better on retriev-
ing disjoint windows. However, these differences
are not significant at the p = 0.01 level. For the
differences between disjoint and sliding windows
for all retrieval models together, we see that re-
trieval of sliding windows gives significantly bet-
ter results than disjoint windows in terms of suc-
cess@150 (p < 0.001).
5.4 The influence of passage retrieval on our
pipeline system
As described in section 5.1, our system is a
pipeline: after passage retrieval, we apply a re-
ranking module that uses syntactic information for
re-scoring the results from the retrieval module. As
input for our re-ranking module we use the out-
put of the retrieval setting with the highest suc-
cess@150 score: Lemur/TFIDF on sliding win-
dows. For 81.7% of the questions in our set,
Lemur/TFIDF retrieved an answer in the top-150.
This means that the maximum success@10 score
that we can obtain by re-ranking is 81.7%.
For weighting the feature values, we re-use the
weights that we had earlier found from training on
our set of 93 questions and the 150 answers that
were retrieved by Wumpus. We again take into
account five-fold cross validation for evaluation.
For a detailed description of our re-ranking mod-
ule and the syntactic features that we exploit, we
refer to (Verberne et al, 2008).
The results from re-ranking are in Table 4.
In the table, four system versions are compared:
(1) the original Wumpus/QAP module, (2) the
original why-pipeline system: Wumpus/QAP with
re-ranking, (3) TFIDF-sliding and (4) the new
31
Table 3: Results for passage retrieval on why-questions against Wikipedia using disjoint windows (DW)
and sliding windows (SW)
Success@10 Success@150 MRR@150
Retrieval model DW SW DW SW DW SW
Baseline: Wumpus/QAP 40.9% 72.0% 0.229
Lemur/TFIDF 43.0% 45.2% 71.1% 81.7% 0.247 0.338
Lemur/Okapi 41.9% 44.1% 67.7% 79.6% 0.243 0.320
Lemur/KL 48.9% 50.0% 72.8% 77.2% 0.263 0.324
pipeline system: TFIDF-sliding with re-ranking.
We again show MRR, success@10 and suc-
cess@150. For each measure, the score of the
highest-scoring setting is printed in bold face.
After applying our re-ranking module (right bot-
tom setting), we find a significant improvement
over bare TFIDF (left bottom setting). In terms
of MRR, we also see an improvement over the re-
sults that we had obtained by re-ranking the Wum-
pus/QAP output (right top setting). However, suc-
cess@10 does not show significant improvement.
The improvement that the re-ranking module gives
is smaller for the TFIDF retrieval results (MRR
goes from 0.338 to 0.359) than for the QAP results
(MRR increases from 0.260 to 0.328). We suspect
that this may be due to the fact that we used feature
weights for re-ranking that we had earlier obtained
from training on the Wumpus/QAP data (see sec-
tion 5.4). It would be better to re-train our feature
weights on the Lemur data. Probably, re-ranking
can then make a bigger contribution than it does
now for the Lemur data.
6 Overall conclusion
In this paper we have investigated the contribu-
tion of sliding windows as apposed to disjoint win-
dows with different retrieval modules for two dif-
ferent QA tasks: the TREC-QA 2002?2003 task
and why-QA.
For the TREC factoid-QA task, we have found
that retrieval of sliding windows outperfoms re-
trieval of disjoint windows in returning as many
relevant passages as possible on top ranks (accord-
ing to the TDRR metric). The experimental results
show that a language model based on Kullback-
Leibler divergence gives better performance than
two vector space models for both types of win-
dows retrieval according to MRR, success@n and
TDRR evaluation metrics. We found that the
number of answered questions (success@n) was
slightly lower when we used sliding windows for
passage retrieval than disjoint windows, but we
think one of the reasons is that sliding windows
are more homogeneous than disjoint windows, and
therefore for some questions more irrelevant pas-
sages are returned at top ranks and relevant pas-
sages are surpressed down.
For the task of retrieving answers to why-
questions from Wikipedia data, we found that the
best retrieval model is TFIDF, and sliding win-
dows give significantly better results than disjoint
windows. We also found better performance for
our complete why-pipeline system after applying
our existing re-ranking module to the passages re-
trieved with TFIDF-sliding.
In general, we find that for QA, sliding win-
dows give better results than disjoint windows in
the passage retrieval step. The best scoring re-
trieval model depends on the task under consid-
eration, because the nature of the documents and
question sets differ. This shows that for each spe-
cific QA task, different retrieval models should be
considered.
In the future, we aim to boost passage retrieval
for QA even more by applying query expansion
techniques that are specific to the QA tasks that
we consider, i.e. TREC factoid-QA and why-QA.
References
Bilotti, M.W., B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or morpho-
logical query expansion. In Proceedings of the SI-
GIR 2004 Workshop IR4QA: Information Retrieval
for Question Answering, July.
Buttcher, S., C.L.A. Clarke, and G.V. Cormack. 2004.
Domain-specific synonym expansion and validation
for biomedical information retrieval (multitext ex-
periments for trec 2004).
Buttcher, S. 2007. The wumpus search engine.
http://www.wumpus-search.org/.
Callan, James P. 1994. Passage-level evidence in doc-
ument retrieval. In SIGIR, pages 302?310.
32
Table 4: Results for the why-QA pipeline system for best-scoring passage retrieval setting compared
against the Wumpus baseline, for both bare retrieval and the complete system with re-ranking
Success@10 Success@150 MRR
Retrieval model Bare +Re-rank Bare +Re-rank Bare +Re-rank
Baseline: Wumpus/QAP-disjoint 43.0% 54.8% 73.1% 73.1% 0.260 0.328
Lemur/TFIDF-sliding 45.2% 55.9% 81.7% 81.7% 0.338 0.359
Denoyer, L. and P. Gallinari. 2006. The Wikipedia
XML corpus. ACM SIGIR Forum, 40(1):64?69.
Hearst, Marti A. and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In
ACM-SIGIR, 1993, pages 59?68.
Hirschman, L. and R. Gaizauskas. 2001. Natural lan-
guage question answering: the view from here. Nat.
Lang. Eng., pages 275?300.
Hovy, E.H., U. Hermjakob, and D. Ravichandran.
2002. A question/answer typology with surface text
patterns. In Proceedings of the Human Language
Technology conference (HLT), San Diego, CA.
Lafferty, J. and C. Zhai. 2001. Document language
models, query models, and risk minimization for in-
formation retrieval. In In Proceedings of SIGIR?01,
pages 111?119.
Lin, J., D. Quan, V. Sinha, K. Bakshi, D. Huynh,
B. Katz, and D.R. Karger. 2003. The role of con-
text in question answering systems. Conference on
Human Factors in Computing Systems, pages 1006?
1007.
Monz, Christof. 2003. Document retrieval in the con-
text of question answering. In ECIR, pages 571?579.
Roberts, I. and R. Gaizauskas. 2004. Evaluating pas-
sage retrieval approaches for question answering. In
In Proceedings of ECIR , 2004.
Robertson, Stephen E. and Steve Walker. 1999.
Okapi/keenbow at trec-8. In Text Retrieval Confer-
ence.
Robertson, Stephen E., Steve Walker, Micheline
Hancock-Beaulieu, and Gatford M. 1995. Okapi at
trec-3. In Text Retrieval Conference, pages 109?26.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative evaluation of passage retrieval al-
gorithms for question answering. In In SIGIR con-
ference on Research and development in informaion
retrieval, 2003, pages 41?47.
Tiedemann, Jo?rg. 2007. Comparing document seg-
mentation strategies for passage retrieval in question
answering. In Proceedings of RANLP 07, Borovets,
Bulgaria.
Verberne, Suzan, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2008. Using Syntactic Infor-
mation for Improving Why-Question Answering. In
Proceedings of The 22nd International Conference
on Computational Linguistics (COLING 2008).
Voorhees, Ellen. 2001. Overview of trec 2001 question
answering track. In In Proceedings of TREC.
Voorhees, Ellen. 2002. Overview of trec 2002 question
answering track. In In Proceedings of TREC.
Voorhees, Ellen. 2003. Overview of trec 2003 question
answering track. In In Proceedings of TREC.
Zhai, ChengXiang and John D. Lafferty. 2004. A study
of smoothing methods for language models applied
to information retrieval. ACM Trans. Inf. Syst., pages
179?214.
33
