Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 86?89, Dublin, Ireland, August 23-29 2014.
NTU-MC Toolkit: Annotating a Linguistically Diverse Corpus
Liling Tan
Universit?t des Saarland
Campus, 66123 Saarbr?cken, Germany
alvations@gmail.com
Francis Bond
Nanyang Technological University
14 Nanyang Drive, Singapore 637332
bond@ieee.org
Abstract
The NTU-MC Toolkit is a compilation of tools to annotate the Nanyang Technological University
- Multilingual Corpus (NTU-MC). The NTU-MC is a parallel corpora of linguistically diverse
languages (Arabic, English, Indonesian, Japanese, Korean, Mandarin Chinese, Thai and Viet-
namese). The NTU-MC thrives on the mantra of "more data is better data and more annotation
is better information". Other than increasing parallel data from diverse language pairs, annotat-
ing the corpus with various layers of information allows corpora linguists to discover linguistic
phenomena and provides computational linguists with pre-annotated features for various NLP
tasks. In addition to the agglomeration existing tools into a single python wrapper library, we
have implemented three tools (Mini-segmenter, GaChalign and Indotag) that (i) pro-
vides users with varying analysis of the corpus, (ii) improves the state-of-art performance and
(iii) reimplements a previously unavailable annotation tool as a free and open tool. This paper
briefly describes the wrapper classes available in the toolkit and introduces and demonstrates the
usage of the Mini-segmenter, GaChalign and Indotag.
1 Introduction
The NTU-MC Toolkit was developed in conjunction with the compilation of the Nanyang Technological
University - Multilingual Corpus (NTU-MC) (Tan and Bond, 2012). It is an agglomeration of existing
state-of-art tools into a single python wrapper library. The NTU-MC Toolkit provides python wrapper
classes for tokenizers and Part-of-Speech (POS) taggers for the respectively languages:
? Stanford Segmenter and POS taggers (Arabic and Chinese)
? POSTECH POSTAG/K tagger (Korean)
? tinysegmenter and MeCab (Japanese)
? JVnTextPro (Vietnamese)
Additionally, we implemented three tools to provide complementary or better annotations, viz.:
? Mini-segmenter (Chinese): Dictionary based Chinese segmenter
? GaChalign (Crosslingual): Gale-Church Sentence-level Aligner with variable parameters
? Indotag (Indonesian): Conditional Random Field (CRF) POS tagger.
The following sections of the paper will briefly describe the wrapper classes available in the toolkit (Sec-
tion 2) and introduce and demonstrate the usage of the Mini-segmenter (Section 3), GaChalign
(Section 4) and Indotag (Section 5).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
86
2 Tokenization and POS Tagger Wrappers
Python wrapper classes were written for (i) Stanford Segmenter and POS tagger (Chang et al., 2008;
Toutanova et al., 2003), (ii) POSTECH POSTAG/K tagger (Lee et al., 2002), (iii) tinysegmenter
and MeCab (Kudo et al., 2004) and (iv) JVnTextPro (Nguyen et al., 2010). Although scientifically
uninteresting, it simplifies usage of annotation tools especially for beginner who are new to Natural
Language Processing or python programming. The wrapper classes are also compatible with corpora
readers of the Natural Language Toolkit (NLTK).
Usage Users can either invoke the wrapper classes programmatically
1
:
$ python
?> from ntumc.tk import postech
?> sentence = u"???????????????????????????Tian
Tian Hainanese Chicken Rice????????(Maxwell Food Centre)????
???, ??????????????????????."
?> postech.postagk(sentence)
[(u??????, ?NNP?), (u????, ?JKB?), (u????, ?MAG?), (u????,
?XR?), (u???, ?XSA?), (u???, ?ETM?), (u??????, ?NNG?), ...]
or via command line:
$ echo "???????????????????????????Tian Tian
Hainanese Chicken Rice????????(Maxwell Food Centre)???????,
??????????????????????." > input.txt
$ python ntumc/tk/postech.py input.txt > output.txt
3 Mini-segmenter
The mini-segmenter is dictionary based Chinese segmenter that capitalizes on token length as
heuristics for Chinese text tokenization. The tool includes a dictionary of Singaporean Chinese NEs
crawled from Wikipedia titles and articles on Singapore.
Motivation The mini-segmenter was created to resolve the problem of segmenting localized Chinese
words from the Singaporean variety of Mandarin Chinese in the NTU-MC. After manual inspection,
the Stanford Chinese segmenter
2
was segmenting the Chinese tokens with the wrong word boundary.
For example, the Stanford Chinese word segmenter wrongly tokenized ??? wujielu ?Orchard
road" as ?_?? wu jielu ?black joint-road". Originally, these topological terms were re-segmented
with a manually crafted dictionary built using Wikipedia?s Chinese translations of English names of
Singapore places and streets. Then we found more localized Named Entities (NEs) for person names,
organizations and food terms. Short of building a manually segmented corpus and retraining the Stan-
ford segmenter models, a simple dictionary approach to segmentation could resolve out-of-domain issue.
Innovation A lightweight lexicon/dictionary based Chinese text segmenter. The advantage of us-
ing a lexicon/dictionary for text segmentation is the ability to localize and scale according to the Chinese
variety or domain. The mini-segmenter ranks the token boundaries based on sum of the square of
the tokens? character length,
?
n
i
len(token
i
)
2
, where n is the number of tokens and len(token) is the
character length of each token. This novel scoring is based on the preference for larger chunks than
smaller chunks in a sentence.
Usage The full documentation of the mini-segmenter can be found on https:
//code.google.com/p/mini-segmenter/
1
The example sentence in English, ?One of the most famous Hainanese chicken rice stalls in Singapore, Tian Tian
Hainanaese Chiken Rice is located in the Maxwell Food Centre, with long queues forming in front of the stall every day."
2
both Penn Chinese Treebank (ctb) and Peking University (pku) models
87
Results We evaluate the mini-segmenter output against the Stanford segmenter output with the
fish-head-curry.txt from the NTU-MC which was was previously selected at random as a text
sample for human annotators to verify the tagger accuracy. The Stanford segmenter with Stanford POS
tagger, it achieved 85.94% POS accuracy with 19% mis-segments. Using the mini-segmenter with
the Stanford POS tagger, it achieved 91.27% POS accuracy with 11.43% mis-segments.
4 GaChalign
The GaChalign tool is sentence alignment tool to align sentences given a bitext. The tool is a modifica-
tion of the original Gale-Church algorithm that capitalized on ratio of characters/tokens of two languages
in the bitext to align the sentences (Gale and Church, 1993).
Motivation The Gale-Church algorithm had parameters tuned to suit Indo-European languages more
specifically German-English language pairs. When using state-of-art sentence alignment tool based on
Gale-Church algorithm to align Chinese, Japanese or Korean texts to their respective English texts, the
NTU-MC reported a poor performance in F-measure metrics adheres to standards set by the ARCADE
II project (Chiao et al. 2006). We want to see whether it is possible to improve the algorithm by tune
algorithm using language-pair specific parameters.
Innovation We replaced the mean, variance and penalty parameters from the Gale-Church algorithm
with language-pair specific parameters automatically calculated from a non-aligned corpus.
Results Our experiment with English-Japanese corpus has shown that (i) simply using the calculated
character mean from the unaligned text improves precision and recall of the algorithm; from 61.0%
(default parameters) to 62.0% (language specific) F-scores) and (ii) using language specific penalties
further increased the F-scores to 62.9%. However, aligning syllabic/logographic language (Japanese) to
alphabetic language (English) remains a challenge for Gale-Church algorithm
3
.
5 Indotag
The Indotag is a probabilistic Conditional Random Field (CRF) Bahasa Indonesian Part of Speech
(POS) tagger with the specifications recommended by (Pisceldo et al., 2009). The pre-trained model is
based on the unigram CRF with 2-left and 2-right context features using the Universitas Indonesia?s 1
million word corpus compiled under the Pan Asia Networking Localization (PANL10N) project.
Motivation To reimplement the Indonesian POS tagger described in Pisceldo et al. (2010) using free
and open data and licensing it as open source tool.
Innovation None or not much. An open source reimplementation of a Bahasa Indonesian POS tagger.
Result The IndoTag achieved 78% accuracy when annotating the the fish-head-curry.txt text
sample from the NTU-MC.
6 Discussion
While English POS tagging reports >97% accuracy (Manning, 2011) and sentence alignments for Indo-
European languages performs well at >96% (Gale and Church, 1993; Varga et al., 2007), there is much
room for improvement with regards to POS tagger accuracy for Asian languages and automatic sen-
tence alignments from syllabic/logographic languages to alphabetic ones. Even though the languages in
the NTU-MC are not considered low-resource languages, the tools to annotate them have limited per-
formance. While the maintainers of the NTU-MC continues to push the performance of the individual
tools for these languages, we urge researchers to work on improving NLP tools/application for Asian
languages.
3
Detailed evaluation on the GaChalign experiments can be found on https://code.google.com/p/
gachalign/
88
7 Conclusion
We have introduced the NTU-MC Toolkit that was compiled to annotated the linguistically diverse NTU-
MC. The toolkit agglomerate existing tools into a single python wrapper library. The toolkit also imple-
mented the novel dictionary-based segmenter (Mini-segmenter) to improve state-of-art performance
for Chinese segmentation, an modified Gale-Church algorithm (GaChalign) to improve sentence align-
ments for syllabic-alphabetic language pairs and reimplemented an open source Indotag Bahasa In-
donesian POS tagger.
Acknowledgements
This research was partially funded by a joint JSPS/NTU grant on Revealing Meaning through Multiple
Languages and the Erasmus Mundus Action 2 program MULTI of the European Union, grant agreement
number 2009-5259-5.
The research leading to these results has received funding from the People Programme (Marie
Curie Actions) of the European Union?s Seventh Framework Programme FP7/2007-2013/ under REA
grant agreement n
?
317471.
References
Pi-Chuan Chang, Michel Galley, and Christopher D Manning. 2008. Optimizing chinese word segmentation for
machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation,
pages 224?232. Association for Computational Linguistics.
William A. Gale and Kenneth Ward Church. 1993. A program for aligning sentences in bilingual corpora. Com-
putational Linguistics, 19(1):75?102.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to japanese
morphological analysis. In EMNLP, pages 230?237.
Gary Geunbae Lee, Jeongwon Cha, and Jong-Hyeok Lee. 2002. Syllable-pattern-based unknown-morpheme
segmentation and estimation for hybrid part-of-speech tagging of korean. Computational Linguistics, 28(1):53?
70.
Christopher D Manning. 2011. Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In
Computational Linguistics and Intelligent Text Processing, pages 171?189. Springer.
Cam-Tu Nguyen, Xuan-Hieu Phan, and Thu-Trang Nguyen. 2010. Jvntextpro: A java-based vietnamese text
processing tool. http://jvntextpro.sourceforge.net/.
Femphy Pisceldo, Ruli Manurung, and Mirna Adriani. 2009. Probabilistic part-of-speech tagging for bahasa
indonesia.
Liling Tan and Francis Bond. 2012. Building and annotating the linguistically diverse ntu-mc (ntu-multilingual
corpus). In International Journal of Asian Language Processing, 22(4), page 161?174.
Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages
173?180. Association for Computational Linguistics.
Daniel Varga, Peter Halacsy, AndraS Kornai, Viktor Nagy, Laszlo Nemeth, and Viktor TrOn. 2007. Parallel
corpora for medium density languages. Recent Advances in Natural Language Processing IV: Selected Papers
from RANLP 2005, 292:247.
89
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 167?170, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
XLING: Matching Query Sentences to a Parallel Corpus using  
Topic Models for Word Sense Disambiguation 
 
Liling Tan and Francis Bond 
Division of Linguistics and Multilingual Studies, 
Nanyang Technological University 
14 Nanyang Drive, Singapore 637332 
alvations@gmail.com, bond@ieee.org 
Abstract 
This paper describes the XLING system partici-
pation in SemEval-2013 Crosslingual Word 
Sense Disambiguation task. The XLING system 
introduces a novel approach to skip the sense 
disambiguation step by matching query sentenc-
es to sentences in a parallel corpus using topic 
models; it returns the word alignments as the 
translation for the target polysemous words. 
Although, the topic-model base matching under-
performed, the matching approach showed po-
tential in the simple cosine-based surface simi-
larity matching. 
1 Introduction 
This paper describes the XLING system, an un-
supervised Cross-Lingual Word Sense Disam-
biguation (CLWSD) system based on matching 
query sentence to parallel corpus using topic 
models. CLWSD is the task of disambiguating a 
word given a context by providing the most ap-
propriate translation in different languages 
(Lefever and Hoste, 2013).  
2 Background  
Topic models assume that latent topics exist in 
texts and each semantic topic can be represented 
with a multinomial distribution of words and 
each document can be classified into different 
semantic topics (Hofmann, 1999). Blei et al 
(2003b) introduced a Bayesian version of topic 
modeling using Dirichlet hyper-parameters, La-
tent Dirichlet Allocation (LDA). Using LDA, a 
set of topics can be generated to classify docu-
ments within a corpus. Each topic will contain a 
list of all the words in the vocabulary of the cor-
pus where each word is assigned a probability of 
occurring given a particular topic. 
3 Approach 
We hypothesized that sentences with different 
senses of a polysemous word will be classified 
into different topics during the LDA process. By 
matching the query sentence to the training sen-
tences by LDA induced topics, the most appro-
priate translation for the polysemous word in the 
query sentence should be equivalent to transla-
tion of word in the matched training sentence(s) 
from a parallel corpus. By pursuing this ap-
proach, we escape the traditional mode of dis-
ambiguating a sense using a sense inventory. 
4 System Description 
The XLING_TnT system attempts the matching 
subtask in three steps (1) Topicalize: match-
ing the query sentence to the training sentences 
by the most probable topic. (2) Rank: the 
matching sentences were ranked according to 
the cosine similarity between the query and 
matching sentences. (3) Translate: provides 
the translation of the polysemous word in the 
matched sentence(s) from the parallel corpus.  
4.1 Preprocessing  
The Europarl version 7 corpus bitexts (English-
German, English-Spanish, English-French, Eng-
lish-Italian and English-Dutch) were aligned at 
word-level with GIZA++ (Och and Ney, 2003). 
The translation tables from the word-alignments 
were used to provide the translation of the poly-
semous word in the Translate step.  
The English sentences from the bitexts were 
lemmatized using a dictionary-based lemmatiz-
167
er: xlemma1. After the lemmatization, English 
stopwords2 were removed from the sentences. 
The lemmatized and stop filtered sentences were 
used as document inputs to train the LDA topic 
model in the Topicalize step.  
Previously, topic models had been incorpo-
rated as global context features into a modified 
naive Bayes network with traditional WSD fea-
tures (Cai et al 2007). We try a novel approach 
of integrating local context (N-grams) by using 
pseudo-word sentences as input for topic induc-
tion. Here we neither lemmatize or remove stops 
words.  For example: 
 
Original Europarl sentence: ?Education and 
cultural policies are important tools for creating 
these values? 
 
Lemmatized and stopped: ?education cultural 
policy be important tool create these values? 
 
Ngram pseudo-word: ?education_and_cultural 
and_cultural_policies cultural_policies_are 
are_important_tools important_tools_for 
tools_for_creating for_creating_these creat-
ing_these_values? 
4.2 Topicalize and Match 
The Topicalize step of the system first (i) 
induced a list of topics and trained a topic model 
for each polysemous word using LDA, then (ii) 
allocated the topic with the highest probability 
to each training sentence. 
Finally, at evaluation, (iii) the query sentences 
were assigned the most probable topic inferred 
using the trained topic models. Then the training 
sentences allocated with the same topic were 
considered as matching sentences for the next 
Rank step.  
4.2.1 Topic Induction 
Topic models were trained using Europarl sen-
tences that contain the target polysemous words; 
one model per target word. The topic models 
were induced using LDA by setting the number 
of topics (#topics) as 50, and the alpha and beta 
                                                          
1  http://code.google.com/p/xlemma/ 
2  Using the Page and Article Analyzer stopwords from    
   http://www.ranks.nl/resources/stopwords.html 
hyper-parameters were symmetrically set at 
1.0/#topics. Blei et al (2003) had shown that the 
perplexity plateaus when #topics ? 50; higher 
perplexity means more computing time needed 
to train the model. 
4.2.2 Topic Allocation 
Each sentence was allocated the most probable 
topic induced by LDA. An induced topic con-
tained a ranked list of tuples where the 2nd ele-
ment in each tuple is a word that associated with 
the topic, the 1st element is the probability that 
the associated word will occur given the topic. 
The probabilities are generatively output using 
Variational Bayes algorithm as described in 
Hoffman et al (2010). For example: 
[(0.0208, 'sport'), (0.0172, 'however'), 
(0.0170, 'quite'), (0.0166, 'maritime'), 
(0.0133, 'field'), (0.0133, 'air-transport'), 
(0.0130, 'appear'), (0.0117, 'arrangement'), 
(0.0117, 'pertain'), (0.0111, 'supervision')] 
4.2.3 Topic Inference 
With the trained LDA model, we inferred the 
most probable topic of the query sentence. Then 
we extracted the top-10 sentences from the train-
ing corpus that shared the same top ranking top-
ic.  
The topic induction, allocation and inference 
were done separately on the lemmatized and 
stopped sentences and on the pseudo-word sen-
tence, resulting in two sets of matching sentenc-
es. Only the sentences that were in both sets of 
matches are considered for the Rank step. 
4.3 Rank 
Matched sentences from the Topicalize step 
were converted into term vectors. The vectors 
were reweighted using tf-idf and ranked accord-
ing to the cosine similarity with the query sen-
tences. The top five sentences were piped into 
the Translate step. 
4.4 Translate 
From the matching sentences, the Translate 
step simply checks the GIZA++ word alignment 
table and outputs the translation(s) of the target 
polysemous word. Each matching sentence, 
168
could output more than 1 translation depending 
on the target word alignment. As a simple way 
of filtering stop-words from target European 
languages, translations with less than 4 charac-
ters were removed. This effectively distills misa-
ligned non-content words, such as articles, pro-
nouns, prepositions, etc. To simplify the lemma-
tization of Spanish and French plural noun suf-
fixes, the ?-es? and ?-s? are stemmed from the 
translation outputs.  
 The XLING_TnT system outputs one transla-
tion for each query sentence for the best result 
evaluation. It output the top 5 translations for the 
out-of-five evaluation. 
4.5 Fallback 
For the out-of-five evaluation, if the query re-
turned less than 5 answers, the first fallback3 
appended the lemma of the Most Frequent Sense 
(according to Wordnet) of the target polysemous 
word in their respective language from the Open 
Multilingual Wordnet.4 If the first fallback was 
insufficient, the second fallback appended the 
most frequent translation of the target polyse-
mous word to the queries? responses. 
4.6 Baseline 
We also constructed a baseline for matching sen-
tences by cosine similarity between the lemmas 
of the query sentence and the lemmas of each 
English sentence in the training corpus.5 The 
baseline system is named XLING_SnT (Similar 
and Translate). The cosine similarity is calculat-
ed from the division of the vector product of the 
query and training sentence (i.e. numerator) by 
the root product of the vector?s magnitude 
squared. 
5 Results 
Tables 1 and 2 present the results for the XLING 
system for best and out-of-five evaluation. Our 
system did worse than the task?s baseline, i.e. 
the Most Frequent Translation (MFT) of the tar-
get word for all languages. Moreover the topic 
                                                          
3    Code sample for the fallback can be found at  
     http://goo.gl/PbdK7 
4    http://www.casta-net.jp/~kuribayashi/multi/ 
5  Code-snippet for the baseline can be found at  
     http://pythonfiddle.com/surface-cosine-similarity  
model based matching did worse than the cosine 
similarity matching baseline. The results show 
that matching on topics did not help. However, 
Li et al (2010) and Anaya-Sanchez et al (2007) 
had shown that pure topic model based unsuper-
vised system for WSD should perform a little 
better than Most Frequent Sense baseline in 
coarse-grain English WSD. Hence it was neces-
sary to perform error analysis and tweaking to 
improve the XLING system. 
 
BEST German Spanish French Italian Dutch 
SnT 
 
8.13  
(10.36) 
19.59 
(24.31) 
17.33 
(11.57) 
12.74 
(11.27) 
9.89 
(9.56) 
TnT 
 
5.28 
(5.82) 
18.60 
(24.31) 
16.48 
(11.63) 
10.70 
(7.54) 
7.40 
(8.54) 
MFT 
 
17.43 
(15.30) 
23.23 
(27.48) 
25.74 
(20.19) 
20.21 
(19.88) 
20.66 
(24.15) 
Table 1: Precision and (Mood) for the best evaluation 
 
OOF German Spanish French Italian Dutch 
SnT 
 
23.71 
(30.57) 
44.83 
(50.04) 
38.44 
(32.45) 
32.38 
(29.17) 
27.11 
(27.31) 
TnT 
 
19.13 
(23.54) 
39.52 
(44.96) 
35.3 
(28.02) 
33.28 
(29.61) 
23.27 
(22.98) 
MFT 
 
38.86 
(44.35) 
53.07 
(57.35) 
51.36 
(47.42) 
42.63 
(41.69) 
43.59 
(41.97) 
Table 2: Precision and (Mood) for the oof evaluation 
6 Error Analysis and Modifications 
Statistically, we could improve the robustness of 
the topic models in the Topicalize step by 
(i) tweaking the Dirichlet hyper-parameters to 
alpha = 50/#topics, beta = 0.01 as suggested by 
Wang et al (2009). 
 
 BEST OOF 
 Precision Mood Precision Mood 
German 6.50 6.71 20.98 25.18 
Spanish 14.77 19.43 40.22 45.67 
French 10.79 7.95 31.26 23.37 
Italian 13.10 10.95 36.56 31.94 
Dutch 7.42 7.47 21.66 20.42 
Table 3: Evaluations on Hyper-parameter tweaks 
 
Although the hyperparameters tweaks improves 
the scores for German and Dutch evaluations it 
brings the overall precision and mood precision 
of the other three languages down. Since the 
documents from each language are parallel, this 
169
suggests that there is some language-dependency 
for LDA?s hyperparameters. 
 By going through the individual queries and 
responses, several issues in the translate 
step need to be resolved to achieve higher preci-
sion; (i) German-English and Dutch-English 
word alignments containing compound words 
need to be segmented (e.g. kraftomnibusverkehr 
?kraft omnibus verkehr) and realigned such that 
the target word coach only aligns to omnibus, 
(ii) lemmatization of Italian, German and Dutch 
is crucial is getting the gold answers of the task 
(e.g. XLING answers omnibussen while the gold 
answers allowed omnibus). The use of target 
language lemmatizers, such as TreeTagger 
(Schmid, 1995) would have benefited the sys-
tem. 
7 Discussion 
The main advantage of statistical language inde-
pendent approaches is the ability to scale the 
system in any possible language. However lan-
guage dependent processing remains crucial in 
building an accurate system, especially lemmati-
zation in WSD tasks (e.g. kraftomnibusverkehr). 
We also hypothesize that more context would 
have improved the results of using topics: dis-
ambiguating senses solely from sentential con-
text is artificially hard. 
8 Conclusion 
Our system has approached the CLWSD task in 
an unconventional way of matching query sen-
tences to parallel corpus using topic models. 
Given no improvement from hyper-parameter 
tweaks, it reiterates Boyd-Graber, Blei and 
Zhu?s (2007) assertion that while topic models 
capture polysemous use of words, they do not 
carry explicit notion of senses that is necessary 
for WSD. Thus our approach to match query 
sentences by topics did not perform beyond the 
MFT baseline in the CLWSD evaluation. 
However, the surface cosine baseline, with-
out any incorporation of any sense knowledge, 
had surprisingly achieved performance closer to 
MFT It provides a pilot platform for future work 
to approach the CLWSD as a vector-based doc-
ument retrieval task on parallel corpora and 
providing the translation from the word align-
ments. 
References  
 enry Anaya-   anche , Aurora  ons-Porrata, and 
Rafael Berlanga-Llavori. 2007. Tkb-uo: Using 
sense clustering for wsd. In Proceedings of the 
Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pp. 322?325. 
Jordan Boyd-Graber, David M. Blei, and Xiaojin 
Zhu. 2007. A Topic Model for Word Sense Dis-
ambiguation. In Proc. of Empirical Methods in 
Natural Language Processing( EMNLP).  
David M. Blei, Andrew Y. Ng, and Michael L. Jor-
dan. 2003. Latent Dirichlet alocation. Journal of 
Machine Learning Research, 3:993?1022. 
Jun-Fu Cai, Wee-Sun Lee and Yee-Whye Teh. 2007. 
Improving word sense disambiguation using topic 
features. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning (EMNLP-CoNLL), pp. 1015?1023. 
Christiane Fellbaum. (ed.) (1998) WordNet: An Elec-
tronic Lexical Database, MIT Press 
Thomas Hofmann. 1999. Probabilistic latent semantic 
indexing. In Proceedings of SIGIR '99, Berkeley, 
CA, USA. 
Matthew Hoffman, David Blei and Francis Bach. 
2010. Online Learning for Latent Dirichlet Alloca-
tion. In Proceedings of NIPS 2010. 
Els Lefever and V?ronique Hoste. 2013. SemEval-
2013 Task 10: Cross-Lingual Word Sense Disam-
biguation, In Proceedings SemEval 2013, in con-
junction with *SEM 2013, Atlanta, USA. 
Linlin Li, Benjamin Roth and Caroline Sporleder. 
Topic Models for Word Sense Disambiguation and 
Token-based Idiom Detection. In Proc. of The 
48th Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2010. Uppsala, 
Sweden. 
Franz Josef Och, Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment 
Models. Computational Linguistics 29:1. pp. 19-
51. 
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German. 
Proceedings of the ACL SIGDAT-Workshop. Dub-
lin, Ireland.  
Yi Wang, Hongjie Bai, Matt Stanton, Wen-Yen 
Chen, Edward Y. Chang. 2009. Plda: Parallel la-
tent dirichlet alocation for large-scale applica-
tions. In Proc. of 5th International Conference on 
Algorithmic Aspects in Information and Manage-
ment. 
170
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541?545,
Dublin, Ireland, August 23-24, 2014.
Sensible: L2 Translation Assistance by Emulating the Manual
Post-Editing Process
Liling Tan, Anne-Kathrin Schumann, Jose M.M. Martinez and Francis Bond
1
Universit?at des Saarland / Campus, Saarbr?ucken, Germany
Nanyang Technological University
1
/ 14 Nanyang Drive, Singapore
alvations@gmail.com, anne.schumann@mx.uni-saarland.de,
j.martinez@mx.uni-saarland.de, bond@ieee.org
Abstract
This paper describes the Post-Editor Z sys-
tem submitted to the L2 writing assis-
tant task in SemEval-2014. The aim of
task is to build a translation assistance
system to translate untranslated sentence
fragments. This is not unlike the task
of post-editing where human translators
improve machine-generated translations.
Post-Editor Z emulates the manual pro-
cess of post-editing by (i) crawling and ex-
tracting parallel sentences that contain the
untranslated fragments from a Web-based
translation memory, (ii) extracting the pos-
sible translations of the fragments indexed
by the translation memory and (iii) apply-
ing simple cosine-based sentence similar-
ity to rank possible translations for the un-
translated fragment.
1 Introduction
In this paper, we present a collaborative submis-
sion between Saarland University and Nanyang
Technological University to the L2 Translation As-
sistant task in SemEval-2014. Our team name
is Sensible and the participating system is Post-
Editor Z (PEZ).
The L2 Translation Assistant task concerns the
translation of an untranslated fragment from a par-
tially translated sentence. For instance, given a
sentence, ?Ich konnte B?arbel noch on the border
in einen letzten S-Bahn-Zug nach Westberlin set-
zen.?, the aim is to provide an appropriate transla-
tion for the underline phrase, i.e. an der Grenze.
The aim of the task is not unlike the task of
post-editing where human translators correct er-
rors provided by machine-generated translations.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The main difference is that in the context of post-
editing the source text is provided. A transla-
tion workflow that incorporates post-editing be-
gins with a source sentence, e.g. ?I could still
sit on the border in the very last tram to West
Berlin.? and the human translator is provided with
a machine-generated translation with untranslated
fragments such as the previous example and some-
times ?fixing? the translation would simply re-
quire substituting the appropriate translation for
the untranslated fragment.
2 Related Tasks and Previous
Approaches
The L2 writing assistant task lies between the
lines of machine translation and crosslingual word
sense disambiguation (CLWSD) or crosslingual
lexical substitution (CLS) (Lefever and Hoste,
2013; Mihalcea et al. 2010).
While CLWSD systems resolve the correct
semantics of the translation by providing the
correct lemma in the target language, CLS at-
tempts to provide also the correct form of the
translation with the right morphology. Machine
translation tasks focus on producing translations
of whole sentences/documents while crosslingual
word sense disambiguation targets a single lexical
item.
Previously, CLWSD systems have tried distri-
butional semantics and string matching methods
(Tan and Bond, 2013), unsupervised clustering of
word alignment vectors (Apidianaki, 2013) and
supervised classification-based approaches trained
on local context features for a window of three
words containing the focus word (van Gompel,
2010; van Gompel and van den Bosch, 2013; Rud-
nick et al., 2013). Interestingly, Carpuat (2013)
approached the CLWSD task with a Statistical MT
system .
Short of concatenating outputs of CLWSD /
CLS outputs and dealing with a reordering issue
541
and responding to the task organizers? call to avoid
implementing a full machine translation system
to tackle the task, we designed PEZ as an Auto-
matic Post-Editor (APE) that attempts to resolve
untranslated fragments.
3 Automatic Post-Editors
APEs target various types of MT errors from de-
terminer selection (Knight and Chander, 1994) to
grammatical agreement (Mare?cek et al., 2011).
Untranslated fragments from machine translations
are the result of out-of-vocabulary (OOV) words.
Previous approaches to the handling of un-
translated fragments include using a pivot lan-
guage to translate the OOV word(s) into a third
language and then back into to the source lan-
guage, thereby extracting paraphrases to OOV
(Callison-burch and Osborne, 2006), combining
sub-lexical/constituent translations of the OOV
word(s) to generate the translation (Huang et al.,
2011) or finding paraphrases of the OOV words
that have available translations (Marton et al.,
2009; Razmara et al., 2013).
1
However the simplest approach to handle un-
translated fragments is to increase the size of par-
allel data. The web is vast and infinite, a human
translator would consult the web when encounter-
ing a word that he/she cannot translate easily. The
most human-like approach to post-editing a for-
eign untranslated fragment is to do a search on
the web or a translation memory and choose the
most appropriate translation of the fragment from
the search result given the context of the machine
translated sentence.
4 Motivation
When post-editing an untranslated fragment, a hu-
man translator would (i) first query a translation
memory or parallel corpus for the untranslated
fragment in the source language, (ii) then attempt
to understand the various context that the fragment
can occur in and (iii) finally he/she would sur-
mise appropriate translations for the untranslated
fragment based on semantic and grammatical con-
straints of the chosen translations.
1
in MT, evaluation is normally performed using automatic
metrics based on automatic evaluation metrics that compares
scores based on string/word similarity between the machine-
generated translation and a reference output, simply remov-
ing OOV would have improved the metric ?scores? of the
system (Habash, 2008; Tan and Pal, 2014).
The PEZ system was designed to emulate the
manual post-editing process by (i) first crawling
a web-based translation memory, (ii) then extract-
ing parallel sentences that contain the untranslated
fragments and the corresponding translations of
the fragments indexed by the translation memory
and (iii) finally ranking them based on cosine sim-
ilarity of the context words.
5 System Description
The PEZ system consists of three components,
viz (i) a Web Translation Memory (WebTM)
crawler, (ii) the XLING reranker and (iii) a longest
ngram/string match module.
5.1 WebTM Crawler
Given the query fragment and the context sen-
tence, ?Die Frau kehrte alone nach Lima zur?uck?,
the crawler queries www.bab.la and returns
sentences containing the untranslated fragment
with various possible tranlsations, e.g:
? isoliert : Darum sollten wir den Kaffee nicht
isoliert betrachten.
? alleine : Die Kommission kann nun aber f?ur
ihr Verhalten nicht alleine die Folgen tragen.
? Allein : Allein in der Europischen Union
sind.
The retrieval mechanism is based on the
fact that the target translations of the queried
word/phrase are bolded on a web-based TM and
thus they can be easily extracted by manipulating
the text between <bold>...</bold> tags. Al-
though the indexed translations were easy to ex-
tract, there were few instances where the transla-
tions were embedded betweeen the bold tags on
the web-based TM.
5.2 XLING Reranker
XLING is a light-weight cosine-based sentence
similarity script used in the previous CLWSD
shared task in SemEval-2013 (Tan and Bond,
2013). Given the sentences from the WebTM
crawler, the reranker first removes all stopwords
from the sentences and then ranks the sentences
based on the number of overlapping stems.
In situations where there are no overlapping
content words from the sentences, XLING falls
back on the most common translation of the un-
translated fragment.
542
en-de en-es fr-en nl-en
acc wac rec acc wac rec acc wac rec acc wac rec
WebTM 0.160 0.184 0.647 0.145 0.175 0.470 0.055 0.067 0.210 0.092 0.099 0.214
XLING 0.152 0.178 0.647 0.141 0.171 0.470 0.055 0.067 0.210 0.088 0.095 0.214
PEZ 0.162 0.233 0.878 0.239 0.351 0.819 0.081 0.116 0.321 0.115 0.152 0.335
Table 1: Results for Best Evaluation of the System Runs.
5.3 Longest Ngram/String Matches
Due to the low coverage of the indexed trans-
lations on the web TM, it is necessary to ex-
tract more candidate translations. Assuming lit-
tle knowledge about the target language, human
translator would find parallel sentences containing
the untranslated fragment and resort to finding re-
peating phrases that occurs among the target lan-
guage sentences.
For instance, when we query the phrase history
book from the context ?Von ihr habe ich mehr gel-
ernt als aus manchem history book.?, the longest
ngram/string matches module retrieves several tar-
get language sentences without any indexed trans-
lation:
? Ich weise darauf hin oder nehme an, dass
dies in den Geschichtsb?uchern auch so
erw?ahnt wird.
? Wenn die Geschichtsb?ucher geschrieben wer-
den wird unser Zeitalter, denke ich, wegen
drei Dingen erinnert werden.
? Ich bin sicher, Pr?asident Mugabe hat sich
nun einen Platz in den Geschichtsb?uchern
gesichert, wenn auch aus den falschen
Gr?unden.
? In den Geschichtsb?uchern wird f?ur jeden
einzelnen Tag der letzten mehr als 227 Jahre
an Gewalttaten oder Tragdien auf dem eu-
rop?aischen Kontinent erinnert.
By simply spotting the repeating word/string
from the target language sentences it is pos-
sible to guess that the possible candidates
for ?history book? are Geschichtsb?ucher or
Geschichtsb?uchern. Computationally, this can
be achieved by looking for the longest matching
ngrams or the longest matching string across the
target language sentences fetched by the WebTM
crawler.
5.4 System Runs
We submitted three system runs to the L2 writing
assistant task in Semeval-2014.
1. WebTM: a baseline configuration which out-
puts the most frequent indexed translation of
the untranslated fragment from the Web TM.
2. XLING: reranks the WebTM outputs based
on cosine similarity.
3. PEZ: similar to the XLING but when the
WebTM fetches no output, the system looks
for longest common substring and reranks the
outputs based on cosine similarity.
6 Evaluation
The evaluation of the task is based on three met-
rics, viz. absolute accuracy (acc), word-based ac-
curacy (wac) and recall (rec).
Absolute accuracy measures the number of
fragments that match the gold translation of the
untranslated fragments. Word-based accuracy as-
signs a score according to the longest consecutive
matching substring between output fragment and
reference fragment; it is computed as such:
wac =
|longestmatch(output,reference)|
max(|output|,|reference|)
Recall accounts for the number of fragments for
which output was given (regardless of whether it
was correct).
7 Results
Table 1 presents the results for the best evalua-
tion scores of the PEZ system runs for the En-
glish to German (en-de), English to Spanish (en-
es), French to English (fr-en) and Dutch to English
(nl-en) evaluations. Figure 1 presents the word ac-
curacy of the system runs for both best and out-of-
five (oof) evaluation
2
.
The results show that using the longest
ngram/string improves the recall and subsequently
the accuracy and word accuracy of the system.
However, this is not true when guessing untrans-
lated fragments from L1 English to L2. This is
due to the low recall of the system when search-
ing for the untranslated fragment in French and
2
Please refer to http://goo.gl/y9f5Na for results
of other competing systems
543
Figure 1: Word Accuracy of System Runs (best on
the left, oof on the right).
Dutch, where the English words/phases indexed in
the TM is much larger than other languages.
8 Error Analysis
We manually inspected the English-German out-
puts from the PEZ system and identified several
particularities of the outputs that account for the
low performance of the system for this language
pair.
8.1 Weird Expressions in the TM
When attempting to translate Nevertheless in the
context of ?Nevertheless hat sich die neue Bun-
desrepublik Deutschland unter amerikanischem
Druck an der militrischen Einmischung auf dem
Balkan beteiligt.? where the gold translation is
Trotzdem or Nichtsdestotrotz. The PEZ system re-
trieves the following sentence pairs that contains a
rarely used expression nichtsdestoweniger from a
literally translated sentence pair in the TM:
? EN: But nevertheless it is a fact that nobody
can really recognize their views in the report.
? DE: Aber nichtsdestoweniger kann sich nie-
mand so recht in dem Bericht wiederfinden.
Another example of weird expression is when
translating ?husband? in the context of ?In der
Silvesternacht sind mein husband und ich auf die
Bahnhofstra?e gegangen.?. PEZ provided a lesser
use yet valid translation Gemahl instead of the
gold translation Mann. In this case, it is also a
matter of register where in a more formal register
one will use Gemahl instead of Mann.
8.2 Missing / Additional Words from
Matches
When extracting candidate translations from the
TM index or longest ngram/string, there are sev-
eral matches where the PEZ system outputs a par-
tial phrase or phrases with additional tokens that
cause the disparity between the absolute accuracy
and word accuracy. An instance of missing words
is as follows:
? Input: Eine genetische Veranlagung
plays a decisive role.
? PEZ: Eine genetische Veranlagung
eine entscheidende rolle.
? Gold: Eine genetische Veranlagung
spielt (dabei) eine entscheidende rolle.
For the addition of superfluous words is as fol-
lows:
? Input: Ger?ate wie Handys sind
not permitted wenn sie nicht unterrichtlichen
Belangen dienen.
? PEZ: Ger?ate wie Handys sind es verboten,
wenn sie nicht unterrichtlichen Belangen
dienen.
? Gold: Ger?ate wie Handys sind verboten
wenn sie nicht unterrichtlichen Belangen di-
enen.
8.3 Case Sensitivity
For the English-German evaluation , there are sev-
eral instances where the PEZ system produces the
correct translation of the phrase but in lower cases
and this resulted in poorer accuracy. This is unique
to German target language and possibly contribut-
ing to the lower scores as compared to the English-
Spanish evaluation.
9 Conclusion
In this paper, we presented the PEZ automatic
post-editor system in the L2 writing assistant task
in SemEval-2014. The PEZ post-editing system is
a resource lean approach to provide translation for
untranslated fragments based on no prior training
data and simple string manipulations from a web-
based translation memory.
The PEZ system attempts to emulate the pro-
cess of a human translator post-editing out-
of-vocabulary words from a machine-generated
544
translation. The best configuration of the PEZ sys-
tem involves a simple string search for the longest
common ngram/string from the target language
sentences without having word/phrasal alignment
and also avoiding the need to handle word reorder-
ing for multi-token untranslated fragments.
Acknowledgements
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union?s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement n
?
317471.
References
Marianna Apidianaki. 2013. Limsi : Cross-lingual
word sense disambiguation using translation sense
clustering. In Second Joint Conference on Lexi-
cal and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
178?182, Atlanta, Georgia, USA, June.
Chris Callison-burch and Miles Osborne. 2006. Im-
proved statistical machine translation using para-
phrases. In In Proceedings of HLT/NAACL-2006,
pages 17?24.
Marine Carpuat. 2013. Nrc: A machine translation ap-
proach to cross-lingual word sense disambiguation
(semeval-2013 task 10). In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 188?192, Atlanta, Georgia, USA, June.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In ACL, pages 57?
60.
Chung-Chi Huang, Ho-Ching Yen, Ping-Che Yang,
Shih-Ting Huang, and Jason S. Chang. 2011. Using
sublexical translations to handle the oov problem in
machine translation. ACM Trans. Asian Lang. Inf.
Process., 10(3):16.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In AAAI, pages 779?784.
David Mare?cek, Rudolf Rosa, Petra Galu?s?c?akov?a, and
Ond?rej Bojar. 2011. Two-step translation with
grammatical post-processing. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
WMT ?11, pages 426?432, Stroudsburg, PA, USA.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
EMNLP, pages 381?390.
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1105?
1115, Sofia, Bulgaria, August.
Alex Rudnick, Can Liu, and Michael Gasser. 2013.
Hltdi: Cl-wsd using markov random fields for
semeval-2013 task 10. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 171?177, Atlanta, Georgia, USA, June.
Liling Tan and Francis Bond. 2013. Xling: Match-
ing query sentences to a parallel corpus using topic
models for wsd. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
167?170, Atlanta, Georgia, USA, June.
Liling Tan and Santanu Pal. 2014. Manawi: Using
multi-word expressions and named entities to im-
prove machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
Baltimore, USA, August.
Maarten van Gompel and Antal van den Bosch. 2013.
Wsd2: Parameter optimisation for memory-based
cross-lingual word-sense disambiguation. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 183?187, Atlanta, Geor-
gia, USA, June.
Maarten van Gompel. 2010. Uvt-wsd1: A cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, SemEval ?10, pages 238?241,
Stroudsburg, PA, USA.
545
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 77?85,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
SeedLing: Building and using a seed corpus
for the Human Language Project
Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer, and Michaela Regneri
Universit?at des Saarlandes
66123 Saarbr?ucken, Germany
{emerson, liling, susfert, apalmer, regneri}
@coli.uni-saarland.de
Abstract
A broad-coverage corpus such as the Hu-
man Language Project envisioned by Ab-
ney and Bird (2010) would be a powerful
resource for the study of endangered lan-
guages. Existing corpora are limited in
the range of languages covered, in stan-
dardisation, or in machine-readability. In
this paper we present SeedLing, a seed
corpus for the Human Language Project.
We first survey existing efforts to compile
cross-linguistic resources, then describe
our own approach. To build the foundation
text for a Universal Corpus, we crawl and
clean texts from several web sources that
contain data from a large number of lan-
guages, and convert them into a standard-
ised form consistent with the guidelines
of Abney and Bird (2011). The result-
ing corpus is more easily-accessible and
machine-readable than any of the underly-
ing data sources, and, with data from 1451
languages covering 105 language fami-
lies, represents a significant base corpus
for researchers to draw on and add to in
the future. To demonstrate the utility of
SeedLing for cross-lingual computational
research, we use our data in the test appli-
cation of detecting similar languages.
1 Introduction
At the time of writing, 7105 living languages
are documented in Ethnologue,
1
but Simons and
Lewis (2011) calculated that 37% of extant lan-
guages were at various stages of losing trans-
misson to new generations. Only a fraction
of the world?s languages are well documented,
fewer have machine-readable resources, and fewer
again have resources with linguistic annotations
1
http://www.ethnologue.com
(Maxwell and Hughes, 2006) - so the time to work
on compiling these resources is now.
Several years ago, Abney and Bird (2010; 2011)
posed the challenge of building a Universal Cor-
pus, naming it the Human Language Project. Such
a corpus would include data from all the world?s
languages, in a consistent structure, facilitating
large-scale cross-linguistic processing. The chal-
lenge was issued to the computational linguistics
community, from the perspective that the language
processing, machine learning, and data manipula-
tion and management tools well-known in com-
putational linguistics must be brought to bear on
the problems of documentary linguistics, if we
are to make any serious progress toward build-
ing such a resource. The Universal Corpus as
envisioned would facilitate broadly cross-lingual
natural language processing (NLP), in particular
driving innovation in research addressing NLP for
low-resource languages, which in turn supports
the language documentation process.
We have accepted this challenge and have be-
gun converting existing resources into a format
consistent with Abney and Bird?s specifications.
We aim for a collection of resources that includes
data: (a) from as many languages as possible, and
(b) in a format both in accordance with best prac-
tice archiving recommendations and also readily
accessible for computational methods. Of course,
there are many relevant efforts toward producing
cross-linguistic resources, which we survey in sec-
tion 2. To the best of our knowledge, though, no
existing effort meets these two desiderata to the
extent of our corpus, which we name SeedLing: a
seed corpus for the Human Language Project.
To produce SeedLing, we have drawn on four
web sources, described in section 3.2. To bring
the four resources into a common format and
data structure (section 3.1), each required differ-
ent degrees and types of cleaning and standardis-
ation. We describe the steps required in section 4,
77
presenting each resource as a separate mini-case
study. We hope that the lessons we learned in
assembling our seed corpus can guide future re-
source conversion efforts. To that end, many of the
resources described in section 2 are candidates for
inclusion in the next stage of building a Universal
Corpus.
We believe the resulting corpus, which at
present covers 1451 languages from 105 language
families, is the first of its kind: large enough and
consistent enough to allow broadly multilingual
language processing. To test this claim, we use
SeedLing in a sample application (section 5): the
task of language clustering. With no additional
pre-processing, we extract surface-level features
(frequencies of character n-grams and words) to
estimate the similarity of two languages. Unlike
most previous approaches to the task, we make
no use of resources curated for linguistic typol-
ogy (e.g. values of typological features as in
WALS (Dryer and Haspelmath, 2013), Swadesh
word lists). Despite our approach being highly
dependent on orthography, our clustering perfor-
mance matches the results obtained by Georgi
et al. (2010) using typolological features, which
demonstrates SeedLing?s utility in cross-linguistic
research.
2 Related Work
In this section, we review existing efforts to com-
pile multilingual machine-readable resources. Al-
though some commercial resources are available,
we restrict attention to freely accessible data.
2
Traditional archives. Many archives exist to
store the wealth of traditional resources produced
by the documentary linguistics community. Such
documents are increasingly being digitised, or
produced in a digital form, and there are a number
of archives which now offer free online access.
Some archives aim for a universal scope, such
as The Language Archive (maintained by the
Max Planck Institute of Psycholinguistics), Col-
lection Pangloss (maintained by LACITO), and
The Endangered Languages Archive (maintained
by SOAS). Most archives are regional, including
AILLA, ANLA, PARADISEC, and many others.
However, there are two main problems common
to all of the above data sources. Firstly, the data
2
All figures given below were correct at the time of writ-
ing, but it must be borne in mind that most of these resources
are constantly growing.
is not always machine readable. Even where the
data is available digitally, these often take the form
of scanned images or audio files. While both can
provide invaluable information, they are extremely
difficult to process with a computer, requiring an
impractical level of image or video pre-processing
before linguistic analysis can begin. Even textual
data, which avoids these issues, may not be avail-
able in a machine-readable form, being stored as
pdfs or other opaque formats. Secondly, when data
is machine readable, the format can vary wildly.
This makes automated processing difficult, espe-
cially if one is not aware of the details of each
project. Even when metadata standards and en-
codings agree, there can be idiosyncractic markup
or non-linguistic information, such as labels for
speakers in the transcript of a conversation.
We can see that there is still much work to be
done by individual researchers in digitising and
standardising linguistic data, and it is outside of
the scope of this paper to attempt this for the above
archives. Guidelines for producing new materi-
als are available from the E-MELD project (Elec-
tronic Metastructure for Endangered Languages
Data), which specifically aimed to deal with the
expanding number of standards for linguistic data.
It gives best practice recommendations, illustrated
with eleven case studies, and provides input tools
which link to the GOLD ontology language, and
the OLAC metadata set. Further recommenda-
tions are given by Bird and Simons (2003), who
describe seven dimensions along which the porta-
bility of linguistic data can vary. Various tools are
available from The Language Archive at the Max
Planck Institute for Psycholinguistics.
Many archives are part of the Open Language
Archive Community (OLAC), a subcommunity
of the Open Archives Initiative. OLAC main-
tains a metadata standard, based on the 15-element
Dublin Core, which allows a user to search
through all participating archives in a unified fash-
ion. However, centralising access to disparate re-
sources, while of course extremely helpful, does
not solve the problem of inconsistent standards.
Indeed, it can even be hard to answer simple ques-
tions like ?how many languages are represented??
In short, while traditional archives are invalu-
able for many purposes, for large-scale machine
processing, they leave much to be desired.
Generic corpus collections. Some corpus col-
lections exist which do not focus on endangered
78
languages, but which nonetheless cover an in-
creasing number of languages.
MetaShare (Multilingual Europe Technology
Alliance) provides data in a little over 100 lan-
guages. While language codes are used, they have
not been standardised, so that multiple codes are
used for the same language. Linguistic Data Con-
sortium (LDC) and the European Language Re-
sources Association (ELRA) both offer data in
multiple languages. However, while large in size,
they cover only a limited number of languages.
Furthermore, the corpora they contain are stored
separately, making it difficult to access data ac-
cording to language.
Parallel corpora. The Machine Translation
community has assembled a number of parallel
corpora, which are crucial for statistical machine
translation. The OPUS corpus (Tiedemann, 2012)
subsumes a number of other well-known parallel
corpora, such as Europarl, and covers documents
from 350 languages, with various language pairs.
Web corpora. There has been increasing inter-
est in deriving corpora from the web, due to the
promise of large amounts of data. The majority
of web corpora are however aimed at either one or
a small number of languages, which is perhaps to
be expected, given that the majority of online text
is written in a handful of high-resource languages.
Nonetheless, there have been a few efforts to apply
the same methods to a wider range of languages.
HC Corpora currently provides download of
corpora in 68 different language varieties, which
vary in size from 2M to 150M words. The cor-
pora are thus of a respectable size, but only 1% of
the world?s languages are represented. A further
difficulty is that languages are named, without the
corresponding ISO language codes.
The Leipzig Corpora Collection (LCC)
3
(Bie-
mann et al., 2007) provides download of corpora
in 117 languages, and dictionaries in a number of
others, bringing the total number of represented
languages up to 230. The corpora are large, read-
ily available, in plain-text, and labelled with ISO
language codes.
The Cr?ubad?an Project aims to crawl the web for
text in low-resource languages, and data is cur-
rently available for 1872 languages. This rep-
resents a significant portion of the world?s lan-
guages; unfortunately, due to copyright restric-
3
http://corpora.uni-leipzig.de
tions, only lists of n-grams and their frequencies
are publically available, not the texts themselves.
While the breadth of languages covered makes this
a useful resource for cross-linguistic research, the
lack of actual texts means that only a limited range
of applications are possible with this data.
Cross-linguistic projects. Responding to the
call to document and preserve the world?s lan-
guages, highly cross-linguistic projects have
sprung up, striving towards the aim of universality.
Of particular note are the Endangered Languages
Project, and the Rosetta Project. These projects
are to be praised for their commitment to univer-
sality, but in their current forms it is difficult to use
their data to perform large-scale NLP.
3 The Data
3.1 Universal Corpus and Data Structure
Building on their previous paper, Abney and Bird
(2011) describe the data structure they envisage
for a Universal Corpus in more detail, and we aim
to adopt this structure where possible. Two types
of text are distinguished:
Aligned texts consist of parallel documents,
aligned at the document, sentence, or word level.
Note that monolingual documents are viewed as
aligned texts only tied to a single language.
Analysed texts, in addition to the raw text, con-
tain more detailed annotations including parts of
speech, morphological information, and syntactic
relations. This is stored as a table, where rows rep-
resent words, and columns represent: document
ID, language code, sentence ID, word ID, word-
form, lemma, morphological information, part of
speech, gloss, head/governor, and relation/role.
Out of our data sources, three can be straight-
forwardly represented in the aligned text struc-
ture. However, ODIN contains richer annotations,
which are in fact difficult to fit into Abney and
Bird?s proposal, and which we discuss in section
3.2 below.
3.2 Data Sources
Although data size matters in general NLP, uni-
versality is the top priority for a Universal Corpus.
We focus on the following data sources, because
they include a large number of languages, include
several parallel texts, and demonstrate a variety of
data types which a linguist might encounter (struc-
tured, semi-structured, unstructured): the Online
79
Langs. Families Tokens Size
ODIN 1,270 100 351,161 39 MB
Omniglot 129 20 31,318 677 KB
UDHR 352 46 640,588 5.2 MB
Wikipedia 271 21 37 GB
Combined 1,451 105
Table 1: Corpus Coverage
Database of Interlinear Text (ODIN), the Om-
niglot website, the Universal Declaration of Hu-
man Rights (UHDR), and Wikipedia.
Our resulting corpus runs the full gamut of text
types outlined by Abney and Bird, ranging from
single-language text (Wikipedia) to parallel text
(UDHR and Omniglot) to IGTs (ODIN). Table 1
gives some coverage statistics, and we describe
each source in the following subsections. For 332
languages, the corpus contains data from more
than one source.
Universal Declaration of Human Rights. The
Universal Declaration of Human Rights (UDHR)
is a document released by the United Nations in
1948, and represents the first global expression of
human rights. It consists of 30 articles, amounting
to about four pages of text. This is a useful doc-
ument for NLP, since it has been translated into a
wide variety of languages, providing a highly par-
allel text.
Wikipedia. Wikipedia is a collaboratively-
edited encyclopedia, appealing to use for NLP
because of its large size and easy availability.
At the time of writing, it contained 30.8 million
articles in 286 languages, which provides a
sizeable amount of monolingual text in a fairly
wide range of languages. Text dumps are made
regularly available, and can be downloaded from
http://dumps.wikimedia.org.
Omniglot. The Omniglot website
4
is an online
encyclopedia of writing systems and languages.
We extract information from pages on ?Useful for-
eign phrases? and the ?Tower of Babel? story, both
of which give us parallel data in a reasonably large
number of languages.
ODIN. ODIN (The Online Database of Inter-
linear Text) is a repository of interlinear glossed
texts (IGTs) extracted from scholarly documents
(Lewis, 2006; Lewis and Xia, 2010). Compared to
other resources, it is notable for the breadth of lan-
4
http://www.omniglot.com
guages included and the level of linguistic annota-
tion. An IGT canonically consists of three lines:
(i) the source, a sentence in a target language, (ii)
the gloss, an analysis of each source element, and
(iii) the translation, done at the sentence level. The
gloss line can additionally include a number of lin-
guistic terms, which means that the gloss is written
in metalanguage rather than natural language. In
ODIN, translations are into English, and glosses
are written in an English-based metalanguage. An
accepted set of guidelines are given by the Leipzig
Glossing Rules,
5
where morphemes within words
are separated by hyphens (or equal signs, for cli-
tics), and the same number of hyphens should ap-
pear in each word of the source and gloss.
The data from ODIN poses the first obstacle to
straightforwardly adopting Abney and Bird?s pro-
posal. The suggested data structure is aligned at
the word level, and includes a specific list of rel-
evant features which should be used to annotate
words. When we try to adapt IGTs into this for-
mat, we run into certain problems. Firstly, there
is the problem that the most fundamental unit of
analysis according to the Leipzig Glossing Rules
is the morpheme, not the word. Ideally, we should
encode this information explicitly in a Universal
Corpus, assigning a unique identifier to each mor-
pheme (instead of, or in addition to each word).
Indeed, Haspelmath (2011) argues that there is no
cross-linguistically valid definition of word, which
undermines the central position of words in the
proposed data structure.
Secondly, it is unclear how to represent the
gloss. Since the gloss line is not written in a natu-
ral language, we cannot treat it as a simple trans-
lation. However, it is not straightforward to incor-
porate it into the proposed structure for analysed
texts, either. One possible resolution is to move
all elements of the gloss written in capital letters to
the MORPH field (as functional elements are usu-
ally annotated in this way), and all remaining el-
ements to the GLOSS field. However, this loses
information, since we no longer know which mor-
pheme has which meaning. To keep all informa-
tion encoded in the IGT, we need to modify Abney
and Bird (2011)?s proposal.
The simplest solution we can see is to allow
morphemes to be a level of structure in the Uni-
versal Corpus, just as documents, sentences, and
5
http://www.eva.mpg.de/lingua/
resources/glossing-rules.php
80
Figure 1: Heatmap of languages in SeedLing according to endangerment status
words already are. The overall architecture re-
mains unchanged. We must then decide how to
represent the glosses.
Even though glosses in ODIN are based on
English, having been extracted from English-
language documents, this is not true of IGTs in
general. For example, it is common for documen-
tary linguists working on indigenous languages of
the Americas to provide glosses and translations
based on Spanish. For this reason, we believe it
would be wise to specify the language used to pro-
duce the gloss. Since it is not quite the language
itself, but a metalanguage, one solution would be
to use new language codes that make it clear both
that a metalanguage is being used, and also what
natural language it is based on. The five-letter
code gloss cannot be confused with any code
in any version of ISO 639 (with codes of length
two to four). Following the convention that sub-
varieties of a language are indicated with suffixes,
we can append the code of the natural language.
For example, glosses into English and Spanish-
based metalanguages would be given the codes
gloss-eng and gloss-spa, respectively.
One benefit of this approach is that glossed texts
are treated in exactly the same way as parallel
texts. There is a unique identifier for each mor-
pheme, and glosses are stored under this identifier
and the corresponding gloss code. Furthermore,
to motivate the important place of parallel texts in
a Universal Corpus, Abney and Bird view trans-
lations into a high-resource reference language as
a convenient surrogate of meaning. By the same
reasoning, we can use glosses to provide a more
detailed surrogate of meaning, only written in a
metalanguage instead of a natural one.
3.3 Representation and Universality
According to Ethnologue, there are 7105 liv-
ing languages, and 147 living language families.
Across all our data sources, we manage to cover
1451 languages in 105 families, which represents
19.0% of the world?s languages. To get a bet-
ter idea of the kinds of languages represented,
we give a breakdown according to their EGIDS
scores (Expanded Graded Intergenerational Dis-
ruption Scale) (Lewis and Simons, 2010) in Fig-
ure 1. The values in each cell have been colored
according to proportion of languages represented,
with green indicating good coverage and red poor.
It?s interesting to note that vigorous languages (6a)
are poorly represented across all data sources, and
worse than more endangered categories. In terms
of language documentation, vigorous languages
are less urgent goals than those in categories 6b
and up, but this highlights an unexpected gap in
linguistic resources.
4 Data Clean-Up, Consistency, and
Standardisation
Consistency in data structures and formatting is
essential to facilitate use of data in computational
linguistics research (Palmer et al., 2010). In the
following subsections, we describe the process-
ing required to convert the data into a standardised
form. We then discuss standardisation of language
codes and file formats.
81
4.1 Case Studies
UDHR. We used the plain-text UDHR files
available from the Unicode website
6
which uses
UTF-8 encoding for all languages. The first four
lines of each file record metadata, and the rest is
the translation of the UDHR. This dataset is ex-
tremely clean, and simply required segmentation
into sentences.
Wikipedia. One major issue with using the
Wikipedia dump is the problem of separating text
from abundant source-specific markup. To con-
vert compressed Wikipedia dumps to textfiles, we
used the WikiExtractor
7
tool. After conversion
into textfiles, we used several regular expressions
to delete residual Wikipedia markup and so-called
?magic words?.
8
Omniglot. The main issue with extracting the
Omniglot data is that the pages are designed to
be human-readable, not machine-readable. Clean-
ing this data required parsing the HTML source,
and extracting the relevant content, which required
different code for the two types of page we con-
sidered (?Useful foreign phrases? and ?Tower of
Babel?). Even after automatic extraction, some
noise in the data remained, such as explanatory
notes given in parentheses, which are written in
English and not the target language. Even though
the total amount of data here is small compared to
our other sources, the amount of effort required
to process it was not, because of these idiosyn-
cracies. We expect that researchers seeking to
convert data from human-readable to machine-
readable formats will encounter similar problems,
but unfortunately there is unlikely to be a one-size-
fits-all solution to this problem.
ODIN. The ODIN data is easily accessible in
XML format from the online database
9
. Data
for each language is saved in a separate XML
file and the IGTs are encoded in tags of the form
<igt><example>...</example></igt>.
For example, the IGT in Figure 2 is represented
by the XML snippet in Figure 3.
The primary problem in extracting the data is a
lack of consistency in the IGTs. In the above ex-
6
http://unicode.org/udhr/d
7
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
8
http://en.wikipedia.org/wiki/Help:
Magic_words
9
http://odin.linguistlist.org/download
21 a. o lesu mai
2sg return here
?You return here.?
Figure 2: Fijian IGT from ODIN
<igt>
<example>
<line>21 a. o lesu mai</line>
<line>2sg return here</line>
<line>?You return here.?</line>
</example>
</igt>
Figure 3: Fijian IGT in ODIN?s XML format
amples, the sentence is introduced by a letter or
number, which needs to be removed; however, the
form of such indexing elements varies. In addi-
tion, the source line in Figure 4 includes two types
of metadata: the language name, and a citation,
both of which introduce noise. Finally, extrane-
ous punctuation such as the quotation marks in the
translation line need to be removed. We used regu-
lar expressions for cleaning lines within the IGTs.
4.2 Language Codes
As Xia et al. (2010) explain, language names do
not always suffice to identify languages, since
many names are ambiguous. For this reason, sets
of language codes exist to more accurately identify
languages. We use ISO 639-3
10
as our standard set
of codes, since it aims for universal coverage, and
has widespread acceptance in the community. The
data from ODIN and the UDHR already used this
standard.
To facilitate the standardization of language
codes, we have written a python API that can be
used to query information about a language or a
code, fetching up-to-date information from SIL
International (which maintains the ISO 639-3 code
set), as well as from Ethnologue.
Wikipedia uses its own set of language codes,
most of which are in ISO 639-1 or ISO 639-3.
The older ISO 639-1 codes are easy to recognise,
being two letters long instead of three, and can
be straightforwardly converted. However, a small
number of Wikipedia codes are not ISO codes at
all - we converted these to ISO 639-3, following
10
http://www-01.sil.org/iso639-3/
default.asp
82
<igt>
<example>
<line>(69) na-Na-tmi-kwalca-t
Yimas (Foley 1991)</line>
<line>3sgA-1sgO-say-rise-PERF
</line>
<line>?She woke me up?
(by verbal action)</line>
</example>
</igit>
Figure 4: Yimas IGT in ODIN?s XML format
documentation from the Wikimedia Foundation.
11
Omniglot does not give codes at all, but only the
language name. To resolve this issue, we automat-
ically converted language names to codes using in-
formation from the SIL website.
Some languages have more than one orthog-
raphy. For example, Mandarin Chinese is writ-
ten with either traditional or simplified charac-
ters; Serbian is written with either the Cyrillic or
the Roman alphabet. For cross-linguistic NLP, it
could be helpful to have standard codes to identify
orthographies, but at present none exist.
4.3 File Formats
It is important to make sure that the data we have
compiled will be available to future researchers,
regardless of how the surrounding infrastructure
changes. Bird and Simons (2003) describe a set of
best practices for maintaining portability of digi-
tal information, outlining seven dimensions along
which this can vary. Following this advice, we
have ensured that all our data is available as plain-
text files, with UTF-8 encoding, labelled with the
relevant ISO 639-3 code. Metadata is stored sepa-
rately. This allows users to easily process the data
using the programming language or software of
their choice.
To allow access to the data following Abney
and Bird?s guidelines, as discussed in section 3,
we have written an API, which we distribute along
with the data. Abney and Bird remain agnostic
to the specific file format used, but if an alterna-
tive format would be preferred, the data would
be straightfoward to convert since it can be ac-
cessed according to these guidelines. As exam-
ples of functionality, our API allows a user to fetch
all sentences in a given language, or all sentences
from a given source.
11
http://meta.wikimedia.org/wiki/
Special_language_codes
5 Detecting Similar Languages
To exemplify the use of SeedLing for compu-
tational research on low-resource languages, we
experiment with automatic detection of similar
languages. When working on endangered lan-
guages, documentary and computational linguists
alike face a lack of resources. It is often helpful to
exploit lexical, syntactic or morphological knowl-
edge of related languages. For example, similar
high-resource languages can be used in bootstrap-
ping approaches, such as described by Yarowsky
and Ngai (2001) or Xia and Lewis (2007).
Language classification can be carried out in
various ways. Two common approaches are ge-
nealogical classification, mapping languages onto
family trees according to their historical related-
ness (Swadesh, 1952; Starostin, 2010); and ty-
pological classification, grouping languages ac-
cording to linguistic features (Georgi et al., 2010;
Daum?e III, 2009). Both of these approaches re-
quire linguistic analysis. By contrast, we use
surface features (character n-gram and word uni-
gram frequencies) extracted from SeedLing, and
apply an off-the-shelf hierarchical clustering al-
gorithm.
12
Specifically, each language is repre-
sented as a vector of frequencies of character bi-
grams, character trigrams, and word unigrams.
Each of these three components is normalised to
unit length. Data was taken from ODIN, Om-
niglot, and the UDHR.
Experimental Setup. We first perform hierar-
chical clustering, which produces a tree structure:
each leaf represents a language, and each node
a cluster. We use linkage methods, which recur-
sively build the tree starting from the leaves. Ini-
tially, each language is in a separate cluster, then
we iteratively find the closest two clusters and
merge them. Each time we do this, we take the
two corresponding subtrees, and introduce a new
node to join them.
We define the distance between two clusters by
considering all possible pairs of languages, with
one from each cluster, and taking the largest dis-
tance. We experimented with other ways to de-
fine the distance between clusters, but results were
poor and we omit results for brevity.
To ease evaluation, we produce a partitional
clustering, by stopping when we reach a certain
number of clusters, set in advance.
12
http://www.scipy.org
83
Precision Recall F-score
SeedLing 0.255 0.205 0.150
Base. 1: random 0.184 0.092 0.068
Base. 2: together 0.061 1.000 0.112
Base. 3: separate 1.000 0.086 0.122
Table 2: Clustering compared with baselines
Figure 5: Performance against number of clusters
Evaluation. We compare our clustering to the
language families in Ethnologue. However, there
are many ways to evaluate clustering quality.
Amig?o et al. (2009) propose a set of criteria which
a clustering evaluation metric should satisfy, and
demonstrate that most popular metrics fail to sat-
isfy at least one of these criteria. However, they
prove that all criteria are satisfied by the BCubed
metric, which we therefore adopt. To calculate the
BCubed score, we take the induced cluster and
gold standard class for each language, and cal-
culate the F-score of the cluster compared to the
class. These F-scores are then averaged across all
languages.
In Table 2, we set the number of clusters to be
105, the number of language families in our data,
and compare this with three baselines: a random
baseline (averaged over 20 runs); putting all lan-
guages in a single cluster; and putting each lan-
guage in a separate cluster. Our clustering outper-
forms all baselines. It is worth noting that pre-
cision is higher than recall, which is perhaps ex-
pected, given that related languages using wildly
differing orthographies will appear distinct.
To allow a closer comparison with Georgi et al.
(2010), we calculate pairwise scores - i.e. consid-
ering if pairs of languages are in the same cluster
or the same class. For 105 clusters, we achieve
a pairwise f-score of 0.147, while Georgi et al.
report 0.140. The figures are not quite compa-
rable since we are evaluating over a different set
of languages; nonetheless, we only use surface
features, while Georgi et al. use typological fea-
tures from WALS. This suggests the possibility for
cross-linguistic research to be conducted based on
shallow features.
In Figure 5, we vary the number of clusters. The
highest f-score is obtained for 199 clusters. There
is a notable jump in performance between 98 and
99, just before the true number of families, 105.
Interpreting the clusters directly is difficult, be-
cause they are noisy. However, the distribution of
cluster sizes mirrors the true distribution - for 105
clusters, we have 48 clusters of size 1 or 2, with
the largest cluster of size 130; while in our gold
standard, there are 51 families with only 1 or 2
languages in the data, with the largest of size 150.
6 Conclusion and Outlook
In this paper, we have described the creation of
SeedLing, a foundation text for a Universal Cor-
pus, following the guidelines of Abney and Bird
(2010; 2011). To do this, we cleaned and standard-
ised data from several multilingual data sources:
ODIN, Omniglot, the UDHR, Wikipedia. The
resulting corpus is more easily machine-readable
than any of the underlying data sources, and has
been stored according to the best practices sug-
gested by Bird and Simons (2003). At present,
SeedLing has data from 19% of the world?s liv-
ing languages, covering 72% of language families.
We believe that a corpus with such diversity of lan-
guages, uniformity of format, cleanliness of data,
and ease of access provides an excellent seed for a
Universal Corpus. It is our hope that taking steps
toward creating this resource will spur both further
data contributions and interesting computational
research with cross-linguistic or typological per-
spectives; we have here demonstrated SeedLing?s
utility for such research by using the data to per-
form language clustering, with promising results.
SeedLing (data, API and documentation) is cur-
rently available via a GitHub repository.
13
We
have yet to fully address questions of long-term
access, and we welcome ideas or collaborations
along these lines.
13
https://github.com/alvations/SeedLing
84
Acknowledgements
We thank the three anonymous reviewers for their
helpful comments. This research was supported
in part by the Cluster of Excellence ?Multi-modal
Computing and Interaction? in the German Excel-
lence Initiative.
References
Steven Abney and Steven Bird. 2010. The Hu-
man Language Project: Building a Universal Cor-
pus of the world?s languages. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 88?97. Association for
Computational Linguistics.
Steven Abney and Steven Bird. 2011. Towards a data
model for the Universal Corpus. In Proceedings of
the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 120?127. Association for Computational Lin-
guistics.
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information retrieval, 12(4):461?486.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig Corpora
Collection-monolingual corpora of standard size.
Proceedings of Corpus Linguistic 2007.
Steven Bird and Gary Simons. 2003. Seven dimen-
sions of portability for language documentation and
description. Language, pages 557?582.
Hal Daum?e III. 2009. Non-parametric bayesian areal
linguistics. In Proceedings of human language tech-
nologies: The 2009 annual conference of the north
american chapter of the association for computa-
tional linguistics, pages 593?601. Association for
Computational Linguistics.
Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, pages 385?393. Association for
Computational Linguistics.
Martin Haspelmath. 2011. The indeterminacy of word
segmentation and the nature of morphology and syn-
tax. Folia Linguistica, 45(1):31?80.
M Paul Lewis and Gary F Simons. 2010. Assessing
endangerment: expanding fishman?s GIDS. Revue
roumaine de linguistique, 2:103?119.
William D Lewis and Fei Xia. 2010. Developing
ODIN: A multilingual repository of annotated lan-
guage data for hundreds of the world?s languages.
Literary and Linguistic Computing, 25(3):303?319.
William D Lewis. 2006. ODIN: A model for adapt-
ing and enriching legacy infrastructure. In e-Science
and Grid Computing, 2006. e-Science?06. Second
IEEE International Conference on, pages 137?137.
IEEE.
Mike Maxwell and Baden Hughes. 2006. Frontiers in
linguistic annotation for lower-density languages. In
Proceedings of the workshop on frontiers in linguis-
tically annotated corpora 2006, pages 29?37. Asso-
ciation for Computational Linguistics.
Alexis Palmer, Taesun Moon, Jason Baldridge, Katrin
Erk, Eric Campbell, and Telma Can. 2010. Compu-
tational strategies for reducing annotation effort in
language documentation. Linguistic Issues in Lan-
guage Technology, 3.
Gary F Simons and M Paul Lewis. 2011. The world?s
languages in crisis: A 20-year update. In 26th
Linguistic Symposium: Language Death, Endanger-
ment, Documentation, and Revitalization. Univer-
sity of Wisconsin, Milwaukee, pages 20?22.
George Starostin. 2010. Preliminary lexicostatistics as
a basis for language classification: a new approach.
Journal of Language Relationship, 3:79?117.
Morris Swadesh. 1952. Lexico-statistic dating of pre-
historic ethnic contacts: with special reference to
north american indians and eskimos. Proceedings of
the American philosophical society, pages 452?463.
J?org Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In LREC, pages 2214?2218.
Fei Xia and William D Lewis. 2007. Multilingual
structural projection across interlinear text. In HLT-
NAACL, pages 452?459.
Fei Xia, Carrie Lewis, and William D Lewis. 2010.
The problems of language identification within
hugely multilingual data sets. In LREC.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of NAACL-2001, pages 200?207.
85
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 201?206,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Manawi: Using Multi-Word Expressions and Named Entities to Improve
Machine Translation
Liling Tan and Santanu Pal
Applied Linguistics, Translation and Interpretation Department
Universit?at des Saarlandes
liling.tan@uni-saarland.de
santanu.pal@uni-saarland.de
Abstract
We describe the Manawi
1
(mAnEv) sys-
tem submitted to the 2014 WMT transla-
tion shared task. We participated in the
English-Hindi (EN-HI) and Hindi-English
(HI-EN) language pair and achieved 0.792
for the Translation Error Rate (TER)
score
2
for EN-HI, the lowest among the
competing systems. Our main innova-
tions are (i) the usage of outputs from
NLP tools, viz. billingual multi-word ex-
pression extractor and named-entity rec-
ognizer to improve SMT quality and (ii)
the introduction of a novel filter method
based on sentence-alignment features. The
Manawi system showed the potential of
improving translation quality by incorpo-
rating multiple NLP tools within the MT
pipeline.
1 Introduction
In this paper, we present Saarland University
(USAAR) submission to Workshop for Machine
Translation 2014 (WMT 2014) using the Manawi
MT system. We participated in the generic trans-
lation shared task for the English-Hindi (EN-HI)
and Hindi-English (HI-EN) language pairs.
Our Manawi system showcased the incorpora-
tion of NLP tools output within the MT pipeline; a
bilingual MWE extractor and a bilingual NE rec-
ognizer for English and Hindi were implemented.
The output from these NLP tools was appended to
the training corpus prior to the SMT model train-
ing with the MOSES toolkit (Koehn et al., 2007).
The resulting system achieves the lowest Transla-
tion Error Rate (TER) among competing systems
for the English-Hindi language pair.
1
Multi-word expression And Named-entity And
Wikipedia titles (Manawi)
2
Lower TER often results in better translation
The rest of the paper is structured as follow:
Section 2 describes the implementation of the NLP
tools; Section 3 outlines the corpus pre-processing
before the MT training process; Section 4 de-
scribes the MT system setup; Section 5 describes
a simple post-processing component to handle
Out-Of-Vocabulary words; Section 6 presents the
WMT shared task results for the Manawi system
and Section 6 concludes the paper.
2 NLP Tools Implementation
2.1 Bilingual MWE in MT
Multi-Word Expressions (MWE) are defined as
?idiosyncratic interpretations that cross word
boundaries? (Sag et al., 2002). MWE can be made
up of collocations (e.g. seem ridiculous : behuda
dikhai), frozen expressions (e.g. exception han-
dling : apavada sancalaka) or name entities (e.g.
Johnny Cash : Johni Kesh). Jackendoff (1997)
claims that the frequency of MWE and the fre-
quency of single words in a speaker?s lexicon are
almost equivalent.
Bilingual MWE has shown to be useful for
a variety of NLP applications such as multilin-
gual information retrieval (Vechtomova, 2005)
and Crosslingual/Multilingual Word Sense Dis-
ambiguation (Tan and Bond, 2013; Finlayson and
Kulkarni, 2011). For machine translation, vari-
ous studies had introduced bilingual MWE to im-
prove MT system performance. Lambert (2005)
introduced bilingual MWE by grouping them as
a single token before training alignment models
and they showed that it improved alignment and
translation quality. Ren et al. (2009) integrated
an in-domain bilingual MWE using log likelihood
ratio based hierarchical reducing algorithm and
gained +0.61 BLEU score. Similarly, Santanu et
al. (2010) single tokenized MWE before training a
phrase-based SMT model and achieved 50% im-
provement in BLEU score.
201
In order to improve the word alignment quality,
Venkatapathy and Joshi (2006) reported a discrim-
inative approach to use the compositionality infor-
mation of verb-based multi-word expressions. Pal
et al. (2011) discussed the effects of incorporating
prior alignment of MWE and NEs directly or indi-
rectly into Phrase-based SMT systems.
2.2 Bilingual MWE Extraction
Monolingual MWE extraction revolves around
three approaches (i) rule-based methods relying
on morphosyntactic patterns, (ii) statistical meth-
ods which use association/frequency measures to
determine ngrams as MWE and (iii) hybrid ap-
proaches that combine the rule-based and statis-
tical methods.
However, where bilingual MWE extraction
techniques are concerned, they operate around
two main modus operandi (i) extracting mono-
lingual MWE separately and aligning them at
word/phrasal level afterwards or (ii) aligning par-
allel text at word/phrasal level and then extracting
MWE.
We implemented a language independent bilin-
gual MWE extractor, (Muwee), that produces a
parallel dictionary of MWE without the need for
any word/phrasal-level alignment. Muwee makes
use of the fact that the number of highly collocated
MWE should be the same for each sentences pair.
Muwee first extracts MWE separately from the
source and target sentences; the MWE are ex-
tracted based on bigrams that reports a Point-
wise Mutual Information (PMI) score of above
10. Then for each parallel sentence, if the number
of MWE are equivalent for the source and target,
the bigrams are joint together as a string and con-
tiguous duplicate words are deleted. The removal
of contiguous duplicate words is grounded on the
fact that linguistically motivated MWE that forms
grammatical phrases had shown to improve SMT
performances (Pal et al., 2013). Figure 1 presents
an example of the MWE extraction process.
Figure 1: Muwee Extraction Process
2.3 Named-entity Recognition
Named-Entity (NE) recognition is the task of iden-
tifying entities such as names of people, organi-
zations and locations. Given a perfect MWE ex-
traction system, NEs would have been captured by
MWE extraction. However, the state-of-art MWE
extractors have yet been perfected.
To compliment the MWE extracted by Muwee,
we implemented a bilingual NE extractor by
combining outputs from the (i) Stanford English
NE Recognizer (NER)
3
and (ii) a Do-It-Yourself
(DIY) Hindi NER using CRF++ toolkit
4
with an-
notated data from NER-SSEA 2008 shared task
(Rajeev Sangal and Singh, 2008). We trained a
Conditional Random Field classifier for the Hindi
NER using unigram features, bigram features and
a context window of two words to the left and to
the right. And we used the DIY Hindi NER and
Stanford NER tool to monolingually annotate the
NEs from training corpus for the EN-HI / HI-EN
language pair.
Similar to the Muwee bilingual extraction cri-
teria, if the number of NEs are the same on the
source and target language, the NEs were joint to-
gether as a string. We note that sometimes the
bilingual NER output contains more than one NE
per sentence. For example, our bilingual NER ex-
tractor outputs ?Kalpna Chawla Gurdeep Pand-
her?, which contains two NEs ?Kalpna Chawla?
and ?Gurdeep Pandher?. Although the resulting
bilingual NE does not provide a perfect NE dic-
tionary, it filters out NEs from the sentence and
improves word alignments at the start of the MT
pipeline.
3 Corpus Preprocessing
The performance of any data driven SMT depends
on the quality of training data. Previous stud-
ies had shown that filtering out low quality sen-
tence pairs improves the quality of machine trans-
lation. For instance, the Moore-Lewis filter re-
moves sentence pairs based on source-side cross-
entropy differences (Moore and Lewis, 2010) and
the Edinburgh?s MT system used the Modified
Moore-Lewis filtering (Axelrod et al., 2011) in
WMT 2013 shared task (Durrani et al., 2013).
CNGL-DCU system extended the Moore-Lewis
filter by incorporating lemmas and named enti-
3
http://nlp.stanford.edu/software/CRF-NER.shtml
4
http://crfpp.googlecode.com
202
ties in their definition of perplexity
5
(Rubino et al.,
2013; Toral, 2013).
The RWTH Aachen system filtered the Com-
mon Crawl Corpus by keeping only sentence pairs
that contains at least 70% of the word from a
known vocabulary dataset extracted from the other
corpora in the WMT 2013 shared task (Peitz et
al., 2013). The Docent system from Uppsala Uni-
versity also performed data cleaning on the Com-
mon Crawl dataset prior to SMT but they were
using more aggressive conditions by (i) remov-
ing documents that were identified correctly us-
ing a language identification module and (ii) re-
moving documents that falls below a threshold
value of alignment points and sentence length ra-
tio (Stymne et al., 2013). Our approach to data
cleaning is similar to the Uppsala?s system but in-
stead of capitalizing on word-alignments features,
we were cleaning the data based on sentence align-
ment features.
3.1 GaCha Filtering: Filter by Character
Mean Ratio
Stymne et al. (2013) improved translation qual-
ity by cleaning the Common Crawl corpus during
the WMT 2013 shared task. They filtered out doc-
uments exceeding 60 words and cleaned the re-
mainder of the corpus by exploiting the number
of alignment points in word alignments between
sentence pairs. Their hypothesis was that sentence
pairs with very few alignment points in the inter-
section would mostly likely not be parallel. This
is based on the fact that when using GIZA++ (Och
and Ney, 2003), the intersection of alignments is
more sparse than the standard SMT symmetriza-
tion heuristics like grow-diag-final-and (Koehn,
2005).
Different from Stymne et al., our hypothesis for
non-parallelness adheres to sentence level align-
ment criteria as defined in the Gale-Church algo-
rithm (Gale and Church, 1993). If a sentence pair
is parallel, the ratio of the number of characters in
the source and target sentence should be coherent
to the global ratio of the number of source-target
characters in a fully parallel corpus. The Gale-
Church algorithm had its parameters tuned to suit
European languages and Tan (2013) had demon-
strated that sentence-level alignments can be im-
proved by using corpus specific parameters. When
5
The exponent of cross-entropy may be regarded as per-
plexity
using variable parameters to the Gale-Church al-
gorithm, Tan showed that instead of the default
parameters set in the original Gale-Church algo-
rithm, using mean ratio of the noisy corpus can
also improve sentence level alignments although
the ratio from a clean corpus would achieve even
better alignments.
Given the premises of the sentence level align-
ment hypothesis, we clean the training corpus by
first calculating the global mean ratio of the num-
ber of characters of source sentence to target sen-
tence and then filter out sentence pairs that exceeds
or fall below 20% of the global ratio. We call this
method, GaCha filtering; this cleaning method is
more aggressive than cleaning methods described
by Stymne et al. but it filters out noisy sen-
tence level alignments created by non-language
specific parameters used by sentence aligners such
as Gale-Church algorithm.
3.2 Filtering Noise in HindEnCorp
After manual inspection 100 random sentence
pairs from the HindEnCorp (Bojar et al., 2014),
we found that documents were often misaligned
at sentence level or contains HTML special char-
acters. To further reduce the noise in the Hin-
dEnCorp, the Manawi system was only trained
a subset of the HindEnCorp from the follow-
ing sources (i) DanielPipes, (ii) TIDES and (iii)
EILMT. Lastly, we filtered the training data on al-
lowing a maximum of 100 tokens per language per
sentence.
Finally, the cleaned data contained 87,692 sen-
tences, only ?36% of the original HindEnCorp
training data.
4 System Setup
Data: To train the baseline translation model,
we have used the cleaned subset of the data as
described in Section 3. For the Manawi model,
we added the NLP outputs from the MWE and
NE extractors presented in Section 2. To train the
monolingual language model, we used the Hindi
sentences from the HindEnCorp.
System: We used the standard log-linear
Phrase based SMT model provided from the
MOSES toolkit.
Configuration: We experimented with various
maximum phrase length for the translation and n-
203
Manawi Submissions (EN-HI) BLEU BLEU TER
(cased)
PB-SMT + MWE + NE 9.9 7.1 0.869
PB-SMT + MWE + NE + Wiki (Manawi) 7.7 7.6 0.864
Manawi + GaCha Filter 8.9 8.9 0.818
Manawi + GaCha Filter + Handle OOV 8.8 8.8 0.800
Manawi + GaCha Filter + Remove OOV 8,9 8.8 0.792
Table 1: Manawi System Submissions @ WMT 2014 Translation Shared Task for English-Hindi
Manawi Submissions (HI-EN) BLEU BLEU TER
(cased)
PB-SMT + MWE + NE + Wiki (Manawi) 7.7 7.6 0.864
Manawi + GaCha Filter 8.9 8.9 0.818
Table 2: Manawi System Submissions @ WMT 2014 Translation Shared Task for Hindi-English
gram settings for the language model. And we
found that using a maximum phrase length of 5
and 4-gram language model produced best result
in terms of BLEU and TER for our baseline model
(i.e. without the incorporation of outputs from the
NLP tools). The other experimental settings were:
? GIZA++ implementation of IBM word align-
ment model 4 with grow-diagonal-final-and
heuristics for performing word alignment and
phrase-extraction (Koehn et al., 2003)
? Minimum Error Rate Training (MERT) (Och,
2003) on a held-out development set, target
language model with Kneser-Ney smoothing
(Kneser and Ney, 1995) using language mod-
els trained with SRILM (Stolcke, 2002)
? Reordering model
6
was trained on bidirec-
tional (i.e. using both forward and back-
ward models) and conditioned on both source
and target language. The reordering model
is built by calculating the probabilities of the
phrase pair being associated with the given
orientation.
Innovation: We demonstrated the incorporation
of multiple NLP tools outputs in the SMT pipline
by simply using automatically extracted bilingual
MWE and NEs as additional parallel data to the
cleaned data and ran the translation and statistical
model as per the baseline configurations.
6
For reordering we used lexicalized reordering model,
which consists of three different types of reordering by
conditioning the orientation of previous and next phrases-
monotone (m), swap (s) and discontinuous (d).
5 Post-processing
The MOSES decoder produces translations with
Out-Of-Vocabulary (OOV) words that were not
translated from the source language. The Manawi
system post-processed the decoder output by (i)
handling OOV words by replacing each OOV
word with the most probable translation using the
lexical files generated by GIZA++ and (ii) remov-
ing OOV words from the decoded outputs.
6 Results
Table 1 summarizes the Manawi system sub-
missions for the English-Hindi language pair for
WMT 2014 generic translation shared task. The
basic Manawi system is a Phrase-based SMT
(PB-SMT) setup using extracted MWE and NEs
and Wikipedia titles as additional parallel data (i.e.
PB-SMT+MWE+NE+Wiki in Table 1). The ba-
sic Manawi system achieved 7.7 BLEU score and
0.864 TER.
After filtering the data before training the trans-
lation model, the Manawi system performed bet-
ter at 8.9 BLEU and 0.818 TER. By adding the
post-processing component, we achieved the low-
est TER score among competing team at 0.792.
7 Conclusion
The Manawi system showed how simple yet ef-
fective pre-processing and integration of output
from NLP tools improves the performance of MT
systems. Using GaCha filtering to remove noisy
data and using automatically extracted MWE and
NEs as additional parallel data improve word and
phrasal alignments at the start of the MT pipeline
204
which eventually improves the quality of machine
translation. The best setup for the Manawi system
achieved the best TER score among the competing
system.
Also, the incremental improvements made by
step-wise implementation of (i) filtering, (ii) in-
corporating outputs from NLP tools and (iii) post-
processing showed that individual components of
the Manawi can be integrated into other MT sys-
tems without detrimental effects.
Acknowledgments
The research leading to these results has received
funding from the People Programme (Marie
Curie Actions) of the European Union?s Seventh
Framework Programme FP7/2007-2013/ under
REA grant agreement n
?
317471.
The authors of this paper also thank our col-
leagues J?org Knappen and Jos?e M.M. Mart??nez
for their help in setting up the server that made the
Manawi system possible.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 355?362. Association for Computational Lin-
guistics.
Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, Pavel
Stra?n?ak, Ale?s Tamchyna, and Dan Zeman. 2014.
Hindi-English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Interna-
tional Language Resources and Evaluation Confer-
ence (LREC?14), Reykjavik, Iceland, may. ELRA,
European Language Resources Association. in prep.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburghs machine transla-
tion systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 112?119.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of the Workshop on
Multiword Expressions: From Parsing and Gener-
ation to the Real World, MWE ?11, pages 20?24,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
William A Gale and Kenneth W Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational linguistics, 19(1):75?102.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT summit, 5:79?
86.
Patrik Lambert. 2005. Data inferred multi-word ex-
pressions for statistical machine translation. In In
MT Summit X.
Robert C Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Santanu Pal, Tanmoy Chakraborty, and Sivaji Bandy-
opadhyay. 2011. Handling multiword expressions
in phrase-based statistical machine translation. In In
Proceedings of the 13th Machine Translation Sum-
mit, pages 215?224. MT Summit 2011.
Santanu Pal, Mahammed Hasanuzzaman, Sudip Ku-
mar Naskar, and Sivaji Bandyopadhyay.
2013. Impact of linguistically motivated
shallow phrases in pb-smt. In ICON 2013
http://sivajibandyopadhyay.com/publications/Icon-
v1.3-camera.pdf. ICON 2013.
Stephan Peitz, Jan-Thorsten Peter Saab Mansour,
Christoph Schmidt, Joern Wuebker, Matthias Huck,
Markus Freitag, and Hermann Ney. 2013. The rwth
aachen machine translation system for wmt 2013. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 191?197.
Dipti Misra Sharma Rajeev Sangal and Anil Kumar
Singh, editors. 2008. Proceedings of the IJCNLP-
08 Workshop on Named Entity Recognition for South
and South East Asian Languages. Asian Federation
of Natural Language Processing, Hyderabad, India,
January.
Zhixiang Ren, Yajuan L?u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, MWE ?09, pages 47?
54, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
205
Raphael Rubino, Antonio Toral, S Cort?es Va?llo, Jun
Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu.
2013. The cngl-dcu-prompsit translation systems
for wmt13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 211?216.
Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 1?15. Springer Berlin Heidelberg.
Pal Santanu, Sudip Kumar Naskar, Pavel Pecina, Sivaji
Bandyopadhyay, and Andy Way. 2010. Handling
named entities and compound verbs in phrase-based
statistical machine translation. In 23rd International
Conference of Computational Linguistics (Coling
2010), Beijing, Chaina, pages 46?54.
Sara Stymne, Christian Hardmeier, J?org Tiedemann,
and Joakim Nivre. 2013. Tunable distortion lim-
its and corpus cleaning for smt. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 225?231.
Liling Tan and Francis Bond. 2013. Xling: Match-
ing query sentences to a parallel corpus using topic
models for word sense disambiguation.
Liling Tan. 2013. Gachalign: Gale-church sentence-
level alignments with variable parameters [soft-
ware]. Retrieved from https://db.tt/LLrul4zP and
https://code.google.com/p/gachalign/.
Antonio Toral. 2013. Hybrid selection of language
model training data using linguistic information and
perplexity. ACL 2013, page 8.
Olga Vechtomova. 2005. The role of multi-word units
in interactive information retrieval. In ECIR, pages
403?420.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for
the word-alignment task. In Proceedings of the
Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties, pages 20?27.
Association for Computational Linguistics.
206
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58?67,
Dublin, Ireland, August 23 2014.
A Report on the DSL Shared Task 2014
Marcos Zampieri
1
, Liling Tan
2
, Nikola Ljube
?
si
?
c
3
, J
?
org Tiedemann
4
Saarland University, Germany
1,2
University of Zagreb, Croatia
3
Uppsala University, Sweden
4
marcos.zampieri@uni-saarland.de, liling.tan@uni-saarland.de
jorg.tiedemann@lingfil.uu.se, nljubesi@ffzg.hr
Abstract
This paper summarizes the methods, results and findings of the Discriminating between Similar
Languages (DSL) shared task 2014. The shared task provided data from 13 different languages
and varieties divided into 6 groups. Participants were required to train their systems to discrimi-
nate between languages on a training and development set containing 20,000 sentences from each
language (closed submission) and/or any other dataset (open submission). One month later, a test
set containing 1,000 unidentified instances per language was released for evaluation. The DSL
shared task received 22 inscriptions and 8 final submissions. The best system obtained 95.7%
average accuracy.
1 Introduction
Discriminating between similar languages is one of the bottlenecks of state-of-the-art language iden-
tification systems. Although in recent years systems have been trained to discriminate between more
languages
1
, they still struggle to discriminate between similar languages such as Croatian and Serbian or
Malay and Indonesian.
From an NLP point of view, the difficulty systems face when discriminating between closely related
languages is similar to the problem of discriminating between standard national language varieties (e.g.
American English and British English or Brazilian Portuguese and European Portuguese), henceforth
varieties. Recent studies show that language varieties can be discriminated automatically using words or
characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance
limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from
different national varieties, modelling pluricentric languages as unique classes.
To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we
decided to organize the Discriminating between Similar Languages (DSL)
2
shared task. This shared task
was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial) in the 2014 edition of COLING.
The motivation behind the DSL shared task is two-fold. Firstly, we have observed an increase of
interest in the topic. This is reflected by a number of papers that have been published about this task in
recent years starting with Ranaivo-Malanc?on (2006) for Malay and Indonesian and Ljube?si?c et al. (2007)
for South Slavic languages. In the DSL shared task we tried to include (depending on the availability of
data) languages that have been studied in previous experiments, such as Croatian, English, Indonesian,
Malay, Portuguese and Spanish.
The second aspect that motivated us to organize this shared task is that, to our knowledge, no shared
task focusing on the discrimination of similar languages has been organized previously. The most sim-
ilar shared tasks to DSL are the DEFT 2010 shared task (Grouin et al., 2010), in which systems were
required to classify French journalistic texts with respect to their geographical location as well as the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Brown (2013) reports results on a system trained to recognize more than 1,100 languages
2
http://corporavm.uni-koeln.de/vardial/sharedtask.html
58
decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual
language identification shared task, a general-purpose language identification task containing data from
74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task
(Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11
different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the
native language of the writer of each text.
2 Related Work
Among the first studies to investigate the question of discriminating between similar languages is the
study published by Ranaivo-Malanc?on (2006). The author presents a semi-supervised model to dis-
tinguish between Indonesian and Malay, two closely related languages from the Austronesian family
represented in the DSL shared task. The study uses the frequency and rank of character trigrams derived
from the most frequent words in each language, lists of exclusive words, and the format of numbers
(Malay uses decimal points whereas Indonesian uses commas). The author compares the performance
of this method with the performance obtained by TextCat (Cavnar and Trenkle, 1994).
Ljube?si?c et al. (2007) proposed a computational model for the identification of Croatian texts in
comparison to Slovene and Serbian, reporting 99% recall and precision in three processing stages. The
approach includes a ?black list?, which increases the performance of the algorithm. Tiedemann and
Ljube?si?c (2012) improved this method and applied it to Bosnian, Croatian and Serbian texts. The study
reports significantly higher performance compared to general purpose language identification methods.
The methods applied to discriminate between texts from different language varieties and dialects are
similar to those applied to similar languages
3
. One of the methods proposed to identify language varieties
is by Huang and Lee (2008). This study presented a bag-of-words approach to classify Chinese texts from
the mainland and Taiwan with results of up to 92% accuracy.
Another study that focused on language varieties is the one published by Zampieri and Gebre (2012).
In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to
identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in
a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The
algorithm was later adapted to classify Spanish texts using not only the classical word and character
n-grams but also POS distribution (Zampieri et al., 2013).
The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminate
between texts from three different English varieties (Canadian, Australian and British) across different
domains. The authors state that the results obtained suggest that each variety contains characteristics that
are consistent across multiple domains, which enables algorithms to distinguish them regardless of the
data source.
Zaidan and Callison-Burch (2013) propose computational methods for the identification of Arabic
language varieties
4
using character and word n-grams. The authors built their own dataset using crowd-
sourcing and investigated annotators? behaviour, agreement and performance when manually tagging
instances with the correct label (variety).
3 Methods
In the following subsections we will describe the methodology adopted for the DSL shared task. Due to
the lack of comparable resources, the first decision we had to take was to create a dataset that could be
used in the shared task and also redistributed to be used in other experiments. We opted for the creation
of a corpus collection based on existing datasets as discussed in 3.1 (Tan et al., 2014).
Groups interested in participating in the DSL shared task had to register themselves in the shared
task website to receive the training and test data. Each group could participate in one or two types of
3
In the DSL shared task and in this paper we did not distinguish between language varieties and similar languages. More
on this discussion can be found in Clyne (1992) and Chamber and Trudgill (1998).
4
Zaidan and Callison-Burch (2013) use the terms ?varieties? and ?dialects? interchangeably whereas Lui and Cook (2013)
use the term ?national dialect? to refer to what previous work describes as ?national variety?.
59
submission as follows:
? Closed Submission: Using only the DSL corpus collection for training.
? Open Submission: Using any other dataset including or not the DSL collection for training.
In the open submission we did not make any distinction between systems using the DSL corpus col-
lection and those that did not. This is different from the types of submissions for the NLI shared task
2013. The NLI shared task offered proposed two types of open submissions: open submission 1 - any
dataset including the aforementioned TOEFL11 dataset (Blanchard et al., 2013) and open submission 2
- any dataset excluding TOEFL11.
For each of these submission types, participants were allowed to submit up to three runs, resulting in
a maximum of six runs in total (three closed submissions and three open submissions).
3.1 Data
As previously mentioned, we decided to compile our own dataset for the shared task. The dataset was
entitled DSL corpus collection and its compilation was motivated by the absence of a resource that
allowed us to evaluate systems on discriminating similar languages. The methods behind the compilation
of this collection and the preliminary baseline experiments are described in Tan et al. (2014).
The DSL corpus collection consists of 18,000 randomly sampled training sentences, 2,000 develop-
ment sentences and 1,000 test sentences for each language (or variety) containing at least 20 tokens
5
each.
The languages are presented in table 1 with their ISO 639-1 language codes
6
. For language varieties the
country code is appended to the ISO code (e.g. en-GB refers to the British variety of English).
Group Language/Variety Code
Bosnian bs
A Croatian hr
Serbian sr
Indonesian id
B Malay my
Czech cz
C Slovak sk
Brazilian Portuguese pt-BR
D European Portuguese pt-PT
Argentine Spanish es-AR
E Castilian Spanish es-ES
British English en-GB
F American English en-US
Table 1: Language Groups - DSL 2014 Shared Task
For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were
selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard
language, which is an important factor to be considered when working with language varieties. Other data
sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore
not suitable for the purpose of the shared task. A number of studies mentioned in the related work section
use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre,
2012)
Given what has been said in this section, we consider the collection to be a suitable comparable corpora
from this task, which was compiled to avoid bias in classification towards source, register and topics. The
5
We considered a token as orthographic units delimited by white spaces.
6
http://www.loc.gov/standards/iso639-2/php/English_list.php
60
DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in
the language/variety, the second column states its group and the last column refers to its language code.
7
3.1.1 Problems with Group F
There are no major problems to report regarding the organization of the shared task nor with the com-
pilation of the DSL corpus collection apart from some issues in the Group F data. The organizers and
a couple of teams participating in the shared task observed very poor performance when distinguishing
instances from group F (British English - American British). For example, the baseline experiments
described in Tan et al. (2014) report a very low 0.59 F-measure for Group F (the lowest score) and 0.84
for Group E (the second lowest score). Some of the teams asked human annotators to try to distinguish
the sentences manually and they concluded that some instances were probably misclassified.
We decided to look more carefully at the data and noticed that the instances were originally tagged
based on the websites (newspapers) that they were retrieved from and not the country of the original
publication. There are, however, many cases of cross citation and republication of texts that the original
data sources did not take into account (e.g. British texts that were later republished by an American
website). As the DSL is a corpus collection and manually checking all 20,000 training and development
instances per language was not feasible, we assumed that the original sources
8
from which the texts were
retrieved provided the correct country of origin. The assumption was correct for all language groups but
English.
To illustrate the issues above we present next some misclassified examples. Two particular cases raised
by the UMich team are the following:
(1) I think they can afford to give North another innings and some time in Shield cricket and take
another middle order batsman. (en-US)
(2) ATHENS, Ohio (AP) Albuquerque will continue its four-game series in Nashville Thursday night
when it takes on the Sounds behind starter Les Walrond (3-4, 4.50) against Gary Glover, who is
making his first Triple-A start after coming down from Milwaukee. (en-GB)
Example number one was tagged as American English because it was retrieved from the online edition
of The New York Times but it was in fact first published in Australia. The second example is a text
published by Associated Press describing an event that took place in Ohio, United States, but it was
tagged as British English because it was retrieved by the UK Yahoo! sports section.
Our solution was to exclude the language group F from the final scores and perform a manual check in
all its 1,000 test instances
9
, thus giving the chance to participants to train their algorithms on other data
sources (open submission).
3.2 Schedule
The DSL shared task spanned from March 20
th
when the training set was released, to June 6
th
when
participants could submit a paper (up to 10 pages) describing their system. We provided one month
between the release of the training and the test set. The schedule of the DSL shared task 2014 can be
seen below.
Event Date
Training Set Release March 20
th
, 2014
Test Set Release April 21
st
, 2014
Submissions Due April 23
rd
, 2014
Results Announced April 30
th
, 2014
Paper Submission June 6
th
, 2014
Table 2: DSL 2014 Shared Task Schedule
7
To obtain the data please visit: https://bitbucket.org/alvations/dslsharedtask2014
8
See Tan et al. (2014) for a complete description of the data sources of the DSL corpus collection.
9
Our manual check suggests that about 25% of the instances in the English dataset was likely to have been misclassified.
61
4 Results
This section summarises the results obtained by all participants of the shared task who submitted final
results.
10
The DSL shared task included 22 enrolled teams from different countries (e.g. Australia,
Estonia, Holland, Germany, United Kingdom and United States). From the 22 enrolled teams, eight of
them submitted their final results. Most of the groups opted to exclusively use the DSL corpus collection
and therefore participated solely in the closed submission track. Two of them compiled comparable
datasets and also participated in the open submission.
Given that the dataset contained misclassified instances, group F (English) was not taken into account
to compute the final shared task scores. In the next subsections we report results in terms of macro-
average F-measure and accuracy.
4.1 Closed Submission
Table 3 presents the best F-measure and Accuracy results obtained by the eight teams that submitted their
results for the closed submission track ordered by accuracy.
Team Macro-avg F-score Overall Accuracy
NRC-CNRC 0.957 0.957
RAE 0.947 0.947
UMich 0.932 0.932
UniMelb-NLP 0.918 0.918
QMUL 0.925 0.906
LIRA 0.721 0.766
UDE 0.657 0.681
CLCG 0.405 0.453
Table 3: Open Submission - Results
In the closed submissions, we observed a group of five teams whose systems (best runs) obtained re-
sults over 90% accuracy. This is comparable to what is described in the state-of-the-art literature for
discriminating similar languages and language varieties (Tiedemann and Ljube?si?c, 2012; Lui and Cook,
2013). These five teams submitted system descriptions that allowed us to look in more detail at successful
approaches for this task. System descriptions will be discussed in section 5.
Three of the eight teams obtained substantially lower scores, from 45.33% to 76.64% accuracy. These
three groups unfortunately did not submit system description papers. From our point of view, this would
create an interesting opportunity to look more carefully at the weaknesses of approaches that did not
obtain good results in this task.
4.2 Open Submission
Only two systems submitted results for the open submission track and their F-measure and Accuracy
results are presented in table 3.
Team Macro-avg F-score Overall Accuracy
UniMelb-NLP 0.878 0.880
UMich 0.858 0.859
Table 4: Closed Submission - Results
The UniMelb-NLP (Lui et al., 2014) group used data from different corpora such as the BNC, EU-
ROPARL and Open Subtitles whereas UMich (King et al., 2014) compiled journalistic corpora from dif-
ferent sizes for each language ranging from 695,597 tokens for Malay to 20,288,294 tokens for British
English.
10
Visit https://bitbucket.org/alvations/dslsharedtask2014/downloads/dsl-results.html
for more detail on the shared task results or at the aforementioned DSL shared task website.
62
Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP sub-
mission was outperformed by UMich system by about 1.5% accuracy in the closed submission, but in
the open submission they scored 2.1% better than UMich. This difference can be explained by investi-
gating these two factors: 1) the quality and amount of the collected training data; 2) the robustness of
the method to obtain correct predictions across different datasets and domains as previously discussed
by Lui and Cook (2013) for English varieties.
4.3 Accuracy per Language Group
In this subsection we look more carefully at the performance of systems in discriminating each class
within groups A to E. Table 5 presents the accuracy scores obtained per language group for each team
sorted alphabetically. The best score per group is displayed in bold.
CLCG LIRA NRC-CNRC QMUL RAE UDE UMich UniMelb-NLP
A 0.338 0.333 0.936 0.879 0.919 0.785 0.919 0.915
B 0.503 0.982 0.996 0.935 0.994 0.892 0.992 0.972
C 0.500 1.000 1.000 0.962 1.000 0.493 0.999 1.000
D 0.496 0.892 0.956 0.905 0.948 0.493 0.926 0.896
E 0.503 0.843 0.910 0.865 0.888 0.694 0.876 0.807
Table 5: Language Groups A to E - Accuracy Results
The top 5 systems plus the LIRA team obtained very good results for groups B (Malay and Indonesian)
and C (Czech and Slovak). Four out of eight systems obtained perfect performance when discriminating
Czech and Slovak texts. Perfect performance was not achieved by any of the systems when distinguishing
Malay from Indonesian texts, but even so, results were fairly high and the best result was 99.6% accuracy
obtained by the NRC-CNRC group. The perfect results obtained by four groups when distinguishing
texts from group C suggest that Czech and Slovak texts are not as similar as we assumed before the
shared task, and that they therefore possess strong systemic and/or orthographic differences that allow
well-trained classifiers to perform perfectly. Figure 1 presents the accuracy results of the top 5 groups.
Figure 1: Language Groups A to E Accuracy - Top 5 Systems
Distinguishing between languages from group A (Bosnian, Croatian and Serbian), the only group con-
taining 3 languages, proved to be a challenging task as discussed in previous research (Ljube?si?c et al.,
2007; Tiedemann and Ljube?si?c, 2012). The best result was again obtained by the NRC-CNRC group
with 93.5% accuracy. The groups containing texts written in different language varieties, namely D (Por-
tuguese) and E (Spanish) were the most difficult to discriminate, particularly the Spanish varieties. These
results also corroborate the findings of previous studies (Zampieri et al., 2013).
63
The QMUL system that was the 5
th
best system in the closed submission track did not outperform any
of the other top 5 systems in groups A, B or C. However, the system did better when distinguishing texts
from the two most difficult language groups (D and E), outperforming the UniMelb-NLP submission on
two occasions. The simplicity of the approach proposed by the QMUL, which the author describes as
?a simple baseline? (Purver, 2014) may be an explanation for the regular performance across different
language groups.
4.4 Results Group F
To document the problems in the group F (British and American English) dataset we included the results
of both the open and closed submissions for this language group in table 6. As previously mentioned,
submitting group F results was optional and we did not include these results in the final shared task
results. Six out of eight systems decided to submit their predictions as closed submissions and the two
groups participating in the open submission track also submitted their group F results.
Team F-score Accuracy Type
UMich 0.639 0.639 Open
UniMelb- NLP 0.581 0.583 Open
NRC-CNRC 0.522 0.524 Closed
LIRA 0.450 0.493 Closed
RAE 0.451 0.481 Closed
UMich 0.463 0.464 Closed
UDE 0.451 0.451 Closed
UniMelb-NLP 0.435 0.435 Closed
Table 6: Group F - Accuracy Results
The results confirm the problems in the DSL dataset discussed in section 3.1.1. After a careful manual
check of the 1,000 test instances, open submissions scores were still substantially lower than the other
groups: 69.9% and 58.3% accuracy. Closed submissions proved to be impossible and only one of the six
systems scored slightly above the 50% baseline.
It should be investigated more carefully in future research whether the poor results for group F reflect
only the problems in the dataset or also the actual difficulty in discriminating between these two varieties
of English. Moderate differences in orthography (e.g. neighbour (UK) and neighbor (US)) as well
as lexical choices (e.g. rubbish (UK) and garbage (US) or trousers (UK) and pants (US)) are present
in texts from these two varieties and these can be informative features for algorithms to discriminate
between them. Discriminating between other English varieties already proved to be a challenging yet
feasible task in previous research (Lui and Cook, 2013).
5 System Descriptions
All eight systems that submitted their final results to the shared task were invited to submit papers de-
scribing their systems and the top 5 systems in the closed track submitted their papers, namely: NRC-
CNRC, RAE, UMich, UniMelb-NLP and QMUL.
The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a two-
step approach to predict first the language group than the language of each instance. The language group
was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier,
and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F
and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian).
An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called
?white lists? inspired by the ?blacklist? classifier (Tiedemann and Ljube?si?c, 2012). These lists are word
lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc?on (2006)
proposed to discriminate between Malay and Indonesian.
64
Two groups used Information Gain (IG) to select the best features for classification, namely UMich
(King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit
open submissions. The UniMelb-NLP team tried different classification methods and features (including
delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf
general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has
been widely used for general-purpose language identification and its performance is regarded superior
to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hier-
archically firstly identifying the language group that a sentence belongs to and subsequently the specific
language, achieving performance comparable to the state-of-the-art, but still slightly below the other
three systems.
The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as fea-
tures. The author investigated the influence of the cost parameter c (from 1.0 to 100.0), in the classifiers?
performance. The cost parameter c is responsible for the trade-off between maximum margin and clas-
sification errors. According to the system description the optimal parameter for this task lies between
30.0 and 50.0. Purver (2014) also notes that the linear SVM classifier performs well with word uni-gram
language models in comparison to methods using character n-grams. This observation corroborates the
findings of previous experiments that rely on words as important features to distinguish similar languages
and varieties (Huang and Lee, 2008; Zampieri, 2013)
The features and algorithms presented so far, as well as the system paper descriptions, are summarised
in table7.
11
Team Algorithm Features System Paper
NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014)
RAE MaxEnt Words 1-2, Char. 1-5, ?Whitelist? (Porta and Sancho, 2014)
UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014)
UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014)
QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014)
Table 7: Top 5 Systems - Features and Algorithms at a Glance
6 Conclusion
Shared tasks are an interesting way of comparing algorithms, computational methods and features using
the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled
an important gap in language identification and will allow other researchers to look in more detail at the
problem of discriminating similar languages. Accurate methods for discriminating similar languages can
help to improve performance not only in language identification but also in a number of NLP tasks and
applications such as part-of-speech-tagging, spell checking and machine translation.
The best system obtained 95.71% accuracy and F-measure for a set of 11 languages and varieties
divided into 5 groups (A to E), using only the DSL corpus collection. Systems that performed best
modelled their algorithms to perform two-step predictions: first the language group, then the actual class
and used characters and words as features. As we regard the corpus to be a balanced sample of the
news domain, the results obtained confirm the assumption that similar languages and varieties possess
systemic characteristics that can be modelled by algorithms in order to distinguish languages from other
similar languages or varieties using lexical or orthographical features.
Another lesson learned from this shared task is regarding the compilation of group F (English) data.
Researchers, including us, often rely on previously annotated meta-data which sometimes may contain
inaccurate information and errors. Corpus collection for this purpose should be thoroughly checked
(manually if possible). The issues with the group F might have discouraged some of the participants to
continue in the shared task (particularly those who were interested only in the discrimination of English
varieties).
11
UniMelb-NLP experimented different methods in their 6 runs. In this report we commented on the algorithm that achieved
the best performance.
65
6.1 Future Perspectives
The shared task was a very fruitful and positive experience for the organizers. We would like to organize
a second edition of the shared task containing, for example, new language groups for which we could
not find suitable corpora before the 2014 edition. This includes, most notably, the cases of Dutch and
Flemish or the varieties of French and German which could not be included in the DSL shared task due
to the lack of available data.
The DSL corpus collection is freely available and can be used as a gold standard for language iden-
tification or to train algorithms for other NLP tasks involving similar languages. We would like to use
the dataset to investigate, for example, lexical variation between similar languages and varieties as pro-
posed by Piersman et al. (2010) and Soares da Silva (2010) or syntactic variation using annotated data
as discussed in Anstein (2013).
At present, we are investigating the influence of the length of texts in the discrimination of similar
languages. It is a well known fact that the longer texts are, the more likely they are to contain features
that allow algorithms to identify their language. However, this variable was not explored within the
scope of the DSL shared task and we are using the DSL dataset and the results for this purpose. Another
direction that our work may take is the linguistic analysis of the most informative features in classification
as was done recently by Diwersy et al. (2014).
Acknowledgements
The authors would like to thank all participants of the DSL shared task for their comments and sugges-
tions throughout the organization of this shared task. We would also like to thank Joel Tetreault and
Binyam Gebrekidan Gebre for their valuable feedback on this report.
References
Stefanie Anstein. 2013. Computational approaches to the comparison of regional variety corpora : prototyping a
semi-automatic system for German. Ph.D. thesis, University of Stuttgart.
Timothy Baldwin and Marco Lui. 2010. Multilingual language identification: ALTW 2010 shared task data. In
Proceedings of Australasian Language Technology Association Workshop, pages 4?7.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Educational Testing Service.
Ralf Brown. 2013. Selecting and weighting n-grams to identify 1100 languages. In Proceedings of the 16th In-
ternational Conference on Text Speech and Dialogue (TSD2013), Lecture Notes in Artificial Intelligence (LNAI
8082), pages 519?526, Pilsen, Czech Republic. Springer.
William Cavnar and John Trenkle. 1994. N-gram-based text catogorization. 3rd Symposium on Document Analy-
sis and Information Retrieval (SDAIR-94).
Jack Chambers and Peter Trudgill. 1998. Dialectology (2nd Edition). Cambridge University Press.
Michael Clyne. 1992. Pluricentric Languages: Different Norms in Different Nations. CRC Press.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A semi-supervised multivariate approach to the study
of language variation. Linguistic Variation in Text and Speech, within and across Languages.
Cyril Goutte, Serge L?eger, and Marine Carpuat. 2014. The NRC system for discriminating similar languages. In
Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),
Dublin, Ireland.
Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum. 2010. Pr?esentation et
r?esultats du d?efi fouille de texte DEFT2010 o`u et quand un article de presse a-t-il ?et?e ?ecrit? Actes du sixi`eme
D
?
Efi Fouille de Textes.
Chu-ren Huang and Lung-hao Lee. 2008. Contrastive approach towards text source classification based on top-
bag-of-word similarity. In Proceedings of PACLIC 2008, pages 404?410.
66
Ben King, Dragomir Radev, and Steven Abney. 2014. Experiments in sentence language identification with
groups of similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages,
Varieties and Dialects (VarDial), Dublin, Ireland.
Nikola Ljube?si?c, Nives Mikelic, and Damir Boras. 2007. Language identification: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces.
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Meeting of the ACL.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of Aus-
tralasian Language Tchnology Workshop, pages 5?15.
Marco Lui, Ned Letcher, Oliver Adams, Long Duong, Paul Cook, and Timothy Baldwin. 2014. Exploring methods
and resources for discriminating similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools
to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Yves Piersman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16:469?491.
Jordi Porta and Jos?e-Luis Sancho. 2014. Using maximum entropy models to discriminate between similar lan-
guages and varieties. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial), Dublin, Ireland.
Matthew Purver. 2014. A simple baseline for discriminating similar language. In Proceedings of the 1st Workshop
on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Bali Ranaivo-Malanc?on. 2006. Automatic identification of close languages - case study: Malay and Indonesian.
ECTI Transactions on Computer and Information Technology, 2:126?134.
Augusto Soares da Silva. 2010. Measuring and parameterizing lexical convergence and divergence between
European and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence. Advances in
Cognitive Sociolinguistics.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared
task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,
Atlanta, GA, USA, June. Association for Computational Linguistics.
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of COLING 2012, pages 2619?2634, Mumbai, India.
Omar F Zaidan and Chris Callison-Burch. 2013. Arabic dialect identification. Computational Linguistics.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS2012, pages 233?237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580?587, Sable
d?Olonne, France.
Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they?
In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics
(CINTI2013), pages 37?41, Budapest, Hungary.
67
