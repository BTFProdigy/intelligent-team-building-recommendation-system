Proceedings of ACL-08: HLT, pages 763?770,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Enriching Morphologically Poor Languages
for Statistical Machine Translation
Eleftherios Avramidis
e.avramidis@sms.ed.ac.uk
Philipp Koehn
pkoehn@inf.ed.ac.uk
School of Informatics
University of Edinburgh
2 Baccleuch Place
Edinburgh, EH8 9LW, UK
Abstract
We address the problem of translating from
morphologically poor to morphologically rich
languages by adding per-word linguistic in-
formation to the source language. We use
the syntax of the source sentence to extract
information for noun cases and verb persons
and annotate the corresponding words accord-
ingly. In experiments, we show improved
performance for translating from English into
Greek and Czech. For English?Greek, we re-
duce the error on the verb conjugation from
19% to 5.4% and noun case agreement from
9% to 6%.
1 Introduction
Traditional statistical machine translation methods
are based on mapping on the lexical level, which
takes place in a local window of a few words. Hence,
they fail to produce adequate output in many cases
where more complex linguistic phenomena play a
role. Take the example of morphology. Predicting
the correct morphological variant for a target word
may not depend solely on the source words, but re-
quire additional information about its role in the sen-
tence.
Recent research on handling rich morphology has
largely focused on translating from rich morphology
languages, such as Arabic, into English (Habash and
Sadat, 2006). There has been less work on the op-
posite case, translating from English into morpho-
logically richer languages. In a study of translation
quality for languages in the Europarl corpus, Koehn
(2005) reports that translating into morphologically
richer languages is more difficult than translating
from them.
There are intuitive reasons why generating richer
morphology from morphologically poor languages
is harder. Take the example of translating noun
phrases from English to Greek (or German, Czech,
etc.). In English, a noun phrase is rendered the same
if it is the subject or the object. However, Greek
words in noun phrases are inflected based on their
role in the sentence. A purely lexical mapping of
English noun phrases to Greek noun phrases suffers
from the lack of information about its role in the sen-
tence, making it hard to choose the right inflected
forms.
Our method is based on factored phrase-based
statistical machine translation models. We focused
on preprocessing the source data to acquire the
needed information and then use it within the mod-
els. We mainly carried out experiments on English
to Greek translation, a language pair that exemplifies
the problems of translating from a morphologically
poor to a morphologically rich language.
1.1 Morphology in Phrase-based SMT
When examining parallel sentences of such lan-
guage pairs, it is apparent that for many English
words and phrases which appear usually in the same
form, the corresponding terms of the richer target
language appear inflected in many different ways.
On a single word-based probabilistic level, it is then
obvious that for one specific English word e the
probability p(f |e) of it being translated into a word
f decreases as the number of translation candidates
increase, making the decisions more uncertain.
763
? English: The president, after reading the
press review and the announcements, left
his office
? Greek-1: The president[nominative], after
reading[3rdsing] the press
review[accusative,sing] and the
announcements[accusative,plur],
left[3rdsing] his office[accusative,sing]
? Greek-2: The president[nominative], after
reading[3rdsing] the press
review[accusative,sing] and the
announcements[nominative,plur],
left[3rdplur] his office[accusative,sing]
Figure 1: Example of missing agreement information, af-
fecting the meaning of the second sentence
One of the main aspects required for the flu-
ency of a sentence is agreement. Certain words
have to match in gender, case, number, person etc.
within a sentence. The exact rules of agreement
are language-dependent and are closely linked to the
morphological structure of the language.
Traditional statistical machine translation models
deal with this problems in two ways:
? The basic SMT approach uses the target lan-
guage model as a feature in the argument
maximisation function. This language model
is trained on grammatically correct text, and
would therefore give a good probability for
word sequences that are likely to occur in a sen-
tence, while it would penalise ungrammatical
or badly ordered formations.
? Meanwhile, in phrase-based SMT models,
words are mapped in chunks. This can resolve
phenomena where the English side uses more
than one words to describe what is denoted on
the target side by one morphologically inflected
term.
Thus, with respect to these methods, there is a prob-
lem when agreement needs to be applied on part of
a sentence whose length exceeds the order of the of
the target n-gram language model and the size of the
chunks that are translated (see Figure 1 for an exam-
ple).
1.2 Related Work
In one of the first efforts to enrich the source in
word-based SMT, Ueffing and Ney (2003) used part-
of-speech (POS) tags, in order to deal with the verb
conjugation of Spanish and Catalan; so, POS tags
were used to identify the pronoun+verb sequence
and splice these two words into one term. The ap-
proach was clearly motivated by the problems oc-
curring by a single-word-based SMT and have been
solved by adopting a phrase-based model. Mean-
while, there is no handling of the case when the pro-
noun stays in distance with the related verb.
Minkov et al (2007) suggested a post-processing
system which uses morphological and syntactic fea-
tures, in order to ensure grammatical agreement on
the output. The method, using various grammatical
source-side features, achieved higher accuracy when
applied directly to the reference translations but it
was not tested as a part of an MT system. Similarly,
translating English into Turkish (Durgar El-Kahlout
and Oflazer, 2006) uses POS and morph stems in
the input along with rich Turkish morph tags on the
target side, but improvement was gained only after
augmenting the generation process with morphotac-
tical knowledge. Habash et al (2007) also inves-
tigated case determination in Arabic. Carpuat and
Wu (2007) approached the issue as a Word Sense
Disambiguation problem.
In their presentation of the factored SMT mod-
els, Koehn and Hoang (2007) describe experiments
for translating from English to German, Spanish and
Czech, using morphology tags added on the mor-
phologically rich side, along with POS tags. The
morphological factors are added on the morpholog-
ically rich side and scored with a 7-gram sequence
model. Probabilistic models for using only source
tags were investigated by Birch et al (2007), who
attached syntax hints in factored SMT models by
having Combinatorial Categorial Grammar (CCG)
supertags as factors on the input words, but in this
case English was the target language.
This paper reports work that strictly focuses on
translation from English to a morphologically richer
language. We go one step further than just using eas-
ily acquired information (e.g. English POS or lem-
mata) and extract target-specific information from
the source sentence context. We use syntax, not in
764
Figure 2: Classification of the errors on our English-
Greek baseline system (ch. 4.1), as suggested by Vilar
et al (2006)
order to aid reordering (Yamada and Knight, 2001;
Collins et al, 2005; Huang et al, 2006), but as a
means for getting the ?missing? morphology infor-
mation, depending on the syntactic position of the
words of interest. Then, contrary to the methods
that added only output features or altered the gen-
eration procedure, we used this information in order
to augment only the source side of a factored transla-
tion model, assuming that we do not have resources
allowing factors or specialized generation in the tar-
get language (a common problem, when translating
from English into under-resourced languages).
2 Methods for enriching input
We selected to focus on noun cases agreement
and verb person conjugation, since they were the
most frequent grammatical errors of our baseline
SMT system (see full error analysis in Figure 2).
Moreover, these types of inflection signify the con-
stituents of every phrase, tightly linked to the mean-
ing of the sentence.
2.1 Case agreement
The case agreement for nouns, adjectives and arti-
cles is mainly defined by the syntactic role that each
noun phrase has. Nominative case is used to define
the nouns which are the subject of the sentence, ac-
cusative shows usually the direct object of the verbs
and dative case refers to the indirect object of bi-
transitive verbs.
Therefore, the followed approach takes advantage
of syntax, following a method similar to Semantic
Role Labelling (Carreras and Marquez, 2005; Sur-
deanu and Turmo, 2005). English, as morpholog-
ically poor language, usually follows a fixed word
order (subject-verb-object), so that a syntax parser
can be easily used for identifying the subject and the
object of most sentences. Considering such annota-
tion, a factored translation model is trained to map
the word-case pair to the correct inflection of the tar-
get noun. Given the agreement restriction, all words
that accompany the noun (adjectives, articles, deter-
miners) must follow the case of the noun, so their
likely case needs to be identified as well.
For this purpose we use a syntax parser to acquire
the syntax tree for each English sentence. The trees
are parsed depth-first and the cases are identified
within particular ?sub-tree patterns? which are man-
ually specified. We use the sequence of the nodes
in the tree to identify the syntactic role of each noun
phrase.
Figure 3: Case tags are assigned on depth-first parse of
the English syntax tree, based on sub-tree patterns
To make things more clear, an example can be
seen in figure 3. At first, the algorithm identifies
the subtree ?S-(NPB-VP)? and the nominative tag is
applied on the NPB node, so that it is assigned to the
word ?we? (since a pronoun can have a case). The
example of accusative shows how cases get trans-
ferred to nested subtrees. In practice, they are recur-
sively transferred to every underlying noun phrase
(NP) but not to clauses that do not need this infor-
mation (e.g. prepositional phrases). Similar rules
are applied for covering a wide range of node se-
quence patterns.
Also note that this method had to be target-
765
oriented in some sense: we considered the target
language rules for choosing the noun case in ev-
ery prepositional phrase, depending on the leading
preposition. This way, almost all nouns were tagged
and therefore the number of the factored words was
increased, in an effort to decrease sparsity. Simi-
larly, cases which do not actively affect morphology
(e.g. dative in Greek) were not tagged during factor-
ization.
2.2 Verb person conjugation
For resolving the verb conjugation, we needed to
identify the person of a verb and add this piece of
linguistic information as a tag. As we parse the
tree top-down, on every level, we look for two dis-
crete nodes which, somewhere in their children, in-
clude the verb and the corresponding subject. Con-
sequently, the node which contains the subject is
searched recursively until a subject is found. Then,
the person is identified and the tag is assigned to the
node which contains the verb, which recursively be-
queaths this tag to the nested subtree.
For the subject selection, the following rules were
applied:
? The verb person is directly connected to the
subject of the sentence and in most cases it is
directly inferred by a personal pronoun (I, you
etc). Therefore, since this is usually the case,
when a pronoun existed, it was directly used as
a tag.
? All pronouns in a different case (e.g. them, my-
self ) were were converted into nominative case
before being used as a tag.
? When the subject of the sentence is not a pro-
noun, but a single noun, then it is in third per-
son. The POS tag of this noun is then used to
identify if it is plural or singular. This was se-
lectively modified for nouns which despite be-
ing in singular, take a verb in plural.
? The gender of the subject does not affect the
inflection of the verb in Greek. Therefore, all
three genders that are given by the third person
pronouns were reduced to one.
In Figure 4 we can see an example of how the
person tag is extracted from the subject of the sen-
Figure 4: Applying person tags on an English syntax tree
tence and gets passed to the relative clause. In par-
ticular, as the algorithm parses the syntax tree, it
identifies the sub-tree which has NP-A as a head
and includes the WHNP node. Consequently, it re-
cursively browses the preceding NPB so as to get
the subject of the sentence. The word ?aspects? is
found, which has a POS tag that shows it is a plural
noun. Therefore, we consider the subject to be of
the third person in plural (tagged by they) which is
recursively passed to the children of the head node.
3 Factored Model
The factored statistical machine translation model
uses a log-linear approach, in order to combine the
several components, including the language model,
the reordering model, the translation models and the
generation models. The model is defined mathemat-
ically (Koehn and Hoang, 2007) as following:
p(f |e) = 1Z exp
n?
i=1
?ihi(f , e) (1)
where ?i is a vector of weights determined during a
tuning process, and hi is the feature function. The
feature function for a translation probability distri-
bution is
hT (f |e) =
?
j
?(ej , f j) (2)
While factored models may use a generation step to
combine the several translation components based
on the output factors, we use only source factors;
766
therefore we don?t need a generation step to combine
the probabilities of the several components.
Instead, factors are added so that both words and
its factor(s) are assigned the same probability. Of
course, when there is not 1-1 mapping between the
word+factor splice on the source and the inflected
word on the target, the well-known issue of sparse
data arises. In order to reduce these problems, de-
coding needed to consider alternative paths to trans-
lation tables trained with less or no factors (as Birch
et al (2007) suggested), so as to cover instances
where a word appears with a factor which it has not
been trained with. This is similar to back-off. The
alternative paths are combined as following (fig. 5):
hT (f |e) =
?
j
hTt(j)(ej , f j) (3)
where each phrase j is translated by one translation
table t(j) and each table i has a feature function hTi .
as shown in eq. (2).
Figure 5: Decoding using an alternative path with differ-
ent factorization
4 Experiments
This preprocessing led to annotated source data,
which were given as an input to a factored SMT sys-
tem.
4.1 Experiment setup
For testing the factored translation systems, we used
Moses (Koehn et al, 2007), along with a 5-gram
SRILM language model (Stolcke, 2002). A Greek
model was trained on 440,082 aligned sentences of
Europarl v.3, tuned with Minimum Error Training
(Och, 2003). It was tuned over a development set
of 2,000 Europarl sentences and tested on two sets
of 2,000 sentences each, from the Europarl and a
News Commentary respectively, following the spec-
ifications made by the ACL 2007 2nd Workshop
on SMT1. A Czech model was trained on 57,464
aligned sentences, tuned over 1057 sentences of the
News Commentary corpus and and tested on two
sets of 964 sentences and 2000 sentences respec-
tively.
The training sentences were trimmed to a length
of 60 words for reducing perplexity and a standard
lexicalised reordering, with distortion limit set to
6. For getting the syntax trees, the latest version
of Collins? parser (Collins, 1997) was used. When
needed, part-of-speech (POS) tags were acquired by
using Brill?s tagger (Brill, 1992) on v1.14. Results
were evaluated with both BLEU (Papineni et al,
2001) and NIST metrics (NIST, 2002).
4.2 Results
BLEU NIST
set devtest test07 devtest test07
baseline 18.13 18.05 5.218 5.279
person 18.16 18.17 5.224 5.316
pos+person 18.14 18.16 5.259 5.316
person+case 18.08 18.24 5.258 5.340
altpath:POS 18.21 18.20 5.285 5.340
Table 1: Translating English to Greek: Using a single
translation table may cause sparse data problems, which
are addressed using an alternative path to a second trans-
lation table
We tested several various combinations of tags,
while using a single translation component. Some
combinations seem to be affected by sparse data
problems and the best score is achieved by using
both person and case tags. Our full method, using
both factors, was more effective on the second test-
set, but the best score in average was succeeded by
using an alternative path to a POS-factored transla-
tion table (table 1). The NIST metric clearly shows
a significant improvement, because it mostly mea-
sures difficult n-gram matches (e.g. due to the long-
distance rules we have been dealing with).
1see http://www.statmt.org/wmt07 referring to sets dev2006
(tuning) and devtest2006, test2007 (testing)
767
4.3 Error analysis
In n-gram based metrics, the scores for all words are
equally weighted, so mistakes on crucial sentence
constituents may be penalized the same as errors
on redundant or meaningless words (Callison-Burch
et al, 2006). We consider agreement on verbs and
nouns an important factor for the adequacy of the re-
sult, since they adhere more to the semantics of the
sentence. Since we targeted these problems, we con-
ducted a manual error analysis focused on the suc-
cess of the improved system regarding those specific
phenomena.
system verbs errors missing
baseline 311 19.0% 7.4%
single 295 4.7% 5.4%
alt.path 294 5.4% 2.7%
Table 2: Error analysis of 100 test sentences, focused on
verb person conjugation, for using both person and case
tags
system NPs errors missing
baseline 469 9.0% 4.9%
single 465 6.2% 4.5%
alt. path 452 6.0% 4.0%
Table 3: Error analysis of 100 test sentences, focused on
noun cases, for using both person and case tags
The analysis shows that using a system with only
one phrase translation table caused a high percent-
age of missing or untranslated words. When a word
appears with a tag with which it has not been trained,
that would be considered an unseen event and re-
main untranslated. The use of the alternative path
seems to be a good solution.
step parsing tagging decoding
VPs 16.7% 25% 58.3%
NPs 39.2% 21.7% 39.1%
avg 31.4% 22.9% 45.7 %
Table 4: Analysis on which step of the translation pro-
cess the agreement errors derive from, based on manual
resolution on the errors of table 3
The impact of the preprocessing stage to the er-
rors may be seen in table 4, where errors are tracked
back to the stage they derived from. Apart from the
decoding errors, which may be attributed to sparse
data or other statistical factors, a large part of the
errors derive from the preprocessing step; either the
syntax tree of the sentence was incorrectly or par-
tially resolved, or our labelling process did not cor-
rectly match all possible sub-trees.
4.4 Investigating applicability to other inflected
languages
The grammatical phenomena of noun cases and verb
persons are quite common among many human lan-
guages. While the method was tested in Greek, there
was an effort to investigate whether it is useful for
other languages with similar characteristics. For this
reason, the method was adapted for Czech, which
needs agreement on both verb conjugation and 9
noun cases. Dative case was included for the indi-
rect object and the rules of the prepositional phrases
were adapted to tag all three cases that can be verb
phrase constituents. The Czech noun cases which
appear only in prepositional phrases were ignored,
since they are covered by the phrase-based model.
BLUE NIST
set devtest test devtest test
baseline 12.08 12.34 4.634 4.865
person+case
altpath:POS 11.98 11.99 4.584 4.801
person
altpath:word 12.23 12.11 4.647 4.846
case
altpath:word 12.54 12.51 4.758 4.957
Table 5: Enriching source data can be useful when trans-
lating from English to Czech, since it is a morpholog-
ically rich language. Experiments shown improvement
when using factors on noun-cases with an alternative path
In Czech, due to the small size of the corpus, it
was possible to improve metric scores only by using
an alternative path to a bare word-to-word transla-
tion table. Combining case and verb tags worsened
the results, which suggests that, while applying the
method to more languages, a different use of the at-
tributes may be beneficial for each of them.
768
5 Conclusion
In this paper we have shown how SMT performance
can be improved, when translating from English
into morphologically richer languages, by adding
linguistic information on the source. Although the
source language misses morphology attributes re-
quired by the target language, the needed infor-
mation is inherent in the syntactic structure of the
source sentence. Therefore, we have shown that
this information can be easily be included in a SMT
model by preprocessing the source text.
Our method focuses on two linguistic phenomena
which produce common errors on the output and are
important constituents of the sentence. In partic-
ular, noun cases and verb persons are required by
the target language, but not directly inferred by the
source. For each of the sub-problems, our algorithm
used heuristic syntax-based rules on the statistically
generated syntax tree of each sentence, in order to
address the missing information, which was conse-
quently tagged in by means of word factors. This
information was proven to improve the outcome of
a factored SMT model, by reducing the grammatical
agreement errors on the generated sentences.
An initial system using one translation table with
additional source side factors caused sparse data
problems, due to the increased number of unseen
word-factor combinations. Therefore, the decoding
process is given an alternative path towards a trans-
lation table with less or no factors.
The method was tested on translating from En-
glish into two morphologically rich languages. Note
that this may be easily expanded for translating from
English into many morphologically richer languages
with similar attributes. Opposed to other factored
translation model approaches that require target lan-
guage factors, that are not easily obtainable for many
languages, our approach only requires English syn-
tax trees, which are acquired with widely avail-
able automatic parsers. The preprocessing scripts
were adapted so that they provide the morphology
attributes required by the target language and the
best combination of factors and alternative paths was
chosen.
Acknowledgments
This work was supported in part under the Euro-
Matrix project funded by the European Commission
(6th Framework Programme). Many thanks to Josh
Schroeder for preparing the training, development
and test data for Greek, in accordance to the stan-
dards of ACL 2007 2nd Workshop on SMT; to Hieu
Hoang, Alexandra Birch and all the members of
the Edinburgh University SMT group for answering
questions, making suggestions and providing sup-
port.
References
Birch, A., Osborne, M., and Koehn, P. 2007. CCG
Supertags in factored Statistical Machine Translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 9?16, Prague, Czech Re-
public. Association for Computational Linguistics.
Brill, E. 1992. A simple rule-based part of speech tag-
ger. Proceedings of the Third Conference on Applied
Natural Language Processing, pages 152?155.
Callison-Burch, C., Osborne, M., and Koehn, P. 2006.
Re-evaluation the role of bleu in machine translation
research. In Proceedings of the 11th Conference of
the European Chapter of the Association for Computa-
tional Linguistics. The Association for Computer Lin-
guistics.
Carpuat, M. and Wu, D. 2007. Improving Statistical Ma-
chine Translation using Word Sense Disambiguation.
In Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 61?72, Prague, Czech Republic.
Carreras, X. and Marquez, L. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of 9th Conference on Computational
Natural Language Learning (CoNLL), pages 169?172,
Ann Arbor, Michigan, USA.
Collins, M. 1997. Three generative, lexicalised models
for statistical parsing. Proceedings of the 35th con-
ference on Association for Computational Linguistics,
pages 16?23.
Collins, M., Koehn, P., and Kuc?erov?, I. 2005. Clause re-
structuring for statistical machine translation. In ACL
?05: Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 531?540,
Morristown, NJ, USA. Association for Computational
Linguistics.
769
Durgar El-Kahlout, i. and Oflazer, K. 2006. Initial explo-
rations in english to turkish statistical machine trans-
lation. In Proceedings on the Workshop on Statistical
Machine Translation, pages 7?14, New York City. As-
sociation for Computational Linguistics.
Habash, N., Gabbard, R., Rambow, O., Kulick, S., and
Marcus, M. 2007. Determining case in Arabic: Learn-
ing complex linguistic behavior requires complex lin-
guistic features. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1084?1092.
Habash, N. and Sadat, F. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAAC L, Companion Volume: Short Pa-
pers, pages 49?52, New York City, USA. Association
for Computational Linguistics.
Huang, L., Knight, K., and Joshi, A. 2006. Statistical
syntax-directed translation with extended domain of
locality. Proc. AMTA, pages 66?73.
Koehn, P. 2005. Europarl: A parallel corpus for statistical
machine translation. MT Summit, 5.
Koehn, P. and Hoang, H. 2007. Factored translation
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-
erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran,
C., Zens, R., Dyer, C., Bojar, O., Constantin, A.,
and Herbst, E. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic. Association for Com-
putational Linguistics.
Minkov, E., Toutanova, K., and Suzuki, H. 2007. Gen-
erating complex morphology for machine translation.
In ACL 07: Proceedings of the 45th Annual Meet-
ing of the Association of Computational linguistics,
pages 128?135, Prague, Czech Republic. Association
for Computational Linguistics.
NIST 2002. Automatic evaluation of machine translation
quality using n-gram co-occurrence statistics.
Och, F. J. 2003. Minimum error rate training in statisti-
cal machine translation. In ACL ?03: Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167, Morristown, NJ,
USA. Association for Computational Linguistics.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. 2001.
BLEU: a method for automatic evaluation of machine
translation. In ACL ?02: Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, pages 311?318, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Stolcke, A. 2002. SRILM-an extensible language model-
ing toolkit. Proc. ICSLP, 2:901?904.
Surdeanu, M. and Turmo, J. 2005. Semantic Role Label-
ing Using Complete Syntactic Analysis. In Proceed-
ings of 9th Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 221?224, Ann Arbor,
Michigan, USA.
Ueffing, N. and Ney, H. 2003. Using pos information
for statistical machine translation into morphologically
rich languages. In EACL ?03: Proceedings of the
tenth conference on European chapter of the Associ-
ation for Computational Linguistics, pages 347?354,
Morristown, NJ, USA. Association for Computational
Linguistics.
Vilar, D., Xu, J., D?Haro, L. F., and Ney, H. 2006. Error
Analysis of Machine Translation Output. In Proceed-
ings of the 5th Internation Conference on Language
Resources and Evaluation (LREC?06), pages 697?702,
Genoa, Italy.
Yamada, K. and Knight, K. 2001. A syntax-based statis-
tical translation model. In ACL ?01: Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 523?530, Morristown, NJ,
USA. Association for Computational Linguistics.
770
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65?70,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Evaluate with Confidence Estimation: Machine ranking of translation
outputs using grammatical features
Eleftherios Avramidis, Maja Popovic, David Vilar, Aljoscha Burchardt
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
name.surname@dfki.de
Abstract
We present a pilot study on an evaluation
method which is able to rank translation out-
puts with no reference translation, given only
their source sentence. The system employs a
statistical classifier trained upon existing hu-
man rankings, using several features derived
from analysis of both the source and the tar-
get sentences. Development experiments on
one language pair showed that the method
has considerably good correlation with human
ranking when using features obtained from a
PCFG parser.
1 Introduction
Automatic evaluation metrics for Machine Transla-
tion (MT) have mainly relied on analyzing both the
MT output against (one or more) reference transla-
tions. Though, several paradigms in Machine Trans-
lation Research pose the need to estimate the quality
through many translation outputs, when no reference
translation is given (n-best rescoring of SMT sys-
tems, system combination etc.). Such metrics have
been known as Confidence Estimation metrics and
quite a few projects have suggested solutions on this
direction. With our submission to the Shared Task,
we allow such a metric to be systematically com-
pared with the state-of-the-art reference-aware MT
metrics.
Our approach suggests building a Confidence Es-
timation metric using already existing human judg-
ments. This has been motivated by the existence
of human-annotated data containing comparisons of
the outputs of several systems, as a result of the
evaluation tasks run by the Workshops on Statistical
Machine Translation (WMT) (Callison-Burch et al,
2008; Callison-Burch et al, 2009; Callison-Burch
et al, 2010). This amount of data, which has been
freely available for further research, gives an op-
portunity for applying machine learning techniques
to model the human annotators? choices. Machine
Learning methods over previously released evalua-
tion data have been already used for tuning com-
plex statistical evaluation metrics (e.g. SVM-Rank
in Callison-Burch et al (2010)). Our proposition
is similar, but works without reference translations.
We develop a solution of applying machine learning
in order to build a statistical classifier that performs
similar to the human ranking: it is trained to rank
several MT outputs, given analysis of possible qual-
itative criteria on both the source and the target side
of every given sentence. As qualitative criteria, we
use statistical features indicating the quality and the
grammaticality of the output.
2 Automatic ranking method
2.1 From Confidence Estimation to ranking
Confidence estimation has been seen from the Nat-
ural Language Processing (NLP) perspective as a
problem of binary classification in order to assess
the correctness of a NLP system output. Previ-
ous work focusing on Machine Translation includes
statistical methods for estimating correctness scores
or correctness probabilities, following a rich search
over the spectrum of possible features (Blatz et al,
2004a; Ueffing and Ney, 2005; Specia et al, 2009;
Raybaud and Caroline Lavecchia, 2009; Rosti et al,
65
2007).
In this work we slightly transform the binary clas-
sification practice to fit the standard WMT human
evaluation process. As human annotators have pro-
vided their evaluation in the form of ranking of five
system outputs at a sentence level, we build our eval-
uation mechanism with similar functionality, aim-
ing to training from and evaluating against this data.
Evaluation scores and results can be then calculated
based on comparative analysis of the performance of
each system.
Whereas latest work, such as Specia et al (2010),
has focused on learning to assess segment perfor-
mance independently for each system output, our
contribution measures the performance by compar-
ing the system outputs with each other and con-
sequently ranking them. The exact method is de-
scribed below.
2.2 Internal pairwise decomposition
We build one classifier over all input sentences.
While the evaluation mechanism is trained and eval-
uated on a multi-class (ranking) basis as explained
above, the classifier is expected to work on a binary
level: we provide the features from the analysis of
the two system outputs and the source, and the clas-
sifier should decide if the first system output is better
than the second one or not.
In order to accomplish such training, the n sys-
tems? outputs for each sentence are broken down to
n ? (n ? 1) pairs, of all possible comparisons be-
tween two system outputs, in both directions (sim-
ilar to the calculation of the Spearman correlation).
For each pair, the classifier is trained with a class
value c, for the pairwise comparison of system out-
puts ti and tj with respective ranks ri and rj , deter-
mined as:
c(ri, rj) =
{
1 ri < rj
?1 ri > rj
At testing time, after the classifier has made all
the pairwise decisions, those need to be converted
back to ranks. System entries are ordered, according
to how many times each of them won in the pair-
wise comparison, leading to rank lists similar to the
ones provided by human annotators. Note that this
kind of decomposition allows for ties when there are
equal times of winnings.
2.3 Acquiring features
In order to obtain features indicating the quality of
the MT output, automatic NLP analysis tools are ap-
plied on both the source and the two target (MT-
generated) sentences of every pairwise comparison.
Features considered can be seen in the following cat-
egories, according to their origin:
? Sentence length: Number of words of source
and target sentences, source-length to target-
length ratio.
? Target language model: Language models
provide statistics concerning the correctness of
the words? sequence on the target language.
Such language model features include:
? the smoothed n-gram probability of the
entire target sentence for a language
model of order 5, along with
? uni-gram, bi-gram, tri-gram probabilities
and a
? count of unknown words
? Parsing: Processing features acquired from
PCFG parsing (Petrov et al, 2006) for both
source and target side include:
? parse log likelihood,
? number of n-best trees,
? confidence for the best parse,
? average confidence of all trees.
Ratios of the above target features to their re-
spective source features were included.
? Shallow grammatical match: The number of
occurences of particular node tags on both the
source and the target was counted on the PCFG
parses. In particular, NPs, VPs, PPs, NNs and
punctuation occurences were counted. Then
the ratio of the occurences of each tag in the
target sentence by its occurences on the source
sentence was also calculated.
2.4 Classifiers
The machine learning core of the system was built
supporting two classification approaches.
66
? Na?ve Bayes allows prediction of a binary
class, given the assumption that the features are
statistically independent.
p(C,F1, . . . , Fn) = p(C)
i=1?
n
p(Fi|C)
p(C) is estimated by relative frequencies of
the training pairwise examples, while p(Fi|C)
for our continuous features are estimated with
LOESS (locally weighted linear regression
similar to Cleveland (1979))
? k-nearest neighbour (knn) algorithm allows
classifying based on the closest training exam-
ples in the feature space.
3 Experiment
3.1 Experiment setup
A basic experiment was designed in order to deter-
mine the exact setup and the feature set of the metric
prior to the shared task submission. The classifiers
for the task were learnt using the German-English
testset of the WMT 2008 and 2010 (about 700 sen-
tences)1. For testing, the classifiers were used to per-
form ranking on a test set of 184 sentences which
had been kept apart from the 2010 data, with the cri-
terion that they do not contain contradictions among
human judgments.
In order to allow further comparison with other
evaluation metrics, we performed an extended ex-
periment: we trained the classifiers over the WMT
2008 and 2009 data and let them perform automatic
ranking on the full WMT 2010 test set, this time
without any restriction on human evaluation agree-
ment.
In both experiments, tokenization was performed
with the PUNKT tokenizer (Kiss et al, 2006; Gar-
rette and Klein, 2009), while n-gram features were
generated with the SRILM toolkit (Stolcke, 2002).
The language model was relatively big and had been
built upon all lowercased monolingual training sets
for the WMT 2011 Shared Task, interpolated on
the 2007 test set. As a PCFG parser, the Berkeley
Parser (Petrov and Klein, 2007) was preferred, due
1data acquired from http://www.statmt.org/wmt11
to the possibility of easily obtaining complex inter-
nal statistics, including n-best trees. Unfortunately,
the time required for parsing leads to significant de-
lays at the overall processing. The machine learn-
ing algorithms were implemented with the Orange
toolkit (Dem?ar et al, 2004).
3.2 Feature selection
Although the automatic NLP tools provided a lot of
features (section 2.3), the classification methods we
used (and particularly na?ve Bayes were the develop-
ment was focused on) would be expected to perform
better given a smaller group of statistically inde-
pendent features. Since exhaustive training/testing
of all possible feature subsets was not possible,
we performed feature selection based on the Reli-
eff method (Kononenko, 1994; Kira and Rendell,
1992). Automatic ranking was performed based on
the most promising feature subsets. The results are
examined below.
3.3 Results
The performance of the classifier is measured after
the classifier output has been converted back to rank
lists, similar to the WMT 2010 evaluation. We there-
fore calculated two types of rank coefficients: aver-
aged Kendall?s tau on a segment level, and Spear-
man?s rho on a system level, based on the percentage
that the each system?s translations performed better
than or equal to the translations of any other system.
The results for the various combinations of fea-
tures and classifiers are depicted on Table 1. Na?ve
Bayes provides the best score on the test set, with
? = 0.81 on a system level and ? = 0.26 on a
segment level, trained with features including the
number of the unknown words, the source-length
by target-length ratio, the VP count ratio and the
source-target ratio of the parsing log-likelihood. The
number of unknown words particularly appears to be
a strong indicator for the quality of the sentence. On
the first part of the table we can also observe that
language model features do not perform as well as
the features deriving from the processing informa-
tion delivered by the parser. On the second part of
the table we compare the use of various grammatical
combinations. The third part contains the correlation
obtained by various similar internal parsing-related
features.
67
features na?ve Bayes knn
rho tau rho tau
basic experiment
ngram 0.19 0.05 0.13 0.01
unk, len 0.67 0.20 0.73 0.24
unk, len, bigram 0.61 0.21 0.74 0.21
unk, len, ngram 0.63 0.19 0.59 0.21
unk, len, trigram 0.67 0.20 0.76 0.21
unk, len, logparse 0.75 0.21 0.74 0.25
unk, len, nparse, VP 0.67 0.24 0.61 0.20
unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24
unk, len, nparse, NP, confbestparse 0.78 0.23 0.74 0.23
unk, len, nparse, VP, confavg 0.75 0.21 0.78 0.23
unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24
unk, len, nparse, VP, logparse 0.81 0.26 0.75 0.23
extended experiment
unk, len, nparse, VP, logparse 0.60 0.23 0.28 0.02
Table 1: System-level Spearman?s rho and segment-level Kendall?s tau correlation coefficients achieved on automatic
ranking (average absolute value)
The correlation coefficients of the extended exper-
iment, allowing comparison with last year?s shared
task, are shown on the last line of the table. With
coefficients ? = 0.60 and ? = 0.23, our metric
performs relatively low compared to the other met-
rics of WMT10 (indicatively iBLEU: ? = 0.95,
? = 0.39 according to Callison-Burch et al (2010).
Though, it still has a position in the list, scoring bet-
ter than several other reference-aware metrics (e.g.
of ? = 0.47 and ? = 0.12 respectively) for the par-
ticular language pair.
4 Discussion
A concern on the use of Confidence Estimation for
MT evaluation has to do with the possibility of a
system ?tricking? such metrics. This would for ex-
ample be the case when a system offers a well-
formed candidate translation and gets a good score,
despite having no relation to the source sentence
in terms of meaning. We should note that we are
not capable of fully investigating this case based
on the current set of experiments, because all of
the systems in our data sets have shown acceptable
scores (11-25 BLEU and 0.58-0.78 TERp accord-
ing to Callison-Burch et al (2010)), when evaluated
against reference translations. Though, we would
assume that we partially address this problem by us-
ing ratios of source to target features (length, syn-
tactic constituents), which means that in order for a
sentence to trick the metric, it would need a com-
parable sentence length and a grammatical structure
that would allow it to achieve feature ratios similar
to the other systems? outputs. Previous work (Blatz
et al, 2004b; Ueffing and Ney, 2005) has used fea-
tures based on word alignment, such as IBM Mod-
els, which would be a meaningful addition from this
aspect.
Although k-nearest-neighbour is considered to be
a superior classifier, best results are obtained by
na?ve Bayes. This may have been due of the fact
that feature selection has led to small sets of uncor-
related features, where na?ve Bayes is known to per-
form well. K-nearest-neighbour and other complex
classification methods are expected to prove useful
when more complex feature sets are employed.
5 Conclusion and Further work
The experiments presented in this article indicate
that confidence metrics trained over human rankings
can be possibly used for several tasks of evaluation,
given particular conditions, where e.g. there is no
reference translation given. Features obtained from
68
a PCFG parser seem to be leading to better correla-
tions, given our basic test set. Although correlation
is not particularly high, compared to other reference-
aware metrics in WMT 10, there is clearly a poten-
tial for further improvement.
Nevertheless this is still a small-scale experiment,
given the restricted data size and the single transla-
tion direction. The performance of the system on
broader training and test sets will be evaluated in the
future. Feature selection is also subject to change
if other language pairs are introduced, while more
sophisticated machine learning algorithms, allowing
richer feature sets, may also lead to better results.
Acknowledgments
This work was done with the support of the
TaraXU? Project2, financed by TSB Technologie-
stiftung Berlin?Zukunftsfonds Berlin, co-financed
by the European Union?European fund for regional
development.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004a. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004b. Confidence estimation for
machine translation. In M. Rollins (Ed.), Mental Im-
agery. Yale University Press.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2http://taraxu.dfki.de
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American statistical association, 74(368):829?836.
Janez Dem?ar, Blaz Zupan, Gregor Leban, and Tomaz
Curk. 2004. Orange: From experimental machine
learning to interactive data mining. In Principles of
Data Mining and Knowledge Discovery, pages 537?
539.
Dan Garrette and Ewan Klein. 2009. An extensi-
ble toolkit for computational semantics. In Proceed-
ings of the Eighth International Conference on Com-
putational Semantics, IWCS-8 ?09, pages 116?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kenji Kira and Larry A. Rendell. 1992. The feature se-
lection problem: traditional methods and a new algo-
rithm. In Proceedings of the tenth national conference
on Artificial intelligence, AAAI?92, pages 129?134.
AAAI Press.
Tibor Kiss, Jan Strunk, Ruhr universit?t Bochum, and
Ruhr universit?t Bochum. 2006. Unsupervised mul-
tilingual sentence boundary detection. In Proceedings
of IICS-04, Guadalajara, Mexico and Springer LNCS
3473.
Igor Kononenko. 1994. Estimating attributes: analy-
sis and extensions of relief. In Proceedings of the
European conference on machine learning on Ma-
chine Learning, pages 171?182, Secaucus, NJ, USA.
Springer-Verlag New York, Inc.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In In HLT-NAACL ?07.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In In ACL ?06, pages 433?
440.
Sylvain Raybaud and Kamel Smaili Caroline Lavecchia,
David Langlois. 2009. Word-and sentence-level con-
fidence measures for machine translation. In Euro-
pean Association of Machine Translation 2009.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of the North
American Chapter of the Association for Compu-
tational Linguistics Human Language Technologies,
pages 228?235.
69
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009. Improv-
ing the confidence of machine translation quality es-
timates. In Machine Translation Summit XII, Ottawa,
Canada.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24:39?50, March.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP 2002, pages 901?904.
Nicola Ueffing and Hermann Ney. 2005. Word-level
confidence estimation for machine translation using
phrase-based translation models. Computational Lin-
guistics, pages 763?770.
70
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99?103,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Evaluation without references:
IBM1 scores as evaluation metrics
Maja Popovic?, David Vilar, Eleftherios Avramidis, Aljoscha Burchardt
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
name.surname@dfki.de
Abstract
Current metrics for evaluating machine trans-
lation quality have the huge drawback that
they require human-quality reference transla-
tions. We propose a truly automatic evalua-
tion metric based on IBM1 lexicon probabili-
ties which does not need any reference transla-
tions. Several variants of IBM1 scores are sys-
tematically explored in order to find the most
promising directions. Correlations between
the new metrics and human judgments are cal-
culated on the data of the third, fourth and fifth
shared tasks of the Statistical Machine Trans-
lation Workshop. Five different European lan-
guages are taken into account: English, Span-
ish, French, German and Czech. The results
show that the IBM1 scores are competitive
with the classic evaluation metrics, the most
promising being IBM1 scores calculated on
morphemes and POS-4grams.
1 Introduction
Currently used evaluation metrics such as BLEU (Pa-
pineni et al, 2002), METEOR (Banerjee and Lavie,
2005), etc. are based on the comparison between
human reference translations and the automatically
generated hypotheses in the target language to be
evaluated. While this scenario helps in the design
of machine translation systems, it has two major
drawbacks. The first one is the practical criticism
that using reference translations is inefficient and ex-
pensive: in real-life situations, the quality of ma-
chine translation must be evaluated without having
to pay humans for producing reference translations
first. The second criticism is methodological: in
using reference translation, the problem of evalu-
ating translation quality (e.g., completeness, order-
ing, domain fit, etc.) is transformed into a kind of
paraphrase evaluation in the target language, which
is a very difficult problem itself. In addition, the
set of selected references always represents only a
small subset of all good translations. To remedy
these drawbacks, we propose a truly automatic eval-
uation metric which is based on the IBM1 lexicon
scores (Brown et al, 1993).
The inclusion of IBM1 scores in translation sys-
tems has shown experimentally to improve transla-
tion quality (Och et al, 2003). They also have been
used for confidence estimation for machine transla-
tion (Blatz et al, 2003). To the best of our knowl-
edge, these scores have not yet been used as an eval-
uation metric.
We carry out a systematic comparison between
several variants of IBM1 scores. The Spearman?s
rank correlation coefficients on the document (sys-
tem) level between the IBM1 metrics and the hu-
man ranking are computed on the English, French,
Spanish, German and Czech texts generated by var-
ious translation systems in the framework of the
third (Callison-Burch et al, 2008), fourth (Callison-
Burch et al, 2009) and fifth (Callison-Burch et al,
2010) shared translation tasks.
2 IBM1 scores
The IBM1 model is a bag-of-word translation model
which gives the sum of all possible alignment proba-
bilities between the words in the source sentence and
the words in the target sentence. Brown et al (1993)
defined the IBM1 probability score for a translation
99
pair fJ1 and eI1 in the following way:
P (fJ1 |eI1) =
1
(I + 1)J
J
?
j=1
I
?
i=0
p(fj |ei) (1)
where fJ1 is the source language sentence of length
J and eI1 is the target language sentence of length I .
As it is a conditional probability distribution, we
investigated both directions as evaluation metrics. In
order to avoid frequent confusions about what is the
source and what the target language, we defined our
scores in the following way:
? source-to-hypothesis (sh) IBM1 score:
IBM1sh =
1
(H + 1)S
S
?
j=1
H
?
i=0
p(sj|hi) (2)
? hypothesis-to-source (hs) IBM1 score:
IBM1hs =
1
(S + 1)H
H
?
i=1
S
?
j=0
p(hi|sj) (3)
where sj are the words of the original source lan-
guage sentence, S is the length of this sentence, hi
are the words of the target language hypothesis, and
H is the length of this hypothesis.
In addition to the standard IBM1 scores calculated
on words, we also investigated:
? MIBM1 scores ? IBM1 scores of word mor-
phemes in each direction;
? PnIBM1 scores ? IBM1 scores of POS n-grams
in each direction.
A parallel bilingual corpus for the desired lan-
guage pair and a tool for training the IBM1 model
are required in order to obtain IBM1 probabilities
p(fj|ei). For the POS n-gram scores, appropriate
POS taggers for each of the languages are necessary.
The POS tags cannot be only basic but must have
all details (e.g. verb tenses, cases, number, gender,
etc.). For the morpheme scores, a tool for splitting
words into morphemes is necessary.
3 Experiments on WMT 2008, WMT 2009
and WMT 2010 test data
3.1 Experimental set-up
The IBM1 probabilities necessary for the IBM1
scores are learnt using the WMT 2010 News
Commentary bilingual corpora consisting of the
Spanish-English, French-English, German-English
and Czech-English parallel texts. Spanish, French,
German and English POS tags were produced using
the TreeTagger1, and the Czech texts are tagged us-
ing the COMPOST tagger (Spoustova? et al, 2009).
The morphemes for all languages are obtained us-
ing the Morfessor tool (Creutz and Lagus, 2005).
The tool is corpus-based and language-independent:
it takes a text as input and produces a segmenta-
tion of the word forms observed in the text. The
obtained results are not strictly linguistic, however
they often resemble a linguistic morpheme segmen-
tation. Once a morpheme segmentation has been
learnt from some text, it can be used for segment-
ing new texts. In our experiments, the splitting are
learnt from the training corpus used for the IBM1
lexicon probabilities. The obtained segmentation is
then used for splitting the corresponding source texts
and hypotheses. Detailed corpus statistics are shown
in Table 1.
Using the obtained IBM1 probabilities of words,
morphemes and POS n-grams, the scores de-
scribed in Section 2 are calculated for the
Spanish-English, French-English, German-English
and Czech-English translation outputs from each
translation direction. For each of the IBM1 scores,
the system level Spearman correlation coefficients ?
with the human ranking are calculated for each doc-
ument. In total, 32 correlation coefficients are ob-
tained for each score ? four English outputs from
the WMT 2010 task, four from the WMT 2009 and
eight from the WMT 2008 task, together with six-
teen outputs in other four target languages. The ob-
tained correlation results were then summarised into
the following three values:
? mean
a correlation coefficient averaged over all trans-
lation outputs;
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
100
Spanish English French English German English Czech English
sentences 97122 83967 100222 94693
running words 2661344 2338495 2395141 2042085 2475359 2398780 2061422 2249365
vocabulary:
words 69620 53527 56295 50082 107278 54270 125614 52081
morphemes 14178 13449 12004 12485 22211 13499 18789 12961
POS tags 69 44 33 44 54 44 611 44
POS-2grams 2459 1443 826 1443 1611 1454 27835 1457
POS-3grams 27350 20474 10409 19838 19928 20769 209481 20522
POS-4grams 135166 121182 62177 114555 114314 123550 637337 120646
Table 1: Statistics of the corpora for training IBM1 lexicon models.
? rank>
percentage of documents where the particular
score has better correlation than the other IBM1
scores;
? rank?
percentage of documents where the particular
score has better or equal correlation than the
other IBM1 scores.
3.2 Comparison of IBM1 scores
The first step towards deciding which IBM1 score
to submit to the WMT 2011 evaluation task was a
comparison of the average correlations i.e. mean
values. These values for each of the IBM1 scores
are presented in Table 2. The left column shows
average correlations of the source-hypothesis (sh)
scores, and the right one of the hypothesis-source
(hs) scores.
mean IBM1sh IBM1hs
words 0.066 0.308
morphemes 0.227 0.445
POS tags 0.006 0.337
POS-2grams 0.058 0.337
POS-3grams 0.172 0.376
POS-4grams 0.196 0.442
Table 2: Average correlations of source-hypothesis (left
column) and hypothesis-source (right column) IBM1
scores.
It can be seen that the morpheme, POS-3gram and
POS-4gram scores have the best correlations in both
directions. Apart from that, it can be observed that
all the hs scores have better correlations than sh
scores. Therefore, all the further experiments will
deal only with the hs scores, and the subscript hs is
omitted.
In the next step, all the hs scores are sorted ac-
cording to each of the three values described in
Section 3.1, i.e. average correlation mean, rank>
and rank?, and the results are shown in Table 3.
The most promising scores according to each of
the three values are morpheme score MIBM1, POS-
3gram score P3IBM1 and POS-4gram score P4IBM1.
3.2.1 Combined IBM1 scores
The last experiment was to combine the most
promising IBM1 scores in order to see if the correla-
tion with human rankings can be further improved.
In general, a combined IBM1 score is defined as
arithmetic mean of various individual IBM1hs scores
described in Section 2:
COMBIBM1 =
K
?
k=1
wk ? IBM1k (4)
The following combinations were investigated:
? P1234IBM1
combination of all POS n-gram scores;
? MP1234IBM1
combination of all POS n-gram scores and the
morpheme score;
? MP34IBM1
combination of the most promising individual
scores, i.e. POS-3gram, POS-4gram and mor-
pheme scores;
101
mean rank> rank?
0.445 morphemes 60.6 POS-4grams 71.3 POS-4grams
0.442 POS-4grams 54.4 morphemes 61.3 POS-3grams
0.376 POS-3grams 50.6 POS-3grams 56.3 morphemes
0.337 POS-2grams 39.4 POS tags 48.1 POS tags
0.337 POS tags 36.3 words 43.7 POS-2grams
0.308 words 35.6 POS-2grams 42.5 words
Table 3: IBM1hs scores sorted by average correlation (column 1), rank> value (column 2) and rank? value (column
3). The most promising scores are those calculated on morphemes (MIBM1), POS-3grams (P3IBM1) and POS-4grams
(P4IBM1).
? MP4IBM1
combination of the two most promising indi-
vidual scores, i.e. POS-4gram score and mor-
pheme score.
For each of the scores, two variants were investi-
gated, with and without (i.e. with uniform) weights
wk. The weigths were choosen proportionally to
the average correlation of each individual score. Ta-
ble 4 contains average correlations for all combined
scores, together with the weight values.
combined score mean
P1234IBM1 0.403
+weights (0.15, 0.15, 0.3, 0.4) 0.414
MP1234IBM1 0.466
+weights (0.2, 0.05, 0.05, 0.2, 0.5) 0.486
MP34IBM1 0.480
+weights (0.25, 0.25, 0.5) 0.498
MP4IBM1 0.494
+weights (0.4, 0.6) 0.496
Table 4: Average correlations of the investigated IBM1hs
combinations. The weight values are choosen accord-
ing to the average correlation of the particular individual
IBM1 score.
The POS n-gram combination alone does not yield
any improvement over the best individual scores.
Introduction of the morpheme score increases the
average correlation, especially when only the best
n-gram scores are chosen. Apart from that, intro-
ducing weights improves the average correlation for
each of the combined scores.
The final step in our experiments consists of rank-
ing the weighted combined scores. The rank> and
rank? values for these scores are presented in Ta-
ble 5. According to the rank> values, the MP4IBM1
score clearly outperforms all other scores. This
score also has the highest mean value together with
the MP34IBM1 score. As for rank? values, all
morpheme-POS scores have similar values signifi-
cantly outperforming the P1234IBM1 score.
combined score rank> rank?
P1234IBM1 25.0 36.4
MP1234IBM1 44.8 68.7
MP34IBM1 39.6 64.6
MP4IBM1 55.2 65.7
Table 5: rank> (column 1) and rank? (column 2) values
of the weighted IBM1hs combinations.
Following all these observations, we decided to
submit the MP4IBM1 score to the WMT 2011 evalu-
ation task.
4 Conclusions and outlook
The results presented in this article show that the
IBM1 scores have the potential to be used as replace-
ment of current evaluation metrics based on refer-
ence translations. Especially the scores abstracting
away from word surface particularities (i.e. vocabu-
lary, domain) based on morphemes, POS-3grams and
4grams show a high average correlation of about 0.5
(the average correlation of the BLEU score on the
same data is 0.566).
An important point for future optimisation is to
investigate effects of the selection of training data
for the IBM1 models (and its similarity to the train-
ing data of the involved statistical translation sys-
tems). Furthermore, investigation of how to assign
the weights for combining the corresponding indi-
102
vidual scores, as well as of the possible impact of
different morpheme splittings should be carried out.
Other direction for future work is combination with
other features (i.e. POS language models).
This method is currently being tested and fur-
ther developed in the framework of the TARAX ?U
project2. In this project, three industry and one re-
search partners develop a hybrid machine transla-
tion architecture that satisfies current industry needs,
which includes a number of large-scale evalua-
tion rounds involving various languages: English,
French, German, Czech, Spanish, Russian, Chinese
and Japanese. By the time of writing this article, the
first human evaluation round in TARAX ?U on a pilot
set of about 7000 sentences is running. The metrics
proposed in this paper will be tested on the TARAX ?U
data as soon as they are available. First results will
be reported in the presentation of this paper.
Acknowledgments
This work has been partly developed within the
TARAX ?U project financed by TSB Technologies-
tiftung Berlin ? Zukunftsfonds Berlin, co-financed
by the European Union ? European fund for regional
development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In Pro-
ceedings of the ACL 05 Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for MT and/or Summa-
rization, pages 65?72, Ann Arbor, MI, June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Final report, JHU/CLSP Summer
Workshop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
Meta-Evaluation of Machine Translation. In Proceed-
ings of the 3rd ACL 08 Workshop on Statistical Ma-
2http://taraxu.dfki.de/
chine Translation (WMT 08), pages 70?106, Colum-
bus, Ohio, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR
(WMT 10), pages 17?53, Uppsala, Sweden, July.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical Re-
port Report A81, Computer and Information Science,
Helsinki University of Technology, Helsinki, Finland,
March.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for statistical machine translation. Technical re-
port, Johns Hopkins University 2003 Summer Work-
shop on Language Engineering, Center for Language
and Speech Processing, Baltimore, MD, USA, August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 02), pages 311?318, Philadel-
phia, PA, July.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 763?771,
Athens, Greece, March.
103
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 84?90,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Quality Estimation for Machine Translation output
using linguistic analysis and decoding features
Eleftherios Avramidis
German Research Center for Artificial Intelligence (DFKI)
Berlin, Germany
eleftherios.avramidis@dfki.de
Abstract
We describe a submission to the WMT12
Quality Estimation task, including an exten-
sive Machine Learning experimentation. Data
were augmented with features from linguis-
tic analysis and statistical features from the
SMT search graph. Several Feature Selec-
tion algorithms were employed. The Quality
Estimation problem was addressed both as a
regression task and as a discretised classifi-
cation task, but the latter did not generalise
well on the unseen testset. The most success-
ful regression methods had an RMSE of 0.86
and were trained with a feature set given by
Correlation-based Feature Selection. Indica-
tions that RMSE is not always sufficient for
measuring performance were observed.
1 Introduction
As Machine Translation (MT) gradually gains a po-
sition into production environments, the need for es-
timating the quality of its output is increasing. Vari-
ous use cases refer to it as input assessment for Hu-
man Post-editing, as an extension for Hybrid MT or
System Combination, or even a method for improv-
ing components of existing MT systems.
With the current submission we are trying to
address the problem of assigning a quality score
to a single MT output per source sentence. Pre-
vious work includes regression methods for in-
dicating a binary value of correctness (Quirk,
2001; Blatz et al, 2004; Ueffing and Ney, 2007),
human-likeness (Gamon et al, 2005) or continu-
ous scores (Specia et al, 2009). As we also work
with continuous scores, we are making an effort
to combine previous feature acquisition sources,
such as language modelling (Raybaud et al, 2009),
language fluency checking (Parton et al, 2011),
parsing (Sa?nchez-Martinez, 2011; Avramidis et al,
2011) and decoding statistics (Specia et al, 2009;
Avramidis, 2011). The current submission combines
such previous observations in a combinatory experi-
mentation on feature sets, feature selection methods
and Machine Learning (ML) algorithms.
The structure of the submission is as follows: The
approach is defined and the methods are described
in section 2, including features acquisition, feature
selection and learning. Section 3 includes informa-
tion about the experiment setup whereas the results
are discussed in Section 4.
2 Methods
2.1 Data and basic approach
This contribution has been built based on the data
released for the Quality Estimation task of the
Workshop on Machine Translation (WMT) 2012
(Callison-Burch et al, 2012). The organizers pro-
vided an English-to-Spanish development set and a
test set of 1832 and 422 sentences respectively, de-
rived from WMT09 and WMT10 datasets. For each
source sentence of the development set, participants
were offered one translation generated by a state-of-
the-art phrase-based SMT system. The quality of
each SMT translation was assessed by human evalu-
ators, who provided a quality score in the range 1-5.
Additionally, statistics and processing information
from the execution of the SMT decoding algorithm
were given.
The approach presented here is making use of the
source sentences, the SMT output and the quality
scores in order to follow a typical ML paradigm:
84
sentence suggestion
. . . los l??deres de la Unio?n han descrito como deducciones pol??tico . . . number agreement
La articular y ideolo?gicamente convencido de asesino de masas . . . transform ?y? to ?e?
Right after hearing about it, he described it as a ?challenge. . . ? disambiguate -ing
Table 1: Sample suggestions generated by rule-based language checking tools, observed in development data
each source and target sentence of the development
set are being analyzed to generate a feature vector.
One training sample is formed out of the feature vec-
tor and the quality score (i.e. as a class value) of each
sentence. A ML algorithm is consequently used to
train a model given the training samples. The per-
formance of each model is evaluated upon a part of
the development set that was kept-out from training.
2.2 Acquiring Features
The features were obtained from two sources: the
decoding process and the analysis of the text of the
source and the target sentence. The two steps are
explained below.
2.2.1 Features from text analysis
The following features were generated with the use
of tools for the statistical and/or linguistic analysis
of the text. The baseline features included:
? Tokens count: Count of tokens in the source
and the translated sentence and their ratio, un-
known words and also occurrences of the target
word within the translated sentence (averaged
for all words in the hypothesis - type/token ra-
tio)
? IBM1-model lookup: Average number of
translations per source word in the sentence,
unweighted or weighted by the inverse fre-
quency of each word in the source corpus
? Language modeling: Language model proba-
bility of the source and translated sentence
? Corpus lookup: percentage of unigrams / bi-
grams / trigrams in quartiles 1 and 4 of fre-
quency (lower and higher frequency words) in
a corpus of the source language
Additionally, the following linguistically motivated
features were also included:
? Parsing: PCFG Parse (Petrov et al, 2006) log-
likelihood, size of n-best tree list, confidence
for the best parse, average confidence of all
parse trees. Ratios of the mentioned target fea-
tures to the corresponding source features.
? Shallow grammatical match: The number
of occurences of particular node tags on both
the source and the target was counted on the
PCFG parses. Additionally, the ratio of the
occurences of each tag in the target sentence
by the corresponding occurences on the source
sentence.
? Language quality check: Source and target
sentences were subject to automatic rule-based
language quality checking, providing a wide
range of quality suggestions concerning style,
grammar and terminology, summed up in an
overall quality score. The process employed
786 rules for English and 70 rules for Spanish.
We counted the occurences of every rule match
in each sentence and the number of characters it
affected. Sample rule suggestions can be seen
in Table 1.
2.2.2 Features from the decoding process
The organisers provided a verbose output of the de-
coding process, including probabilistic scores from
all steps of the execution of the translation search.
We added the scores appearing once per sentence
(i.e. referring to the best hypothesis), whereas for
the ones being modified over the generation graph,
their average (avg), variance (var) and standard de-
viation (std) was calculated. These features are:
? the log of the phrase translation probability
(pC) and the phrase future cost estimate (c)
? the score component vector including the dis-
tortion scores (d1...7), word penalty, translation
scores (e.g. a1: inverse phrase translation prob-
ability, a2: inverse lexical weighting)
85
2.3 Feature Selection
Experience has shown difficulties in including hun-
dreds of features into training a statistical model.
Several algorithms (such as Na??ve Bayes) require
statistically-independent features. For others, a
search space of hundreds of features may impose
increased computational complexity, which is often
unsustainable in the time and resources allocated.
In these cases we therefore applied several common
Feature Selection approaches, in order to reduce the
available features to an affordable number.
We used the Feature Selection algorithms of Re-
lieff (Kononenko, 1994), Information Gain and
Gain Ratio (Kullback and Leibler, 1951), and
Correlation-based Feature Selection (Hall, 2000).
The latter is known for producing feature sets highly
correlated with the class, yet uncorrelated with each
other; selection was done in two variations, greedy
stepwise and best first.
The data were discretised according to the algo-
rithm requirements and features were scored in a 10-
fold cross-validation.
2.4 Machine Learning
We tried to approach the issue with two distinct
modelling approaches, classification and regression.
2.4.1 Classification algorithms
In an effort to interpret Quality Estimation as a
classification problem, we expect to build models
that are able to assign a discrete value, as a mea-
sure of sentence quality. This bears some relation to
the way the quality scores were generated; humans
were asked to provide an (integer) quality score in
the range 1-5. In our case, we try to build classifiers
that do the same, but are also able to assign values
with smaller intervals. For this purpose, we set up
4 sub-experiments, where the class value in our data
was rounded up to intervals of 0.25, 0.5, 0.7 and 1.0
respectively.
In this part of the experiment we used the Na??ve
Bayes, k-nearest-neighbours (kNN), Support Vector
Machines (SVM) and Tree classification algorithms.
Na??ve Bayes? probabilities for our continuous fea-
tures were estimated with locally weighted linear re-
gression (Cleveland, 1979).
2.4.2 Regression algorithms
Regression algorithms produce a model for di-
rectly predicting a quality score with continu-
ous values. Experimentation here included Par-
tial Least Squares Regression (Stone and Brooks,
1990), Multivariate Adaptive Regression Splines ?
MARS (Friedman, 1991), Lasso (Tibshirani, 1994)
and Linear Regression.
3 Experiment and Results
3.1 Implementation
PCFG parsing features were generated on the out-
put of the Berkeley Parser (Petrov and Klein,
2007), trained over an English and a Spanish tree-
bank (Mariona Taule? and Recasens, 2008). N-
gram features have been generated with the SRILM
toolkit (Stolcke, 2002). The Acrolinx IQ1 was used
to parse the source side, whereas the Language Tool2
was applied on both sides.
The feature selection and learning algorithms
were implemented with the Orange (Dems?ar et al,
2004) and Weka (Hall et al, 2009) toolkits.
3.2 Experiment structure
The methods explained in the previous section pro-
vide a wide range of experiment parameters. Con-
sequently, we tried to extensively test all the possi-
ble parameter combinations. The development data
were separated in two sets, one ?training? set and
one ?keep-out? set, used to test the predictions. In
order to give learners better coverage over the data,
the development set was split in two ways (70%
training - 30% test and 90% training - 10% test), so
that all experiments get performed under both set-
tings. The scores of these two were averaged3.
3.3 Results
The small size of the dataset alowed for fast train-
ing and testing of the discrete classification problem,
where we could execute 370 experiments. The re-
gression problem was considerably slower, as only
36 experiments concluded in time.
1http://www.acrolinx.com (proprietary)
2http://languagetool.org (open-source)
3Given the disparity of the test sizes, it would have in prin-
ciple been better to use a weighted average. Though, this would
not have lead to significant differences in the results.
86
5-fold avg 70-30%, 90-10% folds
algorithm feat. set discr. CA AUC RMSE MAE interval
Tree #17, #20 0.25 15.40 54.10 0.84 0.67 1.5 5.0
Tree #23 0.25 14.60 53.50 0.85 0.68 2.0 5.0
Tree #12 0.25 13.90 52.00 0.86 0.69 1.8 5.0
Tree #4 0.25 14.50 53.70 0.86 0.69 2.0 5.0
SVM #16 0.25 16.00 60.40 0.86 0.69 3.2 3.2
kNN #22 0.25 12.30 55.50 1.00 0.78 2.0 5.0
Tree #21 0.50 22.70 54.60 0.87 0.69 2.0 5.0
SVM #19 0.50 22.40 60.20 0.91 0.73 2.8 5.0
kNN #12 0.50 20.00 54.70 0.98 0.78 2.2 5.0
Naive #6 0.50 21.20 59.40 0.99 0.76 1.2 5.0
Tree #9 0.70 32.70 53.30 0.89 0.71 3.5 4.9
kNN #12 0.70 28.20 56.10 0.93 0.73 2.5 4.9
SVM #18 0.70 30.90 55.60 0.97 0.77 3.5 4.2
Tree #22 1.00 40.30 55.70 0.90 0.71 2.0 5.0
kNN #22 1.00 40.90 59.10 0.96 0.76 2.5 5.0
Naive #23 1.00 41.00 65.50 1.02 0.78 1.2 5.0
SVM #6 1.00 36.60 51.10 1.02 0.84 3.0 4.0
Table 2: Indicative discretised classification results, sorted by best performance and discretisation interval. Classifica-
tion Accuracy (AC), Area Under Curve (AUC), Root Mean Square Error (RMSE) and Mean Average Error (MAE),
Largest Error Percentage (LEP) and Smallest Error Percentage (SEP)
Feature generation resulted (described in Section
2.2) into 266 features, while 90 of them derived from
language checking. Feature selection suggested sev-
eral feature sets containing between 30 and 80 fea-
tures. We ended up defining 22 feature sets, includ-
ing the full feature set, the baseline feature set and
a couple of manually selected feature sets. Unfor-
tunately, due to size restrictions, not all features can
be listed; though, indicative feature sets are listed in
Table 5.
The most important results of the classification
approach can be seen in Table 2 and the results of
the regression approach in Tables 3 (development
set) and 4 (shared task test set).
4 Discussion
4.1 Machine Learning Conclusions
Discrete classifiers (section 2.4.1) do not yield en-
couraging accuracy, as acceptable levels of accu-
racies appear only with a discretisation interval of
1.00, which though cannot be accepted due to its
high Root Mean Square Error (RMSE). On the de-
velopment keep-out set, the discretised Tree classi-
fier seemingly outperforms all other methods (in-
cluding the regression learners), since it yields a
RMSE of 0.84, given several different feature vec-
tors. Unfortunately, when applied to the final un-
known test data, these classifiers performed obvi-
ously bad, providing the same single value for all
sentences. We could attribute this to overfitting vs.
sparse data and consider how we can handle this bet-
ter in further work.
Another remarkable observation was the incapa-
bility of the RMSE to objectively show the qual-
ity of the model, in situations where the predicted
values are very close or equal to the average of
all real values. A Support Vector Machine with
RMSE = 0.86 ranked 3rd among the classifiers, al-
though it ?cheated? by producing only the average
value: 3.25. This leads to the conclusion that the
selection of the best algorithm is not just dictated
by the lowest RMSE, but it should consider several
other indications such as the standard deviation.
We therefore resort to the regression learners
(section 2.4.2), whose scores are not worse, having
a RMSE of 0.855. We have to notice that the four
87
avg. 70-30%, 90-10% folds
algorithm f. set RMSE MAE interval
PLS #19 0.86 0.69 2.5 4.3
Lasso #19 0.86 0.68 2.7 4.4
Linear #19 0.86 0.68 2.6 4.5
MARS #19 0.86 0.68 2.6 4.7
PLS #18 0.86 0.69 2.7 4.4
Linear #18 0.86 0.69 2.8 4.4
Lasso #18 0.86 0.69 2.8 4.4
MARS #16 0.87 0.69 2.4 4.6
MARS #18 0.86 0.69 2.4 4.5
MARS #4 0.86 0.69 3.4 4.5
PLS #16 0.87 0.70 2.1 4.8
PLS #4 0.87 0.70 2.1 5.4
Linear #4 0.88 0.70 2.4 4.8
Linear #16 0.88 0.70 1.4 4.9
Lasso #4 0.88 0.70 1.9 5.3
MARS #2 0.90 0.72 3.0 4.5
Lasso #16 0.90 0.71 2.7 4.5
Linear #2 0.90 0.72 3.0 4.0
Lasso #2 0.90 0.72 3.0 4.0
PLS #2 0.90 0.73 3.0 3.9
Tree #21 1.08 0.86 1.5 5.0
Tree #19 1.19 0.96 1.6 5.0
Tree #16 1.23 0.98 1.6 5.0
Tree #18 1.25 0.98 1.4 5.0
Table 3: Regression results. Root Mean Square Error
(RMSE) and Mean Average Error (MAE), Largest Error
Percentage (LEP) and Smallest Error Percentage (SEP).
Bold face indicates submitted sets
regression algorithms have comparable performance
given the same features.
The best-performing feature set (#19) which was
chosen as the first submission (DFKI cfs-plsreg)
trained with PLS regression, contains features in-
dicated by Correlation-based Feature Selection, run
with bestfirst on a 10-fold cross-validation. We used
the features which were selected on the 100% or
90% of the folds. An equally best-performing fea-
ture set (#18) has resulted from exactly the same fea-
ture selection execution, but contains only features
which were selected in all folds.
The second submission (DFKI grcfs-mars) was
chosen to differentiate both the feature set and the
learning method, with respect to a decent interval.
Feature set #16 is the result of the Correlation-based
learner feat. name RMSE MAE
MARS #16 grcfs?mars 0.98 0.82
PLS #19 cfs-plsreg 0.99 0.82
Table 4: Results of the submitted methods on the official
testset
Feature Selection, run in a greedy-stepwise mode.
The regression was trained with MARS.
The baseline feature set (#2) performed worse.
Noticeable was the RMSE of the feature set #4, with
features selected based on their Gain Ratio, but we
did not submit this due to its very narrow interval.
4.2 Feature conclusions
The best performing feature set gives interesting
hints on what worked as a best indication of trans-
lation quality. We would try to summarize them as
follows:
? The language checking of the source sen-
tence detected complex or embedded sentences,
which are often not handled properly by SMT
due to their complicated structure.
? The language checking of the target sentence
detected several agreement issues.
? Parsing provided of source and target count
of verbs, nouns, adjectives and secondary sen-
tences; with the assumption that translations
are relatively isomorphic, the loss of a verb or
a noun or the inability to properly handle a sec-
ondary sentence, would mean a considerably
bad translation outcome. The number of parse
trees generated for each sentence can be an in-
dication of ambiguity.
? Punctuation (dots, commas) often indicates a
complex sentence structure.
? The most useful decoding features were the in-
verse phrase translation probability (a1), the in-
verse lexical weighting (a2), the phrase proba-
bility (pC) and future cost estimate (c) as well
as statistics over their incremental values along
the search graph.
88
feature
set type source target
#19 Baseline LM, %bi q4, punct LM, punct
Checker complex sent, embedded sent pp v plural, nom adj masc
Parsing trees, CC, NP, NN, JJ, comma trees, S, CC, VB, VP, NN, JJ, dot
Decoding avg(a2), a1, a2
#16 Baseline LM, seen, punct, %uni q1, %bi q1,
%bi q4, %tri q4
LM, target occ
Checker score: style, spelling, quality;
verb: agr, form, obj inf, close to subj;
avoid parenth, complex sent,
these those noun, np num agr,
noun adj conf, repeat subj, wrong seq,
wrong word, disamb that, use rel pron,
use article, avoid dangling, repeat modal,
use complement
double punct, to too confusion,
word repeat, det nom sing, pp v plural,
pp v sing, nom adj plural,
comma parenth space, nom adj fem,
nom adj masc, nom adj sing,
det nom fem, del nom sing,
del nom masc, det nom plur
Parsing trees, S, CC, JJ, comma, VB, NP, NN, VP trees, S, CC, JJ, NP, VB, NN, VP, dot, PP
Decoding avg(pC), avg(a1), std(pC), var(c), std(lm),
avg(a2), d2, std(c), a1, a2
Table 5: Indicative feature sets for the most successful quality estimation models. Features explained at section 2.2
Acknowledgments
This work has been developed within the TaraXU?
project financed by TSB Technologiestiftung Berlin
? Zukunftsfonds Berlin, co-financed by the Euro-
pean Union ? European fund for regional develop-
ment. Many thanks to Lukas Poustka for technical
help on feature acquisition, to Melanie Siegel for the
proprietary language checking tool, and to the re-
viewers for the useful comments.
References
Eleftherios Avramidis, Maja Popovic, David Vilar,
Aljoscha Burchardt, and Maja Popovic?. 2011. Evalu-
ate with Confidence Estimation : Machine ranking of
translation outputs using grammatical features. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation, pages 65?70, July.
Eleftherios Avramidis. 2011. DFKI System Combina-
tion with Sentence Ranking at ML4HMT-2011. In
Proceedings of the International Workshop on Using
Linguistic Information for Hybrid Machine Transla-
tion (LIHMT 2011) and of the Shared Task on Applying
Machine Learning Techniques to Optimising the Di-
vision of Labour in Hybrid Machine Translation (M.
Sha. Center for Language and Speech Technologies
and Applications (TALP), Technical University of Cat-
alonia.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
William S Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American statistical association, 74(368):829?836.
Janez Dems?ar, Blaz Zupan, Gregor Leban, and Tomaz
Curk. 2004. Orange: From Experimental Machine
Learning to Interactive Data Mining. In Principles of
Data Mining and Knowledge Discovery, pages 537?
539.
Jerome H. Friedman. 1991. Multivariate Adaptive Re-
gression Splines. The Annals of Statistics, 19(1):1?67,
March.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without reference
translations : Beyond language modeling. Language,
(2001):103?111.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
89
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
Mark A Hall. 2000. Correlation-based Feature Selec-
tion for Discrete and Numeric Class Machine Learn-
ing. In Pat Langley, editor, Proceedings of 17th In-
ternational Conference on Machine Learning, pages
359?366. Morgan Kaufmann Publishers Inc.
Igor Kononenko. 1994. Estimating attributes: anal-
ysis and extensions of RELIEF. In Proceedings of
the European conference on machine learning on Ma-
chine Learning, pages 171?182, Secaucus, NJ, USA.
Springer-Verlag New York, Inc.
S Kullback and R A Leibler. 1951. On information and
sufficiency. Annals of Mathematical Statistics, 22:49?
86.
M Anto`nia Mart?? Mariona Taule? and Marta Recasens.
2008. AnCora: Multilevel Annotated Corpora for
Catalan and Spanish. In Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, May.
European Language Resources Association (ELRA).
Kristen Parton, Joel Tetreault, Nitin Madnani, and Martin
Chodorow. 2011. E-rating Machine Translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 108?115, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In In HLT-NAACL 07.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433?440, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Christopher B Quirk. 2001. Training a Sentence-Level
Machine Translation Confidence Measure. Evalua-
tion, pages 825?828.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Sma??li. 2009. New Confidence Measures
for Statistical Machine Translation. Proceedings of the
International Conference on Agents, pages 394?401.
Felipe Sa?nchez-Martinez. 2011. Choosing the best ma-
chine translation system to translate a sentence by us-
ing only source-language information. In Mikel L For-
cada, Heidi Depraetere, and Vincent Vandeghinste, ed-
itors, Proceedings of the 15th Annual Conference of
the European Associtation for Machine Translation,
number May, pages 97?104, Leuve, Belgium. Euro-
pean Association for Machine Translation.
Lucia Specia, M. Turchi, N. Cancedda, M. Dymetman,
and N. Cristianini. 2009. Estimating the Sentence-
Level Quality of Machine Translation Systems. In
13th Annual Meeting of the European Association for
Machine Translation (EAMT-2009), pages pp. 28?35,
Barcelona, Spain.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901?904. ISCA, September.
M Stone and R J Brooks. 1990. Continuum regres-
sion: cross-validated sequentially constructed predic-
tion embracing ordinary least squares, partial least
squares and principal components regression. Journal
of the Royal Statistical Society Series B Methodologi-
cal, 52(2):237?269.
R Tibshirani. 1994. Regression shrinkage and selection
via the lasso.
Nicola Ueffing and Hermann Ney. 2007. Word-
Level Confidence Estimation for Machine Translation.
Computational Linguistics, 33(1):9?40.
90
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 329?336,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Machine learning methods for comparative and time-oriented
Quality Estimation of Machine Translation output
Eleftherios Avramidis and Maja Popovic?
German Research Center for Artificial Intelligence (DFKI GmbH)
Language Technology Lab
Alt Moabit 91c, 10559 Berlin
eleftherios.avramidis@dfki.de and maja.popovic@dfki.de
Abstract
This paper describes a set of experi-
ments on two sub-tasks of Quality Esti-
mation of Machine Translation (MT) out-
put. Sentence-level ranking of alternative
MT outputs is done with pairwise classi-
fiers using Logistic Regression with black-
box features originating from PCFG Pars-
ing, language models and various counts.
Post-editing time prediction uses regres-
sion models, additionally fed with new
elaborate features from the Statistical MT
decoding process. These seem to be better
indicators of post-editing time than black-
box features. Prior to training the models,
feature scoring with ReliefF and Informa-
tion Gain is used to choose feature sets of
decent size and avoid computational com-
plexity.
1 Introduction
During the recent years, Machine Translation
(MT) has reached levels of performance which al-
low for its integration into real-world translation
workflows. Despite the high speed and various ad-
vantages of this technology, the fact that the MT
results are rarely perfect and often require man-
ual corrections has raised a need to assess their
quality, predict the required post-editing effort and
compare outputs from various systems on applica-
tion time. This has been the aim of current re-
search on Quality Estimation, which investigates
solutions for several variations of such problems.
We describe possible solutions for two prob-
lems of MT Quality Estimation, as part of
the 8th Shared Task on Machine Translation:
(a) sentence-level quality ranking (1.2) of multi-
ple translations of the same source sentence and
(b) prediction of post-editing time (1.3). We
present our approach on acquiring (section 2.1)
and selecting features (section 2.2), we explain
the generation of the statistical estimation systems
(section 2.3) and we evaluate the developed solu-
tions with some of the standard metrics (section 3).
2 Methods: Quality Estimation as
machine learning
These two Quality Estimation solutions have been
seen as typical supervised machine learning prob-
lems. MT output has been given to humans, so that
they perform either (a) ranking of the multiple MT
system outputs in terms of meaning or (b) post-
editing of single MT system output, where time
needed per sentence is measured. The output of
these tasks has been provided by the shared task
organizers as a training material, whereas a small
keep-out set has been reserved for testing pur-
poses.
Our task is therefore to perform automatic qual-
ity analysis of the translation output and the trans-
lation process in order to provide features for the
supervised machine learning mechanism, which is
then trained over the corresponding to the respec-
tive human behaviour. The task is first optimized
in a development phase in order to produce the two
best shared task submissions for each task. These
are finally tested on the keep-out set so that their
performance is compared with the ones submitted
by all other shared-task participants.
2.1 Feature acquisition
We acquire two types of sentence-level features,
that are expected to provide hints about the quality
of the generated translation, depending on whether
they have access to internal details of the MT de-
coding process (glass-box) or they are only de-
rived from characteristics of the processed and
generated sentence text (black-box).
329
2.1.1 Black-box features
Features of this type are generated as a result of
automatic analysis of both the source sentence and
the MT output (when applicable), whereas many
of them are already part of the baseline infrastruc-
ture. For all features we also calculate the ratios
of the source to the target sentence. These features
include:
PCFG Features: We parse the text with a PCFG
grammar (Petrov et al, 2006) and we derive the
counts of all node labels (e.g. count of VPs, NPs
etc.), the parse log-likelihood and the number of
the n-best parse trees generated (Avramidis et al,
2011).
Rule-based language correction is a result of
hand-written controlled language rules, that indi-
cate mistakes on several pre-defined error cate-
gories (Naber, 2003). We include the number of
errors of each category as a feature.
Language model scores include the smoothed
n-gram probability and the n-gram perplexity of
the sentence.
Count-based features include count and per-
centage of tokens, unknown words, punctuation
marks, numbers, tokens which do or do not con-
tain characters ?a-z?; the absolute difference be-
tween number of tokens in source and target nor-
malized by source length, number of occurrences
of the target word within the target hypothesis av-
eraged for all words in the hypothesis (type/token
ratio).
Source frequency: A set of eight features in-
cludes the percentage of uni-grams, bi-grams and
tri-grams of the processed sentence in frequency
quartiles 1 (lower frequency words) and 4 (higher
frequency words) in the source side of a parallel
corpus (Callison-Burch et al, 2012).
Contrastive evaluation scores: For the ranking
task, each translation is scored with an automatic
metric (Papineni et al, 2002; Lavie and Agarwal,
2007), using the other translations as references
(Soricut et al, 2012).
2.1.2 Glass-box features
Glass-box features are available only for the time-
prediction task, as a result of analyzing the verbose
output of the Minimum Bayes Risk decoding pro-
cess.
Counts from the best hypothesis: Count
of phrases, tokens, average/minimum/maximum
phrase length, position of longest and shortest
phrase in the source sentence; count of words
unknown to the phrase table, average number of
unknown words first/last position of an unknown
word in the sentence normalized to the number of
tokens, variance and deviation of the position of
the unknown words.
Log probability (pC) and future cost esti-
mate (c) of the phrases chosen as part of the best
translation: minimum and maximum values and
their position in the sentence averaged to the num-
ber of sentences, and also their average, variance,
standard deviation; count of the phrases whose
probability or future cost estimate is lower and
higher than their standard deviation; the ratio of
these phrases to the total number of phrases.
Alternative translations from the search path
of the decoder: average phrase length, average of
the average/variance/standard deviation of phrase
log probability and future cost estimate, count of
alternative phrases whose log probability or future
cost estimate is lower and higher than their stan-
dard deviation.
2.2 Feature selection
Feature acquisition results in a huge number of
features. Although the machine learning mech-
anisms already include feature selection or regu-
larization, huge feature sets may be unusable for
training, due to the high processing needs and the
sparsity or noise they may infer. For this purpose
we first reduce the number of features by scoring
them with two popular correlation measurement
methods.
2.2.1 Information gain
Information gain (Hunt et al, 1966) estimates the
difference between the prior entropy of the classes
and the posterior entropy given the attribute val-
ues. It is useful for estimating the quality of each
attribute but it works under the assumption that
features are independent, so it is not suitable when
strong feature inter-correlation exists. Information
gain is only used for the sentence ranking task af-
ter discretization of the feature values.
2.2.2 ReliefF
ReliefF assesses the ability of each feature to dis-
tinguish between very similar instances from dif-
330
ferent classes (Kononenko, 1994). It picks up a
number of instances in random and calculates a
feature contribution based on the nearest hits and
misses. It is a robust method which can deal with
incomplete and noisy data (Robnik-S?ikonja and
Kononenko, 2003).
2.3 Machine learning algorithms
Machine learning is performed for the two sub-
tasks using common pairwise classification and
regression methods, respectively.
2.3.1 Ranking with pairwise binary
classifiers
For the sub-task on sentence-ranking we used pair-
wise classification, so that we can take advantage
of several powerful binary classification methods
(Avramidis, 2012). We used logistic regression,
which optimizes a logistic function to predict val-
ues in the range between zero and one (Cameron,
1998), given a feature set X:
P (X) = 11 + e?1(a+bX) (1)
The logistic function is fitted using the Newton-
Raphson algorithm to iteratively minimize the
least squares error computed from training data
(Miller, 2002). Experiments are repeated with two
variations of Logistic Regression concerning inter-
nal features treatment: Stepwise Feature Set Selec-
tion (Hosmer, 1989) and L2-Regularization (Lin
et al, 2007).
2.3.2 Regression
For the sub-task on post-editing time prediction,
we experimented with several regression meth-
ods, such as Linear Regression, Partial Least
Squares (Stone and Brooks, 1990), Multivariate
Adaptive Regression Splines (Friedman, 1991),
LASSO (Tibshirani, 1996), Support Vector Regres-
sion (Basak et al, 2007) and Tree-based regres-
sors. Indicatively, Linear regression optimizes co-
efficient ? for predicting a value y, given a feature
vector X:
y = X? + ? (2)
2.4 Evaluation
The ranking task is evaluated by measuring cor-
relation between the predicted and the human
ranking, with the use of Kendall tau (Kendall,
1938) including penalization of ties. We addi-
tionally consider two more metrics specialized in
ranking tasks: Mean Reciprocal Rank - MRR
(Voorhees, 1999) and Normalized Discounted Cu-
mulative Gain - NDGC (Ja?rvelin and Keka?la?inen,
2002), which give better scores to models when
higher ranks (i.e. better translations) are ordered
correctly, as these are more important than lower
ranks.
The regression task is evaluated in terms of Root
Mean Square Error (RMSE) and Mean Average
Error (MAE).
3 Experiment and Results
3.1 Implementation
Relieff is implemented for k=5 nearest neighbours
sampling m=100 reference instances. Information
gain is calculated after discretizing features into
n=100 values
N-gram features are computed with the SRILM
toolkit (Stolcke, 2002) with an order of 5, based
on monolingual training material from Europarl
(Koehn, 2005) and News Commentary (Callison-
Burch et al, 2011). PCFG parsing features are
generated on the output of the Berkeley Parser
(Petrov and Klein, 2007) trained over an English,
a German and a Spanish treebank (Taule? et al,
2008). The open source language tool1 is used
to annotate source and target sentences with lan-
guage suggestions. The annotation process is or-
ganised with the Ruffus library (Goodstadt, 2010)
and the learning algorithms are executed using the
Orange toolkit (Dems?ar et al, 2004).
3.2 Sentence-ranking
The sentence-ranking sub-task has provided train-
ing data for two language pairs, German-English
and English-Spanish. For both sentence pairs,
we train the systems using the provided an-
notated data sets WMT2010, WMT2011 and
WMT2012, while the data set WMT2009 is used
for the evaluation during the development phase.
Data sets are analyzed with black-box feature gen-
eration. For each language pair, the two systems
with the highest correlation are submitted.
We start the development with two feature sets
that have shown to perform well in previous ex-
periments: #24 (Avramidis, 2012) including fea-
tures from PCFG parsing, and #31 which is the
baseline feature set of the previous year?s shared
task (Callison-Burch et al, 2012) and we combine
them (#33). Additionally, we create feature sets by
1Open source at http://languagetool.org
331
de-en en-es
id feature-set tau MRR NDGC tau MRR NDGC
#24 previous (Avramidis, 2012) 0.28 0.57 0.78 0.09 0.52 0.75
#31 baseline WMT2012 0.04 0.51 0.74 -0.16 0.43 0.69
#32 vanilla WMT2013 0.04 0.51 0.74 -0.13 0.45 0.70
#33 combine #24 and #31 0.29 0.57 0.78 0.10 0.53 0.75
#41 ReliefF 15 best 0.20 0.56 0.77 0.02 0.48 0.72
#411 ReliefF 5 best 0.22 0.53 0.76 0.19 0.49 0.73
#42 InfGain 15 best 0.15 0.53 0.75 -0.14 0.43 0.69
#43 combine #41 and #42 0.22 0.56 0.77 -0.12 0.44 0.70
#431 combine #41, #42 and #24 0.27 0.60 0.80 0.11 0.54 0.75
Table 1: Development experiments for task 1.2, reporting correlation and ranking scores, tested on the
development set WMT2009.
target feature ?
avg target word occurrence 2.18
pseudoMETEOR 0.71
count of unknown words 0.55
count of dots -0.25
count of commas 0.15
count of tokens -0.13
count of VPs -0.06
PCFGlog -0.02
lmprob 0.01
Table 3: Beta coefficients of the best fitted logistic
regression on the German-English data set (set #33
with Stepwise Feature Set Selection)
scoring features with ReliefF (features #41x) and
Information Gain (#42). Many combinations of all
the above feature-sets are tested and the most im-
portant of them are shown in Table 1. Feature sets
are described briefly in Table 2.
For German-English, we experiment with 14
feature sets, using both variations of Logistic Re-
gression. The two highest tau scores are given by
Stepwise Feature Set Selection using feature sets
#33 and #24. We see that although baseline fea-
tures #31 alone have very low correlation, when
combined with previously successful #24, provide
the best system in terms of tau. Feature set #431
(which combines the 15 features scored higher
with ReliefF, the 15 features scored higher with In-
formation Gain and the feature set #24) succeeds
pretty well on the additional metrics MRR and
NDGC, but it provides slightly lower tau correla-
tion.
For English-Spanish, the correlation of the pro-
duced systems is significantly lower and it ap-
pears that the L2-regularized logistic regression
performs better as classification method. We ex-
periment with 24 feature sets, after more scor-
ing with ReliefF and Inf. Gain. Surprisingly
enough, Kendall tau correlation indicates that the
best model is trained only with features based
target feature ?
count of unknown words -0.55
count of VPs 0.19
count of of PCFG parse trees -0.16
count of tokens 0.15
% of tokens with only letters -0.07
lmprob -0.06
pseudoMETEOR precision -0.05
source/target ratio of parse trees -0.03
Table 4: Most indicative beta coefficients of
the best fitted logistic regression on the English-
Spanish data set (set #431 with L2-regularization)
on counts of numbers and punctuation, combined
with contrastive BLEU score. This seems to rather
overfit a peculiarity of the particular development
set and indeed performs much lower on the final
test set of the shared task (tau=0.04). The second
best feature set (#431) has been described above
and luckily generalizes better on an unknown set.
It is interesting to see that this issue would have
been avoided, if the decision was taken based on
the ranking metrics MRR and NDGC, which pri-
oritize other feature sets. We assume that further
work is needed to see whether these measures are
more expressive and reliable than Kendall tau for
similar tasks.
The fitted ? coefficients (in tables 3 and 4) give
an indication of the importance of each feature
(see equation 1), for each language pair. In both
language pairs, target-side features prevail upon
other features. On the comparison of the models
for the two language pairs (and the ? coefficients
as well) we can see that the model settings and
performance may vary from one language pair to
another. This also requires further investigation,
given that Kendall tau and the other two metrics
indicate different models as the best ones.
The fact that the German-English set is bet-
ter fitted with Stepwise Feature Set Selec-
332
set features
#24 From previous work (Avramidis, 2012):
[s+t]: PCFGlog , count of: unknown words, tokens, PCFG trees, VPs
[t]: pseudoMETEOR
#31 Baseline from WMT12 (Callison-Burch et al, 2012)
[s+t]: tokensavg , lmprob, count of: commas, dots, tokens, avg translations per source word
[s]: avg freq. of low and high freq. bi-grams/tri-grams, % of distinct uni-grams in the corpus
[t]: type/token radio
#32 All 50 ?vanilla? features provided by shared-task baseline software ?Quest?
#411 ReliefF best 5 features
[s+t]: % of numbers, difference between periods of source and target (plain and averaged)
[t]: pseudoBLEU
Table 2: Description of most important feature sets for task 1.2, before internal feature selection of
Logistic Regression is applied. [s] indicates source, [t] indicates target
de-en en-es
set StepFSS L2reg StepFSS L2reg
#24 0.28 0.25 0.09 0.09
#33 0.29 0.26 0.08 0.10
#411 0.22 0.17 -0.25 0.19
#431 0.27 0.25 0.09 0.11
Table 5: Higher Kendall tau correlation (on the
dev. set) is achieved on German-English by us-
ing Stepwise Feature Set Selection, whereas on
English-Spanish by using L2-regularization
tion, whereas the English-Spanish one with L2-
Regularization (table 5) may be explained by
the statistical theory about these two methods:
The Stepwise method has has been proven to be
too bound to particular characteristics of the de-
velopment set (Flom and Cassell, 2007). L2-
Regularization has been suggested as an alterna-
tive, since it generalizes better on broader data
sets, which is probably the case for English-
Spanish.
Our method also seems to perform well when
compared to evaluation metrics which have access
to reference translations, as shown in this year?s
Metrics Shared Task (Macha?c?ek and Ondr?ej,
2013).
3.3 Post-editing time prediction
The training for the model predicting post-editing
time is performed over the entire given data set
and the evaluation is done with 10-fold cross-
validation. We evaluated 8 feature sets with 6 re-
gression methods each, ending up with 48 experi-
ments.
The evaluation of the most indicative regression
models (two best performing ones per feature set)
can be seen in Table 6. We start with a glass-
1 7310 19 28 37 46 55 64 82 91 1001091181271361451541631721811901992082172262350
50
100
150
200
250
300
350
400
450
500
REFHYP
Figure 1: Graphical representation of the values
predicted by the linear regression model with fea-
ture set #6 (blue) against the actual values of the
development set (red)
box feature set, scored with ReliefF and conse-
quently add black-box features. We note the mod-
els that have the lowest Root Mean Square Error
and Mean Average Error.
Our best model seems to be the one built linear
regression using feature set #6. This feature set is
chosen by collecting the 17 best features as scored
by ReliefF and includes both black-box and glass-
box features. How well this model fits the devel-
opment test is represented in Figure 1.
The second best feature set (#8) includes 29
glass-box features with the highest absolute Reli-
efF, joined with the black-box features of the suc-
cessful feature set #6.
More details about the contribution of the most
important features in the linear regression (equa-
tion 2) can be seen in table 7, where the fitted ?
coefficients of each feature are given. The vast
majority of the best contributing features are glass-
box features. Some draft conclusions out of the
coefficients may be that post-editing time is lower
when:
333
id feature set method RMSE MAE
#1 20 glass-box features with highest absolute ReliefF MARS 91.54 59.07SVR 93.57 55.87
#2 9 glass-box features with highest positive ReliefF Lasso 83.20 51.57Linear 83.32 51.72
#3 16 glass-box features with highest positive ReliefF Lasso 77.54 47.16Linear 77.60 47.27
#4 22 glass-box features with highest positive ReliefF Lasso 76.05 46.37Linear 76.17 46.48
#5 Combination of feature sets #1 and #2 MARS 91.54 59.07SVR 93.57 55.87
#6 17 features of any type with highest positive ReliefF Linear 74.70 45.20Lasso 74.75 44.99
#8 Combination of #5 and #6 + counts of tokens Lasso 75.14 44.99PLS 77.63 47.48
#6 First submission Linear 84.27 52.41
#8 Second submission PLS 88.34 53.49
Best models 82.60 47.52
Table 6: Development and submitted experiments for task 1.3
? the longest of the source phrases used for pro-
ducing the best hypothesis appears closer to
the end of the sentence
? the phrases with the highest and the lowest
probability appear closer to the end of the
translated sentence
? there are more determiners in the source
and/or less determiners in the translation
? there are more verbs in the translation and/or
less verbs in the source
? there are fewer alternative phrases with very
high probability
Further conclusions can be drawn after examining
these observations along with the exact operation
of the statistical MT system, which is subject to
further work.
4 Conclusion
We describe two approaches for two respective
problems of quality estimation, namely sentence-
level ranking of alternative translations and pre-
diction of time for post-editing MT output. We
present efforts on compiling several feature sets
and we examine the final contribution of the fea-
tures after training Machine Learning models.
Elaborate decoding features seem to be quite help-
ful for predicting post-editing time.
feature ?
best hyp: position of the longest aligned
phrase in the source sentence averaged to
the number of phrases
-16.652
best hyp: position of phrase with highest
prob. averaged to the num. of phrases -14.824
source: number of determiners -9.312
best hyp: number of determiners 6.189
best hyp: position of phrase with lowest
prob. averaged to the num. of phrases -5.261
best hyp: position of phrase with lowest
future cost estimate averaged to the
number of phrases
-4.282
best hyp: number of verbs -2.818
best hyp: position of phrase with highest
future cost estimate averaged to the
number of phrases
1.002
search: number of alternative phrases
with very low future cost est. -0.528
source: number of verbs 0.467
search: number of alternative phrases
with very high probability 0.355
search: total num. of translation options -0.153
search: number of alternative phrases
with very high future cost estimate -0.142
best hyp: number of parse trees 0.007
source: number of parse trees 0.002
search: total number of hypotheses 0.001
Table 7: Linear regression coefficients for feature
set #6 indicate the contribution of each feature in
the fitted model
334
Acknowledgments
This work has been developed within the TaraXU?
project, financed by TSB Technologiestiftung
Berlin ? Zukunftsfonds Berlin, co-financed by the
European Union ? European fund for regional de-
velopment. Many thanks to Prof. Hans Uszko-
reit for the supervision, Dr. Aljoscha Burchardt,
and Dr. David Vilar for their useful feedback and
to Lukas Poustka for his technical help on feature
acquisition.
References
Avramidis, E. (2012). Comparative Quality Estima-
tion: Automatic Sentence-Level Ranking of Multi-
ple Machine Translation Outputs. In Proceedings
of 24th International Conference on Computational
Linguistics, pages 115?132, Mumbai, India. The
COLING 2012 Organizing Committee.
Avramidis, E., Popovic, M., Vilar, D., and Burchardt,
A. (2011). Evaluate with Confidence Estimation :
Machine ranking of translation outputs using gram-
matical features. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 65?
70, Edinburgh, Scotland. Association for Computa-
tional Linguistics.
Basak, D., Pal, S., and Patranabis, D. C. (2007).
Support vector regression. Neural Information
Processing-Letters and Reviews, 11(10):203?224.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of the
2012 Workshop on Statistical Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montre?al,
Canada. Association for Computational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and Zaidan,
O. (2011). Findings of the 2011 Workshop on Sta-
tistical Machine Translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 22?64, Edinburgh, Scotland. Association for
Computational Linguistics.
Cameron, A. (1998). Regression analysis of count
data. Cambridge University Press, Cambridge UK;
New York NY USA.
Dems?ar, J., Zupan, B., Leban, G., and Curk, T. (2004).
Orange: From Experimental Machine Learning to
Interactive Data Mining. In Principles of Data Min-
ing and Knowledge Discovery, pages 537?539.
Flom, P. L. and Cassell, D. L. (2007). Stopping step-
wise: Why stepwise and similar selection methods
are bad, and what you should use. In NorthEast
SAS Users Group Inc 20th Annual Conference, Bal-
timore, Maryland. 2007.
Friedman, J. H. (1991). Multivariate Adaptive Regres-
sion Splines. The Annals of Statistics, 19(1):1?67.
Goodstadt, L. (2010). Ruffus: a lightweight Python
library for computational pipelines. Bioinformatics,
26(21):2778?2779.
Hosmer, D. (1989). Applied logistic regression. Wiley,
New York [u.a.], 8th edition.
Hunt, E., Martin, J., and Stone, P. (1966). Experiments
in Induction. Academic Press, New York.
Ja?rvelin, K. and Keka?la?inen, J. (2002). Cumulated
gain-based evaluation of IR techniques. ACM Trans.
Inf. Syst., 20(4):422?446.
Kendall, M. G. (1938). A New Measure of Rank Cor-
relation. Biometrika, 30(1-2):81?93.
Koehn, P. (2005). Europarl: A parallel corpus for sta-
tistical machine translation. Proceedings of the tenth
Machine Translation Summit, 5:79?86.
Kononenko, I. (1994). Estimating attributes: analy-
sis and extensions of RELIEF. In Proceedings of
the European conference on machine learning on
Machine Learning, pages 171?182, Secaucus, NJ,
USA. Springer-Verlag New York, Inc.
Lavie, A. and Agarwal, A. (2007). METEOR: An Au-
tomatic Metric for MT Evaluation with High Levels
of Correlation with Human Judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007). Trust
region Newton methods for large-scale logistic re-
gression. In Proceedings of the 24th international
conference on Machine learning - ICML ?07, pages
561?568, New York, New York, USA. ACM Press.
Macha?c?ek, M. . and Ondr?ej, B. (2013). Results of the
WMT13 Metrics Shared Task. In Proceedings of the
8th Workshop on Machine Translation, Sofia, Bul-
garia. Association for Computational Linguistics.
Miller, A. (2002). Subset Selection in Regression.
Chapman & Hall, London, 2nd edition.
Naber, D. (2003). A rule-based style and grammar
checker. Technical report, Bielefeld University,
Bielefeld, Germany.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Petrov, S., Barrett, L., Thibaux, R., and Klein, D.
(2006). Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
335
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 433?440, Syd-
ney, Australia. Association for Computational Lin-
guistics.
Petrov, S. and Klein, D. (2007). Improved inference for
unlexicalized parsing. In Proceedings of the 2007
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Rochester, New York. Association for Compu-
tational Linguistics.
Robnik-S?ikonja, M. and Kononenko, I. (2003). Theo-
retical and Empirical Analysis of ReliefF and RRe-
liefF. Machine Learning, 53(1-2):23?69.
Soricut, R., Wang, Z., and Bach, N. (2012). The SDL
Language Weaver Systems in the WMT12 Quality
Estimation Shared Task. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
pages 145?151, Montre?al, Canada. Association for
Computational Linguistics.
Stolcke, A. (2002). SRILM ? An Extensible Language
Modeling Toolkit. In Proceedings of the Seventh
International Conference on Spoken Language Pro-
cessing, pages 901?904. ISCA.
Stone, M. and Brooks, R. J. (1990). Continuum re-
gression: cross-validated sequentially constructed
prediction embracing ordinary least squares, par-
tial least squares and principal components regres-
sion. Journal of the Royal Statistical Society Series
B Methodological, 52(2):237?269.
Taule?, M., Mart??, A., and Recasens, M. (2008). An-
Cora: Multilevel Annotated Corpora for Catalan and
Spanish. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. European Lan-
guage Resources Association (ELRA).
Tibshirani, R. (1996). Regression shrinkage and selec-
tion via the lasso. Series B:267?288.
Voorhees, E. (1999). TREC-8 Question Answering
Track Report. In 8th Text Retrieval Conference,
pages 77?82, Gaithersburg, Maryland, USA.
336
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 302?306,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Efforts on Machine Learning over
Human-mediated Translation Edit Rate
Eleftherios Avramidis
German Research Center for Artificial Intelligence (DFKI)
Language Technology Lab
Alt Moabit 91c, 10559 Berlin, Germany
eleftherios.avramidis@dfki.de
Abstract
In this paper we describe experiments on
predicting HTER, as part of our submis-
sion in the Shared Task on Quality Esti-
mation, in the frame of the 9th Workshop
on Statistical Machine Translation. In our
experiment we check whether it is possi-
ble to achieve better HTER prediction by
training four individual regression models
for each one of the edit types (deletions,
insertions, substitutions, shifts), however
no improvements were yielded. We also
had no improvements when investigat-
ing the possibility of adding more data
from other non-minimally post-edited and
freely translated datasets. Best HTER pre-
diction was achieved by adding dedupli-
cated WMT13 data and additional features
such as (a) rule-based language correc-
tions (language tool) (b) PCFG parsing
statistics and count of tree labels (c) posi-
tion statistics of parsing labels (d) position
statistics of tri-grams with low probability.
1 Introduction
As Machine Translation (MT) gets integrated into
regular translation workflows, its use as base for
post-editing is radically increased. As a result,
there is a great demand for methods that can auto-
matically assess the MT outcome and ensure that
it is useful for the translator and can lead to more
productive translation work.
Although many agree that the quality of the
MT output itself is not adequate for the profes-
sional standards, there has not yet been a widely-
accepted way to measure its quality on par with
human translations. One such metric, the Hu-
man Translation Edit Rate (HTER) (Snover et
al., 2006), is the focus of the current submission.
HTER is highly relevant to the need of adapting
MT to the needs of translators, as it aims to mea-
sure how far it is from an acceptable equivalent
translation done by humans.
HTER is used here in the frame of Quality Es-
timation, i.e. having the goal of being able to pre-
dict the post-editing effort in a real case environ-
ment, right before the translation is given to the
user, without real access to the correct translation.
For this purpose the text of the source and the pro-
duced translation is analyzed by automatic tools
in order to infer indications (numerical features)
that may be relevant to the quality of the transla-
tion. These features are used in a statistical model
whose parameters are estimated with common su-
pervised Machine Learning techniques.
This work presents an extensive search over var-
ious set-ups and parameters for such techniques,
aiming to build a model that better predicts HTER
over the data of the Shared Task of the 9th Work-
shop on Statistical Machine Translation.
2 New approaches being tested
2.1 Break HTER apart
HTER is a complex metric, in the sense that it is
calculated as a linear function over specific types
of edit distance. The official algorithm performs
a comparison between the MT output and the cor-
rected version of this output by a human translator,
who performed the minimum number of changes.
The comparison results in counting the number of
insertions, deletions, substitutions and shifts (e.g.
reordering). The final HTER score is the total
number of edits divided by the number of refer-
ence words.
HTER =
#insertions + #dels + #subs + #shifts
#reference words
We notice that the metric is clearly based on four
edit types that are seemingly independent of each
other. This poses the question whether the existing
302
approach of learning the entire metric altogether
introduces way too much complexity in the ma-
chine learning process. Instead, we test the hy-
pothesis that it is more effective to build a separate
model for each error type and then put the output
of each model on the overall HTER fraction shown
above.
Following this idea, we score the given transla-
tions again in order to produce all four HTER fac-
tors (insertions, deletions, substitutions and shifts)
and we train four regression models accordingly.
This way, each model can be optimized separately,
in order to better fit the particular error type, unaf-
fected by the noise that other error types may infer.
2.2 Rounding of individual edit type
predictions
Due to the separate model per error type, it is pos-
sible to perform corrections on the predicted error
count for each error type, before the calculation of
the entire HTER score. This may be helpful, given
the observation that continuous statistical models
may produce a real number as prediction for the
count of edits, whereas the actual requirement is
an integer.
Here, we take this opportunity and test the hy-
pothesis that prediction of the overall HTER is bet-
ter, if the output of the four individual models is
rounded to the closest integer, before entered in
the HTER ratio.
2.3 More data by approximating minimal
post-edits
We investigate whether prediction performance
can be improved by adding further data. This rises
from the fact that the original number of sentences
is relatively small, given the amount of usable fea-
tures. Unfortunately, the amount of openly avail-
able resources of minimally post-edited transla-
tions are few, given the fact that this relies on a
costly manual process usually done by profession-
als.
Consequently, we add more training samples,
using reference translations of the source which
are not post-edited. In order to ensure that the ad-
ditional data still resemble minimally post-edited
translations as required for HTER, we include
those additional sentences only if they match spe-
cific similarity criteria. In particular, the trans-
lations are filtered, based on the amount of edits
between the MT output and the reference transla-
tion; sentences with an amount of edits above the
threshold are omitted.
3 Methods
3.1 Machine Learning on a regression
problem
Fitting a statistical model in order to predict con-
tinuous values is clearly a regression problem. The
task takes place on a sentence level, given a set of
features describing the source and translation text,
and the respective edit score for the particular sen-
tence.
For this purpose we use Support Vector Regres-
sion - SVR (Basak et al., 2007), which uses lin-
ear learning machines in order to map a non-linear
function into a feature space induce by a high-
dimensional kernel. Similar to the baseline, the
RBF kernel was used, whose parameters where
adjusted via a grid search, cross-validated (10
folds) on all data that was available for each vari-
ation of the training.
3.2 Features
As explained, the statistical model predicts the
edit counts based on a set of features. Our anal-
ysis focuses on ?black-box? features, which only
look superficially on the given text and the pro-
duced translation, without further knowledge on
how this translation was produced. These features
depend on several automatic extraction mecha-
nisms, mostly based on existing language process-
ing tools.
3.2.1 Baseline features
A big set of features is adopted from the baseline
of the Shared Task description:
Language models: provide the smoothed n-
gram probability and the n-gram perplexity of the
sentence.
Source frequency: A set of eight features in-
cludes the percentage of uni-grams, bi-grams and
tri-grams of the processed sentence in frequency
quartiles 1 (lower frequency words) and 4 (higher
frequency words) in the source side of a parallel
corpus (Callison-Burch et al., 2012).
Count-based features include count and per-
centage of tokens, unknown words, punctuation
marks, numbers, tokens which do or do not con-
tain characters ?a-z?; the absolute difference be-
tween number of tokens in source and target nor-
malized by source length, number of occurrences
303
of the target word within the target hypothesis av-
eraged for all words in the hypothesis (type/token
ratio).
3.2.2 Additional features
Additionally to the baseline features, the following
feature groups are considered:
Rule-based language correction is a result of
hand-written controlled language rules, that indi-
cate mistakes on several pre-defined error cate-
gories (Naber, 2003). We include the number of
errors of each category as a feature.
Parsing Features: We parse the text with a
PCFG grammar (Petrov et al., 2006) and we de-
rive the counts of all node labels (e.g. count of
verb phrases, noun phrases etc.), the parse log-
likelihood and the number of the n-best parse trees
generated (Avramidis et al., 2011). In order to re-
duce unnecessary noise, in some experiments we
separate a group of ?basic? parsing labels, which
include only verb phrases, noun phrases, adjec-
tives and subordinate clauses.
Position statistics: This are derivatives of the
previous feature categories and focus on the po-
sition of unknown words, or node tree tags. For
each of them, we calculate the average position in-
dex over the sentence and the standard deviation of
these indices.
3.3 Evaluation
All specific model parameters were tested with
cross validation with 10 equal folds on the train-
ing data. Cross validation is useful as it reduces
the possibility of overfitting, yet using the entire
amount of data.
The regression task is evaluated in terms of
Mean Average Error (MAE).
4 Experiment setup
4.1 Implementation
The open source language tool
1
is used to an-
notate source and target sentences with automati-
cally detected monolingual error tags. Language
model features are computed with the SRILM
toolkit (Stolcke, 2002) with an order of 5, based on
monolingual training material from Europarl v7.0
(Koehn, 2005) and News Commentary (Callison-
Burch et al., 2011). For the parsing parsing fea-
tures we used the Berkeley Parser (Petrov and
1
Open source at http://languagetool.org
datasets feature set MAE
wmt14 baseline 0.142
wmt14 all features 0.143
wmt14,wmt13 baseline 0.140
wmt14,wmt13 all features 0.138
Table 1: Better scores are achieved when training
with both WMT14 and deduplicated WMT13 data
Klein, 2007) trained over an English and a Span-
ish treebank (Taul?e et al., 2008).
2
Baseline fea-
tures are extracted using Quest and HTER edits
and scores are recalculated by modifying the orig-
inal TERp code. The annotation process is or-
ganised with the Ruffus library (Goodstadt, 2010)
and the learning algorithms are executed using the
Scikit Learn toolkit (Pedregosa et al., 2011).
4.2 Data
In our effort to reproduce HTER in a higher gran-
ularity, we noticed that HTER scoring on the of-
ficial data was reversed: the calculation was per-
formed by using the MT output as reference and
the human post-edition as hypothesis. Therefore,
the denominator on the ?official? scores is the
number of tokens on the MT output. This makes
the prediction even easier, as this number of tokens
is always known.
Apart from the data provided by the WMT14,
we include additional minimally post-edited data
from WMT13. It was observed that about 30% of
the WMT13 data already occurred in the WMT14
set. Since this would negatively affect the credibil-
ity of the cross-fold evaluation (section 3.3) and
also create duplicates, we filtered out incoming
sentences with a string match higher than 85% to
the existing ones.
The rest of the additional data (section 2.3)
was extracted from the test-sets of shared tasks
WMT2008-2011.
5 Results
5.1 Adding data from previous year
Adding deduplicated data from the HTER predic-
tion task of WMT13 (Section 4.2) leads to an im-
provement of about 0.004 of MAE for the best
feature-set, as it can be seen by comparing the re-
spective entries of the two horizontal blocks of Ta-
ble 1.
2
although the Spanish grammar performed purely in this
case and was eliminated as a feature
304
feature set MAE
baseline (b) 0.140
b + language tool 0.141
b + source parse 0.141
b + parse pos 0.142
b + basic parse pos 0.139
b + parse count 0.139
b + low prob trigram pos 0.139
all without char count 0.139
all without lang. tool 0.139
all features 0.138
Table 2: Comparing models built with several dif-
ferent feature sets, including various combinations
of the features described in section 3.2. All models
trained on combination of WMT14 and WMT13
data
5.2 Feature sets
We tested separately several feature sets, addition-
ally to the baseline feature set and the feature set
containing all features. The feature sets tested
are based on the feature categories explained in
Section 3.2.2 and the results are seen in Table 2.
One can see that there is little improvement on the
MAE score, which is achieved best by using all
features.
Adding individual categories of features on the
baseline has little effect. Namely, the language
tool annotation, the source parse features and the
source and target parse positional features deteri-
orate the MAE score, when added to the baseline
features.
On the contrary, there is a small positive con-
tribution by using the position statistics of only
the ?basic? parsing nodes (i.e. noun phrases, verb
phrases, adjectives and subordinate clauses). Sim-
ilarly positive is the effect of the count of parsed
node labels for source and target and the features
indicating the position of tri-grams with low prob-
ability (lower than the deviation of the mean). Al-
though language tool features deteriorate the score
of the baseline model when added, their absense
has a negative effect when compared to the full
feature set.
5.3 Separate vs. single HTER predictor
Table 3 includes comparisons of models that test
the hypothesis mentioned in Section 2.1. For both
models trained over the baseline or with additional
features, the MAE score is higher (worse), when
features mode MAE std +/-
baseline single 0.140 0.012
baseline combined 0.148 0.018
baseline combined round 0.152 0.018
all single 0.138 0.009
all combined 0.160 0.019
all combined round 0.162 0.020
Table 3: The combination of 4 different estima-
tors (combined) does not bring any improvement,
when compared to the single HTER estimator.
Models trained on both WMT14 and WMT13 data
separate models are trained. This indicates that
our hypothesis does not hold, at least for the cur-
rent setting of learning method and feature sets.
Rounding up individual edit type predictions to the
closes integer, before the calculation of the HTER
ratio, deteriorates the scores even more.
5.4 Effect of adding non-postedited sentences
In Table 4 we can see that adding more data, which
are not minimally post-edited (but normal refer-
ences), does not contribute to a better model, even
if we limit the number of edits. The lowest MAE
is 0.176, when compared to the one of our best
model which is 0.138.
The best score when additional sentences are
imported, is achieved by allowing sentences that
have between up to edits, and particularly up to 3
substitutions and up to 1 deletion. Increasing the
number of edits on more than 4, leads to a further
deterioration of the model. One can also see that
adding training instances where MT outputs did
not require any edit, also yields scores worse than
the baseline.
6 Conclusion and further work
In our submission, we process the test set with the
model using all features (Table 2). We addition-
ally submit the model trained with additional fil-
tered sentences, as indicated in the second row of
Table 4.
One of the basic hypothesis of this experiment,
that each edit type can better be learned individu-
ally, was not confirmed given these data and set-
tings. Further work could include more focus on
the individual models and more elaborating on
features that may be specific for each error type.
305
del ins sub shifts total add. sentences MAE std+/-
0 0 0 0 0 275 0.177 0.049
1 0 3 0 4 480 0.176 0.040
1 0 2 0 3 433 0.177 0.040
0 0 4 0 4 432 0.177 0.040
2 1 0 0 3 296 0.177 0.048
2 0 3 0 5 530 0.178 0.038
4 0 2 0 6 485 0.178 0.041
4 4 0 0 8 310 0.178 0.046
2 1 0 1 4 309 0.178 0.047
1 0 5 0 6 558 0.179 0.039
1 4 5 0 10 1019 0.200 0.031
Table 4: Indicative MAE scores achieved by adding filtered not minimally post-edited WMT translation
References
Eleftherios Avramidis, Maja Popovi?c, David Vilar, and
Aljoscha Burchardt. 2011. Evaluate with Confi-
dence Estimation : Machine ranking of translation
outputs using grammatical features. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 65?70, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Debasish Basak, Srimanta Pal, and Dipak Chandra Pa-
tranabis. 2007. Support vector regression. Neu-
ral Information Processing-Letters and Reviews,
11(10):203?224.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, pages 22?64, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Leo Goodstadt. 2010. Ruffus: a lightweight Python
library for computational pipelines. Bioinformatics,
26(21):2778?2779.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. Proceedings of the
tenth Machine Translation Summit, 5:79?86.
Daniel Naber. 2003. A rule-based style and gram-
mar checker. Technical report, Bielefeld University,
Bielefeld, Germany.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in python. Journal
of Machine Learning Research, 12:2825?2830.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the 2007 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Rochester, New York. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Matthew Snover, B Dorr, Richard Schwartz, L Micci-
ulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of Association for Machine Translation
in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901?904. ISCA, September.
Mariona Taul?e, Ant`onia Mart??, and Marta Recasens.
2008. AnCora: Multilevel Annotated Corpora for
Catalan and Spanish. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco,
May. European Language Resources Association
(ELRA).
306
