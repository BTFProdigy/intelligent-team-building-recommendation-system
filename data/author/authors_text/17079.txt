Proceedings of NAACL-HLT 2013, pages 569?578,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
On Quality Ratings for Spoken Dialogue Systems ? Experts vs. Users
Stefan Ultes, Alexander Schmitt, and Wolfgang Minker
Ulm University
Albert-Einstein-Allee 43
89073 Ulm, Germany
{stefan.ultes,alexander.schmitt,wolfgang.minker}@uni-ulm.de
Abstract
In the field of Intelligent User Interfaces, Spo-
ken Dialogue Systems (SDSs) play a key role
as speech represents a true intuitive means
of human communication. Deriving informa-
tion about its quality can help rendering SDSs
more user-adaptive. Work on automatic esti-
mation of subjective quality usually relies on
statistical models. To create those, manual
data annotation is required, which may be per-
formed by actual users or by experts. Here,
both variants have their advantages and draw-
backs. In this paper, we analyze the relation-
ship between user and expert ratings by in-
vestigating models which combine the advan-
tages of both types of ratings. We explore two
novel approaches using statistical classifica-
tion methods and evaluate those with a pre-
existing corpus providing user and expert rat-
ings. After analyzing the results, we eventu-
ally recommend to use expert ratings instead
of user ratings in general.
1 Introduction and Motivation
In human-machine interaction it is important that
user interfaces can adapt to the specific requirements
of its users. Handicapped persons or angry users, for
example, have specific needs and should be treated
differently than regular users.
Speech is a major component of modern user in-
terfaces as it is the natural means of human com-
munication. Therefore, it seems logical to use Spo-
ken Dialogue Systems (SDS) as part of Intelligent
User Interfaces enabling speech communication of
different complexity reaching from simple spoken
commands up to complex dialogues. Besides the
spoken words, the speech signal also may be used
to acquire information about the user state, e.g.,
about their emotional state (cf., e.g., (Polzehl et
al., 2011))). By additional analysis of the human-
computer-dialogues, even more abstract informa-
tion may be derived, e.g., the quality of the system
(cf., e.g., (Engelbrecht and Mo?ller, 2010)). System
quality information may be used to adapt the sys-
tem?s behavior online during the ongoing dialogue
(cf. (Ultes et al, 2012)).
For determining the quality of Spoken Dialogue
Systems, several aspects are of interest. Mo?ller et
al. (2009) presented a taxonomy of quality criteria.
They describe quality as a bipartite issue consisting
of Quality of Service (QoS) and Quality of Experi-
ence (QoE). Quality of Service describes objective
criteria like dialogue duration or number of turns.
While these are well-defined items that can be de-
termined easily, Quality of Experience, which de-
scribes the user experience with subjective criteria,
is more vague and without a sound definition, e.g.,
User Satisfaction (US).
Subjective aspects like US are either determined
by using questionnaires like SASSI (Hone and Gra-
ham, 2000) or the ITU-standard augmented frame-
work for questionnaires (Mo?ller, 2003), or by us-
ing single-valued ratings, i.e., a rater only applies
one single score. In general, two major categories
of work on determining single-valued User Satisfac-
tion exist. The satisfaction ratings are applied either
? by users during or right after the dialogue or
? by experts by listening to recorded dialogues.
569
In this work, users or user raters are people who
actually perform a dialogue with the system and ap-
ply ratings while doing so. There is no constraint
about their expertise in the field of Human Com-
puter Interaction or Spoken Dialogue Systems: They
may be novices or have a high expertise. With ex-
perts or expert raters, we refer to people who are
not participating in the dialogue thus constituting
a completely different set of people. Expert raters
listen to recorded dialogues after the interactions
and rate them by assuming the point of view of the
actual person performing the dialogue. These ex-
perts are supposed to have some experience with di-
alogue systems. In this work, expert raters were ?ad-
vanced students of computer science and engineer-
ing? (Schmitt et al, 2011a).
For User Satisfaction, ratings applied by the users
seem to be clearly the better choice over ratings ap-
plied by third persons. However, determining true
User Satisfaction is only possible by asking real
users interacting with the system. Ideally, the ratings
are applied by users talking to a system employed in
the field, e.g., commercial systems, as these users
have real concerns.
For such Spoken Dialogue Systems, though, it
is not easy to get users to apply quality ratings
to the dialogue ? especially for each system-user-
exchange. The users would have to rate either by
pressing a button on the phone or by speech, which
would significantly influence the performance of the
dialogue. Longer dialogues imply longer call dura-
tions which cost money. Further, most callers only
want to quickly get some information from the sys-
tem. Therefore, it may be assumed that most users
do not want to engage in dialogues which are ar-
tificially made longer. This also inhabits the risk
that users who participated in long dialogues do
not want to call again. Therefore, collecting rat-
ings applied by users are considered to be expensive.
One possible way of overcoming the problem of rat-
ing input would be to use some special installation
which enables the users to provide ratings more eas-
ily (cf. (Schmitt et al, 2011b)). However, this is also
expensive and the system?s usability would be very
restricted. Further, this setup could most likely only
be used in a lab situation.
Expert raters, on the other hand, are able to simply
listen to the recorded dialogues and to apply ratings,
e.g., by using a specialized rating software. This
process is much easier and does not require the same
amount of effort needed for acquiring user ratings.
Further, as already pointed out, we refer to experts
as people who have some basic understanding of di-
alogue systems but are not required to be high-level
experts in the field. That is why we believe that these
people can be found easily.
As both categories of ratings have their advan-
tages and disadvantages, this contribution aims at
learning about the differences and similarities of
user and expert ratings with the ultimate goal of
either being able to predict user ratings more effi-
ciently or of advocating for replacing the use of user
ratings by using only expert ratings in general.
Therefore, this work analyzes the relation be-
tween quality ratings applied by user and expert
raters by analyzing approaches which take advan-
tage of both categories: Using the less expensive
rating process with expert raters and still predict-
ing real User Satisfaction ratings. Moreover, this
works? goal is to shed light on the question whether
information about one rating (in this case the less
expensive expert ratings) may be used to predict the
other rating (the more expensive user ratings). For
this, we present two approaches applying two differ-
ent statistical classification methods for a showcase
corpus. Results of both methods are compared to a
given baseline.
The remainder of this paper is organized as fol-
lows. First, we give a brief overview of work done
in both categories (user ratings vs. expert ratings) in
Section 2 and present our choice of data the analy-
sis in this paper is based on in Section 3. Further,
evaluation metrics are illustrated in Section 4 and
approaches on facilitating prediction of user rater
scores by expert rater information are presented in
Section 5 followed by an evaluation and discussion
of the results in Section 6.
2 Significant Related Work
Predicting User Satisfaction for SDSs has been in
the focus of research for many years, most famously
the PARADISE framework by Walker et al (1997).
The authors assume a linear dependency between
quantitative parameters derived from the dialogue
and US, modeling this dependency using linear re-
570
gression. Unfortunately, for generating the regres-
sion model, weighting factors have to be computed
for each system anew. This generates high costs
as dialogues have to be performed with real users
where each user further has to complete a question-
naire after completing the dialogue. Moreover, in
the PARADISE framework, only quality measure-
ment for the whole dialogue (or system) is allowed.
However, this is not suitable for using quality infor-
mation for online adaption of the dialogue (cf. (Ultes
et al, 2012)). Furthermore, PARADISE relies on
questionnaires while we focus on work using single-
valued ratings.
Numerous work on predicting User Satisfaction
as a single-valued rating task for each system-user-
exchange has been performed in both categories.
This work is briefly presented in the following.
2.1 Expert Ratings
Higashinaka et al (2010a) proposed a model to pre-
dict turn-wise ratings for human-human dialogues
(transcribed conversation) and human-machine di-
alogues (text from chat system). Ratings ranging
from 1-7 were applied by two expert raters label-
ing ?Smoothness?, ?Closeness?, and ?Willingness?
not achieving a Match Rate per Rating (MR/R)1 of
more than 0.2-0.24. This results are only slightly
above the random baseline of 0.14. Further work
by Higashinaka et al (2010b) uses ratings for over-
all dialogues to predict ratings for each system-
user-exchange. Again, evaluating in three user
satisfaction categories ?Smoothness?, ?Closeness?,
and ?Willingness? with ratings ranging from 1-7
achieved best performance of 0.19 MR/R.
Interaction Quality (IQ) has been introduced by
Schmitt et al (2011a) as an alternative performance
measure to User Satisfaction. In their terminology,
US ratings are only applied by users. As their pre-
sented measure uses ratings applied by expert raters,
a different term is used. Each system-user exchange
was annotated by three different raters using strict
guidelines. The ratings ranging from 1-5 are used
as target variable for statistical classifiers using a set
of automatically derivable interaction parameters as
input. They achieve a MR/R of 0.58.
1MR/R is equal to Unweighted Average Recall (UAR)
which is explained in Section 4.
2.2 User Ratings
An approach presented by Engelbrecht et al (2009)
uses Hidden Markov Models (HMMs) to model the
SDS as a process evolving over time. User Satisfac-
tion was predicted at any point within the dialogue
on a 5 point scale. Evaluation was performed based
on labels the users applied themselves during the di-
alogue.
Hara et al (2010) derived turn level ratings from
an overall score applied by the users after the dia-
logue. Using n-gram models reflecting the dialogue
history, the achieved results for recognizing User
Satisfaction on a 5 point scale showed to be hardly
above chance.
Work by Schmitt et al (2011b) deals with deter-
mining User Satisfaction from ratings applied by the
users themselves during the dialogues. A statistical
classification model was trained using automatically
derived interaction parameter to predict User Satis-
faction for each system-user-exchange on a 5-point
scale achieving an MR/R of 0.49.
3 Corpus
The corpus used by Schmitt et al (2011b) not only
contains user ratings but also expert ratings which
makes it a perfect candidate for our research pre-
sented in this paper. Adopting the terminology by
Schmitt et al, user ratings are described as User Sat-
isfaction (US) whereas expert ratings are referred to
with the term Interaction Quality (IQ) (cf. (Schmitt
et al, 2011a)). The data used for all experiments
of this work was collected by Schmitt et al (2011b)
during a lab user study with 38 users in the domain
of the ?Let?s Go Bus Information? system (Raux et
al., 2006) of the Carnegie Mellon University in Pitts-
burgh. 128 calls were collected consisting of a total
of 2,897 system-user exchanges. Both ratings, IQ
and US, are at a scale from 1 to 5 where 1 stands for
?extremely unsatisfied? and 5 for ?satisfied?. Each
dialogue starts with a rating of 5 as the user is ex-
pected to be satisfied in the beginning because noth-
ing unsatisfying has happened yet.
Further, the corpus also provides interaction pa-
rameters which may be used as input variables
for the IQ and US recognition models. These
parameters have been derived automatically from
three dialogue modules: Automatic Speech Recog-
571
s 1
u 1
s 2
u 2
s 3
u 3
s n
u n
?
e 1
e 2
e 3
e n
e n
?
e n-
1
e n-
2
e 1
e 2
e 3
e n+
1
?
ex
ch
an
ge
 le
ve
l p
ara
me
ter
s
wi
nd
ow
 le
ve
l p
ara
me
ter
s: 
{#
}, {
M
ea
n}
, e
tc.
dia
log
ue
 le
ve
l p
ara
me
ter
s: 
#, 
M
ea
n, 
etc
.
Figure 1: The three different modeling levels representing the interaction at exchange en: The most detailed exchange
level, comprising parameters of the current exchange; the window level, capturing important parameters from the
previous n dialog steps (here n = 3); the dialog level, measuring overall performance values from the entire previous
interaction.
nition, Spoken Language Understanding, and Dia-
logue Management. Furthermore, the parameters
are modeled on three different levels (see Figure 1):
? Exchange level parameters can be derived di-
rectly from the respective dialogue modules,
e.g., ASRConfidence.
? Dialogue level parameters consist of counts (#),
means (Mean), etc. of the exchange level pa-
rameters calculated from all exchanges of the
whole dialogue up to the current exchange, e.g.,
MeanASRConfidence.
? Window level parameters consist of counts
({#}), means ({Mean}), etc. of the exchange
level parameters calculated from the last three
exchanges, e.g., {Mean}ASRConfidence.
4 Evaluation metrics
For measuring the performance of the classification
algorithms, we rely on Unweighted Average Recall
(UAR), Cohen?s Kappa and Spearman?s Rho. The
latter two also represent a measure for similarity of
paired data. All measures will be briefly described
in the following:
Unweighted Average Recall The Unweighted Av-
erage Recall (UAR) is defined as the sum of all
class-wise recalls rc divided by the number of
classes |C|:
UAR =
1
|C|
?
c?C
rc . (1)
Recall rc for class c is defined as
rc =
1
|Rc|
|Rc|?
i=1
?hiri , (2)
where ? is the Kronecker-delta, hi and ri rep-
resent the corresponding hypothesis-reference-
pair of rating i, and |Rc| the total number of
all ratings of class c. In other words, UAR
for multi-class classification problems is the ac-
curacy corrected by the effects of unbalanced
data.
Cohen?s Kappa To measure the relative agreement
between two corresponding sets of ratings, the
number of label agreements corrected by the
chance level of agreement divided by the max-
imum proportion of times the labelers could
agree is computed. ? is defined as
? =
p0 ? pc
1? pc
, (3)
where p0 is the rate of agreement and pc is the
chance agreement (Cohen, 1960). As US and
IQ are on an ordinal scale, a weighting factor w
is introduced reducing the discount of disagree-
ments the smaller the difference is between two
ratings (Cohen, 1968):
w =
|r1 ? r2|
|rmax ? rmin|
. (4)
Here, r1 and r2 denote the rating pair and rmax
and rmin the maximal and minimal rating. This
results inw = 0 for agreement andw = 1 if the
ratings have maximal difference.
Spearman?s Rho The correlation of two variables
describes the degree by that one variable can be
expressed by the other. Spearman?s Rank Cor-
relation Coefficient is a non-parametric method
assuming a monotonic function between the
572
two variables (Spearman, 1904). It is defined
by
? =
?
i(xi ? x?)(yi ? y?)??
i(xi ? x?)
2
?
i(yi ? y?)
2
, (5)
where xi and yi are corresponding ranked rat-
ings and x? and y? the mean ranks. Thus, two
sets of ratings can have total correlation even if
they never agree. This would happen if all rat-
ings are shifted by the same value, for example.
5 Recognition of US Using IQ Information
As discussed in Section 1, automatic recognition of
ratings applied by users as performed by Schmitt et
al. (2011b) for User Satisfaction is time-consuming
and expensive. Therefore, approaches are presented
which facilitate expert ratings, i.e., Interaction Qual-
ity, with the hope of making US recognition more
feasible. IQ an US are strongly related as both met-
rics represent the same quantity applied by differ-
ent rater groups. Results of the Mann-Whitney U
test, which is used to test for significant difference
between Interaction Quality and User Satisfaction,
show their difference (p < 0.05) but values for Co-
hen?s Kappa (Cohen, 1960) and Spearman?s Rank
Correlation Coefficient (Spearman, 1904) empha-
size the that IQ and US are quite similar. Achieving
? = 0.5 can be considered as a moderate agreement
according to Landis and Koch?s Kappa Benchmark
Scale (Landis and Koch, 1977). Furthermore, a cor-
relation of ? = 0.66 (p < 0.01) indicates a strong
relationship between IQ and US (Cohen, 1988).
While it has been shown that user and expert rat-
ings are similar, it is desirable nonetheless to being
able to predict real user ratings. These ratings are the
desired kind of ratings when it comes to subjective
dialogue system assessment. Only users can give a
rating about their satisfaction level, i.e., how they
like the system and the interaction with the system.
However, user ratings are expensive as elaborated in
Section 1. Therefore, we investigate approaches to
recognize US which rely on means of IQ recogni-
tion.
5.1 Belief-Based Sequential Recognition
Methods used for IQ and US recognition by Schmitt
et al (2011b; 2011a) suffer from the fact that the
sequential character of the data is modeled inade-
quately as they assume statistical independence be-
tween the single exchanges (recognition of IQ and
US does not depend on the respective value of the
previous exchange). Hence, we present a Marko-
vian approach overcoming these issues. A probabil-
ity distribution over all US states, called belief state,
is updated after each system-user-exchange taking
also into account the belief state of the previous ex-
change. This belief update2 is equivalent to the For-
ward Algorithm known from Hidden Markov Mod-
els (cf. (Rabiner, 1989)). In doing so, the new US
probabilities also depend on the US values of the
previous exchange. Moreover, a latent variable is
introduced in order to decouple the target variable
US with the variable the observation probability de-
pends on IQ. This results in an indirect approach
for recognizing User Satisfaction that is based on the
more affordable recognition of Interaction Quality
assuming that a universal mapping between IQ and
US exists.
Thus, to determine the probability b(US) of hav-
ing the true User Satisfaction label US after the cur-
rent system-user-exchange, we rely on Interaction
Quality recognition, whose observation probability
is depicted as P (o|IQ). Furthermore, for coupling
both quantities, we introduce a coherence probabil-
ity P (IQ|US). Belief update for estimating the new
values for b?(US?) is as follows:
b?(US?) = ? ?
?
IQ?
P (o?|IQ?) ? P (IQ?|US?)
?
?
US
P (US?|US)b(US) (6)
The observation probability P (o?|IQ?) is modeled
using confidence scores of classifiers applied for IQ
recognition. Further, we compute the sum over all
previous US beliefs b(US) weighted by the transi-
tion probability P (US?|US). Both, transition and
coherence probability have been computed by tak-
ing the frequency of their occurrences in the training
data. The ? factor is used for normalization only.
Since we are aiming at generating an estimate U?S
2Terminology is taken from Partially Observable Markov
Decision Processes, cf. (Kaelbling et al, 1998)
573
at each exchange, it is calculated by
U?S = arg max
US?
b?(US?) (7)
generating a sequence of estimates for each dia-
logue.
As the action of the system a can be expected to
influence the satisfaction level of the user, action-
dependency is added to Equation 6 resulting in
b?(US?) = ? ?
?
IQ?
P (o?|IQ?) ? P (IQ?|US?, a)
?
?
US
P (US?|US, a)b(US). (8)
Hence, each system action a influences coherence
and transition probabilities. It should be noted that
action-dependency can only be introduced as in a
SDS each turn a system action is selected and ex-
ecuted by the dialogue manager.
5.2 Model Exchange
While in Belief-Based Sequential Recognition, prob-
ability models are used for coupling expert and user
ratings explicitly, a simpler approach has also been
examined. A statistical classifier trained on the tar-
get variable IQ is used to evaluate classification of
the target variable US. This seems to be reasonable
as the set of scores and meaning of the scores of both
metrics are equivalent. Furthermore, necessary pre-
requisites are fulfilled: the sample corpus contains
both labels, the labels for US and IQ correspond, and
both recognition approaches are based on the same
feature set.
6 Experiments and Results
For evaluating Belief-Based Sequential Recognition,
not only the absolute performance is of interest but
also how this performance is influenced by the char-
acteristics of the observation probability, i.e., the
performance of the applied statistical classification
approach and the variance of their confidence scores.
In order to obtain different confidence characteris-
tics, multiple classification algorithms, or algorithm
variants respectively, are needed. Hence, five statis-
tical classifiers have been chosen arbitrarily to pro-
duce the observation probabilities for Belief-Based
Sequential Recognition:
? SVM3 with cubic kernel
? SVM with RBF-kernel
? Naive Bayes
? Naive Bayes with kernel
? Rule Induction
In contrast to Schmitt et al (2011b; 2011a), a re-
duced feature set was used consisting of 43 parame-
ters as some textual parameters were removed which
are very specific and take many different values, e.g.,
UTTERANCE (the system utterance) or INTERPRE-
TATION (the interpretation of the speech input).
The resulting feature set consists of the following
parameters (parameter names are in accordance with
the parameter names of the LEGO corpus (Schmitt
et al, 2012)):
Exchange Level ACTIVITY, ACTIVITYTYPE,
UTD, BARGED-IN?, ASRCONFIDENCE,
MEANASRCONFIDENCE, TURNNUMBER,
MODALITY, LOOPNAME, ASRRECOGNI-
TIONSTATUS, ROLEINDEX, ROLENAME,
NOISE?, HELPREQUEST?, REPROMPT?,
WPST, WPUT
Dialogue Level #BARGEINS #ASRSUCCESS,
#HELPREQUESTS, #TIMEOUTS, #TIME-
OUTS ASRREJECTIONS, #ASRREJEC-
TIONS, #REPROMPTS, #SYSTEMQUES-
TIONS, #SYSTEMTURNS, #USERTURNS,
%BARGEINS, %ASRSUCCESS, %HEL-
PREQUESTS, %TIMEOUTS, %TIME-
OUTS ASRREJECTIONS, %ASRREJEC-
TIONS, %REPROMPTS
Window Level {#}TIMEOUTS ASRREJCTIONS,
{#}HELPREQUESTS, {#}ASRREJECTIONS,
{MEAN}ASRCONFIDENCE, {#}TIMEOUTS,
{#}REPROMPTS, {#}SYSTEMQUESTIONS,
{#}ASRSUCCESS, {#}BARGEINS
All results are evaluated with respect to the ref-
erence experiment of direct US recognition (US
recognition using models trained on US). This is
performed in accordance to Schmitt et al (2011b)
using the statistical classification algorithms stated
3Support Vector Machine, cf. (Vapnik, 1995)
574
Table 1: Results (UAR, Cohen?s Kappa, and Spearman?s
Rho) of 10-fold cross-validation for US recognition of US
recognition using models trained on US
Classifier UAR ? ?
SVM (cubic Kernel) 0.39 0.33 0.48
SVM (RBF-Kernel) 0.39 0.42 0.55
Naive Bayes 0.36 0.40 0.55
Naive Bayes (Kernel) 0.42 0.44 0.59
Rule Induction 0.50 0.51 0.61
Table 2: Results (UAR, Cohen?s Kappa, and Spearman?s
Rho) of 10-fold cross-validation for US recognition of the
Model Exchange approach (trained on IQ, evaluated on
US)
Classifier UAR ? ?
SVM (cubic Kernel) 0.34 0.42 0.55
SVM (RBF-Kernel) 0.34 0.42 0.58
Naive Bayes 0.35 0.40 0.57
Naive Bayes (Kernel) 0.34 0.37 0.60
Rule Induction 0.34 0.42 0.59
above. The performance of the reference experiment
is shown in Table 1.
Using the same feature set, these classification al-
gorithms are also applied for the evaluation of the
Model Exchange approach using 10-fold cross val-
idation. Note that the parameters of the classifiers
also remained the same. The data was partitioned
randomly on exchange level, i.e., without regarding
their belonging to a specific dialogue. The measured
results of the Model Exchange approach for the five
classification methods can be seen in Table 2.
While the results are significantly above chance4,
comparing them to the reference experiment reveals
that in terms of UAR the reference experiment out-
performs Model Exchange for all five classifiers.
The achieved ? and ? values show similar scores
for both the reference experiment and the Model Ex-
change approach. However, in the data used for the
experiments, the amount of occurrences of the rat-
ings was not balanced (equal for all classes) which
has been identified as the most likely reason for this
effect.
Experiments for Belief-Based Sequential Recog-
nition have also been performed using 10-fold cross
validation. As complete dialogues and the order
4UAR of 0.2 for five classes
Table 3: Results (UAR, Cohen?s Kappa, and Spearman?s
Rho) of 10-fold cross-validation for US recognition of
action-independent Belief-Based Sequential Recognition
Classifier UAR ? ?
SVM (cubic Kernel) 0.28 0.36 0.48
SVM (RBF-Kernel) 0.30 0.40 0.54
Naive Bayes 0.32 0.39 0.54
Naive Bayes (Kernel) 0.33 0.45 0.61
Rule Induction 0.33 0.47 0.63
Table 4: Results (UAR, Cohen?s Kappa, and Spearman?s
Rho) of 10-fold cross-validation for US recognition of
action-dependent Belief-Based Sequential Recognition
Classifier UAR ? ?
SVM (cubic Kernel) 0.28 0.35 0.48
SVM (RBF-Kernel) 0.29 0.40 0.54
Naive Bayes 0.32 0.40 0.55
Naive Bayes (Kernel) 0.34 0.44 0.60
Rule Induction 0.35 0.47 0.62
of exchanges within the dialogues are important for
this approach, the data was partitioned randomly on
the dialogue level. As previously explained, for the
probability distributions of the observation proba-
bility model, classification results of IQ recognition
with 10-fold cross validation has been used in order
to get good estimates for the whole data set. Re-
sults for the action-independent version can be seen
in Table 3.
For the action-dependent version, four different
basic actions ANNOUNCEMENT, CONFIRMATION,
QUESTION, and WAIT have been used, generat-
ing results presented in Table 4. The results il-
lustrate that neither action-independent nor action-
dependent Belief-Based Sequential Recognition can
outperform the reference experiment (cf. Table 1).
Still, both variants achieve results clearly above
chance. Again, the unbalanced data causes ? and
? to be similar to the reference experiment.
A comparison of the action-independent with the
action-dependent approach shows almost no differ-
ences in their performances. Only a slight tendency
towards better UARs for action-dependency can be
spotted.
Figure 2 displays the performances of both vari-
ants of Belief-Based Sequential Recognition along
with performance of IQ recognition and the vari-
ance ?2 of the corresponding confidence distribu-
575
0.200.300.400.500.60 0.000.10
SVM
?(cub
ic)
SVM
?(RBF
)
Baye
s
Baye
s?(K)
Rule
IQ???
IQ
US?B
elief
US?B
elief
?Acti
on
Figure 2: UAR of IQ recognition and Belief-Based Se-
quential Recognition along with ?2 of confidence distri-
butions of IQ recognition
Table 5: Recognition performance and variance of confi-
dence distributions for IQ recognition
Classifier ?2 UAR ? ?
SVM (cubic Kernel) 0.03 0.38 0.54 0.69
SVM (RBF-Kernel) 0.05 0.48 0.65 0.77
Naive Bayes 0.13 0.49 0.57 0.71
Naive Bayes (Kernel) 0.12 0.52 0.59 0.73
Rule Induction 0.13 0.55 0.68 0.79
tion (cf. Table 5). It can easily be seen that with
rising UAR for IQ recognition, ?2 also rises. This
directly transfers to the performance of the Belief-
Based Sequential Recognition. The more accu-
rate the observation performance, the more accurate
the belief prediction. Furthermore, when compar-
ing the action-dependent to the action-independent
variant of Belief-Based Sequential Recognition, bet-
ter IQ performance and therefore a higher variance
also causes slightly better results for the action-
dependent variant. These differences, however, are
only marginally. Therefore, they do not allow for
drawing a conclusion.
7 Conclusions
For estimating User Satisfaction-like ratings, two
categories exist: work relying on user ratings and
work relying on expert ratings. To learn something
about their differences and similarities, we explored
the possibility of using the information encoded in
the expert ratings to predict user ratings with the
hope to get acceptable user rating prediction results.
Therefore, we investigated if it is possible to de-
termine the preferred true User Satisfaction value
based on less expensive expert ratings. For this, a
corpus containing both kinds of ratings was chosen,
i.e., User Satisfaction (US) and Interaction Qual-
ity (IQ) ratings. Furthermore, interaction parame-
ters were used to create statistical recognition mod-
els for predicting IQ and US, respectively. Two ap-
proaches have been investigated: Belief-Based Se-
quential Recognition, which is based on an HMM-
like structure with IQ as an additional latent variable,
and Model Exchange, which uses statistical models
trained on IQ to recognize US. Unfortunately, nei-
ther Belief-Based Sequential Recognition nor Model
Exchange achieved results with an acceptable UAR.
The high correlation between expert and user rat-
ings, depicted by high values for Cohen?s ? and
Spearman?s ?, already allow the conclusion that ex-
pert ratings can be used as a good replacement for
user ratings. Moreover, the presented recognition re-
sults of the Model Exchange approach being clearly
above chance underpin the strong similarity of IQ
and US. Furthermore, IQ recognition is much more
reliable and accurate than US recognition (shown by
higher UAR, ? and ? values).
While the experiments disproved the hope of get-
ting acceptable user rating prediction results, the ob-
tained results confirmed the similarity between both
kinds of ratings. And as it is not necessary to use
user ratings for most applications, e.g., for using the
quality information to automatically improve the in-
teraction (cf. (Ultes et al, 2012)), we believe that it
suffices to use expert ratings as those can be acquired
easier and less expensively and are similar enough
to user ratings. Prompting the user to apply quality
ratings in everyday situations with real-life systems
will always be annoying to the user while recording
of such interactions are always much easier to rate.
By providing a study for determining quality rat-
ings of dialogues, we hope to encourage other re-
searchers to look into this research for other param-
eters, e.g., emotion recognition.
References
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. In Educational and Psychological Mea-
surement, volume 20, pages 37?46, April.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
576
agreement provision for scaled disagreement or partial
credit. Psychological bulletin, 70(4):213.
Jacob Cohen. 1988. Statistical power analysis for the
behavioral sciences. New Jersey: Lawrence Erlbaum
Associates, July.
Klaus-Peter Engelbrecht and Sebastian Mo?ller. 2010. A
User Model to Predict User Satisfaction with Spoken
Dialog Systems. In Gary Geunbae Lee, Joseph Mari-
ani, Wolfgang Minker, and Satoshi Nakamura, editors,
Spoken Dialogue Systems for Ambient Environments.
2nd Int. Workshop on Spoken Dialogue Systems Tech-
nology, Lecture Notes in Artificial Intelligence, pages
150?155. Springer, October.
Klaus-Peter Engelbrecht, Florian Go?dde, Felix Hartard,
Hamed Ketabdar, and Sebastian Mo?ller. 2009. Mod-
eling user satisfaction with hidden markov model. In
SIGDIAL ?09: Proceedings of the SIGDIAL 2009 Con-
ference, pages 170?177, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.
2010. Estimation method of user satisfaction using
n-gram-based dialog history model for spoken dialog
system. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010a. Issues in
predicting user satisfaction transitions in dialogues:
Individual differences, evaluation criteria, and predic-
tion models. In Gary Lee, Joseph Mariani, Wolfgang
Minker, and Satoshi Nakamura, editors, Spoken
Dialogue Systems for Ambient Environments, volume
6392 of Lecture Notes in Computer Science, pages
48?60. Springer Berlin / Heidelberg.
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010b. Modeling
user satisfaction transitions in dialogues from over-
all ratings. In Proceedings of the SIGDIAL 2010
Conference, pages 18?27, Tokyo, Japan, September.
Association for Computational Linguistics.
Kate S. Hone and Robert Graham. 2000. Towards a tool
for the subjective assessment of speech system inter-
faces (sassi). Nat. Lang. Eng., 6(3-4):287?303.
L. P. Kaelbling, M. L. Littman, and A. R. Cassandra.
1998. Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101(1-
2):99?134.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174, March.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, C. Ku?hnel,
I. Wechsung, and B. Weiss. 2009. A taxonomy of
quality of service and quality of experience of multi-
modal human-machine interaction. In Quality of Mul-
timedia Experience, 2009. QoMEx 2009. International
Workshop on, pages 7?12, July.
Sebastian Mo?ller. 2003. Subjective Quality Evalua-
tion of Telephone Services Based on Spoken Dia-
logue Systems. ITU-T Recommendation P.851, Inter-
national Telecommunication Union, Geneva, Switzer-
land, November. Based on ITU-T Contr. COM 12-59
(2003).
Tim Polzehl, Alexander Schmitt, and Florian Metze.
2011. Salient features for anger recognition in german
and english ivr portals. In Wolfgang Minker, Gary Ge-
unbae Lee, Satoshi Nakamura, and Joseph Mariani,
editors, Spoken Dialogue Systems Technology and De-
sign, pages 83?105. Springer New York. 10.1007/978-
1-4419-7934-6 4.
Lawrence R. Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
lets go! experience. In Proc. of the International Con-
ference on Speech and Language Processing (ICSLP),
September.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011a. Modeling and predicting quality in
spoken human-computer interaction. In Proceedings
of the SIGDIAL 2011 Conference, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011b. A statistical approach for estimat-
ing user satisfaction in spoken human-machine inter-
action. In Proceedings of the IEEE Jordan Confer-
ence on Applied Electrical Engineering and Comput-
ing Technologies (AEECT), Amman, Jordan, Decem-
ber. IEEE.
Alexander Schmitt, Stefan Ultes, and Wolfgang Minker.
2012. A parameterized and annotated corpus of the
cmu let?s go bus information system. In International
Conference on Language Resources and Evaluation
(LREC).
C. Spearman. 1904. The proof and measurement of as-
sociation between two things. American Journal of
Psychology, 15:88?103.
Stefan Ultes, Alexander Schmitt, and Wolfgang Minker.
2012. Towards quality-adaptive spoken dialogue man-
agement. In NAACL-HLT Workshop on Future di-
rections and needs in the Spoken Dialog Commu-
577
nity: Tools and Data (SDCTD 2012), pages 49?52,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Marilyn Walker, Diane Litman, Candace A. Kamm, and
Alicia Abella. 1997. Paradise: a framework for eval-
uating spoken dialogue agents. In Proceedings of the
eighth conference on European chapter of the Associ-
ation for Computational Linguistics, pages 271?280,
Morristown, NJ, USA. Association for Computational
Linguistics.
578
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 49?52,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Towards Quality-Adaptive Spoken Dialogue Management
Stefan Ultes, Alexander Schmitt, Wolfgang Minker
Dialogue Systems - Ulm University
Albert-Einstein-Allee 43
89081 Ulm, Germany
{stefan.ultes,alexander.schmitt,wolfgang.minker}@uni-ulm.de
Abstract
Information about the quality of a Spoken Di-
alogue System (SDS) is usually used only for
comparing SDSs with each other or manually
improving the dialogue strategy. This infor-
mation, however, provides a means for inher-
ently improving the dialogue performance by
adapting the Dialogue Manager during the in-
teraction accordingly. For a quality metric to
be suitable, it must suffice certain conditions.
Therefore, we address requirements for the
quality metric and, additionally, present ap-
proaches for quality-adaptive dialogue man-
agement.
1 Introduction
For years, research has been focused on enabling
Spoken Dialogue Systems (SDSs) to behave more
adaptively to the user?s expectations and needs.
Mo?ller et al (2009) presented a taxonomy for qual-
ity of human-machine interaction, i.e., Quality of
Service (QoS) and Quality of Experience (QoE). For
QoE, several aspects are identified. They contribute
to good user experience, e.g., interaction quality, us-
ability and acceptability. These aspects can be com-
bined to the term User Satisfaction (US), describ-
ing the degree by which the user is satisfied with the
system?s performance. The dialogue community has
been investigating this aspect for years. Most promi-
nently is the PARADISE framework by Walker et al
(2000) which maps objective performance metrics
of an SDS to subjective user ratings.
Recent work mostly discusses how to evaluate
Spoken Dialogue Systems. However, the issue of
how this information can be useful for improv-
ing dialogue performance remains hardly addressed.
Hence, we focus on exploring techniques for incor-
porating dialogue quality information into the Dia-
logue Manager (DM). This is accompanied by the
problem of defining characteristics of a suitable dia-
logue quality metric.
In Section 2, we present related work both on
measuring dialogue quality and on approaches for
incorporating user state information into the DM.
In Section 3, requirements for a quality metric are
presented along with a suitable example. Section 4
presents our ongoing and future work on incorpo-
rating quality measures into dialogue strategies. Fi-
nally, Section 5 concludes this work.
2 Related Work
In recent years, several studies have been published
on determining the qualitative performance of a
SDS. Engelbrecht et al (2009) predicted User Sat-
isfaction on a five-point scale at any point within the
dialogue using Hidden Markov Models (HMMs).
Evaluation was based on labels the users applied
themselves during a Wizard-of-Oz experiment. To
guarantee for comparable conditions, the dialogue
flow was controlled by predefined scenarios creat-
ing transcripts with equal length for each scenario.
Further work based on HMMs was presented by
Higashinaka et al (2010). The HMM was trained on
US rated at each exchange. These exchange ratings
were derived from ratings for the whole dialogue.
The authors compare their approach with HMMs
trained on manually annotated exchanges achieving
a better performance for the latter.
49
In order to predict US, Hara et al (2010) created
n-gram models from dialogue acts (DA). Based on
dialogues from real users interacting with a music
retrieval system, overall ratings for the whole dia-
logue have been labeled on a five point scale after
the interaction. An accuracy (i.e., rate of correctly
predicted ratings) of 34% by a 3-gram model was
the best performance which could be achieved.
Dealing with true User Satisfaction, Schmitt et al
presented their work about statistical classification
methods for automatic recognition of US (Schmitt
et al, 2011b). The data was collected in a lab
study where the users themselves had to rate the
conversation during the ongoing dialogue. Labels
were applied on a scale from 1 to 5. Perform-
ing automatic classification using a Support Vector
Machine (SVM), they achieved an Unweighted Av-
erage Recall (UAR) of 49.2 (i.e., average rate of
correctly predicted ratings, compensated for unbal-
anced data).
An approach for affective dialogue modeling
based on Partially Observable Markov Decision
Processes (POMDPs) was presented by Bui et al
(2007). Adding stress to the dialogue state enables
the dialogue manager to adapt to the user. To make
belief-update tractable, the authors introduced Dy-
namic Decision Networks as means for reducing
complexity.
Pittermann et al (2007) presented another ap-
proach for adaptive dialogue management. The au-
thors incorporated emotions by modeling the dia-
logue in a semi-stochastic way. Thus, an emotional
dialogue model was created as a combination of a
probabilistic emotional model and probabilistic dia-
logue model defining the current dialogue state.
3 Interaction Quality Metric
In order to enable the Dialogue Manager to be
quality-adaptive, the quality metric must suffice cer-
tain criteria. In this Section, we identify the impor-
tant issues and render the requirements for a suitable
quality metric.
3.1 General Aspects
For adapting the dialogue strategy to the quality of
the dialogue, the quality metric is required to imple-
ment certain characteristics. We identify the follow-
ing items:
? exchange-level quality measurement,
? automatically derivable features,
? domain-independent features,
? consistent labeling process,
? reproducible labels and
? unbiased labels.
The performance of a Spoken Dialogue System
may be evaluated either on the dialogue level or on
the exchange level. As dialogue management is per-
formed after each system-user exchange, dynamic
adaption of the dialogue strategy to the dialogue
performance requires exchange-level performance
measures. Therefor, Dialogue-level approaches are
of no use. Furthermore, previous presented meth-
ods for exchange-level quality measuring could not
achieve satisfying accuracy in predicting dialogue
quality (Engelbrecht et al, 2009; Higashinaka et al,
2010).
Features serving as input variables for a classi-
fication algorithm must be automatically derivable
from the dialogue system modules. This is impor-
tant because other features, e.g., manually annotated
dialogue acts (Higashinaka et al, 2010; Hara et al,
2010), produce high costs and are also not available
immediately during run-time in order to use them as
additional input to the Dialogue Manager. Further-
more, for creating a general quality metric, features
have to be domain-independent, i.e., not depending
on the task domain of the dialogue system.
Another important issue is the consistency of the
labels. Labels applied by the users themselves are
subject to large fluctuations among the different
users (Lindgaard and Dudek, 2003). As this results
in inconsistent labels, which do not suffice for creat-
ing a generally valid quality model, ratings applied
by expert raters yield more consistent labels. The
experts are asked to estimate the user?s satisfaction
following previously established rating guidelines.
Furthermore, expert labelers are also not prone to be
influenced by certain aspects of the SDS, which are
not of interest in this context, e.g., the character of
the synthesized voice. Therefore, they create less bi-
ased labels.
50
3.2 Interaction Quality
As metric, which fulfills all previously addressed
requirements, we present the Interaction Quality
(IQ) metric, see also (2011a). Based on dialogues
from the ?Let?s Go Bus Information System? of the
Carnegie Mellon University in Pittsburgh (Raux et
al., 2006), IQ is labeled on a five point scale. The
labels are (from best (5) to worst (1)) ?satisfied?,
?slightly unsatisfied?, ?unsatisfied?, ?very unsatis-
fied? and ?extremely unsatisfied?. They are applied
by expert raters following rating guidelines, which
have been established to allow consistent and repro-
ducible ratings.
Additionally, domain-independent features used
for IQ recognition have been derived from the di-
alogue system modules automatically for each ex-
change grouped on three levels: the exchange level,
the dialogue level, and the window level. As parame-
ters like ASRCONFIDENCE or UTTERANCE can di-
rectly be acquired from the dialogue modules they
constitute the exchange level. Based on this, counts,
sums, means, and frequencies of exchange level pa-
rameters from multiple exchanges are computed to
constitute the dialogue level (all exchanges up to the
current one) and the window level (the three previous
exchanges).
A corpus containing the labeled data has been
published recently (Schmitt et al, in press) contain-
ing 200 calls annotated by three expert labelers, re-
sulting in a total of 4,885 labeled exchanges. Us-
ing statistical classification of IQ based on SVMs
achieves an Unweighted Average Recall of 0.58
(Schmitt et al, 2011a).
4 Quality-Adaptive Spoken Dialogue
Management
The goal of our work is to enable Dialogue Man-
agers to directly adapt to information about the qual-
ity of the ongoing dialogue. We present two differ-
ent approaches that outline our ongoing and future
work.
4.1 Dialogue Design-Patterns for Quality
Adaption
Rule-based Dialogue Managers are still state-of-the-
art for commercial SDSs. It is hardly arguable that
making the rules quality-dependent is a promising
way for dialogue improvement. However, the num-
ber of possibilities for adapting the dialogue strategy
to the dialogue quality is high. Based on the Speech-
Cycle RPA Dialogue Manager, we are planning on
identifying common dialogue situations in order to
create design-patterns. These patterns can be ap-
plied as a general means of dealing with situations
that arise by introducing quality-adaptiveness to the
dialogue.
4.2 Statistical Quality-Adaptive Dialogue
Management
For the incorporation of Interaction Quality into a
statistical DM, two approaches have been found.
First, based on work on factored Partially Observ-
able Markov Decision Processes by Williams and
Young (2007) and similar to Bui et al (2006), we
presented our own approach for incorporating addi-
tional user state information (Ultes et al, 2011).
In the factored POMDP by Williams and Young
(2007), the state of the underlying process is de-
fined as s = (u, g, h). To incorporate IQ, it is
extended by adding the IQ-state siq, resulting in
s = (u, g, h, siq).
Following the concept of user acts, we further in-
troduce IQ-acts iq that describe the current qual-
ity predicted by the classification algorithm for the
current exchange. Incorporating IQ acts into obser-
vation o results in the two-dimensional observation
space
O = U ? IQ,
where U denotes the set of all user actions and IQ
the set of all possible Interaction Quality values.
Second, for training an optimal policy for ac-
tion selection in POMDPs, a reward function has
to be defined. Common reward functions are task-
oriented and based on task success and dialogue
length. As an example, a considerable positive re-
ward is given for reaching the task goal, a consider-
able negative reward for aborting the dialogue, and a
small negative reward for each exchange in order to
keep the dialogue short. Interaction Quality scores
offer an interesting and promising way of defining a
reward function, e.g., by rewarding improvements in
IQ. By that, strategies that try to keep the quality at
an overall high can be trained allowing for a better
user experience.
51
5 Conclusion
For incorporating information about the dialogue
quality into the Dialogue Manager, we identified
characteristics of a quality metric defining neces-
sary prerequisites for being used during dialogue
management. Further, the Interaction Quality met-
ric has been proposed as measure, which suffices all
requirements. In addition, we presented concrete ap-
proaches of incorporating IQ into the DM outlining
our ongoing and future work.
Acknowledgements
We would like to thank Maxine Eskenazi, Alan
Black, Lori Levin, Rita Singh, Antoine Raux and
Brian Langner from the Lets Go Lab at Carnegie
Mellon University, Pittsburgh, for providing the Lets
Go Sample Corpus. We would further like to thank
Roberto Pieraccini and David Suendermann from
SpeechCycle, Inc., New York, for providing the
SpeechCycle RPA Dialogue Manager.
References
T. H. Bui, J. Zwiers, M. Poel, and A. Nijholt. 2006. To-
ward affective dialogue modeling using partially ob-
servable markov decision processes. In Proceedings
of workshop emotion and computing, 29th annual Ger-
man conference on artificial intelligence.
T. H. Bui, M. Poel, A. Nijholt, and J. Zwiers. 2007.
A tractable ddn-pomdp approach to affective dialogue
modeling for general probabilistic frame-based dia-
logue systems. In Proceedings of the 5th IJCAI Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, pages 34?37.
Klaus-Peter Engelbrecht, Florian Go?dde, Felix Hartard,
Hamed Ketabdar, and Sebastian Mo?ller. 2009. Mod-
eling user satisfaction with hidden markov model. In
SIGDIAL ?09: Proceedings of the SIGDIAL 2009 Con-
ference, pages 170?177. ACL.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.
2010. Estimation method of user satisfaction using
n-gram-based dialog history model for spoken dia-
log system. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. ELRA.
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010. Modeling
user satisfaction transitions in dialogues from over-
all ratings. In Proceedings of the SIGDIAL 2010
Conference, pages 18?27, Tokyo, Japan, September.
Association for Computational Linguistics.
Gitte Lindgaard and Cathy Dudek. 2003. What is this
evasive beast we call user satisfaction? Interacting
with Computers, 15(3):429?452.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, C. Ku?hnel,
I. Wechsung, and B. Weiss. 2009. A taxonomy of
quality of service and quality of experience of multi-
modal human-machine interaction. In Quality of Mul-
timedia Experience, 2009. QoMEx 2009. International
Workshop on, pages 7?12, July.
Johannes Pittermann, A. Pittermann, Hong Meng, and
W. Minker. 2007. Towards an emotion-sensitive
spoken dialogue system - classification and dialogue
modeling. In Intelligent Environments, 2007. IE 07.
3rd IET International Conference on, pages 239 ?246,
September.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
lets go! experience. In Proc. of the International Con-
ference on Speech and Language Processing (ICSLP),
September.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011a. Modeling and predicting quality in
spoken human-computer interaction. In Proceedings
of the SIGDIAL 2011 Conference, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011b. A statistical approach for estimat-
ing user satisfaction in spoken human-machine inter-
action. In Proceedings of the IEEE Jordan Confer-
ence on Applied Electrical Engineering and Comput-
ing Technologies (AEECT), Amman, Jordan, Decem-
ber. IEEE.
Alexander Schmitt, Stefan Ultes, and Wolfgang Minker.
in-press. A parameterized and annotated corpus of the
cmu let?s go bus information system. In International
Conference on Language Resources and Evaluation
(LREC).
Stefan Ultes, Tobias Heinroth, Alexander Schmitt, and
Wolfgang Minker. 2011. A theoretical framework for
a user-centered spoken dialog manager. In Proceed-
ings of the Paralinguistic Information and its Integra-
tion in Spoken Dialogue Systems Workshop, pages 241
? 246. Springer, September.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usabil-
ity with paradise. Nat. Lang. Eng., 6(3-4):363?377.
Jason D. Williams and Steve J. Young. 2007. Par-
tially observable markov decision processes for spo-
ken dialog systems. Computer Speech and Language,
(21):393?422.
52
Proceedings of the SIGDIAL 2013 Conference, pages 122?126,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Improving Interaction Quality Recognition Using Error Correction
Stefan Ultes
Ulm University
Albert-Einstein-Allee 43
89081 Ulm, Germany
stefan.ultes@uni-ulm.de
Wolfgang Minker
Ulm University
Albert-Einstein-Allee 43
89081 Ulm, Germany
wolfgang.minker@uni-ulm.de
Abstract
Determining the quality of an ongoing in-
teraction in the field of Spoken Dialogue
Systems is a hard task. While existing
methods employing automatic estimation
already achieve reasonable results, still
there is a lot of room for improvement.
Hence, we aim at tackling the task by es-
timating the error of the applied statistical
classification algorithms in a two-stage ap-
proach. Correcting the hypotheses using
the estimated model error increases per-
formance by up to 4.1 % relative improve-
ment in Unweighted Average Recall.
1 Introduction
Evaluating the quality of Spoken Dialogue Sys-
tems (SDSs) has long since been a challenging
task. While objective metrics like task completion
and dialogue duration are not human-centered,
subjective measures compensate for this by mod-
eling the user?s subjective experience. This infor-
mation may be used to increase the dialogue sys-
tem?s performance (cf. (Ultes et al, 2012b)).
In human-machine dialogues, however, there is
no easy way of deriving the user?s satisfaction
level. Moreover, asking real users for answering
questions about the system performance requires
them to spend more time talking to the machine
than necessary. It can be assumed that a regular
user does not want to do this as human-machine
dialogues usually have no conversational charac-
ter but are task oriented. Hence, automatic ap-
proaches are the preferred choice.
Famous work on determining the satisfaction
level automatically is the PARADISE framework
by Walker et al (1997). Assuming a linear depen-
dency between objective measures and User Satis-
faction (US), a linear regression model is applied
to determine US on the dialogue level. This is not
only very costly, as dialogues must be performed
with real users, but also inadequate if quality on a
finer level is of interest, e.g., on the exchange level.
To overcome this issue, work by Schmitt et
al. (2011) introduced a new metric for measuring
the performance of an SDS on the exchange level
called Interaction Quality (IQ). They used statisti-
cal classification methods to automatically derive
the quality based on interaction parameters. Qual-
ity labels were applied by expert raters after the di-
alogue on the exchange level, i.e., for each system-
user-exchange. Automatically derived parameters
were then used as features for creating a statistical
classification model using static feature vectors.
Based on the same data, Ultes et al (2012a) put
an emphasis on the sequential character of the IQ
measure by applying temporal statistical classifi-
cation using Hidden Markov Models (HMMs) and
Continuous Hidden Markov Models (CHMMs).
However, statistical classifiers usually do not
achieve perfect performance, i.e., there will al-
ways be misclassification. While most work fo-
cuses on applying different statistical models and
improving them (Section 2), learning the error to
correct the result afterwards represents a different
approach. Therefore, we present our approach on
estimating the error of IQ recognition models to
correct their hypothesis in order to eventually yield
better recognition rates (Section 4). The definition
of IQ and data used for the evaluation of our ap-
proach (Section 5) is presented in Section 3. Our
approach is also compared to a simple hierarchical
approach also discussed in Section 5.
2 Related Work on Dialogue Quality
Besides Schmitt et al, other research groups have
performed numerous work on predicting subjec-
tive quality measures on an exchange level, all not
incorporating any form of error correction.
Engelbrecht et al (2009) presented an approach
using Hidden Markov Models (HMMs) to model
122
e n
?
e n-1
e n-2
e1
e n+1
?
exc
han
ge l
eve
l pa
ram
eter
s
win
dow
 lev
el p
aram
eter
s
dial
ogu
e le
vel 
para
met
ers
Figure 1: The three different modeling levels rep-
resenting the interaction at exchange en.
the SDS as a process evolving over time. Perfor-
mance ratings on a 5 point scale (?bad?, ?poor?,
?fair?, ?good?, ?excellent?) have been applied by
the users during the dialogue.
Higashinaka et al (2010) proposed a model for
predicting turn-wise ratings for human-human dia-
logues analyzed on a transcribed conversation and
human-machine dialogues with text from a chat
system. Ratings ranging from 1 to 7 were ap-
plied by two expert raters labeling for smoothness,
closeness, and willingness.
Hara et al (2010) derived turn level ratings from
overall ratings of the dialogue which were applied
by the users afterwards on a five point scale. Us-
ing n-grams to model the dialogue, results for dis-
tinguishing between six classes at any point in the
dialogue showed to be hardly above chance.
3 The LEGO Corpus
For estimating the Interaction Quality (IQ), the
LEGO corpus published by Schmitt et al (2012)
is used. IQ is defined similarly to user satisfac-
tion: While the latter represents the true disposi-
tion of the user, IQ is the disposition of the user as-
sumed by an expert rater. The LEGO corpus con-
tains 200 calls (4,885 system-user-exchanges) to a
bus information system (cf. (Raux et al, 2006)).
Labels for IQ on a scale from 1 (extremely un-
satisfied) to 5 (satisfied) have been assigned by
three expert raters with an inter-rater agreement of
? = 0.54. In order to ensure consistent labeling,
the expert raters had to follow labeling guidelines
(cf. (Schmitt et al, 2012)).
Parameters used as input variables for the IQ
model have been derived from the dialogue sys-
tem modules automatically for each exchange on
three levels: the exchange level, the dialogue level,
and the window level (see Figure 1). As parame-
ters like the confidence of the speech recognizer
can directly be acquired from the dialogue mod-
ules, they constitute the exchange level. Based on
this, counts, sums, means, and frequencies of ex-
change level parameters from multiple exchanges
are computed to constitute the dialogue level (all
exchanges up to the current one) and the window
level (the three previous exchanges). A complete
list of parameters is listed in (Schmitt et al, 2012).
Schmitt et al (2011) performed IQ recognition
on this data using linear SVMs. They achieved an
Unweighted Average Recall (UAR) of 0.58 based
on 10-fold cross-validation. Ultes et al (2012a)
applied HMMs and CHMMs using 6-fold cross
validation and a reduced feature set achieving an
UAR of 0.44 for HMMs and 0.39 for CHMMs.
4 Error Estimation Model
Error correction may be incorporated into the sta-
tistical classification process by a two-stage ap-
proach, which is depicted in Figure 2.
At the first stage, a statistical classification
model is created using interaction parameters as
input and IQ as target variable. For this work,
a Support Vector Machine (SVM) and a Rule
Learner are applied. At the second stage, the er-
ror er of the hypothesis h0 is calculated by
er = h0 ? r , (1)
where the reference r denotes the true IQ value.
In order to limit the number of error classes, the
signum function is applied. It is defined as
sgn(x) :=
?
??
??
?1 if x < 0 ,
0 if x = 0 ,
1 if x > 0 .
(2)
Therefore, the error is redefined as
er = sgn(h0 ? r) . (3)
Next, a statistical model is created similarly to
stage one but targeting the error er. The difference
is that the input parameter set is extended by the IQ
hypothesis h0 of stage one. Here, two approaches
are applied: Creating one model which estimates
all error classes (?1,0,1) and creating two mod-
els where each estimates positive (0,1) or negative
error (?1,0). For the latter variant, the error of
the class which is not estimated by the respective
model is mapped to 0. By this, the final error hy-
pothesis he may be calculated by simple addition
of both estimated error values:
he = he?1 + he+1 . (4)
Combining the hypothesis of the error estima-
tion he with the hypothesis of the IQ estimation h0
123
12
IQ
model
Interaction
Parameters
Hypothesis
IQ h0
Reference
IQ r
? ReferenceError IQ er?
Parameters
+
Hypothesis
Error IQ
modelError IQmodel
Hypothesis
Error IQ he
? FinalHypothesis IQ
hf
input
target
input
target
Figure 2: The complete IQ estimation process including error correction. After estimating IQ in Stage 1
(upper frame), the error is estimated and the initial hypothesis is corrected in Stage 2 (lower frame).
at stage one produces the final hypothesis hf de-
noting the Interaction Quality estimation corrected
by the estimated error of the statistical model:
hf = h0 ? he . (5)
As the error estimation will not work perfectly,
it might recognize an error where there is none or
? even worse ? it might recognize an error contrary
to the real error, e.g., ?1 instead of +1. Therefore,
the corrected hypothesis might be out of range. To
keep hf within the defined bounds of IQ, a lim-
iting functions is added to the computation of the
final hypothesis resulting in
hf = max(min(h0 ? he), bu), bl) , (6)
where bu denotes the upper bound of the IQ labels
and bl the lower bound.
5 Experiments and Results
All experiments are conducted using the LEGO
corpus presented in Section 3. By applying 5-fold
cross validation, hypotheses for each system-user-
exchange which is contained in the LEGO corpus
are estimated. Please note that some textual inter-
action parameters are discarded due to their task-
dependent nature leaving 45 parameters1.
For evaluation, we rely on two measures: The
unweighted average recall (UAR) and the root
1Removed parameters: Activity, LoopName, Prompt,
RoleName, SemanticParse, SystemDialogueAct, UserDia-
logueAct, Utterance
mean squared error (RMSE). UAR represents the
accuracy corrected by the effects of unbalanced
data and is also used by cited literature. RMSE is
used since the error correction method is limited
to correcting the results only by one. For bigger
errors, the true value cannot be reached.
The performances of two different statistical
classification methods are compared, both applied
for stage one and stage two: Support Vector Ma-
chine (SVM) (Vapnik, 1995) using a linear ker-
nel, which is also used by Schmitt et al (2011),
and Rule Induction (RI) based on Cohen (1995).
Furthermore, a normalization component is added
performing a range normalization of the input pa-
rameters in both stages. This is necessary for using
the implementation of the statistical classification
algorithms at hand.
For error estimation, two variants are explored:
using one combined model for all three error
classes (?1,0,+1) and using two separate models,
one for distinguishing between ?1 and 0 and one
for distinguishing between +1 and 0 with com-
bining their results afterwards. While using RI for
error estimation yields reasonable performance re-
sults for the combined model, it is not suitable for
error estimation using two separate models as all
input vectors are mapped to 0. Hence, for the two
model approach, only the SVM is applied .
Results for applying error correction (EC) are
presented in Table 1. Having an SVM at stage one
(column SVM), recognition performance is rela-
tively improved by up to 4.6 % using EC. With RI
124
Table 1: Results for IQ recognition: UAR and
RMSE for IQ recognition without stage two, with
error correction at stage two, and with a simple hi-
erarchical approach.
UAR RMSE
stage two SVM RI SVM RI
none 51.1% 60.3% 0.97 0.88
error correction
SVM 50.7% 59.6% 0.97 0.83
RI 52.5% 58.1% 0.88 0.85
2xSVM 53.2% 60.6% 0.88 0.85
simple hierarchical approach
SVM 50.2% 57.6% 0.97 0.85
RI 58.9% 58.7% 0.88 0.88
at stage one, performance is only increased by up
to 0.5 % which has shown to be not significant us-
ing the Wilcoxon test. The relative improvements
in UAR are depicted in Figure 3.
Furthermore, these results are compared to a
simple hierarchical approach (SH) where the hy-
pothesis h0 of the stage one classifier is used as
an additional feature for the stage two classifier
targeting IQ directly. Here, the performance of
the stage two classifier is of most interest since
this approach can be viewed as one stage classi-
fication with an additional feature. The results in
Table 1 show that RI does not benefit from addi-
tional information (comparison of last row with
one stage RI recognition). SVM recognition at
stage two, though, shows better results. While its
performance is reduced using the SVM hypothe-
sis as additional feature, adding the RI hypothesis
improved UAR up to 12.6 % relatively. However,
there is no reasonable scenario where one would
not use the better performing RI in favor of using
its results as additional input for SVM recognition.
The question remains why SVM benefits from
Error Correction as well as from adding additional
input parameters while RI does not. It remains un-
clear if this is an effect of the task characteristics
combined with the characteristics of the classifi-
cation method. It may as well be caused by low
classification performance. A classifier with low
performance might be more likely to improve its
performance by additional information or EC.
6 Conclusion
In this work, we presented an approach for im-
proving the recognition of Interaction Quality by
estimating the error of the classifier in order to cor-
rect the hypothesis. For the resulting two-staged
?0.8% ?1.3%
2.7%
?3.7%
4.1%
0.5%
?4%
?3%
?2%
?1%
0%
1%
2%
3%
4%
SVM RI
error?correction?(SVM)error?correction?(RI)error?correction?(2?x?SVM)
Figure 3: The relative improvement of EC in UAR
grouped by stage one classifiers SVM and RI.
approach, two different statistical classification al-
gorithm were applied for both stages, i.e., SVM
and Rule Learner. Performance could be improved
for both stage one classifiers using separate er-
ror models relatively improving IQ recognition by
up to 4.1 %. The proposed error correction ap-
proach has been compared to a simple hierarchi-
cal approach where the hypohtesis of stage one
is used as additional feature of stage two classi-
fication. This apprach relatively improved SVM
recognition by up to 12.6 % using a Rule Learner
hypothesis as additional feature. However, as one-
stage Rule Learner classification already provides
better results than this hierarchical approach, is
does not seem reasonable to employ this config-
uration. Nonethelesse, why only the SVM could
benefit from additional information (error correc-
tion or simple hierarchical appraach) remains un-
clear and should be investigated in future work.
Moreover, some aspects of the error correc-
tion approach have to be discussed controversially,
e.g., applying the signum function for calculating
the error. While the obvious advantage is to limit
the number of error classes a statistical classifica-
tion algorithm has to estimate, it also prohibits of
being able to correct all errors. If the absolute er-
ror is bigger than one it can never be corrected.
Acknowledgments
This work was supported by the Transregional
Collaborative Research Centre SFB/TRR 62
?Companion-Technology for Cognitive Technical
Systems? which is funded by the German Re-
search Foundation (DFG).
125
References
William W. Cohen. 1995. Fast effective rule induc-
tion. In Proceedings of the 12th International Con-
ference on Machine Learning, pages 115?123. Mor-
gan Kaufmann, July.
Klaus-Peter Engelbrecht, Florian Go?dde, Felix Har-
tard, Hamed Ketabdar, and Sebastian Mo?ller. 2009.
Modeling user satisfaction with hidden markov
model. In SIGDIAL ?09: Proceedings of the SIG-
DIAL 2009 Conference, pages 170?177, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.
2010. Estimation method of user satisfaction us-
ing n-gram-based dialog history model for spoken
dialog system. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation (LREC?10), Valletta, Malta,
May. European Language Resources Association
(ELRA).
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010. Issues in pre-
dicting user satisfaction transitions in dialogues: In-
dividual differences, evaluation criteria, and predic-
tion models. In Gary Lee, Joseph Mariani, Wolf-
gang Minker, and Satoshi Nakamura, editors, Spo-
ken Dialogue Systems for Ambient Environments,
volume 6392 of Lecture Notes in Computer Sci-
ence, pages 48?60. Springer Berlin / Heidelberg.
10.1007/978-3-642-16202-2 5.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year
of lets go! experience. In Proc. of the Interna-
tional Conference on Speech and Language Process-
ing (ICSLP), September.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011. Modeling and predicting quality in
spoken human-computer interaction. In Proceed-
ings of the SIGDIAL 2011 Conference, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Alexander Schmitt, Stefan Ultes, and Wolfgang
Minker. 2012. A parameterized and annotated cor-
pus of the cmu let?s go bus information system. In
International Conference on Language Resources
and Evaluation (LREC).
Stefan Ultes, Robert ElChabb, and Wolfgang Minker.
2012a. Application and evaluation of a conditioned
hidden markov model for estimating interaction
quality of spoken dialogue systems. In Joseph Mari-
ani, Laurence Devillers, Martine Garnier-Rizet, and
Sophie Rosset, editors, Proceedings of the 4th In-
ternational Workshop on Spoken Language Dialog
System (IWSDS), pages 141?150. Springer, Novem-
ber.
Stefan Ultes, Alexander Schmitt, and Wolfgang
Minker. 2012b. Towards quality-adaptive spoken
dialogue management. In NAACL-HLT Workshop
on Future directions and needs in the Spoken Di-
alog Community: Tools and Data (SDCTD 2012),
pages 49?52, Montre?al, Canada, June. Association
for Computational Linguistics.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Marilyn Walker, Diane Litman, Candace A. Kamm,
and Alicia Abella. 1997. Paradise: a framework
for evaluating spoken dialogue agents. In Proceed-
ings of the eighth conference on European chap-
ter of the Association for Computational Linguistics,
pages 271?280, Morristown, NJ, USA. Association
for Computational Linguistics.
126
Proceedings of the SIGDIAL 2014 Conference, pages 208?217,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Interaction Quality Estimation in Spoken Dialogue Systems Using
Hybrid-HMMs
Stefan Ultes
Ulm University
Albert-Einstein-Allee 43
89081 Ulm, Germany
stefan.ultes@uni-ulm.de
Wolfgang Minker
Ulm University
Albert-Einstein-Allee 43
89081 Ulm, Germany
wolfgang.minker@uni-ulm.de
Abstract
Research trends on SDS evaluation are
recently focusing on objective assess-
ment methods. Most existing methods,
which derive quality for each system-
user-exchange, do not consider tempo-
ral dependencies on the quality of pre-
vious exchanges. In this work, we in-
vestigate an approach for determining In-
teraction Quality for human-machine dia-
logue based on methods modeling the se-
quential characteristics using HMM mod-
eling. Our approach significantly outper-
forms conventional approaches by up to
4.5% relative improvement based on Un-
weighted Average Recall metrics.
1 Introduction
Spoken Dialogue Systems (SDSs) play a key role
in achieving natural human-machine interaction.
One reason is that speech is one major chan-
nel of natural human communication. Assess-
ing the quality of such SDSs has been discussed
frequently in recent years. The basic principles
which all approaches underlie have been analyzed
by M?oller et al. (2009) creating a taxonomy for
quality of human-machine interaction, i.e., Qual-
ity of Service (QoS) and Quality of Experience
(QoE). Quality of Service describes objective cri-
teria like total number of turns. The recent shift of
interest in dialogue assessment methods towards
subjective criteria is described as Quality of Expe-
rience, putting the user in the spotlight of dialogue
assessment. For QoE, M?oller et al. (2009) iden-
tified several aspects contributing to a good user
experience, e.g., usability or acceptability. These
aspects can be combined under the term user sat-
isfaction, describing the degree by which the user
is satisfied with the system?s performance. By as-
sessing QoE, the hope of the research community
is to better measure the human-like quality of an
SDS. While this information may be used during
the design process, enabling automatically derived
user satisfaction within the dialogue management
allows for adaption of the ongoing dialogue (Ultes
et al., 2012b).
First work on deriving subjective metrics au-
tomatically has been performed by Walker et
al. (1997) resulting in the PARADISE framework,
which is the current quasi-standard in this field.
Briefly explained, a linear dependency is assumed
between dialogue parameters and user satisfaction
to estimate qualitative performance on the dia-
logue level.
Measuring the performance of complete dia-
logues does not allow for adapting to the user dur-
ing the dialogue (Ultes et al., 2012b). Hence,
performance measures which provide a measure-
ment for each system-user-exchange
1
are of inter-
est. Approaches based on Hidden Markov Models
(HMMs) are widely used for sequence modeling.
Therefore, Engelbrecht et al. (2009) used these
models for predicting the dialogue quality on the
exchange level. Similar to this, we presented work
on estimating Interaction Quality using HMMs
and Conditioned HMMs (Ultes et al., 2012a). In
this contribution, we investigate an approach for
recognizing the dialogue quality using a hybrid
Markovian model. Here, hybrid means combin-
ing statistical approaches such as Support Vector
Machines with Hidden Markov Models by model-
ing the observation probability of the HMMs us-
ing classification. While this is the first time hy-
brid approaches are used for estimating Interaction
Quality, they are well-known and have been used
before for other classification tasks (e.g. (Valstar
and Pantic, 2007; Onaran et al., 2011)).
This paper is outlined as follows: Related work
on subjective quality measurement on the ex-
1
A system-user-exchange consists of a system dialogue
turn followed by a user dialogue turn
208
change level is presented in Section 2. All experi-
ments in this work are based on the Interaction
Quality metric of the LEGO corpus described in
Section 3. We motivate for introducing time de-
pendency and present our own approach on rec-
ognizing Interaction Quality using a Markovian
model presented in Section 4 and briefly present
the classification algorithms used for the experi-
ments in Section 5. Experiments are presented in
Section 6 and their results discussion in Section 7.
2 Significant Related Work
Much research on predicting subjective quality
measures on an exchange level has been per-
formed hitherto. However, most of this body of
work lacks of either taking account of the sequen-
tial structure of the dialogue or resulting in insuf-
ficient performance.
Engelbrecht et al. (2009) presented an approach
using Hidden Markov Models (HMMs) to model
the SDS as a process evolving over time. Perfor-
mance ratings on a 5 point scale (?bad?, ?poor?,
?fair?, ?good?, ?excellent?) have been applied by
the users of the SDS during the dialogue. The in-
teraction was halted while the user rated. A HMM
was created consisting of 5 states (one for each
rating) and a 6-dimensional input vector. While
Engelbrecht et al. (2009) relied on only 6 input
variables, we will pursue an approach with 29 in-
put variables. Moreover, we will investigate dia-
logues of a real world dialogue system annotated
with quality labels by expert annotators.
Higashinaka et al. (2010) proposed a model for
predicting turn-wise ratings for human-human di-
alogues. Ratings ranging from 1 to 7 were applied
by two expert annotators labeling for smooth-
ness, closeness, and willingness. They achieved
an UAR
2
of only 0.2-0.24 which is only slightly
above the random baseline of 0.14.
Hara et al. (2010) derived turn level ratings from
overall ratings of the dialogue which were applied
by the users after the interaction on a five point
scale within an online questionnaire. Using n-
grams to model the dialogue by calculating n-gram
occurrence frequencies for each satisfaction value
showed that results for distinguishing between six
classes at any point in the dialogue to be hardly
above chance.
A more robust measure for user satisfaction has
been presented by Schmitt et al. (2011) within
2
Unweighted Average Recall, see Section 6
s
u
s
u
s
u
s
u
?
s 1
u 1
s 2
u 2
s 3
u 3
s n
u n
?
e 1
e 2
e 3
e n
Figure 1: A dialogue may be separated into a se-
quence of system-user-exchanges where each ex-
change e
i
consists of a system turn s
i
followed by
a user turn u
i
.
their work about Interaction Quality (IQ) for Spo-
ken Dialogue Systems. In contrast to user satis-
faction, the labels were applied by expert annota-
tors after the dialogue at the exchange level. Auto-
matically derived parameters were used as features
for creating a statistical model using static fea-
ture vectors. Schmitt et al. (2011) performed IQ
recognition on the LEGO corpus (see Section 3)
using linear SVMs. They achieved an UAR
2
of
0.58 based on 10-fold cross-validation which is
clearly above the random baseline of 0.2. Ultes
et al. (2012a) put an emphasis on the sequential
character of the IQ measure by applying a Hid-
den Markov Models (HMMs) and a Conditioned
Hidden Markov Models (CHMMs). Both have
been applied using 6-fold cross validation and a
reduced feature set of the LEGO corpus achieving
an UAR
2
of 0.44 for HMMs and 0.39 for CHMMs.
While Ultes et al. (2012a) used generic Gaussian
Mixture Models to model the observation proba-
bilities, we use confidence distributions of static
classification algorithms.
3 The LEGO Corpus
For Interaction Quality (IQ) estimation, we use the
LEGO corpus published by Schmitt et al. (2012).
Interaction Quality is defined similarly to user sat-
isfaction: While the latter represents the true dis-
position of the user, IQ is the disposition of the
user assumed by an expert annotator. Here, ex-
pert annotators are people who listen to recorded
dialogues after the interactions and rate them by
assuming the point of view of the actual person
performing the dialogue. These experts are sup-
posed to have some experience with dialogue sys-
tems. In this work, expert annotators were ?ad-
vanced students of computer science and engineer-
ing? (Schmitt et al., 2011), i.e., grad students.
The LEGO corpus is based on 200 calls to
209
s 1
u 1
s 2
u 2
s 3
u 3
s n
u n
?
e 1
e 2
e 3
e n
e n
?
e n-
1
e n-
2
e 1
e 2
e 3
e n+
1
?
exc
ha
ng
e le
vel
 pa
ram
ete
rs
win
do
w l
eve
l p
ara
me
ter
s: {
#},
 {M
ean
}, e
tc.
dia
log
ue 
lev
el p
ara
me
ter
s: #
, M
ean
, et
c.
Figure 2: The three different modeling levels representing the interaction at exchange e
n
: The most
detailed exchange level, comprising parameters of the current exchange; the window level, capturing
important parameters from the previous n dialog steps (here n = 3); the dialog level, measuring overall
performance values from the entire previous interaction.
the ?Let?s Go Bus Information System? of the
Carnegie Mellon University in Pittsburgh (Raux et
al., 2006) recorded in 2006. Labels for IQ have
been assigned by three expert annotators to 200
calls consisting of 4,885 system-user-exchanges
(see Figure 1) in total with an inter-annotator
agreement of ? = 0.54. This may be considered
as a moderate agreement (cf. Landis and Koch?s
Kappa Benchmark Scale (1977)) which is quite
good considering the difficulty of the task that re-
quired to rate each exchange. For instance, if one
annotator reduces the IQ value only one exchange
earlier than another annotator, both already dis-
agree on two exchanges. The final label was as-
signed to each exchange by using the median of
all three individual ratings.
IQ was labeled on a scale from 1 (extremely un-
satisfied) to 5 (satisfied) considering the complete
dialogue up to the current exchange. Thus, each
exchange has been rated without regarding any up-
coming user utterance. As the users are expected
to be satisfied at the beginning, each dialogue?s
initial rating is 5. In order to ensure consistent la-
beling, the expert annotators had to follow labeling
guidelines (Schmitt et al., 2012).
An example of an annotated dialogue is shown
in Table 5. It starts off with a good IQ until the
system provides some results and then falls drasti-
cally as the user input does not correspond to what
the system expects. Thus, the system remains in a
loop until the user reacts appropriately.
Parameters used as input variables for the IQ
model have been derived from the dialogue system
modules automatically for each exchange. Fur-
thermore, parameters on three levels have been
created: the exchange level, the dialogue level,
and the window level (see Figure 2). As parame-
ters like ASRCONFIDENCE (confidence of speech
recognition) or UTTERANCE (word sequence rec-
ognized by speech recognition) can directly be
acquired from the dialogue modules they consti-
tute the exchange level. Counts, sums, means,
and frequencies of exchange level parameters from
multiple exchanges are computed to constitute the
dialogue level (all exchanges up to the current
one) and the window level (the three previous ex-
changes).
4 Hybrid-HMM
As Schmitt et al. (2011) model the sequential
character of the data only indirectly by design-
ing special features, our approach applies Marko-
vian modeling to directly model temporal de-
pendencies. Temporal dependencies on previous
system-user-exchanges are not taken into account
by Schmitt et al.; only parameters derived from
the current exchange are used. However, we found
out that Interaction Quality is highly dependent on
the IQ value of the previous exchange. Adding
the parameter IQ
prev
describing the previous IQ
value to the input vector to the IQ model consist-
ing of several parameters results in an extended in-
put vector. Calculating the Information Gain Ra-
tio (IGR) of each parameter of the extended input
vector shows that IQ
prev
achieves the highest IGR
value of 1.0. In other words, IQ
prev
represents the
parameter which contains the most information for
the classification task.
While performing IQ recognition on the ex-
tended features set using the annotated IQ values
results in an UAR of 0.82, rather using the esti-
mated IQ value results in an UAR of only 0.43.
Consequently, other configurations have to be in-
vestigated. Here, Markovian approaches offer a
self-contained concept of using these temporal de-
pendencies. However, Ultes et al. (2012a) showed
that applying neither a classical HMM nor a con-
ditioned HMM yields results outperforming static
approaches.
Therefore, in this Section we present a Hybrid-
210
HMM approach, which is based on the classical
HMM and takes advantage of good performing
existing static classification approaches. The clas-
sical HMM, specifically used for time-sequential
data, consists of a set of states S with transition
probability matrix A and initial probability vec-
tor pi over a set of observations B (also called vo-
cabulary) and an observation function b
q
t
depen-
dent on the state q
t
. For calculating the proba-
bility p(q
t
|O
t
, ?) of seeing observation sequence
O
t
= (o
1
, o
2
, . . . , o
t
) while being in state q
t
at
time t given the HMM ?, the Forward Algorithm
is used:
p(q
t
= s
j
|O
t
, ?) = ?
t
(j)
=
|S|
?
i=1
?
t?1
(i)a
ij
b
j
(o
t
) . (1)
Here, a
ij
describes the transition probability of
transitioning from state s
i
to state s
j
. To find
a suitable model ?, the HMM must be trained,
for example, by using the Baum-Welch algorithm.
Usually, the observation function b
q
t
is modeled
with Gaussian mixture models (GMMs). For more
information on general HMMs, please refer to Ra-
biner et al. (1989).
For determining the most likely class ??
t
at time
t, where each state j ? S is associated with one
class ?, the following equation is used:
??
t
= arg max
j
?
t
(j) . (2)
For applying an HMM while exploiting exist-
ing statistical classification approaches, the obser-
vation function b
j
(o
t
) is modeled by using con-
fidence score distributions of statistical classifiers,
e.g., a Support Vector Machine in accordance with
Schmitt et al. (2011) (see Section 5). Furthermore,
the transition function a
ij
is computed by taking
the frequencies of the state transitions contained
in the given corpus. Therefore, an ergodic HMM
is used comprising five states with each represent-
ing one of the five IQ scores.
Moreover, in SDSs, a system action act is per-
formed at the end of each system turn. This can
be utilized by adding an additional dependency on
this action to the state transition function a
ij
. By
augmenting Equation 1, this results in
?
t
(j) =
|S|
?
i=1
?
t?1
(i)a
ij,act
b
j
(o
t
) . (3)
This refinement models differences in state tran-
sitions evoked by different system actions, e.g.,
a different transition probability is expected if a
WAIT action is performed compared to a CONFIR-
MATION. Equation 3 is equal to the belief up-
date equation known from the Partially Observ-
able Markov Decision Process formalism (Kael-
bling et al., 1998).
Therefore, two versions of the Hybrid-HMM
are evaluated: an action-independent version as in
Equation 1 and an action-dependent version as in
Equation 3.
5 Classifier Types
For modeling the observation probability b
j
(o
t
)of
the hybrid HMM, multiple classification schemes
have been applied to investigate the influence of
observation distributions with different character-
istics on the overall performance.
In general, classification means estimating a
class ?? to the given observation o by comparing
the class-wise probabilities p(?|o). In this work,
this probability may be used to model the observa-
tion probability b
j
(o) of the HMM by the posterior
probability
p(?|o) = b
j
(o) (4)
for j = ?.
As not all classification algorithms provide a
posterior probability, it may be replaced by the
confidence distribution. A general description of
the classification algorithms used in this work are
described in the following Section along with a
motivation for the feature subset of the LEGO cor-
pus used for estimating the Interaction Quality in
this work.
5.1 Support Vector Machine
For a two class problem, a Support Vector Ma-
chine (SVM) (Vapnik, 1995) is based on the con-
cept of linear discrimination with maximum mar-
gin by defining a hyperplane separating the two
classes. The estimated class ?? for observation vec-
tor ~o is based on the sign of the decision function
k(~o) =
N
?
i=1
?
i
z
i
K(~m
i
, ~o) + b , (5)
where ~m
i
represent support vectors defining the
hyper plane (together with b), z
i
the known class
~m
i
belongs to, ?
i
the weight of ~m
i
, and K(?, ?) a
211
kernel function. The kernel function is defined as
K(~m, ~m
?
) = ??(~m), ?(~m
?
)? , (6)
where ?(~m) represents a transformation function
mapping ~m into a space ? of different dimension-
ality and ??, ?? defines a scalar product in ?. By
using the kernel function, the linear discrimina-
tion may happen in a space of high dimensional-
ity without explicitly transforming the observation
vectors into said space.
The SVM implementation which is used in this
contribution is libSVM (Chang and Lin, 2011). As
this algorithm does not provide class probabilities
directly, the respective confidence scores are used.
5.2 Naive Bayes
For deriving the posterior probability, the Naive
Bayes classifier may be used. It calculates the pos-
terior probability P (?|o) of having class ? when
seeing the n-dimensional observation vector ~o by
applying Bayes rule (Duda et al., 2001):
P (?|~o) =
p(~o|?) ? P (?)
p(~o)
. (7)
In general, observations, i.e., elements of the
observation vector, may be correlated with each
other and introducing independence assumptions
between these elements does usually not reflect
the true state of the world. However, correlations
are often not very high thus simplifying the Bayes
problem has proved to result in reasonable perfor-
mance. This is utilized by the Naive Bayes classi-
fier by assuming said independence thus calculat-
ing
p(~o|?) =
n
?
i=1
p(o
i
|?) . (8)
5.3 Rule Induction
The classification algorithm Rule Induction or
Rule Learner is based on the idea of defining rules
to assign classes ?? to observation vectors ~o. In this
work, the algorithm RIPPER (Repeated Incremen-
tal Pruning to Produce Error Reduction) (Cohen,
1995) is used where each rule consists of conjunc-
tions of A
n
= v, where A
n
is a nominal attribute,
or A
c
? ?,A
c
? ?, where A
c
is a continuous at-
tribute. Each part of the observation vector ~o is re-
flected by one of the attributes. The basic process
of the algorithm for generating rules is divided into
three steps: First, rules are grown by adding at-
tributes to the rule. Second, the rules are pruned.
If the resulting rule set is not of sufficient perfor-
mance, all training examples which are covered by
the generated rules are removed from the example
set and a new rule is created.
5.4 Feature selection
As stated previously, all experiments are based on
the LEGO corpus presented in Section 3. In order
to keep the presented results comparable to pre-
vious work based on HMM and CHMM (Ultes et
al., 2012a), a reduced parameter set is used. Pa-
rameters with constant values for most exchanges
have been excluded. These would result in rows
of zeros during computation of the covariance ma-
trices of the feature vectors, which are needed for
HMM and CHMM classification. A row of ze-
ros in the covariance matrix will make it non-
invertible, which will cause errors during the com-
putation of the emission probabilities.
Therefore, a feature set consisting of 29 inter-
action parameters is used for both defining a base-
line and for evaluating the Hybrid-HMM. The set
consists of the following parameters (for an expla-
nation of the features, please refer to (Schmitt et
al., 2012)):
Exchange Level ASRRECOGNITIONSTATUS, ACTIVITY-
TYPE, ASRCONFIDENCE, ROLEINDEX, ROLENAME,
UTD, REPROMPT?, BARGED-IN?, DD, WPST,
WPUT
Dialogue Level MEANASRCONFIDENCE, #ASRREJEC-
TIONS, #TIMEOUTS ASRREJ, #BARGEINS, %ASR-
REJECTIONS, %TIMEOUTS ASRREJ, %BARGEINS,
#REPROMPTS,
%REPROMPTS, #SYSTEMQUESTIONS
Window Level #TIMEOUTS ASRREJ, #ASRREJEC-
TIONS, #BARGEINS, %BARGEINS, #SYSTEMQUES-
TIONS, MEANASRCONFIDENCE, #ASRSUCCESS,
#RE-PROMPT
For act in Equation 3, the exchange level pa-
rameter ACTIVITYTYPE is used which may take
one out of the four values ?Announcement?, ?Con-
firmation?, ?Question?, or ?wait?. Their distribu-
tion within the LEGO corpus is depicted in Fig-
ure 3.
6 Experiments and Results
All experiments are conducted using 6-fold cross-
validation
3
. This includes the baseline approach
3
Six folds have been selected as a reasonable trade-off be-
tween validity and computation time.
212
3066
2477
3266
274
01000200030004000
Anno
unce
men
tC
onfir
mati
on
Ques
tion
wait
Figure 3: Distribution of the four values for act
in Equation 3 in the LEGO corpus. While ?wait?
occurs rarely, the other three main actions occur at
roughly the same frequency.
(also producing the observation probabilities of
the Hybrid-HMM approach) and the evaluation of
the Hybrid-HMM. For the latter, two phases of
cross-validation were applied.
Interaction Quality estimation is done by
using three commonly used evaluation met-
rics: Unweighted Average Recall (UAR), Co-
hen?s Kappa (Cohen, 1960) and Spearman?s
Rho (Spearman, 1904). These are also selected
as the same metrics have been used in Schmitt et
al. (2011) as well.
Recall in general is defined as the rate of cor-
rectly classified samples belonging to one class.
The recall in UAR for multi-class classification
problems with N classes recall
i
is computed for
each class i and then averaged over all class-wise
recalls:
UAR =
1
N
N
?
i=1
recall
i
. (9)
Cohen?s Kappa measures the relative agree-
ment between two corresponding sets of ratings.
In our case, we compute the number of label
agreements corrected by the chance level of agree-
ment divided by the maximum proportion of times
the labelers could agree. However, Cohen?s
weighted Kappa is applied as ordinal scores are
compared (Cohen, 1968). A weighting factor w is
introduced reducing the discount of disagreements
the smaller the difference is between two ratings:
w =
|r
1
? r
2
|
|r
max
? r
min
|
. (10)
Here, r
1
and r
2
denote the rating pair and r
max
and r
min
the maximum and minimum ratings pos-
sible.
Table 1: Results for IQ recognition of the statis-
tical classifiers: UAR, ? and ? for linear SVM,
Bayes classification and Rule Induction. ?
2
repre-
sents the variances of the confidence scores.
UAR ? ? ?
2
SVM (linear) .495 .611 .774 .020
Bayes .467 .541 .716 .127
Rule Induction .596 .678 .790 .131
Correlation between two variables describes the
degree by which one variable can be expressed by
the other. Spearman?s Rho is a non-parametric
method assuming a monotonic function between
the two variables (Spearman, 1904).
6.1 Baseline
As baseline, we adapted the approach of Schmitt
et al. (2011). While they focused only on an SVM
with linear kernel, we investigate three different
static classification approaches. Different clas-
sifiers will produce different confidence distribu-
tions. These distributions will have different char-
acteristics which is of special interest for evaluat-
ing the Hybrid-HMM as will be discussed in Sec-
tion 7. The confidence characteristics are repre-
sented by the variance of the confidence scores
?
2
. This variance is used as indicator for how cer-
tain the classifier is about its results. If one IQ
value has a high confidence while all others have
low confidence, the classifier is considered to be
very certain. This also results in a high variance.
Vice versa, if all IQ values have almost equal con-
fidence indicates high uncertainty. This will result
in a low variance.
The classification algorithms, which have been
selected arbitrarily, are SVM with linear kernel,
Naive Bayes, and Rule Induction (see Section 5).
The results in Table 1 show that an SVM with lin-
ear kernel (as used by Schmitt et al. (2011)) per-
forms second best with an UAR of 0.495 after
Rule Induction with an UAR of 0.596. The re-
sults of the SVM differ from the results obtained
by Schmitt et al. (UAR of 0.58) as we used a re-
duced feature set while they used all available fea-
tures.
6.2 Hybrid-HMM
For evaluating the Hybrid-HMM on Interaction
Quality recognition, three aspects are of inter-
est. Most prominent is whether the presented ap-
proaches outperform the baseline, i.e., the clas-
213
?3.7%
+4.0%?*
+2.0%?*
?2.3%
+4.5%?*
+2.1%?*
+2.2%?*
+4.1%?*
+2.0%?*
?7?%?5?%?3?%?1?%+1?
%
+3?
%
+5?
%
SVM (lin
ear
)
Bay
es
Rul
e
Ind
uct
ion
AI
AD
HC
Figure 4: Relative difference of UAR in percent between the baseline performance and the Hybrid-
HMM for the action-independent (AI), action-dependent (AD) and handcrafted (HC) transition matrix.
Differences marked with an * are significant (Wilcoxon test (Wilcoxon, 1945), ? < 0.05).
Table 2: Results for the Hybrid-HMM approach:
UAR, ? and ? for the action-independent (AI) and
action-dependent (AD) versions.
UAR ? ?
AI AD AI AD AI AD
SVM (linear) .477 .484 .599 .598 .770 .771
Bayes .486 .489 .563 .564 .737 .741
Rule Induction .608 .609 .712 .714 .826 .824
sifier which produces the observation probabili-
ties. Moreover, performance values of action-
dependent approaches and action-independent ap-
proaches are compared. In addition, the results are
analyzed with respect to the characteristic of the
confidence distribution.
For producing the confidence scores represent-
ing the observation probabilities, the statistical
classification algorithms presented in Section 6.1
are used. The initial distribution pi for each HMM
was chosen in accordance with the annotation
guidelines of the LEGO corpus starting each di-
alogue with an IQ score of 5 resulting in
pi
5
= P (IQ = 5) = 1.0
pi
4
= pi
3
= pi
2
= pi
1
= P (IQ 6= 5) = 0.0 .
Results of the experiments with action-dependent
(AD) and action-independent (AI) transition func-
tion may be seen in Table 2. Again, Rule Induction
performed best with Naive Bayes on the second
and SVM on the third place.
7 Discussion
While previous work on applying the HMM and
CHMM for IQ recognition could not outperform
the baseline (Ultes et al., 2012a), Hybrid-HMM
experiments show a significant improvement in
UAR, Cohen?s ? and Spearman?s ? for Naive
Bayes and Rule Induction. While performance
declines for the linear SVM, this difference has
shown to be not significant.
The relative difference of the Hybrid-HMM
compared to the respective baseline approaches
using an action-dependent and an action-
independent transition matrix is depicted in
Figure 4. Improvement for the Bayes method was
the highest significantly increasing UAR by up to
4.5% relative to the baseline. However, adding
action-dependency to the Hybrid-HMM does not
show any effect. This may be a result of using
ACTIVITYTYPE instead of the actual action.
However, using the actual action would result in
the need for more data as it contains 45 different
values. Significance for all results has been
calculated using the Wilcoxon test (Wilcoxon,
1945) by pair-wise comparison of the estimated
IQ values of all exchanges. All results except for
the decline in SVM performance are significant
with ? < 0.05.
Correlating the confidence variances shown in
Table 1 with the improvements of the Hybrid-
HMM reveals that for methods with a high
variance?and therefore with a greater certainty
about the classification result?, an improvement
could be accomplished. However, the perfor-
214
Table 3: Results of Hybrid-HMM with hand-
crafted transition matrix of the action-independent
version.
UAR ? ?
SVM (linear) .506 .642 .797
Bayes .487 .563 .734
Rule Induction .608 .712 .825
Table 4: Handcrafted transition matrix based on
empirical data.
P
P
P
P
P
P
from
to
1 2 3 4 5
1 0.7 0.3 0 0 0
2 0.25 0.5 0.25 0 0
3 0 0.25 0.5 0.25 0
4 0 0 0.25 0.5 0.25
5 0 0 0 0.3 0.7
mance declined for classification approaches with
a low confidence variance, which can be seen as a
sign for uncertain classification results.
While the results for Hybrid-HMM are encour-
aging, creating a simple handcrafted transition
matrix for the action-independent version shown
in Table 4 achieved even more promising results
as performance for all classifier types could be im-
proved significantly compared to the baseline (see
Table 3). The handcrafted matrix was created in a
way to smooth the resulting estimates as only tran-
sitions from one IQ rating to its neighbors have a
probability greater than zero. Drastic changes in
the estimated IQ value compared to the previous
exchange are thus less likely. The exact values
have been derived empirically. By applying this
handcrafted transition matrix, even SVM perfor-
mance with linear kernel could be improved sig-
nificantly by 2.2% in UAR (see Figure 4) com-
pared to the baseline.
For creating the Interaction Quality scores, an-
notation guidelines were used resulting in certain
characteristics of IQ. Therefore, it may be as-
sumed that the effect of exploiting the dependency
on previous states is just a reflection of the guide-
lines. While this might be true, applying a Hy-
brid HMM for IQ recognition is reasonable as, de-
spite the guidelines, the IQ metric itself is strongly
related to user satisfaction, i.e., ratings applied
by users (without guidelines), achieving a Spear-
man?s ? of 0.66 (? < 0.01) (Ultes et al., 2013).
8 Conclusions
As previously published, approaches for recogniz-
ing the Interaction Quality of Spoken Dialogue
Systems are based on static classification without
temporal dependency on previous values, a Hy-
brid Hidden Markov Model approach has been in-
vestigated based on three static classifiers. The
Hybrid-HMM achieved a relative improvement up
to 4.5% and a maximum of 0.61 UAR. Analyz-
ing the experiments revealed that, while an im-
provement could be achieved with the Hybrid-
HMM approach, handcrafting a transition model
achieved even better results as performance for all
analyzed classifier types could be improved signif-
icantly. Furthermore, applying the Hybrid-HMM
approach only yields improved performance if the
basic classifier itself has a high confidence about
its results.
Further research should be conducted investi-
gating the question how the presented approach as
well as the Interaction Quality paradigm in general
will generalize for different dialogue domains. As
IQ is designed to be domain independent, it may
be expected that the Hybrid-HMM will be appli-
cable for different dialogue domains as well.
Finally, it is notable that rule induction outper-
formed SVM approaches in the baseline by 10 per-
centage points. While this contribution does not
focus on this, analyzing the model may help in un-
derstanding the problem of estimating Interaction
Quality better, especially since rule-based recog-
nition methods allow easy interpretation.
Acknowledgments
This work was supported by the Transregional
Collaborative Research Centre SFB/TRR 62
?Companion-Technology for Cognitive Technical
Systems? which is funded by the German Re-
search Foundation (DFG).
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. In Educational and Psychological
Measurement, volume 20, pages 37?46, April.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or par-
tial credit. Psychological bulletin, 70(4):213.
William W. Cohen. 1995. Fast effective rule induc-
tion. In Proceedings of the 12th International Con-
215
ference on Machine Learning, pages 115?123. Mor-
gan Kaufmann, July.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern Classification (2nd Edition). Wiley-
Interscience, 2 edition, November.
Klaus-Peter Engelbrecht, Florian G?odde, Felix Har-
tard, Hamed Ketabdar, and Sebastian M?oller. 2009.
Modeling user satisfaction with hidden markov
model. In SIGDIAL ?09: Proceedings of the SIG-
DIAL 2009 Conference, pages 170?177, Morris-
town, NJ, USA. ACL.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.
2010. Estimation method of user satisfaction us-
ing n-gram-based dialog history model for spo-
ken dialog system. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, May.
ELRA.
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010. Issues in pre-
dicting user satisfaction transitions in dialogues: In-
dividual differences, evaluation criteria, and predic-
tion models. In Spoken Dialogue Systems for Am-
bient Environments, volume 6392 of Lecture Notes
in Computer Science, pages 48?60. Springer Berlin
/ Heidelberg.
L. P. Kaelbling, M. L. Littman, and A. R. Cassandra.
1998. Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101(1-
2):99?134.
J. R. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174, March.
Sebastian M?oller, Klaus-Peter Engelbrecht, C. K?uhnel,
I. Wechsung, and B. Weiss. 2009. A taxonomy of
quality of service and quality of experience of mul-
timodal human-machine interaction. In Quality of
Multimedia Experience, 2009. QoMEx 2009. Inter-
national Workshop on, pages 7?12, July.
Ibrahim Onaran, N Firat Ince, A Enis Cetin, and Aviva
Abosch. 2011. A hybrid svm/hmm based system for
the state detection of individual finger movements
from multichannel ecog signals. In Neural Engi-
neering (NER), 2011 5th International IEEE/EMBS
Conference on, pages 457?460. IEEE.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year
of let?as go! experience. In Proc. of the Interna-
tional Conference on Speech and Language Process-
ing (ICSLP), September.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011. Modeling and predicting quality in
spoken human-computer interaction. In Proceed-
ings of the SIGDIAL 2011 Conference, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Alexander Schmitt, Stefan Ultes, and Wolfgang
Minker. 2012. A parameterized and annotated cor-
pus of the cmu let?s go bus information system. In
International Conference on Language Resources
and Evaluation (LREC).
Charles Edward Spearman. 1904. The proof and mea-
surement of association between two things. Ameri-
can Journal of Psychology, 15:88?103.
Stefan Ultes, Robert ElChabb, and Wolfgang Minker.
2012a. Application and evaluation of a condi-
tioned hidden markov model for estimating inter-
action quality of spoken dialogue systems. In Pro-
ceedings of the 4th International Workshop on Spo-
ken Language Dialog System (IWSDS), pages 141?
150. Springer, November.
Stefan Ultes, Alexander Schmitt, and Wolfgang
Minker. 2012b. Towards quality-adaptive spoken
dialogue management. In NAACL-HLT Workshop
on Future directions and needs in the Spoken Dialog
Community: Tools and Data (SDCTD 2012), pages
49?52, Montr?eal, Canada, June. ACL.
Stefan Ultes, Alexander Schmitt, and Wolfgang
Minker. 2013. On quality ratings for spoken dia-
logue systems ? experts vs. users. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 569?
578. ACL, June.
Michel F. Valstar and Maja Pantic. 2007. Combined
support vector machines and hidden markov mod-
els for modeling facial action temporal dynamics.
In Human-Computer Interaction, volume 4796 of
Lecture Notes in Computer Science, pages 118?127.
Springer Berlin Heidelberg.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Marilyn Walker, Diane Litman, Candace A. Kamm,
and Alicia Abella. 1997. Paradise: a framework
for evaluating spoken dialogue agents. In Proceed-
ings of the eighth conference on European chap-
ter of the Association for Computational Linguistics,
pages 271?280, Morristown, NJ, USA. ACL.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics bulletin, 1(6):80?83.
216
Appendix
Table 5: Example of a dialogue annotated with Interaction Quality for each system-user-exchange. As
the system fails to interact properly with the user in the end, the IQ value drops down to one. Utterance
represents either the system utterance or the recognized user utterance. (CallID 2061123000 of the
LEGO corpus.)
Utterance IQ
S: Welcome to the CMU Let?s Go bus information system.
U: 5
S: To get help at any time, just say Help or press zero.
U: 5
S: What can I do for you?
U: PENN AT BUTLER TIME IS IT IS THE CENTRE AT HIGHLAND FROM RUNS THE 61A EIGHT THIRTY 61B CAN?T OAKLAND 5
S: Leaving from OAKLAND . Is this correct?
U: OAKLAND 4
S: Leaving from OAKLAND . Is this correct?
U: YES 4
S: Right
U: - 4
S: Where do you want to go?
U: SQUIRREL HILL OF FIFTH AVENUE 4
S: Going to FIFTH AVENUE . Is this correct?
U: YES 5
S: Alright
U: - 5
S: When do you wanna travel?
U: SUNDAY 4
S: At what time do you wanna travel?
U: AT FIVE FOUR VERY ABOUT ELEVEN O?CLOCK 4
S: Leaving at 11 a.m. . Is this correct?
U: YES 5
S: Right
U: - 5
S: Hold on. Let me check that for you.
U: - 5
S: OAKLAND and FIFTH AVENUE are both the same stop. Please provide a different start or end point.
U: - 3
S: To ask about a different trip, you can say, start a new query. If you are finished, you can say goodbye.
U: 61A 2
S: You can say start a new query, or goodbye.
U: FIVE DUQUESNE 1
S: To ask about a different trip, you can say, start a new query. If you are finished, you can say goodbye.
U: LAST THE NEXT 1
S: You can say start a new query, or goodbye.
U: START WH 1
S: To ask about a different trip, you can say, start a new query. If you are finished, you can say goodbye.
U: THE 1
S: You can say start a new query, or goodbye.
U: THANK YOU QUERY ELEVEN BACK 1
S: To ask about a different trip, you can say, start a new query. If you are finished, you can say goodbye.
U: GOODBYE 1
S: Thank you for using the cmu Let?s Go Bus Information System. Goodbye.
U: - 1
217
