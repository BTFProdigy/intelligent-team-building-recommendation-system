Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 292?295,
New York City, June 2006. c?2006 Association for Computational Linguistics
AUTOMATED QUALITY MONITORING FOR CALL CENTERS USING SPEECH AND NLP
TECHNOLOGIES
G. Zweig, O. Siohan, G. Saon, B. Ramabhadran, D. Povey, L. Mangu and B. Kingsbury
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598
ABSTRACT
This paper describes an automated system for assigning qual-
ity scores to recorded call center conversations. The system com-
bines speech recognition, pattern matching, and maximum entropy
classification to rank calls according to their measured quality.
Calls at both ends of the spectrum are flagged as ?interesting? and
made available for further human monitoring. In this process, the
ASR transcript is used to answer a set of standard quality control
questions such as ?did the agent use courteous words and phrases,?
and to generate a question-based score. This is interpolated with
the probability of a call being ?bad,? as determined by maximum
entropy operating on a set of ASR-derived features such as ?max-
imum silence length? and the occurrence of selected n-gram word
sequences. The system is trained on a set of calls with associated
manual evaluation forms. We present precision and recall results
from IBM?s North American Help Desk indicating that for a given
amount of listening effort, this system triples the number of bad
calls that are identified, over the current policy of randomly sam-
pling calls. The application that will be demonstrated is a research
prototype that was built in conjunction with IBM?s North Ameri-
can call centers.
1. INTRODUCTION
Every day, tens of millions of help-desk calls are recorded at call
centers around the world. As part of a typical call center operation
a random sample of these calls is normally re-played to human
monitors who score the calls with respect to a variety of quality
related questions, e.g.
? Was the account successfully identified by the agent?
? Did the agent request error codes/messages to help deter-
mine the problem?
? Was the problem resolved?
? Did the agent maintain appropriate tone, pitch, volume and
pace?
This process suffers from a number of important problems: first,
the monitoring at least doubles the cost of each call (first an opera-
tor is paid to take it, then a monitor to evaluate it). This causes the
second problem, which is that therefore only a very small sample
of calls, e.g. a fraction of a percent, is typically evaluated. The
third problem arises from the fact that most calls are ordinary and
uninteresting; with random sampling, the human monitors spend
most of their time listening to uninteresting calls.
This work describes an automated quality-monitoring system
that addresses these problems. Automatic speech recognition is
used to transcribe 100% of the calls coming in to a call center,
and default quality scores are assigned based on features such as
key-words, key-phrases, the number and type of hesitations, and
the average silence durations. The default score is used to rank
the calls from worst-to-best, and this sorted list is made available
to the human evaluators, who can thus spend their time listening
only to calls for which there is some a-priori reason to expect that
there is something interesting.
The automatic quality-monitoring problem is interesting in
part because of the variability in how hard it is to answer the ques-
tions. Some questions, for example, ?Did the agent use courteous
words and phrases?? are relatively straightforward to answer by
looking for key words and phrases. Others, however, require es-
sentially human-level knowledge to answer; for example one com-
pany?s monitors are asked to answer the question ?Did the agent
take ownership of the problem?? Our work focuses on calls from
IBM?s North American call centers, where there is a set of 31 ques-
tions that are used to evaluate call-quality. Because of the high de-
gree of variability found in these calls, we have investigated two
approaches:
1. Use a partial score based only on the subset of questions
that can be reliably answered.
2. Use a maximum entropy classifier to map directly from
ASR-generated features to the probability that a call is bad
(defined as belonging to the bottom 20% of calls).
We have found that both approaches are workable, and we present
final results based on an interpolation between the two scores.
These results indicate that for a fixed amount of listening effort,
the number of bad calls that are identified approximately triples
with our call-ranking approach. Surprisingly, while there has been
significant previous scholarly research in automated call-routing
and classification in the call center , e.g. [1, 2, 3, 4, 5], there has
been much less in automated quality monitoring per se.
2. ASR FOR CALL CENTER TRANSCRIPTION
2.1. Data
The speech recognition systems were trained on approximately
300 hours of 6kHz, mono audio data collected at one of the IBM
call centers located in Raleigh, NC. The audio was manually tran-
scribed and speaker turns were explicitly marked in the word tran-
scriptions but not the corresponding times. In order to detect
speaker changes in the training data, we did a forced-alignment of
the data and chopped it at speaker boundaries. The test set consists
of 50 calls with 113 speakers totaling about 3 hours of speech.
2.2. Speaker Independent System
The raw acoustic features used for segmentation and recognition
are perceptual linear prediction (PLP) features. The features are
292
Segmentation/clustering Adaptation WER
Manual Off-line 30.2%
Manual Incremental 31.3%
Manual No Adaptation 35.9%
Automatic Off-line 33.0%
Automatic Incremental 35.1%
Table 1. ASR results depending on segmentation/clustering and
adaptation type.
Accuracy Top 20% Bottom 20%
Random 20% 20%
QA 41% 30%
Table 2. Accuracy for the Question Answering system.
mean-normalized 40-dimensional LDA+MLLT features. The SI
acoustic model consists of 50K Gaussians trained with MPE and
uses a quinphone cross-word acoustic context. The techniques are
the same as those described in [6].
2.3. Incremental Speaker Adaptation
In the context of speaker-adaptive training, we use two forms
of feature-space normalization: vocal tract length normalization
(VTLN) and feature-space MLLR (fMLLR, also known as con-
strained MLLR) to produce canonical acoustic models in which
some of the non-linguistic sources of speech variability have been
reduced. To this canonical feature space, we then apply a discrim-
inatively trained transform called fMPE [7]. The speaker adapted
recognition model is trained in this resulting feature space using
MPE.
We distinguish between two forms of adaptation: off-line and
incremental adaptation. For the former, the transformations are
computed per conversation-side using the full output of a speaker
independent system. For the latter, the transformations are updated
incrementally using the decoded output of the speaker adapted sys-
tem up to the current time. The speaker adaptive transforms are
then applied to the future sentences. The advantage of incremental
adaptation is that it only requires a single decoding pass (as op-
posed to two passes for off-line adaptation) resulting in a decoding
process which is twice as fast. In Table 1, we compare the per-
formance of the two approaches. Most of the gain of full offline
adaptation is retained in the incremental version.
2.3.1. Segmentation and Speaker Clustering
We use an HMM-based segmentation procedure for segmenting
the audio into speech and non-speech prior to decoding. The rea-
son is that we want to eliminate the non-speech segments in order
to reduce the computational load during recognition. The speech
segments are clustered together in order to identify segments com-
ing from the same speaker which is crucial for speaker adaptation.
The clustering is done via k-means, each segment being modeled
by a single diagonal covariance Gaussian. The metric is given by
the symmetric K-L divergence between two Gaussians. The im-
pact of the automatic segmentation and clustering on the error rate
is indicated in Table 1.
Accuracy Top 20% Bottom 20%
Random 20% 20%
ME 49% 36%
Table 3. Accuracy for the Maximum Entropy system.
Accuracy Top 20% Bottom 20%
Random 20% 20%
ME + QA 53% 44%
Table 4. Accuracy for the combined system.
3. CALL RANKING
3.1. Question Answering
This section presents automated techniques for evaluating call
quality. These techniques were developed using a train-
ing/development set of 676 calls with associated manually gen-
erated quality evaluations. The test set consists of 195 calls.
The quality of the service provided by the help-desk represen-
tatives is commonly assessed by having human monitors listen to
a random sample of the calls and then fill in evaluation forms. The
form for IBM?s North American Help Desk contains 31 questions.
A subset of the questions can be answered easily using automatic
methods, among those the ones that check that the agent followed
the guidelines e.g.
? Did the agent follow the appropriate closing script?
? Did the agent identify herself to the customer?
But some of the questions require human-level knowledge of the
world to answer, e.g.
? Did the agent ask pertinent questions to gain clarity of the
problem?
? Were all available resources used to solve the problem?
We were able to answer 21 out of the 31 questions using pat-
tern matching techniques. For example, if the question is ?Did
the agent follow the appropriate closing script??, we search for
?THANK YOU FOR CALLING?, ?ANYTHING ELSE? and
?SERVICE REQUEST?. Any of these is a good partial match for
the full script, ?Thank you for calling, is there anything else I can
help you with before closing this service request?? Based on the
answer to each of the 21 questions, we compute a score for each
call and use it to rank them. We label a call in the test set as being
bad/good if it has been placed in the bottom/top 20% by human
evaluators. We report the accuracy of our scoring system on the
test set by computing the number of bad calls that occur in the
bottom 20% of our sorted list and the number of good calls found
in the top 20% of our list. The accuracy numbers can be found in
Table 2.
3.2. Maximum Entropy Ranking
Another alternative for scoring calls is to find arbitrary features in
the speech recognition output that correlate with the outcome of a
call being in the bottom 20% or not. The goal is to estimate the
probability of a call being bad based on features extracted from
the automatic transcription. To achieve this we build a maximum
293
Fig. 1. Display of selected calls.
entropy based system which is trained on a set of calls with asso-
ciated transcriptions and manual evaluations. The following equa-
tion is used to determine the score of a call C using a set of N
predefined features:
P (class/C) = 1Z exp(
N
X
i=1
?ifi(class, C)) (1)
where class ? {bad, not ? bad}, Z is a normalizing factor, fi()
are indicator functions and {?i}{i=1,N} are the parameters of the
model estimated via iterative scaling [8].
Due to the fact that our training set contained under 700 calls,
we used a hand-guided method for defining features. Specifi-
cally, we generated a list of VIP phrases as candidate features,
e.g. ?THANK YOU FOR CALLING?, and ?HELP YOU?. We
also created a pool of generic ASR features, e.g. ?number of hes-
itations?, ?total silence duration?, and ?longest silence duration?.
A decision tree was then used to select the most relevant features
and the threshold associated with each feature. The final set of fea-
tures contained 5 generic features and 25 VIP phrases. If we take a
look at the weights learned for different features, we can see that if
a call has many hesitations and long silences then most likely the
call is bad.
We use P (bad|C) as shown in Equation 1 to rank all the calls.
Table 3 shows the accuracy of this system for the bottom and top
20% of the test calls.
At this point we have two scoring mechanisms for each call:
one that relies on answering a fixed number of evaluation ques-
tions and a more global one that looks across the entire call for
hints. These two scores are both between 0 and 1, and therefore
can be interpolated to generate one unique score. After optimizing
the interpolation weights on a held-out set we obtained a slightly
higher weight (0.6) for the maximum entropy model. It can be
seen in Table 4 that the accuracy of the combined system is greater
that the accuracy of each individual system, suggesting the com-
plementarity of the two initial systems.
4. END-TO-END SYSTEM PERFORMANCE
4.1. Application
This section describes the user interface of the automated quality
monitoring application. As explained in Section 1, the evalua-
Fig. 2. Interface to listen to audio and update the evaluation form.
tor scores calls with respect to a set of quality-related questions
after listening to the calls. To aid this process, the user interface
provides an efficient mechanism for the human evaluator to select
calls, e.g.
? All calls from a specific agent sorted by score
? The top 20% or the bottom 20% of the calls from a specific
agent ranked by score
? The top 20% or the bottom 20% of all calls from all agents
The automated quality monitoring user interface is a J2EE web
application that is supported by back-end databases and content
management systems 1 The displayed list of calls provides a link
to the audio, the automatically filled evaluation form, the overall
score for this call, the agent?s name, server location, call id, date
and duration of the call (see Figure 1). This interface now gives
the agent the ability to listen to interesting calls and update the
answers in the evaluation form if necessary (audio and evaluation
form illustrated in 2). In addition, this interface provides the eval-
uator with the ability to view summary statistics (average score)
and additional information about the quality of the calls. The over-
all system is designed to automatically download calls from mul-
tiple locations on a daily-basis, transcribe and index them, thereby
making them available to the supervisors for monitoring. Calls
spanning a month are available at any given time for monitoring
purposes.
4.2. Precision and Recall
This section presents precision and recall numbers for the
identification of ?bad? calls. The test set consists of 195 calls that
were manually evaluated by call center personnel. Based on these
manual scores, the calls were ordered by quality, and the bottom
20% were deemed to be ?bad.? To retrieve calls for monitoring,
we sort the calls based on the automatically assigned quality score
and return the worst. In our summary figures, precision and recall
are plotted as a function of the number of calls that are selected
for monitoring. This is important because in reality only a small
number of calls can receive human attention. Precision is the ratio
1In our case, the backend consists of DB2 and IBM?s Websphere Infor-
mation Integrator for Content and the application is hosted on Websphere
5.1.)
294
020
40
60
80
100
0 20 40 60 80 100
Observed
Ideal
Random
Fig. 3. Precision for the bottom 20% of the calls as a function of
the number of calls retrieved.
0
20
40
60
80
100
0 20 40 60 80 100
Observed
Ideal
Random
Fig. 4. Recall for the bottom 20% of the calls.
of bad calls retrieved to the total number of calls monitored, and
recall is the ratio of the number of bad calls retrieved to the total
number of bad calls in the test set. Three curves are shown in each
plot: the actually observed performance, performance of random
selection, and oracle or ideal performance. Oracle performance
shows what would happen if a perfect automatic ordering of the
calls was achieved.
Figure 3 shows precision performance. We see that in the
monitoring regime where only a small fraction of the calls are
monitored, we achieve over 60% precision. (Further, if 20% of
the calls are monitored, we still attain over 40% precision.)
Figure 4 shows the recall performance. In the regime of low-
volume monitoring, the recall is midway between what could be
achieved with an oracle, and the performance of random-selection.
Figure 5 shows the ratio of the number of bad calls found with
our automated ranking to the number found with random selection.
This indicates that in the low-monitoring regime, our automated
technique triples efficiency.
4.3. Human vs. Computer Rankings
As a final measure of performance, in Figure 6 we present a
scatterplot comparing human to computer rankings. We do not
have calls that are scored by two humans, so we cannot present a
human-human scatterplot for comparison.
5. CONCLUSION
This paper has presented an automated system for quality moni-
toring in the call center. We propose a combination of maximum-
entropy classification based on ASR-derived features, and question
answering based on simple pattern-matching. The system can ei-
ther be used to replace human monitors, or to make them more
1
1.5
2
2.5
3
3.5
4
4.5
5
0 20 40 60 80 100
Observed
Ideal
Fig. 5. Ratio of bad calls found with QTM to Random selection as
a function of the number of bad calls retrieved.
0
20
40
60
80
100
120
140
160
180
200
0 20 40 60 80 100 120 140 160 180 200
Fig. 6. Scatter plot of Human vs. Computer Rank.
efficient. Our results show that we can triple the efficiency of hu-
man monitors in the sense of identifying three times as many bad
calls for the same amount of listening effort.
6. REFERENCES
[1] J. Chu-Carroll and B. Carpenter, ?Vector-based natural lan-
guage call routing,? Computational Linguistics, 1999.
[2] P. Haffner, G. Tur, and J. Wright, ?Optimizing svms for com-
plex call classification,? 2003.
[3] M. Tang, B. Pellom, and K. Hacioglu, ?Call-type classifica-
tion and unsupervised training for the call center domain,? in
ARSU-2003, 2003.
[4] D. Hakkani-Tur, G. Tur, M. Rahim, and G. Riccardi, ?Unsu-
pervised and active learning in automatic speech recognition
for call classification,? in ICASSP-04, 2004.
[5] C. Wu, J. Kuo, E.E. Jan, V. Goel, and D. Lubensky, ?Improv-
ing end-to-end performance of call classification through data
confusion reduction and model tolerance enhancement,? in
Interspeech-05, 2005.
[6] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and
G. Zweig, ?The ibm 2004 conversational telephony system
for rich transcription,? in Eurospeech-2005, 2005.
[7] D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau,
and G. Zweig, ?fMPE: Discriminatively trained features for
speech recognition,? in ICASSP-2005, 2004.
[8] A. Berger, S. Della Pietra, and V. Della Pietra, ?A maximum
entropy approach to natural language processing,? Computa-
tional Linguistics, vol. 22, no. 1, 1996.
295
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 459?467,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tied?Mixture Language Modeling in Continuous Space
Ruhi Sarikaya
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
sarikaya@us.ibm.com
Mohamed Afify
Orange Labs.
Cairo, Egypt
mohamed afify2001@yahoo.com
Brian Kingsbury
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
bedk@us.ibm.com
Abstract
This paper presents a new perspective to the
language modeling problem by moving the
word representations and modeling into the
continuous space. In a previous work we in-
troduced Gaussian-Mixture Language Model
(GMLM) and presented some initial experi-
ments. Here, we propose Tied-Mixture Lan-
guage Model (TMLM), which does not have
the model parameter estimation problems that
GMLM has. TMLM provides a great deal of
parameter tying across words, hence achieves
robust parameter estimation. As such, TMLM
can estimate the probability of any word that
has as few as two occurrences in the train-
ing data. The speech recognition experiments
with the TMLM show improvement over the
word trigram model.
1 Introduction
Despite numerous studies demonstrating the serious
short-comings of the n?gram language models, it
has been surprisingly difficult to outperform n?gram
language models consistently across different do-
mains, tasks and languages. It is well-known that n?
gram language models are not effective in modeling
long range lexical, syntactic and semantic dependen-
cies. Nevertheless, n?gram models have been very
appealing due to their simplicity; they require only
a plain corpus of data to train the model. The im-
provements obtained by some more elaborate lan-
guage models (Chelba & Jelinek, 2000; Erdogan et
al., 2005) come from the explicit use of syntactic and
semantic knowledge put into the annotated corpus.
In addition to the mentioned problems above, tra-
ditional n?gram language models do not lend them-
selves easily to rapid and effective adaptation and
discriminative training. A typical n?gram model
contains millions of parameters and has no structure
capturing dependencies and relationships between
the words beyond a limited local context. These pa-
rameters are estimated from the empirical distribu-
tions, and suffer from data sparseness. n?gram lan-
guage model adaptation (to new domain, speaker,
genre and language) is difficult, simply because of
the large number of parameters, for which large
amount of adaptation data is required. Instead of up-
dating model parameters with an adaptation method,
the typical practice is to collect some data in the tar-
get domain and build a domain specific language
model. The domain specific language model is in-
terpolated with a generic language model trained
on a larger domain independent data to achieve ro-
bustness. On the other hand, rapid adaptation for
acoustic modeling, using such methods as Maxi-
mum Likelihood Linear Regression (MLLR) (Leg-
etter & Woodland, 1995), is possible using very
small amount of acoustic data, thanks to the inher-
ent structure of acoustic models that allow large de-
grees of parameter tying across different words (sev-
eral thousand context dependent states are shared
by all the words in the dictionary). Likewise,
even though discriminatively trained acoustic mod-
els have been widely used, discriminatively trained
languages models (Roark et al, 2007) have not
widely accepted as a standard practice yet.
In this study, we present a new perspective to the
language modeling. In this perspective, words are
not treated as discrete entities but rather vectors of
real numbers. As a result, long?term semantic re-
lationships between the words could be quantified
and can be integrated into a model. The proposed
formulation casts the language modeling problem as
459
an acoustic modeling problem in speech recognition.
This approach opens up new possibilities from rapid
and effective adaptation of language models to using
discriminative acoustic modeling tools and meth-
ods, such as Minimum Phone Error (MPE) (Povey
& Woodland, 2002) training to train discriminative
language models.
We introduced the idea of language modeling in
continuous space from the acoustic modeling per-
spective and proposed Gaussian Mixture Language
Model (GMLM) (Afify et al, 2007). However,
GMLM has model parameter estimation problems.
In GMLM each word is represented by a specific set
of Gaussian mixtures. Robust parameter estimation
of the Gaussian mixtures requires hundreds or even
thousands of examples. As a result, we were able
to estimate the GMLM probabilities only for words
that have at least 50 or more examples. Essentially,
this was meant to estimate the GMLM probabilities
for only about top 10% of the words in the vocab-
ulary. Not surprisingly, we have not observed im-
provements in speech recognition accuracy (Afify et
al., 2007). Tied-Mixture Language Model (TMLM)
does not have these requirements in model estima-
tion. In fact, language model probabilities can be es-
timated for words having as few as two occurrences
in the training data.
The concept of language modeling in continuous
space was previously proposed (Bengio et al, 2003;
Schwenk & Gauvain, 2003) using Neural Networks.
However, our method offers several potential advan-
tages over (Schwenk & Gauvain, 2003) including
adaptation, and modeling of semantic dependencies
because of the way we represent the words in the
continuous space. Moreover, our method also al-
lows efficient model training using large amounts of
training data, thanks to the acoustic modeling tools
and methods which are optimized to handle large
amounts of data efficiently.
It is important to note that we have to realize the
full potential of the proposed model, before investi-
gating the potential benefits such as adaptation and
discriminative training. To this end, we propose
TMLM, which does not have the problems GMLM
has and, unlike GMLM we report improvements in
speech recognition over the corresponding n?gram
models.
The rest of the paper is organized as follows. Sec-
tion 2 presents the concept of language modeling
in continuous space. Section 3 describes the tied?
mixture modeling. Speech recognition architecture
is summarized in Section 4, followed by the experi-
mental results in Section 5. Section 6 discusses var-
ious issues with the proposed method and finally,
Section 7 summarizes our findings.
2 Language Modeling In Continuous
Space
The language model training in continuous space
has three main steps; namely, creation of a co?
occurrence matrix, mapping discrete words into a
continuous parameter space in the form of vectors
of real numbers and training a statistical parametric
model. Now, we will describe each step in detail.
2.1 Creation of a co?occurrence Matrix
There are many ways that discrete words can
be mapped into a continuous space. The ap-
proach we take is based on Latent Semantic Analy-
sis (LSA) (Deerwester et al, 1990), and begins
with the creation of a co?occurrence matrix. The
co?occurrence matrix can be constructed in sev-
eral ways, depending on the morphological com-
plexity of the language. For a morphologically
impoverished language, such as English the co?
occurrence matrix can be constructed using word bi-
gram co?occurrences. For morphologically rich lan-
guages, there are several options to construct a co?
occurrence matrix. For example, the co?occurrence
matrix can be constructed using either words (word?
word co?occurrences) or morphemes (morpheme?
morpheme co?occurrences), which are obtained af-
ter morphologically tokenizing the entire corpus.
In addition to word?word or morpheme?morpheme
co?occurrence matrices, a word?morpheme co?
occurrence matrix can also be constructed. A word
w can be decomposed into a set of prefixes, stem
and suffixes: w = [pfx1 + pfx2 + pfxn + stem+
sfx1+sfx2+sfxn]. The columns of such a matrix
contain words and the rows contain the correspond-
ing morphological decomposition (i.e. morphemes)
making up the word. The decomposition of this ma-
trix (as will be described in the next sub-section) can
allow joint modeling of words and morphemes in
one model.
460
In this study, we use morpheme level bigram co?
occurrences to construct the matrix. All the mor-
pheme1 bigrams are accumulated for the entire cor-
pus to fill in the entries of a co?occurrence matrix,
C, where C(wi, wj) denotes the counts for which
word wi follows word wj in the corpus. This is a
large, but very sparse matrix, since typically a small
number of words follow a given word. Because of
its large size and sparsity, Singular Value Decom-
position (SVD) is a natural choice for producing a
reduced-rank approximation of this matrix.
The co?occurrence matrices typically contain a
small number of high frequency events and a large
number of less frequent events. Since SVD derives
a compact approximation of the co?occurrence ma-
trix that is optimal in the least?square sense, it best
models these high-frequency events, which may not
be the most informative. Therefore, the entries of
a word-pair co?occurrence matrix are smoothed ac-
cording to the following expression:
C?(wi, wj) = log(C(wi, wj) + 1) (1)
Following the notation presented in (Bellegarda,
2000) we proceed to perform the SVD as follows:
C? ? USV T (2)
where U is a left singular matrix with row vectors
ui (1 ? i ? M) and dimension M ? R. S is a
diagonal matrix of singular values with dimension
R?R. V is a right singular matrix with row vectors
vj (1 ? j ? N) and dimension N ? R. R is the
order of the decomposition and R ? min(M,N).
M and N are the vocabulary sizes on the rows
and columns of the co?occurrence matrix, respec-
tively. For word?word or morpheme?morpheme
co?occurrence matrices M = N , but for word?
morpheme co?occurrence matrix, M is the number
of unique words in the training corpus and N is the
number of unique morphemes in morphologically
tokenized training corpus.
2.2 Mapping Words into Continuous Space
The continuous space for the words listed on the
rows of the co?occurrence matrix is defined as the
space spanned by the column vectors of AM?R =
1For the generality of the notation, from now on we use
?word? instead of ?morpheme?.
US. Similarly, the continuous space for the words
on the columns are defined as the space spanned
by the row vectors of BR?N = SV T . Here, for
a word?word co?occurrence matrix, each of the
scaled vectors (by S) in the columns of A and rows
of B are called latent word history vectors for the
forward and backward bigrams, respectively. Now,
a bigram wij = (wi, wj) (1 ? i, j ? M ) is repre-
sented as a vector of dimension M ? 1, where the
ith entry of wij is 1 and the remaining ones are zero.
This vector is mapped to a lower dimensional vector
w?ij by:
w?ij = ATwij (3)
where w?ij has dimension of R ? 1. Similarly, the
backward bigram wji (1 ? j, i ? N ) is mapped to a
lower dimensional vector w?ji by:
w?ji = Bwji (4)
where w?ji has dimension of R ? 1. Note that for a
word?morpheme co?occurrence matrix the rows of
B would contain latent morpheme vectors.
Since a trigram history consists of two bigram his-
tories, a trigram history vector is obtained by con-
catenating two bigram vectors. Having generated
the features, now we explain the structure of the
parametric model and how to train it for language
modeling in continuous space.
2.3 Parametric Model Training in Continuous
Space
Recalling the necessary inputs to train an acoustic
model for speech recognition would be helpful to
explain the new language modeling method. The
acoustic model training in speech recognition needs
three inputs: 1) features (extracted from the speech
waveform), 2) transcriptions of the speech wave-
forms and 3) baseforms, which show the pronuncia-
tion of each word in the vocabulary. We propose to
model the language model using HMMs. The HMM
parameters are estimated in such way that the given
set of observations is represented by the model in
the ?best? way. The ?best? can be defined in vari-
ous ways. One obvious choice is to use Maximum
Likelihood (ML) criterion. In ML, we maximize the
probability of a given sequence of observations O,
belonging to a given class, given the HMM ? of the
class, with respect to the parameters of the model ?.
461
This probability is the total likelihood of the obser-
vations and can be expressed mathematically as:
Ltot = p(O|?) (5)
However, there is no known way to analytically
solve for the model ? = {A,B, pi} , which max-
imize the quantity Ltot, where A is the transi-
tion probabilities, B is the observation probabili-
ties, and pi is the initial state distribution. But we
can choose model parameters such that it is locally
maximized, using an iterative procedure, like Baum-
Welch method (Baum et al, 1970).
The objective function given in Eq. 5 is the same
objective function used to estimate the parameters
of an HMM based acoustic model. By drawing an
analogy between the acoustic model training and
language modeling in continuous space, the history
vectors are considered as the acoustic observations
(feature vectors) and the next word to be predicted is
considered as the label the acoustic features belong
to, and words with their morphological decomposi-
tions can be considered as the lexicon or dictionary.
Fig. 1 presents the topology of the model for model-
ing a word sequence of 3 words. Each word is mod-
eled with a single state left?to?right HMM topology.
Using a morphologically rich language (or a char-
acter based language like Chinese) to explain how
HMMs can be used for language modeling will be
helpful. In the figure, let the states be the words and
the observations that they emit are the morphemes
(or characters in the case of Chinese). The same
topology (3 states) can also be used to model a sin-
gle word, where the first state models the prefixes,
the middle state models the stem and the final state
models the suffixes. In this case, words are repre-
sented by network of morphemes. Each path in a
word network represents a segmentation (or ?pro-
nunciation?) of the word.
The basic idea of the proposed modeling is to cre-
ate a separate model for each word of the language
and use the language model corpus to estimate the
parameters of the model. However, one could argue
that the basic model could be improved by taking
the contexts of the morphemes into account. Instead
of building a single HMM for each word, several
models could be trained according to the context of
the morphemes. These models are called context?
<s> </s>s1 s2 s3
a11
a12
22a
a23
a33
Observation
Sequence
O1 O2 O3 O4 O5
Figure 1: HMM topology for language modeling in con-
tinuous space.
dependent morphemes. The most obvious choice is
to use both left and right neighbor of a morpheme as
context, and creating, what we call tri?morphemes.
In principal even if context-dependent morphemes
could improve the modeling accuracy, the number
of models increase substantially. For a vocabulary
size of V , the number of tri?morpheme could be as
high as V 3. However, most of the tri?morphemes
are either rare or will not be observed in the training
data altogether.
Decision tree is one approach that can solve this
problem. The main idea is to find similar tri?
morphemes and share the parameters between them.
The decision tree uses a top-down approach to split
the samples, which are in a single cluster at the root
of the tree, into smaller clusters by asking questions
about the current morpheme and its context. In our
case, the questions could be syntactic and/or seman-
tic in nature.
What we hope for is that in the new continuous
space there is some form of distance or similarity
between histories such that histories not observed in
the data for some words are smoothed by similar ob-
served histories.
2.4 Summary of the Continuous Language
Model Training and Using it for Decoding
In the upper part of Fig. 2 the language model train-
ing steps are shown. The training process starts with
the language model training corpus. From the sen-
tences a bigram word co?occurrence matrix is con-
structed. This is a square matrix where the num-
ber of rows (columns) equal to the vocabulary size
of the training data. The bigram co?occurrence ma-
462
trix is decomposed using SVD. The columns of the
left?singular matrix obtained from SVD is used to
map the bigram word histories into a lower dimen-
sional continuous parameter space. The projected
word history vectors are stacked together depending
on the size of the n?gram. For example, for trigram
modeling two history vectors are stacked together.
Even though, we have not done so, at this stage one
could cluster the word histories for robust parame-
ter estimation. Now, the feature vectors, their corre-
sponding transcriptions and the lexicon (baseforms)
are ready to perform the ?acoustic model training?.
One could use maximum likelihood criterion or any
other objective function such as Minimum Phone Er-
ror (MPE) training to estimate the language model
parameters in the continuous space.
The decoding phase could employ an adaptation
step, if one wants to adapt the language model to
a different domain, speaker or genre. Then, given
a hypothesized sequence of words the decoder ex-
tracts the corresponding feature vectors. The fea-
ture vectors are used to estimate the likelihood of
the word sequence using the HMM parameters. This
likelihood is used to compute the probability of the
word sequence. Next, we introduce Tied?Mixture
Modeling, which is a special HMM structure to ro-
bustly estimate model parameters.
3 Tied?Mixture Modeling
Hidden Markov Models (HMMs) have been exten-
sively used virtually in all aspects of speech and
language processing. In speech recognition area
continuous-density HMMs have been the standard
for modeling speech signals, where several thousand
context?dependent states have their own Gaussian
density functions to model different speech sounds.
Typically, speech data have hundreds of millions of
frames, which are sufficient to robustly estimate the
model parameters. The amount of data for language
modeling is orders of magnitude less than that of
the acoustic data in continuous space. Tied?Mixture
Hidden Markov Models (TM?HMMs) (Bellegarda
& Nahamoo, 1989; Huang & Jack, 1988) have a bet-
ter decoupling between the number of Gaussians and
the number of states compared to continuous den-
sity HMMs. The TM?HMM is useful for language
modeling because it allows us to choose the num-
HMM Training(e.g. ML, MMIE,MPE)
Model Adaptation
Decoding/Search
Model Training
Word History Clustering
Adaptation Data
Probability Computation
Nw1
to Create Feature Vector
,...,2ww DimensionalityReduction (e.g.,SVD
Decomposition)
Create Bigram
Co-occurance Matrix
Map Word Historiesinto Continuous Space
 i-2i-1
 i-1i
f(w     |   w    )
f(w    |   w     )Stack the History Vectors
Figure 2: Language Model Training and Adaptation in
Continuous Space.
ber of Gaussian densities and the number of mixture
weights independently. Much more data is required
to reliably estimate Gaussian densities than to esti-
mate mixture weights.
The evaluation of the observation density func-
tions for TM?HMMs can be time consuming due to
the large mixture weight vector and due to the fact
that for each frame all Gaussians have to be evalu-
ated. However, there are a number of solutions pro-
posed in the past that significantly reduces the com-
putation (Duchateau et al, 1998).
The function p(w | h), defined in a continu-
ous space, represents the conditional probability of
the word w given the history h. In general, h
contains previous words and additional information
(e.g. part-of-speech (POS) tags for the previous
words) that may help to the prediction of the next
word. Unlike TM?HMMs, using a separate HMM
for each word as in the case of Gaussian Mixture
Models (GMMs), to represent the probability distri-
bution functions results in the estimation problems
for the model parameters since each n?gram does
not have hundreds of examples. TM?HMMs use
Gaussian mixture probability density functions per
463
state in which a single set of Gaussians is shared
among all states:
p(o|w) =
J?
j
cw,jNj(o, ?w,j ,?w,j) (6)
where w is the state, Nj is the jth Gaussian, and o
is the observation (i.e. history) vectors. and J is the
number of component mixtures in the TM-HMM.
In order to avoid zero variance in word mapping
into continuous space, all the latent word vectors are
added a small amount of white noise.
The TM?HMM topology is given in Fig. 3. Each
state models a word and they all share the same set of
Gaussian densities. However, each state has a spe-
cific set of mixture weights associated with them.
This topology can model a word?sequence that con-
sist of three words in them. The TM?HMM esti-
mates the probability of observing the history vec-
tors (h) for a given word w. However, what we need
is the posterior probability p(w | h) of observing w
as the next word given the history, h. This can be
obtained using the Bayes rule:
p(w|h) = p(h|w)p(w)p(h) (7)
= p(h|w)p(w)?V
v=1 p(h|v)p(v)
(8)
where p(w) is the unigram probability of the word
w. The unigram probabilities can also be substituted
for more accurate higher order n?gram probabilities.
If this n?gram has an order that is equal to or greater
than the one used in defining the continuous contexts
h, then the TMLM can be viewed as performing a
kind of smoothing of the original n?gram model:
Ps(w | h) = P (w | h)p(h | w)?V
v=1 P (v | h)p(h | v)
(9)
where Ps(w | h) and P (w | h) are the smoothed
and original n?grams.
The TM?HMM parameters are estimated through
an iterative procedure called the Baum-Welch, or
forward-backward, algorithm (Baum et al, 1970).
The algorithm locally maximizes the likelihood
function via an iterative procedure. This type of
33
<s> </s>s1 s2 s3
a11
a12
22a
a23
a
Figure 3: Tied-Mixture HMM topology for language
modeling in continuous space. The mixtures are tied
across states. Each state represents a word. The TM-
HMM is completely defined with the mixture weights,
mixture densities and transition probabilities.
training is identical to training continuous density
HMMs except the Gaussians are tied across all arcs.
For the model estimation equations the readers are
referred to (Bellegarda & Nahamoo, 1989; Huang &
Jack, 1988).
Next, we introduce the speech recognition system
used for the experiments.
4 Speech Recognition Architecture
The speech recognition experiments are carried out
on the Iraqi Arabic side of an English to Iraqi Ara-
bic speech-to-speech translation task. This task cov-
ers the military and medical domains. The acoustic
data has about 200 hours of conversational speech
collected in the context of a DARPA supported
speech-to-speech (S2S) translation project (Gao et
al., 2006).
The feature vectors for training acoustic models
are generated as follows. The speech data is sampled
at 16kHz and the feature vectors are computed every
10ms. First, 24-dimensional MFCC features are ex-
tracted and appended with the frame energy. The
feature vector is then mean and energy normalized.
Nine vectors, including the current vector and four
vectors from its right and left contexts, are stacked
leading to a 216-dimensional parameter space. The
feature space is finally reduced from 216 to 40 di-
mensions using a combination of linear discriminant
analysis (LDA), feature space MLLR (fMLLR) and
feature space MPE (fMPE) training (Povey et al,
464
2005). The baseline speech recognition system used
in our experiments is the state?of?the?art and pro-
duces a competitive performance.
The phone set consists of 33 graphemes represent-
ing speech and silence for acoustic modeling. These
graphemes correspond to letters in Arabic plus si-
lence and short pause models. Short vowels are im-
plicitly modeled in the neighboring graphemes. Fea-
ture vectors are first aligned, using initial models,
to model states. A decision tree is then built for
each state using the aligned feature vectors by ask-
ing questions about the phonetic context; quinphone
questions are used in this case. The resulting tree has
about 3K leaves. Each leaf is then modeled using
a Gaussian mixture model. These models are first
bootstrapped and then refined using three iterations
of forward?backward training. The current system
has about 75K Gaussians.
The language model training data has 2.8M words
with 98K unique words and it includes acoustic
model training data as a subset. The morpholog-
ically analyzed training data has 58K unique vo-
cabulary items. The pronunciation lexicon consists
of the grapheme mappings of these unique words.
The mapping to graphemes is one-to-one and there
are very few pronunciation variants that are sup-
plied manually mainly for numbers. A statistical tri-
gram language model using Modified Kneser-Ney
smoothing (Chen& Goodman, 1996) has been built
using the training data, which is referred to as Word-
3gr.
For decoding a static decoding graph is com-
piled by composing the language model, the pro-
nunciation lexicon, the decision tree, and the HMM
graphs. This static decoding scheme, which com-
piles the recognition network off?line before decod-
ing, is widely adopted in speech recognition (Ri-
ley et al, 2002). The resulting graph is further op-
timized using determinization and minimization to
achieve a relatively compact structure. Decoding is
performed on this graph using a Viterbi beam search.
5 Experimental Results
We used the following TMLM parameters to build
the model. The SVD projection size is set to 200
(i.e. R = 200) for each bigram history. This re-
sults into a trigram history vector of size 400. This
?8 ?7 ?6 ?5 ?4 ?3 ?2 ?1 0?9
?8
?7
?6
?5
?4
?3
?2
?1
0
N?gram Probability
TML
M P
roba
bility
Figure 4: Scatter plot of the n?gram and TMLM proba-
bilities.
vector is further projected down to a 50 dimensional
feature space using LDA transform. The total num-
ber of Gaussian densities used for the TM?HMM is
set to 1024. In order to find the overall relationship
between trigram and TMLM probabilities we show
the scatter plot of the trigram and TMMT probabili-
ties in Fig. 4. While calculating the TMLM score the
TMLM likelihood generated by the model is divided
by 40 to balance its dynamic range with that of the
n?gram model. Most of the probabilities lie along
the diagonal line. However, some trigram proba-
bilities are modulated making TMLM probabilities
quite different than the corresponding trigram prob-
abilities. Analysis of TMLM probabilities with re-
spect to the trigram probabilities would be an inter-
esting future research.
We conducted the speech recognition language
modeling experiments on 3 testsets: TestA, TestB
and TestC. All three test sets are from July?07
official evaluations of the IBM?s speech-to-speech
translation system by DARPA. TestA consists of
sentences spoken out in the field to the IBM?s S2S
system during live evaluation. TestB contains sen-
tences spoken in an office environment to the live
S2S system. Using on-the-spot speakers for TestA
and TestB meant to have shorter and clean sentences.
Finally TestC contains pre-recorded sentences with
much more hesitations and more casual conversa-
tions compared to the other two testsets. TestA,
TestB and TestC have 309, 320 and 561 sentences,
respectively.
465
LM TestA TestB TestC All
Word-3gr 18.7 18.6 38.9 32.9
TMLM 18.8 18.9 38.2 32.5
TMLM + Word-3gr 17.6 18.0 37.4 31.9
Table 1: Speech Recognition Language Model Rescoring
Results.
In order to evaluate the performance of the
TMLM, a lattice with a low oracle error rate was
generated by a Viterbi decoder using the word tri-
gram model (Word-3gr) model. From the lattice at
most 30 (N=30) sentences are extracted for each ut-
terance to form an N-best list. The N?best error rate
for the combined test set (All) is 22.7%. The N?
best size is limited (it is not in the hundreds), simply
because of faster experiment turn-around. These ut-
terances are rescored using TMLM. The results are
presented in Table 1. The first two rows in the ta-
ble show the baseline numbers for the word trigram
(Word?3gr) model. TestA has a WER of 18.7% sim-
ilar to that of TestB (18.6%). The WER for TestC
is relatively high (38.9%), because, as explained
above, TestC contains causal conversation with hes-
itations and repairs, and speakers do not necessar-
ily stick to the domain. Moreover, when users are
speaking to a device, as in the case of TestA and
TestB, they use clear and shorter sentences, which
are easier to recognize. The TMLM does not pro-
vide improvements for TestA and TestB but it im-
proves the WER by 0.7% for TestC. The combined
overall result is a 0.4% improvement over base-
line. This improvement is not statistically signifi-
cant. However, interpolating TMLM with Word-3gr
improves the WER to 31.9%, which is 1.0% better
than that of the Word-3gr. Standard p-test (Matched
Pairs Sentence-Segment Word Error test available
in standard SCLITEs statistical system comparison
program from NIST) shows that this improvement
is significant at p < 0.05 level. The interpolation
weights are set equally to 0.5 for each LM.
6 Discussions
Despite limited but encouraging experimental re-
sults, we believe that the proposed perspective is a
radical departure from the traditional n?gram based
language modeling methods. The new perspective
opens up a number of avenues which are impossible
to explore in one paper.
We realize that there are a number of outstand-
ing issues with the proposed perspective that re-
quire a closer look. We make a number of deci-
sions to build a language model within this perspec-
tive. The decisions are sometimes ad hoc. The de-
cisions are made in order to build a working sys-
tem and are by no means the best decisions. In
fact, it is quite likely that a different set of de-
cisions may result into a better system. Using a
word?morpheme co?occurrence matrix instead of a
morpheme?morpheme co?occurrence matrix is one
such decision. Another one is the clustering/tying
of the rarely observed events to achieve robust para-
meter estimation both for the SVD and TMLM pa-
rameter estimation. We also use a trivial decision
tree to build the models where there were no con-
text questions. Clustering morphemes with respect
to their syntactic and semantic context is another
area which should be explored. In fact, we are in
the process of building these models. Once we have
realized the full potential of the baseline maximum
likelihood TMLM, then we will investigate the dis-
criminative training methods such as MPE (Povey
& Woodland, 2002) to further improve the language
model performance and adaptation to new domains
using MLLR (Legetter & Woodland, 1995).
We also realize that different problems such as
segmentation (e.g. Chinese) of words or morpholog-
ical decomposition of words into morphemes can be
addressed within the proposed perspective.
7 Conclusions
We presented our progress in improving continuous-
space language modeling. We proposed the Tied-
Mixture Language Model (TMLM), which allows
for robust parameter estimation through the use
of tying and improves on the previously presented
GMLM. The new formulation lets us train a para-
metric language model using off?the?shelf acoustic
model training tools. Our initial experimental results
validated the proposed approach with encouraging
results.
466
References
M.Afify, O. Siohan and R. Sarikaya. 2007. Gaussian
Mixture Language Models for Speech Recognition,
ICASSP, Honolulu, Hawaii.
C.J. Legetter and P.C. Woodland. 1995. Maximum like-
lihood linear regression for speaker adaptation of con-
tinuous density hidden Markov models, Computer
Speech and Language, vol.9, pp. 171-185.
J. Bellegarda. 2000. Large Vocabulary Speech Recogni-
tion with Multispan Language Models, IEEE Trans-
actions on Speech and Audio Processing, vol. 8, no. 1,
pp. 76-84.
H. Schwenk, and J.L. Gauvain. 2003. Using Continuous
Space Language Models for Conversational Telephony
Speech Recognition, IEEE Workshop on Spontaneous
Speech Processing and Recognition, Tokyo, Japan.
J. Duchateau, K. Demuynck, D.V. Compernolle and P.
Wambacq. 1998. Improved Parameter Tying for Ef-
ficient Acoustic Model Evaluation in Large Vocabu-
lary Continuous Speech Recognition. Proc. of ICSLP,
Sydney, Australia.
Y. Gao, L. Gu, B. Zhou, R. Sarikaya, H.-K. Kuo. A.-
V.I. Rosti, M. Afify, W. Zhu. 2006. IBM MASTOR:
Multilingual Automatic Speech-to-Speech Translator.
Proc. of ICASSP, Toulouse, France.
S. Chen, J. Goodman. 1996. An Empirical Study of
Smoothing Techniques for Language Modeling, ACL,
Santa Cruz, CA.
J. Bellagarda and D. Nahamoo. 1989. Tied mixture con-
tinuous parameter models for large vocabulary isolated
speech recognition, Proc. of ICASSP, pp. 13-16.
X.D. Huang and M.A. Jack. 1988. Hidden Markov Mod-
elling of Speech Based on a Semicontinuous Model,
Electronic Letters, 24(1), pp. 6-7, 1988.
D. Povey and P.C. Woodland. 2002. Minimum phone er-
ror and I-smoothing for improved discriminative train-
ing, Proc. of ICASSP, pp. 105?108, Orlando, Florida.
D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau,
G. Zweig. 2005. fMPE: Discriminatively Trained
Features for Speech Recognition, Proc. of ICASSP,
pp. 961?964, Philadelphia, PA.
C. Chelba and F. Jelinek. 2000. Structured language
modeling, Computer Speech and Language, 14(4),
283?332, 2000.
H. Erdogan, R. Sarikaya, S.F. Chen, Y. Gao and M.
Picheny. 2005. Using Semantic Analysis to Improve
Speech Recognition Performance, Computer Speech
& Language Journal, vol. 19(3), pp: 321?343.
B. Roark, M. Saraclar, M. Collins. 2007. Using Seman-
tic Analysis to Improve Speech Recognition Perfor-
mance, Computer Speech & Language, vol. 21(2), pp:
373?392.
M. Riley, E. Bocchieri, A. Ljolje and M. Saraclar. 2007.
The AT&T 1x real-time Switchboard speech-to-text
system, NIST RT02 Workshop, Vienna, Virginia.
Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. 2003.
A Neural Probabilistic Language Model. Journal of
Machine Learning Research, vol. 3, 11371155.
S. Deerwester, Susan Dumais, G. W. Furnas, T. K. Lan-
dauer, R. Harshman. 1990. Indexing by Latent Se-
mantic Analysis, Journal of the American Society for
Information Science, 41 (6): 391?407.
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970. A
Maximization Techniques Occurring in the Statistical
Analysis of Probabilistic Functions of Markov Chains,
The Annals of Mathematical Statistics, 41(1):164?171.
467
Proceedings of NAACL HLT 2009: Short Papers, pages 277?280,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Fast decoding for open vocabulary spoken term detection
1B. Ramabhadran,1A. Sethy, 2J. Mamou?1 B. Kingsbury, 1 U. Chaudhari
1IBM T. J. Watson Research Center
Yorktown Heights,NY
2IBM Haifa Research Labs
Mount Carmel,Haifa
Abstract
Information retrieval and spoken-term detec-
tion from audio such as broadcast news, tele-
phone conversations, conference calls, and
meetings are of great interest to the academic,
government, and business communities. Mo-
tivated by the requirement for high-quality in-
dexes, this study explores the effect of using
both word and sub-word information to find
in-vocabulary and OOV query terms. It also
explores the trade-off between search accu-
racy and the speed of audio transcription. We
present a novel, vocabulary independent, hy-
brid LVCSR approach to audio indexing and
search and show that using phonetic confu-
sions derived from posterior probabilities es-
timated by a neural network in the retrieval
of OOV queries can help in reducing misses.
These methods are evaluated on data sets from
the 2006 NIST STD task.
1 Introduction
Indexing and retrieval of speech content in vari-
ous forms such as broadcast news, customer care
data and on-line media has gained a lot of interest
for a wide range of applications from market in-
telligence gathering, to customer analytics and on-
line media search. Spoken term detection (STD) is
a key information retrieval technology which aims
open vocabulary search over large collections of
spoken documents. An approach for solving the out-
of-vocabulary (OOV) issues (Saraclar and Sproat,
2004) consists of converting speech into phonetic,
?TThe work done by J. Mamou was partially funded by the
EU projects SAPIR and HERMES
syllabic or word-fragment transcripts and represent-
ing the query as a sequence of phones, syllables or
word-fragments respectively. Popular approaches
include subword decoding (Clements et al, 2002;
Mamou et al, 2007; Seide et al, 2004; Siohan and
Bacchiani, 2005) and representations enhanced with
phone confusion probabilities and approximate sim-
ilarity measures (Chaudhari and Picheny, 2007).
2 Fast Decoding Architecture
The first step in converting speech to a searchable in-
dex involves the use of an ASR system that produces
word, word-fragment or phonetic transcripts. In
this paper, the LVCSR system is a discriminatively
trained speaker-independent recognizer using PLP-
derived features and a quinphone acoustic model
with approximately 1200 context dependent states
and 30000 Gaussians. The acoustic model is trained
on 430 hours of audio from the 1996 and 1997 En-
glish Broadcast News Speech corpus (LDC97S44,
LDC98S71) and the TDT4 Multilingual Broadcast
News Speech corpus (LDC2005S11).
The language model used for decoding is a tri-
gram model with 84087 words trained on a collec-
tion of 335M words from the following data sources:
Hub4 Language Model data, EARS BN03 closed
captions and GALE Broadcast news and conversa-
tions data. A word-fragment language model is built
on this same data after tokenizing the text to frag-
ments using a fragment inventory of size 21000. A
greedy search algorithm assigns the longest possi-
ble matching fragment first and iteratively uses the
next longest possible fragment until the entire pro-
nunciation of the OOV term has been represented
277
0 5 10 15 20 25 30
30
40
50
60
70
80
90
Real Time Factor
W
ER
Figure 1: Speed vs WER
by sub-word units.
The speed and accuracy of the decoding are con-
trolled using two forms of pruning. The first is the
standard likelihood-based beam pruning that is used
in many Viterbi decoders. The second is a form
of Gaussian shortlisting in which the Gaussians in
the acoustic model are clustered into 1024 clusters,
each of which is represented by a single Gaussian.
When the decoder gets a new observation vector, it
computes the likelihood of the observation under all
1024 cluster models and then ranks the clusters by
likelihood. Observation likelihoods are then com-
puted only for those mixture components belonging
to the top maxL1 clusters; for components outside
this set a default, low likelihood is used. To illus-
trate the trade-offs in speed vs. accuracy that can
be achieved by varying the two pruning parame-
ters, we sweep through different values for the pa-
rameters and measure decoding accuracy, reported
as word error rate (WER), and decoding speed, re-
ported as times faster than real time (xfRT). For ex-
ample, a system that operates at 20xfRT will require
one minute of time (measured as elapsed time) to
process 20 minutes of speech. Figure 1 illustrates
this effect on the NIST 2006 Spoken Term Detec-
tion Dev06 test set.
3 Lucene Based Indexing and Search
The main difficulty with retrieving information from
spoken data is the low accuracy of the transcription,
particularly on terms of interest such as named en-
tities and content words. Generally, the accuracy
of a transcript is measured by its word error rate
(WER), which is characterized by the number of
substitutions, deletions, and insertions with respect
to the correct audio transcript. Mamou (Mamou
et al, 2007) presented the enhancement in recall
and precision by searching on word confusion net-
works instead of considering only the 1-best path
word transcript. We used this model for searching
in-vocabulary queries.
To handle OOV queries, a combination of
word and phonetic search was presented by
Mamou (Mamou et al, 2007). In this paper, we ex-
plore fuzzy phonetic search extending Lucene1, an
Apache open source search library written in Java,
for indexing and search. When searching for these
OOVs in word-fragment indexes, they are repre-
sented phonetically (and subsequently using word-
fragments) using letter-to-phoneme (L2P) rules.
3.1 Indexing
Each transcript is composed of basic units (e.g.,
word, word-fragment, phones) associated with a be-
gin time, duration and posterior probability. An
inverted index is used in a Lucene-based indexing
scheme. Each occurrence of a unit of indexing u in
a transcript D is indexed on its timestamp. If the
posterior probability is provided, we store the confi-
dence level of the occurrence of u at the time t that
is evaluated by its posterior probability Pr(u|t,D).
Otherwise, we consider its posterior probability to
be one. This representation allows the indexing of
different types of transcripts into a single index.
3.2 Retrieval
Since the vocabulary of the ASR system used to gen-
erate the word transcripts is known, we can easily
identify IV and OOV parts of the query. We present
two different algorithms, namely, exact and fuzzy
search on word-fragment transcripts. For search
on word-fragment or phonetic transcripts, the query
terms are converted to their word-fragment or pho-
netic representation.
Candidate lists of each query unit are extracted
from the inverted index. For fuzzy search, we re-
trieve several fuzzy matches from the inverted in-
dex for each unit of the query using the edit distance
weighted by the substitution costs provided by the
confusion matrix. Only the matches whose weighted
1http://lucene.apache.org/
278
edit distance is below a given threshold are returned.
We use a dynamic programming algorithm to incor-
porate the confusion costs specified in the matrix
in the distance computation. Our implementation is
fail-fast since the procedure is aborted if it is discov-
ered that the minimal cost between the sequences is
greater than a certain threshold.
The score of each occurrence aggregates the pos-
terior probability of each indexed unit. The occur-
rence of each unit is also weighted (user defined
weight) according to its type, for example, a higher
weight can be assigned to word matches instead of
word-fragment or phonetic matches. Given the na-
ture of the index, a match for any query term cannot
span across two consecutively indexed units.
3.3 Hybrid WordFragment Indexing
For the hybrid system we limited the word portion
of the ASR system?s lexicon to the 21K most fre-
quent (frequency greater than 5) words in the acous-
tic training data. This resulted in roughly 11M
(3.1%) OOV tokens in the hybrid LM training set
and 1127(2.5%) OOV tokens in the evaluation set.
A relative entropy criterion described in (Siohan and
Bacchiani, 2005) based on a 5-gram phone language
model was used to identify fragments. We selected
21K fragments to complement the 21K words result-
ing in a composite 42K vocabulary. The language
model text (11M (3.1%) fragment tokens and 320M
word tokens) was tokenized to contain words and
word-fragments (for the OOVs) and the resulting hy-
brid LM was used in conjunction with the acoustic
models described in Section 2.
4 Neural Network Based Posteriors for
Fuzzy Search
In assessing the match of decoded transcripts with
search queries, recognition errors must be accounted
for. One method relies on converting both the de-
coded transcripts and queries into phonetic represen-
tations and modeling the confusion between phones,
typically represented as a confusion matrix. In this
work, we derive this matrix from broadcast news de-
velopment data. In particular, two systems: HMM
based automatic speech recognition (ASR) (Chaud-
hari and Picheny, 2007) and a neural network based
acoustic model (Kingsbury, 2009), are used to ana-
lyze the data and the results are compared to produce
confusion estimates.
Let X = {xt} represent the input feature frames
and S the set of context dependent HMM states.
Associated with S is a many to one map M from
each member sj ? S to a phone in the phone set
pk ? P. This map collapses the beginning, mid-
dle, and end context dependent states to the central
phone identity. The ASR system is used to generate
a state based alignment of the development data to
the training transcripts. This results in a sequence
of state labels (classes) {st}, st ? S , one for each
frame of the input data. Note that the aligned states
are collapsed to the phone identity with M, so the
frame class labels are given by {ct}, ct ? P.
Corresponding to each frame, we also use the
state posteriors derived from the output of a Neu-
ral Network acoustic model and the prior probabil-
ities computed on the training set. Define Xt =
{. . . , xt, . . .} to be the sub-sequence of the input
speech frames centered around time index t. The
neural network takes Xt as input and produces
lt(sj) = y(sj|Xt)? l(sj), sj ? S
where y is the neural network output and l is the
prior probability, both in the log domain. Again, the
state labels are mapped using M, so the above pos-
terior is interpreted as that for the collapsed phone:
lt(sj) ? lt(M(sj)) = lt(pj), pj = M(sj).
The result of both analyses gives the following set of
associations:
c0 ? l0(p0), l0(p1), l0(p2), . . .
c1 ? l1(p0), l1(p1), l1(p2), . . .
.
.
ct ? lt(p0), lt(p1), lt(p2), . . .
Each log posterior li(pj) is converted into a count
ni,j = ceil[N ? eli(pj)],
where N is a large constant, i ranges over the
time index, and j ranges over the context dependent
states. From the counts, the confusion matrix entries
are computed. The total count for each state is
nj(k) =
?
i:ci=pj
ni,k,
279
where k is an index over the states.
?
????
n1(1) n1(2) . . .
n2(1) n2(2) . . .
.
.
?
????
The rows of the above matrix correspond to the ref-
erence and the columns to the observations. By nor-
malizing the rows, the entries can be interpreted as
?probability? of an observed phone (indicated by the
column) given the true phone.
5 Experiments and Results
The performance of a spoken term detection system
is measured using DET curves that plot the trade-off
between false alarms (FAs) and misses. This NIST
STD 2006 evaluation metric used Actual/Maximum
Term Weighted Value (ATWV/MTWV) that allows
one to weight FAs and Misses per the needs of the
task at hand (NIST, 2006).
Figure 2 illustrates the effect of speed on ATWV
on the NIST STD 2006 Dev06 data set using 1107
query terms. As the speed of indexing is increased to
many times faster than real time, the WER increases,
which in turn decreases the ATWV measure. It can
be seen that the use of word-fragments improves
the performance on OOV queries thus making the
combined search better than simple word search.
The primary advantage of using a hybrid decoding
scheme over a separate word and fragment based
decoding scheme is the speed of transforming the
audio into indexable units. The blue line in the fig-
ure illustrates that when using a hybrid setup, the
same performance can be achieved at speeds twice
as fast. For example, with the combined search
on two different decodes, an ATWV of 0.1 can be
achieved when indexing at a speed 15 times faster
than real time, but with a hybrid system, the same
performance can be reached at an indexing speed 30
times faster than real time. The ATWV on the hybrid
system also degrades gracefully with faster speeds
when compared to separate word and word-fragment
systems. Preliminary results indicate that fuzzy
search on one best output gives the same ATWV
performance as exact search (Figure 2) on consen-
sus output. Also, a closer look at the retrieval results
of OOV terms revealed that many more OOVs are
retrieved with the fuzzy search.
0 5 10 15 20 25 30 35
?0.4
?0.2
0
0.2
0.4
0.6
0.8
1
Real Time Factor
AT
W
V
 
 
exactWord
exactWordAndFrag
exactHybrid
Figure 2: Effect of WER on ATWV. Note that the cuves
for exactWord and exactWordAndFrag lie on top of each
other.
6 CONCLUSION
In this paper, we have presented the effect of rapid
decoding on a spoken term detection task. We
have demonstrated that hybrid systems perform well
and fuzzy search with phone confusion probabilities
help in OOV retrieval.
References
U. V. Chaudhari and M. Picheny. 2007. Improvements in
phone based audio search via constrained match with
high order confusion estimates. In Proc. of ASRU.
M. Clements, S. Robertson, and M. S. Miller. 2002.
Phonetic searching applied to on-line distance learning
modules. In Proc. of IEEE Digital Signal Processing
Workshop.
B. Kingsbury. 2009. Lattice-based optimization
of sequence classification criteria for neural-network
acoustic modeling. In Proc. of ICASSP.
J. Mamou, B. Ramabhadran, and O. Siohan. 2007. Vo-
cabulary independent spoken term detection. In Proc.
of ACM SIGIR.
NIST. 2006. The spoken term de-
tection (STD) 2006 evaluation plan.
http://www.nist.gov/speech/tests/std/docs/std06-
evalplan-v10.pdf.
M. Saraclar and R. Sproat. 2004. Lattice-based search
for spoken utterance retrieval. In Proc. HLT-NAACL.
F. Seide, P. Yu, C. Ma, and E. Chang. 2004. Vocabulary-
independent search in spontaneous speech. In Proc. of
ICASSP.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary in-
dependent audio search using path based graph index-
ing. In Proc. of Interspeech.
280
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20?28,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Deep Neural Network Language Models
Ebru Ar?soy, Tara N. Sainath, Brian Kingsbury, Bhuvana Ramabhadran
IBM T.J. Watson Research Center
Yorktown Heights, NY, 10598, USA
{earisoy, tsainath, bedk, bhuvana}@us.ibm.com
Abstract
In recent years, neural network language mod-
els (NNLMs) have shown success in both
peplexity and word error rate (WER) com-
pared to conventional n-gram language mod-
els. Most NNLMs are trained with one hid-
den layer. Deep neural networks (DNNs) with
more hidden layers have been shown to cap-
ture higher-level discriminative information
about input features, and thus produce better
networks. Motivated by the success of DNNs
in acoustic modeling, we explore deep neural
network language models (DNN LMs) in this
paper. Results on a Wall Street Journal (WSJ)
task demonstrate that DNN LMs offer im-
provements over a single hidden layer NNLM.
Furthermore, our preliminary results are com-
petitive with a model M language model, con-
sidered to be one of the current state-of-the-art
techniques for language modeling.
1 Introduction
Statistical language models are used in many natural
language technologies, including automatic speech
recognition (ASR), machine translation, handwrit-
ing recognition, and spelling correction, as a crucial
component for improving system performance. A
statistical language model represents a probability
distribution over all possible word strings in a lan-
guage. In state-of-the-art ASR systems, n-grams are
the conventional language modeling approach due
to their simplicity and good modeling performance.
One of the problems in n-gram language modeling
is data sparseness. Even with large training cor-
pora, extremely small or zero probabilities can be
assigned to many valid word sequences. Therefore,
smoothing techniques (Chen and Goodman, 1999)
are applied to n-grams to reallocate probability mass
from observed n-grams to unobserved n-grams, pro-
ducing better estimates for unseen data.
Even with smoothing, the discrete nature of n-
gram language models make generalization a chal-
lenge. What is lacking is a notion of word sim-
ilarity, because words are treated as discrete enti-
ties. In contrast, the neural network language model
(NNLM) (Bengio et al, 2003; Schwenk, 2007) em-
beds words in a continuous space in which proba-
bility estimation is performed using single hidden
layer neural networks (feed-forward or recurrent).
The expectation is that, with proper training of the
word embedding, words that are semantically or gra-
matically related will be mapped to similar loca-
tions in the continuous space. Because the prob-
ability estimates are smooth functions of the con-
tinuous word representations, a small change in the
features results in a small change in the probabil-
ity estimation. Therefore, the NNLM can achieve
better generalization for unseen n-grams. Feed-
forward NNLMs (Bengio et al, 2003; Schwenk
and Gauvain, 2005; Schwenk, 2007) and recur-
rent NNLMs (Mikolov et al, 2010; Mikolov et al,
2011b) have been shown to yield both perplexity and
word error rate (WER) improvements compared to
conventional n-gram language models. An alternate
method of embedding words in a continuous space
is through tied mixture language models (Sarikaya
et al, 2009), where n-grams frequencies are mod-
eled similar to acoustic features.
To date, NNLMs have been trained with one hid-
20
den layer. A deep neural network (DNN) with mul-
tiple hidden layers can learn more higher-level, ab-
stract representations of the input. For example,
when using neural networks to process a raw pixel
representation of an image, lower layers might de-
tect different edges, middle layers detect more com-
plex but local shapes, and higher layers identify ab-
stract categories associated with sub-objects and ob-
jects which are parts of the image (Bengio, 2007).
Recently, with the improvement of computational
resources (i.e. GPUs, mutli-core CPUs) and better
training strategies (Hinton et al, 2006), DNNs have
demonstrated improved performance compared to
shallower networks across a variety of pattern recog-
nition tasks in machine learning (Bengio, 2007;
Dahl et al, 2010).
In the acoustic modeling community, DNNs
have proven to be competitive with the well-
established Gaussian mixture model (GMM) acous-
tic model. (Mohamed et al, 2009; Seide et al, 2011;
Sainath et al, 2012). The depth of the network (the
number of layers of nonlinearities that are composed
to make the model) and the modeling a large number
of context-dependent states (Seide et al, 2011) are
crucial ingredients in making neural networks com-
petitive with GMMs.
The success of DNNs in acoustic modeling leads
us to explore DNNs for language modeling. In this
paper we follow the feed-forward NNLM architec-
ture given in (Bengio et al, 2003) and make the neu-
ral network deeper by adding additional hidden lay-
ers. We call such models deep neural network lan-
guage models (DNN LMs). Our preliminary experi-
ments suggest that deeper architectures have the po-
tential to improve over single hidden layer NNLMs.
This paper is organized as follows: The next sec-
tion explains the architecture of the feed-forward
NNLM. Section 3 explains the details of the baseline
acoustic and language models and the set-up used
for training DNN LMs. Our preliminary results are
given in Section 4. Section 5 summarizes the related
work to our paper. Finally, Section 6 concludes the
paper.
2 Neural Network Language Models
This section describes a general framework for feed-
forward NNLMs. We will follow the notations given
























  
 
	

	

	

	
