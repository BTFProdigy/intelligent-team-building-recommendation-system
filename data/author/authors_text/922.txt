Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 41?48,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Positioning for Conceptual Development
using Latent Semantic Analysis
Fridolin Wild, Bernhard Hoisl
Vienna University of Economics
and Business Administration
Gaston Burek
University of Tu?bingen
Computational Linguistics Division
Abstract
With increasing opportunities to learn on-
line, the problem of positioning learners
in an educational network of content of-
fers new possibilities for the utilisation of
geometry-based natural language process-
ing techniques.
In this article, the adoption of latent se-
mantic analysis (LSA) for guiding learners
in their conceptual development is investi-
gated. We propose five new algorithmic
derivations of LSA and test their validity
for positioning in an experiment in order to
draw back conclusions on the suitability of
machine learning from previously accred-
ited evidence. Special attention is thereby
directed towards the role of distractors and
the calculation of thresholds when using
similarities as a proxy for assessing con-
ceptual closeness.
Results indicate that learning improves po-
sitioning. Distractors are of low value and
seem to be replaceable by generic noise
to improve threshold calculation. Fur-
thermore, new ways to flexibly calculate
thresholds could be identified.
1 Introduction
The path to new content-rich competencies is
paved by the acquisition of new and the reorgani-
sation of already known concepts. Learners will-
ing to take this journey, however, are imposed with
the problem of positioning themselves to that point
in a learning network of content, where they leave
their known trails and step into the unknown ? and
to receive guidance in subsequent further concep-
tual development.
More precisely, positioning requires to map
characteristics from a learner?s individual epis-
temic history (including both achievements and
shortcomings) to the characteristics of the avail-
able learning materials and to recommend reme-
dial action on how to achieve selected conceptual
development goals (Van Bruggen et al, 2006).
The conceptual starting points of learners nec-
essary to guide the positioning process is reflected
in the texts they are writing. Through structure
and word choice, most notably the application of
professional language, arrangement and meaning
of these texts give cues about the level of compe-
tency1 development.
As learning activities increasingly leave digital
traces as evidence for prior learning, positioning
support systems can be built that reduce this prob-
lem to developing efficient and effective match-
making procedures.
Latent semantic analysis (LSA) (Deerwester et
al., 1990) as one technology in the family of
geometry-based natural language models could in
principle provide a technological basis for the po-
sitioning aims outlined above. The assumption un-
derlying this is that the similarity to and of learn-
ing materials can be used as a proxy for similar-
ity in learning outcomes, i.e. the developmental
change in conceptual coverage and organisation
caused by learning.
In particular, LSA utilises threshold values for
the involved semantic similarity judgements. Tra-
ditionally the threshold is obtained by calculat-
ing the average similarity between texts that cor-
respond to the same category. This procedure can
be inaccurate if a representative set of documents
for each category is not available. Furthermore,
similarity values tend to decrease with increasing
corpora and vocabulary sizes. Also, the role of
distractors in this context, i.e. negative evidence
as reference material to sharpen classification for
positioning, is largely unknown.
With the following experiment, we intend to
1See (Smith, 1996) for a clarification of the difference of
competence and competency
41
validate that geometrical models (particularly la-
tent semantic analysis) can produce near human
results regarding their propositions on how to ac-
count written learner evidence for prior learning
and positioning these learners to where the best-
suiting starting points are. We will show that latent
semantic analysis works for positioning and that it
can provide effective positioning.
The main focus of this contribution is to inves-
tigate whether machine learning proves useful for
the positioning classifiers, whether distractors im-
prove results, and what the role of thresholds for
the classifiers is.
The rest of this paper is structured as follows.
At first, positioning with LSA and related work
are explained. This is followed by an outline of
our own approach to positioning. Subsequently,
a validation experiment for the set of new algo-
rithms is outlined with which new light is shed on
the utilisation of LSA for positioning. The results
of this experiment are analysed in the following
section in oder to, finally, yield conclusions and
an outlook.
2 Positioning with LSA
According to (Kalz et al, 2007), positioning ?is
a process that assists learners in finding a start-
ing point and an efficient route through the [learn-
ing] network that will foster competence build-
ing?. Often, the framework within which this
competence development takes places is a formal
curriculum offered by an educational provider.
Not only when considering a lifelong learner,
for whom the borders between formal and infor-
mal learning are absolutely permeable, recogni-
tion of prior learning turns out to be crucial for po-
sitioning: each individual background differs and
prior learning needs to be respected or even ac-
credited before taking up new learning activities ?
especially before enrolling in a curriculum.
Typically, the necessary evidence of prior learn-
ing (i.e., traces of activities and their outcomes)
are gathered in a learner?s portfolio. This portfolio
is then analysed to identify both starting points and
a first navigation path by mapping evidence onto
the development plans available within the learn-
ing network.
The educational background represented in the
portfolio can be of formal nature (e.g. certi-
fied exams) in which case standard admission
and exemption procedures may apply. In other
cases such standard procedures are not available,
therefore assessors need to intellectually evaluate
learner knowledge on specific topics. In proce-
dures for accreditation of prior learning (APL), as-
sessors decide whether evidence brought forward
may lead to exemptions from one or more courses.
For supporting the positioning process (as e.g.
needed for APL) with technology, three different
computational classes of approaches can be distin-
guished: mapping procedures based on the analy-
sis of informal descriptions with textmining tech-
nologies, meta-data based positioning, and posi-
tioning based on ontology mappings (Kalz et al,
2007). Latent semantic analysis is one of many
possible techniques that can be facilitated to sup-
port or even partially automate the analysis of in-
formal portfolios.
2.1 LSA
LSA is an algorithm applied to approximate the
meaning of texts, thereby exposing semantic struc-
ture to computation. LSA combines the classi-
cal vector-space model with a singular value de-
composition (SVD), a two-mode factor analysis.
Thus, bag-of-words representations of texts can be
mapped into a modified vector space that is as-
sumed to reflect semantic structure.
The basic idea behind LSA is that the colloca-
tion of terms of a given document-term-space re-
flects a higher-order ? latent semantic ? structure,
which is obscured by word usage (e.g. by syn-
onyms or ambiguities). By using conceptual in-
dices that are derived statistically via a truncated
SVD, this variability problem is believed to be
overcome.
In a typical LSA process, first a document-term
matrix is constructed from a given text base of n
documents containing m terms. This matrix M
of the size m ? n is then resolved by the SVD
into the term vector matrix T (constituting the left
singular vectors), the document vector matrix D
(constituting the right singular vectors) being both
orthonormal and the diagonal matrix S.
Multiplying the truncated matrices Tk, Sk and
Dk results in a new matrix Mk (see Figure 1)
which is the least-squares best fit approximation
of M with k singular values (Berry et al, 1994).
2.2 Related Work
LSA has been widely used in learning applications
such as automatic assessment of essays, provision
42
Figure 1: Reconstructing a textmatrix from the
lower-order latent-semantic space.
of feedback, and selection of suitable materials ac-
cording to the learner?s degree of expertise in spe-
cific domains.
The Intelligent Essay Assessor (IEA) is an ex-
ample of the first type of applications where the
semantic space is build from materials on the topic
to be evaluated. In (Foltz et al, 1999) the finding is
reported that the IEA rating performance is close
to the one of human raters.
In (van Bruggen et al, 2004) authors report that
LSA-based positioning requires creating a latent-
semantic space from text documents that model
learners? and public knowledge on a specific sub-
ject. Those texts include written material of learn-
ers? own production, materials that the learner has
studied and learned in the past, and descriptions of
learning activities that the learner has completed
in the past. Public knowledge on the specific sub-
ject includes educational materials of all kind (e.g.
textbooks or articles).
In this case the description of the activity needs
to be rich in the sense of terminology related to
the domain of application. LSA relies on the use
of rich terminology to characterize the meaning.
Following the traditional LSA procedure, the
similarity (e.g. cosine) between LSA vector mod-
els of the private and public knowledge is then cal-
culated to obtain the learner position with respect
to the public knowledge.
3 Learning Algorithms for Positioning
In the following, we design an experiment, con-
duct it, and evaluate the results to shed new light
on the use of LSA for positioning.
The basic idea of the experiment is to investi-
gate whether LSA works for advising assessors on
acceptance (or rejection) of documents presented
by the learner as evidence of previous conceptual
knowledge on specific subjects covered by the cur-
riculum. The assessment is in all cases done by
comparing a set of learning materials (model solu-
tions plus previously accepted/rejected reference
material) to the documents from learners? portfo-
lios using cosines as a proxy for their semantic
similarity.
In this comparison, thresholds for the cosine
measure?s values have to be defined above which
two documents are considered to be similar. De-
pending on how exactly the model solutions and
additional reference material are utilised, different
assessment algorithms can be developed.
To validate the proposed positioning services
elaborated below, we compare the automatic rec-
ommendations for each text presented as evidence
with expert recommendations over the same text
(external validation).
To train the thresholds and as a method for as-
sessing the provided evidence, we propose to use
the following five different unsupervised and su-
pervised positioning rules. These configurations
differ in the way how their similarity threshold
is calculated and against which selection of doc-
uments (model solutions and previously expert-
evaluated reference material) the ?incoming? docu-
ments are compared. We will subsequently run the
experiment to investigate their effectiveness and
compare the results obtained with them.
Figure 2: The five rules.
The visualisation in Figure 2 depicts the work-
ing principle of the rules described below. In each
panel, a vector space is shown. Circles depict ra-
dial cosine similarity. The document representa-
tives labelled with gn are documents with positive
evidence (?good? documents), the ones labelled
with bn are those with negative. The test docu-
43
ments carry the labels en (?essay?).
Best of Golden: The threshold is computed by
averaging the similarity of all three golden stan-
dard essays to each other. The similarity of the
investigated essay is compared to the best three
golden standard essays (=machine score). If the
machine score correlates above the threshold with
the human judgement, the test essay is stated cor-
rect. This rule assumes that the gold standards
have some variation in the correlation among each
other and that using the average correlation among
the gold standards as a threshold is taking that into
account.
Best of Good: Best essays of the humanly
judged good ones. The assumption behind this is
that with more positive examples to evaluate an in-
vestigated essay against, the precision of the eval-
uation should rise. The threshold is the average of
the positive evidence essays among each other.
Average to Good> Average among Good: Tests
if the similarity to the ?good? examples is higher
than the average similarity of the humanly judged
good ones. Assumption is that the good evi-
dence gathered circumscribes that area in the la-
tent semantic space which is representative of the
abstract model solution and that any new essay
should be within the boundaries characterised by
this positive evidence thus having a higher correla-
tion to the positive examples then they have among
each other.
Best of Good > Best of Bad: Tests whether the
maximum similarity to the good essays is higher
than the maximum similarity to bad essays. If a
tested essay correlates higher to the best of the
good than to the best of the bad, then it is clas-
sified as accepted.
Average of Good > average of Bad: The same
with average of good > average of bad. Assump-
tion behind this is again that both bad and good
evidence circumscribe an area and that the incom-
ing essay is in either the one or the other class.
4 Corpus and Space Construction
The corpus for building the latent semantic space
is constructed with 2/3 German language corpus
(newspaper articles) and 1/3 domain-specific (a
textbook split into smaller units enhanced by a col-
lection of topic related documents which Google
threw up among the first hits). The corpus has a
size of 444k words (59.719 terms, 2444 textual
units), the mean document length is 181 words
with a standard deviation of 156. The term fre-
quencies have a mean of 7.4 with a standard devi-
ation of 120.
The latent semantic space is constructed over
this corpus deploying the lsa package for R (Wild,
2008; Wild and Stahl, 2007) using dimcalc share
as the calculation method to estimate a good num-
ber of singular values to be kept and the standard
settings of textmatrix() to pre-process the raw
texts. The resulting space utilises 534 dimensions.
For the experiment, 94 essays scored by a hu-
man evaluator on a scale from 0 to 4 points where
used. The essays have a mean document length
of 22.75 terms with a standard deviation of 12.41
(about one paragraph).
To estimate the quality of the latent semantic
space, the learner writings were folded into the
semantic space using fold in(). Comparing the
non-partitioned (i.e. 0 to 4 in steps of .5) human
scores with the machine scores (average similar-
ity to the three initial model solutions), a highly
significant trend can be seen that is far from be-
ing perfect but still only slightly below what two
human raters typically show.
Figure 3: Human vs. Machine Scores.
Figure 3 shows the qualitative human expert
judgements versus the machine grade distribution
using the non-partitioned human scores (from 0 to
4 points in .5 intervals) against the rounded aver-
age cosine similarity to the initial three model so-
lutions. These machine scores are rounded such
that they ? again ? create the same amount of in-
tervals. As can be seen in the figure, the extreme
44
of each score level is displayed in the upper and
lower whisker. Additionally, the lower and upper
?hinge? and the median are shown. The overall
Spearman?s rank correlation of the human versus
the (continuous) machine scores suggests a with
.51 medium effect being highly significant on a
level with the p-value below .001. Comparing this
to untrained human raters, who typically correlate
around .6, this is in a similar area, though the ma-
chine differences can be expected to be different
in nature.
A test with 250 singular values was conducted
resulting in a considerately lower Spearman cor-
relation of non-partitioned human and machine
scores.
Both background and test corpus have deliber-
ately been chosen from a set of nine real life cases
to serve as a prototypical example.
For the experiment, the essay collection was
split by half into training (46) and test (48) set
for the validation. Each set has been partitioned
into roughly an equal number of accepted (scores
< 2, 22 essays in training set, 25 in test) and re-
jected essays (scores >= 2, 24 essays in training,
23 in test). All four subsets, ? test and training
partitioned into accepted and rejected ?, include a
similarly big number of texts.
In order to cross validate, the training and test
sets were random sampled ten times to get rid of
influences on the algorithms from the sort order of
the essays. Both test and training sets were folded
into the latent semantic space. Then, random sub
samples (see below) of the training set were used
to train the algorithms, whereas the test set of 48
test essays in each run was deployed to measure
precision, recall, and the f-measure to analyse the
effectiveness of the rules proposed.
Similarity is used as a proxy within the al-
gorithms to determine whether a student writing
should be accepted for this concept or rejected. As
similarity measure, the cosine similarity cosine()
was used.
In each randomisation loop, the share of ac-
cepted and rejected essays to learn from was var-
ied in a second loop of seven iterations: Always
half of the training set essays were used and the
amount of accepted essays was decreased from 9
to 2 while the number of rejected essays was in-
creased from 2 to 9. This way, the influence of the
number of positive (and negative) examples could
be investigated.
This mixture of accepted and rejected evidence
to learn from was diversified to investigate the
influence of learning from changing shares and
rising or decreasing numbers of positive and/or
negative reference documents ? as well as to
analyse the influence of recalculated thresholds.
While varying these training documents, the hu-
man judgements were given to the machine in or-
der to model learning from previous human asses-
sor acceptance and rejection.
5 Findings
5.1 Precision versus Recall
The experiments where run with the five different
algorithms and with the sampling procedures de-
scribed above. For each experiment precision and
recall where measured to find out if an algorithm
can learn from previous inputs and if it is better or
worse compared to the others.
As mentioned above, the following diagrammes
depict from left to right a decreasing number of ac-
cepted essays available for training (9 down to 2)
while the number of rejected essays made avail-
able for training is increased (from 2 to 9).
Rule 1 to 3 do not use these negative samples,
rule 1 does not even use the positive samples but
just three additional model solutions not contained
in the training material of the others. The curves
show the average precision, recall, and f-measure2
of the ten randomisations necessary for the cross
validation. The size of the circles along the curves
symbolises the share of accepted essays in the
training set.
Figure 4: Rule 1: Best of Three Golden
2F = 2 ? precision?recallprecision+recall
45
Figure 4 shows that recall and precision stay sta-
ble as there are no changes to the reference ma-
terial taken into account: all essays are evaluated
using three fixed ?gold standard? texts. This rule
serves as a baseline benchmark for the other re-
sults.
Figure 5: Rule 2: Best of Good
Figure 5 depicts a falling recall when having
less positively judged essays in the training sam-
ple. In most cases, the recall is visibly higher than
in the first rule, ?Best of Gold?, especially when
given enough good examples to learn from. Preci-
sion is rather stable. We interpret that the falling
recall can be led back to the problem of too few
examples that are then not able to model the target
area of the latent semantic space.
Figure 6: Rule 3: Avg of Good > Avg among
Good
Figure 6 displays that the recall worsens and is
very volatile3. Precision, however, is very stable
3We analysed the recall in two more randomisations of the
and slightly higher than in the previous rule, es-
pecially with rising numbers of positive examples.
It seems that the recall is very dependant on the
positive examples whether they are able to char-
acterise representative boundaries: seeing recall
change with varying amounts of positive exam-
ples, this indicates that the boundaries are not very
well chosen. We assume that this is related to con-
taining ?just pass? essays that were scored with 2.0
or 2.5 points and distort the boundaries of the tar-
get area in the latent semantic concept space.
Figure 7: Rule 4: Best of Good > Best of Bad
Figure 7 exhibits a quickly falling recall, though
starting on a very high level, whereas precision
is relatively stable. Having more negative evi-
dence clearly seems to be counter productive and
it seems more important to have positive examples
to learn from. We have two explanations for this:
First, bad examples scatter across the space and it
is likely for a good essay to correlate higher with
a bad one when there is only a low number of pos-
itive examples. Second, bad essays might contain
very few words and thus expose correlation arte-
facts that would in principle be easy to detect, but
not with LSA.
Figure 8 depicts a recall that is generically
higher than in the ?Best of Gold? case, while pre-
cision is in the same area. Recall seems not to be
so stable but does not drop with more bad samples
(and less good ones) to learn from such as in the
?Best of Good? case. We interpret that noise can be
added to increase recall while still only a low num-
ber of positive examples is available to improve it.
whole experiment; whereas the other rules showed the same
results, the recall of this rule was unstable over the test runs,
but in tendency lower than in the other rules.
46
Figure 8: Rule 5: Avg of Good > Avg of Bad
5.2 Clustering
To gain further insight about the location of the
94 essays and three gold standards in the higher
order latent-semantic space, a simple cluster anal-
ysis of their vectors was applied. Therefore, all
document-to-document cosine similarities were
calculated, filtered by a threshold of .65 to capture
only strong associations, and, subsequently, a net-
work plot of this resulting graph was visualised.
Figure 9: Similarity Network (cos >= .65).
As can be seen in the two charts, the humanly
positively judged evidence seems to cluster quite
well in the latent-semantic space when visualised
as a network plot. Through filtering the docu-
ment vectors by the vocabulary used only in the
accepted, rejected, or both classes, an even clearer
picture could be generated, shown in Figure 10.
Both figures clearly depict a big connected com-
ponent consisting mainly out of accepted essays,
whereas the rejected essays mainly spread in the
Figure 10: Network with filtered vocabulary.
unconnected surrounding. The rejected essays are
in general not similar to each other, whereas the
accepted samples are.
The second Figure 10 is even more homoge-
neous than the first due to the use of the restricted
vocabulary (i.e. the terms used in all accepted and
rejected essays).
6 Conclusion and Outlook
Distractors are of low value in the rules tested. It
seems that generic noise can be added to keep re-
call higher when only a low number of positive ex-
amples can be utilised. An explanation for this can
be found therein that there are always a lot more
heterogeneous ways to make an error. Homogene-
ity can only be assumed for the positive evidence,
not for negative evidence.
Noise seems to be useful for the calculation
of thresholds. Though it will need further inves-
tigation whether our new hypothesis works that
bad samples can be virtually anything (that is not
good).
Learning helps. The recall was shown to im-
prove in various cases, while precision stayed at
the more or less same level as the simple baseline
rule. Though the threshold calculation using the
difference to good and bad examples seemed to
bear the potential of increasing precision.
Thresholds and ways how to calculate them are
evidently important. We proposed several well
working ways on how to construct thresholds from
evidence that extend the state of the art. Thresh-
olds usually vary with changing corpus sizes and
the measures proposed can adopt to that.
We plan to investigate the use of support vec-
47
tor machines in the latent semantic space in order
to gain more flexible means of characterising the
boundaries of the target area representing a con-
cept.
It should be mentioned that this experiment
demonstrates that conceptual development can be
measured and texts and their similarity can serve
as a proxy for that. Of course the experiment we
have conducted bears the danger to bring results
that are only stable within the topical area chosen.
We were able to demonstrate that textual rep-
resentations work on a granularity level of around
23 words, i.e. with the typical length of a free text
question in an exams.
While additionally using three model solutions
or at least two positive samples, we were able to
show that using a textbook split into paragraph-
sized textual units combined with generic back-
ground material, valid classifiers can be built with
relative ease. Furthermore, reference material to
score against can be collected along the way.
The most prominent open problem is to try and
completely get rid of model solutions as reference
material and to assess the lower level concepts
(terms and term aggregates) directly to further re-
duce corpus construction and reference material
collection. Using clustering techniques, this will
mean to identify useful ways for efficient visuali-
sation and analysis.
7 Acknowledgements
This work has been financially supported by the
European Union under the ICT programme of the
7th Framework Programme in the project ?Lan-
guage Technology for Lifelong Learning?.
References
Michael Berry, Susain Dumais, and Gavin O?Brien.
1994. Using linear algebra for intelligent informa-
tion retrieval. Technical Report CS-94-270, Depart-
ment of Computer Science, University of Tennessee.
Scott Deerwester, Susan Dumais, Georg W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal
of the American Society for Information Science,
41(6):391?407.
Peter Foltz, Darrell Laham, and Thomas K. Landauer.
1999. Automated essay scoring: Applications to ed-
ucational technology. In Collis and Oliver, editors,
Proceedings of World Conference on Educational
Multimedia, Hypermedia and Telecommunications
1999, pages 939?944, Chesapeake, VA. AACE.
Marco Kalz, Jan Van Bruggen, Ellen Rusmann, Bas
Giesbers, and Rob Koper. 2007. Positioning of
learners in learning networks with content analysis,
metadata and ontologies. Interactive Learning En-
vironments, (2):191?200.
Mark K. Smith. 1996. Competence and competency.
http://www.infed.org/biblio/b-comp.htm.
Jan van Bruggen, Peter Sloep, Peter van Rosmalen,
Francis Brouns, Hubert Vogten, Rob Koper, and
Colin Tattersall. 2004. Latent semantic analysis as
a tool for learner positioning in learning networks
for lifelong learning. British Journal of Educational
Technology, (6):729?738.
Jan Van Bruggen, Ellen Rusman, Bas Giesbers, and
Rob Koper. 2006. Content-based positioning in
learning networks. In Kinshuk, Koper, Kommers,
Kirschner, Sampson, and Didderen, editors, Pro-
ceedings of the 6th IEEE International Conference
on Advanced Learning Technologies, pages 366?
368, Kerkrade, The Netherlands.
Fridolin Wild and Christina Stahl. 2007. Investigating
unstructured texts with latent semantic analysis. In
Lenz and Decker, editors, Advances in Data Analy-
sis, pages 383?390, Berlin. Springer.
Fridolin Wild. 2008. lsa: Latent semantic analysis. r
package version 0.61.
48
Indexing Student Essays Paragraphs using LSA over an Integrated Ontological 
Space  
Gaston G.  Burek  Maria Vargas-Vera 
 
Emanuela Moreale 
Knowledge Media Institute,  
The Open University   
Milton Keynes , UK, MK7 6AA 
{g.g.burek,m.vargas-vera,e.moreale}@open.ac.uk
 
 
Abstract 
A full understanding of text is out of reach of 
current human language technology. However, a 
shallow Natural Language Processing (NLP) 
approach can be used to provide automated help in 
the evaluation of essays. The main idea of this 
paper is that Latent Semantic Indexing (LSA) can 
be used in conjunction with ontologies and First 
order Logic (FOL) to locate segments relevant to a 
question in a student essay. Our test bed, in a first 
instance, is a set of ontologies such the AKT 
reference ontology (describing academic life), 
Newspaper and a Koala ontology (concerning 
koalas? habitat). 
1 Introduction 
This paper describes a novel methodology 
aiming to support evaluators during the essay 
marking process. This approach allows measuring 
semantic similarity between structured (i.e. 
ontology and binary relations derived from the 
essay question) and unstructured (i.e. text 
processed as a bag of words) information by means 
of Latent Semantic Analysis (LSA) and the cosine 
similarity measure (Deewerster et al, 1990). 
  
Previous studies (Foltz et al, 1998; Wiemer-
Hastings and Graesser, 1999) have used LSA to 
measure text coherence and comprehension by 
comparing units of text (i.e. sentences, terms or 
paragraphs) to determine how semantically related 
they are. The work presented in this paper  is based 
on the use of ?pseudo? documents: these are 
temporary documents containing a description of  
knowledge entities extracted from available 
domain ontologies (i.e. ontological relations). Both 
pseudo documents and paragraphs in student 
essays are represented as vectors. Essay paragraphs 
are indexed according to a measure of semantic 
similarity (called cosine similarity). The 
ontological space acts as a mediated schema, a set 
of virtual relations among knowledge entities 
related by their degree of similarity. A new 
knowledge entity can be added in this space and 
automatically a similarity measure is calculated for 
all the entities within the space.  
1.1 Motivation and Context 
The main motivation for this work derives from 
a need for semantics in essay evaluation, whether 
by a tutor or by the student author in the process of 
writing. Page (Page, 1968) makes a useful 
distinction between marking for syntax (i.e. 
linguistic style) and for content (subject matter) 
which we will use in our outline. Based on this 
distinction, four main approaches to essay 
assessment have been reported (Williams, 2001). 
Early systems such as PEG (Page, 1966) relied 
mainly on syntactic and linguistic features and 
required a sample of the essays to be marked by a 
number of human judges. E-rater (Burstein et al, 
1998) uses a combination of statistical and natural 
language processing techniques for the purpose of 
extracting linguistic features of the essays to be 
graded. Again, the essays are evaluated against a 
set of human?graded essays acting as a benchmark. 
In the LSA method of essay grading, an LSA space 
is constructed based on domain specific material 
and the student essays. LSA grading performance 
is about as reliable as human graders (Foltz, 1996). 
Text categorisation (Larkey, 1998) also requires a 
database of graded essays, so that new essays can 
be categorised in relation to them.  
 
In short, the approaches seen so far 1 have either 
concentrated on syntactic and linguistic features or 
used domain knowledge in the form of keywords 
and documents about the domain. What we are 
proposing in this paper is that a further distinction 
should be made between using implicit (keywords, 
documents) and explicit content representations 
(see Fig.1, our contribution is marked in bold). We 
                                                     
1 Kukich presents in her article Beyond Automated 
Essay Scoring  a time line of research developments in 
the field of writing evaluation (Kukich, 2000). 
 
then argue the case for adding explicit domain 
knowledge in the form of domain ontologies. In 
particular, we merge ontologies, LSA and FOL. An 
advantage of this approach is that it does not 
require a corpus of graded essays, except for 
validation. This feature enables tutors (or students 
in need of feedback) to evaluate essays on 
particular topics even when there are no pre-scored 
essay examples available. Effectively, this 
capability may reduce the overall time required to 
prepare a reliable evaluation scheme for a new 
essay question.   
 
Figure 1 - Grading Criteria for Student Essays 
2 LSA and the Cosine Similarity 
In the vector space model, a term-to-document 
matrix is built in which the entries are weighted 
frequencies of pre-processed terms occurring in a 
collection of documents. Dimension reduction 
methods (such as LSA), when applied to the 
semantic vector space model, improve information 
retrieval, information filtering and word sense 
disambiguation. The reduction in dimensions 
reduces the noise in text categorisation, reduces the 
computational complexity of cluster creation, and 
produces the best statistical approximation to the 
original vector space model. Likelihood curves 
characterise with a quantity the level of 
significance of the reduced model dimensions. 
Also, the significance of each dimension follows a 
Zipf distribution (Li, 1992) indicating that the 
reduced model dimensions represent latent 
concepts (Ding, 1999). The dimensions in the 
reduced vector space model can be compared 
measuring semantic similarity between each of 
them by means of the cosine similarity. The cosine 
of the angle between two vectors is defined as the 
inner product between the vectors v and w divided 
by the product of the length of the two vectors. 
 
||||.||||
.
wv
wvCos =?  
3 Indexing Essays Paragraphs 
An index of relations within the ontologies 
related to the semantic space is obtained for each 
binary relation derived from the essay question. 
Then a subset containing the higher ranked 
relations is selected and the similarity between 
each of the relations in the subset and all the 
documents containing essays paragraphs is also 
calculated by applying LSA. Finally, an average 
similarity value is obtained for the paragraph over 
the number of relations in the subset. 
3.1 An Ontology Integration Method to Build 
the Semantic Space 
A collection of ?pseudo? documents is created 
for each of the classes within the ontologies 
describing the domains tackled in the essay. The 
ontologies are described quantitatively using 
probabilistic knowledge (Florescu et al, 1997). 
 
Each of these documents contains information 
(name, properties and relations) about a class. The 
documents are represented by a vector space model 
(Salton et al, 1971) where each column in the 
term-to-document matrix represents the ontological 
classes and the rows represent terms occurring in 
the pseudo documents describing those knowledge 
entities. 
 
Relations within the available ontologies are also 
represented by a vector space model where the 
columns in the term?to?document matrix are a 
combination of two or more vectors from the term?
to?document matrix representing classes. Each 
column represents the relation held between the 
combined classes. A new column representing the 
binary relation derived from the essay question is 
added to the new matrix: this new column contains 
the weighted frequencies of terms appearing as 
arguments within the relation. For each essay 
question, one or more binary relations are derived 
through parsing. For instance: given the query ?Do 
koalas live in the jungle?? the binary relation is 
live_in (koala, jungle). In the case of this example, 
the vector representing the question contains a 
frequency of one in the rows corresponding to the 
terms koala and jungle.  
 
LSA is applied to the term?to?document matrix 
representing the ontological relations, the vector 
space model is reduced and the cosine similarity is 
calculated to obtain the semantic similarity 
between the vectors of the reduced space model. 
For each column, a ranking of similarity with the 
rest of the columns will be obtained.  
 
 
 
  Researcher   (APO) 
StandartAd 
(NO) 
Salesperson
(NO) 
Student 
(KO) 
Parent 
(KO) 
Koala 
(KO) 
Newspaper 
(APO) 
Newspaper
(NO) 
Researcher (APO) 1.0000 0.5160 0.5215 0.8811 0.8524 0.8536 0.5036 0.5905 
Standart Ad. (NO) 0.5160 1.0000 0.9999 0.0496 -0.0078 -0.0057 0.9998 0.9959 
Salesperson (NO) 0.5215 0.9999 1.0000 0.0561 -0.0013 0.0007 0.9997 0.9965 
Student (KO) 0.8811 0.0496 0.0562 1.0000 0.9983 0.9984 0.0353 0.1388 
Parent (KO) 0.8524 -0.0078 -0.0014 0.9983 1.0000 0.9999 -0.0222 0.0815 
Koala (KO) 0.8536 -0.0057 0.0008 0.9985 0.9999 1.0000 -0.0201 0.0837 
Newspaper (APO) 0.5036 0.9998 0.9997 0.0353 -0.0222 -0.0201 1.0000 0.9945 
Newspaper (NO) 0.5905 0.9959 0.9965 0.1388 0.0815 0.0837 0.9945 1.0000 
Table 1 ? Cosine similarity for classes belonging to different ontologies.  
 
3.1.1 Weighting Scheme 
Given the term?to?document matrix containing 
a frequency f ij ,the occurrence of a term in all the 
pseudo documents j is weighted  to obtain matrix. 
The entries of the matrix are defined as,  
 
ij ij ij ja l g d= ,
 
where,  lij is the local weight for term i in the 
pseudo document j, gj is the global weight for term 
i in the collection  and dij  is a normalisation factor. 
Then, as defined by Guo and Berry (Guo and 
Berry, 2003), 
 
( ) ( )( )
2
2
2
log
log 1 1
log
ij ij
j
ij ij
p p
a f
n
??= + +???
? ?????
, 
where, 
 
ij
ij
ij
j
f
p
f
= ? . 
4 Experiments on semantic similarity  
In order to evaluate how well LSA captures 
similarity, this section will describe three 
preliminary experiments for measuring semantic 
similarity between knowledge entities (i.e. binary 
relations and classes) of three different ontologies, 
the Aktive Portal Ontology (APO), the Koala 
Ontology (KO) and the Newspaper Ontology 
(NO). 
4.1 Experiment 1 
The aim of this experiment is to evaluate how 
well LSA captures similarity between classes that 
belong to different ontologies. Eight classes have 
been selected randomly from within three 
ontologies and described in ?pseudo? documents. 
The words included in each of the documents 
correspond to the names of the classes and slots 
related to the class described. The terms have been 
stemmed and stop words deleted before applying 
LSA to the term?to?document matrix built using 
the weighted frequencies of the term occurring 
within the eight documents describing the classes. 
Terms have been weighted according to the 
weighting scheme presented in section 2.1.1 with 
dj=1, the only difference being that terms 
corresponding to classes names have been 
multiplied by two. The similarity measures for the 
eight classes are obtained (See Table 1) after 
applying LSA with a rank of two and the cosine 
similarity measure to the term?to?document 
matrix.  
 
The results from this experiment show that, in 
terms of the cosine similarity measure, the class 
?Researcher? appears to be very similar to the class 
?Student? in a different ontology. The same results 
also show that the two classes ?Newspaper? 
belonging to two different ontologies are very 
similar to each other.  
4.2 Experiment 2 
The aim of this experiment is to evaluate the 
ability of LSA to measure similarity between a 
predicate argument and different classes. The 
query is represented as an added column in the 
term?to?document matrix which already contains 
as columns the same documents representing the 
eight classes used in the first experiment. The 
column representing the query argument contains 
only one term corresponding to the name of one of 
the classes within the ontologies used in this 
experiment. The frequency of this term is the entry 
in the added column with a frequency of one 
multiplied by two as all the other terms 
representing names of classes.  The results for the 
cosine similarity measure between the eight classes 
plus the query containing the term ?student?, 
?newspaper? and ?animal? after applying LSA  
with a rank of four (see Table 2) indicate that the 
most similar classes for the query containing the 
 
term ?student? are the following classes:  
?Student? from KO, ?Researcher? from APO, and 
?Parent? from KO.   
 
 
Argument 
(student) 
Argument 
(animal) 
Argument 
(newspaper) Classes 
0.0018 0.0000 0.0099 Researcher (APO) 
0.0000 0.0000 0.1403 Standart  Ad. (NO)
-0.0001 0.0000 -0.0042 Salesperson (NO) 
0.4473 0.0000 -0.0080 Student (KO) 
-0.0013 0.5563 -0.0084 Parent (KO) 
-0.0006 0.4374 -0.0085 Koala (KO) 
0.0000 0.0000 0.5127 Newspaper (APO) 
-0.0001 0.0000 0.8112 Newspaper (NO) 
1.0000 1.0000 1.0000 Queries 
Table 2 ? Semantic similarity between arguments 
and classes belonging to different ontologies 
 
For the query containing the term ?newspaper? 
the results shows that the most similar classes are 
?Newspaper? from APO, ?Newspaper? from NO 
and ?Standard Advertising? also from NO.  
Finally, for the query containing the term 
?animal?, the most similar classes in order of 
similarity closeness are ?Parent? from KO and 
?Koala? also from KO.  
 
 The results of this experiment indicate that LSI 
may be accurately used as a measure of similarity 
between a keyword representing a query predicate 
argument and a set of documents representing 
classes that belong to a set of different available 
ontologies.   
4.3 Experiment 3 
The aim of this experiment is to evaluate the 
cosine similarity measure as a measure of semantic 
similarity between binary relations derived from a 
question or query and relations held between two 
classes. This measure is based on the same 
methodology and procedures applied to both 
experiments described above. For this experiment, 
eighteen classes have been selected arbitrarily from 
the three available ontologies (see Table 3).   
 
The binary relations held among the selected 
classes are represented as documents in a term?to? 
document matrix that is the union of the two    
pseudo documents describing the related classes. 
Following the same procedure as in the previous 
experiment, a new column representing the binary 
relation derived from a question is added to the 
matrix, but in this case it contains the terms 
describing the two arguments of the binary 
relation.  
 
 
Newspapers Ontology (NO) 
ID Relation Relation name Class1 Class2 
OBR1 Sales Person Advertisement Salesperson 
OBR2 Purchaser Advertisement Person 
OBR3 Published in Content Newspaper 
OBR4 Content Newspaper Content 
OBR5 Employees Organisation Employee 
OBR6 Prototype Newspaper Prot. Newspaper 
 
Aktive Portal Ontology (APO) 
ID Relation Relation name Class1 Class2 
OBR7 Has gender Researcher Gender 
OBR8 Has appellation Researcher Appellation 
OBR9 Owned by Newspaper Legal Agent 
OBR10 Has Size Organisation Organisationsize
OBR11 Headed by Organisation Afiliated Person
OBR12 Organisation part of Organisation Organisation 
 
Koala Ontology (KO) 
ID  Relation Relation Name Class 1 Class 2 
OBR13 Has gender Animal Gender 
OBR14 Has habitat Animal Appellation 
OBR15 Has children Animal Animal 
Table 3 ? Ontological Binary Relations (OBR) 
used in Experiment 3    
 
The cosine similarity between fifteen predicates 
and the available relations after applying LSA with 
a rank of four (see Table 4) show that, in eight of 
the fifteen cases, the similarity value is higher for 
the relations held between classes than between 
predicate arguments. In the rest of the cases, the 
similarity values are very close for two or more 
relations including the one held between classes 
that are the same as the predicate arguments. 
Another interesting observation is that, Question 
Binary Relation 3 (QBR3) has a cosine value more 
similar to Ontological Binary Relation 9 (OBR9), 
OBR3 and OBR4. In the case of QBR5, the cosine 
value is higher when measuring similarity with 
OBR11 and OBR12 than, for example, the cosine 
value when measuring similarity with  OBR3 and 
OBR4. Similar results were obtained for QBR6 
where, apart from OBR6, OBR9 has the cosine 
value closest to one. Other similar results are 
obtained for QBR11 and QBR12 where OBR5 is 
closer to a value of one than OBR7, OBR8 and 
OBR9. 
 
 
 
  
 QBR1 QBR2 QBR3 QBR4 QBR5 QBR6 QBR7 QBR8 QBR9 QBR10 QBR11 QBR12 QBR13 QBR14 QBR15
OBR1 0.3520 0.3033 0.1993 0.1993 0.2588 0.1713 -0.0007 0.0000 0.0487 -0.0030 0.0051 -0.0048 0.0000 0.0000 0.0000
OBR2 0.3628 0.3286 0.2170 0.2170 0.1896 0.1864 0.0006 0.0000 0.0528 -0.0033 0.0053 -0.0053 0.0000 0.0000 0.0000
OBR3 0.0900 0.0023 0.2864 0.2864 0.0002 0.2631 -0.0005 0.0000 0.0771 0.0017 0.0223 0.0027 0.0000 0.0000 0.0000
OBR4 0.0900 0.0023 0.2864 0.2864 0.0002 0.2631 -0.0005 0.0000 0.0771 0.0017 0.0223 0.0027 0.0000 0.0000 0.0000
OBR5 -0.0007 0.0039 -0.0013 -0.0013 0.3925 0.0000 0.0001 0.0000 -0.0006 0.0304 0.0566 0.0468 0.0000 0.0000 0.0000
OBR6 -0.0003 0.0024 0.2730 0.2730 0.0003 0.3284 0.0010 0.0000 0.0880 0.0011 0.0184 0.0017 0.0000 0.0000 0.0000
OBR7 0.0000 0.0032 -0.0004 -0.0004 0.0001 -0.0004 0.9572 0.3621 -0.0013 -0.0016 0.0143 -0.0012 0.0130 0.0130 0.0000
OBR8 0.0000 0.0032 -0.0004 -0.0004 0.0001 -0.0004 0.9567 0.3666 -0.0014 -0.0016 0.0143 -0.0012 0.0109 0.0109 0.0000
OBR9 0.0002 0.0002 0.2971 0.2971 -0.0029 0.2738 0.1477 -0.0028 0.9300 0.0147 0.0264 0.0082 -0.0002 -0.0002 0.0000
OBR10 -0.0002 0.0115 0.0014 0.0014 0.0633 0.0014 0.0999 0.0458 0.3012 0.4894 0.5181 0.3599 0.0190 0.0190 0.0000
OBR11 -0.0002 0.0113 0.0012 0.0012 0.0545 0.0012 0.1153 0.0454 0.2882 0.4304 0.4759 0.3161 0.0196 0.0196 0.0000
OBR12 -0.0002 0.0119 0.0015 0.0015 0.0522 0.0014 0.1019 0.0478 0.3146 0.4189 0.4623 0.3061 0.0221 0.0221 0.0000
OBR13 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.7312 0.0000 0.0000 0.0000 0.0000 0.0000 0.5397 0.5397 0.4910
OBR14 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.7490 0.0000 0.0000 0.0000 0.0000 0.0000 0.5202 0.5202 0.4767
OBR15 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.7550 0.0000 0.0000 0.0000 0.0001 0.0000 0.5594 0.5594 0.5261
QBR 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
Table 4 ?Cosine similarity between the Question Binary Relations (QBR) and the Ontological Binary 
Relations (OBR).  
 
The results of this experiment indicate that the 
presented methodology is able to detect similarity 
between compact representations (binary relation 
arguments) and more expanded representations 
such as the pseudo documents representing the 
binary relations within the three available 
ontologies.  
4.4 Experiments discussion  
We expect that using LSA together with the 
cosine similarity measure, we will be able to pick 
up semantic similarity between the compacted and 
expanded representations of the binary relation and 
paragraphs from student essays. The main 
difference between our approach and other essay 
scoring approaches (e.g. The Intelligent Essay 
Assessor; Laundauer et al, 2000) where the scores 
are calibrated using LSA with pre-scored essay 
examples, is that our approach scores paragraphs 
using LSA and the cosine similarity with 
ontologies describing the essay domain. The 
experiment results in the previous sections validate 
our view showing that the cosine similarity may be 
used as a reliable score for semantic similarity 
between knowledge entities belonging to different 
data sources (i.e. terms, classes and binary 
relations). 
5 Conclusion and Future Work  
This paper introduces the idea of ?explicit 
content? and its use in essay evaluation. The main 
contribution of the paper is then the idea that 
ontologies and First Order Logic (FOL) can be 
used together with LSA to locate segments 
relevant to a question in a student essay.   
 
Our main interest is to provide help to tutors in 
grading and to students for feedback purposes. In 
fact, even outside the realms of grading, we believe 
that it will help annotate and rank paragraphs more 
relevant to queries. In our proposal, we went about 
doing this supplementing the widely-used LSA 
method with added semantics (ontologies) and 
First Order Logic (FOL). Our approach therefore 
attempts to bridge the gap between statistical and 
semantic approaches. 
  
There is clearly a lot more work needed to make 
this technology work well enough for large-scale 
deployment. Further work may include a 
visualisation service that also provides a 
visualisation of annotation of segments relevant to 
the current question types around the lines of the 
work described in (Moreale and Vargas-Vera, 
2003; Moreale and Vargas-Vera, 2004).  
6 Acknowledgements 
This work was funded by the Advanced 
Knowledge Technologies (AKT) Interdisciplinary 
Research Collaboration (IRC), which is sponsored 
by the UK Engineering and Physical Sciences 
Research Council under grant number 
GR/N157764/01.   
 
 
References  
J. Burstein, K. Kukich, S. Wolff, C. Lu and M. 
Chodorow. 1998. Enriching Automated Essay 
Scoring Using Discourse Marking. Proceedings 
of the Workshop on Discourse Relations and 
Discourse Markers, Annual Meeting of the 
Association of Computational Linguistics, 
August, Montreal, Canada.
C. H. Q. Ding. 1999. A Similarity-Based 
Probability Model for Latent Semantic Indexing. 
Proc. 22nd ACM SIGIR Conference, p. 59?65. 
S.C. Deerwester, S. T. Dumais, T. K. Landauer, G. 
W. Furnas, R.A. Harshman. 1990. Indexing by 
Latent Semantic Analysis. JASIS  41(6): 391?
407. 
D. Florescu, D. Koller, A. Levy. 1997. Using 
Probabilistic Information in Data Integration. 
Proceedings of the 23rd VLDB Conference, 
Athens, Greece. 
P.W. Foltz,W. Kintsch, and T.K. Landauer. 1998. 
The Measurement of Textual Coherence with 
Latent Semantic Analysis, Discourse Processes, 
Vol. 25, Nos. 2?3, 1998, p. 285?308. 
P.W. Foltz. 1996. Latent semantic analysis for 
text-based research. Behavior Research Methods, 
Instruments and Computers, 28, 197-202.  
D. Guo and M. W. Berry. Knowledge ?Enhanced 
Latent Semantic Indexing. Information Retrieval, 
6 (2): 225-250, 2003. 
K. Kukich. 2000. The Debate on automated essay 
grading, Beyond Automated Essay Scoring. 
IEEE Transactions on Intelligent Systems. 
September/October 15 (5):27?31.
L.S. Larkey. 1998. Automatic Essay Grading 
Using Text Categorization Techniques. 
Proceedings of the Twenty First Annual 
International ACM SIGIR Conference on 
Research and Development in Information 
Retrieval, Melbourne, Australia, p. 90?95.  
T. K. Laundauer , D. Laham and P.W.Foltz. 2000. 
The Debate on automated essay grading, The 
Intelligent Essay Assessor. IEEE Transactions 
on Intelligent Systems. September/October 15 
(5):27?31.
W.  Li. 1992. Random texts exhibit Zipf's-law-like 
word frequency distribution. IEEE Transactions 
on Information Theory, 38(6):1842?1845. 
E. Moreale and M. Vargas-Vera. 2004. A 
Question-Answering System Using 
Argumentation. Third International Mexican 
Conference on Artificial Intelligence (MICAI-
2004), Lecture Notes in Computer Science 
(LNCS 2972), Springer-Verlag, p. 26?30. ISBN 
3-540-21459-3.  
E. Moreale and M. Vargas-Vera. 2003. Genre 
Analysis and the Automated Extraction of 
Arguments from Student Essays.  The Seventh 
International Computer Assisted Assessment 
Conference (CAA-2003). Loughborough 
University, 8-9. 
E.B. Page. 1968. Analyzing Student Essays by 
Computer. International Review of Education, 
14, 210?225. 
E.B. Page. 1966. The Imminence of Grading 
Essays by Computer. Phi Delta Kappan, p. 238?
243. 
G. Salton, A. Wong, and C. Yang. 1971. A Vector 
Space Model for Automatic Indexing. 
Communications of the ACM, 18(11):613?620, 
1971. 
P.  Wiemer-Hastings and A.C Graesser. 2000. 
Select-a-Kibitzer: A Computer Tool that Gives 
Meaningful Feedback on Student Compositions. 
Interactive Learning Environments 2000.Vol.8, 
No.2, p. 49?169. Curtin University of 
Technology 
R. Williams. 2001. Automated essay grading: An 
evaluation of four conceptual models. In A. 
Herrmann and M. M. Kulski (Eds), Expanding 
Horizons in Teaching and Learning. Proceedings 
of the 10th Annual Teaching Learning Forum, 7-
9 February 2001. Perth: Curtin University of 
Technology. 
 
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 113?118,
Prague, June 2007. c?2007 Association for Computational Linguistics
SVO triple based Latent Semantic Analysis for recognising textual
entailment
Gaston Burek Christian Pietsch
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, MK7 6AA, UK
{g.g.burek,c.pietsch,a.deroeck}@open.ac.uk
Anne De Roeck
Abstract
Latent Semantic Analysis has only recently
been applied to textual entailment recogni-
tion. However, these efforts have suffered
from inadequate bag of words vector repre-
sentations. Our prototype implementation
for the Third Recognising Textual Entail-
ment Challenge (RTE-3) improves the ap-
proach by applying it to vector represen-
tations that contain semi-structured repre-
sentations of words. It uses variable size
n-grams of word stems to model indepen-
dently verbs, subjects and objects displayed
in textual statements. The system perfor-
mance shows positive results and provides
insights about how to improve them further.
1 Introduction
The Third Recognising Textual Entailment Chal-
lenge (RTE-3) task consists in developing a system
for automatically determining whether or not a hy-
pothesis (H) can be inferred from a text (T), which
could be up to a paragraph long.
Our entry to the RTE-3 challenge is a system
that takes advantage of Latent Semantic Analysis
(LSA) (Landauer and Dumais, 1997). This numer-
ical method for reducing noise generated by word
choices within texts is extensively used for docu-
ment indexing and word sense disambiguation. Re-
cently, there have also been efforts to use techniques
from LSA to recognise textual entailment (Clarke,
2006; de Marneffe et al, 2006). However, we argue
that these efforts (like most LSA approaches in the
past) suffer from an inadequate vector representation
for textual contexts as bags of words. In contrast,
we have applied LSA to vector representations of
semi-structured text. Our representation takes into
account the grammatical role (i.e. subject, verb or
object) a word occurs in.
Within this system report, we describe and dis-
cuss our methodology in section 2, our current im-
plementation in section 3, and system results in sec-
tion 4. We conclude in section 5 with a discussion
of the results obtained and with the presentation of
possible steps to improve our system?s performance.
2 Methodology for detecting Textual
Entailment
2.1 Textual entailment formalisation
Our approach addresses the problem of the semantic
gap that exists between low level linguistic entities
(words) and concepts. Concepts can be described
by means of predicate-argument structures or by a
set of alternative natural language realisations. In
this work we use terminology co-occurrence infor-
mation to identify when different spans of text have
common semantic content even if they do not share
vocabulary. To achieve this, we use variable size n-
grams to independently model subject, verb and ob-
ject, and capture semantics derived from grammati-
cal structure. In order to detect textual entailment we
measure the semantic similarity between n-grams in
each T?H pair.
2.2 Using n-grams to align SVOs
To align subjects, verbs and objects within H and T,
we build the set of all n-grams for T, and do the same
113
for H. Section 3.5 describes this process in more de-
tail.
2.3 Deriving word senses with Latent Semantic
Analysis
Our approach is based on the assumption that a
word sense can be derived from the textual contexts
in which that words occurs. This assumption was
formalised in the Distributional Hypothesis (Harris,
1954).
We implemented a vector space model (Salton et
al., 1975) to capture word semantics from linguis-
tic (i.e. grammatical role) and contextual (i.e. fre-
quency) information about each word. To avoid high
matrix sparsity our vector space model uses second
order co-occurrence (Widdows, 2004, p. 174).
We assumed that the corpus we generated the vec-
tor space model from has a probabilistic word distri-
bution that is characterised by a number of seman-
tic dimensions. The LSA literature seems to agree
that optimal number of dimensions is somewhere
between two hundred and one thousand depending
on corpus and domain. As specified by LSA we ap-
plied Singular Value Decomposition (SVD) (Berry,
1992) to identify the characteristic semantic dimen-
sions.
The resulting model is a lower dimensional pro-
jection of the original model that captures indi-
rect associations between the vectors in the original
model. SVD reduces the noise in word categori-
sations by producing the best approximation to the
original vector space model.
3 Implementation
3.1 Development data set
The development data set consists of eight hundred
T?H pairs, half of them positive. By positive pair
we mean a T?H pair in which T entails H. All other
pairs we call negative. Each T?H pair belongs to a
particular sub-task. Those sub-tasks are Information
Extraction (IE), Information Retrieval (IR), Ques-
tion Answering (QA) and Summarisation (SUM). In
the current prototype, we ignored annotations about
sub-tasks.
3.2 Corpus analysis
3.2.1 Corpora used
The only knowledge source we used in our imple-
mentation was a parsed newswire corpus (Reuters
News Corpus) (Lewis et al, 2004). To derive con-
textual information about the meaning of words con-
stituting the SVOs, we analysed the Reuters corpus
as explained below.
3.2.2 SVO triple extraction
For parsing the corpus, we used Minipar1 because
of its speed and because its simple dependency triple
output format (-t option) contains word stems and
the grammatical relations between them. A simple
AWK script was used to convert the parse results into
Prolog facts, one file for each sentence. A straight-
forward Prolog program then identified SVOs in
each of these fact files, appending them to one big
structured text file.
Our algorithm currently recognises intransitive,
transitive, ditransitive, and predicative clauses. In-
transitive clauses are encoded as SVOs with an
empty object slot. Transitive clauses result in a fully
instantiated SVOs. Ditransitive clauses are encoded
as two different SVOs: the first containing subject,
verb and direct object; the second triple containing
the subject again, an empty verb slot, and the indi-
rect object. Predicatives (e.g. ?somebody is some-
thing?) are encoded just like transitive clauses.
In this first prototype, we only used one word
(which could be a multi-word expression) for sub-
ject, verb and object slot respectively. We realise
that this approach ignores much information, but
given a large corpus, it might not be detrimental to
be selective.
3.2.3 SVO Stemming and labeling
To reduce the dimensionality of our vector space
model we stem the SVOs using Snowball2. Then,
we calculate how many times stems co-occur as sub-
ject, verb or object with another stem within the
same SVO instance.
1Minipar can be downloaded from http://www.cs.ualberta.
ca/?lindek/minipar.htm. It is based on Principar, which is de-
scribed in Lin (1994).
2Snowball is freely available from http://snowball.tartarus.
org/. The English version is based on the original Porter Stem-
mer (Porter, 1980).
114
To keep track of the grammatical role (i.e. subject,
verb, object) of the words we stem them and label
the stems with the corresponding role.
3.3 Building vector spaces to represent stem
semantics
From the corpus, we built a model (S,V,O) of the
English (news) language consisting of three matri-
ces: S for subjects, V for verbs, and O for objects.
We built the three stem-to-stem matrices from
labeled stem co-occurrences within the extracted
triples. The entries to the matrices are the frequen-
cies of the co-occurrence of each labeled stem with
itself or with another labeled stem. In our current
prototype, due to technical restrictions explained in
Section 3.4, each matrix has 1000 rows and 5000
columns.
Columns of matrix S contain entries for stems la-
beled as subject, columns of matrix V contain en-
tries for stems labeled as verb, and columns of ma-
trix O contain entries for stems labeled as object.
The frequency entries of each matrix correspond to
the set of identically labeled stems with the highest
frequency.
Rows of the three matrices contain entries corre-
sponding to the same set of labeled stems. Those la-
beled stems are the ones with the highest frequency
in the set of all labeled stems. Of these, 347 stems
are labeled as subject, 356 are labeled as verb, and
297 are labeled as object. Each row entry is the fre-
quency of co-occurrence of two labeled stems within
the same triple.
Finally, each column entry is divided by the num-
ber of times the labeled stem associated with that
column occurs within all triples.
3.4 Calculating the singular value
decomposition
We calculated the Singular Value Decompositions
(SVDs) for S, V and O. Each SVD of a matrix A is
defined as a product of three matrices:
A = U ? S ? V ? (1)
SVD is a standard matrix operation which is sup-
ported by many programming libraries and com-
puter algebra applications. The problem is that only
very few can handle the large matrices required for
real-world LSA. It is easy to see that the memory re-
quired for representing a full matrix of 64 bit float-
ing point values can easily exceed what current hard-
ware offers. Fortunately, our matrices are sparse, so
a library with sparse matrix support should be able
to cope. Unfortunately, these are hard to find out-
side the Fortran world. We failed to find any Java li-
brary that can perform SVD on sparse matrices.3 We
finally decided to use SVDLIBC, a modernised ver-
sion of SVDPACKC using only the LAS2 algorithm.
In pre-tests with a matrix derived from a different
text corpus (18371 rows ? 3469 columns, density
0.73%), it completed the SVD task within ten min-
utes on typical current hardware. However, when
we try to use it for this task on a matrix S of dimen-
sion 5000 ? 5000 (density 1.4%), SVDLIBC did not
terminate4. In theory, there is a Singular Value De-
composition for every given matrix, so we assume
this is an implementation flaw in either SVDLIBC or
GCC. With no time left to try Fortran alternatives,
we resorted to reducing the size of our three matri-
ces to 1000 ? 5000, thus losing much information
in our language model.
3.5 Looking for evidence of H in T using
variable size n-grams
3.5.1 Building variable size n-grams
OurMinipar triple extraction algorithm is not able
to handle SVOs that are embedded within other
SVOs (as e.g. in ?Our children play a new game that
involves three teams competing for a ball.?). There-
fore, in order to determine if SVOs displayed in H
are semantically similar to any of those displayed in
T, we generate all n-grams of all lengths for each T
and H: one set for subjects, one for verbs and another
one for objects.
Example: ?The boy played tennis.?
Derived n-grams: the; the boy; the boy played; the
boy played tennis; boy; boy played; boy played
3The popular JAMA library and the related Jampack library
have no sparse matrix support at all. MTJ and Colt do support
sparse matrices but cannot perform SVD on them without first
converting them to full matrices.
4We tried various hardware / operating system / compiler
combinations. On Linux systems, SVDLIBC would abort after
about 15 minutes with an error message ?imtqlb failed to con-
verge?. On Solaris and Mac OS X systems, the process would
not terminate within several days.
115
tennis; played; played tennis; tennis.
We use n-grams to model subjects, verbs and ob-
jects of SVOs within T and H.
3.5.2 How to compare n-grams
We generate three vector representations for each
n-gram. To do this, we add up columns from the
Reuters Corpus derived matrices. To build the first
vector representation, we use the S matrix, to build
the second vector we use the V matrix, and to build
the third vector we use the O matrix. Each of
the three representations is the result of adding the
columns corresponding to each stem within the n-
gram.
To calculate the semantic similarity between n-
grams, we fold the three vector representations of
each n-gram into one of the dimensionally reduced
matrices S200, V200 or O200. Vector representation
originating from the S matrix are folded into S200.
We proceed analogously for vector representations
originating from V200 and O200. We apply equation
2 to fold vectors from Gr where r ? {S,V,O}. G
is a matrix which consists of all the vector represen-
tations of all the n-grams modeling T or H. Sr200 and
U r200 are SVD results reduced to 200 dimensions.
Gr? ? U r200 ? (S
r
200)
?1 = Gr200 (2)
For each T?H pair we calculate the dot product
between the G matrices for H and T as expressed in
equation 3
textGr200 ?
hypothesis Gr?200 = O
r (3)
The resulting matrix Or contains the dot product
similarity between all pairs of n-grams within the
same set. Finally, for each T?H pair we obtain three
similarity values s, v, o by selecting the entry of Or
with the highest value.
3.5.3 Scoring
Now we have calculated almost everything we
need to venture a guess about textual entailment.
For each T?H pair, we have three scores s, v, o
for for subject, verb and object slot respectively. The
remaining task is to combine them in a meaningful
way in order to make a decision. This requires some
amount of training which in the current prototype
is as simple as computing six average values: s?p,
s? v? o?
positive 0.244 5.05 ? 10?7 0.323
negative 0.196 4.76 ? 10?7 0.277
Table 1: Values computed for s?p, v?p, o?p, s?n, v?n, o?n
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 0  100  200  300  400  500  600  700  800
d
o
t
 
p
r
o
d
u
c
t
 
s
i
m
i
l
a
r
i
t
y
n-grams
Figure 1: Subject similarities s = maxOS for all
H?T pairs
v?p, o?p are the average scores of subject, verb and
object slots over those T?H pairs for which textual
entailment is known to hold. Conversely, s?n, v?n, o?n
are the averages for those pairs that do not stand in
a textual entailment relation. The (rounded) values
were determined are shown in table 1.
Note that the average values for non-entailment
are always lower than the average values for entail-
ment, which indicates that our system indeed tends
to discriminate correctly between these cases.
The very low values for the verb similarities (fig-
ure 3) compared to subject similarities (figure 1) and
object similarities (figure 2) remind us that before
we can combine slot scores, they should be scaled
to a comparable level. This is achieved by divid-
ing each slot score by its corresponding average. Ig-
noring the difference between positive and negative
pairs for a moment, the basic idea of our scoring al-
gorithm is to use the following threshold:
s
s?
+
v
v?
+
o
o?
= 3 (4)
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8
 0  100  200  300  400  500  600  700  800
d
o
t
 
p
r
o
d
u
c
t
 
s
i
m
i
l
a
r
i
t
y
n-grams
Figure 2: Object similarities o = maxOO for all
H?T pairs
116
 0 2e-07
 4e-07 6e-07
 8e-07 1e-06
 1.2e-06 1.4e-06
 1.6e-06
 0  100  200  300  400  500  600  700  800
d
o
t
 
p
r
o
d
u
c
t
 
s
i
m
i
l
a
r
i
t
y
n-grams
Figure 3: Verb similarities v = maxOV for all H?T
pairs
At this point we observed that scaling the verb
similarities so much seemed to make results worse.
It seems to be necessary to introduce weights:
?
s
s?
+ ?
v
v?
+ ?
o
o?
= ? + ?+ ? (5)
Without loss of generality, we may assume ? = 1:
?
s
s?
+
v
v?
+ ?
o
o?
= ? + 1 + ? (6)
The complete scoring formula with both positive
and negative scores is shown below. We assumed
that the weights ? and ? are the same in the positive
and in the negative case, so ? = ?p = ?n and ? =
?p = ?n.
?
s
s?p
+
v
v?p
+?
o
o?p
+?
s
s?n
+
v
v?n
+?
o
o?n
= 2(?+1+?)
(7)
At this point, some machine learning over the de-
velopment data set should be performed in order to
determine optimal values for ? and ?. For lack of
time, we simply performed a dozen or so of test runs
and finally set ? = ? = 3.
Our entailment threshold is thus simplified:
3
s
s?p
+
v
v?p
+ 3
o
o?p
+ 3
s
s?n
+
v
v?n
+ 3
o
o?n
= q (8)
If q > 14, our prototype predicts textual entail-
ment. Otherwise, it predicts non-entailment.
4 Results
Using the scoring function described in section
3.5.3, our system achieved an overall accuracy of
0.5638 on the development dataset. Table 2 shows
results for the system run on the test dataset. On this
unseen dataset, the overall accuracy decreased only
all IE IR QA SUM
accuracy 0.5500 0.4950 0.5750 0.5550 0.5750
av. prec. 0.5514 0.4929 0.5108 0.5842 0.6104
Table 2: Results on the test set
slightly to 0.5500. We take this as a strong indica-
tion that the thresholds we derived from the develop-
ment dataset work well on other comparable input.
Results show that our system has performed signifi-
cantly above the 0.5 baseline that would result from
a random decision.
As shown in section 3.5.3, the values in the three
similarity plots (see figures 1, 2 and 3) obtained with
the development set seem to be scattered around the
means. Therefore it seems that the threshold values
used to the decide whether or not T entails H do not
fully reflect the semantics underlying textual entail-
ment.
The nature of the SVD calculations do not allow
us directly to observe the performance of the vari-
able size n-grams in independently aligning subject,
verb and objects from T and from H. Nevertheless
we can infer from figures 1, 2 and 3 that many of
the values shown seem to be repeated. These value
configurations can be observed in the three horizon-
tal lines. These lines better visible in figures 2 and
3 are the effect of (a) many empty vectors resulting
from the rather low number of stems represented by
columns in our Reuters-derived matrices S, V and
O, and (b) the effect of folding the n-gram vector
representations into reduced matrices with two hun-
dred dimensions.
5 Conclusion
Even though our system was developed from scratch
in a very short period of time, it has already out-
performed other LSA-based approaches to recognis-
ing textual entailment (Clarke, 2006), showing that
it is both feasible and desirable to move away from
a bag-of-words semantic representation to a semi-
structured (here, SVO) semantic representation even
when using LSA techniques.
Our system displays several shortcomings and
limitations owing to its immature implementation
state. These will be addressed in future work, and
we are confident that without changing its theoret-
ical basis, this will improve performance dramati-
117
cally. Envisaged changes include:
? using larger matrices as input to SVD
? using the complete Reuters corpus, and adding
Wikinews texts
? performing corpus look-up for unknown words
? extracting larger chunks from S and O slots
? using advanced data analysis and machine
learning techniques to improve our scoring
function
In addition, our approach currently does not take
into consideration the directionality of the entail-
ment relationship between the two text fragments. In
cases where T1 entails T2 but T2 does not entail T1,
our approach will treat (T1, T2) and (T2, T1) as the
same pair. We expect to correct this misrepresenta-
tion by evaluating the degree of specificity of words
composing the SVOs in asymmetric entailment rela-
tionships where the first text fragment is more gen-
eral than the second one. For that purpose, one can
use term frequencies as an indicator of specificity
(Spa?rck Jones, 1972).
Obviously, system performance could be further
improved by taking a hybrid approach as e.g. in de
Marneffe et al (2006), but we find it more instruc-
tive to take our pure LSA approach to its limits first.
6 Acknowledgements
We are grateful to Prof. Donia Scott, head of the Nat-
ural Language Generation group within the Centre
for Research in Computing of the Open University,
who made us aware of the RTE-3 Challenge and en-
couraged us to participate.
References
[Berry1992] M. W. Berry. 1992. Large-scale sparse sin-
gular value computations. The International Journal
of Supercomputer Applications, 6(1):13?49, Spring.
[Clarke2006] Daoud Clarke. 2006. Meaning as context
and subsequence analysis for entailment. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
[de Marneffe et al2006] Marie-Catherine de Marneffe,
Bill MacCartney, Trond Grenager, Daniel Cer, Anna
Rafferty, and Christopher D. Manning. 2006. Learn-
ing to distinguish valid textual entailments. In Pro-
ceedings of the Second PASCAL Challenges Workshop
on Recognising Textual Entailment, Venice, Italy.
[Harris1954] Zelig S. Harris. 1954. Distributional struc-
ture. WORD, 10:146?162. Reprinted in J. Fodor and J.
Katz, The structure of language: Readings in the phi-
losophy of language, pp. 33?49, Prentice-Hall, 1964.
[Landauer and Dumais1997] T. K. Landauer and S. T. Du-
mais. 1997. A solution to Plato?s Problem. The Latent
Semantic Analysis theory of the acquisition, induction
and representation of knowledge. Psychological Re-
view, 104(2):211?240.
[Lewis et al2004] D. D. Lewis, Y. Yang, T. Rose, and
F. Li. 2004. RCV1: A new benchmark collection
for text categorization research. Journal of Machine
Learning Research, 5:361?397.
[Lin1994] Dekang Lin. 1994. PRINCIPAR ? an effi-
cient, broad-coverage, principle-based parser. In Proc.
COLING-94, pages 42?488, Kyoto, Japan.
[Porter1980] M. F. Porter. 1980. An algorithm for suffix
stripping. Program, 14(3):130?137.
[Salton et al1975] G. Salton, A. Wong, and C. S. Yang.
1975. A vector space model for automatic indexing.
Commun. ACM, 18(11):613?620.
[Spa?rck Jones1972] Karen Spa?rck Jones. 1972. A statis-
tical interpretation of term specificity and its applica-
tion in retrieval. Journal of Documentation, 28(1):11?
21. Reprinted 2004 in 60(5):493?502 and in Spa?rck
Jones (1988).
[Spa?rck Jones1988] Karen Spa?rck Jones. 1988. A statis-
tical interpretation of term specificity and its applica-
tion in retrieval. Document retrieval systems, pages
132?142.
[Widdows2004] DominicWiddows. 2004. Geometry and
Meaning. Number 172 in CSLI Lecture Notes. Uni-
versity of Chicago Press.
118
