Proceedings of the Workshop on Embodied Language Processing, pages 41?50,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
Coordination in Conversation and Rapport 
Justine Cassell, Alastair J. Gill and Paul A. Tepper 
Center for Technology & Social Behavior 
Northwestern University 
2240 Campus Drive, Evanston, IL 60208 
{justine, alastair, ptepper}@northwestern.edu 
 
Abstract 
We investigate the role of increasing 
friendship in dialogue, and propose a first 
step towards a computational model of the 
role of long-term relationships in language 
use between humans and embodied conver-
sational agents. Data came from a study of 
friends and strangers, who either could or 
could not see one another, and who were 
asked to give directions to one-another, 
three subsequent times. Analysis focused 
on differences in the use of dialogue acts 
and non-verbal behaviors, as well as co-
occurrences of dialogue acts, eye gaze and 
head nods, and found a pattern of verbal 
and nonverbal behavior that differentiates 
the dialogue of friends from that of strang-
ers, and differentiates early acquaintances 
from those who have worked together be-
fore. Based on these results, we present a 
model of deepening rapport which would 
enable an ECA to begin to model patterns 
of human relationships. 
1 Introduction 
 What characterizes the language of people who 
have known one another for a long time? In the US 
one thinks of groups of friends, leaning in towards 
one another, laughing, telling jokes at one an-
other?s expense, and interrupting one another in 
their eagerness to contribute to the conversation.  
The details may differ from culture to culture, but 
the fact of differences between groups of friends 
and groups of strangers are probably universal. 
Which characteristics, if any, reliably differentiates 
friends and strangers? Which can make a new 
friend feel welcome? An old friend feel appreci-
ated? Advances in natural language are ensuring 
that embodied conversational agents (ECAs) are 
increasingly scintillating, emotionally and socially 
expressive, and personality-rich. However, for the 
most part, those same ECAs demonstrate amnesia, 
beginning every conversation with a user as if it is 
their first, and never getting past the stage of intro-
ductory remarks. 
As the field of ECAs matures, and these systems 
are found on an increasing number of platforms, 
for an increasing number of applications, we feel 
that it is time to ensure that ECAs be able to en-
gage in deepening relationships that make their 
collaboration with humans productive and satisfy-
ing over long periods of time. To this end, in this 
paper we examine the verbal and nonverbal corre-
lates of friendship in an empirical study, and then 
take first steps towards a model of deepening 
friendship and rapport in ECAs. The current study 
is a part of a larger research program into linguistic 
and social coordination devices from the utterance 
level to the relationship level ? how they work in 
humans, how they can be modeled in virtual hu-
mans, and how virtual humans can be used to teach 
people who wish to learn these skills.  
2 Background & Theory 
As people become closer, their conversational 
style changes. They may raise more topics in the 
course of a conversation, refer more to themselves 
as a single unit than as two people, and be more 
responsive to one another?s talk (Cassell & Tver-
sky, 2005; Hornstein, 1982). They also are likely 
to sustain eye contact longer, smile more, and lean 
more towards one another (Grahe & Bernieri, 
1999; Richmond & McCroskey, 1995). In addition, 
friends appear to have fewer difficulties with lexi-
cal search, perhaps because they can rely on 
greater shared knowledge, and are more likely to 
talk at the same time, and to negotiate turn-taking 
in a less rigid manner, both through gaze and ges-
41
ture (Welji & Duncan, 2005). Tickle-Degnen & 
Rosenthal (1990) propose a model of deepening 
rapport over time based on the relationship among 
three components: positivity, mutual attention and 
coordination. As shown in Figure 1, as friendship 
deepens, the importance of positivity decreases, 
while the importance of coordination increases. 
Attention to the conversational partner, however, is 
hypothesized to remain constant. That is, strangers 
are more likely to be polite and uniformly positive 
in their talk, but also more likely to be awkward 
and badly coordinated with their interlocutors.   
As a relationship progresses and impressions 
have been formed and accepted, disagreement be-
comes acceptable and important. This may entail 
an increase in face-threatening issues and behav-
iors (cf. Brown & Levinson, 1987) accompanied 
by a decrease in the need to mediate these threats. 
At this stage in the relationship, coordination be-
comes highly important, so that the conversation 
will be less awkward and there is less likelihood of 
misunderstanding. Attention to one another, how-
ever, does not change. Tickle-Degnen & Rosenthal 
point out that these features are as likely to be ex-
pressed nonverbally (through smiles, nods, and 
posture shifts, for example) as verbally.  
One criticism of Tickle-Degnen & Rosenthal, 
and similar work, is that positive feelings for, and 
knowledge about, the other person are not distin-
guished (Cappella, 1990). That is, what might be 
perceived as lack of rapport could actually be a 
lack of familiarity with a partner?s behavioral cues 
for indicating misunderstanding or requesting in-
formation.  
This conflation may come from the fact that the 
word rapport is used both to refer to the phenome-
non of instant responsiveness (?we just clicked?) 
and that of deepening interdependence over time. 
ECA research has been divided between a focus on 
instant rapport (Gratch et al, 2006; Maatman, 
Gratch, & Marsella, 2005) and a focus on estab-
lishing and maintaining relationships over time 
(Bickmore & Picard, 2005; Cassell & Bickmore, 
2002; Stronks, Nijholt, van der Vet, & Heylen, 
2002). Perhaps due to difficulties with analyzing 
dyadic interdependent processes, and modeling 
them in computational systems, much of the work 
in both traditions still takes a signaling approach, 
whereby particular signals (such as nodding or 
small talk) demonstrate the responsiveness, extro-
version, or rapport-readiness of the agent, but are 
decontextualized from the actions of the dyad 
(Duncan, 1990). Although this approach is well 
paired to current technological constraints, it may 
not adequately account for the contingency of in-
terpersonal interaction and conversation. In addi-
tion, in none of these previous studies was there a 
focus on how verbal and nonverbal devices actu-
ally change over the course of a relationship, and 
how those devices are interdependent between 
speaker and listener. An instant rapport approach is 
useful for building systems that are initially attrac-
tive to users; but a system that signals increasing 
familiarity and intimacy through its linguistic and 
nonverbal behaviors may encourage users to stay 
with the system over a longer period of time. 
In the current work, we concentrate how dis-
course and nonverbal behavior changes over time, 
and across the dyad, as this perspective allows us 
to highlight the similarities between interpersonal 
coordination and knowledge coordination of the 
kind that has been studied in both conversational 
analysis and psycholinguistics.  
Work on conversational analysis demonstrates 
the importance of knowledge coordination compo-
nents such as turn-taking and adjacency pairs 
(e.g.Goodwin, 1981; Schegloff & Sacks, 1973). 
Inspired by this approach, work by Clark and col-
laborators on grounding and conversation as joint 
action has made demonstrated coordination and 
cooperation as defining characteristics of conversa-
tion (Clark, 1996; Clark & Brennan, 1991; Clark & 
Wilkes-Gibbs, 1986). This work has in turn, re-
ceived a significant amount of attention in compu-
tational linguistics, specifically in the study of dia-
logue (Matheson, Poesio, & Traum, 2000; Nakano, 
Reinstein, Stocky, & Cassell, 2003; Traum, 1994; 
Traum & Dillenbourg, 1998). To develop a model 
of nonverbal grounding, Nakano et al (2003) stud-
Early Late
Time of Interaction
Im
p
o
rt
a
n
ce
 f
o
r 
R
a
p
p
o
rt
Mutual Attention
Positivity
Coordination
Figure 1. Three component model of rapport 
(from Tickle-Degen & Rosenthal, 1990). 
42
ied people giving directions with respect to a map 
placed in between them. In that study, we observed 
that when a direction-receiver looked up from the 
map while the direction-giver was still giving di-
rections, the giver would initiate grounding behav-
ior such as a repeat or a rephrase.  
The literature reviewed above leads us to believe 
that there is an integral relationship between social 
and knowledge coordination. In this paper, we at-
tempt to draw conclusions about the changes in 
social and linguistic coordination over the short- 
and long-term in a way that illuminates that poten-
tial relationship, and that is also computationally 
viable. In order to do this, we replicate the task we 
used in our earlier grounding study (Nakano et al, 
2003); that is we use a direction-giving task, where 
half the subjects can see one another, and half are 
divided by a screen. Here, however, half of the 
subjects in each visibility condition are friends and 
half are strangers. And to study the potential de-
velopment of rapport across the experimental pe-
riod, each pair performs three subsequent direc-
tion-giving tasks.  
In the next section, we discuss the experimental 
procedure further. In section 4, we introduce first 
steps towards a new computational model of rap-
port that incorporates conversational coordination 
and grounding, based on our empirical findings. 
3 The Experiment 
3.1 Method 
Participants We collected eight task-based con-
versations (N = 16): in each dyad, one participant 
was accompanied by the experimenter and fol-
lowed a specific route from one place in the rococo 
university building where the experiment was run 
to another place in the building. S/he gave the 
other participant directions on how to reach that 
location, without the use of maps or other props. 
The direction-receiver (Receiver) was instructed to 
ask the direction-giver (Giver) as many questions 
as needed to understand the directions. After the 
conversation, the Receiver had to find the location. 
During recruitment the Giver was always selected 
as someone familiar with the building, while the 
Receiver was unfamiliar. All subjects were under-
graduate students, and were motivated by surprise 
gifts hidden at the target location. 
Design. We manipulated long-term rapport, visi-
bility, and subsequent route in a 2 ? 2 ? 3 design. 
We operationalized long-term rapport as a binary, 
between-subjects variable, with conditions Friends 
(self-reported as friends for at least one year) and 
Strangers. To study the effect of non-verbal behav-
ior, we manipulated visibility as a second between-
subject variable. To do this, half of the participants 
could see each other, and half were separated by a 
dividing panel. To study the effect of acquaintance 
across the experimental period, each dyad com-
pleted the task three consecutive times, going to 
three different locations. 
Data Coding All dyads were videotaped using a 
six-camera array, capturing the participants? body 
movements from the front, side, and above, along 
with close-up views of their faces. From each 
dyad, we made time-aligned transcriptions (using 
Praat). Non-verbal behavior was coded using An-
vil. From the transcripts, the following 9 DAMSL 
Dialogue Acts (Core & Allen, 1997) were coded: 
Acknowledgments, Answers, Assert, Completion, 
Influence, Information Request, Reassert, Repeat-
Rephrase, and Signal Non Understanding. Non-
verbal behavior in giver and receiver was coded 
using the following categories, based on Nakano, 
et al (2003): 
? Look at Speaker ? looking at the speaker?s 
eyes, eye region or face. 
? Look at Hearer ? looking at the hearer?s eyes, 
eye region or face. 
? Head nod [speaker or hearer] ? Head moves 
up and down in a single continuous movement 
on a vertical axis, but eyes do not go above 
the horizontal axis. 
3.2 Results 
We first provide basic statistics on the experimen-
tal manipulations and then examine the role of 
friendship and visibility on verbal and non-verbal 
behavior. 
Basic Statistics: Overall, we find that Friend dy-
ads use a significantly greater number of turns per 
minute than Strangers (t(6)= 2.45, p<.05, two tail), 
however, there is no difference in the mean num-
ber of seconds it took for dyads to complete the 
task. This lack of significance may have been due 
to variance among the dyads, since the mean 
length was 847 seconds for friends and 1049 for 
43
strangers. Given the instructional nature of the 
task, this means that Friends were more likely to 
intervene in the direction-giving than were Strang-
ers, even though ? for most of the dyads ? friends 
appear to take less time to finish. No difference 
was found in turns per minute for Visible and Non-
visible dyads; nor is there a difference in length in 
seconds. For routes, there is no difference in turns 
per minute, however for the length of the route in 
seconds there is a difference (F(2,21)=10.66; 
p<.006) such that the mean length of Route 1 is 
165 seconds; Route 2 is 395 seconds; Route 3 is 
387. For this reason, all statistics below are nor-
malized as a function of the length of that dyad?s 
data in seconds, and graphs are plotted to show 
least squares mean. 
Verbal and Nonverbal behavior: We examine 
the relationship between friendship and visibility 
of both Giver and Receiver across the three route 
tasks. Each of the DAMSL dialogue act variables 
and Non-verbal behavior variables was entered as 
the dependent variable in building mixed method  
models using the JMP statistical package (Version 
6, SAS Institute Inc., Cary, NC, 1989-2005); 
Speaker (direction-giver or receiver), Visibility, 
Friendship and Route were entered as predictor 
variables; experimental dialogue number was also 
entered as a source of random variance. We report 
the results in Tables 1 and 2 (for DAMSL and 
Non-verbal behavior variables respectively). 
Verbal Behavior: In terms of overall variance ex-
plained, we find that Acknowledgments is best 
accounted for by the model (Adjusted R Square of 
0.91), whilst Completion is least well accounted 
for (Adjusted R2 of 0.06).  
Turning first to main effects, for Visibility, 
Visible-Givers use Acknowledgements, Assert, 
Influence, and Reassert dialogue acts more fre-
quently than Non-visible Givers (post-hoc t tests at 
p <.05) 
Visible-Receivers use Acknowledgement, Re-
peat-rephrase, Signal Non Understanding these 
features more frequently than Non-visible Receiv-
ers. (post-hoc t tests at p <.05) 
For Friendship, no differences were found for 
production of DAMSL acts by givers. For receiv-
ers, receiver-strangers use more acknowledge-
ments than receiver friends.  
0
5
10
15
20
25
30
Nonvisible Visible
L
e
a
st
 S
q
 M
e
a
n
s 
fo
r 
N
o
rm
A
ck
n
o
w
le
d
g
m
e
n
t
Friends Strangers  
Figure 2: Giver Acknowledgment by condition 
 
0
2
4
6
8
10
12
14
16
Nonvisible Visible
L
e
a
st
 S
q
 M
e
a
n
s 
fo
r 
N
o
rm
A
ck
n
o
w
le
d
g
e
m
e
n
t
Friends Strangers  
Figure 3: Receiver Acknowledgment by condition 
 
These main effects are mediated by an interac-
tion between Visibility?Friendship for Acknowl-
edgements. Here, as shown in Figure 2 and 3 we 
see that in the nonvisibility condition, there is no 
difference in the use of acknowledgements per 
second between friends and strangers; on the other 
hand, strangers use more acknowledgements in the 
visible condition (p<.05). A very similar interac-
tion was found for Signal Non Understanding (at 
the trend level of p<.08). 
Route is only a main effect predictor of Signal 
Non Understanding as used by receivers, who pro-
duce it significantly more frequently during the 
third route task than the first. Since signaling one?s 
lack of understanding is potentially face-
DV  Source DF DF Total F Ratio 
ACK  V*F 1 20 10.64** 
COMP  V*F 1 4 9.78* 
 V*F 1 20 3.31? SNU 
 Rte 2 20 3.38* 
Note: ?p<0.08; *p < 0.05; **p < 0.01;  
Table 1: Verbal behavior 
 
Abbreviation: ACK=Acknowledgment; COMP=Completion; 
SNU=Signal Non Understanding; Sources abbreviated as: F 
= Friendship; V = Visibility; Rte = Route 
44
threatening, this result may indicate that both 
friends and strangers become more comfortable 
with one another by the third route. 
Nonverbal Behavior: Variance explained by the 
non-verbal models is the greatest for Look At 
Speaker (Adjusted R2 0.83) and least for Speaker 
Nod (0.38). With respect to the main effects result-
ing from the analysis of the non-verbal behaviors, 
we find the following.  
Visibility: Givers nod more in the visible condi-
tion when the receiver is speaking than they do in 
the Non-visible condition.  
Route: For both givers and receivers, there is an 
increase in use of Look At Speaker and Look At 
Hearer, over time; in both cases significantly 
greater instances of these variables occurred during 
Route task 2 and 3, compared to Route 1. Once 
again, these results may indicate increasing coor-
dination in conversational behavior for both 
Friends and Strangers. 
In fact, in the case of head nods, we note an in-
teresting pattern of coordination between speaker 
and hearer head-nods across the routes that differs 
for friends and strangers. For friends, both Re-
ceiver and Giver head nods in response to Receiver 
talk reduce in frequency between the first and sec-
ond routes (Giver t(8)=-2.36; p<0.05; Receiver 
t(8)=-2.28; p<0.05). For strangers, no such ac-
commodation over time occurs. Conversely, for 
friends when the Giver is speaking, both giver and 
receiver head nods increase over the three routes 
(significant only for Receiver t(8)=2.38; p<0.05). 
For strangers, however, head nods decrease (Giver 
t(8)=-2.80; p<0.05, Receiver t(8)=3.92; p<0.01). 
This means that speaker and hearer are increas-
ingly coordinated across the routes, particularly 
when they are friends. 
Interaction of verbal and nonverbal behavior  
So far we have concentrated on how individual 
verbal and nonverbal behaviors differ across con-
ditions. However, this does not take account of the 
interactive nature of the task and the focus of this 
paper. We therefore examine how specific respon-
sive nonverbal behaviors (looking at 
speaker/hearer and head nods) co-occur before, 
during, or after the DAMSL variables. Examina-
tion of the residuals of chi square analysis was 
used to identify co-occurrence of DAMSL dia-
logue acts with nonverbal behavior for each 
Speaker (Giver or Receiver) and condition 
(Friend/Stranger, Visible /Nonvisible). Significant 
over-use or underuse of these verbal/nonverbal co-
occurrences was then compared using the log-
likelihood statistic (Rayson, 2003) to dialogues in 
the other conditions (e.g., Giver-Friend-Visible 
with Giver-Friend-Nonvisible, and Giver-Stranger-
Visible for Head-nods, and just Friends with 
Strangers for the Gaze data). This technique, which 
we used in our earlier grounding experiment 
(Nakano et al, 2003) allows us insight into the 
probable causality of the behaviors of speaker and 
hearer, across verbal and nonverbal behavior.  
When direction-givers are speaking 
Head-nods. Givers did not nod significantly more 
or less frequently across Friends/Strangers condi-
tions when they were speaking. 
Gaze. More than in friendship dialogues, when 
strangers are speaking, and the direction-giver is 
acknowledging, the direction-receiver is likely to 
look at the Giver (G2=17.14; p<0.0001). 
More than in friendship dialogues, in Stranger 
dialogues, both before and after the direction-giver 
asserts something, the Receiver is likely to look at 
the Giver (G2=5.09; p<0.05, and G2=4.16, p<0.05, 
respectively). 
More than in friendship dialogues, both before 
and during the Giver?s use of Repeat-Rephrase 
utterances, the Receiver is likely to look at the 
Giver (G2=35.02; p<0.0001, and G2=60.74; 
p<0.0001, respectively). 
More than in friendship dialogues, both before 
and during the Giver?s use of Info-Request dia-
logue acts, the Receiver is likely to look at the 
Giver (G2=39.01; p<0.0001, and G2=9.60; p<0.01, 
respectively).  
This means that right after a direction receiver 
looks at the direction-giver, the giver produces an 
Assertion, a Repeat-Rephrase, or an Information 
DV  Source DF DF Total F Ratio 
Look At  
Speaker 
 Rte 2 10 18.03*** 
Hearer 
Nod 
 SPKR*F*Rte 2 20 5.14* 
Speaker  
Nod 
 SPKR*F*Rte 2 20 4.21* 
*p<0.05;**p<0.01; ***p<0.001 
Table 2. Non Verbal Behaviors.  
Sources abbreviated as: SPKR = Speaker; F = Friend-
ship; V = Visibility; Rte = Route,  
45
Request. As with Nakano et al, the stranger?s gaze 
towards the direction-giver can be seen as a signal 
of non-understanding and, in these contexts, it 
evokes one of these three grounding responses 
from the direction-giver.  
For friends, on the other hand, gaze towards the 
speaker evokes the next segment of the directions, 
and is therefore functioning as a signal of under-
standing. That is, more than in stranger dialogues, 
both before and during the Giver?s use of Influence 
dialogue acts (utterances such as ?turn right?), Re-
ceivers are more to look at the Giver (G2=4.77; 
p<0.05, and G2=31.92; p<0.0001, respectively). 
When direction-receivers are speaking 
Head-nods. As shown in Figure 4, Strangers used 
more head nods than Friends during their use of 
Acknowledgment dialogue acts in the visible con-
dition (G2 = 10.48, p<.01), however they do not 
differ from friends in the nonvisible condition (G2 
= 0.01, ns).  
89.52
70
41.46
71.43
0
10
20
30
40
50
60
70
80
90
100
Visible NonvisibleR
el
at
iv
e 
F
re
q
u
en
cy
 o
f R
ec
ei
ve
r N
o
d
s 
D
u
ri
n
g
 T
h
ei
r 
U
se
 o
f A
ck
n
o
w
le
d
g
m
en
t
Strangers Friends  
Figure 4: Receiver nods during Acknowledgment 
 
0
2.5
23.08
0
0
5
10
15
20
25
30
Visible Nonvisible
R
el
at
iv
e 
F
re
q
u
en
cy
 o
f R
ec
ei
ve
r 
N
o
d
s 
D
u
ri
n
g
 T
h
ei
r 
u
se
 o
f I
n
fo
-R
eq
u
es
t
Strangers Friends  
Figure 5: Receiver nods during Info Request 
Conversely, as shown in Figure 5, when receiv-
ers are making an Info-Request in the visible con-
dition (G2 =14.13, p<.001), Friends nod much more 
often than Strangers; but do not differ from Strang-
ers in the nonvisible condition (G2 = 1.44, ns). 
Once again, here the friends are marking their un-
derstanding, by nodding, even while they request 
further information. 
Gaze. Before the Receiver?s use of Acknowl-
edgment dialogue acts in Stranger dialogues, the 
Giver is more likely to look at the Receiver 
(G2=10.79; p<0.01). After the Receiver has used an 
Acknowledgment in a Stranger dialogue, s/he is 
more likely to look at the Giver a (G2=14.79; 
p<0.001). This means that among strangers the 
giver and receiver are likely to engage in mutual 
gaze around the acknowledgement dialogue act. 
During and after a Repeat-Rephrase dialogue act 
in Friends dialogues, the Receiver is more likely to 
look at the Giver (G2=10.37; p<0.01 and G2=6.72; 
p<0.01, respectively). Before the Receiver uses a 
Repeat-Rephrase dialogue act in a Friends dia-
logues, the Giver is more likely to look at the Re-
ceiver (G2=9.08; p<0.01). This means that among 
friends, giver and receiver engage in mutual gaze 
around the repeat and rephrase dialogue act. 
3.3 Discussion 
Our analysis of verbal and non-verbal behaviors 
reveals consistent differences across the short term, 
comparing subsequent direction-giving tasks, and 
across the long term, comparing strangers to 
friends.  
Strangers ? Knowledge coordination 
With respect to the co-ordination of verbal and 
nonverbal behavior, it is apparent that, among 
strangers, the Receiver?s use of Acknowledgments 
is strongly associated with characteristic gaze pat-
terns of signaling non-understanding. In these 
Stranger dyads, the Giver looks at the Receiver to 
signal the need for feedback. The Receiver then 
nods to emphasize comprehension while uttering 
the Acknowledgment (e.g., ?okay?), and then looks 
back at the Giver. This pattern is very specific to 
Strangers, and in the case of the Receiver?s use of 
head nod, specific to the visible condition. Simi-
larly, among strangers, when the Giver acknowl-
edges the receiver?s correct understanding, the Re-
ceiver looks at the Giver. This pattern of gaze re-
quest, and grounding response, happens repeatedly 
and often (Acknowledgements being used more 
frequently by strangers), ensuring coordination 
among strangers, but at the cost of frequent explicit 
requests. Of course, since Acknowledgements are 
generally backchannel utterances, used to indicate 
46
mutual understanding, one explanation for the 
higher frequency of acknowledgements, head nods 
and eye gaze by strangers, especially in the earlier 
tasks, is over-generation aimed at showing atten-
tion. Although over-generation could achieve these 
goals, it can also result in creating a false impres-
sion of mutual understanding and it is notable that 
these behaviors decrease over time. 
We also find that in the Strangers condition, Re-
ceivers are more likely to look at the Giver before 
and during the Giver?s use of Repeat-Rephrase 
(i.e., repeating back to the Receiver some earlier 
information), and also before and during the 
Giver?s use of Info-Request acts (that is the Giver 
asking the Receiver a question such as ?do you get 
that??). 
From the frequent and repeated use of Acknowl-
edgments and gaze (implying something like 
?okay? are you sure you?re okay? really??), to 
the Receiver?s gaze-anticipation of the Giver?s Re-
peat-Rephrase and Info-Request, we infer a much 
more effortful interaction for Strangers, and one 
that, in fact, for most dyads, takes longer.  
In line with Welji and Duncan (2005), we found 
evidence that the task may demand additional cog-
nitive resources for Strangers, with the Receiver in 
the Strangers dialogues breaking gaze at the Giver 
to apparently consult some internal representation 
of the space just described by the Givers Assert 
(e.g., ?you?ll find some blue couches?), before re-
turning attention, and gaze, once again to the 
Giver. 
We also note a greater use overall of Acknowl-
edgment and Completions by Strangers and in 
visible situations; Receivers in the visible situa-
tions also use more Signal Non Understanding. 
Taken together, these findings indicate that coordi-
nation and achieving mutual understanding is more 
effortful for Strangers: Friends use fewer dialogue 
acts such as Acknowledgment, Completion, and 
Signal Non Understanding, indicating that there is 
less need to negotiate understanding, and that they 
are more likely to have some kind of shared repre-
sentation. Because of this, the Friends dialogues 
and task performance would appear to be more 
efficient, with less grounding required and less mu-
tual gaze around their use of Acknowledgments, 
Info-Requests and Repeat-Rephrase.  
The fact that Friends are better able to calibrate 
the task than Strangers is also demonstrated by the 
results found for Route. Both Friend and Stranger 
dyads increase their gaze towards one another from 
Route 1 to Route 2. But Friends shift the way they 
use head nods over the course of the three routes. 
They begin in Route 1 by producing them in con-
junction with Receiver talk (acknowledgment, re-
quest for further information, repeating directions 
back). However, by Route 2, the friends are nod-
ding when the direction-giver speaks, marking that 
they don?t need further information but have un-
derstood on the first try. On the contrary, Strangers 
continue to nod just as much with receiver talk, 
and decrease their nods with giver talk; perhaps 
since by Route 2, it is clear that Strangers don?t 
understand on the first try. 
Tickle-Degnen and Rosenthal (1990) predict 
greater coordination as a relationship progresses. 
We found better coordination, but that was re-
vealed, paradoxically, through fewer coordination 
devices and fewer dialogue acts in each turn, both 
comparing from Route 1 to Route 3, and compar-
ing Strangers to Friends.  
Friends ? Positivity 
In the Friends dialogues, we find a notable col-
location of non-verbal behavior and the Receiver?s 
use of Repeat-Rephrase utterance (i.e., repeating 
the Giver?s utterance back to ensure correct inter-
pretation). This is in contrast to the findings for 
Stranger dyads which found nonverbal behaviors 
found in conjunction with the Giver?s reactive use 
of Repeat-Rephrase ? i.e., the Giver?s questioning 
of the Receiver?s understanding ? perhaps after a 
breakdown in mutual understanding. In the Friend 
dyads, it is the Receiver who proactively checks 
correct understanding of the Giver?s utterance be-
fore the interaction continues.  
Tickle-Degnen & Rosenthal predict a reduction 
in the importance of positivity as rapport increases 
over time in a relationship. We found some evi-
dence to support this, since such questioning of the 
Giver in itself may be viewed as face-threatening 
behavior. However, in the Friends dialogues, this 
Repeat-Rephrase appears anticipated ? or sanc-
tioned ? by the Giver who looks at the Receiver 
prior to the utterance. Further, during and after the 
Receivers? use of the Repeat-Rephrase utterance, 
they also look at the Giver, which again would be 
expected to be viewed as a threat to face.  
Similarly, the Receiver gazes at the Giver before 
and during the Giver?s use of Influence dialogue 
acts (explicit commands, such as ?turn left?). Such 
47
direct gaze, along with a reduced number of medi-
ating dialogue acts such as Acknowledgments, ap-
pears to indicate that Friends dialogues are less 
concerned with avoiding face-threatening behav-
ior, and as such would appear less concerned with 
maintaining positivity during the interaction.  
Note that, almost paradoxically, Friends demon-
strate their increased ability to coordinate their in-
teraction through a diminished use of explicit co-
ordination devices. This speeds up the interaction, 
and reduces the number of overall dialogue acts. 
And, finally, differences between Friends and 
Strangers are vastly diminished when the interlocu-
tors cannot see one another. This leads us to be-
lieve that nonverbal behaviors in addition to gaze 
and head nods may be playing a role in how 
Friends coordinate with one another; an advantage 
which is taken away when they can only hear one 
another?s voices.  
4 Towards a Computational Model 
In the short-term context of conversation, mainte-
nance of mutual attention and incremental coordi-
nation of beliefs are requisites for grounding and 
turn-taking. In prior computational systems, 
grounding has been achieved by marking the status 
of conversational contributions as provisional (un-
grounded) or shared (grounded). Conversational 
actions by either the user or the system can trigger 
updates that change provisional information to 
shared. Acknowledgements, for example, are ex-
plicit ways of achieving grounding, but moving on 
to the next stage of the task is equally effective, as 
it presupposes that prior utterances have been 
taken up (Traum, 1994). In a model such as this, 
grounding occurs at the turn level. In order to han-
dle the multimodal phenomena that participate in 
grounding in face-to-face conversation, as Nakano 
et al (Nakano et al, 2003) have shown, a model of 
knowledge coordination needs to have more fre-
quently updated access to potential grounding 
events. In that implementation, we continuously 
polled for inputs, so as to capture the updates in 
grounding that occur between typical linguistic 
segments. We believe that the focus on time and 
process that allowed us to look at events of a 
smaller granularity in our earlier work on nonver-
bal grounding behavior will also allow us to extend 
up to events of a larger granularity, such as stages 
in a relationship. That is, we believe that the results 
described in earlier sections of this paper can be 
taken into account in a computational system by 
maintaining a model of the state of shared and pri-
vate information across several interactions (sev-
eral years, if possible). In this way, the shared his-
tory of two interlocutors (the user and the system) 
can be translated into patterns of linguistic behav-
ior, such as reduced use of acknowledgements, and 
reduced positivity, with increased interruption and 
information requests. This is similar to Cheng, 
Cavedon & Dale (2004)?s approach to direction-
giving. In this approach, the system maintains a 
history of places it has given directions to before. 
Using this task history, it is able to generate shorter 
directions at later stages in the dialogue. In our im-
plementation, however, the very style of the inter-
action is modified by the shared history of the user 
and the system. In the sense that we are modifying 
the linguistic style of the dialogue based on psy-
chological attributes, our approach is similar to 
work by Mairesse & Walker (2007) and Isard et al 
(2006). In both cases, a broad set of natural lan-
guage generation parameters is employed to gener-
ate language that differs along a personality di-
mension, based on a number of previous empirical 
studies. In the current approach, however, the fea-
tures that are modified derive from the interde-
Figure 6. Proposed architecture for modeling coordination within and across conversation 
48
pendence of the system with a particular user.  
Some of the features that are present in the con-
versations of friends, such as interjections and 
completion of one another?s utterances, are still 
beyond current computational abilities, as they 
would require online, real-time processing and un-
derstanding of utterances with incremental plan-
ning and generation of responses. We are inter-
ested in pursuing this feature of the system as dia-
logue technologies improve. 
5 Conclusion 
In this paper, we have compared direction-
giving between friends and strangers, and within 
these two groups we have compared three subse-
quent direction-giving episodes. In order to deter-
mine the effect played specifically by nonverbal 
behavior in short- and long-term rapport, half of 
our participants could see one another, while the 
other half were divided by a screen. Our experi-
mental and analytic methodology drew from both 
the social psychological, conversational analysis, 
and conversation as joint action traditions. Conse-
quently, our results were able to demonstrate the 
ways in which the verbal and nonverbal devices 
that index rapport relate to the role those same de-
vices play in knowledge coordination. Based on 
this commonality, we proposed a computationally 
viable model of deepening friendship within and 
across subsequent tasks that extends our previous 
work on grounding in face-to-face interaction. The 
work we have presented here therefore differs sub-
stantially from previous work on rapport and rela-
tionship building in embodied conversational 
agents. We did not start out with a definition of 
rapport but instead investigated those behaviors 
that characterize dyads who have self-identified as 
friends or strangers. And rather than looking at 
rapid assessment of rapport (the feeling of ?click-
ing?) we looked at the long-term version: acquiring 
a sense of mutual interdependence. Finally, rather 
than looking at how to get ECAs to engage users 
into establishing a relationship, or into letting 
down their guard, we examined those behaviors 
that characterize the dyadic interaction at each 
stage.  
All of these topics, however, are clearly inter-
related, and future research will benefit from tak-
ing a greater number of them into account in both 
data analysis, and the implementation of ECAs. 
Future research in our own lab will also have to be 
more explicit about how to implement the compu-
tational model that we have started to lay out here. 
Additional subjects in a similar experiment will no 
doubt facilitate that task. 
As we increasingly understand better how con-
versation changes when people come to know one 
another, we expect to apply these results to our 
ongoing research on virtual peers that can teach 
children with autism how to sustain interpersonal 
relationships (Tartaro & Cassell, 2006) and to our 
work on building the survey interviewers of the 
future, who can both engage their survey-takers 
and keep them honest (Cassell & Miller, in press). 
More generally, however, we hope to increasingly 
implement ECAs who will stick around for the 
long haul. 
Acknowledgments 
Thanks to Kristina Striegnitz , Will Thompson, Tara 
Latta and Nate Cantelmo for their help, and Darren 
Gergle for his superior statistical knowledge. We are 
grateful to Motorola for funding that supported some of 
the research reported here. 
References 
Bickmore, T., & Picard, R. (2005). Establishing and 
Maintaining Long-Term Human-Computer 
Relationships. ACM Transactions on Computer 
Human Interaction (ToCHI), 12(2), 293-327. 
Brown, P., & Levinson, S. (1987). Politeness: Some 
Universals in Language Usage. Studies in 
International Sociolinguistics. New York: 
Cambridge University Press. 
Cappella, J. N. (1990). On Defining Conversational 
Coordination and Rapport. Psychological Inquiry, 
1(4), 303-305. 
Cassell, J., & Bickmore, T. (2002). Negotiated 
Collusion: Modeling Social Language and its 
Relationship Effects in Intelligent Agents. User 
Modeling and Adaptive Interfaces, 12, 1-44. 
Cassell, J., & Miller, P. (in press). Is it Self-
Administration if the Computer Gives you 
Encouraging Looks? In F. G. Conrad & M. F. 
Schober (Eds.), Envisioning the Survey Interview of 
the Future. New York: John Wiley & Sons. 
Cassell, J., & Tversky, D. (2005). The Language of 
Online Intercultural Community Formation. Journal 
of Computer-Mediated Communication, 10(2), 
article 2. 
Cheng, H., Cavedon, L., & Dale, R. (2004, 28th-29th 
August). Generating Navigation Information Based 
49
on the Driver's Route Knowledge. Paper presented 
at the COLING 2004 Workshop on Robust and 
Adaptive Information Processing for Mobile Speech 
Interfaces. Geneva,Switzerland. 
Clark, H. H. (1996). Using Language. Cambridge: 
Cambridge University Press. 
Clark, H. H., & Brennan, S. E. (1991). Grounding in 
communication. In L. B. Resnick, J. M. Levine & S. 
D. Teasley (Eds.), Perspectives on socially shared 
cognition (pp. 127-149). Washington DC: American 
Psychological Association. 
Clark, H. H., & Wilkes-Gibbs, D. (1986). Referring as a 
collaborative process. Cognition, 22, 1-39. 
Core, M., & Allen, J. (1997). Coding Dialogue with the 
DAMSL Annotation Scheme. Proceedings of the 
AAAI Fall Symposium on Communicative Action in 
Humans and Machines. Boston, MA. 
Duncan, S., Jr. (1990). Measuring Rapport. 
Psychological Inquiry, 1(4), 310-312. 
Goodwin, C. (1981). Conversational Organization: 
Interaction between speakers and hearers. New 
York: Academic Press. 
Grahe, J. E., & Bernieri, F. J. (1999, Win). The 
importance of nonverbal cues in judging rapport. 
Journal of Nonverbal Behavior, 23(4), 253-269. 
http://www.springeronline.com  
Gratch, J., Okhmatovskaia, A., Lamothe, F., Marsella, 
S., Morales, M., van der Werf, R. J., et al (2006). 
Virtual Rapport. Proceedings of the 5th 
International Conference on Interactive Virtual 
Agents (IVA). Marina del Rey, CA. 
Hornstein, G. A. (1982). Variations in conversational 
style as a function of the degree of intimacy between 
members of a dyad. Unpublished Doctoral, Clark 
University. 
Isard, A., Brockmann, C., & Oberlander, J. (2006). 
Individuality and alignment in generated dialogues. 
Proceedings of the 4th International Natural 
Language Generation Conference (pp. 22-29). 
Sydney, Australia. 
Maatman, M., Gratch, J., & Marsella, S. (2005). Natural 
Behavior of a Listening Agent. Paper presented at 
the 5th International Conference on Interactive 
Virtual Agents (IVA). Kos, Greece. 
Mairesse, F., & Walker, M. (2007). PERSONAGE: 
Personality generation for dialogue. Proceedings of 
the 45th Annual Meeting of the Association for 
Computational Linguistics (ACL). Prague. 
Matheson, C., Poesio, M., & Traum, D. (2000). 
Modelling Grounding and Discourse Obligations 
Using Update Rules. Proceedings of the 1st Annual 
Meeting of the North American Association for 
Computational Linguistics (NAACL2000). Seattle, 
WA. 
Nakano, Y. I., Reinstein, G., Stocky, T., & Cassell, J. 
(2003, July 7-12). Towards a Model of Face-to-Face 
Grounding. Proceedings of the Annual Meeting of 
the Association for Computational Linguistics (p. 
553?561). Sapporo, Japan: Association for 
Computational Linguistics  
Rayson, P. (2003). Matrix: A statistical method and 
software tool for linguistic analysis through corpus 
comparison. Unpublished doctoral thesis, Lancaster 
University, Lancaster. 
Richmond, V. P., & McCroskey, J. C. (1995). 
Immediacy. In Nonverbal Behavior in Interpersonal 
Relations (pp. 195-217). Boston: Allyn & Bacon. 
Schegloff, E. A., & Sacks, H. (1973). Opening up 
closings. Semiotica, 8, 289-327. 
Stronks, B., Nijholt, A., van der Vet, P., & Heylen, D. 
(2002). Designing for friendship: Becoming friends 
with your ECA. Proceedings of the Embodied 
conversational agents - let's specify and evaluate 
them! (pp. 91-97). Bologna, Italy: ACM Press. 
Tartaro, A., & Cassell, J. (2006, August 28 - September 
1). Authorable virtual peers for autism spectrum 
disorders. Proceedings of the Combined workshop 
on Language-Enabled Educational Technology and 
Development and Evaluation for Robust Spoken 
Dialogue Systems at the 17th European Conference 
on Artificial Intellegence. Riva Del Garda, Italy. 
Tickle-Degnen, L., & Rosenthal, R. (1990). The nature 
of rapport and its nonverbal correlates. 
Psychological Inquiry, 1(4), 285-293. 
Traum, D. R. (1994). A Computational Theory of 
Grounding in Natural Language Conversation. 
University of Rochester, Rochester, NY. 
Traum, D. R., & Dillenbourg, P. (1998). Towards a 
Normative Model of Grounding in Collaboration. 
Proceedings of the ESSLLI-98 workshop on Mutual 
Knowledge, Common Ground and Public 
Information. Saarbrucken, Germany. 
Welji, H., & Duncan, S. (2005). Collaboration and 
Narration: The role of shared knowledge in the 
speech and gesture production of friends and 
strangers. Paper presented at the International 
Society of Gesture Studies Conference. Lyon, France. 
50
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 37?45,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Reactive Redundancy and Listener Comprehension in Direction-Giving 
 
Rachel E. Baker Alastair J. Gill, Justine Cassell 
Department of Linguistics Center for Technology and Social Behavior 
Northwestern University Northwestern University 
Evanston, IL 60208 Evanston, IL 60208 
r-baker2@northwestern.edu {alastair,justine}@northwestern.edu 
 
 
Abstract 
We explore the role of redundancy, both in 
anticipation of and in response to listener 
confusion, in task-oriented dialogue. We 
find that direction-givers provide redundant 
utterances in response to both verbal and 
non-verbal signals of listener confusion. 
We also examine the effects of prior ac-
quaintance and visibility upon redundancy. 
As expected, givers use more redundant ut-
terances overall, and more redundant utter-
ances in response to listener questions, 
when communicating with strangers. We 
discuss our findings in relation to theories 
of redundancy, the balance of speaker and 
listener effort, and potential applications. 
1 Introduction 
Our everyday conversations represent a carefully 
negotiated balance between the perceived needs of 
the speaker and the listener. These opposing forces 
affect every aspect of language from phonetics to 
pragmatics. A careful balance between these two 
forces allows speakers to produce language that is 
both efficient and effective at communicating a 
message (Lindblom, 1990; Horn, 1993). Of course, 
the same balance is not appropriate for every situa-
tion. When accuracy is critical to the message, or 
when the speaker perceives the listener to have 
difficulty understanding, the speaker is more likely 
to prioritize clarity over efficiency, resulting in 
more explicit communication. In contrast, during 
casual conversation or when speed is a factor, the 
speaker may choose a more reduced, efficient, 
communication style (Lindblom, 1990; Horton and 
Keysar, 1996). A number of scholars have pointed 
out that speakers seem to use the information 
available to themselves rather than that available to 
the listener to guide certain linguistic decisions, 
such as clarity of pronunciation and choice of syn-
tactic structure (Bard et al, 2000; Branigan et al, 
2003). However, these studies examine utterance 
form, while our study examines content, which is 
more influenced by audience design (Branigan et 
al., 2003). In every utterance, a speaker either re-
duces the likelihood of listener misunderstanding 
by being more explicit, or reduces their own effort 
by providing a minimal amount of information. 
Regardless of whether speakers pro-actively moni-
tor the information needs of listeners, they do need 
to respond when listeners say or do something to 
indicate confusion. Developing a better under-
standing of the factors that affect how and when 
speakers respond to signs of listener confusion is 
important at both theoretical and applied levels: 
first, it can better explain the variation in discourse 
strategies used in different communicative situa-
tions; second, it can help in the design of dialogue 
systems (Kopp et al, 2008; Theune et al, 2007). 
In this study, we examine what types of listener 
behavior increase the likelihood that a speaker will 
produce a redundant utterance. We also examine 
how communicative context affects the amount 
redundancy a speaker produces overall (Walker, 
1992, 1996) and a speaker?s use of redundancy in 
response to listener confusion. In contrast to pre-
vious work, we study reactive redundancy, or re-
dundancy produced in response to signs of listener 
confusion. We investigate two factors that may 
influence a speaker?s tendency to produce redun-
dant utterances and to respond to listener confusion 
with redundancy: the relationship between the in-
terlocutors and their visual contact.  
In the following section, we review relevant li-
terature and present our hypotheses; we then de-
scribe the direction-giving experiment which we 
used to examine redundancy in task-oriented di-
alogue, and present our results; we discuss our re-
sults in light of the literature and conclude by 
noting potential applications and future work.  
37
2 Related Work and Predictions 
2.1 Redundancy 
Grice?s (1975) second Maxim of Quantity: ?Do not 
make your contribution more informative than is 
required? has led to the general impression that 
redundancy (providing discourse-old information) 
is avoided in language (Stalnaker, 1978), with this 
mirrored by work in natural language generation 
(Dalianis, 1999). However, Walker (1992, 1996) 
points out that such conclusions relating to redun-
dancy are often based on flawed assumptions. For 
example, they assume that agents have unlimited 
working memory and the ability to automatically 
generate all the inferences entailed by every utter-
ance, that utterance production should be mini-
mized, and that assertions by Agent A are accepted 
by default by Agent B (Walker, 1996: 183).  
In fact, redundancy can serve many desirable 
purposes in communication. Redundancy has been 
shown to increase text cohesion and readability 
(Horning, 1991) as well as provide evidence of 
understanding and grounding, make a proposition 
salient, and make inferences explicit (Walker, 
1996). A computer simulation of a cooperative task 
dialogue between two agents suggested that the use 
of certain types of redundant utterances improved 
the performance of the pair (Walker, 1996). 
Fussell and Krauss (1989a) point out that there 
are two methods that speakers can use to tailor 
their message for the listener. The first method in-
volves predicting what information it is necessary 
to communicate, using knowledge of the listener?s 
interests and background. The second method in-
volves modifying the message in response to lis-
tener feedback. Walker?s model only captures the 
use of redundancy in the service of the first me-
thod. We will refer to this type of redundancy as 
proactive redundancy, whereby a speaker provides 
redundant information without waiting for the lis-
tener to express a need for it. The advantages of 
providing redundant information proactively in-
clude being able to integrate the redundant infor-
mation with the new information, and avoiding 
conflict by removing the necessity for the listener 
to express a lack of understanding (Brown and Le-
vinson, 1987).  
We hypothesize that speakers also use redun-
dancy reactively, after the listener signals a lack of 
understanding, either verbally or non-verbally. 
This is redundancy in service of Fussell and 
Krauss? second method of message-tailoring. The 
advantages of providing redundant information 
reactively include increasing the efficiency of the 
exchange by only providing redundant information 
that the listener communicates a need for, and re-
ducing the burden on the speaker of having to de-
cide when to include redundant information.  
One important distinction between proactive 
and reactive redundancy is the grounding status of 
the redundant information. Reactive redundancy is 
likely to provide information that has not been ac-
cepted by the listener, and is therefore not part of 
the common ground (Clark and Schaeffer, 1989), 
even though it is discourse-old. In contrast, proac-
tive redundancy is likely to provide information 
from the interlocutors? common ground. Indeed, 
Walker (1996) describes Attitude redundant utter-
ances as providing evidence of grounding. Walk-
er?s other types of proactive redundancy 
(Consequence and Attention) make inferences 
based on grounded utterances explicit and make 
elements of the common ground salient again.  
Reactive redundancy is one type of repair, like 
expansions and replacements, which can be used in 
response to non-understanding or misunderstand-
ing (Hirst et al, 1994). The type of miscommuni-
cation may influence a speaker?s choice of repair 
strategy, with reactive redundancy being an appro-
priate response to mishearing or misremembering.  
However, producing redundant information, 
even when the listener signals a need for it, incurs 
a cost. Including redundant information increases 
the length of the dialogue and the speaker?s effort, 
and decreases the amount of new information pro-
vided within a certain length of time. In these cases 
the speaker must decide how much redundant in-
formation to provide and when to provide it.  
2.2 Signals of Confusion 
Listeners can express a need for information to be 
repeated or restated in a number of ways, both ver-
bally and non-verbally. Brinton et al (1988) used 
questions and statements of confusion (?I didn?t 
understand?) as signs of communication break-
downs. Morrow et al (1993) describe inaccurate 
and partial repetitions of instructions as elements 
of miscommunication. This prior work leads us to 
examine questions, utterances signaling non-
understanding (e.g. ?I don?t remember what?s 
38
next?), incorrect repetitions (e.g. ?take the third 
right? after the direction-giver said ?take the 
second right?) and abandoned utterances (e.g. 
?Then I?ll turn??) as possible signs of listener 
confusion. We predict redundancy after such 
statements because they all indicate that a piece of 
information has not been understood. 
We also examine eye-gaze as a non-verbal 
marker of listener comprehension. Goodwin (1981) 
described gaze towards the speaker as a sign of 
listener attention. However, Nakano et al (2003) 
found that speakers seemed to interpret a listener 
gazing at them rather than at a map as a sign of 
listener misunderstanding. Therefore, shifting eye-
gaze away from the speaker can signal that a lis-
tener is losing attention, perhaps due to confusion, 
while shifting gaze towards the speaker can signal 
misunderstanding. In this study there is no map, 
and listeners who can see the speaker spend most 
of the conversation gazing at the speaker. Still, due 
to the opposing findings in the literature, we ana-
lyze eye-gaze shifts both towards and away from 
the speaker as potential signs of listener confusion.  
2.3 Relationship and Communication 
Speakers are more explicit when communicating 
with strangers or people with whom they share less 
common ground. This explicitness can take the 
form of highly informative self-introductions on 
the phone (Hornstein, 1985), longer descriptions of 
abstract figures (Fussell and Krauss, 1989b), and 
explicit references to utterance topics (Svedsen and 
Evjemo, 2003). These studies indicate that speak-
ers attempt to make up for the small amount of 
common ground they share with strangers by in-
cluding more information in the discourse itself.  
Another difference between friends and non-
friends is that acquaintances tend to be more for-
mal, more concerned with self presentation, less 
negative, and less likely to disagree than friends 
(Schlenker, 1984; Tickle-Degnen and Rosenthal, 
1990; Planalp and Benson, 1992). Therefore, we 
expect that in an initial interaction, a speaker will 
try to appear competent and avoid conflict.  
As noted above, speakers talking to strangers 
are more explicit, leading us to predict more re-
dundancy overall. They are also more likely to try 
to impress their interlocutor and avoid conflict, 
leading to more reactive redundancy in response to 
confusion when the pair are strangers.  
2.4 Visibility and Communication 
Visibility also has a number of effects on commu-
nication. One of the most basic is that when inter-
locutors cannot see each other they cannot use non-
verbal signals to communicate, so they must rely 
on verbal communication. For example, the use of 
eye-gaze as a sign of listener attention (Argyle and 
Cook, 1976; Goodwin, 1981) is only possible 
when interlocutors can see each other. When they 
cannot see each other, they must indicate attention 
verbally or do without this information.  
Visibility affects both the form and the out-
comes of a conversation. When interlocutors can-
not see each other, conversations are longer and 
contain more, shorter, utterances than when they 
can (Nakano et al, 2003). Interlocutors in an in-
vestment game who could not see each other also 
did not establish trust to the same extent as those 
who met face-to-face (Bos et al, 2002).  
Because speakers who cannot see each other 
have fewer channels of communication available to 
them, their interaction can be more difficult than a 
face-to-face interaction. We predict that this will 
lead them to use more redundancy and more reac-
tive redundancy in an effort to be clear. 
2.5 Hypotheses 
In order to study how responsive speakers are to 
signs of listener confusion, we must first determine 
what signs speakers respond to. In this study we 
examine a number of verbal and non-verbal signs 
speakers may use to gauge listener confusion. In 
particular, we expect that speakers will provide 
redundancy in response to both verbal signs like 
questions, statements of non-understanding, incor-
rect statements, and abandoned utterances, and 
non-verbal signs like eye-gaze changes. We expect 
that speakers will strike a different balance be-
tween efficiency (minimizing speaker effort) and 
clarity (minimizing listener effort) depending on 
the relationship between the speaker and listener, 
and the physical context of the interaction. We ex-
pect speakers to use redundancy strategies focused 
on minimizing speaker effort when addressing 
friends and people they can see. Such strategies 
involve less redundancy (and therefore less speak-
ing), and less reactive redundancy (requiring less 
listener monitoring). Conversely, we expect to find 
redundancy strategies maximizing clarity when 
39
speakers address strangers and people they cannot 
see. Such strategies involve more redundancy 
overall (providing the listener with more informa-
tion in general) as well as more reactive redundan-
cy (which provides the listener with the specific 
information they may require). 
Hypothesis 1 - Redundancy and Non-
Understanding 
(a) Verbal cues - Direction-givers will provide 
redundancy when the receiver verbally expresses a 
lack of understanding by asking a question, aban-
doning an utterance, making an incorrect statement 
or explicitly expressing non-understanding.  
(b) Non-verbal cues - Givers will provide redun-
dancy when the receiver non-verbally expresses a 
lack of understanding by shifting eye-gaze.  
Hypothesis 2 - Redundancy and Relationship 
Givers will prioritize clarity over efficiency in their 
redundancy use when speaking to strangers, pro-
viding (a) more redundancy and (b) more reactive 
redundancy than when speaking to friends.  
Hypothesis 3 - Redundancy and Visual Contact 
Givers will prioritize clarity over efficiency in their 
redundancy use when they cannot see their partner, 
providing (a) more redundancy and (b) more reac-
tive redundancy than when they can see them. 
3 Methods 
3.1 Participants 
Twenty-four university students participated, re-
sulting in twelve dyads. All were paid $10 for their 
participation and received $5 gift certificates if 
they successfully completed the task. In each dyad 
the direction-giver was familiar with the building 
in which the experiment took place, and the direc-
tion-receiver was unfamiliar with it. Half the dyads 
were pairs of friends and half were strangers.  
3.2 Procedure 
The task consisted of three consecutive direction-
giving sessions, as described in Cassell et al 
(2007). At the start of each session, the experimen-
ter led the direction-giver to a point in the building, 
and back to the experiment room. Half of the dyads 
sat facing each other during the direction-giving 
(the Vision condition) and half sat back-to-back 
with a screen between them (the No-vision condi-
tion). The direction-giver then explained the route 
to the direction-receiver. There were no time limits 
or restrictions on what could be said, but the dyads 
could not use maps or props. When the dyad de-
cided that direction-giving was complete, they sig-
naled the experimenter, who the receiver led to the 
goal, following the directions.  
The direction-giving sessions were videotaped. 
Participants? speech was transcribed and coded for 
possible redundancy triggers and redundant utter-
ances using the coding scheme described below. 
The time-aligned codings for the giver and receiver 
were aligned with each other using scripts that cal-
culated which of the receiver?s utterances or ac-
tions directly preceded which of the giver?s 
utterances. The scripts classify a receiver?s utter-
ance or action as ?preceding? a giver?s utterance if 
its start precedes the start of the giver?s utterance 
and its end is not more than two seconds before the 
start of the giver?s utterance. The two-second limit 
was used to avoid positing connections between a 
giver?s utterance and receiver utterances that came 
long before it.  
3.3 Data Coding 
Each dialogue was divided into clauses, defined as 
units that include a subject and predicate and ex-
press a proposition. Each clause was coded using a 
modified version of DAMSL (Core and Allen, 
1997). Direction-givers? and receivers? speech was 
coded differently because we only studied redun-
dancy produced by the giver. We coded the receiv-
er?s speech for signs of confusion. We describe the 
labels we used in more detail below.  
Each direction-giver?s clauses were coded for 
Statements and Info-requests. The Info-request tag 
marks questions and other requests for informa-
tion. In a Statement, a speaker makes a claim about 
the world. The class of Statements was broken 
down into Non-redundant, in which the speaker is 
trying to change or add to the hearer?s beliefs, and 
Redundant, which contain only information that 
has already been stated or entailed. 
Each direction-receiver?s clauses were coded 
for Statements, Info-requests, Signal non-
understandings (S.N.U.), and Abandoned utter-
ances. The receiver?s Statements were classified as 
either Correct or Incorrect. If an utterance explicit-
ly expressed non-understanding of an earlier utter-
ance it was coded as Signal non-understanding. 
This label was only used for direct statements of 
non-understanding, such as ?I didn?t follow that,? 
40
and not for signals of non-understanding covered 
by other labels such as Info-requests and Incorrect 
Statements. Utterances that were abandoned (the 
speaker stops the utterance and it provides no con-
tent to the dialogue) were coded as Abandoned. 
Receiver utterances that were not coded as Info-
requests, Incorrect Statements, Signal-non-
understandings, or Abandoned, were coded as No-
trigger. No-trigger utterances included correct 
statements and statements about task management.  
4 Results 
We found that a large proportion of giver utter-
ances were redundant, ranging from 17% to 38% 
with a mean of 25%. Examples of redundancy 
from our recordings are listed in the Appendix.  
We first analyzed the data using a hierarchical 
loglinear analysis with the variables: visual condi-
tion (Vision, No-vision), relationship (Friends, 
Strangers), receiver-utterance (Info-request, Incor-
rect statement, Signal non-understanding, Aban-
doned, No-trigger), and giver-utterance 
(Redundant, Non-redundant). The overall model is 
significant (?2(39,5294)=13254.157,p<.001), justify-
ing chi-square comparisons of individual factors 
within the model. We report tests of partial associ-
ation and chi-square tests to indicate where signifi-
cant differences lie between groups.  
4.1 Redundancy and Non-Understanding 
Verbal Signals of Non-Understanding 
We tested part (a) of Hypothesis 1 by running a 
test of partial associations (adjusted for all effects 
in the model) and an unpartialled chi-square (ig-
noring variables not included in the effect being 
tested). These showed a significant association be-
tween receiver-utterance and giver-utterance type 
(Partial ?2(4,5294)=117.7, p<.001; 
?2(4,5294)=121.2,p<.001). 
Chi-square tests comparing giver-utterances fol-
lowing predicted redundancy triggers to giver-
utterances after No-trigger receiver utterances, in-
dicate that Info-requests, Incorrect statements and 
Abandoned utterances all significantly increase the 
likelihood that the giver will produce a redundant 
utterance (?2(1,4907)=57.3,p<.001; ?2(1,4562)=28.4, 
p<.001; ?2(1,4651)=49.1,p<.001, respectively). Expli-
cit Signal-non-understandings do not have signifi-
cant effects on the likelihood of a redundant-
utterance (?2(1,4539)=.3,p=.619). Figure 1 shows the 
percentages of giver utterances that were redundant 
following various receiver dialogue acts.  
Non-Verbal Signals of Non-Understanding 
We tested part (b) of Hypothesis 1 with a separate 
hierarchical loglinear analysis examining only the 
dyads in the Vision condition for the effects of: 
relationship, receiver-utterance, giver-utterance, 
and receiver-gaze (Gaze-to, Gaze-away, and No-
gaze-change). The first- and second-order effects 
are significant (?2(59,2815)=9582.4, p<.001).  
A test of partial associations and a chi-square 
test indicate a significant association between giv-
er-utterance and receiver-gaze (Partial ?2(2,2815)= 
22.7, p<.001; ?2(2,2815)=24.7,p<.001). Chi-square 
tests comparing receiver gaze changes to non-
changes show that redundant utterances are signifi-
cantly more likely after a gaze change toward the 
giver (?2(1,2433)=21.5,p<.001) and after a gaze 
change away from the giver (?2(1,2475)=6.5,p<.05) 
than after no gaze change. A chi-square test com-
paring gaze change toward the giver to gaze 
change away from the giver shows that the differ-
ence between them is not significant (?2(1,722)=2.7, 
p=.098). These effects are shown in Figure 2.  
 
 
     
52.2% 48.7% 41.0%
27.3% 24.0%
0%
10%
20%
30%
40%
50%
60%
Red
und
ant 
 GIv
er  U
ttera
nces
 (%) **
*
 
Figure 1. Percent of redundant giver utterances fol-
lowing various receiver dialogue acts. 
 
        
39.7%
33.8%
27.4%
0%5%
10%15%
20%25%
30%35%
40%45%
Towards Away No changeRed
und
ant 
 Giv
er  U
ttera
nces
 (%)
Gaze
* *
 
Figure 2. Percent of redundant giver utterances fol-
lowing receiver eye-gaze changes toward and away 
from the giver, and following no gaze change 
 
41
4.2 Redundancy and Relationship 
Part (a) of Hypothesis 2 was confirmed by the sig-
nificant association between relationship and giv-
er-utterance (Partial ?2(1,5294)=13.3, p<.001; 
?2(1,5294)=6, p<.05) in our original analysis. A larger 
percentage of giver utterances are redundant in the 
Strangers condition (27.8%) than in the Friends 
condition (24.8%). 
 
To examine part (b) of Hypothesis 2 we ran a 
hierarchical loglinear analysis after collapsing all 
receiver-utterances into question/non-question cat-
egories. This reveals a significant partial associa-
tion among giver-utterance, receiver-utterance, and 
relationship (Partial ?2(1,5294)=7.5, p<.01). A chi-
square test comparing utterances after questions in 
the Friends and Strangers conditions shows that 
redundant utterances are significantly more likely 
after questions in the Strangers condition than the 
Friends condition (?2(1,412)= 14.6, p<.0005), as 
shown in Figure 3.  
Three-way interactions among giver-utterance, 
receiver-utterance and relationship are not signifi-
cant in any of the other analyses. 
4.3 Redundancy and Visual Contact 
There is a trend-level association between visual 
condition and giver-utterance type (Partial ?2(1,5294) 
=4.6,p<.05; ?2(1,5294)=3.3,p=.071). Contrary to Hy-
pothesis 3, a larger percentage of utterances are 
redundant in the Vision condition (27.7%) than in 
the No-vision condition (25.5%). No significant 
association was found among giver-utterance, re-
ceiver-utterance, and visual condition, even when 
collapsed into question/non-question categories. 
5 Discussion 
This study set out to discover what verbal and non-
verbal behaviors increase the likelihood of redun-
dant utterances in direction-givers? speech. We 
also examined whether the interlocutors? relation-
ship or visual contact influence whether speakers 
provide redundant utterances in anticipation of and 
in response to listener confusion. We found that 
givers used a large proportion of redundant utter-
ances, (around 25% of utterances). Walker (1996) 
found that about 12% of utterances were redundant 
in a corpus of recordings from a call-in financial 
radio show. The higher proportion of redundant 
utterances in our study is predicted by Walker?s 
(1996) model, in which a task?s tolerance for com-
prehension errors influences whether redundant 
utterances are produced. In a radio advice show, a 
misunderstanding may be more easily recovered 
from than in direction-giving, in which one wrong 
turn could make it impossible to reach the goal.  
In addition to revealing the impact of task toler-
ance to error on redundancy, this study sheds light 
on other circumstances that influence redundancy 
use. Givers produced reactive redundancy in re-
sponse to the verbal triggers: Info-requests, Aban-
doned utterances, and Incorrect statements. 
However, even these triggers were not always fol-
lowed by redundancy. In fact, only around 50% of 
the utterances following these triggers were redun-
dant. Such a low response rate is surprising until 
we consider the diversity of utterances covered by 
these labels. For instance, some Info-requests seek 
new information (e.g. ?What?s at the top of the 
stairs??), and some receiver utterances are aban-
doned because the giver interrupts with new in-
formation. Our study lays the groundwork for 
future examinations of speaker responses to listen-
er confusion, which can refine these broad catego-
ries. We must also consider the variability in 
responses to listener confusion. We found that giv-
ers are more likely to provide redundant utterances 
in response to questions when speaking to stran-
gers, but this is only one of many factors that could 
affect levels of responsiveness, including speaker 
personality, time pressure, and task difficulty.  
The non-significant effect of Signals non-
understandings on redundancy is surprising. This 
may be due to the small number of examples of 
this category in our recordings. We found only 44 
instances of Signal non-understandings, in contrast 
to, for example, 156 Abandoned utterances.  
The non-verbal cue gaze change also increased 
the likelihood of a redundant utterance. Interesting-
ly, gaze changes both to and away from the giver 
              
37%
24%
52%
27%
0%
10%
20%
30%
40%
50%
60%
Question Non-question
Re
du
nd
an
t  
Gi
ve
r 
 
Ut
te
ra
nc
es
 
(%
) FriendsStrangers
*
 
Figure 3. Percent of redundant giver utterances fol-
lowing questions and non-questions, by relationship. 
 
42
triggered redundancy. This is consistent with both 
Nakano et al?s (2003) finding that gazing at the 
speaker signals listener misunderstanding and 
Goodwin?s (1981) finding that gazing away from 
the speaker indicates a lack of listener attention.  
It is interesting that 24% of giver utterances fol-
lowing No-trigger receiver utterances were redun-
dant. These probably include both redundant 
utterances triggered by signs of listener confusion 
that we did not code for, and proactive redundancy. 
Proactive redundancy can appear within the first 
description of some directions (see the No-trigger 
example in the Appendix) and when the whole set 
of directions is repeated as a memory aid.  
The relationship between the interlocutors does 
affect the amount of redundancy speakers produce 
overall and in response to listener signs of confu-
sion. Strangers used more redundant utterances 
than friends and provided more redundant utter-
ances after questions. This supports our hypothesis 
that direction-givers speaking to strangers will pri-
oritize clarity over efficiency. The more consistent 
use of reactive redundancy in the Strangers condi-
tion may be due to speakers? tendency to avoid 
confrontation with strangers. When responding to 
questions from friends, direction-givers may pro-
vide some new information because they know that 
their friend will feel comfortable asking another 
question if their answer is unclear. However, when 
answering questions from a stranger, the giver may 
wish to avoid the embarrassment of further confu-
sion by repeating more discourse-old information.  
However, contrary to our predictions, we did 
not find more redundancy or more reactive redun-
dancy in the No-vision condition than the Vision 
condition. In fact, we found numerically more re-
dundancy in the Vision condition. Given the low 
level of significance, we do not discuss this in de-
tail, however we suggest that this could be due to 
the fact that there are more ways of signaling non-
understanding available to the receivers in the Vi-
sion condition (both verbal and non-verbal). There-
fore, even if givers do not increase their rates of 
reactive redundancy in the Vision condition, they 
could provide more reactive redundancy (and more 
redundancy overall) because they are receiving 
more cues to react to. Not all situations leading to 
communication difficulties encourage more redun-
dancy or more reactive redundancy, but the in-
creased explicitness and positivity typical of 
conversation between strangers do encourage it.   
6 Conclusion 
This study explored the use of redundancy in task-
oriented dialogue, specifically the effects of listen-
er behavior and communicative context on the 
amount of redundancy produced. We found that 
direction-givers provided redundant utterances in 
response to verbal and non-verbal signs of listener 
confusion. As predicted, givers were more likely to 
prioritize clarity over efficiency in their redundan-
cy use (using more redundancy overall and more 
redundancy in response to questions) when speak-
ing to strangers than friends. Contrary to our pre-
dictions, givers did not provide more redundant 
utterances when they could not see their listener.   
Direction-giving, due to its high memory load 
and the need for the receiver to understand the giv-
er almost completely, is a type of discourse that 
may encourage more redundancy than other types. 
Indeed, we note that our data have a much greater 
proportion of redundancies than discussions taken 
from radio talk shows (Walker, 1996). Future work 
should examine the nature of proactive and reac-
tive redundancy in more varied discourse contexts, 
such as negotiation, teaching, and play. It should 
also explore the effects of memory load on redun-
dancy by varying task complexity, which may be 
easier with a more controlled task like the Map-
task. Researchers could study the relationship be-
tween saliency and redundancy by studying 
correlations between a segment?s salience and its 
likelihood of being used in a redundant utterance.  
Our findings can be used to improve the com-
municative efficacy of natural language generation 
systems like those used in Embodied Conversa-
tional Agents (ECAs; Kopp et al, 2008). For ex-
ample, like strangers, direction-giving ECAs could 
use increased overall and reactive redundancy to 
compensate for the lack of shared common ground 
with the human user of the system. Analyses of the 
syntactic structures of different types of redundant 
utterances will be important for incorporating these 
results into generation systems. 
Acknowledgments 
We thank Paul Tepper, Gregory Ward, Darren 
Gergle, Alex Podbelski, and our anonymous re-
viewers for their helpful advice and hard work. We 
are grateful for generous funding from Motorola 
and NSF HCC 0705901. 
43
References  
M. Argyle and M. Cook. 1976. Gaze and Mutual Gaze. 
Cambridge University Press, New York. 
E. Bard, A. Anderson, C. Sotillo, M. Aylett, G. Doher-
ty-Sneddon and A. Newlands. 2000. Controlling the 
intelligibility of referring expressions in dialogue. J. 
Memory and Language, 42(1):1-22. 
N. Bos, J. Olson, D. Gergle, G. Olson, and Z. Wright. 
2002. Effects of four computer-mediated communica-
tion channels on trust development. In Proceedings of 
SIGCHI 2002, pages 135-140, Minneapolis, MN. 
H. P. Branigan, J. F. McLean, and H. Reeve. 2003. 
Something old, something new: Addressee knowledge 
and the given-new contract. In Proceedings of the 
25th Annual Conference of the Cognitive Science So-
ciety, pages 180-185, Boston, MA. 
B. Brighton, M. Fujiki, and E. Sonnenberg. 1988. Res-
ponses to requests for clarification by linguistically 
normal and language-impaired children in conversa-
tion. J. Speech and Hearing Disorders, 53:383-391. 
P. Brown and S. C. Levinson. 1987. Politeness: Some 
Universals in Language Usage. Cambridge Universi-
ty Press, Cambridge, UK. 
J. Cassell, A. J. Gill, and P. Tepper. 2007. Coordination 
in conversation and rapport. In Proceedings of the 
Workshop on Embodied Language Processing at 
ACL, pages 41-50, Prague. 
H. H. Clark and E. F. Schaeffer. 1989. Contributing to 
discourse. Cognitive Science,13:259-294.  
M. G. Core and J. F. Allen. 1997. Coding dialogs with 
the DAMSL annotation scheme. In Proceedings of 
AAAI Fall Symposium on Communicative Action in 
Humans and Machines, pages 28-35, Boston, MA. 
H. Dalianis. 1999. Aggregation in natural language gen-
eration. J. Computational Intelligence, 15(4):384-414. 
S. R. Fussell and R. M. Krauss. 1989 a. The effects of 
intended audience on message production and com-
prehension: Reference in a common ground frame-
work. European J. Social Psychology, 25:203-219. 
S. R. Fussell and R. M. Kraus. 1989 b. Understanding 
friends and strangers: The effects of audience design 
on message comprehension. European J. Social Psy-
chology, 19:445-454. 
C. Goodwin. 1981. Conversational Organization: Inte-
raction between Speakers and Hearers. Academic 
Press, New York. 
H. P. Grice. 1975. Logic and conversation. In P. Cole 
and J. Morgan, editors, Syntax and Semantics III ? 
Speech Acts. Academic Press,New York, pages 41-58. 
G. Hirst, S. McRoy, P. Heeman, P. Edmonds, and D. 
Horton. 1994. Repairing conversational misunders-
tandings and non-understandings. Speech Communi-
cation, 15: 213-230. 
L. Horn. 1993. Economy and redundancy in a dualistic 
model of natural language. SKY 1993: Yearbook of 
the Linguistic Association of Finland: 33-72. 
A. Horning. 1991. Readable writing: The role of cohe-
sion and redundancy. J. Advanced Composition, 
11:135-145. 
G. Hornstein. 1985. Intimacy in conversational style as 
a function of the degree of closeness between mem-
bers of a dyad. J. Personality and Social Psychology, 
49(3):671-681. 
W. Horton and B. Keysar. 1996. When do speakers take 
into account common ground? Cognition, 59:91?117. 
S. Kopp, P. Tepper, K. Ferriman, K. Striegnitz and J. 
Cassell. 2008. Trading spaces: How humans and hu-
manoids use speech and gesture to give directions. In 
T. Nishida, editor, Conversational Informatics. John 
Wiley & Sons, New York, pages 133-160. 
B. Lindlom. 1990. Explaining phonetic variation: A 
sketch of the H and H theory. In W. Hardcastle & A. 
Marchal, editors, Speech Production and Speech 
Modeling. Kluwer, Dordrecht, pages 403-439. 
D. Morrow, A. Lee, and M. Rodvold. 1993. Analysis of 
problems in routine controller-pilot communication. 
International J. Aviation Psychology. 3(4): 285-302. 
Y. Nakano, G. Reinstein, T. Stocky, and J. Cassell. 
2003. Towards a model of face-to-face grounding. In 
Proceedings of ACL 2003, pages 553-561, Sapporo, 
Japan. 
S. Planalp and A. Benson. 1992. Friends' and acquain-
tances' conversations I: Perceived differences. J. So-
cial and Personal Relationships, 9:483-506.  
B. Schlenker. 1984. Identities, identifications, and rela-
tionships. In V. Derlega, editor, Communication, In-
timacy and Close Relationships. Academic Press, 
New York, pages 71-104. 
R. Stalnaker. 1978. Assertion. In P. Cole, editor, Syntax 
and Semantics, Volume 9: Pragmatics. Academic 
Press, New York, pages 315-332. 
G. Svendsen and B. Evjemo. 2003. Implicit referring as 
an indication of familiarity in face-to-face and phone 
conversations. In Proceedings of INTERACT '03: 
pages 920-923, Zurich.  
M. Theune, D. Hofs and M. van Kessel. 2007. The Vir-
tual guide: A direction giving embodied conversa-
tional agent. In Proceedings of Interspeech 2007, 
pages 2197-2200, Antwerp, Belgium. 
L. Tickle-Degnen and R. Rosenthal. 1990. The nature of 
rapport and its nonverbal correlates. Psychological 
Inquiry, 1(4):285-293. 
M. A. Walker. 1992. Redundancy in collaborative di-
alogue. In Proceedings of the 14th International Con-
ference on Computational Linguistics, pages 345-351, 
Nantes, France. 
M. A. Walker. 1996. The effect of resource limits and 
task complexity on collaborative planning in dialo-
gue. Artificial Intelligence Journal, 85:181-243. 
44
Appendix: Examples from Dialogues 
 
In the following examples, utterances in italics are 
the triggers produced by the receiver, and under-
lined utterances are redundant.  Commas indicate 
pauses.  Receiver utterances in square brackets 
overlap with the portion of the preceding giver ut-
terance in brackets.   
 
 
Question Example 
Giver (G): as soon as you come outta the door, 
uhh on the second floor you?ll [see like a win-
dow] in front of you 
Receiver (R): [mmhm] 
G: [and then], you?ll wanna take a left 
R: [hm] 
? 
G: if you look to your left you?ll see the exit sign, 
uhh with for the stairwell 
R: ok so then I go to this second floor 
G: mmhm 
R: and then do I go right? 
G: no 
R: or left? 
G: you go left [once you come outta] the second 
floor 
R: [you go left] 
 
 
Incorrect Statement Example 
G: and you?re gonna go towards the computer, and 
pass the computer, and there will be, copy ma-
chines on your right after you pass the computer  
R: mhmm  
G: so after you, walk, just past the copy machines 
you?re gonna want to take a hard left, almost like 
a U-turn 
? 
G: once you turn to the right at after the first stairs 
you?ll you?ll see a computer 
R: oh a computer right ok and then I?m gonna take 
a really hard left like a U-turn 
G: right well you go past the computer and then 
you?ll see copying machines 
R: oh ok  
G: and then but, the copy machines are like maybe 
three five feet after the computer 
R: ok 
G: and then that?s when you take the hard left 
 
Abandoned Example 
G: and then you?re gonna hear some kids and 
people talking and stuff, you?re gonna be head-
ing toward the clinic 
R: oh okay 
G: okay, the clinic you?re is gonna come up on 
your right, [there?s gonna] be, kind of, semi cir-
cular blue couches  
R: [okay], uhhuh 
G: down there, the stapler, is on the floor, right 
next to a pillar, [um] so basically you?re gonna 
like, you?re gonna kind of, turn right to look into 
the clinic 
R: [okay], okay 
G: and then, the stapler?s kinda just over there to 
the left, on the floor by one of the pillars 
? 
G: and you?re gonna hear people talking and 
there?s gonna [be kids] 
R: [okay] so and then the, pillar its? like gonna be 
one of the pillars on the, right by like I guess it?s 
on the  
G: basically, basically um you walk into, the clin-
ic, and there?s blue, couches 
R: mmhm 
G: and then it?s just a little bit over to the left 
R: oh okay 
G: on the floor 
 
 
No-Trigger Example 
G: open the door, and you?re gonna see a set of 
stairs 
R: okay  
G: go down those stairs, to the second floor 
R: mmhm 
G so you?re gonna be on the third floor, you?re 
gonna then you?re gonna take the stairs down to 
the second floor 
R: okay 
45
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 40?48,
Utica, May 2012. c?2012 Association for Computational Linguistics
Perceptions of Alignment and Personality in Generated Dialogue
Alastair J. Gill
University of Surrey
Guildford GU2 7XH, UK
A.Gill@surrey.ac.uk
Carsten Brockmann and Jon Oberlander
University of Edinburgh
Edinburgh EH8 9AB, UK
Carsten.Brockmann@gmx.net
J.Oberlander@ed.ac.uk
Abstract
Variation in language style can lead to differ-
ent perceptions of the interaction, and differ-
ent behaviour outcomes. Using the CRAG 2
language generation system we examine how
accurately judges can perceive character per-
sonality from short, automatically generated
dialogues, and how alignment (similarity be-
tween speakers) alters judge perceptions of the
characters? relationship. Whilst personality
perception of our dialogues is consistent with
perceptions of human behaviour, we find that
the introduction of alignment leads to nega-
tive perceptions of the dialogues and the inter-
locutors? relationship. A follow up evaluation
study of the perceptions of different forms of
alignment in the dialogues reveals that while
similarity at polarity, topic and construction
levels is viewed positively, similarity at the
word level is regarded negatively. We discuss
our findings in relation to the literature and in
the context of dialogue systems.
1 Introduction
Personality describes characteristics which are cen-
tral to human behaviour, and has implications for
social interactions: It can affect performance on col-
laborative processes, and can increase engagement
when incorporated within virtual agents (Hernault
et al, 2008). In addition, personality has also been
shown to influence linguistic style, both in written
and spoken language (Pennebaker and King, 1999;
Gill and Oberlander, 2002). Whilst individuals of-
ten possess individual styles of self-expression, such
as those influenced by personality, in a conversation
they may align or match the linguistic style of their
partner: For example, by entraining, or converging,
on a mutual vocabulary. Such alignment is associ-
ated with increased familiarity, trust, and task suc-
cess (Shepard et al, 2001). People also adjust their
linguistic styles when interacting with computers,
and this affects their perceptions of the interaction
(Porzel et al, 2006). However, when humans ? or
machines ? are faced with a choice of matching the
language of their conversational partner, this often
raises a conflict: matching the language of an in-
terlocutor may mean subduing one?s own linguistic
style. Better understanding these processes relating
to language choice and interpersonal perception can
inform our knowledge of human behaviour, but also
have important implications for the design of dia-
logue systems and user interfaces.
In this paper, we present and evaluate novel
automated natural language generation techniques,
via the Critical Agent Dialogue system version 2
(CRAG 2), which enable us to generate dynamic,
short-term alignment effects along with stable, long-
term personality effects. We use it to investigate
the following questions: Can personality be accu-
rately judged from short, automatically generated di-
alogues? What are the effects of alignment between
characters? How is the quality of the characters? re-
lationship perceived? Additionally, in our evaluation
study we examine perceptions of the different forms
of alignment present in the dialogues, for example at
the word, phrase or polarity levels. In the following
we review relevant literature, before describing the
CRAG 2 system and experimental method, and then
presenting our results and discussion.
40
2 Background
Researchers from several traditions have studied as-
pects of similarity in dialogue, naming it: entrain-
ment, alignment, priming, accommodation, coordi-
nation or convergence. For current purposes, we
gloss over some important differences, and borrow
the term ?alignment?, because we will go on to adopt
Pickering and Garrod?s theoretical mechanisms in
our system. Alignment usually means that if some-
thing has happened once in a dialogue (for instance,
referring to an object as a vase), it is likely to happen
again?and hence, alternatives become less likely
(for instance, referring to the same object as a jug)
(Pickering and Garrod, 2004). From this view, inter-
locutors align the representations they use in produc-
tion and comprehension and the process is an auto-
matic, labour-saving device, but there are of course
limits to periods over which alignment processes op-
erate; in corpus studies long-term adaptation pre-
dicts communicative success (Reitter, 2008). Al-
ternative approaches view similarity as a process of
negotiation leading to the establishment of common
ground (Brennan and Clark, 1996), or a relatively
conscious process resulting from attraction (Shepard
et al, 2001). Although increased similarity (conver-
gence) is generally regarded positively, it can some-
times arise during disagreement (Niederhoffer and
Pennebaker, 2002), with cultural differences influ-
encing both convergence and perceptions of others
(Bortfeld and Brennan, 1997). Wizard-of-Oz stud-
ies have also shown convergence with a natural lan-
guage interface (Brennan, 1996; Porzel et al, 2006).
Embodied conversational agents (Cassell et al,
2000) are implemented computer characters that ex-
hibit multimodal behaviour; the technology can be
exploited to give life to automatically generated
scripted dialogues and to make them more engag-
ing (van Deemter et al, 2008; Hernault et al, 2008).
Aspects of the agents? personalities and their inter-
ests can be pre-configured and affect their dialogue
strategies; the generation is template-based. A com-
mon way to describe personality is using the Big
Five traits: Extraversion (preference for, and behav-
ior in, social situations); Neuroticism (tendency to
experience negative thoughts and feelings); Open-
ness (reflects openness to new ideas); Agreeableness
(how we tend to interact with others); and Consci-
entiousness (how organised and persistent we are in
pursuing our goals). Relationships between person-
ality dimensions and language use appear to be ro-
bust: For instance, in monological writing (essays
and e-mails) high Extraverts use more social words,
positive emotion words, and express more certainty;
high Agreeableness scorers use more first person
singular and positive emotion words, and fewer ar-
ticles and negative emotion words (Pennebaker and
King, 1999; Gill and Oberlander, 2002).
Personality can not only be projected through, but
also perceived from, asynchronous textual commu-
nication. The extraversion dimension is generally
perceived most accurately in a variety of contexts,
while it was more difficult for raters to recognise
neuroticism (Gill et al, 2006; Li and Chignell, 2010)
Taking into account the difference between the lan-
guage actually used by people with certain person-
ality, and the language which others expect them
to use, natural language generation (NLG) systems
can exploit either to project personality. Perhaps the
closest previous work to what we present here is the
Personality Generator (PERSONAGE) (Mairesse and
Walker, 2010) which mapped psychological find-
ings relating to the personality to the components
of the NLG system (e.g., content planning, sen-
tence planning and realisation). Evaluation by hu-
man raters showed similar accuracy in perception
of extraversion in the generated language compared
with human-authored texts. There is evidence that
computer users attribute personality to interfaces,
and rate more highly those interfaces that exploit
language associated with the user?s own personal-
ity, and become more similar to the user over time
(Isbister and Nass, 2000).
We now turn to describing our automated natu-
ral language generation techniques, implemented in
CRAG 2, followed by a description of our experi-
mental method and evaluation.
3 Generation Method
Dialogues are composed by CRAG 2, a Java pro-
gram that provides a framework for generating dia-
logues between two computer characters discussing
a movie. For more details of this system, see Brock-
mann (2009). Within CRAG 2, linguistic personal-
ity and alignment are modelled using the OPENNLP
41
CCG Library (OPENCCG) natural language realiser
(White, 2006b). The realiser consults a grammar
adapted to the movie review domain to allow the
generation of utterances about the following top-
ics: Action scenes, characters, dialogue, film, music,
plot or special effects. The realiser also has access
to a set of n-gram language models, used to com-
pute probability scores of word sequences. The gen-
eral conversational language model (LM) is based
on data from the SWITCHBOARD corpus and a small
corpus of movie reviews. The general LM is used for
fallback probabilities, and is integrated with the per-
sonality and alignment language models (described
below) using linear interpolation.
3.1 Personality Models
Language models were trained on a corpus of web-
logs from authors of known personality (Nowson et
al., 2005). For each personality dimension, the lan-
guage data were divided up into high, medium and
low bands so that the probability of a word sequence
given a personality type could be derived; see Now-
son et al (2005) for further discussion of the pos-
itively skewed distribution of the openness dimen-
sion in bloggers. Each individual weblog was used
5 times, once for each dimension. The five models
corresponding to the character?s assigned personal-
ity are uniformly interpolated to give the final per-
sonality model, which is then combined with the
general model (respective weights, 0.7 and 0.3).
3.2 Alignment via Cache Language Models
Meanwhile, alignment is modelled via cache lan-
guage models (CLMs). For each utterance to be
generated, a language model is computed based on
the utterance that was generated immediately before
it. This CLM is then combined with the personality
LM. A character?s propensity to align corresponds
to the weight given to the CLM during this combi-
nation, and can be set to a value between 0 and 1.
3.3 Character Specification and Dialogue
Generation
The characters are parameterised for their per-
sonality by specifying values (on a scale from
0 to 100) for the five dimensions: extraver-
sion (E), neuroticism (N), agreeableness (A),
conscientiousness (C) and openness (O). This pa-
rameterisation determines the extent to which utter-
ances are weighted for their overlap with the per-
sonality generation model for each trait. Also, each
character receives an agenda of topics they wish
to discuss, along with polarities (POSITIVE/NEGA-
TIVE) that indicate their opinion on each topic.
The character with the higher E score begins the
dialogue, and their first topic is selected. Once
an utterance has been generated, the other charac-
ter is selected, and the system selects which topic
should come next. This process continues until
there are no topics left on the agenda of the cur-
rent speaker. The system creates a simple XML
representation of the character?s utterance, using
the specified topic and polarity. Following the
method described in Foster and White (2004), the
basic utterance specification is transformed, using
stylesheets written in the Extensible Stylesheet Lan-
guage Transformations (XSLT) language, into an
OPENCCG logical form. We make use of the fa-
cility for defining optional and alternative inputs
(White, 2006a) and underspecified semantics to
mildly overgenerate candidate utterances.
Optional interjections (I mean, you know, sort of )
and conversational markers (right, but, and, well)
are added where appropriate given the discourse his-
tory. Using synonyms (e.g., plot = story, comedy =
humour) and combining sentence types and optional
expressions, up to 3000 possibilities are created per
utterance, and the best candidate is chosen by the
specific combination of n-gram models appropriate
for dialogue history, personality and alignment.
4 Experimental Method
4.1 Participants
Data were collected from 80 participants with a va-
riety of educational and occupational backgrounds
using an online study (via the Language Experi-
ments Portal; www.language-experiments.org). To
ensure integrity of responses, submissions taking
less than five minutes (five cases), or more than 45
minutes (one case) were examined in relation to the
other responses before being included in the analy-
sis. The demographics were as follows: 43 partici-
pants (54%) were native, and 37 (46%) non-native,
speakers of English; 34 (42%) male, 46 (58%) fe-
42
Personality Par- Propen-
Dialogue ameter Setting sity to
Type Character E N A C O Align
1) High E I 75 50 25 25 50 0
vs. Low E II 25 50 75 75 50 0 or 0.7
2) Low E I 25 50 25 25 50 0
vs. High E II 75 50 75 75 50 0 or 0.7
3) High N I 50 75 25 25 50 0
vs. Low N II 50 25 75 75 50 0 or 0.7
4) Low N I 50 25 25 25 50 0
vs. High N II 50 75 75 75 50 0 or 0.7
Table 1: Dialogue type parameter settings.
male. Median age range was 25?29 (mode = 20?
24). Other demographic information (right/left-
handedness, area of upbringing, occupation) were
collected, but are not considered here.
4.2 Materials
To be able to compare human judges? perceptions
of characters demonstrating different personalities,
and dialogues without and with alignment, dialogues
were generated in four different dialogue types, as
shown in Table 1. Each dialogue type sets the two
computer characters to opposing extremes on either
the E or the N dimension, while keeping the respec-
tive other dimension at a middle, or neutral, level
(for example, in Dialogue Type 1, Character I is
High E, Character II is Low E, and both charac-
ters are Mid N). Furthermore, Character I is always
Low A and C, and Character II is always High A and
C. All characters are set to Mid O.
Two dialogues were generated per type, giving a
total of 8 dialogues, with aligning versions of each of
these dialogues subsequently generated (giving 16
dialogues in total). The movie under discussion and
the characters? respective agendas and their opinions
about the topics were randomly assigned. Each dia-
logue was eight utterances long, with characters tak-
ing turns, each of them producing four utterances
altogether. In each alignment dialogue, the High
A/High C Character II aligned. The weight for the
cache language model was set to 0.7. In both align-
ing and non-aligning versions of the dialogues, ut-
terances for the non-aligning speaker were the same.
The generation of utterances for the aligning speaker
was seeded with the respective previous utterance
functioning as the dialogue history. From the list
of generated possible utterances, the top-ranked ut-
terance was chosen.
4.2.1 Example Dialogue
To give an impression of the generated dialogues,
Table 2 shows an example of Dialogue Type 1
(High E versus Low E) where the characters dis-
cuss the movie Mystic River (the first row of Ta-
ble 1 gives the full parameter settings). The other
generation parameters are (valence of opinions fol-
lows each topic): Character I, agenda (PLOT/?,
CHARACTERS/?, MUSIC/?, FILM/?); further opin-
ions (SPECIAL EFFECTS/?, ACTION SCENES/+,
DIALOGUE/?); Character II, agenda (ACTION
SCENES/+, SPECIAL EFFECTS/+, PLOT/?, DI-
ALOGUE/?); further opinions (CHARACTERS/?,
FILM/?, MUSIC/?).
Alignment is switched on for Character II, with
examples of their utterances from the non-aligning
dialogues included beneath. As can be seen, the
aligned utterances closely match the surface form
of their primes. The personality language models?
influence on language choice can be seen when the
High E Character I uses the phrase I mean I didn?t
think, while the (non-aligning) Low E Character II
prefers I (kind of) thought throughout. Finally, the
Low A/Low C Character I has a strategy of switch-
ing topics in order to follow his own agenda.
4.3 Procedure
An introductory web page explained the task and de-
scribed the four personality dimensions of interest
(E, N, A and C; adjectives relating to each personal-
ity type were included for reference after each dia-
logue). The first page also contained an example di-
alogue similar to those that would be displayed dur-
ing the experiment. The participants were then pre-
sented with eight dialogues (two per type; one with,
one without alignment). The dialogues were pre-
sented in random order and randomly chosen from
the pool of available dialogues. One complete di-
alogue was shown at a time, and the participants
were asked to judge which of the two characters
scored higher on the E, N, A and C dimensions (us-
ing the previous personality descriptions and adjec-
tives). They also answered questions probing their
43
PLOT:
I ?I mean I didn?t think that the story was great.?
II ?I mean the story wasn?t great.?
Without alignment:
?Yes I mean I thought that it wasn?t nicely tied up.?
CHARACTERS:
I ?And I mean I thought there was no characterization at all.?
II ?I mean I thought there was no characterization at all.?
Without alignment:
?Yeah there was no characterization at all I mean.?
MUSIC:
I ?I mean I didn?t think that the music was great.?
II ?I mean the music wasn?t great.?
Without alignment:
?Yeah I kind of thought that the music was also very bad
you know.?
FILM:
I ?I mean I didn?t think the film was good.?
II ?I mean the film wasn?t good.?
Without alignment:
?I mean I kind of thought that it was a bit dull.?
Table 2: Example Dialogue.
perceptions of the characters? relationship. They as-
sessed on a seven-point Likert scale how well the
characters ?got on? with each other (very badly?very
well), interpreted as indicating positivity or rapport
between characters, and how smoothly the conver-
sation went (not at all smoothly?very smoothly), in-
dicating how natural and coherent the interactions
were. The participants were asked to rate each dia-
logue independently from the others.The experiment
was open to both native and non-native speakers of
English; upon supplying an email address, partici-
pants were entered into a draw for an Amazon gift
token. All data were analysed anonymously. Note
that this is a further evaluation of data previously
presented in Brockmann (2009).
5 Experimental Results
5.1 Personality perception
To study the perception of personality in our di-
alogues, a nominal logistic regression was run on
the perception ratings obtained from the judges.
Here agreement between generated personality and
rater judgements was coded as a binary value
(agreement=1; disagreement=0), and entered into
the regression model as the dependent variable
(DV). The following independent variables (IVs)
were entered into the model: Dialogue Alignment as
a binary variable (alignment=1; no alignment=0);
Personality Trait judged as a categorical variable
(?Extraversion?, ?Neuroticism??, ?Agreeableness?,
?Conscientiousness?). We also included an inter-
action variable, Generated Alignment ? Personality
Trait Rated. We ran this model in order to under-
stand how each of the independent variables, such
as Personality Trait judged, or combinations of vari-
ables (in the case of the interactions) best explain the
accuracy of the personality perception judgements
relative to our generated personality language (the
DV). Throughout this section we report the parame-
ter estimates and corresponding one degree of free-
dom for the more conservative Likelihood Ratio Chi
Square effect tests for N=1920 (with the exception
of the four-level variable, Personality Trait DF=3,
and Participant ID DF=79).
The whole model is significant (?2 = 128.22,
p < .0041, R Square (U)= .05; although note that
R Square (U) is not comparable to regular R Square,
and therefore cannot be interpreted as a percentage
of variance explained; model DF= 89). To investi-
gate effects of native/non-native speaker effects on
personality judgement accuracy, this variable was
included in earlier models as a binary variable (Na-
tive Speaker: native=1; non-native=0), but no sig-
nificant effect was found (?2 = 0.98, p = .3228).
Therefore data from all participants are included in
the analyses here, and the native/non-native variable
is not included in the model. For the interactions,
there is a significant relationship between Dialogue
Alignment and accuracy in judgement of Personal-
ity Trait (?2 = 13.67, p = .0034). Further exami-
nation of this relationship shows that in the case of
Agreeableness, accuracy decreases when alignment
is present in the dialogue (?2 = 10.90, p = .0010),
whereas in the case of Conscientiousness, percep-
tion accuracy significantly increases with alignment
(?2 = 4.38, p= .0364). This is shown in Figure 1.
There is a significant main effect for Personal-
ity Trait judged (?2 = 17.04, p = .0007): param-
eter estimates show that accuracy of judgement is
significantly more accurate for Extraversion (?2 =
7.21, p = .0073), but less accurate for Agreeable-
ness (?2 = 5.54, p = .0186) and Conscientiousness
(?2 = 8.09, p = .0044). No main effect was found
for Dialogue Alignment relative to accuracy of per-
sonality judgement (?2 = 2.16, p= .1420).
44
A C E N
Personality Trait
Agr
eem
ent 
(Ge
ner
ated
 Pe
rson
ality
 vs.
 Ra
ter 
Jud
gem
ents
)
0.0
0.2
0.4
0.6
0.8
1.0 ?
?
No AlignmentAlignment
Figure 1: Accuracy of personality judgements.
5.2 Ratings of ?Getting on? and ?Smoothness?
In the following we are interested in examining what
dialogue characteristics lead to the rater judgements
of ?getting on?. Using an ordinal logistic regression
(DV: how well the characters were judged to ?get
on?, seven point scale from ?very badly? to ?very
well?) the following independent variables, coded as
described in the previous section, were entered into
the model: Dialogue Alignment and Native Speaker
(Personality Trait was also entered into the model,
but did not reach significance). Participant ID was
included in the model to account for the repeated
measures design. Again, we use likelihood ratio
effect tests and note parameter estimates for one
degree of freedom (N=2560). The whole model
is significant (?2 = 1396.75, p < .0001, R Square
(U)= .15; model DF=89): A main effect for Dia-
logue Alignment (?2 = 244.94, p < .0001), shows
alignment decreased perceptions of ?getting on?.
Similarly, ordinal logistic regressions were used
to probe influencing factors in decisions of rating
dialogue smoothness (DV: smoothness rated on a
seven point scale from ?not at all smoothly? to ?very
smoothly?). The following independent variables,
coded as described in the previous section, were en-
tered into the model: Dialogue Alignment and Na-
tive Speaker (again Personality Trait did not reach
significance for inclusion). Again, Participant ID
was included in the model to account for the re-
peated measures design (parameter estimates and
likelihood ratio effect tests are for one degree of
freedom, N=2560, Condition, DF=3; Participant
ID, DF=78). The whole model is significant (?2 =
1291.28, p < .0001), with an R Square (U) of 0.13
(model DF=89). There are strong main effects for
Dialogue Alignment (?2 = 188.27, p < .0001), and
Native Speaker (?2= 110.00, p< .0001). Examina-
tion of the parameter estimates reveals negative rela-
tionships between ratings of smoothness and Native
Speaker, and Dialogue Alignment, implying that na-
tive speakers significantly rated the dialogues as be-
ing less smooth than the non-native speakers, and
also that dialogues with alignment were rated sig-
nificantly less smooth than those without alignment.
6 Evaluation Method
To better understand the linguistic alignment pro-
cesses which drive the participants? judgements in
the previous experiment, we performed further anal-
ysis. In particular, we coded the forms of alignment
present in each utterance of each dialogue, relative
to the previous utterance. The forms of alignment
were coded as follows: Polarity (matching a posi-
tive or negative opinion), Topic (whether the topic is
the same or shifts), Word (instances of alignment of
individual words of the previous utterance), Phrase
(alignment of phrases), Construction (alignment at
a grammatical construction level). Each instance of
alignment for a given utterance was counted, with
an overall score generated for the whole dialogue.
This coding procedure was performed by one re-
searcher and subsequently evaluated by a second,
with disputes resolved by mutual agreement. In the
following analysis we do not distinguish between di-
alogues intentionally generated with alignment and
those without, but instead include all dialogues in
the analysis to examine which objectively measured
forms of alignment relate to the judges? perceptions
for personality, ?getting on? and ?smoothness?.
7 Evaluation Results
7.1 Alignment Forms and Personality
Accuracy of judgements of personality ratings and
dialogue alignment was analysed for each of the four
45
personality traits (A, C, E, N) independently using
nominal logistic regression (DV: rater vs. gener-
ated personality agreement coded 0 or 1; IVs: occur-
rence scores for Polarity, Topic, Word, Phrase, and
Construction). For Agreeableness the whole model
is significant (?2 = 85.74, p < .0001, R Square
(U)= .10; model DF=5, N=640), with Topic align-
ment (?2 = 16.68, p < .0001), followed by Polar-
ity (?2 = 10.13, p= .0015) and Construction (?2 =
6.19, p = .0128) alignment all positively related to
perceptions of Agreeableness. For Conscientious-
ness (whole model ?2 = 11.26, p= .0465, R Square
(U)= .01; DF=5, N=640), Polarity alignment is in-
versely related to perceptions of Conscientiousness
(?2 = 5.12, p = .0236). In the case of Neuroti-
cism and Extraversion, the models are not significant
(?2 = 5.37, p = .3719, and ?2 = 1.49, p = .2226,
respectively; both DF=5, N=320).
7.2 Alignment Forms and ?Getting On? and
?Smoothness?
The relationship between the different forms of
alignment present in the dialogues and the judges?
ratings of ?getting on? and ?smoothness? were eval-
uated in two separate ordinal logistic models, in
which they were entered as the dependent variable.
The five alignment types (Polarity, Topic, Word,
Phrase, and Construction) were entered as indepen-
dent variables. Participant ID was also entered into
the model as an independent variable, since multiple
responses were collected from each participant.
Ratings of ?getting on? (whole model ?2 =
1595.10, p < .0001, R Square (U)= .17; DF=84,
N=2560) show that Polarity (?2 = 385.45, p <
.0001), Construction (?2 = 72.30, p < .0001) and
Topic (?2= 16.68, p= .0014) alignment all relate to
greater scores of perceived getting on. Conversely,
Word alignment leads to reduced scores of perceived
getting on (?2 = 14.13, p = .0002). For ratings of
dialogue ?smoothness? (?2 = 1519.31, p= .0014, R
Square (U)= .16; DF=84, N=2560), again Polarity
(?2 = 209.55, p < .0001), Topic (?2 = 39.39, p <
.0001) and Construction (?2 = 28.01, p < .0001)
alignment all lead to increased ratings of ?smooth-
ness?. Similarly, Word alignment has a negative
impact upon perceptions of dialogue ?smoothness?
(?2 = 29.24, p < .0001).
8 Discussion
We now discuss the perception and evaluation re-
sults of the CRAG 2 system in greater detail. In
terms of personality perception, extraversion is ac-
curately perceived, with agreeableness and consci-
entiousness less so, which matches findings from
personality perception studies in other contexts, in-
cluding text based computer-mediated communica-
tion (Li and Chignell, 2010; Gill et al, 2006). It
is interesting to note, however, that alignment helps
perception of conscientiousness, but hurts ratings of
agreeableness. Reduced accuracy in perception of
agreeableness, which is important to relationships,
may have a negative impact on the use of dialogues
in collaborative settings (Rammstedt and Schupp,
2008). Further work could usefully examine ways in
which these characteristics can be generated in more
readily perceptible ways. Interestingly, personality
perception is unaffected by whether the judges are
native English speakers or not. This is a notable
finding, and apparently implies that the social infor-
mation relating to personality is available in the text
only environment, or through the generation pro-
cess, it is equally accessible to native and non-native
English speakers. Native speaking judges were more
critical in rating dialogue smoothness and characters
getting on, perhaps indicating a finer-grained aware-
ness of linguistic cues in interpersonal interaction,
or else just greater confidence in making negative
judgements of their native language.
Our finding that our generated alignment actually
decreases the perceived positivity of the relationship
is contrary to what is generally predicted by the lit-
erature (Brennan and Clark, 1996; Shepard et al,
2001; Pickering and Garrod, 2004); but cf. Nieder-
hoffer and Pennebaker (2002). Likewise, we would
also have expected the dialogues with alignment to
have been perceived to have gone more smoothly.
However, in our evaluation of the different types
of alignment, we note that alignment per se is not
necessarily a bad thing: Generally alignment of Po-
larity, Topic, and Construction are seen positively
leading to higher ratings of ?getting on?, ?smooth-
ness?, and increased accurate perception of Agree-
ableness; repetition of individual words is however
viewed negatively, and leads to lower ratings of ?get-
ting on? and ?smoothness?.
46
There are a number of possible explanations for
these negative responses to our generated dialogue
alignment. They hinge on understanding what is
involved in generating alignment, or similar be-
haviour, in dialogue participants. First, it could be
that our dialogues encode the ?wrong? type of simi-
larity. For example, the alignment and entrainment
approaches to similarity usually study task-based di-
alogues, which often focus on establishing a shared
vocabulary for referencing objects (i.e., at the word
level). In such cases, the similarity arises either
through priming mechanisms, or the establishment
of common ground. Given that we used an align-
ment model to generate similarity in our dialogues,
this kind of repetition or similarity may seem incon-
gruent or out of place in dialogues that are not task-
based (cf. negative impact of word-level alignment).
A second explanation might be that similarity re-
lates to positive outcomes when it occurs over a
longer, rather than shorter, period of time (Reit-
ter, 2008). In the current study the dialogues con-
sisted of eight turns, thus similarity was not gener-
ated over a long period. Indeed, linguistic similarity
over a longer period of time may be more consis-
tent with perceptions of social similarity, such as in-
group, rather than outgroup, membership (Shepard
et al, 2001). Indeed, in such contexts word choice
is an important feature in dialogue and would be use-
ful to incorporate into a dialogue model to simulate
ingroup membership.
Third, in communication accommodation theory
it is ?convergence? ? the process of increasing sim-
ilarity between interlocutors ? which is important,
rather than similarity alone. In the current study,
convergence was not examined since the dialogues
were generated with static levels of alignment.
So how do these findings relate back to the area of
dialogue generation for applied contexts? Similarly
to findings for the PERSONAGE system (Mairesse
and Walker, 2010), personality in our generated di-
alogues is perceived with similar accuracy to the
way humans perceive personality of other humans.
This suggests that our CRAG 2 system can create
believable characters to whom the user can poten-
tially relate while auditing the dialogues, or using a
dialogue-based interface. That alignment can have
negative effects on dialogue perception we propose
is due to the form of alignment depicted in these gen-
erated dialogues (i.e., task-based nature emphasising
similarity at the word level), rather than alignment in
general. We do not take this result to necessarily in-
dicate that alignment in generated dialogues should
be avoided. Rather, its implementation should be
carefully considered, especially to ensure that the
form of similarity achieved makes sense in the com-
municative context. Indeed, as we show in the eval-
uation of the generated dialogues, alignment at the
Polarity, Topic, and Construction levels is gener-
ally viewed positively, however in contrast align-
ment at the Word level tends to be viewed more neg-
atively. One of the key suggestions arising from this
study is that the different forms of dialogue simi-
larity cannot simply be used interchangeably, with
alignment found in task-based dialogues which may
include many instances of word-level repetition and
alignment not necessarily appropriate in non-task
dialogues, and thus not automatically resulting in
perceptions of positivity. We note that non-native
speakers were more forgiving in their ratings of the
dialogues containing alignment. Given that they
were equally able to perceive the personality of the
characters, this may be due to non-native speakers
having fewer expectations of alignment behaviour
in dialogue. Indeed in some contexts, greater align-
ment, and thus repetition, may be beneficial for non-
native speakers auditing dialogues.
To conclude, personality in our generated dia-
logues was perceived with comparable accuracy to
human texts, but alignment or similarity between
speakers ? especially at the word level ? regarded
negatively. We would like to see future work exam-
ine further the responses to different forms of align-
ment, including convergence, in generated dialogue.
9 Acknowledgements
We acknowledge Edinburgh-Stanford Link funding,
and the partial support of the Future and Emerging
Technologies programme FP7-COSI-ICT of the Eu-
ropean Commission (project QLectives, grant no.:
231200). We thank Amy Isard, Scott Nowson and
Michael White for their assistance in this work. A
version of the paper was presented at the Twentieth
Society for Text and Discourse conference; thanks
to Herb Clark, Max Louwerse and Michael Schober
for their insights regarding linguistic similarity.
47
References
[Bortfeld and Brennan1997] H. Bortfeld and S. E. Bren-
nan. 1997. Use and acquisition of idiomatic expres-
sions in referring by native and non-native speakers.
Discourse Processes, 23:119?147.
[Brennan and Clark1996] Susan E. Brennan and Her-
bert H. Clark. 1996. Conceptual pacts and lexi-
cal choice in conversation. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
22(6):1482?1493, November.
[Brennan1996] Susan E. Brennan. 1996. Lexical entrain-
ment in spontaneous dialog. In International Sympo-
sium on Spoken Dialog, pages 41?44.
[Brockmann2009] Carsten Brockmann. 2009. Personal-
ity and Alignment Processes in Dialogue: Towards a
Lexically-Based Unified Model. Ph.D. thesis, Univer-
sity of Edinburgh, UK.
[Cassell et al2000] Justine Cassell, Joseph Sullivan, Scott
Prevost, and Elizabeth Churchill, editors. 2000. Em-
bodied Conversational Agents. MIT Press, Cam-
bridge, MA, USA.
[Foster and White2004] Mary Ellen Foster and Michael
White. 2004. Techniques for text planning with
XSLT. In Proceedings of the 4th Workshop on NLP
and XML (NLPXML-04) at the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04), pages 1?8, Barcelona, Spain.
[Gill and Oberlander2002] Alastair J. Gill and Jon Ober-
lander. 2002. Taking care of the linguistic features of
extraversion. In Proceedings of the 24th Annual Con-
ference of the Cognitive Science Society (CogSci2002),
pages 363?368, Fairfax, VA, USA.
[Gill et al2006] Alastair J. Gill, Jon Oberlander, and Eliz-
abeth Austin. 2006. Rating e-mail personality at zero
acquaintance. Personality and Individual Differences,
40(3):497?507.
[Hernault et al2008] Hugo Hernault, Paul Piwek, Helmut
Prendinger, and Mitsuru Ishizuka. 2008. Generating
dialogues for virtual agents using nested textual coher-
ence relations. In Proceedings of Intelligent Virtual
Agents, pages 139?145.
[Isbister and Nass2000] Katherine Isbister and Clifford
Nass. 2000. Consistency of personality in inter-
active characters: verbal cues, non-verbal cues, and
user characteristics. International Journal of Human?
Computer Studies, 53(2):251?267.
[Li and Chignell2010] J. Li and M. Chignell. 2010. Birds
of a feather: How personality influences blog writ-
ing and reading. Int. J. Human-Computer Studies,
68:589?602.
[Mairesse and Walker2010] Franc?ois Mairesse and Mari-
lyn Walker. 2010. Towards personality-based user
adaptation: Psychologically informed stylistic lan-
guage generation. User Modeling and User-Adapted
Interaction, 20(3):227?278.
[Niederhoffer and Pennebaker2002] Kate G. Niederhof-
fer and James W. Pennebaker. 2002. Linguistic style
matching in social interaction. Journal of Language
and Social Psychology, 21(4):337?360.
[Nowson et al2005] S. Nowson, J. Oberlander, and A.J.
Gill. 2005. Weblogs, genres and individual differ-
ences. In Proceedings of the 27th Annual Conference
of the Cognitive Science Society, pages 1666?1671.
[Pennebaker and King1999] James W. Pennebaker and
Laura A. King. 1999. Linguistic styles: Language
use as an individual difference. Journal of Personality
and Social Psychology, 77(6):1296?1312.
[Pickering and Garrod2004] Martin J. Pickering and Si-
mon Garrod. 2004. Toward a mechanistic psychol-
ogy of dialogue. Behavioral and Brain Sciences,
27(2):169?225.
[Porzel et al2006] Robert Porzel, Annika Scheffler, and
Rainer Malaka. 2006. How entrainment increases di-
alogical efficiency. In Proceedings of Workshop on on
Effective Multimodal Dialogue Interfaces.
[Rammstedt and Schupp2008] Beatrice Rammstedt and
Ju?rgen Schupp. 2008. Only the congruent survive ?
personality similarities in couples. Personality and In-
dividual Differences, 45(6):533?535.
[Reitter2008] David Reitter. 2008. Context Effects in
Language Production: Models of Syntactic Priming in
Dialogue Corpora. Ph.D. thesis, University of Edin-
burgh, UK.
[Shepard et al2001] Carolyn A. Shepard, Howard Giles,
and Beth A. Le Poire. 2001. Communication accom-
modation theory. In W. Peter Robinson and Howard
Giles, editors, The New Handbook of Language and
Social Psychology, chapter 1.2, pages 33?56. JohnWi-
ley & Sons, Chichester, UK.
[van Deemter et al2008] Kees van Deemter, Brigitte
Krenn, Paul Piwek, Martin Klesen, Marc Schro?der,
and Stefan Baumann. 2008. Fully generated scripted
dialogue for embodied agents. Artificial Intelligence,
172(10):1219?1244.
[White2006a] Michael White. 2006a. CCG chart real-
ization from disjunctive inputs. In Proceedings of the
4th International Natural Language Generation Con-
ference (INLG-06), pages 9?16, Sydney, Australia.
[White2006b] Michael White. 2006b. Efficient realiza-
tion of coordinate structures in Combinatory Catego-
rial Grammar. Research on Language and Computa-
tion, 4(1):39?75.
48
