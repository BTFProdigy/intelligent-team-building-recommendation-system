Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 138?141,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation with Maximum Entropy
and N-gram Language Model
Wang Xinhao, Lin Xiaojun, Yu Dianhai, Tian Hao, Wu Xihong
National Laboratory on Machine Perception,
School of Electronics Engineering and Computer Science,
Peking University, China, 100871
{wangxh,linxj,yudh,tianhao,wxh}@cis.pku.edu.cn
Abstract
This paper presents the Chinese word seg-
mentation systems developed by Speech
and Hearing Research Group of Na-
tional Laboratory on Machine Perception
(NLMP) at Peking University, which were
evaluated in the third International Chi-
nese Word Segmentation Bakeoff held by
SIGHAN. The Chinese character-based
maximum entropy model, which switches
the word segmentation task to a classi-
fication task, is adopted in system de-
veloping. To integrate more linguistics
information, an n-gram language model
as well as several post processing strate-
gies are also employed. Both the closed
and open tracks regarding to all four cor-
pora MSRA, UPUC, CITYU, CKIP are
involved in our systems? evaluation, and
good performance are achieved. Espe-
cially, in the closed track on MSRA, our
system ranks 1st.
1 Introduction
Chinese word segmentation is one of the core tech-
niques in Chinese language processing and attracts
lots of research interests in recent years. Sev-
eral promising methods are proposed by previous
researchers, in which Maximum Entropy (ME)
model has turned out to be a successful way for
this task (Hwee Tou Ng et al, 2004; Jin Kiat
Low et al, 2005). By employing Maximum En-
tropy (ME) model, the Chinese word segmentation
task is regarded as a classification problem, where
each character will be classified to one of the four
classes, i.e., the beginning, middle, end of a multi-
character word and a single-character word.
However, in a high degree, ME model pays its
emphasis on Chinese characters while debases the
consideration on the relationship of the context
words. Motivated by this view, several strategies
used for reflecting the context words? relationship
and integrating more linguistics information, are
employed in our systems.
As known, an n-gram language model could ex-
press the relationship of the context words well, it
therefore as a desirable choice is imported in our
system to modify the scoring of the ME model.
An analysis on our preliminary experiments shows
the combination ambiguity is another issue that
should be specially tackled, and a division and
combination strategy is then adopted in our sys-
tem. To handle the numeral words, we also intro-
duce a number conjunction strategy. In addition,
to deal with the long organization names problem
in MSRA corpus, a post processing strategy for
organization name is presented.
The remainder of this paper is organized as fol-
lows. Section 2 describes our system in detail.
Section 3 presents the experiments and results.
And in last section, we draw our conclusions.
2 System Description
With the ME model, n-gram language model, and
several post processing strategies, our systems are
established. And detailed description on these
components are given in following subsections.
2.1 Maximum Entropy Model
The ME model used in our system is based on the
previous works (Jin Kiat Low et al, 2005; Hwee
Tou Ng et al, 2004). As mentioned above, the
ME model based word segmentation is a 4-classes
learning process. Here, we remarked four classes,
i.e. the beginning, middle, end of a multi-character
138
word and a single-character word, as b, m, e and s
respectively.
In ME model, the following features (Jin Kiat
Low et al, 2005) are selected:
a) cn (n = ?2,?1, 0, 1, 2)
b) cncn+1 (n = ?2,?1, 0, 1)
c) c?1c+1
where cn indicates the character in the left or right
position n relative to the current character c0.
For the open track especially, three extended
features are extracted with the help of an external
dictionary as follows:
d) Pu (c0)
e) L and t0
f) cnt0 (n = ?1, 0, 1)
where Pu(c0) denotes whether the current charac-
ter is a punctuation, L is the length of word W that
conjoined from the character and its context which
matching a word in the external dictionary as long
as possible. t0 is the boundary tag of the character
in W.
With the features, a ME model is trained which
could output four scores for each character with
regard to four classes. Based on scores of all char-
acters, a completely segmented semiangle matrix
can be constructed. Each element wji in this ma-
trix represents a word that starts at the ith charac-
ter and ends at jth character, and its value ME(j, i),
the score for these (j ? i+1) characters to form a
word, is calculated as follow:
ME[j, i] = ? log p(w = ci...cj)
= ? log[p(bci)p(mci+1)...
p(mcj?1)p(ecj )]
(1)
As a consequence, the optimal segmentation re-
sults corresponding to the best path with the low-
est overall score could be reached via a dynamic
programming algorithm. For example:
@?c????(I was 19 years old that year)
Table 1 shows its corresponding matrix. In this
example, the ultimate segmented result is:
@ ?c ? ???
2.2 Language Model
N-gram language model, a widely used method
in natural language processing, can represent the
context relation of words. In our systems, a bi-
gram model is integrated with ME model in the
phase of calculating the path score. In detail, the
score of a path will be modified by adding the bi-
gram of words with a weight ? at the word bound-
aries. The approach used for modifying path score
is based on the following formula.
V [j, i] = ME[j, i]
+mini?1k=1{[(V [i ? 1, k]
+?Bigram(wk,i?1, wi,j)}
(2)
where V[j,i] is the score of local best path which
ends at the jth character and the last word on the
path is wi,j = ci...cj , the parameter ? is optimized
by the test set used in the 2nd International Chi-
nese Word Segmentation Bakeoff. When scoring
the path, if one of the words wk,i?1 and wi,j is out
of the vocabulary, their bigram will backoff to the
unigram. And the unigram of the OOV word will
be calculated as:
Unigram(OOV Word) = pl (3)
where p is the minimal unigram value of words in
vocabulary; l is the length of the word acting as
a punishment factor to avoid overemphasizing the
long OOV words.
2.3 Post Processing Strategies
The analysis on preliminary experiments, where
the ME model and n-gram language model are in-
volved, lead to several post processing strategies
in developing our final systems.
2.3.1 Division and Combination Strategy
To handle the combination ambiguity issue,
we introduce a division and combination strategy
which take in use of unigram and bigram. For
each two words A and B, if their bigrams does
not exist while there exists the unigram of word
AB, then they can be conjoined as one word. For
example, ??ff(August)? and ???(revolution)?
are two segmented words, and in training set the
bigram of ??ff? and ???? is absent, while
the word ??ff??(the August Revolution)? ap-
peares, then the character string ??ff??? is
conjoined as one word. On the other hand, for a
word C which can be divided as AB, if its uni-
gram does not exit in training set, while the bigram
of its subwords A and B exits, then it will be re-
segmented. For example, Taking the word ??L
N?U?(economic system reform)? for instance,
if its corresponding unigram is absent in training
set, while the bigram of two subwords ??LN
139
@ ? c ? ? ? ?
1 2 3 4 5 6 7
@ 1 6.3180e-07
? 2 33.159 7.5801
c 3 26.401 0.0056708 5.2704
? 4 71.617 45.221 49.934 3.1001e-07
? 5 83.129 56.734 61.446 33.869 7.0559
? 6 90.021 63.625 68.337 40.760 12.525 12.534
? 7 77.497 51.101 55.813 28.236 0.0012012 10.077 10.055
Table 1: A completely segmented matrix
?(economic system)? and ?U?(reform)? exists,
as a consequence, it will be segmented into two
words ??LN?? and ?U??.
2.3.2 Numeral Word Processing Strategy
The ME model always segment a numeral
word into several words. For instance, the word
?4.34(RMB Yuan 4.34)?, may be segmented
into two words ?4.? and ?34?. To tackle this
problem, a numeral word processing strategy is
used. Under this strategy, those words that contain
Arabic numerals are manually marked in the train-
ing set firstly, then a list of high frequency charac-
ters which always appear alone between the num-
bers in the training set can be extracted, based on
which numeral word issue can be tackled as fol-
lows. When segmenting one sentence, if two con-
joint words are numeral words, and the last char-
acter of the former word is in the list, then they are
combined as one word.
2.3.3 Long Organization Name Processing
Strategy
Since an organization name is usually an OOV,
it always will be segmented as several words, es-
pecially for a long one, while in MSRA corpus, it
is required to be recognized as one word. In our
systems, a corresponding strategy is presented to
deal with this problem. Firstly a list of organiza-
tion names is manually selected from the training
set and stored in the prefix-tree based on charac-
ters. Then a list of prefixes is extracted by scan-
ning the prefix-tree, that is, for each node, if the
frequencies of its child nodes are all lower than the
predefined threshold k and half of the frequency of
the current node, the string of the current node will
be extracted as a prefix; otherwise, if there exists
a child node whose frequency is higher than the
threshold k, scan the corresponding subtree. In the
same way, the suffixes can also be extracted. The
only difference is that the order of characters is in-
verse in the lexical tree.
During recognizing phase, to a successive
words string that may include 2-5 words, will be
combined as one word, if all of the following con-
ditions are satisfied.
a) Does not include numbers, full stop or comma.
b) Includes some OOV words.
c) Has a tail substring matching some suffix.
d) Appears more than twice in the test data.
e) Has a higher frequency than any of its substring which
is an OOV word or combined by multiple words.
f) Satisfy the condition that for any two successive words
w1 w2 in the strings, freq(w1w2)/freq(w1)?0.1, unless w1
contains some prefix in its right.
3 Experiments and Results
We have participated in both the closed and open
tracks of all the four corpora. For MSRA corpus
and other three corpora, we build System I and
System II respectively. Both systems are based on
the ME model and the Maximum Entropy Toolkit
1, provided by Zhang Le, is adopted.
Four systems are derived from System I with re-
gard to whether or not the n-gram language model
and three post processing strategies are used on the
closed track of MSRA corpus. Table 2 shows the
results of four derived systems.
System R P F ROOV RIV
IA 95.0 95.7 95.3 66.0 96.0
IB 96.0 95.6 95.8 60.3 97.3
IC 96.4 96.0 96.2 60.3 97.7
ID 96.4 96.1 96.3 61.2 97.6
Table 2: The effect of MEmodel, n-gram language
model and three post processing strategies on the
closed track of MSRA corpus.
System IA only adopts the ME model. System
IB integrates the ME model and the bigram lan-
guage model. System IC integrates the division
and combination strategy and the numeral words
1http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html
140
processing strategy. System ID adds the long or-
ganization name processing strategy.
For the open track of MSRA, an external dictio-
nary is utilized to extract the e and f features. The
external dictionary is built from six sources, in-
cluding the Chinese Concept Dictionary from In-
stitute of Computational Linguistics, Peking Uni-
versity(72,716 words), the LDC dictionary(43,120
words), the Noun Cyclopedia(111,633), the word
segmentation dictionary from Institute of Com-
puting Technology, Chinese Academy of Sci-
ences(84,763 words), the dictionary from Insti-
tute of Acoustics, and the dictionary from Insti-
tute of Computational Linguistics, Peking Univer-
sity(68,200 words) and a dictionary collected by
ourselves(63,470 words).
The union of the six dictionaries forms a big
dictionary, and those words appearing in five or
six dictionaries are extracted to form a core dic-
tionary. If a word belongs to one of the following
dictionaries or word sets, it is added into the exter-
nal dictionary.
a) The core dictionary.
b) The intersection of the big dictionary and the training
data.
c) The words appearing in the training data twice or more
times.
Those words in the external dictionaries will be
eliminated, if in most cases they are divided in
the training data. Table 3 shows the effect of ME
model, n-gram language model, three post pro-
cessing strategies on the open track of MSRA.
Here System IO only adopts the basic features,
while the external dictionary based features are
used in four derived systems related to open track:
IA, IB, IC, ID.
System R P F ROOV RIV
IO 96.0 96.5 96.3 71.1 96.9
IA 97.5 96.9 97.2 65.9 98.6
IB 97.6 96.8 97.2 64.8 98.7
IC 97.7 97.0 97.4 66.8 98.8
ID 97.7 97.1 97.4 67.5 98.8
Table 3: The effect of MEmodel, n-gram language
model, three post processing strategies on the open
track of MSRA.
System II only adopts ME model, the division
and combination strategy and the numeral word
processing strategy. In the open track of the cor-
pora CKIP and CITYU, the training set and test set
from the 2nd Chinese Word Segmentation Backoff
are used for training. For the corpora UPUC and
CITYU, the external dictionaries are used, which
is constructed in the same way as that in the open
track of MSRA Corpus. Table 4 shows the official
results of system II on UPUC, CKIP and CITYU.
Corpus R P F ROOV RIV
UPUC-C 93.6 92.3 93.0 68.3 96.1
UPUC-O 94.0 90.7 92.3 56.1 97.6
CKIP-C 95.8 94.8 95.3 64.6 97.2
CKIP-O 95.8 94.8 95.3 64.7 97.2
CITYU-C 96.9 97.0 97.0 77.3 97.8
CITYU-O 97.9 97.6 97.7 81.3 98.5
Table 4: Official results of our systems on UPUC
CKIP and CITYU
On the UPUC corpus, an interesting observation
is that the performance of the open track is worse
than the closed track. The investigation and analy-
sis lead to a possible explanation. That is, the seg-
mentation standard of the dictionaries, which are
used to construct the external dictionary, is differ-
ent from that of the UPUC corpus.
4 Conclusion
In this paper, a detailed description on several Chi-
nese word segmentation systems are presented,
where ME model, n-gram language model as well
as three post processing strategies are involved. In
the closed track of MSRA, the integration of bi-
gram language model greatly improves the recall
ratio of the words in vocabulary, although it will
impairs the performance of system in recognizing
the words out of vocabulary. In addition, three
strategies are introduced to deal with combination
ambiguity, numeral word, long organization name
issues. And the evaluation results reveal the valid-
ity and effectivity of our approaches.
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
A maximum Entropy Approach to Chinese Word
Segmentation. 2005. Preceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pp. 161-164.
Hwee Tou Ng and Jin Kiat Low. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? 2004. Preceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing(EMNLP), pp. 277-284.
Zhang Huaping and Liu Qun. Model of Chinese
Words Rough Segmentation Based on N-Shortest-
Paths Method. 2002. Journal of Chinese Informa-
tion Processing, 28(1):pp. 1-7.
141
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57?67,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Policy Learning for Domain Selection in an Extensible Multi-domain
Spoken Dialogue System
Zhuoran Wang
Mathematical & Computer Sciences
Heriot-Watt University
Edinburgh, UK
zhuoran.wang@hw.ac.uk
Hongliang Chen, Guanchun Wang
Hao Tian, Hua Wu
?
, Haifeng Wang
Baidu Inc., Beijing, P. R. China
SurnameForename@baidu.com
?
wu hua@baidu.com
Abstract
This paper proposes a Markov Decision
Process and reinforcement learning based
approach for domain selection in a multi-
domain Spoken Dialogue System built on
a distributed architecture. In the proposed
framework, the domain selection prob-
lem is treated as sequential planning in-
stead of classification, such that confir-
mation and clarification interaction mech-
anisms are supported. In addition, it is
shown that by using a model parameter ty-
ing trick, the extensibility of the system
can be preserved, where dialogue com-
ponents in new domains can be easily
plugged in, without re-training the domain
selection policy. The experimental results
based on human subjects suggest that the
proposed model marginally outperforms a
non-trivial baseline.
1 Introduction
Due to growing demand for natural human-
machine interaction, over the last decade Spo-
ken Dialogue Systems (SDS) have been increas-
ingly deployed in various commercial applications
ranging from traditional call centre automation
(e.g. AT&T ?Lets Go!? bus information sys-
tem (Williams et al., 2010)) to mobile personal
assistants and knowledge navigators (e.g. Ap-
ple?s Siri
R
?
, Google Now
TM
, Microsoft Cortana,
etc.) or voice interaction for smart household ap-
pliance control (e.g. Samsung Evolution Kit for
Smart TVs). Furthermore, latest progress in open-
vocabulary Automatic Speech Recognition (ASR)
is pushing SDS from traditional single-domain in-
formation systems towards more complex multi-
domain speech applications, of which typical ex-
amples are those voice assistant mobile applica-
tions.
Recent advances in SDS have shown that sta-
tistical approaches to dialogue management can
result in marginal improvement in both the nat-
uralness and the task success rate for domain-
specific dialogues (Lemon and Pietquin, 2012;
Young et al., 2013). State-of-the-art statistical
SDS treat the dialogue problem as a sequential
decision making process, and employ established
planning models, such as Markov Decision Pro-
cesses (MDPs) (Singh et al., 2002) or Partially Ob-
servable Markov Decision Processes (POMDPs)
(Thomson and Young, 2010; Young et al., 2010;
Williams and Young, 2007), in conjunction with
reinforcement learning techniques (Jur?c???cek et al.,
2011; Jur?c???cek et al., 2012; Ga?si?c et al., 2013a)
to seek optimal dialogue policies that maximise
long-term expected (discounted) rewards and are
robust to ASR errors.
However, to the best of our knowledge, most of
the existing multi-domain SDS in public use are
rule-based (e.g. (Gruber et al., 2012; Mirkovic
and Cavedon, 2006)). The application of statistical
models in multi-domain dialogue systems is still
preliminary. Komatani et al. (2006) and Nakano
et al. (2011) utilised a distributed architecture (Lin
et al., 1999) to integrate expert dialogue systems in
different domains into a unified framework, where
a central controller trained as a data-driven clas-
sifier selects a domain expert at each turn to ad-
dress user?s query. Alternatively, Hakkani-T?ur et
al. (2012) adopted the well-known Information
State mechanism (Traum and Larsson, 2003) to
construct a multi-domain SDS and proposed a dis-
criminative classification model for more accurate
state updates. More recently, Ga?si?c et al. (2013b)
proposed that by a simple expansion of the kernel
function in Gaussian Process (GP) reinforcement
learning (Engel et al., 2005; Ga?si?c et al., 2013a),
one can adapt pre-trained dialogue policies to han-
dle unseen slots for SDS in extended domains.
In this paper, we use a voice assistant applica-
57
User Interface Manager
ASR
User Intention
Identier Central Controller
SLU NLG
Domain Expert
(Travel Info)
SLU NLG
Domain Expert
(Restaurant Search)
SLU NLG
Domain Expert
(Movie Search)
SLU NLG
Domain Expert
(etc.)
Web 
Search
Weather
Report
QA
etc.
Ou
t-o
f-d
om
ain
 Se
rvi
ce
s
Service
Ranker
Mobile Devices
Flight Ticket 
Booking
Train Ticket 
Booking
Hotel  
Booking
speech
text
text, clicks
query,
intention label,
condence
TTS Web PageRendering etc.
Figure 1: The distributed architecture of the voice assistant system (a simplified illustration).
tion (similar to Apple?s Siri but in Chinese lan-
guage) as an example to demonstrate a novel
MDP-based approach for central interaction man-
agement in a complex multi-domain dialogue sys-
tem. The voice assistant employs a distributed ar-
chitecture similar to (Lin et al., 1999; Komatani et
al., 2006; Nakano et al., 2011), and handles mixed
interactions of multi-turn dialogues across differ-
ent domains and single-turn queries powered by
a collection of information access services (such
as web search, Question Answering (QA), etc.).
In our system, the dialogues in each domain are
managed by an individual domain expert SDS, and
the single-turn services are used to handle those
so-called out-of-domain requests. We use fea-
turised representations to summarise the current
dialogue states in each domain (see Section 3 for
more details), and let the central controller (the
MDP model) choose one of the following system
actions at each turn: (1) addressing user?s query
based on a domain expert, (2) treating it as an
out-of-domain request, (3) asking user to confirm
whether he/she wants to continue a domain ex-
pert?s dialogue or to switch to out-of-domain ser-
vices, and (4) clarifying user?s intention between
two domains. The Gaussian Process Temporal
Difference (GPTD) algorithm (Engel et al., 2005;
Ga?si?c et al., 2013a) is adopted here for policy op-
timisation based on human subjects, where a pa-
rameter tying trick is applied to preserve the ex-
tensibility of the system, such that new domain
experts (dialogue systems) can be flexibly plugged
in without the need of re-training the central con-
troller.
Comparing to the previous classification-based
methods (Komatani et al., 2006; Nakano et al.,
2011), the proposed approach not only has the
advantage of action selection in consideration of
long-term rewards, it can also yield more robust
policies that allow clarifications and confirmations
to mitigate ASR and Spoken Language Under-
standing (SLU) errors. Our human evaluation re-
sults show that the proposed system with a trained
MDP policy achieves significantly better natural-
ness in domain switching tasks than a non-trivial
baseline with a hand-crafted policy.
The remainder of this paper is organised as
follows. Section 2 defines the terminology used
throughout the paper. Section 3 briefly overviews
the distributed architecture of our system. The
MDP model and the policy optimisation algorithm
are introduced in Section 4 and Section 5, respec-
tively. After this, experimental settings and eval-
uation results are described in Section 6. Finally,
we discuss some possible improvements in Sec-
tion 7 and conclude ourselves in Section 8.
2 Terminology
A voice assistant application provides a unified
speech interface to a collection of individual infor-
mation access systems. It aims to collect and sat-
isfy user requests in an interactive manner, where
58
different types of interactions can be involved.
Here we focus ourselves on two interaction scenar-
ios, i.e. task-oriented (multi-turn) dialogues and
single-turn queries.
According to user intentions, the dialogue inter-
actions in our voice assistant system can further be
categorised into different domains, of which each
is handled by a separate dialogue manager, namely
a domain expert. Example domains include travel
information, restaurant search, etc. In addition,
some domains in our system can be further de-
composed into sub-domains, e.g. the travel in-
formation domain consists of three sub-domains:
flight ticket booking, train ticket booking and hotel
reservation. We use an integrated domain expert to
address queries in all its sub-domains, so that rel-
evant information can be shared across those sub-
domains to allow intelligent induction in the dia-
logue flow.
For convenience of future reference, we call
those single-turn information access systems out-
of-domain services or simply services for short.
The services integrated in our system include web
search, semantic search, QA, system command ex-
ecution, weather report, chat-bot, and many more.
3 System Architecture
The voice assistant system introduced in this pa-
per is built on a distributed architecture (Lin et al.,
1999), as shown in Figure 1, where the dialogue
flow is processed as follows. Firstly, a user?s query
(either an ASR utterance or directly typed in text)
is passed to a user intention identifier, which la-
bels the raw query with a list of intention hypothe-
ses with confidence scores. Here an intention label
could be either a domain name or a service name.
After this, the central controller distributes the raw
query together with its intention labels and confi-
dence scores to all the domain experts and the ser-
vice modules, which will attempt to process the
query and return their results to the central con-
troller.
The domain experts in the current implementa-
tion of our system are all rule-based SDS follow-
ing the RavenClaw framework proposed in (Bo-
hus and Rudnicky, 2009). When receiving a query,
a domain expert will use its own SLU module to
parse the utterance or text input and try to update
its dialogue state in consideration of both the SLU
output and the intention labels. If the dialogue
state in the domain expert can be updated given
the query, it will return its output, internal ses-
sion record and a confidence score to the central
controller, where the output can be either a natu-
ral language utterance realised by its Natural Lan-
guage Generation (NLG) module or a set of data
records obtained from its database (if a database
search operation is triggered), or both. If the do-
main expert cannot update its state using the cur-
rent query, it will just return an empty result with
a low confidence score. Similar procedures ap-
ply to those out-of-domain services as well, but
there are no session records or confidence scores
returned. Finally, given all the returned informa-
tion, the central controller chooses, according to
its policy, the module (either a domain expert or a
service) whose results will be provided to the user.
When the central controller decides to pass a
domain expert?s output to the user, we regard the
domain expert as being activated. Also note here,
the updated state of a domain expert in a turn will
not be physically stored, unless the domain expert
is activated in that turn. This is a necessary mech-
anism to prevent an inactive domain expert being
misled by ambiguous queries in other domains.
In addition, we use a well-engineered priority
ranker to rank the services based on the num-
bers of results they returned as well as some prior
knowledge about the quality of their data sources.
When the central controller decides to show user
the results from an out-of-domain service, it will
choose the top one from the ranked list.
4 MDP Modelling of the Central Control
Process
The main focus of this paper is to seek a policy for
robustly switching the control flow among those
domain experts and services (the service ranker in
practice) during a dialogue, where the user may
have multiple or compound goals (e.g. booking a
flight ticket, booking a restaurant in the destina-
tion city and checking the weather report of the
departure or destination city).
In order to make the system robust to ASR er-
rors or ambiguous queries, the central controller
should also have basic dialogue abilities for confir-
mation and clarification purposes. Here we define
the confirmation as an action of asking whether a
user wants to continue the dialogue in a certain do-
main. If the system receives a negative response at
this point, it will switch to out-of-domain services.
On the other hand, the clarification action is de-
59
fined between domains, in which case, the system
will explicitly ask the user to choose between two
domain candidates before continuing the dialogue.
Due to the confirmation and clarification mech-
anisms defined above, the central controller be-
comes a sequential decision maker that must take
the overall smoothness of the dialogue into ac-
count. Therefore, we propose an MDP-based ap-
proach for learning an optimal central control pol-
icy in this section.
The potential state space of our MDP is huge,
which in principle consists of the combinations of
all possible situations of the domain experts and
the out-of-domain services, therefore function ap-
proximation techniques must be employed to en-
able tractable computations. However, when de-
veloping such a complex application as the voice
assistant here, one also needs to take the extensi-
bility of the system into account, so that new do-
main experts can be easily integrated into the sys-
tem without major re-training or re-engineering of
the existing components. Essentially, it requires
the state featurisation and the central control pol-
icy learnt here to be independent of the number of
domain experts. In Section 4.3, we show that such
a property can be achieved by a parameter tying
trick in the definition of the MDP.
4.1 MDP Preliminaries
Let P
X
denote the set of probability distributions
over a set X . An MDP is defined as a five tuple
?S,A, T,R, ??, where the components are defined
as follows. S and A are the sets of system states
and actions, respectively. T : S ? A ? P
S
is the
transition function, and T (s
?
|s, a) defines the con-
ditional probability of the system transiting from
state s ? S to state s
?
? S after taking action
a ? A. R : S ? A ? P
R
is the reward function
with R(s, a) specifying the distribution of the im-
mediate rewards for the system taking action a at
state s. In addition, 0 ? ? ? 1 is the discount
factor on the summed sequence of rewards.
A finite-horizon MDP operates as follows. The
system occupies a state s and takes an action a,
which then will make it transit to a next state s
?
?
T (?|s, a) and receive a reward r ? R(s, a). This
process repeats until a terminal state is reached.
For a given policy pi : S ? A, the value
function V
pi
is defined to be the expected cumula-
tive reward, as V
pi
(s
0
) = E
[
?
n
t=0
?
t
r
t
|
s
t
,pi(s
t
)
]
,
where s
0
is the starting state and n is the plan-
ning horizon. The aim of policy optimisation is
to seek an optimal policy pi
?
that maximises the
value function. If T and R are given, in conjunc-
tion with a Q-function, the optimal value V
?
can
be expressed by recursive equations as Q(s, a) =
R(s, a) + ?
?
s
?
?S
T (s
?
|s, a)V
?
(s
?
) and V
?
(s) =
max
a?A
Q(s, a) (here we assume R(s, a) is de-
terministic), which can be solved by dynamic pro-
gramming (Bellman, 1957). For problems with
unknown T or R, such as dialogue systems, the
Q-values are usually estimated via reinforcement
learning (Sutton and Barto, 1998).
4.2 Problem Definition
Let D denote the set of the domain experts in our
voice assistant system, and s
d
be the current di-
alogue state of domain expert d ? D at a certain
timestamp. We also define s
o
as an abstract state to
describe the current status of those out-of-domain
services. Then mathematically we can represent
the central control process as an MDP, where its
state s is a joint set of the states of all the domain
experts and the services, as s = {s
d
}
d?D
? {s
o
}.
Four types of system actions are defined as fol-
lows.
? present(d): presenting the output of do-
main expert d to user;
? present ood(null): presenting the re-
sults of the top-ranked out-of-domain service
given by the service ranker;
? confirm(d): confirming whether user
wants to continue with domain expert d (or
to switch to out-of-domain services);
? clarify(d,d
?
): asking user to clarify
his/her intention between domains d and d
?
.
For convenience of notations, we use a(x) to
denote a system action of our MDP, where a ?
{present,present ood,confirm,clarify},
x ? {d,null, (d, d
?
)}
d,d
?
?D,d6=d
?
, x = null
only applies to present ood, and x = (d, d
?
)
only applies to clarify actions.
4.3 Function Approximation
Function approximation is a commonly used tech-
nique to estimate the Q-values when the state
space of the MDP is huge. Concretely, in our case,
we assume that:
Q(s, a(x)) = f(?(s, a(x)); ?) (1)
60
where ? : S ? A ? R
K
is a feature function
that maps a state-action pair to an K-dimensional
feature vector, and f : R
K
? R is a function of
?(s, a(x)) parameterised by ?. A frequent choice
of f is the linear function, as:
Q(s, a(x)) = ?
>
?(s, a(x)) (2)
After this, the policy optimisation problem be-
comes learning the parameter ? to approximate the
Q-values based on example dialogue trajectories.
However, a crucial problem with the standard
formulation in Eq. (2) is that the feature function
? is defined over the entire state and action spaces.
In this case, when a new domain expert is inte-
grated into the system, both the state space and the
action space will be changed, therefore one will
have to re-define the feature function and conse-
quentially re-train the model. In order to achieve
an extensible system, we make some simplifica-
tion assumptions and decompose the feature func-
tion as follows. Firstly, we let:
?(s, a(x)) = ?
a
(s
x
) (3)
=
?
?
?
?
?
?
?
?
pr
(s
d
) if a(x) =present(d)
?
ood
(s
o
) if a(x) =present ood()
?
cf
(s
d
) if a(x) =confirm(d)
?
cl
(s
d
, s
d
?
) if a(x) =clarify(d,d
?
)
where the feature function is reduced to only de-
pend on the state of the action?s operand, instead
of the entire system state. Then, we make those ac-
tions a(x) that have a same action type (a) but op-
erate different domain experts (x) share the same
parameter, i.e.:
Q(s, a(x)) = ?
>
a
?
a
(s
x
) (4)
This decomposition and parameter tying trick pre-
serves the extensibility of the system, because both
?
>
a
and ?
a
are independent of x, when there is a
new domain expert
?
d, we can directly substitute
its state s
?
d
into Eq. (3) and (4) to compute its cor-
responding Q-values.
4.4 Features
Based on the problem formulation in Eq. (3) and
(4), we shall only select high-level summary fea-
tures to sketch the dialogue state and dialogue his-
tory of each domain expert, which must be ap-
plicable to all domain experts, regardless of their
domain-specific characteristics or implementation
differences. Suppose that the dialogue states of the
# Feature Range
1
the number of unfilled
required slots of a domain
expert
{0, . . . ,M}
2
the number of filled required
slots of a domain expert
{0, . . . ,M}
3
the number of filled optional
slots of a domain expert
{0, . . . , L}
4
whether a domain expert has
executed a database search
{0, 1}
5
the confidence score
returned by a domain expert
[0, 1.2]
6
the total number of turns that
a domain expert has been
activated during a dialogue
Z
+
7
e
?t
a
where t
a
denotes the
relative turn of a domain
expert being last activated,
or 0 if not applicable
[0, 1]
8
e
?t
c
where t
c
denotes the
relative turn of a domain
expert being last confirmed,
or 0 if not applicable
[0, 1]
9
the summed confidence
score from the user intention
identifier of a query being
for out-of-domain services
[0, 1.2N ]
Table 1: A list of all features used in our model.
M and L respectively denote the maximum num-
bers of required and optional slots for the domain
experts. N is the maximum number of hypotheses
that the intention identifier can return. Z
+
stands
for the non-negative integer set.
domain experts can be represented as slot-value
pairs
1
, and for each domain there are required slots
and optional slots, where all required slots must
be filled before the domain expert can execute a
database search operation. The features investi-
gated in the proposed framework are listed in Ta-
ble 1.
Detailed featurisation in Eq. (3) is explained
as follows. For ?
pr
, we choose the first 8 fea-
tures plus a bias dimension that is always set to
1
This is a rather general assumption. Informally speak-
ing, for most task-oriented SDS, one can extract a slot-value
representation from their dialogue models, of which exam-
ples include the RavenClaw architecture (Bohus and Rud-
nicky, 2009), the Information State dialogue engine (Traum
and Larsson, 2003), MDP-SDS (Singh et al., 2002) or
POMDP-SDS (Thomson and Young, 2010; Young et al.,
2010; Williams and Young, 2007).
61
?1. Whilst, feature #9 plus a bias is used to de-
fine ?
ood
. All the features are used in ?
cf
, as to
do a confirmation, one needs to consider the joint
situation in and out of the domain. Finally, the
feature function for a clarification action between
two domains d and d
?
is defined as ?
cl
(s
d
, s
d
?
) =
exp{?|?
pr
(s
d
) ? ?
pr
(s
d
?
)|}, where we use | ? |
to denote the element-wise absolute of a vector
operand. The intuition here is that the more dis-
tinguishable the (featurised) states of two domain
experts are, the less we tend to clarify them.
For those domain experts that have multiple
sub-domains with different numbers of required
and optional slots, the feature extraction procedure
only applies to the latest active sub-domain.
In addition, note that, the confidence scores pro-
vided by the user intention identifier are only used
as features for out-of-domain services. This is be-
cause in the current version of our system, the con-
fidence estimation of the intention identifier for
domain-dependent dialogue queries is less reliable
due to the lack of context information. In contrast,
the confidence scores returned by the domain ex-
perts will be more informative at this point.
5 Policy Learning with GPTD
In traditional statistical SDS, dialogue policies are
usually trained using reinforcement learning based
on simulated dialogue trajectories (Schatzmann
et al., 2007; Keizer et al., 2010; Thomson and
Young, 2010; Young et al., 2010). Although the
evaluation of the simulators themselves could be
an arguable issue, there are various advantages,
e.g. hundreds of thousands of data examples can
be easily generated for training and initial policy
evaluation purposes, and different reinforcement
learning models can be compared without incur-
ring notable extra costs.
However, for more complex multi-domain SDS,
particularly a voice assistant application like ours
that aims at handling very complicated (ideally
open-domain) dialogue scenarios, it would be dif-
ficult to develop a proper simulator that can rea-
sonably mimic real human behaviours. There-
fore, in this work, we learn the central control
policy directly with human subjects, for which
the following properties of the learning algorithm
are required. Firstly and most importantly, the
learner must be sample-efficient as the data collec-
tion procedure is costly. Secondly, the algorithm
should support batch reinforcement learning. This
is because when using function approximation, the
learning process may not strictly converge, and the
quality of the sequence of generated policies tends
to oscillate after a certain number of improving
steps at the beginning (Bertsekas and Tsitsiklis,
1996). If online reinforcement learning is used,
we will be unable to evaluate the generated policy
after each update, and hence will not know which
policy to keep for the final evaluation. Therefore,
we do a batch policy update and iterate the learn-
ing process for a number of batches, such that the
data collection phase in a new iteration yields an
evaluation of the policy obtained from the previ-
ous iteration at the same time.
To fulfill the above two requirements, the Gaus-
sian Process Temporal Difference (GPTD) algo-
rithm (Engel et al., 2005) is a proper choice, due to
its sample efficiency (Fard et al., 2011) and batch
learning ability (Engel et al., 2005), as well as its
previous success in dialogue policy learning with
human subjects (Ga?si?c et al., 2013a). Note that,
GPTD can also admit recursive (online) compu-
tations, but here we focus ourselves on the batch
version.
A Gaussian Process (GP) is a generative model
of Bayesian inference that can be used for func-
tion regression, and has the superiority of obtain-
ing good posterior estimates with just a few obser-
vations (Rasmussen and Williams, 2006). GPTD
models the Q-function as a zero mean GP which
defines correlations in different parts of the fea-
turised state and action spaces through a kernel
function ?, as:
Q(s, a(x)) ? GP(0, ?((s
x
, a), (s
x
, a))) (5)
Given a sequence of t state-action pairs X
t
=
[(s
0
, a
0
(x
0
)), . . . , (s
t
, a
t
(x
t
))] from a collection
of dialogues and their corresponding immedi-
ate rewards r
t
= [r
0
, . . . , r
t
], the posterior of
Q(s, a(x)) for an arbitrary new state-action pair
(s, a(x)) can be computed as:
Q(s, a(x))|
X
t
,r
t
? N
(
?
Q(s, a(x)), cov (s, a(x))
)
(6)
?
Q(s, a(x)) = k
t
(s
x
, a)
>
H
>
t
G
?1
t
r
t
(7)
cov (s, a(x)) = ?((s
x
, a), (s
x
, a))
? k
t
(s
x
, a)
>
H
>
t
G
?1
t
H
t
k
t
(s
x
, a) (8)
G
t
= H
t
K
t
H
>
t
+ ?
2
H
t
H
>
t
(9)
62
Ht
=
?
?
?
?
?
1 ?? ? ? ? 0 0
0 1 ? ? ? 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 ? ? ? 0 1 ??
?
?
?
?
?
(10)
where K
t
is the Gram matrix with elements
K
t
(i, j) = ?((s
i
x
i
, a
i
), (s
j
x
j
, a
j
)), k
t
(s
x
, a) =
[?((s
i
x
i
, a
i
), (s
x
, a))]
t
i=0
is a vector, and ? is a
hyperparameter specifying the diagonal covari-
ance values of the zero-mean Gaussian noise. In
addition, we use cov (s, a(x)) to denote (for short)
the self-covariance cov (s, a(x), s, a(x)).
In our case, as different feature functions ?
a
are
defined for different action types, the kernel func-
tion is defined to be:
?((s
x
, a), (s
?
x
?
, a
?
)) = [[a = a
?
]]?
a
(s
x
, s
?
x
?
) (11)
where [[?]] is an indicator function and ?
a
is the ker-
nel function defined corresponding to the feature
function ?
a
.
Given a state, a most straightforward policy is
to select the action that corresponds to the max-
imum mean Q-value estimated by the GP. How-
ever, since the objective is to learn the Q-function
associated with the optimal policy by interacting
directly with users, the policy must exhibit some
form of stochastic behaviour in order to explore
alternatives during the process of learning. In this
work, the strategy employed for the exploration-
exploitation trade-off is that, during exploration,
actions are chosen according to the variance of
the GP estimate for the Q-function, and during
exploitation, actions are chosen according to the
mean. That is:
pi(s) =
{
arg max
a(x)
?
Q(s, a(x)) : w.p. 1? 
arg max
a(x)
cov (s, a(x)) : w.p. 
(12)
where 0 <  < 1 is a pre-defined exploration rate,
and will be exponentially reduced at each batch
iteration during our learning process.
Note that, in practice, not all the actions are
valid at every possible state. For example, if a do-
main expert d has never been activated during a
dialogue and can neither process the user?s current
query, the actions with an operand d will be re-
garded as invalid at this state. When executing the
policy, we only consider those valid actions for a
given state.
Score Interpretation
5
The domain selections are totally
correct, and the entire dialogue flow
is fluent.
4
The domain selections are totally
correct, but the dialogue flow is
slightly redundant.
3
There are accidental domain
selections errors, or the dialogue
flow is perceptually redundant.
2
There are frequent domain selections
errors, or the dialogue flow is
intolerably redundant.
1
Most domain selections are
incorrect, or the dialogue is
incompletable.
Table 2: The scoring standard in our experiments.
6 Experimental Results
6.1 Training
We use the batch version of GPTD as described
in Section 5 to learn the central control policy
with human subjects. There are three domain ex-
perts available in our current system, but during
the training only two domains are used, which are
the travel information domain and the restaurant
search domain. We reserve a movie search domain
for evaluating the generalisation property of the
learnt policy (see Section 6.2). The learning pro-
cess started from a hand-crafted policy. Then 15
experienced users
2
volunteered to contribute dia-
logue examples with multiple or compound goals
(see Figure 4 for an instance), from whom we
collected around 50?70 dialogues per day for 5
days
3
. After each dialogue, the users were asked
to score the system from 5 to 1 according to a scor-
ing standard shown in Table 2. The scores were
taken as the (delayed) rewards to train the GPTD
model, where we set the rewards for intermediate
turns to 0. The working policy was updated daily
based on the data obtained up to that day. The
data collected on the first day was used for pre-
liminary experiments to choose the hyperparame-
2
Overall user satisfactions may rely on various aspects of
the entire system, e.g. the data source quality of the services,
the performance of each domain expert, etc. It will be diffi-
cult to make non-experienced users to score the central con-
troller isolatedly.
3
Not all the users participated the experiments everyday.
There were 311 valid dialogues received in total, with an av-
erage length of 9 turns.
63
2	 ?
3	 ?
4	 ?
5	 ?
Figure 2: Average scores and standard deviations
during policy iteration.
0.7	 ?
0.72	 ?
0.74	 ?
0.76	 ?
0.78	 ?
0.8	 ?
0.82	 ?
0.84	 ?
0.86	 ?
0.88	 ?
0.9	 ?
Figure 3: Domain selection accuracies during pol-
icy iteration.
ters of the model, such as the kernel function, the
kernel parameters (if applicable), and ? and ? in
the GPTD model. We initially experimented with
linear, polynomial and Gaussian kernels, with dif-
ferent configurations of ? and ? values, as well
as kernel parameter values. It was found that
the linear kernel in combination with ? = 5 and
? = 0.99 works more appropriate than the other
settings. This configuration was then fixed for the
rest iterations.
The learning process was iterated for 4 days af-
ter the first one. On each day, we computed the
mean and standard deviation of the user scores
as an evaluation of the policy learnt on the pre-
vious day. The learning curve is illustrated in Fig-
ure 2. Note here, as we were actually executing a
stochastic policy according to Eq. (12), to calcu-
late the values in Figure 2 we ignored those dia-
logues that contain any actions selected due to the
exploration. We also manually labelled the cor-
rectness of domain selection at every turn of the
dialogues. The domain selection accuracies of the
obtained policy sequence are shown in Figure 3,
where similarly, those exploration actions as well
Policy
Scenario
Baseline GPTD
p-value
(i) 4.5?0.8 4.2?0.8 0.387
(ii) 3.4?0.9 4.2?0.8 0.018
(iii) 4.1?1.0 4.3?1.0 0.0821
(iv) 3.9?1.1 4.5?0.8 0.0440
Table 3: Paired comparison experiments between
the system with a trained GPTD policy and the
rule-based baseline.
as the clarification and confirmation actions were
excluded from the calculations. Although the do-
main selection accuracy is not the target that our
learning algorithm aims to optimise, it reflects the
quality of the learnt policies from a different angle
of view.
It can be found in Figure 2 that the best policy
was obtained in the third iteration, and after that
the policy quality oscillated. The same finding is
indicated in Figure 3 as well, when we use the do-
main selection accuracy as the evaluation metric.
Therefore, we kept the policy corresponding to the
peak point of the learning curve for the compari-
son experiments below.
6.2 Comparison Experiments
We conducted paired comparison experiments in
four scenarios to compare between the system
with the GPTD-learnt central control policy and a
non-trivial baseline. The baseline is a publicly de-
ployed version of the voice assistant application.
The central control policy of the baseline system is
handcrafted, which has a separate list of semantic
matching rules for each domain to enable domain
switching.
The first two scenarios aim to test the perfor-
mance of the two systems on (i) switching between
a domain expert and out-of-domain services, and
(ii) switching between two domain experts, where
only the two training domains (travel information
and restaurant search) were considered. Scenar-
ios (iii) and (iv) are similar to scenarios (i) and (ii)
respectively, but at this time, the users were re-
quired to carry out the tests surrounding the movie
search domain (which is addressed by a new do-
main expert not used in the training phase). There
were 13 users who participated this experiment.
In each scenario, every user was required to test
the two systems with an identical goal and similar
queries. After each test, the users were asked to
64
score the two systems separately according to the
scoring standard in Table 2.
The average scores received by the two systems
are shown in Table 3, where we also compute the
statistical significance (the p-values) of the results
based on paired t-tests. It can be found that the
learnt policy works significantly better than the
rule-based policy in scenarios (ii) and (iv), but in
scenarios (i) and (iii) the differences between two
systems are statistically insignificant. Moreover,
the learnt policy preserves the extensibility of the
entire system as expected, of which strong evi-
dences are given by the results in scenarios (iii)
and (iv).
6.3 Policy Analysis
To better understand the policy learnt by the
GPTD model, we look into the obtained weight
vectors, as shown in Table 4. It can be found that
confidence score (#5) is an informative feature for
all the system actions, while the relative turn of a
domain being last activated (#7) is an additional
strong evidence for a confirmation decision. In
addition, the similarity between the dialogue com-
pletion status (#1 & #2) of two ambiguous domain
experts and the relative turns of them being last
confirmed (#8) tend to be extra dominating fea-
tures for clarification decisions, besides the close-
ness of the confidence scores returned by the two
domain experts.
A less noticeable but important phenomenon is
observed for feature #6, i.e. the total number of
active turns of a domain expert during a dialogue.
Concretely, feature #6 has a small negative effect
on presentation actions but a small positive con-
tribution to confirmation actions. Such weights
could correspond to the discount factor?s penalty
to long dialogues in the value function. How-
ever, it implies an unexpected effect in extreme
cases, which we explain in detail as follows. Al-
though the absolute weights for feature #6 are tiny
for both presentation and confirmation actions, the
feature value will grow linearly during a dialogue.
Therefore, when a dialogue in a certain domain
last rather long, there tend to be very frequent con-
firmations. A possible solution to this problem
could be either ignoring feature #6 or twisting it to
some nonlinear function, such that its value stops
increasing at a certain threshold point. In addition,
to cover sufficient amount of those ?extreme? ex-
amples in the training phase could also be an alter-
Feature Weights
#
present confirm clarify
1 0.09 0.02 0.60
p
r
e
s
e
n
t
o
o
d
2 0.20 0.29 0.53
3 0.18 0.29 0.16
4 -0.10 0.16 0.25
5 0.75 0.57 0.54
6 -0.02 0.11 0.13
7 0.25 1.19 0.36
8 -0.22 -0.19 0.69
9 ? 0.20 ? 0.47
Bias -1.79 ? ? -2.42
Table 4: Feature weights learnt by GPTD. See Ta-
ble 1 for the meanings of the features.
native solution, as our current training set contains
very few examples that exhibit extraordinary long
domain persistence.
7 Further Discussions
The proposed approach is a rather general frame-
work to learn extensible central control policies
for multi-domain SDS based on distributed archi-
tectures. It does not rely on any internal represen-
tations of those individual domain experts, as long
as a unified featurisation of their dialogue states
can be achieved.
However, from the entire system point of view,
the current implementation is still preliminary.
Particularly, the confirmation and clarification
mechanisms are isolated, for which the surface re-
alisations sometimes may sound stiff. This phe-
nomenon explains one of the reasons that make
the proposed system slightly less preferred by the
users than the baseline in scenario (i), when the
interaction flows are relative simple. A possi-
ble improvement here could be associating the
confirmation and clarification actions in the cen-
tral controller to the error handling mechanisms
within each domain expert, and letting domain ex-
perts generate their own utterances on receiving a
confirmation/clarification request from the central
controller.
Online reinforcement learning with real user
cases will be another undoubted direction of fur-
ther improvement of our system. The key chal-
lenge here is to automatically estimate user?s satis-
factions, which will be transformed to the rewards
for the reinforcement learners. Strong feedbacks
such as clicks or actual order placements can be
65
collected. But to regress user?s true satisfaction,
other environment features must also be taken into
account. Practical solutions are still an open issue
at this stage, and are left to our future work.
8 Conclusion
In this paper, we introduce an MDP framework
for learning domain selection policies in a com-
plex multi-domain SDS. Standard problem for-
mulation is modified with tied model parameters,
so that the entire system is extensible and new
domain experts can be easily integrated without
re-training the policy. This expectation is con-
firmed by empirical experiments with human sub-
jects, where the proposed system marginally beats
a non-trivial baseline and demonstrates proper ex-
tensibility. Several possible improvements are dis-
cussed, which will be the central arc of our future
research.
Acknowledgements
The research in this paper is supported by China?s
973 Programme no. 2014CB340505. The first au-
thor is partially funded by the EC FP7 programme
under grant agreement no. 287615 (PARLANCE)
and a SICSA PECE grant. The authors would also
like to thank Qiaoqiao She, Duo Cai and the HCI-
APP group at Baidu for volunteering to participate
in the human subject experiments.
References
Richard Ernest Bellman. 1957. Dynamic Program-
ming. Princeton University Press, Princeton, NJ.
Dimitri P. Bertsekas and John N. Tsitsiklis. 1996.
Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Dan Bohus and Alexander I. Rudnicky. 2009. The
RavenClaw dialog management framework: Archi-
tecture and systems. Computer Speech and Lan-
guage, 23(3):332?361.
Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with Gaussian processes. In
Proceedings of the 22nd International Conference
on Machine Learning (ICML), pages 201?208.
Mahdi Milani Fard, Joelle Pineau, and Csaba
Szepesv?ari. 2011. PAC-Bayesian policy evaluation
for reinforcement learning. In Proceedings of the
27th Conference on Uncertainty in Artificial Intelli-
gence (UAI), pages 195?202.
USER/ASR:      ?????? (Flight ticket to Qingdao.)
SYSTEM: ?????????????????????        ???????        (Okay, flight ticket to Qingdao. When are you        departing, and where are you departing from?)     [Travel Info. Domain Expert]
USER/ASR:  ???? (I?m leaving tomorrow.)
SYSTEM: ???????????????????????     (Okay, flight ticket to Qingdao, leaving         tomorrow. Where are you departing from?)      [Travel Info. Domain Expert]
USER/ASR:  ?? (Beijing)
SYSTEM: [Results Displayed     - Travel Info. Domain Expert]
USER:    ??????????        (Are there any good Sichuan cuisines in Qingdao?)ASR:       ???????????        (ungrammatical utterance, partially with     ?delicious? and ?Sichuan food?.)
SYSTEM: ?????????    (Do you want to continue with flight ticket booking?)    [Confirmation - Central Controller]
USER:      ?? (No)
SYSTEM: [Web Search Service: ?????????????     ]
USER/ASR:  ????? (I?m going to Qingdao tomorrow.)
SYSTEM: [Results Displayed     - Travel Info. Domain Expert]
USER/ASR:  ????????? (Restaurants near Qingdao Airport)
SYSTEM: ????????   (I found the following restaurants for you.)        [Results Displayed     - Restaurant Search Domain Expert]
USER/ASR:  ????????? (Will it rain in Qingdao tomorrow?)
SYSTEM: ?????????16??20????5-6?     (Qingdao is rainy tomorrow. The temperature is 16 to 20         degree centigrade with moderate east wind.)     [Results Displayed     - Weather Report Service]
Figure 4: An example dialogue containing multi-
ple user goals. The icons stand for graphical user
interface based nonverbal outputs.
Milica Ga?si?c, Catherine Breslin, Matthew Henderson,
Dongho Kim, Martin Szummer, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2013a. On-
line policy optimisation of Bayesian spoken dia-
logue systems via human interaction. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
8367?8371.
Milica Ga?si?c, Catherine Breslin, Matthew Hender-
son, Dongho Kim, Martin Szummer, Blaise Thom-
son, Pirros Tsiakoulis, and Steve Young. 2013b.
POMDP-based dialogue manager adaptation to ex-
tended domains. In Proceedings of the 14th annual
SIGdial Meeting on Discourse and Dialogue, pages
214?222.
Thomas Robert Gruber, Adam John Cheyer, Dag
66
Kittlaus, Didier Rene Guzzoni, Christopher Dean
Brigham, Richard Donald Giuli, Marcello Bastea-
Forte, and Harry Joseph Saddler. 2012. Intelligent
automated assistant. United States Patent No. US
20120245944 A1.
Dilek Z. Hakkani-T?ur, Gokhan T?ur, Larry P. Heck,
Ashley Fidler, and Asli C?elikyilmaz. 2012. A dis-
criminative classification-based approach to infor-
mation state updates for a multi-domain dialog sys-
tem. In Proceedings of the 13th Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH).
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2011. Natural actor and belief critic: Reinforcement
algorithm for learning parameters of dialogue sys-
tems modelled as POMDPs. ACM Transactions on
Speech and Language Processing, 7(3):6:1?6:25.
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech & Language, 26(3):168?192.
Simon Keizer, Milica Ga?si?c, Filip Jur?c???cek, Franc?ois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Parameter estimation for agenda-
based user simulation. In Proceedings of the 11th
annual SIGdial Meeting on Discourse and Dialogue,
pages 116?123.
Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,
Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,
and Hiroshi G. Okuno. 2006. Multi-domain spo-
ken dialogue system with extensibility and robust-
ness against speech recognition errors. In Proceed-
ings of the 7th SIGdial Workshop on Discourse and
Dialogue, pages 9?17.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-Driven Methods for Adaptive Spoken Dia-
logue Systems: Computational Learning for Conver-
sational Interfaces. Springer.
Bor-shen Lin, Hsin-min Wang, and Lin-Shan Lee.
1999. A distributed architecture for cooperative
spoken dialogue agents with coherent dialogue state
and history. In Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop
(ASRU).
Danilo Mirkovic and Lawrence Cavedon. 2006. Di-
alogue management using scripts. United States
Patent No. US 20060271351 A1.
Mikio Nakano, Shun Sato, Kazunori Komatani, Kyoko
Matsuyama, Kotaro Funakoshi, and Hiroshi G.
Okuno. 2011. A two-stage domain selection frame-
work for extensible multi-domain spoken dialogue
systems. In Proceedings of the 12th annual SIGdial
Meeting on Discourse and Dialogue, pages 18?29.
Carl Edward Rasmussen and Christopher K. I.
Williams, editors. 2006. Gaussian Processes for
Machine Learning. MIT Press.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In Proceedings of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 149?152.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research, 16(1):105?133.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
David R. Traum and Staffan Larsson. 2003. The In-
formation State approach to dialogue management.
In Jan van Kuppevelt and Ronnie W. Smith, editors,
Current and New Directions in Discourse and Dia-
logue, pages 325?353. Springer.
Jason D. Williams and Steve Young. 2007. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D. Williams, Iker Arizmendi, and Alistair
Conkie. 2010. Demonstration of AT&T ?Let?s Go?:
A production-grade statistical spoken dialog system.
In Proceedings of the 3rd IEEE Workshop on Spoken
Language Technology (SLT).
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The Hidden Information State model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Ga?si?c, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialogue systems: a review. Proceedings of
the IEEE, PP(99):1?20.
67
Proceedings of NAACL-HLT 2013, pages 563?568,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Compound Embedding Features for Semi-supervised Learning   Mo Yu1, Tiejun Zhao1, Daxiang Dong2, Hao Tian2 and Dianhai Yu2 Harbin Institute of Technology, Harbin, China Baidu Inc., Beijing, China {yumo,tjzhao}@mtlab.hit.edu.cn {dongdaxiang,tianhao,yudianhai}@baidu.com      Abstract 
To solve data sparsity problem, recently there has been a trend in discriminative methods of NLP to use representations of lexical items learned from unlabeled data as features. In this paper, we investigated the usage of word representations learned by neural language models, i.e. word embeddings. The direct us-age has disadvantages such as large amount of computation, inadequacy with dealing word ambiguity and rare-words, and the problem of linear non-separability. To overcome these problems, we instead built compound features from continuous word embeddings based on clustering. Experiments showed that the com-pound features not only improved the perfor-mances on several NLP tasks, but also ran faster, suggesting the potential of embeddings.  
1 Introduction Supervised learning methods have achieved great successes in the field of Natural Language Pro-cessing (NLP). However, in practice most methods are usually limited by the problem of data sparsity, since it is impossible to obtain sufficient labeled data for all NLP tasks. In these situations semi-supervised learning can help to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representa-tions, which are feature vectors of lexical items, from unlabeled data and then plug them into a su-pervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-of-
the-art supervised systems on a variety of tasks (Koo et al, 2008; Huang and Yates, 2009; T?ck-str?m et al, 2012).  With the development of neural language mod-els (NLM) (Bengio et al, 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embed-dings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and seman-tic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al, 2011a, 2011b) and help these systems perform compara-bly with the state-of-the-art models based on hand-crafted features. They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements (Turian et al 2010). Although the direct usage of continuous embed-dings has been proved to be an effective method for enhancing the state-of-the-art supervised mod-els, it has some disadvantages, which made them be out-performed by simpler Brown cluster fea-tures (Turian et al 2010) and made them computa-tionally complicated. Firstly, embeddings of rare words are insufficiently trained since they are only updated few times and are close to their random initial values. As shown in (Turian et al 2010), this is the main reason that models with embedding features made more errors than those with Brown cluster features. Secondly, in NLMs, each word has its unique representation, so it is difficult to represent different senses for ambiguous words. Thirdly, word embeddings are unsuitable for linear models in some tasks as will be proved in Section 
563
4.2. This is possibly because in these tasks, either the target labels are correlated with combinations of different dimensions of word embeddings, or discriminative information may be coded in differ-ent intervals in the same dimension. So treating embeddings directly as inputs to a linear model could not fully utilize them. Moreover, since em-beddings are dense vectors, it will introduce large amount of computations when they are directly used as inputs, making the method impractical. In this paper, we first introduced the idea of clustering embeddings to overcome the last two disadvantages discussed above. The high-dimensional cluster features make samples from different classes better separated by linear models. And models with these features can still run fast because the clusters are sparse and discrete.  Second, we proposed the compound features based on clustering. Compound features, which are conjunctive features of neighboring words, have been widely used in NLP models for improving the performances because they are more discriminative. Compound features of embeddings can also help a model to better predict labels associated with rare-words and ambiguous words, because compound features composed of embeddings of nearby words can help to better describe the property of these words. Compound features are difficult to build on dense embeddings. However they are easy to in-duce from the sparse embedding clusters proposed in this paper.   Experiments on chunking and NER showed that based on the same embeddings, the compound fea-tures managed to achieve better performances. Moreover, we proposed analyses to reveal the rea-sons for the improvements of embedding-clusters and compound features. They suggest that these features can better deal with rare-words and word ambiguity, and are more suitable for linear models. In addition, although Brown clustering was con-sidered better in (Turian et al2010), our experi-ment results and comparisons showed that our compound features from embedding clustering is at least comparable with those from Brown clustering. Since embeddings can greatly benefit from the im-provement and developing of deep learning in the future, we believe that our proposed method has a large space of performance growth and will benefit more applications in NLP. In the rest of the paper, Section 2 introduces how compound embedding features were obtained. 
Section 3 gives experimental results. In Section 4, we give analysis about the advantages of com-pound features. Section 5 gives the conclusions. 2 Clustering of Word Embeddings  
2.1 Learning Word Embeddings Word embeddings in this paper were trained by NLMs (Bengio et al, 2003). The model predicts the scores of probabilities of words given their context information in the sentences. It first con-verts the current word and its context words (e.g. n-1 words before it as in n-gram models) into em-beddings. Then these embeddings are put together and propagate forward on the network to compute the score of current word. After minimizing the loss on training data, embeddings are learned and can be further used as smoothing representations for words. 2.2 Clustering of embeddings In order to get compound features of embeddings, we first induce discrete clusters from the embed-dings. Concretely, the k-means clustering algo-rithm is used. Each word is treated as a single sample. A cluster is represented as the mean of the embeddings of words assigned to it. Similarities between words and clusters are measured by Eu-clidean distance. As discussed and experimented later, different numbers of ks contain information of different granularity. So we combine clustering results achieved by different ks as features to better utilize the embeddings. 2.3 Compound features Based on embedding clusters, more powerful com-pound features can be built. Compound features are conjunctions between basic features of words and their contexts, which are widely used in NLP. Koo et al (2008) also observed that compound features of Brown clusters achieved more im-provements on parsing.     It is also necessary to build compound embed-ding features since they can better deal with rare-words and ambiguous words. For example, alt-hough embedding of a rare-word is not fully trained and hence inaccurate, embeddings of its context words can still be accurate as long as they 
564
are not rare and are fully trained. So we could uti-lize the combination of embeddings before and after the word to predict its tag correctly. We con-ducted analysis to verify our theory in Section4. We combined the compound features together with other state-of-the-art human-craft features in supervised models. Examples of the resulted fea-ture templates in chunking and NER are shown in Table 1 & 2. The feature 
1101 ccyy ??  in the last row is an example of compound feature made up of the embedding clusters of words before and af-ter current word. Compound feature extraction can similarly be applied to form compound features of Brown clusters. For example, Brown clusters can replace embedding clusters in 3th row of Table 1. Words }1,0{,1}2:2{, , ???? iiiii www  POS }2,1{,1}2:2{, , ????? iiiii ppp  Cluster 11}1,0{,1}2:2{, ,, ccccc iiiii ?????  Transition },,,{ 1100001 cccpwyy ??  Table 1: Chunking features. Cluster features are suitable for both Brown clusters and embedding clusters. Sym-bol iy is the tag predicted on word iw . Words }1,0{,1}2:2{, , ???? iiiii www  Pre/suffix 1: }4:1{,0:1 }4:2{,0 , ?? ?? iiii ww  Orthography ( ) ( )00 , wCapwHyp  POS }2,1{,1}2:2{, , ????? iiiii ppp  Chunking }2,1{,1}2:2{, , ????? iiiii bbb  Cluster 11}1,0{,1}2:2{, ,, ccccc iiiii ?????  Transition },,,{ 1100001 cccpwyy ??  Table 2: NER features. Hyp indicates if word contains hyphen and Cap indicates if first letter is capitalized.  3 Experiments 
3.1 Experimental settings We tested our compound features on the same chunking (CoNLL2000) and NER (CoNLL2003) tasks in (Turian et al, 2010). The Brown cluster features were used for comparison, which shared the same feature template used by clusters of em-beddings. To compare with the work of (Turian et al 2010), which aimed to solve the same problem but using embedding directly, we used the same word embeddings (CW 50) and Brown clusters (1000 clusters) they provided. The embeddings in (Turian et al 2010) are trained on RCV corpus, while the CoNLL2000 data is a part of the WSJ corpus. Since we believe that word representations 
trained on similar domain may better help to im-prove the results, we also used embeddings and Brown clusters trained on unlabeled WSJ data from (Nivre et al 2007) for comparison. Moreover, we wish to find out whether our method extends well to languages other than Eng-lish. So we conducted experiments on Chinese NER, where large amount of training data exists, which makes improving accuracies more difficult. We used data from People?s Daily (Jan.-Jun. 1998) and converted them following the style of Penn CTB (Xue et al 2005). Data from April was cho-sen as test set (1,309,616 words in 55,177 sentenc-es), others for training (6,119,063 words in 255,951 sentences). The Chinese word representa-tions were trained on Chinese Wikipedia until March 2011. The features used in Chinese NER are similar to those in English, except for the or-thography, pre/suffixes, and chunking features. We did little pre-processing work for the train-ing of word representations on WSJ data. The da-tasets were tokenized and capital words were kept. For training of Chinese Wikipedia, we retained the bodies of all articles and replaced words with fre-quencies lower than 10 as an ?UK_WORD? token. On each dataset, we induced embeddings with 64 dimensions based on 7-gram models and 1000 Brown clusters. The method in (Schwenk, 2007) was used to accelerate the training processes of NLMs. All the NLMs were trained for 5 epochs.  For clustering of embeddings we choose k=500 and 2500 since such combination performed best on development set as shown in the next section. We chose the Sofia-ml toolkit (Sculley 2010) for clustering of embeddings in order to save time. In the experiments CRF models were used and were optimized by ASGD (implemented by L?on Bottou). For comparison we re-implemented the direct usage of embeddings in (Turian et al 2010) with CRFsuite (Okazaki, 2007) since their features contain continuous values. 3.2 Performances Table 3 shows the chunking results. The results reported in (Turian et al 2010) were denoted as ?direct?. Based on the same word representations, our compound features got better performances in all cases. The embedding features trained on unla-beled WSJ data yield further improvements, show-
565
ing that word representations from similar domains can better help the supervised tasks. System Direct Compound Baseline 93.75 +Embedding (RCV) 94.10 94.19 +Brown (RCV) 94.11 94.24 +Brown&Emb (RCV) 94.35 94.42 +Embedding (WSJ) 94.20 94.37 +Brown (WSJ) 94.25 94.36 +Brown&Emb (WSJ) 94.43 94.58 Table 3:  F1-scores of chunking In the experiments of NER, first we evaluated how the numbers of clusters k will affect the per-formances on development set (Figure 1). The re-sults showed that both the cluster features (excluding all compound embedding features) and compound features could achieve better results than direct usage of the same embeddings. It also showed that the performances did not vary much when k was between 500 and 3000. When k=2500, the result was a little higher than others. We finally chose combination of k=500 and 2500, which achieved best results on development set.  
 Figure 1: Relation between numbers of clusters k and performances on development set. The performances of NER on test set are shown in Table 4. Our baseline is slightly lower than that in (Turian et al 2010), because the first-order CRF cannot utilize context information of NE tags. Despite of this, same conclusions with chunking held.  System Direct Compound Baseline 83.78 +Embedding 87.38 88.46 +Brown 88.14 88.23 +Brown&Embedding 88.85 89.06 Table 4:  F1-scores of English NER on test data Performances on Chinese NER are shown in Table 5. Similar results were observed as in Eng-lish NER, showing that our method extends to oth-er languages as well. 
System Direct Compound Baseline 88.24 +Embedding 89.98 90.37 +Brown 90.24 90.55 +Brown&Embedding 90.66 90.96 Table 5:  F1-scores of Chinese NER on test data Above results gave evidences that although clus-tering embeddings may lose some information, the derived compound features did have better perfor-mances. The compound features can also improve the performances of Brown clusters, but not as much as they did on embeddings. And the combi-nation of embedding-clusters and Brown-clusters could further improve the performances, since they made use of different type of context information.  The compound features also reduced the time cost of using embedding features. For example, the time for tagging one sentence in English NER was reduced from 5.6 ms to 1.6 ms, shown in Table 6. Embedding Time (ms) Baseline 1.2 Embeddings (direct) 5.6 Embeddings (compound) 1.6 Table 6:  Running time of different features  4 Analysis  Our compound embedding features greatly out-performed the direct usage of same embeddings on English NER. In this section we conducted anal-yses to show the reasons for the improvements. 4.1 Rare-words and ambiguous words To show the compound features have stronger abil-ities to handle rare words, we counted the numbers of errors made on words with different frequencies on unlabeled data. Here the word frequencies are from the results of Brown clustering provided by (Turian et al 2010). We compared our compound embedding features with direct usage of embed-dings as well as Brown clusters, which is believed to work better on rare words. Figure 2(a) shows that the compound features indeed resulted in few-er errors than the two baseline methods in most cases. Errors of embeddings occurred on words with frequencies lower than 2K and those in the range of 16 to 256 were reduced by 10.55% and 24.44%, respectively. Our compound features also reduced the errors caused by ambiguous words, as shown in Figure 
566
2(b), where the numbers of senses for a word are measured by the numbers of different POS tags it has in Penn Treebank. 12.1% of the errors on am-biguous words were reduced, comparing to 8.4% of the errors on unambiguous ones. 
 (a) 
 (b) Figure 2: Errors incurred on words with different fre-quencies (a) and ambiguous words (b) in NER. 4.2 Linear separability of embeddings Another reason for the good performances of com-pound features on NER is that they made linear models better separate named entities (NEs) and non-NEs, which are more difficult to be linearly separated when embeddings are directly used as features. Here we designed an experiment to prove this. Based on training data of CoNLL2003, a clas-sification task was built to tell whether a word be-longs to NE or not. Linear SVM and a non-linear model Multilayer Perceptron (MLP) were used to build the classifiers. As shown in Table 7, when embeddings were directly used as features, MLP performed much better than linear SVM. And the linear model was under-fitting on this task since it had similar accuracies on both training set and de-velopment set. Above observations showed that linear models could not separate NEs and non-NEs well in the space of embeddings. When clusters of embeddings were used as fea-tures, the accuracies of linear models increased even when there were only one or two non-zero 
features for each sample. At the same time the per-formances of MLP decreased because of the loss of information during clustering. The gaps between accuracies of linear models and non-linear ones decreased in the spaces of clusters, showing that cluster features are more suitable for linear models. At last, the compound features made the linear model out-perform all non-linear ones, since extra context information could be utilized. Embeddings Models Accuracy   direct linear 94.38  direct MLP 96.87  cluster 1000 linear 95.31  cluster 1000 MLP 95.32  cluster 500+2500 linear 96.10  cluster 500+2500 MLP 96.02  compound linear 97.30 Table 7:  Performances of linear and non-linear models on development set with different embedding features. 5 Conclusion and perspectives In this paper, we first introduced the idea of clus-tering embeddings and then proposed the com-pound features based on clustering, in order to overcome the disadvantages of the direct usage of continuous embeddings. Experiments showed that the compound features built on the same original word representation features (either embeddings or Brown clusters) achieve better performances on the same tasks. Further analyses showed that the com-pound features reduced errors on rare-words and ambiguous words and could be better utilized by linear models. The usage of word embeddings also has some limitations, e.g. they are weak in capturing struc-tural information of languages, which is necessary in NLP. In the future, we will research on task-specific representations for sub-structures, such as phrases and sub-trees based on word embeddings and documents representations (Xu et al, 2012). Acknowledgments We would like to thank Dr. Hua Wu, Haifeng Wang, Jie Zhou and Rui Zhang for many discus-sions and thank the anonymous reviewers for their valuable suggestions.  This work was supported by National Natural Science Foundation of China (61173073), and the Key Project of the National High Technology Research and Development Pro-gram of China (2011AA01A207). 
567
References  Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic language models. The Journal of Machine Learning Research, 3:1137?1155. Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160?167. ACM. Finkel, J., Grenager, T., and Manning, C. (2005). Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363?370. Association for Computational Linguistics. Huang, F. and Yates, A. (2009). Distributional representations for handling sparsity in supervised sequence labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 495?503. Association for Computational Linguistics. Koo, T., Carreras, X., and Collins, M. (2008). Simple semi-supervised dependency parsing.  In Proceed-ings of Association for Computational Linguistics, pages 595?603. Association for Computational Linguistics. Mnih, A. and Hinton, G. E. (2009). A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081?1088. Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson, J., Riedel, S., and Yuret, D. (2007). The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915?932. Okazaki, N. (2007). Crfsuite: a fast implementation of conditional random fields (crfs). URL http://www.chokkan.org/software/crfsuite. Schwenk, H. (2007). Continuous space language models. Computer Speech & Language, 21(3):492?518. Sculley, D. (2010). Web-scale k-means clustering. In Proceedings of the 19th international conference on World Wide Web, pages 1177?1178. ACM. Socher, R., Huang, E., Pennington, J., Ng, A., and Manning, C. (2011a). Dynamic pooling and unfolding recursive auto-encoders for paraphrase 
detection. Advances in Neural Information Processing Systems, 24:801?809. Socher, R., Pennington, J., Huang, E., Ng, A., and Manning, C. (2011b). Semi-supervised recursive auto-encoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151?161. Association for Computational Linguistics. T?ckstr?m, O., McDonald, R., and Uszkoreit, J. (2012). Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477?487, Montr?al, Canada, June 3-8, 2012. Turian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: a simple and general method for semi-supervised learning. In Annual Meeting-Association For Computational Linguistics. Urbana, 51:61801. Xu, Z., Chen, M., Weinberger, K., and Sha, F. An alternative text representation to TF-IDF and Bag-of-Words. In Proceedings of 21st ACM Conf. of Information and Knowledge Management (CIKM), Hawaii, 2012. Xue, N., Xia, F., Chiou, F., and Palmer, M. (2005). The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207. 
568
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 312?317,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Cross-lingual Projections between Languages from Different Families
Mo Yu1 Tiejun Zhao1 Yalong Bai1 Hao Tian2 Dianhai Yu2
1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
{yumo,tjzhao,ylbai}@mtlab.hit.edu.cn
2Baidu Inc., Beijing, China
{tianhao,yudianhai}@baidu.com
Abstract
Cross-lingual projection methods can ben-
efit from resource-rich languages to im-
prove performances of NLP tasks in
resources-scarce languages. However,
these methods confronted the difficulty of
syntactic differences between languages
especially when the pair of languages
varies greatly. To make the projection
method well-generalize to diverse lan-
guages pairs, we enhance the projec-
tion method based on word alignments
by introducing target-language word rep-
resentations as features and proposing a
novel noise removing method based on
these word representations. Experiments
showed that our methods improve the per-
formances greatly on projections between
English and Chinese.
1 Introduction
Most NLP studies focused on limited languages
with large sets of annotated data. English and
Chinese are examples of these resource-rich lan-
guages. Unfortunately, it is impossible to build
sufficient labeled data for all tasks in all lan-
guages. To address NLP tasks in resource-scarce
languages, cross-lingual projection methods were
proposed, which make use of existing resources
in resource-rich language (also called source lan-
guage) to help NLP tasks in resource-scarce lan-
guage (also named as target language).
There are several types of projection methods.
One intuitive and effective method is to build a
common feature space for all languages, so that
the model trained on one language could be di-
rectly used on other languages (McDonald et al,
2011; Ta?ckstro?m et al, 2012). We call it di-
rect projection, which becomes very popular re-
cently. The main limitation of these methods is
that target language has to be similar to source
language. Otherwise the performance will de-
grade especially when the orders of phrases be-
tween source and target languages differ a lot.
Another common type of projection methods
map labels from resource-rich language sentences
to resource-scarce ones in a parallel corpus us-
ing word alignment information (Yarowsky et al,
2001; Hwa et al, 2005; Das and Petrov, 2011).
We refer them as projection based on word align-
ments in this paper. Compared to other types of
projection methods, this type of methods is more
robust to syntactic differences between languages
since it trained models on the target side thus fol-
lowing the topology of the target language.
This paper aims to build an accurate projec-
tion method with strong generality to various pairs
of languages, even when the languages are from
different families and are typologically divergent.
As far as we know, only a few works focused
on this topic (Xia and Lewis 2007; Ta?ckstro?m
et al, 2013). We adopted the projection method
based on word alignments since it is less affected
by language differences. However, such methods
also have some disadvantages. Firstly, the models
trained on projected data could only cover words
and cases appeared in the target side of parallel
corpus, making it difficult to generalize to test data
in broader domains. Secondly, the performances
of these methods are limited by the accuracy of
word alignments, especially when words between
two languages are not one-one aligned. So the ob-
tained labeled data contains a lot of noises, making
the models built on them less accurate.
This paper aims to build an accurate projection
method with strong generality to various pairs of
languages. We built the method on top of projec-
tion method based on word alignments because of
its advantage of being less affected by syntactic
differences, and proposed two solutions to solve
the above two difficulties of this type of methods.
312
Firstly, we introduce Brown clusters of target
language to make the projection models cover
broader cases. Brown clustering is a kind of word
representations, which assigns word with similar
functions to the same cluster. They can be ef-
ficiently learned on large-scale unlabeled data in
target language, which is much easier to acquire
even when the scales of parallel corpora of minor
languages are limited. Brown clusters have been
first introduced to the field of cross-lingual projec-
tions in (Ta?ckstro?m et al, 2012) and have achieved
great improvements on projection between Euro-
pean languages. However, their work was based
on the direct projection methods so that it do not
work very well between languages from different
families as will be shown in Section 3.
Secondly, to reduce the noises in projection, we
propose a noise removing method to detect and
correct noisy projected labels. The method was
also built on Brown clusters, based on the assump-
tion that instances with similar representations of
Brown clusters tend to have similar labels. As far
as we know, no one has done any research on re-
moving noises based on the space of word repre-
sentations in the field of NLP.
Using above techniques, we achieved a projec-
tion method that adapts well on different language
pairs even when the two languages differ enor-
mously. Experiments of NER and POS tagging
projection from English to Chinese proved the ef-
fectiveness of our methods.
In the rest of our paper, Section 2 describes the
proposed cross-lingual projection method. Evalu-
ations are in Section 3. Section 4 gives concluding
remarks.
2 Proposed Cross-lingual Projection
Methods
In this section, we first briefly introduce the cross-
lingual projection method based on word align-
ments. Then we describe how the word represen-
tations (Brown clusters) were used in the projec-
tion method. Section 2.3 describes the noise re-
moving methods.
2.1 Projection based on word alignments
In this paper we consider cross-lingual projec-
tion based on word alignment, because we want
to build projection methods that can be used be-
tween language pairs with large differences. Fig-
ure 1 shows the procedure of cross-lingual projec-
tion methods, taking projection of NER from En-
glish to Chinese as an example. Here English is
the resource-rich language and Chinese is the tar-
get language. First, sentences from the source side
of the parallel corpus are labeled by an accurate
model in English (e.g., ?Rongji Zhu? and ?Gan
Luo? were labeled as ?PER?), since the source
language has rich resources to build accurate NER
models. Then word alignments are generated from
the parallel corpus and serve as a bridge, so that
unlabeled words in the target language will get the
same labels with words aligning to them in the
source language, e.g. the first word ??(??)??
in Chinese gets the projected label ?PER?, since it
is aligned to ?Rongji? and ?Zhu?. In this way, la-
bels in source language sentences are projected to
the target sentences.
... ...
... ...O inspected
??
(O)O have
?
(O)O others
?? (O)O and
PER Yi ? (O)
PER Wu ?? (PER)
O ,
PER Gan
? (O)
PER Luo
?(??)? (PER)
O ,
PER Rongji
PER Zhu
Figure 1: An example of projection of NER. La-
bels of Chinese sentence (right) in brackets are
projected from the source sentence.
From the projection procedure we can see that a
labeled dataset of target language is built based on
the projected labels from source sentences. The
projected dataset has a large size, but with a lot
of noises. With this labeled dataset, models of the
target language can be trained in a supervised way.
Then these models can be used to label sentences
in target language. Since the models are trained
on the target language, this projection approach is
less affected by language differences, comparing
with direct projection methods.
2.2 Word Representation features for
Cross-lingual Projection
One disadvantage of above method is that the cov-
erage of projected labeled data used for training
313
Words wi,i?{?2:2}, wi?1/wi,i?{0,1}
Cluster ci,i?{?2:2}, ci?1/ci,i?{?1,2}, c?1/c1
Transition y?1/y0/{w0, c0, c?1/c1}
Table 1: NER features. ci is the cluster id of wi.
target language models are limited by the cover-
age of parallel corpora. For example in Figure 1,
some Chinese politicians in 1990?s will be learned
as person names, but some names of recent politi-
cians such as ?Obama?, which did not appeared in
the parallel corpus, would not be recognized.
To broader the coverage of the projected data,
we introduced word representations as features.
Same or similar word representations will be as-
signed to words appearing in similar contexts,
such as person names. Since word representations
are trained on large-scale unlabeled sentences in
target language, they cover much more words than
the parallel corpus does. So the information of a
word in projected labeled data will apply to other
words with the same or similar representations,
even if they did not appear in the parallel data.
In this work we use Brown clusters as word rep-
resentations on target languages. Brown clustering
assigns words to hierarchical clusters according to
the distributions of words before and after them.
Taking NER as an example, the feature template
may contain features shown in Table 1. The cluster
id of the word to predict (c0) and those of context
words (ci, i ? {?2,?1, 1, 2}), as well as the con-
junctions of these clusters were used as features in
CRF models in the same way the traditional word
features were used. Since Brown clusters are hi-
erarchical, the cluster for each word can be rep-
resented as a binary string. So we also use prefix
of cluster IDs as features, in order to compensate
for clusters containing small number of words. For
languages lacking of morphological changes, such
as Chinese, there are no pre/suffix or orthography
features. However the cluster features are always
available for any languages.
2.3 Noise Removing in Word Representation
Space
Another disadvantage of the projection method is
that the accuracy of projected labels is badly af-
fected by non-literate translation and word align-
ment errors, making the data contain many noises.
For example in Figure 1, the word ???(Wu Yi)?
was not labeled as a named entity since it was
not aligned to any words in English due to the
alignment errors. A more accurate model will be
trained if such noises can be reduced.
A direct way to remove the noises is to mod-
ify the label of a word to make it consistent with
the majority of labels assigned to the same word in
the parallel corpus. The method is limited when a
word with low frequency has many of its appear-
ances incorrectly labeled because of alignment er-
rors. In this situation the noises are impossible to
remove according to the word itself. The error in
Figure 1 is an example of this case since the other
few occurrences of the word ???(Wu Yi)? also
happened to fail to get the correct label.
Such difficulties can be easily solved when we
turned to the space of Brown clusters, based on
the observation that words in a same cluster tend
to have same labels. For example in Figure 1, the
word ???(Wu Yi)?, ??(??)?(Zhu Rongji)?
and ???(Luo Gan)? are in the same cluster, be-
cause they are all names of Chinese politicians
and usually appear in similar contexts. Having ob-
served that a large portion of words in this cluster
are person names, it is reasonable to modified the
label of ???(Wu Yi)? to ?PER?.
The space of clusters is also less sparse so it is
also possible to use combination of the clusters to
help noise removing, in order to utilize the context
information of data instances. For example, we
could represent a instance as bigram of the cluster
of target word and that of the previous word. And
it is reasonable that its label should be same with
other instances with the same cluster bigrams.
The whole noise removing method can be rep-
resented as following: Suppose a target word wi
was assigned label yi during projection with prob-
ability of alignment pi. From the whole projected
labeled data, we can get the distribution pw(y) for
the word wi, the distribution pc(y) for its cluster
ci and the distribution pb(y) for the bigram ci?1ci.
We choose y?i = y?, which satisfies
y? = argmaxy(?y,yipi + ?x?{w,c,b}px(y)) (1)
?y,yi is an indicator function, which is 1 when
y equals to yi. In practices, we set pw/c/b(y) to 0
for the ys that make the probability less than 0.5.
With the noise removing method, we can build a
more accurate labeled dataset based on the pro-
jected data and then use it for training models.
314
3 Experimental Results
3.1 Data Preparation
We took English as resource-rich language and
used Chinese to imitate resource-scarce lan-
guages, since the two languages differ a lot. We
conducted experiments on projections of NER and
POS tagging. The resource-scarce languages were
assumed to have no training data. For the NER
experiments, we used data from People?s Daily
(April. 1998) as test data (55,177 sentences). The
data was converted following the style of Penn
Chinese Treebank (CTB) (Xue et al, 2005). For
evaluation of projection of POS tagging, we used
the test set of CTB. Since English and Chinese
have different annotation standards, labels in the
two languages were converted to the universal
POS tag set (Petrov et al, 2011; Das and Petrov,
2011) so that the labels between the source and tar-
get languages were consistent. The universal tag
set made the task of POS tagging easier since the
fine-grained types are no more cared.
The Brown clusters were trained on Chinese
Wikipedia. The bodies of all articles are retained
to induce 1000 clusters using the algorithm in
(Liang, 2005) . Stanford word segmentor (Tseng
et al, 2005) was used for Chinese word segmenta-
tion. When English Brown clusters were in need,
we trained the word clusters on the tokenized En-
glish Wikipedia.
We chose LDC2003E14 as the parallel corpus,
which contains about 200,000 sentences. GIZA++
(Och and Ney, 2000) was used to generate word
alignments. It is easier to obtain similar amount
of parallel sentences between English and minor
languages, making the conclusions more general
for problems of projection in real applications.
3.2 Performances of NER Projection
Table 2 shows the performances of NER projec-
tion. We re-implemented the direct projection
method with projected clusters in (Ta?ckstro?m et
al., 2012). Although their method was proven to
work well on European language pairs, the results
showed that projection based on word alignments
(WA) worked much better since the source and tar-
get languages are from different families.
After we add the clusters trained on Chinese
Wikipedia as features as in Section 2.2, a great
improvement of about 9 points on the average F1-
score of the three entity types was achieved, show-
ing that the word representation features help to
System avgPrec
avg
Rec
avg
F1
Direct projection 47.48 28.12 33.91
Proj based on WA 71.6 37.84 47.66
+clusters(from en) 63.96 46.59 53.75
+clusters(ch wiki) 73.44 47.63 56.60
Table 2: Performances of NER projection.
recall more named entities in the test set. The per-
formances of all three categories of named entities
were improved greatly after adding word repre-
sentation features. Larger improvements were ob-
served on person names (14.4%). One of the rea-
sons for the improvements is that in Chinese, per-
son names are usually single words. Thus Brown-
clustering method can learn good word representa-
tions for those entities. Since in test set, most enti-
ties that are not covered are person names, Brown
clusters helped to increase the recall greatly.
In (Ta?ckstro?m et al, 2012), Brown clusters
trained on the source side were projected to the
target side based on word alignments. Rather than
building a same feature space for both the source
language and the target language as in (Ta?ckstro?m
et al, 2012), we tried to use the projected clus-
ters as features in projection based on word align-
ments. In this way the two methods used exactly
the same resources. In the experiments, we tried
to project clusters trained on English Wikipedia
to Chinese words. They improved the perfor-
mance by about 6.1% and the result was about
20% higher than that achieved by the direct pro-
jection method, showing that even using exactly
the same resources, the proposed method out-
performed that in (Ta?ckstro?m et al, 2012) much
on diverse language pairs.
Next we studied the effects of noise removing
methods. Firstly, we removed noises according to
Eq(1), which yielded another huge improvement
of about 6% against the best results based on clus-
ter features. Moreover, we conducted experiments
to see the effects of each of the three factors. The
results show that both the noise removing methods
based on words and on clusters achieved improve-
ments between 1.5-2 points. The method based on
bigram features got the largest improvement of 3.5
points. It achieved great improvement on person
names. This is because a great proportion of the
vocabulary was made up of person names, some of
which are mixed in clusters with common nouns.
315
While noise removing method based on clusters
failed to recognize them as name entities, cluster
bigrams will make use of context information to
help the discrimination of these mixed clusters.
System PER LOC ORG AVG
By Eq(1) 59.77 55.56 72.26 62.53
By clusters 49.75 53.10 72.46 58.44
By words 49.00 54.69 70.59 58.09
By bigrams 58.39 55.01 66.88 60.09
Table 3: Performances of noise removing methods
3.3 Performances of POS Projection
In this section we test our method on projection
of POS tagging from English to Chinese, to show
that our methods can well extend to other NLP
tasks. Unlike named entities, POS tags are asso-
ciated with single words. When one target word
is aligned to more than one words with different
POS tags on the source side, it is hard to decide
which POS tag to choose. So we only retained the
data labeled by 1-to-1 alignments, which also con-
tain less noises as pointed out by (Hu et al, 2011).
The same feature template as in the experiments
of NER was used for training POS taggers.
The results are listed in Table 4. Because of the
great differences between English and Chinese,
projection based on word alignments worked bet-
ter than direct projection did. After adding word
cluster features and removing noises, an error re-
duction of 12.7% was achieved.
POS tagging projection can benefit more from
our noise removing methods than NER projection
could, i.e. noise removing gave rise to a higher
improvement (2.7%) than that achieved by adding
cluster features on baseline system (1.5%). One
possible reason is that our noise removing meth-
ods assume that labels are associated with single
words, which is more suitable for POS tagging.
Methods Accuracy
Direct projection (Ta?ckstro?m) 62.71
Projection based on WA 66.68
+clusters (ch wiki) 68.23
+cluster(ch)&noise removing 70.92
Table 4: Performances of POS tagging projection.
4 Conclusion and perspectives
In this paper we introduced Brown clusters of
target languages to cross-lingual projection and
proposed methods for removing noises on pro-
jected labels. Experiments showed that both the
two techniques could greatly improve the perfor-
mances and could help the projection method well
generalize to languages differ a lot.
Note that although projection methods based on
word alignments are less affected by syntactic dif-
ferences, the topological differences between lan-
guages still remain an importance reason for the
limitation of performances of cross-lingual projec-
tion. In the future we will try to make use of repre-
sentations of sub-structures to deal with syntactic
differences in more complex tasks such as projec-
tion of dependency parsing. Future improvements
also include combining the direct projection meth-
ods based on joint feature representations with the
proposed method as well as making use of pro-
jected data from multiple languages.
Acknowledgments
We would like to thank the anonymous review-
ers for their valuable comments and helpful sug-
gestions. This work was supported by National
Natural Science Foundation of China (61173073),
and the Key Project of the National High Technol-
ogy Research and Development Program of China
(2011AA01A207).
References
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra,
and J.C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational linguistics,
18(4):467?479.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 600?609.
P.L. Hu, M. Yu, J. Li, C.H. Zhu, and T.J. Zhao.
2011. Semi-supervised learning framework for
cross-lingual projection. In Web Intelligence
and Intelligent Agent Technology (WI-IAT), 2011
IEEE/WIC/ACM International Conference on, vol-
ume 3, pages 213?216. IEEE.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural language
engineering, 11(3):311?326.
316
W. Jiang and Q. Liu. 2010. Dependency parsing and
projection based on word-pair classification. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL, volume 10,
pages 12?20.
P. Liang. 2005. Semi-supervised learning for natural
language. Ph.D. thesis, Massachusetts Institute of
Technology.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
62?72. Association for Computational Linguistics.
F.J. Och and H. Ney. 2000. Giza++: Training of statis-
tical translation models.
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure.
O Ta?ckstro?m, R McDonald, and J Nivre. 2013. Tar-
get language adaptation of discriminative transfer
parsers. Proceedings of NAACL-HLT.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and
C. Manning. 2005. A conditional random field
word segmenter for sighan bakeoff 2005. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chi-
nese Language Processing, volume 171. Jeju Island,
Korea.
F Xia and W Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 452?459.
N. Xue, F. Xia, F.D. Chiou, and M. Palmer. 2005. The
penn chinese treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings
of the first international conference on Human lan-
guage technology research, pages 1?8. Association
for Computational Linguistics.
317
