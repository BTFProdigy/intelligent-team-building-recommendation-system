Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 19?26,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Inferring Tutorial Dialogue Structure with Hidden Markov Modeling 
 
 
Kristy 
Elizabeth  
  Boyera 
Eun Young  
 Haa 
     Robert  
Phillipsab 
 Michael     
     D.  
  Wallisab 
Mladen A.  
 Vouka 
James C.  
 Lestera 
 
 
aDepartment of Computer Science, North Carolina State University 
bApplied Research Associates 
Raleigh, NC, USA 
 
{keboyer, eha, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
 
 
Abstract 
The field of intelligent tutoring systems has 
seen many successes in recent years.  A 
significant remaining challenge is the 
automatic creation of corpus-based tutorial 
dialogue management models.  This paper 
reports on early work toward this goal.  We 
identify tutorial dialogue modes in an 
unsupervised fashion using hidden Markov 
models (HMMs) trained on input 
sequences of manually-labeled dialogue 
acts and adjacency pairs.  The two best-fit 
HMMs are presented and compared with 
respect to the dialogue structure they 
suggest; we also discuss potential uses of 
the methodology for future work. 
1 Introduction 
 
The field of intelligent tutoring systems has made 
great strides toward bringing the benefits of one-
on-one tutoring to a wider population of learners.  
Some intelligent tutoring systems, called tutorial 
dialogue systems, support learners by engaging in 
rich natural language dialogue, e.g., (Graesser et 
al. 2003; Zinn, Moore & Core 2002; Evens & 
Michael 2006; Aleven, Koedinger & Popescu 
2003; Litman et al 2006; Arnott, Hastings & 
Allbritton 2008; VanLehn et al 2002).  However, 
creating these systems comes at a high cost: it 
entails handcrafting each pedagogical strategy the 
tutor might use and then realizing these strategies 
in a dialogue management framework that is also 
custom-engineered for the application.  It is hoped 
that the next generation of these systems can 
leverage corpora of tutorial dialogue in order to 
provide more robust dialogue management models 
that capture the discourse phenomena present in 
effective natural language tutoring.   
The structure of tutorial dialogue has 
traditionally been studied by manually examining 
corpora and focusing on cognitive and 
motivational aspects of tutorial strategies (e.g., 
Lepper et al 1993; Graesser, Person & Magliano 
1995).  While these approaches yielded 
foundational results for the field, such analyses 
suffer from two serious limitations:  manual 
approaches are not easily scalable to different or 
larger corpora, and the rigidity of handcrafted 
dialogue structure tagging schemes may not 
capture all the phenomena that occur in practice.   
In contrast, the stochastic nature of dialogue 
lends itself to description through probabilistic 
models.  In tutorial dialogue, some early work has 
adapted language processing techniques, namely n-
gram analyses, to examine human tutors? responses 
to student uncertainty (Forbes-Riley & Litman 
2005), as well as to find correlations between local 
tutoring strategies and student outcomes (Boyer et 
al. 2008).  However, this work is limited by its 
consideration of small dialogue windows. 
Looking at a broader window of turns is often 
accomplished by modeling the dialogue as a 
Markov decision process.  With this approach, 
19
techniques such as reinforcement learning can be 
used to compare potential policies in terms of 
effectiveness for student learning.  Determining 
relevant feature sets (Tetreault & Litman 2008) 
and conducting focussed experiments for localized 
strategy effectiveness (Chi et al 2008) are active 
areas of research in this line of investigation.  
These approches often fix the dialogue structures 
under consideration in order to compare the 
outcomes associated with those structures or the 
features that influence policy choice.    
    In contrast to treating dialogue structure as a 
fixed entity, one approach for modeling the 
progression of complete dialogues involves 
learning the higher-level structure in order to infer 
succinct probabilistic models of the interaction.  
For example, data-driven approaches for 
discovering dialogue structure have been applied to 
corpora of human-human task-oriented dialogue 
using general models of task structure (Bangalore, 
Di Fabbrizio & Stent 2006).  Encouraging results 
have emerged from using a general model of the 
task structure to inform automatic dialogue act 
tagging as well as subtask segmentation.  
    Our current work examines a modeling 
technique that does not require a priori knowledge 
of the task structure:  specifically, we propose to 
use hidden Markov models (HMMs) (Rabiner 
1989) to capture the structure of tutorial dialogue 
implicit within sequences of tagged dialogue acts.  
Such probablistic inference of discourse structure 
has been used in recent work with HMMs for topic 
identification (Barzilay & Lee 2004) and related 
graphical models for segmenting multi-party 
spoken discourse (Purver et al 2006).  
Analogously, our current work focuses on 
identifying dialogic structures that emerge during 
tutorial dialogue.  Our approach is based on the 
premise that at any given point in the tutorial 
dialogue, the collaborative interaction is ?in? a 
dialogue mode (Cade et al 2008) that characterizes 
the nature of the exchanges between tutor and 
student; these modes correspond to the hidden 
states in the HMM.  Results to date suggest that 
meaningful descriptive models of tutorial dialogue 
can be generated by this simple stochastic 
modeling technique.  This paper focuses on the 
comparison of two first-order HMMs:  one trained 
on sequences of dialogue acts, and the second 
trained on sequences of adjacency pairs.   
 
2 Corpus Analysis 
The HMMs were trained on a corpus of human-
human tutorial dialogue collected in the domain of 
introductory computer science.  Forty-three 
learners interacted remotely with one of fourteen 
tutors through a keyboard-to-keyboard remote 
learning environment yielding 4,864 dialogue 
moves. 
2.1 Dialogue Act Tagging 
The tutoring corpus was manually tagged with 
dialogue acts designed to capture the salient 
characteristics of the tutoring process (Table 1). 
 
Tag Act Example 
Q Question Where should I  
Declare i? 
EQ Evaluation Question How does that look? 
S Statement You need a  
closing brace. 
G Grounding Ok.  
EX Extra-Domain You may use  
your book. 
PF Positive Feedback Yes, that?s right. 
LF Lukewarm Feedback Sort of. 
NF Negative Feedback No, that?s not right. 
Table 1. Dialogue Act Tags 
 
    The correspondence between utterances and 
dialogue act tags is one-to-one; compound 
utterances were split by the primary annotator prior 
to the inter-rater reliability study.1  This dialogue 
act tagging effort produced sequences of dialogue 
acts that have been used in their un-altered forms 
to train one of the two HMMs presented here 
(Section 3).      
2.2 Adjacency Pair Identification 
In addition to the HMM trained on sequences of 
individual dialogue acts, another HMM was 
trained on sequences of dialogue act adjacency 
pairs.  The importance of adjacency pairs is well-
established in natural language dialogue (e.g., 
Schlegoff & Sacks 1973), and adjacency pair 
analysis has illuminated important phenomena in 
tutoring as well (Forbes-Riley et al 2007).  The 
                                                           
1 Details of the study procedure used to collect the corpus, as 
well as Kappa statistics for inter-rater reliability, are reported 
in (Boyer et al 2008). 
20
intuition behind adjacency pairs is that certain 
dialogue acts naturally occur together, and by 
grouping these acts we capture an exchange 
between two conversants in a single structure.  
This formulation is of interest for our purposes 
because when treating sequences of dialogue acts 
as a Markov process, with or without hidden states, 
the addition of adjacency pairs may offer a 
semantically richer observation alphabet.   
    To find adjacency pairs we utilize a ?2 test for 
independence of the categorical variables acti and 
acti+1 for all sequential pairs of dialogue acts that 
occur in the corpus.  Only pairs in which 
speaker(acti) ? speaker(acti+1) were considered.  
Table 2 displays a list of all dependent adjacency 
pairs sorted by descending (unadjusted) statistical 
significance; the subscript on each dialogue act tag 
indicates tutor (t) or student (s). 
    An adjacency pair joining algorithm was applied 
to join statistically significant pairs of dialogue 
acts (p<0.01) into atomic units according to a 
priority determined by the strength of the statistical 
significance.  Dialogue acts that were ?left out? of 
adjacency pair groupings were treated as atomic 
elements in subsequent analysis.  Figure 1 
illustrates the application of the adjacency pair 
joining algorithm on a sequence of dialogue acts 
from the corpus. 
 
 
Figure 1.  DA Sequence Before/After Joining 
3 HMM of Dialogue Structure 
A hidden Markov model is defined by three 
constituents:  1) the set of hidden states (dialogue 
modes), each characterized by its emission 
probability distribution over the possible 
observations (dialogue acts and/or adjacency 
pairs), 2) the transition probability matrix among 
observations (dialogue acts and/or adjacency 
pairs), 2) the transition probability matrix among 
 
acti acti+1 
P(acti+1|   
    acti) 
P(acti+1| 
   ?acti) 
?2 
val p-val 
EQs PFt 0.48 0.07 654 <0.0001 
Gs Gt 0.27 0.03 380 <0.0001 
EXs EXt 0.34 0.03 378 <0.0001 
EQt PFs 0.18 0.01 322 <0.0001 
EQt Ss 0.24 0.03 289 <0.0001 
EQs LFt 0.13 0.01 265 <0.0001 
Qt Ss 0.65 0.04 235 <0.0001 
EQt LFs 0.07 0.00 219 <0.0001 
Qs St 0.82 0.38 210 <0.0001 
EQs NFt 0.08 0.01 207 <0.0001 
EXt EXs 0.19 0.02 177 <0.0001 
NFs Gt 0.29 0.03 172 <0.0001 
EQt NFs 0.11 0.01 133 <0.0001 
Ss Gt 0.16 0.03 95 <0.0001 
Ss PFt 0.30 0.10 90 <0.0001 
St Gs 0.07 0.04 36 <0.0001 
PFs Gt 0.14 0.04 34 <0.0001 
LFs Gt 0.22 0.04 30 <0.0001 
St EQs 0.11 0.07 29 <0.0001 
Gt EXs 0.07 0.03 14 0.002 
St Qs 0.07 0.05 14 0.0002 
Gt Gs 0.10 0.05 9 0.0027 
EQt EQs 0.13 0.08 8 0.0042 
Table 2. All Dependent Adjacency Pairs 
 
hidden states, and 3) the initial hidden state 
(dialogue mode) probability distribution.   
3.1  Discovering Number of Dialogue Modes 
In keeping with the goal of automatically 
discovering dialogue structure, it was desirable to 
learn n, the best number of hidden states for the 
HMM, during modeling.  To this end, we trained 
and ten-fold cross-validated seven models, each 
featuring randomly-initialized parameters, for each 
number of hidden states n from 2 to 15, inclusive.2  
The average log-likelihood fit from ten-fold cross-
                                                           
2 n=15 was chosen as an initial maximum number of states 
because it comfortably exceeded our hypothesized range of 3 
to 7 (informed by the tutoring literature).  The Akaike 
Information Criterion measure steadily worsened above n = 5, 
confirming no need to train models with n > 15. 
21
validation was computed across all seven models 
for each n, and this average log-likelihood ln was 
used to compute the Akaike Information Criterion, 
a maximum-penalized likelihood estimator that 
prefers simpler models (Scott 2002).  This 
modeling approach was used to train HMMs on 
both the dialogue act and the adjacency pair input 
sequences. 
3.2  Best-Fit Models 
The input sequences of individual dialogue acts 
contain 16 unique symbols because each of the 8 
dialogue act tags (Table 1) was augmented with a 
label of the speaker, either tutor or student.  The 
best-fit HMM for this input sequence contains 
nDA=5 hidden states.  The adjacency pair input 
sequences contain 39 unique symbols, including all 
dependent adjacency pairs (Table 2) along with all 
individual dialogue acts because each dialogue act 
occurs at some point outside an adjacency pair.  
The best-fit HMM for this input sequence contains 
nAP=4 hidden states.  In both cases, the best-fit 
number of dialogue modes implied by the hidden 
states is within the range of what is often 
considered in traditional tutorial dialogue analysis 
(Cade et al 2008; Graesser, Person & Magliano 
1995).   
4 Analysis 
Evaluating the impact of grouping the dialogue 
acts into adjacency pairs requires a fine-grained 
examination of the generated HMMs to gain 
insight into how each model interprets the student 
sessions.     
4.1 Dialogue Act HMM 
Figure 2 displays the emission probability 
distributions for the dialogue act HMM.  State 0DA, 
Tutor Lecture,3 is strongly dominated by tutor 
statements with some student questions and 
positive tutor feedback.  State 1DA constitutes 
Grounding/Extra-Domain, a conversational state 
consisting of acknowledgments, backchannels, and 
discussions that do not relate to the computer 
science task.  State 2DA, Student Reflection, 
                                                           
3 For simplicity, the states of each HMM have been named 
according to an intuitive interpretation of the emission 
probability distribution.   
generates student evaluation questions, statements, 
and positive and negative feedback.  State 3DA is 
comprised of tutor utterances, with positive 
feedback occurring most commonly followed by 
statements, grounding, lukewarm feedback, and 
negative feedback.  This state is interpreted as a 
Tutor Feedback mode.  Finally, State 4DA, Tutor 
Lecture/Probing, is characterized by tutor 
statements and evaluative questions with some 
student grounding statements.   
 
 
Figure 2.  Emission Probability Distributions for 
Dialogue Act HMM 
 
    The state transition diagram (Figure 3) illustrates 
that Tutor Lecture (0DA) and Grounding/Extra-
Domain (1DA) are stable states whose probability of 
self-transition is high:  0.75 and 0.79, respectively.  
Perhaps not surprisingly, Student Reflection (2DA) 
is most likely to transition to Tutor Feedback (3DA) 
with probability 0.77.  Tutor Feedback (3DA) 
transitions to Tutor Lecture (0DA) with probability 
0.60, Tutor Lecture/Probing (4DA) with probability 
0.26, and Student Reflection (2DA) with probability 
0.09.  Finally, Tutor Lecture/Probing (4DA) very 
often transitions to Student Reflection (2DA) with 
probability 0.82. 
 
22
 
Figure 3. Transition diagram for dialogue act HMM 
4.2 Adjacency Pair HMM 
Figure 4 displays the emission probability 
distributions for the HMM that was trained on the 
input sequences of adjacency pairs.  State 0AP, 
Tutor Lecture, consists of tutorial statements, 
positive feedback, and dialogue turns initiated by 
student questions.  In this state, student evaluation 
questions occur in adjacency pairs with positive 
tutor feedback, and other student questions are 
answered by tutorial statements.  State 1AP, Tutor 
Evaluation, generates primarily tutor evaluation 
questions, along with the adjacency pair of tutorial 
statements followed by student acknowledgements.  
State 2AP generates conversational grounding and 
extra-domain talk; this Grounding/Extra-Domain 
state is dominated by the adjacency pair of student 
grounding followed by tutor grounding.  State 3AP 
is comprised of several adjacency pairs:  student 
questions followed by tutor answers, student 
statements with positive tutor feedback, and 
student evaluation questions followed by positive 
feedback.  This Question/Answer state also 
generates some tutor grounding and student 
evaluation questions outside of adjacency pairs.   
 
 
Figure 4.  Emission Probability Distributions for 
Adjacency Pair HMM 
 
 
Figure 5. Transition diagram for adjacency pair HMM 
0DA 
3DA 
2DA 
1DA 
4DA 
p > 0.5 
0.1 ? p ? 0.50 
0.05 ? p < 0.1 
0AP 
3AP 
2AP 
1AP 
p > 0.5 
0.1 ? p ? 0.50 
0.05 ? p < 0.1 
23
 4.3 Dialogue Mode Sequences 
In order to illustrate how the above models fit the 
data, Figure 6 depicts the progression of dialogue 
modes that generate an excerpt from the corpus. 
 
 
Figure 6.  Best-fit sequences of hidden states 
In both models, the most commonly-occurring 
dialogue mode is Tutor Lecture, which generates 
45% of observations in the dialogue act model and 
around 60% in the adjacency pair model.  
Approximately 15% of the dialogue act HMM 
observations are fit to each of states Student 
Reflection, Tutor Feedback, and Tutor 
Lecture/Probing.  This model spends the least 
time, around 8%, in Grounding/Extra Domain.  
The adjacency pair model fits approximately 15% 
of its observations to each of Tutor Evaluation and 
Question/Answer, with around 8% in 
Grounding/Extra-Domain.   
4.4 Model Comparison 
While the two models presented here describe the 
same corpus, it is important to exercise caution 
when making direct structural comparisons.  The 
models contain neither the same number of hidden 
states nor the same emission symbol alphabet; 
therefore, our comparison will be primarily 
qualitative.  It is meaningful to note, however, that 
the adjacency pair model with nAP=4 achieved an 
average log-likelihood fit on the training data that 
was 5.8% better than the same measure achieved 
by the dialogue act model with nDA=5, despite the 
adjacency pair input sequences containing greater 
than twice the number of unique symbols.4   
                                                           
4 This comparison is meaningful because the models depicted 
here provided the best fit among all sizes of models trained for 
the same input scenario. 
    Our qualitative comparison begins by examining 
the modes that are highly similar in the two 
models.  State 2AP generates grounding and extra-
domain statements, as does State 1DA.  These two 
states both constitute a Grounding/Extra-Domain 
dialogue mode.  One artifact of the tutoring study 
design is that all sessions begin in this state due to 
a compulsory greeting that signaled the start of 
each session.  More precisely, the initial state 
probability distribution for each HMM assigns 
probability 1 to this state and probability 0 to all 
other states.     
    Another dialogue mode that is structurally 
similar in the two models is Tutor Lecture, in 
which the majority of utterances are tutor 
statements.  This mode is captured in State 0 in 
both models, with State 0AP implying more detail 
than State 0DA because it is certain in the former 
that some of the tutor statements and positive 
feedback occurred in response to student questions.  
While student questions are present in State 0DA, no 
such precise ordering of the acts can be inferred, as 
discussed in Section 1.    
    Other states do not have one-to-one 
correspondence between the two models.  State 
2DA, Student Reflection, generates only student 
utterances and the self-transition probability for the 
state is very low; the dialogue usually visits State 
2DA for one turn and then transitions immediately 
to another state.  Although this aspect of the model 
reflects the fact that students rarely keep the floor 
for more than one utterance at a time in the corpus, 
such quick dialogue mode transitions are 
inconsistent with an intuitive understanding of 
tutorial dialogue modes as meta-structures that 
usually encompass more than one dialogue turn.  
This phenomenon is perhaps more accurately 
captured in the adjacency pair model.  For 
example, the dominant dialogue act of State 2DA is 
a student evaluation question (EQs).  In contrast, 
these dialogue acts are generated as part of an 
adjacency pair by State 3AP; this model joins the 
student questions with subsequent positive 
feedback from the tutor rather than generating the 
question and then transitioning to a new dialogue 
mode.  Further addressing the issue of frequent 
state transitions is discussed as future work in 
Section 6. 
     
24
5 Discussion and Limitations 
Overall, the adjacency pair model is preferable for 
our purposes because its structure lends itself more 
readily to interpretation as a set of dialogue modes 
each of which encompasses more than one 
dialogue move.  This structural property is 
guaranteed by the inclusion of adjacency pairs as 
atomic elements.  In addition, although the set of 
emission symbols increased to include significant 
adjacency pairs along with all dialogue acts, the 
log-likelihood fit of this model was slightly higher 
than the same measure for the HMM trained on the 
sequences of dialogue acts alone.  The remainder 
of this section focuses on properties of the 
adjacency pair model. 
    One promising result of this early work emerges 
from the fact that by applying hidden Markov 
modeling to sequences of adjacency pairs, 
meaningful dialogue modes have emerged that are 
empirically justified.  The number of these 
dialogue modes is consistent with what researchers 
have traditionally used as a set of hypothesized 
tutorial dialogue modes.  Moreover, the 
composition of the dialogue modes reflects some 
recognizable aspects of tutoring sessions:  tutors 
teach through the Tutor Lecture mode and give 
feedback on student knowledge in a Tutor 
Evaluation mode.  Students ask questions and state 
their own perception of their knowledge in a 
Question/Answer mode.  Both parties engage in 
?housekeeping? talk containing such things as 
greetings and acknowledgements, and sometimes, 
even in a controlled environment, extra-domain 
conversation occurs between the conversants in the 
Grounding/Extra-Domain mode.   
    Although the tutorial modes discovered may not 
map perfectly to sets of handcrafted tutorial 
dialogue modes from the literature (e.g., Cade et 
al. 2008), it is rare for such a perfect mapping to 
exist even between those sets of handcrafted 
modes.  In addition, the HMM framework allows 
for succinct probabilistic description of the 
phenomena at work during the tutoring session:  
through the state transition matrix, we can see the 
back-and-forth flow of the dialogue among its 
modes. 
6 Conclusions and Future Work 
Automatically learning dialogue structure is an 
important step toward creating more robust tutorial 
dialogue management systems.  We have presented 
two hidden Markov models in which the hidden 
states are interpreted as dialogue modes for task-
oriented tutorial dialogue.  These models were 
learned in an unsupervised fashion from manually-
labeled dialogue acts.  HMMs offer concise 
stochastic models of the complex interaction 
patterns occurring in natural language tutorial 
dialogue.  The evidence suggests this 
methodology, which as presented requires only a 
sequence of dialogue acts as input, holds promise 
for automatically discovering the structure of 
tutorial dialogue.   
    Future work will involve conducting evaluations 
to determine the benefits gained by using HMMs 
compared to simpler statistical models.  In 
addition, it is possible that more general types of 
graphical models will prove useful in overcoming 
some limitations of HMMs, such as their arbitrarily 
frequent state transitions, to more readily capture 
the phenomena of interest.  The descriptive insight 
offered by these exploratory models may also be 
increased by future work in which the input 
sequences are enhanced with information about the 
surface-level content of the utterance.  In addition, 
knowledge of the task state within the tutoring 
session can be used to segment the dialogue in 
meaningful ways to further refine model structure.   
    It is also hoped that these models can identify 
empirically-derived tutorial dialogue structures that 
can be associated with measures of effectiveness 
such as student learning (Soller & Stevens 2007).  
These lines of investigation could inform the 
development of next-generation natural language 
tutorial dialogue systems.   
Acknowledgments 
Thanks to Marilyn Walker and Dennis Bahler for 
insightful early discussions on the dialogue and machine 
learning aspects of this work, respectively.  This 
research was supported by the National Science 
Foundation under Grants REC-0632450, IIS-0812291, 
CNS-0540523, and GRFP.  Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not necessarily 
reflect the views of the National Science Foundation. 
25
 
References 
Aleven, V., K. Koedinger, and O. Popescu. 2003. A 
tutorial dialog system to support self-explanation: 
Evaluation and open questions. Proceedings of the 
11th International Conference on Artificial 
Intelligence in Education: 39-46. 
Arnott, E., P. Hastings, and D. Allbritton. 2008. 
Research methods tutor: Evaluation of a dialogue-
based tutoring system in the classroom. Behavioral 
Research Methods 40(3): 694-698. 
Bangalore, S., Di Fabbrizio, G., and Stent, A. 2006. 
Learning the structure of task-driven human-human 
dialogs.  Proceedings of the 21st International 
Conference on Computational Linguistics and 44th 
Annual Meeting of the ACL: 201-208. 
Barzilay, R., and Lee, L. 2004. Catching the drift: 
Probabilistic content models, with applications to 
generation and summarization.  Proceedings of 
NAACL HLT: 113?120. 
Boyer, K. E., Phillips, R., Wallis, M., Vouk, M., and 
Lester, J. 2008. Balancing cognitive and 
motivational scaffolding in tutorial dialogue.  
Proceedings of the 9th International Conference on 
Intelligent Tutoring Systems: 239-249. 
Cade, W., Copeland, J., Person, N., and D'Mello, S. 
2008. Dialog modes in expert tutoring.  Proceedings 
of the 9th International Conference on Intelligent 
Tutoring Systems: 470-479. 
Chi, M., Jordan, P., VanLehn, K., and Hall, M. 2008. 
Reinforcement learning-based feature selection for 
developing pedagogically effective tutorial dialogue 
tactics.  Proceedings of the 1st International 
Conference  on Educational Data Mining: 258-265. 
Evens, M., and J. Michael. 2006. One-on-one tutoring 
by humans and computers. Lawrence Erlbaum 
Associates, Mahwah, New Jersey. 
Forbes-Riley, K., and Litman, D. J. 2005. Using 
bigrams to identify relationships between student 
certainness states and tutor responses in a spoken 
dialogue corpus. Proceedings of the 6th SIGdial 
Workshop on Discourse and Dialogue: 87-96. 
Forbes-Riley, K., Rotaru, M., Litman, D. J., and 
Tetreault, J. 2007. Exploring affect-context 
dependencies for adaptive system development. 
Proceedings of NAACL HLT: 41-44. 
Graesser, A., G. Jackson, E. Mathews, H. Mitchell, A. 
Olney, M. Ventura, and P. Chipman. 2003. 
Why/AutoTutor: A test of learning gains from a 
physics tutor with natural language dialog. 
Proceedings of the Twenty-Fifth Annual Conference 
of the Cognitive Science Society: 1-6. 
Graesser, A. C., N. K. Person, and J. P. Magliano. 1995. 
Collaborative dialogue patterns in naturalistic one-
to-one tutoring. Applied Cognitive Psychology 9(6): 
495?522. 
Lepper, M. R., M. Woolverton, D. L. Mumme, and J. L. 
Gurtner. 1993. Motivational techniques of expert 
human tutors: Lessons for the design of computer-
based tutors. Pages 75-105 in S. P. Lajoie, and S. J. 
Derry, editors. Computers as cognitive tools. 
Lawrence Erlbaum Associates, Hillsdale, New 
Jersey. 
Litman, D. J., C. P. Ros?, K. Forbes-Riley, K. VanLehn, 
D. Bhembe, and S. Silliman. 2006. Spoken versus 
typed human and computer dialogue tutoring. 
International Journal of Artificial Intelligence in 
Education 16(2): 145-170. 
Purver, M., Kording, K. P., Griffiths, T. L., and 
Tenenbaum, J. B. 2006. Unsupervised topic 
modelling for multi-party spoken discourse.  
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the ACL: 17-24. 
Rabiner, L. R. 1989. A tutorial on hidden Markov 
models and selected applications in speech 
recognition. Proceedings of the IEEE 77(2): 257-
286. 
Schlegoff, E., and H. Sacks. 1973. Opening up closings. 
Semiotica 7(4): 289-327. 
Scott, S. L. 2002. Bayesian methods for hidden Markov 
models: Recursive computing in the 21st century. 
Journal of the American Statistical Association 
97(457): 337-352. 
Soller, A., and R. Stevens. 2007. Applications of 
stochastic  analyses for collaborative learning and 
cognitive assessment. Pages 217-253 in G. R. 
Hancock, and K. M. Samuelsen, editors. Advances 
in latent variable mixture models. Information Age 
Publishing. 
Tetreault, J. R., and D. J. Litman. 2008. A 
reinforcement learning approach to evaluating state 
representations in spoken dialogue systems. Speech 
Communication 50(8-9): 683-696. 
VanLehn, K., P. W. Jordan, C. P. Rose, D. Bhembe, M. 
Bottner, A. Gaydos, M. Makatchev, U. 
Pappuswamy, M. Ringenberg, and A. Roque. 2002. 
The architecture of Why2-atlas: A coach for 
qualitative physics essay writing. Proceedings of 
Intelligent Tutoring Systems Conference: 158?167. 
Zinn, C., Moore, J. D., and Core, M. G. 2002. A 3-tier 
planning architecture for managing tutorial dialogue.  
Proceedings of the 6th International Conference on 
Intelligent Tutoring Systems: 574-584. 
26
Proceedings of NAACL HLT 2009: Short Papers, pages 49?52,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Modeling Dialogue Structure with  Adjacency Pair Analysis and Hidden Markov Models  Kristy Elizabeth Boyer*1 Robert Phillips1,2 Eun Young Ha1 Michael D.  Wallis1,2 Mladen A.  Vouk1 James C. Lester1  1Department of Computer Science North Carolina State University Raleigh, NC, USA  2Applied Research Associates Raleigh, NC, USA  *keboyer@ncsu.edu  Abstract 
Automatically detecting dialogue structure within corpora of human-human dialogue is the subject of increasing attention.  In the do-main of tutorial dialogue, automatic discovery of dialogue structure is of particular interest because these structures inherently represent tutorial strategies or modes, the study of which is key to the design of intelligent tutor-ing systems that communicate with learners through natural language.  We propose a methodology in which a corpus of human-human tutorial dialogue is first manually an-notated with dialogue acts.  Dependent adja-cency pairs of these acts are then identified through ?2 analysis, and hidden Markov mod-eling is applied to the observed sequences to induce a descriptive model of the dialogue structure.       
1 Introduction Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven ap-proaches (e.g., Bangalore et al, 2006).  Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (Forbes-Riley et al, 2007), but also contribute to our understanding of 
the cognitive and affective processes involved in learning through tutoring (VanLehn et al, 2007).        Although traditional top-down approaches (e.g., Cade et al, 2008) and some empirical work on analyzing the structure of tutorial dialogue (Forbes-Riley et al, 2007) have yielded significant results, the field is limited by the lack of an auto-matic, data-driven approach to identifying dialogue structure.  An empirical approach to identifying tutorial dialogue strategies, or modes, could ad-dress this limitation by providing a mechanism for describing in succinct probabilistic terms the tuto-rial strategies that actually occur in a corpus.      Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al, 2000), we pro-pose a system that uses HMMs to capture the structure of tutorial dialogue implicit within se-quences of already-tagged dialogue acts.  This ap-proach operates on the premise that at any given point in the tutorial dialogue, the collaborative in-teraction is in a dialogue mode that characterizes the nature of the exchanges between tutor and stu-dent.  In our model, a dialogue mode is defined by a probability distribution over the observed sym-bols (e.g., dialogue acts and adjacency pairs).      Our previous work has noted some limitations of first-order HMMs as applied to sequences of individual dialogue acts (Boyer et al, in press).  Chief among these is that HMMs allow arbitrarily frequent transitions between hidden states, which does not conform well to human intuition about how tutoring strategies are applied.  Training an HMM on a sequence of adjacency pairs rather than individual dialogue acts is one way to generate a 
49
more descriptive model without increasing model complexity more than is required to accommodate the expanded set of observation symbols.  To this end, we apply the approach of Midgley et al (2006) for empirically identifying significant adja-cency pairs within dialogue, and proceed by treat-ing adjacency pairs as atomic units for the purposes of training the HMM.   2 Corpus Analysis This analysis uses a corpus of human-human tuto-rial dialogue collected in the domain of introduc-tory computer science.  Forty-three learners interacted remotely with a tutor through a key-board-to-keyboard remote learning environment yielding 4,864 dialogue moves.    The tutoring corpus was manually tagged with dialogue acts designed to capture the salient char-acteristics of the tutoring process (Table 1).  Tag Act Example Q Question Where should I  declare i? EQ Evaluation Question How does that look? S Statement You need a  closing brace. G Grounding Ok.  EX Extra-Domain You may use  your book. PF Positive Feedback Yes, that?s right. LF Lukewarm Feedback Sort of. NF Negative Feedback No, that?s not right. Table 1. Dialogue Act Tags  The correspondence between utterances and dia-logue act tags is one-to-one.  Compound utterances (i.e., a single utterance comprising more than one dialogue act) were split by the primary annotator prior to the inter-rater reliability study.1      The importance of adjacency pairs is well-established in natural language dialogue (e.g., Schlegoff & Sacks, 1973), and adjacency pair analysis has illuminated important phenomena in tutoring as well (Forbes-Riley et al, 2007).  For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.  How-ever, as noted in (Midgley et al, 2006), in order to                                                            1 Details of the study procedure used to collect the corpus, as well as Kappa statistics for inter-rater reliability, are reported in (Boyer et al, 2008). 
establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher prob-ability of the second member occurring.  For this analysis we utilize a ?2 test for independence of the categorical variables acti and acti+1 for all two-way combinations of dialogue act tags.  Only pairs in which speaker(acti)?speaker(acti+1) were consid-ered.  Other dialogue acts were treated as atomic elements in subsequent analysis, as discussed in Section 3.  Table 2 displays a list of the dependent pairs sorted by descending (unadjusted) statistical significance; the subscript indicates tutor (t) or stu-dent (s).  acti acti+1 P(acti+1|       acti) P(acti+1|    ?acti) ?2 val p-val EQs PFt 0.48 0.07 654 <0.0001 Gs Gt 0.27 0.03 380 <0.0001 EXs EXt 0.34 0.03 378 <0.0001 EQt PFs 0.18 0.01 322 <0.0001 EQt Ss 0.24 0.03 289 <0.0001 EQs LFt 0.13 0.01 265 <0.0001 Qt Ss 0.65 0.04 235 <0.0001 EQt LFs 0.07 0.00 219 <0.0001 Qs St 0.82 0.38 210 <0.0001 EQs NFt 0.08 0.01 207 <0.0001 EXt EXs 0.19 0.02 177 <0.0001 NFs Gt 0.29 0.03 172 <0.0001 EQt NFs 0.11 0.01 133 <0.0001 Ss Gt 0.16 0.03 95 <0.0001 Ss PFt 0.30 0.10 90 <0.0001 St Gs 0.07 0.04 36 <0.0001 PFs Gt 0.14 0.04 34 <0.0001 LFs Gt 0.22 0.04 30 <0.0001 St EQs 0.11 0.07 29 <0.0001 Gt EXs 0.07 0.03 14 0.002 St Qs 0.07 0.05 14 0.0002 Gt Gs 0.10 0.05 9 0.0027 EQt EQs 0.13 0.08 8 0.0042 Table 2. Dependent Adjacency Pairs 3 HMM on Adjacency Pair Sequences The keyboard-to-keyboard tutorial interaction re-sulted in a sequence of utterances that were anno-tated with dialogue acts.  We have hypothesized that a higher-level dialogue structure, namely the tutorial dialogue mode, overlays the observed dia-logue acts.  To build an HMM model of this struc-
50
ture we treat dialogue mode as a hidden variable and train a hidden Markov model to induce the dialogue modes and their associated dialogue act emission probability distributions.    An adjacency pair joining algorithm (Figure 1) was applied to each sequence of dialogue acts.  This algorithm joins pairs of dialogue acts into atomic units according to a priority determined by the strength of the adjacency pair dependency.  Sort adjacency pair list L by descending statistical significance For each adjacency pair (act1, act2) in L         For each dialogue act sequence (a1, a2, ?, an)          in the corpus                 Replace all pairs (ai=act1, ai+1=act2) with a                 new single act (act1act2) Figure 1.  Adjacency Pair Joining Algorithm     Figure 2 illustrates the application of the adja-cency pair joining algorithm on a sequence of dia-logue acts.  Any dialogue acts that were not grouped into adjacency pairs at the completion of the algorithm are treated as atomic units in the HMMianalysis.   Original Dialogue Act Sequence: Qs - St - LFt - St - St - Gs - EQs - LFt - St - St - Qs - St After Adjacency Pair Joining Algorithm: QsSt - LFt - St - StGs - EQsLFt - St - St - QsSt Figure 2.  DA Sequence Before/After Joining     The final set of observed symbols consists of 39 tags: 23 adjacency pairs (Table 2) plus all individ-ual dialogue acts augmented with a tag for the speaker (Table 1).      It was desirable to learn n, the best number of hidden states, during modeling rather than specify-ing this value a priori.  To this end, we trained and ten-fold cross-validated seven models (each featur-ing randomly-initialized parameters) for each number of hidden states n from 2 to 15, inclusive.2  The average log-likelihood was computed across all seven models for each n, and this average log-                                                           2 n=15 was chosen as an initial maximum number of states because it comfortably exceeded our hypothesized range of 3 to 7 (informed by the tutoring literature).  The Akaike Infor-mation Criterion measure steadily worsened above n = 5, con-firming no need to train models with n > 15. 
likelihood ln was used to compute the Akaike In-formation Criterion, a maximum-penalized likeli-hood estimator that penalizes more complex models (Scott, 2002).  The best fit was obtained with n=4 (Figure 3).  The transition probability distribution among hidden states is depicted in Figure 4, with the size of the nodes indicating rela-tive frequency of each hidden state; specifically, State 0 accounts for 63% of the corpus, States 1 and 3 account for approximately 15% each, and State 2 accounts for 7%.  
  Figure 3.  Dialogue Act Emission Probability  Distribution by Dialogue Mode3 4 Discussion and Future Work This exploratory application of hidden Markov models involves training an HMM on a mixed in-put sequence consisting of both individual dialogue acts and adjacency pairs.  The best-fit HMM con-sists of four hidden states whose emission symbol probability distributions lend themselves to inter-pretation as tutorial dialogue modes.  For example, State 0 consists primarily of tutor statements and positive feedback, two of the most common dia-logue  acts  in our corpus.  The transition probabili- 
51
 Figure 4.  Transition Probability Distribution4  ties also reveal that State 0 is highly stable; a self-transition is most likely with probability 0.835.  State 3 is an interactive state featuring student re-flection in the form of questions, statements, and requests for feedback.  The transition probabilities show that nearly 60% of the time the dialogue transitions from State 3 to State 0; this may indi-cate that after establishing what the student does or does not know in State 3, the tutoring switches to a less collaborative ?teaching? mode represented by State 0.        Future evaluation of the HMM presented here will include comparison with other types of graphical models.  Another important step is to correlate the dialogue profile of each tutoring ses-sion, as revealed by the HMM, to learning and af-fective outcomes of the tutoring session.  This type of inquiry can lead directly to design recommenda-tions for tutorial dialogue systems that aim to maximize particular learner outcomes.  In addition, leveraging knowledge of the task state as well as surface-level utterance content below the dialogue act level are promising directions for refining the descriptive and predictive power of these models.      Acknowledgements  This research was supported by the National Science Foundation under Grants REC-0632450, IIS-0812291, CNS-0540523, and GRFP.  Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the 
authors and do not necessarily reflect the views of the National Science Foundation.  References Boyer, K.E., Phillips, R., Wallis, M., Vouk, M., & Lester, J. (2008).  Balancing cognitive and moti-vational scaffolding in tutorial dialogue.  Pro-ceedings of the 9th International Conference on Intelligent Tutoring Systems, Montreal, Canada, 239-249. Boyer, K.E., Ha, E.Y., Wallis, M., Phillips, R., Vouk, M. & Lester, J. (in press).  Discovering tutorial dialogue strategies with hidden Markov models.  To appear in Proceedings of the 14th International Conference on Artificial Intelligence in Educa-tion, Brighton, U.K. Bangalore, S., DiFabbrizio, G., Stent, A. (2006).  Learning the structure of task-driven human-human dialogs.  Proceedings of ACL, Sydney, Australia, 201-208. Cade, W., Copeland, J., Person, N., & D'Mello, S. (2008). Dialog modes in expert tutoring. Proceed-ings of the 9th International Conference on Intel-ligent Tutoring Systems, Montreal, Canada, 470-479.  Forbes-Riley, K., Rotaru, M., Litman, D. J., & Tetreault, J. (2007). Exploring affect-context de-pendencies for adaptive system development. Proceedings of NAACL HLT, Companion Volume, 41-44.  Midgley, T. D., Harrison, S., & MacNish, C. (2007). Empirical verification of adjacency pairs using dialogue segmentation. Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, 104-108.  Schlegoff, E.A., Sacks, H. (1973).  Opening up clos-ings.  Semiotica, 8(4), 289-327. Scott, S. L. (2002). Bayesian methods for hidden Markov models: Recursive computing in the 21st century. Journal of the American Statistical Asso-ciation, 97(457), 337-351. Stolcke, A., Coccaro, N., Bates, R., Taylor, P., Van Ess-Dykema, C., Ries, K., Shirberg, E., Jurafsky, D., Martin, R., Meteer, M. (2000).  Dialog act modeling for automatic tagging and recognition of conversational speech.  Computational Linguistics 26(3), 339-373. VanLehn, K., Graesser, A., Jackson, G.T., Jordan, P., Olney, A., Rose, C.P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.  
52
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 53?61,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learner Characteristics and Feedback in Tutorial Dialogue 
 
 
Kristy Elizabeth 
Boyera 
Robert  
Phillipsab 
Michael D. 
Wallisab 
Mladen A. 
Vouka 
James C. 
Lestera 
 
aDepartment of Computer Science, North Carolina State University 
bApplied Research Associates, Inc. 
Raleigh, North Carolina, USA 
{keboyer, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
 
 
 
 
Abstract 
Tutorial dialogue has been the subject of in-
creasing attention in recent years, and it has 
become evident that empirical studies of hu-
man-human tutorial dialogue can contribute 
important insights to the design of computa-
tional models of dialogue.  This paper reports 
on a corpus study of human-human tutorial 
dialogue transpiring in the course of problem-
solving in a learning environment for intro-
ductory computer science.  Analyses suggest 
that the choice of corrective tutorial strategy 
makes a significant difference in the outcomes 
of both student learning gains and self-
efficacy gains.  The findings reveal that tuto-
rial strategies intended to maximize student 
motivational outcomes (e.g., self-efficacy 
gain) may not be the same strategies that 
maximize cognitive outcomes (i.e., learning 
gain).  In light of recent findings that learner 
characteristics influence the structure of tuto-
rial dialogue, we explore the importance of 
understanding the interaction between learner 
characteristics and tutorial dialogue strategy 
choice when designing tutorial dialogue sys-
tems.  
1 Introduction 
Providing intelligent tutoring systems (ITSs) with 
the ability to engage learners in rich natural lan-
guage dialogue has been a goal of the ITS commu-
nity since the inception of the field.  Tutorial 
dialogue has been studied in the context of a num-
ber of systems devised to support a broad range of 
conversational phenomena.  Systems such as 
CIRCSIM (Evens and Michael 2006), BEETLE (Zinn 
et al 2002), the Geometry Explanation Tutor 
(Aleven et al 2003), Why2/Atlas (VanLehn et al 
2002), ITSpoke (Litman et al 2006), SCOT (Pon-
Barry et al 2006), ProPL (Lane and VanLehn 
2005) and AutoTutor (Graesser et al 2003) support 
research that has begun to the see the emergence of 
a core set of foundational requirements for mixed-
initiative natural language interaction that occurs in 
the kind of tutorial dialogue investigated here.  
Moreover, recent years have witnessed the appear-
ance of corpus studies empirically investigating 
speech acts in tutorial dialogue (Marineau et al 
2000), dialogues? correlation with learning 
(Forbes-Riley et al 2005, Core et al 2003, Ros? et 
al. 2003, Katz et al 2003), student uncertainty in 
dialogue (Liscombe et al 2005, Forbes-Riley and 
Litman 2005), and comparing text-based and spo-
ken dialogue (Litman et al 2006). 
     Recent years have also seen the emergence of a 
broader view of learning as a complex process in-
volving both cognitive and affective states.  To 
empirically explore these issues, a number of ITSs 
such as AutoTutor (Jackson et al 2007), Betty?s 
Brain (Tan and Biswas 2006), ITSpoke (Forbes-
Riley et al 2005), M-Ecolab (Rebolledo-Mendez 
et al 2006), and MORE (del Soldato and Boulay 
1995) are being used as platforms to investigate the 
impact of tutorial interactions on affective and mo-
tivational outcomes (e.g., self-efficacy) along with 
purely cognitive measures (i.e., learning gains).  A 
central problem in this line of investigation is iden-
53
tifying tutorial strategies (e.g., Graesser et al 
1995) that can appropriately balance the tradeoffs 
between cognitive and affective student outcomes 
(Lepper et al 1993).  While a rich set of cognitive 
and affective tutorial strategies is emerging (e.g., 
Porayska-Pomsta et al 2004), the precise nature of 
the interdependence between these types of strate-
gies is not well understood.  In addition, it may be 
the case that different populations of learners en-
gage in qualitatively different forms of dialogue.  
Students with particular characteristics may have 
specific dialogue profiles, and knowledge of such 
profiles could inform the design of tutorial systems 
whose strategies leverage the characteristics of the 
target population.  The extent to which different 
tutorial strategies, and specific instances of them in 
certain contexts, may be used to enhance tutorial 
effectiveness is an important question to designers 
of ITSs.    
     Given that human-human tutorial dialogue of-
fers a promising model for effective communica-
tion (Chi et al 2001), our methodology is to study 
naturally occurring tutorial dialogues in a task-
oriented learning environment to investigate the 
relationship between the structure of tutorial dia-
logue, the characteristics of learners, and the im-
pact of cognitive and motivational corrective 
tutorial strategies on learning and self-efficacy 
(Boyer et al in press).  A text-based dialogue inter-
face was incorporated into a learning environment 
for introductory computer science.  In the envi-
ronment, students undertook a programming task 
and conversed with human tutors while designing, 
implementing, and testing Java programs.    
     The results of the study suggest that the choice 
of corrective tutorial strategy has a significant im-
pact on the learning gains and self-efficacy of stu-
dents.  These findings reinforce those of other 
studies (e.g., Lepper et al 1993, Person et al 1995, 
Keller et al 1983) that indicate that some cognitive 
and motivational goals may be at odds with one 
other because a tutorial strategy designed to maxi-
mize one set of goals (e.g., cognitive goals) can 
negatively impact the other.  We contextualize our 
findings in light of recent results that learner char-
acteristics such as self-efficacy influence the struc-
ture of task-oriented tutorial dialogue (Boyer et al 
2007), and may therefore produce important inter-
action effects when considered alongside tutorial 
strategy.    
     This paper is organized as follows.  Section 2 
describes the corpus study, including experimental 
design and tagging of dialogue and student prob-
lem-solving actions.  Section 3 presents analysis 
and results.  Discussion and design implications 
are considered in Section 4, and concluding re-
marks follow in Section 5.  
 
2 Corpus Study 
The corpus was gathered by logging text-based 
dialogues between tutors and novice computer sci-
ence students.  The learning task was to complete a 
Java programming problem that required students 
to apply fundamental concepts such as iteration, 
modularization, and sequential-access data struc-
tures.  This study was conducted to compare the 
impact of certain corrective cognitive and motiva-
tional tutorial strategies on student learning and 
self-efficacy in human-human tutoring.  Specifi-
cally, the study considered the motivational strate-
gies of praise and reassurance (Lepper et al 1993) 
and the category of informational tutorial utter-
ances termed cognitive feedback (Porayska-Pomsta 
et al 2004, Tan and Biswas 2006) that followed 
questionable student problem-solving action.  Fol-
lowing the approach of Forbes-Riley (2005) and 
others (Marineau et al 2000), utterances from a 
corpus of human-human tutorial dialogues were 
annotated with dialogue acts.  Then, adopting the 
approach proposed by Ohlsson et al (2007), statis-
tical modeling techniques were employed to quan-
tify the relative impact of these different tutorial 
strategies on the outcomes of interest (in this case, 
learning and self-efficacy gains).     
 
2.1 Experimental Design 
Subjects were students enrolled in an introductory 
computer science course and were primarily 
freshman or sophomore engineering majors in dis-
ciplines such as mechanical, electrical, and com-
puter engineering. 
     The corpus was gathered from tutor-student 
interactions between 43 students and 14 tutors dur-
ing a two-week study.  Tutors and students were 
completely blind to each other?s characteristics as 
they worked together remotely from separate labs.  
Tutors observed student problem-solving actions 
54
(e.g., programming, scrolling, executing programs) 
in real time.  Tutors had varying levels of tutoring 
experience, and were not instructed about specific 
tutorial strategies. 
     Subjects first completed a pre-survey including 
items about self-efficacy, attitude toward computer 
science, and attitude toward collaboration.  Sub-
jects then completed a ten item pre-test over spe-
cific topic content.  The tutorial session was 
controlled at 55 minutes for all subjects, after 
which subjects completed a post-survey and post-
test containing variants of the items on the pre- 
versions.   
 
2.2 Problem-Solving Tagging 
The raw corpus contains 4,864 dialogue moves:  
1,528 student utterances and 3,336 tutor utterances.  
As a chronology of tutorial dialogue interleaved 
with student problem-solving (programming) ac-
tions that took place during the tutoring sessions, 
the corpus contains 29,996 programming key-
strokes and 1,277 periods of scrolling ? all per-
formed by students.  Other problem-solving 
actions, such as opening and closing files or run-
ning the program, were sparse and were therefore 
eliminated from the analyses.  Of the 3,336 tutor 
utterances, 1,243 occur directly after ?question-
able? student problem-solving action.  (The notion 
of ?questionable? is defined below.)  This subset of 
tutorial utterances serves as the basis for the tuto-
rial strategy comparison. 
     Student problem-solving actions were logged 
throughout tutoring sessions.  Two actions were 
under consideration for the analysis:  typing in the 
programming interface and scrolling in the pro-
gram editor window.  To interpret the raw logged 
student problem-solving actions, these events were 
automatically tagged using a heuristic measure for 
correctness: if a problem-solving action was a pro-
gramming keystroke (character) that survived until 
the end of the session, this event was tagged prom-
ising, to indicate it was probably correct.  If a prob-
lem-solving act was a programming keystroke 
(character) that did not survive until the end of the 
session, the problem-solving act was tagged ques-
tionable.  Both these heuristics are based on the 
observation that in this tutoring context, students 
solved the problem in a linear fashion and tutors 
did not allow students to proceed past a step that 
had incorrect code in place.  Finally, periods of 
consecutive scrolling were also marked question-
able because in a problem whose entire solution 
fits on one printed page, scrolling was almost uni-
formly undertaken by a student who was confused 
and looking for answers in irrelevant skeleton code 
provided to support the programming task.   
 
2.3 Dialogue Act Tagging 
Because utterances communicate through two 
channels, a cognitive channel and a motiva-
tional/affective channel, each utterance was 
annotated with both a required cognitive dialogue 
tag (Table 1) and an optional motiva-
tional/affective dialogue tag (Table 2).  While no 
single standardized dialogue act tag set has been 
identified for tutorial dialogue, the tags applied 
here were drawn from several schemes in the tuto-
rial dialogue and broader dialogue literature.  A 
coding scheme for tutorial dialogue in the domain 
of qualitative physics influenced the creation of the 
tag set (Forbes-Riley et al 2005), as did the four-
category scheme (Marineau et al 2000).  A more 
expansive general dialogue act tag set alo contrib-
uted commonly occurring acts (Stolcke et al 
2000).  The motivational tags were drawn from 
work by Lepper (1993) on motivational strategies 
of human tutors.   
     Table 1 displays the cognitive subset of this 
dialogue act tag set, while Table 2 displays the mo-
tivational/affective tags.  It should be noted that a 
cognitive tag was required for each utterance, 
while a motivational/affective tag was applied only 
to the subset of utterances that communicated in 
that channel.  If an utterance constituted a strictly 
motivational/affective act, its cognitive channel 
was tagged with EX (EXtra-domain) indicating 
there was no relevant cognitive content.  On the 
other hand, some utterances had both a cognitive 
component and a motivational/affective compo-
nent.  For example, a tutorial utterance of, ?That 
looks great!? would have been tagged as positive 
feedback (PF) in the cognitive channel, and as 
praise (P) in the motivational/affective channel.  In 
contrast, the tutorial move ?That?s right,? would be 
tagged as positive feedback (PF) in the cognitive 
channel and would not be annotated with a motiva-
tional/affective tag.  Table 3 shows an excerpt 
from the corpus with dialogue act tags applied. 
55
     The entire corpus was tagged by a single human 
annotator, with a second tagger marking 1,418 of 
the original 4,864 utterances.  The resulting kappa 
statistics were 0.76 in the cognitive channel and 
0.64 in the motivation channel.   
3 Analysis and Results 
Overall, these tutoring sessions were effective: 
they yielded learning gains (difference between 
posttest and pretest) with mean 5.9% and median 
7.9%, which were statistically significant 
(p=0.038), and they produced self-efficacy gains
Table 1:  Cognitive Channel Dialogue Acts 
56
(difference between pre-survey and post-survey 
scores) with mean 12.1% and median 12.5%, 
which were also statistically significant 
(p<0.0001).  Analyses revealed that statistically 
significant relationships hold between tutorial 
strategy and learning, as well as between tutorial 
strategy and self-efficacy gains.   
 
3.1 Analysis 
First, the values of learning gain and self-efficacy 
gain were grouped into binary categories (?Low?, 
?High?) based on the median value.  We then ap-
plied multiple logistic regression with the gain 
category as the predicted value.  Tutorial strategy, 
incoming self-efficacy rating, and pre-test score 
were predictors in the model.  The binarization 
approach followed by multiple logistic regression 
was chosen over multiple linear regression on a 
continuous response variable because the learning 
instruments (10 items each) and self-efficacy ques-
tionnaires (5 items each) yielded few distinct val-
ues of learning gain, meaning the response variable 
(learning gain and self-efficacy gain, respectively) 
would not have been truly continuous in nature.  
Logistic regression is used for binary response 
variables; it computes the odds of a particular out-
come over another (e.g., ?Having high learning 
gain versus low learning gain?) given one value of 
the predictor variable over another (e.g., ?The cor-
rective tutorial strategy chosen was positive cogni-
tive feedback instead of praise?). 
 
Table 2:  Motivational/Affective Channel Dialogue Acts 
57
3.2 Results 
After accounting for the effects of pre-test score 
and incoming self-efficacy rating (both of which 
were significant in the model with p<0.001), ob-
servations containing tutorial encouragement were 
56% less likely to result in high learning gain than 
observations without explicit tutorial encourage-
ment (p=0.001).  On the other hand, an analogous 
model of self-efficacy gain revealed that tutorial 
encouragement was 57% more likely to result in 
high self-efficacy gain compared to tutorial re-
sponses that had no explicit praise or reassurance 
(p=0.054).  These models suggested that the pres-
ence of tutorial encouragement in response to 
questionable student problem-solving action may 
enhance self-efficacy gain but detract from learn-
ing gain. 
    Another significant finding was that observa-
tions in which the tutor used cognitive feedback 
plus praise were associated with 40% lower likeli-
hood of high learning gain than observations in 
which the tutor used purely cognitive feedback.  
No impact was observed on self-efficacy gain.  
These results suggest that in response to question-
able student problem-solving action, to achieve 
learning gains, purely cognitive feedback is pre-
ferred over cognitive feedback plus praise, while 
self-efficacy gain does not appear to be impacted 
either way. 
     Among students with low incoming self-
efficacy, observations in which the tutor employed 
a standalone motivational act were 300% as likely 
to be in the high self-efficacy gain group as obser-
vations in which the tutor employed a purely cog-
nitive statement or a cognitive statement combined 
with encouragement (p=0.039).  In contrast, among 
students with high initial self-efficacy, a purely 
motivational tactic resulted in 90% lower odds of 
being in the high self-efficacy gain group.  These 
results suggest that standalone praise or reassur-
ance may be useful for increasing self-efficacy 
gain among low initial self-efficacy students, but 
may decrease self-efficacy gain in high initial self-
efficacy students.   
     Considering strictly cognitive feedback, posi-
tive feedback resulted in 190% increased odds of 
high student self-efficacy gain compared to the 
other cognitive strategies (p=0.0057).  Positive 
cognitive feedback did not differ significantly from 
other types of cognitive strategies in a Chi-square 
comparison with respect to learning gains 
(p=0.390).  The models thus suggest when dealing 
with questionable student problem-solving action, 
positive cognitive feedback is preferable to other 
types of cognitive feedback for eliciting self-
efficacy gains, but this type of feedback is not 
Table 3:  Dialogue Excerpts 
58
found to be better or worse than other cognitive 
feedback for effecting learning gains. 
 
4 Discussion 
The study found that the presence of direct tutorial 
praise or encouragement in response to question-
able student problem-solving action increased the 
odds that the student reported high self-efficacy 
gain while lowering the odds of high learning gain.  
The study also found that, with regard to learning 
gains, purely cognitive feedback was preferable to 
cognitive feedback with an explicitly motivational 
component.  These empirical findings are consis-
tent with theories of Lepper et al (1993) who 
found that some cognitive and affective goals in 
tutoring are ?at odds.?  As would be predicted, the 
results also echo recent quantitative results from 
other tutoring domains such as qualitative physics 
(Jackson et al 2007) and river ecosystems (Tan 
and Biswas 2006) that, in general, overt motiva-
tional feedback contributes to motivation but cog-
nitive feedback matters more for learning.   
      Of the corrective tutorial strategies that were 
exhibited in the corpus, positive cognitive feed-
back emerged as an attractive approach for re-
sponding to plausibly incorrect student problem-
solving actions.  Responding positively (e.g., 
?Right?) to questionable student actions is an ex-
ample of indirect correction, which is recognized 
as a polite strategy (e.g., Porayska-Pomsta et al 
2004).  A qualitative investigation of this phe-
nomenon revealed that in the corpus, tutors gener-
ally followed positive feedback in this context with 
more substantive cognitive feedback to address the 
nature of the student?s error.  As such, the positive 
feedback approach seems to have an implicit, yet 
perceptible, motivational component while retain-
ing its usefulness as cognitive feedback. 
    This study found that explicit motivational acts, 
when applied as corrective tutorial approaches, had 
different impacts on different student subgroups.  
Students with low initial self-efficacy appeared to 
benefit more from praise and reassurance than stu-
dents with high initial self-efficacy.  In a prior cor-
pus study to investigate the impact of learner 
characteristics on tutorial dialogue (Boyer et al 
2007), we also found that learners from different 
populations exhibited significantly different dia-
logue profiles.  For instance, high self-efficacy 
students made more declarative statements, or as-
sertions, than low self-efficacy students.  In addi-
tion, tutors paired with high self-efficacy students 
gave more conversational acknowledgments than 
tutors paired with low self-efficacy students, de-
spite the fact that tutors were not made aware of 
any learner characteristics before the tutoring ses-
sion.  Additional dialogue profile differences 
emerged between high and low-performing stu-
dents, as well as between males and females.  To-
gether these two studies suggest that learner 
characteristics influence the structure of tutorial 
dialogue, and that the choice of tutorial strategy 
may impact student subgroups in different ways.             
 
5 Conclusion 
The work reported here represents a first step to-
ward understanding the effects of learner charac-
teristics on task-oriented tutorial dialogue and the 
use of feedback.  Results suggest that positive cog-
nitive feedback may prove to be an appropriate 
strategy for responding to questionable student 
problem-solving actions in task-oriented tutorial 
situations because of its potential for addressing 
the sometimes competing cognitive and affective 
needs of students.  For low self-efficacy students, it 
was found that direct standalone encouragement 
can be used to bolster self-efficacy, but care must 
be used in correctly diagnosing student self-
efficacy because the same standalone encourage-
ment does not appear helpful for high self-efficacy 
students.  These preliminary findings highlight the 
importance of understanding the interaction be-
tween learner characteristics and tutorial strategy 
as it relates to the design of tutorial dialogue sys-
tems. 
     Several directions for future work appear prom-
ising.  First, it will be important to explore the in-
fluence of learner characteristics on tutorial 
dialogue in the presence of surface level informa-
tion about students? utterances.  This line of inves-
tigation is of particular interest given recent results 
indicating that lexical cohesion in tutorial dialogue 
with low-performing students is found to be highly 
correlated with learning (Ward and Litman 2006).   
Second, while the work reported here has consid-
ered a limited set of motivational dialogue acts, 
namely praise and reassurance, future work should 
target an expanded set of affective dialogue acts to 
59
facilitate continued exploration of motivational and 
affective phenomena in this context.  Finally, the 
current results reflect human-human tutoring 
strategies that proved to be effective; however, it 
remains to be seen whether these same strategies 
can be successfully employed in tutorial dialogue 
systems.  Continuing to identify and empirically 
compare the effectiveness of alternative tutorial 
strategies will build a solid foundation for choos-
ing and implementing strategies that consider 
learner characteristics and successfully balance the 
cognitive and affective concerns surrounding the 
complex processes of teaching and learning 
through tutoring. 
Acknowledgments 
The authors wish to thank Scott McQuiggan and 
the members of the Intellimedia Center for Intelli-
gent Systems for their ongoing intellectual contri-
butions, and the Realsearch Group at NC State 
University for extensive project development sup-
port.  This work was supported in part by the Na-
tional Science Foundation through Grant REC-
0632450, an NSF Graduate Research Fellowship, 
and the STARS Alliance Grant CNS-0540523.  
Any opinions, findings, conclusions or recommen-
dations expressed in this material are those of the 
author(s) and do not necessarily reflect the views 
of the National Science Foundation.  Support was 
also provided by North Carolina State University 
through the Department of Computer Science and 
the Office of the Dean of the College of Engineer-
ing.   
 
References  
Kristy Elizabeth Boyer, Robert Phillips, Michael Wallis, 
Mladen Vouk, and James Lester.  In press.  Balanc-
ing cognitive and motivational scaffolding in tutorial 
dialogue.  To appear in Proceedings of the 9th Inter-
national Conference on Intelligent Tutoring Systems. 
Kristy Elizabeth Boyer, Mladen Vouk, and James Les-
ter.  2007.  The influence of learner characteristics on 
task-oriented tutorial dialogue.  Proceedings of 
AIED, pp. 127-134.  IOS Press. 
Vincent Aleven, Kenneth R. Koedinger, and Octav 
Popescu.  2003.  A tutorial dialog system to support 
self-explanation: Evaluation and open questions.  
Proceedings of the 11th International Conference on 
Artificial Intelligence in Education, pp. 39-46.  Am-
sterdam.  IOS Press. 
Vincent Aleven, Bruce McLaren, Ido Roll, and Kenneth 
Koedinger.  2004.  Toward tutoring help seeking: 
Applying cognitive modeling to meta-cognitive 
skills.  J. C. Lester, R. M. Vicari, and F. Paragua?u 
(Eds.), Proceedings of the 7th International Confer-
ence on Intelligent Tutoring Systems, pp. 227-239.  
Berlin: Springer Verlag. 
Albert Bandura.  2006.  Guide for constructing self-
efficacy scales.  T. Urdan and F. Pajares (Eds.): Self-
Efficacy Beliefs of Adolescents, pp. 307-337.  Infor-
mation Age Publishing, Greenwich, Connecticut. 
Michelene T. H. Chi, Nicholas De Leeuw, Mei-Hung 
Chiu, and Christian LaVancher.  1994.  Eliciting self-
explanations improves understanding.  Cognitive Sci-
ence, 18:439-477. 
Michelene T. H. Chi, Stephanie A. Siler, Heisawn 
Jeong, Takashi Yamauchi, and Robert G. Hausmann.  
2001.  Learning from human tutoring.  Cognitive Sci-
ence, 25(4):471-533. 
Mark G. Core, Johanna D. Moore, and Claus Zinn.  
2003.  The role of initiative in tutorial dialogue.  Pro-
ceedings of the Tenth Conference on European 
Chapter of the Association for Computational Lin-
guistics, pp. 67-74. 
Teresa del Soldato and Benedict du Boulay.  1995.  Im-
plementation of motivational tactics in tutoring sys-
tems.  Journal of Artificial Intelligence in Education, 
6(4):337-378.  Association for the Advancement of 
Computing in Education, USA. 
Martha Evens and Joel Michael.  2006.  One-on-One 
Tutoring by Humans and Computers.  Mahwah, New 
Jersey: Lawrence Erlbaum Associates. 
Kate Forbes-Riley and Diane Litman.  2005.  Using 
bigrams to identify relationships between student cer-
tainness states and tutor responses in a spoken dia-
logue corpus.  Proceedings of the 6th SIGdial 
Workshop on Discourse and Dialogue.  Lisbon, Por-
tugal. 
Kate Forbes-Riley, Diane Litman, Alison Huettner, and 
Arthur Ward.  2005.  Dialogue-learning correlations 
in spoken dialogue tutoring.  Looi, C-k., Mccalla, G., 
Bredeweg, B., Breuker, J. (Eds.): Proceedings of 
AIED, pp. 225-232.  IOS Press. 
Arthur C. Graesser, George T. Jackson, Eric Mathews, 
Heather H. Mitchell, Andrew Olney, Mathew Ven-
tura, Patrick Chipman, Donald R. Franceschetti, 
Xiangen Hu, Max M. Louwerse, Natalie K. Person, 
and the Tutoring Research Group.  2003.  
Why/AutoTutor: A test of learning gains from a 
physics tutor with natural language dialog.  Proceed-
ings of the Twenty-Fifth Annual Conference of the 
Cognitive Science Society, pp. 474-479. 
Arthur C. Graesser, Natalie K. Person, and Joseph P. 
Magliano.  1995.  Collaborative dialogue patterns in 
naturalistic one-to-One tutoring.  Applied Cognitive 
Psychology, 9(6):495-522.  John Wiley & Sons, Ltd. 
60
G. Tanner Jackson and Art Graesser.  2007.  Content 
matters: An investigation of feedback categories 
within an ITS.  Luckin, R., Koedinger, K. R., Greer, 
J. (Eds.): Proceedings of AIED 2007, 158:127-134.  
IOS Press. 
Sandra Katz, David Allbritton, and John Connelly.  
2003.  Going beyond the problem given: How human 
tutors use post-solution discussions to support trans-
fer.  International Journal of Artificial Intelligence in 
Education, 13:79-116. 
John M. Keller.  1983.  Motivational design of instruc-
tion.  Reigeluth, C.M. (Ed.): Instructional-Design 
Theories and Models: An Overview of Their Current 
Status, pp. 383-429.  Lawrence Erlbaum Associates, 
Inc., Hillsdale, NJ. 
H. Chad Lane and Kurt VanLehn.  2005.  Teaching the 
tacit knowledge of programming to novices with 
natural language tutoring.  Computer Science Educa-
tion, 15:183-201. 
Mark R. Lepper, Maria Woolverton, Donna L. Mumme, 
and Jean-Luc Gurtner.  1993.  Motivational tech-
niques of expert human tutors: Lessons for the design 
of computer-based tutors.  Lajoie, S.P., Derry, S. J. 
(Eds.): Computers as Cognitive Tools, pp. 75-105. 
Lawrence Erlbaum Associates, Inc., Hillsdale NJ. 
Jackson Liscombe, Julia Hirschberg, and Jennifer J. 
Venditti.  2005.  Detecting certainness in spoken tu-
torial dialogues.  Proceedings of Interspeech, 2005. 
Diane J. Litman, Carolyn P. Ros?, Kate Forbes-Riley, 
Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-
man.  2006.  Spoken versus typed human and com-
puter dialogue tutoring.  International Journal of 
Artificial Intelligence in Education, 16:145-170. 
Johanna Marineau, Peter Wiemer-Hastings, Derek 
Harter, Brent Olde, Patrick Chipman, Ashish Kar-
navat, Victoria Pomeroy, Sonya Rajan, Art Graesser, 
and the Tutoring Research Group.  2000.  Classifica-
tion of speech acts in tutorial dialog.  Proceedings of 
the Workshop on Modeling Human Teaching Tactics 
and Strategies of ITS 2000, pp. 65-71.  Montreal, 
Canada. 
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, 
Davide Fossati, Xin Lu, and Trina C. Kershaw.  
2007.  Beyond the code-and-count analysis of tutor-
ing dialogues.  Luckin, R., Koedinger, K. R., Greer, 
J. (Eds.): Proceedings of AIED 2007, 158:349-356.  
IOS Press. 
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and 
Arthur C. Graesser.  1995.  Pragmatics and peda-
gogy: Conversational rules and politeness strategies 
may inhibit effective tutoring.  Cognition and In-
struction, 13(2):161-188.  Lawrence Erlbaum Asso-
ciates, Inc., Hillsdale, NJ. 
Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt, 
Brady Clark, and Stanley Peters.  2006.  Responding 
to student uncertainty in spoken tutorial dialogue sys-
tems.  International Journal of Artificial Intelligence 
in Education, 16:171-194. 
Ka?ka Porayska-Pomsta and Helen Pain.  2004.  Provid-
ing cognitive and affective scaffolding through teach-
ing strategies:  Applying linguistic politeness to the 
educational context.  J.C. Lester, Vicari, R. M., Para-
gua?u, F. (Eds.): Proceedings of ITS 2004, LNCS 
3220:77-86.  Springer-Verlag Berlin / Heidelberg. 
Genaro Rebolledo-Mendez, Benedict du Boulay, and 
Rosemary Luckin.  2006.  Motivating the learner: an 
empirical evaluation. Ikeda, M., Ashlay, K. D., Chan, 
T.-W. (Eds.): Proceedings of ITS 2006,  LNCS 
4053:545-554.  Springer Verlag Berlin / Heidelberg. 
Carolyn P. Ros?, Dumisizwe Bhembe, Stephanie Siler, 
Ramesh Srivastava, and Kurt VanLehn.  2003.  The 
role of why questions in effective human tutoring.  
Hoppe, U., Verdejo, F., Kay, J. (Eds.): Proceedings 
of AIED 2003, pp. 55-62.  IOS Press. 
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth 
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol Van Ess-Dykema, and 
Marie Meteer.  Dialogue act modeling for automatic 
tagging and recognition of conversational speech.  
2000.  Computational Linguistics, 26:339-373. 
Jason Tan and Gautam Biswas.  2006.  The role of feed-
back in preparation for future learning:  A case study 
in learning by teaching environments.  Ikeda, M., 
Ashley, K., Chan, T.-W. (Eds.): Proceedings of ITS 
2006, LNCS 4053:370-381. Springer-Verlag Berlin / 
Heidelberg. 
Kurt VanLehn, Pamela W. Jordan, Carolyn P. Ros?, 
Dumisizwe Bhembe, Michael Bottner, Andy Gaydos, 
Maxim Makatchev, Umarani Pappuswamy, Michael 
Ringenberg, Antonio Roque, Stephanie Siler, and 
Ramesh Srivastava.  2002.  The architecture of 
Why2-Atlas: A coach for qualitative physics essay 
writing.  Proceedings of the 6th International Con-
ference on Intelligent Tutoring Systems, LNCS 
2363:158-167. 
Arthur Ward and Diane Litman.  2006.  Cohesion and 
learning in a tutorial spoken dialog system.  Proceed-
ings of the 19th International FLAIRS (Florida Artifi-
cial Intelligence Research Society) Conference.  
Melbourne Beach, FL. 
Claus Zinn, Johanna D. Moore, and Mark G. Core.  
2002.  A 3-tier planning architecture for managing 
tutorial dialogue.  Intelligent Tutoring Systems, Sixth 
International Conference.  LNCS 2363:574-584.  
Springer-Verlag, London, UK. 
 
 
61
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1190?1199,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Affect-Enriched Dialogue Act Classification Model  for Task-Oriented Dialogue 
Kristy  Elizabeth  Boyer Joseph F. Grafsgaard Eun Young  Ha Robert  Phillips* James C.  Lester  Department of Computer Science North Carolina State University Raleigh, NC, USA  * Dual Affiliation with Applied Research Associates, Inc. Raleigh, NC, USA  {keboyer, jfgrafsg, eha, rphilli, lester}@ncsu.edu 
 
 
Abstract 
Dialogue act classification is a central chal-lenge for dialogue systems. Although the im-portance of emotion in human dialogue is widely recognized, most dialogue act classifi-cation models make limited or no use of affec-tive channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dia-logue that models facial expressions of users, in particular, facial expressions related to con-fusion. The findings indicate that the affect-enriched classifiers perform significantly bet-ter for distinguishing user requests for feed-back and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively lever-age affective channels to improve dialogue act classification.  1 Introduction Dialogue systems aim to engage users in rich, adaptive natural language conversation. For these systems, understanding the role of a user?s utter-ance in the broader context of the dialogue is a key challenge (Sridhar, Bangalore, & Narayanan, 2009). Central to this endeavor is dialogue act classification, which categorizes the intention be-hind the user?s move (e.g., asking a question, providing declarative information). Automatic dia-logue act classification has been the focus of a 
large body of research, and a variety of approach-es, including sequential models (Stolcke et al, 2000), vector-based models (Sridhar, Bangalore, & Narayanan, 2009), and most recently, feature-enhanced latent semantic analysis (Di Eugenio, Xie, & Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users? expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp & Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, & Fernandez-Dols, 2003; Schmidt & Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dia-logue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spo-ken utterances to aid in online dialogue act classi-fication (Sridhar, Bangalore, & Narayanan, 2009). Other research on emotion in dialogue has in-volved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, & Tetreault, 2009; L?pez-C?zar, Silovsky, & Griol, 2010), but this work has not explored leveraging affect information for automatic user dialogue act classification. Outside of dialogue, sentiment anal-ysis within discourse is an active area of research (L?pez-C?zar et al, 2010), but it is generally lim-
1190
ited to modeling textual features and not multi-modal expressions of emotion such as facial ac-tions. Such multimodal expressions have only just begun to be explored within corpus-based dialogue research (Calvo & D'Mello, 2010; Cavicchio, 2009).   This paper presents a novel affect-enriched dia-logue act classification approach that leverages knowledge of users? facial expressions during computer-mediated textual human-human dia-logue. Intuitively, the user?s affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user may be more likely to ask a ques-tion). We focus specifically on occurrences of stu-dents? confusion-related facial actions during task-oriented tutorial dialogue.  Confusion was selected as the focus of this work for several reasons. First, confusion is known to be prevalent within tutoring, and its implications for student learning are thought to run deep (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005). Second, while identifying the ?ground truth? of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular faci-al action units and confusion during learning (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007). Finally, automatic facial action recognition technologies are developing rap-idly, and confusion-related facial action events are among those that can be reliably recognized auto-matically (Bartlett et al, 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems.  2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classifi-cation within dialogue systems, it has been an ac-tive area of research for some time. Early work on automatic dialogue act classification modeled dis-course structure with hidden Markov models, ex-perimenting with lexical and prosodic features, and applying the dialogue act model as a constraint to 
aid in automatic speech recognition (Stolcke et al, 2000). In contrast to this sequential modeling ap-proach, which is best suited to offline processing, recent work has explored how lexical, syntactic, and prosodic features perform for online dialogue act tagging (when only partial dialogue sequences are available) within a maximum entropy frame-work (Sridhar, Bangalore, & Narayanan, 2009). A recently proposed alternative approach involves treating dialogue utterances as documents within a latent semantic analysis framework, and applying feature enhancements that incorporate such infor-mation as speaker and utterance duration (Di Eugenio et al, 2010). Of the approaches noted above, the modeling framework presented in this paper is most similar to the vector-based maximum entropy approach of Sridhar et al (2009). Howev-er, it takes a step beyond the previous work by in-cluding multimodal affective displays, specifically facial expressions, as features available to an af-fect-enriched dialogue act classification model. 2.2 Detecting Emotions in Dialogue Detecting emotional states during spoken dialogue is an active area of research, much of which focus-es on detecting frustration so that a user can be automatically transferred to a human dialogue agent (L?pez-C?zar et al, 2010). Research on spo-ken dialogue has leveraged lexical features along with discourse cues and acoustic information to classify user emotion, sometimes at a coarse grain along a positive/negative axis (Lee & Narayanan, 2005). Recent work on an affective companion agent has examined user emotion classification within conversational speech (Cavazza et al, 2010). In contrast to that spoken dialogue research, the work in this paper is situated within textual dialogue, a widely used modality of communica-tion for which a deeper understanding of user af-fect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial ac-tions, electrocardiograms, skin conductance, and posture sensors (Calvo & D'Mello, 2010). A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009). Like that work, the current project utilizes facial action tagging, for 
1191
which promising automatic technologies exist (Bartlett et al, 2006; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009). However, we leverage the recognized expressions of emotion for the task of dialogue act classification.  2.3 Categorizing Emotions within Dialogue and Discourse Sets of emotion taxonomies for discourse and dia-logue are often application-specific, for example, focusing on the frustration of users who are inter-acting with a spoken dialogue system (L?pez-C?zar et al, 2010), or on uncertainty expressed by students while interacting with a tutor (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007). In con-trast, the most widely utilized emotion frameworks are not application-specific; for example, Ekman?s Facial Action Coding System (FACS) has been widely used as a rigorous technique for coding fa-cial movements based on human facial anatomy (Ekman & Friesen, 1978).  Within this framework, facial movements are categorized into facial action units, which represent discrete movements of mus-cle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman?s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion ex-pressed within blog text (Das & Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell?s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not fre-quently experience Ekman?s basic emotions of happiness, sadness, anger, fear, surprise, and dis-gust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo & D'Mello, 2010). Our work leverages Ekman?s facial tagging scheme to identi-fy a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007).   2.4 Importance of Confusion in Tutorial Dia-logue Among the affective states that students experience during tutorial dialogue, confusion is prevalent, and its implications for student learning are signif-
icant. Confusion is associated with cognitive dise-quilibrium, a state in which students? existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005). Students may express such confu-sion within dialogue as uncertainty, to which hu-man tutors often adapt in a context-dependent fashion (Forbes-Riley et al, 2007). Moreover, im-plementing adaptations to student uncertainty with-in a dialogue system can improve the effectiveness of the system (Forbes-Riley et al, 2009).  For tutorial dialogue, the importance of under-standing student utterances is paramount for a sys-tem to positively impact student learning (Dzikovska, Moore, Steinhauser, & Campbell, 2010). The importance of frustration as a cogni-tive-affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act classi-fication of student utterances. This paper explores the use of facial expression features in this way.  3 Task-Oriented Dialogue Corpus The corpus was collected during a textual human-human tutorial dialogue study in the domain of introductory computer science (Boyer, Phillips, et al, 2010). Students solved an introductory com-puter programming problem and carried on textual dialogue with tutors, who viewed a synchronized version of the students? problem-solving work-space. The original corpus consists of 48 dia-logues, one per student. Each student interacted with one of two tutors. Facial videos of students were collected using built-in webcams, but were not shown to the tutors. Video quality was ranked based on factors such as obscured foreheads due to hats or hair, and improper camera position result-ing in students? faces not being fully captured on the video. The highest-quality set contained 14 videos, and these videos were used in this analysis. They have a total running time of 11 hours and 55 minutes, and include dialogues with three female subjects and eleven male subjects.  3.1 Dialogue act annotation The dialogue act annotation scheme (Table 1) was applied manually. The kappa statistic for inter-annotator agreement on a 10% subset of the corpus was ?=0.80, indicating good reliability.   
1192
Table 1. Dialogue act tags and relative frequencies across fourteen dialogues in video corpus Student Dialogue Act Example Rel. Freq. EXTRA-DOMAIN (EX) Little sleep deprived today .08 GROUNDING (G) Ok or Thanks .21 NEGATIVE FEEDBACK WITH ELABORATION (NE) I?m still confused on what this next for loop is doing. .02 NEGATIVE FEEDBACK (N) I don?t see the diff. .04 POSITIVE FEEDBACK WITH ELABORATION (PE) 
It makes sense now that you explained it, but I never used an else if in any of my other programs .04 POSITIVE FEEDBACK (P) Second part complete. .11 QUESTION (Q) Why couldn?t I have said if (i<5) .11 STATEMENT (S) i is my only index .07 
REQUEST FOR FEEDBACK (RF) So I need to create a new method that sees how many elements are in my array? .16 RESPONSE (RSP) You mean not the length but the contents .14 UNCERTAIN FEEDBACK WITH ELABORATION (UE) I?m trying to remember how to copy arrays .008 UNCERTAIN FEEDBACK (U) Not quite yet .008  3.2 Task action annotation The tutoring sessions were task-oriented, focusing on a computer programming exercise. The task had several subtasks consisting of programming mod-ules to be implemented by the student. Each of those subtasks also had numerous fine-grained goals, and student task actions either contributed or did not contribute to the goals. Therefore, to obtain a rich representation of the task, a manual annota-tion along two dimensions was conducted (Boyer, Phillips, et al, 2010). First, the subtask structure was annotated hierarchically, and then each task action was labeled for correctness according to the requirements of the assignment. Inter-annotator agreement was computed on 20% of the corpus at the leaves of the subtask tagging scheme, and re-
sulted in a simple kappa of ?=.56. However, the leaves of the annotation scheme feature an implicit ordering (subtasks were completed in order, and adjacent subtasks are semantically more similar than subtasks at a greater distance); therefore, a weighted kappa is also meaningful to consider for this annotation. The weighted kappa is ?weighted=.80. An annotated excerpt of the corpus is displayed in Table 2.   Table 2. Excerpt from corpus illustrating annota-tions and interplay between dialogue and task 13:38:09 Student: How do I know where to end? [RF] 13:38:26 Tutor: Well you told me how to get how many elements in an array by using .length right? 13:38:26 Student: [Task action:  Subtask 1-a-iv, Buggy] 13:38:56 Tutor: Great 13:38:56 Student: [Task action: Subtask 1-a-v, Correct] 13:39:35 Student: Well is it "array.length"? [RF]  **Facial Expression: AU4 13:39:46 Tutor: You just need to use the correct array name 13:39:46 Student: [Task action:  Subtask 1-a-iv, Buggy] 3.3 Lexical and Syntactic Features In addition to the manually annotated dialogue and task features described above, syntactic features of each utterance were automatically extracted using the Stanford Parser (De Marneffe et al, 2006). From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). Our prior work has shown that these lexical and syntactic features are highly predictive of dialogue acts dur-ing task-oriented tutorial dialogue (Boyer, Ha et al 2010).  
1193
4 Facial Action Tagging An annotator who was certified in the Facial Ac-tion Coding System (FACS) (Ekman, Friesen, & Hager, 2002) tagged the video corpus consisting of fourteen dialogues. The FACS certification process requires annotators to pass a test designed to ana-lyze their agreement with reference coders on a set of spontaneous facial expressions (Ekman & Rosenberg, 2005). This annotator viewed the vide-os continuously and paused the playback whenever notable facial displays of Action Unit 4 (AU4: Brow Lowerer) were seen. This action unit was chosen for this study based on its correlations with confusion in prior research (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007). To establish reliability of the annotation, a se-cond FACS-certified annotator independently an-notated 36% of the video corpus (5 of 14 dialogues), chosen randomly after stratification by gender and tutor. This annotator followed the same method as the first annotator, pausing the video at any point to tag facial action events. At any given time in the video, the coder was first identifying whether an action unit event existed, and then de-scribing the facial movements that were present. The annotators also specified the beginning and ending time of each event. In this way, the action unit event tags spanned discrete durations of vary-ing length, as specified by the coders. Because the two coders were not required to tag at the same point in time, but rather were permitted the free-dom to stop the video at any point where they felt a notable facial action event occurred, calculating agreement between annotators required discretiz-ing the continuous facial action time windows across the tutoring sessions. This discretization was performed at granularities of 1/4, 1/2, 3/4, and 1 second, and inter-rater reliability was calculated at each level of granularity (Table 3). Windows in which both annotators agreed that no facial action event was present were tagged by default as neu-tral. Figure 1 illustrates facial expressions that dis-play facial Action Unit 4. 
  Table 3. Kappa values for inter-annotator agree-ment on facial action events  Granularity  ? sec ? sec ? sec 1 sec Presence of AU4 (Brow Lowerer)  .84 .87 .86 .86   
  
  Figure 1. Facial expressions displaying AU4 (Brow Lowerer)  Despite the fact that promising automatic ap-proaches exist to identifying many facial action units (Bartlett et al, 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009), manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emo-tions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other sponta-neous student facial expressions that differ from those associated with confusion.    
1194
   
  Figure 2. Other facial expressions from the corpus 5 Models The goal of the modeling experiment was to de-termine whether the addition of confusion-related facial expression features significantly boosts dia-logue act classification accuracy for student utter-ances.  5.1 Features We take a vector-based approach, in which the fea-tures consist of the following:  Utterance Features ? Dialogue act features: Manually annotated dialogue act for the past three utterances. These features include tutor dialogue acts, annotated with a scheme analogous to that used to annotate student utterances (Boyer et al, 2009). ? Speaker: Speaker for past three utterances ? Lexical features: Word unigrams ? Syntactic features: Top-most syntactic node and its first two children  Task-based Features ? Subtask: Hierarchical subtask structure for past three task actions (semantic pro-gramming actions taken by student) ? Correctness: Correctness of past three task actions taken by student ? Preceded by task: Indicator for whether the most recent task action immediately pre-ceded the target utterance, or whether it 
was immediately preceded by the last dia-logue move  Facial Expression Features ? AU4_1sec: Indicator for the display of the brow lowerer within 1 second prior to this utterance being sent, for the most recent three utterances ?  AU4_5sec: Indicator for the display of the brow lowerer within 5 seconds prior to this utterance being sent, for the most recent three utterances ? AU4_10sec: Indicator for the display of the brow lowerer within 10 seconds prior to this utterance being sent, for the most recent three utterances  5.2 Modeling Approach A logistic regression approach was used to classify the dialogue acts based on the above feature vec-tors. The Weka machine learning toolkit (Hall et al, 2009) was used to learn the models and to first perform feature selection in a best-first search. Lo-gistic regression is a generalized maximum likeli-hood model that discriminates between pairs of output values by calculating a feature weight vec-tor over the predictors.  The goal of this work is to explore the utility of confusion-related facial features in the context of particular dialogue act types. For this reason, a specialized classifier was learned by dialogue act. 5.3 Classification Results The classification accuracy and kappa for each specialized classifier is displayed in Table 4. Note that kappa statistics adjust for the accuracy that would be expected by majority-baseline chance; a kappa statistic of zero indicates that the classifier performed equal to chance, and a positive kappa statistic indicates that the classifier performed bet-ter than chance. A kappa of 1 constitutes perfect agreement. As the table illustrates, the feature se-lection chose to utilize the AU4 feature for every dialogue act except STATEMENT (S). When consid-ering the accuracy of the model across the ten folds, two of the affect-enriched classifiers exhibit-ed statistically significantly better performance. For GROUNDING (G) and REQUEST FOR FEEDBACK (RF), the facial expression features significantly 
1195
improved the classification accuracy compared to a model that was learned without affective features.  6 Discussion Dialogue act classification is an essential task for dialogue systems, and it has been addressed with a variety of modeling approaches and feature sets. We have presented a novel approach that treats facial expressions of students as constraining fea-tures for an affect-enriched dialogue act classifica-tion model in task-oriented tutorial dialogue. The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and REQUEST FOR FEEDBACK.   Table 4. Classification accuracy and kappa for spe-cialized DA classifiers. Statistically significant differences (across ten folds, one-tailed t-test) are shown in bold.    Classifier with AU4 Classifier without AU4  Dialogue Act % acc ? % acc ? p-value EX 90.7 .62 89.0 .28 >.05 G 92.6 .76 91 .71 .018 P 93 .49 92.2 .40 >.05 Q 94.6 .72 94.2 .72 >.05 S Not chosen in feat. sel. 93 .22 n/a RF 90.7 .62 88.3 .53 .003 
RSP 93 .68 95 .75 >.05 NE * *  N * * PE * * U * * UE * * *Too few instances for ten-fold cross-validation. 
6.1 Features Selected for Classification Out of more than 1500 features available during feature selection, each of the specialized dialogue act classifiers selected between 30 and 50 features in each condition (with and without affect fea-tures). To gain insight into the specific features that were useful for classifying these dialogue acts, it is useful to examine which of the AU4 history features were chosen during feature selection.  For GROUNDING, features that indicated the presence of absence of AU4 in the immediately preceding utterance, either at the 1 second or 5 se-cond granularity, were selected. Absence of this confusion-related facial action unit was associated with a higher probability of a grounding act, such as an acknowledgement. This finding is consistent with our understanding of how students and tutors interacted in this corpus; when a student experi-enced confusion, she would be unlikely to then make a simple grounding dialogue move, but in-stead would tend to inspect her computer program, ask a question, or wait for the tutor to explain more. For REQUEST FOR FEEDBACK, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified). This finding suggests that there may be some lag between the student experiencing confusion and then choosing to make a request for feedback, and that the confusion-related facial expressions may re-emerge as the student is making a request for feedback, since the five-second window prior to the student sending the textual dialogue message would overlap with the student?s construction of the message itself.    Although the improvements seen with AU4 fea-tures for QUESTION, POSITIVE FEEDBACK, and EXTRA-DOMAIN acts were not statistically reliable, examining the AU4 features that were selected for classifying these moves points toward ways in which facial expressions may influence classifica-tion of these acts (Table 5).      
1196
Table 5. Number of features, and AU4 features selected, for specialized DA classifiers  Dialogue Act # fea-tures selected AU4 features selected G 43 One utterance ago: AU4_1sec, AU4_5sec 
RF 37 Three utterances ago: AU4_10sec Target utterance: AU4_5sec EX 50 Three utterances ago: AU4_1sec P 36 Current utterance: AU4_10sec Q 30 One utterance ago: AU4_5sec  6.2 Implications The results presented here demonstrate that lever-aging knowledge of user affect, in particular of spontaneous facial expressions, may improve the performance of dialogue act classification models. Perhaps most interestingly, displays of confusion-related facial actions prior to a student dialogue move enabled an affect-enriched classifier to rec-ognize requests for feedback with significantly greater accuracy than a classifier that did not have access to the facial action features. Feedback is known to be a key component of effective tutorial dialogue, through which tutors provide adaptive help (Shute, 2008). Requesting feedback also seems to be an important behavior of students, characteristically engaged in more frequently by women than men, and more frequently by students with lower incoming knowledge than by students with higher incoming knowledge (Boyer, Vouk, & Lester, 2007). 6.3 Limitations The experiments reported here have several nota-ble limitations. First, the time-consuming nature of manual facial action tagging restricted the number of dialogues that could be tagged. Although the highest quality videos were selected for annotation, other medium quality videos would have been suf-ficiently clear to permit tagging, which would have increased the sample size and likely revealed sta-tistically significant trends. For example, the per-
formance of the affect-enriched classifier was bet-ter for dialogue acts of interest such as positive feedback and questions, but this difference was not statistically reliable.  An additional limitation stems from the more fundamental question of which affective states are indicated by particular external displays. The field is only just beginning to understand facial expres-sions during learning and to correlate these facial actions with emotions. Additional research into the ?ground truth? of emotion expression will shed additional light on this area. Finally, the results of manual facial action annotation may constitute up-per-bound findings for applying automatic facial expression analysis to dialogue act classification. 7 Conclusions and Future Work Emotion plays a vital role in human interactions. In particular, the role of facial expressions in human-human dialogue is widely recognized. Facial ex-pressions offer a promising channel for under-standing the emotions experienced by users of dialogue systems, particularly given the ubiquity of webcam technologies and the increasing number of dialogue systems that are deployed on webcam-enabled devices. This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act REQUEST FOR FEEDBACK, and the conversational moves of GROUNDING. These early findings highlight the importance of future work in this area. Dialogue act classifica-tion models have not fully leveraged some of the techniques emerging from work on sentiment anal-ysis. These approaches may prove particularly use-ful for identifying emotions in dialogue utterances. Another important direction for future work in-volves more fully exploring the ways in which af-fect expression differs between textual and spoken dialogue. Finally, as automatic facial tagging tech-nologies mature, they may prove powerful enough to enable broadly deployed dialogue systems to feasibly leverage facial expression data in the near future.    
1197
Acknowledgments This work is supported in part by the North Caroli-na State University Department of Computer Sci-ence and by the National Science Foundation through Grants REC-0632450, IIS-0812291, DRL-1007962 and the STARS Alliance Grant CNS-0739216. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the Na-tional Science Foundation.  References  A. Andreevskaia and S. Bergler. 2008. When specialists and generalists work together: Overcoming do-main dependence in sentiment tagging. Proceed-ings of the Annual Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL HLT), 290-298.  M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. 2006. Fully Automatic Facial Action Recognition in Spontaneous Behav-ior. 7th International Conference on Automatic Face and Gesture Recognition (FGR06), 223-230.  K.E. Boyer, M. Vouk, and J.C. Lester. 2007. The influ-ence of learner characteristics on task-oriented tu-torial dialogue. Proceedings of the International Conference on Artificial Intelligence in Educa-tion, 365?372.  K.E. Boyer, E.Y. Ha, R. Phillips, M.D. Wallis, M. Vouk, and J.C. Lester. 2010. Dialogue act model-ing in a complex task-oriented domain. Proceed-ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 297-305.  K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2009. Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of the Annual Con-ference of the North American Chapter of the As-sociation for Computational Linguistics: Short Papers, 49-52.  K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2010. Leveraging hidden dialogue state to select tutorial moves. Proceed-ings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, 66-73. R.A. Calvo and S. D?Mello. 2010. Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications. IEEE Transactions on Affec-tive Computing, 1(1): 18-37. 
M. Cavazza, R.S.D.L. C?mara, M. Turunen, J. Gil, J. Hakulinen, N. Crook, et al 2010. How was your day? An affective companion ECA prototype. Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 277-280.  F. Cavicchio. 2009. The modulation of cooperation and emotion in dialogue: the REC Corpus. Proceed-ings of the ACL-IJCNLP 2009 Student Research Workshop, 43-48.  J.F. Cohn, L.I. Reed, Z. Ambadar, J. Xiao, and T. Mori-yama. 2004. Automatic Analysis and Recognition of Brow Actions and Head Motion in Spontaneous Facial Behavior. IEEE International Conference on Systems, Man and Cybernetics, 610-616. S.D. Craig, S. D?Mello, A. Witherspoon, J. Sullins, and A.C. Graesser. 2004. Emotions during learning: The first steps toward an affect sensitive intelli-gent tutoring system. In J. Nall and R. Robson (Eds.), E-learn 2004: World conference on E-learning in Corporate, Government, Healthcare, & Higher Education, 241-250.  D. Das and S. Bandyopadhyay. 2009. Word to sentence level emotion tagging for Bengali blogs. Proceed-ings of the ACL-IJCNLP Conference, Short Pa-pers, 149-152.  S. Dasgupta and V. Ng. 2009. Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification. Proceedings of the 46th Annual Meeting of the ACL and the 4th IJCNLP, 701-709.  B. Di Eugenio, Z. Xie, and R. Serafin. 2010. Dialogue Act Classification, Higher Order Dialogue Struc-ture, and Instance-Based Learning. Dialogue & Discourse, 1(2): 1-24.  M. Dzikovska, J.D. Moore, N. Steinhauser, and G. Campbell. 2010. The impact of interpretation problems on tutorial dialogue. Proceedings of the 48th Annual Meeting of the Association for Com-putational Linguistics, Short Papers, 43-48.  S. D?Mello, S.D. Craig, J. Sullins, and A.C. Graesser. 2006. Predicting Affective States expressed through an Emote-Aloud Procedure from AutoTu-tor?s Mixed- Initiative Dialogue. International Journal of Artificial Intelligence in Education, 16(1): 3-28. P. Ekman. 1999. Basic Emotions. In T. Dalgleish and M. J. Power (Eds.), Handbook of Cognition and Emotion. New York: Wiley. P. Ekman, W.V. Friesen. 1978. Facial Action Coding System. Palo Alto, CA: Consulting Psychologists Press. P. Ekman, W.V. Friesen, and J.C. Hager. 2002. Facial Action Coding System: Investigator?s Guide. Salt Lake City, USA: A Human Face. 
1198
P. Ekman and E.L. Rosenberg (Eds.). 2005. What the Face Reveals: Basic and Applied Studies of Spon-taneous Expression Using the Facial Action Cod-ing System (FACS) (2nd ed.). New York: Oxford University Press. K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2007. Exploring affect-context depend-encies for adaptive system development. The Con-ference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL HLT), Short Papers, 41-44.  K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2009. Adapting to student uncertainty improves tutoring dialogues. Proceedings of the 14th International Conference on Artificial Intelli-gence in Education (AIED), 33-40.  A.C. Graesser, S. Lu, B. Olde, E. Cooper-Pye, and S. Whitten. 2005. Question asking and eye tracking during cognitive disequilibrium: comprehending illustrated texts on devices when the devices break down. Memory & Cognition, 33(7): 1235-1247.  S. Greene and P. Resnik. 2009. More than words: Syn-tactic packaging and implicit sentiment. Proceed-ings of the 2009 Annual Conference of the North American Chapter of the ACL and Human Lan-guage Technologies (NAACL HLT), 503-511.  M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explora-tions, 11(1): 10?18.  R. Iida, S. Kobayashi, and T. Tokunaga. 2010. Incorpo-rating extra-linguistic information into reference resolution in collaborative task dialogue. Proceed-ings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, 1259-1267.  M.L. Knapp and J.A. Hall. 2006. Nonverbal Communi-cation in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning. C.M. Lee, S.S. Narayanan. 2005. Toward detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2): 293-303.  R. L?pez-C?zar, J. Silovsky, and D. Griol. 2010. F2?New Technique for Recognition of User Emotion-al States in Spoken Dialogue Systems. Proceed-ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 281-288.  B.T. McDaniel, S. D?Mello, B.G. King, P. Chipman, K. Tapp, and A.C. Graesser. 2007. Facial Features for Affective State Detection in Learning Envi-ronments. Proceedings of the 29th Annual Cogni-tive Science Society, 467-472. D. McNeill. 1992. Hand and mind: What gestures reveal about thought. Chicago: University of Chicago Press. 
A. Mehrabian. 2007. Nonverbal Communication. New Brunswick, NJ: Aldine Transaction. T. Nguyen. 2010. Mood patterns and affective lexicon access in weblogs. Proceedings of the ACL 2010 Student Research Workshop, 43-48.  M. Pantic and M.S. Bartlett. 2007. Machine Analysis of Facial Expressions. In K. Delac and M. Grgic (Eds.), Face Recognition, 377-416. Vienna, Aus-tria: I-Tech Education and Publishing. J.A. Russell. 2003. Core affect and the psychological construction of emotion. Psychological Review, 110(1): 145-172. J.A. Russell, J.A. Bachorowski, and J.M. Fernandez-Dols. 2003. Facial and vocal expressions of emo-tion. Annual Review of Psychology, 54, 329-49. K.L. Schmidt and J.F. Cohn. 2001. Human Facial Ex-pressions as Adaptations: Evolutionary Questions in Facial Expression Research. Am J Phys An-thropol, 33: 3-24. V.J. Shute. 2008. Focus on Formative Feedback. Re-view of Educational Research, 78(1): 153-189.  V.K.R Sridar, S. Bangalore, and S.S. Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4): 407-422. Elsevier Ltd.  A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, et al 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Con-versational Speech. Computational Linguistics, 26(3): 339-373.  C. Toprak, N. Jakob, and I. Gurevych. 2010. Sentence and expression level annotation of opinions in us-er-generated discourse. Proceedings of the 48th Annual Meeting of the Association for Computa-tional Linguistics, 575-584.  T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-ing Contextual Polarity: An Exploration of Fea-tures for Phrase-Level Sentiment Analysis. Computational Linguistics, 35(3): 399-433.  Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 31(1): 39-58. 
1199
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 66?73,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Leveraging Hidden Dialogue State to Select Tutorial Moves  Kristy Elizabeth    Boyera Robert    Phillipsab Eun Young Haa Michael D.    Wallisab Mladen A.   Vouka James C. Lestera   aDepartment of Computer Science, North Carolina State University bApplied Research Associates Raleigh, NC, USA  {keboyer, rphilli, eha, mdwallis, vouk, lester}@ncsu.edu    Abstract 
A central challenge for tutorial dialogue systems is selecting an appropriate move given the dialogue context. Corpus-based approaches to creating tutorial dialogue management models may facilitate more flexible and rapid development of tutorial dialogue systems and may increase the effectiveness of these systems by allowing data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling. 1 Introduction A central challenge for dialogue systems is selecting appropriate system dialogue moves (Bangalore, Di Fabbrizio, & Stent, 2008; Frampton & Lemon, 2009; Young et al, 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, & Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 
derived from observing human tutors (e.g., Aleven, McLaren, Roll, & Koedinger, 2004; Evens & Michael, 2006; Graesser, Chipman, Haynes, & Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, & Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al, 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al, 2008; Hardy et al, 2006; Young et al, 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, & Hall, 2008; Tetreault & Litman, 2008). Ultimately, devising data-driven approaches for developing tutorial dialogue systems may constitute a key step towards achieving the high learning gains that have been observed with expert human tutors.  The work presented in this paper focuses on learning a model of tutorial moves within a corpus of human-human dialogue in the task-oriented domain of introductory computer science. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact by the user (the student). The modification of this artifact, in our case a computer program, is the focus of the dialogues. Our corpus consists of textual dialogue utterances and a separate synchronous stream of 
66
task actions. Our goal is to extract a data-driven dialogue management model from the corpus, as evidenced by predicting system (tutor) dialogue acts.  In this paper, we present an annotation approach that addresses dialogue utterances and task actions, and we propose a unified sequential representation for these separate synchronous streams of events. We explore the predictive power of three stochastic models ? first-order Markov models, hidden Markov models, and hierarchical hidden Markov models ? for predicting tutor dialogue acts in the unified sequences. By leveraging these models to capture effective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, & Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton & Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, & Litman, 2007; Henderson, Lemon, & Georgila, 2008; Heeman, 2007; Young et al, 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault & Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al, 2008).  While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the focus of work in other task-oriented domains such as finance (Hardy et al, 2006) and catalogue ordering (Bangalore et al, 2008). Like the parse-based models of Bangalore et al, our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio & Mikheev, 1998).  
For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens & Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007; Forbes-Riley & Litman, 2009), and KSC-PAL (Kersey, Di Eugenio, Jordan, & Katz, 2009) projects have made extensive use of data-driven techniques based on human corpora. Perhaps most directly comparable to the current work are the bigram models of Forbes-Riley et al; we explore first-order Markov models, which are equivalent to bigram models, for predicting tutor dialogue acts.  In addition, we present HMMs and HHMMs trained on our corpus. We found that both of these models outperformed the bigram model for predicting tutor moves. 3 Corpus and Annotation The corpus was collected during a human-human tutoring study in which tutors and students worked to solve an introductory computer programming problem (Boyer et al, in press). The dialogues were effective: on average, students exhibited a 7% absolute gain from pretest to posttest (N=48, paired t-test p<0.0001).  The corpus contains 48 textual dialogues with a separate, synchronous task event stream. Tutors and students collaborated to solve an introductory computer programming problem using an online tutorial environment with shared workspace viewing and textual dialogue. Each student participated in exactly one tutoring session. The corpus contains 1,468 student utterances, 3,338 tutor utterances, and 3,793 student task actions. In order to build the dialogue model, we annotated the corpus with dialogue act tags and task annotation labels. 3.1 Dialogue Act Annotation  We have developed a dialogue act tagset inspired by schemes for conversational speech (Stolcke et al, 2000), task-oriented dialogue (Core & Allen, 1997), and tutoring (Litman & Forbes-Riley, 2006). The dialogue act tags are displayed in Table 1. Overall reliability on 10% of the corpus for two annotators was ?=0.80.   
67
 Table 1. Dialogue act tags 
DA? Description?
Stu.?Rel.?Freq.?
Tut.?Rel.?Freq.? ??ASSESSING?QUESTION?(AQ)? Request?for?feedback?on?task?or?conceptual?utterance.? .20? .11? .91?EXTRA?DOMAIN?(EX)? Asides?not?relevant?to?the?tutoring?task.? .08? .04? .79?GROUNDING?(G)? Acknowledgement/thanks? .26? .06? .92?LUKEWARM?CONTENT?FEEDBACK?(LCF)? Negative?assessment?with?explanation.? .01? .03? .53?LUKEWARM?FEEDBACK?(LF)? Lukewarm?assessment?of?task?action?or?conceptual?utterance.? .02? .03? .49?NEGATIVE?CONTENT?FEEDBACK?(NCF)?
Negative?assessment?with?explanation.? .01? .10? .61?
NEGATIVE?FEEDBACK?(NF)? Negative?assessment?of?task?action?or?conceptual?utterance.? .05? .02? .76?POSITIVE?CONTENT?FEEDBACK?(PCF)? Positive?assessment?with?explanation.? .02? .03? .43?POSITIVE?FEEDBACK?(PF)? Positive?assessment?of?task?action?or?conceptual?utterance.? .09? .16? .81?QUESTION?(Q)? Task?or?conceptual?question.? .09? .03? .85?STATEMENT?(S)? Task?or?conceptual?assertion.? .16? .41? .82?
3.2 Task Annotation The dialogues focused on the task of solving an introductory computer programming problem. The task actions were recorded as a separate but synchronous event stream. This stream included 97,509 keystroke-level user task events. These events were manually aggregated and annotated for subtask structure and then for correctness. The task annotation scheme was hierarchical, reflecting the nested nature of the subtasks. An excerpt from the task annotation scheme is depicted in Figure 1; the full scheme contains 66 leaves. The task annotation scheme was designed to reflect the different depth of possible subtasks nested within the overall task. Each labeled task action was also judged for correctness according to the requirements of the task, with categories CORRECT, BUGGY, INCOMPLETE, and DISPREFERRED (technically 
correct but not accomplishing the pedagogical goals of the task). Each group of task keystrokes that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced (giving judges maximum flexibility to apply the tags). To ensure a conservative reliability statistic, all unmatched subtask tags were treated as disagreements. The resulting unweighted kappa statistic was ?simple= 0.58, but the weighted Kappa ?weighted=0.86 is more meaningful because it takes into account the ordinal nature of the labels that result from sequential subtasks. On task actions for which the two judges agreed on subtask tag, the agreement statistic for correctness was ?simple=0.80. 
 Figure 1. Portion of task annotation scheme 3.3 Adjacency Pair Joining Some dialogue acts establish an expectation for another dialogue act to occur next (Schegloff & Sacks, 1973). Our previous work has found that identifying the statistically significant adjacency pairs in a corpus and joining them as atomic observations prior to model building produces more interpretable descriptive models. The models reported here were trained on hybrid sequences of dialogue acts and adjacency pairs. A full description of the adjacency pair identification methodology and joining algorithm is reported in (Boyer et al, 2009). A partial list of the most highly statistically significant adjacency pairs, 
68
which for this work include task actions, is displayed in Table 2.   Table 2. Subset of significant adjacency pairs CORRECTTASKACTION?CORRECTTASKACTION;??EXTRADOMAINS?EXTRADOMAINT;?GROUNDINGS?GROUNDINGT;?ASSESSINGQUESTIONT?POSITIVEFEEDBACKS;??ASSESSINGQUESTIONS?POSITIVEFEEDBACKT;?QUESTIONT?STATEMENTS;?ASSESSINGQUESTIONT?STATEMENTS;?EXTRADOMAINT?EXTRADOMAINS;?QUESTIONS?STATEMENTT;?NEGATIVEFEEDBACKS?GROUNDINGT;?INCOMPLETETASKACTION?INCOMPLETETASKACTION;?POSITIVEFEEDBACKS?GROUNDINGT;??BUGGYTASKACTION?BUGGYTASKACTION 4 Models We learned three types of models using cross-validation with systematic sampling of training and testing sets. 
4.1 First-Order Markov Model The simplest model we discuss is the first-order Markov model (MM), or bigram model (Figure 2). A MM that generates observation (state) sequence o1o2?ot is defined in the following way. The observation symbols are drawn from the alphabet ?={?1, ?2, ?, ?M}, and the initial probability distribution is ?=[?i] where ?i is the probability of a sequence beginning with observation symbol ?i. The transition probability distribution is A=[aij], where aij is the probability of observation j occurring immediately after observation i. 
 Figure 2. Time-slice topology of MM  We trained MMs on our corpus of dialogue acts and task events using ten-fold cross-validation to produce a model that could be queried for the next predicted tutorial dialogue act given the history.  
4.2 Hidden Markov Model A hidden Markov model (HMM) augments the MM framework, resulting in a doubly stochastic structure (Rabiner, 1989). For a first-order HMM, the observation symbol alphabet is defined as above, along with a set of hidden states S={s1,s2,?,sN}. The transition and initial probability distributions are defined analogously to MMs, except that they operate on hidden states 
rather than on observation symbols (Figure 3). That is, ?=[?i] where ?i is the probability of a sequence beginning in hidden state si. The transition matrix is A=[aij], where aij is the probability of the model transitioning from hidden state i to hidden state j. This framework constitutes the first stochastic layer of the model, which can be thought of as modeling hidden, or unobservable, structure. The second stochastic layer of the model governs the production of observation symbols: the emission probability distribution is B=[bik] where bik is the probability of state i emitting observation symbol k. 
 Figure 3. Time-slice topology of HMM  The notion that dialogue has an overarching unobservable structure that influences the observations is widely accepted. In tutoring, this overarching structure may correspond to tutorial strategies. We have explored HMMs? descriptive power for extracting these strategies (Boyer et al, 2009), and this paper explores the hypothesis that HMMs provide better predictive power than MMs on our dialogue sequences. We trained HMMs on the corpus using the standard Baum-Welch expectation maximization algorithm and applied state labels that reflect post-hoc interpretation (Figure 4).  
 Figure 4. Portion of learned HMM 
69
4.3 Hierarchical Hidden Markov Model Hierarchical hidden Markov models (HHMMs) allow for explicit representation of multilevel stochastic structure. A complete formal definition of HHMMs can be found in (Fine, Singer, & Tishby, 1998), but here we present an informal description.  HHMMs include two types of hidden states: internal nodes, which do not produce observation symbols, and production nodes, which do produce observations. An internal node includes a set of substates that correspond to its potential children, S={s1, s2, ?, sN}, each of which is itself the root of an HHMM. The initial probability distribution ?=[?i] for each internal node governs the probability that the model will make a vertical transition to substate si from this internal node; that is, that this internal node will produce substate si as its leftmost child. Horizontal transitions are governed by a transition probability distribution similar to that described above for flat HMMs. Production nodes are defined by their observation symbol alphabet and an emission probability distribution over the symbols; HHMMs do not require a global observation symbol alphabet. The generative topology of our HHMMs is illustrated in Figure 5. 
 Figure 5. Generative topology of HHMM  HHMMs of arbitrary topology can be trained using a generalized version of the Baum-Welch algorithm (Fine et al, 1998). Our HHMMs featured a pre-specified model topology based on known task/subtask structure. A Bayesian view of a portion of the best-fit HHMM is depicted in Figure 6.  This model was trained using five-fold cross-validation to address the absence of symbols from the training set that were present in the testing set, a sparsity problem that arose from splitting the data hierarchically. 
Figure 6. Portion of learned HHMM 
70
5 Results We trained and tested MMs, HMMs, and HHMMs on the corpus and compared prediction accuracy for tutorial dialogue acts by providing the model with partial sequences from the test set and querying for the next tutorial move. The baseline prediction accuracy for this task is 41.1%, corresponding to the most frequent tutorial dialogue act (STATEMENT). As depicted in Figure 7, a first-order MM performed worse than baseline (p<0.001)1 at 27% average prediction accuracy (
? 
? ? MM=6%). HMMs performed better than baseline (p<0.0001), with an average accuracy of 48% (
? 
? ? HMM=3%). HHMMs averaged 57% accuracy, significantly higher than baseline (p=0.002) but weakly significantly higher than HMMs (p=0.04), and with high variation (
? 
? ? HHMM=23%). 
 Figure 7. Average prediction accuracies of three model types on tutor dialogue acts  To further explore the performance of the HHMMs, Figure 8 displays their prediction accuracy on each of six labeled subtasks. These subtasks correspond to the top level of the hierarchical task/subtask annotation scheme. The UNDERSTAND THE PROBLEM subtask corresponds to the initial phase of most tutoring sessions, in which the student and tutor agree to some extent on a problem-solving plan. Subtasks 1, 2, and 3 account for the implementation and debugging of three distinct modules within the learning task, and Subtask 4 involves testing and assessing the student?s finalized program. The EXTRA-DOMAIN subtask involves side conversations whose topics are outside of the domain.  The HHMM performed as well as or better (p<0.01) than baseline on the first three in-domain subtasks. The performance on SUBTASK 4 was not distinguishable from baseline (p=0.06); relatively few students reached this subtask. The model did                                                 1 All p-values in this section were produced by two-sample one-tailed t-tests with unequal sample variances. 
not outperform baseline (p=0.40) for the UNDERSTAND THE PROBLEM subtask, and qualitative inspection of the corpus reveals that the dialogue during this phase of tutoring exhibits limited regularities between students.  
 Figure 8. Average prediction accuracies of HHMMs by subtask 6 Discussion The results support our hypothesis that HMMs, because of their capacity for explicitly representing dialogue structure at an abstract level, perform better than MMs for predicting tutor moves. The results also suggest that explicitly modeling hierarchical task structure can further improve prediction accuracy of the model. The below-baseline performance of the bigram model illustrates that in our complex task-oriented domain, an immediately preceding event is not highly predictive of the next move. While this finding may not hold for conversational dialogue or some task-oriented dialogue with a more balanced distribution of utterances between speakers, the unbalanced nature of our tutoring sessions may not be as easily captured.  In our corpus, tutor utterances outnumber student utterances by more than two to one. This large difference is due to the fact that tutors frequently guided students and provided multi-turn explanations, the impetus for which are not captured in the corpus, but rather, involve external pedagogical goals. The MM, or bigram model, has no mechanism for capturing this layer of stochastic behavior. On the other hand, the HMM can account for unobserved influential variables, and the HHMM can do so to an even greater extent by explicitly modeling task/subtask structure. Considering the performance of the HHMM on individual subtasks reveals interesting properties of our dialogues. First, the HHMM is unable to outperform baseline on the UNDERSTAND THE PROBLEM subtask. To address this issue, our ongoing work investigates taking into account 
71
student characteristics such as incoming knowledge level and self-confidence. On all four in-domain subtasks, the HHMM achieved a 30% to 50% increase over baseline. For extra-domain dialogues, which involve side conversations that are not task-related, the HHMM achieved 86% prediction accuracy on tutor moves, which constitutes a 115% improvement over baseline. This high accuracy may be due in part to the fact that out-of-domain asides were almost exclusively initiated by the student, and tutors rarely engaged in such exchanges beyond providing a single response. This regularity likely facilitated prediction of the tutor?s dialogue moves during out-of-domain talk. We are aware of only one recent project that reports extensively on predicting system actions from a corpus of human-human dialogue. Bangalore et al?s (2008) flat task/dialogue model in a catalogue-ordering domain achieved a prediction accuracy of 55% for system dialogue acts, a 175% improvement over baseline. When explicitly modeling the hierarchical task/subtask dialogue structure, they report a prediction accuracy of 35.6% for system moves, approximately 75% above baseline (Bangalore & Stent, 2009). These findings were obtained by utilizing a variety of lexical and syntactic features along with manually annotated dialogue acts and task/subtask labels. In comparison, our HHMM achieved an average 42% improvement over baseline using only annotated dialogue acts and task/subtask labels. In ongoing work we are exploring the utility of additional features for this prediction task. Our best model performed better than baseline by a significant margin. The absolute prediction accuracy achieved by the HHMM was 57% across the corpus, which at first blush may appear too low to be of practical use. However, the choice of tutorial move involves some measure of subjectivity, and in many contexts there may be no uniquely appropriate dialogue act. Work in other domains has dealt with this uncertainty by maintaining multiple hypotheses (Wright Hastie, Poesio, & Isard, 2002) and by mapping to clustered sets of moves rather than maintaining policies for each possible system selection (Young et al, 2009). Such approaches may prove useful in our domain as well, and may help to more fully realize 
the potential of a learned dialogue management model.  7 Conclusion and Future Work Learning models that predict system moves within a corpus is a first step toward building fully data-driven dialogue management models. We have presented Markov models, hidden Markov models, and hierarchical hidden Markov models trained on sequences of manually annotated dialogue acts and task events. Of the three models, the hierarchical models appear to perform best in our domain, which involves an intrinsically hierarchical task/subtask structure.  The models? performance points to promising future work that includes utilizing additional lexical and syntactic features along with fixed user (student) characteristics within a hierarchical hidden Markov modeling framework. More broadly, the results point to the importance of considering task structure when modeling a complex domain such as those that often accompany task-oriented tutoring. Finally, a key direction for data-driven dialogue management models involves learning unsupervised dialogue act and task classification models.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Ai, H., Tetreault, J. R., & Litman, D. J. (2007). Comparing user simulation models for dialog strategy learning. Proceedings of NAACL HLT, Companion Volume, Rochester, New York. 1-4.  Aleven, V., McLaren, B., Roll, I., & Koedinger, K. (2004). Toward tutoring help seeking: Applying cognitive modeling to meta-cognitive skills. Proceedings of ITS, 227-239.  Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  
72
Bangalore, S., & Stent, A. J. (2009). Incremental parsing models for dialog task structure. Proceedings of the EACL, 94-102.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of NAACL HLT (Short Papers), 19-26. Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (In press). Characterizing the effectiveness of tutorial dialogue with hidden Markov models. Proceedings of ITS, Pittsburgh, Pennsylvania.  Chi, M., Jordan, P., VanLehn, K., & Hall, M. (2008). Reinforcement learning-based feature selection for developing pedagogically effective tutorial dialogue tactics. Proceedings of EDM, Montreal, Canada. 258-265.  Chi, M., Jordan, P., VanLehn, K., & Litman, D. (2009). To elicit or to tell: Does it matter? Proceedings of AIED, 197-204.  Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.   Evens, M., & Michael, J. (2006). One-on-one tutoring by humans and computers. Mahwah, New Jersey: Lawrence Erlbaum Associates. Fine, S., Singer, Y., & Tishby, N. (1998). The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1), 41-62.  Forbes-Riley, K., Rotaru, M., Litman, D. J., & Tetreault, J. (2007). Exploring affect-context dependencies for adaptive system development. Proceedings of NAACL HLT (Short Papers), 41-44.  Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Graesser, A. C., Chipman, P., Haynes, B. C., & Olney, A. (2005). AutoTutor: An intelligent tutoring system with mixed-initiative dialogue. IEEE Transactions on Education, 48(4), 612-618.  Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Communication, 48(3-4), 354-373.  Heeman, P. A. (2007). Combining reinforcement learning with information-state update rules. Proceedings of NAACL HLT, 268-275.  
Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4), 487-511.  Jordan, P., Makatchev, M., Pappuswamy, U., VanLehn, K., & Albacete, P. (2006). A natural language tutorial dialogue system for physics. Proceedings of FLAIRS, 521-526.  Kersey, C., Di Eugenio, B., Jordan, P., & Katz, S. (2009). KSC-PaL: A peer learning agent that encourages students to take the initiative. Proceedings of the NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Boulder, Colorado. 55-63.  Levin, E., Pieraccini, R., & Eckert, W. (2000). A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1), 11-23.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Poesio, M., & Mikheev, A. (1998). The predictive power of game structure in dialogue act recognition: Experimental results using maximum entropy estimation. Proceedings of ICSLP, 90-97.  Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Schegloff, E., & Sacks, H. (1973). Opening up closings. Semiotica, 7(4), 289-327.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373.  Tetreault, J. R., & Litman, D. J. (2008). A reinforcement learning approach to evaluating state representations in spoken dialogue systems. Speech Communication, 50(8-9), 683-696.  VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., & Rose, C. P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.  
73
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297?305,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Dialogue Act Modeling in a Complex Task-Oriented Domain 
  Kristy Elizabeth Boyer Eun Young Ha Robert Phillips* Michael D. Wallis* Mladen A. Vouk James C. Lester  Department of Computer Science, North Carolina State University Raleigh, North Carolina, USA  *Dual affiliation with Applied Research Associates, Inc. Raleigh, North Carolina, USA  {keboyer,?eha,?rphilli,?mdwallis,?vouk,?lester}@ncsu.edu? Abstract 
Classifying the dialogue act of a user utterance is a key functionality of a dialogue management system. This paper presents a data-driven dialogue act classifier that is learned from a corpus of human textual dialogue. The task-oriented domain involves tutoring in computer programming exercises. While engaging in the task, students generate a task event stream that is separate from and in parallel with the dialogue. To deal with this complex task-oriented dialogue, we propose a vector-based representation that encodes features from both the dialogue and the hierarchically structured task for training a maximum likelihood classifier. This classifier also leverages knowledge of the hidden dialogue state as learned separately by an HMM, which in previous work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al, 2008; Frampton & Lemon, 2009; Hardy et al, 2006; Sridar et al, 2009; Young et al, 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which 
provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al, 2009; Stolcke et al, 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al, 2008; Chotimongkol, 2008; Hardy et al, 2006). Our work adopts this approach for a corpus of human-human dialogue in a task-oriented tutoring domain. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact, in our case a computer program, by the user during the course of the dialogue. Our corpus consists of human-human textual dialogue utterances and a separate, parallel stream of user-generated task actions. We utilize structural features including task/subtask, speaker, and hidden dialogue state along with lexical and syntactic features to interpret user (student) utterances.  This paper makes three contributions. First, it addresses representational issues in creating a dialogue model that integrates task actions with hierarchical task/subtask structure. The task is captured within a separate synchronous event stream that exists in parallel with the dialogue. Second, this paper explores the performance of dialogue act classifiers using different lexical/syntactic and structural feature sets. This comparison includes one model trained entirely on lexical/syntactic features, an important step toward robust unsupervised dialogue act tagging 
297
(Sridhar et al, 2009). Finally, it investigates whether the addition of HMM and task/subtask features improves the performance of the dialogue act classifiers. The findings support this hypothesis for three student dialogue moves, each with important implications for tutorial dialogue.  2 Related Work A variety of modeling approaches have been investigated for statistical dialogue act classification, including sequential approaches and vector-based classifiers. Sequential approaches typically formulate dialogue as a Markov chain in which an observation depends on a finite number of preceding observations. HMM-based approaches make use of the Markov assumption in a doubly stochastic framework that allows fitting optimal dialogue act sequences using the Viterbi algorithm (Rabiner, 1989; Stolcke et al, 2000). Like this work, the approach reported here adopts a first-order Markov formulation to train an HMM on sequences of dialogue acts, but the prediction of this HMM is subsequently encoded in a feature vector for training a vector-based classifier. Vector-based approaches, such as maximum entropy modeling, also frequently take into account both lexical/syntactic and structural features. Lexical and syntactic cues are extracted from local utterance context, while structural features involve longer dialogue act sequences and, in task-oriented domains, task/subtask history. Work by Bangalore et al (2008) on learning the structure of human-human dialogue in a catalogue-ordering domain (also extended to the Maptask and Switchboard corpora) utilizes features including words, part of speech tags, supertags, and named entities, and structural features including dialogue acts and task/subtask labels. In order to perform incremental decoding of dialogue acts and task/subtask structure, they take a greedy approach that does not require the search of complete dialogue sequences. Our work also accomplishes left-to-right incremental interpretation with a greedy approach. Our feature vectors differ from the aforementioned work slightly with respect to lexical/syntactic features and notably in the addition of a set of structural features generated by a separately trained HMM, as described in Section 4.2.  Recent work has explored the use of lexical, syntactic, and prosodic features for online dialogue act tagging (Sridhar et al, 2009); that 
work explores the notion that structural (history) features could be omitted altogether from incremental left-to-right decoding, resulting in computationally inexpensive and robust dialogue act classification. Although our textual dialogue does not feature prosodic cues, we report on the use of lexical/syntactic features alone to perform dialogue act classification, a step toward a fully unsupervised approach.   Like Bangalore et al (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti?s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al, 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al, 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al, 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multi-party discourse, also implicitly capture a task structure (Purver et al, 2006).  Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of task actions. To illustrate, consider a catalogue-ordering task in which one subtask is to obtain the customer?s name. The fulfillment of this subtask occurs entirely through the dialogue, and the resulting artifact (a completed order) is produced by the system. In contrast, our task involves the user constructing a solution to a computer programming problem. The fulfillment of this task occurs partially in the dialogue through tutoring, and partially in a separate synchronous stream of user-driven task actions about which the tutor must reason. The stream of user-driven task actions produces an artifact of value in itself (a functioning computer program), and that artifact is the subject of much of the dialogue. We propose a representation that integrates task actions and dialogue acts from these streams into a shared vector-based representation, and we investigate the use of the resulting structural, lexical, and syntactic features for dialogue act classification.  
298
3 Corpus and Annotation The corpus was collected during a controlled human-human tutoring study in which tutors and students worked through textual dialogue to solve an introductory computer programming problem. The dialogues were effective: on average, students exhibited significant learning and self-confidence gains (Boyer et al, 2009).   The corpus contains 48 dialogues each with a separate, synchronous task event stream as depicted in Excerpt 1 of the appendix. There is exactly one dialogue (tutoring session) per student. The corpus captures approximately 48 hours of dialogue and contains 1,468 student utterances and 3,338 tutor utterances. Because the dialogue was textual, utterance segmentation consisted of splitting at existing sentence boundaries when more than one dialogue act was present in the utterance. This segmentation was conducted manually by the principal dialogue act annotator.1  The corpus was manually annotated with dialogue act labels and task/subtask features. Lexical and syntactic features were extracted automatically. The remainder of this section describes the manual annotation. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by schemes for conversational speech (Stolcke et al, 2000) and task-oriented dialogue (Core & Allen, 1997). It was also influenced by tutoring-specific tagsets (Litman & Forbes-Riley, 2006). Inter-rater reliability for the dialogue act tagging on 10% of the corpus selected via stratified (by tutor) random sampling was ?=0.80. The dialogue act tags, their relative frequencies, and their individual kappa scores from manual annotation are displayed in Table 1.  3.2 Task Annotation All task actions were generated by the student while implementing the solution to an introductory computer programming problem in Java. These task actions were recorded as a separate event stream in parallel with the dialogue corpus. This stream included 97,509 keystroke-level user task events, which were manually aggregated into task/subtask event clusters and annotated for subtask structure and then for correctness. A total of 3,793 aggregated                                                 1 Automatic segmentation is a challenging problem in itself and is left to future work. 
student subtask actions were identified through manual annotation. The task annotation scheme is hierarchical, reflecting the nested nature of the subtasks. A subset of this task annotation scheme is depicted in Figure 1. In the models reported in this paper, the 66 leaves of the task/subtask hierarchy were encoded in the input feature vectors.   Table 1. Student dialogue acts Student?Dialogue?Act? Rel.?Freq.? Human???ACKNOWLEDGMENT?(ACK)? .17? .90?REQUEST?FOR?FEEDBACK?(RF)? .20? .91?EXTRA?DOMAIN?(EX)? .08? .79?GREETING?(GR)? .04? .92?UNCERTAIN?FEEDBACK?WITH?ELABORATION?(UE)? .01? .53?UNCERTAIN?FEEDBACK?(U)? .02? .49?NEGATIVE?FEEDBACK?WITH?ELABORATION?(NE)? .01? .61?NEGATIVE?FEEDBACK?(N)? .05? .76?POSITIVE?FEEDBACK?WITH?ELABORATION?(PE)? .02? .43?POSITIVE?FEEDBACK?(P)? .09? .81?QUESTION?(Q)? .09? .85?STATEMENT?(S)? .16? .82?THANKS?(T)? .05? 1?
 Each group of task events that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. The judge aggregated the raw task keystrokes and tagged the task/subtask hierarchy for each cluster. (Please see Excerpt 1 in the appendix.) A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced, an approach that was intended to give judges maximum flexibility to cluster task actions and subsequently apply the tags. All unmatched subtask tags were treated as disagreements. The resulting kappa statistic at the leaves was ?= 0.58. However, we also observe that the sequential nature of the subtasks within the larger task produces an ordinal relationship between subtasks. For example, in Figure 1, the ?distance? between subtasks 1-a and 1-b can be thought of as ?less than? the distance between subtasks 1-a vs. 3-d because those subtasks are farther from each other within the larger task. The weighted Kappa statistic (Artstein & Poesio, 2008) takes into account such an ordinal relationship and its implicit distance function. The weighted Kappa is 
299
?weighted=0.86, which indicates acceptable inter-rater reliability on the task/subtask annotation. 
 Figure 1. Portion of task annotation scheme  Along with its tag for hierarchical subtask structure, each task event was also judged for correctness according to the requirements of the task as depicted in Table 2. The agreement statistic for correctness was calculated for task events on which the two judges agreed on subtask tag. The resulting unweighted agreement statistic for correctness was ?=0.80.  Table 2. Task correctness labels  Label? Description?CORRECT? Fully? satisfying? the? requirements? of?the? learning? task.? Does? not? require?tutorial?remediation.?BUGGY? Violating? the? requirements? of? the?learning?task.?Often?requires?tutorial?remediation.?INCOMPLETE? Not? violating,? but? not? yet? fully?satisfying,? the? requirements? of? the?learning? task.? May? require? tutorial?remediation.?DISPREFERRED? Technically? satisfying? the?requirements? of? the? learning? task,?but? not? adhering? to? its? pedagogical?intentions.? Usually? requires? tutorial?remediation.?4 Features The vector-based representation for training the dialogue act classifiers integrates several sources of features: lexical and syntactic features, and structural features that include dialogue act labels, task/subtask labels, and set of hidden dialogue state prediction features.   
4.1 Lexical and Syntactic Features Lexical and syntactic features were automatically extracted from the utterances using the Stanford Parser default tokenizer and part of speech (pos) tagger (De Marneffe et al, 2006). The parser created both phrase structure trees and typed dependencies for individual sentences. From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Typed dependencies between pairs of words were extracted from each sentence. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). The pos features were extracted in a similar way. Unigram and bigram word and pos tags were included for feature selection in the classifiers.   4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on manually labeled dialogue acts and task/subtask features (Boyer et al, 2009). These HMMs performed significantly better than bigram models for predicting human tutor moves (Boyer et al, 2010), which indicates that the hidden dialogue state leveraged by the HMMs has predictive value even in the presence of ?true? (manually annotated) dialogue act labels. Therefore, we hypothesized that an HMM could also improve the performance of models to classify student dialogue acts. To explore this hypothesis, we trained an HMM utilizing the methodology described in (Boyer et al, 2009) and used it to generate hidden dialogue state predictions in the form of a probability distribution over possible user utterances at each step in the dialogue. This set of stochastic features was subsequently passed to the classifier as part of the input vector (Figure 2).  4.3 Input Vectors The features were combined into a shared vector-based representation for training the classifier. As depicted in Table 3, the components of the 
300
feature vector include binary existence vectors for lexical and syntactic features for the current (target) utterance as well as for three utterances of left context (this left context may include both tutor and student utterances, which are distinguished by a separate indicator for the speaker). The task/subtask and correctness history features encode the separate stream of task events. There is no one-to-one correspondence between these history features and the left-hand dialogue context, because several task events could have occurred between a pair of dialogue events (or vice versa). This distinction is indicated in the table by the representation of dialogue time steps as [t, t-1,?] and task history steps as [task(t), task(t-1),?]. In total, the feature vectors included 11,432 attributes that were made available for feature selection.  
 Figure 2. Generation of hidden dialogue state prediction features 5 Experiments This section describes the learning of maximum likelihood vector-based models for classification of user dialogue acts. In addition to investigating the accuracy of the overall model, we also performed experiments regarding the utility of feature types for discriminating between particular dialogue acts of interest.    The classifiers are based on logistic regression, which learns a discriminant for each pair of dialogue acts by assigning weights in a maximum likelihood fashion. 2  The logistic regression models were learned using the Weka machine learning toolkit (Hall et al, 2009). For                                                 2 In general, the model that maximizes likelihood also maximizes entropy under the same constraints (Berger et al, 1996).  
feature selection, we performed attribute subset evaluation with a best-first approach that greedily searched the space of possible features using a hill climbing approach with backtracking. The prediction accuracy of the classifiers was determined through ten-fold cross-validation on the corpus, and the results below are presented in terms of prediction accuracy (number of correct classifications divided by total number of classifications) as well as by the kappa statistic, which adjusts for expected agreement by chance.    Table 3. Feature vectors 
Feature?vector?f? Description?[wt,1,?wt,|w|, pt,1,?,pt,|p|, dt,1,?,dt,|d|, st,1,?,st,|s|] 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?current?target?utterance?t??[wt-k,1,?wt-k,|w|, pt-k,1,?,pt-k,|p|, dt-k,1,?,dt-k,|d|, st-k,1,?,st-k,|s|]  where k=1,?,3 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?three?utterances?of?left?context?
[p(o1),?,p(o|S|)] Probability?distribution?for?emission?symbols?in?predicted?next?hidden?state?as?generated?by?HMM??[dat-1, dat-2, dat-3] Dialogue?act?left?context??[spt-1,spt-2, spt-3]? Speaker?label?left?context?[tktask(t-1), tktask(t-2), tktask(t-3)] Three?steps?of?subtask?history?(each?level?of?hierarchy?represented?as?a?separate?feature)??[ctask(t-1), ctask(t-2), ctask(t-3)]  
Three?steps?of?task?correctness?history?
pt Indicator?for?whether?the?target?utterance?was?immediately?preceded?by?a?task?event? 5.1 Overall Classification Task The overall dialogue act classification model was trained to classify each utterance with respect to the thirteen dialogue acts (Table 1). For this task, the feature selection algorithm selected 63 attributes including some syntax, dependency, pos, and word attributes as well as dialogue act, speaker, and task/subtask features. No hidden dialogue state features or task correctness attributes were selected. The overall classification accuracy was 62.8%. This accuracy constitutes a 369% improvement over baseline chance of 17% (the relative frequency of the most frequently occurring dialogue act, ACK). An alternate nontrivial baseline is a bigram model on true dialogue acts (including speaker tags); this model?s accuracy was 36.8%. The 
301
overall kappa for the full classifier was ?=.57. The confusion matrix for this model is depicted in Figure 3.        In addition to the classifier described above, we experimented with classifiers that used only the lexical and syntactic features of each utterance. This approach is of interest in part because it avoids the error propagation that can happen when a model relies on a series of its own previous classifications as features. The classifier that used only the set of lexical and syntactic features achieved a prediction accuracy of 60.2% and ?=.53 using 85 attributes.   
 5.2 Binary Dialogue Act Classification In tutoring, some student dialogue acts are particularly important to identify because of their implications for the tutor?s response or for the student model. For example, a student?s REQUEST FOR FEEDBACK requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base. UNCERTAIN FEEDBACK is another dialogue act of high importance because identifying it allows the tutor to respond in an affectively advantageous way (Forbes-Riley & Litman, 2009).  To explore which features are useful for classifying particular dialogue acts, we constructed binary dialogue act classifiers, one for each dialogue act, by preprocessing the dialogue act labels from the set of thirteen down to TRUE or FALSE depending on whether the label of the utterance matched the target dialogue act for that specialized classifier. Table 4 displays the features that were selected for each binary classifier, along with the percent accuracy and kappa for each model. Note that for some dialogue acts the chance baseline is very high, and therefore even a model with high prediction accuracy achieves a low kappa.         As depicted in Table 4, for several dialogue act models, the feature selection algorithm retained subtask and HMM features.   
Table 4. Binary DA classifiers  
DA? #?Features?Selected? %?Correct? Model???
ACK? 51? Lexical/syntax,?HMM,?DA?history?(preceding=S),?speaker?history?(preceding=Tutor)?? .933? .75?RF? 42? Lexical/syntax,?DA?history,?preceded?by?subtask? .905? .72?
EX? 57? Dependency,?pos,?word,?HMM,?DA?history?(preceding=EX),?subtask? .939? .45?
GR? 11? Syntax,?pos,?word,?DA?(previous=EMPTY),?speaker,?subtask?? .998? .97?UE? 21? Dependency,?pos,?word,?subtask? .991? .33?U? 63? Syntax,?dependency,?pos,?word,?HMM,?subtask? .979? .21?
NE? 44? Dependency,?pos,?word,?HMM,?DA?history?(2?ago=UNCERTAIN),?subtask? .987? 0?N? 83? Lexical/syntax,?DA?history,?subtask? .966? .76?PE? 90? Dependency,?pos,?word,?HMM,?subtask? .976? .10?
P? 110? Dependency,?pos,?word,?HMM,?DA?history?(previous=REQUEST?FEEDBACK)? .945? .58?Q? 43? Syntax,?dep,?pos,?word,?HMM,?subtask? .940? .60?S? 92? Syntax,?pos,?word,?HMM,?DA?history?(previous=EMPTY?or?Q)? .901? .57?
T? 29? Syntax,?pos,?word,?DA?history?(previous=POSITIVE)?(3?ago=POSITIVE)? .992? .92?    In an experiment to quantify the utility of these features, it was found that for many dialogue acts, a binary dialogue act classifier that was trained using only lexical and syntactic features achieved the same or better classification accuracy than the model that was given all features (Figure 4). For comparison, the modified baseline model used the last three true dialogue acts (with speaker tags); this model achieved better than chance for four dialogue acts and achieved nearly as well as the full model for GREETING (GR). The models that were given all possible features for selection outperformed the lexical/syntax-only model for seven of the thirteen dialogue acts (GREETING (GR), REQUEST FOR FEEDBACK (RF), POSITIVE FEEDBACK (P), POSITIVE ELABORATED FEEDBACK (PE), UNCERTAIN ELABORATED FEEDBACK (UE), NEGATIVE FEEDBACK (N), and EXTRA-DOMAIN (EX)); however, it should be noted that none of these differences in performance is statistically reliable at the p=0.05 level.   
Figure 3. Confusion matrix 
302
 Figure 4. Kappa for binary DA classifiers by features available for selection 6 Discussion We have presented a maximum likelihood classifier that assigns dialogue act labels to user utterances from a corpus of human-human tutorial dialogue given a set of lexical, syntactic, and structural features. Overall, this classifier achieved 62.8% accuracy in ten-fold cross-validation on the corpus. This performance is on par with other automatic dialogue act tagging models, both sequential and vector-based, in task-oriented domains that do not feature complex, user-driven parallel tasks. In a catalogue ordering domain with an integrated task and dialogue model, Bangalore et al (2009) report 75% classification accuracy for user utterances using a maximum entropy classifier, a 275% improvement over baseline. Poesio & Mikheev (1998) report 54% classification accuracy by utilizing conversational game structure and speaker changes in the Maptask corpus, an improvement of 170% over baseline. Recent work on Maptask reports a classification accuracy of 65.7% using local utterance (such as lexical/syntactic) features alone, with prosodic cues yielding further slight improvement (Sridhar et al, 2009). This classifier is analogous to our lexical/syntactic feature model, which achieved 60.2% accuracy. The results of these models demonstrate that, consistent with the findings in other task-oriented domains, lexical/syntactic features are highly useful for classifying student dialogue moves in this complex task-oriented domain. Models trained using those lexical/syntactic features 
performed almost universally better (with the exception of the binary classifier for GREETING) than models that were given the same left context of true dialogue act tags.  It was hypothesized that leveraging both the hidden dialogue state and hierarchical subtask features would improve the performance of the classifiers. There is some evidence that the subtask structure was helpful for the overall classifier; however, no HMM features were kept during feature selection for the overall model. Of the binary models, approximately half performed better than the overall model in terms of true positive rate; of those, three did so by including HMM or task/subtask features among the selected attributes to differentiate different tones of student feedback. However, this difference in performance was not statistically reliable. This finding suggests that, given lexical and syntactic features which are strong predictors of dialogue acts, the hidden dialogue state as captured by an an HMM may not contribute significantly to the dialogue act classification task. 7 Conclusion and Future Work Dialogue modeling for complex task-oriented domains poses significant challenges. An effective dialogue model allows systems to detect user dialogue acts so that they can respond in a manner that maximizes the chance of success. Experiments with the data-driven classifiers presented in this paper demonstrate that lexical/syntactic features can effectively classify student dialogue acts in the task-oriented tutoring domain. For POSITIVE, NEGATIVE, and UNCERTAIN ELABORATED student feedback acts, which play a key role in tutorial dialogue system, the addition of hidden dialogue state features (as learned by an HMM) and task/subtask features (annotated manually) improve classification accuracy, but not statistically reliably.    The overarching goal of this work is to create a data-driven tutorial dialogue system that learns its behavior from corpora of effective human tutoring. The dialogue act classification models reported here constitute an important step toward that goal, by integrating the dialogue stream with a parallel user-driven task event stream. The next generation of data-driven systems should leverage models that capture the rich interplay between dialogue and task. Future work will focus on data-driven approaches to task recognition and tutorial planning. Additionally, as dialogue system research addresses 
303
increasingly complex task-oriented domains, it becomes increasingly important to investigate unsupervised approaches for dialogue act classification and task recognition.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Allen, J., Ferguson, G., & Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of the IUI, 1-8.  Artstein, R., & Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4), 555-596.  Austin, J. L. (1962). How to do things with words. Oxford: Oxford University Press. Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  Berger, A. L., Pietra, V. J. D., & Pietra, S. A. D. (1996). A maximum entropy approach to natural language processing. Comp. Ling., 22(1), 71.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden markov models. Proceedings of NAACL-HLT, Short Papers, 49-52.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Leveraging hidden dialogue state to select tutorial moves. Proceedings of the 5th NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Los Angeles, California.  Chotimongkol, A. (2008). Learning the structure of task-oriented conversations from the corpus of in-domain dialogs. (Unpublished Ph.D. Dissertation). Carnegie Mellon University School of Computer Science. Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.  De Marneffe, M. C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency parses 
from phrase structure parses. Proceedings of LREC, Genoa, Italy.   Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009). The WEKA data mining software: An update. SIGKDD Explorations, 11(1) Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Comm., 48(3-4), 354-373.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Loper, E., & Bird, S. (2004). NLTK: The natural language toolkit. Proceedings of the ACL Demonstration Session, Barcelona, Spain. 214-217.  Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137.  Purver, M., Kording, K. P., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic modelling for multi-party spoken discourse. Proceedings of the ACL, Sydney, Australia. , 44(1) 17.  Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Sridhar, V. K. R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4), 407-422.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Comp. Ling., 26(3), 339-373.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.   
304
    
Time Stamp Dialogue Stream Task Stream  2008-04-11 18:23:45 Student:  so do i have to manipulate the array this time? [Q]   2008-04-11 18:23:53 Tutor:  this time, we need to do two things [S]    2008-04-11 18:24:02 Tutor:  first, we need to create a new array to hold the changed values [S]    2008-04-11 18:24:28     i 2008-04-11 18:24:28     n 2008-04-11 18:24:28     t 2008-04-11 18:24:28     \sp 1-a-i BUGGY 2008-04-11 18:24:35     \del  2008-04-11 18:24:36     \sp  2008-04-11 18:24:36     d 2008-04-11 18:24:36     o 2008-04-11 18:24:36     u 2008-04-11 18:24:36     b 2008-04-11 18:24:37     l 2008-04-11 18:24:37     e 2008-04-11 18:24:37     \sp 2008-04-11 18:24:39     [] 
1-a-i CORRECT 
2008-04-11 18:24:40     \sp  2008-04-11 18:24:42     n 2008-04-11 18:24:42     e 2008-04-11 18:24:42     w 2008-04-11 18:24:43     \sp 2008-04-11 18:24:44     \del 2008-04-11 18:24:45     T 2008-04-11 18:24:46     \del 2008-04-11 18:24:54     T 2008-04-11 18:24:54     i 2008-04-11 18:24:54     m 2008-04-11 18:24:54     e 2008-04-11 18:24:54     s 2008-04-11 18:24:55     3 2008-04-11 18:24:57     ; 
1-a-ii CORRECT 
2008-04-11 18:25:11 Student:  good? [RF]    2008-04-11 18:25:14 Tutor:  good so far, yes [PF]    2008-04-11 18:25:29 Student:  so now i have to change parts of the times array right? [Q]    2008-04-11 18:25:34 Tutor:  not quite [LF]    2008-04-11 18:25:57 Tutor:  So, when you create a new object, like a String for example, you'd say something like  String s = new String() [S]    2008-04-11 18:25:59 Tutor:  right? [AQ]    2008-04-11 18:26:06 Student:  right [P]    2008-04-11 18:26:14 Tutor:  arrays are similar [S]         
    
Appendix 
Excerpt 1. Parallel synchronous dialogue and task event streams with annotations. (Note tutor dialogue acts: AQ=ASSESSING QUESTION, LF=LUKEWARM FEEDBACK, PF=POSITIVE FEEDBACK) 
305
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 49?58,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
The Impact of Task-Oriented Feature Sets on  HMMs for Dialogue Modeling   Kristy Elizabeth Boyer  Eun Young Ha   Robert Phillips*   James Lester   Department of Computer Science North Carolina State University *Dual affiliation with Applied Research Associates, Inc. Raleigh, North Carolina, USA {keboyer, eha, rphilli, lester}@ncsu.edu 
  
Abstract 
Human dialogue serves as a valuable model for learning the behavior of dialogue systems. Hidden Markov models? sequential structure is well suited to modeling human dialogue, and their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process with a layer of implicit, highly influen-tial structure. HMMs have been shown to be effective for a variety of descriptive and pre-dictive dialogue tasks. For task-oriented dia-logue, understanding the learning behavior of HMMs is an important step toward building unsupervised models of human dialogue. This paper examines the behavior of HMMs under six experimental conditions including different task-oriented feature sets and preprocessing approaches. The findings highlight the im-portance of providing HMM learning algo-rithms with rich task-based information. Additionally, the results suggest how specific metrics should be used depending on whether the models will be employed primarily in a de-scriptive or predictive manner.  1 Introduction Human dialogue serves as a valuable model for learning the behavior of dialogue systems. For this reason, corpus-based approaches to dialogue man-agement tasks have been an increasingly active area of research (Bangalore, Di Fabbrizio, & Stent, 2006; Di Eugenio, Xie, & Serafin, 2010; Georgila, Lemon, Henderson, & Moore, 2009; Rotaru & Litman, 2009). Modeling the dialogue policies that 
humans employ permits us to directly extract con-versational and task-based expertise. These tech-niques hold great promise for scaling gracefully to large corpora, and for transferring well across do-mains.    The richness and flexibility of human dialogue introduce nondeterministic and complex patterns that present challenges for machine learning ap-proaches. One approach that has been successfully employed in dialogue modeling is the hidden Mar-kov model (HMM) (Rabiner, 1989). These models are well suited to the sequential nature of dialogue (Stolcke et al, 2000). Moreover, their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process whose observations are influenced by a layer of implicit, yet highly rel-evant, structure (Boyer et al, 2009; Woszczyna & Waibel, 1994).  HMMs have been shown to perform well on important dialogue management tasks such as au-tomatic dialogue act classification (Stolcke et al, 2000). Our work has employed HMMs for a differ-ent goal: learning dialogue policies, or strategies, from corpora (Boyer, Phillips, et al, 2010; Boyer, Phillips, Ingram, et al, in press). This work can be viewed from two perspectives. First, a descriptive goal of the work is to learn models that describe the nature of human dialogues in succinct probabilistic terms, in a way that facilitates important qualitative investigations. The second and complementary goal is predictive: learning models that accurately pre-dict the dialogue moves of humans, in order to cap-ture a dialogue policy that can be used within a system.   
49
Both of these goals are of paramount im-portance in tutorial dialogue, in which tutors and students engage in dialogue in support of a learning task (Boyer, Ha, et al, 2010; VanLehn et al, 2007). Descriptive modeling represents a critical step to-ward more fully understanding the phenomena that contribute to the high effectiveness of human tutor-ing, which has to date been unmatched by tutorial dialogue systems. Predictive models, on the other hand, may be used directly as dialogue policies within systems.  The HMMs considered here were learned from an annotated corpus of textual human-human tuto-rial dialogue. In this domain, HMMs have been shown to correspond qualitatively to widely held conceptions of tutorial dialogue strategies, and ad-jacency pair analysis before model learning has been shown to enhance this qualitative correspond-ence (Boyer et al, 2009). Moreover, HMMs can identify in an unsupervised fashion structural com-ponents that correlate with student knowledge gain (Boyer, Phillips, Ingram, et al, in press).  However, to date, several important questions have not been explored. The answers to these ques-tions have implications for learning HMMs for task-oriented dialogues. The questions include the following: 1) How reliably does the HMM learning framework converge to the hyperparameter N, the best-fit number of hidden states? 2) What are the effects of preprocessing approaches, specifically, adjacency pair analysis, on the resulting HMMs? 3) How do different feature sets for task-oriented dialogue impact the descriptive fit and predictive power of learned HMMs? This paper addresses these questions. The findings suggest that model stability and predictive power benefit from the richest possible input sequences, which include not only dialogue acts but also information about the task state and the absence of particular tutor dia-logue moves. Additionally, we find that traditional measures of HMM goodness-of-fit may not identify the most highly predictive models under some con-ditions. 2 Background HMMs have been used for dialogue modeling tasks for many years. Early work utilized HMMs to model underlying linguistic structure for the pur-poses of identifying speech acts and reducing per-plexity for speech recognition (Stolcke et al, 2000; 
Woszczyna & Waibel, 1994). These projects treat-ed underlying dialogue structure as the hidden lay-er, and dialogue utterances as observations. This treatment is analogous to the work presented in this paper, except that our observations are dialogue act tags only, rather than being constituent words in each utterance. Our goals are also different: to cre-ate a qualitatively interpretable model of dialogue structure that corresponds to widely accepted no-tions of task-oriented dialogue, and to learn a high-ly predictive dialogue policy from a human-human dialogue corpus.  HMMs rely on treating dialogue as a sequential Markov process in which each observation depends only on a finite set of preceding observations. Some other approaches that rely on this assumption treat dialogue as a Markov decision process or partially observable Markov decision process, in which state changes are associated with actions and rewards (e.g., Young et al, 2010). Such work focuses on learning an optimal policy, typically utilizing a combination of human and simulated dialogue cor-pora. Reinforcement learning techniques can then be applied to learn the optimal policy based on the observed rewards. In contrast, we start with a rich corpus of human-human dialogue, which may have poor coverage in some areas (though the dialogue act tags were empirically derived and therefore mit-igate this problem to some extent), and subsequent-ly learn a model that explains the variance in that human corpus as well as possible. Capturing the dialogue policy implicit within a corpus of human-human dialogue has been ex-plored in other work in a catalogue-ordering do-main (Bangalore, Di Fabbrizio, & Stent, 2006). That work utilized maximum entropy modeling to predict human agents? dialogue moves within a vector-based framework. Although a vector-based approach differs in many regards from the sequen-tial HMM approach described here, both approach-es assume a dependence only on a finite history. HMMs accomplish this through graphical depend-encies, while vector-based approaches accomplish it by including features for a restricted window of left-hand context. The results of this catalogue-ordering project highlight how challenging it is to predict human agents? dialogue moves in a task-oriented domain. 
50
3 Corpus  The corpus was collected during a human-human tutoring study. Students solved an introductory computer programming problem in the Java pro-gramming language. Tutors were located in a sepa-rate room and communicated with students through textual dialogue while viewing a synchronized view of the student?s problem-solving workspace. Forty-eight students interacted for approximately one hour each with a tutor. Students exhibited sta-tistically significant learning gains from pretest to posttest, indicating that the tutoring was effective (Boyer, Phillips, Ingram, et al, in press). The cor-pus contains 1,468 student moves and 3,338 tutor moves. Overlapping utterances, which are common in dialogue platforms such as instant messaging, were prevented by permitting only one user to con-struct a dialogue message at a time. Because the corpus is textual, utterances were segmented at tex-tual message boundaries except when the lead dia-logue annotator noted the presence of two separate dialogue acts within non-overlapping chunks of text. In these events the utterance was segmented by the primary annotator prior to being tagged by the second dialogue act annotator.  In addition to dialogue act annotation, the cor-pus was manually annotated for task structure and correctness (Section 3.2), and for delayed tutor feedback (Seciton 3.3). The appendix displays an excerpt from the annotated corpus.  3.1 Dialogue Act Annotation As part of prior work, the corpus was annotated with dialogue acts for both tutor (Boyer, Phillips, Ingram, et al, in press) and student (Boyer, Ha, et al, 2010) utterances (Table 1). One annotator tagged the entire corpus, while a second annotator independently tagged a randomly selected 10% of tutoring sessions. The inter-annotator agreement Kappa score was 0.80.  3.2 Task Annotation The corpus includes 97,509 keystroke-level task events (computer programming actions), all taken by the student. Tutors viewed synchronously, but could not edit, the computer program. The task ac-tions were manually clustered and labeled for sub-task structure (Boyer, Phillips, et al, 2010). The task structure annotation was hierarchical, with 
leaves corresponding to specific subtasks such as creating a temporary variable in order to swap two variables? values (subtask 3-c-iii-2). Each problem-solving cluster, or subtask, was then labeled for correctness (Table 2). These correctness labels are utilized in the models presented in this paper. The Kappa agreement statistic for the correctness anno-tation on 20% of the corpus was 0.80. Table 1. Dialogue act tags Dialogue Act Tutor Example ASSESSING Q. Which type should that be? EXTRA-DOMAIN A coordinator will be there soon. GROUNDING Ok. LUKEWARM FDBK That?s close. LUKEWARM CONTENT FDBK Almost there, but the second parameter isn?t quite right. NEGATIVE FDBK That?s not right. NEGATIVE CONTENT FDBK No, the counter has to be an int. POSITIVE  FDBK Perfect. POSITIVE CONTENT FDBK Right, the array is a local varia-ble. QUESTION Which approach do you prefer? RESPONSE It will be an int. STATEMENT They start at 0. Table 2. Task correctness tags Correctness Tag Description CORRECT Fully conforming to the require-ments of the task. BUGGY Violating the requirements of the task. These task events typically require tutorial remediation. INCOMPLETE Not violating, but not yet fulfilling, the requirements of the task. 
DISPREFERRED Technically fulfilling requirements but not utilizing the target con-cepts being tutored. These events typically require tutorial remediation. 3.3 Annotation for Delayed Tutor Feedback The dialogue act and task annotations reflect posi-tive evidence regarding what did occur in the dia-logues. An additional annotation was introduced for what did not occur?specifically, instances in which tutors did not to make a dialogue move in response to students? relevant task actions. The task in our corpus is computer programming, so bugs in the task correspond to errors either in syntax or se-
51
mantics of the computer program compared to the desired outcome. The human tutors were working with only one student at a time and were carefully monitoring student task actions during the dialogue, so we take the absence of a dialogue move at a rel-evant point to be an intentional choice by the tutor to delay feedback as part of the tutorial strategy. The automatic annotation for delayed feedback in-troduced two new event tags: NO-MENTION of cor-rectly completed subtasks, and NO-REMEDIATION of existing bugs within the task.  The intuition behind these tags is that within a learned dialogue policy, specifically modeling when not to intervene is crucial. Typically human tutors mention correctly completed subtasks, but at times other tutorial goals eclipse the importance of doing so. The NO-MENTION tag captures these in-stances. On the other hand, typically when working with novices, human tutors remediate an existing bug quickly. However, tutors may choose to delay this remediation for a variety of reasons such as remediating a different bug instead or asking a con-ceptual question to encourage the student to reflect on the issue. The NO-REMEDIATION tag captures these instances of the absence of remediation given that a bug was present. These two annotations for delayed feedback were performed automatically (Boyer, Phillips, Ha, et al, in press).  3.4 Adjacency Pair Modeling Prior work has demonstrated that adjacency pairs can be identified in an unsupervised fashion from a corpus (Midgley, Harrison, & MacNish, 2006). This technique relies on statistical analysis to de-termine the significant dependencies that exist be-tween pairs of dialogue acts, or in our task-oriented corpus, pairs of dialogue acts or task actions. After the pairs of dependent events are identified, they are joined within the corpus algorithmically (Boyer et al, 2009). Joining a pair of dependent moves in this way is equivalent to introducing a deterministic (probability=1) succession between observation symbols. This type of dependency cannot be learned in the traditional first-order HMM frame-work, but is desirable when two observations are strongly linked.1                                                             1 Enhanced HMM structures, such as autoregressive HMMs, which allow for direct graphical links between observation symbols, can learn such a dependency but only in stochastic terms. 
The experiment that is described in Section 4 utilizes different feature sets to learn and compare HMMs. Table 3 shows these feature sets and their most highly statistically significant adjacency pairs. Table 3. Experimental conditions and top three ad-jacency pairs (subscripts denote speaker, Student or Tutor) Condition Description Significant Adjacency Pairs DAONLY Dialogue acts only QS~RspT  GroundS~GroundT AssessQT~PosFdbkS 
DATASK Dialogue acts & task cor-rectness events 
QS~RspT CorrectTaskS~CorrectTaskS GroundS~GroundT 
DATASK-DELAY 
Dialogue acts, task correctness, & delayed feedback  
QS~RspT NoRemediateT~BuggyTaskS CorrectTaskS~CorrectTaskS 
4 Models HMMs were selected as the modeling framework for this work because their sequential nature is well suited to the structure of human dialogue, and their ?hidden? variable corresponds to widely held con-ceptions of dialogue as having an unobservable, but influential, layer of stochastic structure. For exam-ple, in tutoring, an ?explanation? mode is common, in which the tutor presents new information and the student provides acknowledgments or takes task actions accordingly. Although the presence of the ?explanation? goal is not directly observable in most dialogues, it may be inferred from the obser-vations. These sequences correspond to the input observations for learning an HMM.  4.1 Hidden Markov Models HMMs explicitly model hidden states within a doubly stochastic structure (Rabiner, 1989). A first-order HMM, in which each hidden state depends only on the immediately preceding hidden state, is defined by the following components: ? ? = {?1, ?2, ?, ?M}, the observation sym-bol alphabet ? S = {s1,s2,?,sN}, the set of hidden states 
52
? ?=[?i], i=1,?,N, the initial probability dis-tribution, where ?i is the probability of the model beginning in hidden state si in S  ? A=[aij], a transition probability distribution, where aij is the probability of the model transitioning from hidden state i to hidden state j for i,j=1,?,N ? B=[bik], an emission probability distribu-tion where bik is the probability of state i (i=1,?,N) emitting (or generating) obser-vation symbol k (k=1,?,M). 4.2 Dialogue Modeling with HMMs In this work, the observation symbol alphabet ? is given. For each experimental condition, ? is either 1) all dialogue act tags, 2) all dialogue acts plus task correctness tags, or 3) dialogue act, task cor-rectness, and delayed feedback tags. The transition probability distribution A, emission probability dis-tribution B, and initial probability distribution ? are learned by the standard Baum-Welch algorithm for optimizing HMM parameters (Rabiner, 1989). This algorithm is susceptible to becoming trapped in local optima, so our approach uses ten-time random restart with new initial parameters for each model to reduce the probability of selecting a model that represents only a local optimum.  The hyperparameter N, which is the best number of hidden states, is also learned rather than fixed. This process involves running the full HMM train-ing algorithm, including random restarts in ten-fold cross-validation, across the data and selecting the N that corresponds to the best mean goodness-of-fit measure. For HMMs, a typical goodness-of-fit measure is log-likelihood, which captures how like-ly the observations would be under the current model. The log is taken for practical reasons, to avoid numerical underflow. Higher log-likelihood corresponds to improved model fit. However, typi-cally it is desirable to penalize a higher number of hidden states, since increasing the model complexi-ty results in tradeoffs that may not be fully warrant-ed by the improvement in model fit. In this work, we utilize the Akaike Information Criterion (AIC), a standard penalized log-likelihood metric (Akaike, 1976).     
AIC = 2*N ? 2*ln(likelihood) Lower values of AIC indicate better model fit. 4.3 Experimental Conditions HMMs were learned using three separate feature sets, each providing a progressively more complete picture of the task-oriented dialogues: dialogue acts only (DAONLY), dialogue acts and task events (DATASK), and dialogue acts with both task cor-rectness events and tags for delayed tutor feedback (DATASKDELAY).  In addition to the three different feature sets, each condition included one of two types of pre-processing. Each type of model was trained on un-altered sequences of the annotated tags, which we refer to as the UNIGRAM condition. Additionally, each type of model was trained on sequences with statistically dependent adjacency pairs joined in a preprocessing step as described in Section 3.4. The UNIGRAM and ADJPAIR conditions were explored for each of the three feature sets, resulting in six experimental conditions. These conditions were chosen in order to explore the convergence behav-ior of HMMs under the different feature sets and preprocessing, and to compare measures of descrip-tive fit with measures of predictive power.  4.4 Learned HMMs Figures 1 and 2 show a subset of the DAONLY UNIGRAM model and the DATASKDELAY ADJPAIR model. These figures depict the structure of our HMMs: each hidden state is associated with an emission probability distribution over the possible observation symbols.  5 Goodness-of-Fit Curves The learning algorithm described in Section 4.2 was applied to input sequences under the six exper-imental conditions to learn the best-fit HMM pa-rameters. Figure 3 displays these AIC results, which are discussed in detail in the remainder of this section.   
53
 Figure 1. Subset of learned HMM (N=13) for DAONLY UNIGRAM condition  
 Figure 2. Subset of learned HMM (N=9) for DATASKDELAY ADJPAIR condition  5.1 Impact of Experimental Conditions  For the DAONLY condition, both the UNIGRAM and ADJPAIR models generally improve until N=12 or 13, after which the fit generally worsens. A differ-
ent pattern emerges for the DATASK condition, in which the UNIGRAM sequences are optimally fit to a model with 16 states, while the ADJPAIR se-quences are fit to a model with 8 states. Finally, for the DATASKDELAY condition, the UNIGRAM se-quences are best fit by a model with 10 hidden states, while the ADJPAIR sequences are fit best by 9. Typically, we see that ADJPAIR sequences are fit to slightly simpler models in terms of the hy-perparameter N, number of hidden states.   
Figure 3. Number of hidden states and cor-responding adjusted AIC, shifted to a mini-mum score of zero indicating the best-fit N 
Adjust
ed AIC
 
a) Dialogue ActsOnly (DAONLY) 
  N (number of hidden states) 
Adjust
ed AIC
 
b) Dialogue Act and Task Events (DATASK) 
  N (number of hidden states) 
Adjust
ed AIC
 
c) Dialogue Act, Task, & Delayed Feedback (DATASKDELAY)  
  N (number of hidden states) 
54
Stability in the hyperparameter N is an im-portant consideration because an underlying as-sumption of our work is that the hidden states correspond to unobserved stochastic structures of the real world process?that is, we hypothesize that a ?true? value for N exists. We would like models to exhibit decreasing variation in goodness of fit measures around this true N. To examine this stability we consider the three best AIC values for each condition and their corresponding Ns: the set {Nk-best | k=1,2,3}. The range of this set indicates how ?far apart? the best three Ns are: for example, in the DAONLY UNIGRAM condition, the top three models have Ns of {13,10,15}, yielding a range of 5. Intuitively, a small value for this metric indicates that the model has converged tightly on N.  Figure 4 shows the stability results for the six different experimental conditions. As shown in the figure, for the DATASK and DATASKDELAY condi-tions, the ADJPAIR models achieve the smallest range among the top three values of N; these mod-els converge most tightly to the ?best? value.   
 Figure 4. Stability of N (range of {N1best, N2best, N3best}) ? smaller implies tighter convergence to ?best? N 6 Predictive Analysis Section 5 presented an analysis of the goodness-of-fit curves of HMMs learned from the corpus. The measures of stability and discrimination for N cap-ture important aspects of the behavior of HMMs toward this parameter, which is conceived of as representing ?true? real-world stochastic behavior. In this way, Section 5 has presented a descriptive view of HMM dialogue models.  This section presents a predictive view of the models. Specifically, we consider prediction accu-racy, defined as the percent of tutor dialogue moves 
that the model is able to correctly predict given the dialogue history sequence up to that point.  6.1 Impact of Dependent Adjacency Pairs We first explore whether the preprocessing step of joining dependent adjacency pairs impacted predic-tion accuracy. The prediction accuracy of the best-fit model in each condition is displayed in Figure 5. This figure includes prediction accuracy on training data, which were used to learn model parameters, as well as prediction accuracy on testing data, which were withheld from model training.  
 Figure 5. Prediction accuracy for tutor moves  As shown in Figure 5, joining the adjacency pairs improved model performance on the training sets of all three conditions, indicating that the varia-tion within the training data was better explained by ADJPAIR models. (This measure of predictive power is different from a goodness-of-fit criterion as described in the previous section, a relationship that will be discussed further in Section 7.) In con-trast to the training set performance, the ADJPAIR models performed better than UNIGRAM models for the testing set only in the DATASKDELAY condi-tion.   6.2 Impact of Task-Oriented Feature Sets As illustrated in Figure 5, the three feature sets per-form similarly under the UNIGRAM condition. This performance is slightly above baseline (DAONLY and DATASK baselines = 0.38; DATASKDELAY baseline = 0.30), and diminishes little between the training and testing sets. In contrast, under the ADJPAIR condition, performance between condi-tions and across training and testing sets varies. The DATask model performs far better on predicting observations in the training than the testing set, 
55
suggesting possible overfitting to the training set. This relationship is discussed further in Section 7. The DATASKDELAY model performs well during both training and testing, though with a slight de-crease in accuracy on the testing set.   6.3 Relationship Between Predictive and De-scriptive Metrics Measures of fit such as log-likelihood and AIC cap-ture the likelihood of observing the data given a model. Predictive accuracy, on the other hand, measures the probability that the model can predict the next observation given a partial sequence. In general, we would expect these measures to corre-late well; however, there is not perfect correlation between these metrics because the mechanism by which log-likelihood (and thereby AIC) is derived involves maximizing likelihood over complete se-quences, while prediction is performed over partial sequences.  To examine how well AIC and prediction accu-racy correlate, Figure 6 displays these values for a subset of the models in the DAONLY UNIGRAM condition and the DATASKDELAY ADJPAIR condi-tion. These two conditions represent the extremes of the experimental conditions, with DAONLY con-taining the least information about the task-oriented dialogue while DATASKDELAY contains the most information.  As shown in Figure 6, the correlation for DAONLY UNIGRAM roughly conforms to what would be expected: lower AIC, indicating better model fit, is associated with the highest prediction accuracies. The relationship is less clear for the DATASKDELAY ADJPAIR condition. While its worst AIC is associated with the lowest prediction accuracy as expected, the best AIC is not associated with the highest prediction accuracy. This phenom-enon may be due to the lack of spread among AIC values overall for this condition; as seen in Figure 3, the DATASKDELAY ADJPAIR condition has the flattest AIC curve of all conditions, indicating that for this condition the difference between best-fit and worst-fit models is smaller than for any other condition. The inconsistent relationship between AIC and prediction accuracy, therefore, may be the product of noise surrounding a large set of ?good? models, whereas for the DAONLY UNIGRAM condi-tion, the set of good models is smaller.   
 7 Discussion The results suggest several important findings re-garding feature sets and preprocessing for learning HMMs of task-oriented dialogue. First, the models? convergence patterns to a best-fit N, number of hidden states, indicate that more information em-bedded within the sequences may correspond with a flatter goodness-of-fit curve. Adding more infor-mation to the input sequences may introduce some regularities that partly mitigate the limitations of even a poorly fit HMM. This additional infor-mation may come in the form of adjacency pairs discovered in an unsupervised fashion, which im-proved the stability of convergence on the best-fit N under the DATASK and DATASKDELAY condi-tions. This increased stability is likely due to the fact that under these conditions, leveraging adja-cency pair information augments the HMM?s struc-ture with contextual dependencies that could otherwise not be learned under the traditional HMM framework.  For predictive accuracy, the benefits of richer input sequences are also highlighted. The most highly predictive models included all three sources 
Predic
tion Ac
curacy
 a) DAOnly UNIGRAM Condition 
  AIC 
Predic
tion Ac
curacy
 b) DATASKDELAY ADJPAIR Condition  
  AIC Figure 6. Prediction accuracy vs. AIC 
56
of information: dialogue acts, task events, and de-layed feedback tags. However, with the addition of this rich information to the input sequences and the accompanying flatter goodness-of-fit curve as dis-cussed above, we noted an irregular pattern of cor-relation between goodness-of-fit and predictive accuracy that is worthy of future exploration. Spe-cifically, it appears that the most highly predictive DATASKDELAY ADJPAIR model, which is the most highly predictive of all models in all conditions, does not correspond to the best (lowest) AIC for that condition (Figure 3). This finding suggests that when a predictive task is the primary goal, a predic-tive metric should be used to select the best-fit model. Additional support for such an approach is provided by the close correspondence between training and testing set prediction accuracy. 8 Conclusion Understanding how HMMs behave under different feature sets is an important step toward learning effective models of task-oriented dialogue. This paper has examined how HMMs converge to a best number of hidden states under different experi-mental conditions. We have also considered how well HMMs under these conditions predict tutor dialogue acts within a corpus of task-oriented tutor-ing, a crucial step toward learning dialogue policies from human corpora. The findings highlight the importance of adding rich task-based features to the input sequences in order to learn HMMs that con-verge tightly on the best-fit number of hidden states. The results also indicate that caution should be used when utilizing traditional goodness-of-fit metrics, which are appropriate for descriptive ap-plications, if the goal is to learn a highly predictive model.  This line of research is part of a larger research program of learning unsupervised models of human task-oriented dialogue that can be used to define the behavior of dialogue systems. Developing a framework for learning a dialogue policy from hu-man corpora, as discussed here, is a critical step toward that goal. Future work should focus on un-supervised dialogue act classification, and address the challenges of user plan recognition.  
Acknowledgments. This work is supported in part by National Science Foundation through Grants REC-0632450, IIS-0812291, DRL-1007962 and the STARS 
Alliance Grant CNS-0739216. Any opinions, findings, conclusions, or recommendations expressed in this re-port are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References Akaike, H. (1976). An information criterion (AIC). Math. Sci., 14(153), 5-9. Bangalore, S., Di Fabbrizio, G., & Stents, A. (2006). Learning the structure of task-driven human-human dialogs. Proceedings of ACL ?06, 201-208.  Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of SIGDIAL (pp. 297-305).  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (in press). Learning a Tutorial Dialogue Policy for Delayed Feedback. Proceedings of the 24th International FLAIRS Con-ference. Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dia-logue structure with adjacency pair analysis and hid-den Markov models. Proceedings of NAACL HLT, Companion Volume, 49-52.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Leveraging Hidden Dialogue State to Select Tutorial Moves. Proceedings of the NAACL HLT 2010 Fifth Work-shop on Innovative Use of NLP for Building Educa-tional Applications (pp. 66-73). Boyer, K. E., Phillips, R., Ingram, A., Young, E., Wallis, M., Vouk, M., et al (in press). Investigating the Re-lationship Between Dialogue Structure and Tutoring Effectiveness: A Hidden Markov Modeling Ap-proach. International Journal of Artificial Intelli-gence in Education. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue Act Classification, Higher Order Dialogue Structure, and Instance-Based Learning. Dialogue & Dis-course, 1(2), 1-24.  Georgila, K., Lemon, O., Henderson, J., & Moore, J. D. (2009). Automatic annotation of context and speech acts for dialogue corpora. Natural Language Engi-neering, 15(3), 315-353.  Midgley, T. D., Harrison, S., & MacNish, C. (2006). Empirical verification of adjacency pairs using dia-logue segmentation. Proceedings of SIGDIAL, 104-108.  Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recogni-tion. Proceedings of the IEEE, 77(2), 257-286.  
57
Rotaru, M., & Litman, D. J. (2009). Discourse Structure and Performance Analysis : Beyond the Correlation. Proceedings of SIGDIAL (pp. 178-187). Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., et al (2000). Dialogue Act Model-ing for Automatic Tagging and Recognition of Con-versational Speech. Computational Linguistics, 26(3), 339-373.  VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., & Rose, C. P. (2007). When Are Tutorial 
Dialogues More Effective Than Reading? Cognitive Science: A Multidisciplinary Journal, 30(1), 3-62.  Woszczyna, M., & Waibel, A. (1994). Inferring linguis-tic structure in spoken language. Proceedings of the International Conference on Spoken Language Pro-cessing (pp. 847-850). Young, S., Ga?i?, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., et al (2010). The Hid-den Information State model: A practical framework for POMDP-based spoken dialogue management. Computer Speech & Language, 24(2), 150-174.   Appendix. Excerpt from task-oriented textual human-human tutoring corpus. Speaker Utterance or Event Tag Student: [Task action on subtask 3-c-i-4] BUGGY Student: [Task action on subtask 3-c-ii-5] CORRECT Tutor: [Does not provide remediation for existing bug] NOREMEDIATION Student: [Task action on subtask 3-c-iii-1] BUGGY Student: i don't remember off the top of my head how the swap function worked. most of the time i just copied and pasted it from some of my older code NEGATIVECONTENTFDBK Tutor: The easiest way to swap x and y is to make a tempo-rary variable  Student: Ok ACK Student: do i need to pass the entire array and the indecies of the items to swap? ASSESSQ Tutor:  if you want to use a seperate method to swap, then yes, you'll have to pass those things  POSCONTENTFDBK Tutor:  [Does not mention a correctly completed subtask]	 ? NOMENTIONCOMP Student: oh. i guess i could just swap it in the same method. it is problably easier that way, and less code. we were showed in class how to do it separately, but i had never thought of doing it the other way.  
STMT 
Student: [Task action on subtask 3-c-iii-2] DISPREFERRED Tutor:  Both ways work, but it?s definitely less code to just do it inside this method.  STMT Student: Ok ACK  
58
