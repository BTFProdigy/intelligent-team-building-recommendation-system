GoDiS  - An  Accommodat ing  Dia logue System 
Sta f fan  Larsson ,  Peter  L jung l6 f ,  Rob in  Cooper ,  E l i sabet  Engdah l ,  S t ina  Er i csson  
Depar tment  of linguistics, GSteborg University 
Box 200-295, Humanisten,  SE-405 30 G5teborg,  Sweden 
{s l ,  peb,  cooper ,  engdah l ,  s t inae}01 ing ,  gu. se 
Abst rac t  
This paper accompanies a demo of the GoDiS sys- 
tem. Work on~hi~ system was reported at IJCAI- 
99 (Bohlin et-al.~ 1999). GoDiS is a prototype 
dialogue system for information-seeking dialogue, 
capable of accommodating questions and tasks to 
enable the user to present information in any de- 
sired order, without explicitly naming the dialogue 
task. GoDiS is implemented using the TRINDIKIT 
software package, which enables implementation f 
these behaviours in a compact and natural way. 
1 In t roduct ion  
This paper accompanies a demo of the GoDiS 1 
system reported at IJCAI-99 (Bohlin et al, 
1999). GoDiS is a prototype dialogue system for 
information-seeking dialogue, capable of accommo- 
dating questions and tasks to enable the user to 
present information in any desired order, without 
explicitly naming the dialogue task. GoDiS is im- 
plemented using the TRINDIKIT 2 software package 
developed in the TRINDI project. The TRINDIKIT 
is a toolkit for building and experimenting with dia- 
logue move engines and information states (IS), We 
use the term information state to mean, roughly, 
the information stored internally by an agent, in 
this case a dialogue system. A dialogue move engine 
(DME) updates the information state on the basis 
of observed ialogue moves and selects appropriate 
moves to be performed. 
2 System Descr ip t ion  
The overall structure of the GoDiS system is 
illustrated below: 
1Work on GoDiS has been supported by the TRINDI 
(Task Oriented Instructional Dialogue), EC Project LE4- 
8314, SDS (Swedish Dialogue Systems), NUTEK/HSFR Lan- 
guage Technology Project F1472/1997, and INDI (Infor- 
mation Exchange in Dialogue), Riksbankens Jubileumsfond 
1997-0134, projects. 
2 .x~.n~. ling, gU. se/research/proJ ects/trlndi/ 
Like any dialogue system built using the 
TRINDIKIT, GoDiS consists of a number of mod- 
ules, an information state, and a number of resources 
hooked up to the information state. 
In addition to the control module, which wires 
together the other modules, there are six modules in 
? GoDiS: input, which receives input3from the user; 
interpret, which interprets utterances as dialogue 
moves with some content; generate, which gener- 
ates natural language from dialogue moves; out- 
put, which produces output to the user; update, 
which updates the information state based on in- 
terpreted moves; and select, which selects the next 
move(s) to perform 4. The last two are DME rood- 
ules, which means that they together make up the 
3GoDiS originally accepted written input only, but it is 
currently being hooked up to a speech recogniser toaccept 
spoken input. 
4This is done by updating the part of the information state 
containing the moves to be performed. 
7 
DME in GoDiS. DME modules consist of a set of up- 
date rules and (optionally) an update algorithm gov- 
erning the order in which rules are applied. Update 
rules are rules for updating the information state. 
They consist of a rule name, a precondition list, and 
an effect list. The preconditions are conditions on 
the information state, and the effects are operations 
on the information state. If the preconditions of a 
rule are true for the information state, then the ef- 
fects of that rule can be applied to the information 
state. 
There are three resources in GoDiS: a lexicon, a 
database and a domain resource containing (among 
other things) domain-specific dialogue plans. Cur- 
rently, there are GoDiS resources for a travel agency 
domain andS-the autoroute domain. Also, for each 
of these domains there are lexicons in both English 
and Swedish. 
The question about what should be included in 
the information state is central to any theory of dia- 
logue management. The notion of information state 
we are putting forward here is basically a simplified 
version of the dialogue game board which has been 
proposed by Ginzburg. We are attempting to use as 
simple a version as possible in order to have a more 
or less practical system to experiment with. 
The main division in the information state is be- 
tween information which is private to the agent and 
that which is (assumed to be) shared between the 
dialogue participants. What we mean by shared in- 
formation here is that which has been established 
(i.e. grounded) during the conversation, akin to 
what Lewis in (Lewis, 1979) called the "conversa- 
tional scoreboard". We represent information states 
of a dialogue participant as a record of the type 
shown in figure 1. 
The private part of the information state includes 
a set of beliefs and a dialogue plan, i.e. is a list 
of dialogue actions that the agent wishes to carry 
out. The plan can be changed during the course 
of the conversation. For example, if a travel agent 
discovers that his customer wishes to get information 
about a flight he will adopt a plan to ask her where 
she wants to go, when she wants to go, what price 
class she wants and so on. The agenda, on the other 
hand, contains the short term goals or obligations 
that the agent has, i.e. what the agent is going to do 
next. For example, if the other dialogue participant 
raises a question, then the agent will normally put 
an action on the agenda to respond to the question. 
This action may or may not be in the agent's plan. 
The private part of the IS also includes "tem- 
porary" shared information that saves the previ- 
ously shared information until the latest utterance is
grounded, i.e. confirmed as having been understood 
8 
by the other dialogue participant 5. In this way it 
is easy to retract the "optimistic" assumption that 
the information was understood if it should turn out 
that the other dialogue participant does not under- 
stand or accept it. If the agent pursues a cautious 
rather than an optimistic strategy then information 
will at first only be placed in the "temporary" slot 
until it has been acknowledged by the other dialogue 
participant whereupon it can be moved to the appro- 
priate shared field. 
The (supposedly) shared part of the IS consists 
of three subparts. One is a set of propositions 
which the agent assumes for the sake of the conversa- 
tion and which are established uring the dialogue. 
The second is a stack of questions under discussion 
(QUD). These are questions that have been raised 
and are currently under discussion in the dialogue. 
The third contains information about the latest ut- 
terance (speaker, moves and integration status). 
3 Accommodat ion in GoDiS 
Dialogue participants can address questions that 
have not been explicitly raised in the dialogue. How- 
ever, it is important hat a question be available to 
the agent who is to interpret it because the utter- 
ance may be elliptical. Here is an example from a 
travel agency dialogue6: 
$J: what month do you want to go 
SP: well around 3rd 4th april / some time 
there 
SP: as cheap as possible 
The strategy we adopt for interpreting elliptical 
utterances i to think of them as short answers (in 
the sense of Ginzburg (Ginzburg, 1998)) to questions 
on QUD. A suitable question here is What kind of 
price does P want for the ticket?. This question 
is not under discussion at the point when P says 
"as cheap as possible". But it can be figured out 
since J knows that this is a relevant question. In 
fact it will be a question which J has as an action 
in his plan to raise. On our analysis it is this fact 
which enables A to interpret he ellipsis. He finds 
the matching question on his plan, accommodates 
by placing it on QUD and then continues with the 
integration of the information expressed by as cheap 
as possible as normal. Note that if such a question is 
? not available then the ellipsis cannot be interpreted 
as in the dialogue below. 
A. What  time are you coming to pick up Maria? 
B. Around 6 p.m. As cheap as possible. 
5In discussing rounding we will assume that  there is just 
one other dialogue participant. 
6This dialogue has been collected by the University of 
Lund as part of the SDS project. We quote a translation 
of the transcription done in GSteborg as part of the same 
project. 
IS : 
PRIVATE : 
SHARED : 
PLAN : STACKSET(AcT1ON)  
AGENDA : STACK(ACTION) 
PaL : SET(PRoP) 
I BEL : SET(PRoP) 
QUD : STACK(QUESTION) 
TMP : \[ SPEAKER : 
LU : \[ MOVES : 
BEL : SET(PRoP)  
QUD : STACKSET(QUESTION) 
SPEAKER : PARTICIPANT 
LU : MOVES : 
PARTICIPANT 
ASsoCSET(MOvE,BOoL) 
ASsOCSET(MOvE,BooL) \] \] 
Figure 1: The type of information state we are assuming 
This dialogue is incoherent if what is being dis- 
cussed is when the child Maria is going to be picked 
up from her friend's house (at least under standard 
dialogue plans-that we might have for such a con- 
versation). 
Question accommodation has been implemented 
in GoDiS using a single information state update 
rule accommodateQuest ion,  seen below. When 
interpreting the latest utterance by the other par- 
ticipant, the system makes the assumption that it 
was a reply move with content A. This assump- 
tion requires accommodating some question Q such 
that A is a relevant answer to Q. The check operator 
"answer-to(A, Q)" is true if A is a relevant answer to 
Q given the current information state, according to 
a (domain-dependent) definition of question-answer 
relevance. 
RULE: accommodateQuest ion  
CLASS: accommodate  
val( SHARED.LU.SPEAKER, us r  ) 
in( SHARED.LU.MOVES, answer(A) ) 
not ( lexicon :: yn_answer(A) ) 
PRE: aSSOC( SHARED.LU.MOVES, answer(A), false ) 
in( PRIVATE.PLAN, raise(Q) ) 
domain :: relevant_answer(Q, A)
del( PRIVATE.PLAN, raise(Q) ) 
EFF: push( SHARED.QUD, Q ) 
After an initial exchange for establishing contact 
the first thing that P says to the travel agent in our 
dialogue is "flights to paris". This is again an el- 
lipsis which on our analysis has to be interpreted as 
the answer to a question (two questions, actually) 
in order to be understandable and relevant. As no 
questions have been raised yet in the dialogue (apart 
from whether the participants have each other's at- 
tention) the travel agent cannot find the appropriate 
question on his plan. Furthermore, as this is the first 
indication of what the customer wants, the travel 
agent cannot have a plan with detailed questions. 
We assume that the travel agent has various plan 
types in his domain knowledge determining what 
kind of conversations heis able to have. Each plan 
is associated with a task. E.g. he is able to book 
trips by various modes of travel, he is able to han- 
dle complaints, book hotels, rental cars etc. What 
he needs to do is take the customer's utterance and 
try to match it against questions in his plan types 
in his domain knowledge. When he finds a suitable 
match he will accommodate the corresponding task, 
thereby providing a plan to ask relevant question 
for flights, e.g. when to travel?, what date? etc. 
Once he has accommodated this task and retrieved 
the plan he can proceed as in the previous example. 
That is, he can accommodate the; QUD with the rel- 
evant question and proceed with the interpretation 
of ellipsis in the normal fashion. 
This example is interesting for a couple of reasons. 
It provides us with an example of "recursive" accom- 
modation. The QUD needs to be accommodated, 
but in order to do this the dialogue task needs to be 
accommodated and the plan retrieved. The other 
interesting aspect of this is that accommodating the 
dialogue task in this way actually serves to drive the 
dialogue forward. That is, the mechanism by which 
the agent interprets this ellipsis, gives him a plan for 
a substantial part of the rest of the dialogue. This is 
a way of capturing the intuition that saying flights to 
Paris to a travel agent immediately makes a number 
of questions become relevant. 
Task accommodation a d retrieval of the relevant 
plan has been implemented in GoDiS by two infor- 
mation state update rules. 
4 Sample dialogues 
We will give some examples of CoDiS dialogues in 
the English travel agency domain. One of the fea- 
tures of the TRINDIKIT is that it provides methods 
for inspecting the information state at any stage of 
a dialogue, and also to see which rules are applied. 
Unfortunately, because of space limitations, we are 
not able to include examples of this in this paper; 
however, it is an integral part of our demo. 
The first dialogue shows the typical behaviour of 
dialogue systems for information-seeking: each ques- 
tion is answered in turn, and the user always gives 
exactly the information requested. 
$S> Welcome to the travel agency! 
$U> hello 
9 
$S> What can I do for you? 
$U> I want price information on a trip 
please 
$S> How do you want to travel? 
$U> by plane 
$S> What city do you want to go to? 
SU> gothenburg 
$S> What city do you want to go from? 
SU> london 
$S> Do you want a return ticket? 
$U> yes 
$S> What month do you want to  leave?  
$U> april 
$S> What class did you have in mind? 
$U> as cheap as possible 
$S> It~wil-i cost 50 crowns 
SU> thanks-, bye! 
$S> Thank you for your visit! 
In contrast, the second dialogue (below) shows 
how GoDiS is able to accommodate the task and in- 
tegrate information i  the order that the user gives 
it. 
$S> Welcome to the travel agency! 
$U> flights to paris 
$S> What city do you want to go from? 
$U> gothenburg, in april, as cheap as 
possible 
$S> Do you want a return ticket7 
$U> no 
$S> It will cost 50 crowns 
SU> thanks~ bye! 
$S> Thank you for your visit! 
5 Limitat ions 
GoDiS is a small-scale prototype and as such it suf- 
fers from the familiar drawbacks of many experi- 
mental systems: its lexicons and databases are very 
small, and the domain knowledge is limited. The di- 
alogue plans cover only a small (but important) part 
of the kind of dialogues that can take place in the 
travel agency and autoroute domains. We hope to 
be able to extend both the number of domains and 
the scale of each domain in the future. 
Also, the focus in the development of GoDiS has 
been on the dialogue management, i.e. the dialogue 
move engine and the information state. Less effort 
has been put into the interpretation a d generation 
modules, which currently use (a slightly extended 
version of) keyword spotting and partially canned 
text, respectively. We hope to replace these by plug- 
ging in existing interpretation a d generation tech- 
nology to GoDiS. 
For the tasks that GoDiS currently is able to han- 
dle, full-blown semantics i not needed. We use a 
very limited semantics where propositions are essen- 
tially feature-value pairs. As a consequence, GoDiS 
10 
is e.g. not able to handle dialogue with more than 
one referent; for this, the information state would 
have to be amended with a set of referents, and 
propositions would have to include referent informa- 
tion. This is an area where we hope to improve 
GoDiS in the near future. 
Speech recognition and synthesis i currently be- 
ing added to GoDiS, but at the time of writing only 
written input and output is available. 
6 Contr ibut ions 
Currently, the main contribution of GoDiS is per- 
haps to show how an extended notion of accommo- 
dation can serve to make dialogue systems easier to 
interact with, by letting the user decide how and 
in what order to present information to the system. 
Also, the fact that accommodation can be imple- 
mented simply by adding three update rules indi- 
cates that information state update rules provide a 
natural and compact way of implementing dialogue 
strategies. An important issue for future research 
is the relation of question and task accommodation 
to plan recognition approaches to dialogue (Sidner, 
1985). 
GoDiS also features a simple grounding strategy 
which is nevertheless ufficient in many cases. The 
grounding mechanism is implemented by three up- 
date rules. It is possible to switch resources in mid- 
dialogue, e.g. to change language. Also, GoDiS 
is easily reconfigurable to new information-seeking 
domains. To adapt GoDiS to a new domain, one 
needs to supply a database, a lexicon and domain 
knowledge, including a set of dialogue plans. The 
GoDiS modules or information state don't need to 
be changed in any way. 
In general, as an example of a dialogue system ira- 
plemented using the TRINDIKIT package, GoDiS 
shows how the information state approach is use- 
ful for clarifying and comparing theories of dialogue, 
and for exploring new solutions. 
References 
P. Bohlin, R. Cooper, E. Engdahl, and S. Lars- 
son. 1999. Information states and dialogue move 
engines. In J. Alexandersson, editor, IJCAI-99 
Workshop on Knowledge and Reasoning in Prac- 
tical Dialogue Systems. 
? J. Ginzburg. 1998. Clarifying utterances. In J. Hul- 
stijn and A. Niholt, editors, Proc. of the Twente 
Workshop on the Formal Semantics and Pragmat- 
ics of Dialogues, pages 11-30, Enschede. Univer- 
siteit Twente, Faculteit Informatica. 
D. K. Lewis. 1979. Scorekeeping in a language 
game. Journal of Philosophical Logic, 8:339-359. 
C. L. Sidner. 1985. Plan parsing for intended re- 
sponse recognition in discourse. Computational 
Intelligence, 1 (1) :1-10, February. 
Document  Transformations and Informat ion States 
Sta f fan  Larsson  
Dept. of linguistics 
GSteborg University 
Sweden 
sibling. ~L. se 
Ann ie  Zaenen 
Xerox Research Centre Europe 
Grenoble Laboratory 
France 
Annie. Zaenen~xrce. xerox, com 
Abst rac t  
We discuss ways to explore how 
instructional material needs to be 
structured to be presented with var- 
ious degrees of interactivity. We 
use the TRINDI 1 information state 
approach to model three different 
degrees of interactivity and present 
IMDiS, a small experimental imple- 
mentation based on the GoDiS dia- 
logue system. 
1 In t roduct ion  
Document transformations is becoming a hot 
topic in industrial research on document cre- 
ation. The reason is practical: with the new 
presentation possibilities, the advantages of 
being able to adapt he 'same' document con- 
tent to different uses - where the difference 
can lie in the support devices, audiences, lan- 
guages or modes of interaction - becomes very 
attractive. It not only becomes attractive, it
also becomes necessary: one needs to present 
material in various contexts (oral presenta- 
tious, internet portals, etc.) and it is very 
costly to develop presentations from scratch 
for these various contexts. 
This situation raises an old question and 
opens a new area of research: can one sep- 
arate content from presentation? The philo- 
sophical answer might be 'no', but in practice 
one doesn't need an absolute answer. As this 
area of research arises more out of practical 
necessity than pure intellectual curiosity, the 
1TRINDI (Task Oriented Instruc- 
tional Dialogue), EC Project LE4-8314, 
www. ling. gu. se/research/proj ec~s/trindi/ 
engineering is preceding the science and it will 
take some time before it rest on explicit solid 
foundations. 
Here we look only at one small aspect of the 
problem: how can we model small changes 
in presentation that are due to various de- 
grees of interactivity between participants in 
instructional exchanges. We start from a tra- 
ditional manual and make some assumptions 
about minimal interactivity which are mod- 
eled through dialogue moves. We conclude 
that in this way we can make the presenta- 
tion of the material more flexible. An impor- 
tant limit on the flexibility is, however, the 
detail with which the discourse structure of 
the manual encodes the task plan underlying 
the activity. 
2 Degrees  o f  In teract iv i ty  and  the  
d i f ference between mono logue  
and  d ia logue  
We take here the position that the main differ- 
ence between dialogue and monologue is that 
the former implies interactivity. With interac- 
tivity we mean here that the participants can 
influence ach other's moves. With respect 
to the area that interests us here, giving in- 
structions to repair devices, a traditional writ- 
ten manual influences the user but not vice 
versa (except hrough notes to the author). 
The user can, however, influence the order in 
which she accesses the material: it is easy to 
stop, to go back or to consult an other section 
(traditional printed material might be argued 
to be better in that respect than presentation 
on a screen, we ignore that difference here). 
We can consider this as a limit case of inter- 
activity. 
112 
Note that interactivity does not necessarily 
imply shared initiative. The literature makes 
a distinction between task and dialogue ini- 
tiative (e.g. (Chu-Carroll and Brown, 1998)) 
but one can have dialogue with both types of 
initiative staying with one side. In the cases 
we discuss below the task initiative stays com- 
pletely with the manual and the dialogue ini- 
tiative only switches to the instructee in the 
case where she can indicate that information 
about some subprocedures can be skipped. 
There is another dimension that often inter- 
venes in discussions about the difference be- 
tween dialogue and written discourse: the for- 
mer is spoken, the latter is written. Given the 
way things are in a natural setting, the writ- 
ten medium tends not to allow interactivity, 
whereas the spoken medium is used mainly in 
interactive settings. Technical changes, how- 
ever, allow us to separate the written/spoken 
opposition from that between interactive and 
non, or minimally, interactive discourse. In- 
structional material can be presented in the 
aural mode without becoming more interac- 
tive e.g. when a recording is played. This can 
be considered as a plus for instructional ma- 
terial because it allows the instructee to use 
her hands and eyes for the task itself but it is 
not an unqualified advantage given that read- 
ing gives much more flexibility than listening 
to a tape. To cash in on the advantages ofthe 
aural presentation, we need to recapture the 
flexibility of access that the written medium 
allows. 
3 Ins t ruc t ions  and  In teract iv i ty  
It is obvious that instructional situations 
profit from an interactive setting. Instruc- 
tional situations are typically situations in 
which some participants (the instructors) 
know a lot that the other participants (the 
instructees) need to know to achieve the com- 
mon goals. In these kinds of situations it is 
important hat all the required and, prefer- 
ably only the required, knowledge gets trans- 
ferred at the moment he instructees need it. 
To achieve this, it is not enough that the 
instructor have all the necessary knowledge, 
she needs also to know which state the in- 
structee is in and how that state changes to 
adapt the transfer of knowledge, hence the 
instructee needs to be able to inform the in- 
structor about his state and influence in this 
way the course of the interaction. 
Currently we have manuals, whose con- 
tent can be presented aurally or in a writ- 
ten form but where both the content and the 
presentation are uniquely determined a pri- 
ori (modulo, the speed and order of read- 
ing mentioned above). Or we have interac- 
tions that can be at a distance but where 
a human instructor needs to be available at 
the time of the action. Making humans with 
the required competence available is expen- 
sive and one would want to achieve some in- 
teractivity without his. But computers tend 
to be frustrating participants in interactive 
settings when one compares them to human 
beings and the study of dialogue concentrates 
mainly on making them as human as possible. 
When one considers the possibility of trans- 
ferring the interactivity from humans to ma- 
chines, there are, however, many intermedi- 
ate possibilities between o interactivity and 
full blown interactivity in free-wheeling di- 
alogue where the participants can ask each 
other questions about anything and nothing 
(for a more thorough discussion about dia- 
logues between humans and computers ee 
(Clark, 1999)). In this paper we consider how 
minimal interactions can be modeled on the 
basis of information which is available in tra- 
ditional instructional manuals. 
In looking at the problem this way one 
has to keep in mind that instructional man- 
uals, although not interactive, are coopera- 
tive constructs: they assume that they par- 
ticipate with the user in a rational cooper- 
ative task and they are built on an implicit 
reader model, specifically they make assump- 
tions about what the user knows and what 
she doesn't know and the granularity of the 
task descriptions that they have to provide. 
They obey in their own way Grice's Maxim 
of Quantity but they need to leave open a 
range of possibilities so they need to provide 
more detail than is necessary in all circum- 
stances. In what follows we can only consider 
113 
cases of over-informedness a  the information 
needed to remedy under-informedness is not 
available. 
4 The  TR INDI  mode l  
The TRINDI project has developed both a 
framework and a toolkit to model various 
types of interactions in terms of information 
state updates. The framework, whose main 
ingredients are information states, dialogue 
moves and updates, is described in (Traum 
et al, 1999). We use the term information 
state to mean, roughly, the information stored 
internally by an agent, in this case a dia- 
logue system. A dialogue move engine up- 
dates the information state on the basis of 
observed ialogue moves and selects appropri- 
ate moves to be performed. In:formation state 
updates are formalised as in~brmation state 
update rules. The importance of the frame- 
work is that new interactive :hypotheses can 
be modeled with minor extensions. The infor- 
mation state approach is implemented in the 
TRINDIKIT (Larsson et al, 2000); (Larsson 
and Traum, To appear), a toolkit for experi- 
menting with the implementation f informa- 
tion states and dialogue move engines and for 
building dialogue systems. It is used in the 
experimental implementation described here. 
Various instantiations of the framework 
articulate further what information states, 
moves, and update rules contain. In this pa- 
per we use one formal representation f in- 
formation states that has been developed in 
the TRINDI, SDS 2 and INDI 3 projects, and 
implemented in the GoDiS dialogue system 
(Bohlin et al, 1999). The central parts of the 
information state in GoDiS are dialogue plans 
and Questions Under Discussion (QUD), a 
notion borrowed from Ginzburg (Ginzburg, 
1998). 
2SDS (Swedish Dial%me Systems), 
NUTEK/HSFR Language Technology Project 
F1472/1997, http://~rm~, ida.liu, se/ nlplab/sds/ 
3INDI (Information Exchange in Dialogue), Riks- 
bankens Jubileumsfond 1997-0134. 
5 Mode l ing  var ious  degrees  of  
in teract iv i ty  in  TR INDI  
We envision the following cases: 
? 1. Traditional manual: no overt inter- 
action, we will consider this as the limit 
case  
? 2. Manual can ask yes/no questions and 
understand two types of user responses: 
- yes/no 
- done/don't understand 
- how? 
? 3. User can indicate whether she already 
knows certain (sub)procedures 
5.1 GoDiS / IMDiS  in format ion  states 
To model the types of interactions above, we 
started from the GoDiS system which is de- 
signed to deal with information-seeking dia- 
logue. The IMDiS information state type is 
shown in Figure 1. 
PRIVATE 
SHARED 
PLAN : StackSet (Action) 
: AGENDA : Stack(Action) 
TMP : (sa,ID.e as SHARED) 
| BEL : Set(Prop) 
\[ QUD : StackSet(Question) 
: | ACTIONS : Stack(Action) 
L LU : Ut te rance  
Figure i: IMDiS information state type 
The main division in the information state 
is between information which is private to the 
agent and that which is shared between the 
dialogue participants. The private part of the 
information state contains a PLAN field hold- 
ing a dialogue plan, i.e. is a list of dialogue 
actions that the agent wishes to carry out. 
The plan can be changed uring the course 
of the conversation. The AGENDA field, on 
the other hand, contains the short term goals 
or obligations that the agent has, i.e. what 
the agent is going to do next. We have in- 
cluded a field TMP that mirrors the shared 
fields. This field keeps track of shared infor- 
mation that has not yet been grounded, i.e. 
confirmed as having been understood by the 
114 
? i 
other dialogue participant. The SHARED field 
is divided into four subfields. One subfield is 
a set of proposit ions which the agent assumes 
for the sake of the conversation. The second 
subfield is for a stack of questions under dis- 
cussion (QUD). These are questions that have 
been raised and are currently under discus- 
sion in the dialogue. The ACTIONS field is a 
stack of (domain) actions which the user has 
been instructed to perform but has not yet 
performed.The LU field contains information 
about the latest utterance. 
To adapt GoDiS to instructional dialogue, 
we added a subfield of SHARED.ACTIONS to 
(the shared part  of) the information state. 
The value of this field is a stack of actions 
which the system has instructed the user to 
perform, but whose performance has not yet 
been confirmed by the user. 
In building the experimental IMDiS, we 
have made several simplifications. We have 
ignored all the natural  language generation 
problems and all the problems related to mak- 
ing text or dialogue natural, e.g. problems re- 
lated to the use of pronouns and other refer- 
ential expressions. To handle these we would 
not only have to discuss basic interactivity 
but also the medium in which the interaction 
takes place: speech or written text. 
The monologue mode (case 1) uses only 2 
moves ( Ins t ruct ,  and In fo rm) .  Since there 
is no user to confirm that actions have been 
performed, all actions are automatical ly con- 
firmed using the update rule autoConf i rm.  
RULE: autoConf i rm 
CLASS: in tegrate  
PRE: { fst( SHARED.ACTIONS, A ) 
pop( SHARED.ACTIONS ) 
EFF: add( SHARED.BEL, done(A) ) 
The dialogue version (cases 2 and 3) 
uses 9 move types, basically the 7 used in 
GoDiS (Ask,  Answer ,  In fo rm,  Repeat ,  
RequestRepeat ,  Greet ,  Qu i t )p lus  in- 
structions ( Ins t ruc t )  and confirmations 
(Conf i rm) .  Confirmations are integrated by 
assuming that the current topmost action 
in SHARED.ACTIONS has been performed, as 
seen in the update rule below. 
RULE: in tegrateUsrConf i rm 
CLASS: in tegrate  
val( SHARED.LU.SPEAKER, nsr ) 
PRE: assoc( SHARED.LU.MOVES, confirm, false ) 
fst( SHARED.ACTIONS, A ) 
set_assoc( SHARED.LU.MOVES, confirm, true ) 
EFF: pop( SHARED.ACTIONS ) 
add( SHARED.BEL, clozte( A ) ) 
This rule says that if the user performed a
Conf i rm move, which has not yet been in- 
tegrated, and A is the "most salient" action, 
then integrate the move by putt ing the propo- 
s i t ion done (A) in the shared beliefs, and tak- 
ing A off the action stack. 
Elliptical "how"-questions from the user 
are interpreted as applying to the currently 
topmost action in the SHARED.ACTIONS stack. 
5.2 Domain  task ,  manua ls  and  
d ia logues  
Let's now see how a monologue and a dialogue 
version of the same task are related. Below we 
have an example from the user manual for the 
HomeCentre, a Xerox MFD. 
? Reinstalling the print head 
? Caution: Make sure that the green carriage lock 
lever is STILL moved all the way forward before 
you reinstall the print head. 
? 1. Line up the hole in the print head with the 
green post on the printer carriage. 
? Lower the print head down gently into position. 
? 2. Gently push the green cartridge lock lever up 
until it snaps into place. 
? This secures the print head. 
? 3. Close the top cover and reattach the scanner. 
? 4. Press and release the yellow LED button. 
? The printer will prepare the cartridge for print- 
ing. 
? Note: If the carriage does not move from the cen- 
ter position after you press the cartridge change 
button, remove and reinstall the print head. 
From this text, one can (re)construct a task 
plan for reinstalling the print head. Such a 
plan may be represented as in figure 2. Note 
115 
NAME rein.stall(prim head) 
PRE movcd_forward(carriage2od0 
DEC 
EFF minstalled(prinL head) 
Figure 2: Task plan 
- -1  action 
complcx action / plan 
final state 
that this is a conditional plan, i.e. it contains 
branching conditions. 
From this task plan, IMDiS generates two 
plans: a monologue plan and a dialogue plan. 
This is done using the "translation schema" 
in Figure 3. 
The difference between the text plan and 
the dialogue plan is in the way that condi- 
tionals in the task plan are interpreted. In 
the monologue plan, they correspond to sim- 
ply informing the user of the conditional. In 
dialogue mode, however, the system raises the 
question whether the condition holds. When 
the system finds out if the condition holds, it 
will instruct he user to execute the appropri- 
ate guarded action. 
Here we can clearly see how dialogue differs 
from monologue as viewed by Carlson or Van 
Kuppevelt ((Carlson, 1983), (~an Kuppevelt, 
1995)). Under these views the writer antici- 
pates the questions the user might have asked 
but given the user is not present he writer 
has to make up for the lack of interactivity. 
The questions that can be reconstructed (or 
accommodated) are different in that case. For 
instance in the example given here, the ques- 
tion could something like "What should the 
user/I make sure of?". These questions are 
valuable to help figure out the discourse struc- 
ture of a monologue. They can also be valu- 
able tools to illustrate the differences between 
dialogue and monologue but they do not give 
much insight in the effects of various degrees 
of interactivity. 
Conditionals are treated as follows by the 
system in dialogue mode: When the system 
has found out what the user's task is, it will 
load the appropriate dialogue plan into the 
PRIVATE.PLAN field of the information state. 
It will then execute the actions in the appro- 
priate order by moving them to the agenda 
and generating appropriate utterances. When 
a conditional statement is topmost on the 
plan, IMDiS will check whether it has been es- 
tablished that the condition holds (by check- 
ing the SHARED.BEL field). Since the system 
has previously asked the user and the user has 
answered, either the condition or its negation 
will be in the set of established propositions. 
If the condition or its negation holds, the con- 
ditional will be popped off the plan and re- 
placed by the first or second guarded action 
(respectively). 
116 
DOMAIN 
l~recondition P 
action A 
if_then (C,A) 
effect E 
MONOLOGUE 
Instruct (check (P)) 
Instruct(A) 
Inform(if_then (C, A) ) 
Inform(E) 
DIALOGUE 
findout(P); 
if_then ( not (P),  
Instruct (achieve (P)) ) 
Instruct(A) 
findout (C) ; 
if-then(C, 
Instruct(A)) 
Inform(E) 
Figure 3: Plan conversion table 
5.3 Monologue and Dialogue 
Behav iour  
In the monologue mode in IMDiS, the 
control module does not call the input and 
interpretation modules. The text is output 
"move by move" as a sequence of utterances 
from the system. 
S: Reinstalling the print head. 
S: Make sure that the green carriage lock 
lever is STILL moved all the way forward 
before you install the print head. 
S: Line up the hole in the print head with 
the green post on the printer carriage 
Compared to the monologue mode, even a 
very restricted ialogue mode offers several 
advantages: 
User a t tent ion  and control The user 
can direct her attention to the machine and 
does not have to look at the manual. As 
we noted in when one goes from written to 
aural presentation, one gains the advantage 
that the user has free hands and eyes but if 
nothing more is done this advantage has to 
be weighted against the disadvantage that 
the user looses all control over the order 
and the speed with which the information 
is presented. We can avoid these draw- 
backs by allowing some limited grounding 
behaviour. Very simple interactions like 
'done' (Confirm) or 'don't understand' 
(RequestRepeat)  give back to the user a 
limited control over the speed and the order 
of the presentation (at least up to allowing 
repetition): the user decides when to move 
on to the next action, by confirming that the 
previous action is done, and by 'don't under- 
stand' she can indicate that she would want 
a repetition of what was said immediately 
before. Here we see how to take advantage 
of the advantages of a different mode of 
presentation (written versus aural) we also 
have to change the type of interactivity. 
S: Has the carriage moved from the center 
position? 
U: I didn't understand 
S: Has the carriage moved from the center 
position? 
Avoid irrelevant informat ion When the 
action to be taken depends on a condition, 
the system does not give irrelevant informa- 
tion. 
S: Has the carriage moved from the center 
position? 
U: yes 
S: The print head is now installed 
Because there is no feedback from the user, 
a manual has always to give all the possibili- 
ties regardless of which one actually pertains. 
The possibility to ask yes/no questions allows 
us to do away with this redundancy. 
5.4 More complex task plans 
In the example above we illustrated how a 
simple task plan can give rise to a dialogue 
and a monologue rendering. We can get some 
added flexibility by giving more structure to 
the task plan. For instance in the example 
above, one can argue that the reiustallation 
proper of the print head is described in point 
1 to 2 and that 3 and 4 describe termination 
117 
NAME reinstall(print head) 
PRE movcxLforward(cartiage lock) 
DEC 
\[ secu. ?(prkR._h(~d~ \[ 
cl t ? cv-- t o e op= 
I press_and_release(yellow_button 1 
yes ? no 
V'Y--~-(moved_from_cent e_____r(prin~head)__/~ 
! I I 
/ " >~? ' J 
EFF reinstallcxl(prim head) 
NAME secure(print:head) 
PRE 
DEC 
I line_up(hole, post) 
I push(leveO I 
6 
EFF secured(printhead) 
I \[ action 
~ \ ]  complex action / plan 
final state 
Figure 4: Revised Task Plan 
conditions. To reflect this we can revise the 
task plan as follows: 
With this structure the user can control 
the level of detail of the instructions given. 
If the user does not know how to perform 
a substep, she can ask the system for more 
detailed instructions. 
U: done, aud now? 
S: Close the top cover 
If the user manages to complete the whole 
action sequence without instructions, he can 
tell the system this and the system proceeds 
to the next relevant action. 
S: put the print head in place 
U: how? 
S: Line up the hole in the print head with 
the green post on the printer carriage 
U: r ight ,  ok 
S: Lower the print head down gently into 
position 
U: ok 
S: Gently push the green car t r idge  lock lever  
up unt i l  i t  snaps into p lace 
U: ok 
S: The pr in t  head is  now secure ly  in p lace 
On the other hand, if the user already 
knows how to perform a substep, the system 
moves on to the next step. 
S: put the print head in place 
S: put the print head in place 
U: how? 
S: Line up the hole in the print head with 
the green post on the printer carriage 
U: r ight ,  done 
S: Lower the pr in t  head down gent ly  in to  
position 
U: done, I remember now (pause) the print 
head is put in place 
S: Ok. Close the top cover 
Here, however, we see the importance of 
the task structure. It is only if we have in- 
formation that gives the structure of the task 
with subtasks that we can model this. Very 
often instructional manuals will give this sub- 
structure, e.g. in the form of subdivisions of 
instructions, but they tend not to be corn- 
118 
pletely consistent in this. It is only when this 
information is given in a consistent way that 
we can exploit it in a transformation from a 
written manual presentation to a more inter- 
active presentation. 
6 D iscuss ion  and  Research  I ssues  
In this experiment we have looked at a few 
differences that occur in the rendering of the 
same information under different conditions 
of interactivity. Our little experiment brought 
out several differences in the 'rendering' of the 
same task plan as a written text and as a min- 
imally interactive dialogue. 
? Conditionals and preconditions are han- 
dled differently if limited confirmations 
are possible. 
? The flexibility of access that written text 
allows needs to be modeled more explic- 
itly in case of aural presentation. This 
can be done minimally by allowing the 
machine to interpret 'done' or 'don't un- 
derstand' as moves that lead to the pre- 
sentation of the next instruction or to a 
repetition of the latest instruction. 
Moreover the granularity with which the 
task plan is represented corresponds to the 
granularity of the control the user has over 
the presentations of the instructions. In this 
example we started from an existing manual 
text. Starting from a written manual helped 
us understand the importance of the informa- 
tion about the task structure. This comes of 
course not as a surprise: when the presenta- 
tion mode is fixed as non-interactive, the the 
discourse structure can be very 'fiat': things 
need to be done in a certain order whether 
they are parts of subtasks or not is not rel- 
evant. It can be argued that giving more 
structure will help a user understand better 
what the instructions achieve but it will not 
influence the execution directly. Material that 
helps the user understand why she is doing 
something is typically given in introductory 
sections and not in the procedures themselves 
in this type of manual. But to make doc- 
ument transformations possible in the sense 
described in the beginning, it is important o 
clearly separate task plans and assumptions 
about interactions, i.e. about how the infor- 
mation states get updated. 4 
Once the task plan is distinguished from the 
dialogue plan, assumptions about the type of 
interactions between participants can change 
the dialogue plan even when the task plan 
remains constant. 
In practice a completely automatic trans- 
formation of a written manual into even lim- 
ited dialogue is most likely not possible, al- 
though one can isolate several inguistic flags 
for some of the aspects we have been dis- 
cussing (e.g. expressions like "make sure 
that..." flag preconditions). A more realistic 
approach would be to create a blueprint doc- 
ument that is marked up to allow the deriva- 
tion of several different types of discourse 
from the beginning on. Such an enterprise 
would need tools such as the TRINDIKIT to 
model the various cases 5 
So far, we have only explored one extreme 
of the monologue-dialogue opposition where 
the interactivity stays very low. Obvious ex- 
tensions are to allow the user to ask informa- 
tion that goes beyond the current procedure, 
e.g. 'where can i find the piece you mention' 
or 'how long does this take: i have only 1/2 
hour here'. Further inquiry into the possible 
interactions will help us to define which infor- 
mation is needed and how it needs to be struc- 
tured to fulfill these various needs. And of 
course we will never reach a system in which 
every user need can be anticipated but then 
even human beings are not that type of sys- 
tem. 
4See (Grosz and Sidner, 1986) for a discussion of 
the importance oftask plans in more explanatory di- 
alogue. 
5It would also need tools that make it easy to model 
the relation between the linguistic expressions used in 
the various renderings of the base document. One can 
see this task as akin to that of multilingual genera- 
tion or even simple document rendering. Formal ap- 
proaches used for those tasks could be adapted to such 
an enterprise. XML supplemented with stylesheets 
and schemata could be another possibility. 
119 
References  
P. Bohlin, R. Cooper, E. Engdahl, and S. Larsson. 
1999. Information states and dialogue move 
engines. In J. Alexandersson, editor, IJCAI- 
99 Workshop on Knowled9e and Reasonin 9 in 
Practical Dialogue Systems. 
L. Carlson. 1983. Dialogue Games. D. Reidel, 
Dordrecht. 
Jennifer Chu-Carroll and Michael K. Brown. 
1998. An evidential model for tracking initia- 
tive in collaborative dialogue interactions. User 
Modeling and User-Adapted Interaction, special 
issue on Computational Models of Mized Initia- 
tive Interaction, 8(3+4):215-253. 
H. Clark. 1999. How do real people communicate 
with virtual partners? Proceedings of AAAI- 
99 Fall Symposium, Pshychological Models of 
Communication i  Collaborative Systems. 
J. Ginzburg. 1998. Clarifying utterances. In 
J. Hulstijn and A. Niholt, editors, Proc. of 
the Twente Workshop on the .Formal Seman- 
tics and Pragmatics of Dialogues, pages 11-30, 
Enschede. Universiteit Twente, Faculteit Infor- 
matica. 
B. J. Grosz and C. L. Sidner. 1986. Atten- 
tion, intention, and the structure of discourse. 
12(3):175-204. 
Staffan Larsson and David Traum. To appear. 
Information state and dialogue management in 
the trindi dialogue move engine toolkit. NLE 
Special Issue on Best Practice in Spoken Lan- 
guage Dialogue Systems Engineering. 
Staffan Larsson, Alexander Berman, Johan Bos, 
Leif GrSnqvist, Peter Ljunglbf, and David 
Traum. 2000. Trindikit 2.0 manual. Techni- 
cal Report Deliverable D5.3 - Manual, Trindi. 
D. Traum, J. Bos, R. Cooper, S. Larsson, I.Lewin, 
C. Matheson, and M. Poesio. 1999. A model of 
dialogue moves and irfformation state revision. 
deliverable D2.1, TRINDI. 
Jan van Kuppevelt. 1995. Discourse structure, 
topicality and questioning. Journal of Linguis- 
tics, 31:109-147. 
120 
Issues Under Negotiation
Staffan Larsson
Dept. of linguistics, Box 200
Go?teborg University
SE 405 30 Go?teborg, Sweden
sl@ling.gu.se
Abstract
In this paper, we give an account of
a simple kind of collaborative negotia-
tive dialogue. We also sketch a formal-
ization of this account and discuss its
implementation in a dialogue system.
1 Introduction
In this paper, we give an issue-based account
of a simple kind of collaborative negotiative dia-
logue1. We start from a previous formal account
of negotiative dialogue (Sidner, 1994a) and ar-
gue for a slightly different idea of what negotia-
tive dialogue is. We want to make a distinction
between the process of accepting an utterance
and its content, which applies to all utterances,
and a concept of negotiation defined, roughly,
as a discussion of several alternative solutions to
some problem. This latter account is formulated
in terms of Issues Under Negotiation (IUN), rep-
resenting the question or problem to be resolved,
and a set of alternative answers, representing
the proposed solutions. We argue that this the-
ory improves on certain aspects of Sidner?s ac-
count while also fitting into a question-based di-
alogue management framework which has previ-
ously been implemented in the GoDiS dialogue
system (Bohlin et al, 1999).
1Work on this paper was supported by SIRIDUS
(Specification, Interaction and Reconfiguration in Dia-
logue Understanding Systems), EC Project IST-1999-
10516, and D?Homme (Dialogues in the Home Machine
Environment), EC Project IST-2000-26280, and STINT
(The Swedish Foundation for International Cooperation
in Research and Higher Education). An extended pre-
sentation of this work appears in (Larsson, 2002).
First, we will give a brief review of Sidner?s
theory and discuss its merits and drawbacks2.
We then provide an alternative account based
on the concept of Issues Under Negotiation. We
explain how IUN can be added to GoDiS, and
give an information state analysis of a simple ne-
gotiative dialogue. Finally, we draw some con-
clusions and point to possible future research ar-
eas.
2 Sidner?s theory of negotiative
dialogue
As the title says, Sidner?s theory is formulated
as ?an artificial discourse language for collabo-
rative negotiation?. This language consists of a
set of messages (or message types) with propo-
sitional contents (?beliefs?). The effects of an
agent transmitting these messages to another
agent is formulated in terms of the ?state of
communication? after the message has been re-
ceived. The state of communication includes in-
dividual beliefs and intentions, mutual beliefs,
and two stacks for Open Beliefs and Rejected
Beliefs. Some of the central messages are
? ProposeForAccept (PFA agt1 belief
agt2): agt1 expresses belief to agt2.
? Reject (RJ agt1 belief agt2): agt1
does not believe belief, which has been
offered as a proposal
2A more in-depth description of Sidner?s account and
its relation to the GoDiS model, including a reformula-
tion of Sidner?s artificial negotiation language in terms
of GoDiS information state updates, can be found in
(Cooper et al, 2001).
     Philadelphia, July 2002, pp. 103-112.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
? AcceptProposal (AP agt1 belief agt2):
agt1 and agt2 now hold belief as a mu-
tual belief
? Counter (CO agt1 belief1 agt2
belief2): Without rejecting belief1,
agt1 offers belief2 to agt2
In addition, there are three kinds of
acknowledgement messages, the most im-
portant being AcknowledgeReceipt (AR agt1
belief agt2), which may occur after a
ProposeForAccept message and results in
belief being pushed on the stack for Open Be-
liefs. Acknowledgement indicates that a previ-
ous message from agt2 about belief has been
heard; the agents will not hold belief as a mu-
tual belief until an AcceptProposal message has
been sent.
While we will not give a detailed analysis of
the effects of each of these acts, some observa-
tions are important for the purposes of this pa-
per. Specifically, a counter-proposal (CO agt1
belief1 agt2 belief2) is analyzed as a com-
posite message consisting of two PFA messages
with propositional contents. The first proposed
proposition is belief2 (the ?new? proposal),
and the second is (Supports (Not belief1)
belief2), i.e. that belief2 supports the nega-
tion of belief1 (the ?old? proposal). Exactly
what is meant by ?supports? here is left unspec-
ified, but perhaps logical entailment is at least
a simple kind of support.
? (PFA agt1 belief2 agt2)
? (PFA agt1 (Supports (Not belief1)
belief2) agt2)
Sidner?s analysis of proposals is only con-
cerned with propositional contents. A Request
for action is modelled as a proposal whose con-
tent is of the form (Should-Do Agt Action).
A question is a proposal for the action to pro-
vide certain information. This brings us to our
first problem with Sidner?s account.
3 Problem 1: Negotiation vs.
utterance acceptance
In Sidner?s theory, all dialogue is negotiative
in the sense that all utterances (except accep-
tances, rejections, and acknowledgements) are
seen as proposals. This is correct if we consider
negotiation as possibly concerning meta-aspects
of the dialogue. Since any utterance (content)
can be rejected, all utterances can indeed be seen
as proposals.
(Clark, 1996) provides a ?ladder? with four
levels of comprehension involved in grounding
of natural language utterances in dialogue.
1. A attends to B?s utterance
2. A perceives B?s utterance
3. A understands B?s utterance
4. A accepts or rejects B?s utterance
So in one sense of ?negotiative?, all dialogue
is negotiative since assertions (and questions, in-
structions etc.) can be rejected or accepted. But
some dialogues are negotiative in another sense,
in that they contain explicitly discussions about
different solutions to a problem. Negotiation, on
this view, is distinct from Clark?s level 4.
There is thus a stronger sense of negotiation
which is not present in all dialogue. A mini-
mum requirement on negotiation in this stronger
sense could be that several alternative solutions
(answers) to a problem (question or issue) can
be discussed and compared before a solution is
finally settled on. Sidner is aware of this as-
pect of negotiation, and notes that ?maintain-
ing more than one open proposal is a common
feature of human discourses and negotiations.?
What we want to do is to find a way of cap-
turing this property independently of grounding
and of other aspects of negotiation, and use it
as a minimal requirement on any dialogue that
is to be regarded as negotiative.
On our view, utterances realizing proposal-
moves are moves on the same level as other dia-
logue moves: greetings, questions, answers etc.,
and can thus be accepted or rejected on this
level. Accepting a proposal-move on the ground-
ing level merely means accepting the content of
the move as a proposal, i.e. as a potential answer
to a question. This is different from accepting
the proposed alternative as the actual solution
to a problem (answer to a question).
To give a concrete example of these different
concepts of negotiativity, we can compare the
dialogues in Figures 1 and 2. The type negotia-
tion in 1 concerns acceptance-level grounding of
the utterance and its content. By contrast, the
type of negotiation in 2 concerns domain-level is-
sues rather than some aspect of grounding. We
won?t have much to say about grounding-related
negotiation in this paper, but see (Lewin et al,
2000) for an account of negotiation related to
utterance grounding.
4 Problem 2: Alternatives and
counterproposals
When analyzing a travel agency dialogue (Sid-
ner, 1994b), the travel agent?s successive pro-
posals of flights are seen as counterproposals to
his own previous proposals, each modelled as
a proposition. The difference between propos-
als and counterproposals is that the latter not
only make a new proposal but also proposes the
proposition that the new proposal conflicts with
the previous proposal (by supporting the nega-
tion of the previous proposal). This can be seen
as an attempt by Sidner to establish the con-
nection between the two proposals as somehow
concerning the same issue.
This analysis is problematic in that it excludes
cases where alternatives are not mutually exclu-
sive, which is natural when e.g. booking a flight
(since the user presumably only want one flight)
but not e.g. when buying a CD (since the user
may want to buy more than one). Also, it seems
odd to make counterproposals to your own pre-
vious proposals, especially since making a pro-
posal commits you to intending the addressee to
accept that proposal rather than your previous
ones. In many cases (including travel agencies)
it seems that the agent may often be quite in-
different to which flight the user selects. Travel
agents may often make several proposals in one
utterance, e.g. ?There is one flight at 7:45 and
one at 12:00?, in which case it does not make
sense to see ?one at 12:00? as a counterproposal
as Sidner defines them.
Of course, one would not want to use the
term ?counterproposal? in these cases; what we
need is some way of proposing alternatives with-
out seeing them as counterproposals. The basic
problem seems to be that when several propos-
als are ?on the table? at once, one needs some
way of representing the fact that they are not
independent of each other. Sidner does this
by adding propositions of the form (Supports
(Not belief1) belief2) to show that belief1
and belief2 are not independent; however, this
proposition not only claims that the proposi-
tions are somehow dependent, but also that they
are (logically or rhetorically) mutually exclusive.
In our view, this indicates a need for a theory
of negotiation which makes it possible to repre-
sent several alternatives as somehow concerning
the same issue, independently of rhetorical or
logical relations between the alternatives. Ne-
gotiation, in our view, should not in general be
seen in terms of proposals and counterproposals,
but in terms of proposing and choosing between
several alternatives.
5 Negotiation as discussing
alternatives
In this section, we will attempt to provide a
more detailed description of negotiative dia-
logue. Clearly, negotiation is a type of problem-
solving (Di Eugenio et al, 1998). We define
negotiative dialogue more specifically to be di-
alogue where DPs3 discuss several alternative
solutions to a problem (issue) before choosing
one of them. In line with our issue-based ap-
proach to dialogue management, we propose to
model negotiable problems (issues) semantically
as questions and alternative solutions as alter-
native answers to a question.
We also propose to keep track of these issues
under negotiation and the answers being consid-
ered as potential solutions to each issue in the
form of a stack (or ordered set) of pairs of issues
3DPs = Dialogue Participants
A> Today is January 6th.
propose proposition
B(alt. 1)> Uhuh
accept proposition
B(alt. 2)> No, it?s not!
reject proposition
Figure 1: Negotiating utterance (content) acceptance
S> where do you want to go?
ask question
U> flights to paris on june 13 please
answer question
S> there is one flight at 07:45 and one at 12:00
propose alternatives, give information about alternatives
U> what airline is the 12:00 one
ask question
S> the 12:00 flight is an SAS flight
answer question
U> I?ll take the 7:45 flight please
accept alternative, answer question ?which flight??
Figure 2: Negotiating alternative solutions to an issue
and sets of answers. We label this stack Issues
Under Negotiation, or IUN.
5.1 Degrees of negotiativity
Starting from this definition, we can distinguish
between fully negotiative dialogue and semi-
negotiative dialogue. In non-negotiative dia-
logue, only one alternative can be discussed. In
semi-negotiative dialogue, a new alternative can
be introduced by revising parameters of the pre-
vious alternative; however, previous alternatives
are not retained. Finally, in negotiative dia-
logue: several alternatives can be introduced,
and old alternatives are retained and can be re-
turned to.
Semi-negotiative information-oriented dia-
logue does not require keeping track of several
alternatives. All that is required is that in-
formation is revisable, and that new database
queries can be formed from old ones by replac-
ing some piece of information. This property
is implemented in a limited way for example in
the Swedish railway information system (a vari-
ant of the Philips system (Aust et al, 1994)),
which after providing information about a trip
will ask the user ?Do you want an earlier or
later train??. This allows the user to modify the
previous query (although in a very limited way)
and get information about further alternatives.
However, it is not possible to compare the alter-
natives by asking questions about them; indeed,
there is no sign that information about previous
alternatives is retained in the system.
5.2 Factors influencing negotiation
There are a number of aspects of the dia-
logue situation which affect the complexity of
negotiative dialogues, and allows further sub-
classification of them. This sub-classification al-
lows us to pick out a subspecies of negotiative
dialogue to implement.
On our definition, negotiation does not re-
quire conflicting goals or interests, and for this
reason it may not correspond perfectly to the
everyday use of the word ?negotiation?. How-
ever, we feel it is useful to keep collaborativity
(i.e. lack of conflicting goals) as a separate di-
mension from negotiation. Also, it is common
practice in other fields dealing with negotiation
(e.g. game theory, economy) to include collabo-
rative negotiation (cf. (Lewin et al, 2000)).
A second factor influencing negotiation is the
distribution of information between DPs. In
some activities, information may be symmet-
rically distributed, i.e. DPs have roughly the
same kind of information, and also the same
kind of information needs (questions they want
answered). This is the case e.g. in the Co-
conut (Di Eugenio et al, 1998) dialogues where
DPs each have an amount of money and they
have to decide jointly on a number of furniture
items to purchase. In other activities, such as a
travel agency, the information and information
needs of the DPs is asymmetrically distributed.
The customer has access to information about
her destination, approximate time of travel etc.,
and wants to know e.g. exact flight times and
prices. The travel agent has access to a database
of flight information, but needs to know when
the customer wants to leave, where she wants to
travel, etc.
A third variable is whether DPs must commit
jointly (as in e.g. the Coconut dialogues) or one
DP can make the commitment by herself (as e.g.
in flight booking). In the latter case, the accep-
tance of one of the alternatives can be modelled
as an answer to an issue on IUN by the DP re-
sponsible for the commitment, without the need
for an explicit agreement from the other DP. In
the former case, a similar analysis is possible,
but here it is more likely that an explicit ex-
pression of agreement is needed from both DPs.
This variable may perhaps be referred to as ?dis-
tribution of decision rights?.
Travel agency dialogue, and dialogue in other
domains with clear differences in information
and decision-right distribution between roles,
has the advantage of making dialogue move in-
terpretation easier since the presence of a certain
bits of information in an utterance together with
knowledge about the role of the speaker and the
role-related information distribution often can
be used to determine dialogue move type. For
example, an utterance containing the phrase ?to
Paris? spoken by a customer in a travel agency
is likely to be intended to provide information
about the customer?s desired destination.
6 Issues Under Negotiation (IUN)
In this section we discuss the notion of Is-
sues Under Negotiation represented by ques-
tions, and how proposals relate to issues on IUN.
We also discuss how this approach differs from
Sidner?s.
6.1 Negotiable issues and activity
Which issues are negotiable depends on the ac-
tivity. For example, it is usually not the case
that the name of a DP is a negotiable issue; this
is why it would perhaps seem counterintuitive
to view an introduction (?Hi, my name is NN?)
as a proposal. However, it cannot be ruled out
that there is some activity where even this may
become a matter of negotiation. Also, it is usu-
ally possible in principle to make any issue into
a negotiable issue, e.g. by raising doubts about
a previous answer.
However, for our current purposes we may
make a distinction between negotiable and non-
negotiable issues in an activity. The advan-
tage of this is that the more complex processing
and domain-specific knowledge required for ne-
gotiable issues are only required for issues which
the system needs to be able to negotiate. The
drawback, of course, is that the system becomes
less flexible; however, there is always the possi-
bility of defining all issues as negotiative if one
so desires.
6.2 Alternatives as answers to issues on
IUN
Given that we analyze Issues Under Negotiation
as questions, it is a natural move to analyze the
alternative solutions to this issue as potential
answers. On this view, a proposal has the effect
of adding an alternative answer to the set of al-
ternative answers to an issue on IUN. An answer
to the question on IUN is equivalent to accept-
ing one of the potential answers as the actual
answer. That is, a question on IUN is resolved
when an alternative answer is accepted.
Here we see how our concept of acceptance
differs from Sidner. On our view a proposal can
be accepted in two different ways: as a proposal,
or as the answer to an issue on IUN. Accepting
a proposal move as adding an alternative corre-
sponds to meta-level acceptance. However, ac-
cepting an alternative as the answer to an issue
on IUN is different from accepting an utterance.
Given the optimistic approach to acceptance, all
proposals will be assumed to be accepted as pro-
posals; however, it takes an answer-move to get
the proposed alternative accepted as the solu-
tion to a problem.
7 Adding IUN to the GoDiS
information state
The ideas described in this paper are currently
being implemented in GoDiS (Bohlin et al,
1999), an experimental dialogue system initially
adapted for the travel agency domain but later
adapted for several other domains. GoDiS is
implemented using the TrindiKit(Larsson and
Traum, 2000; Larsson et al, 2000), a toolkit for
experimenting with information states and di-
alogue move engines and for building dialogue
systems.
The notion of information state used by
GoDiS is basically a version of the dialogue game
board which has been proposed by (Ginzburg,
1998). We represent information states of a dia-
logue participant as a record of the type shown
in Figure 3.
The main division in the information state
is between information which is private to the
agent and that which is shared between the di-
alogue participants. The private part of the in-
formation state contains a plan field holding
a dialogue plan, i.e. is a list of dialogue ac-
tions that the agent wishes to carry out. The
agenda field, on the other hand, contains the
short term goals or obligations that the agent
has, i.e. what the agent is going to do next.
We have included a field tmp that mirrors the
shared fields. This field keeps track of shared
information that has not yet been confirmed as
grounded, i.e. as having been understood by the
other dialogue participant. The shared field is
divided into a local and a global part. The local
part contains information about the latest utter-
ance, and information which may be relevant for
interpreting it. The first subfield is for a stack
of questions under discussion (qud). These are
questions that can currently be answered ellip-
tically. The lu field contains information about
the speaker of, and the moves performed in, lat-
est utterance.
The global part contains shared informa-
tion which reflects the global state of the dia-
logue. It contains a set of propositions (com-
mitments) which the agent assumes have been
jointly committed to in the dialogue (com).
In order to include Issues Under Negotiation
and alternative answers to issues on IUN in
the information state, we have also added
a new information state field of type Open-
Stack(Pair(Question,Set(Answer)))4.
We define update rules for updating the infor-
mation state based on the recognized move(s).
The rules are defined in terms of preconditions
and effects on the information state; the effects
are a list of operations to be executed if the pre-
conditions are true.
Regarding the semantics of questions, propo-
sitions, and short answers, we use a simpli-
fied version of first order logic with the addi-
tion of lambda abstracts for representing ques-
tions. Questions and answers can be combined
to form propositions. For example, the con-
tent of ?when do you want to leave?? can
be represented as ?x.desired dept time(x),
the answer ?twelve thirty? as 12:30, and the
proposition resulting from combining the two
desired dept time(12:30). As a further ex-
ample, the proposition that a certain flight
(denoted f1) departs at 7:45 is represented as
dept time(f1,0745). For a more comprehen-
sive description of the semantics used, see (Lars-
son, 2002).
4The choice of a stack is motivated by the fact that
several issues may, in principle, be under negotiation at
once, and that some issues may be subordinate to others.
An open stack is a stack where non-topmost elements are
accessible for inspection and deletion.
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
private :
?
?
?
?
?
agenda : Stack(Action)
plan : Stack(PlanConstruct)
bel : Set(Prop)
tmp : (same as shared)
?
?
?
?
?
shared :
?
?
?
?
?
?
?
?
global :
[
com : Set(Prop)
iun : OpenStack(Pair(Question,Set(Answer)))
]
local :
?
?
?
qud : Stack(Question)
lu :
[
speaker : Participant
moves : Set(Move)
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: Extended GoDiS information state type, including IUN
8 An example
In the (invented) example in 4, the question on
IUN is ?x.desired flight(x), i.e. ?Which flight
does the user want??. The user supplies infor-
mation about her desired destination and depar-
ture date; this utterance is interpreted as a set of
answer-moves by the system since it provides an-
swers to questions that the system has asked or
was going to ask. As a response to this, GoDiS
performs a database search which returns two
flights f1 and f2 matching the specification, and
stores the database results in private.bel. The
system then proposes these flights as answers to
the IUN. The system also supplies some infor-
mation about them. As a result, the IUN is now
associated with two alternative answers, f1 and
f2. Finally, the user provides an answer to the
question on IUN, thereby accepting one of these
alternatives as the flight she wants to take.
Because of space limitations, this dialogue
does not include any discussion or comparison
of alternatives, but it could easily be extended
to cover e.g. the dialogue in 2.
In a travel agency setting, it can be argued
that the informational distribution and decision
rights associated with the roles of customer and
clerk in a travel agency are sufficient to distin-
guish proposals for acceptances, but in a more
complex setting the move recognition will re-
quire more information about surface form and
dialogue context.
9 Relation to previous work
Much work on negotiative dialogue, including
that of Sidner, is based on variations on the BDI
model of classic AI and uses generalised plan-
ning and plan recognition as an integral part of
dialogue processing (e.g. (Cohen and Levesque,
1991), (Grosz and Kraus, 1993) (Chu-Carroll
and Carberry, 1994)). The risk with this kind
of very general reasoning is that it may become
computationally complex and expensive. There-
fore, we believe it is useful to start with a simple
theory involving reasoning specific to a certain
kind of dialogue and see how far that takes us,
and at what point general planning and reason-
ing is really necessary.
In working on GoDiS our strategy has been
to start from a basic issue-based approach and
gradually extending it, while trying to keep
things as simple as possible. We believe that
the issue-based approach can be extended with,
and is compatible with, planning and general
reasoning mechanisms. This is also in line with
the idea behind the information state update ap-
proach as implemented in TrindiKit, i.e. to al-
low for experimentation with different kinds of
information states and information state update
strategies at various levels of complexity (rather
than being confined to choosing between over-
simplified finite state / frame-based approaches
on the one hand, or complex plan-based ap-
proaches on the other).
Also, most of the plan-based dialogue research
is focused on relatively complex dialogues in-
volving collaborative planning. The model pre-
sented here is not committed to the view that
negotiation only takes place in the context of col-
laborative planning. In the sense of negotiative
dialogue used here, i.e. dialogue involving sev-
A> flights to paris, june 13
answer(desired dest city(paris))
answer(desired dept date(13/6))
B> OK, there?s one flight leaving at 07:45 and one at 12:00
propose(f1)
propose(f2)
inform(dept time(f1,07:45))
inform(dept time(f2,12:00))
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
private =
?
?
?
?
?
?
agenda = ? findout(?x.desired flight(x)) ?
plan = ? findout(?x.credit-card-no(x))book ticket ?
bel =
{ flight(f1)
dept time(f1,0745)
. . .
}
?
?
?
?
?
?
shared =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
global =
?
?
?
?
?
?
com =
?
?
?
?
?
?
?
dept time(f1,0745)
dept time(f2,1200)
desired dest city(paris)
desired dept date(13/6)
. . .
?
?
?
?
?
?
?
iun = ? ?x.desired flight(x)?
{
f1, f2
}
?
?
?
?
?
?
?
local =
?
?
?
?
qud = ??
lu =
?
?
?
speaker = sys
moves =
{ propose(f1)
propose(f2)
. . .
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
A> I?ll take the 07:45 one
answer(desired flight(X)&dept time(X, 07:45))
(after contextual interpretation: answer(desired flight(f1)))
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
private =
?
?
?
?
agenda = ? findout(?x.credit-card-no(x)) ?
plan = ? book ticket ?
bel =
{ flight(f1)
dept time(f1,0745)
. . .
}
?
?
?
?
shared =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
global =
?
?
?
?
?
?
?
com =
?
?
?
?
?
?
?
?
?
?
?
desired flight(f1)
dept time(f1,0745)
dept time(f2,1200)
desired dest city(paris)
desired dept date(13/6)
. . .
?
?
?
?
?
?
?
?
?
?
?
iun = ??
?
?
?
?
?
?
?
local =
?
?
?
qud = ??
lu =
?
?
speaker = sys
moves =
{
answer(desired flight(f1))
. . .
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 4: Example dialogue
eral alternative solutions to some problem, ne-
gotiation may also concern matters of fact. This
can be useful e.g. in tutorial dialogue where a
tutor asks a question, gives some alternative an-
swers, and the student?s task is to reason about
the different alternatives and decide on one of
them. In the travel agency domain, it is often
not necessary to explicitly represent e.g. the fact
that the deciding on a flight is a precondition of
a general plan for traveling; instead, we can rep-
resent it simply as a fact concerning which flight
the user wants to take.
(Rose? et al, 1995) provide an account of dis-
course structure for dialogues involving several
alternatives (or ?threads?), which appears to be
compatible with the view presented here. How-
ever, the focus on discourse structures rather
than information states and dialogue processing
(in terms of information state updates) makes
this work less relevant to dialogue systems de-
sign, at least from the perspective of an infor-
mation state update approach.
10 Conclusions and future work
On our approach, an Issue Under Negotiation
is represented as a question , e.g. what flight
the user wants. In general, this means view-
ing problems as issues and solutions as answers.
This approach has several advantages. Firstly, it
provides a straightforward an intuitively sound
way of capturing the idea that negotiative di-
alogue involves several alternative solutions to
some issue or problem, and that proposals in-
troduce such alternatives. Secondly, it distin-
guishes two types of negotiation (dialogue nego-
tiation and issue negotiation) and clarifies the
relation between them. Thirdly, since this ac-
count is a natural extension of the general ideas
behind the GoDiS system (e.g. using issues as
a basis for dialogue management), it allows the
use of independently motivated theory and ma-
chinery to handle a new problem. Apart from
implementing the theory described in this pa-
per, possible future work includes applying the
theory to new domains and extending it to han-
dle more complex kinds of negotiation, possibly
making use of work on collaboration and social
action such as (Castelfranchi et al, 2000).
References
H. Aust, M. Oerder, F. Seide, and V. Steinbiss. 1994.
Experience with the Philips automatic train table
information system. In Proc. of the 2nd Workshop
on Interactive Voice Technology for Telecommuni-
cations Applications (IVTTA), pages 67?72, Ky-
oto, Japan.
P. Bohlin, R. Cooper, E. Engdahl, and S. Lars-
son. 1999. Information states and dialogue move
engines. In J. Alexandersson, editor, IJCAI-99
Workshop on Knowledge and Reasoning in Prac-
tical Dialogue Systems, pages 25?32.
C. Castelfranchi, F. Dignum, C. Jonker, and
J. Treur. 2000. Deliberate normative agents:
Principles and architecture. In Jennings and Les-
perance (Jennings and Lesperance, 2000), pages
206?220.
Jennifer Chu-Carroll and Sandra Carberry. 1994. A
plan-based model for response generation in col-
laborative task-oriented dialogues. In AAAI, Vol.
1, pages 799?805.
H. H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Phillip R. Cohen and Hector J. Levesque. 1991.
Teamwork. Nous, 35.
Robin Cooper, Stina Ericsson, Staffan Larsson,
and Ian Lewin. 2001. An information state
update approach to collaborative negotiation.
In Peter Ku?hnlein, Hannes Rieser, and Henk
Zeevat, editors, BI-DIALOG 2001?Proceedings
of the 5th Workshop on Formal Semantics
and Pragmatics of Dialogue, pages 270?9,
http://www.uni-bielefeld.de/BIDIALOG,
June, 14th?16th. ZiF, Univ. Bielefeld.
B. Di Eugenio, P.W. Jordan, R.H. Thomason, and
J.D. Moore. 1998. An empirical investigation of
proposals in collaborative dialogues. In Proceed-
ings of ACL?COLING 98: 36th Annual Meeting of
the Association of Computational Linguistics and
17th International Conference on Computational
Linguistics, pages 325?329.
J. Ginzburg. 1998. Clarifying utterances. In J. Hul-
stijn and A. Niholt, editors, Proc. of the Twente
Workshop on the Formal Semantics and Pragmat-
ics of Dialogues, pages 11?30, Enschede. Univer-
siteit Twente, Faculteit Informatica.
Barbara [J.] Grosz and Sarit Kraus. 1993. Collab-
orative plans for group activities. In Proceedings
IJCAI-93, pages 367?373.
N. Jennings and Y Lesperance, editors. 2000.
Proceedings of the 6th International Workshop
on Agent Theories, Architectures, and Languages
(ATAL?1999), Springer Lecture Notes in AI 1757.
Springer Verlag, Berlin.
Staffan Larsson and David Traum. 2000. Informa-
tion state and dialogue management in the trindi
dialogue move engine toolkit. NLE Special Is-
sue on Best Practice in Spoken Language Dialogue
Systems Engineering, pages 323?340.
Staffan Larsson, Alexander Berman, Johan Bos,
Leif Gro?nqvist, Peter Ljunglo?f, and David Traum.
2000. Trindikit 2.0 manual. Technical Report De-
liverable D5.3 - Manual, Trindi.
Staffan Larsson. 2002. Issue-based Dialogue Man-
agement. Ph.D. thesis, Go?teborg University.
Ian Lewin, Robin Cooper, Stina Ericsson, and C.J.
Rupp. 2000. Dialogue moves in negotiative dia-
logues. Project deliverable 1.2, SIRIDUS.
C. P. Rose?, B. Di Eugenio, L. S. Levin, and C. Van
Ess-Dykema. 1995. Discourse processing of di-
alogues with multiple threads. In Proc. ACL,
Boston, MIT, June.
Candace L. Sidner. 1994a. An artificial discourse
language for collaborative negotiation. In Proceed-
ings of the forteenth National Conference of the
American Association for Artificial Intelligence
(AAAI-94), pages 814?819.
Candace. L. Sidner. 1994b. Negotiation in collabo-
rative activity: A discourse analysis. Knowledge-
Based Systems, 7(4):265?267.
Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 1?9,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Towards a formal view of corrective feedback
Staffan Larsson and Robin Cooper
Department of Philosophy, Linguistics and Theory of Science
University of Gothenburg
{sl,cooper}@ling.gu.se
Abstract
This paper introduces a formal view of
the semantics and pragmatics of corrective
feedback in dialogues between adults and
children. The goal of this research is to
give a formal account of language coor-
dination in dialogue, and semantic coor-
dination in particular. Accounting for se-
mantic coordination requires (1) a seman-
tics, i.e. an architecture allowing for dy-
namic meanings and meaning updates as
results of dialogue moves, and (2) a prag-
matics, describing the dialogue moves in-
volved in semantic coordination. We illus-
trate the general approach by applying it
to some examples from the literature on
corrective feedback, and provide a fairly
detailed discussion of one example using
TTR (Type Theory with Records) to for-
malize concepts. TTR provides an analy-
sis of linguistic content which is structured
in order to allow modification and similar-
ity metrics, and a framework for describ-
ing dialogue moves and resulting updates
to linguistic resources.
1 Introduction
Here are a few examples of corrective feedback:
A: That?s a nice bear.
B: Yes, it?s a nice panda.
Abe: I?m trying to tip this over, can you tip it
over? Can you tip it over?
Mother: Okay I?ll turn it over for you.
Adam: Mommy, where my plate?
Mother: You mean your saucer?
Naomi: Birdie birdie.
Mother: Not a birdie, a seal.
Naomi: mittens.
Father: gloves.
The first one is made up, the others are quoted
from various sources in (Clark and Wong, 2002)
and (Clark, 2007). In general, corrective feedback
can be regarded as offering an alternative form to
the one that the speaker used. We are interested
in interactions such as these since we believe that
dialogue interaction plays an important role in es-
tablishing a shared language, not only in first (or
second) language acquisition but also in the coor-
dination of meaning in adult language, in histori-
cal language change, and in language evolution.
Two agents do not need to share exactly the
same linguistic resources (grammar, lexicon etc.)
in order to be able to communicate, and an agent?s
linguistic resources can change during the course
of a dialogue when she is confronted with a (for
her) innovative use. For example, research on
alignment shows that agents negotiate domain-
specific microlanguages for the purposes of dis-
cussing the particular domain at hand (Clark and
Wilkes-Gibbs, 1986; Garrod and Anderson, 1987;
Pickering and Garrod, 2004; Brennan and Clark,
1996; Healey, 1997; Larsson, 2007). We will use
the term semantic coordination to refer to the pro-
cess of interactively coordinating the meanings of
linguistic expressions.
This paper presents work towards a formal the-
ory of corrective feedback, and semantic coordina-
tion in general. It takes a view of natural languages
as toolboxes for constructing domain-specific mi-
crolanguages, and provides an analysis of linguis-
tic content which is structured in order to allow
modification of, and similarity metrics over, mean-
ings.
According to (Cooper and Ranta, 2008), a
?language? such as Swedish or English is to be
1
regarded as a collection of resources (a ?tool-
box?) which can be used to construct local micro-
languages. We take the view that speakers of natu-
ral languages are constantly in the process of cre-
ating new language to meet the needs of novel sit-
uations in which they find themselves.
Accounting for corrective feedback requires (1)
dynamic representations of concepts which can be
modified in various ways, in a process of seman-
tic coordination, and (2) a description of dialogue
strategies involved in semantic coordination.
Accordingly, the research effort which the work
presented here is part of aims towards an account
of semantic coordination in dialogue, consisting of
two parts:
1. semantics: an account of how meanings (and
concepts) can be updated
2. pragmatics: an account of how meanings
(and concepts) are coordinated in dialogue
and how dialogue moves governing coordi-
nation are related to semantic updates
These parts will be presented below, starting
with the pragmatics. At the end of the paper, we
will step back and consider the implications of our
tentative results.
2 The pragmatics of corrective feedback
To get a handle on the pragmatic processes in-
volved in corrective feedback, we will show how
such interactions can be analysed in terms of di-
alogue moves related to semantic updates. This
approach builds on, and extends, the Information
State Update approach to dialogue management
(Traum and Larsson, 2003).
2.1 A taxonomy of corrective feedback
Below, we classify our examples into four kinds of
corrective feedback.
? Example 1: ?In-repair?
? Abe: I?m trying to tip this over, can you
tip it over? Can you tip it over?
? Mother: Okay I?ll turn it over for you.
? Example 2: Clarification request
? Adam: Mommy, where my plate?
? Mother: You mean your saucer?
? Example 3: ?Explicit replace?
? Naomi: Birdie birdie.
? Mother: Not a birdie, a seal.
? Example 4: ?Bare? correction
? Naomi: mittens.
? Father: gloves.
2.2 Dialogue moves for corrective feedback
We will now introduce a representation of dia-
logue moves used in corrective feedback. The gen-
eral format we will use is
? offer-form: TYPE(ARGS)
where ARGS may include one or several of the
following:
? proposed form (P below)
? replaced form (R below)
? sentence frame (F below)
In the representation above, TYPE is one of the
following, corresponding to the kinds of corrective
feedback distinguished above:
? in-repair
? cr
? explicit-replace
? bare
In-repair The in-repair type of corrective feed-
back takes two arguments, the proposed form and
a sentence frame. It is generally preceded by an
utterance containing the sentence frame applied to
the replaced form.
? offer-form:in-repair(P , F )
For illustration, let?s look again at our example,
now with typography indicating PROPOSED
FORM, replaced form and sentence frame:
A(be): Can you tip it over?
M(other): Okay I?ll TURN it over for you.
In relation to A?s utterance, M ?s utterance
contains the same sentence frame F , roughly
?[Mother] it over?. However, they differ in that
whereas M ?s utterance has the proposed word P
= ?TURN?, A?s utterance has R = ?tip?. If we
2
say that sentence frames can be applied to ex-
pressions, resulting in the ? ? in the frame being
replaced with the expression (much as in lambda
reduction), we can say that A?s utterance has the
form ?F (R)? = ?[Mother] it over?(?turn?) =
?[Mother] turn it over? whereas M ?s utterance
has the form ?F (R)?. M ?s utterance corresponds
to the dialogue move:
offer-form:in-repair(?turn?, ?[M] [it] over?)
Note that the syntactic parallelism is not com-
plete; we have ignored the complication that one
utterance contains ?can? and the other ?will? (in
reduced form). The notion of sentence frame used
here is a simplification of a more complex relation
of syntactic and semantic parallelism which needs
to be further explored.
Note also that in addition to providing correc-
tive feedback, M ?s utterance also accepts the con-
tent of the previous contribution. Note that M
might instead have said something like ?No, but
I?ll turn it over for you?.
Clarification requests As in the case of in-
repair offers, offers involving clarification requests
also provide the proposed form together with a
sentence frame linking the move to a previous ut-
terance by the child; presuming that the latter has
the form ?... F (R)?, the offer can be represented
as
offer-form:cr(P , F )
Let?s revisit our example, making explicit the
P , F and R parameters:
A(dam): Mommy, where my plate?
M(other): You mean your SAUCER?
Here, we have F = ?[Adam?s] ?, R = ?plate?
and P = ?SAUCER?. Accordingly, we can de-
scribe M ?s utterance as a dialogue move:
offer-form:cr(?saucer?, ?[A?s] ?)
Typically, CRs have the interpretation ?you
mean/want F (P )??. In addition to offering an al-
ternative form P of expression, a clarification re-
quest also explicitly raises the issue whether the
offer of P is accepted, and is typically followed
by a positive (or negative) answer by the child.
Note that CRs, as well as some other types
of offers, may not be intended as corrections by
the adult, but simply as attempts at understanding
what the child wants to communicate. The cru-
cial point for our purposes here is the effect these
moves have on the addressee, rather than the un-
derlying intention. In general, if I learn something
from someone else, it may not be of great impor-
tance for my learning if they intended for me to
learn it or not.
Explicit replace In contrast to in-repairs and
clarification requests, explicit offers of replace-
ments need not rely on sentence frames to figure
out the replaced form, as it is (as the name
indicates) explicitly mentioned in the offer.
N(aomi): Birdie birdie
M(other): Not a birdie, a SEAL
We represent this kind of dialogue move thus:
offer-form:explicit-replace(P , R)
In the example, the move is offer-
form:explicit-replace(?seal?, ?birdie?). Explicit
replace offers are preceded by an utterance
consisting of or containing the replaced form R,
and typically have the form ?(that?s) not DET R,
(that?s) DET P ? or similar.
Explicit replace offers differ from in-repairs and
clarification requests by clearly signalling that the
replaced form is not appropriate, and by being
clearly intended as providing corrections rather
than (just) figuring out what the child is trying to
communicate.
Bare offers Bare offers are the formally sim-
plest kind of corrective feedback, consisting
simply of the proposed form.
Naomi: Mittens
Father: GLOVES.
The dialogue move representation is
offer-form:bare(P )
In the example, the move is offer-
form:bare(?gloves?). Since neither sentence
frame or replaced form is provided, the replaced
form must be figured out from the conversational
situation as a whole. Just as explicit replace offers,
bare offers are primarily intended as providing
3
corrections.
2.3 Generalising the dialogue move
representation
The different variants for corrective feedback all
do basically the same work; they indicate that
the child needs to modify his or her take on the
meaning of the proposed term, and perhaps also
the replaced term. A possible difference is that
some forms more clearly provide evidence that the
replaced form is not appropriate, whereas others
leave this open. Ignoring this complication for the
moment, we can provide a general form for the
various types of offers of new forms, with the pro-
posed form and the replaced form as arguments:
offer-form(P , R)
Using this representation, the dialogue move
analyses above can be reformulated as, in the order
they appear above:
? offer-form(?turn?, ?tip?)
? offer-form(?saucer?, ?plate?)
? offer-form(?seal?, ?birdie?)
? offer-form(?gloves?, ?mittens?)
In moves which do not explicitly indicate the re-
placed form R, contextual interpretation involves
chart alignment and reasoning about active edges
(represented here by the sentence frame) to locate
an expression R parallel to P in the previous ut-
terance.
2.4 Agents that coordinate resources
As in the information state update approach in
general, dialogue moves are associated with in-
formation state updates. For semantic coordina-
tion, the kind of update is rather different from
the one associated with dialogue moves for coor-
dinating on task-related information, and involves
updating the linguistic resources available to the
agent (grammar, lexicon, semantic interpretation
rules etc.), rather than e.g. the conversational
scoreboard as such. Our view is that agents do
not just have monolithic linguistic resources as is
standardly assumed. Rather they have generic re-
sources which they modify to construct local re-
sources for sublanguages for use in specific sit-
uations. Thus an agent A may associate a lin-
guistic expression c with a particular concept (or
collection of concepts if c is ambiguous) [c]A in
its generic resource. In a particular domain ? c
may be associated with a modified version of [c]A,
[c]A? . In some cases [c]
A
? may contain a smaller
number of concepts than [c]A, representing a de-
crease in ambiguity. Particular concepts in [c]A?
may be a refinement of one in [c]A, that is, the do-
main related concepts have an extension which is
a proper subset of the extension of the correspond-
ing generic concept. This will, however, not be the
case in general. For example, a black hole in the
physics domain is not normally regarded as an ob-
ject described by the generic or standard meaning
of black hole provided by our linguistic resources
outside the physical domain. Similarly a variable
in the domain of logic is a syntactic expression
whereas a variable in experimental psychology is
not and quite possibly the word variable is not
even a noun in generic linguistic resources.
Our idea is that the motor for generating new
such local resources in an agent lies in coordinat-
ing resources with another agent in a particular
communicative situation s. The event s might be
a turn in a dialogue, as in the examples we are dis-
cussing in this paper, or, might, for example, be
a reading event. In a communicative situation s,
an agent A may be confronted with an innovative
utterance c, that is, an utterance which either uses
linguistic expressions not already present in A?s
resources or linguistic expressions known by A
but associated with an interpretation distinct from
that provided by A?s resources. At this point, A
has to accommodate an interpretation for c which
is specific to s, [c]As , and which may be anchored
to the specific objects under discussion in s.
Whereas in a view of semantics inherited from
formal logic there is a pairing between a linguis-
tic expression c and an interpretation c? (or a set of
several interpretations if c is ambiguous), we want
to see c as related to several interpretations: [c]As
for communicative situations s, [c]A? for domains
? (where we imagine that the domains are col-
lected into a complex hierarchy or more and less
general domains) and ultimately a general linguis-
tic resource which is domain independent, [c]A.
We think of the acquisition of a pairing of an ex-
pression c with an interpretation c? as a progres-
sion from an instance where c? is [c]As for some
particular communicative situation s, through po-
tentially a series of increasingly general domains
?where c? is regarded as being one of the interpre-
tations in [c]A? and finally arriving at a state where
4
c? is associated with c as part of a domain indepen-
dent generic resource, that is, c? is in [c]A. There
is no guarantee that any expression-interpretation
pair will survive even beyond the particular com-
municative situation in which A first encountered
it. For example, the kind of ad hoc coinages
described in (Garrod and Anderson, 1987) us-
ing words like leg to describe part of an oddly
shaped maze in the maze game probably do not
survive beyond the particular dialogue in which
they occur. The factors involved in determin-
ing how a particular expression-interpretation pair
progresses we see as inherently stochastic with pa-
rameters including the degree to which A regards
their interlocutor as an expert, how many times the
pairing has been observed in other communicative
situations and with different interlocutors, the util-
ity of the interpretation in different communicative
situations, and positive or negative feedback ob-
tained when using the pairing in a communicative
situation. For example, an agent may only allow a
pairing to progress when it has been observed in at
least n different communicative situations at least
m of which were with an interlocutor considered
to be an expert, and so on. We do not yet have a
precise proposal for a theory of these stochastic as-
pects but rather are seeking to lay the groundwork
of a semantic treatment on which such a theory
could be built.
3 The semantics of corrective feedback
3.1 Representing concepts using TTR
We shall make use of type theory with records
(TTR) as characterized in Cooper (2005; 2008)
and elsewhere. The advantage of TTR is that it
integrates logical techniques such as binding and
the lambda-calculus into feature-structure like
objects called record types. Thus we get more
structure than in a traditional formal semantics
and more logic than is available in traditional
unification-based systems. The feature structure
like properties are important for developing
similarity metrics on meanings and for the
straightforward definition of meanings modifi-
cations involving refinement and generalization.
The logical aspects are important for relating
our semantics to the model and proof theoretic
tradition associated with compositional semantics.
Below is an example of a record type:
[
REF : Ind
SIZE : size(REF, MuchBiggerThanMe)
SHAPE : shape(REF, BearShape)
]
A record of this type has to have fields with
the same labels as those in the type. (It may also
include additional fields not required by the type.)
In place of the types which occur to the right of
?:? in the record type, the record must contain an
object of that type. Here is an example of a record
of the above type:
?
?
?
REF = obj123
SIZE = sizesensorreading85
SHAPE = shapesensorreading62
COLOUR = coloursensorreadning78
?
?
?
Thus, for example, what occurs to the right of
the ?=? in the REF field of the record is an object
of type Ind, that is, an individual. Types which
are constructed with predicates like size and shape
are sometimes referred to as ?types of proof?. The
idea is that something of this type would be a proof
that a given individual (the first argument) has a
certain size or shape (the second argument). One
can have different ideas of what kind of objects
count as proofs. Here we are assuming that the
proof-objects are readings from sensors. This is a
second way (in addition to the progression of lo-
cal resources towards general resources) that our
theory interfaces with a statistical non-categorical
world. We imagine that the mapping from sensor
readings to types involves sampling of analogue
data in a way that is not unsimilar to the digiti-
zation process involved, for example, in speech
recognition. Again we have nothing detailed to
say about this at the moment, although we regard
it as an important part of our theory that it is able
to make a connection between the realm of feature
vectors and the realm of model-theoretic seman-
tics.
Types constructed with predicates may also be
dependent. This is represented by the fact that ar-
guments to the predicate may be represented by
labels used on the left of the ?:? elsewhere in the
record type. This means, for example, that in con-
sidering whether a record is of the record type, you
will need to find a proof that the object which is in
the REF-field of the record has the size represented
by MuchBiggerThanMe. That is, this type depends
on the value for the REF-field.
Some of our types will contain manifest fields
(Coquand et al, 2004) like the REF-field in the
following type:
5
[
REF=obj123 : Ind
SIZE : size(REF, MuchBiggerThanMe)
SHAPE : shape(REF, BearShape)
]
[
REF=obj123:Ind
]
is a convenient notation for
[
REF : Indobj123
]
where Indobj123 is a singleton
type. If a : T , then Ta is a singleton type and
b : Ta (i.e. b is of type Ta) iff b = a. Manifest
fields allow us to progressively specify what val-
ues are required for the fields in a type.
An important notion in this kind of type theory
is that of subtype. For example,
[
REF : Ind
SIZE : size(REF, MuchBiggerThanMe)
]
is a subtype of
[
REF : Ind
]
as is also
[
REF=obj123 : Ind
]
The subtype relation corresponds to that of sub-
sumption in typed feature structures. This gives us
the ability to create type hierarchies corresponding
to ontologies (in the sense, for example, of OWL).
Such ontologies (coded in terms of record types)
play an important role in our notion of resources
available to an agent. In fact, modelling concepts
in terms of record types commits us to a view of
concepts which is very closely related to work on
ontologies. But our view of the creation of lo-
cal situation specific and domain related resources
in addition to generic resources means that agents
have access not to a single generic ontology but
also situation specific and domain related ontolo-
gies. And, perhaps most important of all, the pro-
cess of semantic coordination with an interlocutor
can involve local ad hoc adjustment to an ontol-
ogy. This plays an important role in characteriz-
ing the options open to an agent when confronted
with an innovative utterance. We attempt to illus-
trate this below by working in more detail through
a specific example.
3.2 ?Panda? as an example of innovative use
We provide an analysis ofB?s utterance in our ini-
tial example as a move of offering ?panda? as an
alternative for ?bear?, and as potentially triggering
an update onA?s concepts for ?bear? and ?panda?.
A: That?s a nice bear
B: Yes, it?s a nice panda
The dialogue move analysis of this example is
offer-form:in-repair(?panda?, ?[it] is a nice ?),
or in the generalised format offer-form(?panda?,
?bear?).
We assume that, before B?s utterance, A has
a single concept of ?bear? in a domain called
?zoo?, that is, a unique member of the collection
[bear]Azoo. We represent it in Figure 1. A?s take on
the communicative situation where B?s utterance
takes place (that is,A?s dialogue information state,
much simplified for expository reasons) is shown
in Figure 2. This is intended to describe a situation
at a zoo, where a bear-shaped object much bigger
than A is in focus (FOO here stands for ?Focused
Object?).
What happens after B?s utterance? First, we as-
sume that B correctly understands A?s utterance
as offering ?panda? as an alternative for ?bear?.
Now, assuming that B has not observed the word
?panda? before,A needs to create a panda-concept
[panda]As , local to the communicative situation s
resulting from B?s utterance. Since ?panda? has
been aligned with ?bear?, it is natural to base
the new panda concept on the bear concept, as-
sociated with the domain. Here A is confronted
with a fundamental choice. Should a condition
?panda(REF)? be added to the concept in addition
to the condition ?bear(REF)? making the panda
concept be a subtype of the bear concept or should
the panda condition replace the bear condition,
making panda and bear sisters in the ontology?
There is not enough evidence in this simple ex-
change to determine this.1 We will choose to re-
place the bear condition with the panda condition.
But there is more that must happen.
A has observed that the use of ?panda? in s
refers to the focused object obj123. Following
the principle of contrast (Clark and Wong, 2002)
which states that ?(s)peakers take every difference
in form to mark a difference in meaning?, B takes
?panda? to have a different meaning than ?bear?
in some respect other than that it is a panda as
opposed to a bear, and looks for something about
obj123 which might distinguish it from previously
observed bears. For example, the child might de-
cide that it is the colour (black and white) that
1And indeed many people can reach adulthood, the
present authors included, without being sure whether pandas
are a kind of bear or not.
6
??
?
?
?
?
REF : Ind
PHYS : phys-obj(REF)
ANIM : animate(REF)
SIZE : size(REF, MuchBiggerThanMe)
SHAPE : shape(REF, BearShape)
BEAR : bear(REF)
?
?
?
?
?
?
Figure 1: A?s ?bear? concept in the domain ?zoo? before the interaction
?
?
?
DOMAIN : zoo
SHARED :
?
?
FOO=obj123 : Ind
COM=
[
C1 : nice(FOO)
C2 : bear(FOO)
]
: RecType
?
?
?
?
?
Figure 2: A?s take on s
distinguishes it from previously observed bears
(which have all been brown)2. A now creates a
situated interpretation [panda]As of ?panda?, based
on [bear]Azoo, as shown in Figure 3.
But now if colour is being used to distin-
guish between bears and pandas in situation s, A
should create a refined bear concept for s, namely
Figure 4 reflecting the hypothesis that bears are
brown. If A is optimistic, possibly influenced
by the degree of expertise which A assigns to B
(?Mummy knows about zoos?), A might imme-
diately associate the concept in Figure 4 with the
zoo domain, that is, make it be a new value for
[bear]Azoo and similarly for a dereferenced version
of Figure 3, that is a version in which the manifest
field is replaced by
[
REF : Ind
]
. Finally, A?s new
take on s is shown in Figure 5; A has accepted that
the focused object is a panda.
4 Conclusion
We have sketched an account of how concepts
can be updated as a result of language use in
interaction. Such processes enable coordination
of domain-specific microlanguages, involving a
domain-specific grammar and lexicon, an ontol-
ogy, and a mapping between lexicon and ontology.
There are many mechanisms for semantic coor-
dination, some of which can be described as cor-
rective feedback: clarification requests, explicit
corrections, meaning accommodation (observing
instances of language use and silently adapting to
successful instances) and explicit negotiation. Se-
mantic coordination, in turn, is a kind of language
coordination (other kinds include e.g. phonetic co-
2This account relies on A having a memory of previously
observed instances of a concept, in addition to the concept it-
self (which in the case of ?bear? does not contain information
about colour).
ordination). Finally, language coordination coex-
ists with information coordination, the exchang-
ing and sharing of information (agreeing on rele-
vant information and future action; maintaining a
shared view on current topics of discussion, rele-
vant questions etc.). Arguably, the main point of
language coordination is to enable information co-
ordination.
Semantic coordination happens in dialogue; it
is part of language coordination; and it is a pre-
requisite for information coordination. If we say
that a linguistic expression c has meaning only
if it is possible to exchange information using c,
then semantic coordination is essential to mean-
ing. A linguistic expression c has meaning in a
language community when the community mem-
bers are sufficiently coordinated with respect to
the meaning of c to allow them to use c to ex-
change information. In other words: meaning
emerges from a process of semantic coordination
in dialogue.
Acknowledgement
This research was supported by The Swedish Bank
Tercentenary Foundation Project P2007/0717, Se-
mantic Coordination in Dialogue.
7
??
?
?
?
?
?
REF=obj123 : Ind
PHYS : phys-obj(REF)
ANIM : animate(REF)
SIZE : size(REF, MuchBiggerThanMe)
SHAPE : shape(REF, BearShape)
COLOUR : colour(REF, BlackAndWhite)
PANDA : panda(REF)
?
?
?
?
?
?
?
Figure 3: A?s situated interpretation of ?panda? in situation s.
?
?
?
?
?
?
?
REF : Ind
PHYS : phys-obj(REF)
ANIM : animate(REF)
SIZE : size(REF, MuchBiggerThanMe)
SHAPE : shape(REF, BearShape)
COLOUR : colour(REF, Brown)
BEAR : bear (REF)
?
?
?
?
?
?
?
Figure 4: A?s local ?bear? concept after integrating B?s utterance
?
?
?
DOMAIN : zoo
SHARED :
?
?
FOO=obj123 : Ind
COM=
[
C1 : nice(FOO)
C2 : panda(FOO)
]
: RecType
?
?
?
?
?
Figure 5: A?s revised take on s
8
References
S. E. Brennan and H. H. Clark. 1996. Conceptual
pacts and lexical choice in conversation. Journal of
Experimental Psychology: Learning, Memory and
Cognition, 22:482?493.
H. H. Clark and D. Wilkes-Gibbs. 1986. Refering as a
collaborative process. Cognition, 22:1?39.
Eve V. Clark and Andrew D. W. Wong. 2002. Prag-
matic directions about language use: Offers of
words and relations. Language in Society, 31:181?
212.
E. V. Clark. 2007. Young children?s uptake of
new words in conversation. Language in Society,
36:157?82.
Robin Cooper and Aarne Ranta. 2008. Natural lan-
guages as collections of resources. In Robin Cooper
and Ruth Kempson, editors, Language in Flux: Re-
lating Dialogue Coordination to Language Varia-
tion, Change and Evolution. College Publications,
London.
Robin Cooper. 2005. Austinian truth, attitudes and
type theory. Research on Language and Computa-
tion, 3:333?362.
Robin Cooper. 2008. Type theory with records and
unification-based grammar. In Fritz Hamm and
Stephan Kepser, editors, Logics for Linguistic Struc-
tures. Mouton de Gruyter.
Thierry Coquand, Randy Pollack, and Makoto
Takeyama. 2004. A logical framework with de-
pendently typed records. Fundamenta Informaticae,
XX:1?22.
Simon C. Garrod and Anthony Anderson. 1987. Say-
ing what you mean in dialogue: a study in concep-
tual and semantic co-ordination. Cognition, 27:181?
218.
P.G.T. Healey. 1997. Expertise or expertese?: The
emergence of task-oriented sub-languages. In M.G.
Shafto and P. Langley, editors, Proceedings of the
19th Annual Conference of the Cognitive Science
Society, pages 301?306.
Staffan Larsson. 2007. Coordinating on ad-hoc se-
mantic systems in dialogue. In Proceedings of the
10th workshop on the semantics and pragmatics of
dialogue.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences, 27(02):169?226, April.
David Traum and Staffan Larsson. 2003. The in-
formation state approach to dialogue management.
In Ronnie Smith and Jan Kuppevelt, editors, Cur-
rent and New Directions in Discourse & Dialogue.
Kluwer Academic Publishers.
9
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 37?40,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Safe In-vehicle Dialogue Using Learned Predictions of User Utterances
Staffan Larsson
Talkamatic AB
F?orsta L?anggatan 18
413 28 G?oteborg
Sweden
staffan@talkamatic.se
Fredrik Kronlid
Talkamatic AB
F?orsta L?anggatan 18
413 28 G?oteborg
Sweden
fredrik@talkamatic.se
Pontus W
?
arnest
?
al
Halmstad University
Box 823
301 18 Halmstad
Sweden
pontus.warnestal@hh.se
Abstract
We present a multimodal in-vehicle dia-
logue system which uses learned predic-
tions of user answers to enable shorter,
more efficient, and thus safer natural lan-
guage dialogues.
1 Background
1.1 Driver Distraction
Driver distraction is a common cause of accidents,
and is often caused by the driver interacting with
technologies such as mobile phones, media play-
ers or navigation systems. A study, commonly
referred to as the ?100 car study? (Neale et al.,
2005) revealed that secondary task distraction is
the largest cause of driver inattention, and that the
handling of wireless devices is the most common
secondary task.
As interaction complexity in the car increases
due to more advanced infotainment systems and
smartphones, drivers are often executing several
tasks in parallel to the primary task of driving.
The increased functionality of these systems has
resulted in large hierarchical information architec-
tures that prolong interaction time, thereby nega-
tively affecting safety as well as user experience
(Kern and Schmidt, 2009).
1.2 Relation to state of the art
State-of-the-art infotainment systems typically do
not include user models at all. Siri, available on
the Apple iPhone 4S and later models, has a static
user model containing personal information ex-
plicitly provided by the user (home address, etc.).
This information is used in voice interactions; for
example, given that the user has entered their fam-
ily relations, phrases like ?Call my wife? can be
used. A different approach is taken in Google
Now, which dynamically learns user patterns from
observations and presents unrequested informa-
tion as ?cards? on the screen. However, Google
Now does not attempt to integrate predictions into
dialogue interaction.
The work reported here explores the use of
adaptive user modeling in multimodal dialogue
systems. User preferences and behaviour patterns
are learnt from observations of user interactions
with the infotainment system and the context in
which these interactions take place, and are used
proactively to predict user answers and thereby en-
able shorter and more efficient interaction. The
underlying motivating assumption is that using
apps and services in an in-vehicle context inher-
ently leads to distraction, and that reducing inter-
action time will reduce driver distraction.
1.3 TDM
Based on Larsson (2002) and later work, Talka-
matic AB has developed the Talkamatic Dialogue
Manager (TDM).
TDM provides a general interaction model
based on interaction which are basic to human-
human linguistic interaction, resulting in a high
degree of naturalness and flexibility which in-
creases usability. The model is domain-
independent which means that dialogue behaviour
can be altered without touching application prop-
erties and vice versa. TDM also offers integrated
multi-modality which allows user to freely switch
between modalities (Larsson et al., 2011).
1.4 Grounding in TDM
Grounding (Clark and Brennan, 1990) is, roughly,
the process of making sure that dialogue partici-
pants agree on what has been said so far and what
it meant. TDM has an extensive model of ground-
ing (Larsson, 2002). It operates on different levels:
? Perception
? Semantic Understanding
37
? Pragmatic Understanding
? Acceptance
System feedback (positive, negative and in
some cases interrogative) can be generated on each
level:
? Examples: ?I didn?t hear? ? negative percep-
tion
? ?To work, is that right?? ? interrogative se-
mantic understanding
? ?OK? ? positive acceptance.
2 Learning and Classification
Many dialogue applications require the user to an-
swer a number of questions. To make dialogue
shorter, we have extended TDM so that it tries to
predict user answers on the basis of a user model
learned from observations of user behaviour. As
an illustration, we use a road information appli-
cation which tries to predict the user?s destina-
tion and thereby eliminate the need to ask the user
about this.
2.1 Learning Method
Initially, a range of learning methods requir-
ing (N-gram, MDP, POMDP) were explored and
evaluated, but the KNN (K-Nearest Neighbours)
(Mitchell, 1997) was considered the best method.
An important advantage is that KNN can learn
from a relatively small set of observations. This
is in contrast to the MDP and POMDP (and to
a lesser extent, N-gram) methods, which require
large amounts of data to generate useful behaviour.
A potential drawback of KNN is that this model
cannot model sequences of user behaviours.
2.2 Parameter Selection
On the basis of user studies provided from the
user partner of the project, it was decided that the
most important user model parameters was posi-
tion, day of the week and hour of the day. The
training data were simulated and correspond to the
behaviour of an archetypal persona provided by
the user partner in the project.
2.3 Learning and Classification
The learning part of the system listens for a num-
ber of events, such as ?start-car?, ?stop-car? etc..
From these events and information about cur-
rent position, the time of the day and the day of
the week, the system creates new data instances.
The system thus learns how the user?s destination
varies depending on these parameters. A sample
dataset is shown in Figure 1, where data points
show destinations of trips initiated at various times
of the week.
When the dialogue manager requests a predic-
tion of the destination, the KNN algorithm tries to
find the K data points closest to the present data
point, and the top alternatives are returned to the
dialogue manager together with confidence scores
indicating the reliability of the predictions.
3 Integration of Classifications into TDM
3.1 Grounding uncertain information
We treat the information emanating from the user
model as uncertain information about a (predicted)
user utterance. Hence, the same mechanisms used
for grounding utterances have been adapted for in-
tegrating user model data.
3.2 Integrating Classifier Output
TDM is based on the Information State Update
(ISU) approach to dialogue management. The in-
formation state in TDM is based on that of the
system described in Larsson (2002) and includes
Questions Under Discussion, a dialogue plan, and
shared commitments.
The rule for integrating the user model data is
a standard ISU rule, consisting of preconditions
and effects on the information state. We describe
these informally below:
PRECONDITIONS
? If there is a propositional answer from the
user model resolving a question in the current
plan...
? and if the confidence score reported from the
user model is sufficient, then...
EFFECTS
? accept the propositional answer (include it
into the shared commitments), and...
? give appropriate feedback to the user depend-
ing on the confidence score:
? High confidence? embedded feedback
? ?Which route do you want to take to
work??.
38
Figure 1: A sample dataset. The horizontal axis shows days of the week (0=Monday, ..., 6=Sunday)
and the vertical axis shows hour of the day. Data points show destinations of trips initiated at the time
indicated by their position. (?Now? is the current time, in this case Thursday at lunchtime.)
? The user can always reject the prediction
by requesting another destination.
? Medium confidence? positive feedback
? ?I assume you?re going to work?.
? If the user says ?no?, the answer is re-
jected
? Silence is interpreted as acceptance.
? Low confidence? interrogative feedback
? ?To work, is that correct??
? In this case, the user needs to explicitly
accept the proposed answer.
? Otherwise, the user is prompted for an
answer.
3.3 GUI output
If the ISU rule above does not apply because of
too low confidence scores, user model informa-
tion is still used in the GUI. When a Wh-question
is raised by the system, the GUI always presents a
list of possible alternatives. High-confidence alter-
natives are highlighted and sorted before the other
alternatives in the list.
4 Resulting behaviour
The demonstrator enables interaction with a learn-
ing dialogue system which uses predictions to sim-
plify interactions. Here is an sample interaction:
User: Traffic information
Car: Ok. What road?
User: E6.
Car: Showing traffic on the E6
If this is repeated on a number of occasions,
eventually the system will use a prediction:
User: Traffic information
Car: Showing traffic on the E6
The system thus reduces the need for repetitive
and information-scarce utterances from the user.
As soon as the system has started identifying a pat-
tern, it will start to suggest the most probable al-
ternatives. Initially, the most probable answers are
presented to the user as the top items in a list. The
alternatives are also marked in a different color to
make them more visible to the user (not shown
here).
User: Traffic information
Car: Ok. What road?
Car GUI: [E6] [E45] [E20] [155]
User: E6.
Car: Showing traffic on the E6
39
After some further use, the system has identi-
fied a pattern which is prominent enough for the
system to make a suggestion:
User: Traffic information
Car: E6, is that right?
User: Yes.
Car: Showing traffic on the E6
After getting further support for its hypothesis,
the system will merely inform the user that an as-
sumption has been made. If the user is satisfied
with the assumption, she does not need to do any-
thing, but can correct or confirm it if desired.
User: Traffic information
Car: I assume E6.
User: [silence]
Car: Showing traffic on the E6
User: Traffic information
Car: I assume E6.
User: No, E45.
Car: Showing traffic on the E45
If the user rejects the system suggestion with-
out giving another answer, the system will show
a menu where the most probable choices are the
topmost ones, and marked in a distinct colour (not
shown here).
User: Traffic information
Car: I assume E6.
User: No.
Car: What road?
Car GUI: [E6] [E45] [E20] [155]
When the system is certain about its hypothe-
sis, the system will simply provide the user with
the desired information without asking the user for
parameters.
User: Traffic information
Car: Showing traffic on the E6
5 Conclusions and further work
We have designed and implemented a mechanism
which learns user patterns and uses them proac-
tively to simplify and shorten dialogue interac-
tions. The idea of learning user patterns from ob-
servations is similar to Google Now. However,
while Google Now uses ?cards? to provide un-
requested information to the user, we show how
predictions can be integrated into spoken or multi-
modal dialogue.
It remains for future work to evaluate the sys-
tem to establish that this actually reduces the dis-
traction rate of drivers. We also want to test the
performance of the learning mechanism by train-
ing it on real observations of user behaviours (as
opposed to simulated data).
The current mechanism only predicts answers
to individual system questions, which may result
in suboptimal behaviour in cases where there are
dependencies between the questions pertaining to
some task. An interesting area for future work is
to instead predict sequences of answers; however,
this would require a more powerful learning and
classification mechanisms.
Acknowledgements
This work was carried out within the FFI project
?Safe Speech by Knowledge? (2012-00941),
funded by VINNOVA, Volvo Car Corporation and
Talkamatic.
References
H. H. Clark and S. E. Brennan. 1990. Grounding
in communication. In L. B. Resnick, J. Levine,
and S. D. Behrend, editors, Perspectives on Socially
Shared Cognition, pages 127 ? 149. APA.
Dagmar Kern and Albrecht Schmidt. 2009. Design
space for driver-based automotive user interfaces. In
Proceedings of the 1st International Conference on
Automotive User Interfaces and Interactive Vehic-
ular Applications, AutomotiveUI ?09, pages 3?10,
New York, NY, USA. ACM.
Staffan Larsson, Alexander Berman, and Jessica
Villing. 2011. Adding a speech cursor to a mul-
timodal dialogue system. In INTERSPEECH 2011,
12th Annual Conference of the International Speech
Communication Association, Florence, Italy, 2011,
pages 3319?3320.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, G?oteborg University.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Vicki L. Neale, Thomas A. Dingus, Sheila G. Klauer,
Jeremy Sudweeks, and Michael Goodman. 2005.
An overview of the 100-car naturalistic study and
findings. Technical report.
40
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 92?96,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Multimodal Menu-based Dialogue with Speech Cursor in DICO II+
Staffan Larsson
University of Gothenburg
Sweden
sl@ling.gu.se
Alexander Berman
Talkamatic AB
Sweden
alex@talkamatic.se
Jessica Villing
University of Gothenburg
Sweden
jessica@ling.gu.se
Abstract
This paper describes Dico II+, an in-vehicle
dialogue system demonstrating a novel com-
bination of flexible multimodal menu-based
dialogueand a ?speech cursor? which enables
menu navigation as well as browsing long list
using haptic input and spoken output.
1 Introduction
Dico is a multimodal in-car dialogue system appli-
cation, originally developed in the DICO (with cap-
ital letters) project by Volvo Technology AB and
the University of Gothenburg. Dico is built on top
of the GoDiS dialogue system platform (Larsson,
2002), which in turn is implemented using TrindiKit
(Traum and Larsson, 2003).
The main goal of the original Dico application
(Olsson and Villing, 2005), (Villing and Larsson,
2006) was to develop an interface that is less dis-
tracting for the driver, and thus both safer and easier
to use than existing interfaces. (Larsson and Villing,
2009) described the Dico II system resulting from
work in the DICO project. Since then, the Dico
demonstrator has been further developed.
In this paper, we describe the Dico II+ demon-
strator which introduces a novel combination of
flexible Multimodal Menu-Based Dialogue and a
Speech Cursor which together enable flexible multi-
modal interaction without the need for looking at the
screen. In the following, we will first argue for the
usefulness of in-vehicle dialogue systems. We will
then briefly present the GoDiS platform which Dico
II+ is based on, as well as some aspects of flexible
dialogue enabled by the GoDiS dialogue manager.
2 In-vehicle dialogue systems
Voice interaction is a very natural means of com-
munication for humans, and enabling spoken inter-
action with technologies may thus make it easier
and less cognitively demanding for people to in-
teract with machines. However, this requires that
the spoken interaction is similar to ordinary spoken
human-human dialogue. A problem with available
in-vehicle voice control technologies is that they are
not flexible enough in terms of the interaction strate-
gies and modalities offered to the user.
3 GoDiS features in Dico
GoDiS (Larsson, 2002) is an experimental di-
alogue system implementing a theory of Issue-
Based Dialogue Management based on Ginzburg?s
concept of Questions Under Discussion (QUD).
GoDiS is implemented using the TrindiKit, a toolkit
for implementing dialogue move engines and dia-
logue systems based on the Information State ap-
proach (Traum and Larsson, 2003). GoDiS has
been adapted to several different dialogue types, do-
mains, and languages, including menu-based mul-
timodal dialogue when acting as an interface to an
mp3 player (Hjelm et al, 2005).
The GoDiS dialogue manager allows the user
to interact more flexibly and naturally with menu-
based interfaces to devices. General dialogue man-
agement issues such as accommodation, task switch-
ing and grounding are handled by the application-
independent dialogue manager. Re-using these tech-
nologies in new applications enables rapid prototyp-
ing of advanced dialogue applications.
92
3.1 Accommodation
The applications in Dico II+ are based on exist-
ing menu interfaces, using a method for converting
menus into dialogue plans (see below). While it is
possible for the novice user to let the system take
initiative and guide the user through the menus step-
by-step, expert users might prefer to use accommo-
dation strategies enabling the user to skip through
the menus and getting right to the point. Two exam-
ples are shown below.
Ex. 1:
S: ?What do you want to do? Go to the
phonebook, manage settings, or...?
U: ?Call Lisa?s home number?
S: ?OK. Calling Lisa?s home number.?
Ex. 2:
U: ?Lisa?s home number?
S: ?OK. Do you want to change the num-
ber or make a call??
U: ?Make a call.?
S: ?OK. Calling Lisa?s home number.?
3.2 Multiple simultaneous tasks and task
switching
GoDiS enables arbitrarily nested subdialogues. It
is possible to start one dialogue to perform a task,
and then start a subdialogue before the first task is
completed. When the second task is completed the
system automatically returns to the first task, and ex-
plicitly signals this. This gives the user freedom to
switch task at any time:
Ex. 3:
U: ?Change Lisa?s home number.?
S: ?Okay. Let?s see. What phone number
do you want instead??
U: ?Check my messages.?
S: ?You have got this message. Hi!
I have got a new home number, it is
(031)234567. Best regards, Lisa.?
S: ?Returning to change an entry. What
phone number do you want instead??
U: ?oh three one twentythree fourtyfive
sixtyseven.?
U: ?Okay. Changing Lisa?s home num-
ber to oh three one two three four five six
seven.?
3.3 Feedback and grounding
The GoDiS dialogue manager provides general feed-
back strategies to make sure that the dialogue part-
ners have contact, that the system can can hear what
the user says, understands the words that are spoken
(semantic understanding), understands the meaning
of the utterance (pragmatic understanding) and ac-
cepts the dialogue moves performed in utterances.
As an example, the single user utterance ?Lisa?
may result in positive feedback on the semantic level
but negative on the pragmatic, resulting in a system
utterance consisting of two feedback moves and a
clarification question: ?Lisa. I don?t quite under-
stand. Do you want to make a call, change an entry
in the phonebook, or delete an entry from the phone-
book??
4 Multimodal menu-based dialogue
Dico II+ implemented a concept of Multimodal
Menu-based Dialogue (MMD). Technologies for
MMD in menu-based applications have already been
developed for other GoDiS applications (Hjelm et
al., 2005) and the ideas behind these solutions were
re-implemented and significantly improved in Dico.
A common argument for using spoken interaction
in a car context is that the driver should be able to
use a system without looking at a screen. However,
there are many situations where current technology
requires the user to look at a screen at some point
in the interaction. The idea behind MMD is that the
user should be able to switch between and combine
modalities freely across and within utterances. This
makes it possible to use the system using speech
only, using traditional GUI interaction only, or us-
ing a combination of the two.
MMD enables integrated multimodality for user
input, meaning that a single contribution can use
several input modalities, e.g. ?Call this contact
[click]? where the [click] symbolises haptic input
(e.g. a mouse click) which in this case selects a spe-
cific contact. For output, MMD uses parallel mul-
93
timodality, i.e., output is generally rendered both
as speech and as GUI output. To use speech only,
the user can merely ignore the graphical output and
not use the haptic input device. To enable interac-
tion using GUI only, speech input and output can be
turned on or off using a button which toggles be-
tween ?speech on? and ?speech off? mode.
The GUI used in Dico II+ is a generic graphi-
cal interface for the GoDiS system, developed by
Talkamatic AB with graphical adaptations for Dico.
It represents GoDiS dialogue moves graphically as
menus using a refined version of the conversion
schema presented in (Larsson et al, 2001) . For
example, alternative questions are represented as
multiple choice menus, and wh-questions are rep-
resented as scrollable lists. Conversely, haptic user
input from the GUI is interpreted as dialogue moves.
Selecting an action in a multiple-choice menu cor-
responds to making a request move, and selecting
an item in a scrollable list corresponds to an answer
move.
5 Speech Cursor
This section describes an important addition to the
GoDiS dialogue manager and Dico demonstrator,
which enables the user to use spoken interaction in
combination with haptic input to access all func-
tionality (including browsing long lists) without ever
having to look at the screen. In combination with
the flexible dialogue capabilities of the GoDiS dia-
logue manager, and the concept of MMD, we believe
that a Speech Cursor provides a powerful and user-
friendly way of interacting with menu-based inter-
faces in cognitively demanding environments such
as the in-vehicle environment.
5.1 The problem
A common argument for using spoken interaction
in a car context is that the driver should be able to
use a system without looking at a screen. However,
there are many situations where current technology
requires the user to look at a screen at some point
in the interaction. This was true also for Dico II
in the case of browsing lists; for example, to find
out which contacts were listed in the phonebook, the
user would at some point have to look at the screen.
Imagine that the user wants to select a song from
a song database, and that the user has made restric-
tions filtering out 30 songs from the database. The
dialogue system asks the user which of the songs she
wants to hear displaying them in a list on the screen.
The user must now either look at the screen and
use a scroll-wheel or similar to select a song, or look
at the screen to see which songs are available, and
then speak the proper song title. This means that
part of the point of using spoken interaction in the
car is lost. The example discusses car use, but is
applicable any time when the user cannot or does
not want to look at a screen, for instance when using
a cellphone walking in a city, or when using a web
application on a portable device.
An existing interaction strategy for addressing the
problems of browsing lists is to allow a kind of meta-
dialogue, where the system verbally presents a num-
ber of items (for instance 5) from the list, then asking
the user if she or he would like to hear the subse-
quent 5 items, until the list has been read in its en-
tirety or until the users responds negatively. While
this strategy in principle solves the problem, it is
rather time-consuming compared to browsing the
list using a screen and a haptic input device (such
as a scroll-wheel); this may decrease the perceived
usability of the voice interface in comparison with
traditional GUI-based interaction.
Some existing voice interaction systems use a
technology to establish understanding which con-
sists of displaying the top N best recognition hy-
potheses to the user, each one associated with a num-
ber, together with a verbal request to the user to
say the number corresponding to the desired result.
This situation, however, requires the user to look at
a screen, and is arguably quite unnatural.
5.2 The solution: Speech Cursor
Dico II+ requires a haptic menu navigation de-
vice, such as a mouse (trackball, touch pad,
TrackPointTM ) with buttons, pointers and drivers,
keyboard with arrow keys, or jog dial/shuttle wheel.
A typical in-vehicle menu navigation device consists
of three or four buttons (UP, DOWN, OK and possi-
ble BACK).
Every time a new item gets focus, the system
reads out a voice icon - a spoken representation of
the item. This representation can be textual, in-
tended to be realised using a TTS, or in the form
94
of audio data, to be played directly. Every time a
new element gets focus, all any ongoing voice out-
put is interrupted by the voice icon for the element
in focus.
6 Combining flexible dialogue and Speech
Cursor
Combining flexible dialogue processing with the
Speech Cursor concept1 enables interaction where
the user may freely choose between using domain-
level spoken utterances (requests, confirmations,
questions, answers etc.) and using the Speech Cur-
sor. This is an improvement over existing technol-
ogy in that it offers a greater variety of interaction
styles which can be freely chosen and combined by
the user, as well as offering flexible dialogue inter-
action. Specifically, it provides flexible spoken dia-
logue interaction in combination with user-friendly
browsing of long lists without the need for looking
at the screen.
Here is a walk-through of a sample interaction:
Ex. 4:
U: ?I want to listen to Madonna?(This ut-
terance uses accommodation to allow the
user to supply unrequested information)
S: ?There are 3 songs by Madonna. What
song do you want?? (Showing list of all
songs by Madonna: ?Like a Prayer?, ?La
Isla Bonita?, ?Music?, ...)
U: [DOWN]
S: ?Like a Prayer from the album Like a
Prayer? (?Like a Prayer? is highlighted)
U: [DOWN]
S: ?La Isla Bonita from the album True
Blue? (?La Isla Bonita? is highlighted)
U: [DOWN]
S: ?Music from the alb+? (?Music? is
highlighted)
U: [UP]
S: ?Like a Prayer?
1The combination of a (simple or flexible) spoken dialogue
system and the voice cursor technology is patent pending.
U: [OK]
S: ?OK, playing Like a Prayer.?
Here is a further example:
Ex. 5:
U: ?I want to add an ABBA song?
S: ?What album?? (Shows ?Waterloo?
and ?Arrival)
U: [DOWN]
S: ?Wat...?
U: [DOWN]
S: ?Arrival?
U: [OK]
S: ?what song?? (Shows ?Mamma Mia?
and ?Money Money Money?)
U: ?Mamma Mia?
A promotional demo film showing interactions
similar to the above is available2, courtesy of Talka-
matic AB.
Acknowledgments
The work reported here was funded DICO, Vinnova
project P28536-1. Additional funding was provided
by Talkamatic AB, and by CLT, the Centre of Lan-
guage Technology at the University of Gothenburg.
Dico II+ was implemented by the authors, Fredrik
Kronlid, Peter Ljunglo?f and Andreas Wiberg. The
authors gratefully acknowledge the assistance of
Volvo Technology AB and the DICO project group.
The GoDiS system is the property of Talkamatic
AB.
References
David Hjelm, Ann-Charlotte Forslund, Staffan Larsson,
and Andreas Wallentin. 2005. DJ GoDiS: Multimodal
menu-based dialogue in an asychronous isu system. In
Claire Gardent and Bertrand Gaiffe, editors, Proceed-
ings of the ninth workshop on the semantics and prag-
matics of dialogue.
2www.youtube.com/watch?v=yvLcQOeBAJE
95
Staffan Larsson and Jessica Villing. 2009. Multimodal
menu-based dialogue in dico ii. In Jens Edlund,
Joakim Gustafson, Anna Hjalmarsson, and Gabriel
Skantze, editors, Proceedings of DiaHolmia, 2009
Workshop on the Semantics and Pragmatics of Dia-
logue.
Staffan Larsson, Robin Cooper, and Stina Ericsson.
2001. menu2dialog. In Proceedings of the 2nd IJCAI
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, pages 41?45.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, Go?teborg University.
Anna Olsson and Jessica Villing. 2005. Dico - a dialogue
system for a cell phone. Master?s thesis, Department
of Linguistics, Goteborg University.
David Traum and Staffan Larsson. 2003. The informa-
tion state approach to dialogue management. In Ron-
nie Smith and Jan Kuppevelt, editors, Current and New
Directions in Discourse & Dialogue. Kluwer Aca-
demic Publishers.
Jessica Villing and Staffan Larsson. 2006. Dico - a mul-
timodal in-vehicle dialogue system. In D. Schlangen
and R. Fernandez, editors, Proceedings of the 10th
workshop on the semantics and pragmatics of dia-
logue.
96
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 151?159,
Dublin, Ireland, August 23-24 2014.
Vagueness and Learning: A Type-Theoretic Approach
Raquel Fern
?
andez
Institute for Logic, Language
and Computation
University of Amsterdam
raquel.fernandez@uva.nl
Staffan Larsson
Department of Philosophy, Linguistics
and Theory of Science
University of Gothenburg
sl@ling.gu.se
Abstract
We present a formal account of the mean-
ing of vague scalar adjectives such as ?tall?
formulated in Type Theory with Records.
Our approach makes precise how percep-
tual information can be integrated into
the meaning representation of these pred-
icates; how an agent evaluates whether an
entity counts as tall; and how the proposed
semantics can be learned and dynamically
updated through experience.
1 Introduction
Traditional semantic theories such as those de-
scribed in Partee (1989) and Blackburn and
Bos (2005) offer precise accounts of the truth-
conditional content of linguistic expressions, but
do not deal with the connection between meaning,
perception and learning. One can argue, however,
that part of getting to know the meaning of lin-
guistic expressions consists in learning to identify
the individuals or the situations that the expres-
sions can describe. For many concrete words and
phrases, this identification relies on perceptual in-
formation. In this paper, we focus on characteris-
ing the meaning of vague scalar adjectives such
as ?tall?, ?dark?, or ?heavy?. We propose a for-
mal account that brings together notions from tra-
ditional formal semanticswith perceptual informa-
tion, which allows us to specify how a logic-based
interpretation function is determined and modified
dynamically by experience.
The need to integrate language and percep-
tion has been emphasised by researchers work-
ing on the generation and resolution of referring
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
expressions (Kelleher et al., 2005; Reiter et al.,
2005; Portet et al., 2009) and, perhaps even more
strongly, on the field of robotics, where ground-
ing language on perceptual information is critical
to allow artificial agents to autonomously acquire
and verify beliefs about the world (Siskind, 2001;
Steels, 2003; Roy, 2005; Skocaj et al., 2010).
Most of these approaches, however, do not build
on theories of formal semantics for natural lan-
guage. Here we choose to formalise our account
in a theoretical framework known as Type Theory
with Records (TTR), which has been shown to be
suitable for formalising classic semantic aspects
such as intensionality, quantification, and nega-
tion (Cooper, 2005a; Cooper, 2010; Cooper and
Ginzburg, 2011) as well as less standard phenom-
ena such as linguistic interaction (Ginzburg, 2012;
Purver et al., 2014), perception and action (Dob-
nik et al., 2013), and semantic coordination and
learning (Larsson, 2009). In this paper we use
TTR to put forward an account of the semantics of
vague scalar predicates like ?tall? that makes pre-
cise how perceptual information can be integrated
into their meaning representation; how an agent
evaluates whether an entity counts as tall; and how
the proposed semantics for these expressions can
be learned and dynamically updated through lan-
guage use.
We start by giving a brief overview of TTR and
explaining how it can be used for classifying en-
tities as being of particular types integrating per-
ceptual information. After that, in Section 3, we
describe the main properties of vague scalar pred-
icates. Section 4 presents a probabilistic TTR for-
malisation of the meaning of ?tall?, which captures
its context-dependence and its vague character. In
Section 5, we then offer an account of how that
meaning representation is acquired and updated
with experience. Finally, in Section 6 we discuss
related work, before concluding in Section 7.
151
2 Meaning as Classification in TTR
In this section we give a brief and hence inevitably
partial introduction to Type Theory with Records.
For more comprehensive introductions, we refer
the reader to Cooper (2005b) and Cooper (2012).
2.1 Type Theory with Records: Main Notions
As in any type theory, the most central notion in
TTR is that of a judgement that an object a is
of type T , written as a : T . In TTR judgements
are seen as fundamentally related to perception, in
the sense that perceiving inherently involves cate-
gorising what we perceive. Some common basic
types in TTR are Ind (the type of individuals) and
R
+
(the type of positive real numbers). All basic
types are members of a special type Type. Given
types T
1
and T
2
, we can create the function type
T
1
? T2 whose domain are objects of type T
1
and whose range are objects of type T
2
. Types
can also be constructed from predicates and ob-
jects P (a
1
, . . . , a
n
). Such types are called ptypes
and correspond roughly to propositions in first or-
der logic. In TTR, propositions are types of proofs,
where proofs can be a variety of things, from situ-
ations to sensor readings (more on this below).
Next, we introduce records and record types.
These are structured objects made up of pairs ?l, v?
of labels and values that are displayed in a matrix:
(1) a. A record type:
?
?
?
?
`
1
: T
1
`
2
: T
2
(`
1
)
. . .
`
n
: T
n
(`
1
, `
2
, . . . , `
n?1
)
?
?
?
?
b. A record: r =
?
?
?
?
?
`
1
= a
1
`
2
= a
2
. . .
`
n
= a
n
. . .
?
?
?
?
?
Record r in (1b) is of the record type in (1a) if
and only if a
1
: T
1
, a
2
: T
2
(a
1
), . . . , and a
n
:
T
n
(a
1
, a
2
, . . . , a
n?1
). Note that the record may
contain more fields but would still be of type (1a)
if the typing condition holds. Records and record
types can be nested so that the value of a label is
itself a record (or record type). We can use paths
within a record or record type to refer to specific
bits of structure: for instance, we can use r.`
2
to
refer to a
2
in (1b).
As can be seen in (1a), the labels `
1
, . . . `
n
in a
record type can be used elsewhere to refer to the
values associated with them. This is a common
way of constructing ptypes where the arguments
of a predicate are entities that have been intro-
duced before in the record type. A sample record
and record type are shown in (2).
(2)
?
?
x = a
c
man
= prf(man(a))
c
run
= prf(run(a))
?
?
:
?
?
x : Ind
c
man
: man(x)
c
run
: run(x)
?
?
In (2), a is an entity of type individual and prf(P )
is used as a placeholder for proofs of ptypes P .
In the record type above, the ptypes man(x) and
run(x) constructed from predicates are dependent
on x (introduced earlier in the record type).
2.2 Perceptual Meaning
Larsson (2013) proposes a system formalised in
TTR where some perceptual aspects of meaning
are represented using classifiers. For example, the
meaning of ?right? (as in ?to the right of ?) involves
a two-input perceptron classifier ?
right
(w, t, r),
specified by a weight vector w and a threshold
t, which takes as input a context r including an
object x and a position-sensor reading sr
pos
. The
sensor reading consists of a vector containing two
real numbers representing the space coordinates of
x. The classifier classifies x as either being to the
right on a plane or not.
1
(3) if r :
[
x : Ind
sr
pos
: RealVector
]
, then
?
right
(w, t, r) =
{
right(r.x) if (r.sr
pos
? w) > t
? right(r.x) otherwise
As output we get a record type containing either a
ptype right(x) or its negation, ? right(x). Larsson
(2013) proposes that readings from sensors may
count as proofs of such ptypes. A classifier can
be used for judging x as being of a particular type
on the grounds of perceptual information. A per-
ceptual proof for right(x) would thus include the
output from the position sensor that is directed to-
wards x. Here, this output would be the space co-
ordinates of x.
3 Vague Scalar Predicates
Scalar predicates such as ?tall?, ?long? and ?ex-
pensive?, also called ?relative gradable adjectives?
(Kennedy, 2007), are interpreted with respect to a
1
We are here assuming that we have a definition of dot
product for TTR vectors a:RealVector
n
and b:RealVector
n
such that a ? b = ?
n
i=1
a
i
b
i
= a
1
b
1
+ a
2
b
2
+ . . .+ a
n
b
n
. We
also implicitly assume that the weight vector and the sensor
reading vector have the same dimensionality.
152
scale, i.e., a dimension such as height, length, or
cost along which entities for which the relevant di-
mension is applicable can be ordered. This makes
scalar predicates compatible with degree morphol-
ogy, like comparative and superlative morphemes
(?taller than?, ?the longest?) and intensifier mor-
phemes such as ?very? or ?quite?. In this pa-
per, our focus is on the so-called positive form of
these adjectives (e.g. ?tall? as opposed to ?taller?
or ?tallest?).
A property that distinguishes the positive form
from the comparative and the superlative forms is
its context-dependance. To take a common exam-
ple: If Sue?s height is 180cm, she may be appro-
priately described as a tall woman, but probably
not as a tall basketball player. Thus, what counts
as tall can vary from context to context, with the
most relevant contextual parameter being a com-
parison class relative to which the adjective is in-
terpreted (e.g., the set of women, the set of bas-
ketball players, etc.). In addition to being context-
dependent, positive-form scalar predicates are also
vague, in the sense that they give rise to borderline
cases, i.e., entities for which it is unclear whether
the predicate holds or not.
Vagueness is certainly a property that affects
most natural language expressions, not only scalar
adjectives. However, scalar adjectives have a
relatively simple semantics (they are often uni-
dimensional) and thus constitute a perfect case-
study for investigating the properties and effects of
vagueness on language use. Gradable adjectives
have received a high amount of attention in the
formal semantics literature. It is common to dis-
tinguish between two main approaches to their se-
mantics: delineation-based and degree-based ap-
proaches. The delineation approach is associated
with the work of Klein (1980), who proposes that
gradable adjectives denote partial functions de-
pendent on a comparison class. They partition the
comparison class into three disjoint sets: a positive
extension, a negative extension, and an extension
gap (entities for which the predicate is neither true
nor false). In contrast, degree-based approaches
assume a measure function m mapping individu-
als x to degrees on a particular scale (degrees of
height, degrees of darkness, etc.) and a standard
of comparison or degree threshold ? (again, de-
pendent on a comparison class) such that x be-
longs to the adjective?s denotation if m(x) > ?
(Kamp, 1975; Pinkal, 1979; Pinkal, 1995; Barker,
2002; Kennedy and McNally, 2005; Kennedy,
2007; Solt, 2011; Lassiter, 2011).
We build on degree approaches but adopt a
perception-based perspective and take a step fur-
ther to formalise how the meaning of these pred-
icates can be learned and constantly updated
through language use.
4 A Perceptual Semantics for ?Tall?
To exemplify our approach, we will use the scalar
predicate ?tall? throughout.
4.1 Context-sensitivity
We first focus on capturing the context-
dependence of relative scalar predicates. For
this we define a type T
ctxt
as follows:
(4) T
ctxt
=
?
?
c : Type
x : c
h : R
+
?
?
The context (ctxt) of a scalar predicate like ?tall?
is a record of the type in (4), which includes: a
type c (typically a subtype of Ind) representing the
comparison class; an individual x within the com-
parison class (the argument of tall); a perceived
measure on the relevant scale(s), in this case the
perceived height h of x expressed as a positive real
number.
The context presupposes the acquisition of sen-
sory input from the environment. In particular, it
assumes that an agent using such a representation
is able to classify the entity in focus x as being
of type c and is able to use some height sensor to
obtain an estimate of x?s height (the value of h is
the sensor reading). We thus forgo the inclusion of
an abstract measure function in the representation.
In an artificial agent, this may be accomplished by
image processing software for detecting and mea-
suring objects in a digital image.
Besides the ctxt, we also assume a standard
threshold of tallness ?
tall
of the type given in (5).
?
tall
is a function from a type specifying a com-
parison class to a height value, which corresponds
to a tallness threshold for that comparison class.
(In Section 5 we will discuss how such a threshold
may be computed.)
(5) ?
tall
: Type? R
+
The meaning of ?tall? involves a classifier for tall-
ness, ?
tall
, of the following type:
(6) ?
tall
: (Type? R
+
, T
ctxt
)? Type
153
We define this classifier as a one-input perceptron
that compares the perceived height h of an indi-
vidual x to the relevant threshold ? determined by
a comparison class c. Thus, if ? : Type? R
+
and
r : T
ctxt
, then:
?
tall
(?, r) =
{
tall(r.x) if r.h > ?(r.c)
?tall(r.x) otherwise
Simplifying somewhat, we can represent the mea-
ning of ?tall?, tall, as a record specifying the type
of context (T
ctxt
) where an utterance of ?tall? can
be made, the parameter of the tallness classifier
(the threshold ?), and a function f which is applied
to the context to produce the content of ?tall?.
(7)
tall =
?
?
?
?
?
?
?
?
?
?
T
ctxt
=
?
?
c : Type
x : c
h : R
+
?
?
? = ?
tall
f = ?r : T
ctxt
.
[
sit = r
sit-type =
[
c
tall
: ?
tall
(?, r)
]
]
?
?
?
?
?
?
?
?
?
?
The output of the function f is an Austinian propo-
sition (Cooper, 2005b): a judgement that a situa-
tion (sit, represented as a record r of type T
ctxt
),
is of a particular type (specified in sit-type). In the
case of tall, the context of utterance (which instan-
tiates r) is judged to be of the type where there is
an individual x which is either tall or not tall, ac-
cording to the output of the classifier ?
tall
. The
context of utterance in the sit field will include the
height-sensor reading, which means that the sen-
sor reading is part of the proof of the sit-type indi-
cating that x is tall (or not, as the case may be).
Thus, to decide whether to refer to some indi-
vidual x as tall or to evaluate someone else?s utter-
ance describing x as tall, an agent applies the func-
tion tall.f to the current situation, represented as a
record r : T
ctxt
. As an example, let us consider a
situation that includes the context in (8), resulting
from observing John Smith as being 1.88 meters
tall (assuming this is our scale of tallness):
(8) ctxt =
?
?
c = Human
x = john smith
h = 1.88
?
?
Let us assume that given the comparison class
Human, ?
tall
(Human) = 1.87. In this case,
tall.f(ctxt) will compute as shown in (9). The re-
sulting Austinian proposition corresponds to the
agent?s judgement that the situation in sit is one
where John Smith counts as tall.
(9) ?r : T
ctxt
.
[
sit = r
sit-type =
[
c
tall
: ?
tall
(?
tall
, r)
]
]
(
?
?
c = Human
x = john smith
h = 1.88
?
?
) =
?
?
?
?
sit =
?
?
c = Human
x = john smith
h = 1.88
?
?
sit-type =
[
c
tall
: tall(john smith)
]
?
?
?
?
4.2 Vagueness
According to the above account, ?tall? has a
precise interpretation: given a degree of height
and a comparison class, the threshold sharply
determines whether tall applies or not. There
are several ways in which one can account for
vagueness?amongst others, by introducing per-
ceptual uncertainty (possibly inaccurate sensor
readings). Here, in line with Lassiter (2011), we
opt for substituting the precise threshold with a
noisy, probabilistic threshold. We consider the
threshold to be a normal random variable, which
can be represented by the parameters of its Gaus-
sian distribution, the mean ? and the standard de-
viation ? (the noise width).
2
To incorporate this modification into our ap-
proach, we update the tallness classifier ?
tall
we
had defined in (6) so that it now takes as parame-
ters ?
tall
and ?
tall
, both of them dependent on the
comparison class and hence of type Type? R
+
.
The output of the classifier is now a probability
rather than a ptype such as tall(x) or?tall(x). Be-
fore indicating how this probability is computed,
we give the type of the vague version of the clas-
sifier in (10) and the vague representation of the
meaning of ?tall? in (11).
(10)?
tall
: (Type?R
+
, Type?R
+
, T
ctxt
)? [0, 1]
(11)
tall =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
T
ctxt
=
?
?
c : Type
x : c
h : R
+
?
?
? = ?
tall
? = ?
tall
f = ?r : T
ctxt
.
?
?
sit = r
sit-type =
[
c
tall
: tall(r.x)
]
prob = ?
tall
(?, ?, r)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
2
Which noise function may be the most appropriate is an
empirical question we do not tackle in this paper. Our choice
of Gaussian noise follows Schmidt et al. (2009)?see Sec-
tion 5.1.
154
The output of the function tall.f is now a prob-
abilistic Austinian proposition (Cooper et al.,
2014). Like before, the proposition expresses a
judgement that a situation sit is of a particular
type. But here the judgement is probabilistic?it
encodes the belief of an agent concerning the like-
lihood that sit is of a type where x counts as tall.
Since we take the noisy threshold to be a normal
random variable, given a particular ? and ?, we
can calculate the probability that the height r.h of
individual r.x counts as tall as follows:
?
tall
(?, ?, r) =
1
2
[
1 + erf
(
r.h? ?(r.c)
?(r.c)
?
2
)]
Here erf is the error function, defined as
3
erf(x) =
2
?
pi
?
x
t=0
e
?t
2
dt
The error function defines a sigmoid shape (see
Figure 1), in line with the upward monotonicity
of ?tall?. The output of ?
tall
(?, ?, r) corresponds
to the probability that h will exceed the normal
random threshold with mean ? and deviation ?.
Figure 1: Plot of the error function.
Let us consider an example. Assume that we have
?
tall
(Human) = 1.87 and ?
tall
(Human) = 0.05
(see Section 5.1 below for justification of the latter
value). Let?s also assume the same ctxt as above
in (8). In this case, tall.f(ctxt) will compute as in
(12), given that
?
tall
(?
tall
, ?
tall
,
?
?
c=Human
x=john smith
h=1.88
?
?
) =
1
2
[
1 + erf
(
1.88? 1.87
0.05
?
2
)]
= 0.579
3
For an explanation of this standard definition, see http:
//en.wikipedia.org/wiki/Error_function,
which is the source of the graph in Figure 1.
(12) ?r : T
ctxt
.
?
?
sit = r
sit-type =
[
c
tall
: tall(r.x)
]
prob = ?
tall
(?
tall
, ?
tall
, r)
?
?
(
?
?
c = Human
x = john smith
h = 1.88
?
?
) =
?
?
?
?
?
?
sit =
?
?
c = Human
x = john smith
h = 1.88
?
?
sit-type =
[
c
tall
: tall(john smith)
]
prob = 0.579
?
?
?
?
?
?
This probability can now be used in further prob-
abilistic reasoning, to decide whether to refer to
an individual x as tall, or to evaluate someone
else?s utterance describing x is tall. For exam-
ple, an agent may map different probabilities to
different adjective qualifiers of tallness to yield
compositional phrases such as ?sort of tall?, ?quite
tall?, ?very tall?, ?extremely tall?, etc. The mean-
ings of these composed adjectival phrases could
specify probability ranges trained independently.
Compositionality for vague perceptual meanings,
and the interaction between compositionality and
learning, is an exciting area for future research.
4
5 Learning from Language Use
In this section we consider possibilities for com-
puting the noisy threshold we have introduced
in the previous section and discuss how such a
threshold and the probabilistic judgements it gives
rise to are updated with language use.
5.1 Computing the Noisy Threshold
We assume that agents keep track of judgements
made by other agents. More concretely, for a
vague scalar predicate like ?tall?, we assume that
an agent will have at its disposal a set of obser-
vations consisting of entities of a particular type
T (a comparison class such as Human) that have
been judged to be tall, together with their observed
heights. Judgements of tallness may vary across
individuals?indeed, such variation (both inter-
and intra-individual) is a hallmark of vague pred-
icates. We use ?
T
tall
to refer to the set of heights
of those entities x : T that have been considered
tall by some individual. From this agent-specific
set of observations, which is constantly updated as
the agent is exposed to new judgements by other
individuals, we want to compute a noisy threshold,
4
See Larsson (2013) for a sketch of compositionality for
perceptual meaning.
155
which the agent uses to make her own judgements
of tallness, as specified in (11).
Different functions can be used to compute ?
tall
and ?
tall
from ?
T
tall
. What constitutes an appro-
priate function is an empirical matter and what
the most suitable function is possibly varies across
predicates (what may apply to ?tall? may not be
suitable for ?dark? or ?expensive?, for example).
Hardly any work has been done on trying to iden-
tify how the threshold is computed from experi-
ence. A notable exception, however, is the work of
Schmidt et al. (2009), who collect judgements of
people asked to indicate which items are tall given
distributions of items of different heights. Schmidt
and colleagues then propose different probabilis-
tic models to account for the data and compare
their output to the human judgements. They ex-
plore two types of models: threshold-based mod-
els and category-based or cluster models. The best
performing models within these two types perform
equally well and the study does not identify any
advantages of one type over the other one. Since
we have chosen threshold models as our case-
study, we focus our attention on those here.
Each of the threshold models tested by Schmidt
et al. (2009) corresponds to a possible way of com-
puting the mean ?
tall
of a noisy threshold from a
set of observations. The best performing threshold
model in their study is the relative height by range
model, where (in our notation):
(13) relative height by range (RH-R): ?
tall
(T ) =
max(?
T
tall
)? k ? (max(?
T
tall
)?min(?
T
tall
))
Here max(?
T
tall
) and min(?
T
tall
) stand for the
maximum and the minimum height, respectively,
of the items that have been judged to be tall
by some individual. According to this threshold
model, any item within the top k% of the range
of heights that have been judged to be tall counts
as tall. The model includes two parameters, k and
a noise-width parameter that in our approach cor-
responds to ?
tall
. Schmidt et al. (2009) report
that the best fit of their data was obtained with
k = 29% and ?
tall
= 0.05.
5.2 Updating Vague Meanings
We now want to specify how the vague meaning
of ?tall? is updated as an agent is exposed to new
judgements via language use. Our setting so far
offers a straightforward solution to this: If a new
entity x : T with height h is referred to as tall, the
agent adds h to its set of observations ?
T
tall
and
recomputes ?
tall
(Human), for instance using RH-
R as defined in (13). If RH-H is used, ideally the
value of k and ?
tall
should be (re)estimated from
?
T
tall
. For the sake of simplicity, however, here
we will assume that these two parameters take the
values experimentally validated by Schmidt et al.
(2009) and are kept constant. An update to ?
tall
will take place if it is the case that h > max(?
T
tall
)
or h < min(?
T
tall
). This in turn will trigger un
update to the probability outputted by ?
tall
.
As an example, let us assume that our
initial set of observations is ?
Human
tall
=
{1.87, 1.92, 1, 90, 1.75, 1.80} (recall this corre-
sponds to the perceived heights of individuals
that have been described as tall by some agent).
This means that max(?
Human
tall
) = 1.92 and
min(?
Human
tall
) = 1.75. Hence, given (13):
(14) ?
tall
(Human) =
1.92? 0.29 ? (1.92? 1.75) = 1.87
Let?s assume we now make an observation where
a person of height 1.72 is judged to be tall. This
will mean that the set of observations is now
?
Human
tall
= {1.87, 1.92, 1, 90, 1.75, 1.80, 1.72}
and consequently min(?
Human
tall
) = 1.72, which
yields an updated mean of the noisy threshold:
(15) ?
tall
(Human) =
1.92? 0.29 ? (1.92? 1.72) = 1.862
If we were to re-evaluate John Smith?s tallness in
light of this observation, we would get a new prob-
ability 0.64 that he is tall (in contrast to the earlier
probability of 0.579 given in (12)).
5.3 Possible Extensions
The set of observations ?
Human
tall
can be derived
from a set of Austinian propositions correspond-
ing to instances where people have been judged
to be tall. To update from an Austinian proposi-
tion p we simply add p.sit.h to ?
tall
Human
and re-
compute ?
tall
(p.c). Note that we are here treating
these Austinian propositions as non-probabilistic.
This seems to make sense since an addressee does
not have direct access to the probability associated
with the judgement of the speaker. If we were to
take these probabilities into account (for instance,
the use of a hedge in ?sort of tall? may be used
to make inferences about such probabilities), and
if those probabilities are not always 1, we would
need a different way of computing ?
tall
than the
156
one specified so far.
Somewhat related to the point above, note that
in our approach we treat all judgements equally,
i.e., we do not distinguish between possible dif-
ferent levels of trustworthiness amongst speakers.
An agent who is told that an entity with height h
is tall adds that observation to its knowledge base
without questioning the reliability of the speaker.
This is clearly a simplification. For instance, there
is developmental evidence showing that children
are more sensitive to reliable speakers than to un-
reliable ones during language acquisition (Scofield
and Behrend, 2008).
6 Other Approaches
Within the literature in formal semantics, Las-
siter (2011) has put forward a proposal that ex-
tends in interesting ways earlier work by Barker
(2002) and shares some aspects with the account
we have presented here. Operating in a probabilis-
tic version of classical possible-worlds semantics,
Lassiter assumes a probability distribution over a
set of possible worlds and a probability distribu-
tion over a set of possible languages. Each pos-
sible language represents a precise interpretation
of a predicate like ?tall?: tall
1
= ?x.x?s height ?
5?6?; tall
2
= ?x.x?s height ? 5?7?; and so forth.
Lassiter thus treats ?metalinguistic belief? (repre-
senting an agent?s knowledge of the meaning of
words) in terms of probability distributions over
precise languages. Since each precise interpreta-
tion of ?tall? includes a given threshold, this can
be seen as defining a probability distribution over
possible thresholds, similarly to the noisy thresh-
old we have used in our account. Lassiter, how-
ever, is not concerned with learning.
Within the computational semantics literature,
DeVault and Stone (2004) describe an imple-
mented system in a drawing domain that is able to
interpret and execute instructions including vague
scalar predicates such as ?Make a small circle?.
Their approach makes use of degree-based seman-
tics, but does not take into account comparison
classes. This is possible in their drawing domain
since the kind of geometric figures it includes
(squares, rectangles, circles) do not have intrinsic
expected properties (size, length, etc). Their focus
is on modelling how the threshold for a predicate
such as ?small? is updated during an interaction
with the system given the local discourse context.
For instance, if the initial context just contains a
square, the size of that square is taken to be the
standard of comparison for the predicate ?small?.
The user?s utterance ?Make a small circle? is then
interpreted as asking for a circle of an arbitrary
size that is smaller than the square.
In our characterisation of the context-sensitivity
of vague gradable adjectives in Section 4.1, we
have focused on their dependence on general com-
parison classes corresponding to types of entities
(such as Human, Woman, etc) with expected prop-
erties such as height. Thus, in contrast to DeVault
and Stone (2004), who focus on the local context
of discourse, we have focused on what could be
called the global context (an agent?s experience re-
garding types of entities and their expected prop-
erties). How these two types of context interact
remains an open question, which we plan to ex-
plore in our future work (see Kyburg and Morreau
(2000), Kemp et al. (2007), and Fern?andez (2009)
for pointers in this direction).
7 Conclusions and future work
Traditional formal semantics theories postulate a
fixed, abstract interpretation function that medi-
ates between natural language expressions and the
world, but fall short of specifying how this func-
tion is determined or modified dynamically by
experience. In this paper we have presented a
characterisation of the semantics of vague scalar
predicates such as ?tall? that clarifies how their
context-dependent meaning and their vague char-
acter are connected with perceptual information,
and we have also shown how this low-level per-
ceptual information (here, real-valued readings
from a height sensor) connects to high level logical
semantics (ptypes) in a probabilistic framework.
In addition, we have put forward a proposal for
explaining how the meaning of vague scalar ad-
jectives like ?tall? is dynamically updated through
language use.
Tallness is a function of a single value (height),
and is in this sense a uni-dimensional pred-
icate. Indeed, most linguistic approaches to
vagueness focus on uni-dimensional predicates
such as ?tall?. However, many vague predicates
are multi-dimensional, including nouns for posi-
tions (?above?), shapes (?hexagonal?), and colours
(?green?), amongst many others. Together with
compositionality (mentioned at the end of Sec-
tion 4.2), generalisation of the present account to
multi-dimensional vague predicates is an interest-
ing area of future development.
157
Acknowledgements
The first author acknowledges the support of the
Netherlands Organisation for Scientific Research
(NWO) and thanks the Centre for Language Tech-
nology at the University of Gothenburg for gen-
erously funding research visits that led to the
work presented in this paper. The second au-
thor acknowledges the support of Vetenskapsr?adet,
project 2009-1569, Semantic analysis of interac-
tion and coordination in dialogue (SAICD); the
Department of Philosophy, Linguistics, and The-
ory of Science; and the Centre for Language Tech-
nology at the University of Gothenburg.
References
Chris Barker. 2002. The dynamics of vagueness. Lin-
guistics & Philosophy, 25(1):1?36.
Patrick Blackburn and Johan Bos. 2005. Represen-
tation and Inference for Natural Language: A First
Course in Computational Semantics. CSLI Publica-
tions.
Robin Cooper and Jonathan Ginzburg. 2011. Negation
in dialogue. In Proceedings of the 15th Workshop on
the Semantics and Pragmatics of Dialogue (SemDial
2011), Los Angeles (USA).
Robin Cooper, Simon Dobnik, Shalom Lappin, and
Staffan Larsson. 2014. A probabilistic rich type
theory for semantic interpretation. In Proceedings
of the EACL Workshop on Type Theory and Natural
Language Semantics (TTNLS).
Robin Cooper. 2005a. Austinian truth, attitudes and
type theory. Research on Language and Computa-
tion, 3(4):333?362, December.
Robin Cooper. 2005b. Austinian truth, attitudes and
type theory. Research on Language and Computa-
tion, 3:333?362.
Robin Cooper. 2010. Generalized quantifiers and clar-
ification content. In Pawe? ?upkowski and Matthew
Purver, editors, Aspects of Semantics and Pragmat-
ics of Dialogue. SemDial 2010, 14th Workshop on
the Semantics and Pragmatics of Dialogue, Pozna?n.
Polish Society for Cognitive Science.
Robin Cooper. 2012. Type theory and semantics in
flux. In Ruth Kempson, Nicholas Asher, and Tim
Fernando, editors, Handbook of the Philosophy of
Science, volume 14: Philosophy of Linguistics. El-
sevier BV. General editors: Dov M. Gabbay, Paul
Thagard and John Woods.
David DeVault and Matthew Stone. 2004. Interpret-
ing vague utterances in context. In Proceedings of
the 20th International Conference on Computational
Linguistics (COLING?04), pages 1247?1253.
Simon Dobnik, Robin Cooper, and Staffan Larsson.
2013. Modelling language, action, and perception
in type theory with records. In Constraint Solving
and Language Processing, Lecture Notes in Com-
puter Science, pages 70?91. Springer.
Raquel Fern?andez. 2009. Salience and feature vari-
ability in definite descriptions with positive-form
vague adjectives. In Workshop on the Production
of Referring Expressions: Bridging the gap between
computational and empirical approaches to refer-
ence (CogSci?09).
Jonathan Ginzburg. 2012. The Interactive Stance. Ox-
ford University Press.
Hans Kamp. 1975. Two theories of adjectives. In
E. Keenan, editor, Formal Semantics of Natural Lan-
guage, pages 123?155. Cambridge University Press.
John Kelleher, Fintan Costello, and Josef van Genabith.
2005. Dynamically structuring, updating and inter-
relating representations of visual and linguistic dis-
course context. Artificial Intelligence, 167(1):62?
102.
Charles Kemp, Amy Perfors, and Joshua B. Tenen-
baum. 2007. Learning overhypotheses with hier-
archical bayesian models. Developmental Science,
10(3):307?321.
Christopher Kennedy and Louise McNally. 2005.
Scale structure, degree modification, and the seman-
tics of gradable predicates. Language, pages 345?
381.
Christopher Kennedy. 2007. Vagueness and grammar:
The semantics of relative and absolute gradable ad-
jectives. Linguistics and Philosophy, 30(1):1?45.
Ewan Klein. 1980. A semantics for positive and
comparative adjectives. Linguistics and Philosophy,
4:1?45.
Alice Kyburg and Michael Morreau. 2000. Fitting
words: Vague language in context. Linguistics and
Philosophy, 23:577?597.
Staffan Larsson. 2009. Detecting and learning from
lexical innovation in dialogue: a ttr account. In
Proceedings of the 5th International Conference on
Generative Approaches to the Lexicon.
Staffan Larsson. 2013. Formal semantics for percep-
tual classification. Journal of Logic and Computa-
tion.
Dan Lassiter. 2011. Vagueness as probabilistic linguis-
tic knowledge. In R. Nowen, R. van Rooij, U. Sauer-
land, and H. C. Schmitz, editors, Vagueness in Com-
munication. Springer.
Barbara Partee. 1989. Possible worlds in model-
theoretic semantics: A linguistic perspective. In
S. Allen, editor, Possible Worlds in Humanities, Arts
and Sciences, pages 93?123. Walter de Gruyter.
158
Manfred Pinkal. 1979. Semantics from different
points of view. In R. B?aurle, U. Egli, and A. von
Stechow, editors, How to Refer with Vague Descrip-
tions, pages 32?50. Springer-Verlag.
Manfred Pinkal. 1995. Logic and lexicon: the seman-
tics of the indefinite, volume 56 of Studies in Lin-
guistics and Philosophy. Springer.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(7):789?816.
Matthew Purver, Julian Hough, and Eleni Gre-
goromichelaki. 2014. Dialogue and compound
contributions. In A. Stent and S. Bangalore, ed-
itors, Natural Language Generation in Interactive
Systems. Cambridge University Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167(1):137?169.
Deb Roy. 2005. Semiotic schemas: A framework for
grounding language in action and perception. Artifi-
cial Intelligence, 167(1):170?205.
L.A. Schmidt, N.D. Goodman, D. Barner, and J.B.
Tenenbaum. 2009. How tall is tall? composition-
ality, statistics, and gradable adjectives. In Proceed-
ings of the 31st annual conference of the cognitive
science society.
Jason Scofield and Douglas A Behrend. 2008. Learn-
ing words from reliable and unreliable speakers.
Cognitive Development, 23(2):278?290.
Jeffrey Mark Siskind. 2001. Grounding the lexical
semantics of verbs in visual perception using force
dynamics and event logic. Journal of Artificial In-
telligence Research, (15):31?90.
Danijel Skocaj, M Janicek, Matej Kristan, Geert-Jan M
Kruijff, Ale?s Leonardis, Pierre Lison, Alen Vrecko,
and Michael Zillich. 2010. A basic cognitive sys-
tem for interactive continuous learning of visual
concepts. In Proceeding of the Workshop on Inter-
active Communication for Autonomous Intelligent
Robots, pages 30?36.
Stephanie Solt. 2011. Notes on the comparison class.
In Vagueness in communication, pages 189?206.
Springer.
Luc Steels. 2003. Evolving grounded communication
for robots. Trends in cognitive sciences, 7(7):308?
312.
159
Proceedings of the SIGDIAL 2013 Conference, pages 354?356,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Integration and test environment for an in-vehicle dialogue system in the
SIMSI project
Staffan Larsson, Sebastian Berlin
University of Gothenburg
Box 200
SE-405 30 Gothenburg
Sweden
sl@ling.gu.se
sebastian.berlin@gu.se
Anders Eliasson
Mecel AB
Box 140 44
SE-400 20 Gothenburg
Sweden
anders.eliasson@mecel.se
Fredrik Kronlid
Talkamatic AB
Fo?rsta la?nggatan 18
SE-413 28 Gothenburg
Sweden
fredrik@talkamatic.se
Abstract
The goal of the SIMSI (Safe In-vehicle
Multimodal Speech Interaction) project is
threefold. Firstly, to integrate a dialogue
system for menu-based dialogue with a
GUI-driven in-vehicle infotainment sys-
tem. Secondly, to further improve the in-
tegrated system with respect to driver dis-
traction, thus making the system safer to
use while driving. Thirdly, to verify that
the resulting system decreases visual dis-
traction and cognitive load during interac-
tion. This demo paper describes the inte-
gration of the two existing systems, and
the test environment designed to enable
evaluation of the system.
1 Background
1.1 Driver distraction and safety
Driver distraction is one common cause of acci-
dents, and is often caused by the driver interact-
ing with technologies such as mobile phones, me-
dia players or navigation systems. The so-called
100-car study (Neale et al, 2005) revealed that
secondary task distraction is the largest cause of
driver inattention, and that the handling of wire-
less devices is the most common secondary task.
The goal of SIMSI is to design systems which en-
able safe interaction with technologies in vehicles,
by reducing the cognitive load imposed by the in-
teraction and minimizing head-down time.
1.2 The Talkamatic Dialogue Manager
Based on Larsson (2002) and later work, Talka-
matic AB has developed the Talkamatic Dialogue
Manager (TDM) with the goal of being the most
competent and usable dialogue manager on the
market, both from the perspective of the user and
from the perspective of the HMI developer. TDM
provides a general interaction model founded in
human interaction patterns, resulting in a high de-
gree of naturalness and flexibility which increases
usability. Also, TDM reduces complexity for de-
velopers and users, helping them to reach their
goals faster and at a lower cost.
A major problem with the current state-of-the-
art in-vehicle spoken dialogue systems is that they
are either too simplistic to be useful to the end
user, or alternatively that they are fairly sophisti-
cated but unmanageable for the manufacturer due
to the size and complexity of the implementation.
TDM offers sophisticated multi-modal interaction
management solutions which allow for easy modi-
fication and development, allowing interaction de-
signers to easily explore new solutions and re-
ducing overhead for new dialogue applications in
terms of code and development man-hours.
TDM deals with several interaction patterns
which are basic to human-human linguistic in-
teraction, and offers truly integrated multimodal-
ity which allows user to freely switch between
(or combine) modalities. All these solutions are
domain-independent which means that they need
not be implemented in each application. Using
Talkamatic technology, dialogue behaviour can be
altered without touching application properties,
and application properties can be updated without
touching the dialogue logic. This makes testing of
different dialogue strategies, prompts etc. consid-
erably quicker and easier than when using regular
state-machine-based dialogue systems.
In addition, as the dialogue strategy is separated
from the application logic, development time for
new dialogue applications can be significantly re-
duced. Furthermore, the developer designing the
application does not need to be a dialogue expert
as the dialogue design is built into the dialogue
manager.
354
1.3 Integrated multimodality in TDM
There are reasons to believe that multi-modal in-
teraction is more efficient and less distracting than
uni-modal interaction (Oviatt et al, 2004). TDM
supports multi-modal interaction where voice out-
put and input (VUI) is combined with a traditional
menu-based GUI with graphical output and hap-
tic input. In cases where a GUI already exists,
TDM can replace the GUI-internal interaction en-
gine, thus adding speech while keeping the origi-
nal GUI design. All system output is realized both
verbally and graphically, and the user can switch
freely between uni-modal (voice or screen/keys)
and multi-modal interaction.
To facilitate the browsing of lists (a well known
interaction problem for dialogue systems), Talka-
matic has developed its Voice-Cursor technology1
(Larsson et al, 2011). It allows a user to browse
a list in a multi-modal dialogue system without
looking at a screen and without being exposed to
large chunks of readout information.
A crucial property of TDM?s integrated multi-
modality is the fact that it enables the driver of a
vehicle to carry out all interactions without ever
looking at the screen, either by speaking to the sys-
tem, by providing haptic input, or by combining
the two. We are not aware of any current mul-
timodal in-vehicle dialogue system offering this
capability. Additional information is available at
www.talkamatic.se.
1.4 Mecel Populus
While TDM offers full menu-based multimodal
interaction, the GUI itself is fairly basic and does
not match the state of the art when it comes to
graphical design. By contrast, Mecel Populus is
an commercial-grade HMI (Human Machine In-
terface) with professionally designed visual out-
put. The Mecel Populus suite is a complete tool
chain for designing, developing and deploying
user interfaces for distributed embedded systems.
It minimizes the time and cost of producing eye-
catching, full-featured HMIs.
The Mecel Populus concept has several unique
features compared to traditional HMI develop-
ment. These features, when combined, remove the
barriers that traditionally exist between the peo-
ple working with requirements, system engineer-
ing, HMI design and implementation. An HMI
is created and verified in Mecel Populus Editor
1Patent Pending
Figure 1: SIMSI system overview
without having to write any software. The HMI is
then downloaded to the target environment where
Mecel Populus Engine executes it. Mecel Popu-
lus has been designed for the automotive industry
to deliver high performance user interfaces with a
short time-to-market and to enable efficient soft-
ware life cycle management. Additional informa-
tion is available at www.mecel.se/products.
2 System integration
The goal of this part of SIMSI is to provide a
project-specific integration of TDM and the Me-
cel Populus platform. In this way, we estab-
lish a commercial-grade HMI for experiments and
demonstrations. At the same time, the integration
of TDM and Populus increases the commercial po-
tential of both platforms, since it integrates a state-
of-the-art HMI tool without voice capabilities and
a dialogue manager with limited graphical capa-
bilities.
The major problem in integrating Populus and
TDM is that both systems keep track of the cur-
rent state of the interaction and manage transitions
between states resulting from user or system ac-
tions. Hence, there is a need to keep the systems in
sync at all times. This is managed by a Transition
Queue (TQ) module which keeps a lock which can
be grabbed by either system at any time, unless
it has already been grabbed by the other system.
The systems then enter into a master-slave rela-
tion where the master is the system which owns
the lock. The master tells the slave how the in-
teraction state is to be updated, and the slave only
waits for messages from the master until the lock
has been returned to the TQ.
355
Figure 2: SIMSI test environment overview
3 Test environment
The purpose of this part of the project is to conduct
ecologically valid test of the applications, and to
begin and continue an iterative development cycle
of testing - evaluation - development. We want to
find the best interaction solutions in cases where it
is not intuitively clear what is best. This involves
implementing variants of a behaviour, testing them
on naive users, collecting data from these interac-
tions, and establishing statistically significant re-
sults based on the collected data.
The test environment consists of two parts, apart
from the dialogue system: a driving simulator
(SCANeR from Octal) and an eye tracker (Smart
Eye Pro from Smarteye). In later tests we will also
include instruments for measuring cognitive load.
In our setup we have three monitors, giving the
user a wide field of view. We also have a gaming
steering wheel, including pedals, gear lever and a
driver?s seat. These are used mainly to control the
driving simulator, but there are also a number of
buttons on the steering wheel which are used to
browse the menus in the HMI and as Push-to-talk
(PTT). An Android tablet (Asus Eee Pad Trans-
former TF101) showing the HMI GUI is placed in
front of the user, trying to match the position of a
display in a car. Both TDM and Populus run on
the same desktop computer as the driving simula-
tor, and a Populus Android app runs on the tablet.
The app allows the user to select items by tapping
them, as well as scrolling in lists in normal smart
phone fashion. The eye tracker runs on a sepa-
rate desktop computer, as it requires a substantial
amount of processing power.
Figure 3: SIMSI test environment in action
Studio software that comes with the driving
simulator is used to design and run scenarios. The
scenarios govern how autonomous traffic should
behave and events, such as weather change and
the state of traffic signals. The simulator logs data
for the environment and each vehicle. Data like
lane deviation (where in the lane the vehicle is)
and how the user handles instruments, e.g. steer-
ing wheel and pedals, can be used to measure cog-
nitive load. At a later stage this kind of data can
also be used to trigger behaviour in the dialogue
system.
The eye tracker uses three cameras to track the
user?s eyes and head at 60 Hz. The cameras are
spaced to give good tracking in the middle of the
scene, where you typically look when you?re driv-
ing, and at the same time capture head movement
to the side. As we are interested in when the user is
looking at the tablet, we placed one of the cameras
specifically to improve eye tracking in this area.
References
Staffan Larsson, Alexander Berman, and Jessica
Villing. 2011. Adding a speech cursor to a mul-
timodal dialogue system. In INTERSPEECH 2011,
12th Annual Conference of the International Speech
Communication Association, Florence, Italy, 2011,
pages 3319?3320.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, Go?teborg University.
Vicki L. Neale, Thomas A. Dingus, Sheila G. Klauer,
Jeremy Sudweeks, and Michael Goodman. 2005.
An overview of the 100-car naturalistic study and
findings.
Sharon L. Oviatt, Rachel Coulston, and Rebecca
Lunsford. 2004. When do we interact multi-
modally?: cognitive load and multimodal commu-
nication patterns. In ICMI, pages 129?136.
356
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 72?79,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Probabilistic Rich Type Theory for Semantic Interpretation
Robin Cooper
1
, Simon Dobnik
1
, Shalom Lappin
2
, and Staffan Larsson
1
1
University of Gothenburg,
2
King?s College London
{cooper,sl}@ling.gu.se, simon.dobnik@gu.se, shalom.lappin@kcl.ac.uk
Abstract
We propose a probabilistic type theory in which a
situation s is judged to be of a type T with probabil-
ity p. In addition to basic and functional types it in-
cludes, inter alia, record types and a notion of typ-
ing based on them. The type system is intensional
in that types of situations are not reduced to sets
of situations. We specify the fragment of a com-
positional semantics in which truth conditions are
replaced by probability conditions. The type sys-
tem is the interface between classifying situations
in perception and computing the semantic interpre-
tations of phrases in natural language.
1 Introduction
Classical semantic theories (Montague, 1974), as
well as dynamic (Kamp and Reyle, 1993) and un-
derspecified (Fox and Lappin, 2010) frameworks
use categorical type systems. A type T identifies
a set of possible denotations for expressions in T ,
and the system specifies combinatorial operations
for deriving the denotation of an expression from
the values of its constituents.
These theories cannot represent the gradience
of semantic properties that is pervasive in speak-
ers? judgements concerning truth, predication, and
meaning relations. In general, predicates do not
have determinate extensions (or intensions), and
so, in many cases, speakers do not make categor-
ical judgements about the interpretation of an ex-
pression. Attributing gradience effects to perfor-
mance mechanisms offers no help, unless one can
show precisely how these mechanisms produce the
observed effects.
Moreover, there is a fair amount of evidence in-
dicating that language acquisition in general cru-
cially relies on probabilistic learning (Clark and
Lappin, 2011). It is not clear how a reasonable
account of semantic learning could be constructed
on the basis of the categorical type systems that ei-
ther classical or revised semantic theories assume.
Such systems do not appear to be efficiently learn-
able from the primary linguistic data (with weak
learning biases), nor is there much psychological
data to suggest that they provide biologically de-
termined constraints on semantic learning.
A semantic theory that assigns probability
rather than truth conditions to sentences is in a
better position to deal with both of these issues.
Gradience is intrinsic to the theory by virtue of
the fact that speakers assign values to declarative
sentences in the continuum of real numbers [0,1],
rather than Boolean values in {0,1}. In addition,
a probabilistic account of semantic learning is fa-
cilitated if the target of learning is a probabilistic
representation of meaning. Both semantic repre-
sentation and learning are instances of reasoning
under uncertainty.
Probability theorists working in AI often de-
scribe probability judgements as involving distri-
butions over worlds. In fact, they tend to limit
such judgements to a restricted set of outcomes
or events, each of which corresponds to a par-
tial world which is, effectively, a type of situa-
tion (Halpern, 2003). A classic example of the re-
duction of worlds to situation types in probability
theory is the estimation of the likelihood of heads
vs tails in a series of coin tosses. Here the world
is held constant except along the dimension of a
binary choice between a particular set of possi-
ble outcomes. A slightly more complex case is
the probability distribution for possible results of
throwing a single die, which allows for six pos-
sibilities corresponding to each of its numbered
faces. This restricted range of outcomes consti-
tutes the sample space.
We are making explicit the assumption, com-
mon to most probability theories used in AI, with
clearly defined sample spaces, that probability
is distributed over situation types (Barwise and
Perry, 1983), rather than over sets of entire worlds.
An Austinian proposition is a judgement that a
72
situation is of a particular type, and we treat it
as probabilistic. In fact, it expresses a subjec-
tive probability in that it encodes the belief of an
agent concerning the likelihood that a situation is
of that type. The core of an Austinian proposi-
tion is a type judgement of the form s : T , which
states that a situation s is of type T . On our ac-
count this judgement is expressed probabilistically
as p(s : T ) = r, where r ? [0,1].
1
On the probabilistic type system that we pro-
pose situation types are intensional objects over
which probability distributions are specified. This
allows us to reason about the likelihood of alter-
native states of affairs without invoking possible
worlds.
Complete worlds are not tractably repre-
sentable. Assume that worlds are maximal con-
sistent sets of propositions (Carnap, 1947). If
the logic of propositions is higher-order, then the
problem of determining membership in such a set
is not complete. If the logic is classically first-
order, then the membership problem is complete,
but undecidable.
Alternatively, we could limit ourselves to
propositional logic, and try to generate a maxi-
mally consistent set of propositions from a single
finite proposition P in Conjunctive Normal Form
(CNF, a conjunction of disjunctions), by simply
adding conjuncts to P . But it is not clear what
(finite) set of rules or procedures we could use to
decide which propositions to add in order to gen-
erate a full description of a world in a systematic
way. Nor is it obvious at what point the conjunc-
tion will constitute a complete description of the
world.
Moreover, all the propositions that P entails
must be added to it, and all the propositions with
which P is inconsistent must be excluded, in or-
der to obtain the maximal consistent set of propo-
sitions that describe a world. But then testing the
satisfiability of P is an instance of the ksat prob-
lem, which, in the general case, is NP-complete.
2
1
Beltagy et al. (2013) propose an approach on which clas-
sical logic-based representations are combined with distribu-
tional lexical semantics and a probabilistic Markov logic, in
order to select among the set of possible inferences from a
sentence. Our concern here is more foundational. We seek to
replace classical semantic representations with a rich proba-
bilistic type theory as the basis of both lexical and composi-
tional interpretation.
2
The ksat problem is to determine whether a formula in
propositional logic has a satisfying set of truth-value assign-
ments. For the complexity results of different types of ksat
problem see Papadimitriou (1995).
By contrast situation types can be as large or as
small as we need them to be. They are not max-
imal in the way that worlds are, and so the issue
of completeness of specification does not arise.
Therefore, they can, in principle, be tractably rep-
resented.
2 Rich Type Theory and Probability
Central to standard formulations of rich type the-
ories (for example, (Martin-L?of, 1984)) is the no-
tion of a judgement a : T , that object a is of type
T . We represent the probability of this judgement
as p(a : T ). Our system (based on Cooper (2012))
includes the following types.
Basic Types are not constructed out of other ob-
jects introduced in the theory. If T is a basic type,
p(a : T ) for any object a is provided by a probabil-
ity model, an assignment of probabilities to judge-
ments involving basic types.
PTypes are constructed from a predicate and
an appropriate sequence of arguments. An exam-
ple is the predicate ?man? with arity ?Ind ,Time?
where the types Ind and Time are the basic type
of individuals and of time points respectively.
Thus man(john,18:10) is the type of situation (or
eventuality) where John is a man at time 18:10.
A probability model provides probabilities p(e :
r(a
1
, . . . , a
n
)) for ptypes r(a
1
, . . . , a
n
). We take
both common nouns and verbs to provide the com-
ponents out of which PTypes are constructed.
Meets and Joins give, for T
1
and T
2
, the meet,
T
1
? T
2
and the join T
1
? T
2
, respectively. a :
T
1
? T
2
just in case a : T
1
and a : T
2
. a : T
1
?
T
2
just in case either a : T
1
or a : T
2
(possibly
both).
3
The probabilities for meet and joint types
are defined by the classical (Kolmogorov, 1950)
equations p(a : T
1
? T
2
) = p(a : T
1
)p(a : T
2
| a : T
1
)
(equivalently, p(a : T
1
? T
2
) = p(a : T
1
, a : T
2
)), and
p(a : T
1
? T
2
) = p(a : T
1
) + p(a : T
2
) ? p(a : T
1
? T
2
),
respectively.
Subtypes A type T
1
is a subtype of type T
2
,
T
1
v T
2
, just in case a : T
1
implies a : T
2
no mat-
ter what we assign to the basic types. If T
1
v T
2
then a : T
1
?T
2
iff a : T
1
and a : T
1
?T
2
iff a : T
2
.
Similarly, if T
2
v T
1
then a : T
1
? T
2
iff a : T
2
and a : T
1
? T
2
iff a : T
1
.
If T
2
v T
1
, then p(a : T
1
? T
2
) = p(a : T
2
),
and p(a : T
1
? T
2
) = p(a : T
1
). If T
1
v T
2
,
3
This use of intersection and union types is not standard in
rich type theories, where product and disjoint union are pre-
ferred following the Curry-Howard correspondence for con-
junction and disjunction.
73
then p(a : T
1
) ? p(a : T
2
). These definitions
also entail that p(a : T
1
? T
2
) ? p(a : T
1
), and
p(a : T
1
) ? p(a : T
1
? T
2
).
We generalize probabilistic meet and join types
to probabilities for unbounded conjunctive and
disjunctive type judgements, again using the clas-
sical equations.
Let
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) be the conjunctive
probability of judgements a
0
: T
0
, . . . , a
n
: T
n
.
Then
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) =
?
p
(a
0
: T
0
, . . . , a
n?1
:
T
n?1
)p(a
n
: T
n
| a
0
: T
0
, . . . , a
n?1
: T
n?1
). If n = 0,
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) = 1.
We interpret universal quantification as an un-
bounded conjunctive probability, which is true if
it is vacuously satisfied (n = 0) (Paris, 2010).
Let
?
p
(a
0
: T
0
, a
1
: T
1
, . . .) be the disjunctive
probability of judgements a
0
: T
0
, a
1
: T
1
, . . ..
It is computed by
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) =
?
p
(a
0
: T
0
, . . . , a
n?1
: T
n?1
) + p(a
n
: T
n
) ?
?
p
(a
0
:
T
0
, . . . , a
n?1
: T
n?1
)p(a
n
: T
n
| a
0
: T
0
, . . . , a
n?1
:
T
n?1
). If n = 0,
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) = 0.
We take existential quantification to be an un-
bounded disjunctive probability, which is false if it
lacks a single non-nil probability instance (n = 0).
Conditional Conjunctive Probabilities are
computed by
?
p
(a
0
: T
0
, . . . , a
n
: T
n
| a : T ) =
?
p
(a
0
: T
0
, . . . , a
n?1
: T
n?1
| a : T )p(a
n
: T
n
|
a
0
: T
0
, . . . , a
n?1
: T
n?1
, a : T )). If n = 0,
?
p
(a
0
:
T
0
, . . . , a
n
: T
n
| a : T ) = 1.
Function Types give, for any types T
1
and T
2
,
the type (T
1
? T
2
). This is the type of total func-
tions with domain the set of all objects of type
T
1
and range included in objects of type T
2
. The
probability that a function f is of type (T
1
? T
2
)
is the probability that everything in its domain is of
type T
1
and that everything in its range is of type
T
2
, and furthermore that everything not in its do-
main which has some probability of being of type
T
1
is not in fact of type T
1
. p(f : (T
1
? T
2
)) =
?
a?dom(f)
p
(a : T
1
, f(a) : T
2
)(1?
?
a6?dom(f)
p
(a : T
1
))
Suppose that T
1
is the type of event where there
is a flash of lightning and T
2
is the type of event
where there is a clap of thunder. Suppose that f
maps lightning events to thunder events, and that
it has as its domain all events which have been
judged to have probability greater than 0 of being
lightning events. Let us consider that all the puta-
tive lightning events were clear examples of light-
ning (i.e. judged with probability 1 to be of type
T
1
) and are furthermore associated by f with clear
events of thunder (i.e. judged with probability 1 to
be of type T
2
). Suppose there were four such pairs
of events. Then the probability of f being of type
(T
1
? T
2
) is (1? 1)
4
, that is, 1.
Suppose, alternatively, that for one of the four
events f associates the lightning event with a silent
event, that is, one whose probability of being of
T
2
is 0. Then the probability of f being of type
(T
1
? T
2
) is (1 ? 1)
3
? (1 ? 0) = 0. One clear
counterexample is sufficient to show that the func-
tion is definitely not of the type.
In cases where the probabilities of the an-
tecedent and the consequent type judgements are
higher than 0, the probability of the entire judge-
ment on the existence of a functional type f will
decline in proportion to the size of dom(f). As-
sume, for example that there are k elements a ?
dom(f), where for each such a p(a : T
1
) =
p(f(a) : T
2
) ? .5. Every a
i
that is added to
dom(f) will reduce the value of p(f : (T
1
?
T
2
)), even if it yields higher values for p(a : T
1
)
and p(f(a) : T
2
). This is due to the fact that we
are treating the probability of p(f : (T
1
? T
2
))
as the likelihood of there being a function that is
satisfied by all objects in its domain. The larger
the domain, the less probable that all elements in
it fulfill the functional relation.
We are, then, interpreting a functional type
judgement of this kind as a universally quantified
assertion over the pairing of objects in dom(f)
and range(f). The probability of such an asser-
tion is given by the conjunction of assertions cor-
responding to the co-occurrence of each element a
in f ?s domain as an instance of T
1
with f(a) as an
instance of T
2
. This probability is the product of
the probabilities of these individual assertions.
This seems reasonable, but it only deals with
functions whose domain is all objects which have
been judged to have some probability, however
low, of being of type T
1
. Intuitively, functions
which leave out some of the objects with lower
likelihood of being of type T
1
should also have a
probability of being of type (T
1
? T
2
). This fac-
tor in the probability is represented by the second
element of the product in the formula.
74
Negation ?T , of type T , is the function type
(T ? ?), where ? is a necessarily empty type
and p(?) = 0. It follows from our rules for func-
tion types that p(f : ?T ) = 1 if dom(f) = ?, that
is T is empty, and 0 otherwise.
We also assign probabilities to judgements con-
cerning the (non-)emptiness of a type, p(T ). we
pass over the details of how we compute the prob-
abilities of such judgements, but we note that our
account of negation entails that p(T ? ?T ) = 1,
and (ii) p(??T ) = p(T ). Therefore, we sustain
classical Boolean negation and disjunction, in con-
trast to Martin-L?of?s (1984) intuitionistic type the-
ory.
Dependent Types are functions from objects to
types. Given appropriate arguments as functions
they will return a type. Therefore, the account of
probabilities associated with functions above ap-
plies to dependent types.
Record Types A record in a type system asso-
ciated with a set of labels is a set of ordered pairs
(fields) whose first member is a label and whose
second member is an object of some type (possibly
a record). Records are required to be functional on
labels (each label in a record can only occur once
in the record?s left projection).
A dependent record type is a set of fields (or-
dered pairs) consisting of a label ` followed by T
as above. The set of record types is defined by:
1. [], that is the empty set or Rec, is a record type. r : Rec
just in case r is a record.
2. If T
1
is a record type, ` is a label not occurring in T
1
,
and T
2
is a type, then T
1
? {?`, T
2
?} is a record type.
r : T
1
? {?`, T
2
?} just in case r : T
1
, r.` is defined (`
occurs as a label in r) and r.` : T
2
.
3. If T is a record type, ` is a label not occuring in
T , T is a dependent type requiring n arguments, and
?pi
1
, . . . , pi
n
? is an n-place sequence of paths in T ,
4
then T ? {?`, ?T , ?pi
1
, . . . , pi
n
???} is a record type.
r : T ? {?`, ?T , ?pi
1
, . . . , pi
n
???} just in case r : T ,
r.` is defined and r.` : T (r.pi
1
, . . . , r.pi
n
).
The probability that an object r is of a record
type T is given by the following clauses:
1. p(r : Rec) = 1 if r is a record, 0 otherwise
2. p(r : T
1
? {?`, T
2
?}) =
?
p
(r : T
1
, r.` : T
2
)
3. If T : (T
1
? (. . . ? (T
n
? Type) . . .)), then
p(r : T ? {?`, ?T , ?pi
1
, . . . , pi
n
???}) =
?
p
(r : T, r.` :
T (r.pi
1
, . . . , r.pi
n
) | r.pi
1
: T
1
, . . . , r.pi
n
: T
n
)
4
In the full version of TTR we also allow absolute paths
which point to particular records, but we will not include
them here.
3 Compositional Semantics
Montague (1974) determines the denotation of a
complex expression by applying a function to an
intensional argument (as in [[ NP ]]([[
?
VP ]])). We
employ a variant of this general strategy by ap-
plying a probabilistic evaluation function [[ ? ]]
p
to
a categorical (non-probabilistic) semantic value.
For semantic categories that are interpreted as
functions, [[ ? ]]
p
yields functions from categorical
values to probabilities. For sentences it produces
probability values.
The probabilistic evaluation function [[ ? ]]
p
pro-
duces a probabilistic interpretation based on a
classical compositional semantics. For sentences
it will return the probability that the sentence is
true. For categories that are interpreted as func-
tions it will return functions from (categorical) in-
terpretations to probabilities. We are not propos-
ing strict compositionality in terms of probabili-
ties. Probabilities are like truth-values (or rather,
truth-values are the limit cases of probabilities).
We would not expect to be able to compute the
probability associated with a complex constituent
on the basis of the probabilities associated with its
immediate constituents, any more than we would
expect to be able to compute a categorical inter-
pretation entirely in terms of truth-functions and
extensions. However, the simultaneous computa-
tion of categorical and probabilistic interpretations
provides us with a compositional semantic system
that is closely related to the simultaneous com-
putation of intensions and extensions in classical
Montague semantics.
The following definition of [[ ? ]]
p
for a fragment
of English is specified on the basis of our proba-
bilistic type system and a non-probabilistic inter-
pretation function [[ ? ]], which we do not give in
this version of the paper. (It?s definition is given
by removing the probability p from the definition
below.)
[[ [
S
S
1
and S
2
] ]]
p
= p(
[
e
1
:[[ S
1
]]
e
2
:[[ S
2
]]
]
)
[[ [
S
S
1
or S
2
] ]]
p
= p(
[
e:[[ S
1
]]?[[ S
2
]]
]
)
[[ [
S
Neg S] ]]
p
= [[ Neg ]]
p
([[ S ]])
[[ [
S
NP VP] ]]
p
= [[ NP ]]
p
([[ VP ]])
[[ [
NP
Det N] ]]
p
= [[ Det ]]
p
([[ N ]])
[[ [
NP
N
prop
] ]]
p
= [[ N
prop
]]
p
[[ [
VP
V
t
NP] ]]
p
= [[ V
t
]]
p
([[ NP ]])
[[ [
VP
V
i
] ]]
p
= [[ V
i
]]
p
[[ [
Neg
?it?s not true that?] ]]
p
= ?T :RecType(p(
[
e:?T
]
))
[[ [
Det
?some?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:some(Q, P )
]
)))
[[ [
Det
?every?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:every(Q, P )
]
)))
[[ [
Det
?most?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:most(Q, P )
]
)))
75
[[ [
N
?boy?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:boy(r.x)
]
))
[[ [
N
?girl?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:girl(r.x)
]
))
[[ [
Adj
?green?] ]]
p
=
?P :Ppty(?r:
[
x:Ind
]
(p((
[
e:green(r.x,P )
]
)))))
[[ [
Adj
?imaginary?] ]]
p
=
?P :Ppty(?r:
[
x:Ind
]
(p((
[
e:imaginary(r.x,P )
]
)))))
5
[[ [
N
prop
?Kim?] ]]
p
= ?P :Ppty(p(P (
[
x=kim
]
)))
[[ [
N
prop
?Sandy?] ]]
p
= ?P :Ppty(p(P (
[
x=sandy
]
)))
[[ [
V
t
?knows?] ]]
p
=
?P:Quant(?r
1
:
[
x:Ind
]
(p(P(?r
2
:(
[
e:know(r
1
.x,r
2
.x)
]
)))))
[[ [
V
t
?sees?] ]]
p
=
?P:Quant(?r
1
:
[
x:Ind
]
(p(P(?r
2
:(
[
e:see(r
1
.x,r
2
.x)
]
)))))
[[ [
V
i
?smiles?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:smile(r.x)
]
))
[[ [
V
i
?laughs?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:laugh(r.x)
]
))
A probability distribution d for this fragment,
based on a set of situations S, is such that:
p
d
(a : Ind) = 1 if a is kim or sandy
6
p
d
(s : T ) ? [0, 1] if s ? S and T is a ptype
p
d
(s : T ) = 0 if s 6? S and T is a ptype
7
p
d
(a : [
?
P ]) = p
d
(P (
[
x=a
]
))
p
d
(some(P,Q)) = p
d
([
?
P ] ? [
?
Q])
p
d
(every(P,Q)) = p
d
([
?
P ]? [
?
Q])
p
d
(most(P,Q)) = min(1,
p
d
([
?
P ]?[
?
Q]
?
most
p
d
([
?
P ])
)
The probability that an event e is of the type in
which the relation some holds of the properties P
andQ is the probability that e is of the conjunctive
type P ?Q. The probability that e is of the every
type for P and Q is the likelihood that it instanti-
ates the functional type P ? Q. As we have de-
fined the probabilities associated with functional
types in terms of universal quantification (an un-
bounded conjunction of the pairings between the
elements of the domain P of the function and its
range Q), this definition sustains the desired read-
ing of every. The likelihood that e is of the type
most for P and Q is the likelihood that e is of
type P ?Q, factored by the product of the contex-
tually determined parameter ?
most
and the likeli-
hood that e is of type P , where this fraction is less
than 1, and 1 otherwise.
Consider a simple example.
[[ [
S
[
NP
[
N
prop
Kim]] [
VP
[
V
i
smiles]]] ]]
p
=
?P :Ppty(p(P (
[
x=kim
]
)))(?r:
[
x:Ind
]
(
[
e:smile(r.x)
]
)) =
p(?r:
[
x:Ind
]
(
[
e:smile(r.x)
]
)(
[
x=kim
]
)) =
p(
[
e:smile(kim)
]
)
5
Notice that we characterize adjectival modifiers as rela-
tions between records of individuals and properties. We can
then invoke subtyping to capture the distinction between in-
tersective and non-intersective modifier relations.
6
This seems an intuitive assumption, though not a neces-
sary one.
7
Again this seems an intuitive, though not a necessary as-
sumption.
Suppose that p
d
(s
1
:smile(kim)) = .7,
p
d
(s
2
:smile(kim)) = .3, p
d
(s
3
:smile(kim)) =
.4, and there are no other situations s
i
such
that p
d
(s
i
:smile(kim)) > 0. Furthermore, let
us assume that these probabilities are indepen-
dent of each other, that is, p
d
(s
3
:smile(kim)) =
p
d
(s
3
:smile(kim) | s
1
:smile(kim), s
2
:smile(kim))
and so on. Then
p
d
(smile(kim))=
?
p
d
(s
1
: smile(kim), s
2
: smile(kim), s
3
: smile(kim)) =
?
p
d
(s
1
: smile(kim), s
2
: smile(kim)) + .4 ? .4
?
p
d
(s
1
:
smile(kim), s
2
: smile(kim)) =
(.7 + .3? .7? .3) + .4? .4(.7 + .3? .7? .3) =
.874
This means that p
d
(
[
e:smile(kim)
]
) = .874.
Hence [[ [
S
[
NP
[
N
prop
Kim]] [
VP
[
V
i
smiles]]] ]]
p
d
= .874
(where [[ ? ]]
p
d
is the result of computing [[ ? ]]
p
with respect to the probability distribution d).
Just as for categorical semantics, we can con-
struct type theoretic objects corresponding to
probabilistic judgements. We call these proba-
bilistic Austinian propositions. These are records
of type?
?
sit : Sit
sit-type : Type
prob : [0,1]
?
?
where [0,1] is used to represent the type of real
numbers between 0 and 1. They assert that the
probability that a situation s is of type Type is the
value of prob.
The definition of [[ ? ]]
p
specifies a compositional
procedure for generating an Austinian proposition
(record) of this type from the meanings of the syn-
tactic constituents of a sentence.
4 An Outline of Semantic Learning
We outline a schematic theory of semantic learn-
ing on which agents acquire classifiers that form
the basis for our probabilistic type system. For
simplicity and ease of presentation we take these
to be Naive Bayes classifiers, which an agent ac-
quires from observation. In future developments
of this theory we will seek to extend the approach
to Bayesian networks (Pearl, 1990).
We assume that agents keep records of observed
situations and their types, modelled as probabilis-
tic Austinian propositions. For example, an obser-
vation of a man running might yield the following
Austinian proposition for some a:Ind, s
1
:man(a),
s
2
:run(a):
76
??
?
?
?
?
?
?
?
sit =
?
?
ref = a
c
man
= s
1
c
run
= s
2
?
?
sit-type =
?
?
ref : Ind
c
man
: man(ref)
c
run
: run(ref)
?
?
prob = 0.7
?
?
?
?
?
?
?
?
?
An agent, A, makes judgements based on a
finite string of probabilistic Austinian proposi-
tions, J, corresponding to prior judgements held
in memory. For a type, T , J
T
represents that set of
Austinian propositions j such that j.sit-type v T .
If T is a type and J a finite string of probabilis-
tic Austinian propositions, then || T ||
J
represents
the sum of all probabilities associated with T in J
(
?
j?J
T
j.prob). P(J) is the sum of all probabilities
in J (
?
j?J
j.prob).
We use prior
J
(T ) to represent the prior proba-
bility that anything is of type T given J, that is
||T ||
J
P(J)
if P(J) > 0, and 0 otherwise.
p
A,J
(s : T ) denotes the probability that agent A
assigns with respect to prior judgements J to s be-
ing of type T . Similarly, p
A,J
(s : T
1
| s : T
2
) is
the probability that agent A assigns with respect
to prior judgements J to s being of type T
1
, given
that A judges s to be of type T
2
.
When an agent A encounters a new situation
s and considers whether it is of type T , he/she
uses probabilistic reasoning to determine the value
of p
A,J
(s : T ). A uses conditional probabilities
to calculate this value, where A computes these
conditional probabilities with the equation p
A,J
(s :
T
1
| s : T
2
) =
||T
1
?T
2
||
J
||T
2
||
J
, if || T
2
||
J
6= 0. Otherwise,
p
A,J
(s : T
1
| s : T
2
) = 0.
This is our type theoretic variant of the stan-
dard Bayesian formula for conditional probabili-
ties: p(A | B) =
|A&B|
|B|
. But instead of counting
categorical instances, we sum the probabilities of
judgements. This is because our ?training data? is
not limited to categorical observations. Instead it
consists of probabilistic observational judgements
that situations are of particular types.
8
Assume that we have the following types:
T
man
=
[
ref : Ind
c
man
: man(ref)
]
and
T
run
=
[
ref : Ind
c
run
: run(ref)
]
8
As a reviewer observes, by using an observer?s previous
judgements for the probability of an event being of a partic-
ular type, as the prior for the rule that computes the proba-
bility of a new event being of that type, we have, in effect,
compressed information that properly belongs in a Bayesian
network into our specification of a naive Bayesian classifier.
This is a simplification that we adopt here for ease of expo-
sition. In future work, we will characterise classifier learning
through full Bayesian networks.
Assume also that J
T
man
?T
run
has three members,
corresponding to judgements by A that a man was
running in three observed situations s
1
, s
3
, and
s
4
, and that these Austinian propositions have the
probabilities 0.6, 0.6. and 0.5 respectively.
Take J
T
man
to have five members correspond-
ing to judgements by A that there was a man in
s
1
, . . . , s
5
, and that the Austinian propositions as-
signing T
man
to s
1
, . . . , s
5
all have probability 0.7.
Given these assumptions, the conditional probabil-
ity that A will assign on the basis of J to someone
runs, given that he is a man is p
A,J
(r : T
run
| r :
T
man
) =
||T
man
?T
run
||
J
||T
man
||
J
=
0.6+0.6+0.5
0.7+0.7+0.7+0.7+0.7
= .486
We use conditional probabilities to construct a
Naive Bayes classifier. A classifies a new situa-
tion s based on the prior judgements J, and what-
ever evidence A can acquire about s. This evi-
dence has the form p
A,J
(s : T
e
1
), . . ., p
A,J
(s : T
e
n
),
where T
e
1
, . . . , T
e
n
are the evidence types. The
Naive Bayes classifier assumes that the evidence is
independent, in that the probability of each piece
of evidence is independent of every other piece of
evidence.
We first formulate Bayes? rule of conditional
probability. This rule defines the conditional prob-
ability of a conclusion r : T
c
, given evidence r :
T
e
1
, r : T
e
2
, . . . , r : T
e
n
, in terms of conditional prob-
abilities of the form p(s
i
: T
e
i
| s
i
: T
c
), 1 ? i ? n,
and priors for conclusion and evidence:
p
A,J
(r : T
c
| r : T
e
1
, . . . , r : T
e
n
) =
prior
J
(T
c
)
||T
e
1
?T
c
||
J
||T
c
||
J
...
||T
e
n
?T
c
||
J
||T
c
||
J
prior
J
(T
e
1
)...prior
J
(T
e
n
)
The conditional probabilities are computed
from observations as indicated above. The rule of
conditional probability allows the combination of
several pieces of evidence, without requiring pre-
vious observation of a situation involving all the
evidence types.
We formulate a Naive Bayes classifier as a func-
tion from evidence types T
e
1
, T
e
2
, . . . , T
e
n
(i.e. from
a record of type T
e
1
? T
e
2
? . . . ? T
e
n
) to conclusion
types T
c
1
, T
c
2
, . . . , T
c
m
. The conclusion is a disjunc-
tion of one or more T ? {T
c
1
, T
c
2
, . . . , T
c
m
}, where
m ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier. This function
is specified as follows.
? : (T
e
1
? . . .?T
e
n
)? (T
c
1
? . . .?T
c
m
) such that ?(r) =
(
?
argmax
T??T
c
1
,...,T
c
m
?
p
A,J
(r : T | r : T
e
1
, . . . , r : T
e
n
)
The classifier returns the type T which max-
imises the conditional probability of r : T given
77
the evidence provided by r. The argmax operator
here takes a sequence of arguments and a func-
tion and yields a sequence containing the argu-
ments which maximise the function (if there are
more than one).
The classifier will output a disjunction in case
both possibilities have the same probability. The
?
operator takes a sequence and returns the dis-
junction of all elements of the sequence.
In addition to computing the conclusion which
receives the highest probability given the evi-
dence, we also want the posterior probability of
the judgement above, i.e. the probability of the
judgement in light of the evidence. We obtain the
non-normalised probabilities (p
nn
A,J
) of the different
possible conclusions by factoring in the probabili-
ties of the evidence:
p
nn
A,J
(r : ?(r)) =
?
T?
?
?1
?(r)
p
A,J
(r : T | r : T
e
1
, . . . , r : T
e
n
)p
A,J
(r :
T
e
1
) . . . p
A,J
(r : T
e
n
)
where
?
?1
is the inverse of
?
, i.e. a function that
takes a disjunction and returns the set of disjuncts.
We then take the probability of r : ?(r) and
normalise over the sum of the probabilities of
all the possible conclusions. This gives us the
normalised probability of the judgement resulting
from classification p(r : ?(r)) =
p
nn
A,J
(r:?(r))
?
1?i?m
p
nn
A,J
(r:T
c
i
)
.
However, since the probabilities of the evidence
are identical for all possible conclusions, we can
ignore them and instead compute the normalised
probability with the following equation (where m
ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier, as above).
p
A,J
(r : ?(r)) =
?
T?
?
?1
?(r)
p
A,J
(r:T |r:T
e
1
,...,r:T
e
n
)
?
1?i?m
p
A,J
(r:T
c
i
|r:T
e
1
,...,r:T
e
n
)
The result of classification can be represented as
an Austinian proposition
?
?
sit = s
sit-type = ?(s)
prob = p
A,J
(s : ?(s))
?
?
which A adds to J as a result of observing and
classifying s, and is thus made available for sub-
sequent probabilistic reasoning.
5 Conclusions and Future Work
We have presented a probabilistic version of a rich
type theory with records, relying heavily on classi-
cal equations for types formed with meet, join, and
negation. This has permitted us to sustain classi-
cal equivalences and Boolean negation for com-
plex types within an intensional type theory. We
have replaced the truth of a type judgement with
the probability of it being the case, and we have
applied this approach to judgements that a situa-
tion if of type T .
Our probabilistic formulation of a rich type the-
ory with records provides the basis for a compo-
sitional semantics in which functions apply to cat-
egorical semantic objects in order to return either
functions from categorical interpretations to prob-
abilistic judgements, or, for sentences, to proba-
bilistic Austinian propositions. One of the inter-
esting ways in which this framework differs from
classical model theoretic semantics is that the ba-
sic types and type judgements at the foundation of
the type system correspond to perceptual judge-
ments concerning objects and events in the world,
rather than to entities in a model and set theoretic
constructions defined on them.
We have offered a schematic view of semantic
learning. On this account observations of situa-
tions in the world support the acquisition of naive
Bayesian classifiers from which the basic proba-
bilistic types of our type theoretical semantics are
extracted. Our type theory is, then, the interface
between observation-based learning of classifiers
for objects and the situations in which they figure
on one hand, and the computation of complex se-
mantic values for the expressions of a natural lan-
guage from these simple probabilistic types and
type judgements on the other. Therefore our gen-
eral model of interpretation achieves a highly in-
tegrated bottom-up treatment of linguistic mean-
ing and perceptually-based cognition that situates
meaning in learning how to make observational
judgements concerning the likelihood of situations
obtaining in the world.
The types of our semantic theory are inten-
sional. They constitute ways of classifying situ-
ations, and they cannot be reduced to set of situa-
tions. The theory achieves fine-grained intension-
ality through a rich and articulated type system,
where the foundation of this system is anchored in
perceptual observation.
The meanings of expressions are acquired on
the basis of speakers? experience in the applica-
tion of classifiers to objects and events that they
encounter. Meanings are dynamic and updated in
light of subsequent experience.
78
Probability is distributed over alternative situ-
ation types. Possible worlds, construed as maxi-
mal consistent sets of propositions (ultrafilters in a
proof theoretic lattice of propositions) play no role
in this framework.
Bayesian reasoning from observation provides
the incremental basis for learning and refining
predicative types. These types feed the combina-
torial semantic procedures for interpreting the sen-
tences of a natural language.
In future work we will explore implementations
of our learning theory in order to study the viabil-
ity of our probabilistic type theory as an interface
between perceptual judgement and compositional
semantics. We hope to show that, in addition to
its cognitive and theoretical interest, our proposed
framework will yield results in robotic language
learning, and dialogue modelling.
Acknowledgments
We are grateful to two anonymous reviewers for
very helpful comments on an earlier draft of this
paper. We also thank Alex Clark, Jekaterina
Denissova, Raquel Fern?andez, Jonathan Ginzburg,
Noah Goodman, Dan Lassiter, Michiel van Lam-
balgen, Poppy Mankowitz, Aarne Ranta, and Pe-
ter Sutton for useful discussion of ideas presented
in this paper. Shalom Lappin?s participation in
the research reported here was funded by grant
ES/J022969/1 from the Economic and Social Re-
search Council of the UK, and a grant from the
Wenner-Gren Foundations. We also gratefully ac-
knowledge the support of Vetenskapsr?adet, project
2009-1569, Semantic analysis of interaction and
coordination in dialogue (SAICD); the Depart-
ment of Philosophy, Linguistics, and Theory of
Science; and the Centre for Language Technology
at the University of Gothenburg.
References
Jon Barwise and John Perry. 1983. Situations and
Attitudes. Bradford Books. MIT Press, Cambridge,
Mass.
I. Beltagy, C. Chau, G. Boleda, D. Garrette, K. Erk,
and R. Mooney. 2013. Montague meets markov:
Deep semantics with probabilistic logical form. In
Second Joint Conference on Lexical and Computa-
tional Semantics, Vol. 1, pages 11?21. Association
of Computational Linguistics, Atlanta, GA.
R. Carnap. 1947. Meaning and Necessity. University
of Chicago Press, Chicago.
A. Clark and S. Lappin. 2011. Linguistic Nativism
and the Poverty of the Stimulus. Wiley-Blackwell,
Chichester, West Sussex, and Malden, MA.
Robin Cooper. 2012. Type theory and semantics in
flux. In Ruth Kempson, Nicholas Asher, and Tim
Fernando, editors, Handbook of the Philosophy of
Science, volume 14: Philosophy of Linguistics. El-
sevier BV, 271?323. General editors: Dov M. Gab-
bay, Paul Thagard and John Woods.
C. Fox and S. Lappin. 2010. Expressiveness and
complexity in underspecified semantics. Linguistic
Analysis, Festschrift for Joachim Lambek, 36:385?
417.
J. Halpern. 2003. Reasoning About Uncertainty. MIT
Press, Cambridge MA.
H. Kamp and U. Reyle. 1993. From Discourse to
Logic: Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.
A.N. Kolmogorov. 1950. Foundations of Probability.
Chelsea Publishing, New York.
Per Martin-L?of. 1984. Intuitionistic Type Theory. Bib-
liopolis, Naples.
Richard Montague. 1974. Formal Philosophy: Se-
lected Papers of Richard Montague. Yale University
Press, New Haven. ed. and with an introduction by
Richmond H. Thomason.
C. Papadimitriou. 1995. Computational Complexity.
Addison-Wesley Publishing Co., Readin, MA.
J. Paris. 2010. Pure inductive logic. Winter School in
Logic, Guangzhou, China.
J. Pearl. 1990. Bayesian decision methods. In
G. Shafer and J. Pearl, editors, Readings in Uncer-
tain Reasoning, pages 345?352. Morgan Kaufmann.
79
