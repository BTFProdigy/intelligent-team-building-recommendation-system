Interlingua-Based Broad-Coverage Korean-to-English 
Translation in CCLINC 
       Young-Suk Lee               Wu Sok Yi            Stephanie Seneff        Clifford J. Weinstein 
     MIT Lincoln Laboratory     MIT Lincoln Laboratory                 MIT/LCS                     MIT Lincoln Laboratory 
         244 Wood Street               244 Wood Street               77 Mass Avenue                     244 Wood Street 
     Lexington, MA 02420         Lexington, MA 02420      Cambridge, MA 02673             Lexington, MA 02420 
                U.S.A                                   U.S.A                                U.S.A                                       U.S.A 
         1-781-981-2703                 1-781-981-4609                1-617-254-0456             1-781-981-7621 
       YSL@LL.MIT.EDU           WUYI@LL.MIT.EDU     SENEFF@LCS.MIT.EDU          CJW@LL.MIT.EDU  
                    
  
ABSTRACT 
 
At MIT Lincoln Laboratory, we have been developing a Korean-
to-English machine translation system CCLINC (Common 
Coalition Language System at Lincoln Laboratory). The CCLINC 
Korean-to-English translation system  consists of two core 
modules, language understanding and generation modules 
mediated by a language neutral meaning representation called a 
semantic frame. The key features of the system include: (i) Robust 
efficient parsing of Korean (a verb final language with overt case 
markers, relatively free word order, and frequent omissions of 
arguments). (ii) High quality translation via word sense 
disambiguation and accurate word order generation of  the target 
language. (iii) Rapid system development and porting to new 
domains via knowledge-based automated acquisition of 
grammars. Having been trained on Korean newspaper articles on 
?missiles? and ?chemical biological warfare,? the system produces 
the translation output sufficient for content understanding of the 
original document. 
 
1. SYSTEM OVERVIEW 
 The CCLINC  The CCLINC Korean-to-English translation 
system is a component of the CCLINC Translingual Information 
System, the focus languages of which are English and Korean, 
[11,17]. Translingual Information System Structure is given in 
Figure 1.  
Given the input text or speech, the language understanding system 
parses the input, and transforms the parsing output into a language 
neutral meaning representation called a semantic frame, [16,17]. 
The semantic frame ? the key properties of which will be 
discussed in Section 2.3 ? becomes the input to the generation 
system. The generation system produces the target to the 
generation system, the semantic frame can be utilized for other 
applications such as translingual information extraction and 
 
 
 
 
  
language translation output after word order arrangement, 
vocabulary replacement, and the appropriate surface form 
realization in the target language, [6]. Besides serving as the input 
question-answering, [12].?  In this paper, we focus on the Korean-
to-English text  translation component of CCLINC.1 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. CCLINC Translingual Information System 
Structure 
 
2. ROBUST PARSING, MEANING 
REPRESENTATION, AND AUTOMATED 
GRAMMAR ACQUISITION 
                                                                
? This work was sponsored by the Defense Advanced Research 
Project Agency under the contract number F19628-00-C-0002. 
Opinions, interpretations, conclusions, and recommendations 
are those of the authors and are not necessarily endorsed by the 
United States Air Force. 
1 For other approaches to Korean-to-English translation, the 
readers are referred to Korean-to-English translation by Egedi, 
Palmer, Park and Joshi 1994, a transfer-based approach using 
synchronous tree adjoining grammar, [5], and Dorr 1997, a 
small-scale interlingua-based approach, using Jackendoff?s 
lexical conceptual structure as the interlingua, [4]. 
  
OTHER LANGUAGES
SEMANTIC FRAMES
(COMMON
COALITION
LANGUAGE)
SEMANTIC FRAMES
(COMMON
COALITION
LANGUAGE)
UNDERSTANDINGUNDERSTANDING UNDERSTANDINGUNDERSTANDING
GENERATIONGENERATION GENERATIONGENERATION
C4I
INFORMATION
ACCESS
C4I
INFORMATION
ACCESS
ENGLISH
TEXT OR
SPEECH
KOREAN
TEXT OR
SPEECH
1.1 Robust Parsing 
The CCLINC parsing module, TINA [16], implements the top-
down chart parsing and the best-first search techniques, driven by 
context free grammars rules compiled into a recursive transition 
network augmented by features, [8]. The following properties of 
Korean induce a great degree of ambiguity in the grammar: (i) 
relatively free word order for arguments --- given a sentence with 
three arguments, subject, object, indirect object, all 6 logical word 
order permutations are possible in reality, (ii) frequent omissions 
of subjects and objects, and (iii) the strict verb finality, [10]. Due 
to the free word order and argument omissions, the first word of 
an input sentence can be many way ambiguous  --- it can be a part 
of a subject, an object, and any other post-positional phrases.2  
The ambiguity introduced by the first input word grows rapidly as 
the parser processes subsequent input words. Verbs,  which 
usually play a crucial role in reducing the ambiguity in English by 
the subcategorization frame information, are not available until 
the end, [1,3,11]. 
Our solution to the ambiguity problem lies in a novel grammar 
writing technique, which reduces the ambiguity of the first input 
word. We hypothesize that (i) the initial symbol in the grammar 
(i.e. Sentence) always starts with the single category generic_np, 
the grammatical function (subject, object) of which is 
undetermined. This ensures that the ambiguity of the first input 
word is reduced to the number of different ways the category 
generic_np can be rewritten. (ii) The grammatical function of the 
generic_np is determined after the parser processes the following 
case marker via a trace mechanism.3   
Figure 2 illustrates a set of sample context free grammar rules, and 
Figure 3 (on the next page) is a sample parse tree for the input 
sentence ?URi Ga EoRyeoUn MunJe Reul PulEox Da (We solved 
a difficult problem).?4 
 
(i)           sentence ? generic_np clause sentence_marker 
(ii) clause ? subject generic_np object verbs 
(iii) subject ? subj_marker np_trace 
Figure 2. Sample context free grammar rules for  Korean 
 
                                                                
2 Post-positional phrases in Korean correspond to pre-positional 
phrases in English. We use the term post-positional phrase to 
indicate that the function words at issue are located after the 
head noun. 
3 The hypothesis that all sentences start with a single category 
generic_np is clearly over simplified. We can easily find a 
sentence starting with other elements such as coordination 
markers which do not fall under generic_np.  For the sentences 
which do not start with the category generic_np, we discard 
these elements for parsing purposes. And this method has 
proven to be quite effective in the overall design of the 
translation system, especially due to the fact that most of  non 
generic_np sentence initial elements (e.g. coordination markers, 
adverbs, etc.) do not contribute to the core meaning of the input 
sentence.  
4 Throughout this paper, ?subj_marker? stands for ?subject 
marker?, and ?obj_marker?, ?object marker?. 
The generic_np dominated by the initial symbol sentence in (i) of 
Figure 2 is parsed as an element moved from the position 
occupied by np_trace in (iii), and therefore corresponds to the 
category np_trace dominated by subject in Figure 3 (placed on 
the next page for space reasons).  All of the subsequent 
generic_np?s, which are a part of a direct object, an indirect 
object, a post-positional phrase, etc. are unitarily handled by the 
same trace mechanism. By hypothesizing that all sentences start 
with generic_np, the system can parse Korean robustly and 
efficiently.  The trace mechanism determines the grammatical 
function of generic_np by repositioning it after the appropriate 
case marker. 
Utilization of overt case markers to improve the parsing efficiency 
precisely captures  the commonly shared intuition for parsing 
relatively free word order languages with overt case markers such 
as Korean and Japanese, compared with parsing relatively strict 
word order languages with no overt case markers such as English:  
In languages like English, the verb of a sentence plays the crucial 
role in reducing the ambiguity via the verb subcategorization 
frame information on the co-occuring noun phrases, [1,3,11].   In 
languages like Korean, however, it is typically the case marker 
which identifies the grammatical function of the co-occuring noun 
phrase, assuming the role similar to that of verbs in English.  The 
current proposal is the first explicit implementation of this 
intuition, instantiated by the novel idea that all noun phrases are 
moved out of  the case marked phrases immediately following 
them. 
 
2.2 Meaning Representation and Generation 
The CCLINC Korean-to-English translation system achieves high 
quality translation by (i) robust mapping of the parsing output into 
the semantic frame, and  (ii) word sense disambiguation on the 
basis of the selection preference between two grammatical 
relations (verb-object, subject-verb, head-modifier) easily 
identifiable from the semantic frame, [13].  The former facilitates 
the accurate word order generation of various target language 
sentences, and the latter, the accurate choice of the target language 
word given multiple translation candidates for the same source 
language word. Given the parsing output in Figure 3, the system 
produces the semantic frame in Figure 4:5 
 
                                                                
5 Strictly speaking, the meaning representation in Figure 4 is not 
truly language neutral in that the terminal vocabularies are 
represented in Korean rather than in interlingua vocabulary. It is 
fairly straightforward to adapt our system to produce the meaning 
representation with the terminal vocabularies specified by an 
interlingua.  However, we have made a deliberate decision to 
leave the Korean vocabularies in the representation largely (1) to 
retain the system efficiency for mapping parsing output into 
meaning representation, and (2) for unified execution of 
automation algorithms for both Korean-to-English and English-
to-Korean translation. And we would like to point out that this 
minor compromise in meaning representation still ensures the 
major benefit of interlingua approach to machine translation, 
namely, 2 x N sets of grammar rules for N language pairs, as 
opposed to 2N. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
          {c statement 
                        :topic {q  pronoun 
                 :name ?URi? } 
               :pred {p pul_v 
                              :topic {q problem 
               :name ?MunJe?  
                              :pred {p EoRyeoUn } } } 
 
Figure 4. Semantic Frame  for the input sentence ?URi Ga 
EoRyeoUn MunJe Reul PulEox Da.? 
The semantic frame captures the core predicate-argument 
structure of the input sentence in a hierarchical manner, [9,10] 
(i.e. the internal argument, typically object, is embedded under the 
verb, and the external argument, typically subject, is at the same 
hierarchy as the main predicate, i.e. verb phrase in syntactic 
terms). The predicate and the arguments along with their 
representation categories are bold-faced in Figure 4. With the 
semantic frame as input, the generation system generates the 
English translation using the grammar rules in (1), and the Korean 
paraphrase using the grammar rules in (2). 
The semantic frame captures the core predicate-argument 
structure of the input sentence in a hierarchical manner, [9,10] 
(i.e. the internal argument, typically object, is embedded under the 
verb, and the external argument, typically subject, is at the same 
hierarchy as the main predicate, i.e. verb phrase in syntactic 
terms). The predicate and the arguments along with their 
representation categories are bold-faced in Figure 4. With the 
semantic frame as input, the generation system generates the 
English translation using the grammar rules in (1), and the Korean 
paraphrase using the grammar rules in (2). 
(1)  a. statement :topic :predicate               
       b. pul_v  :predicate :topic 
(2) a. statement :topic :predicate 
      b. pul_v  :topic :predicate 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1b) and (2b) state that the topic category for the object follows 
the verb predicate in English, whereas it precedes the verb 
predicate in Korean. 
The predicate-argument structure also provides a means for word 
sense disambiguation, [13,15]. The verb pul_v is at least two-way 
ambiguous between  solve and  untie. Word sense disambiguation 
is performed by applying the rules, as in (3). 
 (3) a .pul_v   b .pul_v 
                   problem pul+solve_v     thread  pul+untie_v 
 
(3a) states that if the verb pul_v occurs with an object of type 
problem, it is disambiguated as pul+solve_v. (3b) states that the  
verb occurring with an object of type thread is disambiguated as 
pul+untie_v. The disambiguated verbs are translated into solve 
and untie, respectively, in the Korean-to-English translation 
lexicon. 
 
1.2 Knowledge-Based Automated Acquisition 
of Grammars 
To overcome the knowledge bottleneck for robust translation and 
efficient system porting in an interlingua-based system [7], we 
have developed a technique for automated acquisition of grammar 
rules which leads to a simultaneous acquisition of  rules for (i) the 
parser, (ii) the mapper between the parser and the semantic frame, 
and (iii) the generator. 
The technique utilizes a list of words and their corresponding 
parts-of-speech in the corpus as the knowledge source, 
presupposes a set of knowledge-based rules to be derived from a 
word and its part-of-speech pair, and gets executed according to 
the procedure given in Figure 5. The rationale behind the 
technique is that (i) given a word and its part-of-speech, most of 
the syntactic rules associated with the word can be automatically 
derived according to the projection principle (the syntactic 
subj_marker 
sentence 
clause sentence_marker 
subject object verbs 
np_trace obj_marker modifier np_trace 
noun adj noun verb 
statement 
Ga URi Reul EoRyeoUn MunJe PulEox Da 
Figure 1. Parse Tree for the Sentence URi Ga   EoRyeoUn MunJe    Reul PulEox 
representation must observe the subcategorization properties of 
each lexical item) and the X-bar schema (major syntactic 
categories such as N, V, Adj, Adv project to the same syntactic 
structures)  in linguistic theories, [2], and (ii) the mapping from 
the syntactic structure to the semantic frame representation is 
algorithmic. The specific rules to be acquired for a language 
largely depend on the grammar of the language  for parsing.  
Some example rules acquired for the verb BaiChiHa (arrange) in 
Korean ? consistent with the parsing technique discussed in 
Section 2.1 ?  are given in (4) through (7). 
 
Initialization: Create the list of words and their parts-of-speech in 
the corpus. 
Grammar Update: For each word and its associated part-of-
speech, check to see whether or not the word and the rules 
associated with the corresponding part-of-speech occur in each 
lexicon and grammar.  
 If they already occur, do nothing. 
 If not:  
(i) Create the appropriate rules and vocabulary items 
for  each entry. 
(ii) Insert the newly created rules and vocabulary items 
into the appropriate positions of the 
grammar/lexicon files for the parser, the grammar 
file for the mapper between the parser and  the 
semantic frame, and the grammar/lexicon files for 
the generator . 
 
Figure 5.  Automated Gammar Acquistion Procedure 
 
(4) Rules for the parser6 
.verbs 
[negation] vBaiChiHa [negation] [aspect] [tense] [auxiliary] 
[negation] [aspect] [tense] [and_verbs] [or_verbs] 
 
.vBaiChiHa 
#BaiChiHa 
 
(5) Rules for the mapper from the parser to the semantic frame 
.bachiha_v 
vBaiChiHa 
                                                                
6 The rules for the parser for the verb tell in English are given 
below, to illustrate the dependency of the rules acquired  to the 
specific implementation of the grammar of the language for 
parsing: 
   .vp_tell 
     vtell [adverb_phrase] dir_object [v_pp]  
 vtell [adverb_phrase] indir_object dir_object  
 vtell [adverb_phrase] dir_object v_to_pp [v_pp] 
 vtell [adverb_phrase] dir_object that_clause 
 vtell [and_verb] [or_verb] [adverb_phrase] dir_object wh_clause 
 
   The contrast in  complexity of verb rules in (4) for Korean, and (i) 
for English, reflects the relative importance of the role played by 
verbs for parsing in each language. That is, verbs play the minimal 
role in Korean, and the major role in English for ambiguity 
reduction and efficiency improvement. 
 
(6) Lexicon for the generation vocabulary 
baichiha_v V2 ?arrang? 
V2    V ?e? ING ?ing? PP ?ed? THIRD ?es? ROOT ?e? 
PAST ?ed? PASSIVE ?ed? 
 
(7) Rules for the generation grammar 
baichiha_v        :predicate :conj :topic :sub_clause 
np-baichiha_v   :noun_phrase :predicate :conj :topic :sub_clause 
 
The system presupposes the flat phrase structure for a sentence in 
Korean, as shown in Figure 3, and therefore the rules for the verbs 
do not require the verb subcategorization information, as in (4). 
The optional elements such as [negation], [tense], etc. are possible 
prefixes and suffixes to be attached to the verb stem, illustrating a 
fairly complex verb morphology in this language. The rules for 
the generation grammar in (7) are the subcategorization frames for 
the verb arrange in English, which is the translation of the 
Korean verb baichiha_v, as given in (6).   
The current technique is quite effective in expanding the system?s 
capability when there is no large syntatically annotated corpus 
available from which we can derive and train the grammar rules,  
[14], and applicable across languages in so far as the notion of 
part-of-speech, the projection principle and the X-bar schema is 
language independent.  With this technique, manual acquisition of 
the knowledge database for the overall translation system is 
reduced to the acquisition of  (i) the bilingual lexicon, and (ii) the 
corpus specific top-level grammar rules which constitute less than 
20% of the total grammar rules in our system. And this has 
enabled us to produce a fairly large-scale interlingua-based 
translation system within a short period of time.  One apparent 
limitation of  the technique, however, is that it still requires the 
manual acquisition of corpus-specific rules  (i.e. the patterns 
which do not fall under the linguistic generalization).  And we are 
currently developing a technique for automatically deriving 
grammar rules and obtaining the rule production probabilities 
from a syntactically annotated corpus. 
 
3. EVALUATION AND RESEARCH 
ISSUES 
We have trained the system with about 1,600 Korean newspaper 
articles on ?missiles? and ?chemical biological warfare?, as in 
Table 1. 
Table 1. Korean-to-English translation training data statistics 
# of 
articles 
# of  
sents/article 
# of 
words/sent 
# of distinct 
      words 
1,631           24 17 15,220 
 
For quality evaluation, we have adopted a 5-point scale evaluation 
score, defined as follows.  Score 4: Translation is both accurate 
and natural. Score 3: Translation is accurate with minor 
grammatical errors which do not affect the intended meaning of 
the input, e.g. morphological errors such as ?swam vs. swimmed.? 
Score 2: Translation is partially accurate, and sufficient for 
content understanding. Most errors are due to inaccurate word 
choice, inaccurate word order, and partial translation. Score 1: 
Translation is word-for-word, and partial content understanding is 
possible. Score 0: There is no translation output, or no content 
understanding is possible.  
We have performed the quality evaluation on 410 clauses from the 
training data, and 80 clauses from the test data. We have 
conducted the evaluation in 3 phases. Eval 1: Baseline evaluation 
after grammar and lexicon acquisition. Eval 2: Evaluation after 
augmenting word sense disambiguation rules. Eval 3: Evaluation 
after augmenting word sense disambiguation rules and accurate 
word order generation rules. The purpose of the 3-phase 
evaluation was to examine the contribution of parsing, word sense 
disambiguation and accurate word order generation to the overall 
translation quality. Once the score had been assigned to each 
clause, the translation score was obtained by the formula:  (Sum 
of  the scores for each clause *  25) / Number of clauses 
evaluated. 
Evaluation results are shown in Table 2 and Table 3 in terms of 
parsing coverage (P) and the translation score (T).7 
Table 2. Translation Quality Evaluation on Training Data 
            Eval 1             Eval 2             Eval 3 
       P         T        P        T         P        T 
      92      58        94       69       94      74 
 
Table 3. Translation Quality Evaluation on Test Data 
            Eval 1             Eval 2             Eval 3 
        P       T         P        T         P       T 
      79      55       89       63       89      65 
 
For both training and test data, the baseline translation quality 
score is over 50, sufficient for content understanding of the 
documents. Word sense disambiguation (Eval 1 vs. Eval 2) 
increases the translation score by about 10%, indicating that 
effective word sense disambiguation has a great potential for 
improving the translation quality.    
We would like to point out  that the evaluations reported in this 
paper are performed on clauses rather than sentences (which often 
consist of more than one clause).  In a very recent evaluation, we 
have found out that evaluations on sentences decrease the overall 
translation score about by 15.  Nevertheless, the translation 
quality is still good enough for content understanding with some 
effort.  The primary cause for the lower translation scores when 
the evaluation unit is a sentence as opposed to a clause is due to 
either an incorrect clause boundary identification, or some 
information (e.g. missing arguments in embedded clauses) which 
cannot be easily recovered after a sentence  is fragmented into 
clauses. This has led to the ability to handle complex sentences as 
                                                                
7 We would like to note that the evaluation reported here was a 
self-evaluation of the system by a system developer, primarily to 
identify the key research issues in system development. We will 
report evaluation results by non system developers who have no 
knowledge of  Korean in the future.  A system evaluation by  a 
non-bilingual speaker will avoid the issue of implicitly utilizing 
the knowledge  the  evaluator has about the source language in 
the evaluation process. 
the primary research issue, and we are working out the solution of 
utilizing syntactically annotated corpus for both grammar and 
probability acquisition, as discussed in Section 2.3. 
 
4. SUMMARY AND ONGOING WORK 
We have described the key features of the CCLINC interlingua-
based Korean-to-English translation system which is capable of 
translating a large quantity of Korean newspaper articles on 
missiles and chemical biological warfare in real time. Translation 
quality evaluations on the training and test data indicate that the 
current system produces translation sufficient for content 
understanding of a document in the training domains.  The key 
research issues identified from the evaluations include (i) parsing 
complex sentences, (ii) automated acquisition of word sense 
disambiguation rules from the training corpus,  and (iii) 
development of discourse module to identify the referents of 
missing arguments.  Our solution to the key technical challenges 
crucially draws upon the utilization of annotated corpora: For 
complex sentence parsing, we acquire both rules and rule 
production probabilities from syntactically annotated corpus. For 
automated word sense disambiguation, we utilize a sense-tagged 
corpus to identify various senses of a word, and obtain 
probabilities for word senses in various contexts.  For discourse 
understanding, we are developing an algorithm for our 2-way 
speech translation work, [12], and plan to expand the module for 
document translations. 
 
5. ACKNOWLEDGMENTS  
We would like to acknowledge Dr. Jun-Tae Yoon, who provided 
us with a high-quality robust Korean morphological analyzer 
called morany during his stay at the Institute for Research in 
Cognitive Science, University of Pennsylvania as a postdoctoral 
fellow. Morany has served as a pre-processor of the understanding 
module in the CCLINC Korean-to-English translation system. 
 
6. REFERENCES 
[1] Srinivas Bangalore and Aravind Joshi. ?Some Novel 
Applications of Explnation-Based Learning for Parsing 
Lexicalized Tree-Adjoining Grammars,? Proceedings of  33rd 
Association for Computational Linguistics.  pp. 268?275. 1995.  
[2] Noam Chomsky. Barriers.  Linguistic Inquiry Monograph 13. 
MIT Press, Cambridge, MA. 1986. 
[3] Michael Collins. Three Generative, Lexicalized Models for 
Statistical Parsing. Procceedings of the 35th Annual Meeting of 
ACL. pp. 16?23. Madrid, Spain. July. 1997. 
[4] Bonnie Dorr. ?LCS-based Korean Parsing and Translation,? 
Ms. Institute for Advanced Computer Studies and Department of 
Computer Science, University of Maryland. 1997. 
[5] Diana Egedi, Martha Palmer, H-S. Park, Aravind Joshi. 
?Korean to English Translation Using Synchronous TAGs,? 
Proceedings of  the First Conference of the Association for 
Machine Translation in the Americas. pp. 48?55. Columbia, 
Maryland. October 1994. 
[6] James Glass, Joe Polifroni and Stephanie Seneff. 
?Multilingual Language Generation across Multiple Domains,? 
Proceedings of International Conference on Spoken Language 
Processing, pp. 983?986. Yokohama, Japan. September, 1994. 
[7] W.J. Hutchins and H.L. Somers. An Introduction to Machine 
Translation. Academic Press. London. 1992. 
[8] James Allen. Natural Language Understanding, 2nd Edition. 
Benjamin-Cummings Publisher. 1995 
[9] Ken Hale. ?Preliminary Remarks on Configurationality,? 
Proceedings of NELS 12,  pp. 86?96. 1982. 
[10] Young-Suk Lee. Scrambling as Case-Driven Obligatory 
Movement. PhD Thesis (IRCS Report No.: 93-06 ). University of  
Pennsylvania. 1993. 
[11] Young-Suk Lee, Clifford Weinstein, Stephanie Seneff, 
Dinesh Tummala, ?Ambiguity Resolution for Machine 
Translation of  Telegraphic Messages,? Proceedings of  the 35th 
Annual Meeting of ACL. pp. 120?127. Madrid, Spain. July 1997. 
[12] Young-Suk Lee and Clifford Weinstein. ?An Integrated 
Approach to English-Korean Translation and Translingual 
Information Access,? Proceedings of CSTAR Workshop.  
Schwetzingen, Germany.  September, 1999. 
[13] Young-Suk Lee, Clifford Weinstein, Stephanie Seneff, 
Dinesh Tummala. ?Word Sense Disambiguation for Machine 
Translation in Limited Domains,? Manuscript. Information 
Systems Technology Group. MIT Lincoln Laboraotry. January 
1999. 
[14]  Mitch Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. ?Building a large annotated corpus of English: the 
Penn Treebank,? Computational Linguistics 19 (2). pp. 313?
330. 1993. 
[15] Philip Resnik. ?Semantic Similarity in a Taxonomy: An 
Information-Based Measure and Its Application to Problems of 
Ambiguity in Natural Language,? Journal of Artificial 
Intelligence Research (JAIR) 11. pp. 95?130. 1999. 
[16] Stephanie Seneff. ?TINA: A Natural Language System for 
Spoken Language Applications,? Computational Linguistics 18 
(1). pp. 61?92. 1992. 
[17] Clifford Weinstein, Young-Suk Lee, Stephanie Seneff, 
Dinesh Tummala, Beth Carlson, John T. Lynch, Jung-Taik 
Hwang, Linda Kukolich. ?Automated English-Korean Translation 
for Enhanced Coalition Communications,? The Lincoln 
Laboratory Journal 10 (1).  pp. 35?60. 1997. 
 
 
 
 
 
 
The Use of Dynamic Segment Scoring for
Language-Independent Question Answering  
Daniel Pack

and Clifford Weinstein
MIT Lincoln Laboratory
244 Wood Street
Lexington, Massachusettes
dpack@ll.mit.edu
cjw@ll.mit.edu
ABSTRACT
This paper presents a novel language-independent question/answering
(Q/A) system based on natural language processing techniques,
shallow query understanding, dynamic sliding window techniques,
and statistical proximity distribution matching techniques. The per-
formance of the proposed system using the latest Text REtrieval
Conference (TREC-8) data was comparable to results reported by
the top TREC-8 contenders.
Keywords
Question/Answer, Natural Language Processing, Query Understand-
ing, Dynamic Sliding Window, Proximity Distribution
1. INTRODUCTION
Over the past decade, the TREC community has invested its ef-
forts on and advanced technologies of automatic information re-
trieval systems. Recently, the same community decided to divide
the traditional information retrieval task to several so called tracks:
the cross-language information retrieval track, the filtering track,
the interactive track, the question and answering track, the query
track, the spoken document retrieval track, and the web track[6].
The decision is mainly due to the mature technologies in the tradi-
tional information retrieval field and the desire to expand the tech-
nologies to additional areas of interest. The goal of the question
and answering track is the development of systems that generate
concise answers to user queries. This goal is similar in nature to
the goal of a traditional information retrieval system where relevant
 
This work was funded by DARPA under Air Force Contract
F19628-00-c-0002. Opinions, interpretations, conclusions, and
recommendations are those of the authors and do not necessarily
represent the views of the agency or the US Air Force.

Daniel Pack is an associate professor of Electrical Engineering
from the Air Force Academy on his sabbatical leave.
.
documents are extracted for user queries; users are then required to
read through the selected documents to find answers. In a ques-
tion answering system, it is the system's responsibility to find the
answers to queries.
Queries Data
Q/A System
A B
Concept Tagging
Stemming
POS Tagging
PreprocessingQuery
Processing
C D
Dynamic  Sliding Windows for Sizing Segments
Keywords/POS/Q-Concept Weighted Matching
Matching Phrases Based on
Statistical Distributions
Final Selection
Frame Final Answers
Final Answer
Formulation
Extraction of 
Candidate
Segments
C
E
Proximity Computation
Query Identification
Concept Tagging
POS Tagging
Stemming
Preprocessing
Translation Module
Answers
F
Translation Module
Data
Processing
Figure 1: The Question and Answering System Architecture
In this paper, we present a Q/A system that combines (1) natural
language processing techniques, (2) query understanding, (3) dy-
namic sliding window techniques, and (4) keyword distance prox-
imity distribution matching techniques for a language-independent
question/answering system. The system architecture is shown in
Figure 1. We call the system language-independent since the sys-
tem architecture remains the same regardless of any particular lan-
guage used. The only requirement is to have a translation module
at the front end and the back end of our system. Developing such
systems is becoming increasingly important as the diverse commu-
nities across national boundaries are brought together through the
internet. The effectiveness of the proposed system architecture is
validated with experimental results.
"I always knew they wanted," he said. "They wanted something about Joe."
<\P>
<\P>
<P>
One day, though, someone ran a different notion by DOM: A book about 1941.
<P>
If ever the major leagues had a magical, almost mythic year, it was 1941. There was Joe 
DiMaggio?s 56-game hitting streak. There was Ted Williams? .406 batting average. There was
the anticipated, but nonetheless gripping, death of Lou Gehrig. There was Mickey Owen?s
dropped third strike in the World Series.
<\P>
<P>
And beyond the outfield walls, there was a worried America, waiting and watching as World War
II headed its way. Two months after the 1941 world Series, the Japanese planes attacked Pearl Harbor.
<\P>
...............................................
<P>
(a) input
they/PRONOUN want/DESIRE he/PRONOUN say/AFFIRMATION they/PRONOUN
want/DESIRE something/SUBSTANTIALITY about/ABOUT joe/PERSON one/NUMBER
day/PERIOD though/COMPENSATION someone/PRONOUN run/CONTINUANCE a /DT
different/DIFFERENCE notion/IDEA by/BY dom/PERSON a/DT book/BOOK about/ABOUT 
1941/TIME if/CIRCUMSTANCE ever/PERPETUITY the/DT major/SIGNIFICANT league/PARTY
had/POSSESSION a/DT magical/SORCERY almost/IMPERFECTION mythic/IMAGINATION
joe/PERSON dimaggio/PERSON ?s/POS 56-game/TIME hit/IMPULSE streak/SEQUENCE  
there/PRESENCE was/EXISTENCE t/PERSON william/PERSON 406/NUMBER bat/AMUSEMENT
average/MEAN there/PRESENCE was/EXISTENCE the/DT anticipated/PERSON but/BUT
nonetheless/COMPENSATION grip/TENACITY death/DEATH of/OF lou/PERSON gehrig/PERSON
there/PRESENCE was/EXISTENCE mickey/PERSON owen/PERSON ?s/POS drop/DESCENT
third/NUMBER strike/ATTACK in/IN the/DT world/WORLD series/SEQUENCE and/AND 
watch/ATTENTION as/AS world/WORLD war/WARFARE ii/NUMBER head/DIRECTOR its/PRONOUN
way/DEGREE two/NUMBER month/PERIOD after/POSTERIORITY the/DT 1941/TIME world/WORLD
series/SEQUENCE the/DT japanese/COUNTRY plane/AIRCRAFT attack/ATTACK peral/ORNAMENT
harbor/STORE ..........................................................
i/PRONOUN always/GENERALITY know/KNOWLEDGE what/WHAT
year/PERIOD it/PRONOUN was/EXISTENCE 1941/TIME there/PRESENCE was/EXISTENCE
(b) output
Figure 2: A sample input and output of the Data Processing
module
2. SYSTEM DESCRIPTION
In this section we present the system architecture of the proposed
Q/A system and describe its components in detail. The system con-
tains five different modules as shown in Figure 1. The top module is
responsible for translating input queries and a set of documents to a
common language. The common coalition language system devel-
oped at MIT Lincoln Laboratory (CCLINC)[8] performs the trans-
lation tasks. For the work reported here, we assume that queries
are in English, documents are in either English or Korean, and an-
swers are returned in English. Our focus in this paper is on the four
modules between the two translation modules (modules contained
in the box with a dotted line) in Figure 1.
The Query Processing module and the Data Processing module
use natural language processing techniques such as parsing, mor-
phological stemming and part of speech and concept tagging for
word sense disambiguation to extract critical query and document
information. In addition, the Query Processing module categorizes
queries and assigns appropriate answer concepts associated with
each query. In the next two modules, candidate segments with op-
timal matching scores of keywords and answer concepts are ex-
tracted using dynamic sliding windowing techniques. The candi-
date segments are then further analyzed based on the similarities of
proximity distributions of search keywords and rank ordered.
A case example, a query and a document segment from the TREC-
8 official data, is used throughout this section to illustrate functions
of the four processing modules. Our illustration starts with the fol-
lowing query entering the Query Processing module.
Query: In what year did Joe DiMaggio compile his 56-game hit-
ting streak?
Several processes take place within the Query Processing mod-
ule: a preprocessing unit removes punctuation marks and extra
spaces; a trained Brill tagger[1] tags each word with correspond-
ing part of speech tags; a set of morphological rules and a con-
cept trained Brill tagger convert words into their root forms and
determine answer concepts; a proximity indexing unit records the
keyword positions in queries; and a query identification/post pro-
cessing unit removes stop words and formats the output, as shown
below.
Output of the Query Processing module: Question Special 101
NNT year TIME 2 NNP joe PERSON 4 NNP dimaggio PERSON
5 VB compile ASSEMBLAGE 6 NN 56-game TIME 8 VB hit IM-
PULSE 9 NN streak SEQUENCE 10
The output contains critical query information including answer
concepts which are identified by categorizing queries using a method
similar in spirit to extracting named entities[5, 4], named focuses[2],
and question-answer tokens[3]. Each stemmed keyword is tagged
with a POS tag, a concept tag, and an index number. The POS
tags are used to discriminate search terms by assigning different
weights, the concept tags are used to identify answer concepts, and
the index numbers are used to compute proximity values between
terms for matching.
Documents, represented with symbol B in Figure 1, go through a
similar procedure in the Data Processing Module as did a query in
the Query Processing Module. Due to the large data size of the doc-
ument collection, the documents are processed off line. The input
and the output of the module for an example document segment
is shown in Figure 2. The output of the data processing module
is processed documents with stemmed words and their associated
concepts, represented with symbol D in Figure 1.
The Extraction of Candidate Segments module selects candidate
segments that contain answers. The size of each candidate segment
is determined by a dynamic sliding window, which uses an iterative
procedure to maximize the score of a segment as its size changes.
To ensure the optimal segmentation of a document, adjacent seg-
ments are overlapped while the size of the window can vary from
one sentence to tens of sentences, as shown in Figure 3. To deter-
mine the optimal size for a current sliding window, the score for
an initial window with one sentence is compared to scores corre-
his/PRONOUN brother/CONSANGUINITY like/SIMILARITY his/PRONOUN 
call/NOMENCLATURE he/PRONOUN automatically/NECESSITY say/AFFIRMATION
they/PRONOUN want/DESIRE he/PRONOUN say/AFFIRMATION they/PRONOUN
day/PERIOD though/COMPENSATION someone/PRONOUN run/CONTINUANCE a /DT
different/DIFFERENCE notion/IDEA by/BY dom/PERSON a/DT book/BOOK about/ABOUT 
had/POSSESSION a/DT magical/SORCERY almost/IMPERFECTION mythic/IMAGINATION
average/MEAN there/PRESENCE was/EXISTENCE the/DT anticipated/PERSON but/BUT
nonetheless/COMPENSATION grip/TENACITY death/DEATH of/OF lou/PERSON gehrig/PERSON
there/PRESENCE was/EXISTENCE mickey/PERSON owen/PERSON ?s/POS drop/DESCENT
third/NUMBER strike/ATTACK in/IN the/DT world/WORLD series/SEQUENCE and/AND 
watch/ATTENTION as/AS world/WORLD war/WARFARE ii/NUMBER head/DIRECTOR its/PRONOUN
way/DEGREE two/NUMBER month/PERIOD after/POSTERIORITY the/DT 1941/TIME world/WORLD
series/SEQUENCE the/DT japanese/COUNTRY plane/AIRCRAFT attack/ATTACK peral/ORNAMENT
harbor/STORE ..........................................................
no/NEGATION i/PRONOUN always/GENERALITY know/KNOWLEDGE what/WHAT
/PERSON one/NUMBERwant/DESIRE something/SUBSTANTIALITY about/ABOUT joe
TIMEprivacy/SECLUSION so/GREATNESS when/            the/DT book/BOOK person/PERSON
TIME1941/            if/CIRCUMSTANCE ever/PERPETUITY the/DT major/SIGNIFICANT league/PARTY
year TIME     /PERIOD it/PRONOUN was/EXISTENCE 1941/              there/PRESENCE was/EXISTENCE 
joe dimaggio 56-game/TIME streakhit/ PERSON                 /PERSON ?s/POS                                   /IMPULSE            /SEQUENCE  
there/PRESENCE was/EXISTENCE t/PERSON william/PERSON 406/NUMBER bat/AMUSEMENT
Matched Words: 
streak
Matched Concept: TIME
joe, dimaggio,
56-game, hit,
Matched Words:  
Matched Concept: TIME
joe
Matched Words: joe, year, dimaggio,
streak
Matched Concept: TIME
56-game, hit,
Figure 3: An example of applying dynamic sliding window
techniques: Three adjacent optimally formulated windows are
shown. The top window segment with four sentences contains
the query concept ?TIME? and matching word ?joe.? The sec-
ond window with five sentences contains the query concept and
six keywords. The last window with two sentences contains the
query concept and five keywords.
sponding to windows with increasing number of sentences. The
scoring criteria is based on appearances of answer concepts and
query keywords in candidate segments. Weighted scores are as-
signed to keywords in segments; the contribution of a match varies
according to the query keyword's part of speech tag. Specifically,
the score for a match decreases according to the following priority
list in the order shown: (1) answer concept, (2) quoted keyword, (3)
proper noun keyword, (4) noun keyword, and (5) all other keyword.
Figure 3 shows an example case of using the dynamic sliding
window technique. In this figure, the darkened window contains the
answer to the example query, 1941. Optimally sized windows form
candidate segments that are rank ordered based on their scores.
Currently, we select and send top 200 segments per query (sym-
bol E in Figure 1) to the Final Answer Formulation module.
The Final Answer Formulation module takes an advantage of the
keyword proximity distributions in queries and the corresponding
statistical keyword distributions in candidate segments to further
distinguish segments with high likelihoods of containing answers
from those that merely contain search terms and query concepts.
The module creates a list of proximity distributions from a keyword
to the rest of keywords as shown in Figure 4. In this figure, the left
hand column shows the distance distributions from a query key-
word to the rest of query keywords. The index numbers for query
keywords are used here to compute the distributions. The right col-
umn shows the corresponding distance distributions in a candidate
segment. Once the distributions are available, the job of the Final
Answer Formulation module is to search for candidate segments
with similar keyword proximity distributions to those appeared in
queries. By distance, we mean the word counts that separate two
terms
Distance
terms
Distance
term n
Distance
term 1
term 2
terms
Distance
terms
Distance
term 1
term 2
term n
Distance
Candidate Segment
terms terms
Query
Figure 4: Matching distance distributions of keywords between
a query and a candidate segment
keywords.
Recall the format of the output from the query processing mod-
ule. Using the differences between index numbers to specify phys-
ical distance relationships among query keywords, we can compute
the corresponding proximity distributions of keywords in candidate
segments. We create a list of distributions by computing proximity
distances from a keyword to the rest of keywords.
1 2 3 4 5 6 7 8
0
2
4
6
8
10
12
14
Term: year
Query
Data
Terms
d
is
ta
n
c
e
Distance Distribution from term 1
(a)
1 2 3 4 5 6 7 8
0
2
4
6
8
10
12
14
Term 5: 56?game
Query
Data
Terms
d
is
ta
n
c
e
Distance Distribution from term 5
(b)
Figure 5: Proximity distribution examples
Figure 5 shows two actual distribution graphs of our example.
Frame (a) shows that the distances from keyword year in query
(dashed line) to other keywords. The vertical axis represents phys-
ical word distance while the horizontal axis denotes query terms.
I II III IV V VI VII
I (0,0) (2,6) (3,7) (4, ) (6,9) (7,10) (8,11)
II (2,6) (0,0) (1,1) (2, ) (4,3) (5,4) (6,5)
III (3,7) (1,1) (0,0) (1, ) (3,2) (4,3) (5,4)
IV (4, ) (2, ) (1, ) (0, ) (2, ) (3, ) (4, )
V (6,9) (4,3) (3,2) (2, ) (0,0) (1,1) (2,2)
VI (7,10) (5,4) (4,3) (3, ) (1,1) (0,0) (1,1)
VII (8,11) (6,5) (5,4) (4, ) (2,2), (1,1) (0,0)
Table 1: Distance pairs separating query keywords
The distance values grow from 2 for keyword joe to 8 for keyword
streak. The solid line shows the distance distribution of the same
keywords appearing in a candidate segment. The numbers vary
from 6 for keyword joe to 11 for keyword streak. The pattern of
gradual increase, however, in both lines indicates a similarity be-
tween the two distributions. The break in the solid line is caused
by the missing term, compile, in the candidate segment. Frame
(b) again shows the proximity distributions from keyword 56-game
to the rest of keywords in the query and the candidate segment.
The distance values for the candidate segment are 9, 3, 2, 1, and
2 while the corresponding distances in the query are 6, 4, 3, 1,
and 2. Note that the last two data points are identical for both dis-
tributions. Again, we find a similar distribution pattern in both the
query and the candidate segment. The similarities between the vari-
ances of the distributions in both a query and a candidate segment
determine the likelihood of the particular segment containing an
answer to the query. Table 1 shows the actual distance differences
between keywords in the query and the candidate segment. Key-
words year, joe, dimaggio, compile, 56-game, hit, and streak are
represented by I, II, III, IV, V, VI, and VII, respectively. For each
pair in the table, the first number represents the distance between
the corresponding keywords (row/column) in the query while the
second number shows the distance between the same keywords in
the candidate segment. Blanks represent that distances can not be
computed because the particular keyword pair could not be found
in the candidate segment.
The similarities between the variances of the distributions in both
a query and a candidate segment determine the likelihood of the
particular segment containing an answer to the query. For the ex-
periments, we used a simplified version of the distribution matching
where only adjacent query term distances were compared.
The equation for assigning a final score for each candidate seg-
ment is as follows.
Segment Score  Normalized Original Score
 Current Pair Proximity Score
 Processed Term Score
where Normalized Original Score represents the score generated by
the Extraction of the Candidate Segment module and
Current Pair Proximity Score 

 diff    x std
max
x

number of term pairs in query
Processed Term Score  current score x
number of term pairs processed in query
number of term pairs in query
where symbol max is a normalization factor and symbol diff is the
proximity difference between a query and a candidate segment for a
given pair of keywords. Symbol std is the standard deviation of the
distance values between two keywords in the candidate segments.
The standard deviation term helps further differentiate scoring be-
tween a common pair and pairs which do not appear often.
Once all candidate segments are scored, the top five1 segments
are selected based on their final scores: a segment with the mini-
mum length was chosen in cases when scores for multiple segments
are equal. The top segment for the example candidate at this point
is
They wanted something about Joe. One day, though, someone
ran a different notion by Dom: A book about 1941. If ever the ma-
jor leagues had a magical, almost mythic year, it was 1941. There
was Joe Dimaggio's 56-game hitting streak.
The selected segments are then sent to the final answer fram-
ing stage where only the corresponding keywords matching desired
question concepts are extracted. The final answer for the example
query is ?1941? which had associated concept tag ?TIME.? This
answer is the output fed into the translation module, if necessary,
shown as symbol F in Figure 1. Presently, our system does not per-
form the final answer framing process using the concept tags. The
system simply applys a set of rules to remove stop words to reduce
the final answer size.
3. EXPERIMENTAL RESULTS
We conducted two different experiments: monolingual and translin-
gual experiments. The monolingual experiment used the TREC-8
questions and the documents extracted by the AT & T information
retrieval engine[5]. For the translingual experiment, our prelimi-
nary experimental results are based on a set of 10 queries in English
and 877 Korean newspaper articles, containing Korean equivalent
word missile.
We adopted the same criteria used at the TREC-8 Q/A track
meeting [7] for our system evaluation. For the monolingual ex-
periment, answers to two queries didn't exist in the original data.
Furthermore, we found that answers to four additional queries were
not contained in the retrieved documents, making the total num-
ber of queries to 194. The system found correct answers in the
top five selections for 73.2% of questions (142/194). Answers to
103 queries were found as the first selections. Table 2 shows the
categorized results based on question types. The average number
of words per answer was 34.68 (approximately 244 bytes/answer).
The value will significantly decrease provided that the final answer
framing stage in the Final Answer Formulation module is imple-
mented.
The current overall score would have placed the system in the top
third at the TREC-8 Q/A meeting[7].2 The current research focus

The particular number, five, is chosen to adhere the criteria of the
TREC Q/A Track evaluation.

We hasten to add that a fair comparison can only be made in the
Type # Q Score Type # Q Score
Who 45/194 0.7378 How 31/194 0.4707
When 18/194 0.5185 Which 7/194 0.7857
Where 21/194 0.5754 Why 2/194 0.625
What 58/194 0.6261 Name 4/194 0.75
Others 7/194 0.1429 Overall 194/194 0.6019
Table 2: Experimental Results using TREC-8 Data
is to further improve the system performance using query concept
term matching in addition to the current query keyword matching.
We also plan to devise better tools to answer non-standard queries.
For the translingual Q/A experiment, the following 10 queries
were used.
	 Which country launched a missile?
	 Which countries are involved in missile development?
	 What is the difference between missile and satellite?
	 What is the status of North Korea's missile technology?
	 What did North Korea request to United States for ceasing of
their missile export?
	 Why did North Korea launch a missile?
	 Where did the missile land?
	 When was a missile launched?
	 What is the South Korean government policy toward North
Korea?
The overall score for the translingual experiment was 0.4833.
This performance is achieved by turning off the proximity distribu-
tion process since the translation did not generate expressions simi-
lar to ones found in the queries3. Answers were not found in the top
five selections for two queries; answers for only two queries were
found as the top selections (20% versus approximately 53% for the
English experiment). The performance discrepancies between the
monolingual Q/A experiment and the translingual Q/A experiment
are twofold. A higher percentage of translingual questions required
a ?deep? level understanding of the queries to identify correct an-
swers in the database. The second, more important factor, was that
the translated documents were not true equivalents of the original
Korean documents. Many sentences were not fully parsed, resort-
ing to a word by word translation without the use of contextual in-
formation. We are currently exploring ways to overcome the prob-
lem. Nevertheless, given the early stage of the system development,
we are encouraged by the high translingual performance of the sys-
tem.
next TREC meeting since our system was able to exploit the pub-
lished queries while other systems did not.


It was difficult to separate the translingual Q/A system perfor-
mance from the performance of the translation system since the
Q/A system results depended on the accurate document translation.
4. CONCLUSION
In this paper, we showed a novel language-independent question
and answering system. The unique features of the system are the
use of the POS tags to distinguish terms appearing in queries for
differential weights, dynamic sliding windows that automatically
adjust the optimal size of a candidate segment containing answers,
and the proximity matching techniques that award similarities be-
tween query keyword distance distributions and the corresponding
distributions in data segments for best fit, which is based on statis-
tical distributions of search terms in the data set. The system also
incorporates popular methods of categorizing queries to identify
desired answers using concept tags and natural language processing
techniques such as the preprocessing, stemming, and POS tagging,
which also contributed to the high performance results reported.
5. REFERENCES
[1] E. Brill, ? A Simple Rule-Based part of Speech Tagger,?
Proceedings of the Third Conference on Applied. Natural
Language Processing,pp.152-155, ACL, Trento, Italy, 31
March - 3 April, 1992.
[2] Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Richard Goodrumm, Roxana Girju, and Vasile
Rus, ?LASSO: A Tool for Surfing the Answer Net,?
Proceedings of the Eighth Text REtrieval Conference, pp.
175-184, November, 1999.
[3] John Prager, Dragomir Radev, Eric Brown, Anni Coden,
Valerie Samn, ?The Use of Predictive Annotation for
Question Answering in TREC-8,? Proceedings of the Eighth
Text REtrieval Conference, pp. 399-410, November, 1999.
[4] Rohini Srihari and Wei Li, ?Information Extraction
Supported Question Answering,? Proceedings of the Eighth
Text REtrieval Conference, pp.185-196, November 1999.
[5] Amit Singhal, John Choi, Donald Hindle, David Lewis,
Fernando Pereira, ?AT & T at Trec-7,? Proceedings of the
Seventh Text REtrieval Conference, pp. 239-252, November,
1998.
[6] Ellen Voorhees and Donna Harman, ?Overview of the Eighth
Text REtrieval Conference(TREC-8),? Proceedings of the
Eighth Text REtrieval Conference, November, 1999.
[7] Ellen Voorhees and Dawn Tice, ?The TREC-8 Question
Answering Track Evaluation,? Proceedings of the Eighth
Text REtrieval Conference, November, 1999.
[8] Clifford Weinstein, Young-Suk Lee, Stephanie Seneff,
Dinesh Tummala, Beth Carlson, John Lynch, Jun-Taik
Hwang, and Linda Kukolich, ?Automated English-Korean
Translation for Enhanced Coalition Communications,? The
Lincoln Laboratory Journal, vol. 10, no. 1, pp. 35-60, 1997.
