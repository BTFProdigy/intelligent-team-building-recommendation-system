COMBINAT ION OF N-GRAMS AND STOCHASTIC  
CONTEXT-FREE GRAMMARS FOR LANGUAGE MODEL ING*  
J os6 -Migue l  Benedf  and  Joan-Andreu  SSnchez  
De, i )artmnento de Sistemas Informgti(:os y Coml)utaci6n 
Universidad Polil;(!ClfiCa (te Valencia 
Cmnino de \Sera s/n,  d6022 Valencia (Slmin) 
e-mail: {.jt)enedi,j andreu} (@dsic.ul)v.es 
Abst rac t  
This t)al)t;r de, scribes a hybrid prol)osal to 
combine n-grams and Stochastic Context-Free 
Grammars (SCFGs) tbr language modeling. A 
classical n-gram model is used to cat)lure the 
local relations between words, while a stochas- 
tic grammatical inodel is considered to repre- 
sent the hmg-term relations between syntactical 
stru(:tm'es. In order to define this granmlatical 
model, which will 1)e used on large-vo(:almlary 
comph'~x tasks, a eategory-t)ased SCFG and a 
prol)abilisti(" model of' word (tistrilmtion in the 
categories have been 1)rol)osed. Methods for 
leanfing these stochastic models tTor complex 
tasks are described, and algorithms for con> 
puting the word transition probal)ilities are also 
1)resented. Filmily, ext)erilnents using the Penn 
Treel)ank corpus improved by 30% the test; set; 
l)erph~xity with regard to the classical n-gram 
models. 
1 I n t roduct ion  
Language modeling is an important asl)e('t to 
consider in large-vocabulary Sl)eeeh recognition 
systenls (Bahl et al, 1983; ,lelinek, 1998). The 
n--grain models are the most widely-used for a 
wide range of domains (Bahl et al, 1983). The 
n-grams are simple and robust models and ad- 
equately capture the local restrictions between 
words. Moreover, it is well-known how to es- 
timate the parameters of the lnodet and how 
to integrate them in a speech recognition sys- 
tem. However, the n-grmn models cannot ad- 
equately characterize the long-term constraints 
of the sentences of the tasks. 
On the other hand, Stochastic Context-Free 
Grammars (SCI)'Gs) allow us a better model- 
* This work has been partially SUl)l)<)rted by the S1)anish 
CICYT under contract (TIC98/0423-C(16). 
ing of long-term relations and work well on 
lhnited-domain tasks of low perplexity. How- 
ever, SCFGs work poorly for large-vocabulary, 
general-purpose tasks because learning SCFGs 
and the (:Olnlmtation of word transition 1)roba - 
bilities present serious 1)roblenls tLr ('olnplex real 
tasks. 
In the literature, a nulnber of works have pro- 
posed ways to generalize the n-gram models (.le- 
linek, 1998; Siu and Ostendorf, 2000) or com- 
1)ining with other structural models (Bellegarda, 
1998; Gilet and Ward, 1998; Chellm and Jelinek, 
1998). 
In this iml)er, we present a confl)ined lan- 
guage model defined as a linear combination of 
n-grams, whk'h are llse(t to capture the local re- 
lations between words, and a stoehasti(: gram- 
matieal model whi(:h is used to represent he 
glottal relation 1)etw(x'dl synl:aetie strll(;tllrt~s, hi 
or(ter to (:at)turc, these lollg-terltl relations an(t to 
solve the main 1)rolflems derived Dora the large- 
vocabulary complex tasks, we 1)l'Ol)ose here to 
detine: a eategory--ba,~ed SCFG and a prolmbilis- 
tic model of word distrilmt;ion in the categories. 
Taking into a(:count his proposal, we also de- 
scribe here how to solve the learning of these 
stochastic models and their integrati(m prol> 
1CIlIS. 
With regard to the learning problem, several 
algorithms that learn SCFGs by means of es- 
timation algorithms have been 1)reposed (Lari 
and Young, 1990; Pereira and Schal)es, 1992; 
Sfinehez and Benedi, 1998), and pronfising re- 
suits have been achieved with category-based 
SCFGs on real tasks (Sfi.nchez and Benedi, 
\]999). 
In relation to the integration problem, we 
l)resent wo algorithms that compute the word 
transition 1)robability: the first algorithm is 
based on the l~efl;-to-ll,ight Inside algorithln 
55 
(LRI) (Jelinek and Lafferty, 1991), and the sec- 
ond is based on an application of a Viterbi 
scheme to the LRI algorithm (the VLRI algo- 
rithm) (S~nehez and Benedf, 1997). 
Finally, in order to evaluate the behavior of 
this proposal, experiments with a part of the 
Wall Street Journal processed in the Penn Tree- 
bank project were carried out and significant im- 
provements with regard to the classical n-gram 
models were achieved. 
2 The  language mode l  
An important problem related to language mod- 
eling is the evaluation of Pr(wk I w l . . .  wk-1). 
In order to compute this probability, we pro- 
pose a hybrid language model defined as a sire- 
ple linear combination of n-gram models and a 
stochastic grammatical model G~: 
Pr(wklwl . . .w~_l)  = c~Pr(~klwk-n.. .wt~-l)  
+(1 - c~) P"(wklw~... wk-,, G~), (1) 
where 0 < c~ < 1 is a weight factor which de- 
pends on the task. 
The expression Pr(w/~lwk_n...wk-,) is the 
word probability of occurrence of w/~ given by 
the n-gram model. The parameters of this 
model can be easily estinmted, and the ex- 
pression Pr(wl~lWl~_n...  wtc-1) can be efficiently 
computed (Bahl et al, 1983; Jelinek, 1998). 
In order to define the stochastic gram- 
matical model G~ of the expression 
Pr(wk\]w~ . . . wk_ j ,  G~) for large-vocabulary 
complex tasks, we propose a combination of 
two different stochastic models: a category- 
based SCFG (G~), that allows us to represent 
the long-term relations between these syntacti- 
cal structures and a probabilistic model of word 
distribution into categories (Cw). 
This proposal introduces two imlmrtant as- 
peels, which are the estimation of the parame- 
ters of the stochastic models, Gc and Cw, and 
the computation of the following expression: 
Pr(~klWl... Wk-1, ac, Cw) 
= Pr(wl . . .  wk.. .  la~, c~) (2) 
IC , C,,,) 
3 Training of the models 
The parameters of the described model are es- 
timated fi'om a training sample, that is, from 
a set of sentences. Each word of the sentence 
has a part-of  speech tag (POStag) associated 
to it. These POStags are considered as word 
categories and are the terminal symbols of the 
SCFG. h 'om this training sample, the parame- 
ters of G~ and C~ can be estimated as tbllows. 
First, tile parameters of Cw, represented by 
Pr(w\[c), are computed as: 
= 
E,o,  (3) 
where N(w,c) is the number of times that the 
word w has been labeled with the POStagc.  It 
is important o note that a word w can belong to 
different categories. In addition, it may hapt)en 
that a word in a test set does not appear in 
the training set, and therefore some smoothing 
technique has to be carried out. 
With regard to the estimation of the category- 
based SCFGs, one of the most widely-known 
methods is the Inside-Outside (IO) algo- 
rithln (Lari and Young, 1990). The application 
of this algorithm presents important problems 
which are accentuated in real tasks: the time 
complexity per iteration and the large number 
of iterations that are necessary to converge. An 
alternative to the IO algorithm is a.n algorithm 
based on the Viterbi score (VS algorithm) (Ney, 
1992). The convergence of the VS algorithm 
is faster than the IO algorithm. However, the 
SCFGs obtained are, in genera.l, not as well 
learned (Simchez et al, 1996). 
Another possibility for estimating SCFCs, 
which is somewhere between the IO and VS al- 
gorithms, has recently been proposed. This ap- 
proach considers only a certain subset of deriva- 
tions in the estimation process. In order to 
select this subset of derivations, two alterna- 
tives have been considered: froln structural in- 
formation content in a bracketed corpus (Pereira 
and Schabes, 1992; Amaya et al, 1999), and 
from statistical information content in the k- 
best derivations (Sgmchez and Benedl, 1998). 
In the first alternative, the IOb and VSb algo- 
rithms which learn SCFGs from partially brack- 
eted corpora were defined (Pereira and Schabes, 
1992; Amaya et al, 1999). In the second alter- 
native, the kVS algorithm for the estimation of 
the probability distributions of a SCFG fl'om the 
k-best derivations was proposed (Shnchez and 
Benedi, 1998). 
56 
All of these algorithms have a tilne (:omi)lexity 
O('n,a\[PI), where 'n is the length of the input 
st;ring, and \[1)1 is the size. of the SCFG. 
These algorithms have been tested in 
real tasks fl)r estimating cat(,gory-1)ased 
SCFOs (Sfinchez and Benedf, 1999) and the 
results obtained justify their applicatiol, in 
complex real tasks. 
4 In tegrat ion  o f  the  mode l  
l?rom exl)ression (2), it can bee se(m that in or- 
der to integrate the too(M, it is necessary to 
efli(:iently ('oml)ute the expression: 
P~0,,~... ',,,k... la,.., <,,). (4) 
In order to describo how this computation (:an 
l)e m~de, we tirst introduce some notation. 
A Court:el-Free, Grammar G is a four-tul)le 
(N, E, P, S), wher(; N is the tinit(; set of nont(;r- 
minals, )2 is the tinite sol; of terminals (N ~-/E = 
0), S ~ N is the axiom or initial symbol and 
1' is the finite set of t)rodu(:tions or ruh;s of the 
tbrm A -+ it, where A c N a.nd c~ C (N U E) + 
(only grmmmtrs with non (;mt)ty rules ar(; con- 
sidered). FOI" siml)li('ity (but without loss of g('.n- 
erality) only (:ontext-iYee grammars in Ch, om.s'ky 
Normal Form are. considere(l, that is, grammars 
with rules of the form A -+ HC or A -> v wh(n'(: 
A ,B ,C  C N and v ~ )2. 
A Stoch, a.stic Contcxt-l';rc.c U'raw, w, wl" G.~ is a 
pair (G,p), where G is a (:ontext-fr(,.(; grain- 
mar and p : P -+\]0,1\] is a 1)robal)ility tim(:- 
{;ion of rule ai)l)li('al;ion su(:h that VA ~ N: 
}~,c(Nu>~)+ p(A --+ ,~) - -  i. 
Now, we pr(:sent two algorithms ill order to 
compute the word transition 1)rol)at)ility. The 
first algorithm is based on the Ll/i algorithm, 
a.nd the second is based on an apt)li('atiou of a 
Viterbi s(:heme to the LRI algorithln (the VLI/\] 
a.lgorithm). 
P robab i l i ty  of  generat ing  an init ial  
subst r ing  
The COmlmtation of (4) is l)as('.d on an algo- 
rithm which is a modith:ation of the I,RI algo- 
rithm (aelinek and Lafl'erty, 1991). This new 
algorithln is based on the detlnition of Pr(A << 
i , j ) )  = Pr(A ~ wi . . .w j  . . .  Ic . .  c,,,) as th(, 
1)robability that A generates the initial sul)string 
wi . . .  w j . . .  given Gc and C.,,,. This can l)e com- 
puted with the following (lynamic 1)rogrmmning 
s(:henl(;: 
c 
+ ~ Q(A ~ D)p(D ~ ~)P"(~"~l~)), 
D 
.i-1 
Pr (d<<i , j )=  E ~ Q(A ~ BC) 
B,CGN l=i 
pl.(J~ < ~:, 1 >) p,.(c << 1 + 1, j ) .  
::: this way, Pr (~,~. . .~,k . . . IG, : ,6 ' , , , )  = 
Pr(S << l,k).  
In this cxi)ression, Q(A ~ D) is the proba- 
bility that D is the leftmost nol:terminal in all 
sentential fOHllS which are derived from A. The 
vahu; Q(A ~ BC)  is the probability that BC 
is th(; initial substring of all sentential forms de- 
riv(;d from i\. Pr(H < i, l  >) is th{; probability 
that the substring "wi... wz is generated from/~ 
given G,: and C.,,,. Its contlmt;ation will be de- 
fined \]ater. 
It shouh:l be noted that th(; combination of 
the models G,. and C~,, in carried out in the vah:e 
P'r(A << i, i). This is the lnain difl:'crcnce with 
resp(wt the \]A/I algorithm. 
P robab i l i ty  of  the  best  der ivat ion  
generat ing  an init ial  subst r ing  
An algorithm whi(:\]l is similar to the previous 
(>he (-m~ l)e (l(~fin(~d t)ased on the \;iterl)i ,~(:lmme. 
In this way, it is l)onsil)le to obtain the \])cst; pars- 
ing of an initial sul)string. This new algorithm is 
also related to the \/'Lll.I algol'ithni (Shn(:hez and 
B('aw, di, 1997) and is 1)ased on the (lciinition of 
P,~'(A << ',:, J)) = P,~'(A ~ ",,i . . . ',,j . . . IG~:, Cw) 
as the probability of the most probal)le 1)arsing 
which generates wi . . .w j . . ,  from A given G,: 
and C,,. This can 1)(i (:omputcd as follows: 
p:(A << i , , : )=  m:?x(p(A ~ c)l),(~,,~l,.'), 
m~Lx(Q(A => D)'p(D ~ c)Pr('wi\[c))), 
D 
PI~'(A << i , j )  = max :mix  (Q(A ~ \]3C) 
H,CcN l=i...j-I 
A 
Pr(\]3 < i, l  >)Pr (C  << l + 1,j)) . 
A A 
Thorcfo,o >4",~...'wk... \[a,:, <,,)  -- p , . (s  << 
1,k). 
In this expression, Q(A ~ D) is the t)rob - 
al)ility that D is the leftmost nontermina, l in 
57 
the most t)robable sentential form which is de- 
rived ti'om d. The value Q(A ~ BC) is the 
probability that BC is the initial substring of 
most the probable sentential form derived from 
A. Pr(B < i, 1 >) is the probability of the most 
probable parse which generates wi ? ? ? wl froli1 B. 
P robab i l i ty  of generating a string 
The wflue Pr(A < i , j  >) = Pr(A d> 
wi...'u;jlG~,Go) is defined as the probability 
that the substring wi... wj is generated from 
A given G~ and C,~. To calculate this proba- 
bility a modification of the well-known Inside 
algorithm (Lari and Young, 1990) is proposed. 
This computation is carried out by using the 
following dynamic progralmning scheme: 
Pr(A < i,i >) -- ~p(A-~ c) Pr(wilc) , 
c 
j - - I  
Pr (A<i , j>)  : E E p(A -+ BC) 
B,CcN l=i 
pr(B < i,l >)Pr (C < 1 + 1,j >). 
In this way, Pr(w~ ...whiGs, C,,) = Pr(S < 
1,n >). 
As we have commented above, the combina- 
tion of the two parts of the grammatical model 
is carried out in the value Pr(A < i, i >). 
Probabi l i ty of the best derivation 
generating a string 
The t)rol)abitity of the best derivation that 
genel'~-gtes a s t r ing ,  P r ( 'u ,1 . . .  ~t/2,~l~c, 6 'w)  , can  
be evaluated using a Viterbi-like scheme (Ney, 
1992). As in the previous case, the computation 
of this probability is based on the definition of 
p .(A < g,j >) = pU-(A <,,) as 
the probability of the best derivation that gen- 
erates the substring wi... wj fi'om A given Gc 
and Cw. Similarly: 
P r (A<i , i>)  = 
Pr(A < i , j  >) = 
Therefore, 
1, n >). 
i nax  \ ] ) (a  -9  C)I)I'('//)i lC) , C 
max n ,ax  -+Be)  
B,CCN ... 
Pr(B < i, l  >)Pr (C  < 1 + 1, j  >) . 
 nla , C,,o) = P -(X < 
Finally, the time complexity of these algo- 
rithms is the same as the algorithms they are 
related to, there%re the time colnplexity is 
O(k:alrl), where tc is the length of the input 
string and IPI is the size of the SCFG. 
5 Exper iments  w i th  the  Penn 
Treebank  Corpus  
The corpus used in the experiments was the part 
of the Wall Street Journal which had been pro- 
cessed in the Petal %'eebank project 1 (Marcus el: 
al., 1993). This corpus consists of English texts 
collected from the Wall Street Journal from edi- 
tions of the late eighties. It contains approx- 
imately one million words. This corpus was 
automatically labelled, analyzed and manually 
checked as described in (Marcus et 31., 1993). 
There are two kinds of labelling: a POStag la- 
belling and a syntactic labelling. The size of 
the vocalmlary is greater than 25,000 diil'erent 
words, the POStag vocabulary is composed of 
45 labels 2 and the syntactic vocabulary is com- 
posed of 14 labels. 
The corpus was divided into sentences accord- 
ing to the bracketing. In this way, we obtained 
a corpus whose main characteristics are shown 
in Table 1. 
Table 1: Characteristics of the Petal Treebank 
corpus once it; was divided into sentences. 
No. of Av. Std. Min. Max. 
senten, length deviation length length 
49,207 23.61 11.13 1 249 
We took advantage of the category-based 
SCFGs estimated in a previous work (Simchez 
and Benedf, 1998). These SCFGs were esti- 
mated with sentences which had less than 15 
words. Therefore, in this work, we assumed such 
restriction. The vocabulary size of the new cor- 
pus was 6,333 different words. For the exper- 
iments, the corpus was divided into a training 
corpus (directories 00 to 1.9) and a test corpus 
(directories 20 to 24). The characteristics of 
these sets can be seen in Table 2. The part of the 
1Release 2 of this data set can be ob- 
tained t'rmn the Linguistic Data Consor- 
tium with Catalogue number LDC94T4B 
(http://www.ldc.upenn.edu/ldc/nofranm.html) 
2There are 48 labels defined in (Marcus et al, 1993), 
however, three of ttmm do not appear in the corpus. 
58 
(-orlms lal)(:led with l)()Stags was used to (:st;i- 
mate the p~wameters of tlm grammati(:al me(M, 
while the non-lad)e\](;(l part was u,s(',d i;o estimate 
th(; parameters (it" the n-grmn lnodc.l. \?(~ now 
des('ribe the estinmtion l)roec,,~s in (l('%ail. 
Table 2: Chm'acteristics of th(: data. s('A;s (h~iined 
for the eXl)eriments wh(',n the senl:en(:(~s wi(;h 
more l;lmn 15 l)OSl;ags were r(;moved. 
Da.ta \[ No. of I Av. Std. \] 
l(,ngl h deviation S(~, J; SellI;ell. \ ] _ .  
~l.i;st . 2,295 1 .1~ 3.55 
The 1)a.rmn(%er,q of a 3-grmn too(l(;1 were ('~s- 
t imatcd with the softw~re tool des('rit)('.(1 in 
(l/,osenfehl, 1995) :t. W(~ u,qed tlm linear ini;(',rl/o- 
la.tion ~qmooth t('~(:lmiqu(~ SUpl)orted by ,;hi,~ tool. 
Th('~ o1:l;-of-v(/(:al)lflary words wcr('~ groul/e(l in 
the same (:\]as,~ and w(u'e used in th(~ ('omt)ula- 
1;ion of i;\]~('~ perl)h~'xity. ~.l'h(,. I:(~sl ,~(:I l)(~rl)l('~xity 
with t:his mo(lel was 180.4. 
T\]w, values ()f ('.xt)r(~,qsi()n (3) wure (:()ml)ut(!(t 
from the t~:.gged and l:on-l:agg(:(t i)alq; O1' \[;lie, 
l:raining corpus. In or(h'a' to avoid mill val- 
ues, the m~seen (~'vents wer('~ lal)ele(1 with ~ Sl)C- 
cial symbol 'w' wlfich did not a pl)ear in (;he 
i ,  s.,:h -? 0, 
Vc C (/, whtu'(~ (/ was I:h('~ set ()f (:at('.g()ri(~s. 
That  is, all th(: ('at(',gori(~s could g(',n(:rat(', i;\]:(: uu-- 
,q('x'~n evenI, This l)rolml)ility took a. v(~ry small 
valu(; (s(',v('.ral ()rd(;rs of magnii;u(l('~ h'~,qs tlmn 
minw~v,c(:c l)r('wlc), where V was the \.'o('alm- 
lary of the tra.ining corpus), and (liffer(mI; vahte.~ 
of this i)robability did not chang('~ tlm r(~sults. 
The i)aramet('~rs of an initial ergodic SCFG 
were estimated with each one of the estimation 
methods mentioned in Se('tion 3. This SCFG 
had 3,374 rules, (:omt)osed fl'om 45 terminal 
syml)ols (the numl)er of l)()Stags) and \]d non- 
terminal symbols (the nmnber of synl;a('l;i(: la- 
bels). The prol)z~l)ilitics were rmidolnly gem'a'- 
ated mid t;hree different seeds were tested, lint 
only one of them is reported given that the re- 
suits were very similar. The training (:orlms was 
the labele(l part of the des('ril)ed (:orlm,q. The 
1)erl)lexity of the labeled part of the test; s(:t for 
all.clcas(~ 2.04 is availal)le at htl;l)://svr- 
www.cng.cmn.ac.uk/~ 1)rcl4/toolkit.html. 
diti'(n'('~nt (~stimation algorithms (;a.n l)c. ,~cen in 
Tal)le 3. 
\[I.'abl(~ 3: PCxl)lexity of the labeled 1)?tl't; Of I;\]lO 
test set with the SCFC, est imated with the 
methods mentioned in Section 3. 
7 1 /'~ vs l a,\ s l ,ot, I \  sb I 
Once we had est imated the lmramei;ers of the 
defined model, we applied expression (1) 1)y us- 
ing the IAI,\] algorithm m~d the VI.\[/,I algorithm 
in ('~xt)w, ssion (d). Th(; test set lWa'l)lexitly that 
was ol)I;ained in flmction t)f (t %r difl't'a'(:nt; esti- 
nm~tion algorithms (VS, kVS, lOb mid VSb) can 
be seen in \]rig. 1. 
In the best case, the tn'ot)osed l~mguage model 
el)rained more than a. 30% inlI)rOVellle:l|; OVer 
re,~ults ol)taincd 1)y the 3-gram lmlguagc motM 
(s(w. ~l'at)le d). This result wa,q ol)t;ainc.d wh('~n 
th(: SCFG usl;imat(~d with the lOb algorit;hm 
wa,~ u,~(;(1. The SCFGs ('.stimat('.(l with ()ther al- 
g()ril;hms also ()l)tain('.d iml)ortanl; ilnlirovt,.nw.lfl;,~ 
(:Oinl)ar(;d l;o \[;he 3-grain. In ~(t(lition, ii; can be 
oliserv('.d i;hat t)oth th(,. LI-(.I algorithm and the 
VIA/I algoril;hm obtained good results. 
Tal)le d: \]3(~,st tx~st lW, rl)lexity for difl'(~,rem; SCFG 
(:.%imation algori thins, and I;h(~ \])er(:cntage of im- 
i)rovt'.mc~lll; with resi)(wl; I;o i;hc 3-gram model. 
VSm kVS lOb VSI) 
\]All 133.6 130.3 124.6 136.3 
i % improv. 25.9% 27.8% 30.9% 24.5% 
\[ V\]~l:l I ~ 137.2 I 13Z4 I V,9.7 I 
\[ %iml)rov. 20.5% 23.0% 26.6% 17.0% 
An important aspect to note is (;hat the 
weight; of the grmmnatical part was approxi- 
mat;ely 50%, which means that this part pro- 
vided iml)ori;mlI; inform~tion to the language 
model. 
6 Conc lus ions  
A nc'w language model has been introduced. 
This new language model is detined as a~ lin- 
ear ('olnl)in~ttion of an n-gram which repre- 
s(mts relations betwe('~n words, and a stochastic 
59 
20O 
190 
180 
> 
"~ 17(1 
160 
150 
14(\] 
13(\] 
120 
20O 
19(\] 
180 
'~ 17(\] 
160 
15(\] 
14(1 
130 
120 
;,, /l:j~ 
3-ffrgtn: 
VSb  
kVS VS 
IOb  
0 3 0 4 0.5 0.6 0.1 0.2 
~/ 
3~gt'anl 
VSb  
VS 
kVS 
lOb  
0.7 0,8 0.9 
/ 
0.1 0 2 0.3 0.4 0.5 0.6 0 7 0.8 0.9 0 (t 
Figure 1: Test set perplexity obtained with 
the proposed language models in function of 
gamma. Different curves correspond to SCFGs 
estimated with different algorithms. The up- 
per graphic correst)onds to the results obtained 
when the LRI algorithm was used in the lan- 
guage models, and the lower graphic corre- 
sponds to the results obtained with the VLRI 
algorithm. 
grammatical model which is used to represent 
the global relation between syntactic structures. 
The stochastic graminatical model is composed 
of a category-based SCFG and a probabilistic 
model of word distribution in the categories. 
Several algorithms have been described to esti- 
mate the parameters of the model flom a the 
smnple. In addition, efficient algorithms tbr 
solving the problem of the interpretation with 
this model have been presented. 
The proposed model has been tested on the 
part of Wall Street .Journal processed in the 
Penn Treebank project, and the results obtained 
improved by more tlmn 30% the test set; per- 
plexity over results obtained by a simple 3-grain 
model. 
Re ferences  
F. Amaya, J.M. Benedi, and J.A. Shuchez. 
1999. Learning of stochastic context-free 
grammars from bracketed corpora by means 
of reestimation algorithms. In M.I. Torres 
and A. Sanfeliu, editors, Proc. VIII Spanish 
Symposium on Pattern Recognition and Im- 
age Analysis, pages 119 126, Bilbao, Est)afia, 
May. AERFAI. 
L.R. Bahl, F. Jelinek, and ILL. Mercer. 1983. 
A maximmn likelihood approach to continu- 
ous speech recognition. IEEE Trans. Pattern 
Analysis and Machine Intelligence, PAMI- 
5(2):179 190. 
J.R. Bellegarda. 1998. A multispan language 
modeling frmnework tbr large vocabulary 
speech recognition. IEEE Trans. Speech and 
Audio Processing, 6(5):456-476. 
C. Chelba and F. Jelinek. 1998. Exploiting syn- 
tactic structure for lm~guage modeling. In 
Proc. COLING, Montreal, Canada. Univer- 
sity of Montreal. 
J. Gilet and W. Ward. 1998. A language model 
combining trigrams and stochastic ontext- 
fl'ee grammars. In In 5th International Con- 
.fercnce on Spoken Language Processing, pages 
2319 2322, Sidney, Australia. 
F. Jelinek and J.D. Lafferty. 1991. Coml)uta- 
tion of the probability of initial substring en- 
eration by stochastic ontext-free grammars. 
Computational Linguistics, 17(3):315 323. 
F. Jelinek. 1998. Statistical Meth, ods for Speech 
Recognition. MIT Press. 
K. Lari and S.J. Young. 1990. The estimation 
of stochastic ontext-fl'ee grmnmars using the 
inside-outside algorithm. Computer, Speech 
and Language, 4:35 56. 
M.P. Marcus, B. Santorini, and M.A. 
Marcinkiewicz. 1993. Buikting a large anno- 
tated corpus of english: the penn treebank. 
Computational Linguistics, 19 (2):313-330. 
H. Ney. 1992. Stochastic grmmnars and pattern 
recognition. In P. Laface and R. De Mori, ed- 
itors, Speech Recognition and Understanding. 
Recent Advances, pages 319 344. Springer- 
Verlag. 
F. Pereira and Y. Schabes. 1992. Inside- 
outside reestimation from partially brack- 
60 
cted corporm In Pwcccding,~' of the 30th An- 
n'ual A4ectin9 of the As,~ociatiou for Comp'uta- 
tional Linguistics, 1)ages 128 135. University 
of l)elawarc. 
ll,. Roscnfl'hl. 1.995. Th(' cmu sta.tisl:i(:al an- 
guage mo(Ming toolkit and its use in the 1994 
art)a csr evaluation. In ARPA Spoken Lan- 
guage Technology Workshop, Austin, Texas, 
USA. 
M. Siu and M. Osto.ndorf. 2000. \;al'iablc 
n-grams mid (;xl;(~.n,siolLs for convcrsatiomtl 
speech langu~g(' mo(h;ling. IEEI~' Tm,',,s. on 
Speech and A'udio P'roc(;s.sing, 8(1) :63 75. 
.I.A. Sfi.nch(;z and ,J.M. B(;ncdf. 11997. ()Olnl)llta- 
tion of the probability of the best (hwiw~t;ion of 
an initial substring fi'om a stochastic (:ontcxt- 
fl'ec; grammar. In A. Smffeliu, .\]..l. Villa.mleVa, 
and .J. \;itri;t, editors, Prm:. VII Spanish, Sym- 
posi'um, on Pattern Rccogu, itio'n and image 
Analysi. L pages 181-186, Barco.hma., Eslmfia, 
April. AERFAI. 
J.A. SSn(:h?;z mid J.M. Bcn('(lf. 1998. Esti- 
mation of the l)robability di,~tributions of 
sto(:ha,sti(: (;oni;('~xt-frc(; grammar,s from th(; \],:- 
1)(;st derivations. In b~, 5th, b~,ter'national Co'n- 
.f('r(:nc(: on Spoke',, Langua9(: l)'ro(:c,ssing, imgc,s 
2d95 2498, Sidney, Australia. 
.J.A. Sgmchcz mid ,\].M. Bcn(;(li. 1999. lx~rn- 
ing of ,%ochast, ic cont(~xt-fl'(~'c grammm's by 
memos of ('stima.tion ~flgol'ithms. In 1)'roe. EU- 
H, OSl)EECl\]'99, volume 4, lmges 1799 1802, 
Budal)cSt , Hungary. 
J.A. S~nchez, J.M. B(;nedf, and F. Casacu- 
berta. 1996. Comparison lmi,ween the insi(h~- 
outside algorithm mid the vitcrl)i a.lgorithm 
for stochastic context-fl'ee grammars. In 
P. Perncr, P. Wang, amd A. I{osenfe.ld, edi- 
tors, Advances in Str'uct'm'al and Syntactical 
Pattcrn ll, ccogu, ition, pages 50 59. Springer- 
Verlag. 
61 
Improvement of a Whole Sentence Maximum Entropy Language Model
Using Grammatical Features  
Fredy Amaya

and Jose? Miguel Bened??
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
Camino de vera s/n, 46022-Valencia (Spain)

famaya, jbenedi  @dsic.upv.es
Abstract
In this paper, we propose adding
long-term grammatical information in
a Whole Sentence Maximun Entropy
Language Model (WSME) in order
to improve the performance of the
model. The grammatical information
was added to the WSME model as fea-
tures and were obtained from a Stochas-
tic Context-Free grammar. Finally, ex-
periments using a part of the Penn Tree-
bank corpus were carried out and sig-
nificant improvements were acheived.
1 Introduction
Language modeling is an important component in
computational applications such as speech recog-
nition, automatic translation, optical character
recognition, information retrieval etc. (Jelinek,
1997; Borthwick, 1997). Statistical language
models have gained considerable acceptance due
to the efficiency demonstrated in the fields in
which they have been applied (Bahal et al, 1983;
Jelinek et al, 1991; Ratnapharkhi, 1998; Borth-
wick, 1999).
Traditional statistical language models calcu-
late the probability of a sentence  using the chain
rule:

	






Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 563?570,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Segmented and unsegmented dialogue-act annotation with statistical
dialogue models?
Carlos D. Mart??nez Hinarejos, Ramo?n Granell, Jose? Miguel Bened??
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
Camino de Vera, s/n, 46022, Valencia
{cmartine,rgranell,jbenedi}@dsic.upv.es
Abstract
Dialogue systems are one of the most chal-
lenging applications of Natural Language
Processing. In recent years, some statis-
tical dialogue models have been proposed
to cope with the dialogue problem. The
evaluation of these models is usually per-
formed by using them as annotation mod-
els. Many of the works on annotation
use information such as the complete se-
quence of dialogue turns or the correct
segmentation of the dialogue. This in-
formation is not usually available for dia-
logue systems. In this work, we propose a
statistical model that uses only the infor-
mation that is usually available and per-
forms the segmentation and annotation at
the same time. The results of this model
reveal the great influence that the availabil-
ity of a correct segmentation has in ob-
taining an accurate annotation of the dia-
logues.
1 Introduction
In the Natural Language Processing (NLP) field,
one of the most challenging applications is dia-
logue systems (Kuppevelt and Smith, 2003). A
dialogue system is usually defined as a com-
puter system that can interact with a human be-
ing through dialogue in order to complete a spe-
cific task (e.g., ticket reservation, timetable con-
sultation, bank operations,. . . ) (Aust et al, 1995;
Hardy et al, 2002). Most dialogue system have a
characteristic behaviour with respect to dialogue
? Work partially supported by the Spanish project
TIC2003-08681-C02-02 and by Spanish Ministry of Culture
under FPI grants.
management, which is known as dialogue strat-
egy. It defines what the dialogue system must do
at each point of the dialogue.
Most of these strategies are rule-based, i.e., the
dialogue strategy is defined by rules that are usu-
ally defined by a human expert (Gorin et al, 1997;
Hardy et al, 2003). This approach is usually diffi-
cult to adapt or extend to new domains where the
dialogue structure could be completely different,
and it requires the definition of new rules.
Similar to other NLP problems (like speech
recognition and understanding, or statistical ma-
chine translation), an alternative data-based ap-
proach has been developed in the last decade (Stol-
cke et al, 2000; Young, 2000). This approach re-
lies on statistical models that can be automatically
estimated from annotated data, which in this case,
are dialogues from the task.
Statistical modelling learns the appropriate pa-
rameters of the models from the annotated dia-
logues. As a simplification, it could be considered
that each label is associated to a situation in the di-
alogue, and the models learn how to identify and
react to the different situations by estimating the
associations between the labels and the dialogue
events (words, the speaker, previous turns, etc.).
An appropriate annotation scheme should be de-
fined to capture the elements that are really impor-
tant for the dialogue, eliminating the information
that is irrelevant to the dialogue process. Several
annotation schemes have been proposed in the last
few years (Core and Allen, 1997; Dybkjaer and
Bernsen, 2000).
One of the most popular annotation schemes at
the dialogue level is based on Dialogue Acts (DA).
A DA is a label that defines the function of the an-
notated utterance with respect to the dialogue pro-
cess. In other words, every turn in the dialogue
563
is supposed to be composed of one or more ut-
terances. In this context, from the dialogue man-
agement viewpoint an utterance is a relevant sub-
sequence . Several DA annotation schemes have
been proposed in recent years (DAMSL (Core and
Allen, 1997), VerbMobil (Alexandersson et al,
1998), Dihana (Alca?cer et al, 2005)).
In all these studies, it is necessary to annotate
a large amount of dialogues to estimate the pa-
rameters of the statistical models. Manual anno-
tation is the usual solution, although is very time-
consuming and there is a tendency for error (the
annotation instructions are not usually easy to in-
terpret and apply, and human annotators can com-
mit errors) (Jurafsky et al, 1997).
Therefore, the possibility of applying statistical
models to the annotation problem is really inter-
esting. Moreover, it gives the possibility of evalu-
ating the statistical models. The evaluation of the
performance of dialogue strategies models is a dif-
ficult task. Although many proposals have been
made (Walker et al, 1997; Fraser, 1997; Stolcke
et al, 2000), there is no real agreement in the NLP
community about the evaluation technique to ap-
ply.
Our main aim is the evaluation of strategy mod-
els, which provide the reaction of the system given
a user input and a dialogue history. Using these
models as annotation models gives us a possible
evaluation: the correct recognition of the labels
implies the correct recognition of the dialogue sit-
uation; consequently this information can help the
system to react appropriately. Many recent works
have attempted this approach (Stolcke et al, 2000;
Webb et al, 2005).
However, many of these works are based on the
hypothesis of the availability of the segmentation
into utterances of the turns of the dialogue. This is
an important drawback in order to evaluate these
models as strategy models, where segmentation is
usually not available. Other works rely on a de-
coupled scheme of segmentation and DA classifi-
cation (Ang et al, 2005).
In this paper, we present a new statistical model
that computes the segmentation and the annota-
tion of the turns at the same time, using a statis-
tical framework that is simpler than the models
that have been proposed to solve both problems
at the same time (Warnke et al, 1997). The results
demonstrate that segmentation accuracy is really
important in obtaining an accurate annotation of
the dialogue, and consequently in obtaining qual-
ity strategy models. Therefore, more accurate seg-
mentation models are needed to perform this pro-
cess efficiently.
This paper is organised as follows: Section 2,
presents the annotation models (for both the un-
segmented and segmented versions); Section 3,
describes the dialogue corpora used in the ex-
periments; Section 4 establishes the experimental
framework and presents a summary of the results;
Section 5, presents our conclusions and future re-
search directions.
2 Annotation models
The statistical annotation model that we used ini-
tially was inspired by the one presented in (Stol-
cke et al, 2000). Under a maximum likeli-
hood framework, they developed a formulation
that assigns DAs depending on the conversation
evidence (transcribed words, recognised words
from a speech recogniser, phonetic and prosodic
features,. . . ). Stolcke?s model uses simple and
popular statistical models: N-grams and Hidden
Markov Models. The N-grams are used to model
the probability of the DA sequence, while the
HMM are used to model the evidence likelihood
given the DA. The results presented in (Stolcke et
al., 2000) are very promising.
However, the model makes some unrealistic as-
sumptions when they are evaluated to be used as
strategy models. One of them is that there is a
complete dialogue available to perform the DA
assignation. In a real dialogue system, the only
available information is the information that is
prior to the current user input. Although this al-
ternative is proposed in (Stolcke et al, 2000), no
experimental results are given.
Another unrealistic assumption corresponds to
the availability of the segmentation of the turns
into utterances. An utterance is defined as a
dialogue-relevant subsequence of words in the cur-
rent turn (Stolcke et al, 2000). It is clear that the
only information given in a turn is the usual in-
formation: transcribed words (for text systems),
recognised words, and phonetic/prosodic features
(for speech systems). Therefore, it is necessary to
develop a model to cope with both the segmenta-
tion and the assignation problem.
Let Ud1 = U1U2 ? ? ?Ud be the sequence of DA
assigned until the current turn, corresponding to
the first d segments of the current dialogue. Let
564
W = w1w2 . . . wl be the sequence of the words
of the current turn, where subsequences W ji =
wiwi+1 . . . wj can be defined (1 ? i ? j ? l).
For the sequence of words W , a segmentation
is defined as sr1 = s0s1 . . . sr, where s0 = 0 and
W = W s1s0+1W
s2
s1+1 . . .W
sr
sr?1+1. Therefore, the
optimal sequence of DA for the current turn will
be given by:
U? = argmax
U
Pr(U |W l1, U
d
1 ) =
argmax
Ud+rd+1
?
(sr1,r)
Pr(Ud+rd+1 |W
l
1, U
d
1 )
After developing this formula and making sev-
eral assumptions and simplifications, the final
model, called unsegmented model, is:
U? = argmax
Ud+rd+1
max
(sr1,r)
d+r?
k=d+1
Pr(Uk|U
k?1
k?n?1) Pr(W
sk?d
sk?(d+1)+1
|Uk)
This model can be easily implemented using
simple statistical models (N-grams and Hidden
Markov Models). The decoding (segmentation
and DA assignation) was implemented using the
Viterbi algorithm. A Word Insertion Penalty
(WIP) factor, similar to the one used in speech
recognition, can be incorporated into the model to
control the number of utterances and avoid exces-
sive segmentation.
When the segmentation into utterances is pro-
vided, the model can be simplified into the seg-
mented model, which is:
U? = argmax
Ud+rd+1
d+r?
k=d+1
Pr(Uk|U
k?1
k?n?1) Pr(W
sk?d
sk?(d+1)+1
|Uk)
All the presented models only take into account
word transcriptions and dialogue acts, although
they could be extended to deal with other features
(like prosody, sintactical and semantic informa-
tion, etc.).
3 Experimental data
Two corpora with very different features were
used in the experiment with the models proposed
in Section 2. The SwitchBoard corpus is com-
posed of human-human, non task-oriented dia-
logues with a large vocabulary. The Dihana corpus
is composed of human-computer, task-oriented di-
alogues with a small vocabulary.
Although two corpora are not enough to let us
draw general conclusions, they give us more reli-
able results than using only one corpus. Moreover,
the very different nature of both corpora makes
our conclusions more independent from the cor-
pus type, the annotation scheme, the vocabulary
size, etc.
3.1 The SwitchBoard corpus
The first corpus used in the experiments was the
well-known SwitchBoard corpus (Godfrey et al,
1992). The SwitchBoard database consists of
human-human conversations by telephone with no
directed tasks. Both speakers discuss about gen-
eral interest topics, but without a clear task to ac-
complish.
The corpus is formed by 1,155 conversations,
which comprise 126,754 different turns of spon-
taneous and sometimes overlapped speech, using
a vocabulary of 21,797 different words. The cor-
pus was segmented into utterances, each of which
was annotated with a DA following the simpli-
fied DAMSL annotation scheme (Jurafsky et al,
1997). The set of labels of the simplified DAMSL
scheme is composed of 42 different labels, which
define categories such as statement, backchannel,
opinion, etc. An example of annotation is pre-
sented in Figure 1.
3.2 The Dihana corpus
The second corpus used was a task-oriented cor-
pus called Dihana (Bened?? et al, 2004). It is com-
posed of computer-to-human dialogues, and the
main aim of the task is to answer telephone queries
about train timetables, fares, and services for long-
distance trains in Spanish. A total of 900 dialogues
were acquired by using the Wizard of Oz tech-
nique and semicontrolled scenarios. Therefore,
the voluntary caller was always free to express
him/herself (there were no syntactic or vocabu-
lary restrictions); however, in some dialogues, s/he
had to achieve some goals using a set of restric-
tions that had been given previously (e.g. depar-
ture/arrival times, origin/destination, travelling on
a train with some services, etc.).
These 900 dialogues comprise 6,280 user turns
and 9,133 system turns. Obviously, as a task-
565
Utterance Label
YEAH, TO GET REFERENCES AND THAT, SO, BUT, UH, I DON?T FEEL COMFORTABLE ABOUT LEAVING MY KIDS IN A BIG
DAY CARE CENTER, SIMPLY BECAUSE THERE?S SO MANY KIDS AND SO MANY <SNIFFING> <THROAT CLEARING>
Yeah, aa
to get references and that, sd
so, but, uh, %
I don?t feel comfortable about leaving my kids in a big day care center, simply because there?s so
many kids and so many <sniffing> <throat clearing> sd
I THINK SHE HAS PROBLEMS WITH THAT, TOO.
I think she has problems with that, too. sd
Figure 1: An example of annotated turns in the SwitchBoard corpus.
oriented and medium size corpus, the total number
of different words in the vocabulary, 812, is not as
large as the Switchboard database.
The turns were segmented into utterances. It
was possible for more than one utterance (with
their respective labels) to appear in a turn (on av-
erage, there were 1.5 utterances per user/system
turn). A three-level annotation scheme of the ut-
terances was defined (Alca?cer et al, 2005). These
labels represent the general purpose of the utter-
ance (first level), as well as more specific semantic
information (second and third level): the second
level represents the data focus in the utterance and
the third level represents the specific data present
in the utterance. An example of three-level anno-
tated user turns is given in Figure 2. The corpus
was annotated by means of a semiautomatic pro-
cedure, and all the dialogues were manually cor-
rected by human experts using a very specific set
of defined rules.
After this process, there were 248 different la-
bels (153 for user turns, 95 for system turns) using
the three-level scheme. When the detail level was
reduced to the first and second levels, there were
72 labels (45 for user turns, 27 for system turns).
When the detail level was limited to the first level,
there were only 16 labels (7 for user turns, 9 for
system turns). The differences in the number of
labels and in the number of examples for each la-
bel with the SwitchBoard corpus are significant.
4 Experiments and results
The SwitchBoard database was processed to re-
move certain particularities. The main adaptations
performed were:
? The interrupted utterances (which were la-
belled with ?+?) were joined to the correct
previous utterance, thereby avoiding inter-
ruptions (i.e., all the words of the interrupted
utterance were annotated with the same DA).
Table 1: SwitchBoard database statistics (mean for
the ten cross-validation partitions)
Training Test
Dialogues 1,136 19
Turns 113,370 1,885
Utterances 201,474 3,718
Running words 1,837,222 33,162
Vocabulary 21,248 2,579
? All the words were transcribed in lowercase.
? Puntuaction marks were separated from
words.
The experiments were performed using a cross-
validation approach to avoid the statistical bias
that can be introduced by the election of fixed
training and test partitions. This cross-validation
approach has also been adopted in other recent
works on this corpus (Webb et al, 2005). In our
case, we performed 10 different experiments. In
each experiment, the training partition was com-
posed of 1,136 dialogues, and the test partition
was composed of 19 dialogues. This proportion
was adopted so that our results could be compared
with the results in (Stolcke et al, 2000), where
similar training and test sizes were used. The
mean figures for the training and test partitions are
shown in Table 1.
With respect to the Dihana database, the prepro-
cessing included the following points:
? A categorisation process was performed for
categories such as town names, the time,
dates, train types, etc.
? All the words were transcribed in lowercase.
? Puntuaction marks were separated from
words.
? All the words were preceded by the speaker
identification (U for user, M for system).
566
Utterance 1st level 2nd level 3rd level
YES, TIMES AND FARES.
Yes, Acceptance Dep Hour Nil
times and fares Question Dep Hour,Fare Nil
YES, I WANT TIMES AND FARES OF TRAINS THAT ARRIVE BEFORE SEVEN.
Yes, I want times and fares of trains that arrive before seven. Question Dep Hour,Fare Arr Hour
ON THURSDAY IN THE AFTERNOON.
On thursday Answer Day Day
in the afternoon Answer Time Time
Figure 2: An example of annotated turns in the Dihana corpus. Original turns were in Spanish.
Table 2: Dihana database statistics (mean for the
five cross-validation partitions)
Training Test
Dialogues 720 180
Turns 12,330 3,083
User turns 5,024 1,256
System turns 7,206 1,827
Utterances 18,837 4,171
User utterances 7,773 1,406
System utterances 11,064 2,765
Running words 162,613 40,765
User running words 42,806 10,815
System running words 119,807 29,950
Vocabulary 832 485
User vocabulary 762 417
System vocabulary 208 174
A cross-validation approach was adopted in Di-
hana as well. In this case, only 5 different parti-
tions were used. Each of them had 720 dialogues
for training and 180 for testing. The statistics on
the Dihana corpus are presented in Table 2.
For both corpora, different N-gram models,
with N = 2, 3, 4, and HMM of one state were
trained from the training database. In the case of
the SwitchBoard database, all the turns in the test
set were used to compute the labelling accuracy.
However, for the Dihana database, only the user
turns were taken into account (because system
turns follow a regular, template-based scheme,
which presents artificially high labelling accura-
cies). Furthermore, in order to use a really sig-
nificant set of labels in the Dihana corpus, we
performed the experiments using only two-level
labels instead of the complete three-level labels.
This restriction allowed us to be more independent
from the understanding issues, which are strongly
related to the third level. It also allowed us to con-
centrate on the dialogue issues, which relate more
Table 3: SwitchBoard results for the segmented
model
N-gram Utt. accuracy Turn accuracy
2-gram 68.19% 59.33%
3-gram 68.50% 59.75%
4-gram 67.90% 59.14%
to the first and second levels.
The results in the case of the segmented ap-
proach described in Section 2 for SwitchBoard are
presented in Table 3. Two different definitions of
accuracy were used to assess the results:
? Utterance accuracy: computes the proportion
of well-labelled utterances.
? Turn accuracy: computes the proportion of
totally well-labelled turns (i.e.: if the la-
belling has the same labels in the same or-
der as in the reference, it is taken as a well-
labelled turn).
As expected, the utterance accuracy results are
a bit worse than those presented in (Stolcke et al,
2000). This may be due to the use of only the
past history and possibly to the cross-validation
approach used in the experiments. The turn accu-
racy was calculated to compare the segmented and
the unsegmented models. This was necessary be-
cause the utterance accuracy does not make sense
for the unsegmented model.
The results for the unsegmented approach for
SwitchBoard are presented in Table 4. In this case,
three different definitions of accuracy were used to
assess the results:
? Accuracy at DA level: the edit distance be-
tween the reference and the labelling of the
turn was computed; then, the number of cor-
rect substitutions (c), wrong substitutions (s),
deletions (d) and insertions (i) was com-
567
Table 4: SwitchBoard results for the unsegmented
model (WIP=50)
N-gram DA acc. Turn acc. Segm. acc.
2-gram 38.19% 39.47% 38.92%
3-gram 38.58% 39.61% 39.06%
4-gram 38.49% 39.52% 38.96%
puted, and the accuracy was calculated as
100 ? c(c+s+i+d) .
? Accuracy at turn level: this provides the pro-
portion of well-labelled turns, without taking
into account the segmentation (i.e., if the la-
belling has the same labels in the same or-
der as in the reference, it is taken as a well-
labelled turn).
? Accuracy at segmentation level: this pro-
vides the proportion of well-labelled and seg-
mented turns (i.e., the labels are the same as
in the reference and they affect the same ut-
terances).
The WIP parameter used in Table 4 was 50,
which is the one that offered the best results. The
segmentation accuracy in Table 4 must be com-
pared with the turn accuracy in Table 3. As Table 4
shows, the accuracy of the labelling decreased dra-
matically. This reveals the strong influence of the
availability of the real segmentation of the turns.
To confirm this hypothesis, similar experiments
were performed with the Dihana database. Ta-
ble 5 presents the results with the segmented cor-
pus, and Table 6 presents the results with the un-
segmented corpus (with WIP=50, which gave the
best results). In this case, only user turns were
taken into account to compute the accuracy, al-
though the model was applied to all the turns (both
user and system turns). For the Dihana corpus,
the degradation of the results of the unsegmented
approach with respect to the segmented approach
was not as high as in the SwitchBoard corpus, due
to the smaller vocabulary and complexity of the
dialogues.
These results led us to the same conclusion,
even for such a different corpus (much more la-
bels, task-oriented, etc.). In any case, these ac-
curacy figures must be taken as a lower bound on
the model performance because sometimes an in-
correct recognition of segment boundaries or dia-
logue acts does not cause an inappropriate reaction
of the dialogue strategy.
Table 5: Dihana results for the segmented model
(only two-level labelling for user turns)
N-gram Utt. accuracy Turn accuracy
2-gram 75.70% 74.46%
3-gram 76.28% 74.93%
4-gram 76.39% 75.10%
Table 6: Dihana results for the unsegmented
model (WIP=50, only two-level labelling for user
turns)
N-gram DA acc. Turn acc. Segm. acc.
2-gram 60.36% 62.86% 58.15%
3-gram 60.05% 62.49% 57.87%
4-gram 59.81% 62.44% 57.88%
An illustrative example of annotation errors in
the SwitchBoard database, is presented in Figure 3
for the same turns as in Figure 1. An error anal-
ysis of the segmented model was performed. The
results reveals that, in the case of most of the er-
rors were produced by the confusion of the ?sv?
and ?sd? classes (about 50% of the times ?sv? was
badly labelled, the wrong label was ?sd?) The sec-
ond turn in Figure 3 is an example of this type of
error. The confusions between the ?aa? and ?b?
classes were also significant (about 27% of the
times ?aa? was badly labelled, the wrong label was
?b?). This was reasonable due to the similar defini-
tions of these classes (which makes the annotation
difficult, even for human experts). These errors
were similar for all the N-grams used. In the case
of the unsegmented model, most of the errors were
produced by deletions of the ?sd? and ?sv? classes,
as in the first turn in Figure 3 (about 50% of the
errors). This can be explained by the presence of
very short and very long utterances in both classes
(i.e., utterances for ?sd? and ?sv? did not present a
regular length).
Some examples of errors in the Dihana corpus
are shown in Figure 4 (in this case, for the same
turns as those presented in Figure 2). In the seg-
mented model, most of the errors were substitu-
tions between labels with the same first level (es-
pecially questions and answers) where the second
level was difficult to recognise. The first and third
turn in Figure 4 are examples of this type of er-
ror. This was because sometimes the expressions
only differed with each other by one word, or
568
Utt Label
1 % Yeah, to get references and that, so, but, uh, I don?t
2 sd
feel comfortable about leaving my kids in a big day care center, simply because
there?s so many kids and so many <sniffing> <throat clearing>
Utt Label
1 sv I think she has problems with that, too.
Figure 3: An example of errors produced by the model in the SwitchBoard corpus
the previous segment influence (i.e., the language
model weight) was not enough to get the appro-
priate label. This was true for all the N-grams
tested. In the case of the unsegmented model, most
of the errors were caused by similar misrecogni-
tions in the second level (which are more frequent
due to the absence of utterance boundaries); how-
ever, deletion and insertion errors were also sig-
nificant. The deletion errors corresponded to ac-
ceptance utterances, which were too short (most
of them were ?Yes?). The insertion errors corre-
sponded to ?Yes? words that were placed after a
new-consult system utterance, which is the case
of the second turn presented in Figure 4. These
words should not have been labelled as a separate
utterance. In both cases, these errors were very
dependant on the WIP factor, and we had to get
an adequate WIP value which did not increase the
insertions and did not cause too many deletions.
5 Conclusions and future work
In this work, we proposed a method for simultane-
ous segmentation and annotation of dialogue ut-
terances. In contrast to previous models for this
task, our model does not assume manual utterance
segmentation. Instead of treating utterance seg-
mentation as a separate task, the proposed method
selects utterance boundaries to optimize the accu-
racy of the generated labels. We performed ex-
periments to determine the effect of the availabil-
ity of the correct segmentation of dialogue turns
in utterances in the statistical DA labelling frame-
work. Our results reveal that, as shown in previ-
ous work (Warnke et al, 1999), having the correct
segmentation is very important in obtaining accu-
rate results in the labelling task. This conclusion
is supported by the results obtained in very differ-
ent dialogue corpora: different amounts of training
and test data, different natures (general and task-
oriented), different sets of labels, etc.
Future work on this task will be carried out
in several directions. As segmentation appears
to be an important step in these tasks, it would
be interesting to obtain an automatic and accu-
rate segmentation model that can be easily inte-
grated in our statistical model. The application of
our statistical models to other tasks (like VerbMo-
bil (Alexandersson et al, 1998)) would allow us to
confirm our conclusions and compare results with
other works.
The error analysis we performed shows the need
for incorporating new and more reliable informa-
tion resources to the presented model. Therefore,
the use of alternative models in both corpora, such
as the N-gram-based model presented in (Webb et
al., 2005) or an evolution of the presented statis-
tical model with other information sources would
be useful. The combination of these two models
might be a good way to improve results.
Finally, it must be pointed out that the main task
of the dialogue models is to allow the most correct
reaction of a dialogue system given the user in-
put. Therefore, the correct evaluation technique
must be based on the system behaviour as well
as on the accurate assignation of DA to the user
input. Therefore, future evaluation results should
take this fact into account.
Acknowledgements
The authors wish to thank Nick Webb, Mark Hep-
ple and Yorick Wilks for their comments and
suggestions and for providing the preprocessed
SwitchBoard corpus. We also want to thank the
anonymous reviewers for their criticism and sug-
gestions.
References
N. Alca?cer, J. M. Bened??, F. Blat, R. Granell, C. D.
Mart??nez, and F. Torres. 2005. Acquisition and
labelling of a spontaneous speech dialogue corpus.
In Proceedings of SPECOM, pages 583?586, Patras,
Greece.
Jan Alexandersson, Bianka Buschbeck-Wolf, Tsu-
tomu Fujinami, Michael Kipp, Stephan Koch, Elis-
569
Utterance 1st level 2nd level
Yes, times Acceptance Dep Hour,Fare
and fares Question Dep Hour,Fare
Yes, I want Acceptance Dep Hour,Fare
times and fares of trains that arrive before seven. Question Dep Hour,Fare
On thursday in the afternoon Answer Time
Figure 4: An example of errors produced by the model in the Dihana corpus
abeth Maier, Norbert Reithinger, Birte Schmitz,
and Melanie Siegel. 1998. Dialogue acts in
VERBMOBIL-2 (second edition). Technical Report
226, DFKI GmbH, Saarbru?cken, Germany, July.
J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dia-
log act segmentation and classification in multiparty
meetings. In Proceedings of the International Con-
ference of Acoustics, Speech, and Signal Process-
ings, volume 1, pages 1061?1064, Philadelphia.
H. Aust, M. Oerder, F. Seide, and V. Steinbiss. 1995.
The philips automatic train timetable information
system. Speech Communication, 17:249?263.
J. M. Bened??, A. Varona, and E. Lleida. 2004. Dihana:
Dialogue system for information access using spon-
taneous speech in several environments tic2002-
04103-c03. In Reports for Jornadas de Seguimiento
- Programa Nacional de Tecnolog??as Informa?ticas,
Ma?laga, Spain.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the damsl annotation scheme. In Work-
ing Notes of AAAI Fall Symposium on Communica-
tive Action in Humans and Machines, Boston, MA,
November.
Layla Dybkjaer and Niels Ole Bernsen. 2000. The
mate workbench.
N. Fraser, 1997. Assessment of interactive systems,
pages 564?614. Mouton de Gruyter.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. In Proc. ICASSP-92, pages 517?
520.
A. Gorin, G. Riccardi, and J. Wright. 1997. How may
i help you? Speech Communication, 23:113?127.
Hilda Hardy, Kirk Baker, Laurence Devillers, Lori
Lamel, Sophie Rosset, Tomek Strzalkowski, Cris-
tian Ursu, and Nick Webb. 2002. Multi-layer di-
alogue annotation for automated multilingual cus-
tomer service. In Proceedings of the ISLE Workshop
on Dialogue Tagging for Multi-Modal Human Com-
puter Interaction, Edinburgh, Scotland, December.
Hilda Hardy, Tomek Strzalkowski, and Min Wu. 2003.
Dialogue management for an automated multilin-
gual call center. In Proceedings of HLT-NAACL
2003 Workshop: Research Directions in Dialogue
Processing, pages 10?12, Edmonton, Canada, June.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board swbd-damsl shallow- discourse-function an-
notation coders manual - draft 13. Technical Report
97-01, University of Colorado Institute of Cognitive
Science.
J. Van Kuppevelt and R. W. Smith. 2003. Current
and New Directions in Discourse and Dialogue, vol-
ume 22 of Text, Speech and Language Technology.
Springer.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, and M. Meteer. 2000. Dialogue act modelling
for automatic tagging and recognition of conversa-
tional speech. Computational Linguistics, 26(3):1?
34.
Marilyn A. Walker, Diane Litman J., Candace A.
Kamm, and Alicia Abella. 1997. PARADISE: A
framework for evaluating spoken dialogue agents.
In Philip R. Cohen and Wolfgang Wahlster, edi-
tors, Proceedings of the Thirty-Fifth Annual Meet-
ing of the Association for Computational Linguis-
tics and Eighth Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 271?280, Somerset, New Jersey. Association
for Computational Linguistics.
V.Warnke, R. Kompe, H. Niemann, and E. No?th. 1997.
Integrated Dialog Act Segmentation and Classifica-
tion using Prosodic Features and Language Models.
In Proc. European Conf. on Speech Communication
and Technology, volume 1, pages 207?210, Rhodes.
V. Warnke, S. Harbeck, E. No?th, H. Niemann, and
M. Levit. 1999. Discriminative Estimation of Inter-
polation Parameters for LanguageModel Classifiers.
In Proceedings of the IEEE Conference on Acous-
tics, Speech, and Signal Processing, volume 1, pages
525?528, Phoenix, AZ, March.
N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue
act classification using intra-utterance features. In
Proceedings of the AAAI Workshop on Spoken Lan-
guage Understanding, Pittsburgh.
S. Young. 2000. Probabilistic methods in spoken di-
alogue systems. Philosophical Trans Royal Society
(Series A), 358(1769):1389?1402.
570
In: Proceedings of CoNLL-2000 and LLL-2000, pages 79-82, Lisbon, Portugal, 2000. 
Using Perfect Sampling in Parameter Estimation of a Whole 
Sentence Maximum Entropy Language Model* 
F. Amaya t and J .  M.  Benedf  
Departamento de Sistemas Inform?ticos y Computac idn 
Universidad Polit6cnica de Valencia 
Camino de vera s/n, 46022-Valencia (Spain) 
{famaya, jbened i}@ds ic .upv .es  
Abst rac t  
The Maximum Entropy principle (ME) is an ap- 
propriate framework for combining information 
of a diverse nature from several sources into the 
same language model. In order to incorporate 
long-distance information into the ME frame- 
work in a language model, a Whole Sentence 
Maximum Entropy Language Model (WSME) 
could be used. Until now MonteCarlo Markov 
Chains (MCMC) sampling techniques has been 
used to estimate the paramenters of the WSME 
model. In this paper, we propose the applica- 
tion of another sampling technique: the Perfect 
Sampling (PS). The experiment has shown a re- 
duction of 30% in the perplexity of the WSME 
model over the trigram model and a reduc- 
tion of 2% over the WSME model trained with 
MCMC. 
1 Int roduct ion  
The language modeling problem may be defined 
as the problem of calculating the probability of 
a string, p(w) = p(wl,. . . ,  Wn). The probability 
p(w) is usually calculated via conditional prob- 
abilities. The n-gram model is one of the most 
widely used language models. The power of the 
n-gram model resides in its simple formulation 
and the ease of training. On the other hand, n- 
grams only take into account local information, 
and important long-distance information con- 
tained in the string wl ... wn cannot be modeled 
by it. In an attempt o supplement the local in- 
formation with long-distance information, hy- 
brid models have been proposed such us (Belle- 
* This work has been partially supported by the Spanish 
CYCIT under contract (TIC98/0423-C06). 
t Granted by Universidad el Cauca, Popay~n (Colom- 
bia) 
garda, 1998; Chelba, 1998; Benedl and Sanchez, 
2000). 
The Maximum Entropy principle is an ap- 
propriate framework for combining information 
of a diverse nature from several sources into 
the same model: the Maximum Entropy model 
(ME) (Rosenfeld, 1996). The information is in- 
corporated as features which are submitted to 
constraints. The conditional form of the ME 
model is: 
1 (1) 
p(ulx) = z(x)  
where Ai are the parameters to be learned (one 
for each feature), the fi are usually characteris- 
tic functions which are associated to the fea- 
tures and Z(x) = ~y exp{~i~l Aifi(x,y)} is 
the normalization constant. The main advan- 
tages of ME are its flexibility (local and global 
information can be included in the model) and 
its simplicity. The drawbacks are that the para- 
menter's estimation is computationally expen- 
sive, specially the evaluation of the normaliza- 
tion constant Z(x) andthat  the grammatical 
information contained in the sentence is poorly 
encoded in the conditional framework. This is 
due to the assumption of independence in the 
conditional events: in the events in the state 
space, only a part of the information contained 
in the sentence influences de calculation of the 
probability (Ristad, 1998). 
2 Who le  Sentence  Max imum 
Ent ropy  Language Mode l  
An alternative to combining local, long-distance 
and structural information contained in the 
sentence, within the maximum entropy frame- 
work, is the Whole Sentence Maximum En- 
tropy model (WSME) (Rosenfeld, 1997). The 
79 
WSME is based in the calculation of unre- 
stricted ME probability p(w) of a whole sen- 
tence w = wl . . .  Wn. The probability distribu- 
tion is the distribution p that has the maximum 
entropy relative to a prior distribution P0 (in 
other words: the distribution that minimize de 
divergence D(pllpo)) (Della Pietra et al, 1995). 
The distribution p is given by: 
m . . 
p(w) = 5po(w)eE~=l ~,:~(w) (2) 
where Ai and f~ are the same as in (1). Z is 
a (global) normalization constant and P0 is a 
prior proposal distribution. The Ai and Z are 
unknown and must be learned. 
The parameters Ai may be interpreted as be- 
ing weights of the features and could be learned 
using some type of iterative algorithm. We have 
used the Improved Iterative Scaling algorithm 
(IIS) (Berger et al, 1996). In each iteration of 
the IIS, we find a 5i value such that adding this 
value to Ai parameters, we obtain an increase 
in the the log-likelihood. The 5i values are ob- 
tained as the solution of the m equations: 
1 
- Z = 0 
w wEN 
(3) 
where /  = 1,. . . ,m, f#(w) = ~=l f i (w)  and 
f~ is a training corpus. Because the domain of 
WSME is not restricted to a part of the sen- 
tence (context) as in the conditional case, it 
allows us to combine global structural syntac- 
tic information which is contained in the sen- 
tence with local and other kinds of long range 
information such us triggers. Furthermore, the 
WSME model is easier to train than the con- 
ditional one, because in the WSME model we 
don't need to estimate the normalization con- 
stant Z during the training time. In contrast, 
for each event (x, y) in the training corpus, we 
have to calculate Z(x) in each iteration of the 
MEC model. 
The main drawbacks of the WSME model are 
its integration with other modules and the cal- 
culation of the expected value in the left part of 
equation (3), because the event space is huge. 
Here we focus on the problem of calculating 
the expected value in (3). The first sum in (3) 
is the expected value of fie ~::#, and it is obvi- 
ously not possible to sum over all the sentences. 
However, we can estimate the mean by using 
the empirical expected value: 
\[ fie~if# \] 1 M Z f/(sJ) (4) Ep k J 
j= l  
where sl , .  ? ?, SM is a random sample from p(w). 
Once the parameters have been learned it is pos- 
sible to estimate the value of the normalization 
constant, because Z = ~w e~l  ~f~(W)p0(w ) = 
F m |e~i=l if~|, and it can be estimated 1 by 
L . I  
means of the sample mean with respect o P0 
(Chen and Rosenfeld, 1999). 
In each iteration of IIS, the calculation of (4) 
requires sampling from a probability distribu- 
tion which is partially known (Z is unknown), 
so the classical sampling techniques are not use- 
ful. In the literature, there are some meth- 
ods like the MonteCarlo Markov Chain meth- 
ods (MCMC) that generate random samples 
from p(w) (Sahu, 1997; Tierney, 1994). With 
the MCMC methods, we can simulate a sample 
approximately from the probability distribution 
and then use the sample to estimate the desired 
expected value in (4). 
3 Per fec t  Sampl ing  
In this paper, we propose the application of an- 
other sampling technique in the parameter esti- 
mation process of the WSME model which was 
introduced by Propp and Wilson (Propp and 
Wilson, 1996): the Perfect Sampling (PS). The 
PS method produces samples from the exact 
limit distribution and, thus, the sampling mean 
given in (4) is less biased than the one obtained 
with the MCMC methods. Therefore, we can 
obtain better estimations of the parameters Ai. 
In PS, we obtain a sample from the limit 
distribution of an ergodic Markov Chain X = 
{Xn; n _> 0}, taking values in the state space S 
(in the WSME case, the state space is the set of 
possible sentences). Because of the ergodicity, 
if the transition law of X is P(x, A) := P(Xn E 
AIXn_i = x), then it has a limit distribution ~-, 
that is: if we start a path on the chain in any 
state at time n = 0, then as n ~ ~,  Xn ~ ~'. 
The first algorithm of the family of PS was pre- 
sented by Propp and Wilson (Propp and Wil- 
son, 1996) under the name Coupling From the 
Past (CFP) and is as follows: start a path in 
80 
every state of S at some time ( -T)  in the past 
such that at time n = 0, all the paths collapse 
to a unique value (due to the ergodicity). This 
value is a sample element. In the majority of 
cases, the state space is huge, so attempting 
to begin a path in every state is not practical. 
Thus, we can define a partial stochastic order 
in the state space and so we only need start two 
paths: one in the minimum and one in the maxi- 
mum. The two paths collapse at time n = 0 and 
the value of the coalescence state is a sample 
element of ~-. The CFP algorithm first deter- 
mines the time T to start and then runs the two 
paths from time ( -T )  to 0. Information about 
PS methods may be consulted in (Corcoran and 
Tweedie, 1998; Propp and Wilson, 1998). 
4 Exper imenta l  work  
In this work, we have made preliminary exper- 
iments using PS in the estimation of the ex- 
pected value (4) during the learning of the pa- 
rameters of a WSME model. We have imple- 
mented the Cai algorithm (Cai, 1999) to obtain 
perfect samples. The Cai algorithm has the ad- 
vantage that it doesn't need the definition of the 
partial order. 
The experiments were carried out using a 
pseudonatural corpus: "the traveler task "1. 
The traveler task consists in dialogs between 
travelers and hotel clerks. The size of the vocab- 
ulary is 693 words. The training set has 490,000 
sentences and 4,748,690 words. The test set has 
10,000 sentences and 97,153 words. 
Three kinds of features were used in the 
WSME model: n-grams (1-grams, 2-grams, 3- 
grams), distance 2 n-grams (d2-2-grams, d2-3- 
grams) and triggers. The proposal prior distri- 
bution used was a trigram model. 
We trained WSME models with different sets 
of features using the two sampling techniques: 
MCMC and PS. We measured the perplexity 
(PP) of each of the models and obtained the 
percentage of improvement in the PP with re- 
spect o a trigram base-line model (see table 1). 
The first model used MCMC techniques ( pecif- 
ically the Independence Metropolis-Hastings al-
gorithm (IMH) 2) and features of n-grams and 
distance 2 n-grams. The second model used a 
1EuTrans ESPRIT-LTR Project 20268 
2IMH has been reported recently as the most useful 
MCMC algorithm used in the WSME training process. 
Method PP % Improvement 
IMH 3.37115 28 
PS 3.46336 26 
IMH-T 3.37198 28 
PS-T 3.26964 30 
Trigram 4.66975 
Table h Test set perplexity of the WSME 
model over the traveler task corpus: IMH with 
features of n-grams and d-n-grams (IMH), PS 
with n-grams and d-n-grams (PS) IMH with 
triggers (IMH-T), PS with triggers (PS-T). The 
base-line model is a trigram model (Trigram) 
PS algorithm and features of n-grams and dis- 
tance 2 n-grams. The third model used the IMH 
algorithm and features of triggers. The fourth 
used PS and features of triggers. Finally, in or- 
der to compare with the classical methods, we 
included the trigram base-line model. 
In all cases, the WSME had a better perfor- 
mance than the n-gram model. From the results 
in Table 1, we see that the use of features of 
triggers improves the performance of the model 
more than the use of n-gram features, this may 
be due to the correlation between the triggers 
and the n-grams, the n-gram information has 
been absorbed by the prior distribution and di- 
minishes the effects of the feature of n-grams. 
We believe this is the reason why PS-T in Ta- 
ble 1 is better than PS. We also see how IMH 
and IHM-T shows the same improvement, i.e. 
the use of triggers does not seem improve the 
perplexity of the model but, this may be due 
to the sampling technique: the parameter val- 
ues depends on the estimation of an expected 
value, and the estimation depends on the sam- 
pling. Finally, the PS-T has better perplexity 
than the IMH-T. The only difference between 
both of these is the sampling technique,neither 
of then has the correlation influence in the fea- 
tures, so we think that the improvement may 
be due to the sampling technique. 
5 Conc lus ion  and  fu ture  works  
We have presented a different approach to the 
sampling step needed in the parameter estima- 
tion of a WSME model. Using this technique, 
we have obtained a reduction of 30% in the per- 
plexity of the WSME model over the base-line 
81 
trigram model and an improvement of 2% over 
the model trained with MCMC techniques. We 
are extending our experiments to a major cor- 
pus: the Wall Street Journal corpus and using a 
set of features which is more general, including 
features that reflect the global structure of the 
sentence. 
We are working on introducing the grammat- 
ical information contained into the sentence to 
the model; we believe that such information im- 
proves the quality of the model significantly. 
Re ferences  
J. R. Bellegarda. 1998. A multispan language 
modeling framework for large vocabulary speech 
recognition. IEEE Transactions on Speech and 
Audio Processing, 6 (5):456-467. 
J.M. Bened~ and J.A. Sanchez. 2000. Combination 
of n-grams and stochastic ontext-free grammars 
for language modeling. International conference 
on computational linguistics (COLIN-A CL). 
A.L. Berger, V.J. Della Pietra, and S.A. Della 
Pietra. 1996. A Maximum Entropy approach to 
natural anguage processing. Computational Lin- 
guistics, 22(1):39-72. 
H. Cai. 1999. Exact Sampling using auxiliary vari- 
ables. Statistical Computing Section, ASA Pro- 
ceedings. 
C. Chelba. 1998. A structured Language Model. 
PhD Dissertation Proposal, The Johns Hopkins 
University. 
S. Chen and R. Rosenfeld. 1999. Efficient sampling 
and feature selection in whole sentence maximum 
entropy language models. Proc. IEEE Int. Con- 
ference on Acoustics, Speech and Signal Process- 
ing (ICASSP). 
J.N. Corcoran and R.L. Tweedie. 1998. Perfect sam- 
pling for Independent Metropolis-Hastings chains. 
preprint. Colorado State University. 
S. Della Pietra, V. Della Pietra, and J. Lafferty. 
1995. Inducing features of random fields. Tech- 
nical Report CMU-CS-95-144, Carnegie Mellon 
University. 
J. G. Propp and D. B. Wilson. 1996. Exact sampling 
with coupled markov chains and applications to 
statistical mechanics. Random Structures and Al- 
gorithms, 9:223-252. 
J. A. Propp and D. B. Wilson. 1998. Coupling from 
the Past: User's Guide. Dimacs series in discrete 
Mathematics and Theoretical Computer Science, 
pages 181-192. 
E. S. Ristad, 1998. Maximum Entropy Modeling 
Toolkit, Version 1.6 Beta. 
R. Rosenfeld. 1996. A Maximum Entropy approach 
to adaptive statistical language modeling. Com- 
puter Speech and Language, 10:187-228. 
R. Rosenfeld. 1997. A whole sentence Maximum En- 
tropy language model. IEEE workshop on Speech 
Recognition and Understanding. 
S. Sahu. 1997. Bayesian data analysis. Technical re- 
port, School of Mathematics, University of Walles. 
L. Tierney. 1994. Markov chains for exploring pos- 
terior distributions. The Annals o/ Statistics, 
22:1701-1762. 
82 
Proceedings of the Workshop on Statistical Machine Translation, pages 130?133,
New York City, June 2006. c?2006 Association for Computational Linguistics
Stochastic Inversion Transduction Grammars for Obtaining Word Phrases
for Phrase-based Statistical Machine Translation
J.A. S?nchez and J.M. Bened?
Departamento de Sistemas Inform?ticos y Computaci?n
Universidad Polit?cnica de Valencia
Valencia, Spain
jandreu@dsic.upv.es jbenedi@dsic.upv.es
Abstract
An important problem that is related to
phrase-based statistical translation mod-
els is the obtaining of word phrases from
an aligned bilingual training corpus. In
this work, we propose obtaining word
phrases by means of a Stochastic Inver-
sion Translation Grammar. Experiments
on the shared task proposed in this work-
shop with the Europarl corpus have been
carried out and good results have been ob-
tained.
1 Introduction
Phrase-based statistical translation systems are cur-
rently providing excellent results in real machine
translation tasks (Zens et al, 2002; Och and Ney,
2003; Koehn, 2004). In phrase-based statistical
translation systems, the basic translation units are
word phrases.
An important problem that is related to phrase-
based statistical translation is to automatically ob-
tain bilingual word phrases from parallel corpora.
Several methods have been defined for dealing with
this problem (Och and Ney, 2003). In this work, we
study a method for obtaining word phrases that is
based on Stochastic Inversion Transduction Gram-
mars that was proposed in (Wu, 1997).
Stochastic Inversion Transduction Grammars
(SITG) can be viewed as a restricted Stochas-
tic Context-Free Syntax-Directed Transduction
Scheme. SITGs can be used to carry out a simulta-
neous parsing of both the input string and the output
string. In this work, we apply this idea to obtain
aligned word phrases to be used in phrase-based
translation systems (S?nchez and Bened?, 2006).
In Section 2, we review the phrase-based machine
translation approach. SITGs are reviewed in Sec-
tion 3. In Section 4, we present experiments on the
shared task proposed in this workshop with the Eu-
roparl corpus.
2 Phrase-based Statistical Machine
Transduction
The translation units in a phrase-based statistical
translation system are bilingual phrases rather than
simple paired words. Several systems that fol-
low this approach have been presented in recent
works (Zens et al, 2002; Koehn, 2004). These sys-
tems have demonstrated excellent translation perfor-
mance in real tasks.
The basic idea of a phrase-based statistical ma-
chine translation system consists of the following
steps (Zens et al, 2002): first, the source sentence is
segmented into phrases; second, each source phrase
is translated into a target phrase; and third, the target
phrases are reordered in order to compose the target
sentence.
Bilingual translation phrases are an important
component of a phrase-based system. Different
methods have been defined to obtain bilingual trans-
lations phrases, mainly from word-based alignments
and from syntax-based models (Yamada and Knight,
2001).
In this work, we focus on learning bilingual word
phrases by using Stochastic Inversion Transduction
Grammars (SITGs) (Wu, 1997). This formalism al-
130
lows us to obtain bilingual word phrases in a natu-
ral way from the bilingual parsing of two sentences.
In addition, the SITGs allow us to easily incorpo-
rate many desirable characteristics to word phrases
such as length restrictions, selection according to the
word alignment probability, bracketing information,
etc. We review this formalism in the following sec-
tion.
3 Stochastic Inversion Transduction
Grammars
Stochastic Inversion Transduction Grammars
(SITGs) (Wu, 1997) can be viewed as a restricted
subset of Stochastic Syntax-Directed Transduction
Grammars. They can be used to simultaneously
parse two strings, both the source and the target
sentences. SITGs are closely related to Stochastic
Context-Free Grammars.
Formally, a SITG in Chomsky Normal Form1
  can be defined as a tuple
	
		
,
where:

is a finite set of non-terminal symbols;

is the axiom of the SITG;

is a finite set
of terminal symbols of language 1; and


is a finite
set of terminal symbols of language 2.

is a finite
set of: lexical rules of the type Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 222?225,
Paris, October 2009. c?2009 Association for Computational Linguistics
Interactive Predictive Parsing 1
Ricardo Sa?nchez-Sa?ez, Joan-Andreu Sa?nchez and Jose?-Miguel Bened??
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
Cam?? de Vera s/n, Valencia 46022 (Spain)
{rsanchez, jandreu, jbenedi}dsic.upv.es
Abstract
This paper introduces a formal framework
that presents a novel Interactive Predic-
tive Parsing schema which can be oper-
ated by a user, tightly integrated into the
system, to obtain error free trees. This
compares to the classical two-step schema
of manually post-editing the erroneus con-
stituents produced by the parsing system.
We have simulated interaction and cal-
culated evalaution metrics, which estab-
lished that an IPP system results in a high
amount of effort reduction for a manual
annotator compared to a two-step system.
1 Introduction
The aim of parsing is to obtain the linguistic in-
terpretation of sentences, that is, their underlying
syntactic structure. This task is one of the fun-
damental pieces needed by a computer to uns-
derstand language as used by humans, and has
many applications in Natural Language Process-
ing (Lease et al, 2006).
A wide array of parsing methods exist, in-
cluding those based on Probabilistic Context-Free
Grammars (PCFGs). (Charniak, 2000; Collins,
2003; Johnson, 1998; Klein and Manning, 2003;
Matsuzaki et al, 2005; Petrov and Klein, 2007).
The most impressive results are achieved by sub-
tree reranking systems, as shown in the semi-
supervised method of (McClosky et al, 2006),
or the forest reranking approximation of (Huang,
2008) in which packed parse forests (compact
structures that contain many possible tree deriva-
tions) are used.
These state-of-the-art parsers provide trees of
excelent quality. However, perfect results are vir-
1Work supported by the MIPRCV ?Consolider Inge-
nio 2010? (CSD2007-00018), iTransDoc (TIN2006-15694-
CO2-01) and Prometeo (PROMETEO/2009/014) reserach
projects, and the FPU fellowship AP2006-01363.
tually never achieved. If the need of one-hundred-
percent error free trees arises, the supervision of a
user that post-edits and corrects the errors is un-
avoidable.
Error free trees are needed in many tasks such as
handwritten mathematical expressions recognition
(Yamamoto et al, 2006), or creation of new gold
standard treebanks (Delaclergerie et al, 2008)).
For example, in the creation of the Penn Tree-
bank grammar, a basic two-stage setup was em-
ployed: a rudimentary parsing system providad a
skeletal syntactic representation, which then was
manually corrected by human annotators (Marcus
et al, 1993).
In this paper, we introduce a new formal frame-
work that tightly integrates the user within the
parsing system itself, rather than keeping him iso-
lated from the automatic tools used in a classi-
cal two-step approach. This approach introduces
the user into the parsing system, and we will call
it ?interactive predictive parsing?, or simply IPP.
An IPP system is interactive because the user is in
continuous contact with the parsing process, send-
ing and receiving feedback. An IPP system is also
predictive because it reacts to the user corrections:
it predicts and suggest new parse trees taking into
account the new gold knowledge received from
the user. Interactive predictive methods have been
studied and successfully used in fields like Auto-
matic Text Recognition (Toselli et al, 2008) and
Statistical Machine Translation (Barrachina et al,
2009; Vidal et al, 2006) to ease the work of tran-
scriptor and translators.
Assessment of the amount of effort saved by the
IPP system will be measured by automatically cal-
culated metrics.
2 Interactive Predictive Parsing
A tree t, associated to a string x1|x|, is composed
by substructures that are usually referred as con-
stituents or edges. A constituent cAij is a span de-
222
fined by a nonterminal symbol (or syntactic tag) A
that covers the substring xij .
Assume that using a given probabilistic context-
free grammar G as the model, the parser analyzes
the input sentence x = x1 . . . x|x| and produces
the parse tree t?
t? = argmax
t?T
pG(t|x), (1)
where pG(t|x) is the probability of parse tree t
given the input string x using model G, and T is
the set of all possible parse trees for x.
In an interactive predictive scenario, after ob-
taining the (probably incorrect) best tree t?, the user
is able to modify the edges cAij that are incorrect.
The system reacts to each of the corrections intro-
duced by the human by proposing a new t?? that
takes into account the corrected edge. The order
in which incorrect constituents are reviewed deter-
mines the amount of effort reduction given by the
degree of correctness of the subsequent proposed
trees.
There exist several ways in which a human ana-
lyzes a sentende. A top-to-bottom may be consid-
ered natural way of proceeding, and we follow this
approach in this work. This way, when a higher
level constituent is corrected, possible erroneous
constituents at lower levels are expectedly auto-
matically recalculated.
The introduced IPP interaction process is sim-
ilar to the ones already established in Computer-
Assisted Text Recognition and Computer-Assisted
Translation 1.
Within the IPP framework, the user reviews the
constituents contained in the tree to assess their
correctness. When the user find an incorrect edge
he modifies it, setting the correct label and span.
This action implicitly validates a subtree that is
composed by the corrected edge plus all its ances-
tor edges, which we will call the validated prefix
tree tp. When the user replaces the constituent cAij
with the correct one c?Aij , the validated prefix tree
is:
tp(c?Aij ) = {cBmn : m ? i, n ? j
d(cBmn) ? d(c?Aij )}
(2)
with d(cDpq) being the depth of constituent cDpq.
1In these fields, the user reads the sentence from left to
right. When the user finds and corrects an erroneus word, he
is implicitly validating the prefix sentence up to that word.
The remaining suffix sentence is recalculated by the system
taking into account the validated prefix sentece.
When a constituent correction is performed, the
prefix tree tp(c?Aij ) is fixed and a new tree t?? that
takes into account the prefix is proposed
t?? = argmax
t?T
pG(t|x, tp(c?Aij )). (3)
Given that we are working with context-free
grammars, the only subtree that effectively needs
to be recalcuted is the one starting from the par-
ent of the corrected edge. Let the corrected edge
be c?Aij and its parent cDst, then the following tree is
proposed
t?? = argmax
t?T
pG(t|x, tp) = (t? \ t?Dst) ? t??Dst , (4)
with
t??Dst = argmax
tDst?Tst
pG(tDst|xmn, c?Aij ) . (5)
Expression (4) represents the newly proposed
tree t??, which consists of original proposed tree
t? minus the subpart of the original proposed tree
t?Dst (whose root is the parent of the corrected edge
cDst) plus the newly calculated subtree t??
D
st (whose
root is also the parent of the corrected constituent
cDst, but also takes into account the corrected one
as shown in Expression (5)).
In Figure 1 we show an example that intends to
clarify the interactive predictive process. First, the
system provides a proposed parse tree (Fig. 1.a).
Then the user, which has in his mind the correct
reference tree, notices that it has two wrong con-
stituents (cX23 and cZ44) (Fig. 1.b), and choses to re-
place cX23 by cB22 (Fig. 1.c). Here, cB22 corresponds
to c?Aij from expressions (3) and (5).
As the user does this correction, the system au-
tomatically validates the correct prefix: all the an-
cestors of the modified constituent (dashed line in
the figure, tp(c?Aij ) from expression (2)). The sys-
tem also invalidates the subtrees related to the cor-
rected constituent (dotted line line in the figure, t?Dst
from expression (4)).
Finally, the system automatically predicts a new
subtree (t??Dst from expression (4)) (Fig. 1.d). No-
tice how cZ34 changes its span and cD44 is introduced
which provides the correct reference parse.
Within the example shown in Figure 1, the user
would obtain the gold tree with just one correction,
rather than the three operations needed on a two-
step system (one deletion, one substitution and one
insertion).
223
SB Z
Y
ba c d
A
DC
(a) Reference tree
S
ba c d
A
CB
X
Y
Z
(b) Iteration 0:
Proposed out-
put tree 1
S
ba c d
A
CB
X Z 423 4
Y
(c) Iteration 0: Er-
roneus constituents
S
ba c d
A
B 2
2 ?
? ?
Y
(d) Iteration 1:
User corrected
constituent
S
B Z
Y
ba c d
A
DC
3
4
(e) Iteration 1:
Proposed output
tree 2
Figure 1: Synthetic example of user interaction with the IPP system.
3 IPP Evaluation
The objective of the experimentation presented
here is to evaluate the amount of effort saved for
the user using the IPP system, compared to the ef-
fort required to manually correct the trees without
the use of an interactive system. In this section, we
define a standard automatic evaluation protocol,
akin to the ones used in Computer-Aided Trans-
lation and Computer Aided Text Recognition.
In the absence of testing of an interactive sys-
tem with real users, the gold reference trees were
used to simulate system interaction by a human
corrector. In order to do this, the constituents in
the proposed tree were automatically reviewed in a
preorder manner 2. In each step, the constituent in
the proposed tree was compared to the correspond-
ing one in the reference tree: if the constituent was
equivalent no action was taken. When one incor-
rect constituent was found in the proposed tree, it
was replaced by the correct one from the reference
tree. This precise step simulated what a human su-
pervisor would do, that is, to type the correct con-
stituent in place of the erroneus one.
The system then performed the predictive step
(i.e. recalculation of subtrees related to the cor-
rected constituent). We kept a correction count,
which was incremented by one after each predic-
tive step.
3.1 Evaluation metrics
For evaluation, first we report a metric represent-
ing the amount of human correcting work needed
to obtain the gold tree in a classical two-step pro-
cess (i.e. the number of operations needed to post-
edit the proposed tree in orther to obtain the gold
2Interaction in this ordered manner guaranteed that the
evaluation protocol only needed to modify the label A and
the end point j of a given edge cAij , while i remained valid
given the modifications of previous constituents.
one). We then compare this value to a metric that
measures the amount of effort needed to obtain
the gold tree with the human interacting within the
presented IPP system.
Parsing quality is generally assessed by the clas-
sical evaluation metrics, precission, recall and F-
measure. We defined the following metric that
measures the amount of effort needed in order to
post-edit a proposed tree and obtain the gold ref-
erence parse tree, akin to the Word Error Rate
used in Statistical Machine Translation and related
fields:
? Tree Constituent Error Rate (TCER): Min-
imum number of constituent substitution,
deletion and insertion operations needed to
convert the proposed parse tree into the corre-
sponding gold reference tree, divided by the
total number of constituents in the reference
tree 3.
The TCER is in fact strongly related to the F-
measure: the higher the F-measure is, the lower
TCER will be.
Finally, the relevant evaluation metric that as-
sessed the IPP system performance represents the
amount effort that the operator would have to
spend using the system in order to obtain the gold
tree, and is directly comparable to the TCER:
? Tree Constituent Action Rate (TCAC): Num-
ber of constituent corrections performed us-
ing the IPP system to obtain the reference
tree, divided by the total number of con-
stituents in the reference tree.
4 Experimental results
An IPP system was implemented over the classical
CYK-Viterbi algorithm. Experimentation was run
3Edit distance is calcualted over the ordered set of tree
constituents. This is an approximation of the edit distance
between trees.
224
over the Penn Tree bank: sections 2 to 21 were
used to obtain a vanilla Penn Treebank Grammar;
test set was the whole section 23.
We obtained several binarized versions of the
train grammar for use with the CYK. The Chom-
sky Normal Form (CNF) transformation method
from the NLTK4 was used to obtain several right-
factored binary grammars of different sizes 5.
A basic schema was introduced for parsing sen-
tences with out-of-vocabulary words: when an
input word could not be derived by any of the
preterminals in the vanilla treebank grammar, a
very small probability for that word was uniformly
added to all of the preterminals.
Results for the metrics discussed on section 3.1
for different markovizations of the train grammar
can be seen in Table 1. We observe that the perc-
etage of corrections needed using the IPP system
is much lower than the rate of needed corrections
just post-editing the proposed trees: from 42% to
46% in effort reduction by the human supervisor.
These results clearly show that an interactive
predictive system can relieve manual annotators of
a lot of burden in their task.
Note that the presented experiments were done
using parsing models that perform far from the lat-
est F1 results; their intention was to assess the util-
ity of the IPP schema. Expected relative reduc-
tions with IPP systems incorporating state-of-the-
art parsers would not be so large.
PCFG Baseline IPP RelRedF1 TCER TCAC
h=0, v=1 0.67 0.40 0.22 45%
h=0, v=2 0.68 0.39 0.21 46%
h=0, v=3 0.70 0.38 0.22 42%
Table 1: Results for the test set: F1 and TCER
for the baseline system; TCAC for the IPP system;
relative reduction beteween TCER and TCAC.
5 Conclusions
We have introduced a novel Interactive Predictive
Parsing framewrok which can be operated by a
user to obtain error free trees. We have simulated
interaction with this system and calculated evalau-
tion metrics, which established that an IPP system
results in a high amount of effort reduction for a
manual annotator compared to a two-step system.
4http://nltk.sourceforge.net/
5This method implements the vertical (v value) and hori-
zontal (h value) markovizations (Klein and Manning, 2003).
Near term future work includes applying the
IPP scenario to state-of-the-art reranking and pars-
ing systems, as well as in the development of adap-
tative parsing systems
References
Barrachina, Sergio, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jess Toms, En-
rique Vidal, Juan-Miguel Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. In Com-
putational Linguistics, 35(1) 3-28.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In NAACL ?00, 132-139.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. In Computational
Linguistics, 29(4):589-637.
De la Clergerie, ?Eric, Olivier Hamon, Djamel Mostefa,
Christelle Ayache, Patrick Paroubek and Anne Vil-
nat. 2008. PASSAGE: from French Parser Evalua-
tion to Large Sized Treebank. In LREC?08.
Huang, Liang. 2008. Forest reranking: discriminative
parsing with non-local features. In ACL ?08.
Johnson, Mark. 1998. PCFG models of linguistic
tree representation. In Computational Linguistics,
24:613-632.
Klein, Dan and Chistopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In ACL ?03, 423-430.
Lease, Matthew, Eugene Charniak, Mark Johnson and
David McClosky. 2006. A look at parsing and its
applications. In National Conference on Artificial
Intelligence, vol. 21-II, 1642-1645.
Marcus, Mitchell P., Mary Ann Marcinkiewicz and
Beatrice Santorini. 1995. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics 19(2), 313-330.
Matsuzaki, Takuya, Yasuke Miyao and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL ?05, 75-82.
McClosky, David, Eugene Charniak and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL ?06
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT ?07.
Toselli, Alejandro, Vero?nica Romero and Enrique Vi-
dal. 2008. Computer Assisted Transcription of Text
Images and Multimodal Interaction. In MLMI ?08.
Vidal, Enrique, Francisco Casacuberta, Luis Ro-
driguez, Jorge Civera and Carlos D. Martnez Hinare-
jos. 2006. Computer-assisted translation using
speech recognition. In IEEE Trans. on Audio,
Speech, and Language Processing, 14(3), 941-951.
Yamamoto, Ryo, Shinji Sako, Takuya Nishimoto and
Shigeki Sagayama. 2006. On-line recognition
of handwritten mathematical expressions based on
stroke-based stochastic context-free grammar. In
10th International Workshop on Frontiers in Hand-
writing Recognition.
225
Coling 2010: Poster Volume, pages 1220?1228,
Beijing, August 2010
Confidence Measures for Error Discrimination
in an Interactive Predictive Parsing Framework1
Ricardo Sa?nchez-Sa?ez, Joan Andreu Sa?nchez and Jose? Miguel Bened
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{rsanchez,jandreu,jbenedi}@dsic.upv.es
Abstract
We study the use of Confidence Measures
(CM) for erroneous constituent discrimi-
nation in an Interactive Predictive Parsing
(IPP) framework. The IPP framework al-
lows to build interactive tree annotation
systems that can help human correctors
in constructing error-free parse trees with
little effort (compared to manually post-
editing the trees obtained from an auto-
matic parser). We show that CMs can
help in detecting erroneous constituents
more quickly through all the IPP process.
We present two methods for precalculat-
ing the confidence threshold (globally and
per-interaction), and observe that CMs re-
main highly discriminant as the IPP pro-
cess advances.
1 Introduction
Within the Natural Language Processing (NLP)
field, we can tell apart two different usage scenar-
ios for automatic systems that output or work with
natural language. On one hand, we have the cases
in which the output of such systems is expected to
be used in a vanilla fashion, that is, without val-
idating or correcting the results produced by the
system. Within this usage scheme, the most im-
portant factor of a given automatic system is the
quality of the results. Although memory and com-
putational requirements of such systems are usu-
ally taken into account, the ultimate aim of most
1Work partially supported by the Spanish MICINN under
the MIPRCV ?Consolider Ingenio 2010? (CSD2007-00018),
MITTRAL (TIN2009-14633-C03-01), Prometeo (PROME-
TEO/2009/014) research projects, and the FPU fellowship
AP2006-01363.
research that relates to this scenario is to minimize
the amount of error (measured with metrics like
Word Error Rate, BLEU, F-Measure, etc.) present
within the results that are being produced.
The second usage scenario arises when there
exists the need for perfect and completely error-
free results, for example, flawlessly translated
sentences or correctly annotated syntactic trees.
In such cases, the intervention of a human valida-
tor/corrector is unavoidable. The corrector will
review and validate the results, making the suit-
able modifications before the system output can
be employed. In these kind of tasks, the most im-
portant factor to be minimized is the human ef-
fort that has to be applied to transform the sys-
tem?s potentially incorrect output into validated
and error-free output. Measuring user effort has
an intrinsic subjectivity that makes it hard to be
quantitatized. Given that the user effort is usually
inversely proportional to the quality of the system
output, most research about problems associated
to this scenario t to minimize just the system?s er-
ror rate as well.
Interactive Predictive NLP Systems
Only recently, more comparable and repro-
ducible evaluation methods for Interactive Natural
Language Systems have started to be developed,
within the context of Interactive Predictive Sys-
tems (IPS). These systems formally integrate the
correcting user into the loop, making him part of
the system right at its theoretical framework. IPSs
allow for human correctors to spare effort because
the system updates its output after each individ-
ual user correction, potentially fixing several er-
rors at each step. Interactive Predictive methods
have been studied and successfully used in fields
1220
like Handwritten Text Recognition (HTR) (Toselli
et al, 2008) and Statistical Machine Translation
(SMT) (Vidal et al, 2006; Barrachina et al, 2009)
to ease the work of transcriptors and translators.
In IPS related research the importance of the
system base error rate per se is diminished. In-
stead, the intention is to measure how well the
user and the system work together. For this, for-
mal user simulation protocols together with new
objective effort evaluation metrics such as the
Word Stroke Ratio (WSR) (Toselli et al, 2008) or
the Key-Stroke and Mouse-Ratio (KSMR) (Bar-
rachina et al, 2009) started to be used as a
benchmark. These ratios reflect the amount of
user effort (whole-word corrections in the case of
WSR; keystrokes plus mouse actions in the case of
KSMR) given a certain output. To get the amount
of user effort into context they should be measured
against the corresponding error ratios of compara-
ble non-interactive systems: Word Error Rate for
WSR and Character Error Rate for KSMR.
This dichotomy in evaluating either system per-
formance or user effort applies to Syntactic Pars-
ing as well. The objective of parsing is to pre-
cisely determine the syntactic structure of sen-
tences written in one of the several languages that
humans use. Very bright research has been carried
out in this field, resulting in several top perform-
ing completely automatic parsers (Collins, 2003;
Klein and Manning, 2003; McClosky et al, 2006;
Huang, 2008; Petrov, 2010). However, these pro-
duce results that are erroneous to some extent, and
as such unsuitable for some applications without a
previous manual correction. There are many prob-
lems where error-free results consisting in per-
fectly annotated trees are needed, such as hand-
written mathematical expression recognition (Ya-
mamoto et al, 2006) or construction of large new
gold treebanks (de la Clergerie et al, 2008).
When using automatic parsers as a baseline for
building perfect syntactic trees, the role of the
human annotator is usually to post-edit the trees
and correct the errors. This manner of operat-
ing results in the typical two-step process for er-
ror correcting, in which the system first gener-
ates the whole output and then the user verifies
or amends it. This paradigm is rather inefficient
and uncomfortable for the human annotator. For
example, a basic two-stage setup was employed
in the creation of the Penn Treebank annotated
corpus: a rudimentary parsing system provided a
skeletal syntactic representation, which then was
manually corrected by human annotators (Marcus
et al, 1994). Additional works within this field
have presented systems that act as a computerized
aid to the user in obtaining the perfect annotation
(Carter, 1997; Oepen et al, 2004; Hiroshi et al,
2005). Subjective measuring of the effort needed
to obtain perfect annotations was reported in some
of these works, but we feel that a more compara-
ble metric is needed.
With the objective of reducing the user effort
and making the laborious task of tree annotation
easier, the authors of (Sa?nchez-Sa?ez et al, 2009a)
devised an Interactive Predictive Parsing (IPP)
framework. That work embeds the human cor-
rector into the automatic parser, and allows him
to interact in real time within the system. In this
manner, the system can use the readily available
user feedback to make predictions about the parts
of the trees that have not been validated by the
corrector. The authors simulated user interaction
and calculated effort evaluation metrics, establish-
ing that an IPP system results in amounts slightly
above 40% of effort reduction for a manual anno-
tator compared to a two-step system.
Confidence Measures in NLP
Annotating trees syntactically, even with the
aid of automatic systems, generally requires hu-
man intervention with a high degree of special-
ization. This fact partially justifies the shortage
in large manually annotated treebanks. Endeavors
directed at easing the burden for the experts per-
forming this task could be of great help.
One approach that can be followed in reducing
user effort within an IPS is adding information
that helps the user to locate the individual errors
in a sentence, so he can correct them in a hastier
fashion. The use of the Confidence Measure (CM)
formalism goes in this direction, allowing us to
assign a probability of correctness for individual
erroneous constituents of a more complex output
block of a NLP system.
In fields such as HTR, SMT or Automatic
Speech Recognition (ASR), the output sentences
1221
have a global probability (or score) that reflects
the likeness of the output sentence being correct.
CMs allow precision beyond the sentence level in
predicting errors: they can be used to label the in-
dividual words as either correct or incorrect. Au-
tomatic systems can use CMs to help the user in
identifying the erroneous parts of the output in a
faster way or to aid with the amendments by sug-
gesting replacement words that are likely to be
correct.
Previous research shows that CMs have been
successfully applied within the ASR (Wessel et
al., 2001), HTR (Tarazo?n et al, 2009; Serrano
et al, 2010) and SMT (Ueffing and Ney, 2007)
fields. In these works, the ability of CMs in de-
tecting erroneous constituents is assessed by the
classical confidence metrics: the Confidence Er-
ror Rate (CER) and the Receiver Operating Char-
acteristic (ROC) (Ueffing and Ney, 2007).
However, until recent advances, the use of CMs
remained largely unexplored in Parsing. Assess-
ing the correctness of the different parts of a pars-
ing tree can be useful in improving the efficiency
and usability of an IPP system, not only by tag-
ging parts with low confidence for the user to re-
view, but also by automating part of the correction
process itself by presenting constituents that yield
a higher confidence when an error is confirmed by
the user.
CMs for parsing in the form of combinations
of features calculated from n-best lists were pro-
posed in (Bened?? et al, 2007). Later on, the au-
thors of (Sa?nchez-Sa?ez et al, 2009b) introduced
a statistical method for calculating a CM for each
of the constituents in a parse tree. In that work,
CMs are calculated using the posterior probability
of each tree constituent, approach which is similar
to the word-graph based methods in the ASR and
SMT fields.
In this paper, we apply Confidence Measures
to the Interactive Predictive Parsing framework to
asses how CMs are increasingly more accurate as
the user validates subtrees within the interactive
process. We prove that after each correction per-
formed by the user, the CMs of the remaining un-
validated constituents are more helpful to detect
errors.
2 Interactive Predictive Parsing
In this section we review the IPP framework
(Sa?nchez-Sa?ez et al, 2009a) and its underlying
operation protocol. In parsing, a syntactic tree t,
attached to a string x = x1 . . . x|x| is composed
by substructures called constituents. A constituent
cAij is defined by the nonterminal symbol (either
a syntactic label or a POS tag) A and its span
ij (the starting and ending indexes which delimit
the part of the input sentence encompassed by the
constituent).
Here follows a general formulation for the non-
interactive syntactic parsing scenario, which will
allow us to better introduce the IPP formulation.
Assume that using a given parsing model G, the
parser analyzes the input sentence x and produces
the most probable parse tree
t? = argmax
t?T
pG(t|x), (1)
where pG(t|x) is the probability of the parse tree
t given the input string x using model G, and T is
the set of all possible parse trees for x.
In the IPP framework, the manual corrector
provides feedback to the system by correcting any
of the constituents cAij from t?. The system reacts
to each of the corrections performed by the human
annotator by proposing a new t?? that takes into ac-
count the correction.
Within the IPP framework, the user reviews the
constituents contained in the tree to assess their
correctness. When the user finds an incorrect con-
stituent he modifies it, setting the correct span and
label. This action implicitly validates what it is
called the validated prefix tree tp.
We define the validated prefix tree to be com-
posed by the partially corrected constituent, all
of its ancestor constituents, and all constituents
whose end span is lower than the start span of the
corrected constituent. When the user replaces the
constituent cAij with the correct one c?Aij , the vali-
dated prefix tree is
tp(c?Aij ) = {cBmn : m ? i, n ? j ,
d(cBmn) ? d(c?Aij )} ?
{cDpq : q < i }
(2)
1222
with d(cZab) being the depth (distance from root)
of constituent cZab.
The validated prefix tree is parallel to the vali-
dated sentence prefix commonly used in Interac-
tive Machine Translation or Interactive Handwrit-
ten Recognition, and is established after each user
action.
This particular definition of the prefix tree de-
termines the fact that the user is expected to re-
view the parse tree in a preorder fashion (left-to-
right depth-first). Note that this specific explo-
ration order allows us to simulate the user inter-
action for the experimentation, as we will explain
below. Also note that other types of prefixes could
be defined, allowing for different tree review or-
ders.
Within the IPP formulation, when a constituent
correction is performed, the prefix tree tp(c?Aij ) is
validated and a new tree t?? that takes into account
the prefix is proposed. Incorporating this new
evidence into expression (1) yields the following
equation
t?? = argmax
t?T
pG(t|x, tp(c?Aij )). (3)
Given the properties of Probabilistic Context-
Free Grammars (PCFG) the only subtree that ef-
fectively needs to be recalculated is the one start-
ing from the parent of the corrected constituent.
This way, just the descendants of the newly intro-
duced constituent, as well as its right hand siblings
(along with their descendants) are calculated.
2.1 User Interaction Operation
The IPP formulation allows for a very straightfor-
ward operation protocol that is performed by the
manual corrector, in which he validates or corrects
the successive output parse trees:
1. The IPP system proposes a full parse tree t
for the input sentence.
2. Then, the user finds the first incorrect con-
stituent exploring the tree in a certain ordered
manner (preorder in our case, given by the
tree prefix definition) and amends it, by mod-
ifying its span and/or label (implicitly vali-
dating the prefix tree tp).
3. The IPP system produces the most probable
tree that is compatible with the validated pre-
fix tree tp as shown in expression (3).
4. These steps are iterated until a final, perfect
parse tree is produced by the system and val-
idated by the user.
It is worth noting that within this protocol, con-
stituents can be automatically deleted or inserted
at the end of any subtree in the syntactic struc-
ture by adequately modifying the span of the left-
neighbouring constituent.
The IPP interaction process is similar to the
ones already established in HTR and SMT. In
these fields, the user reads the output sentence
from left to right. When the user finds and corrects
an erroneous word, he is implicitly validating the
prefix sentence up to that word. The remaining
suffix sentence is recalculated by the system tak-
ing into account the validated prefix sentence.
Fig. 1 shows an example that intends to clar-
ify the Interactive Predictive process. First, the
system provides a tentative parse tree (Fig. 1.b).
Then the user, which has the correct reference tree
(Fig. 1.a) in mind, notices that it has two wrong
constituents (cX23 and cZ44) (Fig. 1.c), and chooses
to replace cX23 by cB22 (Fig. 1.d). Here, cB22 cor-
responds to c?Aij of expression (3). As the user
does this correction, the system automatically val-
idates the prefix (dashed line in Fig. 1.d, tp(c?Aij )
of expression (2)). The system also invalidates
the subtrees outside the prefix (dotted line line in
Fig. 1.d). Finally, the system automatically pre-
dicts a new subtree (Fig. 1.e). Notice how cZ34
changes its span and cD44 is introduced which pro-
vides the correct reference parse.
For further exemplification, Sa?nchez-Sa?ez
et al (2010) demonstrate an IPP based
annotation tool that can be accessed at
http://cat.iti.upv.es/ipp/.
Within the IPP scenario, the user has to man-
ually review all the system output and correct or
validate it, which is still a considerable amount of
effort. CMs can ease this work by helping to spot
the erroneous constituents.
1223
SB Z
Y
ba c d
A
DC
(a) Reference tree
S
ba c d
A
CB
X
Y
Z
(b) Iteration 0: Pro-
posed output tree 1
S
ba c d
A
CB
X Z 423 4
Y
(c) Iteration 0: Erro-
neous constituents
Y
S
ba c d
A
B 2
2 ?
? ?
(d) Iteration 1:
User corrected
constituent
S
B Z
Y
ba c d
A
DC
3
4
(e) Iteration 1:
Proposed output
tree 2
Figure 1: Synthetic example of user interaction with the IPP system.
3 Confidence Measures
Probabilistic calculation of Confidence Measures
(Sa?nchez-Sa?ez et al, 2009b) for all tree con-
stituents can be introduced within the IPP process.
The CM of each constituent is its posterior
probability, which can be considered as a measure
of the degree to which the constituent is believed
to be correct for a given input sentence x. This is
formulated as follows
pG(cAij |x) =
pG(cAij ,x)
pG(x)
=
?
t??T ; c?Aij ?t? ?(c
A
ij , c?Aij ) pG(t?|x)
pG(x)
(4)
with ?() being the Kronecker delta function. Nu-
merator in expression (4) stands for the probabil-
ity of all parse trees for x that contain the con-
stituent cAij (see Fig. 2).
S
A
?A(i, j)
?A(i, j)
x1 xi?1 xi xj xj+1 x|x|
Figure 2: The product of the inside and outside
probabilities for each constituent comprises the
upper part of expression (5)
The posterior probability is computed with the
inside ? and outside ? probabilities (Baker, 1979)
C(tAij) = pG(cAij |x) =
pG(cAij ,x)
pG(x)
= ?A(i, j) ?A(i, j)?S(1, |x|)
.
(5)
It should be clear that the calculation of con-
fidence measures reviewed here is generalizable
for any problem that employs PCFGs, and not
just NLP tasks. In the experiments presented in
the following section we show that CMs are in-
creasingly discriminant when used within the IPP
framework to detect erroneous constituents.
4 Experiments
Evaluation of the quality of CMs within the IPP
framework is done in a completely automatic
fashion by simulating user interaction. Section 4.1
introduces the evaluation protocol and metrics
measuring CM quality (i.e., their ability to de-
tect incorrect constituents). The experimentation
framework and the results are discussed in sec-
tion 4.2.
4.1 Evaluation Methods
4.1.1 IPP Evaluation
A good measure of the performance of an In-
teractive Predictive System is the amount of ef-
fort saved by the users of such a system. It is
subjective and expensive to test an IPS with real
users, so these systems are usually evaluated us-
ing automatically calculated metrics that assess
the amount of effort saved by the user.
1224
As already mentioned, the objective of an IPP
based system is to be employed by annotators to
construct correct syntactic trees with less effort.
Evaluation of an IPP system was previously done
by comparing the IPP usage effort (the number of
corrections using the IPP system) against the es-
timated effort required to manually post-edit the
trees after obtaining them with a traditional au-
tomatic parsing system (the amount of incorrect
constituents) (Sa?nchez-Sa?ez et al, 2009a).
In the case of IPP, the gold reference trees are
used to simulate system interaction by a human
corrector and provide a comparable benchmark.
This automatic evaluation protocol is similar to
the one presented in section 2.1:
1. The IPP system proposes a full parse tree t
for the input sentence.
2. The user simulation subsystem finds the first
incorrect constituent by exploring the tree in
the order defined by the prefix tree definition
(preorder) and comparing it with the refer-
ence. When the first erroneous constituent
is found, it is amended by being replaced in
the output tree by the correct one, operation
which implicitly validates the prefix tree tp.
3. The IPP system produces the most probable
tree that is compatible with the validated pre-
fix tree tp.
4. These steps are iterated until a final, perfect
parse tree is produced by the IPP system and
validated against the reference by the user
simulation subsystem.
In this work, metrics assessing the quality of
CM are introduced within this automatic protocol.
We calculate and report them after each of the it-
erations in the IPP process.
4.1.2 Confidence Measure Evaluation
Metrics
The CM of each tree constituent, computed as
shown in expression (4) can be seen as its prob-
ability of being correct. Once all CM are calcu-
lated, a confidence threshold ? ? [0, 1] can be
chosen. Constituents are then marked using ? : the
ones with a confidence above this threshold are
marked as correct, and the rest as incorrect. Com-
paring the confidence marks in the output tree
with the reference, we obtain the false rejection
Nf (?) ? [0, Nc] (number of correct constituents
in the output tree wrongly marked as incorrect by
their CM) and the true rejection Nt(?) ? [0, Ni]
(number of incorrect constituents in the output
tree that are indeed detected as incorrect by their
confidence).
The amount of correct and incorrect con-
stituents in each tree is Nc and Ni respectively. In
the ideal case of perfectly error discriminant CM,
using the best threshold would yield Nf (?) = 0
and Nt(?) = Ni.
A evaluation metric that assess the ability of
CMs in telling apart correct constituents from in-
correct ones is the Confidence Error Rate (CER):
CER(?) = Nf (?) + (Ni ?Nt(?))Nc +Ni
. (6)
The CER is the number of errors incurred by the
CMs divided by the total number of constituents.
The CER can be compared with the Absolute
Constituent Error Rate (ACER), which is the CER
obtained assuming that all constituents are marked
as correct (the only possible assumption when CM
are not available):
ACER = CER(0) = NiNc +Ni
. (7)
4.2 Experimental Framework
Our experiments were carried out over the Wall
Street Journal Penn Treebank (PTB) manually an-
notated corpus. Three sets were defined over the
PTB: train (sections 2 to 21), test (section 23),
and development (the first 346 sentences of sec-
tion 24). Before carrying out experimentation, the
NoEmpties transformation was applied to all sets
(Klein and Manning, 2001).
We implemented the CYK-Viterbi parsing al-
gorithm as the parse engine within the IPP
framework. This algorithm uses grammars in
the Chomsky Normal Form (CNF) so we em-
ployed the open source Natural Language Toolkit2
(NLTK) to obtain several right-factored binary
2http://www.nltk.org/
1225
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(a) PCFG: h=0,v=1
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(b) PCFG: h=0,v=2
Figure 3: CER results over IPP system interaction. Threshold fixed at before the interactive process.
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(a) PCFG: h=0,v=1
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(b) PCFG: h=0,v=2
Figure 4: CER results over IPP system interaction. Threshold optimized for each step of the interactive
process.
grammars with different markovization parame-
ters from the training set (Klein and Manning,
2003).
The purpose of our experimentation is to de-
termine if CMs can successfully discriminate er-
roneous constituents from correct ones within an
IPP process, that is, if they help the user to find
errors in a hastier manner. For this we need to
assess if there exists discriminant information in
the CMs corresponding to the constituents of the
unvalidated part of the successive IPP-proposed
trees.
With this objective in mind, we introduced a
CM calculation step after each user interaction
within the IPP process. CMs for all constituents in
each tree were obtained as described in section 3.
After each simulated interaction, we also calcu-
lated the ACER and CER over all the syntactic
constituents of the whole test set.
Each IPP user interaction yields a parse tree
which can be seen as the concatenation of two
parts: the validated prefix tree (which is known
to be correct because the user, or the user simula-
tion subsystem in this case, has already reviewed
it) and a new suffix tree which is calculated by
the IPP system based on the validated prefix, as
shown in section 2.
The fact that the validated prefix is already
known to be correct is taken into account by the
CM calculation process, and the confidence of the
constituents in the prefix tree is automatically set
to their maximum score, equal to 1. This fact
causes that the CMs become more discriminant
after each interaction, because a larger part of the
1226
tree (the prefix) has a completely correct confi-
dence. The key point here is to measure if this
increasingly reduced CER (CM error rate) main-
tains its advantage over the also increasingly re-
duced ACER (absolute constituent error rate with-
out taking CMs into account) which would mean
that the CMs retain their discriminant power and
can be useful as an aid for a human annotator us-
ing an IPP system.
Two batches of experiments were performed
and, in each of them, two different markovizations
of the vanilla PCFG were tested as the parsing
model.
In the first battery of experiments, the confi-
dence threshold ? was optimized over the devel-
opment set before starting the IPP process, re-
maining the same during the user interaction. The
results can be seen in Fig. 3, which shows the
obtained baseline ACER and the CER (the con-
fidence assessing metric) for the test set after each
user interaction. We see how CMs retain all of
their error detection capabilities during the IPP
process: in the h0v1 PCFG they are able to dis-
cern about 25% of incorrect constituents at most
stages of the IPP process, with a slight bump up to
27% after about 7 user interactions; for the h0v2
PCFG they are able to detect about 18% of incor-
rect constituents at the first interactions, but go up
to detect 27% of errors after about 7 or more in-
teractions.
In the second experimental setup, a different
threshold for each interaction step was calcu-
lated by performing the IPP user simulation pro-
cess over the development set and optimizing
the threshold value. The results can be seen in
Fig. 4. We observe improvements in the discrim-
inant ability of confidence values after 8 user in-
teractions, with them being capable to detect more
errors towards the end of each IPP session: about
34% of errors for h0v1, and 49% of them for h0v2.
The calculated thresholds have also been plot-
ted in the aforementioned figures. For the per-
interaction threshold experimentation, we can see
how the threshold gets fine-tuned as the IPP pro-
cess advances. The lower threshold values for the
last interactions were expected due to the fact that
more constituents have been validated and have
the maximum confidence. This method for pre-
calculating one specific threshold for each of the
iterations could be useful when incorporating CM
to a real IPP based annotator.
5 Conclusions and Future Work
We have proved that using Confidence Measures
can be used to discriminate incorrect constituents
from correct ones over an Interactive Predictive
Parsing process. We have show two methods
for calculating the threshold used to mark con-
stituents as correct/incorrect, showing the advan-
tage of precalculating a specific threshold for each
of the interaction steps.
Immediate future work involves implementing
CMs as a visual aid in a real IPP system like
the one presented in (Sa?nchez-Sa?ez et al, 2010).
Through he use of CMs, all constituents in the
successive trees could be color-coded according
to their correctness confidence, so the user could
focus and make corrections faster.
Future research paths can deal with applying
CMs to improve the output of completely auto-
matic parsers, for example, using them as a com-
ponent of an n-best re-ranking system.
Additionally, the IPP framework is also suit-
able for studying and applying training algorithms
within the Active Learning and Adaptative/Online
Parsing paradigms. This kind of systems could
improve their models at operating time, by incor-
porating new ground truth data as it is provided by
the user.
References
Baker, JK. 1979. Trainable grammars for speech
recognition. Journal of the Acoustical Society of
America, 65:132.
Barrachina, S., O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Toma?s,
E. Vidal, and J.M. Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. Compu-
tational Linguistics, 35(1):3?28.
Bened??, J.M., J.A. Sa?nchez, and A. Sanch??s. 2007.
Confidence measures for stochastic parsing. In
Proc. of RANLP, pages 58?63, Borovets, Bulgaria,
27-29 September.
Carter, D. 1997. The TreeBanker. A tool for super-
vised training of parsed corpora. In Proc. of EN-
VGRAM Workshop, pages 9?15.
1227
Collins, M. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
de la Clergerie, E.V., O. Hamon, D. Mostefa, C. Ay-
ache, P. Paroubek, and A. Vilnat. 2008. Passage:
from French parser evaluation to large sized tree-
bank. Proc. of LREC, 100:2.
Hiroshi, I., N. Masaki, H. Taiichi, T. Takenobu, and
T. Hozumi. 2005. eBonsai: An integrated environ-
ment for annotating treebanks. In Proc. of IJCNLP,
pages 108?113.
Huang, L. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL.
Klein, D. and C.D. Manning. 2001. Parsing with
treebank grammars: Empirical bounds, theoretical
models, and the structure of the Penn treebank. In
Proc. of ACL, pages 338?345, Morristown, USA.
ACL.
Klein, D. and C.D. Manning. 2003. Accurate unlex-
icalized parsing. In Proc. of ACL, volume 1, pages
423?430, Morristown, USA. ACL.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
McClosky, D., E. Charniak, and M. Johnson. 2006.
Effective self-training for parsing. In Proc. of
NAACL-HLT, pages 152?159.
Oepen, S., D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods. Research on Lan-
guage & Computation, 2(4):575?596.
Petrov, S. 2010. Products of Random Latent Variable
Grammars. Proc. of NAACL-HLT.
Sa?nchez-Sa?ez, R., J.A. Sa?nchez, and J.M. Bened??.
2009a. Interactive predictive parsing. In Proc. of
IWPT?09, pages 222?225, Paris, France, October.
ACL.
Sa?nchez-Sa?ez, R., J.A. Sa?nchez, and J.M. Bened??.
2009b. Statistical confidence measures for proba-
bilistic parsing. In Proc. of RANLP, pages 388?392,
Borovets, Bulgaria, September.
Sa?nchez-Sa?ez, R., L.A. Leiva, J.A. Sa?nchez, and J.M.
Bened??. 2010. Interactive predictive parsing using
a web-based architecture. In Proc. of NAACL-HLT,
Los Angeles, United States of America, June.
Serrano, N., A. Sanchis, and A. Juan. 2010. Bal-
ancing error and supervision effort in interactive-
predictive handwriting recognition. In Proc. of IUI,
pages 373?376. ACM.
Tarazo?n, L., D. Pe?rez, N. Serrano, V. Alabau,
O. Ramos Terrades, A. Sanchis, and A. Juan. 2009.
Confidence Measures for Error Correction in Inter-
active Transcription of Handwritten Text. In Proc.
of ICIAP, pages 567?574, Vietri sul Mare, Italy,
September. LNCS.
Toselli, A.H., V. Romero, and E. Vidal. 2008. Com-
puter assisted transcription of text images and mul-
timodal interaction. In Proc. MLMI, volume 5237,
pages 296?308. Springer.
Ueffing, N. and H. Ney. 2007. Word-level confidence
estimation for machine translation. Computational
Linguistics, 33(1):9?40.
Vidal, E., F. Casacuberta, L. Rodr??guez, J. Civera, and
C. Mart??nez. 2006. Computer-assisted translation
using speech recognition. IEEE TASLP, 14(3):941?
951.
Wessel, F., R. Schluter, K. Macherey, and H. Ney.
2001. Confidence measures for large vocabu-
lary continuous speech recognition. IEEE TSAP,
9(3):288?298.
Yamamoto, R., S. Sako, T. Nishimoto, and
S. Sagayama. 2006. On-line recognition of
handwritten mathematical expressions based on
stroke-based stochastic context-free grammar. In
Proc of ICFHR, pages 249?254.
1228
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 244?254,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Interactive Machine Translation using Hierarchical Translation Models
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, Jose?-Miguel Bened??, Francisco Casacuberta
D. de Sistemas Informa?ticos y Computacio?n
Universitat Polite`cnica de Vale`ncia
Camino de Vera s/n, 46021 Valencia (Spain)
{jegonzalez,dortiz,jbenedi,fcn}@dsic.upv.es
Abstract
Current automatic machine translation sys-
tems are not able to generate error-free trans-
lations and human intervention is often re-
quired to correct their output. Alternatively,
an interactive framework that integrates the
human knowledge into the translation pro-
cess has been presented in previous works.
Here, we describe a new interactive ma-
chine translation approach that is able to work
with phrase-based and hierarchical translation
models, and integrates error-correction all in
a unified statistical framework. In our experi-
ments, our approach outperforms previous in-
teractive translation systems, and achieves es-
timated effort reductions of as much as 48%
relative over a traditional post-edition system.
1 Introduction
Research in the field of machine translation (MT)
aims to develop computer systems which are able
to translate between languages automatically, with-
out human intervention. However, the quality of
the translations produced by any automatic MT sys-
tem still remain below than that of human transla-
tion. Typical solutions to reach human-level quality
require a subsequent manual post-editing process.
Such decoupled post-edition solution is rather inef-
ficient and tedious for the human translator. More-
over, it prevents the MT system from taking advan-
tage of the knowledge of the human translator and,
reciprocal, the human translator cannot take advan-
tage of the adapting ability of MT technology.
An alternative way to take advantage of the exist-
ing MT technology is to use them in collaboration
with human translators within a computer-assisted
translation (CAT) or interactive framework (Isabelle
and Church, 1998). The TransType and TransType2
projects (Foster et al, 1998; Langlais and Lapalme,
2002; Barrachina et al, 2009) entailed an interesting
focus shift in CAT technology by aiming interaction
directly at the production of the target text. These
research projects proposed to embed an MT system
within an interactive translation environment. This
way, the human translator can ensure a high-quality
output while the MT system ensures a significant
gain of productivity. Particularly interesting is the
interactive machine translation (IMT) approach pro-
posed in (Barrachina et al, 2009). In this scenario,
a statistical MT (SMT) system uses the source sen-
tence and a previously validated part (prefix1) of its
translation to propose a suitable continuation. Then
the user finds and corrects the next system error,
thereby providing a longer prefix which the system
uses to suggests a new, hopefully better continua-
tion. The reported results showed that IMT can save
a significant amount of human effort.
Barrachina et al. (2009) provide a thorough de-
scription of the IMT approach and describe algo-
rithms for its practical implementation. Neverthe-
less, we identify two basic problems for which we
think there is room for improvement. The first prob-
lem arises when the system cannot generate the pre-
fix validated by the user. To solve this problem,
the authors simply provide an ad-hoc heuristic error-
correction technique. The second problem is how
the system deals with word reordering. Particularly,
the models used by the system were either mono-
1We use the terms prefix and suffix to denote any sub-string
at the beginning and end respectively of a string of characters
(including spaces and punctuation). These terms do not imply
any morphological significance as they usually do in linguistics.
244
tonic by nature or non-monotonic but heuristically
defined (not estimated from training data).
We work on the foundations of Barrachina et
al., (2009) and provide formal solutions to these
two challenges. On the one hand, we adopt the
statistical formalization of the IMT framework de-
scribed in (Ortiz-Mart??nez, 2011), which includes
a stochastic error-correction model in its formaliza-
tion to address prefix coverage problems. Moreover,
we refine this formalization proposing an alternative
error-correction formalization for the IMT frame-
work (Section 2). Additionally, we also propose a
specific error-correction model based on a statisti-
cal interpretation of the Levenshtein distance (Lev-
enshtein, 1966). These formalizations provide a
unified statistical framework for the IMT model in
comparison to the ad-hoc heuristic error-correction
methods previously used.
In order to address the problem of properly deal
with reordering in IMT, we introduce the use of hi-
erarchical MT models (Chiang, 2005; Zollmann and
Venugopal, 2006). These methods provide a natural
approach to handle long range dependencies and al-
low the incorporation of reordering information into
a consistent statistical framework. Here, we also de-
scribe how state-of-the-art hierarchical MT models
can be extended to handle IMT (Sections 3 and 4).
We evaluate the proposed IMT approach on two
different translation task. The comparative results
against the IMT approach described by Barrachina
et al, (2009) and a conventional post-edition ap-
proach show that our IMT formalization for hier-
archical SMT models indeed outperform other ap-
proaches (Sections 5 and 6). Moreover, it leads to
large reductions in the human effort required to gen-
erate error-free translations.
2 Statistical Framework
2.1 Statistical Machine Translation
Assuming that we are given a sentence s in a source
language, the translation problem can be stated as
finding its translation t in a target language of max-
imum probability (Brown et al, 1993):
t? = argmax
t
Pr(t | s) (1)
= argmax
t
Pr(t) ? Pr(s | t) (2)
source (s): Para ver la lista de recursos
desired translation (t?): To view a listing of resources
IT-0
p
ts To view the resources list
IT-1
p To view
k a
ts list of resources
IT-2
p To view a list
k list i
ts list i ng resources
IT-3
p To view a listing
k o
ts o f resources
END p To view a listing of resources
Figure 1: IMT session to translate a Spanish sentence into
English. The desired translation is the translation the hu-
man user wants to obtain. At IT-0, the system suggests
a translation (ts). At IT-1, the user moves the mouse to
accept the first eight characters ?To view ? and presses the
a key (k), then the system suggests completing the sen-
tence with ?list of resources? (a new ts). Iterations 2 and
3 are similar. In the final iteration, the user accepts the
current translation.
The terms in the latter equation are the lan-
guage model probability Pr(t) that represents the
well-formedness of t (n-gram models are usu-
ally adopted), and the (inverted) translation model
Pr(s | t) that represents the relationship between the
source sentence and its translation.
In practice all of these models (and possibly oth-
ers) are often combined into a log-linear model for
Pr(t | s) (Och and Ney, 2002):
t? = argmax
t
{
N?
n=1
?n ? log(fn(t, s))
}
(3)
where fn(t, s) can be any model that represents an
important feature for the translation, N is the num-
ber of models (or features), and ?n are the weights
of the log-linear combination.
2.2 Statistical Interactive Machine Translation
Unfortunately, current MT technology is still far
from perfect. This implies that, in order to achieve
good translations, manual post-editing is needed.
An alternative to this decoupled approach (first
MT, then manual correction) is given by the IMT
245
paradigm (Barrachina et al, 2009). Under this
paradigm, translation is considered as an iterative
left-to-right process where the human and the com-
puter collaborate to generate the final translation.
Figure 1 shows an example of the IMT approach.
There, a source Spanish sentence s =?Para ver la
lista de recursos? is to be translated into a target En-
glish sentence t?. Initially, with no user feedback, the
system suggests a complete translation ts =?To view
the resources list?. From this translation, the user
marks a prefix p =?To view? as correct and begins
to type the rest of the target sentence. Depending on
the system or the user?s preferences, the user might
type the full next word, or only some letters of it (in
our example, the user types the single next charac-
ter ?a?). Then, the MT system suggests a new suffix
ts =?list of resources? that completes the validated
prefix and the input the user has just typed (p =?To
view a?). The interaction continues with a new pre-
fix validation followed, if necessary, by new input
from the user, and so on, until the user considers the
translation to be complete and satisfactory.
The crucial step of the process is the production
of the suffix. Again decision theory tells us to max-
imize the probability of the suffix given the avail-
able information. Formally, the best suffix of a given
length will be:
t?s = argmax
ts
Pr(ts | s,p) (4)
which can be straightforwardly rewritten as:
t?s = argmax
ts
Pr(p, ts | s) (5)
= argmax
ts
Pr(p, ts) ? Pr(s | p, ts) (6)
Note that, since p ts = t, this equation is very
similar to Equation (2). The main difference is that
now the search process is restricted to those target
sentences t that contains p as prefix. This implies
that we can use the same MT models (including
the log-linear approach) if the search procedures are
adequately modified (Och et al, 2003). Finally, it
should be noted that the statistical models are usu-
ally defined at word level, while the IMT process
described in this section works at character level. To
deal with this problem, during the search process it
is necessary to verify the compatibility between t
and p at character level.
2.3 IMT with Stochastic Error-Correction
A common problem in IMT arises when the user sets
a prefix which cannot be explained by the statistical
models. To solve this problem, IMT systems typi-
cally include ad-hoc error-correction techniques to
guarantee that the suffixes can be generated (Bar-
rachina et al, 2009). As an alternative to this heuris-
tic approach, Ortiz-Mart??nez (2011) proposed a for-
malization of the IMT framework that does include
stochastic error-correction models in its statistical
formalization. The starting point of this alternative
IMT formalization accounts for the problem of find-
ing the translation t that, at the same time, better
explains the source sentence s and the prefix given
by the user p:
t? = argmax
t
Pr(t | s,p) (7)
= argmax
t
Pr(t) ? Pr(s,p | t) (8)
The following na??ve Bayes? assumption is now
made: the source sentence s and the user prefix p are
statistically independent variables given the transla-
tion t, obtaining:
t? = argmax
t
Pr(t) ? Pr(s | t) ? Pr(p | t) (9)
where Pr(t) can be approximated with a language
model, Pr(s | t) can be approximated with a trans-
lation model, and Pr(p | t) can be approximated
by an error correction model that measures the com-
patibility between the user-defined prefix p and the
hypothesized translation t.
Note that the translation result, t?, given by Equa-
tion (9) may not contain p as prefix because every
translation is compatible with p with a certain prob-
ability. Thus, despite being close, Equation (9) is not
equivalent to the IMT formalization in Equation (6).
To solve this problem, we define an alignment,
a, between the user-defined prefix p and the hy-
pothesized translation t, so that the unaligned words
of t, in an appropriate order, constitute the suffix
searched in IMT. This allows us to rewrite the error
correction probability as follows:
Pr(p | t) =
?
a
Pr(p,a | t) (10)
To simplify things, we assume that p is mono-
tonically aligned to t, leaving the potential word-
reordering to the language and translation models.
246
Under this assumption, a determines an alignment
for t, such that t = tpts, where tp is fully-aligned to
p and ts remains unaligned. Taking all these things
into consideration, and following a maximum ap-
proximation, we finally arrive to the expression:
(t?, a?) = argmax
t,a
Pr(t)?Pr(s | t)?Pr(p,a | t) (11)
where the suffix required in IMT is obtained as the
portion of t? that is not aligned with the user prefix.
In practice, we combine the models in Equa-
tion (11) in a log-linear fashion as it is typically done
in SMT (see Equation (3)).
2.4 Alternative Formalization for IMT with
Stochastic Error-Correction
Alternatively to Equation (11), we can operate from
Equation (9) and reach a different formalization for
IMT with error-correction. We can re-write the first
and last terms of Equation (9) as:
Pr(t) ? Pr(p | t) = Pr(p) ? Pr(t | p) (12)
As in the previous section, we introduce an align-
ment variable, a, between t and p, giving:
Pr(t | p) =
?
a
Pr(t,a | p) (13)
=
?
a
Pr(a | p) ? Pr(t | p,a) (14)
If we consider monotonic alignments, a defines
again an alignment between a prefix of the system
translation (tp) and the user prefix, producing the
suffix required in IMT (ts) as the unaligned part.
Thus, we can re-write Pr(t | p,a) as:
Pr(t | p,a) = Pr(tp, ts | p,a) (15)
? Pr(tp | p,a) ? Pr(ts | p,a) (16)
where Equation (16) has been obtained following a
na??ve Bayes? decomposition.
Combining equations (12), (14), and (16) into
Equation (9), and following a maximum approxima-
tion for the summation of the alignment variable a,
we arrive to the following expression:
(t?, a?) = argmax
t,a
Pr(s |t)?Pr(tp |p,a)?Pr(ts |p,a) (17)
where Pr(p) and Pr(a|p) have been dropped down
because the former does not participate in the maxi-
mization and the latter is assumed uniform.
The terms in this equation can be interpreted sim-
ilarly as those in Equation (9): Pr(s | t) is the trans-
lation model, Pr(tp | p,a) is the error-correction
probability that measures the compatibility between
the prefix tp of the hypothesized translation and the
user-defined prefix p, and Pr(ts | p,a) is the lan-
guage model for the corresponding suffix ts condi-
tioned by the user-defined prefix. Again, in the ex-
periments we combine the different models in a log-
linear fashion.
The main difference between the two alternative
IMT formalization (Equations (11) and (17)) is that
in the latter the suffix to be returned is conditioned
by the user-validated prefix p. Thus, in the fol-
lowing we will refer to Equation (11) as indepen-
dent suffix formalization while we will denote Equa-
tion (17) by conditioned suffix formalization.
3 Statistical Models
We now present the statistical models used to esti-
mate the probability distributions described in the
previous section. Section 3.1 describes the error-
correction model, while Section 3.2 describes the
models for the conditional translation probability.
3.1 Statistical Error-Correction Model
Following the vast majority of IMT systems de-
scribed in the literature, we implement an error-
correction model based on the concept of edit dis-
tance (Levenshtein, 1966). Typically, IMT systems
use non-probabilistic error correction models. The
first stochastic error correction model for IMT was
proposed in (Ortiz-Mart??nez, 2011) and it is based
on probabilistic finite state machines. Here, we pro-
pose a simpler approach which can be seen as a
particular case of the previous one. Specifically,
the proposed approach models the edit distance as a
Bernoulli process where each character of the candi-
date string has a probability pe of being erroneous.
Under this interpretation, the number of characters
that need to be edited E in a sentence of length n
is a random variable that follows a binomial distri-
bution, E ? B(n, pe), with parameters n and pe.
The probability of performing exactly k edits in a
247
sentence of n characters is given by the following
probability mass function:
f(k;n, pe) =
n!
k!(n? k)!
pke(1? pe)
n?k (18)
Note that this error-correction model penalizes
equally all edit operations. Alternatively, we can
model the distance with a multinomial distribution
and assign different probabilities to different types
of edit operations. Nevertheless, we adhere to the
binomial approximation due to its simplicity.
Finally, we compute the error-correction proba-
bility between two strings from the total number of
edits required to transform the candidate translation
into the reference translation. Specifically, we define
the error-correction distribution in Equation (11) as:
Pr(p,a | t) ?
|p|!
k!(|p| ? k)!
pke(1? pe)
|p|?k (19)
where k = Lev(p, ta) is the character-level Lev-
enshtein distance between the user defined prefix p
and the prefix ta of the hypothesized translation t
defined by alignment a. The error-correction prob-
ability Pr(tp | p,a) in Equation (17) is computed
analogously.
The probability of edition pe is the single free pa-
rameter of this formulation. We will use a separate
development corpus to find an adequate value for it.
3.2 Statistical Machine Translation Models
Next sections briefly describe the statistical transla-
tion models used to estimate the conditional proba-
bility distribution Pr(s | t). A detailed description
of each model can be found in the provided citations.
3.2.1 Phrase-Based Translation Models
Phrase-based translation models (Koehn et al,
2003) are an instance of the noisy-channel approach
in Equation (2). The translation of a source sentence
s is obtained through a generative process composed
of three steps: first, the s is divided into K segments
(phrases), next, each source phrase, s?, is translated
into a target phrase t?, and finally the target phrases
are reordered to compose the final translation.
The usual phrase-based implementation of the
translation probability takes a log-linear form:
Pr(s | t) ? ?1 ? |t|+ ?2 ?K+
K?
k=1
[
?3 ? log(P (s?k | t?k)) + ?4 ? d(j)
]
(20)
where P (s? | t?) is the translation probability between
source phrase s? and target phrase t?, and d(j) is a
function (distortion model) that returns the score of
translating the k-th source phrase given that it is sep-
arated j words from the (k?1)-th phrase. Weights ?1
and ?2 play a special role since they are used to con-
trol the number of words and the number of phrases
of the target sentence to be generated, respectively.
3.2.2 Hierarchical Translation Models
Phrase-based models have shown a very strong
performance when translating between languages
that have similar word orders. However, they are not
able to adequately capture the complex relationships
that exist between the word orders of languages of
different families such as English and Chinese. Hi-
erarchical translation models provide a solution to
this challenge by allowing gaps in the phrases (Chi-
ang, 2005):
yu X1 you X2? have X2 with X1
where subscripts denote placeholders for sub-
phrases. Since these rules generalize over possi-
ble phrases, they act as discontinuous phrase pairs
and may also act as phrase-reordering rules. Hence,
they are not only considerably more powerful than
conventional phrase pairs, but they also integrate re-
ordering information into a consistent framework.
These hierarchical phrase pairs are formalized as
rewrite rules of a synchronous context-free grammar
(CFG) (Aho and Ullman, 1969):
X ?< ?,?,?> (21)
where X is a non-terminal, ? and ? are both strings
of terminals (words) and non-terminals , and ? is
a one-to-one correspondence between non-terminal
occurrences in ? and ?. Given the example above,
? ??yu X1 you X2?, ? ??have X2 with X1?, and?
is indicated by the subscript numbers.
Additionally, two glue rules are also defined:
S ?<S1X2 , S1X2> S ?<X1 , X1>
248
These give the model the option to build only par-
tial translations using hierarchical phrases, and then
combine them serially as in a phrase-based model.
The typical implementation of the hierarchical
translation model also takes the form of a log-linear
model. Let s? and t? be the source and target strings
generated by a derivation ? of the grammar. Then,
the conditional translation probability is given by:
Pr(s? | t?) ? ?1 ? |t?|+ ?2 ? |?|+ ?3 ?#g(?)+
?
r??
[?4 ? w(r)] (22)
where |?| denotes the total number of rules used
in ?, #g(?) returns the number of applications of
the glue rules, r ? ? are the rules in ?, and w(r)
is the weight of rule r. Weights ?1 and ?2 have
a similar interpretation as for phrase-based models,
they respectively give some control over the total
number of words and rules that conform the trans-
lation. Additionally, ?3 controls the model?s prefer-
ence for hierarchical phrases over serial combination
of phrases. Note that no distortion model is included
in the previous equation. Here, reordering is defined
at rule level by the one-to-one non-terminal corre-
spondence. In other words, reordering is a property
inherent to each rule and it is the individual score of
each rule what defines, at each step of the derivation,
the importance of reordering.
It should be noted that the IMT formalizations
presented in Section 2 can be applied to other hier-
archical or syntax-based SMT models such as those
described in (Zollmann and Venugopal, 2006; Shen
et al, 2010).
4 Search
In offline MT, the generation of the best trans-
lation for a given source sentence is carried out
by incrementally generating the target sentence2.
This process fits nicely into a dynamic program-
ming (DP) (Bellman, 1957) framework, as hypothe-
ses which are indistinguishable by the models can
be recombined. Since the DP search space grows
exponentially with the size of the input, standard DP
search is prohibitive, and search algorithms usually
resort to a beam-search heuristic (Jelinek, 1997).
2Phrase-based systems follow a left-to-right generation or-
der while hierarchical systems rely on a CYK-like order.
6
1
5
I saw a man with a telescope
2 3
4
I saw a man with a telescope
I saw with a telescope a man
Figure 2: Example of a hypergraph encoding two differ-
ent translations (one solid and one dotted) for the Spanish
sentence ?Vi a un hombre con un telescopio?.
Due to the demanding temporal constraints inher-
ent to any interactive environment, performing a full
search each time the user validates a new prefix is
unfeasible. The usual approach is to rely on a certain
representation of the search space that includes the
most probable translations of the source sentence.
The computational cost of this approach is much
lower, as the whole search for the translation must
be carried out only once, and the generated represen-
tation can be reused for further completion requests.
Next, we introduce hypergraphs, the formalism
chosen to represent the search space of both phrase-
based and hierarchical systems (Section 4.1). Then,
we describe the algorithms implemented to search
for suffix completions in them (Section 4.2).
4.1 Hypergraphs
A hypergraph is a generalization of the concept
of graph where the edges (now called hyperedges)
may connect several nodes (hypernodes) at the same
time. Formally, a hypergraph is a weighted acyclic
graph represented by a pair < V, E >, where V is a
set of hypernodes and E is a set of hyperedges. Each
hyperedge e ? E connects a head hypernode and a
set of tail hypernodes. The number of tail nodes is
called the arity of the hyperedge and the arity of a
hypergraph is the maximum arity of its hyperedges.
We can use hypergraphs to represent the deriva-
tions for a given CFG. Each hypernode represents
a partial translation generated during the decoding
process. Each ingoing hyperedge represents the rule
with which the corresponding non-terminal was sub-
stituted. Moreover, hypergraphs can represent a
whole set of possible translations. An example is
249
shown in Figure 2. Two alternative translations are
constructed from the leave nodes (1, 2 and 3) up to
the root node (6) of the hypergraph. Additionally,
hypernodes and hyperedges may be shared among
different derivations if they represent the same in-
formation. Thus, we can achieve a compact repre-
sentation of the translation space that allows us to
derive efficient search algorithms.
Note that word-graphs (Ueffing et al, 2002),
which are used to represent the search space for
phrase-based models, are a special case of hyper-
graphs in which the maximum arity is one. Thus,
hypergraphs allow us to represent both phrase-based
and hierarchical systems in a unified framework.
4.2 Suffix Search on Hypergraphs
Now, we describe a unified search process to obtain
the suffix ts that completes a prefix p given by the
user according to the two IMT formulations (Equa-
tion (11) and Equation (17)) described in Section 2.
Given an hypergraph, certain hypernodes define a
possible solution to the maximization defined in the
two IMT formulations. Specifically, only those hy-
pernodes that generate a prefix of a potential trans-
lation are to be taken into account3. The prob-
ability of the solution defined by each hypernode
has two components, namely the probability of the
SMT model (given by the language and translation
models) and the probability of the error-correction
model. On the one hand, the SMT model probabil-
ity is given by the translation of maximum probabil-
ity through the hypernode. On the other hand, the
error-correction probability is computed between p
and the partial translation of maximum probability
actually covered by the hypernode. Among all the
solutions defined by the hypernodes, we finally se-
lect that of maximum probability.
Once the best-scoring hypernode is identified, the
rest of the translation not covered by it is returned as
the suffix completion required in IMT.
5 Experimental Framework
The models and search procedure introduced in the
previous sections were assessed through a series of
3For example, in Figure 2 the hypernodes that generate pre-
fixes are those labeled with numbers 1 (?I saw?), 4 (?I saw with
a telescope) and 6 (?I saw a man with a telescope? and ?I saw
with a telescope a man?).
EU (Es/En)
Train Development Test
Sentences 214K 400 800
Token 5.9M / 5.2M 12K / 10K 23K / 20K
Vocabulary 97K / 84K 3K / 3K 5K / 4K
TED (Zh/En)
Train Development Test
Sentences 107K 934 1664
Token 2M / 2M 22K / 20K 33K / 32K
Vocabulary 42K / 52K 4K / 3K 4K / 4K
Table 1: Main figures of the processed EU and TED cor-
pora. K and M stand for thousands and millions of ele-
ments respectively.
IMT experiments with different corpora. These cor-
pora, the experimental methodology, and the evalu-
ation measures are presented in this section.
5.1 EU and TED corpora
We tested the proposed methods in two different
translation tasks each one involving a different lan-
guage pair: Spanish-to-English (Es?En) for the EU
(Bulletin of the European Union) task, and Chinese-
to-English (Zh?En) for the TED (TED4 talks) task.
The EU corpora were extracted from the Bul-
letin of the European Union, which exists in all of-
ficial languages of the European Union and is pub-
licly available on the Internet. Particularly, the cho-
sen Es?En corpus was part of the evaluation of the
TransType2 project (Barrachina et al, 2009). The
TED talks is a collection of recordings of public
speeches covering a variety of topics, and for which
high quality transcriptions and translations into sev-
eral languages are available. The Zh?En corpus
used in the experiments was part of the MT track
in the 2011 evaluation campaign of the workshop on
spoken language translation (Federico et al, 2011).
Specifically, we used the dev2010 partition for de-
velopment and the test2010 partition for test.
We process the Spanish and English parts of the
EU corpus to separate words and punctuation marks
keeping sentences truecase. Regarding the TED cor-
pus, we tokenized and lowercased the English part
(Chinese has no case information), and split Chi-
nese sentences into words with the Stanford word
4www.ted.com
250
segmenter (Tseng et al, 2005). Table 1 shows the
main figures of the processed EU and TED corpora.
5.2 Model Estimation and User Simulation
We used the standard configuration of the Moses
toolkit (Koehn et al, 2007) to estimate one phrase-
based and one hierarchical model for each cor-
pus; log-linear weights were optimized by minimum
error-rate training (Och, 2003) with the development
partitions. Then, the optimized models were used to
generate the word-graphs and hypergraphs with the
translations of the development and test partitions.
A direct evaluation of the proposed IMT proce-
dures involving human users would have been slow
and expensive. Thus, following previous works in
the literature (Barrachina et al, 2009; Gonza?lez-
Rubio et al, 2010), we used the references in the
corpora to simulate the translations that a human
user would want to obtain. Each time the system
suggested a new translation, it was compared to
the reference and the longest common prefix (LCP)
was obtained. Then, the first non-matching charac-
ter was replaced by the corresponding character in
the reference and a new system suggestion was pro-
duced. This process is iterated until a full match with
the reference was obtained.
Finally, we used this user simulation to optimize
the value for the probability of edition pe in the
error-correction model (Section 3.1), and for the log-
linear weights in the proposed IMT formulations. In
this case, these values were chosen so that they min-
imize the estimated user effort required to interac-
tively translate the development partitions.
5.3 Evaluation Measures
Different measures have been adopted to evaluate
the proposed IMT approach. On the one hand, dif-
ferent IMT systems can be compared according to
the effort needed by a human user to generate the de-
sired translations. This effort is usually estimated as
the number of actions performed by the user while
interacting with the system. In the user simulation
described above these actions are: looking for the
next error and moving the mouse pointer to that po-
sition (LCP computation), and correcting errors with
some key strokes. Hence, we implement the follow-
ing IMT effort measure (Barrachina et al, 2009):
Key-stroke and mouse-action ratio (KSMR):
number of key strokes plus mouse movements per-
formed by the user, divided by the total number of
characters in the reference.
On the other hand, we also want to compare the
proposed IMT approach against a conventional CAT
approach without interactivity, such as a decoupled
post-edition system. For such systems, character-
level user effort is usually measured by the Charac-
ter Error Rate (CER). However, it is clear that CER
is at a disadvantage due to the auto-completion ap-
proach of IMT. To perform a fairer comparison be-
tween post-edition and IMT, we implement a post-
editing system with autocompletion. Here, when
the user enters a character to correct some incor-
rect word, the system automatically completes the
word with the most probable word in the task vo-
cabulary. To evaluate the effort of a user using such
a system, we implement the following measure pro-
posed in (Romero et al, 2010):
Post-editing key stroke ratio (PKSR): using
a post-edition system with word-autocompleting,
number of user key strokes divided by the total num-
ber of reference characters.
The counterpart of PKSR in an IMT scenario
is (Barrachina et al, 2009):
Key-stroke ratio (KSR): number of key strokes,
divided by the number of reference characters.
PKSR and KSR are fairly comparable and the rel-
ative difference between them gives us a good es-
timate of the reduction in human effort that can be
achieved by using IMT instead of a conventional
post-edition system.
We also evaluate the quality of the automatic
translations generated by the MT models with the
widespread BLEU score (Papineni et al, 2002).
Finally, we provide both confidence intervals for
the results and statistical significance of the ob-
served differences in performance. Confidence in-
tervals were computed by pair-wise re-sampling as
in (Zhang and Vogel, 2004) while statistical signifi-
cance was computed using the Tukey?s HSD (honest
significance difference) test (Hsu, 1996).
251
EU TED
WG HG WG HG
1-best BLEU [%] 45.0 45.1 11.0 11.2
1000-best Avg. BLEU [%] 43.6 44.2 10.2 11.0
Table 2: BLEU score of the word-graphs (WG) and hy-
pergraphs (HG) used to implement the IMT procedures.
IMT EU TED
Setup PB HT PB HT
ISF 27.4?.5? 26.5?.5? 53.0?.4? 52.3?.4?
CSF 26.6?.5? 25.1?.5F 52.2?.4? 50.8?.4F
Table 3: IMT results (KSMR [%]) for the EU and
TED tasks using the independent suffix formalization
(ISF) and the conditioned suffix formalization (CSF). PB
stands for phrase-based model and HT stands for hierar-
chical translation model. For each task, the best result
is displayed boldface, an asterisk ? denotes a statistically
significant better result (99% confidence) with respect to
ISF with PB, and a star F denotes a statistically signifi-
cant difference with respect to all the other systems.
6 Results
We start by reporting conventional MT quality re-
sults to test if the generated word-graphs and hyper-
graphs encode translations of similar quality. Ta-
ble 2 displays the quality (BLEU (Papineni et al,
2002)) of the automatic translations generated for
the test partitions. The lower 1-best BLEU results
obtained for TED show that this is a much more dif-
ficult task than EU. Additionally, the similar aver-
age BLEU results obtained for the 1000-best transla-
tions indicate that word-graphs and hypergraphs en-
code translations of similar quality. Thus, the IMT
systems that use these word-graphs and hypergraphs
can be compared in a fair way.
Then, we evaluated different setups of the pro-
posed IMT approach. Table 3 displays the IMT re-
sults obtained for the EU and TED tasks. We report
KSMR (as a percentage) for the independent suffix
formalization (ISF) and the conditioned suffix for-
malization (CSF) using both phase-based (PB) and
hierarchical (HT) translation models. The KSMR
result of ISF using a phrase-based model can be con-
sidered our baseline since this setup is comparable
to that used in (Barrachina et al, 2009). Results for
HT consistently outperformed the corresponding re-
sults for PB. Similarly, results for CSF were con-
EU TED
PE IMT PE IMT
PKSR [%] KSR [%] PKSR [%] KSR [%]
27.1 14.1 (48%) 40.8 29.7 (27.2%)
Table 4: Estimation of the effort required to translate
the test partition of the EU and TED tasks using post-
editing with word-completion (PE) and IMT under the
independent suffix formalization (IMT). We used hierar-
chical MT in both approaches. In parenthesis we display
the estimated effort reduction of IMT with respect to PE.
sistently better than those for ISF. More specifically,
no statistically significant difference were found be-
tween ISF with HT and CSF with PB but both sta-
tistically outperformed the baseline (ISF with PB).
Finally, CSF with HT statistically outperformed the
other three configurations reducing KSMR by ?2.2
points with respect to the baseline. We hypothe-
size that the better results of HT can be explained
by its more efficient representation of word reorder-
ing. Regarding the CSF, its better results are due to
the better suffixes that can be obtained by taking into
account the actual prefix validated by the user.
Finally, we compared the estimated human effort
required to translate the test partitions of the EU and
TED corpora with the best IMT configuration (inde-
pendent suffix formalization with hierarchical trans-
lation model) and a conventional post-editing (PE)
CAT system with word-completion. That is, when
the user corrects a character, the PE system auto-
matically proposes a different word that begins with
the given word prefix but, obviously, the rest of the
sentence is not changed. According to the results,
the estimated human effort to generate the error-free
translations was significantly reduced with respect
to using the conventional PE approach. IMT can
save about 48% of the overall eastimated effort for
the EU task and about 27% for the TED task.
7 Summary and Future Work
We have proposed a new IMT approach that uses hi-
erarchical SMT models as its underlying translation
technology. This approach is based on a statistical
formalization previously described in the literature
that includes stochastic error correction. Addition-
ally, we have proposed a refined formalization that
improves the quality of the IMT suffixes by taking
252
into account the prefix validated by the user. More-
over, since word-graphs constitute a particular case
of hypergraphs, we are able to manage both phrase-
based and hierarchical translation models in a uni-
fied IMT framework.
Simulated results on two different translation
tasks showed that hierarchical translation models
outperform phrase-based models in our IMT frame-
work. Additionally, the proposed alternative IMT
formalization also allows to improve the results of
the IMT formalization previously described in the
literature. Finally, the proposed IMT system with
hierarchical SMT models largely reduces the esti-
mated user effort required to generate correct trans-
lations in comparison to that of a conventional post-
edition system. We look forward to corroborating
these result in test with human translators.
There are many ways to build on the work de-
scribed here. In the near future, we plan to explore
the following research directions:
? Alternative IMT scenarios where the user is not
bounded to correct translation errors in a left-
to-right fashion. In such scenarios, the user will
be allowed to correct errors at any position in
the translation while the IMT system will be
required to derive translations compatible with
these isolated corrections.
? Adaptive translation engines that take advan-
tage of the user?s corrections to improve its sta-
tistical models. As the translator works and
corrects the proposed translations, the transla-
tion engine will be able to make better predic-
tions. One of the first works on this topic was
proposed in (Nepveu et al, 2004). More re-
cently, Ortiz-Mart??nez et al (2010) described a
set of techniques to obtain an incrementally up-
dateable IMT system, solving technical prob-
lems encountered in previous works.
? More sophisticated measures to estimate the
human effort. Specifically, measures that esti-
mate the cognitive load involve in reading, un-
derstanding and detecting an error in a trans-
lation (Foster et al, 2002), in contrast KSMR
simply considers a constant cost. This will lead
to a more accurate estimation of the improve-
ments that may be expected by a human user.
Acknowledgments
Work supported by the European Union 7th Frame-
work Program (FP7/2007-2013) under the Cas-
MaCat project (grans agreement no 287576), by
Spanish MICINN under grant TIN2012-31723, and
by the Generalitat Valenciana under grant ALMPR
(Prometeo/2009/014).
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syn-
tax directed translations and the pushdown assembler.
Journal of Computer and Systems Science, 3(1):37?
56, February.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi, An-
tonio Lagarda, Hermann Ney, Jesu?s Toma?s, Enrique
Vidal, and Juan-Miguel Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. Computa-
tional Linguistics, 35:3?28, March.
Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ, USA, 1 edi-
tion.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19:263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
M. Federico, L. Bentivogli, M. Paul, and S. Stu?ker. 2011.
Overview of the iwslt 2011 evaluation campaign. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 11?20.
George Foster, Pierre Isabelle, and Pierre Plamondon.
1998. Target-text mediated interactive machine trans-
lation. Machine Translation, 12(1/2):175?194, Jan-
uary.
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In Proceedings of the 2002 conference on Empirical
methods in natural language processing - Volume 10,
pages 148?155.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and Fran-
cisco Casacuberta. 2010. Balancing user effort and
translation error in interactive machine translation via
confidence measures. In Proceedings of the ACL 2010
Conference Short Papers, pages 173?177.
Jason Hsu. 1996. Multiple Comparisons: Theory and
Methods. Chapman and Hall/CRC.
253
Pierre Isabelle and Ken Church. 1998. Special issue on:
New tools for human translators, volume 12. Kluwer
Academic Publishers, January.
Frederick Jelinek. 1997. Statistical methods for speech
recognition. MIT Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, demonstration session, June.
Philippe Langlais and Guy Lapalme. 2002. TransType:
development-evaluation cycles to boost translator?s
productivity. Machine Translation, 17(2):77?98,
September.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710, February.
Laurent Nepveu, Guy Lapalme, Philippe Langlais, and
George Foster. 2004. Adaptive language and trans-
lation models for interactive machine translation. In
Proceedings of the conference on Empirical Methods
on Natural Language Processing, pages 190?197.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295?302.
Franz Josef Och, Richard Zens, and Hermann Ney.
2003. Efficient search for interactive statistical ma-
chine translation. In Proceedings of the European
chapter of the Association for Computational Linguis-
tics, pages 387?393.
Franz Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, pages 160?167. Association for
Computational Linguistics.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for inter-
active statistical machine translation. In Proceedings
of the 2010 Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 546?554.
Daniel Ortiz-Mart??nez. 2011. Advances in Fully-
Automatic and Interactive Phrase-Based Statistical
Machine Translation. Ph.D. thesis, Universitat
Polite`cnica de Vale`ncia. Advisors: Ismael Garc??a
Varea and Francisco Casacuberta.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318. Associa-
tion for Computational Linguistics.
Veronica Romero, Alejandro H. Toselli, and Enrique Vi-
dal. 2010. Character-level interaction in computer-
assisted transcription of text images. In Proceedings
of the 12th International Conference on Frontiers in
Handwriting Recognition, pages 539?544.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine translation.
Computational Linguistics, 36(4):649?671, Decem-
ber.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing.
Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002.
Generation of word graphs in statistical machine trans-
lation. In Proceedings of the conference on Empirical
Methods in Natural Language Processing, pages 156?
163.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proceedings of The international Confer-
ence on Theoretical and Methodological Issues in Ma-
chine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141.
254
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 653?656,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Enlarged Search Space for SITG Parsing
Guillem Gasc?, Joan-Andreu S?nchez, Jos?-Miguel Bened?
Institut Tecnol?gic d?Inform?tica, Universitat Polit?cnica de Val?ncia
Cam? de Vera s/n, Val?ncia, 46022, Spain
ggasco@iti.upv.es , {jandreu,jbenedi}@dsic.upv.es
Abstract
Stochastic Inversion Transduction Grammars
constitute a powerful formalism in Machine
Translation for which an efficient Dynamic
Programming parsing algorithm exists. In this
work, we review this parsing algorithm and
propose important modifications that enlarge
the search space. These modifications allow
the parsing algorithm to search for more and
better solutions.
1 Introduction
Syntax Machine Translation has received great at-
tention in the last few years, especially for pairs of
languages that are sufficiently non-monotonic. Sev-
eral works have explored the use of syntax for Ma-
chine Translation (Wu, 1997; Chiang, 2007). In
(Wu, 1997), Stochastic Inverse Transduction Gram-
mars (SITGs) were introduced for describing struc-
turally correlated pairs of languages. SITGs can be
used to simultaneously analyze two strings from dif-
ferent languages and to correlate them. An efficient
Dynamic Programming parsing algorithm for SITGs
was presented in (Wu, 1997). This algorithm is sim-
ilar to the CKY algorithm for Probabilistic Context
Free Grammars. The parsing algorithm does not al-
low the association of two items that have the empty
string in one of their sides. This limitation restricts
the search space and prevents the algorithm from ex-
ploring some valid parse trees.
In this paper, we review Wu?s parsing algorithm
for SITGs (referred to as the original algorithm) and
propose some modifications to increase the search
space in order to make it possible to find these valid
parse trees.
2 SITG Parsing
SITGs (Wu, 1997) can be viewed as a restricted
subset of Stochastic Syntax-Directed Transduction
Grammars (Maryanski and Thomason, 1979). For-
mally, a SITG in Chomsky Normal Form can be
defined as a set of lexical rules that are noted as
A ? x/?, A ? ?/y, A ? x/y; direct syntac-
tic rules that are noted as A ? [BC]; and inverse
syntactic rules that are noted as A ? ?BC?, where
A,B,C are non-terminal symbols, x, y are terminal
symbols, ? is the empty string, and each rule has a
probability value p attached. The sum of the proba-
bilities of the rules with the same non-terminal in the
left side must be equal to 1. When a direct syntactic
rule is used in parsing, both strings are parsed with
the syntactic rule A ? BC . When an inverse rule is
used in parsing, one string is parsed with the syntac-
tic rule A ? BC , and the other string is parsed with
the syntactic rule A ? CB.
An efficient Viterbi-like parsing algorithm that is
based on a Dynamic Programming Scheme was pro-
posed in (Wu, 1997). It allows us to obtain the most
probable parse tree that simultaneously analyzes two
strings, X = x1...x|X| and Y = y1...x|Y |, i.e. the
bilingual string X/Y . It has a time complexity of
O(|X|3|Y |3|R|), where |R| is the number of rules
of the grammar.
The parsing algorithm is based on the definition
of:
?ijkl(A) = P?r(A ?? xi+1 ? ? ? xj/yk+1 ? ? ? yl)
as the maximum probability of any parsing tree that
simultaneously generates the substrings xi+1 ? ? ? xj
and yk+1 ? ? ? yl from the non-terminal symbol A .
In (Wu, 1997), the parsing algorithm was defined
as follows:
653
1. Initialization
?i?1,i,k?1,k(A) = p(A ? xi/yk)
1 ? i ? |X|, 1 ? k ? |Y |,
?i?1,i,k,k(A) = p(A ? xi/?)
1 ? i ? |X|, 0 ? k ? |Y |,
?i,i,k?1,k(A) = p(A ? ?/yk)
0 ? i ? |X|, 1 ? k ? |Y |,
2. Recursion
For all A ? N and
i, j, k, l such that
?
?
?
0 ? i < j ? |X|,
0 ? k < l ? |Y |,
j ? i + l ? k > 2,
(1)
?ijkl(A) = max(?[]ijkl(A), ?
??
ijkl(A))
where
?[]ijkl(A)
= max
B,C?N
i?I?j,k?K?l
(I?i)(j?I)+(K?k)(l?K)>0
p(A ? [BC])?iIkK(B)?IjKl(C) (2)
???ijkl(A)
= max
B,C?N
i?I?j,k?K?l
(I?i)(j?I)+(K?k)(l?K)>0
p(A ? ?BC?)?iIKl(B)?IjkK(C) (3)
This algorithm cannot provide the correct parsing
tree in some situations. For example, consider the
SITG shown in Fig. 1. If the input pair is a/b,
p S ? [SS] p S ? ?SS?
q S ? ?/b q S ? a/?
1? 2p? 2q S ? a/b
Figure 1: Example SITG.
this SITG provides the parse tree (a) that is shown in
Fig. 2 with probability 1 ? 2p ? 2q. However, the
parse tree (b) is more likely if 1 ? 2p ? 2q < 2pq.
The above parsing algorithm is not able to obtain
this parse tree due to the restriction j? i+ l?k > 2
in (1). This restriction does not allow the algo-
rithm to consider two subproblems in which each
substring has length 1 which have not been previ-
ously considered in the initialization step. Chang-
ing this restriction to j ? i + l ? k ? 2 is not
enough to tackle this situation since the restriction
(b)(a)
SS
SS
a/b
a/? ?/b
Figure 2: Parse tree (a) can be obtained with Wu?s algo-
rithm for a/b, but parse tree (b) cannot be obtained.
(I?i)(j?I)+(K?k)(l?K) 6= 0 in expression (2)
is not accomplished given that I = i or I = j, and
K = k or l = K (similarly in expression (3)).
From now on, we will use the term non-explored
trees to denote the set of trees that are possible when
rules of the grammar are applied but cannot be ex-
plored with Wu?s algorithm. In fact, this situation
appears for other paired strings (see Fig. 3) in which
a string in one side is associated with the empty
string in the other side through rules that are not lexi-
cal rules. For example, in Fig. 3b, substring aa could
be associated with ?. However,this parse tree cannot
be obtained with the algorithm due to the search re-
strictions described above.
(b)(a)
SS
S
S
S
SS
S
a/ba/?
a/?a/?
?/b
Figure 3: Parse tree (a) can be obtained with Wu?s algo-
rithm for aa/b, but parse tree (b) would be more probable
if pq2 > 1? 2p? 2q.
The changes needed in the algorithm to be able to
find the sort of parsing trees described above are the
following:
? Changing restriction j ? i+ l? k > 2 in (1) to
j? i+ l?k ? 2. Note that this new restriction
is redundant and could be removed.
? Changing restriction (I? i)(j?I)+(K?k)(l?
K) 6= 0 to ((j?I)+(l?K))?((I?i)+(K?k)) 6=
0 in (2) and to ((j ? I) + (K ? k)) ? ((I ? i) +
(l ? K)) 6= 0 in (3) in order to guarantee the
algorithm?s termination.
3 Search under SITG Constraints
The modifications that have been introduced in Sec-
tion 2 enlarge the search space and allow the parsing
654
algorithm to explore a greater number of possible so-
lutions. We illustrate this situation with an example.
Consider the SITG introduced in Figure 1. Fig. 4
shows the possible complete matched trees for the
input pair a/b that are considered in the search pro-
cess with the modifications introduced.
(b)(a) (c) (d) (e)
S
S
SS
S
SS
S
S
S
S
S
Sa/b
aaaa
bbbb
?
?
?
??
??
?
Figure 4: Parse trees for input pair a/b that are taken into
account in the search process with the modifications.
Without these modifications, the parsing algo-
rithm only takes into account tree (a) of Fig. 4. For
this grammar, we have computed the growth in num-
ber of complete matched trees. Table 1 shows how
the search space grows notably with the modifica-
tions introduced.
n Wu?s alg. Modified alg. ratio
1 1 5 0.200
2 34 290 0.117
3 1,928 34,088 0.057
4 131,880 5,152,040 0.026
5 10,071,264 890,510,432 0.011
6 827,969,856 167,399,588,160 0.005
Table 1: Growth in number of explored trees for the orig-
inal and modified parsing algorithms (n is the length of
the input pair strings and the last column represents the
ratio between columns two and three).
As a preliminary experiment and in order to eval-
uate empirically the Wu?s parsing algorithm versus
the modified algorithm, we parsed first 100K sen-
tence of German-English Europarl corpus. The lex-
ical rules in the Bracketing SITG used for pars-
ing were obtained from a probabilistic dictionary
by aligning with IBM3 model (NULL aligments
were also included). In this experiment, the modi-
fied algorithm obtained a more probable parse tree
for 6% of the sentences. If we added brackets to
the sentences separately with monolingual parsers,
we could use a parsing algorithm similar to the al-
gorithm that is described in (S?nchez and Bened?,
2006). The monolingual brackets restricted the
parse tree to those that were compatible with the
brackets. In that case the modified algorithm ob-
tained a more probable parse tree for 14% of the
sentences.
4 Inside Probability
The parsing algorithm described above computes
the most likely parse tree for a given paired string
X/Y . However, in some cases (Wu, 1995; Huang
and Zhou, 2009), we need the inside probability
(?0,|X|,0,|Y |(S)), i.e., the probability that the gram-
mar assigns to the whole set of parse trees that yield
X/Y . If the maximizations are replaced by sums,
the algorithm can be used to compute the inside
probability. However, as stated above, the origi-
nal algorithm cannot find the whole set of trees for
a given paired string in some cases. These non-
explored trees have a probability greater than 0.
As an example, we computed the amount of prob-
ability lost in the inside computation using the origi-
nal algorithm with the grammar shown in Fig. 1. Let
? be the amount of probability of the non-explored
trees (the lost probability). It must be noted that
since height 1 trees are all reachable, we must accu-
mulate lost probability for trees of height 2 or more.
Hence, let ? be the amount of lost probability for
trees of height 2 or more. Note that all such trees
must have initially used the production S ? SS in-
versely or directly. Thus, ? = 2p ? ?. Fig. 5 shows
the kinds of non-explored trees. Then ? is:
? = 4?q2+2?2p?(1?2p)??+(2p)2 ?(2?(1??)+?2)
The first addend is the probability of the non-
explored trees of height 2 (Fig. 5a). The second ad-
dend is the probability that one of the subtrees uses
a syntactic production, this new subtree produces
a non-explored tree (2p ? ?) and the other subtree
(a) (b) (c)
S
S
SS
S
SS
S
S
Figure 5: Partial representation of non-explored parse
trees from the non-terminal string SS introduced after
the first derivation step: (a) both non-terminals yield a
terminal in one side and the empty string in the other;
(b) one of the non-terminals uses a lexical production
and the other non-terminal yields a non-explored tree; (c)
both non-terminals use a syntactic production and one (or
both) yields a non-explored tree.
655
Figure 6: Amount of lost probability for values of p and q.
rewrites itself using a lexical production (1 ? 2p).
Note that the non-explored tree can be yielded from
either the left or the right non-terminal, (Fig. 5b).
The third addend is the probability that both non-
terminals use a syntactic production (2p)2 and ei-
ther one (2(?)(1??)) or both (?2) subtrees are non-
explored trees (Fig. 5c). If we isolate ?, we get
? = 2p ? 1? 4p ?
?
16p2 ? 8p + 1 + 64p2q2
4p2
Since the solution with the positive square root
takes values greater than 1, we can discard it.
Fig. 6 shows the probability accumulated in the
non-explored trees for values of p and q between
0 and 0.25 (higher values of p produce inconsistent
SITGs). That is the amount of probability lost in the
inside parsing for the whole language generated by
the grammar shown in Fig. 1.
In order to prove the loss of probability produced
by the original algorithm, we use the grammar in
Fig. 1 with p = q = 0.2. We parse all the paired
strings X/Y such that |X| + |Y | ? l, where l is a
fixed maximum length. We repeat the same exper-
iment using the modified algorithm. Fig. 7 shows
the accumulated inside probabilities for both origi-
nal and modified algorithms and the theoretical max-
imums (1?? for the original algorithm and 1 for the
modified algorithm). Note that the computed results
approach the theoretical maximums and the modi-
fied algorithm covers the whole search space.
5 Conclusions
SITGs have proven to be a powerful tool in Syntax
Machine Translation. However, the algorithms have
been proposed do not explore all the possible parse
trees. This work proposes modifications of the algo-
rithms to be able to explore the whole search space.
Figure 7: Accumulated inside probability for the original
and modified algorithms.
Using an example, we have shown that the modifi-
cations allow a complete search. As future work, we
plan to proove the correctness of the modified algo-
rithm and to study the impact of these modifications
on the use of SITGs for Machine Translation, and
the estimation of SITGs.
Acknowledgments
Work supported by the EC (FSE), the Spanish Gov-
ernment (MICINN, "Plan E") under grants MIPRCV
"Consolider Ingenio 2010" CSD2007-00018, iTrans2
TIN2009-14511 and the Generalitat Valenciana grant
Prometeo/2009/014 and BFPI/2007/117.
References
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
S. Huang and B. Zhou. 2009. An em algorithm for scfg
in formal syntax-based translation. In ICASSP, pages
4813?4816, Taiwan, China, April.
F.J. Maryanski and M.T. Thomason. 1979. Properties of
stochastic syntax-directed tranlation schemata. Jour-
nal of Computer and Information Sciences, 8(2):89?
110.
J.A. S?nchez and J.M. Bened?. 2006. Stochastic in-
version transduction grammars for obtaining word
phrases for phrase-based statistical machine transla-
tion. In Proc. of Workshop on Statistical Machine
Translation. HLT-NAACL 06, pages 130?133.
D. Wu. 1995. Trainable coarse bilingual grammars for
parallel text bracketing. In Proceedings of the Third
Annual Workshop on Very Large Corpora, pages 69?
81.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
656
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 37?40,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interactive Predictive Parsing using a Web-based Architecture?
Ricardo Sa?nchez-Sa?ez? Luis A. Leiva? Joan-Andreu Sa?nchez? Jose?-Miguel Bened???
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{rsanchez,luileito,jandreu,jbenedi}@{?dsic,?iti}.upv.es
Abstract
This paper introduces a Web-based demon-
stration of an interactive-predictive framework
for syntactic tree annotation, where the user is
tightly integrated into the interactive parsing
system. In contrast with the traditional post-
editing approach, both the user and the sys-
tem cooperate to generate error-free annotated
trees. User feedback is provided by means of
natural mouse gestures and keyboard strokes.
1 Introduction
There is a whole family of problems within the pars-
ing world where error-free results, in the form of
perfectly annotated trees, are needed. Constructing
error-free trees is a necessity in many tasks, such
as handwritten mathematical expression recognition
(Yamamoto et al, 2006), or new gold standard tree-
bank creation (de la Clergerie et al, 2008). It is
a fact that current state-of-the-art syntactic parsers
provide trees that, although of excellent quality, still
contain errors. Because of this, the figure of a human
corrector who supervises the annotation process is
unavoidable in this kind of problems.
When using automatic parsers as a baseline for
building perfect syntactic trees, the role of the hu-
man annotator is to post-edit the trees and correct the
errors. This manner of operating results in the typ-
ical two-step process for error correcting, in which
the system first generates the whole output and then
?Work partially supported by the Spanish MICINN under
the MIPRCV ?Consolider Ingenio 2010? (CSD2007-00018),
MITTRAL (TIN2009-14633-C03-01), Prometeo (PROME-
TEO/2009/014) research projects, and the FPU fellowship
AP2006-01363. The authors wish to thank Vicent Alabau for
his invaluable help with the CAT-API library.
the user verifies or amends it. This paradigm is
rather inefficient and uncomfortable for the human
annotator. For example, in the creation of the Penn
Treebank annotated corpus, a basic two-stage setup
was employed: a rudimentary parsing system pro-
vided a skeletal syntactic representation, which then
was manually corrected by human annotators (Mar-
cus et al, 1994). Other tree annotating tools within
the two-step paradigm exist, such as the TreeBanker
(Carter, 1997) or the Tree Editor TrEd1.
With the objective of reducing the user effort and
making this laborious task easier, we devised an In-
teractive Predictive framework. Our aim is to put
the user into the loop, embedding him as a part of
the automatic parser, and allowing him to interact in
real time within the system. In this manner, the sys-
tem can use the readily available user feedback to
make predictions about the parts that have not been
validated by the corrector.
In this paper, we present a Web-based demo,
which implements the Interactive Predictive Parsing
(IPP) framework presented in (Sa?nchez-Sa?ez et al,
2009). User feedback (provided by means of key-
board and mouse operations) allows the system to
predict new subtrees for unvalidated parts of the an-
notated sentence, which in turn reduces the human
effort and improves annotation efficiency.
As a back-end for our demo, we use a more pol-
ished version of the CAT-API library, the Web-based
Computer Assisted Tool introduced in (Alabau et al,
2009). This library allows for a clean application de-
sign, in which both the server side (the parsing en-
gine) and the client side (which draws the trees, cap-
tures and interprets the user feedback, and requests
1http://ufal.mff.cuni.cz/?pajas/tred/
37
(a) System: output tree 1 (b) User: span modification (c) System: output tree 2
Figure 1: An interaction example on the IPP system.
parsed subtrees to the server) are independent. One
of the features that steam from the CAT-API library
is the ability for several annotators to work concur-
rently on the same problem-set, each in a different
client computer sharing the same parsing server.
Interactive predictive methods have been success-
fully demonstrated to ease the work of transcrip-
tors and translators in fields like Handwriting Text
Recognition (Romero et al, 2009; Toselli et al,
2008) and Statistical Machine Translation (Ortiz et
al., 2010; Vidal et al, 2006). This new paradigm
enables the collaboration between annotators across
the globe, granting them a physical and geographical
freedom that was inconceivable in the past.
2 Interactive Predictive Parsing
A tree t, associated to a string x1|x|, is composed
by substructures that are usually referred as con-
stituents. A constituent cAij is defined by the non-
terminal symbol A (either a syntactic label or a POS
tag) and its span ij (the starting and ending indexes
which delimit the part of the input sentence encom-
passed by the constituent).
Here follows a general formulation for the non-
interactive parsing scenario. Using a grammatical
model G, the parser analyzes the input sentence x =
{x1, . . . , x|x|} and produces the parse tree t?
t? = arg max
t?T
pG(t|x), (1)
where pG(t|x) is the probability of parse tree t given
the input string x using model G, and T is the set of
all possible parse trees for x.
In the interactive predictive scenario, after obtain-
ing the (probably incorrect) best tree t?, the user is
able to individually correct any of its constituents
cAij . The system reacts to each of the corrections in-
troduced by the human, proposing a new t?? that takes
into account the afore-mentioned corrections.
The action of modifying an incorrect constituent
(either setting the correct span or the correct label)
implicitly validates a subtree that is composed by
the partially corrected constituent, all of its ancestor
constituents, and all constituents whose end span is
lower than the start span of the corrected constituent.
We will name this subtree the validated prefix tree
tp. When the user replaces the constituent cAij with
the correct one c?Aij , the validated prefix tree is:
tp(c?Aij ) = {cBmn : m ? i, n ? j,
d(cBmn) ? d(c?Aij )} ?
{cDpq : p >= 1 , q < i}
(2)
with d(cBmn) being the depth of constituent cBmn.
When a constituent correction is performed, the
prefix tree tp(c?Aij ) is fixed and a new tree t?? that takes
into account the prefix is proposed
t?? = arg max
t?T
pG(t|x, tp(c?Aij )). (3)
Given that we are working with context-free
grammars, the only subtree that effectively needs to
be recalculated is the one starting from the parent of
the corrected constituent.
3 Demo outline
A preview version of the demonstration can be ac-
cessed at http://cat.iti.upv.es/ipp/.
The user is presented with the sentences in the se-
lected corpus, and starts parsing them one by one.
They make corrections in the trees both with the key-
board and the computer mouse. The user feedback
38
is decoded on the client side which in turn requests
subtrees to the parse engine.
Two kind of operations can be performed over
constituents: span modification (performed either by
dragging a line from the constituent to the word that
corresponds to the span?s upper index, or deleting
a tree branch by clicking on it), and label substitu-
tion (done by typing the correct one on its text field).
Modifying the span of a constituent invalidates its
label, so the server recalculates it as part of the suf-
fix. Modifying the label of a constituent validates its
span.
When the user is about to perform an opera-
tion, the affected constituent and the prefix that will
be validated are highlighted. The target span of
the modified constituent is visually shown as well.
When the user obtains the correctly annotated tree,
they can accept it by by clicking on a new sentence.
As already mentioned, the user is tightly inte-
grated into the interactive parsing process. They fol-
low a predetermined protocol in which they correct
and/or validate the annotated parse trees:
1. The parsing server proposes a full parse tree t
for the input sentence. The tree t is shown to
the user by the client (Fig. 1a).
2. The user finds the first2 incorrect constituent c
and starts amending it, either by changing its
label or changing its span (Fig. 1b, note how
the label is greyed out as it is discarded with
the span modification). This operation implic-
itly validates the prefix tree tp (highlighted in
green).
3. The system decodes the user feedback (i.e.,
mouse gestures or keyboard strokes) which can
either affect the label or the span of the incor-
rect constituent c:
(a) If the span of c is modified, the label is
not assumed to be correct. A partial con-
stituent c?, which includes span but no la-
bel, is decoded from the user feedback.
(b) If the label of c is modified, the span is
assumed to be correct. The corrected con-
stituent c? is decoded from the user feed-
back.
2The tree visiting order is left-to-right depth-first.
This step only deals with analysing the user
feedback, the parsing server will not be con-
tacted until the next step.
4. Either the partially corrected constituent c? or
the corrected constituent c? is then used by the
client to create a new extended consolidated
prefix that combines the validated prefix and the
user feedback: either tpc? or tpc?. The client
sends the extended prefix tree to the parsing
server and requests a suitable continuation for
the parse tree, or tree suffix ts:
(a) If the extended prefix is partial (tpc?), the
first element of ts is the label completing
c?, followed by the remaining calculated
whole constituents.
(b) If the extended prefix is complete (tpc?),
the parsing server produces a suitable tree
suffix ts which contains the remaining cal-
culated whole constituents.
5. The client concatenates the suffix returned by
the server to the validated extended prefix, and
shows the whole tree to the client (Fig. 1c).
6. These previous steps are iterated until a final,
perfect parse tree is produced by the server and
validated by the user.
Note that within this protocol, constituents can be
deleted or inserted by adequately modifying the span
of the left-neighbouring constituent.
4 Demo architecture
The proposed system coordinates client-side script-
ing with server-side technologies, by using the CAT-
API library (Alabau et al, 2009).
4.1 Server side
The server side of our system is a parsing en-
gine based on a customized CYK-Viterbi parser,
which uses a Probabilistic Context-Free Grammar in
Chomsky Normal Form obtained from sections 2 to
21 of the UPenn Treebank as a model (see (Sa?nchez-
Sa?ez et al, 2009) for details).
The client can request to the parsing server the
best subtree for any given span of the input string.
For each requested subtree, the client can either pro-
vide the starting label or not. If the starting subtree
39
label is not provided, the server calculates the most
probable label. The server also performs transparent
tree debinarization/binarization when communicat-
ing with the client.
4.2 Client side
The client side has been designed taking into ac-
count ergonomic issues in order to facilitate the in-
teraction.
The prototype is accessed through a Web browser,
and the only requirement is the Flash plugin (98% of
market penetration) installed in the client machine.
The hardware requirements in the client are very
low on the client side, as the parsing is process per-
formed remotely on the server side: any computer
(including netbooks) capable of running a modern
Web browser is enough.
Each validated user interaction is saved as a log
file on the server side, so a tree?s annotation session
can be later resumed.
4.2.1 Communication protocol
This demo exploits the WWW to enable the con-
nection of simultaneous accesses across the globe.
This architecture also provides cross-platform com-
patibility and requires neither computational power
nor disk space on the client?s machine.
Client and server communicate via asynchronous
HTTP connections, providing thus a richer interac-
tive experience ? no page refreshes is required when
parsing a new sentence. Moreover, the Web client
communicates with the IPP engine through binary
TCP sockets. Thus, response times are quite slow ? a
desired requirement for the user?s solace. Addition-
ally, cross-domain requests are possible, so the user
could switch between different IPP engines within
the same UI.
5 Evaluation results
We have carried out experiments that simulate user
interaction using section 23 of the Penn Treebank.
The results suggest figures ranging from 42% to
46% of effort saving compared to manually post-
editing the trees without an interactive system. In
other words, for every 100 erroneous constituents
produced by a parsing system, an IPP user would
correct only 58 (the other 42 constituents being au-
tomatically recalculated by the IPP system). Again,
see (Sa?nchez-Sa?ez et al, 2009) for the details on ex-
perimentation.
5.1 Conclusions and future work
We have introduced a Web-based interactive-
predictive system that, by using a parse engine in
an integrated manner, aids the user in creating cor-
rectly annotated syntactic trees. Our system greatly
reduces the human effort required for this task com-
pared to using a non-interactive automatic system.
Future work includes improvements to the client
side (e.g., confidence measures as a visual aid, mul-
timodality), as well as exploring other kinds of pars-
ing algorithms for the server side (e.g., adaptative
parsing).
References
V. Alabau, D. Ortiz, V. Romero, and J. Ocampo. 2009. A
multimodal predictive-interactive application for com-
puter assisted transcription and translation. In ICMI-
MLMI ?09, 227?228.
D. Carter. 1997. The TreeBanker. A tool for supervised
training of parsed corpora. In ENVGRAM?97, 9?15.
E.V. de la Clergerie, O. Hamon, D. Mostefa, C. Ayache,
P. Paroubek, and A. Vilnat. 2008. Passage: from
French parser evaluation to large sized treebank. In
LREC?08, 100:P2.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313?330.
D. Ortiz, L. A. Leiva, V. Alabau, and F. Casacuberta.
2010. Interactive machine translation using a web-
based architecture. In IUI?10, 423?425.
V. Romero, L. A. Leiva, A. H. Toselli, and E. Vidal.
2009. Interactive multimodal transcription of text
imagse using a web-based demo system. In IUI?09,
477?478.
R. Sa?nchez-Sa?ez, J.A. Sa?nchez, and J.M. Bened??. 2009.
Interactive predictive parsing. In IWPT?09, 222?225.
A.H. Toselli, V. Romero, and E. Vidal. 2008. Computer
assisted transcription of text images and multimodal
interaction. In MLMI?08, 5237: 296?308.
E. Vidal, F. Casacuberta, L. Rodr??guez, J. Civera, and
C. Mart??nez. 2006. Computer-assisted translation us-
ing speech recognition. IEEE Trans. on Audio, Speech
and Language Processing, 14(3):941?951.
R. Yamamoto, S. Sako, T. Nishimoto, and S. Sagayama.
2006. On-line recognition of handwritten mathe-
matical expressions based on stroke-based stochastic
context-free grammar. In 10th Frontiers in Handwrit-
ing Recognition, 249?254.
40
