Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1104?1113, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Monte Carlo MCMC: Efficient Inference by Approximate Sampling
Sameer Singh
University of Massachusetts
140 Governor?s Drive
Amherst MA
sameer@cs.umass.edu
Michael Wick
University of Massachsetts
140 Governor?s Drive
Amherst, MA
mwick@cs.umass.edu
Andrew McCallum
University of Massachusetts
140 Governor?s Drive
Amherst MA
mccallum@cs.umass.edu
Abstract
Conditional random fields and other graphi-
cal models have achieved state of the art re-
sults in a variety of tasks such as coreference,
relation extraction, data integration, and pars-
ing. Increasingly, practitioners are using mod-
els with more complex structure?higher tree-
width, larger fan-out, more features, and more
data?rendering even approximate inference
methods such as MCMC inefficient. In this
paper we propose an alternative MCMC sam-
pling scheme in which transition probabilities
are approximated by sampling from the set
of relevant factors. We demonstrate that our
method converges more quickly than a tradi-
tional MCMC sampler for both marginal and
MAP inference. In an author coreference task
with over 5 million mentions, we achieve a 13
times speedup over regular MCMC inference.
1 Introduction
Conditional random fields and other graphical mod-
els are at the forefront of many natural language
processing (NLP) and information extraction (IE)
tasks because they provide a framework for discrim-
inative modeling while succinctly representing de-
pendencies among many related output variables.
Previously, most applications of graphical models
were limited to structures where exact inference is
possible, for example linear-chain CRFs (Lafferty
et al 2001). More recently, there has been a de-
sire to include more factors, longer range depen-
dencies, and more sophisticated features; these in-
clude skip-chain CRFs for named entity recogni-
tion (Sutton and McCallum, 2004), probabilistic
DBs (Wick et al 2010), higher-order models for
dependency parsing (Carreras, 2007), entity-wise
models for coreference (Culotta et al 2007; Wick
et al 2009), and global models of relations (Hoff-
mann et al 2011). The increasing sophistication of
these individual NLP components compounded with
the community?s desire to model these tasks jointly
across cross-document considerations has resulted
in graphical models for which inference is compu-
tationally intractable. Even popular approximate in-
ference techniques such as loopy belief propagation
and Markov chain Monte Carlo (MCMC) may be
prohibitively slow.
MCMC algorithms such as Metropolis-Hastings
are usually efficient for graphical models because
the only factors needed to score a proposal are those
touching the changed variables. However, MCMC
is slowed in situations where a) the model exhibits
variables that have a high-degree (neighbor many
factors), b) proposals modify a substantial subset of
the variables to satisfy domain constraints (such as
transitivity in coreference), or c) evaluating a single
factor is expensive, for example when features are
based on string-similarity. For example, the seem-
ingly innocuous proposal changing the entity type of
a single entity requires examining all its mentions,
i.e. scoring a linear number of factors (in the num-
ber of mentions of that entity). Similarly, evaluating
coreference of a mention to an entity also requires
scoring factors to all the mentions of the entity. Of-
ten, however, the factors are somewhat redundant,
for example, not all mentions of the ?USA? entity
need to be examined to confidently conclude that it
is a COUNTRY, or that it is coreferent with ?United
1104
States of America?.
In this paper we propose an approximate MCMC
framework that facilitates efficient inference in high-
degree graphical models. In particular, we approx-
imate the acceptance ratio in the Metropolis Hast-
ings algorithm by replacing the exact model score
with a stochastic approximation that samples from
the set of relevant factors. We explore two sampling
strategies, a fixed proportion approach that samples
the factors uniformly, and a dynamic alternative that
samples factors until the method is confident about
its estimate of the model score.
We evaluate our method empirically on both syn-
thetic and real-world data. On synthetic classi-
fication data, our approximate MCMC procedure
obtains the true marginals faster than a traditional
MCMC sampler. On real-world tasks, our method
achieves 7 times speedup on citation matching, and
13 times speedup on large-scale author disambigua-
tion.
2 Background
2.1 Graphical Models
Factor graphs (Kschischang et al 2001) succinctly
represent the joint distribution over random vari-
ables by a product of factors that make the depen-
dencies between the random variables explicit. A
factor graph is a bipartite graph between the vari-
ables and factors, where each (log) factor f ? F is
a function that maps an assignment of its neighbor-
ing variables to a real number. For example, in a
linear-chain model of part-of-speech tagging, transi-
tion factors score compatibilities between consecu-
tive labels, while emission factors score compatibil-
ities between a label and its observed token.
The probability distribution expressed by the fac-
tor graph is given as a normalized product of the fac-
tors, which we rewrite as an exponentiated sum:
p(y) =
exp?(y)
Z
(1)
?(y) =
?
f?F
f(yf ) (2)
Z =
?
y?Y
exp?(y) (3)
Intuitively, the model favors assignments to the ran-
dom variables that yield higher factor scores and will
assign higher probabilities to such configurations.
The two common inference problems for graphi-
cal models in NLP are maximum a posterior (MAP)
and marginal inference. For models without latent
variables, the MAP estimate is the setting to the
variables that has the highest probability under the
model:
yMAP = argmax
y
p(y) (4)
Marginal inference is the problem of finding
marginal distributions over subsets of the variables,
used primarily in maximum likelihood gradients and
for max marginal inference.
2.2 Markov chain Monte Carlo (MCMC)
Often, computing marginal estimates of a model is
computationally intractable due to the normalization
constant Z, while maximum a posteriori (MAP) is
prohibitive due to the search space of possible con-
figurations. Markov chain Monte Carlo (MCMC) is
important tool for performing sample- and search-
based inference in these models. A particularly suc-
cessful MCMC method for graphical model infer-
ence is Metropolis-Hastings (MH). Since sampling
from the true model p(y) is intractable, MH instead
uses a simpler distribution q(y?|y) that conditions
on a current state y and proposes a new state y? by
modifying a few variables. This new assignment is
then accepted with probability ?:
? = min
(
1,
p(y?)
p(y)
q(y|y?)
q(y?|y)
)
(5)
Computing this acceptance probability is often
highly efficient because the partition function can-
cels, as do all the factors in the model that do not
neighbor the modified variables. MH can be used
for both MAP and marginal inference.
2.2.1 Marginal Inference
To compute marginals with MH, the variables are
initialized to an arbitrary assignment (i.e., randomly
or with some heuristic), and sampling is run until the
samples {yi|i = 0, ? ? ? , n} become independent of
the initial assignment. The ergodic theorem provides
the MCMC analog to the law-of-large-numbers, jus-
tifying the use of the generated samples to compute
the desired statistics (such as feature expectations or
variable marginals).
1105
2.2.2 MAP Inference
Since MCMC can efficiently explore the high
density regions for a given distribution, the distri-
bution p can be modified such that the high-density
region of the new distribution represents the MAP
configuration of p. This is achieved by adding a tem-
perature term ? to the distribution p, resulting in the
following MH acceptance probability:
? = min
(
1,
(
p(y?)
p(y)
) 1
?
)
(6)
Note that as ? ? 0, MH will sample closer to the
MAP configuration. If a cooling schedule is imple-
mented for ? then the MH sampler for MAP infer-
ence can be seen as an instance of simulated anneal-
ing (Bertsimas and Tsitsiklis, 1993).
3 Monte Carlo MCMC
In this section we introduce our approach for ap-
proximating the acceptance ratio of Metropolis-
Hastings that samples the factors, and describe two
sampling strategies.
3.1 Stochastic Proposal Evaluation
Although one of the benefits of MCMC lies in its
ability to leverage the locality of the proposal, for
some information extraction tasks this can become a
crucial bottleneck. In particular, evaluation of each
sample requires computing the score of all the fac-
tors that are involved in the change, i.e. all fac-
tors that neighbor any variable in the set that has
changed. This evaluation becomes a bottleneck for
tasks in which a large number of variables is in-
volved in each proposal, or in which the model con-
tains a number of high-degree variables, resulting in
a large number of factors, or in which computing
the factor score involves an expensive computation,
such as string similarity between mention text.
Instead of evaluating the log-score ? of the model
exactly, this paper proposes a Monte-Carlo estima-
tion of the log-score. In particular, if the set of fac-
tors for a given proposal y? y? is F(y,y?), we use
a sampled subset of the factors S ? F(y,y?) as an
approximation of the model score. In the following
we use F as an abbreviation for F(y,y?). Formally,
?(y) =
?
f?F
f(yf ) = |F| ? EF [f(yf )]
?S(y) = |F| ? ES [f(yf )] (7)
We use the sample log-score (?S) in the acceptance
probability ? to evaluate the samples. Since we are
using a stochastic approximation to the model score,
in general we need to take more MCMC samples
before we converge, however, since evaluating each
sample will be much faster (O(|S|) as opposed to
O(|F|)), we expect overall sampling to be faster.
In the next sections we describe several alternative
strategies for sampling the set of factors S. The pri-
mary restriction on the set of samples S is that their
mean should be an unbiased estimator ofEF[f ]. Fur-
ther, time taken to obtain the set of samples should
be negligible when compared to scoring all the fac-
tors in F. Note that there is an implicit minimum of
1 to the number of the sampled factors.
3.2 Uniform Sampling
The most direct approach for subsampling the set
of F is to perform uniform sampling. In particular,
given a proportion parameter 0 < p ? 1, we select a
random subset Sp ? F such that |Sp| = p ? |F|. Since
this approach is agnostic as to the actual factors
scores, ES[f ] ? EF[f ]. A low p leads to fast evalua-
tion, however it may require a large number of sam-
ples due to the substantial approximation. On the
other hand, although a higher p will converge with
fewer samples, evaluating each sample is slower.
3.3 Confidence-Based Sampling
Selecting the best value for p is difficult, requiring
analysis of the graph structure, and statistics on the
distribution of the factors scores; often a difficult
task in real-world applications. Further, the same
value for p can result in different levels of approxi-
mation for different proposals, either unnecessarily
accurate or problematically noisy. We would prefer
a strategy that adapts to the distribution of the scores
in F.
Instead of sampling a fixed proportion of factors,
we can sample until we are confident that the cur-
rent set of samples Sc is an accurate estimate of the
true mean of F. In particular, we maintain a run-
ning count of the sample mean ESc [f ] and variance
1106
?Sc , using them to compute a confidence interval IS
around our estimate of the mean. Since the num-
ber of sampled factors S could be a substantial frac-
tion of the set of factors F,1 we also incorporate fi-
nite population control (fpc) in our sample variance
computation. We compute the confidence interval as
follows:
?2S =
1
|S| ? 1
?
f?S
(f ? ES [f ])
2 (8)
IS = 2z
?S
?
|S|
?
|F| ? |S|
|F| ? 1
(9)
where we set the z to 1.96, i.e. the 95% confidence
interval. This approach starts with an empty set of
samples, S = {}, and iteratively samples factors
without replacement to add to S, until the confidence
interval around the estimated mean falls below a user
specified maximum interval width threshold i. As a
result, for proposals that contain high-variance fac-
tors, this strategy examines a large number of fac-
tors, while proposals that involve similar factors will
result in fewer samples. Note that this user-specified
threshold is agnostic to the graph structure and the
number of factors, and instead directly reflects the
score distribution of the relevant factors.
4 Experiments
In this section we evaluate our approach for both
marginal and MAP inference.
4.1 Marginal Inference on Synthetic Data
Consider the task of classifying entities into a set of
types, for example, POLITICIAN, VEHICLE, CITY,
GOVERMENT-ORG, etc. For knowledge base con-
struction, this prediction often takes place on the
entity-level, as opposed to the mention-level com-
mon in traditional NLP. To evaluate the type at the
entity-level, the scored factors examine features of
all the entity mentions of the entity, along with the
labels of all relation mentions for which it is an ar-
gument. See Yao et al(2010) and Hoffmann et al
(2011) for examples of such models. Since a sub-
set of the mentions can be sufficiently informative
for the model, we expect our stochastic MCMC ap-
proach to work well.
1Specifically, the fraction may be higher than > 5%
Label
(a) Binary Classification
Model (n = 100)
-4.8 -4 -3.2 -2.4 -1.6 -0.8 0 0.8 1.6 2.4 3.2 4 4.8 5.6 6.4 7.2
-0.4
-0.3
-0.2
-0.1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Label 1Label 0
(b) Distribution of Factor scores
Figure 1: Synthetic Model for Classification
1 0 2 0 3 0 100 200 1000 10000 100000 1000000Number of Factors Examined
0.0000.025
0.0500.075
0.1000.125
0.1500.175
0.2000.225
0.2500.275
0.3000.325
0.3500.375
0.4000.425
0.450
Erro
r in M
argin
al
p:1. p:0.75 p:0.5 p:0.2p:0.1 i:0.1 i:0.05 i:0.01i:0.005 i:0.001
Figure 2: Marginal Inference Error for Classification
on Synthetic Data
We use synthetic data for such a model to evaluate
the quality of marginals returned by the Gibbs sam-
pling form of MCMC. Since the Gibbs algorithm
samples each variable using a fixed assignment of
its neighborhood, we represent generating a single
sample as classification. We create star-shaped mod-
els with a single unobserved variable (entity type)
that neighbors many unary factors, each represent-
ing a single entity- or a relation-mention factor (See
Figure 1a for an example). We generate a synthetic
dataset for this model, creating 100 variables con-
sisting of 100 factors each. The scores of the fac-
tors are generated from gaussians, N(0.5, 1) for the
positive label, and N(?0.5, 1) for the negative label
(note the overlap between the weights in Figure 1b).
Although each structure contains only a single vari-
able, and no cycles, it is a valid benchmark to test
our sampling approach since the effects of the set-
ting of burn-in period and the thinning samples are
not a concern.
We perform standard Gibbs sampling, and com-
1107
pare the marginals obtained during sampling with
the true marginals, computed exactly. We evalu-
ate the previously described uniform sampling and
confidence-based sampling, with several parameter
values, and plot the L1 error to the true marginals
as more factors are examined. Note that here, and
in the rest of the evaluation, we shall use the num-
ber of factors scored as a proxy for running time,
since the effects of the rest of the steps of sam-
pling are relatively negligible. The error in compar-
ison to regular MCMC (p = 1) is shown in Fig-
ure 2, with standard error bars averaging over 100
models. Initially, as the sampling approach is made
more stochastic (lowering p or increasing i), we see
a steady improvement in the running time needed
to obtain the same error tolerance. However, the
amount of relative improvements slows as stochas-
ticity is increased further; in fact for extreme values
(i = 0.05, p = 0.1) the chains perform worse than
regular MCMC.
4.2 Entity Resolution in Citation Data
To evaluate our approach on a real world dataset,
we apply stochastic MCMC for MAP inference on
the task of citation matching. Given a large number
of citations (that appear at the end of research pa-
pers, for example), the task is to group together the
citations that refer to the same paper. The citation
matching problem is an instance of entity resolution,
in which observed mentions need to be partitioned
such that mentions in a set refer to the same under-
lying entity. Note that neither the identities, or the
number of underlying entities is known.
In this paper, the graphical model of entity reso-
lution consists of observed mentions (mi), and pair-
wise binary variables between all pairs of mentions
(yij) which represent whether the corresponding ob-
served mentions are coreferent. There is a local
factor for each coreference variable yij that has a
high score if the underlying mentions mi and mj
are similar. For the sake of efficiency, we only in-
stantiate and incorporate the variables and factors
when the variable is true, i.e. if yij = 1. Thus,
?(y) =
?
e
?
mi,mj?e
f(yij). The set of possible
worlds consists of all settings of the y variables that
are consistent with transitivity, i.e. the binary vari-
ables directly represent a valid clustering over the
mentions. An example of the model defined over 5
m2
m1
m3
m5
m4
1
1
1
1
y
12
y
23
y
13
y
45
Figure 3: Graphical Model for Entity Resolution:
defined over 5 mentions, with the setting of the vari-
ables resulting in 2 entities. For the sake of brevity,
we?ve only included variables set to 1; binary vari-
ables between mentions that are not coreferent have
been omitted.
mentions is given in Figure 3. This representation
is equivalent to Model 2 as introduced in McCal-
lum and Wellner (2004). As opposed to belief prop-
agation and other approximate inference techniques,
MCMC is especially appropriate for the task as it
can directly enforce transitivity.
When performing MCMC, each sample is a set-
ting to all the y variables that is consistent with tran-
sitivity. To maintain transitivity during sampling,
Metropolis Hastings is used to change the binary
variables in a way that is consistent with moving in-
dividual mentions. Our proposal function selects a
random mention, and moves it to a random entity,
changing all the pairwise variables with mentions in
its old entity, and the pairwise variables with men-
tions in its new entity. Thus, evaluation of such a
proposal function requires scoring a number of fac-
tors linear in the size of the entities, which, for large
datasets, can be a significant bottleneck. In prac-
tice, however, these set of factors are often highly
redundant, as many of the mentions that refer to the
same entity contain redundant information and fea-
tures, and entity membership may be efficiently de-
termined by observing a subset of its mentions.
We evaluate on the Cora dataset (McCallum et
al., 1999), used previously to evaluate a number
of information extraction approaches (Pasula et al
2003), including MCMC based inference (Poon and
Domingos, 2007; Singh et al 2009). The dataset
1108
10000 100000 1000000 10000000 100000000Number of Factors Examined
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCu
bed 
F1
p:1. p:0.5 p:0.2 p:0.1i:20. i:2. i:1. i:0.5 i:0.1
Figure 4: Citation Resolution Accuracy Plot for uni-
form and variance-based sampling compared to reg-
ular MCMC (p = 1)
consists of 1295 mentions, that refer to 134 true un-
derlying entities. We use the same features for our
model as (Poon and Domingos, 2007), using true
author, title, and venue segmentation for features.
Since our focus is on evaluating scalability of in-
ference, we combine all the three folds of the data,
and train the model using Samplerank (Wick et al
2011).
We run MCMC on the entity resolution model us-
ing the proposal function described above, running
our approach with different parameter values. Since
we are interested in the MAP configuration, we use
a temperature term for annealing. As inference pro-
gresses, we compute BCubed2 F1 of the current
sample, and plot it against the number of scored fac-
tors in Figure 4. We observe consistent speed im-
provements as stochasticity is improved, with uni-
form sampling and confidence-based sampling per-
forming competitively. To compute the speedup, we
measure the number of factors scored to obtain a de-
sired level of accuracy (90% F1), shown for a di-
verse set of parameters in Table 1. With a very
large confidence interval threshold (i = 20) and
small proportion (p = 0.1), we obtain up to 7 times
speedup over regular MCMC. Since the average en-
tity size in this data set is < 10, using a small pro-
portion (and a wide interval) is equivalent to picking
a single mention to compare against.
2B3 is a coreference evaluation metric, introduced by Bagga
and Baldwin (1998)
Method Factors Examined Speedup
Baseline 57,292,700 1x
Uniform Sampling
p = 0.75 34,803,972 1.64x
p = 0.5 28,143,323 2.04x
p = 0.3 17,778,891 3.22x
p = 0.2 12,892,079 4.44x
p = 0.1 7,855,686 7.29x
Variance-Based Sampling
i = 0.001 52,522,728 1.09x
i = 0.01 51,547,000 1.11x
i = 0.1 47,165,038 1.21x
i = 0.5 32,828,823 1.74x
i = 1 18,938,791 3.02x
i = 2 11,134,267 5.14x
i = 5 9,827,498 5.83x
i = 10 8,675,833 6.60x
i = 20 8,295,587 6.90x
Table 1: Speedups on Cora to obtain 90% B3 F1
4.3 Large-Scale Author Coreference
As the body of published scientific work continues
to grow, author coreference, the problem of clus-
tering mentions of research paper authors into the
real-world authors to which they refer, is becoming
an increasingly important step for performing mean-
ingful bibliometric analysis. However, scaling typi-
cal pairwise models of coreference (e.g., McCallum
and Wellner (2004)) is difficult because the number
of factors in the model grows quadratically with the
number of mentions (research papers) and the num-
ber of factors evaluated for every MCMC proposal
scales linearly in the size of the clusters. For author
coreference, the number of author mentions and the
number of references to an author entity can often be
in the millions, making the evaluation of the MCMC
proposals computationally expensive.
We use the publicly available DBLP dataset3 of
BibTex entries as our unlabeled set of mentions,
which contains nearly 5 million authors. For eval-
uation of accuracy, we also include author mentions
from the Rexa corpus4 that contains 2, 833 mentions
3http://www.informatik.uni-trier.de/
?ley/db/
4http://www2.selu.edu/Academics/Faculty/
aculotta/data/rexa.html
1109
10000000 100000000 1000000000 10000000000Number of Factors Examined
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCub
ed F1
p:1. p:0.5 p:0.2 p:0.1p:0.01 i:10. i:1. i:0.1
(a) Accuracy versus Number of Factors scored
10000000 100000000Number of Samples
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCub
ed F1
p:1. p:0.5 p:0.2 p:0.1p:0.01 i:10. i:1. i:0.1
(b) Accuracy versus Number of Samples
Figure 5: Performance of Different Sampling Strategies and Parameters for coreference over 5 million
mentions. Plot with p refer to uniform sampling with proportion p of factors picked, while plots with i
sample till confidence intervals are narrower than i.
labeled for coreference.
We use the same Metropolis-Hastings scheme that
we employ in the problem of citation matching. As
before, we initialize to the singleton configuration
and run the experiments for a fixed number of sam-
ples, plotting accuracy versus the number of factors
evaluated (Figure 5a) as well as accuracy versus the
number of samples generated (Figure 5b). We also
tabulate the relative speedups to obtain the desired
accuracy level in Table 2. Our proposed method
achieves substantial savings on this task: speedups
of 13.16 using the variance sampler and speedups
of 9.78 using the uniform sampler. As expected,
when we compare the performance using the num-
ber of generated samples, the approximate MCMC
chains appear to converge more slowly; however, the
overall convergence for our approach is substantially
faster because evaluation of each sample is signif-
icantly cheaper. We also present results on using
extreme approximations (for example, p = 0.01),
resulting in convergence to a low accuracy.
5 Discussion and Related Work
MCMC is a popular method for inference amongst
researchers that work with large and dense graphi-
cal models (Richardson and Domingos, 2006; Poon
and Domingos, 2006; Poon et al 2008; Singh et al
2009; Wick et al 2009). Some of the probabilistic
Method Factors Examined Speedup
Baseline 1,395,330,603 1x
Uniform
p = 0.5 689,254,134 2.02x
p = 0.2 327,616,794 4.26x
p = 0.1 206,157,705 6.77x
p = 0.05 152,069,987 9.17x
p = 0.02 142,689,770 9.78x
Variance
i = 0.00001 1,442,091,344 0.96x
i = 0.0001 1,419,110,724 0.98x
i = 0.001 1,374,667,077 1.01x
i = 0.1 1,012,321,830 1.38x
i = 1 265,327,983 5.26x
i = 10 179,701,896 7.76x
i = 100 106,850,725 13.16x
Table 2: Speedups on DBLP to reach 80% B3 F1
programming packages popular amongst NLP prac-
titioners also rely on MCMC for inference and learn-
ing (Richardson and Domingos, 2006; McCallum et
al., 2009). Although most of these methods apply
MCMC directly, the rate of convergence of MCMC
has become a concern as larger and more densely-
factored models are being considered, motivating
the need for more efficient sampling that uses par-
allelism (Singh et al 2011; Gonzalez et al 2011)
1110
and domain knowledge for blocking (Singh et al
2010). Thus we feel providing a method to speed up
MCMC inference can have a significant impact.
There has also been recent work in designing
scalable approximate inference techniques. Belief
propagation has, in particular, has gained some re-
cent interest. Similar to our approach, a number
of researchers propose modifications to BP that per-
form inference without visiting all the factors. Re-
cent work introduces dynamic schedules to priori-
tize amongst the factors (Coughlan and Shen, 2007;
Sutton and McCallum, 2007) that has been used to
only visit a small fraction of the factors (Riedel and
Smith, 2010). Gonzalez et al(2009) utilize these
schedules to facilitate parallelization.
A number of existing approaches in statistics
are also related to our contribution. Leskovec and
Faloutsos (2006) propose techniques to sample a
graph to compute certain graph statistics with asso-
ciated confidence. Christen and Fox (2005) also pro-
pose an approach to efficiently evaluate a proposal,
however, once accepted, they score all the factors.
Murray and Ghahramani (2004) propose an approx-
imate MCMC technique for Bayesian models that
estimates the partition function instead of comput-
ing it exactly.
Related work has also applied such ideas for
robust learning, for example Kok and Domingos
(2005), based on earlier work by Hulten and Domin-
gos (2002), uniformly sample the groundings of an
MLN to estimate the likelihood.
6 Conclusions and Future Work
Motivated by the need for an efficient inference tech-
nique that can scale to large, densely-factored mod-
els, this paper considers a simple extension to the
Markov chain Monto Carlo algorithm. By observ-
ing that many graphical models contain substantial
redundancy among the factors, we propose stochas-
tic evaluation of proposals that subsamples the fac-
tors to be scored. Using two proposed sampling
strategies, we demonstrate improved convergence
for marginal inference on synthetic data. Further,
we evaluate our approach on two real-world entity
resolution datasets, obtaining a 13 times speedup on
a dataset containing 5 million mentions.
Based on the ideas presented in the paper, we will
consider additional sampling strategies. In partic-
ular, we will explore dynamic sampling, in which
we sample fewer factors during the initial, burn-
in phase, but sample more factors as we get close
to convergence. Motivated by our positive results,
we will also study the application of this approach
to other approximate inference techniques, such as
belief propagation and variational inference. Since
training is often a huge bottleneck for information
extraction, we will also explore its applications to
parameter estimation.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by ARFL
under prime contract number is FA8650-10-C-7059,
and the University of Massachusetts gratefully ac-
knowledges the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
The U.S. Government is authorized to reproduce
and distribute reprint for Governmental purposes
notwithstanding any copyright annotation thereon.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect those of
the sponsor.
References
[Bagga and Baldwin1998] Amit Bagga and Breck Bald-
win. 1998. Algorithms for scoring coreference
chains. In International Conference on Language Re-
sources and Evaluation (LREC) Workshop on Linguis-
tics Coreference, pages 563?566.
[Bertsimas and Tsitsiklis1993] D. Bertsimas and J. Tsit-
siklis. 1993. Simulated annealing. Statistical Science,
pages 10?15.
[Carreras2007] Xavier Carreras. 2007. Experiments
with a higher-order projective dependency parser. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 957?961.
[Christen and Fox2005] J. Andre?s Christen and Colin
Fox. 2005. Markov chain monte carlo using an ap-
proximation. Journal of Computational and Graphi-
cal Statistics, 14(4):pp. 795?810.
[Coughlan and Shen2007] James Coughlan and Huiying
Shen. 2007. Dynamic quantization for belief propa-
1111
gation in sparse spaces. Computer Vision and Image
Understanding, 106:47?58, April.
[Culotta et al007] Aron Culotta, Michael Wick, and An-
drew McCallum. 2007. First-order probabilistic mod-
els for coreference resolution. In North American
Chapter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT).
[Gonzalez et al009] Joseph Gonzalez, Yucheng Low,
and Carlos Guestrin. 2009. Residual splash for op-
timally parallelizing belief propagation. In Artificial
Intelligence and Statistics (AISTATS).
[Gonzalez et al011] Joseph Gonzalez, Yucheng Low,
Arthur Gretton, and Carlos Guestrin. 2011. Paral-
lel gibbs sampling: From colored fields to thin junc-
tion trees. In Artificial Intelligence and Statistics (AIS-
TATS), Ft. Lauderdale, FL, May.
[Hoffmann et al011] Raphael Hoffmann, Congle
Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S.
Weld. 2011. Knowledge-based weak supervision
for information extraction of overlapping relations.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 541?550, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
[Hulten and Domingos2002] Geoff Hulten and Pedro
Domingos. 2002. Mining complex models from ar-
bitrarily large databases in constant time. In Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD), pages 525?531, New York, NY, USA.
ACM.
[Kok and Domingos2005] Stanley Kok and Pedro
Domingos. 2005. Learning the structure of markov
logic networks. In International Conference on
Machine Learning (ICML), pages 441?448, New
York, NY, USA. ACM.
[Kschischang et al001] Frank R. Kschischang, Bren-
dan J. Frey, and Hans Andrea Loeliger. 2001. Factor
graphs and the sum-product algorithm. IEEE Transac-
tions of Information Theory, 47(2):498?519, Feb.
[Lafferty et al001] John D. Lafferty, Andrew McCal-
lum, and Fernando Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting and
labeling sequence data. In International Conference
on Machine Learning (ICML).
[Leskovec and Faloutsos2006] Jure Leskovec and Chris-
tos Faloutsos. 2006. Sampling from large graphs.
In International Conference on Knowledge Discovery
and Data Mining (KDD), pages 631?636, New York,
NY, USA. ACM.
[McCallum and Wellner2004] Andrew McCallum and
Ben Wellner. 2004. Conditional models of identity
uncertainty with application to noun coreference. In
Neural Information Processing Systems (NIPS).
[McCallum et al999] Andrew McCallum, Kamal
Nigam, Jason Rennie, and Kristie Seymore. 1999.
A machine learning approach to building domain-
specific search engines. In International Joint
Conference on Artificial Intelligence (IJCAI).
[McCallum et al009] Andrew McCallum, Karl Schultz,
and Sameer Singh. 2009. FACTORIE: Probabilistic
programming via imperatively defined factor graphs.
In Neural Information Processing Systems (NIPS).
[Murray and Ghahramani2004] Iain Murray and Zoubin
Ghahramani. 2004. Bayesian learning in undirected
graphical models: Approximate MCMC algorithms.
In Uncertainty in Artificial Intelligence (UAI).
[Pasula et al003] H. Pasula, B. Marthi, B. Milch,
S. Russell, and I. Shpitser. 2003. Identity uncertainty
and citation matching. In Neural Information Process-
ing Systems (NIPS).
[Poon and Domingos2006] Hoifung Poon and Pedro
Domingos. 2006. Sound and efficient inference with
probabilistic and deterministic dependencies. In AAAI
Conference on Artificial Intelligence.
[Poon and Domingos2007] Hoifung Poon and Pedro
Domingos. 2007. Joint inference in informa-
tion extraction. In AAAI Conference on Artificial
Intelligence, pages 913?918.
[Poon et al008] Hoifung Poon, Pedro Domingos, and
Marc Sumner. 2008. A general method for reduc-
ing the complexity of relational inference and its ap-
plication to MCMC. In AAAI Conference on Artificial
Intelligence.
[Richardson and Domingos2006] Matthew Richardson
and Pedro Domingos. 2006. Markov logic networks.
Machine Learning, 62(1-2):107?136.
[Riedel and Smith2010] Sebastian Riedel and David A.
Smith. 2010. Relaxed marginal inference and its ap-
plication to dependency parsing. In North American
Chapter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT),
pages 760?768.
[Singh et al009] Sameer Singh, Karl Schultz, and An-
drew McCallum. 2009. Bi-directional joint in-
ference for entity resolution and segmentation us-
ing imperatively-defined factor graphs. In Machine
Learning and Knowledge Discovery in Databases
(Lecture Notes in Computer Science) and European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML PKDD), pages 414?429.
[Singh et al010] Sameer Singh, Michael L. Wick, and
Andrew McCallum. 2010. Distantly labeling data for
large scale cross-document coreference. Computing
Research Repository (CoRR), abs/1005.4298.
[Singh et al011] Sameer Singh, Amarnag Subramanya,
Fernando Pereira, and Andrew McCallum. 2011.
1112
Large-scale cross-document coreference using dis-
tributed inference and hierarchical models. In Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL HLT).
[Sutton and McCallum2004] Charles Sutton and Andrew
McCallum. 2004. Collective segmentation and label-
ing of distant entities in information extraction. Tech-
nical Report TR#04-49, University of Massachusetts,
July.
[Sutton and McCallum2007] Charles Sutton and Andrew
McCallum. 2007. Improved dynamic schedules for
belief propagation. In Uncertainty in Artificial Intelli-
gence (UAI).
[Wick et al009] Michael Wick, Aron Culotta, Khasha-
yar Rohanimanesh, and Andrew McCallum. 2009.
An entity-based model for coreference resolution.
In SIAM International Conference on Data Mining
(SDM).
[Wick et al010] Michael Wick, Andrew McCallum, and
Gerome Miklau. 2010. Scalable probabilistic
databases with factor graphs and mcmc. International
Conference on Very Large Databases (VLDB), 3:794?
804, September.
[Wick et al011] Michael Wick, Khashayar Rohani-
manesh, Kedar Bellare, Aron Culotta, and Andrew
McCallum. 2011. Samplerank: Training factor graphs
with atomic gradients. In International Conference on
Machine Learning (ICML).
[Yao et al010] Limin Yao, Sebastian Riedel, and An-
drew McCallum. 2010. Collective cross-document
relation extraction without labelled data. In Empirical
Methods in Natural Language Processing (EMNLP).
1113
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 73?81,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Minimally-Supervised Extraction of Entities from Text Advertisements
Sameer Singh
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
sameer@cs.umass.edu
Dustin Hillard
Advertising Sciences
Yahoo! Labs Silicon Valley
Santa Clara, CA 95054
dhillard@yahoo-inc.com
Chris Leggetter
Advertising Sciences
Yahoo! Labs Silicon Valley
Santa Clara, CA 95054
cjl@yahoo-inc.com
Abstract
Extraction of entities from ad creatives is an
important problem that can benefit many com-
putational advertising tasks. Supervised and
semi-supervised solutions rely on labeled data
which is expensive, time consuming, and dif-
ficult to procure for ad creatives. A small
set of manually derived constraints on fea-
ture expectations over unlabeled data can be
used to partially and probabilistically label
large amounts of data. Utilizing recent work
in constraint-based semi-supervised learning,
this paper injects light weight supervision
specified as these ?constraints? into a semi-
Markov conditional random field model of en-
tity extraction in ad creatives. Relying solely
on the constraints, the model is trained on a set
of unlabeled ads using an online learning al-
gorithm. We demonstrate significant accuracy
improvements on a manually labeled test set
as compared to a baseline dictionary approach.
We also achieve accuracy that approaches a
fully supervised classifier.
1 Introduction
Growth and competition in web search in recent
years has created an increasing need for improve-
ments in organic and sponsored search. While foun-
dational approaches still focus on matching the exact
words of a search to potential results, there is emerg-
ing need to better understand the underlying intent in
queries and documents. The implicit intent is partic-
ularly important when little text is available, such as
for user queries and advertiser creatives.
This work specifically explores the extraction of
named-entities, i.e. discovering and labeling phrases
in ad creatives. For example, for an ad ?Move to
San Francisco!?, we would like to extract the entity
san francisco and label it a CITY. Similarly, for an
ad ?Find DVD players at Amazon?, we would ex-
tract dvd players as a PRODUCT and amazon as a
ORGNAME. The named-entities provide important
features to downstream tasks about what words and
phrases are important, as well as information on the
intent. Much recent research has focused on extract-
ing useful information from text advertisement cre-
atives that can be used for better retrieval and rank-
ing of ads. Semantic annotation of queries and ad
creatives allows for more powerful retrieval models.
Structured representations of semantics, like the one
studied in our task, can be directly framed as infor-
mation extraction tasks, such as segmentation and
named-entity recognition.
Information extraction methods commonly rely
on labeled data for training the models. The hu-
man labeling of ad creatives would have to pro-
vide the complete segmentation and entity labels for
the ads, which the information extraction algorithm
would then rely on as the truth. For entity extraction
from advertisements this involves familiarity with
a large number of different domains, such as elec-
tronics, transportation, apparel, lodging, sports, din-
ing, services, etc. This leads to an arduous and time
consuming labeling process that can result in noisy
and error-prone data. The problem is further com-
pounded by the inherent ambiguity of the task, lead-
ing to the human editors often presenting conflicting
and incorrect labeling.
Similar problems, to a certain degree, are also
faced by a number of other machine learning tasks
where completely relying on the labeled data leads
to unsatisfactory results. To counter the noisy
and sparse labels, semi-supervised learning meth-
73
ods utilize unlabeled data to improve the model
(see (Chapelle et al, 2006) for an overview). Fur-
thermore, recent work on constraint-based semi-
supervised learning allows domain experts to eas-
ily provide additional light supervision, enabling the
learning algorithm to learn using the prior domain
knowledge, labeled and unlabeled data (Chang et
al., 2007; Mann and McCallum, 2008; Bellare et al,
2009; Singh et al, 2010).
Prior domain knowledge, if it can be easily ex-
pressed and incorporated into the learning algo-
rithm, can often be a high-quality and cheap sub-
stitute for labeled data. For example, previous
work has often used dictionaries or lexicons (lists
of phrases of a particular label) to bootstrap the
model (Agichtein and Ganti, 2004; Canisius and
Sporleder, 2007), leading to a partial labeling of the
data. Domain knowledge can also be more proba-
bilistic in nature, representing the probability of cer-
tain token taking on a certain label. For most tasks,
labeled data is a convenient representation of the do-
main knowledge, but for complex domains such as
structured information extraction from ads, these al-
ternative easily expressible representations may be
as effective as labeled data.
Our approach to solving the the named entity ex-
traction problem for ads relies completely on do-
main knowledge not expressed as labeled data, an
approach that is termed minimally supervised. Each
ad creative is represented as a semi-Markov condi-
tional random field that probabilistically represents
the segmentation and labeling of the creative. Exter-
nal domain knowledge is expressed as a set of targets
for the expectations of a small subset of the features
of the model. We use alternating projections (Bel-
lare et al, 2009) to train our model using this knowl-
edge, relying on the rest of the features of the model
to ?dissipate? the knowledge. Topic model and co-
occurrence based features help this propagation by
generalizing the supervision to a large number of
similar ads.
This method is applied to a large dataset of text
advertisements sampled from a variety of different
domains. The minimally supervised model performs
significantly better than a model that incorporates
the domain knowledge as hard constraints. Our
model also performs competitively when compared
to a supervised model trained on labeled data from a
similar domain (web search queries).
Background material on semi-CRFs and con-
straint based semi-supervised learning is summa-
rized in Section 2. In Section 3, we describe the
problem of named entity recognition in ad creatives
as a semi-CRF, and describe the features in Sec-
tion 4. The constraints that we use to inject super-
vision into our model are listed in Section 5. We
demonstrate the success of our approach in Sec-
tion 6. This work is compared with related literature
in Section 7.
2 Background
This section covers introductory material on
the probabilistic representation of our model
(semi-Markov conditional random fields) and the
constraint-driven semi-supervised method that we
use to inject supervision into the model.
2.1 Semi-Markov Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) use a Markov random field to model the
conditional probability P (y|x). CRFs are com-
monly used to learn sequential models, where the
Markov field is a linear-chain, and y is a linear se-
quence of labels and each label yi ? Y . Let f be a
vector of local feature functions f = ?f1, . . . , fK?,
each of which maps a pair (x,y) and an index i to
a measurement fk(i,x,y) ? <. Let f(i,x,y) be
the vector of these measurements, and let F(x,y) =
?|x|
i f(i,x,y). CRFs use these feature functions in
conjunction with the parameters ? to represent the
conditional probability as follows:
P (y|x, ?) =
1
Z(x)
e??F(x,y)
where Z(x) =
?
y? e
??F(x,y?).
For sequential models where the same labels ap-
pear within a sequence as contiguous blocks (e.g.,
named entity recognition) it is more convenient to
represent these blocks directly as segments. This
representation was formulated as semi-Markov con-
ditional random fields (Semi-CRFs) in (Sarawagi
and Cohen, 2004). The segmentation of a sequence
is represented by s = ?s1, . . . , sp? where each seg-
ment sj = ?tj , uj , yj? consists of a start position
tj , an end position uj , and a label yj ? Y . Similar
to the CRF, let g be the vector of segment feature
74
functions g = ?g1, . . . , gK?, each of which maps
the pair (x, s) and an index j to a measurement
gk(j,x, s) ? <, and G(x, s) =
?|s|
j g(j,x, s). The
conditional probability is represented as:
P (s|x, ?) =
1
Z(x)
e??G(x,s)
where Z(x) =
?
s? e
??G(x,s?). To assert the Marko-
vian assumption, each gk(j,x, s) only computes
features based on x, sj , and yj?11.
An exact inference algorithm was described in
(Sarawagi and Cohen, 2004), and was later im-
proved to be more efficient (Sarawagi, 2006).
2.2 Constraint Driven Learning Using
Alternating Projections
Recent work in semi-supervised learning uses
constraints as external supervision (Chang et al,
2007; Mann and McCallum, 2008; Bellare et al,
2009; Singh et al, 2010). These external constraints
are specified as constraints on the expectations of a
set of auxiliary features g? = {g?1, . . . , g
?
k} over the
unlabeled data. In particular, given the targets u =
{u1, . . . , uk} corresponding to the auxiliary features
g?, the constraints can take different forms, for ex-
ampleL2 penalty ( 12??ui?
?
j Ep[g
?
i(xj , s)]?
2
2 = 0),
L1 box constraints (|ui ?
?
j Ep[g
?
i(xj , s)]| ? ?)
and Affine constraints2 (Ep[g?i(x, s)] ? ui). In this
work, we only use the affine form of the constraints.
For an example, using domain knowledge, we
may know that token ?arizona? should get the label
STATE in at least half of the occurrences in our data.
To capture this, we introduce an auxiliary feature g? :
[[Label=STATE given Token=?arizona?]]. The
affine constraint is written as Ep[g?(x, y)] ? 0.5.
These constraints have been incorporated into
learning using Alternating Projections (Bellare et
al., 2009). Instead of directly optimizing an ob-
jective function that includes the constraints, this
method considers two distributions, p? and q?,?,
where p?(s|x) = 1Z(x)e
??G(x,s) is the usual semi-
Markov model, and q?,? = 1Z(x)e
(??G(x,s)+??G?(x,s))
is an auxiliary distribution that satisfies the con-
straints and has low divergence with the model p?.
1i.e. gk(j,x, s) can be written as gk(yj?1,x, sj)
2where Ep[g] represents the expectation of g over the unla-
beled data using the model p.
In the batch setting, parameters ? and ? are
learned using an EM-like algorithm, where ? is fixed
while optimizing ? and vice versa. Each of the up-
dates in these steps decomposes according to the in-
stances, leading to a stochastic gradient based online
algorithm, as follows:
1. For t = 1, . . . , T , let ? = 1t+t0 where t0 =
1/?0, ?0 the initial learning rate. Let labeled
and unlabeled data set sizes be m and n ? m
respectively. Let the initial parameters be ?0
and ?0, and ? be the weight of L2 regulariza-
tion on ?.
2. For a new labeled instance xt with segmen-
tation st, set ?t = ?t?1 and ?t = ?t?1 +
?
[
g(xt, st)? Ep?t?1 [g(xt, s)]?
??t?1
n
]
.
3. For a new unlabeled instance xt, ?t =
?t?1 + ?
[
u
(n?m) ? Eq?t?1,?t?1 [g
?(xt, s)]
]
and ?t = ?t?1 +
?
[
Eq?t?1,?t?1 [g(xt, s)]? Ep?t?1 [g(xt, s)]?
??t?1
n
]
.
Online training enables scaling the approach to
large data sets, as is the case with ads. In our ap-
proach we rely only on unlabeled data (m = 0, and
step 2 of the above algorithm does not apply).
3 Model
Most text ads consist of a brief title and an ac-
companying abstract that provides additional infor-
mation. The objective of our paper is to extract
the named-entity phrases within these titles and ab-
stracts, then label them with a type from a pre-
determined taxonomy. An example of such an ex-
traction is shown in Fig 1.
We represent the ad creatives as a sequence of
individual tokens, with a special token inserted be-
tween the title and the abstract of the ad. The dis-
tribution over possible phrases and labels of the ad
is expressed as a semi-Markov conditional random
field, as described earlier in Section 2.1.
3.1 Label Taxonomy
In most applications of CRFs and semi-CRFs, the
domain of labels is a fixed set Y , where each label
indexes into one value. Instead, in our approach, we
represent our set of labels as a taxonomy (tree). The
labels higher in the taxonomy are more generic (for
75
Ad Title: Bradley International Airport Hotel
Ad Abstract: Marriott Hartford, CT Airport hotel - free shuttle service & parking.
Output: Bradley International Airport Hotel
Marriott Hartford, CT Airport hotel free shuttle service & parking.
Label Segment
PLACE: AIRPORT Bradley International
BUSINESS: TRAVEL Hotel
ORGNAME: LODGING Marriott
PLACE: CITY Hartford
PLACE: STATE CT
BUSINESS: TRAVEL hotel
PRODUCT: TRAVEL shuttle service & parking.
Figure 1: Example Prediction: An example of an ad creative (title and abstract), along with a set of probable ex-
tracted entities. Note that even in this relatively simple example, there is some ambiguity about what is the correct
segmentation and labeling.
instance, PLACE) and the labels lower in the taxon-
omy are more specific (for instance, STATE may be
a child of PLACE). The taxonomy of labels that we
use for tagging phrases is shown in Figure 2.
When the model predicts a label for a segment,
it can be from any of the levels in the tree. The
benefits of this is multi-fold. First, this allows the
model to be flexible in predicting labels at a lower
(or higher) level based on its confidence. For ex-
ample, the model may have enough evidence to la-
bel ?san francisco? a CITY, however, for ?georgia?
it may not have enough context to discriminate be-
tween STATE or COUNTRY, but could confidently
label it a PLACE. Secondly, this also allows us to
design the features over multiple levels of label gran-
ularity, which leads to a more expressive model. Ex-
pectation constraints can be specified over this ex-
panded set of features, at any level of the taxonomy.
In order to incorporate the nested labels into our
model, we observe that every feature that fires for
a non-leaf label should also fire for all descendants
of that label, e.g. every feature that is active for la-
bel PLACE should also be active for a label CITY,
COUNTRY, etc 3. Following the observation, for ev-
ery feature gk(x, ?tj , uj , yj?) that is active, we also
3Note that this argument works similarly for the taxonomy
represented as a DAG, where the descendants are of a node are
all nodes reachable from it. We do not explore this structure of
the taxonomy in this paper.
fire ?y? ? desc(yj), gk(x, ?tj , uj , y??)4. The same
procedure is applied to the constraints.
4 Features
Our learning algorithm relies on constraints g? as
supervision to extract entities, but even though con-
straints are designed to be generic they do not cover
the whole dataset. The learning algorithm needs
to propagate the supervision to instances where the
constraints are not applicable, guided by the set
of feature functions g. More expressive and rele-
vant features will provide better propagation. Even
though these feature functions represent the ?unsu-
pervised? part of the model (in that they are only
dependent on the unlabeled sequences), they play
an important role in propagating the supervision
throughout the dataset.
4.1 Sequence and Segment Features
Our first set of features are the commonly used
features employed in linear-chain sequence models
such as CRFs and HMMs. These consist of factors
between each token and its corresponding label, and
neighboring labels. They also include transition fac-
tors between the labels. These are local feature func-
tions that are defined only over pairs of token-wise
4This example describes when gk(yj?1,x, sj) ignores
yj?1. For the usual case gk(yj?1,x, sj), features between all
pairs of descendants of yj?1 and yj are enabled.
76
Proper Nouns Common Nouns
PLACE
CITY STATE
COUNTRY CONTINENT
AIRPORT ZIPCODE
PERSON
MANUFACTURER
PRODUCTNAME
MEDIATITLE
EVENT
PRODUCT and BUSINESS
FINANCE MEDIA
EDUCATION APPAREL
TRAVEL AUTO
TECHNOLOGY RESTAURANT
ORGNAME
AIRLINE SPORTSLEAGUE APPAREL AUTO
MEDIA TECHNOLOGY FINANCE LODGING
EDUCATION SPORTSTEAM RESTAURANT
OCCASION
Figure 2: Label Taxonomy: The set of labels that are used are shown grouped by the parent label. PRODUCT and
BUSINESS labels have been merged for brevity, i.e. there are two labels of each child label shown (e.g. PRODUCT:
AUTO and BUSINESS: AUTO). An additional label OTHER is used for the tokens that do not belong to any entities.
labels yj and yj?1. To utilize the semi-Markov rep-
resentation that allows features over the predicted
segmentation, we add the segment length and pre-
fix/suffix tokens of the segment as features.
4.2 Segment Clusters
Although the sequence and segment features cap-
ture a lot of useful information, they are not suffi-
cient for propagation. For example, if we have a
constraint about the token ?london? being a CITY,
but not about ?boston?, the model can only rely on
similar contexts between ?london? and ?boston? to
propagate the information. To allow more compli-
cated propagation to occur, we use features based
on a clustering of segments.
The segment cluster features are based on simi-
larity between segments from English sentences. A
large corpus of English documents were taken from
web, from which 5.1 billion unique sentences were
extracted. Using the co-occurrence of segments in
the sentences as a distance measure, K-Means is
used to identify clusters of segments as described in
(Pantel et al, 2009). The cluster identity of each seg-
ment is added as a feature to the model, capturing
the intuition that segments that appear in the same
cluster should get the same label.
4.3 Topic Model
Most of the ads lie in separate domains with
very little overlap, for example travel and electron-
ics. Additional information about the domain can
be very useful for identifying entities in the ad. For
example, consider the token ?amazon?. It may be
difficult to discern whether the token refers to the
geographical region or the website from just the fea-
tures in the model, however given that the domain
of the ad is travel (or conversely, electronics), the
choice becomes easier.
The problem of domain identification is often
posed as a document classification task, which re-
quires labeled data to train and thus is not applica-
ble for our task. Additionally, we are not concerned
with accurately specifying the exact domain of each
ad, instead any information about similarity between
ads according to their domains is helpful. This kind
of representation can be obtained in an unsupervised
fashion by using topic cluster models (Steyvers and
Griffiths, 2007; Blei et al, 2003). Given a large
set of unlabeled documents, topic models define a
distribution of topics over each document, such that
documents that are similar to each other have similar
topic distributions.
The LDA (Blei et al, 2003) implementation of
topic models in the Mallet toolkit (McCallum, 2002)
was used to construct a model with 1000 topics for
a dataset containing 3 million ads. For each ad, the
discrete distribution over the topics, in conjunction
with each possible label, was added as a feature.
This captures a potential for each label given an ap-
proximation of the ad?s domain captured as topics.
77
5 Constraints
Constraints are used to inject light supervision
into the learning algorithm and are defined as tar-
gets u for expectations of features G? over the data.
Any feature that can be included in the model can be
used as a constraint. This allows us to capture a va-
riety of different forms of domain knowledge, some
of which we shall explore in this section.
Labeled data xl, sl can be incorporated as a spe-
cial case when constraints have a target expectation
of 1.0 for the features that are defined only for the
sequence xl and with segmentation sl. This allows
us to easily use labeled data in form of constraints,
but in this work we do not include any labeled data.
A more interesting case is that of partial labeling,
where the domain expert may have prior knowledge
about the probability that certain tokens and/or con-
texts result in a specific label. These constraints
can cover more instances than labeled data, however
they only provide partial and stochastic labels. All
of the constraints described in this section are also
included as simple features.
Many different methods have been suggested in
recent work for finding the correct target values for
the feature expectations. First, if ample labeled data
is available, features expectations can be calculated,
and assumptions can be made that the same expec-
tations hold for the unlabeled data. This method
cannot be applied to our work due to lack of la-
beled data. Second, for certain constraints, the prior
knowledge can be used directly to specify these val-
ues. Third, if the constraints are an output of a
previous machine learning model, we can use that
model?s confidence in the prediction as the target
expectation of the constraint. Finally, a search for
the ideal values of the target expectations can be
performed by evaluating on small evaluation data.
Our target values for feature expectations were set
based on domain knowledge, then adjusted manu-
ally based on minimal manual examination of ex-
amples on a small held-out data set.
5.1 Dictionary-Based
Dictionary constraints are the form of constraints
that apply to the feature between an individual token
and its label. For a set of tokens in the dictionary, the
constraints specify which label they are likely to be.
Dictionaries can be easily constructed using various
sources, for example product databases, lexicons,
manual collections, or predictions from other mod-
els. These dictionary constraints are often used to
bootstrap models (Agichtein and Ganti, 2004; Cani-
sius and Sporleder, 2007) and have also been used in
the ads domain (Li et al, 2009). For our application,
we rely on dictionary constraints from two sources.
First, the predictions of a previous model are used
to construct a dictionary. A model for entity extrac-
tion is trained on a large amount of labeled search
query data. The domain and style of web queries
differs from advertisements, but the set of labels is
essentially the same. The supervised query entity
extraction model is used to infer segments and la-
bels for the ads domain, and each of the predicted
segments are added to the dictionary of the corre-
sponding predicted label. Even though the predic-
tions of the model are not perfect (see Section 6.1)
the predictions of some of the labels are of high pre-
cision, and thus can be used for supervision in form
of noisy dictionary constraints.
The second source of prior information for dictio-
nary constraints are external databases. Lists of vari-
ous types of places can be obtained easily, for exam-
ple CITY, COUNTRY, STATE, AIRPORT, etc. Ad-
ditionally, product databases available internally to
our research group are used for MANUFACTURERS,
BRANDS, PRODUCTS, MEDIATITLE, etc. Some of
these databases are noisy, and the constraints based
on them are given lower target expectations.
5.2 Pattern-Based
Prior knowledge can often be easily expressed as
patterns that appear for a specific domain. Pattern
based matching has been used to express supervision
for information extraction tasks (Califf and Mooney,
1999; Muslea, 1999). The usual use case involves
a domain expert specifying a number of ?prototyp-
ical? patterns, while additional patterns are discov-
ered based on these initial patterns.
We incorporate noisy forms of patterns as con-
straints. Simple regular expression based patterns
were used to identify and label segments for a few
domains (e.g. ?flights to {PLACE}? and ?looking
for {PRODUCT}??). We do not employ a pattern-
discovery algorithm for finding other contexts; the
model propagates these labels, as before, using the
78
features of the rest of the model. However if the
output of a pattern-discovery algorithm is available,
it can be directly incorporated into the model as ad-
ditional constraints.
5.3 Domain-Based
A number of label-independent constraints are
also added to avoid unrealistic segmentation predic-
tions. For example, an expectation over segment
lengths was included, which denotes that the seg-
ment length is usually 1 or 2, and almost never more
than 6. A constraint is also added to avoid segments
that overlap the separator token between title and
abstract by ensuring that the segment that includes
the separator token is always of length 1 and of la-
bel OTHER. Finally, an additional constraint ensures
that the label OTHER is the most common label.
6 Results
The feature expectations of the model are cal-
culated with modifications to an open source
semi-CRF package5. We collect two datasets of
ad creatives randomly sampled from Yahoo!?s ads
database: a smaller dataset contains 14k ads and a
larger dataset of 42k ads. The ads were not restricted
to any particular domain (such as travel, electronics,
etc.). The average length of the complete ad text
was ?14 tokens. Preprocessing of the text involved
lower-casing, basic cleaning, and stemming.
The training time for each iteration through the
data was ?90 minutes for the smaller dataset and
?360 minutes for the larger dataset. Inference over
the dataset, using Viterbi decoding for semi-CRFs,
took a total of ?8 and ?32 minutes. The initial
learning rate ? is set to 10.0.
6.1 Discussion
We compare our approach to a baseline ?Dictio-
nary? system that deterministically selects a label
based on the dictionaries described in Section 5.1.
A segment is given a label corresponding to the dic-
tionary it appears in, or OTHER if it does not ap-
pear in any dictionary. In addition, we compare to
an external supervised system that has been trained
on tens-of-thousands of manually-annotated search
queries that use the same taxonomy (the same sys-
tem as used in Section 5.1 to derive dictionaries).
5Available on http://crf.sourceforge.net/
This CRF-based model contains mostly the same
features as our unsupervised system, and approxi-
mates what a fully supervised system might achieve,
although it is trained on search queries. Results for
our approach and these two systems are presented
in Table 1. Our evaluation data consists of 2,157
randomly sampled ads that were manually labeled
by professional editors. This labeled data size was
too small to sufficiently train a supervised semi-CRF
model that out-performed the dictionary baseline for
our task (which consists of 45 potential labels).
We measure the token-wise accuracy and macro
F-score over the manually labeled dataset. Typi-
cally, these metrics measure only exact matches be-
tween the true and the predicted label, but this leads
to cases where the model may predict PLACE for a
true CITY. To allow a ?partial credit? for these cases,
we introduce ?weighted? version of these measures,
where a predicted label is given 0.5 credit if the true
label is its direct child or parent, and 0.25 credit if
the true label is a sibling. Our F-score measures the
recall of all true labels except OTHER and similarly
the precision of all predicted labels except OTHER.
We focus on these labels because the OTHER la-
bel is mostly uninformative for downstream tasks.
The token-wise accuracy over all labels (including
OTHER) is included as ?Overall Accuracy?.
Our method significantly outperforms the base-
line dictionary method while approaching the results
obtained with the sophisticated supervised model.
Overall accuracy is 50% greater than the dictionary
baseline, and comes within 10% of the supervised
model6. Increasing unlabeled data from 14k to 42k
ads provides an increase in overall accuracy and
non-OTHER precision, but somewhat reduces recall
for the remaining labels. We also include the F2-
score which gives more weight to recall, because
we are interested in extracting informative labels for
downstream models (which may be able to com-
pensate for a lower precision in label prediction).
Our model trained on 14k samples out-performs the
query-based supervised model in terms of F2, which
is promising for future work that will incorporate
predicted labels in ad retrieval and ranking systems.
6Comparisons and trends for normal and weighted measures
are consistent throughout the results.
79
Table 1: Evaluation: Token-wise accuracy and F-score for the methods evaluated on labeled data (Normal / Weighted)
Metric Dictionary Our Method (14k) Our Method (42k) Query-based Sup. Model
Overall Accuracy 0.454 / 0.466 0.596 / 0.627 0.629 / 0.649 0.665 / 0.685
non-OTHER Recall 0.170 / 0.205 0.329 / 0.412 0.271 / 0.325 0.286 / 0.342
non-OTHER Precision 0.136 / 0.163 0.265 / 0.333 0.297 / 0.357 0.392 / 0.469
F1-score 0.151 / 0.182 0.293 / 0.368 0.283 / 0.340 0.331 / 0.395
F2-score 0.162 / 0.195 0.313 / 0.393 0.276 / 0.331 0.303 / 0.361
7 Related Work
Extraction of structured information from text is
of interest to a large number of communities. How-
ever, in the ads domain, the task has usually been
simplified to that of classification or ranking. Pre-
vious work has focused on retrieval (Raghavan and
Iyer, 2008), user click prediction (Shaparenko et
al., 2009; Richardson et al, 2007; Ciaramita et al,
2008), ad relevance (Hillard et al, 2010) and bounce
rate prediction (Sculley et al, 2009). As far we
know, our method is the only one that aims to solve a
much more complex task of segmentation and entity
extraction from ad creatives. Supervised methods
are a poor choice to solve this task as they require
large amounts of labeled ads, which is expensive,
time-consuming and noisy. Most semi-supervised
methods also rely on some labeled data, and scale
badly with the size of unlabeled data, which is in-
tractable for most ad databases.
Considerable research has been undertaken to ex-
ploit forms of domain knowledge other than la-
beled data to efficiently train a model while utiliz-
ing the unlabeled data. These include methods that
express domain knowledge as constraints on fea-
tures, which have shown to provide high accuracy
on natural language datasets (Chang et al, 2007;
Chang et al, 2008; Mann and McCallum, 2008;
Bellare et al, 2009; Singh et al, 2010). We use
the method of alternating projections for constraint-
driven learning (Bellare et al, 2009) since it spec-
ifies constraints on feature expectations instead of
less intuitive constraints on feature parameters (as
in (Chang et al, 2008)). Additionally, the alternat-
ing projection method is computationally more effi-
cient than Generalized Expectation (Mann and Mc-
Callum, 2008) and can be applied in an online fash-
ion using stochastic gradient.
Our approach is most similar to (Li et al, 2009),
which uses semi-supervised learning for CRFs to ex-
tract structured information from user queries. They
also use a constraint-driven method that utilizes an
external data source. Their method, however, relies
on labeled data for part of the supervision while our
method uses only unlabeled data. Also, evaluation
was only shown for a small domain of user queries,
while our work does not restrict itself to any specific
domain of ads for evaluation.
8 Conclusions
Although important for a number of tasks in spon-
sored search, extraction of structured information
from text advertisements is not a well-studied prob-
lem. The difficulty of the problem lies in the expen-
sive, time-consuming and error-prone labeling pro-
cess. In this work, the aim was to explore machine
learning methods that do not use labeled data, re-
lying instead on light supervision specified as con-
straints on feature expectations. The results clearly
show this minimally-supervised method performs
significantly better than a dictionary based baseline.
Our method also approaches the performance of a
supervised model trained to extract entities from
web search queries. These findings strongly suggest
that domain knowledge expressed in forms other
than directly labeled data may be preferable in do-
mains for which labeling data is unsuitable.
The most important limitation lies in the fact
that specifying the target expectations of constraints
is an ad-hoc process, and robustness of the semi-
supervised learning method to noise in these target
values needs to be investigated. Further research
will also explore using the extracted entities from
advertisements to improve downstream sponsored
search tasks.
80
References
Eugene Agichtein and Venkatesh Ganti. 2004. Min-
ing reference tables for automatic text segmentation.
In KDD: ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 20?29,
New York, NY, USA.
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In UAI: Conference on Uncertainty
in Artificial Intelligence.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal on Machine
Learning Research, 3:993?1022.
Mary Elaine Califf and Raymond J. Mooney. 1999. Re-
lational learning of pattern-match rules for information
extraction. In AAAI / IAAI ?99: National conference
on Artificial intelligence and the Innovative Applica-
tions of Artificial Intelligence conference, pages 328?
334.
Sander Canisius and Caroline Sporleder. 2007. Boot-
strapping information extraction from field books.
In EMNLP-CoNLL: Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 827?836.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In ACL: Annual meeting of the Asso-
ciation for Computational Linguistics, pages 280?287.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In AAAI: National Conference on Artificial
Intelligence, pages 1513?1518.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors.
2006. Semi-Supervised Learning (Adaptive Computa-
tion and Machine Learning). The MIT Press, Septem-
ber.
Massimiliano Ciaramita, Vanessa Murdock, and Vassilis
Plachouras. 2008. Online learning from click data for
sponsored search. In WWW: International World Wide
Web Conference.
Dustin Hillard, Stefan Schroedl, Eren Manavoglu, Hema
Raghavan, and Chris Leggetter. 2010. Improving
ad relevance in sponsored search. In WSDM: Inter-
national conference on Web search and data mining,
pages 361?370.
John Lafferty, Andrew Mccallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML:
International Conference on Machine Learning, pages
282?289.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. In SIGIR: In-
ternational Conference on research and development
in information retrieval, pages 572?579. ACM.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In ACL: Annual meet-
ing of the Association for Computational Linguistics,
pages 870?878.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Ion Muslea. 1999. Extraction patterns for information
extraction tasks: A survey. In AAAI: Workshop on Ma-
chine Learning for Information Extraction, pages 1?6.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
EMNLP: Conference on Empirical Methods in Natu-
ral Language Processing, pages 938?947.
Hema Raghavan and Rukmini Iyer. 2008. Evaluating
vector-space and probabilistic models for query to ad
matching. In SIGIR Workshop on Information Re-
trieval in Advertising (IRA).
Matthew Richardson, Ewa Dominowska, and Robert
Ragno. 2007. Predicting clicks: estimating the click-
through rate for new ads. In WWW: International
World Wide Web Conference.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In NIPS: Neural Information Processing Sys-
tems.
Sunita Sarawagi. 2006. Efficient inference on sequence
segmentation models. In ICML: International Confer-
ence on Machine Learning, pages 793?800.
D. Sculley, Robert G. Malkin, Sugato Basu, and
Roberto J. Bayardo. 2009. Predicting bounce
rates in sponsored search advertisements. In KDD:
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data mining, pages 1325?1334.
Benyah Shaparenko, Ozgur Cetin, and Rukmini Iyer.
2009. Data driven text features for sponsored search
click prediction. In AdKDD: Workshop on Data min-
ing and audience intelligence for advertising.
Sameer Singh, Limin Yao, Sebastian Riedel, and Andrew
McCallum. 2010. Constraint-driven rank-based learn-
ing for information extraction. In North American
Chapter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT).
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
Topic Models. Lawrence Erlbaum Associates.
81
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 729?732,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Constraint-Driven Rank-Based Learning for Information Extraction
Sameer Singh Limin Yao Sebastian Riedel Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst MA 01003
{sameer,lmyao,riedel,mccallum}@cs.umass.edu
Abstract
Most learning algorithms for undirected
graphical models require complete inference
over at least one instance before parameter up-
dates can be made. SampleRank is a rank-
based learning framework that alleviates this
problem by updating the parameters during in-
ference. Most semi-supervised learning algo-
rithms also perform full inference on at least
one instance before each parameter update.
We extend SampleRank to semi-supervised
learning in order to circumvent this compu-
tational bottleneck. Different approaches to
incorporate unlabeled data and prior knowl-
edge into this framework are explored. When
evaluated on a standard information extraction
dataset, our method significantly outperforms
the supervised method, and matches results of
a competing state-of-the-art semi-supervised
learning approach.
1 Introduction
Most supervised learning algorithms for undirected
graphical models require full inference over the
dataset (e.g., gradient descent), small subsets of the
dataset (e.g., stochastic gradient descent), or at least
a single instance (e.g., perceptron, Collins (2002))
before parameter updates are made. Often this is the
main computational bottleneck during training.
SampleRank (Wick et al, 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within inference. Ev-
ery pair of samples generated during inference is
ranked according to the model and the ground truth,
and the parameters are updated when the rankings
disagree. SampleRank has enabled efficient learn-
ing for massive information extraction tasks (Culotta
et al, 2007; Singh et al, 2009).
The problem of requiring a complete inference it-
eration before parameters are updated also exists in
the semi-supervised learning scenario. Here the sit-
uation is often considerably worse since inference
has to be applied to potentially very large unlabeled
datasets. Most semi-supervised learning algorithms
rely on marginals (GE, Mann and McCallum, 2008)
or MAP assignments (CODL, Chang et al, 2007).
Calculating these is computationally inexpensive for
many simple tasks (such as classification and re-
gression). However, marginal and MAP inference
tends to be expensive for complex structured pre-
diction models (such as the joint information extrac-
tion models of Singh et al (2009)), making semi-
supervised learning intractable.
In this work we employ a fast rank-based learning
algorithm for semi-supervised learning to circum-
vent the inference bottleneck. The ranking function
is extended to capture both the preference expressed
by the labeled data, and the preference of the domain
expert when the labels are not available. This allows
us to perform SampleRank as is, without sacrificing
its scalability, which is crucial for future large scale
applications of semi-supervised learning.
We applied our method to a standard information
extraction dataset used for semi-supervised learning.
Empirically we demonstrate improvements over the
supervised model, and closely match the results of a
competing state-of-the-art semi-supervised learner.
2 Background
Conditional random fields (Lafferty et al, 2001) are
undirected graphical models represented as factor
729
graphs. A factor graph G = {?i} defines a prob-
ability distribution over assignments y to a set of
output variables, conditioned on an observation x.
A factor ?i computes the inner product between
the vector of sufficient statistics f(xi,yi) and pa-
rameters ?. Let Z(x) be the data-dependent par-
tition function used for normalization. The proba-
bility distribution defined by the graph is:
p(y|x,?) =
1
Z(x)
?
?i?G
e??f(xi,yi)
2.1 Rank-Based Learning
SampleRank (Wick et al, 2009) is a rank-based
learning framework for that performs parameter up-
dates within MCMC inference. Every pair of con-
secutive samples in the MCMC chain is ranked ac-
cording to the model and the ground truth, and the
parameters are updated when the rankings disagree.
This allows the learner to acquire more supervision
per sample, and has led to efficient training of mod-
els for which inference is very expensive (Singh
et al, 2009).
SampleRank considers two ranking functions: (1)
the unnormalized conditional probability (model
ranking), and (2) a truth function F(y) (objective
ranking) which is defined as ?L(y,yL), the neg-
ative loss between the possible assignment y and
the true assignment yL. The truth function can take
different forms, such as tokenwise accuracy or F1-
measure with respect to some labeled data.
In order to learn the parameters for which model
rankings are consistent with objective rankings,
SampleRank performs the following update for each
consecutive pair of samples ya and yb of the MCMC
chain. Let ? be the learning rate, and ? =
f(xi,yai )? f(xi,y
b
i ), then ? is updated as follows:
?
+
?
?
??
??
?? if p(y
a|x)
p(yb|x) < 1 ? F(y
a) > F(yb)
??? if p(y
a|x)
p(yb|x) > 1 ? F(y
a) < F(yb)
0 otherwise.
This update is usually fast: in order to calculate
the required model ratio, only factors that touch
changed variables have to be taken into account.
SampleRank has been incorporated into the FAC-
TORIE toolkit for probabilistic programming with
imperatively-defined factor graphs (McCallum et al,
2009).
3 Semi-Supervised Rank-Based Learning
To apply SampleRank to the semi-supervised set-
ting, we need to specify the truth function F over
both labeled and unlabeled data. For labeled data
YL, we can use the true labels. These are not avail-
able for unlabeled data YU , and we present alterna-
tive ways of defining a truth function FU : YU ? <
for this case.
3.1 Self-Training
Self-training, which uses predictions as truth, fits di-
rectly into our SampleRank framework. After per-
forming SampleRank on training data (using FL),
MAP inference is performed on the unlabeled data.
The prediction y?U is used as the ground truth for
the unlabeled data. Thus the self-training objective
function Fs over the unlabeled data can be defined
as Fs(y) = ?L(y, y?U ).
3.2 Encoding Constraints
Constraint-driven semi-supervised learning uses
constraints to incorporate external domain knowl-
edge when labels are missing (Chang et al, 2007;
Mann and McCallum, 2008; Bellare et al, 2009).
Constraints prefer certain label configurations over
others. For example, one constraint may be that oc-
currences of the word ?California? are preferred to
have the label ?location?.
We can encode constraints directly into the objec-
tive function FU . Let a constraint i be specified as
?pi, ci?, where ci(y) denotes whether assignment y
satisfies the constraint i (+1), violates it (?1), or the
constraint does not apply (0), and pi is the constraint
strength. Then the objective function is:
Fc(y) =
?
i
pici(y)
3.3 Incorporating Model Predictions
When the objective function Fc is used, every pre-
diction on unlabeled data is ranked only according to
the constraints, and thus the model is trained to sat-
isfy all the constraints. This is a problem when the
constraints prefer a wrong solution while the model
favors the correct solution, resulting in SampleR-
ank updating the model away from the true solution.
To avoid this, the ranking function needs to balance
preferences of the constraints and the current model.
730
One option is to incorporate the self-training ob-
jective function Fs. A new objective function that
combines self-training with constraints can be de-
fined as:
Fsc(y) = Fs(y) + ?sFc(y)
= ?L(y, y?U ) + ?s
?
i
pici(y)
This objective function has at least two limita-
tions. First, self-training involves a complete infer-
ence step to obtain y?U . Second, the model might
have low confidence in its prediction (this is the case
when the underlying marginals are almost uniform),
but the self-training objective des not take this into
account. Hence, we also propose an objective func-
tion that incorporates the model score directly, i.e.
Fmc(y) = log p(y|x,?) + logZ(x) + ?mFc(y)
=
?
?i
? ? f(xi,yi) + ?m
?
i
pici(y)
This objective does not require inference, and also
takes into account model confidence.
In both objective functions Fsc and Fmc, ? con-
trols the relative contribution of the constraint pref-
erences to the objective function. With higher ?,
SampleRank will make updates that never try to vi-
olate constraints, while with low ?, SampleRank
trusts the model more. ? corresponds to constraint
satisfaction weights ? used in (Chang et al, 2007).
4 Related Work
Chang et al propose constraint-driven learn-
ing (CODL, Chang et al, 2007) which can be in-
terpreted as a variation of self-training: Instances
are selected for supervision based not only on the
model?s prediction, but also on their consistency
with a set of user-defined constraints. By directly in-
corporating the model score and the constraints (as
inFmc in Section 3.3) we follow the same approach,
but avoid the expensive ?Top-K? inference step.
Generalized expectation criterion (GE, Mann and
McCallum, 2008) and Alternating Projections (AP,
Bellare et al, 2009) encode preferences by speci-
fying constraints on feature expectations, which re-
quire expensive inference. Although AP can use on-
line training, it still involves full inference over each
instance. Furthermore, these methods only support
constraints that factorize according to the model.
Li (2009) incorporates prior knowledge into con-
ditional random fields as variables. They require full
inference during learning, restricting the application
to simple models. Furthermore, higher-order con-
straints are specified using large cliques in the graph,
which slow down inference. Our approach directly
incorporates these constraints into the ranking func-
tion, with no impact on inference time.
5 Experiments
We carried out experiments on the Cora citation
dataset. The task is to segment each citation into
different fields, such as ?author? and ?title?. We use
300 instances as training data, 100 instances as de-
velopment data, and 100 instances as test data. Some
instances from the training data are selected as la-
beled instances, and the remaining data (including
development) as unlabeled. We use the same token-
label constraints as Chang et al (2007).
We use the objective functions defined in Sec-
tion 3, specifically self-training (Self:Fs), direct
constraints (Cons:Fc), the combination of the two
(Self+Cons:Fsc), and combination of the model
score and the constraints (Model+Cons:Fmc). We
set pi = 1.0, ? = 1.0, ?s = 10, and ?m = 0.0001.
Average token accuracy for 5 runs is reported and
compared with CODL1 in Table 1. We also report
supervised results from (Chang et al, 2007) and
SampleRank. All of our methods show vast im-
provement over the supervised method for smaller
training sizes, but this difference decreases as the
training size increases. When the complete training
data is used, additional unlabeled data hurts our per-
formance. This is not observed in CODL since they
use more unlabeled data, which may also explain
their slightly higher accuracy. Note that Self+Cons
performs better than Self or Cons individually.
Model+Cons also performs competitively, and
may potentially outperform other methods if a bet-
ter ?m is chosen. Note, however, that ?m is much
harder to tune than ?s since ?m weighs the contri-
bution of the unnormalized model score, the range
1We report inference without constraints results from
CODL. Their results that incorporated constraints were higher,
but we do not implement this alternative due to the difficulty in
balancing the model score and constraint weights.
731
Method 5 10 15 20 25 300
Sup. (CODL) 55.1 64.6 68.7 70.1 72.7 86.1
SampleRank 66.5 74.6 75.6 77.6 79.5 90.7
CODL 71 76.7 79.4 79.4 82 88.2
Self 67.6 75.1 75.8 78.6 80.4 88
Cons 67.2 75.3 77.5 78.6 79.4 88.3
Self+Cons 71.3 77 77.5 79.5 81.1 87.4
Model+Cons 69.8 75.4 75.7 79.3 79.3 90.6
Table 1: Tokenwise Accuracy: for different methods as we vary the size of the labeled data
of which depends on many different factors such as
properties of the data, the learning rate, number of
samples, proposal function, etc. For self+cons (?s),
the ranges of the predictions and constraint penalties
are fixed and known, making the task simpler.
Self training takes 90 minutes to run on average,
while Self+Cons and Model+Cons need 100 min-
utes. Since the Cons method skips the inference
step over unlabeled data, it takes only 30 minutes
to run. As the size of the model and unlabeled data
set grows, this saving will become more significant.
Running time of CODL was not reported.
6 Conclusion
This work extends the rank-based learning frame-
work to semi-supervised learning. By integrating
the two paradigms, we retain the computational effi-
ciency provided by parameter updates within infer-
ence, while utilizing unlabeled data and prior knowl-
edge. We demonstrate accuracy improvements on a
real-word information extraction dataset.
We believe that the method will be of greater ben-
efit to learning in complex factor graphs such as
joint models over multiple extraction tasks. In future
work we will investigate our approach in such set-
tings. Additionally, various sensitivity, convergence,
and robustness properties of the method need to be
analyzed.
Acknowledgments
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by SRI Inter-
national subcontract #27-001338 and ARFL prime
contract #FA8750-09-C-0181, and in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation under
NSF grant #IIS-0326249. Any opinions, findings
and conclusions or recommendations expressed in
this material are the authors? and do not necessarily
reflect those of the sponsor.
References
Kedar Bellare, Gregory Druck, and Andrew McCallum.
Alternating projections for learning with expectation
constraints. In UAI, 2009.
Mingwei Chang, Lev Ratinov, and Dan Roth. Guiding
semi-supervision with constraint-driven learning. In
ACL, 2007.
Michael Collins. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithm. In ACL, 2002.
Aron Culotta, Michael Wick, and Andrew McCallum.
First-order probabilistic models for coreference reso-
lution. In NAACL/HLT, 2007.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: probabilistic models for
segmenting and labeling sequence data. In ICML,
2001.
Xiao Li. On the use of virtual evidence in conditional
random fields. In EMNLP, 2009.
Gideon S. Mann and Andrew McCallum. Generalized ex-
pectation criteria for semi-supervised learning of con-
ditional random fields. In ACL, 2008.
Andrew McCallum, Karl Schultz, and Sameer Singh.
FACTORIE: probabilistic programming via impera-
tively defined factor graphs. In NIPS, 2009.
Sameer Singh, Karl Schultz, and Andrew McCallum.
Bi-directional joint inference for entity resolution
and segmentation using imperatively-defined factor
graphs. In ECML/PKDD, 2009.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. SampleRank: Learning pref-
erences from atomic gradients. In NIPS Workshop on
Advances in Ranking, 2009.
732
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 793?803,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Large-Scale Cross-Document Coreference Using
Distributed Inference and Hierarchical Models
Sameer Singh? Amarnag Subramanya? Fernando Pereira? Andrew McCallum?
? Department of Computer Science, University of Massachusetts, Amherst MA 01002
? Google Research, Mountain View CA 94043
sameer@cs.umass.edu, asubram@google.com, pereira@google.com, mccallum@cs.umass.edu
Abstract
Cross-document coreference, the task of
grouping all the mentions of each entity in a
document collection, arises in information ex-
traction and automated knowledge base con-
struction. For large collections, it is clearly
impractical to consider all possible groupings
of mentions into distinct entities. To solve
the problem we propose two ideas: (a) a dis-
tributed inference technique that uses paral-
lelism to enable large scale processing, and
(b) a hierarchical model of coreference that
represents uncertainty over multiple granular-
ities of entities to facilitate more effective ap-
proximate inference. To evaluate these ideas,
we constructed a labeled corpus of 1.5 million
disambiguated mentions in Web pages by se-
lecting link anchors referring to Wikipedia en-
tities. We show that the combination of the
hierarchical model with distributed inference
quickly obtains high accuracy (with error re-
duction of 38%) on this large dataset, demon-
strating the scalability of our approach.
1 Introduction
Given a collection of mentions of entities extracted
from a body of text, coreference or entity resolu-
tion consists of clustering the mentions such that
two mentions belong to the same cluster if and
only if they refer to the same entity. Solutions to
this problem are important in semantic analysis and
knowledge discovery tasks (Blume, 2005; Mayfield
et al, 2009). While significant progress has been
made in within-document coreference (Ng, 2005;
Culotta et al, 2007; Haghighi and Klein, 2007;
Bengston and Roth, 2008; Haghighi and Klein,
2009; Haghighi and Klein, 2010), the larger prob-
lem of cross-document coreference has not received
as much attention.
Unlike inference in other language processing
tasks that scales linearly in the size of the corpus,
the hypothesis space for coreference grows super-
exponentially with the number of mentions. Conse-
quently, most of the current approaches are devel-
oped on small datasets containing a few thousand
mentions. We believe that cross-document coref-
erence resolution is most useful when applied to a
very large set of documents, such as all the news ar-
ticles published during the last 20 years. Such a cor-
pus would have billions of mentions. In this paper
we propose a model and inference algorithms that
can scale the cross-document coreference problem
to corpora of that size.
Much of the previous work in cross-document
coreference (Bagga and Baldwin, 1998; Ravin and
Kazi, 1999; Gooi and Allan, 2004; Pedersen et al,
2006; Rao et al, 2010) groups mentions into entities
with some form of greedy clustering using a pair-
wise mention similarity or distance function based
on mention text, context, and document-level statis-
tics. Such methods have not been shown to scale up,
and they cannot exploit cluster features that cannot
be expressed in terms of mention pairs. We provide
a detailed survey of related work in Section 6.
Other previous work attempts to address some of
the above concerns by mapping coreference to in-
ference on an undirected graphical model (Culotta
et al, 2007; Poon et al, 2008; Wellner et al, 2004;
Wick et al, 2009a). These models contain pair-
wise factors between all pairs of mentions captur-
ing similarity between them. Many of these mod-
els also enforce transitivity and enable features over
793
Filmmaker
Rapper
BEIJING, Feb. 21? Kevin Smith, who played the god of war in the "Xena"...
... The Physiological Basis of Politics,? by Kevin B. Smith, Douglas Oxley, Matthew Hibbing...
The filmmaker Kevin Smith returns to the role of Silent Bob...
Like Back in 2008, the Lions drafted Kevin Smith, even though Smith was badly...
Firefighter Kevin Smith spent almost 20 years preparing for Sept. 11. When he...
...shorthanded backfield in the wake of Kevin Smith's knee injury, and the addition of Haynesworth...
...were coming,'' said Dallas cornerback Kevin Smith. ''We just didn't know when...
...during the late 60's and early 70's, Kevin Smith worked with several local...
...the term hip-hop is attributed to Lovebug Starski. What does it actually mean...
Nothing could be more irrelevant to Kevin Smith's audacious ''Dogma'' than ticking off...
Cornerback
Firefighter
Actor
Running back
Author
Figure 1: Cross-Document Coreference Problem: Example mentions of ?Kevin Smith? from New York
Times articles, with the true entities shown on the right.
entities by including set-valued variables. Exact in-
ference in these models is intractable and a number
of approximate inference schemes (McCallum et al,
2009; Rush et al, 2010; Martins et al, 2010) may
be used. In particular, Markov chain Monte Carlo
(MCMC) based inference has been found to work
well in practice. However as the number of men-
tions grows to Web scale, as in our problem of cross-
document coreference, even these inference tech-
niques become infeasible, motivating the need for
a scalable, parallelizable solution.
In this work we first distribute MCMC-based in-
ference for the graphical model representation of
coreference. Entities are distributed across the ma-
chines such that the parallel MCMC chains on the
different machines use only local proposal distribu-
tions. After a fixed number of samples on each ma-
chine, we redistribute the entities among machines
to enable proposals across entities that were pre-
viously on different machines. In comparison to
the greedy approaches used in related work, our
MCMC-based inference provides better robustness
properties.
As the number of mentions becomes large, high-
quality samples for MCMC become scarce. To
facilitate better proposals, we present a hierarchi-
cal model. We add sub-entity variables that repre-
sent clusters of similar mentions that are likely to
be coreferent; these are used to propose composite
jumps that move multiple mentions together. We
also introduce super-entity variables that represent
clusters of similar entities; these are used to dis-
tribute entities among the machines such that similar
entities are assigned to the same machine. These ad-
ditional levels of hierarchy dramatically increase the
probability of beneficial proposals even with a large
number of entities and mentions.
To create a large corpus for evaluation, we iden-
tify pages that have hyperlinks to Wikipedia, and ex-
tract the anchor text and the context around the link.
We treat the anchor text as the mention, the con-
text as the document, and the title of the Wikipedia
page as the entity label. Using this approach, 1.5
million mentions were annotated with 43k entity la-
bels. On this dataset, our proposed model yields a
B3 (Bagga and Baldwin, 1998) F1 score of 73.7%,
improving over the baseline by 16% absolute (corre-
sponding to 38% error reduction). Our experimen-
tal results also show that our proposed hierarchical
model converges much faster even though it contains
many more variables.
2 Cross-document Coreference
The problem of coreference is to identify the sets of
mention strings that refer to the same underlying en-
tity. The identities and the number of the underlying
entities is not known. In within-document corefer-
ence, the mentions occur in a single document. The
number of mentions (and entities) in each document
is usually in the hundreds. The difficulty of the task
arises from a large hypothesis space (exponential in
the number of mentions) and challenge in resolv-
ing nominal and pronominal mentions to the correct
named mentions. In most cases, named mentions
794
are not ambiguous within a document. In cross-
document coreference, the number of mentions and
entities is in the millions, making the combinatorics
even more daunting. Furthermore, naming ambigu-
ity is much more common as the same string can
refer to multiple entities in different documents, and
distinct strings may refer to the same entity in differ-
ent documents.
We show examples of ambiguities in Figure 1.
Resolving the identity of individuals with the same
name is a common problem in cross-document
coreference. This problem is further complicated
by the fact that in some situations, these individ-
uals may belong to the same field. Another com-
mon ambiguity is that of alternate names, in which
the same entity is referred to by different names or
aliases (e.g. ?Bill? is often used as a substitute for
?William?). The figure also shows an example of
the renaming ambiguity ? ?Lovebug Starski? refers
to ?Kevin Smith?, and this is an extreme form of al-
ternate names. Rare singleton entities (like the fire-
fighter) that may appear only once in the whole cor-
pus are also often difficult to isolate.
2.1 Pairwise Factor Model
Factor graphs are a convenient representation for a
probability distribution over a vector of output vari-
ables given observed variables. The model that we
use for coreference represents mentions (M) and en-
tities (E) as random variables. Each mention can
take an entity as its value, and each entity takes a set
of mentions as its value. Each mention also has a
feature vector extracted from the observed text men-
tion and its context. More precisely, the probability
of a configuration E = e is defined by
p(e) ? exp
?
e?e
{?
m,n?e,n 6=m ?a(m,n)
+
?
m?e,n/?e ?r(m,n)
}
where factor ?a represents affinity between men-
tions that are coreferent according to e, and factor
?r represents repulsion between mentions that are
not coreferent. Different factors are instantiated for
different predicted configurations. Figure 2 shows
the model instantiated with five mentions over a two-
entity hypothesis.
For the factor potentials, we use cosine sim-
ilarity of mention context pairs (?mn) such that
m1
m2
m3
m4
m5
e1
e2
Figure 2: Pairwise Coreference Model: Factor
graph for a 2-entity configuration of 5 mentions.
Affinity factors are shown with solid lines, and re-
pulsion factors with dashed lines.
?a(m,n) = ?mn ? b and ?r(m,n) = ?(?mn ? b),
where b is the bias. While one can certainly make
use of a more sophisticated feature set, we leave this
for future work as our focus is to scale up inference.
However, it should be noted that this approach is
agnostic to the particular set of features used. As
we will note in the next section, we do not need to
calculate features between all pairs of mentions (as
would be prohibitively expensive for large datasets);
instead we only compute the features as and when
required.
2.2 MCMC-based Inference
Given the above model of coreference, we seek the
maximum a posteriori (MAP) configuration:
e? = argmaxe p(e)
= argmaxe
?
e?e
{?
m,n?e,n 6=m ?a(m,n)
+
?
m?e,n/?e ?r(m,n)
}
Computing e? exactly is intractable due to the
large space of possible configurations.1 Instead,
we employ MCMC-based optimization to discover
the MAP configuration. A proposal function q is
used to propose a change e? to the current config-
uration e. This jump is accepted with the following
Metropolis-Hastings acceptance probability:
?(e, e?) = min
(
1,
(
p(e?)
p(e)
)1/t q(e)
q(e?)
)
(1)
1Number of possible entities is Bell(n) in the number of
mentions, i.e. number of partitions of n items
795
where t is the annealing temperature parameter.
MCMC chains efficiently explore the high-
density regions of the probability distribution. By
slowly reducing the temperature, we can decrease
the entropy of the distribution to encourage con-
vergence to the MAP configuration. MCMC has
been used for optimization in a number of related
work (McCallum et al, 2009; Goldwater and Grif-
fiths, 2007; Changhe et al, 2004).
The proposal function moves a randomly chosen
mention l from its current entity es to a randomly
chosen entity et. For such a proposal, the log-model
ratio is:
log
p(e?)
p(e)
=
?
m?et
?a(l,m) +
?
n?es
?r(l, n)
?
?
n?es
?a(l, n)?
?
m?et
?r(l,m) (2)
Note that since only the factors between mention l
and mentions in es and et are involved in this com-
putation, the acceptance probability of each proposal
is calculated efficiently.
In general, the model may contain arbitrarily
complex set of features over pairs of mentions, with
parameters associated with them. Given labeled
data, these parameters can be learned by Percep-
tron (Collins, 2002), which uses the MAP config-
uration according to the model (e?). There also exist
more efficient training algorithms such as SampleR-
ank (McCallum et al, 2009; Wick et al, 2009b) that
update parameters during inference. However, we
only focus on inference in this work, and the only
parameter that we set manually is the bias b, which
indirectly influences the number of entities in e?. Un-
less specified otherwise, in this work the initial con-
figuration for MCMC is the singleton configuration,
i.e. all entities have a size of 1.
This MCMC inference technique, which has been
used in McCallum and Wellner (2004), offers sev-
eral advantages over other inference techniques: (a)
unlike message-passing-methods, it does not require
the full ground graph, (b) we only have to exam-
ine the factors that lie within the changed entities
to evaluate a proposal, and (c) inference may be
stopped at any point to obtain the current best con-
figuration. However, the super exponential nature of
the hypothesis space in cross-doc coreference ren-
ders this algorithm computationally unsuitable for
large scale coreference tasks. In particular, fruit-
ful proposals (that increase the model score) are ex-
tremely rare, resulting in a large number of propos-
als that are not accepted. We describe methods to
speed up inference by 1) evaluating multiple pro-
posal simultaneously (Section 3), and 2) by aug-
menting our model with hierarchical variables that
enable better proposal distributions (Section 4).
3 Distributed MAP Inference
The key observation that enables distribution is that
the acceptance probability computation of a pro-
posal only examines a few factors that are not com-
mon to the previous and next configurations (Eq. 2).
Consider a pair of proposals, one that moves men-
tion l from entity es to entity et, and the other that
moves mention l? from entity e?s to entity e
?
t. The
set of factors to compute acceptance of the first pro-
posal are factors between l and mentions in es and
et, while the set of factors required to compute ac-
ceptance of the second proposal lie between l? and
mentions in e?s and e
?
t. Since these set of factors
are completely disjoint from each other, and the re-
sulting configurations do not depend on each other,
these two proposals are mutually-exclusive. Differ-
ent orders of evaluating such proposals are equiv-
alent, and in fact, these proposals can be proposed
and evaluated concurrently. This mutual-exclusivity
is not restricted only to pairs of proposals; a set of
proposals are mutually-exclusive if no two propos-
als require the same factor for evaluation.
Using this insight, we introduce the following ap-
proach to distributed cross-document coreference.
We divide the mentions and entities among multiple
machines, and propose moves of mentions between
entities assigned to the same machine. These jumps
are evaluated exactly and accepted without commu-
nication between machines. Since acceptance of a
mention?s move requires examining factors that lie
between other mentions in its entity, we ensure that
all mentions of an entity are assigned the same ma-
chine. Unless specified otherwise, the distribution is
performed randomly. To enable exploration of the
complete configuration space, rounds of sampling
are interleaved by redistribution stages, in which the
entities are redistributed among the machines (see
Figure 3). We use MapReduce (Dean and Ghe-
796
Distributor
Inference
Inference
Figure 3: Distributed MCMC-based Inference:
Distributor divides the entities among the machines,
and the machines run inference. The process is re-
peated by the redistributing the entities.
mawat, 2004) to manage the distributed computa-
tion.
This approach to distribution is equivalent to in-
ference with all mentions and entities on a single
machine with a restricted proposer, but is faster
since it exploits independencies to propose multiple
jumps simultaneously. By restricting the jumps as
described above, the acceptance probability calcu-
lation is exact. Partitioning the entities and propos-
ing local jumps are restrictions to the single-machine
proposal distribution; redistribution stages ensure
the equivalent Markov chains are still irreducible.
See Singh et al (2010) for more details.
4 Hierarchical Coreference Model
The proposal function for MCMC-based MAP infer-
ence presents changes to the current entities. Since
we use MCMC to reach high-scoring regions of the
hypothesis space, we are interested in the changes
that improve the current configuration. But as the
number of mentions and entities increases, these
fruitful samples become extremely rare due to the
blowup in the possible space of configurations, re-
sulting in rejection of a large number of proposals.
By distributing as described in the previous section,
we propose samples in parallel, improving chances
of finding changes that result in better configura-
tions. However, due to random redistribution and a
naive proposal function within each machine, a large
fraction of proposals are still wasted. We address
these concerns by adding hierarchy to the model.
4.1 Sub-Entities
Consider the task of proposing moves of mentions
(within a machine). Given the large number of
mentions and entities, the probability that a ran-
domly picked mention that is moved to a random
entity results in a better configuration is extremely
small. If such a move is accepted, this gives us ev-
idence that the mention did not belong to the pre-
vious entity, and we should also move similar men-
tions from the previous entity simultaneously to the
same entity. Since the proposer moves only a sin-
gle mention at a time, a large number of samples
may be required to discover these fruitful moves.
To enable block proposals that move similar men-
tions simultaneously, we introduce latent sub-entity
variables that represent groups of similar mentions
within an entity, where the similarity is defined by
the model. For inference, we have stages of sam-
pling sub-entities (moving individual mentions) in-
terleaved with stages of entity sampling (moving all
mentions within a sub-entity). Even though our con-
figuration space has become larger due to these ex-
tra variables, the proposal distribution has also im-
proved since it proposes composite moves.
4.2 Super-Entities
Another issue faced during distributed inference is
that random redistribution is often wasteful. For ex-
ample, if dissimilar entities are assigned to a ma-
chine, none of the proposals may be accepted. For a
large number of entities and machines, the probabil-
ity that similar entities will be assigned to the same
machine is extremely small, leading to a larger num-
ber of wasted proposals. To alleviate this problem,
we introduce super-entities that represent groups of
similar entities. During redistribution, we ensure all
entities in the same super-entity are assigned to the
same machine. As for sub-entities above, inference
switches between regular sampling of entities and
sampling of super-entities (by moving entities). Al-
though these extra variables have made the config-
uration space larger, they also allow more efficient
distribution of entities, leading to useful proposals.
4.3 Combined Hierarchical Model
Each of the described levels of the hierarchy are sim-
ilar to the initial model (Section 2.1): mentions/sub-
entities have the same structure as the entities/super-
entities, and are modeled using similar factors. To
represent the ?context? of a sub-entity we take the
union of the bags-of-words of the constituent men-
tion contexts. Similarly, we take the union of sub-
797
Super-Entities
Entities
Mentions
Sub-Entities
Figure 4: Combined Hierarchical Model with factors instantiated for a hypothesis containing 2 super-
entities, 4 entities, and 8 sub-entities, shown as colored circles, over 16 mentions. Dotted lines represent
repulsion factors and solid lines represent affinity factors (the color denotes the type of variable that the
factor touches). The boxes on factors were excluded for clarity.
entity contexts to represent the context of an entity.
The factors are instantiated in the same manner as
Section 2.1 except that we change the bias factor
b for each level (increasing it for sub-entities, and
decreasing it for super-entities). The exact values
of these biases indirectly determines the number of
predicted sub-entities and super-entities.
Since these two levels of hierarchy operate at
separate granularities from each other, we combine
them into a single hierarchical model that contains
both sub- and super-entities. We illustrate this hi-
erarchical structure in Figure 4. Inference for this
model takes a round-robin approach by fixing two
of the levels of the hierarchy and sampling the third,
cycling through these three levels. Unless specified
otherwise, the initial configuration is the singleton
configuration, in which all sub-entities, entities, and
super-entities are of size 1.
5 Experiments
We evaluate our models and algorithms on a number
of datasets. First, we compare performance on the
small, publicly-available ?John Smith? dataset. Sec-
ond, we run the automated Person-X evaluation to
obtain thousands of mentions that we use to demon-
strate accuracy and scalability improvements. Most
importantly, we create a large labeled corpus using
links to Wikipedia to explore the performance in the
large-scale setting.
5.1 John Smith Corpus
To compare with related work, we run an evalua-
tion on the ?John Smith? corpus (Bagga and Bald-
win, 1998), containing 197 mentions of the name
?John Smith? from New York Times articles (la-
beled to obtain 35 true entities). The bias b for
our approach is set to result in the correct number
of entities. Our model achieves B3 F1 accuracy of
66.4% on this dataset. In comparison, Rao et al
(2010) obtains 61.8% using the model most similar
to ours, while their best model (which uses sophis-
ticated topic-model features that do not scale easily)
achieves 69.7%. It is encouraging to note that our
approach, using only a subset of the features, per-
forms competitively with related work. However,
due to the small size of the dataset, we require fur-
ther evaluation before reaching any conclusions.
5.2 Person-X Evaluation
There is a severe lack of labeled corpora for cross-
document coreference due to the effort required
to evaluate the coreference decisions. Related
approaches have used automated Person-X evalu-
ation (Gooi and Allan, 2004), in which unique
person-name strings are treated as the true entity
labels for the mentions. Every mention string is
replaced with an ?X? for the coreference system.
We use this evaluation methodology on 25k person-
name mentions from the New York Times cor-
pus (Sandhaus, 2008) each with one of 50 unique
strings. As before, we set the bias b to achieve the
same number of entities. We use 1 million samples
in each round of inference, followed by random re-
distribution in the flat model, and super-entities in
the hierarchical model. Results are averaged over
five runs.
798
Figure 5: Person-X Evaluation of Pairwise model:
Performance as number of machines is varied, aver-
aged over 5 runs.
Number of Entities 43,928
Number of Mentions 1,567,028
Size of Largest Entity 6,096
Average Mentions per Entity 35.7
Variance of Mentions per Entity 5191.7
Table 1: Wikipedia Link Corpus Statistics. Size
of an entity is the number of mentions of that entity.
Figure 5 shows accuracy compared to relative
wallclock running time for distributed inference on
the flat, pairwise model. Speed and accuracy im-
prove as additional machines are added, but larger
number of machines lead to diminishing returns for
this small dataset. Distributed inference on our hi-
erarchical model is evaluated in Figure 6 against in-
ference on the pairwise model from Figure 5. We
see that the individual hierarchical models perform
much better than the pairwise model; they achieve
the same accuracy as the pairwise model in approx-
imately 10% of the time. Moreover, distributed in-
ference on the combined hierarchical model is both
faster and more accurate than the individual hierar-
chical models.
5.3 Wikipedia Link Corpus
To explore the application of the proposed approach
to a larger, realistic dataset, we construct a corpus
based on the insight that links to Wikipedia that ap-
pear on webpages can be treated as mentions, and
since the links were added manually by the page au-
thor, we use the destination Wikipedia page as the
Figure 6: Person-X Evaluation of Hierarchical
Models: Performance of inference on hierarchical
models compared to the pairwise model. Experi-
ments were run using 50 machines.
entity the link refers to.
The dataset is created as follows: First, we crawl
the web and select hyperlinks on webpages that link
to an English Wikipedia page.2 The anchors of
these links form our set of mentions, with the sur-
rounding block of clean text (obtained after remov-
ing markup, etc.) around each link being its con-
text. We assign the title of the linked Wikipedia
page as the entity label of that link. Since this set
of mentions and labels can be noisy, we use the
following filtering steps. All links that have less
than 36 words in their block, or whose anchor text
has a large string edit distance from the title of the
Wikipedia page, are discarded. While this results in
cases in which ?President? is discarded when linked
to the ?Barack Obama? Wikipedia page, it was nec-
essary to reduce noise. Further, we also discard
links to Wikipedia pages that are concepts (such as
?public_domain?) rather than entities. All enti-
ties with less than 6 links to them are also discarded.
Table 1 shows some statistics about our automat-
ically generated data set. We randomly sampled 5%
of the entities to create a development set, treating
the remaining entities as the test set. Unlike the
John Smith and Person-X evaluation, this data set
also contains non-person entities such as organiza-
tions and locations.
For our models, we augment the factor potentials
with mention-string similarity:
2e.g. http://en.wikipedia.org/Hillary_Clinton
799
?a/r(m,n) = ? (?mn ? b+ wSTREQ(m,n))
where STREQ is 1 if mentions m and n are string
identical (0 otherwise), and w is the weight to this
feature.3 In our experiments we found that setting
w = 0.8 and b = 1e? 4 gave the best results on the
development set.
Due to the large size of the corpus, existing cross-
document coreference approaches could not be ap-
plied to this dataset. However, since a majority
of related work consists of using clustering after
defining a similarity function (Section 6), we pro-
vide a baseline evaluation of clustering with Sub-
Square (Bshouty and Long, 2010), a scalable, dis-
tributed clustering method. Subsquare takes as in-
put a weighted graph with mentions as nodes and
similarity between mentions used as edge weights.
Subsquare works by stochastically assigning a ver-
tex to the cluster of one its neighbors if they have
significant neighborhood overlap. This algorithm
is an efficient form of approximate spectral cluster-
ing (Bshouty and Long, 2010), and since it is given
the same distances between mentions as our models,
we expect it to get similar accuracy. We also gen-
erate another baseline clustering by assigning men-
tions with identical strings to the same entity. This
mention-string clustering is also used as the initial
configuration of our inference.
Figure 7: Wikipedia Link Evaluation: Perfor-
mance of inference for different number of machines
(N = 100, 500). Mention-string match clustering is
used as the initial configuration.
3Note that we do not use mention-string similarity for John
Smith or Person-X as the mention strings are all identical.
Method
Pairwise B3 Score
P/ R F1 P/ R F1
String-Match 30.0 / 66.7 41.5 82.7 / 43.8 57.3
Subsquare 38.2 / 49.1 43.0 87.6 / 51.4 64.8
Our Model 44.2 / 61.4 51.4 89.4 / 62.5 73.7
Table 2: F1 Scores on the Wikipedia Link Data.
The results are significant at the 0.0001 level over
Subsquare according to the difference of proportions
significance test.
Inference is run for 20 rounds of 10 million sam-
ples each, distributed over N machines. We use
N = 100, 500 and the B3 F1 score results obtained
set for each case are shown in Figure 7. It can
be seen that N = 500 converges to a better solu-
tion faster, showing effective use of parallelism. Ta-
ble 2 compares the results of our approach (at con-
vergence for N = 500), the baseline mention-string
match and the Subsquare algorithm. Our approach
significantly outperforms the competitors.
6 Related Work
Although the cross-document coreference problem
is challenging and lacks large labeled datasets, its
ubiquitous role as a key component of many knowl-
edge discovery tasks has inspired several efforts.
A number of previous techniques use scoring
functions between pairs of contexts, which are then
used for clustering. One of the first approaches
to cross-document coreference (Bagga and Bald-
win, 1998) uses an idf-based cosine-distance scor-
ing function for pairs of contexts, similar to the one
we use. Ravin and Kazi (1999) extend this work to
be somewhat scalable by comparing pairs of con-
texts only if the mentions are deemed ?ambiguous?
using a heuristic. Others have explored multiple
methods of context similarity, and concluded that
agglomerative clustering provides effective means
of inference (Gooi and Allan, 2004). Pedersen et
al. (2006) and Purandare and Pedersen (2004) inte-
grate second-order co-occurrence of words into the
similarity function. Mann and Yarowsky (2003) use
biographical facts from the Web as features for clus-
tering. Niu et al (2004) incorporate information ex-
traction into the context similarity model, and anno-
tate a small dataset to learn the parameters. A num-
ber of other approaches include various forms of
800
hand-tuned weights, dictionaries, and heuristics to
define similarity for name disambiguation (Blume,
2005; Baron and Freedman, 2008; Popescu et al,
2008). These approaches are greedy and differ in the
choice of the distance function and the clustering al-
gorithm used. Daume? III and Marcu (2005) propose
a generative approach to supervised clustering, and
Haghighi and Klein (2010) use entity profiles to as-
sist within-document coreference.
Since many related methods use clustering, there
are a number of distributed clustering algorithms
that may help scale these approaches. Datta et
al. (2006) propose an algorithm for distributed k-
means. Chen et al (2010) describe a parallel spectral
clustering algorithm. We use the Subsquare algo-
rithm (Bshouty and Long, 2010) as baseline because
it works well in practice. Mocian (2009) presents a
survey of distributed clustering algorithms.
Rao et al (2010) have proposed an online deter-
ministic method that uses a stream of input mentions
and assigns them greedily to entities. Although it
can resolve mentions from non-trivial sized datasets,
the method is restricted to a single machine, which
is not scalable to the very large number of mentions
that are encountered in practice.
Our representation of the problem as an undi-
rected graphical model, and performing distributed
inference on it, provides a combination of advan-
tages not available in any of these approaches. First,
most of the methods will not scale to the hundreds
of millions of mentions that are present in real-world
applications. By utilizing parallelism across ma-
chines, our method can run on very large datasets
simply by increasing the number of machines used.
Second, approaches that use clustering are limited
to using pairwise distance functions for which ad-
ditional supervision and features are difficult to in-
corporate. In addition to representing features from
all of the related work, graphical models can also
use more complex entity-wide features (Culotta et
al., 2007; Wick et al, 2009a), and parameters can
be learned using supervised (Collins, 2002) or semi-
supervised techniques (Mann and McCallum, 2008).
Finally, the inference for most of the related ap-
proaches is greedy, and earlier decisions are not re-
visited. Our technique is based on MCMC inference
and simulated annealing, which are able to escape
local maxima.
7 Conclusions
Motivated by the problem of solving the corefer-
ence problem on billions of mentions from all of the
newswire documents from the past few decades, we
make the following contributions. First, we intro-
duce distributed version of MCMC-based inference
technique that can utilize parallelism to enable scal-
ability. Second, we augment the model with hierar-
chical variables that facilitate fruitful proposal distri-
butions. As an additional contribution, we use links
to Wikipedia pages to obtain a high-quality cross-
document corpus. Scalability and accuracy gains of
our method are evaluated on multiple datasets.
There are a number of avenues for future work.
Although we demonstrate scalability to more than a
million mentions, we plan to explore performance
on datasets in the billions. We also plan to examine
inference on complex coreference models (such as
with entity-wide factors). Another possible avenue
for future work is that of learning the factors. Since
our approach supports parameter estimation, we ex-
pect significant accuracy gains with additional fea-
tures and supervised data. Our work enables cross-
document coreference on very large corpora, and we
would like to explore the downstream applications
that can benefit from it.
Acknowledgments
This work was done when the first author was an
intern at Google Research. The authors would
like to thank Mark Dredze, Sebastian Riedel, and
anonymous reviewers for their valuable feedback.
This work was supported in part by the Center
for Intelligent Information Retrieval, the Univer-
sity of Massachusetts gratefully acknowledges the
support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181., in part by an award
from Google, in part by The Central Intelligence
Agency, the National Security Agency and National
Science Foundation under NSF grant #IIS-0326249,
in part by NSF grant #CNS-0958392, and in part
by UPenn NSF medium IIS-0803847. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
801
References
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In International Conference on Computational
Linguistics, pages 79?85.
A. Baron and M. Freedman. 2008. Who is who and what
is what: experiments in cross-document co-reference.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 274?283.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Matthias Blume. 2005. Automatic entity disambigua-
tion: Benefits to NER, relation extraction, link anal-
ysis, and inference. In International Conference on
Intelligence Analysis (ICIA).
Nader H. Bshouty and Philip M. Long. 2010. Find-
ing planted partitions in nearly linear time using ar-
rested spectral clustering. In Johannes Fu?rnkranz
and Thorsten Joachims, editors, Proceedings of the
27th International Conference on Machine Learning
(ICML-10), pages 135?142, Haifa, Israel, June. Omni-
press.
Yuan Changhe, Lu Tsai-Ching, and Druzdzel Marek.
2004. Annealed MAP. In Uncertainty in Artificial In-
telligence (UAI), pages 628?635, Arlington , Virginia.
AUAI Press.
Wen-Yen Chen, Yangqiu Song, Hongjie Bai, Chih-Jen
Lin, and Edward Y. Chang. 2010. Parallel spectral
clustering in distributed systems. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithm. In Annual Meeting of the
Association for Computational Linguistics (ACL).
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT).
S. Datta, C. Giannella, and H. Kargupta. 2006. K-Means
Clustering over a Large, Dynamic Network. In SIAM
Data Mining Conference (SDM).
Hal Daume? III and Daniel Marcu. 2005. A Bayesian
model for supervised clustering with the Dirichlet pro-
cess prior. Journal of Machine Learning Research
(JMLR), 6:1551?1577.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. Sympo-
sium on Operating Systems Design & Implementation
(OSDI).
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 744?751.
Chung Heong Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT), pages 9?16.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 870?878.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In North Amer-
ican Chapter of the Association for Computational
Linguistics - Human Language Technologies (NAACL
HLT), pages 33?40.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 34?44, Cambridge, MA, October.
Association for Computational Linguistics.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed,
T. Finin, C. Fink, M. Freedman, N. Garera, P. Mc-
Namee, et al 2009. Cross-document coreference res-
olution: A key technology for learning by reading. In
AAAI Spring Symposium on Learning by Reading and
Learning to Read.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems (NIPS).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Horatiu Mocian. 2009. Survey of Distributed Clustering
Techniques. Ph.D. thesis, Imperial College of London.
802
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Annual Meeting of the Association for Computational
Linguistics (ACL).
Cheng Niu, Wei Li, and Rohini K. Srihari. 2004. Weakly
supervised learning for cross-document person name
disambiguation supported by information extraction.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), page 597.
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zor-
nitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name
discrimination using second order co-occurrence fea-
tures. In International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 208?222.
Hoifung Poon, Pedro Domingos, and Marc Sumner.
2008. A general method for reducing the complexity
of relational inference and its application to MCMC.
In AAAI Conference on Artificial Intelligence.
Octavian Popescu, Christian Girardi, Emanuele Pianta,
and Bernardo Magnini. 2008. Improving cross-
document coreference. Journe?es Internationales
d?Analyse statistique des Donne?es Textuelles, 9:961?
969.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Conference on Computational Natu-
ral Language Learning (CoNLL), pages 41?48.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In International Conference on Computa-
tional Linguistics (COLING), pages 1050?1058, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham
Clinton the president? disambiguating names across
documents. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 9?16.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?11, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2010. Distributed map in-
ference for undirected graphical models. In Neural
Information Processing Systems (NIPS), Workshop on
Learning on Cores, Clusters and Clouds.
Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Uncertainty in Artificial
Intelligence (UAI), pages 593?601.
Michael Wick, Aron Culotta, Khashayar Rohanimanesh,
and Andrew McCallum. 2009a. An entity-based
model for coreference resolution. In SIAM Interna-
tional Conference on Data Mining (SDM).
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009b. Samplerank: Learn-
ing preferences from atomic gradients. In Neural In-
formation Processing Systems (NIPS), Workshop on
Advances in Ranking.
803
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379?388,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Discriminative Hierarchical Model for Fast Coreference at Large Scale
Michael Wick
University of Massachsetts
140 Governor?s Drive
Amherst, MA
mwick@cs.umass.edu
Sameer Singh
University of Massachusetts
140 Governor?s Drive
Amherst, MA
sameer@cs.umass.edu
Andrew McCallum
University of Massachusetts
140 Governor?s Drive
Amherst, MA
mccallum@cs.umass.edu
Abstract
Methods that measure compatibility between
mention pairs are currently the dominant ap-
proach to coreference. However, they suffer
from a number of drawbacks including diffi-
culties scaling to large numbers of mentions
and limited representational power. As these
drawbacks become increasingly restrictive,
the need to replace the pairwise approaches
with a more expressive, highly scalable al-
ternative is becoming urgent. In this paper
we propose a novel discriminative hierarchical
model that recursively partitions entities into
trees of latent sub-entities. These trees suc-
cinctly summarize the mentions providing a
highly compact, information-rich structure for
reasoning about entities and coreference un-
certainty at massive scales. We demonstrate
that the hierarchical model is several orders
of magnitude faster than pairwise, allowing us
to perform coreference on six million author
mentions in under four hours on a single CPU.
1 Introduction
Coreference resolution, the task of clustering men-
tions into partitions representing their underlying
real-world entities, is fundamental for high-level in-
formation extraction and data integration, including
semantic search, question answering, and knowl-
edge base construction. For example, coreference
is vital for determining author publication lists in
bibliographic knowledge bases such as CiteSeer and
Google Scholar, where the repository must know
if the ?R. Hamming? who authored ?Error detect-
ing and error correcting codes? is the same? ?R.
Hamming? who authored ?The unreasonable effec-
tiveness of mathematics.? Features of the mentions
(e.g., bags-of-words in titles, contextual snippets
and co-author lists) provide evidence for resolving
such entities.
Over the years, various machine learning tech-
niques have been applied to different variations of
the coreference problem. A commonality in many
of these approaches is that they model the prob-
lem of entity coreference as a collection of deci-
sions between mention pairs (Bagga and Baldwin,
1999; Soon et al, 2001; McCallum and Wellner,
2004; Singla and Domingos, 2005; Bengston and
Roth, 2008). That is, coreference is solved by an-
swering a quadratic number of questions of the form
?does mention A refer to the same entity as mention
B?? with a compatibility function that indicates how
likely A and B are coreferent. While these models
have been successful in some domains, they also ex-
hibit several undesirable characteristics. The first is
that pairwise models lack the expressivity required
to represent aggregate properties of the entities. Re-
cent work has shown that these entity-level prop-
erties allow systems to correct coreference errors
made from myopic pairwise decisions (Ng, 2005;
Culotta et al, 2007; Yang et al, 2008; Rahman and
Ng, 2009; Wick et al, 2009), and can even provide
a strong signal for unsupervised coreference (Bhat-
tacharya and Getoor, 2006; Haghighi and Klein,
2007; Haghighi and Klein, 2010).
A second problem, that has received significantly
less attention in the literature, is that the pair-
wise coreference models scale poorly to large col-
lections of mentions especially when the expected
379
Name:,Jamie,Callan,Ins(tu(ons:-CMU,LTI.,Topics:{WWW,,IR,,SIGIR},
Name:Jamie,Callan,Ins(tu(ons:,Topics:-IR,
Name:,J.,Callan,Ins(tu(ons:-CMU,LTI,Topics:-WWW,
Name:,J.,Callan,Ins(tu(ons:-LTI,Topics:-WWW,
Name:,James,Callan,Ins(tu(ons:-CMU,Topics:{WWW,,IR,,largeIscale},
Coref?-
Jamie,Callan,Topics:-IR,
J.,Callan,Inst:-LTI, J.,Callan,Topic:-WWW,
J.,Callan,Inst:-CMU,Jamie,Callan,Topics:-IR, J.,Callan,Inst:-CMU, James,Callan,Topics:-WWW,Inst:CMU,
J.,Callan,Topics:-IR,Inst:-CMU,
J.,Callan,Topics:-LIS,
Figure 1: Discriminative hierarchical factor graph for coreference: Latent entity nodes (white boxes)
summarize subtrees. Pairwise factors (black squares) measure compatibilities between child and parent
nodes, avoiding quadratic blow-up. Corresponding decision variables (open circles) indicate whether one
node is the child of another. Mentions (gray boxes) are leaves. Deciding whether to merge these two entities
requires evaluating just a single factor (red square), corresponding to the new child-parent relationship.
number of mentions in each entity cluster is also
large. Current systems cope with this by either
dividing the data into blocks to reduce the search
space (Herna?ndez and Stolfo, 1995; McCallum et
al., 2000; Bilenko et al, 2006), using fixed heuris-
tics to greedily compress the mentions (Ravin and
Kazi, 1999; Rao et al, 2010), employing special-
ized Markov chain Monte Carlo procedures (Milch
et al, 2006; Richardson and Domingos, 2006; Singh
et al, 2010), or introducing shallow hierarchies of
sub-entities for MCMC block moves and super-
entities for adaptive distributed inference (Singh et
al., 2011). However, while these methods help man-
age the search space for medium-scale data, eval-
uating each coreference decision in many of these
systems still scales linearly with the number of men-
tions in an entity, resulting in prohibitive computa-
tional costs associated with large datasets. This scal-
ing with the number of mentions per entity seems
particularly wasteful because although it is common
for an entity to be referenced by a large number
of mentions, many of these coreferent mentions are
highly similar to each other. For example, in author
coreference the two most common strings that refer
to Richard Hamming might have the form ?R. Ham-
ming? and ?Richard Hamming.? In newswire coref-
erence, a prominent entity like Barack Obama may
have millions of ?Obama? mentions (many occur-
ring in similar semantic contexts). Deciding whether
a mention belongs to this entity need not involve
comparisons to all contextually similar ?Obama?
mentions; rather we prefer a more compact repre-
sentation in order to efficiently reason about them.
In this paper we propose a novel hierarchical dis-
criminative factor graph for coreference resolution
that recursively structures each entity as a tree of la-
tent sub-entities with mentions at the leaves. Our
hierarchical model avoids the aforementioned prob-
lems of the pairwise approach: not only can it jointly
reason about attributes of entire entities (using the
power of discriminative conditional random fields),
but it is also able to scale to datasets with enor-
mous numbers of mentions because scoring enti-
ties does not require computing a quadratic number
of compatibility functions. The key insight is that
each node in the tree functions as a highly compact
information-rich summary of its children. Thus, a
small handful of upper-level nodes may summarize
millions of mentions (for example, a single node
may summarize all contextually similar ?R. Ham-
ming? mentions). Although inferring the structure
of the entities requires reasoning over a larger state-
space, the latent trees are actually beneficial to in-
ference (as shown for shallow trees in Singh et
al. (2011)), resulting in rapid progress toward high
probability regions, and mirroring known benefits
of auxiliary variable methods in statistical physics
(such as Swendsen and Wang (1987)). Moreover,
380
each step of inference is computationally efficient
because evaluating the cost of attaching (or detach-
ing) sub-trees requires computing just a single com-
patibility function (as seen in Figure 1). Further,
our hierarchical approach provides a number of ad-
ditional advantages. First, the recursive nature of the
tree (arbitrary depth and width) allows the model to
adapt to different types of data and effectively com-
press entities of different scales (e.g., entities with
more mentions may require a deeper hierarchy to
compress). Second, the model contains compatibil-
ity functions at all levels of the tree enabling it to si-
multaneously reason at multiple granularities of en-
tity compression. Third, the trees can provide split
points for finer-grained entities by placing contex-
tually similar mentions under the same subtree. Fi-
nally, if memory is limited, redundant mentions can
be pruned by replacing subtrees with their roots.
Empirically, we demonstrate that our model is
several orders of magnitude faster than a pairwise
model, allowing us to perform efficient coreference
on nearly six million author mentions in under four
hours using a single CPU.
2 Background: Pairwise Coreference
Coreference is the problem of clustering mentions
such that mentions in the same set refer to the same
real-world entity; it is also known as entity disam-
biguation, record linkage, and de-duplication. For
example, in author coreference, each mention might
be represented as a record extracted from the author
field of a textual citation or BibTeX record. The
mention record may contain attributes for the first,
middle, and last name of the author, as well as con-
textual information occurring in the citation string,
co-authors, titles, topics, and institutions. The goal
is to cluster these mention records into sets, each
containing all the mentions of the author to which
they refer; we use this task as a running pedagogical
example.
Let M be the space of observed mention records;
then the traditional pairwise coreference approach
scores candidate coreference solutions with a com-
patibility function ? : M ? M ? < that mea-
sures how likely it is that the two mentions re-
fer to the same entity.1 In discriminative log-
1We can also include an incompatibility function for when
linear models, the function ? takes the form of
weights ? on features ?(mi,mj), i.e., ?(mi,mj) =
exp (? ? ?(mi,mj)). For example, in author coref-
erence, the feature functions ? might test whether
the name fields for two author mentions are string
identical, or compute cosine similarity between the
two mentions? bags-of-words, each representing a
mention?s context. The corresponding real-valued
weights ? determine the impact of these features on
the overall pairwise score.
Coreference can be solved by introducing a set of
binary coreference decision variables for each men-
tion pair and predicting a setting to their values that
maximizes the sum of pairwise compatibility func-
tions. While it is possible to independently make
pairwise decisions and enforce transitivity post hoc,
this can lead to poor accuracy because the decisions
are tightly coupled. For higher accuracy, a graphi-
cal model such as a conditional random field (CRF)
is constructed from the compatibility functions to
jointly reason about the pairwise decisions (McCal-
lum and Wellner, 2004). We now describe the pair-
wise CRF for coreference as a factor graph.
2.1 Pairwise Conditional Random Field
Each mention mi ? M is an observed variable, and
for each mention pair (mi,mj) we have a binary
coreference decision variable yij whose value de-
termines whether mi and mj refer to the same en-
tity (i.e., 1 means they are coreferent and 0 means
they are not coreferent). The pairwise compatibility
functions become the factors in the graphical model.
Each factor examines the properties of its mention
pair as well as the setting to the coreference decision
variable and outputs a score indicating how likely
the setting of that coreference variable is. The joint
probability distribution over all possible settings to
the coreference decision variables (y) is given as a
product of all the pairwise compatibility factors:
Pr(y|m) ?
n?
i=1
n?
j=1
?(mi,mj , yij) (1)
Given the pairwise CRF, the problem of coreference
is then solved by searching for the setting of the
coreference decision variables that has the highest
probability according to Equation 1 subject to the
the mentions are not coreferent, e.g., ? :M?M?{0, 1} ? <
381
Jamie,Callan, Jamie,Callan,
J.,Callan,
J.,Callan, J.,Callan,
J.,Callan, Jamie,Callan, Jamie,Callan,
v,Jamie,Callan,
J.,Callan,
v,v,v,
J.,Callan,J.,Callan, J.,Callan,
J.,Callan,
Jamie,Callan,
Figure 2: Pairwise model on six mentions: Open
circles are the binary coreference decision variables,
shaded circles are the observed mentions, and the
black boxes are the factors of the graphical model
that encode the pairwise compatibility functions.
constraint that the setting to the coreference vari-
ables obey transitivity;2 this is the maximum proba-
bility estimate (MPE) setting. However, the solution
to this problem is intractable, and even approximate
inference methods such as loopy belief propagation
can be difficult due to the cubic number of determin-
istic transitivity constraints.
2.2 Approximate Inference
An approximate inference framework that has suc-
cessfully been used for coreference models is
Metropolis-Hastings (MH) (Milch et al (2006), Cu-
lotta and McCallum (2006), Poon and Domingos
(2007), amongst others), a Markov chain Monte
Carlo algorithm traditionally used for marginal in-
ference, but which can also be tuned for MPE in-
ference. MH is a flexible framework for specify-
ing customized local-search transition functions and
provides a principled way of deciding which local
search moves to accept. A proposal function q takes
the current coreference hypothesis and proposes a
new hypothesis by modifying a subset of the de-
cision variables. The proposed change is accepted
with probability ?:
? = min
(
1,
P r(y?)
Pr(y)
q(y|y?)
q(y?|y)
)
(2)
2We say that a full assignment to the coreference variables
y obeys transitivity if ? ijk yij = 1 ? yjk = 1 =? yik = 1
When using MH for MPE inference, the second term
q(y|y?)/q(y?|y) is optional, and usually omitted.
Moves that reduce model score m y be accepted and
an optional temperature can be used for annealing.
The primary advantages of MH for coreference are
(1) only the compatibility functions of the changed
decision variables need to be evaluated to ccept a
move, and (2) the proposal function can enforce the
transitivity constraint by exploring only variable set-
tings that result in valid coreference partitionings.
A commonly used propos l distribution for coref-
erence is the following: (1) randomly select two
mentions (mi,mj), (2) if the mentions (mi,mj) are
in the same entity cluster according to y then move
one mention into a singleton cluster (by setting the
necessary decision variables to 0), otherwise, move
mention mi so it is in the same cluster as mj (by
setting the necessary decision variables). Typically,
MH is employed by first initializing to a singleton
configuration (all entities have one mention), and
then executing the MH for a certain number of steps
(or until the predicted coreference hypothesis stops
changing).
This proposal distribution always moves a sin-
gle mention m from some entity ei to another en-
tity ej and thus the configuration y and y? only dif-
fer by the setting of decision variables governing to
which entity m refers. In order to guarantee transi-
tivity and a valid coreference equivalence relation,
we must properly remove m from ei by untethering
m from each mention in ei (this requires computing
|ei| ? 1 pairwise factors). Similarly?again, for the
sake of transitivity?in order to complete the move
into ej we must coref m to each mention in ej (this
requires computing |ej | pairwise factors). Clearly,
all the other coreference decision variables are in-
dependent and so their corresponding factors can-
cel because they yield the same scores under y and
y?. Thus, evaluating each proposal for the pairwise
model scales linearly with the number of mentions
assigned to the entities, requiring the evaluation of
2(|ei|+ |ej | ? 1) compatibility functions (factors).
3 Hierarchical Coreference
Instead of only capturing a single coreference clus-
tering between mention pairs, we can imagine mul-
tiple levels of coreference decisions over different
382
granularities. For example, mentions of an author
may be further partitioned into semantically similar
sets, such that mentions from each set have topically
similar papers. This partitioning can be recursive,
i.e., each of these sets can be further partitioned, cap-
turing candidate splits for an entity that can facilitate
inference. In this section, we describe a model that
captures arbitrarily deep hierarchies over such lay-
ers of coreference decisions, enabling efficient in-
ference and rich entity representations.
3.1 Discriminative Hierarchical Model
In contrast to the pairwise model, where each en-
tity is a flat cluster of mentions, our proposed model
structures each entity recursively as a tree. The
leaves of the tree are the observed mentions with
a set of attribute values. Each internal node of the
tree is latent and contains a set of unobserved at-
tributes; recursively, these node records summarize
the attributes of their child nodes (see Figure 1), for
example, they may aggregate the bags of context
words of the children. The root of each tree repre-
sents the entire entity, with the leaves containing its
mentions. Formally, the coreference decision vari-
ables in the hierarchical model no longer represent
pairwise decisions directly. Instead, a decision vari-
able yri,rj = 1 indicates that node-record rj is the
parent of node-record ri. We say a node-record ex-
ists if either it is a mention, has a parent, or has at
least one child. Let R be the set of all existing node
records, let rp denote the parent for node r, that is
yr,rp = 1, and ?r? 6= rp, yr,r? = 0. As we describe
in more detail later, the structure of the tree and the
values of the unobserved attributes are determined
during inference.
In order to represent our recursive model of coref-
erence, we include two types of factors: pairwise
factors ?pw that measure compatibility between a
child node-record and its parent, and unit-wise fac-
tors ?rw that measure compatibilities of the node-
records themselves. For efficiency we enforce that
parent-child factors only produce a non-zero score
when the corresponding decision variable is 1. The
unit-wise factors can examine compatibility of set-
tings to the attribute variables for a particular node
(for example, the set of topics may be too diverse
to represent just a single entity), as well as enforce
priors over the tree?s breadth and depth. Our recur-
sive hierarchical model defines the probability of a
configuration as:
Pr(y, R|m) ?
?
r?R
?rw(r)?pw(r, r
p) (3)
3.2 MCMC Inference for Hierarchical models
The state space of our hierarchical model is substan-
tially larger (theoretically infinite) than the pairwise
model due to the arbitrarily deep (and wide) latent
structure of the cluster trees. Inference must simul-
taneously determine the structure of the tree, the la-
tent node-record values, as well as the coreference
decisions themselves.
While this may seem daunting, the structures be-
ing inferred are actually beneficial to inference. In-
deed, despite the enlarged state space, inference
in the hierarchical model is substantially faster
than a pairwise model with a smaller state space.
One explanatory intuition comes from the statisti-
cal physics community: we can view the latent tree
as auxiliary variables in a data-augmentation sam-
pling scheme that guide MCMC through the state
space more efficiently. There is a large body of lit-
erature in the statistics community describing how
these auxiliary variables can lead to faster conver-
gence despite the enlarged state space (classic exam-
ples include Swendsen and Wang (1987) and slice
samplers (Neal, 2000)).
Further, evaluating each proposal during infer-
ence in the hierarchical model is substantially faster
than in the pairwise model. Indeed, we can replace
the linear number of factor evaluations (as in the
pairwise model) with a constant number of factor
evaluations for most proposals (for example, adding
a subtree requires re-evaluating only a single parent-
child factor between the subtree and the attachment
point, and a single node-wise factor).
Since inference must determine the structure of
the entity trees in addition to coreference, it is ad-
vantageous to consider multiple MH proposals per
sample. Therefore, we employ a modified variant
of MH that is similar to multi-try Metropolis (Liu
et al, 2000). Our modified MH algorithm makes k
proposals and samples one according to its model
ratio score (the first term in Equation 2) normalized
across all k. More specificaly, for each MH step, we
first randomly select two subtrees headed by node-
383
records ri and rj from the current coreference hy-
pothesis. If ri and rj are in different clusters, we
propose several alternate merge operations: (also in
Figure 3):
? Merge Left - merges the entire subtree of rj into
node ri by making rj a child of ri
?Merge Entity Left - merges rj with ri?s root
?Merge Left and Collapse - merges rj into ri then
performs a collapse on rj (see below).
? Merge Up - merges node ri with node rj by cre-
ating a new parent node-record variable rp with ri
and rj as the children. The attribute fields of rp are
selected from ri and rj .
Otherwise ri and rj are subtrees in the same entity
tree, then the following proposals are used instead:
? Split Right - Make the subtree rj the root of a new
entity by detaching it from its parent
? Collapse - If ri has a parent, then move ri?s chil-
dren to ri?s parent and then delete ri.
? Sample attribute - Pick a new value for an at-
tribute of ri from its children.
Computing the model ratio for all of coreference
proposals requires only a constant number of com-
patibility functions. On the other hand, evaluating
proposals in the pairwise model requires evaluat-
ing a number of compatibility functions equal to the
number of mentions in the clusters being modified.
Note that changes to the attribute values of the
node-record and collapsing still require evaluating
a linear number of factors, but this is only linear in
the number of child nodes, not linear in the number
of mentions referring to the entity. Further, attribute
values rarely change once the entities stabilize. Fi-
nally, we incrementally update bags during corefer-
ence to reflect the aggregates of their children.
4 Experiments: Author Coreference
Author coreference is a tremendously important
task, enabling improved search and mining of sci-
entific papers by researchers, funding agencies, and
governments. The problem is extremely difficult due
to the wide variations of names, limited contextual
evidence, misspellings, people with common names,
lack of standard citation formats, and large numbers
of mentions.
For this task we use a publicly available collec-
tion of 4,394 BibTeX files containing 817,193 en-
tries.3 We extract 1,322,985 author mentions, each
containing first, middle, last names, bags-of-words
of paper titles, topics in paper titles (by running la-
tent Dirichlet alocation (Blei et al, 2003)), and last
names of co-authors. In addition we include 2,833
mentions from the REXA dataset4 labeled for coref-
erence, in order to assess accuracy. We also include
?5 million mentions from DBLP.
4.1 Models and Inference
Due to the paucity of labeled training data, we did
not estimate parameters from data, but rather set
the compatibility functions manually by specifying
their log scores. The pairwise compatibility func-
tions punish a string difference in first, middle, and
last name, (?8); reward a match (+2); and reward
matching initials (+1). Additionally, we use the co-
sine similarity (shifted and scaled between ?4 and
4) between the bags-of-words containing title to-
kens, topics, and co-author last names. These com-
patibility functions define the scores of the factors
in the pairwise model and the parent-child factors
in the hierarchical model. Additionally, we include
priors over the model structure. We encourage each
node to have eight children using a per node factor
having score 1/(|number of children?8|+1), manage
tree depth by placing a cost on the creation of inter-
mediate tree nodes ?8 and encourage clustering by
placing a cost on the creation of root-level entities
?7. These weights were determined by just a few
hours of tuning on a development set.
We initialize the MCMC procedures to the single-
ton configuration (each entity consists of one men-
tion) for each model, and run the MH algorithm de-
scribed in Section 2.2 for the pairwise model and
multi-try MH (described in Section 3.2) for the hi-
erarchical model. We augment these samplers us-
ing canopies constructed by concatenating the first
initial and last name: that is, mentions are only
selected from within the same canopy (or block)
to reduce the search space (Bilenko et al, 2006).
During the course of MCMC inference, we record
the pairwise F1 scores of the labeled subset. The
source code for our model is available as part of the
FACTORIE package (McCallum et al, 2009, http:
3http://www.iesl.cs.umass.edu/data/bibtex
4http://www2.selu.edu/Academics/Faculty/
aculotta/data/rexa.html
384
!"#!$#
!%#
!"#!$# !"#!$# !$#
&"#
!"#!$#
&"#&"#
!"#$%&'()%)*' +*,-*'.*/' +*,-*'0"$)1'.*/' +*,-*'23' +*,-*'.*/'%"4'56&&%3(*'
!"#
!"#$%&'7)%)*'
&"#&$#
!'"#
&"#&$#
!'"#
73&#)8#-9)'
&$# !'"#
56&&%3(*'
Figure 3: Example coreference proposals for the case where ri and rj are initially in different clusters.
//factorie.cs.umass.edu/).
4.2 Comparison to Pairwise Model
In Figure 4a we plot the number of samples com-
pleted over time for a 145k subset of the data. Re-
call that we initialized to the singleton configuration
and that as the size of the entities grows, the cost of
evaluating the entities in MCMC becomes more ex-
pensive. The pairwise model struggles with the large
cluster sizes while the hierarchical model is hardly
affected. Even though the hierarchical model is eval-
uating up to four proposals for each sample, it is still
able to sample much faster than the pairwise model;
this is expected because the cost of evaluating a pro-
posal requires evaluating fewer factors. Next, we
plot coreference F1 accuracy over time and show in
Figure 5a that the prolific sampling rate of the hierar-
chical model results in faster coreference. Using the
plot, we can compare running times for any desired
level of accuracy. For example, on the 145k men-
tion dataset, at a 60% accuracy level the hierarchical
model is 19 times faster and at 90% accuracy it is
31 times faster. These performance improvements
are even more profound on larger datasets: the hi-
erarchical model achieves a 60% level of accuracy
72 times faster than the pairwise model on the 1.3
million mention dataset, reaching 90% in just 2,350
seconds. Note, however, that the hierarchical model
requires more samples to reach a similar level of ac-
curacy due to the larger state space (Figure 4b).
4.3 Large Scale Experiments
In order to demonstrate the scalability of the hierar-
chical model, we run it on nearly 5 million author
mentions from DBLP. In under two hours (6,700
seconds), we achieve an accuracy of 80%, and in
under three hours (10,600 seconds), we achieve an
accuracy of over 90%. Finally, we combine DBLP
with BibTeX data to produce a dataset with almost 6
million mentions (5,803,811). Our performance on
this dataset is similar to DBLP, taking just 13,500
seconds to reach a 90% accuracy.
5 Related Work
Singh et al (2011) introduce a hierarchical model
for coreference that treats entities as a two-tiered
structure, by introducing the concept of sub-entities
and super-entities. Super-entities reduce the search
space in order to propose fruitful jumps. Sub-
entities provide a tighter granularity of coreference
and can be used to perform larger block moves dur-
ing MCMC. However, the hierarchy is fixed and
shallow. In contrast, our model can be arbitrarily
deep and wide. Even more importantly, their model
has pairwise factors and suffers from the quadratic
curse, which they address by distributing inference.
The work of Rao et al (2010) uses streaming
clustering for large-scale coreference. However, the
greedy nature of the approach does not allow errors
to be revisited. Further, they compress entities by
averaging their mentions? features. We are able to
provide richer entity compression, the ability to re-
visit errors, and scale to larger data.
Our hierarchical model provides the advantages
of recently proposed entity-based coreference sys-
tems that are known to provide higher accuracy
(Haghighi and Klein, 2007; Culotta et al, 2007;
Yang et al, 2008; Wick et al, 2009; Haghighi and
Klein, 2010). However, these systems reason over a
single layer of entities and do not scale well.
Techniques such as lifted inference (Singla and
Domingos, 2008) for graphical models exploit re-
dundancy in the data, but typically do not achieve
any significant compression on coreference data be-
385
Samples versus Time
0 500 1,000 1,500 2,000
Running time (s)
0
50,000
100,000
150,000
200,000
250,000
300,000
350,000
400,000
Nu
mb
er o
f Sa
mp
les
Hierar Pairwise
(a) Sampling Performance
Accuracy versus Samples
0 50,000 100,000 150,000 200,000
Number of Samples
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
F1 
Acc
ura
cy
Hierar Pairwise
(b) Accuracy vs. samples (convergence accuracy as dashes)
Figure 4: Sampling Performance Plots for 145k mentions
Accuracy versus Time
0 250 500 750 1,000 1,250 1,500 1,750 2,000
Running time (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
F1 
Ac
cur
acy
Hierar Pairwise
(a) Accuracy vs. time (145k mentions)
Accuracy versus Time
0 10,000 20,000 30,000 40,000 50,000 60,000
Running time (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
F1 
Acc
ura
cy
Hierar Pairwise
(b) Accuracy vs. time (1.3 million mentions)
Figure 5: Runtime performance on two datasets
cause the observations usually violate any symmetry
assumptions. On the other hand, our model is able
to compress similar (but potentially different) obser-
vations together in order to make inference fast even
in the presence of asymmetric observed data.
6 Conclusion
In this paper we present a new hierarchical model
for large scale coreference and demonstrate it on
the problem of author disambiguation. Our model
recursively defines an entity as a summary of its
children nodes, allowing succinct representations of
millions of mentions. Indeed, inference in the hier-
archy is orders of magnitude faster than a pairwise
CRF, allowing us to infer accurate coreference on
six million mentions on one CPU in just 4 hours.
7 Acknowledgments
We would like to thank Veselin Stoyanov for his feed-
back. This work was supported in part by the CIIR, in
part by ARFL under prime contract #FA8650-10-C-7059,
in part by DARPA under AFRL prime contract #FA8750-
09-C-0181, and in part by IARPA via DoI/NBC contract
#D11PC20152. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon.
Any opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
386
References
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: annotations, experiments, and ob-
servations. In Proceedings of the Workshop on Coref-
erence and its Applications, CorefApp ?99, pages 1?8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
Dirichlet model for unsupervised entity resolution. In
SDM.
Mikhail Bilenko, Beena Kamath, and Raymond J.
Mooney. 2006. Adaptive blocking: Learning to scale
up record linkage. In Proceedings of the Sixth Interna-
tional Conference on Data Mining, ICDM ?06, pages
87?96, Washington, DC, USA. IEEE Computer Soci-
ety.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal on Machine
Learning Research, 3:993?1022.
Aron Culotta and Andrew McCallum. 2006. Prac-
tical Markov logic containing first-order quantifiers
with application to identity uncertainty. In Human
Language Technology Workshop on Computationally
Hard Problems and Joint Inference in Speech and Lan-
guage Processing (HLT/NAACL), June.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT).
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 848?855.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Mauricio A. Herna?ndez and Salvatore J. Stolfo. 1995.
The merge/purge problem for large databases. In Pro-
ceedings of the 1995 ACM SIGMOD international
conference on Management of data, SIGMOD ?95,
pages 127?138, New York, NY, USA. ACM.
Jun S. Liu, Faming Liang, and Wing Hung Wong. 2000.
The multiple-try method and local optimization in
metropolis sampling. Journal of the American Statis-
tical Association, 96(449):121?134.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems (NIPS).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 169?178.
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2006.
BLOG: Relational Modeling with Unknown Objects.
Ph.D. thesis, University of California, Berkeley.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31:705?767.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Annual Meeting of the Association for Computational
Linguistics (ACL).
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In AAAI Conference
on Artificial Intelligence, pages 913?918.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 968?977, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In International Conference on Computa-
tional Linguistics (COLING), pages 1050?1058, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham
Clinton the president? disambiguating names across
documents. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 9?16.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sameer Singh, Michael L. Wick, and Andrew McCallum.
2010. Distantly labeling data for large scale cross-
document coreference. Computing Research Reposi-
tory (CoRR), abs/1005.4298.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
387
Parag Singla and Pedro Domingos. 2005. Discrimina-
tive training of Markov logic networks. In AAAI, Pitts-
burgh, PA.
Parag Singla and Pedro Domingos. 2008. Lifted first-
order belief propagation. In Proceedings of the 23rd
national conference on Artificial intelligence - Volume
2, AAAI?08, pages 1094?1099. AAAI Press.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27(4):521?544.
R.H. Swendsen and J.S. Wang. 1987. Nonuniversal crit-
ical dynamics in MC simulations. Phys. Rev. Lett.,
58(2):68?88.
Michael Wick, Aron Culotta, Khashayar Rohanimanesh,
and Andrew McCallum. 2009. An entity-based model
for coreference resolution. In SIAM International
Conference on Data Mining (SDM).
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In Association for Computational Linguistics,
pages 843?851.
388
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 153?162,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Dynamic Knowledge-Base Alignment for Coreference Resolution
Jiaping Zheng Luke Vilnis Sameer Singh Jinho D. Choi Andrew McCallum
School of Computer Science
University of Massachusetts
Amherst MA 01003
{jzheng,luke,sameer,jdchoi,mccallum}@cs.umass.edu
Abstract
Coreference resolution systems can benefit
greatly from inclusion of global context,
and a number of recent approaches have
demonstrated improvements when precom-
puting an alignment to external knowledge
sources. However, since alignment itself
is a challenging task and is often noisy, ex-
isting systems either align conservatively,
resulting in very few links, or combine the
attributes of multiple candidates, leading
to a conflation of entities. Our approach
instead performs joint inference between
within-document coreference and entity
linking, maintaining ranked lists of candi-
date entities that are dynamically merged
and reranked during inference. Further, we
incorporate a large set of surface string vari-
ations for each entity by using anchor texts
from the web that link to the entity. These
forms of global context enables our system
to improve classifier-based coreference by
1.09 B3 F1 points, and improve over the
previous state-of-art by 0.41 points, thus
introducing a new state-of-art result on the
ACE 2004 data.
1 Introduction
Coreference resolution is the task of identifying
sets of noun phrase mentions from a document
that refer to the same real-world entities. For ex-
ample, in the following excerpt: ?The Chicago
suburb of Arlington Heights is the first stop for
?George W. Bush?1 today. ?The Texas governor?2
stops in ?Gore?s home state?3 of ?Tennessee?4 this
afternoon. . . ?, (m1,m2) and (m3,m4) define the
coreferent pairs. Coreference resolution forms an
important component for natural language process-
ing and information extraction pipelines due to its
utility in relation extraction, cross-document coref-
erence, text summarization, and question answer-
ing. The task of coreference is challenging for
automated systems as the local information con-
tained in the document is often not enough to accu-
rately disambiguate mentions, for example, corefer-
encing (m1,m2) requires identifying that George
W. Bush (m1) is the governor of Texas (m2), and
similarly for (m3,m4). External knowledge-bases
such as FrameNet (Baker et al, 1998), Wikipedia,
Yago (Suchanek et al, 2007), and Freebase (Bol-
lacker et al, 2008), can be used to provide global
context, and there is a strong need for coreference
resolution systems to accurately use such sources
for disambiguation.
Incorporating external knowledge bases into
coreference has been the subject of active recent
research. Ponzetto and Strube (2006) and Ratinov
and Roth (2012) precompute a fixed alignment of
the mentions to the knowledge base entities. The
attributes of these entities are used during corefer-
ence by incorporating them in the mention features.
Since alignment of mentions to the external enti-
ties is itself a difficult task, these systems favor
high-precision linking. Unfortunately, this results
in fewer alignments, and improvements are only
shown on mentions that are easier to align and core-
fer (such as the non-transcript documents in Rati-
nov and Roth (2012)). Alternatively, Rahman and
Ng (2011) link each mention to multiple entities in
the knowledge base, improving recall at the cost
of lower precision; the attributes of all the linked
entities are aggregated as features. Although this
approach is more robust to noise in the documents,
the features of a mention merge the different as-
pects of the entities, for example a ?Michael Jordan?
mention will contain features for both the scientist
and basketball personas.
Instead of fixing the alignment of the mentions to
the knowledge base, our proposed approach main-
tains a ranked list of candidate entities for each
mention. To expand the set of surface strings that
153
may be used to refer to each entity, the attributes
of each candidate contain anchor texts (the visible
text) of the links on the web that refer to that entity
candidate. When mentions are compared during
inference, we use the features computed from the
top ranked entity candidate of the antecedent men-
tion. As mentions are merged, the ranked lists of
candidate entities are also merged and reranked, of-
ten changing the top-ranked entity candidate used
in subsequent comparisons. The large set of sur-
face string variations and constant reranking of the
entity candidates during inference allows our ap-
proach to correct mistakes in alignment and makes
external information applicable to a wider variety
of mentions.
Our paper provides the following contributions:
(1) an approach that jointly reasons about both
within-doc entities and their alignment to KB-
entities by dynamically adjusting a ranked list of
candidate alignments, during coreference, (2) Uti-
lization of a larger set of surface string variations
for each entity candidate by using links that appear
all over the web (Spitkovsky and Chang, 2012), (3)
A combination of these approaches that improves
upon a competitive baseline without a knowledge
base by 1.09 B3 F1 points on the ACE 2004 data,
and outperforms the state-of-the-art coreference
system (Stoyanov and Eisner, 2012) by 0.41 B3
F1 points, and (4) Accurate predictions on docu-
ments that are difficult for coreference, such as the
transcript documents that were omitted from the
evaluation in Ratinov and Roth (2012), and docu-
ments that contain a large number of mentions.
2 Baseline Pairwise System
In this section we describe a variant of a commonly-
used coreference resolution system that does not
utilize external knowledge sources. This widely
adopted model casts the problem as a series of
binary classifications (Soon et al, 2001; Ng and
Cardie, 2002; Ponzetto and Strube, 2006; Bengston
and Roth, 2008; Stoyanov et al, 2010). Given
a document with its mentions, the system itera-
tively checks each mention mj for coreference with
preceding mentions using a classifier. A corefer-
ence link may be created between mj and one of
these preceding mentions using one of the follow-
ing strategies. The CLOSESTLINK (Soon et al,
2001) method picks the closest mention to mj that
is positively classified, while the BESTLINK (Ng
and Cardie, 2002) method links mj to the preced-
Types Features
String-
Similarity
mention string match, head string match,
head substring match, head word pair, men-
tion substring match, acronym
Syntax number match, gender match, apposition,
relative pronoun, mention type, modifier
match, head word POS tags
Semantic synonym, antonym, hypernym, modifier re-
lations, both mentions are surrounded by a
verb meaning ?to say?, demonym match
Other predicted entity type, predicted entity type
match, both mentions in same sentence, sen-
tence/token distance, capitalization
Table 1: Features of the baseline model. Extensions
to Bengston and Roth (2008) are italicized.
ing mention that was scored the highest. If none
of the preceding mentions are classified as positive
(for CLOSESTLINK), or are above a threshold (for
BESTLINK), then mj is left unlinked. After all the
mentions have been processed, the links are used
to generate a transitive closure that corresponds to
the recognized entities in the document.
2.1 Pairwise Mention Features
The features used to train our classifier are similar
to those in Bengston and Roth (2008), including
lexical, syntactical, semantic, predicted NER types,
etc., with the exclusion of their ?learned features?
that require additional classifiers. Further, we in-
clude features that compare the mention strings, the
distance between the two mentions in terms of the
number of sentences and tokens, and the POS tags
of the head words. We also use the conjunctions of
these features as in Bengston and Roth (2008), as
well as the BESTLINK approach. The complete set
of features are listed in Table 1.
The training for our system is similar
to Bengston and Roth (2008). The positive train-
ing examples are generated from mentions and
their immediate preceding antecedent. The neg-
ative examples are generated from mentions and
all their preceding non-coreferent mentions. If the
mention is not a pronoun, preceding pronouns are
not used to create training examples, and they are
also excluded during inference. In contrast to aver-
aged perceptron used in Bengston and Roth (2008),
our baseline system is trained using hinge-loss, `2-
regularized SVM.
2.2 Merging Pairwise Features
When a mention mj is compared against a preced-
ing mention mi, information from other mentions
154
that are already coreferent with mi may be helpful
in disambiguating mj as they may contain infor-
mation that is not available from mi. Let M be
the mentions between mi and mj that are coref-
erent with mi. Let mq ? M be the mention that
is closest to mj . All the features from the pair
(mq,mj), except those that characterize one men-
tion (for example, mention type of mj), are added
to the features between (mi,mj). This extends a
similar approach by Lee et al (2011) that merges
only the attributes of mentions (such as gender, but
not all pairwise features).
2.3 Pruning Comparisons During Training
A potential drawback of including all the negative
examples as in Bengston and Roth (2008) is that
the negative instances far outnumber the positive
ones, which is challenging for training a classifier.
In their system, the positive training examples only
constitute 1.6% of the total training instances. By
contrast, Soon et al (2001) reduce the number of
negative instances by using only mentions between
the mention and its closest coreferent pair as neg-
ative examples. Instead of just using the closest
coreferent mention, we extend this approach to
use the k closest of coreferent preceding mentions,
where k is tuned using the development data.
3 Dynamic Linking to Knowledge-Base
In this section, we describe our approach to coref-
erence resolution that incorporates external knowl-
edge sources. The approach is an extension of the
pairwise model described earlier, with the inclusion
of a ranked list of entities, and using a larger set of
surface string variations.
3.1 Algorithm
We describe our overall approach in Algorithm 1.
The system assumes that the data is annotated with
true mention boundaries and mention types. We
additionally tokenize the document text and tag the
tokens with their parts of speech for use as features.
First, an empty entity candidate list is created for
each mention in the document. For each proper
noun mention, we query a knowledge base for an
ordered list of Wikipedia articles that may refer
to it, and add these to the mention?s candidate list.
Other mentions? candidates lists are left empty.
After this pre-processing, each mention mi
is compared against its preceding mentions
m1 . . .mi?1 and their top-ranked entity candi-
Algorithm 1 Dynamic Linking to Wikipedia
1: Input: Mentions {mj}
2: Initialize blank entity lists {Em} . Section 3.2
3: for m ? Proper Noun Mentions do
4: LINKWIKIPEDIA(m, Em) . Section 3.2
5: POPULATEENTITYATTRS(Em) . Section 3.3
6: end for
7: for mi ?Mentions do
8: Antecedents? {m1...mi?1}
9: for m? ? Antecedents do
10: t? TOPRANKEDATTRS(Em?) . Section 3.4
11: s? SCORE(m?, mi, t) . Section 3.4
12: Scoresm?? s
13: end for
14: m? ? argmaxm? Scoresm?15: if Scoresm? > threshold then
16: MARKCOREFERENT(m?, mi)
17: MERGEENTITYLISTS(Em? , Emi ) . Section 3.418: end if
19: end for
20: return Coreferent mention clusters
date using a classifier. Amongst antecedents
m1 . . .mi?1 that score above a threshold, the
highest-scoring one mj is marked as coreferent
with mi and the two candidate lists that correspond
to mi and mj are merged. Merging two mentions
results in the merging and reranking of their respec-
tive entity candidate lists, described below. If no
antecedents score above a threshold, we leave the
mention in its singleton cluster.
3.2 Linking to Wikipedia
To create the initial entity candidate lists for
proper noun mentions, we query a knowledge base
searcher (Dalton and Dietz, 2013) with the text
of these mentions. These queries return scored,
ranked lists of entity candidates (Wikipedia arti-
cles), which we associate with each proper noun
mention, leaving the rest of the candidate lists
empty. Linking is often noisy, so only selecting the
high-precision links as in Ratinov and Roth (2012)
results in too few matches, while picking an aggre-
gation of all links results in more noise due to lower
precision (Rahman and Ng, 2011). Additionally,
since linking is often performed in pre-processing,
two mentions that are determined coreferent dur-
ing inference could still be linked to different KB
entities. To avoid these problems, we keep a list of
candidate links for each mention, merging the lists
when two mentions are determined coreferent, and
rerank this list during inference.
3.3 Populating Entity Attributes
After linking to Wikipedia, we have a list of can-
didate KB entities for each mention. Each entity
155
has access to external information keyed on the
Wikipedia article, but this information could more
generally come from any knowledge base. Given
these entities, there are many possible features that
may be used for disambiguation of the mentions,
such as gender and fine-grained Wikipedia cate-
gories as used by Ratinov and Roth (2012), how-
ever most of these features may not be relevant to
the task of within-document coreference. Instead,
an important resource for linking non-proper men-
tions of an entity is to identify the possible name
variations of the entity. For example, it would be
useful to know that Massachusetts is also referred
to as ?The 6th State?, however this information is
not readily available from Wikipedia.1
We instead use the corpus described
in Spitkovsky and Chang (2012) that con-
sists of anchor texts of links to Wikipedia that
appear on web pages. This collection of anchor
texts is sufficiently extensive to cover many
common misspellings of entity names, as well as
many name variations missing from Wikipedia.
For example, for the entity ?Massachusetts?, our
anchor texts include misspellings like ?Massachus-
setts? and ?Messuchusetts?, and the (debatably)
affectionate nickname of ?Taxachusetts??none of
which are found in Wikipedia. Using these anchor
texts, each entity candidate provides a rich set of
name variations that we use for disambiguation, as
described in the next section.
3.4 Inference with Dynamic Linking
The input to our inference algorithm consists of a
number of mentions, a list of ranked entity candi-
dates for the proper noun mentions that are present
in the KB, and a list of attributes (in this case, name
variations) for each entity candidate.
Scoring: Our underlying model is a pairwise
classification approach as described in Section 2.
Similar to existing coreference systems such as
Bengston and Roth (2008) and Rahman and Ng
(2011), we perform coreference resolution using
greedy left-to-right pairwise mention classification,
clustering each mention with its highest-scoring
antecedent (or leaving it as a singleton temporarily
if no score is above a threshold). We add the same
additional features and perform feature merging
operation (Section 2.2) as in our baseline system.
1Some of this information is available as redirects and
from links within Wikipedia, however these do not accurately
reflect all the variations of the name.
The top-ranked entity candidate of the an-
tecedent mention is used during coreference to
provide additional features for the pairwise classi-
fier. Only using the top-ranked entity candidate al-
lows the system to maintain a consistent one entity
per cluster hypothesis, reducing the noise resulting
from conflated entities. The attributes for this top-
ranked entity consist of name variations. We add a
binary feature, and conjunctions of this with other
features, if the text of the right mention matches
one of these name variations.
Entity List Merging: Once a mention pair is
scored as coreferent, their corresponding entity can-
didates are merged. Merging is performed by sim-
ply combining the two lists of candidates. Note that
there is only one candidate list for a given group of
coreferent mentions at any point in inference: if m1
and m2 have been previously marked as coreferent,
and m3 is marked as coreferent with m2, m1?s en-
tity candidates will then contain those from m3 for
future classification decisions.
Re-Ranking: After the two entity candidate lists
are merged, we rerank the candidates to identify
the top-ranked one. We sort the new list of candi-
date entities by the number of times each candidate
occurs in the list, breaking ties by their original
relevance from the KB. For example, if two men-
tions disagree on the top-ranked KB search result,
but agree on the second one, after being clustered
they will both use the second search result when
creating feature vectors for future coreference de-
cisions. Even though other candidates besides the
top-ranked one are ignored for a single classifica-
tion decision, they may become top-ranked after
merging with later candidate sets.
This approach allows our system to use the inter-
mediate results of coreference resolution to re-link
mentions to KB entities, reducing the noise and
contradictory features from incorrect links. Addi-
tionally, features from the KB are added to non-
proper noun mentions once those mentions are
linked with a populated entity, allowing the results
of coreference to enrich non-proper noun mentions
with KB-based features. The initial proper noun
queries effectively seed the linking process, and
KB data is then dynamically spread to the other
mentions through coreference.
3.5 Example
We describe a run of our approach on an exam-
ple in Figure 1. Consider three mentions, each
156
?about navigation charts that he had 
ordered from a company based in the 
state of Washington. He assumed ?
?opened one of them to discover the 
absentee ballot of Steven H. Forrester 
of Bellevue, Wash?.
...were not meaningful because 
counting in Washington State has 
been completed...
(a) Example Excerpts with Mentions
Washington, DC
Washington State
...
Car Wash
The Wash
...
Washington State
Washington State
...
Washington
Wash
Washington 
State
(b) Initial Alignment (top-ranked in bold)
Washington State
Washington, DC
Car Wash
The Wash
...
Washington State
...
Washington
Wash
Washington 
State
(c) Merged and Reranked Alignment
Figure 1: Example of Dynamic Alignment
paired with a top-ranked KB candidate: ?Washing-
ton?, ?Wash?, and ?Washington State?. For the
first two mentions, clearly the top entity candidate
is incorrect; hence approaches that rely on a fixed
alignment will perform poorly. In particular, since
?Washington State? mention is not compatible with
the top-ranked entities of the first two mentions
(Washington, D.C. and Car Wash respectively), ap-
proaches that do not modify the ranking during
inference may not resolve them. However, the cor-
rect candidate Washington State does appear in the
candidate entities of the first two mentions, albeit
with a lower rank. In our approach, clustering
the first two mentions causes the shared candidate
Washington State to move to the top of the list. The
coreference system is now able to easily identify
that the ?Washington State? mention is compati-
ble with the Washington State entity formed by the
previous two mentions, providing evidence that the
final mention should be clustered with either of
them in subsequent comparisons.
4 Experiments
4.1 Setup
We evaluate our system on the ACE 2004 anno-
tated dataset (Doddington et al, 2004). Following
the setup in Bengston and Roth (2008), we split
the corpus into training, development, and test sets,
resulting in 268 documents in the train set, 107
documents in the test set, and 68 documents in the
development set. The data is processed using stan-
dard open source tools to segment the sentences
and tokenize the corpus, and using the OpenNLP2
tagger to obtain the POS tags. The hyperparame-
ters of our system, such as regularization, initial
number of candidates, and the number of compar-
2http://opennlp.apache.org/
isons during training (k in Section 2.3) are tuned
on the development data when trained on the train
set. The models we use to evaluate on the test data
set are trained on the training and development sets,
following the standard evaluation for coreference
first used by Culotta et al (2007).
To provide the initial ranked list of entity candi-
dates from Wikipedia, we query the KB Bridge sys-
tem (Dalton and Dietz, 2013) with the proper name
mentions. KB Bridge is an information-retrieval-
based entity linking system that connects the query
mentions to Wikipedia entities using a sequential
dependence model. This system has been shown to
match or outperform the top performing systems in
the 2012 TAC KBP entity linking task.
4.2 Methods
Our experiments investigate a number of baselines
that are similar or identical to existing approaches.
Wikipedia Linking: As a simple baseline, we
directly evaluate the quality of the alignment for
coreference by merging all pairs of proper noun
mentions that share at least one common candi-
date, as per KB bridge. Further, the non-pronoun
mentions are linked to these proper nouns if the
mention string matches any of the entity titles or
anchor texts.
Bengston and Roth (2008): A pairwise corefer-
ence model containing a rich set of features, as de-
scribed and evaluated in Bengston and Roth (2008).
Baseline: Our implementation of a pairwise
model that is similar to the approach in Bengston
and Roth (2008) with the differences described in
Section 2. This is our baseline system that performs
coreference without the use of external knowledge.
Incidentally, it outperforms Bengston and Roth
(2008).
Dynamic linking: This is our complete system as
157
described in Section 3, in which the list of candi-
dates associated with each mention is reranked and
modified during inference.
Static linking: Identical to dynamic linking ex-
cept that entity candidate lists are not merged dur-
ing inference (i.e., Algorithm 1 without line 17).
This approach is comparable to the fixed alignment
model, as in the approaches of Ponzetto and Strube
(2006) and Ratinov and Roth (2012).
4.3 Results
As in Bengston and Roth (2008), we evaluate our
system primarily using the B3 metric (Bagga and
Baldwin, 1998), but also include pairwise, MUC
and CEAF(m) metrics. The performance of our
systems on the test data set is shown in Table 2.
These results use true mentions provided in the
dataset, since, as suggested by Ng (2010), corefer-
ence resolvers that use different mention detectors
(extraction from parse tree, detector trained from
gold boundaries, etc.) should not be compared.
Our baseline system outperforms Bengston and
Roth (2008) by 0.32 B3 F1 points on this data set.
Incorporating Wikipedia and anchor text informa-
tion from the web with a fixed alignment (static
linking) further improves our performance by 0.54
B3 F1 points. Using dynamic linking, which im-
proves the alignment during inference, achieves
another 0.55 F1 point improvement, which is 1.09
F1 above our baseline, 1.41 F1 above the current
best pairwise classification system (corresponding
to an error reduction of 7.4%), and 0.4 F1 above the
current state-of-art on this dataset (Stoyanov and
Eisner, 2012). The improvement of the dynamic
linking approach over our baselines is consistent
across the various evaluation metrics.
5 Discussion
We also explore our system?s performance on sub-
sets of the ACE dataset, and on the OntoNotes
dataset.
5.1 Document Length
Coreference becomes more difficult as the number
of mentions is increased since the number of pair-
wise comparisons increases quadratically with the
number of mentions. We observe this phenomenon
in our dataset: the performance on the smallest
third of the documents (when sorted according to
number of mentions) is 8.5-10% higher than on the
largest third of the documents, as per the B3 metric.
55   10 15 20 25 30 35 40 45 50
   
   
0.6
0.8
1
1.2
1.4
1.6
1.8
Top X% of Docs by Number of Mentions
Im
pro
vem
en
t o
ver
 Ba
sel
ine
Dynamic Linking
Static Linking
Figure 2: Improvements on the top X% of docu-
ments ranked by the number of mentions.
Method Non-Transcripts Transcripts
Baseline 82.50 79.77
RR 2012 83.03 -
Static Linking 83.06 80.25
Dynamic Linking 83.32 81.13
Table 3: B3 F1 accuracy on transcripts and non-
transcripts from the ACE test data. RR 2012 only
evaluate on non-transcripts.
However, we expect dynamic linking of entities to
be more beneficial on these larger documents as
our system can use the information from a larger
number of mentions to improve the alignment dur-
ing inference. Static linking, on the other hand, is
unlikely to obtain higher improvements with the
larger number of mentions in the document as the
alignment is fixed.
We perform the following experiment to analyze
the performance with varying numbers of mentions.
We sort all the documents in the test set according
to their number of mentions, and evaluate on the top
X% of this list (where X is 10, 33, 40, 50). As the
results demonstrate in Figure 2, the improvement
of the static linking approach stays fairly even as
X is varied. Even though the experiments suggest
that the larger documents are tougher to corefer-
ence,3 dynamic linking provides higher improve-
ments when the documents contain a larger number
of mentions.
5.2 Performance on Transcripts
The quality of alignment and the coreference pre-
dictions for a document is influenced by the quality
of the mentions in the document. In particular,
3i.e., the absolute values are lower for these splits. The
baseline system obtains 83.08, 79.29, 79.64, and 79.77 respec-
tively for X = 10, 33, 40, 50.
158
Method Pairwise MUC CEAF B
3
P / R F1 P / R F1 P / R F1 P / R F1
Culotta et al (2007) - - - - - - 86.7 73.2 79.3
Raghunathan et al (2010) 71.6 46.2 56.1 80.4 71.8 75.8 - - 86.3 75.4 80.4
Stoyanov and Eisner (2012) - - - 80.1 - - - 81.8
Wiki-linking 64.15 14.99 24.30 74.41 28.39 41.10 58.54 58.4 58.47 92.89 57.21 70.81
Bengston and Roth (2008) - - 82.7 69.9 75.8 - - 88.3 74.5 80.8
Baseline 66.56 47.07 55.14 82.84 72.02 77.05 75.58 75.40 75.49 87.02 75.97 81.12
Static Linking 82.53 40.80 54.61 88.39 66.93 76.18 75.33 75.35 75.44 93.10 72.72 81.66
Dynamic Linking 72.20 47.40 57.23 85.07 72.02 78.01 76.55 76.37 76.46 89.37 76.12 82.21
Table 2: Evaluation on the ACE test data, with the system trained on the train and development sets.
   
   
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Im
pro
vem
en
t o
ver
 Ba
sel
ine
non-trans
non-trans     
transcripts
Static Linking Dynamic Linking
trans
Figure 3: Comparison on the transcripts data.
ACE contains a large number of broadcast news
documents, many of which consist of transcribed
data containing noise in the form of incomplete
sentences and disfluencies. Since these transcripts
provide an additional challenge for alignment and
coreference, Ratinov and Roth (2012) only use the
set of non-transcripts for their evaluation.
Using dynamic linking and a large set of surface
string variations, our approach may be able to pro-
vide an improvement even on the transcripts. To
identify the transcripts in the test set, we use the
approximation from Ratinov and Roth (2012) that
considers a document to be non-transcribed if it
contains proper noun mentions and at least a third
of those start with a capital letter. The performance
is shown in Table 3, while the improvement over
our baseline is shown in Figure 3.
Our static linking matches the performance of
Ratinov and Roth (2012) on the non-transcripts.
Further, the improvement of static linking on the
transcripts over the baseline is lower than that on
the non-transcript data, suggesting that noisy men-
tions and text result in poor quality alignment. Dy-
namic linking, on the other hand, not only outper-
forms all other systems, but also shows a higher im-
provement over the baseline on the transcripts than
on non-transcripts. This indicates that dynamic
linking approach is robust to noise, and its wider
variety of surface strings and flexible alignments
are especially useful for transcripts.
5.3 OntoNotes
We also run our systems on the OntoNotes dataset,
which was used for evaluation in CoNLL 2011
Shared Task (Pradhan et al, 2011). The dataset
consists of 2083 documents from a much larger va-
riety of genres, such as conversations, magazines,
web text, etc. Further, the dataset alo consists of
mentions that refer to events, most of which do not
appear as Wikipedia pages. Since only the non-
singleton mentions are annotated in the training set,
we also include additional noun phrase mentions
during training. We obtain B3 F1 of 65.3, 67.6, and
67.7 for our baseline, static linking, and dynamic
linking respectively.4 When compared to the par-
ticipants of the closed task, the dynamic linking
system outperforms all but two on this metric, sug-
gesting that dynamic alignment is beneficial even
when the features have not been engineered for
events or for different genres.
6 Related Work
Within-document coreference has been well-
studied for a number of years. A variety of ap-
proaches incorporate linguistic knowledge as rules
iteratively applied to identify the chains, such
as Haghighi and Klein (2009), Raghunathan et
al. (2010), Stoyanov et al (2010). Alternatively
(and similar to our approach), others represent this
knowledge as features in a machine learning model.
Early applications of such models include Soon et
al. (2001), Ng and Cardie (2002) and (Bengston
and Roth, 2008). There are also a number of tech-
niques that represent entities explicitly (Culotta et
4with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 &
49.8, respectively for baseline, static and dynamic linking.
159
al., 2007; Wick et al, 2009; Haghighi and Klein,
2010; Stoyanov and Eisner, 2012).
This work is an extension of recent approaches
that incorporate external knowledge sources to im-
prove within-document coreference. Ponzetto and
Strube (2006) identify Wikipedia candidates for
each mention as a preprocessing step, and incor-
porate them as features in a pairwise model. Our
method differs in that we draw such features from
entity candidates during inference, and also main-
tain and update a set of candidate entity links
instead of selecting only one. Rahman and Ng
(2011) introduce similar features from a more ex-
tensive set of knowledge sources (such as YAGO
and FrameNet) into a cluster-based model whose
features change as inference proceeds. However,
the features for each cluster come from a combina-
tion of all entities aligned to the cluster mentions.
We improve upon this approach by maintaining a
list of the candidate entities for each mention clus-
ter, modifying this list during the course of infer-
ence, and using features from only the top-ranked
candidate at any time. Further, they do not provide
a comparison on a standard dataset.
Ratinov and Roth (2012) extend the multi-sieve
coreference model (Raghunathan et al, 2010) by
identifying at most a single candidate for each men-
tion, and incorporating high-precision attributes
extracted from Wikipedia. The high-precision
mention-candidate pairings are precomputed and
fixed; additionally, the features for an entity are
based on the predictions of the previous sieves, thus
fixed while a sieve is applied. With these restric-
tions, they show improvements over the state-of-
the-art on a subset of ACE mentions that are more
easily aligned to Wikipedia, while our approach
demonstrates improvements on the complete set of
mentions including the tougher to link mentions
from the transcripts.
There are a number of approaches that provide
an alignment from mentions in a document to
Wikipedia. Wikifier (Ratinov et al, 2011) analyzes
the context around the mentions and the entities
jointly, and was used to align mentions for corefer-
ence in Ratinov and Roth (2012). Dalton and Dietz
(2013) introduce an approximation to the above ap-
proach, but incorporate retrieval-based supervised
reranking that provides multiple candidates and
scores; this approach performed competitively on
previous TAC-KBP entity linking benchmarks (Di-
etz and Dalton, 2012). Alignment to an external
knowledge-base has improved performance for a
number of NLP and information extraction tasks,
such as named-entity recognition (Cucerzan, 2007;
Han and Zhao, 2009), cross-document corefer-
ence (Finin et al, 2009; Singh et al, 2010), and
relation-extraction (Riedel et al, 2010; Hoffmann
et al, 2011).
7 Conclusions
In this paper, we incorporate external knowledge to
improve within-document coreference. Instead of
fixing the alignment a priori, our approach main-
tains a ranked list of candidate entities for each
mention, and merges and reranks the list during
inference. Further, we consider a large set of sur-
face string variations for each entity by using an-
chor texts from the web. These external sources
allow our system to achieve a new state-of-the-art
on the ACE data. We also demonstrate improve-
ments on documents that are difficult for alignment
and coreference, such as transcripts and documents
containing a large number of mentions.
A number of possible avenues for future study
are apparent. First, our alignment to a knowledge-
base can benefit from more document-aware link-
ing to entities, such as the Wikifier (Ratinov et al,
2011). Second, we would like to augment mention
features with additional information available from
the knowledge base, such as Wikipedia categoriza-
tion and gender attributes. We also want to investi-
gate a cluster ranking model, as used in (Rahman
and Ng, 2011; Stoyanov and Eisner, 2012), to ag-
gregate the features of all the coreferent mentions
as inference progresses.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval, in part by
DARPA under agreement number FA8750-13-2-
0020, in part by NSF medium IIS-0803847 and
in part by an award from Google. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprint for Governmental purposes notwithstanding
any copyright annotation thereon. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are the authors? and neces-
sarily those of the sponsor.
160
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In International
Conference on Language Resources and Evaluation
(LREC) Workshop on Linguistics Coreference, pages
563?566.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, SIGMOD ?08, pages 1247?1250, New York,
NY, USA. ACM.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 708?716.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for corefer-
ence resolution. In North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies (NAACL HLT).
Jeffrey Dalton and Laura Dietz. 2013. A neighbor-
hood relevance model for entity linking. In Open
Research Areas in Information Retrieval (OAIR).
Laura Dietz and Jeffrey Dalton. 2012. Across-
document neighborhood expansion: UMass at TAC
KBP 2012 entity linking. In Text Analysis Confer-
ence (TAC).
G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The Automatic Content Extraction (ACE)
program?tasks, data, and evaluation. In Pro-
ceedings of LREC, volume 4, pages 837?840.
Citeseer.
Tim Finin, Zareen Syed, James Mayfield, Paul Mc-
Namee, and Christine Piatko. 2009. Using Wiki-
tology for cross-document entity coreference resolu-
tion. In AAAI Spring Symposium on Learning by
Reading and Learning to Read.
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic fea-
tures. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Xianpei Han and Jun Zhao. 2009. Named entity disam-
biguation by leveraging Wikipedia semantic knowl-
edge. In Conference on Information and Knowledge
Management (CIKM), pages 215?224.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 541?550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford?s multi-pass sieve
coreference resolution system at the CoNLL-2011
shared task. In Conference on Computational
Natural Language Learning (CoNLL), pages 28?34.
Association for Computational Linguistics.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 104?111.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: the first fifteen years. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 1396?
1411, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 192?199.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in ontonotes. In Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 1?27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 492?501. Association for Computational Lin-
guistics.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 814?824, Portland, Oregon, USA,
June.
161
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
Empirical Methods in Natural Language Processing
(EMNLP).
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Sameer Singh, Michael L. Wick, and Andrew McCal-
lum. 2010. Distantly labeling data for large scale
cross-document coreference. Computing Research
Repository (CoRR), abs/1005.4298.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544, Dec.
Valentin I. Spitkovsky and Angel X. Chang. 2012. A
cross-lingual dictionary for english wikipedia con-
cepts. In International Conference on Language Re-
sources and Evaluation (LREC).
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Computational Linguis-
tics (COLING).
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with reconcile. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 156?161, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, WWW ?07, pages 697?
706, New York, NY, USA. ACM.
Michael Wick, Aron Culotta, Khashayar Rohani-
manesh, and Andrew McCallum. 2009. An entity-
based model for coreference resolution. In SIAM In-
ternational Conference on Data Mining (SDM).
162
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 45?49,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Low-Dimensional Embeddings of Logic
Tim Rockt
?
aschel
?
Matko Bosnjak
?
Sameer Singh
?
Sebastian Riedel
?
?
Department of Computer Science, University College London, UK
?
Computer Science & Engineering, University of Washington, Seattle
{t.rocktaschel,m.bosnjak,s.riedel}@cs.ucl.ac.uk, sameer@cs.washington.edu
Abstract
Many machine reading approaches, from
shallow information extraction to deep
semantic parsing, map natural language
to symbolic representations of meaning.
Representations such as first-order logic
capture the richness of natural language
and support complex reasoning, but often
fail in practice due to their reliance on log-
ical background knowledge and the diffi-
culty of scaling up inference. In contrast,
low-dimensional embeddings (i.e. distri-
butional representations) are efficient and
enable generalization, but it is unclear how
reasoning with embeddings could support
the full power of symbolic representations
such as first-order logic. In this proof-of-
concept paper we address this by learning
embeddings that simulate the behavior of
first-order logic.
1 Introduction
Much of the work in machine reading follows an
approach that is, at its heart, symbolic: language
is transformed, possibly in a probabilistic way,
into a symbolic world model such as a relational
database or a knowledge base of first-order for-
mulae. For example, a statistical relation extractor
reads texts and populates relational tables (Mintz
et al., 2009). Likewise, a semantic parser can
turn sentences into complex first-order logic state-
ments (Zettlemoyer and Collins, 2005).
Several properties make symbolic representa-
tions of knowledge attractive as a target of ma-
chine reading. They support a range of well under-
stood symbolic reasoning processes, capture se-
mantic concepts such as determiners, negations
and tense, can be interpreted, edited and curated
by humans to inject prior knowledge. However, on
practical applications fully symbolic approaches
have often shown low recall (e.g. Bos and Markert,
2005) as they are affected by the limited coverage
of ontologies such as WordNet. Moreover, due to
their deterministic nature they often cannot cope
with noise and uncertainty inherent to real world
data, and inference with such representations is
difficult to scale up.
Embedding-based approaches address some of
the concerns above. Here relational worlds are de-
scribed using low-dimensional embeddings of en-
tities and relations based on relational evidence in
knowledge bases (Bordes et al., 2011) or surface-
form relationships mentioned in text (Riedel et al.,
2013). To overcome the generalization bottleneck,
these approaches learn to embed similar entities
and relations as vectors close in distance. Subse-
quently, unseen facts can be inferred by simple and
efficient linear algebra operations (e.g. dot prod-
ucts).
The core argument against embeddings is their
supposed inability to capture deeper semantics,
and more complex patterns of reasoning such
as those enabled by first-order logic (Lewis and
Steedman, 2013). Here we argue that this does
not need to be true. We present an approach that
enables us to learn low-dimensional embeddings
such that the model behaves as if it follows a com-
plex first-order reasoning process?but still oper-
ates in terms of simple vector and matrix repre-
sentations. In this view, machine reading becomes
the process of taking (inherently symbolic) knowl-
edge in language and injecting this knowledge into
a sub-symbolic distributional world model. For
example, one could envision a semantic parser that
turns a sentence into a first-order logic statement,
45
worksFor(A), profAt(A)
profAt(B) ...
profAt(x) => worksFor(x)
worksFor(B)
Curate
IE
SP
worksFor
profAt
A
Logical Inference Algebra
B
C
worksFor(B)
D
Logic Embedded LogicEvidence
Figure 1: Information extraction (IE) and semantic
parsing (SP) extract factual and more general log-
ical statements from text, respectively. Humans
can manually curate this knowledge. Instead of
reasoning with this knowledge directly (A) we in-
ject it into low dimensional representations of en-
tities and relations (B). Linear algebra operations
manipulate embeddings to derive truth vectors (C),
which can be discretized or thresholded to retrieve
truth values (D).
just to then inject this statement into the embed-
dings of relations and entities mentioned in the
sentence.
2 Background
Figure 1 shows our problem setup. We as-
sume a domain of a set of entities, such as
SMITH and CAMBRIDGE, and relations among
these (e.g. profAt(?, ?)). We start from a
knowledge base of observed logical statements,
e.g., profAt(SMITH, CAMBRIDGE) or ?x, y :
profAt(x, y) =? worksFor(x, y). These state-
ments can be extracted from text through informa-
tion extraction (for factual statements), be the out-
put from a semantic parsing (for first-order state-
ments) or come from human curators or external
knowledge bases.
The task at hand is to predict the truth
value of unseen statements, for example
worksFor(SMITH, CAMBRIDGE). Assuming we
have the corresponding formulae, logical infer-
ence can be used to arrive at this statement (arrow
A in Figure 1). However, in practice the relevant
background knowledge is usually missing. By
contrast, a range of work (e.g. Bordes et al., 2011;
Riedel et al., 2013) has successfully predicted
unseen factual statements by learning entity and
relation embeddings that recover the observed
facts and generalize to unseen facts through
dimensionality reduction (B). Inference in these
approaches amounts to a series of algebraic
operations on the learned embeddings that returns
a numeric representation of the degree of truth
(C), which can be thresholded to arrive back at a
true or false statement (D) if needed.
Our goal in this view is to generalize (B) to al-
low richer logical statements to be recovered by
low-dimensional embeddings. To this end we first
describe how richer logical statements can be em-
bedded at full dimension where the number of di-
mensions equals to the number of entities in the
domain.
2.1 Tensor Calculus
Grefenstette (2013) presents an isomorphism be-
tween statements in predicate logic and expres-
sions in tensor calculus. Let [?] denote this map-
ping from a logical expression F to an expression
in tensor algebra. Here, logical statements evaluat-
ing to true or false are mapped to [true]
:
=
> =
[
1 0
]
T
and [false]
:
= ? =
[
0 1
]
T
re-
spectively.
Entities are represented by logical constants and
mapped to one-hot vectors where each component
represents a unique entity. For example, let k = 3
be the number of entities in a domain, then SMITH
may be mapped to [SMITH] =
[
1 0 0
]
T
. Unary
predicates are represented as 2?k matrices, whose
columns are composed of > and ? vectors. For
example, for a isProfessor predicate we may get
[isProfessor ] =
[
1 0 1
0 1 0
]
.
In this paper we treat binary relations as unary
predicates over constants ?X, Y? that correspond to
pairs of entities X and Y in the domain.
1
The application of a unary predicate to a con-
stant is realized through matrix-vector multiplica-
tion. For example, for profAt and the entity pair
?X, Y? we get
[profAt(?X, Y?)] = [profAt ] [?X, Y?] .
In Grefenstette?s calculus, binary boolean oper-
ators are mapped to mode 3 tensors. For example,
for the implication operator holds:
[ =? ]
:
=
[
1 0 1 1
0 1 0 0
]
.
Let A and B be two logical statements that,
when evaluated in tensor algebra, yield a vector
1
This simplifies our exposition and approach, and it can
be shown that both representations are logically equivalent.
46
in {>,?}. The application of a binary operator
to statements A and B is realized via two con-
secutive tensor-vector products in their respective
modes (see Kolda and Bader (2009) for details),
e.g.,
[A =? B]
:
= [ =? ]?
1
[A]?
2
[B] .
3 Method
Grefenstette?s mapping to tensors exactly recov-
ers the behavior of predicate logic. However, it
also inherits the lack of generalization that comes
with a purely symbolic representation. To over-
come this problem we propose an alternate map-
ping. We retain the representation of truth val-
ues and boolean operators as the 2 ? 1 and the
2 ? 2 ? 2 sized tensors respectively. However,
instead of mapping entities and predicates to one-
hot representations, we estimate low-dimensional
embeddings that recover the behavior of their one-
hot counterparts when plugged into a set of tensor-
logic statements.
In the following we first present a general learn-
ing objective that encourages low-dimensional
embeddings to behave like one-hot representa-
tions. Then we show how this objective can be
optimized for facts and implications.
3.1 Objective
Let R be the set of all relation embeddings and
P be the set of all entity pair embeddings. Given
a knowledge base (KB) of logical formulae K
which we assume to hold, the objective is
min
[p]?P, [R]?R
?
F?K
?[F ]?>?
2
. (1)
That is, we prefer embeddings for which the given
formulae evaluate to the vector representation for
truth. The same can be done for negative data by
working with ?, but we omit details for brevity.
To optimize this function we require the gradi-
ents of ?[F ]?>?
2
terms. Below we discuss these
for two types of formulae: ground atoms and first-
order formulae.
3.2 Ground Atoms
The KB may contain ground atoms (i.e. facts) of
the form F = R(p) for a pair of entities p and a
relation R. These atoms correspond to observed
cells in an entity-pair-relation matrix, and inject-
ing these facts into the embedding roughly corre-
sponds to matrix factorization for link prediction
or relation extraction (Riedel et al., 2013).
Let ??
F
:
= ([F ]?>) / ?[F ]?>?
2
, then it is
easy to show that the gradients with respect to re-
lation embedding [R] and entity pair embedding
[p] are
?/? [p] = [R] ??
F
and ?/? [R] = ??
F
? [p] .
3.3 First-order Formulae
Crucially, and in contrast to matrix factorization,
we can inject more expressive logical formulae
than just ground atoms. For example, the KB
K may contain a universally quantified first-order
rule such as ?x : R
1
(x) =? R
2
(x). Assum-
ing a finite domain, this statement can be unrolled
into a conjunction of propositional statements of
the form F = R
1
(p) =? R
2
(p), one for each
pair p. We can directly inject these propositional
statements into the embeddings, and their gradi-
ents are straightfoward to derive. For example,
?/? [R
1
] = (([ =? ]?
2
[R
2
(p)]) ??
F
)? [p] .
3.4 Learning and Inference
We learn embeddings for entity pairs and relations
by minimizing objective 1 using stochastic gradi-
ent descent (SGD). To infer the (two-dimensional)
truth value (C in Figure 1) of a formula F in em-
bedded logic we evaluate [F ]. An easier to intpret
one-dimensional representation can be derived by
(
?[F ] ,
[
1 ?1
]
T
?+ 1
)
/2,
followed by truncation to the interval [0, 1]. Other
ways of projecting [F ] to R, such as using cosine
similarity to >, are possible as well.
4 Experiments
We perform experiments on synthetic data defined
over 7 entity pairs and 6 relations. We fix the em-
bedding size k to 4 and train the model for 100
epochs using SGD with `
2
-regularization on the
values of the embeddings. The learning rate and
the regularization parameter are set to 0.05.
The left part of Table 1 shows the observed
(bold) and inferred truth values for a set of fac-
tual staments of the form R(p), mapped to R as
discussed above. Due to the generalization ob-
tained by low-dimensional embeddings, the model
infers that, for example, SMITH is an employee
at CAMBRIDGE and DAVIES lives in LONDON.
However, we would like the model to also capture
that every professor works for his or her university
47
With Factual Constraints With Factual and First-Order Constraints
profAt worksFor employeeAt registeredIn livesIn bornIn profAt worksFor employeeAt registeredIn livesIn bornIn
?JONES, UWASH? 1.00 1.00 1.00 0.00 0.18 0.01 0.98 0.98 0.95 0.03 0.00 0.04
?TAYLOR, UCL? 1.00 1.00 0.98 0.00 0.20 0.00 0.98 0.96 0.95 0.05 0.00 0.06
?SMITH, CAMBRIDGE? 0.98
>
0.00
>
0.64 0.75 0.07 0.72 0.92
>
0.97
>
0.89 0.04 0.04 0.05
?WILLIAMS, OXFORD?
?
0.02 1.00 0.08 0.00 0.93 0.02
?
0.05 0.91 0.02 0.05 0.87 0.06
?BROWN, CAMBRIDGE?
?
0.00 0.97 0.02
?
0.01 0.95 0.06
?
0.01 0.90 0.00
?
0.07 0.92 0.07
?DAVIES, LONDON? 0.00 0.00 0.00 0.99
>
0.50 1.00 0.01 0.00 0.00 0.98
>
0.98 0.97
?EVANS, PARIS? 0.00 0.00 0.00 1.00
>
0.48 1.00 0.00 0.00 0.00 0.97
>
1.00 0.96
Table 1: Reconstructed matrix without (left) and with (right) the first-order constraints profAt =?
worksFor and registeredIn =? livesIn . Predictions for training cells of factual constraints [R(p)] =
> are shown in bold, and true and false test cells are denoted by
>
and
?
respectively.
and that, when somebody is registered in a city, he
or she also lives in that city.
When including such first-order constraints
(right part of Table 1), the model?s predictions
improve concerning different aspects. First, the
model gets the implication right, demonstrating
that the low-dimensional embeddings encode first-
order knowledge. Second, this implication transi-
tively improves the predictions on other columns
(e.g. SMITH is an employee at CAMBRIDGE).
Third, the implication works indeed in an asym-
metric way, e.g., the model does not predict that
WILLIAMS is a professor at OXFORD just because
she is working there.
5 Related Work
The idea of bringing together distributional se-
mantics and formal logic is not new. Lewis and
Steedman (2013) improve the generalization per-
formance of a semantic parser via the use of dis-
tributional representations. However, their target
representation language is still symbolic, and it is
unclear how this approach can cope with noise and
uncertainty in data.
Another line of work (Clark and Pulman, 2007;
Mitchell and Lapata, 2008; Coecke et al., 2010;
Socher et al., 2012; Hermann and Blunsom, 2013)
uses symbolic representations to guide the com-
position of distributional representations. Read-
ing a sentence or logical formula there amounts
to compositionally mapping it to a k-dimensional
vector that then can be used for downstream tasks.
We propose a very different approach: Reading a
sentence amounts to updating the involved entity
pair and relation embeddings such that the sen-
tence evaluates to true. Afterwards we cannot use
the embeddings to calculate sentence similarities,
but to answer relational questions about the world.
Similar to our work, Bowman (2014) provides
further evidence that distributed representations
can indeed capture logical reasoning. Although
Bowman demonstrates this on natural logic ex-
pressions without capturing factual statements,
one can think of ways to include the latter in
his framework as well. However, the ap-
proach presented here can conceptually inject
complex nested logical statements into embed-
dings, whereas it is not obvious how this can be
achieved in the neural-network based multi-class
classification framework proposed by Bowman.
6 Conclusion
We have argued that low dimensional embeddings
of entities and relations may be tuned to simu-
late the behavior of logic and hence combine the
advantages of distributional representations with
those of their symbolic counterparts. As a first
step into this direction we have presented an ob-
jective that encourages embeddings to be consis-
tent with a given logical knowledge base that in-
cludes facts and first-order rules. On a small syn-
thetic dataset we optimize this objective with SGD
to learn low-dimensional embeddings that indeed
follow the behavior of the knowledge base.
Clearly we have only scratched the surface
here. Besides only using toy data and logical for-
mulae of very limited expressiveness, there are
fundamental questions we have yet to address.
For example, even if the embeddings could en-
able perfect logical reasoning, how do we pro-
vide provenance or proofs of answers? More-
over, in practice a machine reader (e.g. a semantic
parser) incrementally gathers logical statements
from text? how could we incrementally inject this
knowledge into embeddings without retraining the
whole model? Finally, what are the theoretical
limits of embedding logic in vector spaces?
48
Acknowledgments
We would like to thank Giorgos Spithourakis,
Thore Graepel, Karl Moritz Hermann and Ed-
ward Grefenstette for helpful discussions, and An-
dreas Vlachos for comments on the manuscript.
This work was supported by Microsoft Research
through its PhD Scholarship Programme. This
work was supported in part by the TerraSwarm Re-
search Center, one of six centers supported by the
STARnet phase of the Focus Center Research Pro-
gram (FCRP) a Semiconductor Research Corpora-
tion program sponsored by MARCO and DARPA.
References
Antoine Bordes, Jason Weston, Ronan Collobert,
and Yoshua Bengio. 2011. Learning structured
embeddings of knowledge bases. In AAAI.
Johan Bos and Katja Markert. 2005. Recognis-
ing textual entailment with logical inference. In
Proc. of HLT/EMNLP, pages 628?635.
Samuel R Bowman. 2014. Can recursive neural
tensor networks learn logical reasoning? In
ICLR?14.
Stephen Clark and Stephen Pulman. 2007. Com-
bining symbolic and distributional models of
meaning. In AAAI Spring Symposium: Quan-
tum Interaction, pages 52?55.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a
compositional distributional model of meaning.
CoRR, abs/1003.4394.
Edward Grefenstette. 2013. Towards a formal dis-
tributional semantics: Simulating logical calculi
with tensors. In Proc. of *SEM, pages 1?10.
Karl Moritz Hermann and Phil Blunsom. 2013.
The role of syntax in vector space models of
compositional semantics. In Proc. of ACL,
pages 894?904.
Tamara G Kolda and Brett W Bader. 2009. Tensor
decompositions and applications. SIAM review,
51(3):455?500.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. In TACL,
volume 1, pages 179?192.
Mike Mintz, Steven Bills, Rion Snow, and Daniel
Jurafsky. 2009. Distant supervision for rela-
tion extraction without labeled data. In Proc.
of ACL-IJCNLP, pages 1003?1011.
Jeff Mitchell and Mirella Lapata. 2008. Vector-
based models of semantic composition. In Proc.
of ACL, pages 236?244.
Sebastian Riedel, Limin Yao, Andrew McCallum,
and Benjamin M Marlin. 2013. Relation ex-
traction with matrix factorization and universal
schemas. In Proc. of NAACL-HLT, pages 74?
84.
Richard Socher, Brody Huval, Christopher D
Manning, and Andrew Y Ng. 2012. Seman-
tic compositionality through recursive matrix-
vector spaces. In Proc. of EMNLP, pages 1201?
1211.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form:
Structured classification with probabilistic cat-
egorial grammars. In Proc. of UAI, pages 658?
666.
49
