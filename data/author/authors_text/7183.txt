Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455?1464,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Probabilistic Model for Associative Anaphora Resolution
Ryohei Sasano and Sadao Kurohashi
Graduate School of Informatics, Kyoto University,
Yoshida-honmachi, Sakyo-ku, Kyoto
{sasano,kuro}@i.kyoto-u.ac.jp
Abstract
This paper proposes a probabilistic model
for associative anaphora resolution in
Japanese. Associative anaphora is a
type of bridging anaphora, in which the
anaphor and its antecedent are not coref-
erent. Our model regards associative
anaphora as a kind of zero anaphora and
resolves it in the same manner as zero
anaphora resolution using automatically
acquired lexical knowledge. Experimen-
tal results show that our model resolves
associative anaphora with good perfor-
mance and the performance is improved
by resolving it simultaneously with zero
anaphora.
1 Introduction
The correct interpretation of anaphora is vital
for natural language understanding. Bridging
anaphora (Clark, 1975) represents a special part of
the general problem of anaphora resolution, which
has been studied and discussed for various lan-
guages and domains (Hahn et al, 1996; Murata et
al., 1999; Poesio et al, 2004; Gasperin and Vieira,
2004; Gasperin and Briscoe, 2008).
Usually bridging anaphora considers two
types:
1
associative anaphors are noun phrases
(NPs) that have an antecedent that is necessary
to their interpretation but the relation between the
anaphor and its antecedent is different from iden-
tity; and indirect anaphors are those that have
an identity relation with their antecedents but the
anaphor and its antecedent have different head
1
The terminology that we use here is introduced by
Hawkins (1978), which is also used in (Vieira et al, 2006).
nouns. In this paper, we focus on associative
anaphora in Japanese.
Associative anaphora resolution is decomposed
into two steps: acquiring lexical knowledge for as-
sociative anaphora resolution, and resolving asso-
ciative anaphora using the acquired knowledge.
Grammatical salience plays a lesser role for
resolving anaphors with full lexical heads, than
for pronominal anaphora (Strube and Hahn, 1999;
Modjeska, 2002). Furthermore, since associative
anaphors and their antecedents usually have differ-
ent head nouns, string matching technique cannot
be applied. Therefore, a large and diverse amount
of lexical knowledge is essential to understand as-
sociative anaphora. For example, to recognize the
meronymic relation between ?a house? and ?the
roof? in (1), such knowledge as ?a roof? is a part
of a building or vehicle is required. To recognize
the attributive relation between ?Prius? and ?the
price? in (2), such knowledge as ?price? is a price
of some goods or service is required.
(1) There was a house. The roof was white.
(2) Toyota launched the hybrid car Prius in
1997. The price was 21.5 million yen.
To acquire such lexical knowledge, various
studies have been carried out. Early studies used
hand-crafted lexical knowledge such as Word-
Net (Strube and Hahn, 1999; Vieira and Poe-
sio, 2000; Meyer and Dale, 2002), but obtained
poor or mediocre results. Hence, Poesio et al
(2002) proposed to exploit ?N
h
of N
m
? phrases
in large corpora to resolve associative anaphora
in English; Murata et al (1999) proposed to ex-
ploit ?N
m
no N
h
? phrases to resolve associative
anaphora in Japanese. Here, the Japanese postpo-
sition ?no? roughly corresponds to ?of,? but it has
1455
much broader usage. These studies obtained rea-
sonable results, but the coverage of the acquired
knowledge was not sufficient. Recently, a num-
ber of researchers argued for using the Web as a
source of lexical knowledge, and theWeb has been
shown to be a useful resource for anaphora resolu-
tion (Bunescu, 2003; Markert et al, 2003; Poesio
et al, 2004).
Hence, in this study, we acquire the lexi-
cal knowledge for associative anaphora resolution
from ?N
m
noN
h
? phrases in the Web by using the
method described in (Sasano et al, 2004). We pro-
posed a method for acquiring such lexical knowl-
edge, called nominal case frames (NCFs), using
an ordinary language dictionary and ?N
m
no N
h
?
phrases, and constructed NCFs from newspaper
corpora. In this study, we aim to acquire a suffi-
cient amount of lexical knowledge by constructing
NCFs from the Web.
As for associative anaphora resolution itself, we
propose an integrated probabilistic model for zero
anaphora and associative anaphora resolution, in
which associative anaphora is regarded as a kind
of zero anaphora and resolved by using the same
model as zero anaphora. Our model assumes zero
pronouns that represent indispensable entities of
target noun phrases, which are called zero adnom-
inal in (Yamura-Takei, 2003), and conducts zero
pronoun resolution.
Let us consider the associative anaphoric re-
lation between ?Prius? and ?kakaku? (price).
Although ?kakaku? itself is considered as the
anaphor from a conventional point of view (3a),
our model assumes a zero pronoun ? and consid-
ers it as the anaphor (3b).
(3) a. Prius - kakaku (price)
[antecedent: Prius, anaphor: kakaku (price)]
b. Prius - (?-no) kakaku (price (of ?))
[antecedent: Prius, anaphor: ?]
The point of this study is three-fold: the ac-
quisition of the lexical knowledge for associative
anaphora resolution from the Web, the application
of zero anaphora resolution model to associative
anaphora resolution, and the integrated resolution
of zero anaphora and associative anaphora.
2 Construction of Nominal Case Frames
Most nouns have their indispensable entities:
?price? is a price of some goods or service, ?roof?
is a roof of some building, and ?coach? is a coach
of some sports. The relation between a noun and
its indispensable entities is parallel to that between
a verb and its arguments or obligatory cases. In
this paper, we call indispensable entities of nouns
obligatory cases. Note that, obligatory does not
mean grammatically obligatory but obligatory to
interpret the meaning of the noun. Associative
anaphora resolution needs comprehensive infor-
mation of obligatory cases of nouns. Nominal case
frames (NCFs) describe such information, and we
construct them from the Web.
2.1 Automatic Construction of NCFs
First, we briefly introduce our method for con-
structing NCFs from raw corpora proposed in
(Sasano et al, 2004).
Whereas verbal case frame construction uses ar-
guments of each verb (Kawahara and Kurohashi,
2002), nominal case frame construction basically
uses adnominal constituents of each noun. How-
ever, while the meaning of a verbal argument can
be distinguished by the postposition, such as ?ga?
(nominative), ?wo? (accusative), and ?ni? (dative),
the meaning of an adnominal constituent can not
be distinguished easily, because most adnominal
constituents appear with the same postposition
?no? (of). Thus, we first conduct a semantic anal-
ysis of adnominal constituents, and then construct
NCFs using the results as follows:
1. Collect syntactically unambiguous noun
phrases ?N
m
no N
h
? from the automatic re-
sulting parses of large corpora.
2. Analyze the relation between N
m
and N
h
by Kurohashi and Sakai?s method (1999) that
exploits an ordinary language dictionary.
3. Depending on the results, classify N
m
, and
obtain preliminary case slots for N
h
.
4. Merge case slots if two preliminary case slots
of N
h
are similar.
5. Consider frequent case slots as obligatory
cases of N
h
. The frequency thresholds are
varied according to semantic analyses.
6. For each meaning of N
h
, collect case slots
and construct case frames.
The point of this method is the integrated use of
an ordinary dictionary and example phrases from
1456
Table 1: Examples of constructed nominal case frames.
Case slot Examples with freq Generalized examples with rate
Definition: the amount of money you have to pay for something.
kakaku (1) [something] sh?ohin(goods):9289, seihin(product):2520, [CT:ARTIFACT]:0.93, ? ? ?
(price) buhin(part):341, yunyuhin(importation):232, ? ? ?
Definition: the structure that covers or forms the top of a building etc.
yane (1) [building] ie(house):2505, kuruma(car):1565, koya(hut):895, [CT:FACILITY]:0.44,
(roof) tatemono(building):883,minka(private house):679, ? ? ? [CT:VEHICLE]:0.13,? ? ?
Definition: the elected leader of the government in a country that has a parliament.
shusho (1) [country] nihon(Japan):2355, kuni(country):272, [NE:LOCATION]:0.82,
(prime minister) doitsu(Germany):157, ch?ugoku(China):130, ? ? ? [CT:VEHICLE]:0.13,? ? ?
Definition: a girl or woman who has the same parents as you.
imouto (1) <relationship> watashi(me):3385, ore(me):1188, boku(me):898, [CT:PERSON]:0.74,
(sister) jibun(oneself):341, tomodachi(friend):537, ? ? ? [NE:PERSON]:0.22, ? ? ?
Definition: a stick or handle on a machine.
reb?a(1) [machine] bu?reki(brake):122, sokketo(sochet):67, [CT:ARTIFACT]:0.61,
(lever) waip?a(wiper):54, souchi(device):52,? ? ? [CT:VEHICLE]:0.04, ? ? ?
Definition: the liver of an animal, used as food.
reb?a(2) [animal] niwatori(chicken):153, buta(pig):153, [CT:ANIMAL]:0.98, ? ? ?
(liver) ushi(cattle):62, doubutsu(animal):25,? ? ?
Definition: someone who takes part in a sport.
senshu(1) [sport] yaky?u(baseball):1252, rir?e(relay):736, [CT:ABSTRACTION]:0.56, ? ? ?
(player) ky?ogi(competition):430, sakk?a(soccer):394, ? ? ?
<affiliation> ch??mu(team):4409, nihon(Japan):3222, [NE:LOCATION]:0.33,
reddu(Reds):771, kankoku(Korea):644,r??gu(league) ? ? ? [CT:ORGANIZATION]:0.30, ? ? ?
* ?[]? and ?<>? denote dictionary-based and semantic feature-based analysis respectively. For details see (Sasano et al, 2004).
large corpora. Dictionary definition sentences are
an informative resource to recognize obligatory
cases of nouns. However, it is difficult to resolve
associative anaphora by using a dictionary as it is,
because all nouns in a definition sentence are not
an obligatory case, and only the frequency infor-
mation of noun phrases tells us which is the oblig-
atory case. On the other hand, a simple method
that just collects and clusters ?N
m
no N
h
? phrases
based on some similarity measure of nouns cannot
construct comprehensive nominal case frames, be-
cause of polysemy and multiple obligatory cases.
For details see (Sasano et al, 2004).
It is desirable to use a probability distribution
for deciding whether a case slot is obligatory or
not. However, it is difficult to estimate a prob-
ability distribution, since we construct nominal
case frames not by using the examples of associa-
tive anaphora itself but by using the examples of
noun phrases ?N
m
no N
h
? (N
h
of N
m
). We use
such noun phrases because indispensable entities
of noun ?N
h
? often appear as ?N
m
.? However, we
can say neither frequently appeared ?N
m
? is an in-
dispensable entity of ?N
h
.? nor an indispensable
entity frequently appears as ?N
m
.? For example,
the name of a country is considered as an indis-
pensable entity of ?shusho? (prime minister), but
does not frequently appear as ?N
m
.?
2
Thus, it is
difficult to estimate a probability distribution and
we use a hard decision.
2.2 NCF Construction from the Web
We constructed nominal case frames from theWeb
Corpus (Kawahara and Kurohashi, 2006), which
comprises 1.6 billion unique Japanese sentences.
In this corpus, there were about 390 million noun
phrases ?N
m
no N
h
,? about 100 million unique
noun phrases, and about 17 million unique head
nouns ?N
h
.? There were about 4.07 million head
nouns that appeared more than 10 times in the cor-
pus, and we used only such head nouns.
The resultant nominal case frames consisted of
about 564,000 nouns including compound nouns.
We show examples of constructed nominal case
frames in Table 1. The average number of case
frames for a noun that has case frames was 1.0031,
and the average number of case slots for a case
frame was 1.0101. However, these statistics dif-
fered with the frequency of the noun. Therefore,
we investigated the statistics of constructed nom-
inal case frames for each group classified by the
frequency of the nouns. Table 2 shows the re-
2
It is because ?the prime minister of Japan? is often men-
tioned by simply ?the prime minister? in Japanese.
1457
Table 2: The statistics of constructed NCFs.
Frequency Proportion # of NCFs # of CSs Coverage
ranking of nouns per noun per NCF
with NCF with NCF
-100 56.0% 1.34 1.07 17.3%
-1000 68.8% 1.17 1.16 25.6%
-10000 51.7% 1.11 1.17 27.0%
-100000 14.8% 1.05 1.13 17.6%
100001- 13.7% 1.0009 1.0053 12.5%
all 13.9% 1.0031 1.0101 100%
Table 3: Evaluation of constructed NCFs.
Precision Recall F-measure
62/70 (0.89) 62/84 (0.74) 0.81
sult. As for the 10,000 most frequently appeared
nouns, which occupied about 70% of all noun ap-
pearances, the average number of case frames for
a noun was 1.11, and the average number of case
slots for a case frame was 1.17.
For evaluating the resultant case frames, we ran-
domly selected 100 nouns from the 10,000 most
frequent nouns, and created gold standard case
frames for these nouns by hand. For each noun,
case frames were given if the noun was considered
to have any indispensable entity, and for each case
frame, obligatory case slots were given manually:
70 case frames were created that had 84 case slots;
56 case frames had only one case slot, the other 14
case frames had two case slots. 30 nouns had no
case frames.
We then evaluated the automatically con-
structed case slots for these selected nouns. The
evaluation result is shown in Table 3: the sys-
tem output 70 case slots, and out of them, 62 case
frames were judged as correct. The F-measure was
0.81. Since the boundary between indispensable
cases and optional cases of a noun is not always
obvious, this score is considered to be reasonable.
2.3 Generalization of Examples
By using nominal case frames constructed from
the Web, sparseness problem was alleviated to
some extent, but still remained. For instance, there
were thousands of named entities (NEs), which
could not be covered intrinsically. To deal with
this sparseness problem, we generalized the exam-
ples of case slots.
First, we used the categories that Japanese mor-
phological analyzer JUMAN
3
adds to common
nouns. In JUMAN, about twenty categories are
defined and tagged to common nouns. For ex-
ample, ?kuruma (car),? ?niwatori (chicken),? and
?tatemono (building)? are tagged as ?VEHICLE,?
?ANIMAL? and ?FACILITY,? respectively. For
each category, we calculated the rate of catego-
rized examples among all case slot examples, and
added it to the case slot as ?[CT:VEHICLE]:0.13.?
We also generalized NEs. We used a com-
mon standard NE definition for Japanese pro-
vided by IREX workshop (1999). We first rec-
ognized NEs in the source corpus by using an
NE recognizer (Sasano and Kurohashi, 2008), and
then constructed NCFs from the NE-recognized
corpus. As well as categories, for each NE
class, we calculated the NE rate among all case
slot examples, and added it to the case slot as
?[NE:PERSON]:0.22.? The generalized examples
are also included in Table 1.
3 Probabilistic Model
In this study, we apply a lexicalized probabilis-
tic model for zero anaphora resolution proposed in
(Sasano et al, 2008) to associative anaphora reso-
lution.
3.1 A Lexicalized Probabilistic Model for
Zero Anaphora Resolution
In English, overt pronouns such as ?she? and
definite noun phrases such as ?the company?
are anaphors that refer to preceding entities (an-
tecedents). On the other hand, in Japanese,
anaphors are often omitted, which are called zero
pronouns, and zero anaphora resolution is one of
the most important techniques for semantic analy-
sis in Japanese.
Here, we introduce our model for zero anaphora
resolution (Sasano et al, 2008). This model first
resolves coreference and identifies discourse enti-
ties; then from the end of each sentence, analyzes
each predicate by the following steps:
1. Select a case frame temporarily.
2. Consider all possible correspondences be-
tween each input argument and a case slot of
the selected case frame.
3. Regard case slots that have no correspon-
dence as zero pronoun candidates.
3
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
1458
4. Consider all possible correspondences be-
tween zero pronoun candidates and existing
entities.
5. For each possible case frame, estimate each
correspondence probabilistically, and select
the most likely case frame and correspon-
dence.
Figure 1 shows an example of correspondences
between case frames and discourse entities.
The probabilistic model gives a probability to
each possible case frame CF and case assignment
CA when target predicate v, input arguments IA
and existing discourse entities ENT are given,
and outputs the case frame and case assignment
that have the highest probability. That is to say,
their model selects the case frame CF
best
and the
case assignment CA
best
that maximize the proba-
bility P (CF,CA|v, IA,ENT ):
(CF
best
, CA
best
)
= argmax
CF,CA
P (CF,CA|v, IA,ENT ) (i)
By decomposing case assignment (CA) into
direct case assignment (DCA) and the indirect
case assignment (ICA) and using several inde-
pendence assumptions, Equation (i) is transformed
into the following equation:
4
(CF
best
, DCA
best
,ICA
best
) =
argmax
CF,DCA,ICA
(
P (CF |v)? P (DCA, IA|CF )
?P (ICA|ENT,CF,DCA)
)
(ii)
Here, P (CF
l
|v) denotes the probability to se-
lect CF
l
when target predicate v is given, and es-
timated by using case structure analysis of large
raw corpora.
P (DCA
k
, IA|CF
l
) denotes the probability to
generate direct case assignment and input argu-
ments when a case frame is given, and estimated
by using case structure analysis of large raw cor-
pora, the frequency of a case slot example in the
automatically constructed verbal case frames, and
the web corpus in which the relation between a
surface case marker and a case slot is manually
annotated.
P (ICA
k
|ENT,CF
l
, DCA
k
) denotes the
probability to generate indirect case assignment
when existing discourse entities, a case frame and
4
For details see (Sasano et al, 2008).
Toyota-wa
Prius-wo
hybrid car
hatsubai.
kaigai-demo
hanbai-shiteiru.
1997-nen
2000-nen-karawa
{Toyota, ?
?
}
{hybrid car, Prius, ?2 }
{kaigai}
Entities
(overseas)
(launch)
(sell)
hatsubai (launch)ganominative company, SONY, firm, ? [NE:ORGANIZATION] 0.15, ?
woaccusative product, CD, model, car,  ?[CT:ARTIFACT] 0.40, ?de      locative area, shop, world, Japan, ?[CT:FACILITY] 0.13, ?
hanbai (sell)ganominative company, Microsoft, ? [NE:ORGANIZATION] 0.16, ?
woaccusative goods, product, ticket, ? [CT:ARTIFACT] 0.40, ?
nidative customer, company, user, ? [CT:PERSON] 0.28, ?de      locative shop, bookstore, site, ? [CT:FACILITY] 0.40, ?:direct case assignment:indirect case assignment (zero anaphora)
Verbal case framesInput sentences
Toyota launched the hybrid car Prius in 1997. ?
?
started selling ?2 overseas in 2000.
{1997-nen}
{2000-nen}
Figure 1: An example of correspondences be-
tween verbal case frames and discourse entities.
direct case assignments are given, and estimated
by using several preferences on the relation
between a zero pronoun and an antecedent, such
as a lexical preference, a surface case preferences,
and a locational preference.
For example, the lexical preference represents
how likely an entity that contains n
j
m
as a con-
tent part is considered to be an antecedent and is
estimated by the following equation.
P (n
j
m
|CF
l
, s
j
, A
?
(s
j
)=1)
P (n
j
m
)
(iii)
where, the function A
?
(s
j
) returns 1 if a case slot
s
j
is filled with an antecedent of a zero pronoun;
otherwise 0. P (n
j
|CF
l
, s
j
, A
?
(s
j
) = 1) is calcu-
lated by using case frames and denotes the proba-
bility of generating a content part n
j
of a zero pro-
noun, when a case frame and a case slot are given
and the case slot is filled with an antecedent of a
zero pronoun.
3.2 Extension to Associative Anaphora
Resolution
We then extend this probabilistic model to associa-
tive anaphora resolution. In this model, associative
anaphora is regarded as a kind of zero anaphora,
that is, the relation between a noun and its oblig-
atory cases is considered to be parallel to that be-
tween a verb and its arguments. Omitted obliga-
tory cases are considered to be zero pronouns and
resolved by the same process as zero anaphora res-
olution.
We conduct associative anaphora resolution for
only non-coreferent noun phrases. This is because
most of the relationships between coreferent noun
1459
Toyota-wa
Prius-wohybrid car
kakaku-wa
215-man-yen-datta.
1997-nen
Hatsubai-tosho
{Toyota, ?
?
}
{hybrid car, Prius, ?2 }
Entities
(price)
(ten thousands)
kakaku (price)
something goods, product, part, importation, ? [CT:ARTIFCAT] 0.40, ?
Nominal case framesInput sentences
{1997-nen}
{215-man-yen}
{kaigai}
Toyota launched the hybrid car Prius ???. The initial price of ?2 was 21.5 million yen.
:indirect case assignment (associative anaphora)(initial)
Figure 2: An example of correspondences be-
tween a nominal case frame and discourse entities.
phrases and its obligatory entities are easy to rec-
ognize by following up the coreference chains.
For example, the second appearance of ?the roof?
in (4) means ?the roof of the house,? and it is
easy to recognize by looking the first appearance
of ?the roof.?
(4) I saw the roof of the house. The roof was
painted dark green.
While verbal case frames describe both obliga-
tory and optional cases, nominal case frames de-
scribe only obligatory cases. Therefore, we con-
sider all case slots of nominal case frames as the
target of associative anaphora resolution.
Let us consider following example:
(5) Toyota-wa 1997-nen hybrid car Prius-wo
year
hatsubai. 2000-nen-kara-wa kaigai-demo
launched year overseas
hanbai-shiteiru. Hatsubai tosho,
selling initial
(?-no) kakaku-wa 215-man yen-datta.
price ten thousands
(Toyota
1
launched the hybrid car Prius
2
in 1997. ?
1
started selling ?
2
overseas in 2000. The initial price
of ?
2
was 21.5 million yen.)
?Kakaku? (price) in this example has an omitted
obligatory case ?[something]? as shown in Table
1. Therefore, our model assumes a zero pronoun
and identifies the antecedent from the existing dis-
course entities, such as {Toyota}, {hybrid-car,
Prius},
5
and {kaigai}. Figure 2 shows an exam-
ple of correspondences between the nominal case
frame of ?kakaku? (price) and discourse entities.
In addition, as well as zero anaphora resolution,
we exploit generalized examples to estimate lexi-
cal preference. When one mention of an entity is
5
?Hybrid car? and ?Prius? are in apposition and these two
phrases are considered to refer to the same discourse entity.
tagged any category or recognized as an NE, our
model also uses the category or the NE class as the
content part of the entity. Specifically, for estimat-
ing Equation (iii), our model also calculates:
P (NE :ARTIFACT |kakaku(1), no, A
?
(no)=1)
P (NE :ARTIFACT )
besides:
P (Prius|kakaku(1), no, A
?
(no) = 1)
P (Prius)
and uses the geometric mean of them.
3.3 Salience Score Filtering
Previous work has reported the usefulness of
salience for anaphora resolution (Lappin and Le-
ass, 1994; Mitkov et al, 2002). In order to con-
sider the salience of a discourse entity, we intro-
duce the concept of salience score, which is calcu-
lated by the following set of simple rules, and only
consider the entities that have the salience score no
less than 1 as candidate antecedents of an associa-
tive anaphor.
? +2 : mentioned with topical marker ?wa,? or
at the end of a sentence.
? +1 : mentioned without topical marker ?wa.?
? +1 : assigned to a zero pronoun.
? ?? : beginning of each sentence.
We call ? a decay rate. If ? ? 1, we do not
filter out any entities. If ? = 0, we only consider
the entities that appears in the same sentence as
candidate antecedents. For example, we consider
the salience score of the discourse entity {hybrid-
car, Prius} in the example (5) when using ?=0.6.
In the first sentence, since {hybrid-car, Prius} is
mentioned twice, the salience score is 2.0. At the
beginning of the second sentence it becomes 1.2,
and after the zero anaphora resolution of ?hanbai?
it becomes 2.2. At the beginning of the third sen-
tence it becomes 1.32.
Note that, this is an ideal case. Practically, some
zero pronouns are not detected and some pronouns
are assigned wrong antecedent; thus the salience
score varies according to the preceding analysis.
In addition, the salience score also depends on
whether we resolve only associative anaphora or
resolve associative anaphora simultaneously with
zero anaphora. If zero pronoun resolution is not
1460
conducted, zero pronouns that represent omitted
cases of verbs are not considered.
For example, in case of {hybrid-car, Prius}
with ? = 0.6, if zero anaphora resolution is not
conducted, the salience score at the beginning of
the third sentence becomes 0.72, because the zero
anaphora resolution of ?hanbai? is not considered;
and thus {hybrid-car, Prius} is not considered as
an antecedent candidate.
In order to recognize discourse structure more
properly, our model basically resolves associa-
tive anaphora simultaneously with zero anaphora,
and aims to consider zero pronouns that represent
omitted cases of verbs.
3.4 Summary of Our model
Our model is summarized as follows:
1. Parse an input text using the Japanese parser
KNP
6
and recognize NEs.
2. Resolve coreference, and link each mention
to an entity or create a new entity.
3. From the end of each sentence, zero anaphora
and associative anaphora resolution is con-
ducted for each verb and non-coreferent noun
by the following steps:
(a) Select a case frame temporarily.
(b) Consider all possible correspondences
between each input argument and a case
slot of the selected case frame.
(c) Regard case slots that have no corre-
spondence as zero pronoun candidates.
(d) Consider all possible correspondences
between zero pronoun candidates and
existing entities that has a salience score
no less than 1.0.
(e) Estimate each correspondence proba-
bilistically, and select the most likely
case frame and a correspondence.
4 Experiments
4.1 Setting
We created an anaphoric relation-tagged corpus
consisting of 186 web documents (979 sentences),
in which all predicate-argument relations and re-
lations between nouns were manually tagged. We
show some examples:
6
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
(6) Toyota-wa 1997-nen Prius-wo hatsubai.
year launch
2000-nen-kara-wa kaigai-demo hanbai.
year overseas sell
(Toyota launched Prius in 1997.
?
1
started selling ?
2
overseas in 2000.)
TAG: hatsubai ? ga:Toyota, wo:Prius,
(NOM) (ACC)
hanbai ? ga:Toyota, wo:Prius
(NOM) (ACC)
For the predicate ?hatsubai? (launch), ?Toyota?
is tagged as ga (nominative) case and ?Prius? is
tagged as wo (accusative) case. For the predicate
?hanbai? (sell), ?Toyota? is tagged as omitted ga
(nominative) case and ?Prius? is tagged as omit-
ted wo (accusative) case, which are indicated in
bold, and such omitted cases are the target of zero
anaphora resolution.
As for relations between nouns, both overt and
implicit relations are tagged with the Japanese
case marker ?no? (adnominal). In addition, rela-
tions between nouns are classified into three cate-
gories: indispensable, possible, and adjunct. Since
it is not always obvious whether the relations are
indispensable or not, borderline relations between
indispensable and adjunct are tagged possible. We
consider only the implicit relations that are tagged
indispensable as the target of associative anaphora
resolution.
(7) Ken-wa imouto-to yatte-kita.
sister came.
(Ken came with ??s sister.)
TAG: imouto ? no:Ken (indispensable)
(ADN)
(8) K?oen-ni ikuto benchi-ga atta.
park went bench was
(I went to the park. There was a bench in ?.)
TAG: benchi ? no:k?oen (possible)
(ADN)
We used 62 documents for testing and used the
other 124 documents for calculating several prob-
abilities. In the 62 test documents, 110 associa-
tive anaphoric relations were tagged. Each param-
eter for the proposed model was estimated using
maximum likelihood from raw corpora, the tagged
corpus, and case frames. As verbal case frames,
we used the case frames constructed from the Web
corpus comprising 1.6 billion sentences (Sasano et
al., 2009).
In order to concentrate on associative anaphora
resolution, we used the correct morphemes, named
entities, syntactic structures, and coreference re-
1461
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Decay Rate ?
Recall
F-measure
.427
Precision
Figure 3: Experimental results of associative
anaphora resolution on several salience decay
rates ?.
lations that were annotated by hand. Since cor-
rect coreference relations were given, the number
of created entities was the same between the gold
standard data and the system output because zero
anaphora and associative anaphora resolution did
not create new entities.
4.2 Results
Figure 3 shows the experimental results of asso-
ciative anaphora resolution, in which we used gen-
eralized examples, resolved zero anaphora auto-
matically, and varied the decay rate ? introduced
in Section 3.3 from 0 to 1. When we used the de-
cay rates smaller than 0.5, the recall score wors-
ened clearly. On the other hand, although we ex-
pected to obtain higher precision with small decay
rate, the highest precision was achieved by the de-
cay rate 0.5. Consequently, we obtained the high-
est F-measure of 0.427 with the decay rate 0.5. In
the following experiments, we fixed the decay rate
0.5.
We utilized two baseline models for demon-
strating the effectiveness of our approach: a ran-
dom model and a salience-based model. The ran-
dom model selects a case frame and its correspon-
dence randomly from all possible case frames and
correspondences. The salience-based model se-
lects a case frame and its correspondence that as-
sign a zero pronoun candidate the existing entity
that have highest salience score. In addition, in or-
der to confirm the effectiveness of generalized ex-
amples of NCFs, we conducted experiments with-
out using generalized examples. Table 4 shows
the experimental results. We can confirm that our
proposed model outperforms two baseline mod-
els. Without using any generalized examples, the
Table 4: Experimental results of associative
anaphora resolution with two baseline models and
our model with/without generalized examples.
Model Recall Precision F-measure
Random* 0.148 0.035 0.056
(16.3/110) (16.3/467.5)
Salience- 0.400 0.135 0.202
based (44/110) (44/325)
Proposed
CT NE
0.318 0.257 0.285
(35/110) (35/136)
?
0.345 0.268 0.302
(38/110) (38/142)
?
0.464 0.333 0.388
(51/110) (51/153)
? ?
0.518 0.363 0.427
(57/110) (57/157)
CT: Using examples generalized by categories.
NE: Using examples generalized by named entities.
* The average of 10 trials is shown.
F-measure was about 0.14 lower than the method
using generalized examples, and we can also con-
firm the effectiveness of the generalized examples.
While generalization of categories much improved
the F-measure, generalization of NEs contributed
little. This is because the NE rate was smaller than
the common noun rate, and so the effect was lim-
ited. This tendency was also seen in zero anaphora
resolution (Sasano et al, 2008).
In order to investigate the effects of zero
anaphora resolution, we tested our model under
three conditions: without zero anaphora resolu-
tion (no resolution), with zero anaphora resolution
(automatically resolved), and with using correct
zero anaphora relations that are manually tagged
(manually identified). The performance of auto-
matic zero anaphora resolution resulted in a recall
of 0.353, a precision of 0.375, and an F-measure of
0.364. Table 5 shows the experimental results. To
resolve associative anaphora simultaneously with
zero anaphora improved F-measure by 0.072; us-
ing correct zero anaphora relations improved F-
measure by 0.103. We can confirm that the per-
formance of associative anaphora resolution is im-
proved by considering zero anaphora.
Note that, strictly speaking, these comparisons
are not fair because we set the decay rate ? to max-
imize the performance when using generalized ex-
amples and resolving zero anaphora automatically.
However, these tendencies described above were
also seen with other decay rates.
1462
Table 5: The effects of zero anaphora resolution.
Zero anaphora Recall Precision F-measure
No resolution 0.373 0.339 0.355
(41/110) (41/121)
Automatically 0.518 0.363 0.427
resolved (57/110) (57/157)
Manually 0.573 0.382 0.458
identified (63/110) (63/165)
4.3 Discussion
By using generalized examples and resolving
simultaneously with zero anaphora, our model
achieved a recall of 0.518 (57/110), but there were
still 53 associative anaphoric relations that were
not recognized. Table 6 shows the causes of them.
22 false negatives were caused by salience score
filtering. Note that, it does not mean that these 22
associative anaphoric relations were always recog-
nized correctly if the correct antecedents were not
filtered by salience score.
Case frame sparseness caused only 5 false neg-
atives. Considering that the recall of nominal case
frames was 74% as shown in Table 3, this seems to
be too few. This is because we do not considered
the relations that tagged possible, and only con-
sidered obviously indispensable relations. From
this result, we can say that coverage of nominal
case frames for nouns that have obviously indis-
pensable entities is much higher than 74%, which
is considered to achieve a coverage of about 95%
(105/110).
4.4 Comparison with previous work
Murata et al (1999) proposed a method of utiliz-
ing ?N
m
no N
h
? phrases for associative anaphora
resolution.
7
They basically used all ?N
m
no N
h
?
phrases from corpora as a lexical knowledge, and
used rule-based approach. They obtained a recall
of 0.63 and a precision of 0.68 by using exam-
ples of ?X no Y? (Y of X), a recall of 0.71 and a
precision of 0.82 by assuming ideal nominal case
frames. One reason of such high performance may
be that they considered referential properties of
noun phrases, such as generic, indefinite, and defi-
nite, while our model does not. We can also say
that their experiments were conducted on small
and supposedly easy corpora. Half of their corpora
7
Murata et al (1999) and we (Sasano et al, 2004) used
the terminology indirect anaphora, but concerned with the
same phenomena as we concerned with in this paper.
Table 6: Causes of false negatives.
Causes Num
Filtered by salience score 22 (15)
Judge as non-anaphoric 13 (14)
Select false antecedents 13 (13)
Case frame sparseness 5 (5)
Total 53 (47)
*?()? denotes the number of causes when
using correct zero anaphora tags.
were occupied by fairy tale, against which domain
specific rules are considered to be effective.
We proposed a rule-based approach for asso-
ciative anaphora resolution based on automati-
cally acquired nominal case frames (Sasano et al,
2004).
7
We obtained a recall of 0.633 and a pre-
cision of 0.508 against news paper articles. How-
ever, we regarded some additional relations that
can be interpreted by considering coreference re-
lations as associative anaphoric relations.
(9) Chechen Ky?owakoku-no shuto-ni ...
Chechen Republic capital
... shuto seiatsu-no saishu dankai-ni ...
capital conquer last stage
(... to the capital of Chechen Republic ... in the last
stage to conquer the capital ...)
For example, although the second mention of
?shuto? (capital) in example (9) means ?Chechen
Ky?owakoku-no shuto? (the capital of Chechen Re-
public), it can be interpreted by recognizing the
coreference relation between the first and second
mentions of ?shuto? (capital). Therefore, as men-
tioned in Section 3.2, we do not consider such re-
lations as associative anaphora in this study; we
included such relations as associative anaphora in
(Sasano et al, 2004). The relatively high score is
caused by this criterion.
5 Conclusion
In this paper, we proposed a probabilistic model
for associative anaphora resolution. Our model
regards associative anaphora as a kind of zero
anaphora and resolves it in the same manner as
zero anaphora resolution that uses automatically
acquired case frames. We also showed that the
performance of associative anaphora resolution
can be improved by resolving it simultaneously
with zero anaphora. As future work, we plan to
consider referential properties of noun phrases in
associative anaphora resolution.
1463
References
Razvan Bunescu. 2003. Associative anaphora res-
olution: A web-based approach. In Proc. of
EACL?03: Workshop on The Computational Treat-
ment of Anaphora, pages 47?52.
Herbert H Clark. 1975. Bridging. In Proc. of the Con-
ference on Theoretical Issues in Natural Language
Processing, pages 169?174.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. In Proc. of
COLING?08, pages 257?264.
Caroline Gasperin and Renata Vieira. 2004. Using
word similarity lists for resolving indirect anaphora.
In Proc. of ACL?04: Workshop on Reference Resolu-
tion and its Applications, pages 40?46.
Udo Hahn, Michael Strube, and Katja Markert. 1996.
Bridging textual ellipsis. In Proc. of COLING?96,
pages 496?501.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: a study in reference and grammaticality pre-
diction. Croom Helm Ltd.
IREX Committee, editor. 1999. Proc. of the IREX
Workshop.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of case frame dictionary for robust Japanese
case analysis. In Proc. of COLING?02, pages 425?
431.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC?06,
pages 1344?1347.
Sadao Kurohashi and Yasuyuki Sakai. 1999. Seman-
tic analysis of Japanese noun phrases: A new ap-
proach to dictionary-based understanding. In Proc.
of ACL?99, pages 481?488.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?562.
Katja Markert, Malvina Nissim, and Natalia N Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proc. of EACL?03: Workshop on the
Computational Treatment of Anaphora, pages 39?
46.
Josef Meyer and Robert Dale. 2002. Using the Word-
Net hierarchy for associative anaphora resolution. In
Proc. of SemaNet?02: Building and Using Semantic
Networks.
Ruslan Mitkov, Richard Evans, and Constantin Or?asan.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In
Proc. of CICLing?02.
Natalia N Modjeska. 2002. Lexical and grammati-
cal role constraints in resolution other-anaphora. In
Proc. of DAARC?02.
Masaki Murata, Hitoshi Isahara, and Makoto Nagao.
1999. Resolution of indirect anaphora in Japanese
sentences using examples ?X no Y?(Y of X). In
Proc. of ACL?99: Workshop on Coreference and Its
Applications.
Massimo Poesio, Tomonori Ishikawa, Sabine Schulte
im Walde, and Renata Vieira. 2002. Acquiring lex-
ical knowledge for anaphora resolution. In Proc. of
LREC?02, pages 1220?1224.
Massimo Poesio, Pahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to Resolve Bridg-
ing References. In Proc. of ACL?04, pages 143?150.
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proc. of IJCNLP?08, pages
607?612.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proc. of COLING?04, pages 1201?
1207.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proc. of
COLING?08, pages 769?776.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case
frame acquisition for discourse analysis. In Proc.
of NAACL-HLT?09, pages 521?529.
Michael Strube and Udo Hahn. 1999. Functional
centering ? grounding referential coherence in in-
formation structure. Computational Linguistics,
25(3):309?344.
Renata Vieira and Massimo Poesio. 2000. An empir-
ically based system for processing definite descrip-
tions. Computational Linguistics, 26(4):539?592.
Renata Vieira, Eckhard Bick, Jorge Coelho, Vinicius
Muller, Sandra Collovini, Jose Souza, and Lucia
Rino. 2006. Semantic tagging for resolution of indi-
rect anaphora. In Proc. of the 7th SIGdial Workshop
on Discourse and Dialogue, pages 76?79.
Mitsuko Yamura-Takei. 2003. Approaches to zero ad-
nominal recognition. In Proc. of ACL?03: Student
Research Workshop, pages 87?94.
1464
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521?529,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Effect of Corpus Size on Case Frame Acquisition
for Discourse Analysis
Ryohei Sasano
Graduate School of Informatics,
Kyoto University
sasano@i.kyoto-u.ac.jp
Daisuke Kawahara
National Institute of Information
and Communications Technology
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
This paper reports the effect of corpus size on
case frame acquisition for discourse analysis
in Japanese. For this study, we collected a
Japanese corpus consisting of up to 100 bil-
lion words, and constructed case frames from
corpora of six different sizes. Then, we ap-
plied these case frames to syntactic and case
structure analysis, and zero anaphora resolu-
tion. We obtained better results by using case
frames constructed from larger corpora; the
performance was not saturated even with a
corpus size of 100 billion words.
1 Introduction
Very large corpora obtained from the Web have
been successfully utilized for many natural lan-
guage processing (NLP) applications, such as prepo-
sitional phrase (PP) attachment, other-anaphora res-
olution, spelling correction, confusable word set dis-
ambiguation and machine translation (Volk, 2001;
Modjeska et al, 2003; Lapata and Keller, 2005; At-
terer and Schu?tze, 2006; Brants et al, 2007).
Most of the previous work utilized only the sur-
face information of the corpora, such as n-grams,
co-occurrence counts, and simple surface syntax.
This may be because these studies did not require
structured knowledge, and for such studies, the size
of currently available corpora is considered to have
been almost enough. For instance, while Brants et
al. (2007) reported that translation quality continued
to improve with increasing corpus size for training
language models at even size of 2 trillion tokens, the
increase became small at the corpus size of larger
than 30 billion tokens.
However, for more complex NLP tasks, such as
case structure analysis and zero anaphora resolution,
it is necessary to obtain more structured knowledge,
such as semantic case frames, which describe the
cases each predicate has and the types of nouns that
can fill a case slot. Note that case frames offer not
only the knowledge of the relationships between a
predicate and its particular case slot, but also the
knowledge of the relationships among a predicate
and its multiple case slots. To obtain such knowl-
edge, very large corpora seem to be necessary; how-
ever it is still unknown how much corpora would be
required to obtain good coverage.
For examples, Kawahara and Kurohashi pro-
posed a method for constructing wide-coverage case
frames from large corpora (Kawahara and Kuro-
hashi, 2006b), and a model for syntactic and case
structure analysis of Japanese that based upon case
frames (Kawahara and Kurohashi, 2006a). How-
ever, they did not demonstrate whether the coverage
of case frames was wide enough for these tasks and
how dependent the performance of the model was on
the corpus size for case frame construction.
This paper aims to address these questions. We
collect a very large Japanese corpus consisting of
about 100 billion words, or 1.6 billion unique sen-
tences from the Web. Subsets of the corpus are ran-
domly selected to obtain corpora of different sizes
ranging from 1.6 million to 1.6 billion sentences.
We construct case frames from each corpus and ap-
ply them to syntactic and case structure analysis, and
zero anaphora resolution, in order to investigate the
521
relationships between the corpus size and the perfor-
mance of these analyses.
2 Related Work
Many NLP tasks have successfully utilized very
large corpora, most of which were acquired from
the Web (Kilgarriff and Grefenstette, 2003). Volk
(2001) proposed a method for resolving PP attach-
ment ambiguities based upon Web data. Modjeska
et al (2003) used the Web for resolving nominal
anaphora. Lapata and Keller (2005) investigated the
performance of web-based models for a wide range
of NLP tasks, such as MT candidate selection, ar-
ticle generation, and countability detection. Nakov
and Hearst (2008) solved relational similarity prob-
lems using the Web as a corpus.
With respect to the effect of corpus size on NLP
tasks, Banko and Brill (2001a) showed that for
content sensitive spelling correction, increasing the
training data size improved the accuracy. Atterer
and Schu?tze (2006) investigated the effect of cor-
pus size in combining supervised and unsupervised
learning for two types of attachment decision; they
found that the combined system only improved the
performance of the parser for small training sets.
Brants et al (2007) varied the amount of language
model training data from 13 million to 2 trillion to-
kens and applied these models to machine transla-
tion systems. They reported that translation qual-
ity continued to improve with increasing corpus size
for training language models at even size of 2 tril-
lion tokens. Suzuki and Isozaki (2008) provided ev-
idence that the use of more unlabeled data in semi-
supervised learning could improve the performance
of NLP tasks, such as POS tagging, syntactic chunk-
ing, and named entities recognition.
There are several methods to extract useful infor-
mation from very large corpora. Search engines,
such as Google and Altavista, are often used to ob-
tain Web counts (e.g. (Nakov and Hearst, 2005;
Gledson and Keane, 2008)). However, search en-
gines are not designed for NLP research and the re-
ported hit counts are subject to uncontrolled vari-
ations and approximations. Therefore, several re-
searchers have collected corpora from the Web by
themselves. For English, Banko and Brill (2001b)
collected a corpus with 1 billion words from vari-
ety of English texts. Liu and Curran (2006) created
a Web corpus for English that contained 10 billion
words and showed that for content-sensitive spelling
correction the Web corpus results were better than
using a search engine. Halacsy et al (2004) created
a corpus with 1 billion words for Hungarian from
the Web by downloading 18 million pages. Others
utilize publicly available corpus such as the North
American News Corpus (NANC) and the Gigaword
Corpus (Graff, 2003). For instance, McClosky et al
(2006) proposed a simple method of self-training a
two phase parser-reranker system using NANC.
As for Japanese, Kawahara and Kurohashi
(2006b) collected 23 million pages and created a
corpus with approximately 20 billion words. Google
released Japanese n-gram constructed from 20 bil-
lion Japanese sentences (Kudo and Kazawa, 2007).
Several news wires are publicly available consisting
of tens of million sentences. Kotonoha project is
now constructing a balanced corpus of the present-
day written Japanese consisting of 50 million words
(Maekawa, 2006).
3 Construction of Case Frames
Case frames describe the cases each predicate has
and what nouns can fill the case slots. In this study,
case frames we construct case frames from raw cor-
pora by using the method described in (Kawahara
and Kurohashi, 2006b). This section illustrates the
methodology for constructing case frames.
3.1 Basic Method
After parsing a large corpus by a Japanese parser
KNP1, we construct case frames from modifier-head
examples in the resulting parses. The problems for
case frame construction are syntactic and semantic
ambiguities. In other words, the resulting parses in-
evitably contain errors and predicate senses are in-
trinsically ambiguous. To cope with these problems,
we construct case frames from reliable modifier-
head examples.
First, we extract modifier-head examples that had
no syntactic ambiguity, and assemble them by cou-
pling a predicate and its closest case component.
That is, we assemble the examples not by predi-
cates, such as tsumu (load/accumulate), but by cou-
1http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
522
Table 1: Examples of Constructed Case Frames.
Case slot Examples Generalized examples with rate
ga (nominative) he, driver, friend, ? ? ? [CT:PERSON]:0.45, [NE:PERSON]:0.08, ? ? ?tsumu (1) wo (accusative) baggage, luggage, hay, ? ? ? [CT:ARTIFACT]:0.31, ? ? ?(load) ni (dative) car, truck, vessel, seat, ? ? ? [CT:VEHICLE]:0.32, ? ? ?
tsumu (2) ga (nominative) player, children, party, ? ? ? [CT:PERSON]:0.40, [NE:PERSON]:0.12, ? ? ?
(accumulate) wo (accusative) experience, knowledge, ? ? ? [CT:ABSTRACT]:0.47, ? ? ?
... ... ...
ga (nominative) company, Microsoft, firm, ? ? ? [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, ? ? ?
hanbai (1) wo (accusative) goods, product, ticket, ? ? ? [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, ? ? ?
(sell) ni (dative) customer, company, user, ? ? ? [CT:PERSON]:0.28, ? ? ?
de (locative) shop, bookstore, site ? ? ? [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, ? ? ?
... ... ...
ples, such as nimotsu-wo tsumu (load baggage) and
keiken-wo tsumu (accumulate experience). Such
couples are considered to play an important role
for constituting sentence meanings. We call the as-
sembled examples as basic case frames. In order
to remove inappropriate examples, we introduce a
threshold ? and use only examples that appeared no
less than ? times in the corpora.
Then, we cluster the basic case frames to merge
similar case frames. For example, since nimotsu-
wo tsumu (load baggage) and busshi-wo tsumu (load
supplies) are similar, they are merged. The similar-
ity is measured by using a Japanese thesaurus (The
National Language Institute for Japanese Language,
2004). Table 1 shows examples of constructed case
frames.
3.2 Generalization of Examples
When we use hand-crafted case frames, the data
sparseness problem is serious; by using case frames
automatically constructed from a large corpus, it was
alleviated to some extent but not eliminated. For in-
stance, there are thousands of named entities (NEs)
that cannot be covered intrinsically. To deal with
this problem, we generalize the examples of the case
slots. Kawahara and Kurohashi also generalized ex-
amples but only for a few types. In this study, we
generalize case slot examples based upon common
noun categories and NE classes.
First, we generalize the examples based upon the
categories that tagged by the Japanese morpholog-
ical analyzer JUMAN2. In JUMAN, about 20 cat-
egories are defined and tagged to common nouns.
For example, ringo (apple), inu (dog) and byoin
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
Table 2: Definition of NE in IREX.
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o?clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
(hospital) are tagged as FOOD, ANIMAL and FA-
CILITY, respectively. For each category, we calcu-
late the ratio of the categorized example among all
case slot examples, and add it to the case slot (e.g.
[CT:FOOD]:0.07).
We also generalize the examples based upon NE
classes. We use a common standard NE defini-
tion for Japanese provided by the IREX (1999).
We first recognize NEs in the source corpus by
using an NE recognizer (Sasano and Kurohashi,
2008); and then construct case frames from the NE-
recognized corpus. Similar to the categories, for
each NE class, we calculate the NE ratio among all
the case slot examples, and add it to the case slot
(e.g. [NE:PERSON]:0.12). The generalized exam-
ples are also included in Table 1.
4 Discourse Analysis with Case Frames
In order to investigate the effect of corpus size
on complex NLP tasks, we apply the constructed
cases frames to an integrated probabilistic model
for Japanese syntactic and case structure analysis
(Kawahara and Kurohashi, 2006a) and a probabilis-
tic model for Japanese zero anaphora resolution
(Sasano et al, 2008). In this section, we briefly de-
scribe these models.
523
4.1 Model for Syntactic and Case Structure
Analysis
Kawahara and Kurohashi (2006a) proposed an in-
tegrated probabilistic model for Japanese syntactic
and case structure analysis based upon case frames.
Case structure analysis recognizes predicate argu-
ment structures. Their model gives a probability to
each possible syntactic structure T and case struc-
ture L of the input sentence S, and outputs the syn-
tactic and case structure that have the highest proba-
bility. That is to say, the system selects the syntactic
structure Tbest and the case structure Lbest that max-
imize the probability P (T,L|S):
(Tbest, Lbest) = argmax
(T,L)
P (T,L|S)
= argmax
(T,L)
P (T,L, S) (1)
The last equation is derived because P (S) is con-
stant. P (T,L, S) is defined as the product of a prob-
ability for generating a clause Ci as follows:
P (T,L, S) = ?
i=1..n
P (Ci|bhi) (2)
where n is the number of clauses in S, and bhi is
Ci?s modifying bunsetsu3. P (Ci|bhi) is approxi-
mately decomposed into the product of several gen-
erative probabilities such as P (A(sj) = 1|CFl, sj)
and P (nj |CFl, sj , A(sj) = 1), where the function
A(sj) returns 1 if a case slot sj is filled with an input
case component; otherwise 0. P (A(sj)=1|CFl, sj)
denotes the probability that the case slot sj is filled
with an input case component, and is estimated from
resultant case structure analysis of a large raw cor-
pus. P (nj |CFl, sj , A(sj) = 1) denotes the proba-
bility of generating a content part nj from a filled
case slot sj in a case frame CFl, and is calculated
by using case frames. For details see (Kawahara and
Kurohashi, 2006a).
4.2 Model for Zero Anaphora Resolution
Anaphora resolution is one of the most important
techniques for discourse analysis. In English, overt
pronouns such as she and definite noun phrases such
as the company are anaphors that refer to preced-
ing entities (antecedents). On the other hand, in
3In Japanese, bunsetsu is a basic unit of dependency, con-
sisting of one or more content words and the following zero or
more function words. It corresponds to a base phrase in English.
Japanese, anaphors are often omitted; these omis-
sions are called zero pronouns. Zero anaphora res-
olution is the integrated task of zero pronoun detec-
tion and zero pronoun resolution.
We proposed a probabilistic model for Japanese
zero anaphora resolution based upon case frames
(Sasano et al, 2008). This model first resolves
coreference and identifies discourse entities; then
gives a probability to each possible case frame CF
and case assignment CA when target predicate v,
input case components ICC and existing discourse
entities ENT are given, and outputs the case frame
and case assignment that have the highest probabil-
ity. That is to say, this model selects the case frame
CFbest and the case assignment CAbest that maxi-
mize the probability P (CF,CA|v, ICC,ENT ):
(CF best, CAbest)
= argmax
(CF,CA)
P (CF,CA|v, ICC,ENT ) (3)
P (CF,CA|v, ICC,ENT ) is approximately de-
composed into the product of several probabilities.
Case frames are used for calculating P (nj |CFl,
sj , A(sj) = 1), the probability of generating a con-
tent part nj from a case slot sj in a case frame
CFl, and P (nj |CFl, sj , A?(sj)=1), the probability
of generating a content part nj of a zero pronoun,
where the function A?(sj) returns 1 if a case slot sj
is filled with an antecedent of a zero pronoun; other-
wise 0.
P (nj |CFl, sj , A?(sj)=1) is similar to P (nj |CFl,
sj , A(sj)=1) and estimated from the frequencies of
case slot examples in case frames. However, while
A?(sj)=1 means sj is not filled with an overt argu-
ment but filled with an antecedent of zero pronoun,
case frames are constructed from overt predicate ar-
gument pairs. Therefore, the content part nj is often
not included in the case slot examples. To cope with
this problem, this model also utilizes generalized ex-
amples to estimate P (nj |CFl, sj , A(sj) = 1). For
details see (Sasano et al, 2008).
5 Experiments
5.1 Construction of Case Frames
In order to investigate the effect of corpus size,
we constructed case frames from corpora of dif-
ferent sizes. We first collected Japanese sentences
524
Table 4: Statistics of the Constructed Case Frames.
Corpus size (sentences) 1.6M 6.3M 25M 100M 400M 1.6G
# of predicate 2460 6134 13532 27226 42739 65679
(type) verb 2039 4895 10183 19191 28523 41732
adjective 154 326 617 1120 1641 2318
noun with copula 267 913 2732 6915 12575 21629
average # of case frames for a predicate 15.9 12.2 13.3 16.1 20.5 25.3
average # of case slots for a case frame 2.95 3.44 3.88 4.21 4.69 5.08
average # of examples for a case slot 4.89 10.2 19.5 34.0 67.2 137.6
average # of unique examples for a case slot 1.19 1.85 3.06 4.42 6.81 9.64
average # of generalized examples for a case slot 0.14 0.24 0.37 0.49 0.67 0.84
File size(byte) 8.9M 20M 56M 147M 369M 928M
Table 3: Corpus Sizes and Thresholds.
Corpus size for caseframe construction 1.6M 6.3M 25M 100M 400M 1.6G(sentences)
Threshold ?introduced in Sec. 3.1 2 3 4 5 7 10
Corpus size toestimate generative 1.6M 3.2M 6.3M 13M 25M 50Mprobability (sentences)
from the Web using the method proposed by Kawa-
hara and Kurohashi (2006b). We acquired approx-
imately 6 billion Japanese sentences consisting of
approximately 100 billion words from 100 million
Japanese web pages. After discarding duplicate sen-
tences, which may have been extracted from mirror
sites, we acquired a corpus comprising of 1.6 bil-
lion (1.6G) unique Japanese sentences consisting of
approximately 25 billion words. The average num-
ber of characters and words in each sentence was
28.3, 15.6, respectively. Then we randomly selected
subsets of the corpus for five different sizes; 1.6M,
6.3M, 25M, 100M, and 400M sentences to obtain
corpora of different sizes.
We constructed case frames from each corpus. We
employed JUMAN and KNP to parse each corpus.
We changed the threshold ? introduced in Section
3.1 depending upon the size of the corpus as shown
in Table 3. Completing the case frame construc-
tion took about two weeks using 600 CPUs. Ta-
ble 4 shows the statistics for the constructed case
frames. The number of predicates, the average num-
ber of examples and unique examples for a case slot,
and whole file size were confirmed to be heavily de-
pendent upon the corpus size. However, the average
number of case frames for a predicate and case slots
for a case frame did not.
5.2 Coverage of Constructed Case Frames
5.2.1 Setting
In order to investigate the coverage of the resul-
tant case frames, we used a syntactic relation, case
structure, and anaphoric relation annotated corpus
consisting of 186 web documents (979 sentences).
This corpus was manually annotated using the same
criteria as Kawahara et al (2004). There were 2,390
annotated relationships between predicates and their
direct (not omitted) case components and 837 zero
anaphoric relations in the corpus.
We used two evaluation metrics depending upon
whether the target case component was omitted or
not. For the overt case component of a predicate, we
judged the target component was covered by case
frames if the target component itself was included in
the examples for one of the corresponding case slots
of the case frame. For the omitted case component,
we checked not only the target component itself but
also all mentions that refer to the same entity as the
target component.
5.2.2 Coverage of Case Frames
Figure 1 shows the coverage of case frames for
the overt argument, which would have tight relations
with case structure analysis. The lower line shows
the coverage without considering generalized exam-
ples, the middle line shows the coverage considering
generalized NE examples, and the upper line shows
the coverage considering all generalized examples.
Figure 2 shows the coverage of case frames for
the omitted argument, which would have tight rela-
tions with zero anaphora resolution. The upper line
shows the coverage considering all generalized ex-
amples, which is considered to be the upper bound
of performance for the zero anaphora resolution sys-
525
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
0.897
0.683
0.649
+NE,CT match
+ NE match
exact match
Figure 1: Coverage of CF (overt argument).
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
0.892
0.608
0.472
+NE,CT match
+ NE match
exact match
Figure 2: Coverage of CF (omitted argument).
tem described in Section 4.2. Comparing with Fig-
ure 1, we found two characteristics. First, the lower
and middle lines of Figure 2 were located lower than
the corresponding lines in Figure 1. This would re-
flect that some frequently omitted case components
are not described in the case frames because the case
frames were constructed from only overt predicate
argument pairs. Secondly, the effect of generalized
NE examples was more evident for the omitted ar-
gument reflecting the important role of NEs in zero
anaphora resolution.
Both figures show that the coverage was improved
by using larger corpora and there was no saturation
even when the largest corpus of 1.6 billion sentences
was used. When the largest corpus and all general-
ized examples were used, the case frames achieved a
coverage of almost 90% for both the overt and omit-
ted argument.
Figure 3 shows the coverage of case frames for
each predicate type, which was calculated for both
overt and omitted argument considering all general-
ized examples. The case frames for verbs achieved
a coverage of 93.0%. There were 189 predicate-
argument pairs that were not included case frames;
0.0
0.2
0.4
0.6
0.8
1.0
1M 10M 100M 1000M
C
ov
er
ag
e
Corpus Size (Number of Sentences)
verb
adjective
noun with copula
0.930
0.788
0.545
Figure 3: Coverage of CF for Each Predicate Type.
11 pairs of them were due to lack of the case frame
of target predicate itself, and the others were due
to lack of the corresponding example. For adjec-
tive, the coverage was 78.8%. The main cause of
the lower coverage would be that the predicate argu-
ment relations concerning adjectives that were used
in restrictive manner, such as ?oishii sushi? (deli-
cious sushi), were not used for case frame construc-
tion, although such relations were also the target of
the coverage evaluation. For noun with copula, the
coverage was only 54.5%. However, most predicate
argument relations concerning nouns with copula
were easily recognized from syntactic preference,
and thus the low coverage would not quite affect the
performance of discourse analysis.
5.3 Syntactic and Case Structure Analysis
5.3.1 Accuracy of Syntactic Analysis
We investigated the effect of corpus size for syn-
tactic analysis described in Section 4.1. We used
hand-annotated 759 web sentences, which was used
by Kawahara and Kurohashi (2007). We evaluated
the resultant syntactic structures with regard to de-
pendency accuracy, the proportion of correct depen-
dencies out of all dependencies4.
Figure 4 shows the accuracy of syntactic struc-
tures. We conducted these experiments with case
frames constructed from corpora of different sizes.
We also changed the corpus size to estimate gen-
erative probability of a case slot in Section 4.1 de-
pending upon the size of the corpus for case frame
construction as shown in Table 3. Figure 4 also in-
4Note that Kawahara and Kurohashi (2007) exclude the de-
pendency between the last two bunsetsu, since Japanese is head-
final and thus the second last bunsetsu unambiguously depends
on the last bunsetsu.
526
0.886
0.888
0.890
0.892
0.894
0.896
1M 10M 100M 1000M
A
cc
ur
ac
y
Corpus Size (Number of Sentences)
0.894
1.6G
p < 0.1
100M
25M
p < 0.01
6.3M
1.6M
400M
p < 0.1
25M
p < 0.01
6.3M
1.6M
100M
p < 0.1
6.3M
1.6M
25M
p < 0.1
6.3M
1.6M6.3M1.6M
with case frames
w/o case frames
Figure 4: Accuracy of Syntactic Analysis. (McNemar?s
test results are also shown under each data point.)
cludes McNemar?s test results. For instance, the dif-
ference between the corpus size of 1.6G and 100M
sentences is significant at the 90% level (p = 0.1),
but not significant at the 99% level (p = 0.01).
In Figure 4, ?w/o case frames? shows the accu-
racy of the rule-based syntactic parser KNP that does
not use case frames. Since the model described
in Section 4.1 assumes the existence of reasonable
case frames, when we used case frames constructed
from very small corpus, such as 1.6M and 6.3M sen-
tences, the accuracy was lower than that of the rule-
based syntactic parser. Moreover, when we tested
the model described in Section 4.1 without any case
frames, the accuracy was 0.885.
We confirmed that better performance was ob-
tained by using case frames constructed from larger
corpora, and the accuracy of 0.8945 was achieved
by using the case frames constructed from 1.6G sen-
tences. However the effect of the corpus size was
limited. This is because there are various causes
of dependency error and the case frame sparseness
problem is not serious for syntactic analysis.
We considered that generalized examples can
benefit for the accuracy of syntactic analysis, and
tried several models that utilize these examples.
However, we cannot confirm any improvement.
5.3.2 Accuracy of Case Structure Analysis
We conducted case structure analysis on 215 web
sentences in order to investigate the effect of cor-
pus size for case structure analysis. The case mark-
ers of topic marking phrases and clausal modifiers
5It corresponds to 0.877 in Kawahara and Kurohashi?s
(2007) evaluation metrics.
0.400
0.500
0.600
0.700
0.800
0.900
1M 10M 100M 1000M
A
cc
ur
ac
y
Corpus Size (Number of Sentences)
0.784
Figure 5: Accuracy of Case Structure Analysis.
Table 5: Corpus Sizes for Case Frame Construction and
Time for Syntactic and Case Structure Analysis.
Corpus size 1.6M 6.3M 25M 100M 400M 1.6G
Time (sec.) 850 1244 1833 2696 3783 5553
were evaluated by comparing them with the gold
standard in the corpus. Figure 5 shows the experi-
mental results. We confirmed that the accuracy of
case structure analysis strongly depends on corpus
size for case frame construction.
5.3.3 Analysis Speed
Table 5 shows the time for analyzing syntactic
and case structure of 759 web sentences. Although
the time for analysis became longer by using case
frames constructed from a larger corpus, the growth
rate was smaller than the growth rate of the size for
case frames described in Table 4.
Since there is enough increase in accuracy of case
structure analysis, we can say that case frames con-
structed larger corpora are desirable for case struc-
ture analysis.
5.4 Zero Anaphora Resolution
5.4.1 Accuracy of Zero Anaphora Resolution
We used an anaphoric relation annotated corpus
consisting of 186 web documents (979 sentences)
to evaluate zero anaphora resolution. We used first
51 documents for test and used the other 135 doc-
uments for calculating several probabilities. In the
51 test documents, 233 zero anaphora relations were
annotated between one of the mentions of the an-
tecedent and corresponding predicate that had zero
pronoun.
In order to concentrate on evaluation for zero
anaphora resolution, we used the correct mor-
527
0.00
0.10
0.20
0.30
0.40
0.50
1M 10M 100M 1000M
F
-m
ea
su
re
Corpus Size (Number of Sentences)
0.417
0.330
0.313
+NE,CT match
+ NE match
exact match
Figure 6: F-measure of Zero Anaphora Resolution.
phemes, named entities, syntactic structures and
coreference relations that were manually annotated.
Since correct coreference relations were given, the
number of created entities was the same between the
gold standard and the system output because zero
anaphora resolution did not create new entities.
The experimental results are shown in Figure 6, in
which F-measure was calculated by:
R = # of correctly recognized zero anaphora# of zero anaphora annotated in corpus ,
P = # of correctly recognized zero anaphora# of system outputted zero anaphora ,
F = 21/R + 1/P .
The upper line shows the performance using all
generalized examples, the middle line shows the
performance using only generalized NEs, and the
lower line shows the performance without using
any generalized examples. While generalized cat-
egories much improved the F-measure, generalized
NEs contributed little. This tendency is similar to
that of coverage of case frames for omitted argument
shown in Figure 2. Unlike syntactic and case struc-
ture analysis, the performance for the zero anaphora
resolution is quite low when using case frames con-
structed from small corpora, and we can say case
frames constructed from larger corpora are essential
for zero anaphora resolution.
5.4.2 Analysis Speed
Table 6 shows the time for resolving zero
anaphora in 51 web documents consisting of 278
sentences. The time for analysis became longer by
using case frames constructed from larger corpora,
Table 6: Corpus Sizes for Case Frame Construction and
Time for Zero Anaphora Resolution.
Corpus size 1.6M 6.3M 25M 100M 400M 1.6G
Time (sec.) 538 545 835 1040 1646 2219
which tendency is similar to the growth of the time
for analyzing syntactic and case structure.
5.5 Discussion
Experimental results of both case structure analy-
sis and zero anaphora resolution show the effective-
ness of a larger corpus in case frame acquisition for
Japanese discourse analysis. Up to the corpus size
of 1.6 billion sentences, or 100 billion words, these
experimental results still show a steady increase in
performance. That is, we can say that the corpus
size of 1.6 billion sentences is not enough to obtain
case frames of sufficient coverage.
These results suggest that increasing corpus size
is more essential for acquiring structured knowledge
than for acquiring unstructured statistics of a corpus,
such as n-grams, and co-occurrence counts; and for
complex NLP tasks such as case structure analysis
and zero anaphora resolution, the currently available
corpus size is not sufficient.
Therefore, to construct more wide-coverage case
frames by using a larger corpus and reveal howmuch
corpora would be required to obtain sufficient cov-
erage is considered as future work.
6 Conclusion
This paper has reported the effect of corpus size
on case frame acquisition for syntactic and case
structure analysis, and zero anaphora resolution in
Japanese. We constructed case frames from cor-
pora of six different sizes ranging from 1.6 million
to 1.6 billion sentences; and then applied these case
frames to Japanese syntactic and case structure anal-
ysis, and zero anaphora resolution. Experimental re-
sults showed better results were obtained using case
frames constructed from larger corpora, and the per-
formance showed no saturation even when the cor-
pus size was 1.6 billion sentences.
The findings suggest that increasing corpus size
is more essential for acquiring structured knowledge
than for acquiring surface statistics of a corpus; and
for complex NLP tasks the currently available cor-
pus size is not sufficient.
528
References
Michaela Atterer and Hinrich Schu?tze. 2006. The ef-
fect of corpus size in combining supervised and un-
supervised training for disambiguation. In Proc. of
COLING-ACL?06, pages 25?32.
Michele Banko and Eric Brill. 2001a. Mitigating the
paucity-of-data problem: Exploring the effect of train-
ing corpus size on classifier performance for natural
language processing. In Proc. of HLT?01.
Michele Banko and Eric Brill. 2001b. Scaling to very
very large corpora for natural language disambigua-
tion. In Proc. of ACL?01, pages 26?33.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proc. of EMNLP-CoNLL?07,
pages 858?867.
Ann Gledson and John Keane. 2008. Using web-search
results to measure word-group similarity. In Proc. of
COLING?08, pages 281?288.
David Graff. 2003. English Gigaword. Technical Report
LDC2003T05, Linguistic Data Consortium, Philadel-
phia, PA USA.
Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras
Rung, Istvan Szakadat, and Vikto Tron. 2004. Creat-
ing open language resources for Hungarian. In Proc.
of LREC?04, pages 203?210.
IREX Committee, editor. 1999. Proc. of the IREX Work-
shop.
Daisuke Kawahara and Sadao Kurohashi. 2006a. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proc. of HLT-
NAACL?06, pages 176?183.
Daisuke Kawahara and Sadao Kurohashi. 2006b.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC?06, pages
1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2007.
Probabilistic coordination disambiguation in a fully-
lexicalized Japanese parser. In Proc. of EMNLP-
CoNLL?07, pages 306?314.
Daisuke Kawahara, Ryohei Sasano, and Sadao Kuro-
hashi. 2004. Toward text understanding: Integrat-
ing relevance-tagged corpora and automatically con-
structed case frames. In Proc. of LREC?04, pages
1833?1836.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistic, 29(3):333?347.
Taku Kudo and Hideto Kazawa. 2007. Web Japanese N-
gram version 1, published by Gengo Shigen Kyokai.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2:1:1?31.
Vinci Liu and James R. Curran. 2006. Web text corpus
for natural language processing. In Proc. of EACL?06,
pages 233?240.
Kikuo Maekawa. 2006. Kotonoha, the corpus develop-
ment project of the National Institute for Japanese lan-
guage. In Proc. of the 13th NIJL International Sympo-
sium, pages 55?62.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proc. of
HLT-NAACL?06, pages 152?159.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proc. of EMNLP-2003,
pages 176?183.
Preslav Nakov and Marti Hearst. 2005. A study of using
search engine page hits as a proxy for n-gram frequen-
cies. In Proc. of RANLP?05.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. of ACL-HLT?08, pages 452?460.
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural lan-
guage processing. In Proc. of IJCNLP?08, pages 607?
612.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proc. of
COLING?08, pages 769?776.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL-HLT?08,
pages 665?673.
The National Language Institute for Japanese Language.
2004. Bunruigoihyo. Dainippon Tosho, (In Japanese).
Martin Volk. 2001. Exploiting the WWW as a corpus
to resolve PP attachment ambiguities. In Proc. of the
Corpus Linguistics, pages 601?606.
529
Automatic Construction of Nominal Case Frames and
its Application to Indirect Anaphora Resolution
Ryohei Sasano, Daisuke Kawahara and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo
{ryohei,kawahara,kuro}@kc.t.u-tokyo.ac.jp
Abstract
This paper proposes a method to auto-
matically construct Japanese nominal case
frames. The point of our method is the in-
tegrated use of a dictionary and example
phrases from large corpora. To examine the
practical usefulness of the constructed nom-
inal case frames, we also built a system of
indirect anaphora resolution based on the
case frames. The constructed case frames
were evaluated by hand, and were confirmed
to be good quality. Experimental results of
indirect anaphora resolution also indicated
the effectiveness of our approach.
1 Introduction
What is represented in a text has originally
a network structure, in which several concepts
have tight relations with each other. However,
because of the linear constraint of texts, most
of them disappear in the normal form of texts.
Automatic reproduction of such relations can be
regarded as the first step of ?text understand-
ing?, and surely benefits NLP applications such
as machine translation, automatic abstraction,
and question answering.
One of such latent relationship is indirect
anaphora, functional anaphora, or bridging ref-
erence, such as the following examples.
(1) I bought a ticket. The price was 20 dollars.
(2) There was a house. The roof was white.
Here, ?the price? means ?the price of a ticket?
and ?the roof? means ?the roof of a house.?
Most nouns have their indispensable or req-
uisite entities: ?price? is a price of some goods
or service, ?roof? is a roof of some building,
?coach? is a coach of some sport, and ?virus?
is a virus causing some disease. The relation
between a noun and its indispensable entity is
parallel to that between a verb and its argu-
ments or obligatory cases. In this paper, we call
indispensable entities of nouns obligatory cases.
Indirect anaphora resolution needs a compre-
hensive information or dictionary of obligatory
cases of nouns.
In case of verbs, syntactic structures such as
subject/object/PP in English or case markers
such as ga, wo, ni in Japanese can be utilized
as a strong clue to distinguish several obliga-
tory cases and adjuncts (and adverbs), which
makes it feasible to construct case frames from
large corpora automatically (Briscoe and Car-
roll, 1997; Kawahara and Kurohashi, 2002).
(Kawahara and Kurohashi, 2004) then utilized
the automatically constructed case frames to
Japanese zero pronoun resolution.
On the other hand, in case of nouns, obliga-
tory cases of noun Nh appear, in most cases, in
the single form of noun phrase ?Nh of Nm? in
English, or ?Nm no Nh? in Japanese. This sin-
gle form can express several obligatory cases,
and furthermore optional cases, for example,
?rugby no coach? (obligatory case concerning
what sport), ?club no coach? (obligatory case
concerning which institution), and ?kyonen ?last
year? no coach? (optional case). Therefore, the
key issue to construct nominal case frames is to
analyze ?Nh of Nm? or ?Nm no Nh? phrases to
distinguish obligatory case examples and others.
Work which addressed indirect anaphora in
English texts so far restricts relationships to a
small, relatively well-defined set, mainly part-of
relation like the above example (2), and utilized
hand-crafted heuristic rules or hand-crafted lex-
ical knowledge such as WordNet (Hahn et al,
1996; Vieira and Poesio, 2000; Strube and
Hahn, 1999). (Poesio et al, 2002) proposed
a method of acquiring lexical knowledge from
?Nh of Nm? phrases, but again concentrated on
part-of relation.
In case of Japanese text analysis, (Murata et
al., 1999) proposed a method of utilizing ?Nm
no Nh? phrases for indirect anaphora resolution
of diverse relationships. However, they basically
used all ?Nm no Nh? phrases from corpora, just
excluding some pre-fixed stop words. They con-
fessed that an accurate analysis of ?Nm no Nh?
phrases is necessary for the further improve-
ment of indirect anaphora resolution.
As a response to these problems and follow-
ing the work in (Kurohashi and Sakai, 1999), we
propose a method to construct Japanese nom-
inal case frames from large corpora, based on
an accurate analysis of ?Nm no Nh? phrases
using an ordinary dictionary and a thesaurus.
To examine the practical usefulness of the con-
structed nominal case frames, we also built a
system of indirect anaphora resolution based on
the case frames.
2 Semantic Feature Dictionary
First of all, we briefly introduce NTT Seman-
tic Feature Dictionary employed in this paper.
NTT Semantic Feature Dictionary consists of a
semantic feature tree, whose 3,000 nodes are se-
mantic features, and a nominal dictionary con-
taining about 300,000 nouns, each of which is
given one or more appropriate semantic fea-
tures.
The main purpose of using this dictionary is
to calculate the similarity between two words.
Suppose the word x and y have a semantic fea-
ture sx and sy, respectively, their depth is dx
and dy in the semantic tree, and the depth of
their lowest (most specific) common node is dc,
the similarity between x and y, sim(x, y), is cal-
culated as follows:
sim(x, y) = (dc ? 2)/(dx + dy).
If sx and sy are the same, the similarity is 1.0,
the maximum score based on this criteria.
We also use this dictionary to specify seman-
tic category of words, such as human, time and
place.
3 Semantic Analysis of Japanese
Noun Phrases Nm no Nh
In many cases, obligatory cases of nouns are
described in an ordinary dictionary for human
being. For example, a Japanese dictionary for
children, Reikai Shougaku Kokugojiten, or RSK
(Tajika, 1997), gives the definitions of the word
coach and virus as follows1:
coach a person who teaches technique in some
sport
virus a living thing even smaller than bacte-
ria which causes infectious disease like in-
fluenza
1Although our method handles Japanese noun
phrases by using Japanese definition sentences, in this
paper we use their English translations for the explana-
tion. In some sense, the essential point of our method is
language-independent.
Based on such an observation, (Kurohashi
and Sakai, 1999) proposed a semantic analy-
sis method of ?Nm no Nh?, consisting of the
two modules: dictionary-based analysis (abbre-
viated to DBA hereafter) and semantic feature-
based analysis (abbreviated to SBA hereafter).
This section briefly introduces their method.
3.1 Dictionary-based analysis
Obligatory case information of nouns in an ordi-
nary dictionary can be utilized to solve the dif-
ficult problem in the semantic analysis of ?Nm
no Nh? phrases. In other words, we can say the
problem disappears.
For example, ?rugby no coach? can be inter-
preted by the definition of coach as follows: the
dictionary describes that the noun coach has an
obligatory case sport, and the phrase ?rugby no
coach? specifies that the sport is rugby. That is,
the interpretation of the phrase can be regarded
as matching rugby in the phrase to some sport
in the coach definition. ?Kaze ?cold? no virus?
is also easily interpreted based on the definition
of virus, linking kaze ?cold? to infectious disease.
Dictionary-based analysis (DBA) tries to find
a correspondence between Nm and an obliga-
tory case of Nh by utilizing RSK and NTT Se-
mantic Feature Dictionary, by the following pro-
cess:
1. Look up Nh in RSK and obtain the defini-
tion sentences of Nh.
2. For each word w in the definition sentences
other than the genus words, do the follow-
ing steps:
2.1. When w is a noun which shows an
obligatory case explicitly, like kotog-
ara ?thing?, monogoto ?matter?, nanika
?something?, and Nm does not have a
semantic feature of human or time,
give 0.8 to their correspondence2.
2.2. When w is other noun, calculate the
similarity between Nm and w by us-
ing NTT Semantic Feature Dictionary,
and give the similarity score to their
correspondence.
3. Finally, if the best correspondence score is
0.75 or more, DBA outputs the best corre-
spondence, which can be an obligatory case
of the input; if not, DBA outputs nothing.
2For the present, parameters in the algorithm were
given empirically, not optimized by a learning method.
Table 1: Examples of rules for semantic feature-based analysis.
1. Nm:human, Nh:relative ? <obligatory case(relative)> e.g. kare ?he? no oba ?aunt?
2. Nm:human, Nh:human ? <modification(apposition)> e.g. gakusei ?student? no kare ?he?
3. Nm:organization, Nh:human ? <belonging> e.g. gakkou ?school? no seito ?student?
4. Nm:agent, Nh:event ? <agent> e.g. watashi ?I? no chousa ?study?
5. Nm:material, Nh:concrete ? <modification(material)> e.g. ki ?wood? no hako ?box?
6. Nm:time, Nh:? ? <time> e.g. aki ?autumn? no hatake ?field?
7. Nm:color, quantity, or figure, Nh:? ? <modification> e.g. gray no seihuku ?uniform?
8. Nm:?, Nh:quantity ? <obligatory case(attribute)> e.g. hei ?wall? no takasa ?height?
9. Nm:?, Nh:position ? <obligatory case(position)> e.g. tsukue ?desk? no migi ?right?
10. Nm:agent, Nh:? ? <possession> e.g. watashi ?I? no kuruma ?car?
11. Nm:place or position, Nh:? ? <place> e.g. Kyoto no mise ?store?
??? meets any noun.
In case of the phrase ?rugby no coach?, ?tech-
nique? and ?sport? in the definition sentences
are checked: the similarity between ?technique?
and ?rugby? is calculated to be 0.21, and the
similarity between ?sport? and ?rugby? is cal-
culated to be 1.0. Therefore, DBA outputs
?sport?.
3.2 Semantic feature-based analysis
Since diverse relations in ?Nm no Nh? are han-
dled by DBA, the remaining relations can be
detected by simple rules checking the semantic
features of Nm and/or Nh.
Table 1 shows examples of the rules. For ex-
ample, the rule 1 means that if Nm has a seman-
tic feature human and Nh relative, <obliga-
tory case> relation is assigned to the phrase.
The rules 1, 2, 8 and 9 are for certain oblig-
atory cases. We use these rules because these
relations can be analyzed more accurately by us-
ing explicit semantic features, rather than based
on a dictionary.
3.3 Integration of two analyses
Usually, either DBA or SBA outputs some re-
lation. When both DBA and SBA output some
relations, the results are integrated (basically, if
DBA correspondence score is higher than 0.8,
DBA result is selected; if not, SBA result is se-
lected). In rare cases, neither analysis outputs
any relations, which means analysis failure.
4 Automatic Construction of
Nominal Case Frames
4.1 Collection and analysis of Nm no Nh
Syntactically unambiguous noun phrases ?Nm
no Nh? are collected from the automatic parse
results of large corpora, and they are analyzed
using the method described in the previous sec-
tion.
Table 2: Preliminary case frames for hisashi
?eaves/visor?.
DBA result
1. a roof that stick out above the window of
a house.
[house] hall:2, balcony:1, building:1, ? ? ?
[window] window:2, ceiling:1, counter:1, ? ? ?
2. the fore piece of a cap.
[cap] cap:8, helmet:1, ? ? ?
SBA result
<place> parking:3, store:3, shop:2, ? ? ?
<mod.> concrete:1, metal:1, silver:1, ? ? ?
No semantic analysis result
<other> part:1, light:1, phone:1, ? ? ?
By just collecting the analysis results of each
head word Nh, we can obtain its preliminary
case frames. Table 2 shows preliminary case
frames for hisashi ?eaves/visor?. The upper part
of the table shows the results by DBA. The line
starting with ?[house]? denotes a group of anal-
ysis results corresponding to the word ?house?
in the first definition sentence. For example,
?hall no hisashi? occurs twice in the corpora,
and they were analyzed by DBA to correspond
to ?house.?
The middle part of the table shows the results
by SBA. Noun phrases that have no semantic
analysis result (analysis failure) are bundled and
named <other>, as shown in the last part of the
table.
A case frame should be constructed for each
meaning (definition) of Nh, and groups start-
ing with ?[...]? or ?<...>? in Table 2 are possi-
ble case slots. The problem is how to arrange
the analysis results of DBA and SBA and how
to distinguish obligatory cases and others. The
following sections explain how to handle these
problems.
Table 3: Threshold to select obligatory slots.
type of case slots threshold of probability
analyzed by DBA 0.5% (1/200)
<obligatory case> 2.5% (1/40)
<belonging> 2.5% (1/40)
<possessive> 5% (1/20)
<agent> 5% (1/20)
<place> 5% (1/20)
<other> 10% (1/10)
<modification> not used
<time> not used
Probability = (# of Nm no Nh) / (# of Nh)
4.2 Case slot clustering
One obligatory case might be separated in pre-
liminary case frames, since the definition sen-
tence is sometimes too specific or too detailed.
For example, in the case of hisashi ?eaves/visor?
in Table 2, [house], [window], and <place>
have very similar examples that mean building
or part of building. Therefore, case slots are
merged if similarity of two case slots is more
than 0.5 (case slots in different definition sen-
tences are not merged in any case). Similarity
of two case slots is the average of top 25% sim-
ilarities of all possible pairs of examples.
In the case of Table 2, the similarity between
[house] and [window] is 0.80, and that between
[house] and <place> is 0.67, so that these three
case slots are merged into one case slot.
4.3 Obligatory case selection
Preliminary case frames contain both obliga-
tory cases and optional cases for the head word.
Since we can expect that an obligatory case
co-occurs with the head word in the form of
noun phrase frequently, we can take frequent
case slots as obligatory case of the head word.
However, we have to be careful to set up
the frequency thresholds, because case slots de-
tected by DBA or <obligatory case> by SBA
are more likely to be obligatory; on the other
hand case slots of <modification> or <time>
should be always optional. Considering these
tendencies, we set thresholds for obligatory
cases as shown in Table 3.
In the case of hisashi ?eaves/visor? in Table 2,
[house-window]-<place> slot and [cap] slot are
chosen as the obligatory cases.
4.4 Case frame construction for each
meaning
Case slots that are derived from each definition
sentence constitute a case frame.
If a case slot of <obligatory case> by SBA
or <other> is not merged into case slots in def-
inition sentences, it can be considered that it
indicates a meaning of Nh which is not covered
in the dictionary. Therefore, such a case slot
constitutes an independent case frame.
On the other hand, when other case slots by
SBA such as <belonging> and <possessive>
are remaining, we have to treat them differently.
The reason why they are remaining is that they
are not always described in the definition sen-
tences, but their frequent occurrences indicate
they are obligatory cases. Therefore, we add
these case slots to the case frames derived from
definition sentences.
Table 4 shows several examples of resul-
tant case frames. Hyoujou ?expression? has a
case frame containing two case slots. Hisashi
?eaves/visor? has two case frames according to
the two definition sentences. In case of hiki-
dashi ?drawer?, the first case frame corresponds
to the definition given in the dictionary, and
the second case frame was constructed from the
<other> case slot, which is actually another
sense of hikidashi, missed in the dictionary. In
case of coach, <possessive> is added to the case
frame which was made from the definition, pro-
ducing a reasonable case frame for the word.
4.5 Point of nominal case frame
construction
The point of our method is the integrated
use of a dictionary and example phrases from
large corpora. Although dictionary definition
sentences are informative resource to indicate
obligatory cases of nouns, it is difficult to do
indirect anaphora resolution by using a dictio-
nary as it is, because all nouns in a definition
sentence are not an obligatory case, and only
the frequency information of noun phrases tells
us which is the obligatory case. Furthermore,
sometimes a definition is too specific or detailed,
and the example phrases can adjust it properly,
as in the example of hisashi in Table 2.
On the other hand, a simple method that
just collects and clusters ?Nm no Nh? phrases
(based on some similarity measure of nouns)
can not construct comprehensive nominal case
frames, because of polysemy and multiple oblig-
atory cases. We can see that dictionary defini-
tion can guide the clustering properly even for
such difficult cases.
Table 4: Examples of nominal case frames.
case slot examples
hisashi :1 ?eaves/visor? (the edges of a roof that stick out above the window of a house etc.)
[house, window] parking, store, hall, ? ? ?
hisashi :2 ?eaves/visor? (the fore piece of a cap.)
[cap] cap, helmet, ? ? ?
hyoujou ?expression? (to express one?s feelings on the face or by gestures.)
[one] people, person, citizen, ? ? ?
[feelings] relief, margin, ? ? ?
hikidashi :1 ?drawer? (a boxlike container in a desk or a chest.)
[desk, chest] desk, chest, dresser, ? ? ?
hikidashi :2 ?drawer? <other> credit, fund, saving, ? ? ?
coach (a person who teaches technique in some sport.)
[sport] baseball, swimming, ? ? ?
<belonging> team, club, ? ? ?
kabushiki ?stock? (the total value of a company?s shares.)
[company] company, corporation, ? ? ?
5 Indirect Anaphora Resolution
To examine the practical usefulness of the con-
structed nominal case frames, we built a pre-
liminary system of indirect anaphora resolution
based on the case frames.
An input sentence is parsed using the
Japanese parser, KNP (Kurohashi and Nagao,
1994). Then, from the beginning of the sen-
tence, each noun x is analyzed. When x has
more than one case frame, the process of an-
tecedent estimation (stated in the next para-
graph) is performed for each case frame, and the
case frame with the highest similarity score (de-
scribed below) and assignments of antecedents
to the case frame are selected as a final result.
For each case slot of the target case frame of
x, its antecedent is estimated. A possible an-
tecedent y in the target sentence and the previ-
ous two sentences is checked. This is done one
by one, from the syntactically closer y. If the
similarity of y to the case slot is equal to or
greater than a threshold ? (currently 0.95), it
is assigned to the case slot.
The similarity between y and a case slot is
defined as the highest similarity between y and
an example in the case slot.
For instance, let us consider the sentence
shown in Figure 1. soccer, at the beginning of
the sentence, has no case frame, and is consid-
ered to have no obligatory case.
For the second noun ticket, soccer, which is
a nominal modifier of ticket, is examined first.
The similarity between soccer and the examples
of the case slot [theater, transport] exceeds the
soccer-no
ticket-ga
takai
nedan-de
urareteita.
expensive
price
be sold
case slot examples result
ticket [theater, transport] stage, game,? ? ? soccer
nedan [things] thing, ticket,? ? ? ticket
ticket a printed piece of paper which shows that you have
paid to enter a theater or use a transport
nedan the amount of money for which things are sold or
bought
Figure 1: Indirect anaphora resolution example.
threshold ?, and soccer is assigned to [theater,
transport].
Lastly, for nedan ?price?, its possible an-
tecedents are ticket and soccer. ticket, which
is the closest from nedan, is checked first. The
similarity between ticket and the examples of
the case slot [things] exceeds the threshold ?,
and ticket is judged as the antecedent of nedan.
6 Experiments
We evaluated the automatically constructed
nominal case frames, and conducted an experi-
ment of indirect anaphora resolution.
6.1 Evaluation of case frames
We constructed nominal case frames from news-
paper articles in 25 years (12 years of Mainichi
newspaper and 13 years of Nihonkeizai newspa-
per). These newspaper corpora consist of about
Table 5: Evaluation result of case frames.
precision recall F
58/70 (0.829) 58/68 (0.853) 0.841
25,000,000 sentences, and 10,000,000 ?Nm no
Nh? noun phrases were extracted from them.
The result consists of 17,000 nouns, the average
number of case frames for a noun is 1.06, and
the average number of case slots for a case frame
is 1.09.
We randomly selected 100 nouns that occur
more than 10,000 times in the corpora, and cre-
ated gold standard case frames by hand. For
each test noun, possible case frames were con-
sidered, and for each case frame, obligatory case
slots were given manually. As a result, 68 case
frames for 65 test nouns were created, and 35
test nouns have no case frames.
We evaluated automatically constructed case
frames for these test nouns against the gold
standard case frames. A case frame which has
the same case slots with the gold standard is
judged as correct. The evaluation result is
shown in Table 5: the system output 70 case
frames, and out of them, 58 case frames were
judged as correct.
The recall was deteriorated by the highly re-
stricted conditions in the example collection.
For instance, maker does not have obligatory
case slot for its products. This is because maker
is usually used in the form of compound noun
phrase, ?products maker?, and there are few
occurrences of ?products no maker?. To ad-
dress this problem, not only ?Nm no Nh? but
also ?Nm Nh? (compound noun phrase) and
?Nm ni-kansuru ?in terms of? Nh? should be
collected.
6.2 Experimental results of indirect
anaphora resolution
We conducted a preliminary experiment of
our indirect anaphora resolution system using
?Relevance-tagged corpus? (Kawahara et al,
2002). This corpus consists of Japanese news-
paper articles, and has relevance tags, including
antecedents of indirect anaphors.
We prepared a small test corpus that con-
sists of randomly selected 10 articles. The test
corpus contains 217 nouns. Out of them, 106
nouns are indirect anaphors, and have 108 an-
tecedents, which is because two nouns have dou-
ble antecedents. 49 antecedents directly depend
on their anaphors, and 59 do not. For 91 an-
tecedents out of 108, a case frame of its anaphor
Table 6: Experimental results of indirect
anaphora resolution.
precision recall F
w dep. 40/46 (0.870) 40/59 (0.678) 0.762
w/o dep. 31/61 (0.508) 31/49 (0.633) 0.564
total 71/107 (0.664) 71/108 (0.657) 0.660
includes the antecedent itself or its similar word
(the similarity exceeds the threshold, 0.95). Ac-
cordingly, the upper bound of the recall of our
case-frame-based anaphora resolution is 84.3%
(91/108).
We ran the system on the test corpus, and
compared the system output and the corpus an-
notation. Table 6 shows the experimental re-
sults. In this table, ?w dep.? (with dependency)
is the evaluation of the antecedents that directly
depend on their anaphors. ?w/o dep.? (with-
out dependency) is the case of the antecedents
that do not directly depend on their anaphors.
Although the analysis of ?w dep.? is intrinsi-
cally easier than that of ?w/o dep.?, the recall
of ?w dep.? was not much higher than that
of ?w/o dep.?. The low recall score of ?w dep.?
was caused by nonexistence of case frames which
include the antecedent itself or its similar word.
The antecedents that directly depend on their
anaphors were often a part of compound noun
phrases, such as ?products maker?, which are
not covered by our examples collection.
Major errors in the analyses of the an-
tecedents that do not directly depend on their
anaphors were caused by the following reasons.
Specific/generic usages of nouns
Some erroneous system outputs were caused by
nouns that have both specific and generic us-
ages.
(3) kogaisya-no
subsidiary
kabushiki-wo
stock
baikyaku-shita.
sell
(? sold the stock of the subsidiary.)
In this case, kogaisya ?subsidiary? is an oblig-
atory information for kabushiki ?stock?, which is
specifically used. kogaisya matches the [kaisya
?company?] case slot in Table 4.
However, kabushiki ?stock? in the following ex-
ample is used generically, and does not need spe-
cific company information.
(4)kabushiki
stock
souba-no
price
oshiage
rise
youin-to naru.
factor become
(? become the rise factor of the stock prices.)
Since the current system cannot judge generic
or specific nouns, an antecedent which corre-
sponds to [kaisha ?company?] is incorrectly esti-
mated.
Beyond selectional restriction of case
frames
Selectional restriction based on the case frames
usually worked well, but did not work to distin-
guish candidates both of which belong to Hu-
man or Organization.
(5) Bush bei
American
seiken-wa
administration
Russia-tono
... Bush daitouryou-ga
president
shutyou-shita.
claim
(Bush American administration ... with
Russia ... President Bush claimed ...)
In this example, daitouryou ?president? re-
quires an obligatory case kuni ?nation?. The sys-
tem estimates its antecedent as Russia, though
the correct answer is bei ?America?. This is be-
cause Russia is closer than beikoku. This prob-
lem is somehow related to world knowledge, but
if the system can carefully exploit the context,
it might be able to find the correct answer from
?Bush bei seiken? ?Bush American administra-
tion?.
7 Conclusion
This paper has first proposed an automatic
construction method of Japanese nominal case
frames. This method is based on semantic anal-
ysis of noun phrases ?Nm no Nh? ?Nh of Nm?.
To examine the practical usefulness of the con-
structed nominal case frames, we built a pre-
liminary system of indirect anaphora resolution
based on the case frames. The evaluation indi-
cated the good quality of the constructed case
frames. On the other hand, the accuracy of our
indirect anaphora resolution system is not satis-
factory. In the future, we are planning to make
the case frames more wide-coverage, and im-
prove the indirect anaphora resolution by con-
sidering larger context and more various factors.
References
Ted Briscoe and John Carroll. 1997. Auto-
matic extraction of subcategorization from
corpora. In Proceedings of the 5th Confer-
ence on Applied Natural Language Process-
ing, pages 356?363.
Udo Hahn, Michael Strube, and Katja Markert.
1996. Bridging textual ellipses. In Proceed-
ings of the 16th International Conference on
Computational Linguistics, pages 496?501.
Daisuke Kawahara and Sadao Kurohashi. 2002.
Fertilization of case frame dictionary for ro-
bust Japanese case analysis. In Proceedings of
the 19th International Conference on Compu-
tational Linguistics, pages 425?431.
Daisuke Kawahara and Sadao Kurohashi. 2004.
Zero pronoun resolution based on automati-
cally constructed case frames and structural
preference of antecedents. In Proceedings of
the 1st International Joint Conference on
Natural Language Processing.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of
the 3rd International Conference on Lan-
guage Resources and Evaluation, pages 2008?
2013.
Sadao Kurohashi and Makoto Nagao. 1994. A
syntactic analysis method of long Japanese
sentences based on the detection of conjunc-
tive structures. Computational Linguistics,
20(4):507?534.
Sadao Kurohashi and Yasuyuki Sakai. 1999.
Semantic analysis of Japanese noun phrases:
A new approach to dictionary-based under-
standing. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, pages 481?488.
Masaki Murata, Hitoshi Isahara, and Makoto
Nagao. 1999. Pronoun resolution in Japanese
sentences using surface expressions and exam-
ples. In Proceedings of the ACL?99 Workshop
on Coreference and Its Applications, pages
39?46.
Massimo Poesio, Tomonori Ishikawa,
Sabine Schulte im Walde, and Renata
Vieira. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proceedings of the
3rd International Conference on Language
Resources and Evaluation, pages 1220?1224.
Michael Strube and Udo Hahn. 1999. Func-
tional centering ? grounding referential coher-
ence in information structure. Computational
Linguistics, 25(3):309?344.
Jun-ichi Tajika, editor. 1997. Reikai Syogaku
Kokugojiten. Sanseido.
Renata Vieira and Massimo Poesio. 2000. An
empirically based system for processing defi-
nite descriptions. Computational Linguistics,
26(4):539?592.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 769?776
Manchester, August 2008
A Fully-Lexicalized Probabilistic Model
for Japanese Zero Anaphora Resolution
Ryohei Sasano
?
Graduate School of Information Science
and Technology, University of Tokyo
ryohei@nlp.kuee.kyoto-u.ac.jp
Daisuke Kawahara
National Institute of Information
and Communication Technology
dk@nict.go.jp
Sadao Kurohashi
Graduate School of Infomatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
This paper presents a probabilistic model
for Japanese zero anaphora resolution.
First, this model recognizes discourse en-
tities and links all mentions to them. Zero
pronouns are then detected by case struc-
ture analysis based on automatically con-
structed case frames. Their appropriate
antecedents are selected from the entities
with high salience scores, based on the
case frames and several preferences on
the relation between a zero pronoun and
an antecedent. Case structure and zero
anaphora relation are simultaneously de-
termined based on probabilistic evaluation
metrics.
1 Introduction
Anaphora resolution is one of the most important
techniques in discourse analysis. In English, def-
inite noun phrases such as the company and overt
pronouns such as he are anaphors that refer to pre-
ceding entities (antecedents). On the other hand,
in Japanese, anaphors are often omitted and these
omissions are called zero pronouns. We focus
on zero anaphora resolution of Japanese web cor-
pus, in which anaphors are often omitted and zero
anaphora resolution plays an important role in dis-
course analysis.
Zero anaphora resolution can be divided into
two phases. The first phase is zero pronoun detec-
tion and the second phase is zero pronoun resolu-
tion. Zero pronoun resolution is similar to coref-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
* Research Fellow of the Japan Society for the Promotion of
Science (JSPS)
erence resolution and pronoun resolution, which
have been studied for many years (e.g. Soon et
al. (2001); Mitkov (2002); Ng (2005)). Isozaki and
Hirao (2003) and Iida et al (2006) focused on zero
pronoun resolution assuming perfect pre-detection
of zero pronouns. However, we consider that zero
pronoun detection and resolution have a tight rela-
tion and should not be handled independently. Our
proposed model aims not only to resolve zero pro-
nouns but to detect zero pronouns.
Zero pronouns are not expressed in a text and
have to be detected prior to identifying their an-
tecedents. Seki et al (2002) proposed a proba-
bilistic model for zero pronoun detection and res-
olution that uses hand-crafted case frames. In
order to alleviate the sparseness of hand-crafted
case frames, Kawahara and Kurohashi (2004) in-
troduced wide-coverage case frames to zero pro-
noun detection that are automatically constructed
from a large corpus. They use the case frames as
selectional restriction for zero pronoun resolution,
but do not utilize the frequency of each example of
case slots. However, since the frequency is shown
to be a good clue for syntactic and case structure
analysis (Kawahara and Kurohashi, 2006), we con-
sider the frequency also can benefit zero pronoun
detection. Therefore we propose a probabilistic
model for zero anaphora resolution that fully uti-
lizes case frames. This model directly consid-
ers the frequency and estimates case assignments
for overt case components and antecedents of zero
pronoun simultaneously.
In addition, our model directly links each zero
pronoun to an entity, while most existing mod-
els link it to a certain mention of an entity. In
our model, mentions and zero pronouns are treated
similarly and all of them are linked to correspond-
ing entities. In this point, our model is similar to
769
Table 1: Examples of Constructed Case Frames.
case slot examples generalized examples with rate
ga (subjective) he, driver, friend, ? ? ? [CT:PERSON]:0.45, [NE:PERSON]:0.08, ? ? ?
tsumu (1)
wo (objective) baggage, luggage, hay, ? ? ? [CT:ARTIFACT]:0.31, ? ? ?
(load)
ni (dative) car, truck, vessel, seat, ? ? ? [CT:VEHICLE]:0.32, ? ? ?
tsumu (2)
ga (subjective) player, children, party, ? ? ? [CT:PERSON]:0.40, [NE:PERSON]:0.12, ? ? ?
(accumulate)
wo (objective) experience, knowledge, ? ? ? [CT:ABSTRACT]:0.47, ? ? ?
.
.
.
.
.
.
.
.
.
ga (subjective) company, Microsoft, firm, ? ? ? [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, ? ? ?
hanbai (1) wo (objective) goods, product, ticket, ? ? ? [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, ? ? ?
(sell) ni (dative) customer, company, user, ? ? ? [CT:PERSON]:0.28, ? ? ?
de (locative) shop, bookstore, site ? ? ? [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, ? ? ?
.
.
.
.
.
.
.
.
.
the coreference model proposed by Luo (2007) and
that proposed by Yang et al (2008). Due to this
characteristic, our model can utilize information
beyond a mention and easily consider salience (the
importance of an entity).
2 Construction of Case Frames
Case frames describe what kinds of cases each
predicate has and what kinds of nouns can fill
these case slots. We construct case frames from
a large raw corpus by using the method proposed
by Kawahara and Kurohashi (2002), and use them
for case structure analysis and zero anaphora res-
olution. This section shows how to construct the
case frames.
2.1 Basic Method
After a large corpus is parsed by a Japanese parser,
case frames are constructed from modifier-head
examples in the resulting parses. The problems of
case frame construction are syntactic and seman-
tic ambiguities. That is to say, the parsing results
inevitably contain errors and predicate senses are
intrinsically ambiguous. To cope with these prob-
lems, case frames are gradually constructed from
reliable modifier-head examples.
First, modifier-head examples that have no syn-
tactic ambiguity are extracted, and they are disam-
biguated by coupling a predicate and its closest
case component. Such couples are explicitly ex-
pressed on the surface of text, and can be consid-
ered to play an important role in sentence mean-
ings. For instance, examples are distinguished not
by predicates (e.g., ?tsumu (load/accumulate))?,
but by couples (e.g., ?nimotsu-wo tsumu (load bag-
gage)? and ?keiken-wo tsumu (accumulate experi-
ence))?. Modifier-head examples are aggregated in
this way, and yield basic case frames.
Thereafter, the basic case frames are clustered
to merge similar case frames. For example, since
?nimotsu-wo tsumu (load baggage)? and ?busshi-
wo tsumu (load supplies)? are similar, they are
clustered. The similarity is measured using a
thesaurus (The National Language Institute for
Japanese Language, 2004). Using this gradual pro-
cedure, we constructed case frames from approx-
imately 1.6 billion sentences extracted from the
web. In Table 1, some examples of the resulting
case frames are shown.
2.2 Generalization of Examples
By using case frames that are automatically con-
structed from a large corpus, sparseness problem
is alleviated to some extent, but still remains. For
instance, there are thousands of named entities
(NEs), which cannot be covered intrinsically. To
deal with this sparseness problem, we general-
ize the examples of case slots. Kawahara and
Kurohashi also give generalized examples such
as ?agent? but only a few types. We generalize
case slot examples based on categories of common
nouns and NE classes.
First, we use the categories that Japanese mor-
phological analyzer JUMAN
1
adds to common
nouns. In JUMAN, about twenty categories are de-
fined and tagged to common nouns. For example,
?ringo (apple),? ?inu (dog)? and ?byoin (hospi-
tal)? are tagged as ?FOOD,? ?ANIMAL? and ?FA-
CILITY,? respectively. For each category, we cal-
culate the rate of categorized example among all
case slot examples, and add it to the case slot as
?[CT:FOOD]:0.07.?
We also generalize NEs. We use a common
standard NE definition for Japanese provided by
IREX workshop (1999). IREX defined eight NE
classes as shown in Table 2. We first recognize
NEs in the source corpus by using an NE recog-
nizer (Sasano and Kurohashi, 2008), and then con-
struct case frames from the NE-recognized corpus.
1
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
770
Table 2: Definition of NE in IREX.
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o?clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
As well as categories, for each NE class, we calcu-
late the NE rate among all case slot examples, and
add it to the case slot as ?[NE:PERSON]:0.12.?
The generalized examples are also included in
Table 1. This information is utilized to estimate the
case assignment probability, which will be men-
tioned in Section 3.2.3.
3 Zero Anaphora Resolution Model
In this section, we propose a probabilistic model
for Japanese zero anaphora resolution.
3.1 Overview
The outline of our model is as follows:
1. Parse an input text using the Japanese parser
KNP
2
and recognize NEs.
2. Conduct coreference resolution and link each
mention to an entity or create new entity.
3. For each sentence, from the end of the sen-
tence, analyze each predicate by the follow-
ing steps:
(a) Select a case frame temporarily.
(b) Consider all possible correspondence
between each input case component and
an case slot of the selected case frame.
(c) Regard case slots that have no corre-
spondence as zero pronoun candidates.
(d) Consider all possible correspondence
between each zero pronoun candidate
and an existing entity.
(e) For each possible case frame, estimate
each correspondence probabilistically,
and select the most likely case frame and
correspondence.
In this paper, we concentrate on three case slots
for zero anaphora resolution: ?ga (subjective),?
?wo (objective)? and ?ni (dative),? which cover
about 90% of zero anaphora.
2
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
Morphological analysis, NE recognition, syn-
tactic analysis and coreference resolution are con-
ducted as pre-processes for zero anaphora resolu-
tion. Therefore, the model has already recognized
existing entities before zero anaphora resolution.
For example, let us consider the following text:
(i) Toyota-wa 1997-nen hybrid car Prius-wo
hatsubai(launch). 2000-nen-karaha kaigai
(overseas)-demo hanbai(sell)-shiteiru.
(Toyota launched the hybrid car Prius in 1997. ?
1
started selling ?
2
overseas in 2000.)
Figure 1 shows the analysis process for this text.
There are three mentions
3
in the first sentence, and
the two mentions, hybrid car and Prius, appear in
apposition. Thus, after the pre-processes, two enti-
ties, {Toyota} and {hybrid-car, Prius}, are created.
Then, case structure analysis for the predicate
hatsubai (launch) is conducted. First, one of the
case frames of hatsubai (launch) is temporarily se-
lected and each input case component is assigned
to an appropriate case slot. For instance, case com-
ponent Toyota is assigned to ga case slot and Prius
is assigned to wo case slot
4
. In this case, though
there is a mention hybrid-car that is not a case
component of hatsubai (launch) by itself, it refers
to the same entity as Prius refers. Thus, there is no
entity that is not linked to hatsubai (launch), and
no further analysis is conducted.
Now, let us consider the second sentence. A
mention kaigai (overseas) appears and a new entity
{kaigai} is created. Then, case structure analysis
for the predicate hanbai (sell) is conducted. There
is only one overt case component kaigai (over-
seas), and it is assigned to a case slot of the se-
lected case frame of hanbai (sell). For instance,
the case frame hanbai(1) in Table 1 is selected and
kaigai (overseas) is assigned to de (locative) case
slot. In this case, the remaining case slots ga, wo
and ni are considered as zero pronouns, and all
possible correspondences between zero pronouns
and remaining entities are considered. As a result
of probabilistic estimation, the entity {Toyota} is
assigned to ga case, the entity {hybrid-car, Prius}
is assigned to wo case and no entity is assigned to
ni case.
Now, we show how to estimate the correspon-
dence probabilistically in the next subsection.
3
In this paper, we do not consider time expressions, such
as 1997, as mentions.
4
Note that since there are some non case-making postposi-
tions in Japanese, such as ?wa? and ?mo,? several correspon-
dences can be considered.
771
Toyota-wa
Prius-wo
hybrid car
hatsubai.
kaigai-demo
hanbai-shiteiru.
1997-nen
2000-nen-karawa
{Toyota, ?
?
}
{hybrid car, 
Prius, ?2 }
{kaigai}
Entities
(overseas)
(launch)
(sell)
hatsubai (launch)
ga
subjective
company, SONY, firm, ? 
[NE:ORGANIZATION] 0.15, ?
wo
objective
product, CD, model, car,  ?
[CT:ARTIFACT] 0.40, ?
de      
locative
area, shop, world, Japan, ?
[CT:FACILITY] 0.13, ?
hanbai (sell)
ga
subjective
company, Microsoft, ? 
[NE:ORGANIZATION] 0.16, ?
wo     
objective
goods, product, ticket, ? 
[CT:ARTIFACT] 0.40, ?
ni
dative
customer, company, user, ? 
[CT:PERSON] 0.28, ?
de      
locative
shop, bookstore, site, ? 
[CT:FACILITY] 0.40, ?
:direct case assignment
:indirect case assignment (zero anaphora)
Case framesInput sentences
Toyota launched the hybrid car Prius in 1997. ?
?
started selling ?2 overseas in 2000.
Figure 1: An Example of Case Assignment CA
k
.
3.2 Probabilistic Model
The proposed model gives a probability to each
possible case frame CF and case assignment CA
when target predicate v, input case components
ICC and existing entities ENT are given. It also
outputs the case frame and case assignment that
have the highest probability. That is to say, our
model selects the case frame CF
best
and the case
assignment CA
best
that maximize the probability
P (CF,CA|v, ICC,ENT ):
(CF
best
, CA
best
)
= argmax
CF,CA
P (CF,CA|v, ICC,ENT ) (1)
Though case assignment CA usually represents
correspondences between input case components
and case slots, in our model it also represents
correspondences between antecedents of zero pro-
nouns and case slots. Hereafter, we call the former
direct case assignment (DCA) and the latter indi-
rect case assignment (ICA). Then, we transform
P (CF
l
, CA
k
|v, ICC,ENT ) as follows:
P (CF
l
, CA
k
|v, ICC,ENT )
=P (CF
l
|v, ICC,ENT )
? P (DCA
k
|v, ICC,ENT,CF
l
)
? P (ICA
k
|v, ICC,ENT,CF
l
, DCA
k
)
?P (CF
l
|v, ICC) ? P (DCA
k
|ICC,CF
l
)
? P (ICA
k
|ENT,CF
l
, DCA
k
) (2)
=P (CF
l
|v)?P (DCA
k
, ICC|CF
l
)/P (ICC|v)
? P (ICA
k
|ENT,CF
l
, DCA
k
) (3)
(
? P (CF
l
|v, ICC) =
P (CF
l
, ICC|v)
P (ICC|v)
=
P (ICC|CF
l
, v) ? P (CF
l
|v)
P (ICC|v)
=
P (ICC|CF
l
) ? P (CF
l
|v)
P (ICC|v)
,
(? CF
l
contains the information about v.)
P (DCA
k
|ICC,CF
l
)
=
P (DCA
k
, ICC|CF
l
)
P (ICC|CF
l
)
)
Equation (2) is derived because we assume that
the case frame CF
l
and direct case assignment
DCA
k
are independent of existing entities ENT ,
and indirect case assignment ICA
k
is independent
of input case components ICC.
Because P (ICC|v) is constant, we can say that
our model selects the case frame CF
best
and the
direct case assignment DCA
best
and indirect case
assignment ICA
best
that maximize the probability
P (CF,DCA, ICA|v, ICC,ENT ):
(CF
best
, DCA
best
, ICA
best
) =
argmax
CF,DCA,ICA
(
P (CF |v) ? P (DCA, ICC|CF )
?P (ICA|ENT,CF,DCA)
)
(4)
The probability P (CF
l
|v), called generative
probability of a case frame, is estimated from
case structure analysis of a large raw corpus. The
following subsections illustrate how to calculate
P (DCA
k
, ICC|CF
l
) and P (ICA
k
|ENT,CF
l
,
DCA
k
).
772
3.2.1 Generative Probability of Direct Case
Assignment
For estimation of generative probability of di-
rect case assignment P (DCA
k
, ICC|CF
l
), we
follow Kawahara and Kurohashi?s (2006) method.
They decompose P (DCA
k
, ICC|CF
l
) into the
following product depending on whether a case
slot s
j
is filled with an input case component or
vacant:
P (DCA
k
, ICC|CF
l
) =
?
s
j
:A(s
j
)=1
P (A(s
j
) = 1, n
j
, c
j
|CF
l
, s
j
)
?
?
s
j
:A(s
j
)=0
P (A(s
j
) = 0|CF
l
, s
j
)
=
?
s
j
:A(s
j
)=1
{
P (A(s
j
) = 1|CF
l
, s
j
)
? P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1)
}
?
?
s
j
:A(s
j
)=0
P (A(s
j
) = 0|CF
l
, s
j
) (5)
where the function A(s
j
) returns 1 if a case slot s
j
is filled with an input case component; otherwise
0, n
j
denotes the content part of the case compo-
nent, and c
j
denotes the surface case of the case
component.
The probabilities P (A(s
j
) = 1|CF
l
, s
j
) and
P (A(s
j
) = 0|CF
l
, s
j
) are called generative prob-
ability of a case slot, and estimated from case
structure analysis of a large raw corpus as well as
generative probability of a case frame.
The probability P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1) is
called generative probability of a case component
and estimated as follows:
P (n
j
, c
j
|CF
l
, s
j
, A(s
j
) = 1)
?P (n
j
|CF
l
, s
j
, A(s
j
)=1)?P (c
j
|s
j
, A(s
j
)=1) (6)
P (n
j
|CF
l
, s
j
, A(s
j
) = 1) means the gener-
ative probability of a content part n
j
from a
case slot s
j
in a case frame CF
l
, and esti-
mated by using the frequency of a case slot
example in the automatically constructed case
frames. P (c
j
|s
j
, A(s
j
) = 1) is approximated by
P (c
j
|case type of(s
j
), A(s
j
)=1) and estimated
from the web corpus in which the relationship be-
tween a surface case marker and a case slot is an-
notated by hand.
3.2.2 Probability of Indirect Case Assignment
To estimate probability of indirect case assign-
ment P (ICA
k
|ENT,CF
l
, DCA
k
) we also de-
compose it into the following product depending
Table 3: Location Classes of Antecedents.
intra-sentence: case components of
L
1
: parent predicate of V
z
L
2
: parent predicate of V
z
? (parallel)
L
3
: child predicate of V
z
L
4
: child predicate of V
z
(parallel)
L
5
: parent predicate of parent noun phrase of V
z
L
6
: parent predicate of parent predicate of V
z
(parallel)
L
7
: other noun phrases following V
z
L
8
: other noun phrases preceding V
z
inter-sentence: noun phrases in
L
9
: 1 sentence before
L
10
: 2 sentences before
L
11
: 3 sentences before
L
12
: more than 3 sentences before
on whether a case slot s
j
is filled with an entity
ent
j
or vacant:
P (ICA
k
|ENT,CF
l
, DCA
k
) =
?
s
j
:A
?
(s
j
)=1
P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
?
?
s
j
:A
?
(s
j
)=0
P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
) (7)
where the function A
?
(s
j
) returns 1 if a case slot
s
j
is filled with an entity ent
j
; otherwise 0. Note
that we only consider case slots ga, wo and ni that
is not filled with an input case component. We
approximate P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
and P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
) as follows:
P (A
?
(s
j
) = 1, ent
j
|ENT,CF
l
, s
j
)
? P (A
?
(s
j
) = 1, ent
j
|ent
j
, CF
l
, s
j
)
= P (A
?
(s
j
) = 1|ent
j
, CF
l
, s
j
) (8)
P (A
?
(s
j
) = 0|ENT,CF
l
, s
j
)
? P (A
?
(s
j
) = 0|case type of(s
j
)) (9)
Equation (8) is derived because we assume
P (A
?
(s
j
) = 1|CF
l
, s
j
) is independent of exist-
ing entities that are not assigned to s
j
. Equation
(9) is derived because we assume P (A
?
(s
j
) = 0)
is independent of ENT and CF
l
, and only de-
pends on the case type of s
j
, such as ga, wo and ni.
P (A
?
(s
j
)=0|case type of(s
j
)) is the probability
that a case slot has no correspondence after zero
anaphora resolution and estimated from anaphoric
relation tagged corpus.
Let us consider the probability P (A
?
(s
j
) =
1|ent
j
, CF
l
, s
j
). We decompose ent
j
into content
part n
j
m
, surface case c
j
n
and location class l
j
n
.
Here, location classes denote the locational rela-
tions between zero pronouns and their antecedents.
We defined twelve location classes as described in
Table 3. In Table 3, V
z
means a predicate that has
a zero pronoun. Note that we also consider the
773
locations of zero pronouns that are linked to the
target entity as location class candidates. Now we
roughly approximate P (A
?
(s
j
)=1|ent
j
, CF
l
, s
j
)
as follows:
P (A
?
= 1|ent
j
, CF
l
, s
j
)
=P (A
?
= 1|n
j
m
, c
j
n
, l
j
n
, CF
l
, s
j
)
=
P (n
j
m
, c
j
n
, l
j
n
|CF
l
, s
j
,A
?
=1)?P (A
?
=1|CF
l
, s
j
)
P (n
j
m
, c
j
n
, l
j
n
|CF
l
, s
j
)
?
P (n
j
m
|CF
l
, s
j
, A
?
=1)
P (n
j
m
|CF
l
, s
j
)
?
P (c
j
n
|CF
l
, s
j
, A
?
=1)
P (c
j
n
|CF
l
, s
j
)
?
P (l
j
n
|CF
l
, s
j
, A
?
=1)
P (l
j
n
|CF
l
, s
j
)
?P (A
?
=1|CF
l
, s
j
) (10)
?
P (n
j
m
|CF
l
, s
j
, A
?
=1)
P (n
j
m
)
?
P (c
j
n
|case type of(s
j
), A
?
=1)
P (c
j
n
)
? P (A
?
=1|l
j
n
, case type of(s
j
)) (11)
(
?
P (l
j
n
|CF
l
, s
j
, A
?
=1)
P (l
j
n
|CF
l
, s
j
)
?P (A
?
=1|CF
l
, s
j
)
=
P (A
?
=1, l
j
n
|CF
l
, s
j
)
P (l
j
n
|CF
l
, s
j
)
=P (A
?
=1|CF
l
, l
j
n
, s
j
)
)
Note that because ent
j
is often mentioned more
than one time, there are several combinations of
content part n
j
m
, surface case c
j
n
and location
class l
j
n
candidates. We select the pair of m and n
with the highest probability.
Equation (10) is derived because we as-
sume n
j
m
, c
j
n
and l
j
n
are independent of each
other. Equation (11) is derived because we ap-
proximate P (A
?
= 1|CF
l
, l
j
n
, s
j
) as P (A
?
=
1|l
j
n
, case type of(s
j
)), and assume P (n
j
m
) and
P (c
j
n
) are independent of CF
l
and s
j
. Since these
approximation is too rough, specifically, P (n
j
m
)
and P (c
j
n
) tend to be somewhat smaller than
P (n
j
m
|CF
l
, s
j
) and P (c
j
n
|CF
l
, s
j
) and equation
(11) often becomes too large, we introduce a
parameter ?(? 1) and use the ?-times value as
P (A
?
= 1|ent
j
, CF
l
, s
j
).
The first term of equation (11) represents how
likely an entity that contains n
j
m
as a content part
is considered to be an antecedent, the second term
represents how likely an entity that contains c
j
n
as
a surface case is considered to be an antecedent,
and the third term gives the probability that an
entity that appears in location class l
j
n
is an an-
tecedent.
The probabilities P (n
j
m
) and P (c
j
n
) are esti-
mated from a large raw corpus. The probabili-
ties P (c
j
n
|case type of(s
j
)) and P (A
?
= 1|l
j
n
,
case type of(s
j
)) are estimated from the web
corpus in which the relationship between an an-
tecedent of a zero pronoun and a case slot, and the
relationship between its surface case marker and a
case slot are annotated by hand. Then, let us con-
sider the probability P (n
j
m
|CF
l
, s
j
, A
?
(s
j
) = 1)
in the next subsection.
3.2.3 Probability of Component Part of Zero
Pronoun
P (n
j
m
|CF
l
, s
j
, A
?
=1) is similar to P (n
j
|CF
l
,
s
j
, A=1) and can be estimated approximately from
case frames using the frequencies of case slot ex-
amples. However, while A
?
(s
j
) = 1 means s
j
is
not filled with input case component but filled with
an entity as the result of zero anaphora resolution,
case frames are constructed by extracting only the
input case component. Therefore, the content part
of a zero anaphora antecedent n
j
m
is often not in-
cluded in the case slot examples. To cope with this
problem, we utilize generalized examples.
When one mention of an entity is tagged any
category or recognized as an NE, we also use the
category or the NE class as the content part of the
entity. For examples, if an entity {Prius} is recog-
nized as an artifact name and assigned to wo case
of the case frame hanbai(1) in Table 1, the system
also calculates:
P (NE :ARTIFACT |hanbai(1),wo, A
?
(wo)=1)
P (NE :ARTIFACT )
besides:
P (Prius|hanbai(1),wo, A
?
(wo) = 1)
P (Prius)
and uses the higher value.
3.3 Salience Score
Previous works reported the usefulness of salience
for anaphora resolution (Lappin and Leass, 1994;
Mitkov et al, 2002). In order to consider salience
of an entity, we introduce salience score, which is
calculated by the following set of simple rules:
? +2 : mentioned with topical marker ?wa?.
? +1 : mentioned without topical marker ?wa?.
? +0.5 : assigned to a zero pronoun.
? ?0.7 : beginning of each sentence.
For examples, we consider the salience score of
the entity {Toyota} in (i) in Section 3.1. In the
first sentence, since {Toyota} is mentioned with
topical marker ?wa?, the salience score is 2. At the
beginning of the second sentence it becomes 1.4,
774
Table 4: Data for Parameter Estimation.
probability data
P (n
j
) raw corpus
P (c
j
) raw corpus
P (c
j
|case type of(s
j
), A(s
j
)=1) tagged corpus
P (c
j
|case type of(s
j
), A
?
(s
j
)=1) tagged corpus
P (n
j
|CF
l
, s
j
, A(s
j
)=1) case frames
P (n
j
|CF
l
, s
j
, A
?
(s
j
)=1) case frames
P (CF
l
|v
i
) case structure analysis
P (A(s
j
)={0, 1} |CF
l
, s
j
) case structure analysis
P (A
?
(s
j
)=0|case type of(s
j
)) tagged corpus
P (A
?
(s
j
)=1|l
j
, case type of(s
j
)) tagged corpus
Table 5: Experimental Results.
R P F
Kawahara & Kurohashi .230 (28/122) .173 (28/162) .197
Proposed (? = 1) .426 (52/122) .271 (52/192) .331
(? = 1/2) .410 (50/122) .373 (50/134) .391
(? = 1/4) .295 (36/122) .419 (36/86) .346
and after assigned to the zero pronoun of ?hanbai?
it becomes 1.9. Note that we use the salience score
not as a probabilistic clue but as a filter to consider
the target entity as a possible antecedent. When we
use the salience score, we only consider the entities
that have the salience score no less than 1.
4 Experiments
4.1 Setting
We created an anaphoric relation-tagged corpus
consisting of 186 web documents (979 sentences).
We selected 20 documents for test and used the
other 166 documents for calculating several proba-
bilities. Since the anaphoric relations in some web
documents were not so clear and too difficult to
recognize, we did not select such documents for
test. In the 20 test documents, 122 zero anaphora
relations were tagged between one of the mentions
of the antecedent and the target predicate that had
the zero pronoun.
Each parameter for proposed model was esti-
mated using maximum likelihood from the data
described in Table 4. The case frames were auto-
matically constructed from web corpus comprising
1.6 billion sentences. The case structure analysis
was conducted on 80 million sentences in the web
corpus, and P (n
j
) and P (c
j
)were calculated from
the same 80 million sentences.
In order to concentrate on zero anaphora resolu-
tion, we used the correct morphemes, named enti-
ties, syntactic structures and coreferential relations
that were annotated by hand. Since correct corefer-
ential relations were given, the number of created
entities was same between the gold standard and
the system output because zero anaphora resolu-
tion did not create new entities.
4.2 Experimental Results
We conducted experiments of zero anaphora reso-
lution. As the parameter ? introduced in Section
3.2.2., we tested 3 values 1, 1/2, and 1/4. For
comparison, we also tested Kawahara and Kuro-
hashi?s (2004) model. The experimental results are
shown in Table 5, in which recall R, precision P
and F-measure F were calculated by:
R =
# of correctly recognized zero anaphora
# of zero anaphora tagged in corpus
,
P =
# of correctly recognized zero anaphora
# of system outputted zero anaphora
,
F =
2
1/R + 1/P
.
Kawahara and Kurohashi?s model achieved al-
most 50% as F-measure against newspaper arti-
cles. However, as a result of our experiment
against web documents, it achieved only about
20% as F-measure. This may be because anaphoric
relations in web documents were not so clear as
those in newspaper articles and more difficult to
recognize. As to the parameter ?, the larger ?
tended to output more zero anaphora, and the high-
est F-measure was achieved against ? = 1/2.
When using ? = 1/2, there were 72 (=122?50)
zero pronouns that were tagged in the corpus and
not resolved correctly. Only 12 of them were cor-
rectly detected and assigned to a wrong entity, that
is, 60 of them were not even detected. Therefore,
we can say our recall errors were mainly caused by
the low recall of zero pronoun detection.
In order to confirm the effectiveness of gener-
alized examples of case slots and salience score,
we also conducted experiments under several con-
ditions. We set ? = 1/2 in these experiments. The
results are shown in Table 6, in which CT means
generalized categories, NE means generalized NEs
and SS means salience score.
Without using any generalized examples, the F-
measure is less than Kawahara and Kurohashi?s
method, which use similarity to deal with sparse-
ness of case slot examples, and we can con-
firm the effectiveness of the generalized examples.
While generalized categories much improved the
F-measure, generalized NEs contribute little. This
may be because the NE rate is smaller than com-
mon noun rate, and so the effect is limited.
We also confirmed that the salience score filter
improved F-measure. Moreover, by using salience
score filter, the zero anaphora resolution becomes
about ten times faster. This is because the system
775
Table 6: Experiments under Several Conditions.
CT NE SS R P F
?
.131 (16/122) .205 (16/78) .160
? ?
.164 (20/122) .247 (20/81) .197
? ?
.402 (49/122) .368 (49/133) .384
? ?
.385 (47/122) .196 (47/240) .260
? ? ?
.410 (50/122) .373 (50/134) .391
can avoid checking entities with low salience as
antecedent candidates.
4.3 Comparison with Previous Works
We compare our accuracies with (Seki et al,
2002). They achieved 48.9% in precision, 88.2%
in recall, and 62.9% in F-measure for zero pro-
noun detection, and 54.0% accuracy for antecedent
estimation on 30 newspaper articles, that is, they
achieved about 34% in F-measure for whole zero
pronoun resolution. It is difficult to directly com-
pare their results with ours due to the difference
of the corpus, but our method achieved 39% in
F-measure and we can confirm that our model
achieves reasonable performance considering the
task difficulty.
5 Conclusion
In this paper, we proposed a probabilistic model
for Japanese zero anaphora resolution. By us-
ing automatically constructed wide-coverage case
frames that include generalized examples and in-
troducing salience score filter, our model achieves
reasonable performance against web corpus. As
future work, we plan to conduct large-scale ex-
periments and integrate this model to a fully-
lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis (Kawahara and
Kurohashi, 2006).
References
Iida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2006.
Exploiting syntactic patterns as clues in zero-
anaphora resolution. In Proceedings of COL-
ING/ACL 2006, pages 625?632.
IREX Committee, editor. 1999. Proc. of the IREX
Workshop.
Isozaki, Hideki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 184?191.
Kawahara, Daisuke and Sadao Kurohashi. 2002.
Fertilization of Case Frame Dictionary for Robust
Japanese Case Analysis. In Proceedings of the 19th
International Conference on Computational Linguis-
tics, pages 425?431.
Kawahara, Daisuke and Sadao Kurohashi. 2004.
Zero pronoun resolution based on automatically con-
structed case frames and structural preference of an-
tecedents. In Proceedings of the 1st International
Joint Conference on Natural Language Processing
(IJCNLP-04), pages 334?341.
Kawahara, Daisuke and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 176?183.
Lappin, Shalom and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?562.
Luo, Xiaoqiang. 2007. Coreference or not: A
twin model for coreference resolution. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 73?80.
Mitkov, Ruslan, Richard Evans, and Constantin Or?asan.
2002. A new, fully automatic version of mitkov?s
knowledge-poor pronoun resolution method. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLing-2002).
Ng, Vincent. 2005. Machine learning for coreference
resolution: From local classification to global rank-
ing. In Proceedings of the 43rd Annual Meeting
of the Asssociation for Computational Linguistics,
pages 157?164.
Sasano, Ryohei and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural lan-
guage processing. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP-08), pages 607?612.
Seki, Kazuhiro, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and res-
olution. In Proceedings of the 19th International
Conference on Computational Linguistics, pages
911?917.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
The National Language Institute for Japanese Lan-
guage. 2004. Bunruigoihyo. Dainippon Tosho, (In
Japanese).
Yang, Xiaofeng, Jian Su, Jun Lang, Ghew Lim Tan,
Ting Liu, and Sheng Li. 2008. An entity-mention
model for coreference resolution with inductive logic
programming. In Proceedings of ACL-08: HLT,
pages 843?851.
776
Japanese Named Entity Recognition
Using Structural Natural Language Processing
Ryohei Sasano?
Graduate School of Information Science
and Technology, University of Tokyo
ryohei@nlp.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Infomatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
This paper presents an approach that uses
structural information for Japanese named
entity recognition (NER). Our NER system
is based on Support Vector Machine (SVM),
and utilizes four types of structural informa-
tion: cache features, coreference relations,
syntactic features and caseframe features,
which are obtained from structural analyses.
We evaluated our approach on CRL NE data
and obtained a higher F-measure than exist-
ing approaches that do not use structural in-
formation. We also conducted experiments
on IREX NE data and an NE-annotated web
corpus and confirmed that structural infor-
mation improves the performance of NER.
1 Introduction
Named entity recognition (NER) is the task of iden-
tifying and classifying phrases into certain classes
of named entities (NEs), such as names of persons,
organizations and locations.
Japanese texts, which we focus on, are written
without using blank spaces. Therefore, Japanese
NER has tight relation with morphological analy-
sis, and thus it is often performed immediately after
morphological analysis (Masayuki and Matsumoto,
2003; Yamada, 2007). However, such approaches
rely only on local context. The Japanese NER sys-
tem proposed in (Nakano and Hirai, 2004), which
achieved the highest F-measure among conventional
systems, introduced the bunsetsu1 feature in order to
consider wider context, but considers only adjacent
bunsetsus.
*Research Fellow of the Japan Society for the Promotion of Science (JSPS)
1Bunsetsu is a commonly used linguistic unit in Japanese,
consisting of one or more adjacent content words and zero or
more following functional words.
On the other hand, as for English or Chinese, var-
ious NER systems have explored global information
and reported their effectiveness. In (Malouf, 2002;
Chieu and Ng, 2002), information about features as-
signed to other instances of the same token is uti-
lized. (Ji and Grishman, 2005) uses the information
obtained from coreference analysis for NER. (Mohit
and Hwa, 2005) uses syntactic features in building a
semi-supervised NE tagger.
In this paper, we present a Japanese NER system
that uses global information obtained from several
structural analyses. To be more specific, our system
is based on SVM, recognizes NEs after syntactic,
case and coreference analyses and uses information
obtained from these analyses and the NER results
for the previous context, integrally. At this point,
it is true that NER results are useful for syntactic,
case and coreference analyses, and thus these analy-
ses and NER should be performed in a complemen-
tary way. However, since we focus on NER, we rec-
ognize NE after these structural analyses.
2 Japanese NER Task
A common standard definition for Japanese NER
task is provided by IREXworkshop (IREX Commit-
tee, 1999). IREX defined eight NE classes as shown
in Table 1. Compared with the MUC-6 NE task def-
inition (MUC, 1995), the NE class ?ARTIFACT,?
which contains book titles, laws, brand names and
so on, is added.
NER task can be defined as a chunking problem
to identify token sequences that compose NEs. The
chunking problem is solved by annotating chunk
tags to tokens. Five chunk tag sets, IOB1, IOB2,
IOE1, IOE2 and IOBES are commonly used. In this
paper, we use the IOBES model, in which ?S? de-
notes a chunk itself, and ?B,? ?I? and ?E? denote the
607
Table 1: Definition of NE in IREX.
NE class Examples
ORGANIZATION NHK Symphony Orchestra
PERSON Kawasaki Kenjiro
LOCATION Rome, Sinuiju
ARTIFACT Nobel Prize
DATE July 17, April this year
TIME twelve o?clock noon
MONEY sixty thousand dollars
PERCENT 20%, thirty percents
beginning, intermediate and end parts of a chunk.
If a token does not belong to any named entity, it is
tagged as ?O.? Since IREX defined eight NE classes,
tokens are classified into 33 (= 8 ? 4 + 1) NE tags.
For example, NE tags are assigned as following:
(1) Kotoshi 4 gatsu Roma ni itta.
this year April Rome to went
B-DATE I-DATE E-DATE S-LOCATION O O
(? went to Rome on April this year.)
3 Motivation for Our Approach
Our NER system utilizes structural information. In
this section, we describe the motivation for our ap-
proach.
High-performance Japanese NER systems are of-
ten based on supervised learning, and most of them
use only local features, such as features obtained
from the target token, two preceding tokens and two
succeeding tokens. However, in some cases, NEs
cannot be recognized by using only local features.
For example, while ?Kawasaki? in the second
sentence of (2) is the name of a person, ?Kawasaki?
in the second sentence of (3) is the name of a soc-
cer team. However, the second sentences of (2) and
(3) are exactly the same, and thus it is impossible to
correctly distinguish these NE classes by only using
information obtained from the second sentences.
(2) Kachi-ha senpatsu-no Kawasaki Kenjiro.
winner starter
Kawasaki-ha genzai 4 shou 3 pai.
now won lost
(The winning pitcher is the starter Kenjiro Kawasaki.
Kawasaki has won 4 and lost 3.)
(3) Dai 10 setsu-wa Kawasaki Frontale-to taisen.
the round against
Kawasaki-ha genzai 4 shou 3 pai.
now won lost
(The 10th round is against Kawasaki Frontale.
Kawasaki has won 4 and lost 3.)
In order to recognize these NE classes, it is essential
to use the information obtained from the previous
context. Therefore, we utilize information obtained
from the NER for the previous context: cache fea-
ture and coreference relation.
For another example, ?Shingishu? in (4) is the
name of city in North Korea. The most important
clue for recognizing ?Shingishu? as ?LOCATION?
may be the information obtained from the head verb,
?wataru (get across).?
(4) Shingishu-kara Ouryokko-wo wataru.
Sinuiju from Amnokkang get across
(? gets across the Amnokkang River from Sinuiju.)
However, when using only local features, the word
?wataru? is not taken into consideration because
there are more than two morphemes between ?shu2?
and ?wataru.? In order to deal with such problem,
we use the information obtained from the head verb:
syntactic feature and caseframe feature.
4 NER Using Structural Information
4.1 Outline of Our NER System
Our NER system performs the chunking process
based on morpheme units because character-based
methods do not outperform morpheme-based meth-
ods (Masayuki and Matsumoto, 2003) and are not
suitable for considering wider context.
A wide variety of trainable models have been ap-
plied to Japanese NER task, including maximum en-
tropy models (Utsuro et al, 2002), support vector
machines (Nakano and Hirai, 2004; Yamada, 2007)
and conditional random fields (Fukuoka, 2006). Our
system applies SVMs because, for Japanese NER,
SVM-based systems achieved higher F-measure
than the other systems. (Isozaki and Kazawa, 2003)
proposed an SVM-based NER system with Viterbi
search, which outperforms an SVM-based NER sys-
tem with sequential determination, and our system
basically follows this system. Our NER system con-
sists of the following four steps:
1. Morphological analysis
2. Syntactic, case and coreference analyses
3. Feature extraction for chunking
4. SVM and Viterbi search based chunking
The following sections describe each of these steps
in detail.
2Since the dictionary for morphological analysis has no en-
try ?Shingishu,? ?Shingishu? is analyzed as consisting of three
morphemes: ?shin,? ?gi? and ?shu.?
608
Input sentence:
Gai mu sho no shin Bei ha .
foreign affairs ministry in pro America group
(Pro-America group in the Ministry of Foreign Affairs.)
Output of JUMAN:
Gaimu sho no shin Bei ha .
noun noun particle noun noun noun
Output of ChaSen:
Gaimusho no shin-Bei ha .
noun particle noun noun
Figure 1: Example of morphological analyses.
4.2 Morphological Analysis
While most existing Japanese NER systems use
ChaSen (Matsumoto et al, 2003) as a morphological
analyzer, our NER system uses a Japanese morpho-
logical analyzer JUMAN (Kurohashi and Kawahara,
2005) because of the following two reasons.
First, JUMAN tends to segment a sentence into
smaller morphemes than ChaSen, and this is a good
tendency for morpheme-based NER systems be-
cause the boundary contradictions between morpho-
logical analysis and NEs are considered to be re-
duced. Figure 1 shows an example of the outputs
of JUMAN and ChaSen. Although both analyses
are reasonable, JUMAN divided ?Gaimusho? and
?shin-Bei? into two morphemes, while ChaSen left
them as a single morpheme. Second, JUMAN adds
categories to some morphemes, which can be uti-
lized for NER. In JUMAN, about thirty categories
are defined and tagged to about one fifth of mor-
phemes. For example, ?ringo (apple),? ?inu (dog)?
and ?byoin (hospital)? are tagged as ?FOOD,? ?AN-
IMAL? and ?FACILITY,? respectively.
4.3 Syntactic, Case and Coreference Analyses
syntactic analysis Syntactic analysis is performed
by using the Japanese parser KNP (Kurohashi and
Nagao, 1994). KNP employs some heuristic rules to
determine the head of a modifier.
case analysis Case analysis is performed by using
the system proposed in (Kawahara and Kurohashi,
2002). This system uses Japanese case frames that
are automatically constructed from a large corpus.
To utilize case analysis for NER, we constructed
case frames that include NE labels in advance. We
explain details in Section 4.4.2. The case analysis is
applied to each predicate in an input sentence. For
details see (Kawahara and Kurohashi, 2002).
coreference analysis Coreference analysis is per-
formed by using the coreference analyzer proposed
by (Sasano et al, 2007). As will be mentioned in
Section 4.4.2, our NER system uses coreference re-
lations only when coreferential expressions do not
share same morphemes. Basically, such coreference
relations are recognized by using automatically ac-
quired synonym knowledge.
4.4 Feature Extraction
4.4.1 Basic Features
As basic features for chunking, our NER system
uses the morpheme itself, character type, POS tag
and category if it exists.
As character types, we defined seven types:
?kanji,? ?hiragana,? ?katakana,? ?kanji with hira-
gana,? ?punctuation mark,? ?alphabet? and ?digit.?
As for POS tag, more than one POS feature are
extracted if the target morpheme has POS ambigu-
ity. In addition, besides POS tag obtained by JU-
MAN, our system also uses POS tag obtained from
Japanese morphological analyzer MeCab3 that uses
IPADIC as a word dictionary (Asahara and Mat-
sumoto, 2002). The JUMAN dictionary has few
named entity entries; thus our system supplements
the lack of lexical knowledge by using MeCab.
4.4.2 Structural Features
Our NER system uses three types of global fea-
tures: cache features, syntactic features and case-
frame features, and a rule that reflects coreference
relations. Although the coreference relations are not
used as features, we describe how to use them in this
section.
cache feature If the same morpheme appears mul-
tiple times in a single document, in most cases the
NE tags of these morphemes have some relation to
each other, and the NER results for previous parts
of the document can be a clue for the analysis for
following parts.
We consider the examples (2) and (3) again. Al-
though the second sentences of (2) and (3) are ex-
actly the same, we can recognize ?Kawasaki? in
the second sentence of (2) is ?S-PERSON? and
?Kawasaki? in the second sentence of (3) is ?S-
ORGANIZATION? by reading the first sentences.
To utilize the information obtained from previous
parts of the document, our system uses the NER
results for previous parts of the document as fea-
tures, called cache features. When analyzing (2),
our system uses the outputs of NE recognizer for
3http://mecab.sourceforge.jp/
609
?Kawasaki? in the first sentence as a feature for
?Kawasaki? in the second sentence. For simplicity,
our system uses correct NE tags when training. That
is, as a feature for ?Kawasaki? in the second sen-
tence of (2), the correct feature ?B-PERSON? is al-
ways added when training, not always added when
analyzing.
coreference rule Coreference relation can be a
clue for NER. This clue is considered by using cache
features to a certain extent. However, if the same
morpheme is not used, cache features cannot work.
For example, ?NHK kokyo gakudan? and ?N-kyo?
in (5) have coreference relation, but they do not
share the same morpheme.
(5) NHK kokyo gakudan-no ongaku kantoku-ni
symphony orchestra musical director
shuunin. N-kyo-to kyoen-shite irai ... .
became perform together since
(He became musical director of the NHK Symphony
Orchestra. Since performing together with N-kyo ... .)
In this case, ?NHK kokyo gakudan? can easily be
recognized as ?ORGANIZATION,? because it ends
with ?kokyo gakudan (symphony orchestra).? Mean-
while, ?N-kyo,? the abbreviation of ?NHK kokyo
gakudan,? cannot easily be recognized as ?ORGA-
NIZATION.?
Therefore, our system uses a heuristic rule that if
a morpheme sequence is analyzed to be coreferential
to a previous morpheme sequence that is recognized
as an NE class, the latter morpheme sequence is rec-
ognized as the same NE class. Since this heuristic
rule is introduced in order to utilize the coreference
relation that is not reflected by cache features, our
system applies this rule only when coreferential ex-
pressions do not have any morphemes in common.
syntactic feature As mentioned in Section 3, our
system utilizes the information obtained from the
head verb. As syntactic features, our system uses the
head verb itself and the surface case of the bunsetsu
that includes the target morpheme.
For the morpheme ?shin? in example (4), the
head verb ?wataru (get across)? and the surface case
?kara (from)? are added as syntactic features.
caseframe feature Syntactic features cannot work
if the head verb does not appear in the training data.
To overcome this data sparseness problem, case-
frame features are introduced.
Table 2: Case frame of ?haken (dispatch).?
case examples
ga Japan:23,party:13,country:12,government:7,
(nominative) company6,ward:6,corps:5,UN:4,US:4,Korea:4,
team:4,. . . (ORGANIZATION,LOCATION)
wo party:1249,him:1017,soldier:932,official:906,
(objective) company6:214,instructor:823,expert:799,
helper:694,staff:398,army:347,. . .
ni Iraq:700,on-the-scene:576,abroad:335,
(locative) home:172,Japan:171,Indirect Ocean:142,
scene:141,China:125,. . . (LOCATION)
For example, although the head verb ?haken (dis-
patch)? can be a clue for recognizing ?ICAO? in
(6) as ?ORGANIZATION,? syntactic features can-
not work if ?haken (dispatch)? did not appear in the
training data.
(6) ICAO-ha genchi-ni senmonka-wo haken-shita.
scene to expert dispatched
(ICAO dispatched experts to the scene)
However, this clue can be utilized if there is knowl-
edge that the ?ga (nominative)? case of ?haken (dis-
patch)? is often assigned by ?ORGANIZATION.?
Therefore, we construct case frames that include
NE labels in advance. Case frames describe what
kinds of cases each verb has and what kinds of nouns
can fill a case slot. We construct them from about
five hundred million sentences. We first recognize
NEs appearing in the sentences by using a primitive
NER system that uses only local features, and then
construct the case frames from the NE-recognized
sentences. To be more specific, if one tenth of the
examples of a case are classified as a certain NE
class, the corresponding label is attached to the case.
Table 2 shows the constructed case frame of ?haken
(dispatch).? In the ?ga (nominative)? case, the NE
labels, ?ORGANIZATION? and ?LOCATION? are
attached.
We then explain how to utilize these case frames.
Our system first performs case analysis, and uses as
caseframe features the NE labels attached in the case
to which the target morpheme is assigned. For in-
stance, by the case analyzer, the postpositional par-
ticle ?-ha? in (6) is recognized as meaning nom-
inative and ?ICAO? is assigned to the ?ga (nom-
inative)? case of the case frame of ?haken (dis-
patch).?Therefore, the caseframe features, ?ORGA-
NIZATION? and ?LOCATION? are added to the
features for the morpheme ?ICAO.?
4.5 SVM and Viterbi Search Based Chunking
To utilize cache features obtained from the previous
parts of the same sentence, our system determines
610
Table 3: Experimental results (F-measure).
CRL IREX WEB
baseline 88.63 85.47 68.98
+ cache 88.81 +0.18* 85.94 +0.47 69.67 +0.69*
+ coreference 88.68 +0.05 86.52 +1.05*** 69.17 +0.19
+ syntactic 88.80 +0.17* 85.77 +0.30 70.25 +1.27**
+ caseframe 88.57?0.06 85.51 +0.04 70.12 +1.14*
+ thesaurus 88.77 +0.14 86.36 +0.89* 68.63?0.35
use all 89.40 +0.77*** 87.72 +2.25*** 71.03 +2.05***
significant at the .1 level:*, .01 level:**, .001 level:***
NE tags clause by clause. The features extracted
from two preceding morphemes and two succeed-
ing morphemes are also used for chunking a target
morpheme. Since SVM can solve only a two-class
problem, we have to extend a binary classifier SVM
to n-class classifier. Here, we employ the one versus
rest method, in which we prepared n binary classi-
fiers and each classifier is trained to distinguish a
class from the rest of the classes.
To consider consistency of NE tags in a clause,
our system uses Viterbi search with some constraints
such as a ?B-DATE? must be followed by ?I-DATE?
or ?E-DATE.? Since SVMs do not output proba-
bilities, our system uses the SVM+sigmoid method
(Platt et al, 2000). That is, a sigmoid function
s(x) = 1/(1+exp(??x)) is applied to map the out-
put of SVM to a probability-like value. Our system
determines NE tags by using these probability-like
values. Our system is trained by TinySVM-0.094
with C = 0.1 and uses a fixed value ? = 10. This
process is almost the same as the process proposed
by Isozaki and Kazawa and for details see (Isozaki
and Kazawa, 2003).
5 Experiments
5.1 Data
For training, we use CRL NE data, which was pre-
pared for IREX. CRL NE data has 18,677 NEs on
1,174 articles in Mainichi Newspaper.
For evaluation, we use three data: CRL NE data,
IREX?s formal test data called GENERAL andWEB
NE data. When using CRL NE data for evalua-
tion, we perform five-fold cross-validation. IREX
test data has 1,510 NEs in 71 articles from Mainichi
Newspaper. Although both CRL NE data and IREX
test data use Mainichi Newspaper, these formats are
not the same. For example, CRL NE data removes
parenthesis expressions, but IREX test data does not.
WEB NE data, which we annotated NEs on corpus
collected from the Web, has 1,686 NEs in 354 arti-
4http://chasen.org/ taku/software/TinySVM/
cles. Although the domain of the web corpus differs
from that of CRL NE data, the format of the web
corpus is the same as CRL NE data format.
5.2 Experiments and Discussion
To confirm the effect of each feature, we conducted
experiments on seven conditions as follows:
1. Use only basic features (baseline)
2. Add cache features to baseline
3. Add the coreference rule to baseline
4. Add parent features to baseline
5. Add caseframe features to baseline
6. Add thesaurus features to baseline
7. Use all structural information and thesaurus
Since (Masayuki andMatsumoto, 2003; Nakano and
Hirai, 2004) reported the performance of NER sys-
tem was improved by using a thesaurus, we also
conducted experiment in which semantic classes ob-
tained from a Japanese thesaurus ?Bunrui Goi Hyo?
(NLRI, 1993) were added to the SVM features. Ta-
ble 3 shows the experimental results.
To judge the statistical significance of the dif-
ferences between the performance of the baseline
system and that of the others, we conducted a
McNemar-like test. First, we extract the outputs that
differ between the baseline method and the target
method. Then, we count the number of the outputs
that only baseline method is correct and that only
target method is correct. Here, we assume that these
outputs have the binomial distribution and apply bi-
nomial test. As significance level, we use .1 level,
.01 level and .001 level. The results of the signifi-
cance tests are also shown in Table 3.
When comparing the performance between data
sets, we can say that the performance for WEB NE
data is much worse than the others. This may be
because the domain of the WEB corpus differs from
that of CRL NE data.
As for the differences in the same data set, cache
features and syntactic features improve the perfor-
mance not dramatically but consistently and inde-
pendently from the data set. The coreference rule
also improves the performance for all data sets, but
especially for IREX test data. This may be because
IREX test data does not remove parenthesis expres-
sions, and thus there are a many coreferential ex-
pressions in the data. Caseframe features improve
the performance for WEB NE data, but do not con-
tribute to the performance for CRL NE data and
611
Table 4: Comparison with previous work.
CRL cross IREX Learning Analysis Features
validation test data Method Units
(Isozaki and Kazawa, 2003) 86.77 85.10 SVM + Viterbi morpheme basic features
(Masayuki and Matsumoto, 2003) 87.21 SVM character +thesaurus
(Fukuoka, 2006) 87.71 Semi-Markov CRF character basic features
(Yamada, 2007) 88.33 SVM + Shift-Reduce morpheme +bunsetsu features
(Nakano and Hirai, 2004) 89.03 SVM character +bunsetsu features & thesaurus
Our system 89.40 87.72 SVM + Viterbi morpheme +structural information & thesaurus
IREX test data. This result shows that caseframe
features are very generalized features and effective
for data of different domain. On the other hand, the-
saurus features improve the performance for CRL
NE data and IREX test data, but worsen the perfor-
mance for WEB NE data. The main cause for this
may be overfitting to the domain of the training data.
By using all structural information, the perfor-
mance is significantly improved for all data sets, and
thus we can say that the structural information im-
proves the performance of NER.
5.3 Comparison with Previous Work
Table 4 shows the comparison with previous work
for CRL NE data and IREX test data. Our system
outperforms all other systems, and thus we can con-
firm the effectiveness of our approach.
6 Conclusion
In this paper, we presented an approach that uses
structural information for Japanese NER. We in-
troduced four types of structural information to an
SVM-based NER system: cache features, coref-
erence relations, syntactic features and caseframe
features, and conducted NER experiments on three
data. As a consequence, the performance of NER
was improved by using structural information and
our approach achieved a higher F-measure than ex-
isting approaches.
References
Masayuki Asahara and Yuji Matsumoto, 2002. IPADIC User
Manual. Nara Institute of Science and Technology, Japan.
Hai Leong Chieu and Hwee Tou Ng. 2002. Named entity
recognition: A maximum entropy approach using global in-
formation. In Proc. of COLING 2002, pages 1?7.
Kenta Fukuoka. 2006. Named entity extraction with semi-
markov conditional random fields (in Japanese). Master?s
thesis, Nara Institute of Science and Technology.
IREX Committee, editor. 1999. Proc. of the IREX Workshop.
Hideki Isozaki and Hideto Kazawa. 2003. Speeding up
support vector machines for named entity recognition (in
japanese). Trans. of Information Processing Society of
Japan, 44(3):970?979.
Heng Ji and Ralph Grishman. 2005. Improving name tagging
by reference resolution and relation detection. In Proc. of
ACL-2005, pages 411?418.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of
Case Frame Dictionary for Robust Japanese Case Analysis.
In Proc. of COLING-2002, pages 425?431.
Sadao Kurohashi and Daisuke Kawahara. 2005. Japanese mor-
phological analysis system JUMAN version 5.1 manual.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long Japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
R. Malouf. 2002. Markov models for language-independent
named entity recognition. In Proc. of CoNLL-2002, pages
187?190.
Asahara Masayuki and Yuji Matsumoto. 2003. Japanese
named entity extraction with redundant morphological anal-
ysis. In Proc. of HLT-NAACL 2003, pages 8?15.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka
Hirano, Hiroshi Matsuda, Kazuma Takaoka, and Masayuki
Asahara. 2003. Morphological analysis System chasen
2.3.3 users manual.
Behrang Mohit and Rebecca Hwa. 2005. Syntax-based semi-
supervised named entity tagging. In Proc. of ACL Interactive
Poster and Demonstration Sessoins, pages 57?60.
MUC-6. 1995. Proc. of the Sixth Message Understanding Con-
ference. Morgan Kaufmann Publishers, INC.
Keigo Nakano and Yuzo Hirai. 2004. Japanese named entity
extraction with bunsetsu features (in Japanese). Trans. of
Information Processing Society of Japan, 45(3):934?941.
The National Language Institute for Japanese Language, NLRI,
editor. 1993. Bunrui Goi Hyo (in Japanese). Shuuei Pub-
lishing.
John C. Platt, Nello Cristiani, and John ShaweTaylor. 2000.
Lage margin DAGs for multiclas classification. In Advances
in Neural Information Processing System 12.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi.
2007. Improving coreference resolution using bridging ref-
erence resolution and automatically acquired synonyms. In
Proc. of DAARC-2007.
Takehito Utsuro, Manabu Sassano, and Kiyotaka Uchimoto.
2002. Combing outputs of multiple named entity chunkers
by stacking. In Proc. of EMNLP-2002.
Hiroyasu Yamada. 2007. Shift reduce chunking for Japanese
named entity extraction (in Japanese). In IPSJ SIG Notes
NL-179-3, pages 13?18.
612
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213?1223,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Knowledge Acquisition for Case Alternation
between the Passive and Active Voices in Japanese
Ryohei Sasano1 Daisuke Kawahara2 Sadao Kurohashi2 Manabu Okumura1
1 Precision and Intelligence Laboratory, Tokyo Institute of Technology
2 Graduate School of Informatics, Kyoto University
{sasano,oku}@pi.titech.ac.jp, {dk,kuro}@i.kyoto-u.ac.jp
Abstract
We present a method for automatically acquir-
ing knowledge for case alternation between
the passive and active voices in Japanese. By
leveraging several linguistic constraints on al-
ternation patterns and lexical case frames ob-
tained from a large Web corpus, our method
aligns a case frame in the passive voice to a
corresponding case frame in the active voice
and finds an alignment between their cases.
We then apply the acquired knowledge to a
case alternation task and prove its usefulness.
1 Introduction
Predicate-argument structure analysis is one of the
fundamental techniques for many natural language
applications such as recognition of textual entail-
ment, information retrieval, and machine transla-
tion. In Japanese, the relationship between a pred-
icate and its argument is usually represented by us-
ing case particles1 (Kawahara and Kurohashi, 2006;
Taira et al, 2008; Yoshikawa et al, 2011). However,
since case particles vary depending on the voices,
we have to take case alternation into account to rep-
resent predicate-argument structure. There are thus
two major types of representations: one uses surface
cases, and the other uses normalized-cases for the
base form of predicates. For example, while the Ky-
oto University Text Corpus (Kawahara et al, 2004),
one of the major Japanese corpora that contains an-
notations of predicate-argument structures, adopts
1Japanese is a head-final language. Word order does not
mark syntactic relations. Instead, postpositional case particles
function as case markers.
the former representation, the NAIST Text Corpora
(Iida et al, 2007), another major Japanese corpus,
adopts the latter representation.
Examples (1) and (2) describe the same event in
the passive and active voices, respectively. When
we use surface cases to represent the relationship be-
tween the predicate and its argument in Example (1),
the case of ?? (woman)? is ga2 and the case of ??
(man)? is ni.2 On the other hand, when we use the
normalized-cases for the base form, the case of ??
(woman)? is wo2 and the case of ?? (man)? is ga,
which are the same as the surface cases in the active
voice as in Example (2).
(1) ?? ?? ????????
woman-ga man-ni was pushed down
(A woman was pushed down by a man.)
(2) ?? ?? ???????
man-ga woman-wo pushed down
(A man pushed down a woman.)
Both representations have their own advantages.
Surface case analysis is easier than normalized-case
analysis, especially when we consider omitted ar-
guments, which are also called zero anaphors (Na-
gao and Hasida, 1998). In Japanese, zero anaphora
frequently occurs, and the omitted unnormalized-
case of a zero anaphor is often the same as the
surface case of its antecedent (Sasano and Kuro-
hashi, 2011). Therefore, surface case analysis suits
zero anaphora resolution. On the other hand, when
2Ga, wo, and ni are typical Japanese postpositional case par-
ticles. In most cases, they indicate nominative, accusative, and
dative, respectively.
1213
we focus on the resulting predicate argument struc-
tures, the normalized-case structure is more useful.
Specifically, since a normalized-case structure rep-
resents the same meaning in the same representa-
tion, normalized-case analysis is useful for recog-
nizing textual entailment and information retrieval.
Therefore, we need a system that first analyzes
surface cases and then alternates the surface cases
with normalized-cases. In particular, we focus on
the transformation of the passive voice into the ac-
tive voice in this paper. Passive-to-active voice
transformation in English can be performed system-
atically, which does not depend on lexical infor-
mation in most cases. However, in Japanese, the
method of transformation depends on lexical infor-
mation. For example, while the case particle ni in
Example (1) is alternated with ga in the active voice,
the case particle ni in Example (3) is not alternated in
the active voice as in Example (4) even though both
their predicates are ???????? (be pushed
down).?
(3) ?? ?? ????????
woman-ga sea-ni was pushed down
(A woman was pushed down into the sea.)
(4) ?? ?? ???????
woman-wo sea-ni pushed down
(? pushed down a woman into the sea.)
The ni case in Example (1) indicates agent. On
the other hand, the ni case in Example (3) indicates
direction. To determine the difference is important
for many NLP applications including machine trans-
lation. In fact, Google Translate (GT)3 translates
Examples (1) and (3) as ?Woman was pushed down
in the man? and ?Woman was pushed down in the
sea,? respectively, which may be because GT cannot
distinguish between the roles of ni in Examples (1)
and (3).
(5) ?? ?? ?????
prize-ga man-ni was awarded
(A prize was awarded to a man.)
In example (5), although the ni-case argument
?? (man)? is the same as in Example (1), the case
particle ni indicates recipient and is not alternated
in the active voice. These examples show that case
3http://translate.google.com, accessed 2013-2-20.
alternation between the passive and active voices in
Japanese depends on not only predicates but also ar-
guments, and we have to consider their combina-
tions. Since it is impractical to manually describe
the case alternation rules for all combinations of
predicates and arguments, we have to acquire such
knowledge automatically.
Thus, in this paper, we present a method for ac-
quiring the knowledge for case alternation between
the passive and active voices in Japanese. Our
method leverages several linguistic constraints on al-
ternation patterns and lexical case frames obtained
from a large Web corpus, which are constructed for
each meaning and voice of each predicate.
2 Related Work
Levin (1993) grouped English verbs into classes on
the basis of their shared meaning components and
syntactic behavior, defined in terms of diathesis al-
ternations. Hence, diathesis alternations have been
the topic of interest for a number of researchers
in the field of automatic verb classification, which
aims to induce possible verb frames from corpora
(e.g., McCarthy 2000; Lapata and Brew 2004; Joa-
nis et al 2008; Schulte im Walde et al 2008; Li and
Brew 2008; Sun and Korhonen 2009; Theijssen et al
2012). Baroni and Lenci (2010) used distributional
slot similarity to distinguish between verbs undergo-
ing the causative-inchoative alternations, and verbs
that do not alternate.
There is some work on passive-to-active voice
transformation in Japanese. Baldwin and Tanaka
(2000) empirically identified the range and fre-
quency of basic verb alternation, including active-
passive alternation, in Japanese. They automatically
extracted alternation types by using hand-crafted
case frames but did not evaluate the quality. Kondo
et al (2001) dealt with case alternation between the
passive and active voices as a subtask of paraphras-
ing a simple sentence. They manually introduced
case alternation rules on the basis of verb types and
case patterns and transformed passive sentences into
active sentences.
Murata et al (2006) developed a machine-
learning-based method for Japanese case alterna-
tion. They extracted 3,576 case particles in passive
sentences from the Kyoto University Text Corpus
1214
Case particle Grammatical function
ga nominative
wo accusative
ni dative
de locative, instrumental
kara ablative
no genitive
Table 1: Examples of Japanese postpositional case parti-
cles and their typical grammatical functions.
and tagged their cases in the active voice. Then,
they trained SVM classifiers using the tagged cor-
pus. Their features for training SVM were made
by using several lexical resources such as IPAL
(IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo
(NLRI, 1993), and the output of Kondo et al?s
method.
3 Lexicalized Case Frames
To acquire knowledge for case alternation, we ex-
ploit lexicalized case frames that are automatically
constructed from 6.9 billion Web sentences by using
Kawahara and Kurohashi (2002)?s method. In short,
their method first parses the input sentences, and
then constructs case frames by collecting reliable
modifier-head relations from the resulting parses.
These case frames are constructed for each predi-
cate like PropBank frames (Palmer et al, 2005), for
each meaning of the predicate like FrameNet frames
(Fillmore et al, 2003), and for each voice. However,
neither pseudo-semantic role labels such as Arg1 in
PropBank nor information about frames defined in
FrameNet are included in these case frames. Each
case frame describes surface cases that each predi-
cate has and instances that can fill a case slot, which
is fully lexicalized like the subcategorization lexicon
VALEX (Korhonen et al, 2006).
We list some Japanese postpositional case parti-
cles with their typical grammatical functions in Ta-
ble 1 and show examples of case frames in Table
2.4 Ideally, one case frame is constructed for each
meaning and voice of the target predicate. However,
since Kawahara and Kurohashi?s method is unsuper-
vised, several case frames are actually constructed
4Niyotte in Table 2 is a Japanese functional phrase that in-
dicates agent in this case. We treat niyotte as a case particle in
this paper for the sake of simplicity.
Case Frame: ????????-4 (be pushed down-4)?
{?? (woman):5,? (I):2,? (woman):2, ? ? ? }-ga
{? (sea):229,? (bottom):115,? (pond):51, ? ? ? }-ni
{??(stepmother):2,????(Pegasus):2, ? ? ? }-niyotte
? ? ?
Case Frame: ????????-5 (be pushed down-5)?
{?? (Kyoko):3,?? (manager):1, ? ? ? }-ga
{?? (someone):143,??? (somebody):85, ? ? ? }-ni
{?? (stair):20,? (ship):7,? (cliff):7, ? ? ? }-kara
? ? ?
Case Frame: ??????-2 (push down-2)?
{? (man):14,?? (lion):5,? (tiger):3, ? ? ? }-ga
{?(child):316,??(child):81,?(person):51, ? ? ? }-wo
{? (sea):580,? (ravine):576,? (river):352 ? ? ? }-ni
? ? ?
Case Frame: ??????-4 (push down-4)?
{?? (someone):14,???? (lion):5, ? ? ? }-ga
{? (person):257,? (I):214,? (child):137, ? ? ? }-wo
{? (cliff):53,?? (stair):28, ? ? ? }-kara
? ? ?
Table 2: Examples of case frames for ???????
? (be pushed down)? and ?????? (push down).?
Words in curly braces denote instances that can fill cor-
responding cases and the numbers following these words
denote their frequency in the corpus.
for each meaning and voice. For example, 59 and
eight case frames were respectively constructed for
the predicate in the passive voice ????????
(be pushed down)? and in the active voice ????
?? (push down)? from 6.9 billion Web sentences.
Table 2 shows the 4th and 5th case frames for ???
????? (be pushed down)? and the 2nd and 4th
case frames for ?????? (push down).?
Table 3 shows an example of case frames for
??? (hit),? which includes no-case. Here, the
Japanese postpositional case particle ?no? roughly
corresponds to ?of,? that is, ?X no Y? means ?Y of
X,? and thus no-case is not an argument of the target
predicate. While Kawahara and Kurohashi?s method
basically collects arguments of the target predicate,
the phrase of no-case that modifies the direct object
of the predicate is also collected as no-case. This
is because, as we will show in the next section, this
phrase can be represented as ga-case in the passive
voice.
1215
Case Frame: ???-2 (hit-2)?
{? (man):51,? (fist):30,?? (someone):23, ? ? ? }-ga
{?? (myself):360,? (I):223, ? ? ? }-no
{? (head):5424,? (face):3215, ? ? ? }-wo
{? (fist):316,?? (palm):157,?? (fist):126, ? ? ? }-de
? ? ?
Table 3: An example of case frames for ??? (hit).?
4 Passive-Active Transformation in
Japanese
Morphologically speaking, the passive voice in
Japanese is expressed by using the auxiliary verbs
??? (reru)? and ???? (rareru),? whose past
forms are ??? (reta)? and ???? (rareta),? re-
spectively. For example, the verb in the base form
?????? (tsukiotosu, push down)? is trans-
formed into the past passive form ???????
? (tsukiotosa-reta, was pushed down).? Case al-
ternations accompany passive-active transformation
in Japanese. There are only two case alternations
at most in passive-active transformation. One is the
case represented as ga in the passive voice, and the
other is the case represented as ga in the active voice.
Japanese passive sentences can be classified into
three types in accordance with what is represented
as ga-case in the passive voice: direct passive, in-
direct passive, and possessor passive.
In direct passive sentence, the object of the pred-
icate in the active voice is represented as ga-case.
Examples (1), (3), and (5) are all direct passive sen-
tences. The case that is represented as ga in the ac-
tive voice is usually represented as ni, niyotte, kara,
or de in the passive sentence. In the first sentence of
Examples (6) and (7),5 ga-cases in the active voice
are represented as niyotte and kara, respectively. On
the other hand, ga-case in the passive sentence is al-
ternated with wo or ni as shown with broken lines in
the second sentence of Examples (6) and (7).
(6) P: ???...... ????? ??????
cause-ga..... man-niyotte was identified
(The cause was identified by a man.)
A: ?? ???...... ?????
man-ga cause-wo...... identified
(A man identified the cause.)
5?P? denotes a passive sentence and ?A? denotes the corre-
sponding active sentence in these examples.
(7) P: ??...... ??? ????????
man-ga..... woman-kara was talked to
(A man was talked to by a woman.)
A: ?? ??...... ??????
woman-ga man-ni.... talked to
(A woman talked to a man.)
Indirect passive is also called adversative pas-
sive, in which an indirectly influenced agent is repre-
sented with ga. For example, ?? (I),? the argument
represented with ga in the first sentence of Exam-
ple (8), does not appear in the active voice, i.e. the
second sentence of Example (8). In the case of in-
direct passive, ga-case in the active sentence is al-
ways alternated with ni-case in the passive sentence
as shown with solid lines in Examples (8).
(8) P: ??...... ??? ?????
I-ga..... child-ni was cried
(I?ve got a child crying.)
A: ??? ????(A child cried.)
child-ga cried
Possessor passive is similar to indirect passive in
that the argument represented with ga-case does not
appear as an argument of the predicate in the ac-
tive voice. Therefore, possessor passive is some-
times treated as a kind of indirect passive. How-
ever, in the case of possessor passive, the argument
appears in the active sentence as a possessor of the
direct object. For example, the ga-case argument
?? (woman)? in the passive sentence of Example
(9) does not appear as an argument of the predicate
???? (hit)? in the active sentence but appears in
the phrase that modifies the direct object ?? (head)?
with the case particle no, which indicates that ??
(woman)? is the possessor of ?? (head).?
(9) P: ??...... ?? ?? ?????
woman-ga..... man-ni head-wo was hit
(A woman was hit on the head by a man.)
A:?? ??...... ?? ????
man-ga woman-no..... head-wo hit
(A man hit the head of a woman.)
In conclusion, the number of case alternation pat-
terns accompanying passive-active transformation in
Japanese is limited. Ga-case in the passive voice can
1216
be alternated only with either wo, ni, or no, or does
not appear in the active voice. Ga-case in the active
voice can be represented only by ni, niyotte, kara,
or de in the passive voice. Hence, it is sufficient to
consider only their combinations.
5 Knowledge Acquisition for Case
Alternation
5.1 Task Definition
Our objective is to acquire knowledge for case al-
ternation between the passive and active voices in
Japanese. We leverage lexical case frames obtained
from a large Web corpus by using Kawahara and
Kurohashi (2002)?s method and align cases of a case
frame in the passive voice and cases of a case frame
in the active voice. As described in Section 2, sev-
eral case frames are constructed for each voice of
each predicate. Our task consists of the following
two subtasks:
1. Identify a corresponding case frame in the ac-
tive voice.
2. Find an alignment between cases of case
frames in the passive and active voice.
Figure 1 shows the overview of our task. If a case
frame in the passive voice is input, we identify a cor-
responding case frame in the active voice, and find
an alignment between cases by using the algorithm
described in Section 5.3. In this example, an active
case frame ??????-4 (push down-4)? is iden-
tified as a corresponding case frame for the input
passive case frame ????????-5 (be pushed
down-5)? and ga, ni, and kara-cases in the passive
case frame are aligned to wo, ga, and kara-cases in
the active case frame, respectively.
5.2 Clues for Knowledge Acquisition
We exploit three clues for corresponding case frame
identification and case alignment as follows:
1. Semantic similarity between the instances of
the aligned cases: simSEM .
2. Case distribution similarity between the corre-
sponding case frames: simDIST .
3. Preference of alternation patterns: fPP .
&DVH)UDPH SXVKGRZQ
^DZRUG 	ZRUGV `JD
^
,SHUVRQ`ZR
^ ERWWRPKHOO`QL

&DVH)UDPH SXVKGRZQ
^PDQ OLRQ `JD
^FKLOGFKLOG`ZR
^VHDUDYLQHProceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023?1032,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Subtree Extractive Summarization via Submodular Maximization
Hajime Morita
Tokyo Institute of Technology, Japan
morita@lr.pi.titech.ac.jp
Hiroya Takamura
Tokyo Institute of Technology, Japan
takamura@pi.titech.ac.jp
Ryohei Sasano
Tokyo Institute of Technology, Japan
sasano@pi.titech.ac.jp
Manabu Okumura
Tokyo Institute of Technology, Japan
oku@pi.titech.ac.jp
Abstract
This study proposes a text summarization
model that simultaneously performs sen-
tence extraction and compression. We
translate the text summarization task into
a problem of extracting a set of depen-
dency subtrees in the document cluster.
We also encode obligatory case constraints
as must-link dependency constraints in or-
der to guarantee the readability of the gen-
erated summary. In order to handle the
subtree extraction problem, we investigate
a new class of submodular maximization
problem, and a new algorithm that has
the approximation ratio 12(1 ? e?1). Ourexperiments with the NTCIR ACLIA test
collections show that our approach outper-
forms a state-of-the-art algorithm.
1 Introduction
Text summarization is often addressed as a task
of simultaneously performing sentence extraction
and sentence compression (Berg-Kirkpatrick et
al., 2011; Martins and Smith, 2009). Joint mod-
els of sentence extraction and compression have
a great benefit in that they have a large degree of
freedom as far as controlling redundancy goes. In
contrast, conventional two-stage approaches (Za-
jic et al, 2006), which first generate candidate
compressed sentences and then use them to gen-
erate a summary, have less computational com-
plexity than joint models. However, two-stage ap-
proaches are suboptimal for text summarization.
For example, when we compress sentences first,
the compressed sentences may fail to contain im-
portant pieces of information due to the length
limit imposed on each sentence. On the other
hand, when we extract sentences first, an impor-
tant sentence may fail to be selected, simply be-
cause it is long. Enumerating a huge number
of compressed sentences is also infeasible. Joint
models can prune unimportant or redundant de-
scriptions without resorting to enumeration.
Meanwhile, submodular maximization has re-
cently been applied to the text summarization task,
and the methods thereof have performed very well
(Lin and Bilmes, 2010; Lin and Bilmes, 2011;
Morita et al, 2011). Formalizing summarization
as a submodular maximization problem has an im-
portant benefit inthat the problem can be solved by
using a greedy algorithm with a performance guar-
antee.
We therefore decided to formalize the task of si-
multaneously performing sentence extraction and
compression as a submodular maximization prob-
lem. That is, we extract subsentences for mak-
ing the summary directly from all available sub-
sentences in the documents and not in a stepwise
fashion. However, there is a difficulty with such
a formalization. In the past, the resulting maxi-
mization problem has been often accompanied by
thousands of linear constraints representing logi-
cal relations between words. The existing greedy
algorithm for solving submodular maximization
problems cannot work in the presence of such nu-
merous constraints although monotone and non-
monotone submodular maximization with con-
straints other than budget constraints have been
studied (Lee et al, 2009; Kulik et al, 2009; Gupta
et al, 2010). In this study, we avoid this difficulty
by reducing the task to one of extracting depen-
dency subtrees from sentences in the source doc-
uments. The reduction replaces the difficulty of
numerous linear constraints with another difficulty
wherein two subtrees can share the same word to-
1023
ken when they are selected from the same sen-
tence, and as a result, the cost of the union of the
two subtrees is not always the mere sum of their
costs. We can overcome this difficulty by tackling
a new class of submodular maximization prob-
lem: a budgeted monotone nondecreasing sub-
modular function maximization with a cost func-
tion, where the cost of an extraction unit varies
depending on what other extraction units are se-
lected. By formalizing the subtree extraction prob-
lem as this new maximization problem, we can
treat the constraints regarding the grammaticality
of the compressed sentences in a straightforward
way and use an arbitrary monotone submodular
word score function for words including our word
score function (shown later). We also propose a
new greedy algorithm that solves this new class of
maximization problem with a performance guar-
antee 12(1? e?1).
We evaluated our method on by using it to per-
form query-oriented summarization (Tang et al,
2009). Experimental results show that it is supe-
rior to state-of-the-art methods.
2 Related Work
Submodularity is formally defined as a property of
a set function for a finite universe V . The function
f : 2V ? R maps a subset S ? V to a real value.
If for any S, T ? V , f(S ? T ) + f(S ? T ) ?
f(S)+f(T ), f is called submodular. This defini-
tion is equivalent to that of diminishing returns,
which is well known in the field of economics:
f(S ?{u})? f(S) ? f(T ?{u})? f(T ), where
T ? S ? V and u is an element of V . Di-
minishing returns means that the value of an el-
ement u remains the same or decreases as S be-
comes larger. This property is suitable for sum-
marization purposes, because the gain of adding a
new sentence to a summary that already contains
sufficient information should be small. Therefore,
many studies have formalized text summarization
as a submodular maximization problem (Lin and
Bilmes, 2010; Lin and Bilmes, 2011; Morita et
al., 2011). Their approaches, however, have been
based on sentence extraction. To our knowledge,
there is no study that addresses the joint task of
simultaneously performing compression and ex-
traction through an approximate submodular max-
imization with a performance guarantee.
In the field of constrained maximization prob-
lems, Kulik et al (2009) proposed an algorithm
that solves the submodular maximization problem
under multiple linear constraints with a perfor-
mance guarantee 1? e?1 in polynomial time. Al-
though their approach can represent more flexible
constraints, we cannot use their algorithm to solve
our problem, because their algorithm needs to enu-
merate many combinations of elements. Integer
linear programming (ILP) formulations can repre-
sent such flexible constraints, and they are com-
monly used to model text summarization (McDon-
ald, 2007). Berg-Kirkpatrick et al (2011) formu-
lated a unified task of sentence extraction and sen-
tence compression as an ILP. However, it is hard to
solve large-scale ILP problems exactly in a practi-
cal amount of time.
3 Budgeted Submodular Maximization
with Cost Function
3.1 Problem Definition
Let V be the finite set of all valid subtrees in
the source documents, where valid subtrees are
defined to be the ones that can be regarded as
grammatical sentences. In this paper, we regard
subtrees containing the root node of the sentence
as valid. Accordingly, V denotes a set of all
rooted subtrees in all sentences. A subtree con-
tains a set of elements that are units in a de-
pendency structure (e.g., morphemes, words or
clauses). Let us consider the following problem
of budgeted monotone nondecreasing submodu-
lar function maximization with a cost function:
maxS?V {f(S) : c (S) ? L} , where S is a sum-
mary represented as a set of subtrees, c(?) is the
cost function for the set of subtrees, L is our bud-
get, and the submodular function f(?) scores the
summary quality. The cost function is not always
the sum of the costs of the covered subtrees, but
depends on the set of the covered elements by the
subtrees. Here, we will assume that the generated
summary has to be as long as or shorter than the
given summary length limit, as measured by the
number of characters. This means the cost of a
subtree is the integer number of characters it con-
tains.
V is partitioned into exclusive subsetsB of valid
subtrees, and each subset corresponds to the orig-
inal sentence from which the valid subtrees de-
rived. However, the cost of a union of subtrees
from different sentences is simply the sum of the
costs of subtrees, while the cost of a union of sub-
trees from the same sentence is smaller than the
sum of the costs. Therefore, the problem can be
represented as follows:
1024
max
S?V
{
f(S) :
?
B?B
c (B ? S) ? L
}
. (1)
For example, if we add a subtree t containing
words {wa,wb,wc} to a summary that already
covers words {wa, wb, wd} from the same sen-
tence, the additional cost of t is only c({wc}) be-
cause wa and wb are already covered1.
The problem has two requirements. The first
requirement is that the union of valid subtrees is
also a valid subtree. The second requirement is
that the union of subtrees and a single valid sub-
tree have the same score and the same cost if they
cover the same elements. We will refer to the sin-
gle valid subtree as the equivalent subtree of the
union of subtrees. These requirements enable us
to represent sentence compression as the extrac-
tion of subtrees from a sentence. This is because
the requirements guarantee that the extracted sub-
trees represent a sentence.
3.2 Greedy Algorithm
We propose Algorithm 1 that solves the maximiza-
tion problem (Eq.1). The algorithm is based on
ones proposed by Khuller et al (1999) and Krause
et al (2005). Instead of enumerating all candidate
subtrees, we use a local search to extract the ele-
ment that has the highest gain per cost. In the al-
gorithm, Gi indicates a summary set obtained by
adding element si to Gi?1. U means the set of
subtrees that are not extracted. The algorithm it-
eratively adds to the current summary the element
si that has the largest ratio of the objective func-
tion gain to the additional cost, unless adding it
violates the budget constraint. We set a parame-
ter r that is the scaling factor proposed by Lin and
Bilmes (2010). After the loop, the algorithm com-
pares Gi with the {s?} that has the largest value of
the objective function among all subtrees that are
under the budget, and it outputs the summary can-
didate with the largest value.
Let us analyze the performance guarantee of Al-
gorithm 12.
1Each subset B corresponds to a kind of greedoid con-
straint. V implicitly constrains the model such that it can
only select valid subtrees from a set of nodes and edges.
2Our performance guarantee is lower than that reported
by Lin and Bilmes (2010). However, their proof is er-
roneous. In their proof of Lemma 2, they derive ?u ?
S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
, for any i(1 ? i ? |G|),
from line 4 of their Algorithm 1, which selects the densest
element out of all available elements. However, the inequal-
ity does not hold for i, for which element u selected on line
4 is discarded on line 5 of their algorithm. The performance
guarantee of their algorithm is actually the same as ours, since
Algorithm 1 Modified greedy algorithm for budgeted
submodular function maximization with a cost function .
1: G0 ? ?
2: U ? V
3: i? 1
4: while U 6= ? do
5: si ? argmaxs?U f(Gi?1?{s})?f(Gi?1)(c(Gi?1?{s})?c(Gi?1))r
6: if c({si} ?Gi?1) ? L then
7: Gi ? Gi?1 ? {si}
8: i? i + 1
9: end if
10: U ? U\{si}
11: end while
12: s?? argmaxs?V,c(s)?L f({s})
13: return Gf = argmaxS?{{s?},Gi} f(S)
Theorem 1 For a normalized monotone submod-
ular function f(?), Algorithm 1 has a constant
approximation factor when r = 1 as follows:
f(Gf ) ?
(1
2(1? e
?1)
)
f(S?), (2)
where S? is the optimal solution and, Gf is the
solution obtained by Greedy Algorithm 1.
Proof. See appendix.
3.3 Relation with Discrete Optimization
We argue that our optimization problem can be
regarded as an extraction of subtrees rooted at a
given node from a directed graph, instead of from
a tree. Let D be the set of edges of the directed
graph, F be a subset of D that is a subtree. In the
field of combinatorial optimization, a pair (D, F)
is a kind of greedoid: directed branching greedoid
(Schmidt, 1991). A greedoid is a generalization of
the matroid concept. However, while matroids are
often used to represent constraints on submodular
maximization problems (Conforti and Cornue?jols,
1984; Calinescu et al, 2011), greedoids have not
been used for that purpose, in spite of their high
representation ability. To our knowledge, this is
the first study that gives a constant performance
guarantee for the submodular maximization under
greedoid (non-matroid) constraints.
the guarantee 12 (1? e?1) was already proved by Krause andGuestrin (2005). We show a counterexample. Suppose that
V is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density
3:cost 1), e4(density 1:cost 1) }, and cost limit K is 10. The
optimal solution is S? = {e1, e2}. Their algorithm selects
e1, e3, e4 in this order. However the algorithm selects e2 on
line 4 after selecting e3, and it drops e2 on line 5. As a result,
e4 selected by the algorithm does not satisfy the inequality
?u ? S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
.
1025
4 Joint Model of Extraction and
Compression
We will formalize the unified task of sentence
compression and extraction as a budgeted mono-
tone nondecreasing submodular function maxi-
mization with a cost function. In this formaliza-
tion, a valid subtree of a sentence represents a
candidate of a compressed sentence. We will re-
fer to all valid subtrees of a given sentence as a
valid set. A valid set corresponds to all candi-
dates of the compression of a sentence. Note that
although we use the valid set in the formaliza-
tion, we do not have to enumerate all the candi-
dates for each sentence. Since, from the require-
ments, the union of valid subtrees is also a valid
subtree in the valid set, the model can extract one
or more subtrees from one sentence, and generate
a compressed sentence by merging those subtrees
to generate an equivalent subtree. Therefore, the
joint model can extract an arbitrarily compressed
sentence as a subtree without enumerating all can-
didates. The joint model can remove the redundant
part as well as the irrelevant part of a sentence, be-
cause the model simultaneously extracts and com-
presses sentences. We can approximately solve the
subtree extraction problem by using Algorithm 1.
On line 5 of the algorithm, the subtree extraction
is performed as a local search that finds maximal
density subtrees from the whole documents. The
maximal density subtree is a subtree that has the
highest score per cost of subtree. We use a cost
function to represent the cost, which indicates the
length of word tokens in the subtree.
In this paper, we address the task of summariza-
tion of Japanese text by means of sentence com-
pression and extraction. In Japanese, syntactic
subtrees that contain the root of the dependency
tree of the original sentence often make gram-
matical sentences. This means that the require-
ments mentioned in Section 3.1 that a union of
valid subtrees is a valid and equivalent tree is of-
ten true for Japanese. The root indicates the pred-
icate of a sentence, and it is syntactically modi-
fied by other prior words. Some modifying words
can be pruned. Therefore, sentence compression
can be represented as edge pruning. The linguis-
tic units we extract are bunsetsu phrases, which
are syntactic chunks often containing a functional
word after one or more content words. We will re-
fer to bunsetsu phrases as phrases for simplicity.
Since Japanese syntactic dependency is generally
defined between two phrases, we use the phrases
as the nodes of subtrees.
In this joint model, we generate a compressed
sentence by extracting an arbitrary subtree from a
dependency tree of a sentence. However, not all
subtrees are always valid. The sentence generated
by a subtree can be unnatural even though the sub-
tree contains the root node of the sentence. To
avoid generating such ungrammatical sentences,
we need to detect and retain the obligatory de-
pendency relations in the dependency tree. We
address this problem by imposing must-link con-
straints if a phrase corresponds to an obligatory
case of the main predicate. We merge obligatory
phrases with the predicate beforehand so that the
merged nodes make a single large node.
Although we focus on Japanese in this pa-
per, our approach can be applied to English and
other languages if certain conditions are satisfied.
First, we need a dependency parser of the lan-
guage in order to represent sentence compression
as dependency tree pruning. Moreover, although,
in Japanese, obligatory cases distinguish which
edges of the dependency tree can be pruned or not,
we need another technique to distinguish them in
other languages. For example we can distinguish
obligatory phrases from optional ones by using se-
mantic role labeling to detect arguments of predi-
cates. The adaptation to other languages is left for
future work.
4.1 Objective Function
We extract subtrees from sentences in order to
solve the query-oriented summarization problem
as a unified one consisting of sentence compres-
sion and extraction. We thus need to allocate a
query relevance score to each node. Off-the-shelf
similarity measures such as the cosine similarity of
bag-of-words vectors with query terms would al-
locate scores to the terms that appear in the query,
but would give no scores to terms that do not ap-
pear in it. With such a similarity, sentence com-
pression extracts nearly only the query terms and
fails to contain important information. Instead,
we used Query SnowBall (QSB) (Morita et al,
2011) to calculate the query relevance score of
each phrase. QSB is a method for query-oriented
summarization, which calculates the similarity be-
tween query terms and each word by using co-
occurrences within the source documents. Al-
though the authors of QSB also provided scores
of word pairs to avoid putting excessive penalties
1026
on word overlaps, we do not score word pairs. The
score function is supermodular as a score function
of subtree extraction3, because the union of two
subtrees can have extra word pairs that are not in-
cluded in either subtree. If the extra pair has a pos-
itive score, the score of the union is greater than
the sum of the score of the subtrees. This violates
the definition of submodularity, and invalidates the
performance guarantee of our algorithms.
We designed our objective function by combin-
ing this relevance score with a penalty for redun-
dancy and too-compressed sentences. Important
words that describe the main topic should occur
multiple times in a good summary. However, ex-
cessive overlap undermines the quality of a sum-
mary, as do irrelevant words. Therefore, the scores
of overlapping words should be lower than thoseof
new words. The behavior can be represented by a
submodular objective function that reduces word
scores depending on those already included in the
summary. Furthermore, a summary consisting of
many too-compressed sentences would lack read-
ability. We thus gives a positive reward to long
sentences. The positive reward leads to a natu-
ral summary being generated with fewer sentences
and indirectly penalizes too short sentences. Our
positive reward for long sentences is represented
as
reward(S) = c(S)? |S|, (3)
where c(S) is the cost of summary S, and |S| is the
number of sentences in S. Since a sentence must
contain more than one character, the reward con-
sistently gives a positive score, and gives a higher
score to a summary that consists of fewer sen-
tences.
Let d be the damping rate, countS(w) be the
number of sentences containing word w in sum-
mary S, words(S) be the set of words included in
summary S, qsb(w) be the query relevance score
of word w, and ? be a parameter that adjusts the
rate of sentence compression. Our score function
for a summary S is as follows:
f(S) =
?
w?words(S)
?
?
?
countS(w)?1?
i=0
qsb(w)di
?
?
?+ ? reward(S).
(4)
An optimization problem with this objective
function cannot be regarded as an ILP problem be-
cause it contains non-linear terms. It is also ad-
3The score is still submodular for the purpose of sentence
extraction.
vantageous that the submodular maximization can
deal with such objective functions. Note that the
objective function is such that it can be calculated
according to the type of word. Due to the na-
ture of the objective function, we can use dynamic
programming to effectively search for the subtree
with the maximal density.
4.2 Local Search for Maximal Density
Subtree
Let us now discuss the local search used on line
5 of Algorithm 1. We will use a fast algorithm to
find the maximal density subtree (MDS) of a given
sentence for each cost in Algorithm 1.
Consider the objective function Eq. 4, We can
ignore the second term of the reward function
while looking for the MDS in a sentence because
the number of sentences is the same for every
MDS in a sentence. That is, the gain function of
adding a subtree to a summary can be represented
as the sum of gains for words:
g(t) =
?
w?t
{gainS(w) + freqt(w)c(w)?},
gainS(w) = qsb(w)dcountS(w),
where freqt(w) is the number of ws in subtree
t, and gainS(w) is the gain of adding the word
w to the summary S. Our algorithm is based on
dynamic programming, and it selects a subtree that
maximizes the gain function per cost.
When the word gain is a constant, the algorithm
proposed by Hsieh et al (2010) can be used to
find the MDS. We extended this algorithm to work
for submodular word gain functions that are not
constant. Note that the gain of a word that oc-
curs only once in the sentence, can be treated as
a constant. In what follows, we will describe an
extended algorithm to find the MDS even if there
is word overlap.
For example, let us describe how to obtain the
MDS in the case of a binary tree. First let us tackle
the case in which the gain is always constant. Let
n be a node in the tree, a and b be child nodes of n,
c(n) be the cost of n, mdsca be the MDS rooted at
a and have cost c. mdsn = {mdsc(n)n , . . . ,mdsLn}
denotes the set of MDSs for each cost and its root
node n. The valid subtrees rooted at n can be ob-
tained by taking unions of n with one or both of
t1 ? mdsa and t2 ? mdsb. mdscn is the union that
has the largest gain over the union with the cost of
c (by enumerating all the unions). The MDS for
1027
the sentence root can be found by calculating each
mdscn from the bottom of the tree to the top.
Next, let us consider the objective function that
returns the sum of values of submodular word gain
functions. When there is no word overlap within
the union, we can obtain mdscn in the same man-
ner as for the constant gain. In contrast, if the
union includes word overlap, the gain is less than
the sum of gains: g(mdscn) ? g(n) + g(mdska) +
g(mdsc?k?c(n)b ), where k and c are variables. Thescore reduction can change the order of the gains
of the union. That is, it is possible that another
union without word overlaps will have a larger
gain. Therefore, the algorithm needs to know
whether each t ? mdsn has the potential to have
word overlaps with other MDSs. Let O be the set
of words that occur twice or more in the sentence
on which the local seach focuses. The algorithm
stores MDS for each o ? O, as well as each cost.
By storing MDS for each o and cost as shown
in Fig. 1, the algorithm can find MDS with the
largest gain over the combinations of subtrees.
Algorithm 2 shows the procedure. In it, t andm
denote subtrees, words(t) returns a set of words
in the subtree, g(t) returns the gain of t, tree(n)
means a tree consisting of node n, and t ?m de-
notes the union of subtrees: t and m. subt in-
dicates a set of current maximal density subtrees
among the combinations calculated before. newt
indicates a set of temporary maximal density sub-
trees for the combinations calculated from line 4
to 8. subt[cost,ws] indicates a element of subt that
has a cost cost and contains a set of words ws.
newt[cost,ws] is defined similarly. Line 1 sets subt
to a set consisting of a subtree that indicates node
n itself. The algorithm calculates maximal den-
sity subtrees within combinations of the root node
n and MDSs rooted at child nodes of n. Line 3
iteratively adds MDSs rooted at a next child node
to the combinations; the algorithm then calculates
MDSs newt between subt and the MDSs of the
child node. The procedure from line 6 to 8 selects
a subtree that has a larger gain from the tempo-
rary maximal subtree and the union of t and m.
The computational complexity of this algorithm is
O(NC2) when there is no word overlap within the
sentence, where C denotes the cost of the whole
sentence, and N denotes the number of nodes in
the sentence. The complexity order is the same
as that of the algorithm of Hsieh et al (2010).
When we treat word overlaps, we need to count
Algorithm 2 Algorithm for finding maximal density
subtree for each cost: MDSs.
Function: MDSs
Require: root node n
1: subt[c(n),words(n)?O] = tree(n)
2: newt = ?
3: for i ? child node of n do
4: for t ?MDSs(i) do
5: for m ? subt do
6: index = [c(t ?m), words(t ?m) ? O]
7: newtindex = argmaxj?{newtindex,t?m} g(j)8: end for
9: end for
10: subt = newt
11: end for
12: return subt
Figure 1: Maximal density subtree extraction. The
right table enumerates the subtrees rooted at w2 in
the left tree for all indices. The number in each
tree node is the score of the word.
all unions of combinations of the stored MDSs.
There are at most (C2|O|) MDSs that the algo-
rithm needs to store at each node. Therefore the
total computational complexity is O(NC222|O|).
Since it is unlikely that a sentence contains many
word tokens of one type, the computational cost
may not be so large in practical situations.
5 Experimental Settings
We evaluate our method on Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al, 2008; Mitamura et
al., 2010). The collections contain questions and
weighted answer nuggets. Our experimental set-
tings followed the settings of (Morita et al, 2011),
except for the maximum summary length. We
generated summaries consisting of 140 Japanese
characters or less, with the question as the query
terms. We did this because our aim is to use our
method in mobile situations. We used ?ACLIA1
test data? to tune the parameters, and evaluated our
method on ?ACLIA2 test? data.
We used JUMAN (Kurohashi and Kawahara,
2009a) for word segmentation and part-of-speech
tagging, and we calculated idf over Mainichi
newspaper articles from 1991 to 2005. For the de-
1028
POURPRE Precision Recall F1 F3
Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174
Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190
Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183
Table 1: Results on ACLIA2 test data.
pendency parsing, we used KNP (Kurohashi and
Kawahara, 2009b). Since KNP internally has a
flag that indicates either an ?obligatory case? or an
?adjacent case?, we regarded dependency relations
flagged by KNP as obligatory in the sentence com-
pression. KNP utilizes Kyoto University?s case
frames (Kawahara and Kurohashi, 2006) as the re-
source for detecting obligatory or adjacent cases.
To evaluate the summaries, we followed the
practices of the TAC summarization tasks (Dang,
2008) and NTCIR ACLIA tasks, and computed
pyramid-based precision with the allowance pa-
rameter, recall, and F? (where ? is 1 or 3)
scores. The allowance parameter was determined
from the average nugget length for each question
type of the ACLIA2 collection (Mitamura et al,
2010). Precision and recall are computed from the
nuggets that the summary covered along with their
weights. One of the authors of this paper man-
ually evaluated whether each nugget matched the
summary. We also used the automatic evaluation
measure, POURPRE (Lin and Demner-Fushman,
2006). POURPRE is based on word matching
of reference nuggets and system outputs. We re-
garded as stopwords the most frequent 100 words
in Mainichi articles from 1991 to 2005 (the doc-
ument frequency was used to measure the fre-
quency). We also set the threshold of nugget
matching as 0.5 and binarized the nugget match-
ing, following the previous study (Mitamura et al,
2010). We tuned the parameters by using POUR-
PRE on the development dataset.
Lin and Bilmes (2011) designed a monotone
submodular function for query-oriented summa-
rization. Their succinct method performed well
in DUC from 2004 to 2007. They proposed a
positive diversity reward function in order to de-
fine a monotone submodular objective function for
generating a non-redundant summary. The diver-
sity reward gives a smaller gain for a biased sum-
mary, because it consists of gains based on three
clusters and calculates a square root score with
respect to each sentence. The reward also con-
tains a score for the similarity of a sentence to
the query, for purposes of query-oriented summa-
Recall Length # of nuggets
Subtree extraction 0.213 11,143 100
Reconstructed (RC) 0.228 13,797 108
Table 2: Effect of sentence compression.
rization. Their objective function also includes a
coverage function based on the similarity wi,j be-
tween sentences. In the coverage function min
function limits the maximum gain ??i?V wi,j ,
which is a small fraction ? of the similarity be-
tween a sentence j and the all source documents.
The objective function is the sum of the positive
reward R and the coverage function L over the
source documents V , as follows:
F(S) = L(S) +
3?
k=1
?kRQ,k(S),
L(S) = ?
i?V
min
??
?
?
j?S
wi,j , ?
?
k?V
wi,k
??
? ,
RQ,k =
?
c?Ck
???? ?
j?S?c
( ?N
?
i?V
wi,j + (1? ?)rj,Q),
where ?, ? and ?k are parameters, and rj,Q repre-
sents the similarity between sentence j and query
Q. We tuned the parameters on the development
dataset. Lin and Bilmes (2011) used three clusters
Ck with different granularities, which were calcu-
lated in advance. We set the granularity to (0.2N ,
0.15N , 0.05N ) according to the settings of them,
where N is the number of sentences in a docu-
ment.
We also regarded as stopwords ???? (tell),?
??? (know),? ?? (what)? and their conjugated
forms, which are excessively common in ques-
tions. For the query expansion in the baseline, we
used Japanese WordNet to obtain synonyms and
hypernyms of query terms.
6 Results
Table 1 summarizes our results. ?Subtree ex-
traction (SbE)? is our method, and ?Sentence ex-
traction (NC)? is a version of our method with-
out compression. The NC has the same objec-
tive function but only extracts sentences. The F1-
measure and F3-measure of our method are 0.159
and 0.190 respectively, while those of the state-of-
1029
the-art baseline are 0.135 and 0.174 respectively.
Unfortunately, since the document set is small, the
difference is not statistically significant. Compar-
ing our method with the one without compression,
we can see that there are improvements in the F1
and F3 scores of the human evaluation, whereas
the POURPRE score of the version of our method
without compression is higher than that of our
method with compression. The compression im-
proved the precision of our method, but slightly
decreased the recall.
For the error analyses, we reconstructed the
original sentences from which our method ex-
tracted the subtrees. Table 2 shows the statistics
of the summaries of SbE and reconstructed sum-
maries (RC). The original sentences covered 108
answer nuggets in total, and 8 of these answer
nuggets were dropped by the sentence compres-
sion. Comparing the results of SbE and RC, we
can see that the sentence compression caused the
recall of SbE to be 7% lower than that of RC.
However, the drop is relatively small in light of
the fact that the sentence compression can discard
19% of the original character length with SbE.
This suggests that the compression can efficiently
prune words while avoiding pruning informative
content.
Since the summary length is short, we can select
only two or three sentences for a summary. As
Morita et al (2011) mentioned, answer nuggets
overlap each other. The baseline objective func-
tion R tends to extract sentences from various
clusters. If the answer nuggets are present in the
same cluster, the objective function does not fit the
situation. However, our methods (SbE and NC)
have a parameter d that can directly adjust overlap
penalty with respect to word importance as well
as query relevance. This may help our methods to
cover similar answer nuggets. In fact, the develop-
ment data resulted in a relatively high parameter d
(0.8) for NC compared with 0.2 for SbE.
7 Conclusions and Future Work
We formalized a query-oriented summarization,
which is a task in which one simultaneously per-
forms sentence compression and extraction, as a
new optimization problem: budgeted monotone
nondecreasing submodular function maximization
with a cost function. We devised an approximate
algorithm to solve the problem in a reasonable
computational time and proved that its approxima-
tion rate is 12(1 ? e?1). Our approach achieved
an F3-measure of 0.19 on the ACLIA2 Japanese
test collection, which is 9.2 % improvement over
a state-of-the-art method using a submodular ob-
jective function.
Since our algorithm requires that the objective
function is the sum of word score functions, our
proposed method has a restriction that we cannot
use an arbitrary monotone submodular function as
the objective function for the summary. Our fu-
ture work will improve the local search algorithm
to remove this restriction. As mentioned before,
we also plan to adapt of our system to other lan-
guages.
Appendix
Here, we analyze the performance guarantee of
Algorithm 1. We use the following notation. S? is
the optimal solution, cu(S) is the residual cost of
subtree u when S is already covered, and i? is the
last step before the algorithm discards a subtree
s ? S? or a part of the subtree s. This is because
the subtree does not belong to either the approxi-
mate solution or the optimal solution. We can re-
move the subtree s? from V without changing the
approximate rate. si is the i-th subtree obtained by
line 5 of Algorithm 1. Gi is the set obtained after
adding subtree si to Gi?1 from the valid set Bi.
Gf is the final solution obtained by Algorithm 1.
f(?) : 2V ? R is a monotone submodular func-
tion.
We assume that there is an equivalent sub-
tree with any union of subtrees in a valid set B:
?t1, t2,?te, te ? {t1, t2}. Note that for any or-
der of the set, the cost or profit of the set is fixed:?
ui?S={u1,...,u|S|} cui(Si?1) = c(S).
Lemma 1 ?X,Y ? V, f(X) ? f(Y ) +?
u?X\Y ?u(Y ), where ?u(S) = f(S ? {u}) ?
f(S).
The inequality can be derived from the definition
of submodularity. 2
Lemma 2 For i = 1, . . . , i?+1, when 0 ? r ? 1,
f(S?)?f(Gi?1)?L
r |S?|1?r
csi (Gi?1)
(f(Gi?1?{si})?f(Gi?1)),
where cu(S)=c(S?{u})?c(S).
Proof. From line 5 of Algorithm 1, we have
?u ? S?\Gi?1,
?u(Gi?1)
cu(Gi?1)r
? ?si(Gi?1)csi(Gi?1)r
.
Let B be a valid set, and union be a func-
tion that returns the union of subtrees. We have
1030
?T ? B, ?b ? B, b = union(T ), because we
have an equivalent tree b ? B for each union
of trees T in a valid set B. That is, for any
set of subtrees, we have an equivalent set of sub-
trees, where bi ? Bi. Without loss of generality,
we can replace the difference set S?\Gi?1 with
a set T ?i?1 = {b0, . . . , b|T ?i?1|} that does not con-tain any two elements extracted from the same
valid set. Thus when 0 ? r ? 1 and 0 ?
i ? i? + 1, ?s?\Gi?1 (Gi?1)cS?\Gi?1 (Gi?1)r =
?T ?i?1 (Gi?1)
cT ?i?1 (Gi?1)
r , and
?bj ? T ?i?1,
?bj (Gi?1)
cbj (Gi?1)r
? ?si (Gi?1)csi (Gi?1)r . Thus,
?T ?i?1 (Gi?1) =
?
u?T ?i?1
?u(Gi?1)
? ?si (Gi?1)csi (Gi?1)r
?
u?T ?i?1
cu(Gi?1)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|
(?
u?T ?i?1
cu(Gi?1)
|T ?i?1|
)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|1?r
(?
u?T ?i?1
cu(?)
)r
? ?si (Gi?1)csi (Gi?1)r |S
?|1?rLr,
where the second inequality is from Ho?lder?s in-
equality. The third inequality uses the submodu-
larity of the cost function,
cu(Gi?1) = c({u} ?Gi?1)? c(Gi?1) ? cu(?)
and the fact that |S?| ? |S?\Gi?1| ? |T ?i?1|, and?
u?T ?i?1 cu(?) = c(T
?
i?1) ? L .
As a result, we have
?s?\Gi?1(Gi?1) = ?T ?i?1(Gi?1)
? ?si(Gi?1)csi(Gi?1)r
|S?|1?rLr.
Let X = S? and Y = Gi?1. Applying Lemma
1 yields
f(S?) ? f(Gi?1) + ?u?S?\Gi?1(Gi?1).
? f(Gi?1) +
?si(Gi?1)
csi(Gi?1)
|S?|1?rLr.
The lemma follows as a result.
Lemma 3 For a normalized monotone submodu-
lar f(?), for i = 1, . . . , i? + 1 and 0 ? r ? 1 and
letting si be the i-th unit added into G and Gi be
the set after adding si, we have
f(Gi) ?
(
1?
i?
k=1
(
1? csk(Gk?1)
r
Lr|S?|1?r
))
f(S?).
Proof. This is proved similarly to Lemma 3 of
(Krause and Guestrin, 2005) using Lemma 2.
Proof of Theorem 1. This is proved similarly to
Theorem 1 of (Krause and Guestrin, 2005) using
Lemma 3.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages
481?490, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Calinescu Calinescu, Chandra Chekuri, Martin Pa?l,
and Jan Vondra?k. 2011. Maximizing a monotone
submodular function subject to a matroid constraint.
SIAM Journal on Computing, 40(6):1740?1766.
Michele Conforti and Ge?rard Cornue?jols. 1984. Sub-
modular set functions, matroids and the greedy al-
gorithm: Tight worst-case bounds and some gener-
alizations of the rado-edmonds theorem. Discrete
Applied Mathematics, 7(3):251 ? 274.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks. In Proceedings of Text Analysis Confer-
ence.
Anupam Gupta, Aaron Roth, Grant Schoenebeck, and
Kunal Talwar. 2010. Constrained non-monotone
submodular maximization: offline and secretary
algorithms. In Proceedings of the 6th interna-
tional conference on Internet and network eco-
nomics, WINE?10, pages 246?257, Berlin, Heidel-
berg. Springer-Verlag.
Sun-Yuan Hsieh and Ting-Yu Chou. 2010. The
weight-constrained maximum-density subtree prob-
lem and related problems in trees. The Journal of
Supercomputing, 54(3):366?380, December.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL ?06, pages 176?183, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39?45.
Andreas Krause and Carlos Guestrin. 2005. A
note on the budgeted maximization on submodular
functions. Technical Report CMU-CALD-05-103,
Carnegie Mellon University.
1031
Ariel Kulik, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In Proceedings of
the twentieth Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA ?09, pages 545?554,
Philadelphia, PA, USA. Society for Industrial and
Applied Mathematics.
Sadao Kurohashi and Daisuke Kawahara, 2009a.
Japanese Morphological Analysis System JUMAN
6.0 Users Manual. http://nlp.ist.i.
kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara, 2009b. KN
parser (Kurohashi-Nagao parser) 3.0 Users Man-
ual. http://nlp.ist.i.kyoto-u.ac.jp/
EN/index.php?KNP.
Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and
Maxim Sviridenko. 2009. Non-monotone submod-
ular maximization under matroid and knapsack con-
straints. In Proceedings of the 41st annual ACM
symposium on Theory of computing, STOC ?09,
pages 323?332, New York, NY, USA. ACM.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 912?920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 510?520,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jimmy Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to com-
plex questions. Information Retrieval, 9(5):565?
587, November.
Andre? F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ?09, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR
research, ECIR?07, pages 557?564, Berlin, Heidel-
berg. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima,
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Rui-
hua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong
Ji, and Noriko Kando. 2008. Overview of the
NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual
Information Access. In Proceedings of the 7th NT-
CIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai,
Noriko Kando, Tatsunori Mori, Koichi Takeda,
Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and
Cheng-Wei Lee. 2010. Overview of the ntcir-8 aclia
tasks: Advanced cross-lingual information access.
In Proceedings of the 8th NTCIR Workshop.
Hajime Morita, Tetsuya Sakai, and Manabu Okumura.
2011. Query snowball: a co-occurrence-based ap-
proach to multi-document summarization for ques-
tion answering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 223?229, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Wolfgang Schmidt. 1991. Greedoids and searches in
directed graphs. Discrete Mathmatics, 93(1):75?88,
November.
Jie Tang, Limin Yao, and Dewei Chen. 2009. Multi-
topic based query-oriented summarization. In Pro-
ceedings of 2009 SIAM International Conference
Data Mining (SDM?2009), pages 1147?1158.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and
Richard Schwartz. 2006. Sentence compression
as a component of a multi-document summariza-
tion system. In Proceedings of the 2006 Doc-
ument Understanding Conference (DUC 2006) at
NLT/NAACL 2006.
1032
