Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 34?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Supporting the Adaptation of Texts for Poor Literacy Readers: a Text 
Simplification Editor for Brazilian Portuguese 
 
Arnaldo Candido Jr., Erick Maziero, Caroline Gasperin, Thiago A. S. Pardo, Lucia Specia, and Sandra M. Aluisio  
 
Center of Computational Linguistics (NILC) / Department of Computer Sciences, University of S?o Paulo 
Av. Trabalhador S?o-Carlense, 400. 13560-970 - S?o Carlos/SP, Brazil 
arnaldoc@icmc.usp.br, egmaziero@gmail.com, {cgasperin,taspardo,lspecia,sandra}@icmc.usp.br 
 
Abstract 
In this paper we investigate the task of text 
simplification for Brazilian Portuguese. Our 
purpose is three-fold: to introduce a 
simplification tool for such language and its 
underlying development methodology, to 
present an on-line authoring system of 
simplified text based on the previous tool, and 
finally to discuss the potentialities of such 
technology for education. The resources and 
tools we present are new for Portuguese and 
innovative in many aspects with respect to 
previous initiatives for other languages.  
1 Introduction 
In Brazil, according to the index used to measure 
the literacy level of the population (INAF - National 
Indicator of Functional Literacy), a vast number of 
people belong to the so called rudimentary and basic 
literacy levels. These people are only able to find 
explicit information in short texts (rudimentary 
level) or process slightly longer texts and make 
simple inferences (basic level). INAF reports that 
68% of the 30.6 million Brazilians between 15 and 
64 years who have studied up to 4 years remain at 
the rudimentary literacy level, and 75% of the 31.1 
million who studied up to 8 years remain at the 
rudimentary or basic levels. 
Reading comprehension entails three elements: 
the reader who is meant to comprehend; the text that 
is to be comprehended and the activity in which 
comprehension is a part of (Snow, 2002). In 
addition to the content presented in the text, the 
vocabulary load of the text and its linguistic 
structure, discourse style, and genre interact with the 
reader?s knowledge. When these factors do not 
match the reader?s knowledge and experience, the 
text becomes too complex for the comprehension to 
occur. In this paper we will focus on the text and the 
aspects of it that make reading difficult or easy. One 
solution to ease the syntactic structure of a text is 
via Text Simplification (TS) facilities.  
TS aims to maximize the comprehension of 
written texts through the simplification of their 
linguistic structure. This may involve simplifying 
lexical and syntactic phenomena, by substituting 
words that are only understood by a few people with 
words that are more usual, and by breaking down 
and changing the syntactic structure of the sentence, 
respectively. As a result, it is expected that the text 
can be more easily understood both by humans and 
computer systems (Mapleson, 2006; Siddharthan, 
2003, Max, 2006). TS may also involve dropping 
parts or full sentences and adding some extra 
material to explain a difficult point. This is the case, 
for example, of the approach presented by Petersen 
and Ostendorf (2007), in which abridged versions of 
articles are used in adult literacy learning. 
It has already been shown that long sentences, 
conjoined sentences, embedded clauses, passives, 
non-canonical word order, and use of low-frequency 
words, among other things, increase text complexity 
for language-impaired readers (Siddharthan, 2002; 
Klebanov et al, 2004; Devlin and Unthank, 2006). 
The Plain English initiative makes available 
guidelines to make texts easier to comprehend: the 
Plain Language1. In principle, its recommendations 
can be applied to any language. Although some of 
them are directly useful for TS systems (e.g., 
subject-verb-object order and active voice), others 
are difficult to specify (e.g., how simple each 
syntactic construction is and which words are 
simple). 
In this paper we present the results of a study of 
syntactic simplification for Brazilian Portuguese 
(BP) and a rule-based syntactic simplification 
system for this language that was developed based 
on this study ? the first of this kind for BP. We also 
present an on-line authoring tool for creating 
simplified texts. One possible application of this 
tool is to help teachers to produce instructional texts 
                                                 
1
 http://www.plainlanguage.gov 
34
to be used in classrooms. The study is part of the 
PorSimples project2 (Simplification of Portuguese 
Text for Digital Inclusion and Accessibility), which 
aims at producing text simplification tools for 
promoting digital inclusion and accessibility for 
people with different levels of literacy, and possibly 
other kinds of reading disabilities. 
This paper is organized as follows. In Section 2 
we present related approaches for text simplification 
with educational purposes. In Section 3 we describe 
the proposed approach for syntactic simplification, 
which is used within an authoring tool described in 
Section 4. In Section 5 we discuss possible uses of 
text simplification for educational purposes.  
2 Related work 
Burstein (2009) presents an NLP-based application 
for educational purposes, named Text Adaptor, 
which resembles our authoring tool. It includes 
complex sentence highlighting, text elaboration 
(word substitutions by easier ones), text 
summarization and translation. The system does not 
perform syntactic simplification, but simply 
suggests, using a shallow parser, that some 
sentences might be too complex. Specific hints on 
the actual source of complexity are not provided. 
Petersen (2007) addresses the task of text 
simplification in the context of second-language 
learning. A data-driven approach to simplification is 
proposed using a corpus of paired articles in which 
each original sentence does not necessarily have a 
corresponding simplified sentence, making it 
possible to learn where writers have dropped or 
simplified sentences. A classifier is used to select 
the sentences to simplify, and Siddharthan?s 
syntactic simplification system (Siddharthan, 2003) 
is used to split the selected sentences. In our 
approach, we do not drop sentences, since we 
believe that all the content must be kept in the text. 
Siddharthan proposes a syntactic simplification 
architecture that relies on shallow text analysis and 
favors time performance. The general goal of the 
architecture is to make texts more accessible to a 
broader audience; it has not targeted any particular 
application. The system treats apposition, relative 
clauses, coordination and subordination. Our 
method, on the other hand, relies on deep parsing 
(Bick, 2000). We treat the same phenomena as 
                                                 
2
 http://caravelas.icmc.usp.br/wiki/index.php/Principal 
Siddharthan, but also deal with Subject-Verb-Object 
ordering (in Portuguese sentences can be written in 
different orders) and passive to active voice 
conversion. Siddharthan's system deals with non-
finite clauses which are not handled by our system 
at this stage. 
Lal and Ruger?s (2002) created a bayesian 
summarizer with a built-in lexical simplification 
module, based on WordNet and MRC psycho-
linguistic database3. The system focuses on 
schoolchildren and provides background 
information about people and locations in the text, 
which are retrieved from databases. Our rule-based 
simplification system only replaces discourse 
markers for more common ones using lexical 
resources built in our project, instead of inserting 
additional information in the text. 
Max (2005, 2006) applies text simplification in 
the writing process by embedding an interactive text 
simplification system into a word processor. At the 
user?s request, an automatic parser analyzes an 
individual sentence and the system applies 
handcrafted rewriting rules. The resulting suggested 
simplifications are ranked by a score of syntactic 
complexity and potential change of meaning. The 
writer then chooses their preferred simplification. 
This system ensures accurate output, but requires 
human intervention at every step. Our system, on 
the other hand, is autonomous, even though the user 
is able to undo any undesirable simplification or to 
choose alternative simplifications. These alternative 
simplifications may be produced in two cases: i) to 
compose a new subject in simplifications involving 
relatives and appositions and ii) to choose among 
one of the coordinate or subordinate simplifications 
when there is ambiguity regarding to conjunctions. 
Inui et al (2003) proposes a rule-based system 
for text simplification aimed at deaf people. The 
authors create readability assessments based on 
questionnaires answered by teachers about the deaf. 
With approximately one thousand manually created 
rules, the authors generate several paraphrases for 
each sentence and train a classifier to select the 
simpler ones. Promising results are obtained, 
although different types of errors on the paraphrase 
generation are encountered, such as problems with 
verb conjugation and regency. In our work we 
produce alternative simplifications only in the two 
cases explained above. 
                                                 
3
 http://www.psych.rl.ac.uk/ 
35
Caseli et al (2009) developed an annotation 
editor to support the building of parallel corpora of 
original and simplified texts in Brazilian 
Portuguese. The tool was used to build a corpus of 
simplified texts aimed at people with rudimentary 
and basic literacy levels. We have used the parallel 
corpus to evaluate our rule-based simplification 
system. The on-line authoring system presented in 
this paper evolved from this annotation editor. 
There are also commercial systems like Simplus4 
and StyleWriter5, which aim to support Plain 
English writing.  
3 A rule-based syntactic simplification 
system 
Our text simplification system comprises seven 
operations (see Sections 3.1 and 3.2), which are 
applied to a text in order to make its syntactic 
structure simpler. These operations are applied 
sentence by sentence, following the 3-stage 
architecture proposed by Siddharthan (2002), which 
includes stages of analysis, transformation and 
regeneration. In Siddharthan?s work, the analysis 
stage performs the necessary linguistic analyses of 
the input sentences, such as POS tagging and 
chunking; the transformation stage applies 
simplification rules, producing simplified versions 
of the sentences; the regeneration stage performs 
operations on the simplified sentences to make them 
readable, like referring expressions generation, cue 
words rearrangement, and sentence ordering. 
Differently from such architecture, currently our 
regeneration stage only includes the treatment of 
cue words and a surface forms (GSF) generator, 
which is used to adjust the verb conjugation and 
regency after some simplification operations. 
As a single sentence may contain more than 
one complex linguistic phenomenon, simplification 
operations are applied in cascade to a sentence, as 
described in what follows. 
3.1 Simplification cases and operations 
As result of a study on which linguistic phenomena 
make BP text complex to read and how these 
phenomena could be simplified, we elaborated a 
manual of BP syntactic simplification (Aluisio et al, 
2008). The rule-based text simplification system 
                                                 
4
 http://www.linguatechnologies.com/english/home.html  
5
 http://www.editorsoftware.com/writing-software 
developed here is based on the specifications in this 
manual. According to this manual, simplification 
operations should be applied when any of the 22 
linguistic phenomena presented in Table 1 is 
detected. 
The possible operations suggested to be applied 
in order to simplify these phenomena are: (a) split 
the sentence, (b) change a discourse marker by a 
simpler and/or more frequent one (the indication is 
to avoid the ambiguous ones), (c) change passive to 
active voice, (d) invert the order of the clauses, (e) 
convert to subject-verb-object ordering, (f) change 
topicalization and detopicalization of adverbial 
phrases and (g) non-simplification.  
Table 1 shows the list of all simplification 
phenomena covered by our manual, the clues used 
to identify the phenomena, the simplification 
operations that should be applied in each case, the 
expected order of clauses in the resulting sentence, 
and the cue phrases (translated here from 
Portuguese) used to replace complex discourse 
markers or to glue two sentences. In column 2, we 
consider the following clues: syntactic information 
(S), punctuation (P), and lexicalized clues, such as 
conjunctions (Cj), relative pronouns (Pr) and 
discourse markers (M), and semantic information 
(Sm, and NE for named entities). 
3.2 Identifying simplification cases and 
applying simplification rules 
Each sentence is parsed in order to identify cases for 
simplification. We use parser PALAVRAS (Bick, 
2000) for Portuguese. This parser provides lexical 
information (morphology, lemma, part-of-speech, 
and semantic information) and the syntactic trees for 
each sentence. For some operations, surface 
information (such as punctuation or lexicalized cue 
phrases) is used to identify the simplification cases, 
as well as to assist simplification process. For 
example, to detect and simplify subjective non-
restrictive relative clauses (where the relative 
pronoun is the subject of the relative clause), the 
following steps are performed: 
1. The presence of a relative pronoun is verified. 
2. Punctuation is verified in order to distinguish it 
from restrictive relative clauses: check if the 
pronoun occurs after a comma or semicolon.  
3. Based on the position of the pronoun, the next 
punctuation symbol is searched to define the 
boundaries of the relative clause. 
36
4. The first part of the simplified text is generated, 
consisting of the original sentence without the 
embedded relative clause. 
5. The noun phrase in the original sentence to 
which the relative clause refers is identified. 
6. A second simplified sentence is generated, 
consisting of the noun phrase (as subject) and 
the relative clause (without the pronoun). 
The identification of the phenomena and the 
application of the operations are prone to errors 
though. Some of the clues that indicate the 
occurrence of the phenomena may be ambiguous. 
For example, some of the discourse markers that are 
used to identify subordinate clauses can indicate 
more than one type of these: for instance, ?como? 
(in English ?like?, ?how? or ?as?) can indicate 
reason, conformative or concessive subordinate 
clauses. Since there is no other clue that can help us 
disambiguate among those, we always select the 
case that occurs more frequently according to a 
corpus study of discourse markers and the rhetoric 
relations that they entitle (Pardo and Nunes, 2008). 
However, we can also treat all cases and let the user 
decide the simplifications that is most appropriate. 
 
Phenomenon Clues Op Clause Order Cue phrase Comments 
1.Passive voice S c   Verb may have to be adapted 
2.Embedded appositive S a Original/ 
App. 
 Appositive: Subject is the head of original + 
to be in present tense + apposition 
3.Asyndetic coordinate clause S a Keep order   New sentences: Subjects are the head of the 
original subject 
4.Additive coordinate clause S, Cj a Keep order Keep marker Marker appears in the beginning of the new 
sentence 
5.Adversative coordinate clause M a, b Keep order But  
6.Correlated coordinate clause M a, b Keep order Also Original markers disappear 
7.Result coordinate clause S, M a, b Keep order As a result  
8.Reason coordinate clause S, M a, b Keep order This happens 
because 
May need some changes in verb 
9.Reason subordinate clause M a, b, 
d 
Sub/Main With this To keep the ordering cause, result 
M a, b Main/Sub Also Rule for such ... as, so ... as markers  10.Comparative subordinate clause 
M g   Rule for the other markers or short sentences 
M a, b, 
d 
Sub/Main But ?Clause 1 although clause 2? is changed to 
?Clause 2. But clause 1? 
11.Concessive subordinate clause 
M a, b Main/Sub This happens 
even if 
Rule for hypothetical sentences 
12.Conditional subordinate clause S, M d Sub/Main  Pervasive use in simple accounts 
13. Result subordinate clause M a, b Main/Sub Thus May need some changes in verb 
14.Final/Purpose subordinate clause S, M a, b Main/Sub The goal is  
15.Confirmative subordinate clause M a, b, 
d 
Sub/Main Confirms 
that 
May need some changes in verb 
M a Sub/Main  May need some changes in verb 16.Time subordinate clause 
M a, b  Then Rule for markers: after that, as soon as  
17. Proportional Subordinate Clause M g    
18. Non-finite subordinate clause S g    
19.Non-restrictive relative clause S, P, Pr a Original/ 
Relative 
 Relative: Subject is the head of original + 
relative (subjective relative clause) 
20.Restrictive relative clause S, Pr a Relative/ 
Original 
 Relative: Subject is the head of original + 
relative  (subjective relative clause) 
21.Non Subject-Verb-Object order S e   Rewrite in Subject-Verb-Object order 
22. Adverbial phrases in theme 
position 
S, NE, 
Sm  
f In study  In study 
Table 1: Cases, operations, order and cue phrases 
Every phenomenon has one or more 
simplification steps associated with it, which are 
applied to perform the simplification operations. 
Below we detail each operation and discuss the 
challenges involved and our current limitations in 
their implementing. 
a) Splitting the sentence - This operation is the 
most frequent one. It requires finding the split point 
37
in the original sentence (such as the boundaries of 
relative clauses and appositions, the position of 
coordinate or subordinate conjunctions) and the 
creation of a new sentence, whose subject 
corresponds to the replication of a noun phrase in 
the original sentence. This operation increases the 
text length, but decreases the length of the 
sentences. With the duplication of the term from the 
original sentence (as subject of the new sentence), 
the resulting text contains redundant information, 
but it is very helpful for people at the rudimentary 
literacy level. 
When splitting sentences due to the presence of 
apposition, we need to choose the element in the 
original sentence to which it is referring, so that this 
element can be the subject of the new sentence. At 
the moment we analyze all NPs that precede the 
apposition and check for gender and number 
agreement. If more than one candidate passes the 
agreement test, we choose the closest one among 
these; if none does, we choose the closest among all 
candidates. In both cases we can also pass the 
decision on to the user, which we do in our 
authoring tool described in Section 4. 
For treating relative clauses we have the same 
problem as for apposition (finding the NP to which 
the relative clause is anchored) and an additional 
one: we need to choose if the referent found should 
be considered the subject or the object of the new 
sentence. Currently, the parser indicates the 
syntactic function of the relative pronoun and that 
serves as a clue. 
b) Changing discourse marker - In most cases 
of subordination and coordination, discourse 
markers are replaced by most commonly used ones, 
which are more easily understood. The selection of 
discourse markers to be replaced and the choice of 
new markers (shown in Table 1, col. 4) are done 
based on the study of Pardo and Nunes (2008). 
c) Transformation to active voice - Clauses in 
the passive voice are turned into active voice, with 
the reordering of the elements in the clause and the 
modification of the tense and form of the verb. Any 
other phrases attached to the object of the original 
sentence have to be carried with it when it moves to 
the subject position, since the voice changing 
operation is the first to be performed. For instance, 
the sentence: 
?More than 20 people have been bitten by gold piranhas 
(Serrasalmus Spilopleura), which live in the waters of the 
Sanchuri dam, next to the BR-720 highway, 40 km from 
the city.? 
is simplified to: 
?Gold piranhas (Serrasalmus Spilopleura), which live in 
the waters of the Sanchuri dam, next to the BR-720 
highway, 40 km from the city, have bitten more than 20 
people.? 
After simplification of the relative clause and 
apposition, the final sentence is: 
?Gold piranhas have bitten more than 20 people. Gold 
piranhas live in the waters of the Sanchuri dam, next to 
the BR-720 highway, 40 km from the city. Gold piranhas 
are Serrasalmus Spilopleura.? 
d) Inversion of clause ordering - This operation 
was primarily designed to handle subordinate 
clauses, by moving the main clause to the beginning 
of the sentence, in order to help the reader 
processing it on their working memory (Graesser et 
al., 2004). Each of the subordination cases has a 
more appropriate order for main and subordinate 
clauses (as shown in Table 1, col. 3), so that 
?independent? information is placed before the 
information that depends on it. In the case of 
concessive subordinate clauses, for example, the 
subordinate clause is placed before the main clause. 
This gives the sentence a logical order of the 
expressed ideas. See the example below, in which 
there is also a change of discourse marker and 
sentence splitting, all operations assigned to 
concessive subordinate clauses:  
?The building hosting the Brazilian Consulate was also 
evacuated, although the diplomats have obtained 
permission to carry on working.? 
Its simplified version becomes:  
?The diplomats have obtained permission to carry on 
working. But the building hosting the Brazilian Consulate 
was also evacuated.? 
e) Subject-Verb-Object ordering - If a sentence 
is not in the form of subject-verb-object, it should be 
rearranged. This operation is based only on 
information from the syntactic parser. The example 
below shows a case in which the subject is after the 
verb (translated literally from Portuguese, 
preserving the order of the elements): 
?On the 9th of November of 1989, fell the wall that for 
almost three decades divided Germany.? 
Its simplified version is: 
?On the 9th of November of 1989, the wall that for almost 
three decades divided Germany fell.? 
Currently the only case we are treating is the non-
canonical order Verb-Object-Subject. We plan to 
treat other non-canonical orderings in the near 
future. Besides that, we still have to define how to 
deal with elliptic subjects and impersonal verbs 
(which in Portuguese do not require a subject). 
38
When performing this operation and the previous 
one, a generator of surface forms (GSF) is used to 
adjust the verb conjugation and regency. The GSF is 
compiled from the Apertium morphological 
dictionaries enhanced with the entries of Unitex-BP 
(Muniz et al, 2005), with an extra processing to 
map the tags of the parser to those existing in 
morphological dictionaries (Caseli et al, 2007) to 
obtain an adjusted verb in the modified sentence. 
f) Topicalization and detopicalization - This 
operation is used to topicalize or detopicalize an 
adverbial phrase. We have not implemented this 
operation yet, but have observed that moving 
adverbial phrases to the end or to the front of 
sentences can make them simpler in some cases. For 
instance, the sentence in the last example would 
become: 
?The wall that for almost three decades divided Germany fell 
on the 9th of November of 1989.? 
We are still investigating how this operation 
could be applied, that is, which situations require 
(de)topicalization. 
3.3 The cascaded application of the rules 
As previously mentioned, one sentence may contain 
several phenomena that could be simplified, and we 
established the order in which they are treated. The 
first phenomenon to be treated is passive voice. 
Secondly, embedded appositive clauses are 
resolved, since they are easy to simplify and less 
prone to errors. Thirdly, subordinate, non-restrictive 
and restrictive relative clauses are treated, and only 
then the coordinate clauses are dealt with.  
As the rules were designed to treat each case 
individually, it is necessary to apply the operations 
in cascade, in order to complete the simplification 
process for each sentence. At each iteration, we (1) 
verify the phenomenon to be simplified following 
the standard order indicated above; (2) when a 
phenomenon is identified, its simplification is 
executed; and (3) the resulting simplified sentence 
goes through a new iteration. This process continues 
until there are no more phenomena. The cascade 
nature of the process is crucial because the 
simplified sentence presents a new syntactic 
structure and needs to be reparsed, so that the 
further simplification operations can be properly 
applied. However, this process consumes time and 
is considered the bottleneck of the system.  
3.4 Simplification evaluation 
We have so far evaluated the capacity of our rule-
based simplifier to identify the phenomena present 
in each sentence, and to recommend the correct 
simplification operation. We compared the 
operations recommended by the system with the 
ones performed manually by an annotator in a 
corpus of 104 news articles from the Zero Hora 
newspaper, which can be seen in our Portal of 
Parallel Corpora of Simplified Texts6. Table 2 
presents the number of occurrences of each 
simplification operation in this corpus. 
Simplification Operations # Sentences 
Non-simplification 2638 
Subject-verb-object ordering 44 
Transformation to active voice 154 
Inversion of clause ordering 265 
Splitting sentences 1103 
Table 2. Statistics on the simplification operations 
The performance of the system for this task is 
presented in Table 3 in terms of precision, recall, 
and F-measure for each simplification operation.  
Operation P R F 
Splitting sentences 64.07 82.63 72.17 
Inversion of clause ordering 15.40 18.91 16.97 
Transformation to active voice 44.29 44.00 44.14 
Subject-verb-object ordering 1.12 4.65 1.81 
ALL 51.64 65.19 57.62 
Non-simplification 64.69 53.58 58.61 
Table 3. Performance on defining simplification 
operations according to syntactic phenomena 
These results are preliminary, since we are still 
refining our rules. Most of the recall errors on the 
inversion of clause ordering are due to the absence 
of a few discourse markers in the list of markers that 
we use to identify such cases. The majority of recall 
errors on sentence splitting are due to mistakes on 
the output of the syntactic parser and to the number 
of ordering cases considered and implemented so 
far. The poor performance for subject-verb-object 
ordering, despite suffering from mistakes of the 
parser, indicates that our rules for this operation 
need to be refined. The same applies to inversion of 
clause ordering. 
We did not report performance scores related to 
the ?changing discourse marker? operation because 
in our evaluation corpus this operation is merged 
with other types of lexical substitution. However, in  
                                                 
6
 http://caravelas.icmc.usp.br/portal/index.php 
39
order to assess if the sentences were correctly 
simplified, it is necessary to do a manual evaluation, 
since it is not possible to automatically compare the 
output of the rule-based simplifier with the 
annotated corpus, as the sentences in the corpus 
have gone through operations that are not performed 
by the simplifier (such as lexical substitution). We 
are in the process of performing such manual 
evaluation. 
4 Simplifica editor: supporting authors 
We developed Simplifica7 (Figure 1), an authoring 
system to help writers to produce simplified texts. It 
employs the simplification technology described in 
the previous section. It is a web-based WYSIWYG 
editor, based on TinyMCE web editor8.  
The user inputs a text in the editor, customizes 
the simplification settings where one or more 
simplifications can be chosen to be applied in the 
text and click on the ?simplify? button. This triggers 
the syntactic simplification system, which returns an 
XML file containing the resulting text and tags 
indicating the performed simplification operations. 
After that, the simplified version of the text is 
shown to the user, and he/she can revise the 
automatic simplification. 
4.1 The XML representation of simplification 
operations 
Our simplification system generates an XML file 
                                                 
7
 http://www.nilc.icmc.usp.br/porsimples/simplifica/ 
8
 http://tinymce.moxiecode.com/ 
describing all simplification operations applied to a 
text. This file can be easily parsed using standard 
XML parsers. Table 5 presents the XML annotation 
to the ?gold piranhas? example in Section 3.2. 
  
<simplification type="passive"> 
<simplification type="appositive"> 
<simplification type="relative"> 
Gold piranhas have bitten more than 20 people. Gold 
piranhas live in the waters of the Sanchuri dam, next to 
the BR-720 highway, 40 km from the city. 
</simplification> 
Gold piranhas are Serrasalmus Spilopleura. 
</simplification> 
</simplification> 
Table 5. XML representation of a simplified text 
In our annotation, each sentence receives a 
<simplification> tag which describes the simplified 
phenomena (if any); sentences that did not need 
simplification are indicated with a <simplification 
type=?no?> tag. The other simplification types refer 
to the eighteen simplification cases presented in 
Table 1. Nested tags indicate multiple operations 
applied to the same sentence. 
4.2 Revising the automatic simplification 
Once the automatic simplification is done, a review 
screen shows the user the simplified text so that 
he/she can visualize all the modifications applied 
and approve or reject them, or select alternative 
simplifications. Figure 1 shows the reviewing screen 
and a message related to the simplification 
performed below the text simplified. 
The user can revise simplified sentences one at a 
time; the selected sentence is automatically 
highlighted. The user can accept or reject a 
 
Figure 1: Interface of the Simplifica system 
40
simplified sentence using the buttons below the text. 
In the beginning of the screen ?Mais op??es?, 
alternative simplifications for the sentence are 
shown: this facility gives the user the possibility to 
resolve cases known to be ambiguous (as detailed in 
Sections 2 and 3.2) for which the automatic 
simplification may have made a mistake. In the 
bottom of the same screen we can see the original 
sentence (?Senten?a original?) to which the 
highlighted sentence refers.  
For the example in Figure 1, the tool presents 
alternative simplifications containing different 
subjects, since selecting the correct noun phrase to 
which an appositive clause was originally linked 
(which becomes the subject of the new sentence) 
based on gender and number information was not 
possible.  
At the end of the process, the user returns to the 
initial screen and can freely continue editing the text 
or adding new information to it. 
5 Text Simplification for education 
Text simplification can be used in several 
applications. Journalists can use it to write simple 
and straightforward news texts. Government 
agencies can create more accessible texts to a large 
number of people. Authors of manuals and technical 
documents can also benefit from the simplification 
technology. Simplification techniques can also be 
used in an educational setting, for example, by a 
teacher who is creating simplified texts to students. 
Classic literature books, for example, can be quite 
hard even to experienced readers. Some genres of 
texts already have simplified versions, even though 
the simplification level can be inadequate to a 
specific target audience. For instance, 3rd and 7th 
grade students have distinct comprehension levels. 
In our approach, the number and type of 
simplification operations applied to sentences 
determine its appropriateness to a given literacy 
level, allowing the creation of multiple versions of 
the same text, with different levels of complexity, 
targeting special student needs. 
The Simplifica editor allows the teacher to adopt 
any particular texts to be used in the class, for 
example, the teacher may wish to talk about current 
news events with his/her students, which would not 
be available via any repository of simplified texts. 
The teacher can customize the text generating 
process and gradually increase the text complexity 
as his/her students comprehension skills evolve. The 
use of the editor also helps the teacher to develop a 
special awareness of the language, which can 
improve his/her interaction with the students.  
Students can also use the system whenever they 
have difficulties to understand a text given in the 
classroom. After a student reads the simplified text, 
the reading of the original text becomes easier, as a 
result of the comprehension of the simplified text. In 
this scenario, reading the original text can also help 
the students to learn new and more complex words 
and syntactic structures, which would be harder for 
them without reading of the simplified text. 
6 Conclusions 
The potentialities of text simplification systems for 
education are evident. For students, it is a first step 
for more effective learning. Under another 
perspective, given the Brazilian population literacy 
levels, we consider text simplification a necessity. 
For poor literacy people, we see text simplification 
as a first step towards social inclusion, facilitating 
and developing reading and writing skills for people 
to interact in society. The social impact of text 
simplification is undeniable. 
In terms of language technology, we not only 
introduced simplification tools in this paper, but also 
investigated which linguistic phenomena should be 
simplified and how to simplify them. We also 
developed a representation schema and designed an 
on-line authoring system. Although some aspects of 
the research are language dependent, most of what 
we propose may be adapted to other languages. 
Next steps in this research include practical 
applications of such technology and the 
measurement of its impact for both education and 
social inclusion. 
Acknowledgments 
We thank the Brazilian Science Foundation FAPESP 
and Microsoft Research for financial support. 
References 
Alu?sio, S.M., Specia, L., Pardo, T.A.S., Maziero, E.G., 
Fortes, R. 2008. Towards Brazilian Portuguese 
Automatic Text Simplification Systems. In the 
Proceedings of the 8th ACM Symposium on Document 
Engineering, pp. 240-248. 
Bick, E. 2000. The parsing system ?Palavras?: 
41
Automatic grammatical analysis of Portuguese in a 
constraint grammar framework. PhD Thesis 
University of ?rhus, Denmark. 
Burstein, J. 2009. Opportunities for Natural Language 
Processing Research in Education. In the  Proceedings 
of CICLing, pp. 6-27.  
Caseli, H., Pereira, T.F., Specia, L., Pardo, T.A.S., 
Gasperin, C., Aluisio, S. 2009. Building a Brazilian 
Portuguese Parallel Corpus of Original and Simplified 
Texts. In the Proceedings of CICLing. 
Caseli, H.M.; Nunes, M.G.V.; Forcada, M.L. 2008. 
Automatic induction of bilingual resources from 
aligned parallel corpora: application to shallow-
transfer machine translation. Machine Translation, V. 
1, p. 227-245. 
Devlin, S., Unthank, G. 2006. Helping aphasic people 
process online information. In the Proceedings of the 
ACM SIGACCESS Conference on Computers and 
Accessibility, pp. 225-226. 
Graesser, A., McNamara, D. S., Louwerse, M., Cai, Z. 
2004. Coh-Metrix: Analysis of text on cohesion and 
language. Behavioral Research Methods, Instruments, 
and Computers, V. 36, pp. 193-202. 
Inui, K., Fujita, A., Takahashi, T., Iida, R., Iwakura, T. 
2003. Text Simplification for Reading Assistance: A 
Project Note. In the Proceedings of the Second 
International Workshop on Paraphrasing, 9 -16.  
Klebanov, B., Knight, K., Marcu, D. 2004. Text 
Simplification for Information-Seeking Applications. 
On the Move to Meaningful Internet Systems. LNCS, 
V.. 3290, pp. 735-747. 
Lal, P., Ruger, S. 2002. Extract-based summarization with 
simplification. In the Proceedings of DUC.  
Mapleson, D.L. 2006. Post-Grammatical Processing for 
Discourse Segmentation. PhD Thesis. School of 
Computing Sciences, University of East Anglia, 
Norwich. 
Max, A. 2005. Simplification interactive pour la 
production de textes adapt es aux personnes souffrant 
de troubles de la compr ehension. In the Proceedings 
of Traitement Automatique des Langues Naturelles 
(TALN).  
Max, A. 2006. Writing for language-impaired readers. In 
the  Proceedings of CICLing, pp. 567-570.  
Muniz, M.C., Laporte, E. Nunes, M.G.V. 2005. UNITEX-
PB, a set of flexible language resources for Brazilian 
Portuguese. In Anais do III Workshop em Tecnologia 
da Informa??o e da Linguagem Humana, V. 1, pp. 1-
10. 
Pardo, T.A.S.  and Nunes, M.G.V. 2008. On the 
Development and Evaluation of a Brazilian 
Portuguese Discourse Parser. Journal of Theoretical 
and Applied Computing, V. 15, N. 2, pp. 43-64. 
Petersen, S.E. 2007. Natural Language Processing Tools 
for Reading Level Assessment and Text Simplification 
for Bilingual Education. PhD Thesis, University of 
Washington.  
Petersen, S.E. and Ostendorf, M. 2007. Text 
Simplification for Language Learners: A Corpus 
Analysis. In the Proceedings of the Speech and 
Language Technology for Education Workshop, pp. 
69-72. 
Specia, L., Alu?sio, S.M., Pardo, T.A.S. 2008. Manual de 
simplifica??o sint?tica para o portugu?s. Technical 
Report NILC-TR-08-06, NILC. 
Siddharthan, A. 2002. An Architecture for a Text 
Simplification System. In the Proceedings of the 
Language Engineering Conference, pp. 64-71. 
Siddharthan, A. 2003. Syntactic Simplification and Text 
Cohesion. PhD Thesis. University of Cambridge. 
Snow, C. 2002. Reading for understanding: Toward an 
R&D program in reading comprehension. Santa 
Monica, CA. 
 
42
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 41?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
SIMPLIFICA: a tool for authoring simplified texts in  
Brazilian Portuguese guided by readability assessments 
 
 
Carolina  Scarton, Matheus de Oliveira,  Arnaldo Candido Jr.,  
Caroline Gasperin and Sandra Maria Alu?sio  
Department of Computer Sciences, University of S?o Paulo 
Av. Trabalhador S?o-Carlense, 400. 13560-970 - S?o Carlos/SP, Brazil 
{carolina@grad,matheusol@grad,arnaldoc@,cgasperin@,sandra@}icmc.usp.br 
 
  
 
 
Abstract 
SIMPLIFICA is an authoring tool for produc-
ing simplified texts in Portuguese. It provides 
functionalities for lexical and syntactic simpli-
fication and for readability assessment. This 
tool is the first of its kind for Portuguese; it 
brings innovative aspects for simplification 
tools in general, since the authoring process is 
guided by readability assessment based on the 
levels of literacy of the Brazilian population. 
1 Introduction 
In order to promote digital inclusion and accessi-
bility for people with low levels of literacy, partic-
ularly access to documents available on the web, it 
is important to provide textual information in a 
simple and easy way. Indeed, the Web Content 
Accessibility Guidelines (WCAG) 2.01 establishes 
a set of guidelines that discuss accessibility issues 
and provide accessibility design solutions. WCAG 
requirements address not only structure and tech-
nological aspects, but also how the content should 
be made available to users. However, Web devel-
opers are not always responsible for content prepa-
ration and authoring in a Website. Moreover, in the 
context of Web 2.0 it becomes extremely difficult 
to develop completely WCAG conformant Web-
sites, since users without any prior knowledge 
about the guidelines directly participate on the con-
tent authoring process of Web applications.  
                                                        
1 http://www.w3.org/TR/WCAG20/ 
In Brazil, since 2001, the INAF index (National 
Indicator of Functional Literacy) has been com-
puted annually to measure the levels of literacy of 
the Brazilian population. The 2009 report pre-
sented a still worrying scenario: 7% of the individ-
uals were classified as illiterate; 21% as literate at 
the rudimentary level; 47% as literate at the basic 
level; and only 25% as literate at the advanced lev-
el (INAF, 2009). These literacy levels are defined 
as: (1) Illiterate: individuals who cannot perform 
simple tasks such as reading words and phrases; 
(2) Rudimentary: individuals who can find expli-
cit information in short and familiar texts (such as 
an advertisement or a short letter); (3) Basic: indi-
viduals who can read and understand texts of aver-
age length, and find information even when it is 
necessary to make some inference; and (4) Ad-
vanced/Fully: individuals who can read longer 
texts, relating their parts, comparing and interpret-
ing information, distinguish fact from opinion, 
make inferences and synthesize. 
We present in this paper the current version of 
an authoring tool named SIMPLIFICA. It helps 
authors to create simple texts targeted at poor lite-
rate readers. It extends the previous version pre-
sented in Candido et al (2009) with two new mod-
ules: lexical simplification and the assessment of 
the level of complexity of the input texts. The 
study is part of the PorSimples project2 (Simplifi-
cation of Portuguese Text for Digital Inclusion and 
Accessibility) (Aluisio et al, 2008).  
This paper is organized as follows. In Section 2 
                                                        
2 http://caravelas.icmc.usp.br/wiki/index.php/Principal 
41
we describe SIMPLIFICA and the underlying 
technology for lexical and syntactic simplification, 
and for readability assessment. In Section 3 we 
summarize the interaction steps that we propose to 
show in the demonstration session targeting texts 
for low-literate readers of Portuguese. Section 4 
presents final remarks with emphasis on why de-
monstrating this system is relevant.  
2 SIMPLIFICA authoring tool  
SIMLIFICA is a web-based WYSIWYG editor, 
based on TinyMCE web editor3. The user inputs a 
text in the editor and customizes the simplification 
settings, where he/she can choose: (i) strong sim-
plification, where all the complex syntactic phe-
nomena (see details in Section 2.2) are treated for 
each sentence, or customized simplification, where 
the user chooses one or more syntactic simplifica-
tion phenomena to be treated for each sentence, 
and (ii) one or more thesauri to be used in the syn-
tactic and lexical simplification processes. Then 
the user activates the readability assessment mod-
ule to predict the complexity level of a text. This 
module maps the text to one of the three levels of 
literacy defined by INAF: rudimentary, basic or 
advanced. According to the resulting readability 
level the user can trigger the lexical and/or syntac-
tic simplifications modules, revise the automatic 
simplification and restart the cycle by checking the 
readability level of the current version of the text.  
Figure 1 summarizes how the three modules are 
integrated and below we describe in more detail 
the SIMPLIFICA modules. 
 
Figure 1. Steps of the authoring process. 
 
                                                        
3 http://tinymce.moxiecode.com/ 
2.1 Lexical Simplification  
Basically, the first part of the lexical simplification 
process consists of tokenizing the original text and 
marking the words that are considered complex. In 
order to judge a word as complex or not, we use 3 
dictionaries created for the PorSimples project: one 
containing words common to youngsters, a second 
one composed by frequent words extracted from 
news texts for children and nationwide newspa-
pers, and a third one containing concrete words.  
The lexical simplification module also uses the 
Unitex-PB dictionary4 for finding the lemma of the 
words in the text, so that it is possible to look for it 
in the simple words dictionaries. The problem of 
looking for a lemma directly in a dictionary is that 
there are ambiguous words and we are not able to 
deal with different word senses. For dealing with 
part-of-speech (POS) ambiguity, we use the 
MXPOST POS tagger5 trained over NILC tagset6. 
After the text is tagged, the words that are not 
proper nouns, prepositions and numerals are se-
lected, and their POS tags are used to look for their 
lemmas in the dictionaries. As the tagger has not a 
100% precision and some words may not be in the 
dictionary, we look for the lemma only (without 
the tag) when we are not able to find the lemma-
tag combination in the dictionary. Still, if we are 
not able to find the word, the lexical simplification 
module assumes that the word is complex and 
marks it for simplification. 
The last step of the process consists in providing 
simpler synonyms for the marked words. For this 
task, we use the thesauri for Portuguese TeP 2.07 
and the lexical ontology for Portuguese PAPEL8. 
This task is carried out when the user clicks on a 
marked word, which triggers a search in the the-
sauri for synonyms that are also present in the 
common words dictionary. If simpler words are 
found, they are listed in order, from the simpler to 
the more complex ones. To determine this order, 
we used Google API to search each word in the 
web: we assume that the higher a word frequency, 
the simpler it is. Automatic word sense disambigu-
ation is left for future work. 
                                                        
4 http://www.nilc.icmc.usp.br/nilc/projects/unitex-pb/web 
/dicionarios.html 
5 http://sites.google.com/site/adwaitratnaparkhi/home 
6 www.nilc.icmc.usp.br/nilc/TagSet/ManualEtiquetagem.htm  
7 http://www.nilc.icmc.usp.br/tep2/ 
8 http://www.linguateca.pt/PAPEL/ 
42
2.2 Syntactic Simplification 
Syntactic simplification is accomplished by a rule-
based system, which comprises seven operations 
that are applied sentence-by-sentence to a text in 
order to make its syntactic structure simpler.  
Our rule-based text simplification system is 
based on a manual for Brazilian Portuguese syntac-
tic simplification (Specia et al, 2008). According 
to this manual, simplification operations should be 
applied when any of the 22 linguistic phenomena 
covered by our system (see Candido et al (2009) 
for details) is detected. Our system treats apposi-
tive, relative, coordinate and subordinate clauses, 
which had already been addressed by previous 
work on text simplification (Siddharthan, 2003). 
Additionally, we treat passive voice, sentences in 
an order other than Subject-Verb-Object (SVO), 
and long adverbial phrases. The simplification op-
erations available to treat these phenomena are: 
split sentence, change particular discourse markers 
by simpler ones, change passive to active voice, 
invert the order of clauses, convert to subject-verb-
object ordering, and move long adverbial phrases. 
Each sentence is parsed in order to identify syn-
tactic phenomena for simplification and to segment 
the sentence into portions that will be handled by 
the operations. We use the parser PALAVRAS 
(Bick, 2000) for Portuguese. Gasperin et al (2010) 
present the evaluation of the performance of our 
syntactic simplification system.  
Since our syntactic simplifications are conserva-
tive, the simplified texts become longer than the 
original ones due to sentence splitting. We ac-
knowledge that low-literacy readers prefer short 
texts, and in the future we aim to provide summa-
rization within SIMPLIFICA (see (Watanabe et al, 
2009)). Here, the shortening of the text is a respon-
sibility of the author. 
2.3 Readability assessment 
With our readability assessment module, we can 
predict the readability level of a text, which cor-
responds to the literacy level expected from the 
target reader: rudimentary, basic or advanced.  
We have adopted a machine-learning classifier 
to identify the level of the input text; we use the 
Support Vector Machines implementation from 
Weka9 toolkit (SMO). We have used 7 corpora 
                                                        
9 http://www.cs.waikato.ac.nz/ml/weka/ 
within 2 different genres (general news and popu-
lar science articles) to train the classifier. Three of 
these corpora contain original texts published in 
online newspapers and magazines. The other cor-
pora contain manually simplified versions of most 
of the original texts. These were simplified by a 
linguist, specialized in text simplification, accord-
ing to the two levels of simplification proposed in 
our project, natural and strong, which result in 
texts adequate for the basic and rudimentary litera-
cy levels, respectively. 
Our feature set is composed by cognitively-
motivated features derived from the Coh-Metrix-
PORT tool10, which is an adaptation for Brazilian 
Portuguese of Coh-Metrix 2.0 (free version of 
Coh-Metrix (Graesser et al 2003)) also developed 
in the context of the PorSimples project. Coh-
Metrix-PORT implements the metrics in Table 1. 
 
Categories Subcategories Metrics 
Shallow 
Readabili-
ty metric 
- Flesch Reading Ease index 
for Portuguese. 
Words and 
textual 
informa-
tion 
Basic counts Number of words, sen-
tences, paragraphs, words 
per sentence, sentences per 
paragraph, syllables per 
word, incidence of verbs, 
nouns, adjectives and ad-
verbs. 
Frequencies Raw frequencies of content 
words and minimum fre-
quency of content words. 
Hyperonymy Average number of hyper-
nyms of verbs. 
Syntactic 
informa-
tion 
Constituents Incidence of nominal 
phrases, modifiers per noun 
phrase and words preced-
ing main verbs. 
Pronouns, 
Types and 
Tokens 
Incidence of personal pro-
nouns, number of pronouns 
per noun phrase, types and 
tokens. 
Connectives Number of connectives, 
number of positive and 
negative additive connec-
tives, causal / temporal / 
logical positive and nega-
tive connectives. 
Logical 
operators 
- Incidence of the particles 
?e? (and), ?ou? (or), ?se? 
(if), incidence of negation 
and logical operators. 
Table 1. Metrics of Coh-Metrix-PORT. 
 
                                                        
10 http://caravelas.icmc.usp.br:3000/ 
43
We also included seven new metrics to Coh-
Metrix-PORT: average verb, noun, adjective and 
adverb ambiguity, incidence of high-level constitu-
ents, content words and functional words.  
We measured the performance of the classifier 
on identifying the levels of the input texts by a 
cross-validation experiment. We trained the clas-
sifier on our 7 corpora and reached 90% F-measure 
on identifying texts at advanced level, 48% at basic 
level, and 73% at rudimentary level. 
 
3. A working session at SIMPLIFICA  
 
In the NAACL demonstration section we aim to 
present all functionalities of the tool for authoring 
simple texts, SIMPLIFICA. We will run all steps 
of the authoring process ? readability assessment, 
lexical simplification and syntactic simplification ? 
in order to demonstrate the use of the tool in pro-
ducing a text for basic and rudimentary readers of 
Portuguese, regarding the lexical and the syntactic 
complexity of an original text.  
We outline a script of our demonstration at 
http://www.nilc.icmc.usp.br/porsimples/demo/dem
o_script.htm. In order to help the understanding by 
non-speakers of Portuguese we provide the transla-
tions of the example texts shown. 
 
4. Final Remarks 
 
A tool for authoring simple texts in Portuguese is 
an innovative software, as are all the modules that 
form the tool. Such tool is extremely important in 
the construction of texts understandable by the ma-
jority of the Brazilian population. SIMPLIFICA?s 
target audience is varied and includes: teachers that 
use online text for reading practices; publishers; 
journalists aiming to reach poor literate readers; 
content providers for distance learning programs; 
government agencies that aim to communicate to 
the population as a whole; companies that produce 
technical manuals and medicine instructions; users 
of legal language, in order to facilitate the under-
standing of legal documents by lay people; and 
experts in language studies and computational lin-
guistics for future research. 
Future versions of SIMPLIFICA will also pro-
vide natural simplification, where the target sen-
tences for simplifications are chosen by a machine 
learning classifier (Gasperin et al, 2009). 
 
Acknowledgments 
We thank FAPESP and Microsoft Research for 
supporting the PorSimples project 
References  
Sandra Alu?sio, Lucia Specia, Thiago Pardo, Erick Ma-
ziero and Renata Fortes. 2008. Towards Brazilian 
Portuguese Automatic Text Simplification Systems. In 
Proceedings of The Eight ACM Symposium on Doc-
ument Engineering (DocEng 2008),  240-248, S?o 
Paulo, Brasil. 
Eckhard Bick. 2000. The Parsing System "Palavras": 
Automatic Grammatical Analysis of Portuguese in a 
Constraint Grammar Framework. PhD thesis. Aa-
rhus University. 
Arnaldo Candido Junior, Erick Maziero, Caroline Gas-
perin, Thiago Pardo, Lucia Specia and Sandra M. A-
luisio. 2009. Supporting the Adaptation of Texts for 
Poor Literacy Readers: a Text Simplification Editor 
for Brazilian Portuguese. In the Proceedings of the 
NAACL HLT Workshop on Innovative Use of NLP 
for Building Educational Applications, pages 34?42, 
Boulder, Colorado, June 2009. 
Caroline Gasperin; Lucia Specia; Tiago Pereira and  
Sandra Alu?sio. 2009. Learning When to Simplify 
Sentences for Natural Text Simplification. In: Pro-
ceedings of ENIA 2009,  809-818. 
Caroline Gasperin, Erick Masiero and Sandra M. Alui-
sio. 2010. Challenging choices for text simplifica-
tion. Accepted for publication in Propor 2010 
(http://www.inf.pucrs.br/~propor2010/). 
Arthur Graesser, Danielle McNamara, Max  Louwerse 
and Zhiqiang Cai. 2004. Coh-Metrix: Analysis of 
text on cohesion and language. In: Behavioral Re-
search Methods, Instruments, and Computers, 36, 
p?ginas 193-202. 
INAF. 2009. Instituto P. Montenegro and A??o Educa-
tiva. INAF Brasil - Indicador de Alfabetismo Funcio-
nal - 2009. Online available at http://www.ibope. 
com.br/ipm/relatorios/relatorio_inaf_2009.pdf  
Advaith Siddharthan. 2003. Syntactic Simplification and 
Text Cohesion. PhD Thesis. University of 
Cambridge. 
Lucia Specia, Sandra Aluisio and Tiago Pardo. 2008. 
Manual de Simplifica??o Sint?tica para o Portugu?s. 
Technical Report NILC-TR-08-06, 27 p. Junho 2008, 
S?o Carlos-SP. 
Willian Watanabe, Arnaldo Candido Junior, Vin?cius 
Uz?da, Renata Fortes, Tiago Pardo and Sandra Alu?-
sio. 2009. Facilita: reading assistance for low-
literacy readers. In Proceedings of the 27th ACM In-
ternational Conference on Design of Communication. 
SIGDOC '09. ACM, New York, NY, 29-36.  
44
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Readability Assessment for Text Simplification  
 
Sandra Aluisio1, Lucia Specia2, Caroline Gasperin1 and Carolina Scarton1 
1Center of Computational Linguistics (NILC) 2Research Group in Computational Linguistics 
University of S?o Paulo University of Wolverhampton 
S?o Carlos - SP, Brazil Wolverhampton, UK 
{sandra,cgasperin}@icmc.usp.br, 
carol.scarton@gmail.com 
L.Specia@wlv.ac.uk 
 
 
 
 
Abstract 
We describe a readability assessment ap-
proach to support the process of text simplifi-
cation for poor literacy readers. Given an in-
put text, the goal is to predict its readability 
level, which corresponds to the literacy level 
that is expected from the target reader: rudi-
mentary, basic or advanced. We complement 
features traditionally used for readability as-
sessment with a number of new features, and 
experiment with alternative ways to model 
this problem using machine learning methods, 
namely classification, regression and ranking. 
The best resulting model is embedded in an 
authoring tool for Text Simplification. 
1 Introduction 
In Brazil, the National Indicator of Functional Lite-
racy (INAF) index has been computed annually 
since 2001 to measure the levels of literacy of the 
Brazilian population. The 2009 report presented a 
worrying scenario: 7% of the individuals are illite-
rate; 21% are literate at the rudimentary level; 47% 
are literate at the basic level; only 25% are literate 
at the advanced level (INAF, 2009). These literacy 
levels are defined as:  
(1) Illiterate: individuals who cannot perform 
simple tasks such as reading words and phrases;  
(2) Rudimentary: individuals who can find ex-
plicit information in short and familiar texts (such 
as an advertisement or a short letter);  
(3) Basic: individuals who are functionally lite-
rate, i.e., they can read and understand texts of av-
erage length, and find information even when it is 
necessary to make some inference; and  
(4) Advanced: fully literate individuals, who can 
read longer texts, relating their parts, comparing 
and interpreting information, distinguish fact from 
opinion, make inferences and synthesize.   
In order to promote digital inclusion and acces-
sibility for people with low levels of literacy, par-
ticularly to documents available on the web, it is 
important to provide text in a simple and easy-to- 
read way. This is a requirement of the Web Con-
tent Accessibility Guidelines 2.0?s principle of 
comprehensibility and accessibility of Web con-
tent1. It states that for texts which demand reading 
skills more advanced than that of individuals with 
lower secondary education, one should offer an al-
ternative version of the same content suitable for 
those individuals. While readability formulas for 
English have a long history ? 200 formulas have 
been reported from 1920 to 1980s (Dubay, 2004) ? 
the only tool available for Portuguese is an adapta-
tion of the Flesch Reading Ease index.  It evaluates 
the complexity of texts in a 4-level scale corres-
ponding to grade levels (Martins et al, 1996).  
In the PorSimples project (Alu?sio et al, 2008) 
we develop text adaptation methods (via text sim-
plification and elaboration approaches) to improve 
the comprehensibility of texts published on gov-
ernment websites or by renowned news agencies, 
which are expected to be relevant to a large au-
dience with various literacy levels. The project 
provides automatic simplification tools to aid (1) 
poorly literate readers to understand online content 
? a browser plug-in for automatically simplifying 
websites ? and (2) authors producing texts for this 
audience ? an authoring tool for guiding the crea-
tion of simplified versions of texts.  
This paper focuses on a readability assessment 
approach to assist the simplification process in the 
authoring tool, SIMPLIFICA. The current version 
of SIMPLIFICA offers simplification operations 
addressing a number of lexical and syntactic phe-
nomena to make the text more readable. The au-
                                                          
1 http://www.w3.org/TR/WCAG20/ 
1
thor has the freedom to choose when and whether 
to apply the available simplification operations, a 
decision based on the level of complexity of the 
current text and on the target reader.  
A method for automatically identifying such 
level of complexity is therefore of great value. 
With our readability assessment tool, the author is 
able to automatically check the complexi-
ty/readability level of the original text, as well as 
modified versions of such text produced as he/she 
applies simplification operations offered by 
SIMPLIFICA, until the text reaches the expected 
level, adequate for the target reader. 
In this paper we present such readability as-
sessment tool, developed as part of the PorSimples 
project, and discuss its application within the au-
thoring tool. Different from previous work, the tool 
does not model text difficulty according to linear 
grade levels (e.g., Heilman et al, 2008), but in-
stead maps the text into the three levels of literacy 
defined by INAF: rudimentary, basic or advanced. 
Moreover, it uses a more comprehensive set of fea-
tures, different learning techniques and targets a 
new language and application, as we discuss in 
Section 4. More specifically, we address the fol-
lowing research questions: 
 
1. Given some training material, is it possible to 
detect the complexity level of Portuguese texts, 
which corresponds to the different literacy levels 
defined by INAF? 
2. What is the best way to model this problem 
and which features are relevant? 
 
We experiment with nominal, ordinal and interval-
based modeling techniques and exploit a number 
of the cognitively motivated features proposed by 
Coh-Metrix 2.0 (Graesser et al, 2004) and adapted 
to Portuguese (called Coh-Metrix-PORT), along 
with a set of new features, including syntactic fea-
tures to capture simplification operations and n-
gram language model features.  
In the remainder of this paper, we first provide 
some background information on the need for a 
readability assessment tool within our text simpli-
fication system (Section 2) and discuss prior work 
on readability assessment (Section 3), to then 
present our features and modeling techniques (Sec-
tion 4) and the experiments performed to answer 
our research questions (Section 5). 
2. Text Simplification in PorSimples 
Text Simplification (TS) aims to maximize reading 
comprehension of written texts through their sim-
plification. Simplification usually involves substi-
tuting complex by simpler words and breaking 
down and changing the syntax of complex, long 
sentences (Max, 2006; Siddharthan, 2003).   
To meet the needs of people with different le-
vels of literacy, in the PorSimples project we pro-
pose two types of simplification: natural and 
strong. The first type results in texts adequate for 
people with a basic literacy level and the second, 
rudimentary level. The difference between these 
two is the degree of application of simplification 
operations to complex sentences. In strong simpli-
fication, operations are applied to all complex syn-
tactic phenomena present in the text in order to 
make it as simple as possible, while in natural sim-
plification these operations are applied selectively, 
only when the resulting text remains ?natural?. 
One example of original text (a), along with its 
natural (b) and strong (c) manual simplifications, is 
given in Table 1. 
 
(a) The cinema theaters around the world were show-
ing a production by director Joe Dante in which a 
shoal of piranhas escaped from a military laborato-
ry and attacked participants of an aquatic show. 
(...) More than 20 people were bitten by palometas 
(Serrasalmus spilopleura, a species of piranhas) 
that live in the waters of the Sanchuri dam. 
(b) The cinema theaters around the world were show-
ing a production by director Joe Dante. In the pro-
duction a shoal of piranhas escaped from a military 
laboratory and attacked participants of an aquatic 
show. (?) More than 20 people were bitten by pa-
lometas that live in the waters of the Sanchuri dam. 
Palometas are Serrasalmus spilopleura, a species 
of piranhas. 
(c) The cinema theaters around the world were show-
ing a movie by director Joe Dante. In the movie a 
shoal of piranhas escaped from a military laborato-
ry. The shoal of piranhas attacked participants of 
an aquatic show. (...). Palometas have bitten more 
than 20 people. Palometas live in the waters of the 
Sanchuri dam. Palometas are Serrasalmus spilop-
leura, a species of piranhas. 
Table 1: Example of original and simplified texts 
 
The association between these two types of simpli-
fication and the literacy levels was identified by 
means of a corpus study. We have manually built a 
corpus of simplified texts at both natural and 
2
strong levels and analyzed their linguistic struc-
tures according to the description of the two litera-
cy levels. We verified that strong simplified sen-
tences are more adequate for rudimentary level 
readers, and natural ones for basic level readers. 
This claim is supported by several studies which 
relate capabilities and performance of the working 
memory with reading levels (Siddharthan, 2003; 
McNamara et al, 2002). 
2.1 The Rule-based Simplification System 
The association between simplification operations 
and the syntactic phenomena they address is im-
plemented within a rule-based syntactic simplifica-
tion system (Candido Jr. et al, 2009). This system 
is able to identify complex syntactic phenomena in 
a sentence and perform the appropriate operations 
to simplify each phenomenon.  
The simplification rules follow a manual for 
syntactic simplification in Portuguese also devel-
oped in PorSimples. They cover syntactic con-
structions such as apposition, relative clauses, 
coordination and subordination, which had already 
been addressed by previous work on text simplifi-
cation (Siddharthan, 2003). Additionally, they ad-
dress the transformation of sentences from passive 
into active voice, normalization of sentences into 
the Subject-Verb-Object order, and simplification 
of adverbial phrases. The simplification operations 
available are: sentence splitting, changing particu-
lar discourse markers by simpler ones, transform-
ing passive into active voice, inverting the order of 
clauses, converting to subject-verb-object order, 
relocating long adverbial phrases.  
2.2 The SIMPLIFICA Tool 
The rule-based simplification system is part of 
SIMPLIFICA, an authoring tool for writers to 
adapt original texts into simplified texts. Within 
SIMPLIFICA, the author plays an active role in 
generating natural or strong simplified texts by ac-
cepting or rejecting the simplifications offered by 
the system on a sentence basis and post-editing 
them if necessary. 
 Despite the ability to make such choices at the 
sentence level, it is not straightforward for the au-
thor to judge the complexity level of the text as 
whole in order to decide whether it is ready for a 
certain audience. This is the main motivation for 
the development of a readability assessment tool.  
The readability assessment tool automatically 
detects the level of complexity of a text at any 
moment of the authoring process, and therefore 
guides the author towards producing the adequate 
simplification level according to the type of reader. 
It classifies a text in one of three levels: rudimenta-
ry, basic or advanced.  
Figure 1 shows the interface of SIMPLIFICA, 
where the complexity level of the current text as 
given by the readability assessment tool is shown 
at the bottom, in red (in this case, ?N?vel Pleno?, 
which corresponds to advanced). To update the 
readability assessment of a text the author can 
choose ?N?vel de Inteligibilidade? (readability lev-
el) at any moment.  
The text shown in Figure 1 is composed of 13 
sentences, 218 words. The lexical simplification 
module (not shown in the Figure 1) finds 10 candi-
date words for simplification in this text, and the 
syntactic simplification module selects 10 sen-
tences to be simplified (highlighted in gray).  
When the author selects a highlighted sentence, 
he/she is presented with all possible simplifications 
proposed by the rule-based system for this sen-
tence. Figure 2 shows the options for the first sen-
tence in Figure 1. The first two options cover non-
finite clause and adverbial adjuncts, respectively, 
while the third option covers both phenomena in 
one single step. The original sentence is also given 
as an option.  
It is possible that certain suggestions of auto-
matic simplifications result in ungrammatical or 
inadequate sentences (mainly due to parsing er-
rors). The author can choose not to use such sug-
gestions as well as manually edit the original or 
automatically simplified versions. The impact of 
the author?s choice on the overall readability level 
of the text is not always clear to the author. The 
goal of the readability assessment function is to 
provide such information. 
Simplified texts are usually longer than the 
original ones, due to sentence  splittings and 
repetition of information to connect such 
sentences.  We  acknowledge  that  low literacy 
readers prefer short texts, but in this tool the 
shortening of the text is a responsibility of the 
author. Our focus is on the linguistic structure of 
the texts; the length of the text actually is a feature 
considered by our readability assessment system. 
3
Figure 1: SIMPLIFICA interface 
Figure 2. Simplification options available for the first sentence of the text presented in Figure 1
3. Readability Assessment 
Recent work on readability assessment for the 
English language focus on: (i) the feature set used 
to capture the various aspects of readability, to 
evaluate the contribution of lexical, syntactic, se-
mantic and discursive features; (ii) the audience of 
the texts the readability measurement is intended 
to; (iii) the genre effects on the calculation of text 
difficult; (iv) the type of learning technique 
which is more appropriate: those producing nomi-
nal, ordinal or interval scales of measurement, and 
(v) providing an application for the automatic as-
sessment of reading difficulty.  
Pitler and Nenkova (2008) propose a unified 
framework composed of vocabulary, syntactic, 
elements of lexical cohesion, entity coherence and 
discourse relations to measure text quality, which 
resembles the composition of rubrics in the area of 
essay scoring (Burstein et al, 2003).  
 The following studies address readability as-
sessment for specific audiences: learners of Eng-
lish as second language (Schwarm and Ostendorf, 
2005; Heilman et al, 2007), people with intellec-
tual disabilities (Feng et al, 2009), and people with 
cognitive impairment caused by Alzheimer (Roark 
at al, 2007). 
Sheehan et al (2007) focus on models for 
literary and expository texts, given that traditional 
metrics like Flesch-Kincaid Level score tend to 
overpredict the difficulty of literary texts and 
underpredict the difficulty of expository texts.  
Heilman et al (2008) investigate an appropriate 
scale of measurement for reading difficulty ? 
nominal, ordinal, or interval ? by comparing the 
effectiveness of statistical models for each type of 
data. Petersen and Ostendorf (2009) use 
classification and regression techniques to predict a 
readability score. 
Miltsakali and Troutt (2007; 2008) propose an 
automatic tool to evaluate reading difficulty of 
Web texts in real time, addressing teenagers and 
adults with low literacy levels. Using machine 
learning, Gl?ckner et al (2006) present a tool for 
automatically rating the readability of German 
texts using several linguistic information sources 
and a global readability score similar to the Flesch 
Reading Ease.   
4
4. A Tool for Readability Assessment 
In this section we present our approach to readabil-
ity assessment.  It differs from previous work in 
the following aspects: (i) it uses a feature set with 
cognitively-motivated metrics and a number of ad-
ditional features to provide a better explanation of 
the complexity of a text; (ii) it targets a new audi-
ence: people with different literacy levels; (iii) it 
investigates different statistical models for non- 
linear data scales: the levels of literacy defined by 
INAF, (iv) it focus on a new application: the use of 
readability assessment for text simplification sys-
tems; and (v) it is aimed at Portuguese. 
4.1 Features for Assessing Readability 
Our feature set (Table 2) consists of 3 groups of 
features. The first group contains cognitively-
motivated features (features 1-42), derived from 
the Coh-Metrix-PORT tool (see Section 4.1.1). 
The second group contains features that reflect the 
incidence of particular syntactic constructions 
which we target in our text simplification system 
(features 43-49). The third group (the remaining 
features in Table 2) contains features derived from 
n-gram language models built considering uni-
grams, bigrams and trigrams probability and per-
plexity plus out-of-vocabulary rate scores. We later 
refer to a set of basic features, which consist of 
simple counts that do not require any linguistic tool 
or external resources to be computed. This set cor-
responds to features 1-3 and 9-11. 
4.1.1 Coh-Metrix-Port 
The Coh-Metrix tool was developed to compute 
features potentially relevant to the comprehension 
of English texts through a number of measures in-
formed by linguistics, psychology and cognitive 
studies. The main aspects covered by the measures 
are cohesion and coherence (Graesser et al, 2004). 
Coh-Metrix 2.0, the free version of the tool, con-
tains 60 readability metrics. The Coh-Metrix-
PORT tool (Scarton et al, 2009) computes similar 
metrics for texts in Brazilian Portuguese. The ma-
jor challenge to create such tool is the lack of some 
of the necessary linguistic resources. The follow-
ing metrics are currently available in the tool (we 
refer to Table 2 for details): 
1. Readability metric: feature 12. 
 
2. Words and textual information:  
 Basic counts: features 1 to 11. 
1 Number of words 
2 Number of sentences 
3 Number of paragraphs 
4 Number of verbs 
5 Number of nouns 
6 Number of adjectives 
7 Number of adverbs 
8 Number of pronouns 
9 Average number of words per sentence 
10 Average number of sentences per paragraph 
11 Average number of syllables per word 
12 Flesch index for Portuguese 
13 Incidence of content words 
14 Incidence of functional words  
15 Raw Frequency of content words  
16 Minimal frequency of content words  
17 Average number of verb hypernyms 
18 Incidence of NPs 
19 Number of NP modifiers 
20 Number of words before the main verb 
21 Number of high level constituents 
22 Number of personal pronouns 
23 Type-token ratio 
24 Pronoun-NP ratio 
25 Number of ?e? (and) 
26 Number of ?ou? (or)  
27 Number of ?se? (if) 
28 Number of negations 
29 Number of logic operators 
30 Number of connectives  
31 Number of positive additive connectives 
32 Number of negative additive connectives 
33 Number of positive temporal connectives 
34 Number of negative temporal connectives 
35 Number of positive causal connectives 
36 Number of negative causal connectives 
37 Number of positive logic connectives 
38 Number of negative logic connectives 
39 Verb ambiguity ratio 
40 Noun ambiguity ratio 
41 Adverb ambiguity ratio 
42 Adjective ambiguity ratio 
43 Incidence of clauses 
44 Incidence of adverbial phrases 
45 Incidence of apposition 
46 Incidence of passive voice 
47 Incidence of relative clauses 
48 Incidence of coordination 
49 Incidence of subordination 
50 Out-of-vocabulary words  
51 LM probability of unigrams  
52 LM perplexity of unigrams  
53 LM perplexity of unigrams, without line break  
54 LM probability of bigrams  
55 LM perplexity of bigrams  
56 LM perplexity of bigrams, without line break  
57 LM probability of trigrams  
58 LM perplexity of trigrams  
59 LM perplexity of trigrams, without line break  
Table 2. Feature set 
5
 Frequencies: features 15 to 16. 
 Hypernymy: feature 17. 
 
3. Syntactic information:  
 Constituents: features 18 to 20. 
 Pronouns: feature 22 
 Types and Tokens: features 23 to 24. 
 Connectives: features 30 to 38. 
 
4. Logical operators: features 25 to 29. 
 
The following resources for Portuguese were used: 
the MXPOST POS tagger (Ratnaparkhi, 1996), a 
word frequency list compiled from a 700 million-
token corpus2, a tool to identify reduced noun 
phrases (Oliveira et al, 2006), a list of connectives 
classified as positives/negatives and according to 
cohesion type (causal, temporal, additive or logi-
cal), a list of logical operators and WordNet.Br 
(Dias-da-Silva et al, 2008).  
In this paper we include seven new metrics to 
Coh-Metrix-PORT: features 13, 14, 21, and 39 to 
42. We used TEP3 (Dias-da-Silva et al, 2003) to 
obtain the number of senses of words (and thus 
their ambiguity level), and the Palavras parser 
(Bick, 2000) to identify the higher level constitu-
ents. The remaining metrics were computed based 
on the POS tags. 
According to a report on the performance of 
each Coh-Metrix-PORT metric (Scarton et al, 
2009), no individual feature provides sufficient in-
dication to measure text complexity, and therefore 
the need to exploit their combination, and also to 
combine them with the other types of features de-
scribed in this section. 
4.1.2 Language-model Features 
Language model features were derived from a 
large corpus composed of a sample of the Brazilian 
newspaper Folha de S?o Paulo containing issues 
from 12 months taken at random from 1994 to 
2005. The corpus contains 96,868 texts and 
26,425,483 tokens. SRILM (Stolcke, 2002), a 
standard language modelling toolkit, was used to 
produce the language model features.  
4.2 Learning Techniques 
Given that the boundaries of literacy level classes 
are one of the subjects of our study, we exploit 
three different types of models in order to check 
                                                          
2 http://www2.lael.pucsp.br/corpora/bp/index.htm 
3 http://www.nilc.icmc.usp.br/tep2/index.htm 
which of them can better distinguish among the 
three literacy levels. We therefore experiment with 
three types of machine learning algorithms: a stan-
dard classifier, an ordinal (ranking) classifier and a 
regressor. Each algorithm assumes different rela-
tions among the groups: the classifier assumes no 
relation, the ordinal classifier assumes that the 
groups are ordered, and the regressor assumes that 
the groups are continuous.  
As classifier we use the Support Vector Ma-
chines (SVM) implementation in the Weka4 toolkit 
(SMO). As ordinal classifier we use a meta clas-
sifier in Weka which takes SMO as the base classi-
fication algorithm and performs pairwise classifi-
cations (OrdinalClassClassifier). For regression we 
use the SVM regression implementation in Weka 
(SMO-reg). We use the linear versions of the algo-
rithms for classification, ordinal classification and 
regression, and also experiment with a radial basis 
function (RBF) kernel for regression. 
5. Experiments 
5.1 Corpora 
In order to train (and test) the different machine 
learning algorithms to automatically identify the 
readability level of the texts we make use of ma-
nually simplified corpora created in the PorSimples 
project. Seven corpora covering our three literacy 
levels (advanced, basic and rudimentary) and two 
different genres were compiled. The first corpus is 
composed of general news articles from the Brazil-
ian newspaper Zero Hora (ZH original). These ar-
ticles were manually simplified by a linguist, ex-
pert in text simplification, according to the two 
levels of simplification: natural (ZH natural) and 
strong (ZH strong). The remaining corpora are 
composed of popular science articles from differ-
ent sources: (a) the Caderno Ci?ncia section of the 
Brazilian newspaper Folha de S?o Paulo, a main-
stream newspaper in Brazil (CC original) and a 
manually simplified version of this corpus using 
the natural (CC natural) and strong (CC strong) 
levels; and (b) advanced level texts from a popular 
science magazine called Ci?ncia Hoje (CH). Table 
3 shows a few statistics about these seven corpora. 
5.2 Feature Analysis 
As a simple way to check the contribution of dif-
ferent features to our three literacy levels, we com- 
                                                          
4 http://www.cs.waikato.ac.nz/ml/weka/ 
6
  
Corpus Doc Sent Words Avg. words 
per text (std. 
deviation) 
Avg. 
words p. 
sentence 
ZH original 104 2184 46190 444.1 (133.7) 21.1 
ZH natural 104 3234 47296 454.7 (134.2) 14.6 
ZH strong 104 3668 47938 460.9 (137.5) 13.0 
CC original 50 882 20263 405.2 (175.6) 22.9 
CC natural 50 975 19603 392.0 (176.0) 20.1 
CC strong 50 1454 20518 410.3 (169.6) 14.1 
CH 130 3624 95866 737.4 (226.1) 26.4 
Table 3. Corpus statistics 
 
puted the (absolute) Pearson correlation between 
our features and the expected literacy level for the 
two sets of corpora that contain versions of the 
three classes of interest (original, natural and 
strong). Table 4 lists the most highly correlated 
features. 
 
 Feature Corr. 
1 Words per sentence 0.693 
2 Incidence of apposition 0.688 
3 Incidence of clauses 0.614 
4 Flesch index  0.580 
5 Words before main verb  0.516 
6 Sentences per paragraph  0.509 
7 Incidence of relative clauses  0.417 
8 Syllables per word 0.414 
9 Number of positive additive connectives  0.397 
10 Number of negative causal connectives 0.388 
Table 4: Correlation between features and literacy levels 
 
Among the top features are mostly basic and syn-
tactic features representing the number of apposi-
tive and relative clauses and clauses in general, and 
also features from Coh-Metrix-PORT. This shows 
that traditional cognitively-motivated features can 
be complemented with more superficial features 
for readability assessment. 
5.3 Predicting Complexity Levels 
As previously discussed, the goal is to predict the 
complexity level of a text as original, naturally or 
strongly simplified, which correspond to the three 
literacy levels of INAF: rudimentary, basic and ad-
vanced level.  
Tables 5-7 show the results of our experiments 
using 10-fold cross-validation and standard classi-
fication (Table 5), ordinal classification (Table 6) 
and regression (Table 7), in terms of F-measure 
(F), Pearson correlation with true score (Corr.) and 
mean absolute error (MAE). Results using our 
complete feature set (All) and different subsets of 
it are shown so that we can analyze the 
performance of each group of features. We also 
experiment with the Flesch index on its own as a 
feature. 
 
Features Class F Corr. MAE 
All original 0.913 0.84 0.276 
natural 0.483 
strong 0.732 
Language 
Model 
original 0.669 0.25 0.381 
natural 0.025 
strong 0.221 
Basic original 0.846 0.76 0.302 
natural 0.149 
strong 0.707 
Syntactic original 0.891 0.82 0.285 
natural 0.32 
strong 0.74 
Coh-
Metrix-
PORT 
original 0.873 0.79 0.290 
natural 0.381 
strong 0.712 
Flesch original 0.751 0.52 0.348 
natural 0.152 
strong 0.546 
Table 5: Standard Classification 
 
Features Class F Corr. MAE 
All original 0.904 0.83 0.163 
natural 0.484 
strong 0.731 
Language 
Model 
original 0.634 0.49 0.344 
natural 0.497 
strong 0.05 
Basic original 0.83 0.73 0.231 
natural 0.334 
strong 0.637 
Syntactic original 0.891 0.81 0.180 
natural 0.382 
strong 0.714 
Coh-
Metrix-
PORT 
original 0.878 0.8 0.183 
natural 0.432 
strong 0.709 
Flesch original 0.746 0.56 0.310 
natural 0.489 
strong 0 
Table 6: Ordinal classification 
 
The results of the standard and ordinal classifica-
tion are comparable in terms of F-measure and cor-
relation, but the mean absolute error is lower for 
the ordinal classification. This indicates that ordi-
nal classification is more adequate to handle our 
classes, similarly to the results found in (Heilman 
et al, 2008). Results also show that distinguishing 
between natural and strong simplifications is a 
harder problem than distinguishing between these 
and original texts. This was expected, since these 
two levels of simplification share many features. 
However, the average performance achieved is 
considered satisfactory. 
Concerning the regression model (Table 7), the 
RBF kernel reaches the best correlation scores 
7
among all models. However, its mean error rates 
are above the ones found for classification. A lin-
ear SVM (not shown here) achieves very poor re-
sults across all metrics. 
   
Features Corr. MAE 
All 0.8502 0.3478 
Language Model 0.6245 0.5448 
Basic 0.7266 0.4538 
Syntactic 0.8063 0.3878 
Coh-Metrix-PORT 0.8051 0.3895 
Flesch 0.5772 0.5492 
Table 7: Regression with RBF kernel 
 
With respect to the different feature sets, we can 
observe that the combination of all features consis-
tently yields better results according to all metrics 
across all our models. The performances obtained 
with the subsets of features vary considerably from 
model to model, which shows that the combination 
of features is more robust across different learning 
techniques. Considering each feature set independ-
ently, the syntactic features, followed by Coh-
Metrix-PORT, achieve the best correlation scores, 
while the language model features performed the 
poorest. 
These results show that it is possible to predict 
with satisfactory accuracy the readability level of 
texts according to our three classes of interest: 
original, naturally simplified and strongly simpli-
fied texts. Given such results we embedded the 
classification model (Table 5) as a tool for read-
ability assessment into our text simplification au-
thoring system. The linear classification is our 
simplest model, has achieved the highest F-
measure and its correlation scores are comparable 
to those of the other models.  
6. Conclusions 
We have experimented with different machine 
learning algorithms and features in order to verify 
whether it was possible to automatically distin-
guish among the three readability levels: original 
texts aimed at advanced readers, naturally simpli-
fied texts aimed at people with basic literacy level, 
and strongly simplified texts aimed at people with 
rudimentary literacy level. All algorithms achieved 
satisfactory performance with the combination of 
all features and we embedded the simplest model 
into our authoring tool. 
As future work, we plan to investigate the con-
tribution of deeper cognitive features to this prob-
lem, more specifically, semantic, co-reference and 
mental model dimensions metrics. Having this ca-
pacity for readability assessment is useful not only 
to inform authors preparing simplified material 
about the complexity of the current material, but 
also to guide automatic simplification systems to 
produce simplifications with the adequate level of 
complexity according to the target user.  
The authoring tool, as well as its text simplifica-
tion and readability assessment systems, can be 
used not only for improving text accessibility, but 
also for educational purposes: the author can pre-
pare texts that are adequate according to the level 
of the reader and it will also allow them to improve 
their reading skills. 
References  
Sandra M. Alu?sio, Lucia Specia, Thiago A. S. Pardo, 
Erick G. Maziero, Renata P. M. Fortes (2008). To-
wards Brazilian Portuguese Automatic Text Simpli-
fication Systems. In the Proceedings of the 8th ACM 
Symposium on Document Engineering, pp. 240-248. 
Eckhard Bick (2000). The Parsing System "Palavras": 
Automatic Grammatical Analysis of Portuguese in a 
Constraint Grammar Framework. PhD Thesis. Uni-
versity of ?rhus, Denmark. 
Jill Burstein, Martin Chodorow and Claudia Leacock 
(2003). CriterionSM Online Essay Evaluation: An 
Application for Automated Evaluation of Student 
Essays. In the Proceedings of the Fifteenth Annual 
Conference on Innovative Applications of Artificial 
Intelligence, Acapulco, Mexico.  
Arnaldo Candido Jr., Erick Maziero, Caroline Gasperin, 
Thiago A. S. Pardo, Lucia Specia, and Sandra M. 
Aluisio (2009). Supporting the Adaptation of Texts 
for Poor Literacy Readers: a Text Simplification 
Editor for Brazilian Portuguese. In NAACL-HLT 
Workshop on Innovative Use of NLP for Building 
Educational Applications, pages 34?42, Boulder?.  
Helena de M. Caseli, Tiago de F. Pereira, L?cia Specia, 
Thiago A. S. Pardo, Caroline Gasperin and Sandra 
Maria Alu?sio (2009). Building a Brazilian Portu-
guese Parallel Corpus of Original and Simplified 
Texts. In the Proceedings of CICLing. 
Max Coltheart (1981). The MRC psycholinguistic data-
base. In Quartely Jounal of Experimental Psycholo-
gy, 33A, pages 497-505. 
Scott Deerwester, Susan T. Dumais, George W. Furnas, 
Thomas K. Landauer e Richard Harshman (1990). 
Indexing By Latent Semantic Analysis. In Journal of 
the American Society For Information Science, V. 
41, pages 391-407. 
Bento C. Dias-da-Silva and Helio R. Moraes (2003). A 
constru??o de um thesaurus eletr?nico para o portu-
gu?s do Brasil. In ALFA- Revista de Ling??stica, V. 
8
47, N. 2, pages 101-115.    
Bento C Dias-da-Silva, Ariani Di Felippo and Maria das 
Gra?as V. Nunes (2008). The automatic mapping of 
Princeton WordNet lexical conceptual relations onto 
the Brazilian Portuguese WordNet database. In Pro-
ceedings of the 6th LREC, Marrakech, Morocco. 
William H. DuBay (2004). The principles of readability. 
Costa Mesa, CA: Impact Information: http://www.i 
mpact-information.com/impactinfo/readability02.pdf 
Christiane Fellbaum (1998). WordNet: An electronic 
lexical database. Cambridge, MA: MIT Press. 
Lijun Feng, No?mie Elhadad and Matt Huenerfauth 
(2009). Cognitively Motivated Features for Reada-
bility Assessment. In the Proceedings of EACL 
2009, pages 229-237. 
Ingo Gl?ckner, Sven Hartrumpf, Hermann Helbig, Jo-
hannes Leveling and Rainer Osswald (2006b). An 
architecture for rating and controlling text readabili-
ty. In Proceedings of KONVENS 2006, pages 32-35. 
Konstanz, Germany.  
Arthur C. Graesser, Danielle S. McNamara, Max M. 
Louwerse and Zhiqiang Cai (2004). Coh-Metrix: 
Analysis of text on cohesion and language. In Beha-
vioral Research Methods, Instruments, and Comput-
ers, V. 36, pages 193-202. 
Ronald K. Hambleton, H. Swaminathan and H. Jane 
Rogers (1991). Fundamentals of item response 
theory. Newbury Park, CA: Sage Press. 
Michael Heilman, Kevyn Collins-Thompson, Jamie 
Callan and Max Eskenazi (2007). Combining lexical 
and grammatical features to improve readability 
measures for first and second language texts. In the 
Proceedings of NAACL HLT 2007, pages 460-467. 
Michael Heilman, Kevyn Collins-Thompson and Max-
ine Eskenazi (2008). An Analysis of Statistical 
Models and Features for Reading Difficulty Predic-
tion. In Proceedings of the 3rd Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 71-79. 
INAF (2009). Instituto P. Montenegro and A??o Educa-
tiva. INAF Brasil - Indicador de Alfabetismo Funcio-
nal - 2009. Available online at http://www. ibope. 
com.br/ipm/relatorios/relatorio_inaf_2009.pdf  
Teresa B. F. Martins, Claudete M. Ghiraldelo, Maria 
das Gra?as V. Nunes e Osvaldo N. de Oliveira Jr. 
(1996). Readability formulas applied to textbooks in 
brazilian portuguese. ICMC Technical Report, N. 
28, 11p.  
Aur?lien Max (2006). Writing for Language-impaired 
Readers. In Proceedings of CICLing, pages 567-570. 
Danielle McNamara, Max Louwerse, and Art Graesser, 
2002. Coh-Metrix: Automated cohesion and coher-
ence scores to predict text readability and facilitate 
comprehension. Grant proposal. http://cohmetrix. 
memphis.edu/cohmetrixpr/publications.html 
Eleni Miltsakaki and Audrey Troutt (2007). Read-X: 
Automatic Evaluation of Reading Difficulty of Web 
Text. In the Proceedings of E-Learn 2007, Quebec, 
Canada. 
Eleni Miltsakaki and Audrey Troutt (2008). Real Time 
Web Text Classification and Analysis of Reading 
Difficulty. In the Proceedings of the 3rd Workshop 
on Innovative Use of NLP for Building Educational 
Applications, Columbus, OH. 
Cl?udia Oliveira, Maria C. Freitas, Violeta Quental, C?-
cero N. dos Santos, Renato P. L. and Lucas Souza 
(2006). A Set of NP-extraction rules for Portuguese: 
defining and learning. In 7th Workshop on Computa-
tional Processing of Written and Spoken Portuguese, 
Itatiaia, Brazil.  
Sarah E. Petersen and Mari Ostendorf (2009). A ma-
chine learning approach to reading level assess-
ment. Computer Speech and Language 23, 89-106. 
Emily Pitler and Ani Nenkova (2008). Revisiting reada-
bility: A unified framework for predicting text quali-
ty. In Proceedings of EMNLP, 2008. 
Adwait Ratnaparkhi (1996). A Maximum Entropy Part-
of-Speech Tagger. In Proceedings of the First Em-
pirical Methods in Natural Language Processing 
Conference, pages133-142. 
Brian Roark, Margaret Mitchell and Kristy Holling-
shead (2007). Syntactic complexity measures for de-
tecting mild cognitive impairment. In the Proceed-
ings of the Workshop on BioNLP 2007: Biological, 
Translational, and Clinical Language Processing, 
Prague, Czech Republic. 
Caroline E. Scarton, Daniel M. Almeida, Sandra M. A-
lu?sio (2009). An?lise da Inteligibilidade de textos 
via ferramentas de Processamento de L?ngua Natu-
ral: adaptando as m?tricas do Coh-Metrix para o 
Portugu?s. In Proceedings of STIL-2009, S?o Carlos, 
Brazil.   
Sarah E. Schwarm and Mari Ostendorf (2005). Reading 
Level Assessment Using Support Vector Machines 
and Statistical Language Models. In the Proceedings 
of the 43rd Annual Meeting of the ACL, pp 523?530. 
Kathleen M. Sheehan, Irene Kostin and Yoko Futagi 
(2007). Reading Level Assessment for Literary and 
Expository Texts. In D. S. McNamara and J. G. 
Trafton (Eds.), Proceedings of the 29th Annual Cog-
nitive Science Society, page 1853. Austin, TX: Cog-
nitive Science Society. 
Advaith Siddharthan (2003). Syntactic Simplification 
and Text Cohesion. PhD Thesis. University of Cam-
bridge. 
Andreas Stolcke. SRILM -- an extensible language 
modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing, 2002. 
9
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 46?53, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Fostering Digital Inclusion and Accessibility:  
The PorSimples project for Simplification of Portuguese Texts 
 
 
Sandra Maria Alu?sio and Caroline Gasperin 
Department of Computer Sciences, University of S?o Paulo 
Av. Trabalhador S?o-Carlense, 400. 13560-970 - S?o Carlos/SP, Brazil 
{sandra,cgasperin}@icmc.usp.br 
 
  
 
 
Abstract 
In this paper we present the PorSimples 
project, whose aim is to develop text adapta-
tions tools for Brazilian Portuguese. The tools 
developed cater for both people at poor litera-
cy levels and authors that want to produce 
texts for this audience. Here we describe the 
tools and resources developed over two years 
of this project and point directions for future 
work and collaboration. Since Portuguese and 
Spanish have many aspects in common, we 
believe our main point for collaboration lies in 
transferring our knowledge and experience to 
researches willing to developed simplification 
and elaboration tools for Spanish. 
1 Introduction 
In Brazil, according to the index used to measure 
the literacy level of the population (INAF - Na-
tional Indicator of Functional Literacy) (INAF, 
2007), only 28% of the population is classified as 
literate at the advanced level, while 65% of the 
population face difficulties in activities involving 
reading and comprehension depending on text 
length and complexity; therefore, their access to 
textual media is limited. The latter ones belong to 
the so-called rudimentary and basic literacy levels. 
These people are only able to find explicit informa-
tion in short texts (rudimentary level) and also 
process slightly longer texts and make simple infe-
rences (basic level).  
The production of texts with different lengths 
and complexities can be addressed by the task of 
Text Adaptation (TA), a very well known practice 
in educational settings. Young (1999) and Burstein 
(2009) mention two different techniques for TA: 
Text Simplification and Text Elaboration.  
The first can be defined as any task that reduces 
the lexical or syntactic complexity of a text, while 
trying to preserve meaning and information. Text 
Simplification can be subdivided into Syntactic 
Simplification, Lexical Simplification, Automatic 
Summarization, and other techniques.  
As to Text Elaboration, it aims at clarifying and 
explaining information and making connections 
explicit in a text, for example, providing short de-
finitions or synonyms for words known to only a 
few speakers of a language. 
The PorSimples project1 (Simplification of Por-
tuguese Text for Digital Inclusion and Accessibili-
ty) (Aluisio et al 2008a) started in November 2007 
and will finish in April 2010. It aims at developing 
technologies to make access to information easier 
for low-literacy individuals, and possibly for 
people with other kinds of reading disabilities, by 
means of Automatic Summarization, Lexical Sim-
plification, Syntactic Simplification, and Text Ela-
boration. More specifically, the goal is to help 
these readers to process documents available on 
the web. Additionally, it could help children learn-
ing to read texts of different genres, adults being 
alphabetized, hearing-impaired people who com-
municate to each other using sign languages and 
people undertaking Distance Education, in which 
text intelligibility is of great importance. 
The focus is on texts published in government 
sites or by relevant news agencies, both of impor-
                                                        
1 http://caravelas.icmc.usp.br/wiki/index.php/Principal 
46
tance to a large audience with various literacy le-
vels. The language of the texts is Brazilian Portu-
guese, for which there are no text simplification 
systems to the best of our knowledge. 
In the project we have developed resources in 
Portuguese for research on text simplification, text 
simplification technology for Portuguese, and cur-
rently we are developing and adapting resources 
and technologies for text elaboration. We have also 
built applications that make the developed technol-
ogy available to the public. In the Sections 2 to 4 
we describe all these outcomes of the project. 
We intend to foster a new interdisciplinary re-
search area to study written text comprehension 
problems via the research on readability assess-
ment, text simplification and elaboration once Por-
Simples ends. In Section 5 we describe future 
work, and in Section 6 we outline potential points 
for collaboration with researchers from Brazil and 
the rest of the Americas. 
2 Resources 
In order to understand the task of text simplifica-
tion in Portuguese and to build training and evalua-
tion data for the systems developed in the project, 
we have created a set of resources that formed the 
basis of PorSimples. Moreover, we are currently 
working on building resources for text elaboration. 
Below we describe these resources. 
2.1 Manual for Syntactic Simplification in Por-
tuguese 
We have created a Manual for Syntactic Simplifi-
cation for Portuguese (Specia et al, 2008). This 
manual recommends how particular syntactic phe-
nomena should be simplified. It is based on a care-
ful study of the Brazilian Portuguese grammar, of 
simplification systems developed for English (for 
example, (Siddharthan, 2003)), and on the Plain 
Language initiative2 (Aluisio et al, 2008b). 
The manual was the basis for the development 
of our rule-based system for syntactic simplifica-
tion described in Section 3.2. 
2.2 Corpora of Simple and Simplified Texts 
We have built 9 corpora within 2 different genres 
(general news and popular science articles). Our 
                                                        
2 http://www.plainlanguage.gov/ 
first corpus is composed of general news articles 
from the Brazilian newspaper Zero Hora (ZH orig-
inal). We had these articles manually simplified by 
a linguist, specialized in text simplification, ac-
cording to the two levels of simplification pro-
posed in PorSimples, natural (ZH natural) and 
strong (ZH strong). The Zero Hora newspaper also 
provides along its articles a simple version of them 
targeting children from 7 to 11 years old; this sec-
tion is called Para seu Filho Ler (ZH PFSL) and 
our corpus from this section contains simple ar-
ticles corresponding to the articles in the ZH origi-
nal corpus plus additional ones.  
Popular science articles compose our next set of 
corpora. We compiled a corpus of these articles 
from the Caderno Ci?ncia issue of the Brazilian 
newspaper Folha de S?o Paulo, a leading newspa-
per in Brazil (CC original). We also had this cor-
pus manually simplified according to the natural 
(CC natural) and strong (CC strong) levels. We 
also collected texts from a popular science maga-
zine called Ci?ncia Hoje (CH) and from its version 
aimed at children from 12-15, called Ci?ncia Hoje 
Crianca (CHC). Table 1 shows a few statistics 
from these corpora.  
2.3 Dictionary of Simple Words 
While for English some lexical resources that help 
to identify difficult words using psycholinguistic 
measures are available, such as the MRC Psycho-
linguistic Database3, no such resources exist for 
Portuguese. In PorSimples, we have compiled a 
dictionary of simple words composed by words 
that are common to youngsters (from Biderman 
(2005)), a list of frequent words from news texts 
for children and nationwide newspapers and a list 
of concrete words (from Janczura et. al (2007)). 
Corpus Art. Sent
. 
Words Avg. words 
per text (std. 
deviation) 
Avg. 
words p. 
sentence 
ZH original 104 2184 46190 444.1 (133.7) 21.1 
ZH natural 104 3234 47296 454.7 (134.2) 14.6 
ZH strong 104 3668 47938 460.9 (137.5) 13.0 
ZH PSFL 166 1224 22148 133.4 (48.6) 18.0 
CC original 50 882 20263 405.2 (175.6) 22.9 
CC natural 50 975 19603 392.0 (176.0) 20.1 
CC strong 50 1454 20518 410.3 (169.6) 14.1 
CH 130 3624 95866 737.4 (226.1) 26.4 
CHC 127 3282 65124 512.7 (185.3) 19.8 
Table 1. Corpus statistics. 
                                                        
3 http://www.psych.rl.ac.uk/ 
47
This dictionary is being used in applications de-
scribed in Section 4, such as SIMPLIFICA and the 
Simplification Annotation Editor. 
3 Simplification & Elaboration technology 
3.1 Lexical Simplification  
Lexical simplification consists on replacing com-
plex words by simpler words. 
The first step of lexical simplification consists of 
tokenizing the original text and selecting the words 
that are considered complex. In order to judge a 
word as complex or not, we use the dictionaries of 
simple words described in Section 2.3.  
The lexical simplification system also uses the 
Unitex-PB dictionary4 for finding the lemma of the 
words in the text, so that it is possible to look for it 
in the simple words dictionaries. The problem of 
looking for a lemma directly in a dictionary is that 
there are ambiguous words and we are not able to 
deal with different word senses. For dealing with 
part-of-speech (POS) ambiguity, we use the 
MXPOST POS tagger5 trained over NILC tagset6. 
Among the words that were selected as com-
plex, the ones that are not proper nouns, preposi-
tions and numerals are processed: their POS tags 
are used to look for their lemmas in the dictiona-
ries. As the tagger has not a 100% precision and 
some words may not be in the dictionary, we look 
for the lemma only (without the tag) when we are 
not able to find the lemma-tag combination in the 
dictionary. Still, if we are not able to find the word, 
the lexical simplification module assumes that the 
word is complex and marks it for simplification. 
The last step of the process consists in providing 
simpler synonyms for the complex words. For this 
task, we use the thesauri for Portuguese TeP 2.07 
and the lexical ontology for Portuguese PAPEL8. 
This task is carried out when the user clicks on a 
marked word, which triggers a search in the the-
sauri for synonyms that are also present in the 
common words dictionary. If simpler words are 
found, they are sorted from the simpler to the more 
complex. To determine this order, we used Google 
                                                        
4 http://www.nilc.icmc.usp.br/nilc/projects/unitex-pb/web 
/dicionarios.html 
5 http://sites.google.com/site/adwaitratnaparkhi/home 
6 www.nilc.icmc.usp.br/nilc/TagSet/ManualEtiquetagem.htm  
7 http://www.nilc.icmc.usp.br/tep2/ 
8 http://www.linguateca.pt/PAPEL/ 
API to search each word in the web: we assume 
that the higher a word frequency, the simpler it is. 
Automatic word sense disambiguation is left for 
future work. In PorSimples, we aim to use Textual 
Entailment (Dagan et al, 2005) as a method for 
gathering resources for lexical simplification. 
3.2 Syntactic Simplification 
Syntactic simplification is accomplished by a rule-
based system, which comprises seven operations 
that are applied sentence-by-sentence to a text in 
order to make its syntactic structure simpler.  
Our rule-based text simplification system is 
based on the manual for Brazilian Portuguese syn-
tactic simplification described in Section 2.1. Ac-
cording to this manual, simplification operations 
should be applied when any of the 22 linguistic 
phenomena covered by our system (see Candido et 
al. (2009) for details) is detected. Our system treats 
appositive, relative, coordinate and subordinate 
clauses, which have already been addressed by 
previous work on text simplification (Siddharthan, 
2003). Additionally, we treat passive voice, sen-
tences in an order other than Subject-Verb-Object 
(SVO), and long adverbial phrases. The simplifica-
tion operations to treat these phenomena are: split 
sentence, change particular discourse markers by 
simpler ones, change passive to active voice, invert 
the order of clauses, convert to subject-verb-object 
ordering, and move long adverbial phrases. 
Each sentence is parsed in order to identify syn-
tactic phenomena for simplification and to segment 
the sentence into portions that will be handled by 
the operations. We use the parser PALAVRAS 
(Bick, 2000) for Portuguese. Gasperin et al (2010) 
present the evaluation of the performance of our 
syntactic simplification system.  
Since our syntactic simplifications are conserva-
tive, the simplified texts become longer than the 
original due to sentence splitting. We acknowledge 
that low-literacy readers prefer short texts; this is 
why we use summarization before applying simpli-
fication in FACILITA (see (Watanabe et al, 
2009)). In the future we aim to provide summariza-
tion also within SIMPLIFICA. These two applica-
tions are described in Section 4. 
3.3 Natural and Strong Simplification 
To attend the needs of people with different levels 
of literacy, PorSimples propose two types of sim-
48
plification: natural and strong. The first is aimed at 
people with a basic literacy level and the second, 
rudimentary level. The difference between these 
two is the degree of application of simplification 
operations to the sentences.  For strong simplifica-
tion we apply the syntactic simplification process 
to all complex phenomena found in the sentence in 
order to make the sentence as simple as possible, 
while for natural simplification the simplification 
operations are applied only when the resulting text 
remains ''natural'', considering the overall complex-
ity of the sentence. This naturalness is based on a 
group of factors which are difficult to define using 
hand-crafted rules, and we intend to learn them 
from examples of natural simplifications. 
We developed a corpus-based approach for se-
lecting sentences that require simplification. Based 
on parallel corpora of original and natural simpli-
fied texts (ZH original, ZH natural, CC original, 
CC natural), we apply a binary classifier to decide 
in which circumstances a sentence should be split 
or not so that the resulting simplified text is natural 
and not over simplified. Sentence splitting is the 
most important and most frequent syntactic simpli-
fication operation, and it can be seen as a key dis-
tinctive feature between natural and strong simpli-
fication. We described this system in detail in 
(Gasperin et al, 2009). 
Our feature set contains 209 features, including 
superficial, morphological, syntactic and dis-
course-related features. We did several feature se-
lection experiments to determine the optimal set of 
features. As classification algorithm we use We-
ka's9 SMO implementation of Support Vector Ma-
chines (SVM). The ZH corpus contains 728 exam-
ples of the splitting operation and 1328 examples 
of non-split sentences, and the CC corpus contains 
59 positive and 510 negatives examples. The clas-
sifier?s average performance scores (optimal fea-
ture set, both corpora as training data, and cross-
validation) are 80.5% precision and 80.7% recall. 
3.4 Readability Assessment 
We developed a readability assessment system that 
can predict the complexity level of a text, which 
corresponds to the literacy level expected from the 
target reader: rudimentary, basic or advanced.  
We have adopted a machine-learning classifier 
                                                        
9 http://www.cs.waikato.ac.nz/ml/weka/ 
to identify the level of the input text; we use the 
Support Vector Machines implementation from 
Weka toolkit (SMO). We have used 7 of our cor-
pora presented in Section 2.2 (all but the ones with 
texts written for children) to train the classifier. 
Our feature set is composed by cognitively-
motivated features derived from the Coh-Metrix-
PORT tool10, which is an adaptation for Brazilian 
Portuguese of Coh-Metrix 2.0 (free version of 
Coh-Metrix (Graesser et al 2004)) also developed 
in the context of the PorSimples project. Coh-
Metrix-PORT implements the metrics in Table 2. 
We also included seven new metrics to Coh-
Metrix-PORT: average verb, noun, adjective and 
adverb ambiguity, incidence of high-level constitu-
ents, content words and functional words.  
 
Categories Subcategories Metrics 
Shallow 
Readabili-
ty metric 
- Flesch Reading Ease index 
for Portuguese. 
Words and 
textual 
informa-
tion 
Basic counts Number of words, sen-
tences, paragraphs, words 
per sentence, sentences per 
paragraph, syllables per 
word, incidence of verbs, 
nouns, adjectives and ad-
verbs. 
Frequencies Raw frequencies of content 
words and minimum fre-
quency of content words. 
Hyperonymy Average number of hyper-
nyms of verbs. 
Syntactic 
informa-
tion 
Constituents Incidence of nominal 
phrases, modifiers per 
noun phrase and words 
preceding main verbs. 
Pronouns, 
Types and 
Tokens 
Incidence of personal pro-
nouns, number of pronouns 
per noun phrase, types and 
tokens. 
Connectives Number of connectives, 
number of positive and 
negative additive connec-
tives, causal / temporal / 
logical positive and nega-
tive connectives. 
Logical 
operators 
- Incidence of the particles 
?e? (and), ?ou? (or), ?se? 
(if), incidence of negation 
and logical operators. 
Table 2. Metrics of Coh-Metrix-PORT. 
We measured the performance of the classifier 
on identifying the levels of the input texts by a 
                                                        
10 http://caravelas.icmc.usp.br:3000/ 
49
cross-validation experiment. We trained the clas-
sifier on our 7 corpora and reached 90% F-measure 
on identifying texts at advanced level, 48% at basic 
level, and 73% at rudimentary level. 
 
3.5 Semantic Role Labeling: Understanding 
Sense Relations between Verb and Arguments 
 
To attend the goal of eliciting sense relations be-
tween verbs and their arguments through the exhi-
bition of question words such as who, what, which, 
when, where, why, how, how much, how many, 
how long, how often and what for, we are specify-
ing a new annotation task that assigns these wh-
question labels to verbal arguments in a corpus of 
simplified texts in Portuguese. The aim is to pro-
vide a training corpus for machine learning, aiming 
at automatic assignment of wh-questions (Duran et 
al., 2010a; Duran et al, 2010b). 
The annotation task involves recognizing seg-
ments that constitute answers to questions made to 
the verbs. Each segment should suitably answer the 
wh-question label. For example, in the sentence 
?Jo?o acordou ?s 6 horas da manh?.? (John woke 
up at 6 in the morning.), two questions come up 
naturally in relation to the verb ?acordar? (wake 
up): 1) Who woke up? and 2) When?. 
Linking the verb and its arguments through wh-
questions is a process that requires text understand-
ing. This is a skill that the target audience of this 
project is weak at. In Figure 1 we show the link 
between the verb and its arguments (which can be 
subject, direct object, indirect object, time or loca-
tion adverbial phrases, and also named entities). 
 
      Who woke up? 
  
John   woke up   at 6 in the morning 
 
When?  __ _________         
Figure 1. Assigning wh-question labels to arguments. 
 
The corpus chosen for this work consists of the 
strong simplified version of 154 texts extracted 
from general news and popular science articles 
(ZH strong and CC strong) which were described 
in Section 2.2.  
Results of such a semantic layer of annotation 
may be used, in addition, to identify adjunct se-
mantic roles and multi-word expressions with spe-
cific adverbial syntactic roles. This training corpus, 
as well as the automatic labeling tool, an ?answer-
questioning? system, will be made publicly availa-
ble at PorSimples site. Besides helping poor-
literacy readers, the assignment of wh-questions 
will be used in the near future to map adjunct se-
mantic roles (ArgMs of Propbank (Palmer et al, 
2005)) in a project to build the PropBank.Br for 
Portuguese language. One may also take profit of 
this automatic tool and its training corpus to im-
prove its opposite, question-answering systems.  
4 Applications 
The text simplification and elaboration technolo-
gies developed in the context of the project are 
available by means of three systems aimed to dis-
tinct users: 
 An authoring system, called SIMPLIFICA11, to 
help authors to produce simplified texts target-
ing people with low literacy levels,  
 An assistive technology system, called FACI-
LITA12, which explores the tasks of summari-
zation and simplification to allow poor literate 
people to read Web content, and 
 A web content adaptation tool, named Educa-
tional FACILITA, for assisting low-literacy 
readers to perform detailed reading. It exhibits 
questions that clarify the semantic relations 
linking verbs to their arguments, highlighting 
the associations amongst the main ideas of the 
texts, named entities, and perform lexical ela-
boration. 
In the following subsections we detail these and 
other systems developed in the project. 
4.1 SIMPLIFICA Authoring Tool  
SIMLIFICA is a web-based WYSIWYG editor, 
based on TinyMCE web editor13. The user inputs a 
text in the editor and customizes the simplification 
settings, where he/she can choose: (i) strong sim-
plification, where all the complex syntactic phe-
nomena (see details in Section 3.2) are treated for 
each sentence, or customized simplification, where 
the user chooses one or more syntactic simplifica-
tion phenomena to be treated for each sentence, 
and (ii) one or more thesauri to be used in the syn-
tactic and lexical simplification processes. Then 
                                                        
11 http://www.nilc.icmc.usp.br/porsimples/simplifica/ 
12 http://vinho.intermidia.icmc.usp.br:3001/facilita/ 
13 http://tinymce.moxiecode.com/ 
50
the user activates the readability assessment mod-
ule to predict the complexity level of a text. This 
module maps the text to one of the three levels of 
literacy defined by INAF: rudimentary, basic or 
advanced. According to the resulting readability 
level the user can trigger the lexical and/or syntac-
tic simplifications modules, revise the automatic 
simplification and restart the cycle by checking the 
readability level of the current version of the text.  
4.2 FACILITA 
FACILITA is a browser plug-in that aims to facili-
tate the reading of online content by poor literate 
people. It includes separate modules for text sum-
marization and text simplification. The user can 
select a text on any website and call FACILITA to 
summarize and simplify this text. The system is 
described in details in Watanabe et al (2009). 
The text summarization module aims to extract 
only the most important information from a text. It 
relies on the EPC-P technique (extraction of key-
words per pattern), which checks the presence of 
keywords in the sentences: sentences that contain 
keywords are retained for the final summary. The 
summarization system is reported in Margarido et 
al. (2008).  
The text simplification module follows the syn-
tactic simplification framework described in Sec-
tion 3.2. We have chosen to run the summarization 
process first and then proceed to the simplification 
of the summarized text since simplification in-
creases text length. 
4.3 Educational FACILITA 
Educational FACILITA14 is a Web application 
aimed at assisting users in understanding textual 
content available on the Web. Currently, it ex-
plores the NLP tasks of lexical elaboration and 
named entity labeling to assist poor literacy readers 
having access to web content. It is described in 
Watanabe et al (2010). 
Lexical Elaboration consists of mechanisms that 
present users with synonymous or short definitions 
for words, which are classified as unusual or diffi-
cult to be understood by the users. This process 
relies on the framework developed for lexical sim-
plification described in Section 3.1. 
                                                        
14 http://vinho.intermidia.icmc.usp.br/watinha/Educational-
Facilita/ 
Named-entity labeling consists of displaying ad-
ditional and complementary semantic and descrip-
tive information about named entities that are con-
tained on the Web sites text. The descriptions are 
extracted from Wikipedia. 
It is expected that these additional information 
presented in the text by the proposed approach 
would help users better understand websites? tex-
tual content and allow users to learn the meaning 
of new or unusual words/expressions. 
4.4 Simplification Annotation Editor 
This editor15 was created to support the manual 
simplification of texts for the creation of our cor-
pus of simplified texts. It records and labels all the 
operations made by the annotator and encode texts 
using a new XCES16-based schema for linking the 
original-simplified information. XCES has been 
used in projects involving both only one language, 
e.g. American National Corpus (ANC)17 (English) 
and PLN-BR18 (Brazilian Portuguese); and mul-
tiple languages as parallel data, e.g.: CroCo19 (Eng-
lish-German). However, to our knowledge, Por-
Simples is the first project to use XCES to encode 
original-simplified parallel texts and also the sim-
plification operations. Two annotation layers have 
been added to the traditional stand-off annotation 
layers in order to store the information related to 
simplification (Caseli et al, 2009). 
4.5 Portal of Parallel Corpora 
The portal20 allows for online querying and 
download of our corpora of simplified texts. The 
queries can include information about syntactic 
constructions, simplification operations, etc. 
5 Future Work 
Our main area for future work lies on the evalua-
tion of the simplified texts resulting from our sys-
tems with the end user, that is, people at low litera-
cy levels. We are carrying out a large-scale study 
with readers who fit in the rudimentary and basic 
literacy levels to verify whether syntactic and lexi-
                                                        
15 http://caravelas.icmc.usp.br/anotador  
16 http://www.w3.org/XML/ 
17 http://americannationalcorpus.org 
18 http://www.nilc.icmc.usp.br/plnbr 
19 http://fr46.uni-saarland.de/croco/index_en.html 
20 http://caravelas.icmc.usp.br/portal/index.php 
51
cal simplification indeed contribute to the under-
standing of Portuguese texts. We are applying 
reading comprehension tests with original texts 
(control group) and manually simplified texts at 
strong level. However we still need to assess the 
impact of automatic lexical and syntactic simplifi-
cation and text elaboration on the understanding of 
a text by the target user of our applications. 
We also intend to investigate how to balance 
simplification/elaboration and text length. We have 
shown that in our syntactic simplification approach 
it is usual to divide long sentences, which reduce 
sentence length but increase text length due to the 
repetition of the subject in the new sentences. On 
the other hand, in summarization-based Text Sim-
plification, such as FACILITA?s approach, text 
length is reduced, but relevant information can be 
lost, which may hinder text comprehensibility. 
Text Elaboration enhances text comprehensibility, 
but it always increases text length, since it inserts 
information and repetition to reinforce understand-
ing and make explicit the connections between the 
parts of a text. Therefore, since we cannot achieve 
all the requisites at once there is a need to evaluate 
each aspect of our systems with the target users.  
We also intend to improve the performance of 
our syntactic simplification approach by experi-
menting with different Portuguese syntactic pars-
ers. Moreover, several methods of text elaboration 
are still under development and will be imple-
mented and evaluated in this current year. 
As future research, we aim to explore the impact 
of simplification on text entailment recognition 
systems. We believe simplification can facilitate 
the alignment of entailment pairs. In the opposite 
direction, text entailment or paraphrase identifica-
tion may help us find word pairs for enriching the 
lexical resources used for lexical simplification. 
6 Opportunities for Collaboration 
Enhancing the accessibility of Portuguese and 
Spanish Web texts is of foremost importance to 
improve insertion of Latin America (LA) into the 
information society and to preserve the diverse 
cultures in LA. We believe several countries in LA 
present similar statistics to Brazil in relation to the 
number of people at low literacy levels. We see our 
experience in developing text simplification and 
elaboration tools for Portuguese as the major con-
tribution that we can offer to other research groups 
in LA. We are interested in actively taking part in 
joint research projects that aim to create text sim-
plification and elaboration tools for Spanish. 
Since all resources that we have developed are 
language-dependent, they cannot be used directly 
for Spanish, but we foresee that due to similarities 
between Portuguese and Spanish a straightforward 
adaptation of solutions at the lexical and syntactic-
al levels can be achieved with reasonable effort. 
We are willing to share the lessons learned during 
the PorSimples project and offer our expertise on 
selecting and creating the appropriate resources 
(e.g. corpora, dictionaries) and technology for text 
simplification and elaboration in order to create 
similar ones for Spanish.  
The advances in text simplification and elabora-
tion methods strongly depend on the availability of 
annotated corpora for several tasks: text simplifica-
tion, text entailment, semantic role labeling, to 
name only a few. English has the major number of 
data resources in Natural Language Processing 
(NLP); Portuguese and Spanish are low-density 
languages. To solve this problem, we believe that 
there is a need for: (i) the development of a new 
area recently coined as Annotation Science; (ii) a 
centralized resource center to create, collect and 
distribute linguistic resources in LA. 
We would appreciate collaboration with re-
searchers in the USA in relation to readability as-
sessment measures, such as those of Coh-Metrix 
(see Section 3.4), whose researchers already devel-
oped up to 500 measures. Only 60 of them are 
open to public access. Besides, the know-how 
needed to develop a proposition bank of Portu-
guese would be welcome since this involves lexi-
cal resources, such as a Verbnet21, which do not 
exist for Portuguese. Other lexical resources such 
as the MRC Psycholinguistic Database, which help 
to identify difficult words using psycholinguistic 
measures, are also urgent for Portuguese since we 
have sparse projects dealing with several aspects of 
this database but no common project to unite them. 
Brazilian research funding agencies, mainly 
CAPES22, CNPq23 and FAPESP24, often release 
calls for projects with international collaboration; 
these could be a path to start the collaborative re-
search suggested above. 
                                                        
21 http://verbs.colorado.edu/~mpalmer/projects/verbnet.html 
22 http://www.capes.gov.br/ 
23 http://www.cnpq.br/ 
24 http://www.fapesp.br/ 
52
Acknowledgments 
We thank FAPESP and Microsoft Research for 
supporting the PorSimples project. 
References  
Sandra Alu?sio, Lucia Specia, Thiago Pardo, Erick Ma-
ziero and Renata Fortes. 2008a. Towards Brazilian 
Portuguese Automatic Text Simplification Systems. 
In: Proceedings of The Eight ACM Symposium on 
Document Engineering (DocEng 2008), 240-248, 
S?o Paulo, Brazil. 
Sandra Alu?sio, Lucia Specia, Thiago Pardo, Erick Ma-
ziero, Helena de M. Caseli, Renata Fortes. 2008b. A 
Corpus Analysis of Simple Account Texts and the 
Proposal of Simplification Strategies: First Steps to-
wards Text Simplification Systems In: Proceedings 
of The 26th ACM Symposium on Design of Commu-
nication (SIGDOC 2008), pp. 15-22. 
Eckhard Bick. 2000. The Parsing System "Palavras": 
Automatic Grammatical Analysis of Portuguese in a 
Constraint Grammar Framework. PhD Thesis. Aar-
hus University. 
Maria Teresa Biderman. 2005. DICION?RIO ILU-
STRADO DE PORTUGU?S. S?o Paulo, Editora 
?tica. 1?. ed. S?o Paulo: ?tica. (2005) 
Jill Burstein. 2009. Opportunities for Natural Language 
Processing Research in Education. In the Proceedings 
of CICLing, 6-27. 
Arnaldo Candido Junior, Erick Maziero, Caroline Gas-
perin, Thiago Pardo, Lucia Specia and Sandra M. 
Aluisio. 2009. Supporting the Adaptation of Texts 
for Poor Literacy Readers: a Text Simplification Edi-
tor for Brazilian Portuguese. In the Proceedings of 
the NAACL HLT Workshop on Innovative Use of 
NLP for Building Educational Applications, pages 
34?42, Boulder, Colorado, June 2009. 
Helena Caseli, Tiago Pereira, Lucia Specia, Thiago Par-
do, Caroline Gasperin and  Sandra Alu?sio. 2009. 
Building a Brazilian Portuguese parallel corpus of 
original and simplified texts. In Alexander Gelbukh 
(ed), Advances in Computational Linguistics, Re-
search in Computer Science, vol 41, pp. 59-70. 10th 
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2009). 
Ido Dagan, Oren Glickman and Bernado Magnini. 2005. 
The PASCAL Recognising Textual Entailment Chal-
lenge. In: Proceedings of The First PASCAL Recog-
nising Textual Entailment Challenge (RTE 1), [S.l.]: 
Springer, 2005. p. 1?8. 
Magali Duran, Marcelo Am?ncio and Sandra Alu?sio. 
2010a. Assigning wh-questions to verbal arguments 
in a corpus of simplified texts. Accepted for publica-
tion at Propor 2010 (http://www.inf.pucrs.br/~pro 
por2010). 
Magali Duran, Marcelo Am?ncio and Sandra Alu?sio. 
2010b. Assigning Wh-Questions to Verbal Argu-
ments: Annotation Tools Evaluation and Corpus 
Building. Accepted for publication in LREC 2010. 
Caroline Gasperin, Lucia Specia, Tiago Pereira and  
Sandra Alu?sio. 2009. Learning When to Simplify 
Sentences for Natural Text Simplification. In: Pro-
ceedings of ENIA 2009, 809-818. 
Caroline Gasperin, Erick Masiero and Sandra M. Alui-
sio. 2010. Challenging choices for text simplifica-
tion. Accepted for publication at Propor 2010 
(http://www.inf.pucrs.br/~propor2010). 
Arthur Graesser, Danielle McNamara, Max  Louwerse 
and Zhiqiang Cai. 2004. Coh-Metrix: Analysis of 
text on cohesion and language. In: Behavioral Re-
search Methods, Instruments, and Computers, 36, 
p?ginas 193-202. 
INAF. 2007. Indicador de Alfabetismo Funcional IN-
AF/Brasil - 2007. Available at http://www.acaoedu 
cativa.org.br/portal/images/stories/pdfs/inaf2007.pdf 
Gerson A Janczura, Goiara M Castilho, Nelson O Ro-
cha, Terezinha de Jesus C. van Erven and Tin Po 
Huang. 2007. Normas de concretude para 909 pala-
vras da l?ngua portuguesa. Psicologia: Teoria e Pes-
quisa Abr-Jun 2007, Vol. 23 n. 2, pp. 195-204. 
Martha Palmer, Daniel Gildea and Paul Kingsbury. 
2005.   The Proposition Bank: A Corpus Annotated 
with Semantic Roles, Computational Linguistics 
Journal, 31:1.  
Advaith Siddharthan. 2003. Syntactic Simplification 
and Text Cohesion. PhD Thesis. University of 
Cambridge. 
Lucia Specia, Sandra Aluisio and Tiago Pardo. 2008. 
Manual de Simplifica??o Sint?tica para o Portugu?s. 
Technical Report NILC-TR-08-06, 27 p. Junho 2008, 
S?o Carlos-SP. 
Willian Watanabe, Arnaldo Candido Junior, Vin?cius 
Uz?da, Renata Fortes, Tiago Pardo and Sandra 
Alu?sio. 2009. Facilita: reading assistance for low-
literacy readers. In: Proceedings of the 27th ACM In-
ternational Conference on Design of Communica-
tion. SIGDOC '09. ACM, New York, NY, 29-36.  
Willian Watanabe, Arnaldo Candido Junior, Marcelo 
Amancio, Matheus de Oliveira, Renata Fortes, Tiago 
Pardo, Renata Fortes, Sandra Alu?sio. 2010. Adapt-
ing web content for low-literacy readers by using lex-
ical elaboration and named entities labeling. Ac-
cepted for publication at W4A 2010 
(http://www.w4a.info/). 
Dolly J. Young. Linguistic simplification of SL reading 
material: effective instructional practice. The Modern 
Language Journal, 83(3):350?366, 1999.  
53
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 74?82,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Identifying and Analyzing
Brazilian Portuguese Complex Predicates
Magali Sanches Duran? Carlos Ramisch? ? Sandra Maria Alu??sio? Aline Villavicencio?
? Center of Computational Linguistics (NILC), ICMC, University of Sa?o Paulo, Brazil
? Institute of Informatics, Federal University of Rio Grande do Sul, Brazil
? GETALP ? LIG, University of Grenoble, France
magali.duran@uol.com.br ceramisch@inf.ufrgs.br
sandra@icmc.usp.br avillavicencio@inf.ufrgs.br
Abstract
Semantic Role Labeling annotation task de-
pends on the correct identification of pred-
icates, before identifying arguments and as-
signing them role labels. However, most pred-
icates are not constituted only by a verb: they
constitute Complex Predicates (CPs) not yet
available in a computational lexicon. In order
to create a dictionary of CPs, this study em-
ploys a corpus-based methodology. Searches
are guided by POS tags instead of a limited list
of verbs or nouns, in contrast to similar stud-
ies. Results include (but are not limited to)
light and support verb constructions. These
CPs are classified into idiomatic and less id-
iomatic. This paper presents an in-depth anal-
ysis of this phenomenon, as well as an original
resource containing a set of 773 annotated ex-
pressions. Both constitute an original and rich
contribution for NLP tools in Brazilian Por-
tuguese that perform tasks involving seman-
tics.
1 Introduction
Semantic Role Labeling (SRL), independently of
the approach adopted, comprehends two steps be-
fore the assignment of role labels: (a) the delimita-
tion of argument takers and (b) the delimitation of
arguments. If the argument taker is not correctly
identified, the argument identification will propa-
gate the error and SRL will fail. Argument tak-
ers are predicates, frequently represented only by a
verb and occasionally by Complex Predicates (CPs),
that is, ?predicates which are multi-headed: they are
composed of more than one grammatical element?
(Alsina et al, 1997, p. 1), like give a try, take care,
take a shower. In SRL, the verbal phrases (VPs)
identified by a parser are usually used to automat-
ically identify argument takers, but do no suffice.
A lexicon of CPs, as well as the knowledge about
verbal chains composition, would complete a fully
automatic identification of argument takers. Con-
sequently, the possibility of disagreement between
SRL annotators would rely only on the assignment
of role labels to arguments. This paper reports the
investigation of such multi-word units, in order to
meet the needs arisen from an SRL annotation task
in a corpus of Brazilian Portuguese1.
To stress the importance of these CPs for SRL,
consider the sentence John takes care of his business
in three alternatives of annotation:
The first annotation shows care of his business as
a unique argument, masking the fact that this seg-
ment is constituted of a predicative noun, care, and
its internal argument, of his business. The second
annotation shows care and of his business as argu-
ments of take, which is incorrect because of his busi-
ness is clearly an argument of care. The third an-
notation is the best for SRL purposes: as a unique
predicate ? take care, take shares its external argu-
1CPs constituted by verbal chains (e.g. have been working)
are not focused here.
74
ment with care and care shares its internal argument
with take.
The goal of this paper is twofold: first, we briefly
describe our computer-aided corpus-based method
used to build a comprehensive machine-readable
dictionary of such expressions. Second and most
important, we analyze these expressions and their
behavior in order to shed some light on the most ad-
equate lexical representation for further integration
of our resource into an SRL annotation task. The re-
sult is a database of 773 annotated CPs, that can be
used to inform SRL and other NLP applications.
In this study we classify CPs into two groups: id-
iomatic CPs and less idiomatic CPs. Idiomatic CPs
are those whose sense may not be inferred from their
parts. Examples in Portuguese are fazer questa?o
(make a point), ir embora (go away), dar o fora (get
out), tomar conta (take care), dar para tra?s (give
up), dar de ombros (shrug), passar mal (get sick).
On the other hand, we use ?less idiomatic CPs? to
refer to those CPs that vary in a continuum of differ-
ent levels of compositionality, from fully composi-
tional to semi-compositional sense, that is, at least
one of their lexical components may be litterally
understood and/or translated. Examples of less id-
iomatic CPs in Portuguese are: dar instruc?a?o (give
instructions), fazer menc?a?o (make mention), tomar
banho (take a shower), tirar foto (take a photo), en-
trar em depressa?o (get depressed), ficar triste (be-
come sad).
Less idiomatic CPs headed by a predicative noun
have been called in the literature ?light verb con-
structions? (LVC) or ?support verb constructions?
(SVC). Although both terms have been employed as
synonyms, ?light verb? is, in fact, a semantic con-
cept and ?support verb? is a syntactic concept. The
term ?light verb? is attributed to Jespersen (1965)
and the term ?support verb? was already used by
Gross in 1981. A light verb is the use of a poly-
semous verb in a non prototypical sense or ?with a
subset of their [its] full semantic features?, North
(2005). On the other hand, a support verb is the
verb that combines with a noun to enable it to fully
predicate, given that some nouns and adjectives may
evoke internal arguments, but need to be associated
with a verb to evoke the external argument, that is,
the subject. As the function of support verb is almost
always performed by a light verb, attributes of LVCs
and SVCs have been merged, making them near syn-
onyms. Against this tendency, this study will show
cases of SVCs without light verbs (trazer preju??zo =
damage, lit. bring damage) and cases of LVCs with-
out support verbs (dar certo = work well, lit. give
correct).
To the best of our knowledge, to date, there is no
similar study regarding these complex predicates in
Brazilian Portuguese, focusing on the development
of a lexical resource for NLP tasks, such as SRL.
The remainder of this paper is organized as follows:
in ?2 we discuss related work, in ?3 we present the
corpus and the details about our methodology, in ?4
we present and discuss the resulting lists of candi-
dates, in ?5 we envisage further work and draw our
conclusions.
2 Related Work
Part of the CPs focused on here are represented by
LVCs and SVCs. These CPs have been studied in
several languages from different points of view: di-
acronic (Ranchhod, 1999; Marchello-Nizia, 1996),
language contrastive (Danlos and Samvelian, 1992;
Athayde, 2001), descriptive (Butt, 2003; Langer,
2004; Langer, 2005) and for NLP purposes (Salkoff,
1990; Stevenson et al, 2004; Barreiro and Cabral,
2009; Hwang et al, 2010). Closer to our study,
Hendrickx et al (2010) annotated a Treebank of 1M
tokens of European Portuguese with almost 2,000
CPs, which include LVCs and verbal chains. This
lexicon is relevant for many NLP applications, no-
tably for automatic translation, since in any task in-
volving language generation they confer fluency and
naturalness to the output of the system.
Work focusing on the automatic extraction of
LVCs or SVCs often take as starting point a list of re-
current light verbs (Hendrickx et al, 2010) or a list
of nominalizations (Teufel and Grefenstette, 1995;
Dras, 1995; Hwang et al, 2010). These approaches
are not adopted here because our goal is precisely
to identify which are the verbs, the nouns and other
lexical elements that take part in CPs.
Similar motivation to study LVCs/SVCs (for
SRL) is found within the scope of Framenet (Atkins
et al, 2003) and Propbank (Hwang et al, 2010).
These projects have taken different decisions on how
to annotate such constructions. Framenet annotates
75
the head of the construction (noun or adjective) as
argument taker (or frame evoker) and the light verb
separately; Propbank, on its turn, first annotates sep-
arately light verbs and the predicative nouns (as
ARG-PRX) and then merges them, annotating the
whole construction as an argument taker.
We found studies regarding Portuguese
LVCs/SVCs in both European (Athayde, 2001;
Rio-Torto, 2006; Barreiro and Cabral, 2009; Duarte
et al, 2010) and Brazilian Portuguese (Neves,
1996; Conejo, 2008; Silva, 2009; Abreu, 2011). In
addition to the variations due to dialectal aspects, a
brief comparison between these papers enabled us
to verify differences in combination patterns of both
variants. In addition, Brazilian Portuguese studies
do not aim at providing data for NLP applications,
whereas in European Portuguese there are at least
two studies focusing on NLP applications: Barreiro
and Cabral (2009), for automatic translation and
Hendrickx et al (2010) for corpus annotation.
3 Corpus, Extraction Tool and Methods
We employ a corpus-based methodology in order to
create a dictionary of CPs. After a first step in which
we use a computer software to automatically extract
candidate n-grams from a corpus, the candidate lists
have been analyzed by a linguist to distinguish CPs
from fully compositional word sequences.
For the automatic extraction, the PLN-BR-FULL2
corpus was used, consisting of news texts from
Folha de Sa?o Paulo from 1994 to 2005, with
29,014,089 tokens. The corpus was first prepro-
cessed for sentence splitting, case homogeniza-
tion, lemmatization and POS tagging using the
PALAVRAS parser (Bick, 2000).
Differently from the studies referred to in Sec-
tion 2, we did not presume any closed list of light
verbs or nouns as starting point to our searches. The
search criteria we used contain seven POS patterns
observed in examples collected during previous cor-
pus annotation tasks3:
1. V + N + PRP: abrir ma?o de (give up, lit. open
hand of );
2www.nilc.icmc.usp.br/plnbr
3V = VERB, N = NOUN, PRP = PREPOSITION, DET =
DETERMINER, ADV = ADVERB, ADJ = ADJECTIVE.
2. V + PRP + N: deixar de lado (ignore, lit. leave
at side);
3. V + DET + N + PRP: virar as costas para
(ignore, lit. turn the back to);
4. V + DET + ADV: dar o fora (get out, lit. give
the out);
5. V + ADV: ir atra?s (follow, lit. go behind);
6. V + PRP + ADV: dar para tra?s (give up, lit.
give to back);
7. V + ADJ: dar duro (work hard, lit. give hard).
This strategy is suitable to extract occurrences
from active sentences, both affirmative and negative.
Cases which present intervening material between
the verb and the other element of the CP are not cap-
tured, but this is not a serious problem considering
the size of our corpus, although it influences the fre-
quencies used in candidate selection. In order to fa-
cilitate human analysis of candidate lists, we used
the mwetoolkit4: a tool that has been developed
specifically to extract MWEs from corpora, which
encompasses candidate extraction through pattern
matching, candidate filtering (e.g. through associa-
tion measures) and evaluation tools (Ramisch et al,
2010). After generating separate lists of candidates
for each pattern, we filtered out all those occurring
less than 10 times in the corpus. The entries re-
sulting of automatic identification were classified by
their frequency and their annotation is discussed in
the following section.
4 Discussion
Each pattern of POS tags returned a large number
of candidates. Our expectation was to identify CPs
among the most frequent candidates. First we an-
notated ?interesting? candidates and then, in a deep
analysis, we judged their idiomaticity. In the Table
1, we show the total number of candidates extracted
before applying any threshold, the number of an-
alyzed candidates using a threshold of 10 and the
number of CPs by pattern divided into two columns:
idiomatic and less idiomatic CPs. Additionally, each
CP was annotated with one or more single-verb
4www.sf.net/projects/mwetoolkit
76
Pattern Extracted Analyzed Less idiomatic Idiomatic
V + N + PRP 69,264 2,140 327 8
V + PRP + N 74,086 1,238 77 8
V + DET + N + PRP 178,956 3,187 131 4
V + DET + ADV 1,537 32 0 0
V + ADV 51,552 3,626 19 41
V + PREP + ADV 5,916 182 0 2
V + ADJ 25,703 2,140 145 11
Total 407,014 12,545 699 74
Table 1: Statistics for the Patterns.
paraphrases. Sometimes it is not a simple task to
decide whether a candidate constitutes a CP, spe-
cially when the verb is a very polysemous one and
is often used as support verb. For example, fazer
exame em/de algue?m/alguma coisa (lit. make exam
in/of something/somebody) is a CP corresponding to
examinar (exam). But fazer exame in another use is
not a CP and means to submit oneself to someone
else?s exam or to perform a test to pass examina-
tions (take an exam). In the following sections, we
comment the results of our analysis of each of the
patterns.
4.1 VERB + NOUN + PREPOSITION
The pattern V + N is very productive, as every com-
plement of a transitive verb not introduced by prepo-
sition takes this form. For this reason, we restricted
the pattern, adding a preposition after the noun with
the aim of capturing only nouns that have their own
complements.
We identified 335 complex predicates, including
both idiomatic and less idiomatic ones. For exam-
ple, bater papo (shoot the breeze, lit. hit chat) or
bater boca (have an argument, lit. hit mouth) are
idiomatic, as their sense is not compositional. On
the other side, tomar conscie?ncia (become aware, lit.
take conscience) and tirar proveito (take advantage)
are less idiomatic, because their sense is more com-
positional. The candidates selected with the pattern
V + N + PRP presented 29 different verbs, as shown
in Figure 15.
Sometimes, causative verbs, like causar (cause)
5We provide one possible (most frequent sense) English
translation for each Portuguese verb.
and provocar (provoke) give origin to constructions
paraphrasable by a single verb. In spite of taking
them into consideration, we cannot call them LVCs,
as they are used in their full sense. Examples:
? provocar alterac?a?o (provoke alteration)= al-
terar (alter);
? causar tumulto (cause riot) = tumultuar (riot).
Some of the candidates returned by this pattern
take a deverbal noun, that is, a noun created from
the verb, as stated by most works on LVCs and
SVCs; but the opposite may also occur: some con-
structions present denominal verbs as paraphrases,
like ter simpatia por (have sympathy for) = simpati-
zar com (sympathize with) and fazer visita (lit. make
visit) = visitar (visit). These results oppose the idea
about LVCs resulting only from the combination of a
deverbal noun and a light verb. In addition, we have
identified idiomatic LVCs that are not paraphrasable
by verbs of the same word root, like fazer jus a (lit.
make right to) = merecer (deserve).
Moreover, we have found some constructions
that have no correspondent paraphrases, like fazer
sucesso (lit. make success) and abrir excec?a?o (lit.
open exception). These findings evidence that, the
most used test to identify LVCs and SVC ? the ex-
istence of a paraphrase formed by a single verb, has
several exceptions.
We have also observed that, when the CP has a
paraphrase by a single verb, the prepositions that in-
troduce the arguments may change or even be sup-
pressed, like in:
? Dar apoio a algue?m = apoiar algue?m (give sup-
port to somebody = support somebody);
77
atear (set (on fire))botar (put)
levar (carry)tornar-se (become)
tra?ar (trace)achar (find)
chamar (call)colocar (put)
ganhar (receive/win)lan?ar (throw)
pegar (take/grab)tirar (remove)
trazer (bring)bater (beat)
ficar (stay)p?r (put)
sentir (feel)firmar (firm)
pedir (ask)abrir (open)
causar (cause)fechar (close)
prestar (provide)provocar (provoke)
tomar (take)ser (be)
dar (give)ter (have)
fazer (make/do)
0 10 20 30 40 50 60 70 80
Idiomatic Non idiomatic
Figure 1: Distribution of verbs involved in CPs, consid-
ering the pattern V + N + PRP.
? Dar cabo de algue?m ou de alguma coisa =
acabar com algue?m ou com alguma coisa (give
end of somebody or of something = end with
somebody or with something).
Finally, some constructions are polysemic, like:
? Dar satisfac?a?o a algue?m (lit. give satisfaction
to somebody) = make somebody happy or pro-
vide explanations to somebody;
? Chamar atenc?a?o de algue?m (lit. call the at-
tention of somebody) = attract the attention of
somebody or reprehend somebody.
4.2 VERB + PREPOSITION + NOUN
The results of this pattern have too much noise, as
many transitive verbs share with this CP class the
same POS tags sequence. We found constructions
with 12 verbs, as shown in Figure 2. We classi-
fied seven of these constructions as idiomatic CPs:
dar de ombro (shrug), deixar de lado (ignore), po?r
de lado (put aside), estar de olho (be alert), ficar
de olho (stay alert), sair de fe?rias (go out on vaca-
tion). The later example is very interesting, as sair
de fe?rias is synonym of entrar em fe?rias (enter on
vacation), that is, two antonym verbs are used to ex-
press the same idea, with the same syntactic frame.
In the remaining constructions, the more frequent
ater (sonfansiter )ntonb)npf
nupter lnfvscter uptyf
ctser vt))funer lnf
)noter cteeyfc-n(ter teesonf
m?er mhpfpner -tonf
cd)dcter mhpfngpeter ngpnef
/ w k/ kw z/ zw ?/ ?w 0/
1asd2tpsc 3dgrsasd2tpsc
Figure 2: Distribution of verbs involved in CPs, consid-
ering the pattern V + PRP + N.
ateere (esonfieio)be (fsieio)rrn
pruie (aieeln)tvie ()icrn
)teare (yebofnuipre (-r yte)mn
ubeie ()seonamivie (aippn
i-ebe (t?ronhre (-r yte)mn
)re (miurndigre (vicr/wtn
k zk ?k 0k 1k 2k 3k
4wbtvi)ba 5to bwbtvi)ba
Figure 3: Distribution of verbs involved in CPs, consid-
ering the pattern V + DET + N + PRP.
verbs are used to give an aspectual meaning to the
noun: cair em, entrar em, colocar em, po?r em (fall
in, enter in, put in) have inchoative meaning, that is,
indicate an action starting, while chegar a (arrive at)
has a resultative meaning.
4.3 VERB + DETERMINER + NOUN +
PREPOSITION
This pattern gave us results very similar to the pat-
tern V + N + PRP, evidencing that it is possible
to have determiners as intervening material between
the verb and the noun in less idiomatic CPs. The
verbs involved in the candidates validated for this
pattern are presented in Figure 3.
The verbs ser (be) and ter (have) are special cases.
Some ter expressions are paraphrasable by an ex-
pression with ser + ADJ, for example:
? Ter a responsabilidade por = ser responsa?vel
por (have the responsibility for = be responsi-
ble for);
? Ter a fama de = ser famoso por (have the fame
of = be famous for);
78
? Ter a garantia de = ser garantido por (have the
guarantee of = be guaranteed for).
Some ter expressions may be paraphrased by a
single verb:
? Ter a esperanc?a de = esperar (have the hope of
= hope);
? Ter a intenc?a?o de = tencionar (have the inten-
tion of = intend);
? Ter a durac?a?o de = durar (have the duration of
= last).
Most of the ser expressions may be paraphrased
by a single verb, as in ser uma homenagem para =
homenagear (be a homage to = pay homage to). The
verb ser, in these cases, seems to mean ?to consti-
tute?. These remarks indicate that the patterns ser +
DET + N and ter + DET + N deserve further anal-
ysis, given that they are less compositional than they
are usually assumed in Portuguese.
4.4 VERB + DETERMINER + ADVERB
We have not identified any CP following this pattern.
It was inspired by the complex predicate dar o fora
(escape, lit. give the out). Probably this is typical in
spoken language and has no similar occurrences in
our newspaper corpus.
4.5 VERB + ADVERB
This pattern is the only one that returned more id-
iomatic than less idiomatic CPs, for instance:
? Vir abaixo = desmoronar (lit. come down =
crumble);
? Cair bem = ser adequado (lit. fall well = be
suitable);
? Pegar mal = na?o ser socialmente adequado (lit.
pick up bad = be inadequate);
? Estar de pe?6 = estar em vigor (lit. be on foot =
be in effect);
? Ir atra?s (de algue?m) = perseguir (lit. go behind
(somebody) = pursue);
6The POS tagger classifies de pe? as ADV.
? Partir para cima (de algue?m) = agredir (lit.
leave upwards = attack);
? Dar-se bem = ter sucesso (lit. give oneself well
= succeed);
? Dar-se mal = fracassar (lit. give oneself bad =
fail).
In addition, some CPs identified through this pat-
tern present a pragmatic meaning: olhar la? (look
there), ver la? (see there), saber la? (know there), ver
so? (see only), olhar so? (look only), provided they are
employed in restricted situations. The adverbials in
these expressions are expletives, not contributing to
the meaning, exception made for saber la?, (lit. know
there) which is only used in present tense and in first
and third persons. When somebody says ?Eu sei la??
the meaning is ?I don?t know?.
4.6 VERB + PREPOSITION + ADVERB
This is not a productive pattern, but revealed two
verbal expressions: deixar para la? (put aside) and
achar por bem (decide).
4.7 VERB + ADJECTIVE
Here we identified three interesting clusters:
1. Verbs of double object, that is, an object
and an attribute assigned to the object. These
verbs are: achar (find), considerar (con-
sider), deixar (let/leave), julgar (judge), man-
ter (keep), tornar (make) as in: Ele acha voce?
inteligente (lit. He finds you intelligent = He
considers you intelligent). For SRL annotation,
we will consider them as full verbs with two in-
ternal arguments. The adjective, in these cases,
will be labeled as an argument. However, con-
structions with the verbs fazer and tornar fol-
lowed by adjectives may give origin to some
deadjectival verbs, like possibilitar = tornar
poss??vel (possibilitate = make possible). Other
examples of the same type are: celebrizar
(make famous), esclarecer (make clear), evi-
denciar (make evident), inviabilizar (make un-
feasible), popularizar (make popular), respon-
sabilizar (hold responsible), viabilizar (make
feasible).
79
2. Expressions involving predicative adjectives,
in which the verb performs a functional role, in
the same way as support verbs do in relation to
nouns. In contrast to predicative nouns, pred-
icative adjectives do not select their ?support?
verbs: they combine with any verb of a restrict
set of verbs called copula. Examples of copula
verbs are: acabar (finish), andar (walk), con-
tinuar (continue), estar (be), ficar (stay), pare-
cer (seem), permanecer (remain), sair (go out),
ser (be), tornar-se (become), viver (live). Some
of these verbs add an aspect to the predica-
tive adjective: durative (andar, continuar, es-
tar, permanecer, viver) and resultative (acabar,
ficar, tornar-se, sair).
? The resultative aspect may be expressed
by an infix, substituting the combina-
tion of V + ADJ by a full verb: ficar
triste = entristecer (become sad) or by
the verbalization of the adjective in reflex-
ive form: ficar tranquilo = tranquilizar-se
(calm down); estar inclu??do = incluir-se
(be included).
? In most cases, adjectives preceded by cop-
ula verbs are formed by past participles
and inherit the argument structure of the
verb: estar arrependido de = arrepender-
se de (lit. be regretful of = regret).
3. Idiomatic CPs, like dar duro (lit. give hard =
make an effort), dar errado (lit. give wrong =
go wrong), fazer bonito (lit. make beautiful =
do well), fazer feio (make ugly = fail), pegar
leve (lit. pick up light = go easy), sair errado
(lit. go out wrong = go wrong), dar certo (lit.
give correct = work well).
4.8 Summary
We identified a total of 699 less idiomatic CPs
and observed the following recurrent pairs of para-
phrases:
? V = V + DEVERBAL N, e.g. tratar = dar trata-
mento (treat = give treatment);
? DENOMINAL V = V + N, e.g. amedrontar =
dar medo (frighten = give fear);
atear (set (on fire))botar (put)
lorrer (run)varantir (vuarantee)
cayer (be-caye)soar (sounm)
tornar?se (belohe)tradar (trale)
tratar (treat)lceirar (shegg)
fagar (shegg)ihavinar (spea/)
partir (ihavine)saber (geaye)
torler (/now)yager (wrinv)
yirar (be wortc)yogtar (turn)
vancar (vo bal/)gandar (releiye-win)
tirar (tcrow)traker (rehoye)
passar (brinv)alcar (pass-spenm)
sevuir (foggow)yer (see)
sentir (feeg)lair (fagg)
yir (lohe)bater (beat)
sair (vo out)firhar (firh)
pemir (as/)lcahar (lagg)
lcevar (arriye)ogcar (goo/)
lausar (lause)felcar (lgose)
geyar (geaye-get)ir (vo)
abrir (open)meizar (geaye-get)
pevar (ta/e-vrab)prestar (proyime)
proyolar (proyo/e)p?r (put)
tohar (ta/e)estar (be)
logolar (put)tornar (turn)
entrar (enter)ser (be)
mar (viye)filar (sta0)
ter (caye)faker (mo-ha/e)
1 21 31 41 51 611 621 631
7miohatil 8on imiohatil
Figure 4: Distribution of verbs involved in CPs, consid-
ering the total number of CPs (i.e. all patterns).
? DEADJECTIVAL V = V + ADJ, e.g. res-
ponsabilizar = tornar responsa?vel (lit. respon-
sibilize = hold responsible).
This will help our further surveys, as we may
search for denominal and deadjectival verbs (which
may be automatically recognized through infix and
suffix rules) to manually identify corresponding
CPs. Moreover, the large set of verbs involved in the
analyzed CPs, summarized in Figure 4, shows that
any study based on a closed set of light verbs will
be limited, as it cannot capture common exceptions
and non-prototypical constructions.
5 Conclusions and Future Work
This study revealed a large number of CPs and pro-
vided us insights into how to capture them with more
precision. Our approach proved to be very useful to
identify verbal MWEs, notably with POS tag pat-
80
terns that have not been explored by other studies
(patterns not used to identify LVCs/SVCs). How-
ever, due to the onus of manual annotation, we as-
sume an arbitrary threshold of 10 occurrences that
removes potentially interesting candidates. Our hy-
pothesis is that, in a machine-readable dictionary,
as well as in traditional lexicography, rare entries
are more useful than common ones, and we would
like to explore two alternatives to address this is-
sue. First, it would be straightforward to apply more
sophisticated filtering techniques like lexical asso-
ciation measures to our candidates. Second, we
strongly believe that our patterns are sensitive to
corpus genre, because the CPs identified are typical
of colloquial register. Therefore, the same patterns
should be applied on a corpus of spoken Brazilian
Portuguese, as well as other written genres like web-
crawled corpora. Due to its size and availability, the
latter would also allow us to obtain better frequency
estimators.
We underline, however, that we should not un-
derestimate the value of our original corpus, as it
contains a large amount of unexplored material. We
observed that only the context can tell us whether
a given verb is being used as a full verb or as a
light and/or support verb7. As a consequence, it
is not possible to build a comprehensive lexicon of
light and support verbs, because there are full verbs
that function as light and/or support verbs in spe-
cific constructions, like correr (run) in correr risco
(run risk). As we discarded a considerable number
of infrequent lexical items, it is possible that other
unusual verbs participate in similar CPs which have
not been identified by our study.
For the moment, it is difficult to assess a quan-
titative measure for the quality and usefulness of
our resource, as no similar work exists for Por-
tuguese. Moreover, the lexical resource presented
here is not complete. Productive patterns, the ones
involving nouns, must be further explored to enlarge
the aimed lexicon. A standard resource for English
like DANTE8, for example, contains 497 support
verb constructions involving a fixed set of 5 support
verbs, and was evaluated extrinsically with regard
to its contribution in complementing the FrameNet
7A verb is not light or support in the lexicon, it is light and/or
support depending on the combinations in which it participates.
8www.webdante.com
data (Atkins, 2010). Likewise, we intend to evalu-
ate our resource in the context of SRL annotation, to
measure its contribution in automatic argument taker
identification. The selected CPs will be employed in
an SRL project and, as soon as we receive feedback
from this experience, we will be able to report how
many CPs have been annotated as argument takers,
which will represent an improvement in relation to
the present heuristic based only on parsed VPs.
Our final goal is to build a broad-coverage lexicon
of CPs in Brazilian Portuguese that may contribute
to different NLP applications, in addition to SRL.
We believe that computer-assisted language learning
systems and other Portuguese as second language
learning material may take great profit from it. Anal-
ysis systems like automatic textual entailment may
use the relationship between CPs and paraphrases to
infer equivalences between propositions. Computa-
tional language generation systems may also want
to choose the most natural verbal construction to use
when generating texts in Portuguese. Finally, we be-
lieve that, in the future, it will be possible to enhance
our resource by adding more languages and by link-
ing the entries in each language, thus developing a
valuable resource for automatic machine translation.
Acknowledgements
We thank the Brazilian research foundation FAPESP
for financial support.
References
De?bora Ta??s Batista Abreu. 2011. A sema?ntica
de construc?o?es com verbos-suporte e o paradigma
Framenet. Master?s thesis, Sa?o Leopoldo, RS, Brazil.
1997. Complex Predicates. CSLI Publications, Stanford,
CA, USA.
Maria Francisca Athayde. 2001. Construc?o?es com
verbo-suporte (funktionsverbgefu?ge) do portugue?s e
do alema?o. Number 1 in Cadernos do CIEG Centro
Interuniversita?rio de Estudos German??sticos. Universi-
dade de Coimbra, Coimbra, Portugal.
Sue Atkins, Charles Fillmore, and Christopher R. John-
son. 2003. Lexicographic relevance: Selecting infor-
mation from corpus evidence. International Journal
of Lexicography, 16(3):251?280.
Sue Atkins, 2010. The DANTE Database: Its Contribu-
tion to English Lexical Research, and in Particular to
Complementing the FrameNet Data. Menha Publish-
ers, Kampala, Uganda.
81
Anabela Barreiro and Lu??s Miguel Cabral. 2009. ReE-
screve: a translator-friendly multi-purpose paraphras-
ing software tool. In Proceedings of the Workshop Be-
yond Translation Memories: New Tools for Transla-
tors, The Twelfth Machine Translation Summit, pages
1?8, Ottawa, Canada, Aug.
Eckhard Bick. 2000. The parsing system Palavras.
Aarhus University Press.
Miriam Butt. 2003. The light verb jungle. In Proceed-
ings of the Workshop on Multi-Verb Constructions,
pages 243?246, Trondheim, Norway.
Ca?ssia Rita Conejo. 2008. O verbo-suporte fazer na
l??ngua portuguesa: um exerc??cio de ana?lise de base
funcionalista. Master?s thesis, Maringa?, PR, Brazil.
Laurence Danlos and Pollet Samvelian. 1992. Transla-
tion of the predicative element of a sentence: category
switching, aspect and diathesis. In Proceedings of the
Fourth International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI),
pages 21?34, Montre?al, Canada.
Mark Dras. 1995. Automatic identification of support
verbs: A step towards a definition of semantic weight.
In Proceedings of the Eighth Australian Joint Confer-
ence on Artificial Intelligence, pages 451?458, Can-
berra, Australia. World Scientific Press.
Ine?s Duarte, Anabela Gonc?alves, Matilde Miguel,
Ama?lia Mendes, Iris Hendrickx, Fa?tima Oliveira,
Lu??s Filipe Cunha, Fa?tima Silva, and Purificac?a?o Sil-
vano. 2010. Light verbs features in European Por-
tuguese. In Proceedings of the Interdisciplinary Work-
shop on Verbs: The Identification and Representation
of Verb Features (Verb 2010), Pisa, Italy, Nov.
Iris Hendrickx, Ama?lia Mendes, S??lvia Pereira, Anabela
Gonc?alves, and Ine?s Duarte. 2010. Complex predi-
cates annotation in a corpus of Portuguese. In Pro-
ceedings of the ACL 2010 Fourth Linguistic Annota-
tion Workshop, pages 100?108, Uppsala, Sweden.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Yuping Zhou, Nianwen
Xue, and Martha Palmer. 2010. Propbank annota-
tion of multilingual light verb constructions. In Pro-
ceedings of the ACL 2010 Fourth Linguistic Annota-
tion Workshop, pages 82?90, Uppsala, Sweden.
Otto Jespersen. 1965. A Modern English Grammar on
Historical Principles. George Allen and Unwin Ltd.,
London, UK.
Stefan Langer. 2004. A linguistic test battery for sup-
port verb constructions. Special issue of Linguisticae
Investigationes, 27(2):171?184.
Stefan Langer, 2005. Semantik im Lexikon, chapter
A formal specification of support verb constructions,
pages 179?202. Gunter Naar Verlag, Tu?bingen, Ger-
many.
Christiane Marchello-Nizia. 1996. A diachronic survey
of support verbs: the case of old French. Langages,
30(121):91?98.
Maria Helena Moura Neves, 1996. Grama?tica do por-
tugue?s falado VI: Desenvolvimentos, chapter Estudo
das construc?o?es com verbos-suporte em portugue?s,
pages 201?231. Unicamp FAPESP, Campinas, SP,
Brazil.
Ryan North. 2005. Computational measures of the ac-
ceptability of light verb constructions. Master?s thesis,
Toronto, Canada.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. Multiword expressions in the wild? the
mwetoolkit comes in handy. In Proc. of the 23rd COL-
ING (COLING 2010) ? Demonstrations, pages 57?
60, Beijing, China, Aug. The Coling 2010 Organizing
Committee.
Elisabete Ranchhod, 1999. Lindley Cintra. Home-
nagem ao Homem, ao Mestre e ao Cidada?o, chap-
ter Construc?o?es com Nomes Predicativos na Cro?nica
Geral de Espanha de 1344, pages 667?682. Cosmos,
Lisbon, Portugal.
Grac?a Rio-Torto. 2006. O Le?xico: sema?ntica e
grama?tica das unidades lexicais. In Estudos sobre
le?xico e grama?tica, pages 11?34, Coimbra, Portugal.
CIEG/FLUL.
Morris Salkoff. 1990. Automatic translation of sup-
port verb constructions. In Proc. of the 13th COLING
(COLING 1990), pages 243?246, Helsinki, Finland,
Aug. ACL.
Hilda Monetto Flores Silva. 2009. Verbos-suporte ou
expresso?es cristalizadas? Soletras, 9(17):175?182.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical measures of the semi-productivity of
light verb constructions. In , Proc. of the ACL Work-
shop on MWEs: Integrating Processing (MWE 2004),
pages 1?8, Barcelona, Spain, Jul. ACL.
Simone Teufel and Gregory Grefenstette. 1995. Corpus-
based method for automatic identification of support
verbs for nominalizations. In Proc. of the 7th Conf.
of the EACL (EACL 1995), pages 98?103, Dublin, Ire-
land, Mar.
82
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 137?147,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards an on-demand Simple Portuguese Wikipedia
Arnaldo Candido Junior Ann Copestake
Institute of Mathematics and Computer Sciences Computer Laboratory 
University of S?o Paulo University of Cambridge 
arnaldoc at icmc.usp.br Ann.Copestake at cl.cam.ac.uk 
Lucia Specia Sandra Maria Alu?sio
Research Group in Computational Linguistics Institute of Mathematics and Computer Sciences 
University of Wolverhampton University of S?o Paulo
l.specia at wlv.ac.uk sandra at icmc.usp.br
Abstract
The  Simple  English Wikipedia  provides  a 
simplified  version  of  Wikipedia's  English 
articles  for  readers  with  special  needs. 
However,  there are fewer efforts  to make 
information  in  Wikipedia  in  other 
languages  accessible  to  a  large  audience. 
This work proposes the use of a syntactic 
simplification  engine  with  high  precision 
rules  to  automatically  generate  a  Simple 
Portuguese Wikipedia on demand, based on 
user interactions with the main Portuguese 
Wikipedia.  Our  estimates  indicated that  a 
human  can  simplify  about  28,000 
occurrences  of  analysed  patterns  per 
million  words,  while  our  system  can 
correctly simplify 22,200 occurrences, with 
estimated f-measure 77.2%. 
1 Introduction
The Simple English Wikipedia1 is an effort to make 
information  in  Wikipedia2 accessible  for  less 
competent  readers  of  English  by  using  simple 
words and grammar.  Examples of  intended users 
include  children  and  readers  with  special  needs, 
such as users with learning disabilities and learners 
of English as a second language. 
Simple English (or Plain English), used in this 
version  of  Wikipedia,  is  a  result  from the  Plain 
English movement that occurred in Britain and the 
United States in the late 1970?s as a reaction to the 
unclear language used in government and business 
forms and documents. Some recommendations on 
how  to  write  and  organize  information  in  Plain 
1 http://simple.wikipedia.org/
2 http://www.wikipedia.org/
Language (the set of guidelines to write simplified 
texts) are related to both syntax and lexical levels: 
use short sentences; avoid hidden verbs; use active 
voice; use concrete, short, simple words. 
A number of resources, such as lists of common 
words3,  are available for the English language to 
help users write in Simple English. These include 
lexical  resources  like  the  MRC  Psycholinguistic 
Database4 which  helps  identify  difficult  words 
using  psycholinguistic  measures.  However, 
resources as such do not exist for Portuguese. An 
exception is a small list of simple words compiled 
as part  of  the  PorSimples project  (Aluisio et  al., 
2008).
Although  the  guidelines  from  the  Plain 
Language  can  in  principle  be  applied  for  many 
languages and text genres, for Portuguese there are 
very  few  efforts  using  Plain  Language  to  make 
information accessible to a large audience. To the 
best  of  our  knowledge,  the  solution  offered  by 
Portugues  Claro5 to  help  organizations  produce 
European  Portuguese  (EP)  documents  in  simple 
language is the only commercial option in such a 
direction.  For  Brazilian  Portuguese  (BP),  a 
Brazilian  Law (10098/2000)  tries  to  ensure  that 
content  in  e-Gov sites  and  services  is  written  in 
simple  and  direct  language  in  order  to  remove 
barriers in communication and to ensure citizens' 
rights  to  information  and communication  access. 
However,  as  it  has  been  shown  in  Martins  and 
Filgueiras  (2007),  content  in  such  websites  still 
needs  considerable  rewriting  to  follow the  Plain 
Language guidelines. 
A few efforts from the research community have 
recently  resulted  in  natural  language  processing 
3 http://simple.wiktionary.org/
4 http://www2.let.vu.nl/resources/elw/resource/mrc.html
5  http://www.portuguesclaro.pt/
137
systems to simplify and make Portuguese language 
clearer. ReEscreve (Barreiro and Cabral, 2009) is a 
multi-purpose  paraphraser that  helps  users  to 
simplify their EP texts by reducing its ambiguity, 
number  of  words  and  complexity.  The  current 
linguistic phenomena paraphrased are support verb 
constructions,  which  are  replaced  by  stylistic 
variants.  In  the  case  of  BP,  the  lack  of 
simplification  systems  led  to  development  of 
PorSimples project (Alu?sio and Gasperin, 2010). 
This  project  uses  simplification  in  different 
linguistic levels to provide simplified text to poor 
literacy readers.
For  English,  automatic  text  simplification  has 
been  exploited  for  helping  readers  with  poor 
literacy (Max, 2006) and readers with other special 
needs,  such  as  aphasic  people  (Devlin  and 
Unthank,  2006;  Carroll  et  al.  1999).  It  has  also 
been used in bilingual education (Petersen, 2007) 
and  for  improving  the  accuracy  of  Natural 
Language Processing (NLP) tasks (Klebanov et al, 
2004; Vickrey and Koller, 2008).
Given the general scarcity of human resources to 
manually simplify large content  repositories such 
as Wikipedia, simplifying texts automatically can 
be  the  only  feasible  option.  The  Portuguese 
Wikipedia,  for  example,  is  the  tenth  largest 
Wikipedia (as of May 2011), with 683,215 articles 
and approximately 860,242 contributors6. 
In  this  paper  we  propose  a  new  rule-based 
syntactic simplification system to create a Simple 
Portuguese Wikipedia  on demand,  based on user 
interactions with the main Portuguese Wikipedia. 
We use a simplification engine to change passive 
into active voice and to break down and change the 
syntax of subordinate clauses.  We focus on these 
operations  because  they  are  more  difficult  to 
process  by  readers  with  learning  disabilities  as 
compared  to  others  such  as  coordination  and 
complex noun phrases (Abedi et al, 2011; Jones et 
al.,  2006;  Chappell,  1985).  User  interaction with 
Wikipedia can be performed by a system like the 
Facilita7 (Watanabe et al, 2009), a browser plug-in 
developed  in  the  PorSimples  project  to  allow 
automatic adaptation (summarization and syntactic 
simplification) of any web page in BP.
This  paper  is  organized  as  follows.  Section  2 
presents related work on syntactic  simplification. 
6 http://meta.wikimedia.org/wiki/List_of_Wikipedias#
Grand Total
7 http://nilc.icmc.usp.br/porsimples/facilita/
Section  3 presents the methodology to build and 
evaluate the simplification engine for BP. Section 4 
presents  the  results  of  the  engine  evaluation. 
Section  5 presents  an  analysis  on  simplification 
issues  and  discusses  possible  improvements. 
Section 6 contains some final remarks.
2 Related work
Given the dependence of  syntactic  simplification 
on  linguistic  information,  successful  approaches 
are  mostly  based  on  rule-based  systems. 
Approaches using operations learned from corpus 
have  not  shown  to  be  able  to  perform  complex 
operations  such  the  splitting  of  sentences  with 
relative clauses (Chandrasekar and Srinivas, 1997; 
Daelemans  et  al.,  2004;  Specia,  2010).  On  the 
other hand. the use of machine learning techniques 
to predict when to simplify a sentence, i.e. learning 
the properties of language that distinguish simple 
from normal  texts,  has  achieved relative  success 
(Napoles and Dredze, 2010). Therefore, most work 
on syntactic simplification still relies on rule-based 
systems to simplify a set of syntactic constructions. 
This is also the approach we follow in this paper. 
In what follows we review some relevant and work 
on syntactic simplification.
The seminal work of Chandrasekar and Srinivas 
(1997) investigated the induction of syntactic rules 
from a corpus annotated with part-of-speech tags 
augmented  by  agreement  and  subcategorization 
information.  They  extracted  syntactic 
correspondences  and  generated  rules  aiming  to 
speed up parsing and improving its accuracy, but 
not  working  on  naturally  occurring  texts. 
Daelemans et  al.  (2004)  compared both machine 
learning  and  rule-based  approaches  for  the 
automatic generation of TV subtitles for hearing-
impaired  people.  In  their  machine  learning 
approach,  a  simplification model  is  learned from 
parallel  corpora  with  TV  programme  transcripts 
and the associated subtitles. Their method used a 
memory-based learner and features such as words, 
lemmas,  POS tags,  chunk tags,  relation tags  and 
proper  name  tags,  among  others  features  (30  in 
total). However, this approach did not perform as 
well  as  the  authors  expected,  making errors  like 
removing sentence subjects or deleting a part of a 
multi-word  unit.   More  recently,  Specia  (2010) 
presented a new approach for text simplification, 
based  on  the  framework  of  Statistical  Machine 
138
Translation. Although the results are promising for 
lexical simplification, syntactic rewriting was not 
captured  by  the  model  to  address  long-distance 
operations,  since  syntactic  information  was  not 
included into the framework.
Inui et al (2003) proposed a rule-based system 
for text simplification aimed at deaf people. Using 
about  one  thousand  manually  created  rules,  the 
authors  generate  several  paraphrases  for  each 
sentence and train a classifier to select the simpler 
ones.  Promising  results  were  obtained,  although 
different  types  of  errors  on  the  paraphrase 
generation are encountered, such as problems with 
verb conjugation and regency.  Our work aims at 
making  Portuguese  Wikipedia  information 
accessible  to  a  large  audience  and  instead  of 
generating  several  possible  outputs  we  generate 
only one based on rules taken from a manual of 
simplification for BP.
Siddharthan  (2006)  proposed  a  syntactic 
simplification  architecture  that  relies  on  shallow 
parsing. The general goal of the architecture is to 
make texts more accessible to a broader audience 
instead of targeting any particular application. The 
system  simplifies  apposition,  relative  clauses, 
coordination  and  subordination.  Our  method,  on 
the other hand, relies on deep parsing (Bick, 2000) 
and focuses  on  changing passive  to  active voice 
and  changing  the  syntax  of  relative  clauses  and 
subordinate sentences.
Max  (2006)  applied  text  simplification  in  the 
writing process by embedding the simplifier into a 
word  processor.  Although  this  system  ensures 
accurate  output,  it  requires  manual  choices.  The 
suggested simplifications are ranked by a score of 
syntactic  complexity  and  potential  change  of 
meaning.  The writer  then chooses their  preferred 
simplification.  Our  method,  on  the  other  hand, 
offers the user only one simplification since it uses 
several  rules  to  better  capture  each  complex 
phenomenon. 
Inspired  by  Siddharthan  (2006),  Jonnalagadda 
and  Gonzalez  (2009)  present  an  approach  to 
syntactic  simplification  addressing  also  the 
problem of accurately determining the grammatical 
correctness  of  the  simplified  sentences.  They 
propose  the  combination  of  the  number  of  null 
links  and  disjunct  cost  (the  level  of 
inappropriateness,  caused  by  using  less  frequent 
rules in the linkage) from the cost vector returned 
by a Link Grammar8 parser. Their motivation is to 
improve the performance of systems for extracting 
Protein-Protein  Interactions  automatically  from 
biomedical  articles  by  automatically  simplifying 
sentences.  Besides  treating  the  syntactic 
phenomena described in Siddharthan (2006), they 
remove  describing  phrases  occurring  at  the 
beginning  of  the  sentences,  like  ?These  results 
suggest that? and ?As reported previously?. While 
they  focus  on  the  scientific  genre,  our  work  is 
focused on the encyclopedic genre.
In order to obtain a text easier to understand by 
children,  De  Belder  and  Moens  (2010)  use  the 
Stanford parser9 to select the following phenomena 
to syntactically simplify the sentences: appositions, 
relative  clauses,  prefix  subordination  and  infix 
subordination  and  coordination.  After  sentence 
splitting, they try to apply the simplification rules 
again to both of the new sentences. However, they 
conclude that  with the set  of  simplification rules 
used,  it  was  not  possible  to  reduce  the  reading 
difficulty for children and foresee the use of other 
techniques for this purpose, such as summarization 
and elaborations for difficult words.
3 Simplification engine
3.1 Engine development
The  development  of  a  syntactic  simplification 
engine  for  a  specific  task  and  audience  can  be 
divided  into  five  distinct  phases:  (a)  target 
audience analysis; (b) review of complex syntactic 
phenomena for such an audience; (c) formulation 
of simplification guidelines; (d) refinement of rules 
based  on  evidence  from  corpora;  and  (e) 
programming and evaluation of rules.
In this paper we focus on the last two phases. 
We  use  the  simplification  guidelines  from  the 
PorSimples  project,  but  these  are  based  on 
grammar  studies  and  corpora  analysis  for  a 
different  text  genre  (news).  Therefore  additional 
corpora  evidence  proved  to  be  necessary.  This 
resulted  in  the  further  refinement  of  the  rules, 
covering  different  cases  for  each  syntactic 
phenomenon.
The Simplification engine relies on the output of 
the  Palavras  Parser  (Bick,  2000)  to  perform 
constituent tree transformations (for example, tree 
8 http://www.abisource.com/projects/link-grammar/
9 http://nlp.stanford.edu/software/lex-parser.shtml
139
splitting).  Each  node  of  a  sentence  tree  is  fed 
(breadth-first  order)  to  the  simplification 
algorithms,  which can simplify the node (and its 
sub-tree) or skip it when the node does not meet 
the simplification prerequisites. Breadth-first order 
is chosen because several operations affect the root 
of a (sub)tree, while none of them affect leaves.
A development  corpus containing examples  of 
cases analysed for each syntactic phenomenon is 
used  to  test  and  refine  the  rules.  The  current 
version of the corpus has 156 sentences extracted 
from news text. The corpus includes negative and 
positive  examples  for  each  rule.  Negative 
examples  should  not  be  simplified.  They  were 
inserted  into  the  corpus  to  avoid  unnecessary 
simplifications. Each rule is first tested against its 
own positive and negative examples.  This test  is 
called  local  test.  After reaching a good precision 
on the local test, the rule is then tested against all 
the  sentences  in  the  corpus,  global  test.  In  the 
current corpus, the global test identified sentences 
correctly   simplified by  at  least  one  rule  (66%), 
sentences incorrectly simplified due to major errors 
in  parsing/rules  (7%)  (ungrammatical  sentences) 
and  non-simplified  sentences  (27%).  The  last 
includes  mainly  negative  examples,  but  also 
includes  sentences  not  selected  due  to  parsing 
errors, sentences from cases not yet implemented, 
and sentences from cases ignored due to ambiguity.
3.2 Passive voice
The default case for dealing with passive voice in 
our simplification engine is illustrated by the pair 
of  original-simplified sentences  in  example10 (1). 
Sentences  belonging  to  this  case  have  a  non-
pronominal subject and a passive agent. Also, the 
predicator has two verbs, the verb  to be followed 
by  a  verb  in  the  past  participle  tense.  The 
simplification consists  in  reordering the sentence 
components,  turning  the  agent  into  subject 
(removing the  by preposition), turning the subject 
into direct  object and adjusting the predicator by 
removing the verb to be and re-inflecting the main 
verb. The new tense of the main verb is the same 
as  the  one  of  the  to  be  verb  and  its  number  is 
defined according to the new subject.
10 Literal translations from Portuguese result in some 
sentences appearing ungrammatical in English. 
O: As[The] transfer?ncias[transfers] 
foram[were:plural] feitas[made] pela[by the] 
empresa[company]. (1)
S: A[The] empresa[company] fez[made:sing] 
as[the] transfer?ncias[transfers].
Other correctly processed cases vary according 
the  number  of  verbs  (three  or  four),  special 
subjects, and special agents. For cases comprising 
three or four verbs, the simplification rule must re-
inflect11 two verbs (2) (one of them should agree 
with  the  subject  and  the  other  receives  its  tense 
from  the  verb  to  be).   There  are  two  cases  of 
special subjects. In the first case, a hidden subject 
is turned into a pronominal direct object (3). In the 
second  case,  a  pronominal  subject  must  be 
transformed to oblique case pronoun and then to 
direct  object.  Special  agents  also  represent  two 
cases. In the first one, oblique case pronouns must 
be  transformed before  turning  the  agent  into  the 
subject. In the second case (4), a non-existent agent 
is turned into an undetermined subject (represented 
here by ?they?).
O: A[The] porta[door] deveria[should] ter[have] 
sido[been] trancada[locked:fem] por[by] John. (2)S: John deveria[should] ter[have] 
trancado[locked:masc] a[the] porta[door].
O: [I] fui[was] encarregado[entrusted] por[by] 
minha[my] fam?lia[family]. (3)S: Minha[My] fam?lia[family] 
encarregou[entrusted] me[me].
O: O[The] ladr?o[thief] foi[was] pego[caught]. (4)
S: [They] pegaram[caught] o[the] ladr?o[thief].
Two cases  are  not  processed because they are 
already considered easy enough: the syndetic voice 
and passive in non-root sentences. In those cases, 
the  proposed  simplification  is  generally  less 
understandable  than  the  original  sentence. 
Sentences  with  split  predicator  (as  in  ?the 
politician was very criticized by his electors?) are 
not  processed  for  the  time  being,  but  should  be 
incorporated in the pipeline in the future.
Table  1 presents the algorithm used to process 
the  default  case  rule  and  verb  case  rules. 
Simplification rules are applied against all nodes in 
constituent tree, one node at a time, using breadth-
first traversing.
11 Some reinflections may not be visible on example 
translation.
140
Step Description
1 Validate these prerequisites or give up:
1.1     Node must be root
1.2     Predictor must have an inflection of auxiliary   
    verb to be
1.3     Main verb has to be in past participle
2 Transform subject into direct object
3 Fix the predicator
3.1 If main verb is finite then:
    main verb gets mode and tense from to be
    main verb gets person according to agent
3.2 Else:
    main verb gets mode and tense from verb to be
    finite verb gets person according to agent
3.3 Remove verb to be
4 Transform passive agent into a new subject
Table 1: Algorithm for default and verb cases
3.3 Subordination
Types of subordinate clauses are presented in Table 
2. Two clauses are not processed: comparative and 
proportional.  Comparative  and  proportional 
clauses will be addressed in future work.
id Clause type Processed
d Relative Restrictive ?
e Relative  Non-restrictive ?
f Reason ?
g Comparative  
h Concessive ?
i Conditional ?
j Result ?
k Confirmative ?
l Final Purpose ?
m Time ?
w Proportional
Table 2: Subordinate clauses
Specific  rules  are  used  for  groups  of  related 
subordinate cases. At least one of two operations 
can  be  found  in  all  rules:  component  reordering 
and sentence splitting. Below, letter codes are used 
to  describe  rules  involving  these  two  and  other 
common operations:
A additional processing
M splitting-order main-subordinate 
P Also processes non-clause phrases and/or non-
finite clauses
R component reordering 
S splitting-order subordinate-main 
c clone subject or turn object of a clause into 
subject in another if it is necessary
d marker deletion
m marker replacement
v verb reinflection
[na] not simplified due ambiguity
[nf] not simplified, future case
[np] not simplified due parsing problems
2...8 covered cases (when more than one applies)
Table  3 presents the marker information. They 
are used to select sentences for simplification, and 
several  of  them  are  replaced  by  easier  markers. 
Cases  themselves  are  not  detailed since they are 
too  numerous  (more  than  40  distinct  cases). 
Operation  codes  used  for  each  marker  are 
described in column ?Op?. It is important to notice 
that  multi-lexeme  markers  also  face  ambiguities 
due to co-occurrence of its component lexemes12. 
The  list  does  not  cover  all  possible  cases,  since 
there  may  be  additional  cases  not  seen  in  the 
corpus. As relative clauses (d and e) require almost 
the same processing, they are grouped together.
Several  clauses  require  additional  processing. 
For  example,  some  conditional  clauses  require 
negating the main clause. Other examples include 
noun phrases replacing clause markers and clause 
reordering, both for relative clauses, as showed in 
(5). The marker  cujo (whose) in the example can 
refer to Northbridge or to the building. Additional 
processing  is  performed  to  try  to  solve  this 
anaphora13,  mostly  using  number  agreement 
between the each possible co-referent and the main 
verb in the subordinate clause. The simplification 
engine can give up in ambiguous cases (focusing 
on  precision)  or  elect  a  coreferent  (focusing  on 
recall),  depending  on  the  number  of  possible 
coreferents  and  on  a  confidence  threshold 
parameter, which was not used in this paper.
O: Ele[He] deve[should] visitar[visit] o[the] 
pr?dio[building] em[in] Northbridge 
cujo[whose] desabamento[landslide] 
matou[killed] 16 pessoas[people].
(5)S: Ele[He] deve[should] visitar[visit] o[the] 
pr?dio[building] em[in] Northbridge. O[The] 
desabamento[landslide] do[of  the] 
pr?dio[building] em[in] Northbridge 
matou[killed] 16 pessoas[people].
12 For example, words ?de?, ?sorte? and ?que? can be 
adjacent  to each other without the meaning of ?de sorte 
que? marker (?so that?).
13 We opted to solve this kind of anaphora instead of using 
pronoun insertion in order to facilitate the reading of the 
text.
141
3.4 Evaluation in the development corpus
Figure 1 provides statistics from the of processing 
all  identified  cases  in  the  development  corpus. 
These statistics cover number of cases rather than 
the  number  of  sentences  containing  cases.  The 
cases  ?incorrect  selection?  and  ?incorrect 
simplification?  affect  precision  by  generating 
ungrammatical  sentences.  The  former  refers  to 
sentences  that  should  not  be  selected  for  the 
simplification  process,  while  the  latter  refers  to 
sentences  correctly  selected  but  wrongly 
simplified.  There  are  three  categories  affecting 
recall, classified according to their priority in the 
simplification  engine.  Pending cases  are 
considered  to  be  representative,  with  higher 
priority.  Possible cases  are  considered  to  be 
unrepresentative. Having less priority, they can be 
handled in future versions of the engine.  Finally, 
Skipped cases  will  not  be  implemented,  mainly 
because  of  ambiguity,  but  also  due  to  low 
representativeness.  It  is  possible  to  observe  that 
categories  reducing  precision  (incorrect  selection 
and simplification) represent a smaller number of 
cases (5%) than categories reducing recall (45%). 
It  is  worth  noticing  that  our  approach  focus  on 
precision  in  order  to  make  the  simplification  as 
automatic  as  possible,  minimizing  the  need  for 
human interaction.
Figure 1: Performance on the development 
corpus
There are some important remarks regarding the 
development corpus used during the programming 
phase.  First,  some  cases  are  not  representative, 
therefore  the  results  are  expected  to  vary 
significantly in real texts. Second, a few cases are 
not orthogonal: i.e.,  there are sentences that can be 
classified  in  more  than  one  case.  Third,  several 
errors  refer  to  sub-cases  of  cases  being  mostly 
correctly  processed,  which are  expected to occur 
less frequently. Fourth, incorrect parsed sentences 
were not take in account in this phase. Although 
there may exist other cases not identified yet, it is 
plausible to estimate that only 5% of known cases 
are affecting the precision negatively.
id Marker Op id Marker Op id Markers Op
de que [that/which] 8MRAdv h se bem que [albeit] Mmv j tanto ? que [so ? that] [nf]
de o qual [which]* 8MRAdv h ainda que [even if] 2Mm j tal ? que [such ? that] [nf]
de como [as] [na] h mesmo que [even if] 2Mm j tamanho ? que [so ? that]* [nf]
de onde [where] [nf] h nem que [even if] 2Mm k conforme [as/according] 3PRAcm
de quando [when] [na] h por mais que [whatever] 2Mm k consoante [as/according] 3PRAcm
de quem [who/whom] [nf] h mas [but] [np] k segundo [as/according] 3PRAcm
de quanto [how much] [nf] i contanto que [provided that] 2Rmv k como [as] [na]
de cujo [whose]* MAd i caso [case] 2Rmv l a fim de [in order to] 2PMcm
de o que [what/which] Sd i se [if/whether] 2Rmv l a fim de que [in order that] 2PMcm
f j? que [since] Scm i a menos que [unless] 2RAmv l para que [so that] 2PMcm
f porquanto [in view of] Scm i a n?o ser que [unless] 2RAmv l porque [because] [na]
f uma vez que [since] Scm i exceto se [unless] 2RAmv m assim que [as soon as] 5PMAcvr
f visto que [since] Scm i salvo se [unless] 2RAmv m depois de [after] 5PMAcvr
f como [for] [na] i antes que [before] Rmv m depois que [after] 5PMAcvr
f porque [because] [na] i sem que [without] Rmv m logo que [once] 5PMAcvr
f posto que [since] [na] i desde que [since] RAmv m antes que [before] PSAcvr
f visto como [seen as] [na] j de forma que [so] 5Mmv m apenas [only] [na]
f pois que [since] [nf] j de modo que [so] 5Mmv m at? que [until] [na]
h apesar de que [although] Mmv j de sorte que [so that] 5Mmv m desde que [since] [na]
h apesar que [despite] Mmv j tamanho que [so that]* 5Mmv m cada vez que [every time] [nf]
h conquanto [although] Mmv j tal que [such that] 5Mmv m sempre que [whenever] [nf]
h embora [albeit] Mmv j tanto que [so that] (1)* [na] m enquanto [while] [nf]
h posto que [since] Mmv j tanto que [so that] (2) [na] m mal [just] [na]
h por muito que [although] Mmv j t?o ? que [so ? that] [nf] m quando [when] [na]
* gender and/or number variation
Table 3: Marker processing
correct 45%
incorrect
simplification 4% incorrectselection 1%
pending 17%
possible 7%
skipped 25%
142
4 Engine evaluation
4.1 Evaluation patterns
The  evaluation  was  performed  on  a  sample  of 
sentences  extracted  from Wikipedia's  texts  using 
lexical patterns. These patterns allows to filter the 
texts,  extracting  only  relevant  sentences  for 
precision and recall evaluation. They were created 
to  cover  both  positive  and  negative  sentences. 
They are applied before parsing or Part of Speech 
(PoS)  analysis.  For  passive  voice  detection,  the 
pattern is  defined as a sequence of  two or  more 
possible verbs (no PoS in use) in which at least one 
of them could be an inflection of verb to be. For 
subordination detection, the pattern is equivalent to 
the  discourse  markers  associated  with  each 
subordination type, as shown in Table 3. 
The  patterns  were  applied  against  featured 
articles  appearing  in  Wikipedia's  front  page  in 
2010 and 2011, including featured articles planned 
to be featured, but not featured yet. A maximum of 
30 sentences resulting from each pattern matching 
were then submitted to the simplification engine. 
Table 4 presents statistics from featured articles. 
texts 165
sentences 83,656
words 1,226,880
applied patterns 57,735
matched sentences 31,080
Table 4: Wikipedia's featured articles (2010/2011)
The number of applied patterns represents both 
patterns to be simplified (s-patterns) and patterns 
not  to  be  simplified  (n-patterns).  N-patterns 
represent both non-processable patterns due to high 
ambiguity (a-patterns) and pattern extraction false 
negatives. We observed a few, but very frequent, 
ambiguous patterns introducing noise, particularly 
se and  como.  In  fact,  these  two  markers  are  so 
noisy that  we were not  be able  to  provide good 
estimations  on  their  true  positives  distribution 
given the 30 sentences limit per pattern. Similarly 
to the number of applied patterns, the number of 
matched sentences correspond to both sentences to 
be simplified and not to be simplified.
Table  5 presents  additional  statistics  about 
characters,  words  and  sentences  calculated  in  a 
sample of 32 articles where the 12 domains of the 
Portuguese Wikipedia are balanced. The number of 
automatic simplified sentence is also presented. In 
Table  5,  simple  words refers  to  percentage  of 
words  which  are  listed  on  our  simple  word  list, 
supposed to be common to youngsters,  extracted 
from the dictionary described in (Biderman, 2005), 
containing 5,900 entries.  Figure 2 presents clause 
distribution per sentence in  the balanced sample. 
Zero  clauses refers  to  titles,  references,  figure 
labels, and other pieces of text without a verb. We 
observed  60%  of  multi-clause  sentences  in  the 
sample.
characters per word 5.22
words per sentence 21.17
words per text 8,476
simple words 75.52%
sentences per text 400.34
passive voice 15.11%
total sentences 13,091
simplified sentences 16,71%
Table 5: Statistics from the balanced text sample
Figure 2: Clauses per sentence in the sample
4.2 Simplification analysis
We manually analysed and annotated all sentences 
in  our  samples.  These  samples  were  used  to 
estimate  several statistics, including the number of 
patterns  per  million  words,  the  system precision 
and  recall  and  the  noise  rate.  We  opted  for 
analysing  simplified  patterns  per  million  words 
instead of  per simplified sentences. First, because 
an analysis based on sentences can be misleading, 
since  there  are  cases  of  long  coordinations  with 
many patterns, as well as succinct sentences with 
no patterns.  Moreover,  one incorrectly  simplified 
marker in a sentence could hide useful statistics of 
correctly  simplified  patterns  and  even  of  other 
incorrectly simplified patterns. 
The samples are composed by s-patterns and n-
patterns  (including  a-patterns).  In  total  1,243 
patterns were annotated.  Table  6 presents pattern 
estimates per million words. 
0 1 2 3 4 5 6 7 8
0,0000
0,0500
0,1000
0,1500
0,2000
0,2500
0,3000
clauses
dis
trib
uti
on
 [0
-1]
>7
143
Total patterns 70,834
Human s-patterns 33,906
Selection s-patterns 27,714
Perfect parser s-patterns 23,969
Obtained s-patterns 22,222
Table 6: Patterns per million words
Total patterns refers to the expected occurrences 
of  s-patterns  and  n-patterns  in  a  corpus  of  one 
million  words.  This  is  the  only  information 
extracted from the full corpus, while the remaining 
figures are estimates from the sample corpus.
Human s-patterns is an estimate of the number 
patterns that a human could simplify in the corpus. 
Unlike  other  s-pattern  estimates,  a-patterns  are 
included, since a human can disambiguate them. In 
other words, this is the total of positive patterns. 
The estimate  does  not  include very rare  (sample 
size equals to zero) or very noisy markers (patterns 
presenting 30 noisy sentences in its sample).
Selection  s-patterns are  an  estimate  of  the 
number  of  patterns  correctly  selected  for 
simplification,  regardless  of  whether  the  pattern 
simplification is correct or incorrect.  Precision and 
recall derived from this measure (Table 7) consider 
incorrectly simplified patterns,  and do not include 
patterns with parsing problems.  Its  purpose is  to 
evaluate how well the selection for simplification 
is performed. Rare or noisy patterns, whose human 
s-patterns  per  sample  is  lower  than  7,  are  not 
included.
Perfect  parser  s-patterns is  an  estimate  very 
similar to selection s-patterns, but considering only 
correctly  simplified  patterns.  As  in  selection  s-
patterns,  incorrect  parsed  sentences  are  not 
included in calculations. This is useful to analyse 
incorrect simplifications due to simplification rule 
problems, ignoring errors originating from parsing.
Finally,  obtained  s-patterns refers  to  the 
estimate of correct simplified patterns,  similar to 
perfect  parser  s-patterns,  but  including 
simplification  problems  caused  by  parsing.  This 
estimate  represents  the  real  performance  to  be 
expected from the system on Wikipedia's texts.
It is important to note that the real numbers of 
selection  s-patterns,  perfect  s-patterns  and 
obtained s-patterns  is expected to be bigger than 
the estimates,  since noisy and rare  pattern could 
not used be used in calculations (due the threshold 
of  7  human  s-patterns  per  sample).  The  data 
presented on Table 6 is calculated using estimated 
local precisions for each pattern. Table  7 presents 
global  precision,  recall  and  f-measure  related  to 
selection,  perfect  parser  and  obtained s-patterns. 
The  real  values  of  the  estimates  are  expected to 
variate up to +/- 2.48% .
Measures Precision Recall F-measure
Selection 99.05% 82.24% 89.86%
Perfect parser 85.66% 82.24% 83.92%
Obtained 79.42% 75.09% 77.20%
Table 7: Global estimated measures
Although the precision of the selection seems to 
be  impressive,  this  result  is  expected,  since  our 
approach  focus  on  the  processing  of  mostly 
unambiguous  markers,  with  sufficient  syntactic 
information. It is also due to the the threshold of 7 
human s-patterns  and the fact  that  a-patterns  are 
not  included.  Due to  these two restrictions,  only 
approximately 31.5% of unique patterns could be 
used for the calculations in Table  7. Interestingly, 
these unique patterns correspond to 82.5% of the 
total estimated human s-patterns. The majority of 
the 17.5%  remaining s-patterns refers to patterns 
too  noisy  to  be  analysed  and  to  a-patterns  (not 
processed  due  ambiguity),  and  also  others  n-
patterns which presented a low representativeness 
in  the  corpus.  The  results  indicate  good 
performance in rule formulation, covering the most 
important (and non-ambiguous) markers, which is 
also confirmed by the ratio between both selection 
s-patterns  and  human  s-patterns  previously 
presented on Table 6. 
An  alternative  analysis,  including  a-patterns, 
lowers recall and f-measure, but not precision (our 
focus in this work). In this case, recall drops from 
75.09% to  62.18%,  while  f-measure  drops  from 
77.20% to 70.18%.
Figure 3: Pattern distribution
Figure  3 presents  the  distribution  of  patterns 
according to their frequency per million words and 
their purity (1 - noisy rate). This data is useful to 
2,0 20,0 200,0 2000,0 20000,0 200000,0
0,0000
0,2000
0,4000
0,6000
0,8000
1,0000
1,2000
b-passiva
de-que
l-a_fim_de
j-tal_que
J-tanto_*_que
Frequency PMW
Pu
rity
144
identify  most  frequent  patterns  (such  as  passive 
voice in  b-passiva)  and patterns with medium to 
high  frequency,  which  are  easy  to  process  (not 
ambiguous), such as l-a_fim_de.
5 Issues on simplification quality
This analysis aims at identifying factors affecting 
the quality of simplifications considered as correct. 
Hence, factors affecting the overall simplified text 
quality  are  also  presented.  In  contrast,  the 
quantitative  analysis  presented  on  Section  4.2 
covered  the  ratio  between  incorrect  and  correct 
simplifications.
Three cases of clause disposition were identified 
as  important  factors  affecting  the  simplified 
sentence  readability.  These  cases  are  presented 
using  the  following  notation:  clauses  are 
represented  in  uppercase  letters;  clause 
concatenation represents coordination; parentheses 
represent  subordination;  c1 and  c2 represent 
clause/sentence  connectors  (including  markers); 
the  entailment  operator  (?)  represents  the 
simplification rule transforming clauses.
? ?A(B(c1 C)) ? A(B). c2 C?: the vertical case. 
In this scenario it is more natural to read c2 as 
connecting  C to the main clause  A, while  c1 
connects  C to  B,  as seen in (6). This is still 
acceptable for several sentences analysed, but 
we  are  considering to  simplify only level  2 
clauses in the future, splitting C  from B only 
if another rule splits A and B first.
? ?A(B)CD  ?  ACD.  c1 B?:  the  horizontal 
case. In this scenario, c1 correctly connects A 
and B, but long coordinations following A can 
impact  negatively on text  reading,  since the 
target  audience  may  forget  about  A when 
starting  to  read  B.  In  this  scenario, 
coordination  compromise  subordination 
simplification,  showing  the  importance  of 
simplifying coordination as well, even though 
they  are  considered  easier  to  read  than 
subordination.
? Mixed  case:  this  scenario  combines  the 
potential problems of horizontal and vertical 
cases.  It  may  occur  in  extremely  long 
sentences.
Besides  clause  disposition  factors,  clause 
inversions can also lead to  problems in sentence 
readability.  In  our  current  system,  inversion  is 
mainly used to produce simplified sentences in the 
cause-effect  order  or  condition-action  order. 
Reordering, despite using more natural orders, can 
transform  anaphors  into  cataphors.  A  good 
anaphora resolution system would be necessary to 
avoid  this  issue.  Another  problem  is  moving 
sentence connectors as in ?A. c1 BC. ? A. B. c2 c1 
C?,  while  ?A.  c1 B.  c2 C?  is  more  natural 
(maintaining c1 position). 
O: Ela[She] dissertou[talked] sobre[about] 
como[how] motivar[to motive] o[the] 
grupo[group] de_modo_que[so that] seu[their] 
desempenho[performance] melhore[improves] (6)S: [He/She] dissertou[talked] sobre[about] 
como[how] motivar[to motive] o[the] 
grupo[group]. Thus, seu[their] 
desempenho[performance] melhore[improves]
We  have  observed  some  errors  in  sentence 
parsing,  related  to  clause  attachment,  generating 
truncated ungrammatical text. As a result, a badly 
simplified key sentence can compromise the text 
readability more than several  correctly simplified 
sentences  can  improve  it,  reinforcing  the 
importance  of  precision  rather  than  recall  in 
automated text simplification.
Experienced  readers  analysed  the  simplified 
versions of the articles and considered them easier 
to read than the original ones in most cases, despite 
simplification  errors.  Particularly,  the  readers 
considered  that  the  readability  would  improve 
significantly  if  cataphor  and horizontal  problems 
were  addressed.  Evaluating  the  simplifications 
with readers from the target audience is left  as a 
future work, after  improvements in the identified 
issues.
6 Conclusions
We  have  presented  a  simplification  engine  to 
process texts from the Portuguese Wikipedia. Our 
quantitative  analysis  indicated  a  good  precision 
(79.42%),  and  reasonable  number  of  correct 
simplifications  per  million  words  (22,222). 
Although our focus was on the encyclopedic genre 
evaluation,  the  proposed  system  can  be  used  in 
other genres as well.
Acknowledgements
We thank FAPESP (p.  2008/08963-4)  and CNPq 
(p. 201407/2010-8) for supporting this work.
145
References
J.  Abedi,  S.  Leon,  J.  Kao,  R.  Bayley,  N.  Ewers,  J. 
Herman and K. Mundhenk. 2011. Accessible Reading 
Assessments for Students with Disabilities: The Role 
of  Cognitive,  Grammatical,  Lexical,  and 
Textual/Visual  Features.  CRESST  Report  785. 
National  Center  for  Research  on  Evaluation, 
Standards,  and  Student  Testing,  University  of 
California, Los Angeles.
S.  M.  Alu?sio,   C.  Gasperin.  2010.  Fostering  Digital 
Inclusion and Accessibility: The PorSimples project 
for Simplification of Portuguese Texts.  Proceedings 
of  the  NAACL  HLT  2010  Young  Investigators 
Workshop  on  Computational  Approaches  to 
Languages of the Americas. : ACL, New York, USA. 
v. 1. p. 46-53.
A.  Barreiro,  L.  M.  Cabral.  2009.  ReEscreve:  a 
translator-friendly  multi-purpose  paraphrasing 
software  tool.  The  Proceedings  of  the  Workshop 
Beyond  Translation  Memories:  New  Tools  for 
Translators,  The  Twelfth  Machine  Translation 
Summit. Ontario, Canada, pp. 1-8. 
E.  Bick.  2006.   The  parsing  system  ?Palavras?: 
Automatic grammatical  analysis of  Portuguese in a 
constraint  grammar  framework.  Thesis  (PhD). 
University of ?rhus, Aarhus, Denmark.
M.  T.  C.  Biderman.   2005.  Dicion?rio  Ilustrado  de 
Portugu?s. Editora ?tica. 1a. ed. S?o Paulo 
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin 
and  J.  Tait.  1999.  Simplifying  Text  for  Language-
Impaired  Readers,.  In  Proceedings  of  the  9th 
Conference  of  the  European  Chapter  of  the 
Association for Computational  Linguistics  (EACL), 
269-270.
R.  Chandrasekar  and  B.  Srinivas.  1997.  Automatic 
Induction  of  Rules  for  Text  Simplification. 
Knowledge-Based Systems, 10, 183-190.
G. E. Chappell.  1985. Description and assessment of 
language disabilities of junior high school students. 
In:  Communication  skills  and  classroom  success: 
Assessment  of  language-learning  disabled  students. 
College- Hill Press,  San Diego,  pp. 207-239.
W.  Daelemans,  A.  Hothker  and  E.  T.  K.  Sang.  2004. 
Automatic Sentence Simplification for Subtitling in 
Dutch  and  English.  In:  Proceedings  of  the  4th 
Conference on Language Resources and Evaluation, 
Lisbon, Portugal 1045-1048.
J. De Belder and M. Moens. 2010.  Text simplification 
for children. Proceedings of the SIGIR Workshop on 
Accessible Search Systems, pp.19-26.
S.  Devlin  and  G.  Unthank.  2006.  Helping  aphasic 
people process online information. In: Proceedings of 
the  ACM  SIGACCESS  2006,  Conference  on 
Computers  and  Accessibility.  Portland,  Oregon, 
USA , 225-226.
K. Inui, A. Fujita, T. Takahashi, R. Iida and T. Iwakura. 
2003.  Text Simplification for Reading Assistance: A 
Project  Note.  In  the  Proceedings  of  the  Second 
International Workshop on Paraphrasing, 9-16.
S.  Jonnalagadda  and  G.  Gonzalez.  2009.  Sentence 
Simplification  Aids  Protein-Protein  Interaction 
Extraction.  Proceedings  of  the  3rd  International 
Symposium on Languages in Biology and Medicine, 
Short  Papers,  pages  109-114,  Jeju  Island,  South 
Korea, 8-10 November 2009.
F.  W.  Jones,  K.  Long  and  W.  M.  L.  Finlay.  2006. 
Assessing the reading comprehension of adults with 
learning disabilities. Journal of Intellectual Disability 
Research, 50(6), 410-418. 
B.  Klebanov,  K.  Knight  and  D.  Marcu.  2004.  Text 
Simplification for Information-Seeking Applications. 
In:  On  the  Move  to  Meaningful  Internet  Systems. 
Volume  3290,  Springer-Verlag,  Berlin  Heidelberg 
New York, 735-747.
S. Martins, L. Filgueiras. 2007. M?todos de Avalia??o 
de Apreensibilidade das Informa??es Textuais:  uma 
Aplica??o  em  S?tios  de  Governo  Eletr?nico.  In 
proceeding  of  Latin  American  Conference  on 
Human-Computer Interaction (CLIHC 2007). Rio de 
Janeiro, Brazil.
A. Max. 2006. Writing for Language-impaired Readers. 
In: Proceedings of Seventh International Conference 
on  Intelligent  Text  Processing  and  Computational 
Linguistics. Mexico City, Mexico. Berlin Heidelberg 
New York, Springer-Verlag, 567-570.
C.  Napoles  and  M.  Dredze.  2010.  Learning  simple 
Wikipedia:  a  cogitation  in  ascertaining  abecedarian 
language. In the  Proceedings of the NAACL HLT 
2010  Workshop  on  Computational  Linguistics  and 
Writing:  Writing  Processes  and  Authoring  Aids 
(CL&W '10), 42-50.
S.  E.  Petersen.  2007.  Natural  Language  Processing 
Tools  for  Reading  Level  Assessment  and  Text 
Simplification for  Bilingual  Education.  PhD thesis. 
University of Washington.
A. Siddharthan. 2006. Syntactic simplification and text 
cohesion.  Research  on  Language  &  Computation, 
4(1):77-109.
L.  Specia.   2010.  Translating  from  Complex  to 
Simplified  Sentences.  9th  International  Conference 
146
on  Computational  Processing  of  the  Portuguese 
Language.  Lecture  Notes  in  Artificial  Intelligence, 
Vol. 6001, Springer, pp. 30-39.
D. Vickrey and D. Koller. 2008. Sentence Simplification 
for Semantic Role Labelling. In: Proceedings of the 
ACL-HLT. 344-352.
W. M. Watanabe,  A. Candido Jr, V. R. Uzeda, R. P. M. 
Fortes,  T.  A.  S.  Pardo  and  S.  M.  Alu?sio.  2009. 
Facilita:  Reading  Assistance  for  Low-literacy 
Readers.  In:  ACM  International  Conference  on 
Design of Communication (SIGDOC 2009), volume 
1, Bloomington, US,   29-36. 
147
