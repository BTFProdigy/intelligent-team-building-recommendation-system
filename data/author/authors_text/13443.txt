Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 1?7,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
NoWaC: a large web-based corpus for Norwegian
Emiliano Guevara
Tekstlab,
Institute for Linguistics and Nordic Studies,
University of Oslo
e.r.guevara@iln.uio.no
Abstract
In this paper we introduce the first version
of noWaC, a large web-based corpus of Bok-
m?l Norwegian currently containing about
700 million tokens. The corpus has been
built by crawling, downloading and process-
ing web documents in the .no top-level in-
ternet domain. The procedure used to col-
lect the noWaC corpus is largely based on
the techniques described by Ferraresi et al
(2008). In brief, first a set of ?seed? URLs
containing documents in the target language
is collected by sending queries to commer-
cial search engines (Google and Yahoo). The
obtained seeds (overall 6900 URLs) are then
used to start a crawling job using the Heritrix
web-crawler limited to the .no domain. The
downloaded documents are then processed in
various ways in order to build a linguistic cor-
pus (e.g. filtering by document size, language
identification, duplicate and near duplicate de-
tection, etc.).
1 Introduction and motivations
The development, training and testing of NLP tools
requires suitable electronic sources of linguistic data
(corpora, lexica, treebanks, ontological databases,
etc.), which demand a great deal of work in or-
der to be built and are, very often copyright pro-
tected. Furthermore, the ever growing importance of
heavily data-intensive NLP techniques for strategic
tasks such as machine translation and information
retrieval, has created the additional requirement that
these electronic resources be very large and general
in scope.
Since most of the current work in NLP is carried
out with data from the economically most impact-
ing languages (and especially with English data),
an amazing wealth of tools and resources is avail-
able for them. However, researchers interested in
?smaller? languages (whether by the number of
speakers or by their market relevance in the NLP
industry) must struggle to transfer and adapt the
available technologies because the suitable sources
of data are lacking. Using the web as corpus is a
promising option for the latter case, since it can pro-
vide with reasonably large and reliable amounts of
data in a relatively short time and with a very low
production cost.
In this paper we present the first version of
noWaC, a large web-based corpus of Bokm?l Nor-
wegian, a language with a limited web presence,
built by crawling the .no internet top level domain.
The computational procedure used to collect the
noWaC corpus is by and large based on the tech-
niques described by Ferraresi et al (2008). Our
initiative was originally aimed at collecting a 1.5?2
billion word general-purpose corpus comparable to
the corpora made available by the WaCky initiative
(http://wacky.sslmit.unibo.it). How-
ever, carrying out this project on a language with a
relatively small online presence such as Bokm?l has
lead to results which differ from previously reported
similar projects. In its current, first version, noWaC
contains about 700 million tokens.
1
1.1 Norwegian: linguistic situation and
available corpora
Norway is a country with a population of ca. 4.8 mil-
lion inhabitants that has two official national written
standards: Bokm?l and Nynorsk (respectively, ?book
language? and ?new Norwegian?). Of the two stan-
dards, Bokm?l is the most widely used, being ac-
tively written by about 85% of the country?s popu-
lation (cf. http://www.sprakrad.no/ for de-
tailed up to date statistics). The two written stan-
dards are extremely similar, especially from the
point of view of their orthography. In addition, Nor-
way recognizes a number of regional minority lan-
guages (the largest of which, North Sami, has ca.
15,000 speakers).
While the written language is generally standard-
ized, the spoken language in Norway is not, and
using one?s dialect in any occasion is tolerated and
even encouraged. This tolerance is rapidly extend-
ing to informal writing, especially in modern means
of communication and media such as internet fo-
rums, social networks, etc.
There is a fairly large number of corpora of the
Norwegian language, both spoken and written (in
both standards). However, most of them are of a
limited size (under 50 million words, cf. http://
www.hf.uio.no/tekstlab/ for an overview).
To our knowledge, the largest existing written cor-
pus of Norwegian is the Norsk Aviskorpus (Hofland
2000, cf. http://avis.uib.no/), an expand-
ing newspaper-based corpus currently containing
700 million words. However, the Norsk Aviskor-
pus is only available though a dedicated web inter-
face for non commercial use, and advanced research
tasks cannot be freely carried out on its contents.
Even though we have only worked on building a
web corpus for Bokm?l Norwegian, we intend to
apply the same procedures to create web-corpora
also for Nynorsk and North Sami, thus covering the
whole spectrum of written languages in Norway.
1.2 Obtaining legal clearance
The legal status of openly accessible web-
documents is not clear. In practice, when one
visits a web page with a browsing program, an
electronic exact copy of the remote document is
created locally; this logically implies that any online
document must be, at least to a certain extent,
copyright-free if it is to be visited/viewed at all.
This is a major difference with respect to other types
of documents (e.g. printed materials, films, music
records) which cannot be copied at all.
However, when building a web corpus, we do not
only wish to visit (i.e. download) web documents,
but we would like to process them in various ways,
index them and, finally, make them available to other
researchers and users in general. All of this would
ideally require clearance from the copyright holders
of each single document in the corpus, something
which is simply impossible to realize for corpora
that contain millions of different documents.1
In short, web corpora are, from the legal point of
view, still a very dark spot in the field of computa-
tional linguistics. In most countries, there is simply
no legal background to refer to, and the internet is a
sort of no-man?s land.
Norway is a special case: while the law explicitly
protects online content as intellectual property,
there is rather new piece of legislation in Forskrift
til ?ndsverkloven av 21.12 2001 nr. 1563, ? 1-4
that allows universities and other research insti-
tutions to ask for permission from the Ministry
of Culture and Church in order to use copyright
protected documents for research purposes that
do not cause conflict with the right holders?
own use or their economic interests (cf. http:
//www.lovdata.no/cgi-wift/ldles?
ltdoc=/for/ff-20011221-1563.html).
We have been officially granted this permission for
this project, and we can proudly say that noWaC
is a totally legal and recognized initiative. The
results of this work will be legally made available
free of charge for research (i.e. non commercial)
purposes. NoWaC will be distributed in association
with the WaCky initiative and also directly from the
University of Oslo.
1Search engines are in a clear contradiction to the copyright
policies in most countries: they crawl, download and index bil-
lions of documents with no clearance whatsoever, and also re-
distribute whole copies of the cached documents.
2
2 Building a corpus of Bokm?l by
web-crawling
2.1 Methods and tools
In this project we decided to follow the methods
used to build the WaCky corpora, and to use the re-
lated tools as much as possible (e.g. the BootCaT
tools). In particular, we tried to reproduce the pro-
cedures described by Ferraresi et al (2008) and Ba-
roni et al (2009). The methodology has already
produced web-corpora ranging from 1.7 to 2.6 bil-
lion tokens (German, Italian, British English). How-
ever, most of the steps needed some adaptation, fine-
tuning and some extra programming. In particu-
lar, given the relatively complex linguistic situation
in Norway, a step dedicated to document language
identification was added.
In short, the building and processing chain used
for noWaC comprises the following steps:
1. Extraction of list of mid-frequency Bokm?l
words from Wikipedia and building query
strings
2. Retrieval of seed URLs from search engines by
sending automated queries, limited to the .no
top-level domain
3. Crawling the web using the seed URLS, limited
to the .no top-level domain
4. Removing HTML boilerplate and filtering doc-
uments by size
5. Removing duplicate and near-duplicate docu-
ments
6. Language identification and filtering
7. Tokenisation
8. POS-tagging
At the time of writing, the first version of noWaC is
being POS-tagged and will be made available in the
course of the next weeks.
2.2 Retrieving seed URLs from search engines
We started by obtaining the Wikipedia text dumps
for Bokm?l Norwegian and related languages
(Nynorsk, Danish, Swedish and Icelandic) and se-
lecting the 2000 most frequent words that are unique
to Bokm?l. We then sent queries of 2 randomly
selected Bokm?l words though search engine APIs
(Google and Yahoo!). A maximum of ten seed
URLs were saved for each query, and the retrieved
URLs were collapsed in a single list of root URLs,
deduplicated and filtered, only keeping those in the
.no top level domain.
After one week of automated queries (limited
to 1000 queries per day per search engine by the
respective APIs) we had about 6900 filtered seed
URLs.
2.3 Crawling
We used the Heritrix open-source, web-scale
crawler (http://crawler.archive.org/)
seeded with the 6900 URLs we obtained to tra-
verse the internet .no domain and to download
only HTML documents (all other document types
were discarded from the archive). We instructed
the crawler to use a multi-threaded breadth-first
strategy, and to follow a very strict politeness policy,
respecting all robots.txt exclusion directives
while downloading pages at a moderate rate (90
second pause before retrying any URL) in order not
to disrupt normal website activity.
The final crawling job was stopped after 15 days.
In this period of time, a total size of 1 terabyte
was crawled, with approximately 90 million URLs
being processed by the crawler. Circa 17 million
HTML documents were downloaded, adding up to
an overall archive size of 550 gigabytes. Only
about 13.5 million documents were successfully re-
trieved pages (the rest consisting of various ?page
not found? replies and other server-side error mes-
sages).
The documents in the archive were filtered by
size, keeping only those documents that were be-
tween 5Kb and 200Kb in size (following Ferraresi
et al 2008 and Baroni et al 2009). This resulted
in a reduced archive of 11.4 million documents for
post-processing.
2.4 Post-processing: removing HTML
boilerplate and de-duplication
At this point of the process, the archive contained
raw HTML documents, still very far from being a
linguistic corpus. We used the BootCaT toolkit (Ba-
roni and Bernardini 2004, cf. http://sslmit.
unibo.it/~baroni/bootcat.html) to per-
form the major operations to clean our archive.
First, every document was processed with the
HTML boilerplate removal tool in order to select
3
only the linguistically interesting portions of text
while removing all HTML, Javascript and CSS code
and non-linguistic material (made mainly of HTML
tags, visual formatting, tables, navigation links, etc.)
Then, the archive was processed with the dupli-
cate and near-duplicate detecting script in the the
BootCaT toolkit, based on a 5-gram model. This is
a very drastic strategy leading to a huge reduction in
the number of kept documents: any two documents
sharing more than 1/25 5-grams were considered du-
plicates, and both documents were discarded. The
overall number of documents in the archive went
down from 11.40 to 1.17 million after duplicate re-
moval.2
2.5 Language identification and filtering
The complex linguistic situation in Norway makes
us expect that the Norwegian internet be at least a
bilingual domain (Bokm?l and Nynorsk). In addi-
tion, we also expect a number of other languages to
be present to a lesser degree.
We used Damir Cavar?s tri-gram algorithm
for language identification (cf. http://ling.
unizd.hr/~dcavar/LID/), training 16 lan-
guage models onWikipedia text from languages that
are closely related to, or that have contact with Bok-
m?l (Bokm?l, Danish, Dutch, English, Faeroese,
Finnish, French, German, Icelandic, Italian, North-
ern Sami, Nynorsk, Polish, Russian, Spanish and
Swedish). The best models were trained on 1Mb
of random Wikipedia lines and evaluated against a
database of one hundred 5 Kb article excerpts for
each language. The models performed very well,
often approaching 100% accuracy; however, the ex-
tremely similar orthography of Bokm?l and Nynorsk
make them the most difficult pair of languages to
spot for the system, one being often misclassified as
the other. In any case, our results were relatively
good: Bokm?l Precision = 1.00, Recall = 0.89, F-
measure = 0.94, Nynorsk Precision = 0.90, Recall =
1.00, F-measure = 0.95.
The language identifying filter was applied on a
document basis, recognizing about 3 out of 4 docu-
2As pointed out by an anonymous reviewer, this drastic re-
duction in number of documents may be due to faults in the
boilerplate removal phase, leading to 5-grams of HTML or sim-
ilar code counting as real text. We are aware of this issue, and
the future versions of noWaC will be revised to this effect.
ments as Bokm?l:
? 72.25% Bokm?l
? 16.00% Nynorsk
? 05.80% English
? 02.43% Danish
? 01.95% Swedish
This filter produced another sensible drop in the
overall number of kept documents: from 1.17 to 0.85
million.
2.6 POS-tagging and lemmatization
At the time of writing noWaC is in the process of be-
ing POS-tagged. This is not at all an easy task, since
the best and most widely used tagger for Norwe-
gian (the Oslo-Bergen tagger, cf. Hagen et al 2000)
is available as a binary distribution which, besides
not being open to modifications, is fairly slow and
does not handle large text files. A number of statisti-
cal taggers have been trained, but we are still unde-
cided about which system to use because the avail-
able training materials for Bokm?l are rather lim-
ited (about 120,000 words). The tagging accuracy
we have obtained so far is still not comparable to
the state-of-the-art (94.32% with TnT, 94.40% with
SVMT). In addition, we are also working on creat-
ing a large list of tagged lemmas to be used with
noWaC. We estimate that a final POS-tagged and
lemmatized version of the corpus will be available
in the next few weeks (in any case, before the WAC6
workshop).
3 Comparing results
While it is still too early for us to carry out a
fully fledged qualitative evaluation of noWaC, we
are able to compare our results with previous pub-
lished work, especially with the WaCky corpora we
tried to emulate.
3.1 NoWaC and the WaCky corpora
As we stated above, we tried to follow the WaCky
methodology as closely as possible, in the hopes that
we could obtain a very large corpus (we aimed at
collecting above 1 billion tokens). However, even
though our crawling job produced a much bigger
initial archive than those reported for German, Ital-
ian and British English in Baroni et al (2009), and
4
even though after document size filtering was ap-
plied our archive contained roughly twice as many
documents, our final figures (number of tokens and
number of documents) only amount to about half the
size reported for the WaCky corpora (cf. table 1).
In particular, we observe that the most significant
drop in size and in number of documents took place
during the detection of duplicate and near-duplicate
documents (drastically dropping from 11.4 million
documents to 1.17 million documents after duplicate
filtering). This indicates that, even if a huge num-
ber of documents in Bokm?l Norwegian are present
in the internet, a large portion of them must be
machine generated content containing repeated n-
grams that the duplicate removal tool successfully
identifies and discards.3
These figures, although unexpected by us, may
actually have a reasonable explanation. If we con-
sider that Bokm?l Norwegian has about 4.8 million
potential content authors (assuming that every Nor-
wegian inhabitant is able to produce web documents
in Bokm?l), and given that our final corpus contains
0.85 million documents, this means that we have so
far sampled roughly one document every five poten-
tial writers: as good as it may sound, it is a highly
unrealistic projection, and a great deal of noise and
possibly also machine generated content must still
be present in the corpus. The duplicate removal tools
are only helping us understand that a speaker com-
munity can only produce a limited amount of lin-
guistically relevant online content. We leave the in-
teresting task of estimating the size of this content
and its growth rate for further research. The Norwe-
gian case, being a relatively small but highly devel-
oped information society, might prove to be a good
starting point.
3.2 Scaling noWaC: how much Bokm?l is
there? How much did we get?
The question arises immediately. We want to know
how representative our corpus is, in spite of the fact
that we now know that it must still contain a great
deal of noise and that a great deal of documents were
plausibly not produced by human speakers.
To this effect, we applied the scaling factors
3Although we are aware that the process of duplicate re-
moval in noWaC must be refined further, constituting in itself
an interesting research area.
methodology used by Kilgarrif (2007) to estimate
the size of the Italian and German internet on the
basis of the WaCky corpora. The method consists
in comparing document hits for a sample of mid-
frequency words in Google and in our corpus before
and after duplicate removal. The method assumes
that Google does indeed apply duplicate removal to
some extent, though less drastically than we have.
Cf. table 2 for some example figures.
From this document hit comparison, two scaling
factors are extracted. The scaling ratio tells us how
much smaller our corpus is compared to the Google
index for Norwegian (including duplicates and non-
running-text). The duplicate ratio gives us an idea
of how much duplicated material was found in our
archive.
Since we do not know exactly how much dupli-
cate detection Google performs, we will multiply
the duplicate ratio by a weight of 0.1, 0.25 and 0.5
(these weights, in turn, assume that Google discards
10 times less, 4 times less and half what our dupli-
cate removal has done ? the latter hypothesis is used
by Kilgarriff 2007).
? Scaling ratio (average):
Google frq. / noWaC raw frq. = 24.9
? Duplicate ratio (average):
noWaC raw frq. / dedup. frq. = 7.8
We can then multiply the number of tokens in our
final cleaned corpus by the scaling ratio and by the
duplicate ratio (weighted) in order to obtain a rough
estimate of how much Norwegian text is contained
in the Google index. We can also estimate howmuch
of this amount is present in noWaC. Cf. table 3.
Using exactly the same procedure as Kilgarrif
(2007) leads us to conclude that noWaC should
contain over 15% of the Bokm?l text indexed by
Google. A much more restrictive estimate gives us
about 3%. More precise estimates are extremely
difficult to make, and these results should be taken
only as rough approximations. In any case, noWaC
certainly is a reasonably representative web-corpus
containing between 3% and 15% of all the currently
indexed online Bokm?l (Kilgarriff reports an esti-
mate of 3% for German and 7% for Italian in the
WaCky corpora).
5
deWaC itWaC ukWaC noWaC
N. of seed pairs 1,653 1,000 2,000 1,000
N. of seed URLs 8,626 5,231 6,528 6,891
Raw crawl size 398GB 379GB 351GB 550GB
Size after document size filter 20GB 19GB 19GB 22GB
N. of docs after document size filter 4.86M 4.43M 5.69M 11.4M
Size after near-duplicate filter 13GB 10GB 12GB 5GB
N. of docs after near-duplicate filter 1.75M 1.87M 2.69M 1.17M
N. of docs after lang-ID ? ? ? 0.85M
N. of tokens 1.27 Bn 1.58 Bn 1.91 Bn 0.69 Bn
N. of types 9.3M 3.6M 3.8M 6.0M
Table 1: Figure comparison of noWaC and the published WaCky corpora (German, Italian and British English data
from Baroni et al 2009)
Word Google frq. noWaC raw frq. noWaC dedup. frq.
bilavgifter 33700 1637 314
mekanikk 82900 3266 661
musikkpris 16700 570 171
Table 2: Sample of Google and noWaC document frequencies before and after duplicate removal.
noWaC Scaling ratio Dup. ratio (weight) Google estimate % in noWaC
0.78 (0.10) 21.8 bn 3.15%
0.69 bn 24.9 1.97 (0.25) 8.7 bn 7.89%
3.94 (0.50) 4.3 bn 15.79%
Table 3: Estimating the size of the Bokm?l Norwegian internet as indexed by Google in three different settings (method
from Kilgarriff 2007)
4 Concluding remarks
Building large web-corpora for languages with a
relatively small internet presence and with a lim-
ited speaker population presents problems and chal-
lenges that have not been found in previous work.
In particular, the amount of data that can be col-
lected with similar efforts is considerably smaller. In
our experience, following as closely as possible the
WaCky corpora methodology yielded a corpus that
is roughly between one half and one third the size of
the published comparable Italian, German and En-
glish corpora.
In any case, the experience has been very success-
ful so far, and the first version of the noWaC cor-
pus is about the same size than the largest currently
available corpus of Norwegian (i.e. Norske Avisko-
rpus, 700 million tokens), and it has been created in
just a minimal fraction of the time it took to build it.
Furthermore, the scaling experiments showed that
noWaC is a very representative web-corpus contain-
ing a significant portion of all the online content in
Bokm?l Norwegian, in spite of our extremely drastic
cleaning and filtering strategies.
There is clearly a great margin for improvement
in almost every processing step we applied in this
work. And there is clearly a lot to be done in or-
der to qualitatively assess the created corpus. In the
future, we intend to pursue this activity by carrying
out an even greater crawling job in order to obtain a
larger corpus, possibly containing over 1 billion to-
kens. Moreover, we shall reproduce this corpus cre-
ation process with the remaining two largest written
languages of Norway, Nynorsk and North Sami. All
of these resources will soon be publicly and freely
available both for the general public and for the re-
search community.
6
Acknowledgements
Building noWaC has been possible thanks to NO-
TUR advanced user support and assistance from the
Research Computing Services group (Vitenskapelig
Databehandling) at USIT, University of Oslo. Many
thanks are due to Eros Zanchetta (U. of Bologna),
Adriano Ferraresi (U. of Bologna) and Marco Ba-
roni (U. of Trento) and two anonymous reviewers
for their helpful comments and help.
References
M. Baroni and S. Bernardini. 2004. Bootcat: Bootstrap-
ping corpora and terms from the web. In Proceedings
of LREC 2004, pages 1313?1316, Lisbon. ELDA.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226, 09.
David Crystal. 2001. Language and the Internet. Cam-
bridge University Press, Cambridge.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini.
2008. Introducing and evaluating ukWaC, a very large
web-derived corpus of English. In Proceedings of the
WAC4 Workshop at LREC 2008.
R. Ghani, R. Jones, and D. Mladenic. 2001. Mining the
web to create minority language corpora. In Proceed-
ings of the tenth international conference on Informa-
tion and knowledge management, pages 279?286.
Johannessen J.B. N?klestad A. Hagen, K. 2000. A
constraint-based tagger for norwegian. Odense Work-
ing Papers in Language and Communication, 19(I).
Knut Hofland. 2000. A self-expanding corpus based on
newspapers on the web. In Proceedings of the Sec-
ond International Language Resources and Evaluation
Conference, Paris. European Language Resources As-
sociation.
Adam Kilgarriff and Marco Baroni, editors. 2006. Pro-
ceedings of the 2nd International Workshop on theWeb
as Corpus (EACL 2006 SIGWACWorkshop). Associa-
tion for Computational Linguistics, East Stroudsburg,
PA.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics, 29(3):333?348.
A. Kilgarriff. 2007. Googleology is bad science. Com-
putational Linguistics, 33(1):147?151.
S. Sharoff. 2005. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Cor-
pus Linguistics, (11):435?462.
7
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 33?37,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Regression Model of Adjective-Noun Compositionality
in Distributional Semantics
Emiliano Guevara
Tekstlab, ILN, University of Oslo
Oslo, Norway
e.r.guevara@iln.uio.no
Abstract
In this paper we explore the computational
modelling of compositionality in distri-
butional models of semantics. In par-
ticular, we model the semantic composi-
tion of pairs of adjacent English Adjec-
tives and Nouns from the British National
Corpus. We build a vector-based seman-
tic space from a lemmatised version of
the BNC, where the most frequent A-N
lemma pairs are treated as single tokens.
We then extrapolate three different mod-
els of compositionality: a simple additive
model, a pointwise-multiplicative model
and a Partial Least Squares Regression
(PLSR) model. We propose two evalu-
ation methods for the implemented mod-
els. Our study leads to the conclusion that
regression-based models of composition-
ality generally out-perform additive and
multiplicative approaches, and also show a
number of advantages that make them very
promising for future research.
1 Introduction
Word-space vector models or distributional mod-
els of semantics (henceforth DSMs), are com-
putational models that build contextual seman-
tic representations for lexical items from corpus
data. DSMs have been successfully used in the
recent years for a number of different computa-
tional tasks involving semantic relations between
words (e.g. synonym identification, computation
of semantic similarity, modelling selectional pref-
erences, etc., for a thorough discussion of the field,
cf. Sahlgren, 2006). The theoretical foundation of
DSMs is to be found in the ?distributional hypoth-
esis of meaning?, attributed to Z. Harris, which
maintains that meaning is susceptible to distribu-
tional analysis and, in particular, that differences
in meaning between words or morphemes in a
language correlate with differences in their distri-
bution (Harris 1970, pp. 784?787).
While the vector-based representation of word
meaning has been used for a long time in com-
putational linguistics, the techniques that are cur-
rently used have not seen much development with
regards to one of the main aspects of semantics in
natural language: compositionality.
To be fair, the study of semantic composition-
ality in DSMs has seen a slight revival in the re-
cent times, cf. Widdows (2008), Mitchell & La-
pata (2008), Giesbrecht (2009), Baroni & Lenci
(2009), who propose various DSM approaches
to represent argument structure, subject-verb and
verb-object co-selection. Current approaches to
compositionality in DSMs are based on the appli-
cation of a simple geometric operation on the basis
of individual vectors (vector addition, pointwise-
multiplication of corresponding dimensions, ten-
sor product) which should in principle approxi-
mate the composition of any two given vectors.
On the contrary, since the the very nature of
compositionality depends on the semantic rela-
tion being instantiated in a syntactic structure, we
propose that the composition of vector representa-
tions must be modelled as a relation-specific phe-
nomenon. In particular, we propose that the usual
procedures from machine learning tasks must be
implemented also in the search for semantic com-
positionality in DSM.
In this paper we present work in progress on
the computational modelling of compositionality
in a data-set of English Adjective-Noun pairs ex-
tracted from the BNC. We extrapolate three differ-
ent models of compositionality: a simple additive
model, a pointwise-multiplicative model and, fi-
nally, a multinomial multiple regression model by
Partial Least Squares Regression (PLSR).
33
2 Compositionality of meaning in DSMs
Previous work in the field has produced a small
number of operations to represent the composi-
tion of vectorial representations of word meaning.
In particular, given two independent vectors v1
and v2, the semantically compositional result v3
is modelled by:
? vector addition, the compositional meaning
of v3 consists of the sum of the independent
vectors for the constituent words:
v1i + v2i = v3i
? pointwise-multiplication (Mitchell and La-
pata 2008), each corresponding pair of com-
ponents of v1 and v2 are multiplied to obtain
the corresponding component of v3:
v1i ? v2i = v3i
? tensor product, v1 ? v2 = v3, where v3 is
a matrix whose ij-th entry is equal to v1iv2j
(cf. Widdows 2008, who also proposes the
related method of convolution product, both
imported from the field of quantum mechan-
ics)
In the DSM literature, the additive model has be-
come a de facto standard approach to approximate
the composed meaning of a group of words (or a
document) as the sum of their vectors (which re-
sults in the centroid of the starting vectors). This
has been successfully applied to document-based
applications such as the computation of document
similarity in information retrieval.
Mitchell & Lapata (2008) indicate that the var-
ious variations of the pointwise-multiplication
model perform better than simple additive mod-
els in term similarity tasks (variations included
combination with simple addition and adding
weights to individual vector components). Wid-
dows (2008) Obtain results indicating that both the
tensor product and the convolution product per-
form better than the simple additive model.
For the sake of simplifying the implementa-
tion of evaluation methods, in this paper we will
compare the first two approaches, vector addition
and vector pointwise-multiplication, with regres-
sion modelling by partial least squares.
3 Partial least squares regression of
compositionality
We assume that the composition of meaning in
DSMs is a function mapping two or more inde-
pendent vectors in a multidimensional space to a
newly composed vector the same space and, fur-
ther, we assume that semantic composition is de-
pendent on the syntactic structure being instanti-
ated in natural language.1
Assuming that each dimension in the starting
vectors v1 and v2 is a candidate predictor, and that
each dimension in the composed vector v3 is a de-
pendent variable, vector-based semantic composi-
tionality can be formulated as a problem of multi-
variate multiple regression. This is, in principle,
a tractable problem that can be solved by stan-
dard machine learning techniques such as multi-
layer perceptrons or support vector machines.
However, given that sequences of words tend to
be of very low frequency (and thus difficult to rep-
resent in a DSM), suitable data sets will inevitably
suffer the curse of dimensionality: we will often
have many more variables (dimensions) than ob-
servations.
Partial Least Squares Regression (PLSR) is a
multivariate regression technique that has been de-
signed specifically to tackle such situations with
high dimensionality and limited data. PLSR is
widely used in in unrelated fields such as spec-
troscopy, medical chemistry, brain-imaging and
marketing (Mevik & Wehrens, 2007).
4 Materials and tools
We use a general-purpose vector space extracted
from the British National Corpus. We used the
Infomap software to collect co-occurrence statis-
tics for lemmas within a rectangular 5L?5R win-
dow. The corpus was pre-processed to represent
frequent Adjective-Noun lemma pairs as a sin-
gle token (e.g. while in the original corpus the
A-N phrase nice house consists in two separate
lemmas (nice and house), in the processed cor-
pus it appears as a single entry nice_house). The
corpus was also processed by stop-word removal.
We extracted a list of A-N candidate pairs with
simple regex-based queries targeting adjacent se-
quences composed of [Det/Art?A?N] (e.g. that lit-
tle house). We filtered the candidate list by fre-
quency (> 400) obtaining 1,380 different A-N
pairs.
The vector space was built with the 40,000 most
frequent tokens in the corpus (a cut-off point that
included all the extracted A-N pairs). The origi-
nal dimensions were the 3,000 most frequent con-
1Mitchell & Lapata (2008) make very similar assumptions
to the ones adopted here.
34
tent words in the BNC. The vector space was
reduced to the first 500 ?latent? dimensions by
SVD as implemented by the Infomap software.
Thus, the resulting space consists in a matrix with
40, 000? 500 dimensions.
We then extracted the vector representation for
each A-N candidate as well as for each indepen-
dent constituent, e.g. vectors for nice_house (v3),
as well as for nice (v1) and house (v2) were saved.
The resulting vector subspace was imported into
the R statistical computing environment for the
subsequent model building and evaluation. In
particular, we produced our regression analysis
with the pls package (Mevik & Wehrens, 2007),
which implements PLSR and a number of very
useful functions for cross-validation, prediction,
error analysis, etc.
By simply combining the vector representations
of the independent Adjectives and Nouns in our
data-set (v1 and v2) we built an additive predic-
tion model (v1 + v2) and a simplified pointwise
multiplicative prediction model (v1? v2) for each
candidate pair.
We also fitted a PLSR model using v1 and v2
as predictors and the corresponding observed pair
v3 as dependent variable. The data were divided
into a training set (1,000 A-N pairs) and a testing
set (the remaining 380 A-N pairs). The model?s
parameters were estimated by performing 10-fold
cross-validation during the training phase.
In what follows we briefly evaluate the three re-
sulting models of compositionality.
5 Evaluation
In order to evaluate the three models of composi-
tionality that were built, we devised two different
procedures based on the Euclidean measure of ge-
ometric distance.
The first method draws a direct comparison of
the different predicted vectors for each candidate
A-N pair by computing the Euclidean distance be-
tween the observed vector and the modelled pre-
dictions. We also inspect a general distance matrix
for the whole compositionality subspace, i.e. all
the observed vectors and all the predicted vectors.
We extract the 10 nearest neighbours for the 380
Adjective-Noun pairs in the test set and look for
the intended predicted vectors in each case. The
idea here is that the best models should produce
predictions that are as close as possible to the orig-
inally observed A-N vector.
Our second evaluation method uses the 10 near-
est neighbours of each of the observed A-N pairs
in the test set as gold-standard (excluding any
modelled predictions), and compares them with
the 10 nearest neighbours of each of the corre-
sponding predictions as generated by the models.
The aim is to assess if the predictions made by
each model share any top-10 neighbours with their
corresponding gold-standard. We award 1 point
for every shared neighbour.
5.1 The distance of predictions
We calculated the Euclidean distance between
each observed A-N pair and the corresponding
prediction made by each model. On general in-
spection, it is clear that the approximation of A-N
compositional vectors made by PLSR is consid-
erably closer than those produced by the additive
and multiplicative models, cf. Table 1.
Min. 1st Q. Median Mean 3rd Q. Max.
ADD 0.877 1.402 1.483 1.485 1.570 1.814
MUL 0.973 0.998 1.002 1.002 1.005 1.019
PLSR 0.624 0.805 0.856 0.866 0.919 1.135
Table 1: Summary of distance values between the 380
observed A-N pairs and the predictions from each model
(ADD=additive, MUL=multiplicative, PLSR=Partial Least
Squares Regression).
We also computed in detail which of the three pre-
dicted composed vectors was closest to the corre-
sponding observation. To this effect we extracted
the 10 nearest neighbours for each A-N pair in the
test set using the whole compositionality subspace
(all the predicted and the original vectors). In 94
cases out of 380, the PLSR intended prediction
was the nearest neighbour. Cumulatively, PLSR?s
predictions were in the top-10 nearest neighbour
list in 219 out of 380 cases (57.6%). The other
models? performance in this test was negligible,
cf. Table 2. Overall, 223 items in the test set had
at least one predicted vector in the top-10 list; of
these, 219 (98%) were generated by PLSR and the
remaining 4 (1%) by the multiplicative model.
1 2 3 4 5 6 7 8 9 10 Tot.
ADD 0 0 0 0 0 0 0 0 0 0 0
MUL 0 1 0 2 1 0 0 0 0 0 4
PLSR 94 51 24 18 10 7 7 5 2 1 219
Table 2: Nearest predicted neighbours and their positions in
the top-10 list.
35
5.2 Comparing prediction neighbours to the
gold standard
Since the main use of DSMs is to extract similar
vectors from a multidimensional space (represent-
ing related documents, distributional synonyms,
etc.), we would like to test if the modelling of se-
mantic compositionality is able to produce predic-
tions that are as similar as possible to the originally
observed data. A very desirable result would be
if any predicted compositional A-N vector could
be reliably used instead of the extracted bigram.
This could only be achieved if a model?s predic-
tions show a similar distributional behaviour with
respect to the observed vector.
To test this idea using our data, we took the
10 nearest neighbours of each of the observed A-
N pairs in the test set as gold standard. These
gold neighbours were extracted from the obser-
vation testing subspace, thus excluding any mod-
elled predictions. This is a very restrictive set-
ting: it means that the gold standard for each of
the 380 test items is composed of the 10 nearest
neighbours from the same 380 items (which may
turn out to be not very close at all). We then ex-
tracted the 10 nearest neighbours for each of the
three modelled predictions, but this time the sub-
space included all predictions, as well as all the
original observations (380? 4 = 1520 items). Fi-
nally, we tested if the predictions made by each
model shared any top-10 neighbours with their
corresponding gold-standard. We awarded 1 point
for every shared neighbour.
The results obtained with these evaluation set-
tings were very poor. Only the additive model
scored points (48), although the performance was
rather disappointing (maximum potential score for
the test was 3,800 points). Both the pointwise mul-
tiplicative model and the PLSR model failed to re-
trieve any of the gold standard neighbours. This
poor results can be attributed to the very restric-
tive nature of our gold standard and, also, to the
asymmetrical composition of the compared data
(gold standard: 3,800 neighbours from a pool of
just 380 different items; prediction space: 11,400
neighbours from a pool of 1,520 items).
However, given the that DSMs are known
for their ability to extract similar items from
the same space, we decided to relax our test
settings by awarding points not only to shared
neighbours, but also to the same model?s predic-
tions of those neighbours. Thus, given a tar-
get neighbour such as good_deal, in our sec-
ond setting we awarded points not only to the
gold standard good_deal, but also to the pre-
dictions good_deal_ADD, good_deal_MUL and
good_deal_PLSR when evaluating each corre-
sponding model. With these settings the compared
spaces become less asymmetrical (gold standard:
7,600 neighbours from a pool of just 380 different
items plus predictions; prediction space: 11,400
neighbours from a pool of 1,520 items). The ob-
tained results show a great improvement (max. po-
tential score 7,600 points):
Shared Neigh. Predicted Neigh. Total
ADD 48 577 625
MUL 0 37 37
PLSR 0 263 263
Not shared: 6,675
Table 3: Shared neighbours with respect to the gold standard
and shared predicted neighbours.
Once again, the additive model showed the best
performance, followed by PLSR. The multiplica-
tive model?s performance was negligible.
While carrying out these experiments, an unex-
pected fact became evident. Each of the models in
turn produces predictions that are relatively close
to each other, regardless of the independent words
that were used to calculate the compositional vec-
tors. This has the consequence that the nearest
neighbour lists for each model?s predictions are,
by and large, populated by items generated in the
same model, as shown in Table 4.
ADD MUL PLSR OBS
ADD 2,144 (56%) ? ? ?
MUL 59 (1%) 3,800 (100%) 998 (26%) 1,555 (40%)
PLSR 1,472 (38%) ? 2,802 (73%) 2,190 (57%)
OBS 125 (3%) ? ? 55 (1%)
Table 4: Origins of neighbours in each models? top-10 list
of neighbours extracted from the full space composed of
observations and predictions (380 ? 4 = 1, 440 items)
(ADD=additive, MUL=multiplicative, PLSR=Partial Least
Squares Regression, OBS=observed vectors) .
Neighbours of predictions from the multiplicative
model are all multiplicative. The additive model
has the most varied set of neighbours, but the
majority of them are additive-neighbours. PLSR
shows a mixed behaviour. However, PLSR pro-
duced neighbours that find their way into the
neighbour sets of both the additive model and the
observations.
These remarks point in the same direction: ev-
36
ery model is a simplified and specialised version
of the original space, somewhat more orderly than
the observed data, and may give different results
depending on the task at stake. PLSR (and to a
lesser extent also the multiplicative model) is par-
ticularly efficient as generator of neighbours for
real vectors, a characteristic that could be applied
to guess distributional synonyms of unseen A-N
pairs. On the other hand, the additive model (and
to a lesser extent PLSR) is especially successful
in attracting gold standard neighbours. Overall,
even at this experimental stage, PLSR is clearly
the model that produces the most consistent re-
sults.
6 Concluding remarks
This paper proposed a novel method to model
the compositionality of meaning in distributional
models of semantics. The method, Partial Least
Squares Regression, is well known in other data-
intensive fields of research, but to our knowledge
had never been put to work in computational dis-
tributional semantics. Its main advantage is the
fact that it is designed to approximate functions
in problems of multivariate multiple regression
where the number of observations is relatively
small if compared to the number of variables (di-
mensions).
We built a DSM targeting a type of semantic
composition that has not been treated extensively
in the literature before, adjacent A-N pairs.
The model built by PLSR performed better than
both a simple additive model and a multiplicative
model in the first proposed evaluation method.
Our second evaluation test (using comparison
to a gold standard) gave mixed results: the best
performance was obtained by the simple additive
model, with PLSR coming in second place.
This is work in progress, but the results look
very promising. Future developments will cer-
tainly focus on the creation of better evaluation
methods, as well as on extending the experi-
ments to other techniques (e.g. convolution prod-
uct as discussed by Widdows, 2008 and Gies-
brecht, 2009). Another important issue that we
still have not touched is the role played by lex-
ical association (collocations) in the prediction
models. We would like to make sure that we
are not modelling the compositionality of non-
compositional examples.
A last word on the view of semantic composi-
tionality suggested by our approach. Modelling
compositionality as a machine learning task im-
plies that a great number of different ?types? of
composition (functions combining vectors) may
be learned from natural language samples. In prin-
ciple, any semantic relation instantiated by any
syntactic structure could be learned if sufficient
data is provided. This approach must be con-
fronted with other linguistic phenomena, also of
greater complexity than just a set of bigrams. Fi-
nally, we might wonder if there is an upper limit to
the number of compositionality functions that we
need to learn in natural language, or if there are
types of functions that are more difficult, or even
impossible, to learn.
Acknowledgements
Thanks are due to Marco Baroni, Stefan Evert,
Roberto Zamparelli and the three anonymous re-
viewers for their assistance and helpful comments.
References
Marco Baroni and Alessandro Lenci. 2009. One
semantic memory, many semantic tasks. In Pro-
ceedings GEMS 2009, 3?11. Athens: Associa-
tion for Computational Linguistics.
Eugenie Giesbrecht. 2009. In Search of Seman-
tic Compositionality in Vector Spaces. In Pro-
ceedings of the 17th International Conference
on Conceptual Structures, ICCS 2009, Moscow,
Russia, pp. 173?184. Berlin: Springer.
Zellig Harris. 1970 [1954]. Distributional struc-
ture. In Papers in structural and transforma-
tional linguistics, 775?794. Dordrecht: Reidel.
Bj?rn-Helge Mevik and Ron Wehrens. 2007. The
pls package: principal component and partial
least squares regression in R. Journal of Statis-
tical Software, 18(2): 1?24.
Jeff Mitchell and Mirella Lapata. 2008. Vector-
based Models of Semantic Composition. In
Proceedings of ACL-08: HLT, 236?244.
Columbus, OH.
Dominic Widdows. 2008. Semantic Vector Prod-
ucts: Some Initial Investigations. Second AAAI
Symposium on Quantum Interaction, Oxford,
26th?28th March 2008. URL: http://www.
puttypeg.com/papers/
Magnus Sahlgren. 2006. The Word Space Model.
Ph.D. dissertation, Stockholm University.
37
Computing Semantic Compositionality
in Distributional Semantics
Emiliano Guevara
Tekstlab, ILN, University of Oslo, e.r.guevara@iln.uio.no
Abstract
This article introduces and evaluates an approach to semantic compositionality in computational lin-
guistics based on the combination of Distributional Semantics and supervised Machine Learning. In
brief, distributional semantic spaces containing representations for complex constructions such as
Adjective-Noun and Verb-Noun pairs, as well as for their constituent parts, are built. These repre-
sentations are then used as feature vectors in a supervised learning model using multivariate multiple
regression. In particular, the distributional semantic representations of the constituents are used to
predict those of the complex structures. This approach outperforms the rivals in a series of experi-
ments with Adjective-Noun pairs extracted from the BNC. In a second experimental setting based on
Verb-Noun pairs, a comparatively much lower performance was obtained by all the models; however,
the proposed approach gives the best results in combination with a Random Indexing semantic space.
1 Introduction
Probably the most important missing ingredient from the current NLP state-of-the-art is the ability to
compute the meaning of complex structures, i.e. semantically compositional structures. In this pa-
per, I propose a methodological approach and a series of experiments designed to teach computers the
ability to compute the compositionality of (relatively simple) complex linguistic structures. This work
uses a combination of Distributional Semantics and Machine Learning techniques. The starting data in
the experiments reported below are multidimensional vectorial semantic representations extracted from
electronic corpora. This work extends the basic methodology presented in Guevara (2010) with new data
collection techniques, improved evaluation metrics and new case studies.
Compositionality is probably one of the defining properties of human language and, perhaps, a nearly
uncontroversial notion among linguists. One of the best-known formulations of compositionality is:
(1) The Principle of Compositionality:
The meaning of a complex expression is a function of the meaning of its parts and of the syntactic
rules by which they are combined. (Partee, ter Meulen and Wall, 1990: 318)
The Principle of Compositionality is a standard notion in many different fields of research, notably in
logic, in philosophy of language, in linguistics and in computer science; this intrinsic multi-disciplinarity
makes tracing back its recent history somewhat difficult.
The recent years have witnessed an ever-increasing interest in techniques that enable computers to
automatically extract semantic information from linguistic corpora. In this paper I will refer to this
new field in general as Distributional Semantics. Distributional Semantics, in short, extracts spatial
representations of meaning from electronic corpora by using distributional (i.e. statistical) patterns of
word usage. The main hypothesis in Distributional Semantics is the so-called distributional hypothesis of
meaning, expressing the fact that ?words that occur in the same contexts tend to have similar meanings?
(Pantel, 2005). The distributional hypothesis of meaning is ascribed to Zellig Harris, who proposed a
general distributional methodology for linguistics.
Since representations in Distributional Semantics are spatial in nature (e.g. vectors representing
points in a multidimensional space), differences in meaning are captured through differences in location:
135
in the multidimensional space, two semantically (i.e. distributionally) similar words are closer than two
words that are dissimilar. See Sahlgren (2006) and Turney and Pantel (2010) for detailed overviews of
the methodology and applications of Distributional Semantics.
2 Compositionality in distributional semantics: state-of-the-art
I stressed above that computers are still not able to deal with the compositionality of meaning. However
basically true, this statement should be qualified somewhat. Previous work in the field has produced a
small number of operations to approximate the composition of vectorial representations of word mean-
ing. In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is
modelled by one of the following four basic operations: vector addition, vector pointwise-multiplication,
tensor product or linear regression.
In the literature on Information Retrieval, vector addition is the standard approach to model the
composed meaning of a group of words (or a document) as the sum of their vectors (see, among many
others, Widdows, 2004: ch. 5). More schematically:
(2) Vector addition: v1i + v2i = v3i
Given two independent vectors v1 and v2, the compositional meaning of v3 consists of the sum
of the corresponding components of the original vectors.
Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector
addition and pointwise-multiplication (and a weighted combination of both), evaluated on a sentence
similarity task inspired by Kintsch (2001). While the additive model captures the compositionality of
meaning by considering all available components, multiplicative models only operate on a subset of
them, i.e. non-zero components. They claim that when we pointwise-multiply the vectors representing
two words, we obtain an output that captures their composition; actually, this operation is keeping in
the output only the components which had corresponding non-zero values: whether this operation has
any relation with semantics is still unclear. However, in their experiments, Mitchell and Lapata prove
that the pointwise-multiplicative model and the weighted combination of the additive and the multiplica-
tive models perform equally well. Of these, only the simple multiplicative model will be tested in the
experiments I present in the following section.
(3) Vector pointwise multiplication: v1i ? v2i = v3i
Each corresponding pair of components of v1 and v2 is multiplied to obtain the corresponding
component of v3.
Widdows (2008) proposes to apply a number of more complex vector operations imported from
quantummechanics to model composition in semantic spaces, in particular tensor product and the related
operation of convolution product. Widdows (2008) obtains results indicating that both the tensor product
and the convolution product perform better than the simple additive model in two small experiments
(relation extraction and phrasal composition). Giesbrecht (2009) presents a more complex task, singling
out non-compositional multiword expressions. Her results clearly show that tensor product outperforms
vector addition, multiplication and convolution.
(4) Tensor product: v1?v2 = v3
where v3 is a matrix whose ij-th entry is equal to v1i ? v2j
However, since the tensor product (also called outer product) of two vectors produces a result with higher
dimensionality (a matrix), it cannot be directly compared against the other methods, which instead gener-
ate compositional representations in the same original space. In the experiments reported in the following
section, we will use the circular convolution composition method (Plate, 1991): in brief, circular convo-
lution is a mathematical operation that effectively compresses the tensor product of two vectors onto the
original space, thus allowing us to compare its outcome with that of the other methods here reviewed.
136
(5) Circular convolution: v1?v2 = v3
where v3 =
n?1
?
j=0 v1j v2i?j
It is interesting to note that a great deal of attention has recently been devoted to the tensor product
as the basic operation for modelling compositionality, even at the sentential level (e.g. Grefenstette et
al. 2010), through a combination of mathematical operations and symbolic models of logic (inspired by
Clark and Pulman, 2007). Although extremely motivating and thought provoking, these proposals have
not been tested on empirical grounds yet.
A common thread ties all the approaches briefly outlined above: all information that is present in
the systems is conveyed by the vectors v1 and v2, e.g. the independent word representations, while
completely disregarding v3 (the composed vector). Furthermore, all of these approaches are based on
the application of a single geometric operation on the independent vectors v1 and v2. It seems highly
unlikely that just one geometric operation could reliably represent all the semantic transformations in-
troduced by all syntactic relations in every language.
Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic
compositionality in distributional spaces by extracting context vectors from the corpus also for the com-
posed vector v3. For example, Guevara collects vector representations for nice and house, but also for the
observed pair nice_house. With these data, a model of Adjective-Noun (AN) compositionality is built by
using a supervised machine learning approach: multivariate multiple linear regression analysis by partial
least squares. This method is able to learn the transformation function that best approximates v3 on the
basis of both v1 and v2. Baroni and Zamparelli (2010) use a slightly different methodology: assuming
that each adjective is a linear transformation function (i.e. the function to be learnt by the algorithm),
they model AN compositionality by approximating v3 only on the basis of v2 (the noun) but running a
different regression analysis for each adjective in their data.
The approach proposed by Guevara (2010) is really only an extension of the full additive model of
Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology
ensures that the weight parameters in the function are estimated optimally by linear regression. In the
following section, I present a new series of experiments that refine, extend and improve this approach to
model the compositionality of adjacent AN and VN pairs by linear regression.
(6) Compositionality by regression: Av1 +Bv2 = v3
where A and B are weight matrices estimated by the supervised learning algorithm using multi-
variate multiple linear regression.
3 Compositionality by regression
Let us reconsider the highly underspecified definition of the Principle of Compositionality. Let us start by
setting the syntactic relation that we want to focus on for the purposes of this study: following Guevara
(2010) and Baroni and Zamparelli (2010), I model the semantic composition of adjacent Adjective-Noun
pairs expressing attributive modification of a nominal head. In a second analogous experiment, I also
model the syntactic relation between adjacent Verb-Noun expressing object selection by the verbal head.
The complex expression and its parts are, respectively, adjacent Adjective-Noun and Verb-Noun1
pairs and their corresponding constituents (respectively, adjectives and nouns, verbs and nouns) extracted
from the British National Corpus. Furthermore, the meaning of both complex expressions and their
constituents is assumed to be the multidimensional context vectors obtained by building semantic spaces.
What remains to be done, therefore, is to model the function combining meanings of the constituent
parts to yield the meaning of the resulting complex expression. This is precisely the main assumption
made in this article. Since we are dealing with multidimensional vector representations of meaning,
we suggest that compositionality can be interpreted as a linear transformation function mapping two
1Actually, the extracted Verb-Noun pairs are not always strictly adjacent, an optional determiner was allowed to occur
between verb and noun. Thus, phrases such as "raise money" and "visit a client" were both included.
137
independent vectors in a multidimensional space into a composed vector in the same space. Moreover,
considering that each component in the independent vectors v1 and v2 is a candidate predictor, and that
each component in the composed vector v3 is a dependent variable, it is proposed to formulate composi-
tionality of meaning in Distributional Semantics as a problem of multivariate multiple regression. Such
a formulation allows us to model compositionality by applying well-known standard machine learning
techniques such as the Multilayer Perceptron or Support Vector Machines.
However, since word sequences in corpora tend to have low frequency distributions (usually lower
than the frequency of their constituents) and very sparse vectorial representations, it is very difficult to
build datasets where the number of observations (the size of the dataset) is greater than the number of
variables considered (the dimensions of the vector in the dataset). This issue is known as the curse
of dimensionality, and specific mathematical techniques have been developed to deal with it. In our
experiments, we use one such regression technique, Partial Least Squares.
3.1 Partial least squares regression
Partial Least Squares Regression (PLS) is a multivariate regression technique that has been designed
specifically to treat cases where the curse of dimensionality is a serious issue. PLS has been successfully
applied in a wide range of different scientific fields such as spectroscopy, chemistry, brain imaging and
marketing (Mevik and Wehrens, 2007).
PLS predicts the output matrix Y from information found in both the input matrix X and in Y. It
does so by looking for a set of latent variables in the data that perform a simultaneous decomposition of
both matrices while trying to explain as much as possible of the covariance between X and Y. Next, PLS
carries out regression using the decomposition of X to predict Y. Thus, PLS performs the prediction by
extracting the latent variables with the best predictive power. PLS is a robust regression technique that
is particularly efficient in situations with a high number of predictors and few observations (Abdi, 2007,
Hastie et al, 2009). Standard linear regression will fail in such cases.
3.2 Experimental setup
3.2.1 Corpus and construction of the dataset
Using a lemmatised and POS tagged version of the BNC, a list of adjacent AN pair candidates was
extracted with simple regex-based queries targeting sequences composed of [Det/Art?A?N] (i.e. pairs
expressing attributive modification of a nominal head like ?that little house?). In order to ensure the
computational attainability of the successive steps, the candidate list was filtered by frequency (> 400)
obtaining 1,367 different AN pairs.
A new version of the BNC was then prepared to represent the selected AN lemma pairs as a single to-
ken; for example, while in the original BNC the phrase [nice houses] consists in two separate POS-tagged
lemmas, nice_AJ and house_NN, in the processed corpus it appears as a single entry nice_AJ_house_NN).
The corpus was also processed by stop-word removal (very high frequency items, mainly functional mor-
phemes). The re-tokenization process of the BNC enables us to extract independent context vectors for
each AN pair in our list (v3) and their corresponding constituents (A and N, respectively v1 and v2),
while ensuring that the extracted vectors do not contain overlapping information.
The same preprocessing steps were carried out to extract VN pair candidates. Sequences composed of
[V-(Det/Art)?N] with an optional determiner were targeted and filtered by frequency (> 400), resulting
in a first list of 545 VN pairs. This list contained a large amount of noise due to lemmatisation and
POS-tagging problems (e.g. housing association), and it also contained many very frequent lexicalized
items (e.g. thank goodness). The list was manually cleaned, resulting in 193 different VN pairs.
3.2.2 Building semantic spaces and composition models
For each syntactic relation (AN and VN), two different semantic spaces were built with the S-Space
package (Jurgen and Stevens, 2010): a Hyperspace Analogue to Language space (HAL, Burgess and
138
Lund, 1997) and a Random Indexing space (RI, Sahlgren, 2006). The spaces were built using the same
vocabulary, the 23,222 elements in the corpus with a frequency ? 100 (comprising both individual
lemmas and all the selected AN pairs) and the same contextual window of 5 words to the left and to the
right of the target (either a word or a AN/VN pair).
HAL is a co-occurrence based semantic space that corresponds very closely to the well-known term-
by-term matrix collection method. However, given the size of our vocabulary, the resulting matrix is
extremely large (23,222 ? 23,222). HAL reduces the dimensionality of the space by computing the
variances of the row and column vectors for each word, and discarding the elements with lowest variance.
The dimensionality of this space was reduced to the 500 most informative dimensions, thus ending with
a size of 23,222 ? 500. The vectors in this space were normalised before the successive steps.
RI avoids the problem of dimensionality of semantic spaces by applying a different strategy to collect
the context vectors. Each word in the corpus is assigned an initial unique and randomly generated index
vector of a fixed size. As the algorithm scans the corpus one token at a time, the vector of the target
word is incrementally updated by combining it with the index vector of the context. In order to keep
the comparability of the built spaces, the RI space was built with 500-dimensional index vectors, thus
obtaining a space of 23,222 ? 500 dimensions. The vectors in this space were also normalised.
With the AN/VN pair vectors and their corresponding constituents (respectively v3, v1 and v2), four
different models of compositionality were built from each semantic space (HAL and RI) in each of the
considered syntactic relations:
? an additive model (ADD) v1i + v2i = v3i
? a simple multiplicative model (MUL) v1i ? v2i = v3i
? a circular convolution model (CON) v1 ? v2 = v3
? a partial least squares regression model (PLS) Av1 +Bv2 = v3
In addition, two baseline models were introduced in the evaluation process. The baseline models were
built by simply extracting the context vectors for the constituents in each pair from each space (A and N,
V and N, respectively v1 and v2).
Of all the considered models, only PLS requires a stage of parameter estimation, i.e. training. In
order to accomplish this, the data were randomly divided into a training set (1,000 AN pairs ? 73%) and
a test set (the remaining 367 AN pairs ? 27%). In the much smaller VN dataset, the training set was built
with 133 pairs (69%) and the test set with 60 pairs (31%). These parameters for the regression models
were estimated by performing a 10-fold cross-validation in the training phase. All the models were built
and evaluated using the R statistical computing environment and simple Python scripts. In particular,
the regression analysis was carried out with the pls package (Mevik and Wehrens, 2007). After various
preliminary trials, the PLS model?s predictions were computed by using the first 50 latent variables.
3.3 Evaluation
The evaluation of models of compositionality is still a very uncertain and problematic issue. Previous
work has relied mainly on ?external? tasks such as rating sentence similarity or detection idioms. These
evaluation strategies are ?external? in the sense that each compared model produces a set of predictions
which are then used in order to reproduce human annotation of datasets that do not have a representation
in the semantic space under consideration. For example, Mitchell and Lapata (2008) use their models to
approximate the human ratings in their sentence similarity dataset. Giesbrecht (2009) also uses human
annotated data (manually classified collocations, compositional and non-compositional) in her evaluation
task. However, any evaluation task requiring hand-annotated datasets will have a considerable cost in
resource building. At present time, there are no suitable datasets in the public domain.
I propose instead to take a radically different point of view, developing ?internal? evaluation tasks
that try to measure how well the proposed models approximate the distributional patterns of corpus-
extracted composed vectors. That is to say, I want to compare the predicted output of every model (i.e. a
predicted context vector for v3) with the real observation of v3 that was collected from the corpus. The
139
following subsections present a few experimental evaluation methods based on neighbour analysis and
on the Euclidean measure of distance.
The evaluation strategies here presented rests on the sensible assumption that if a model of AN
compositionality is reliable, its predicted output for any AN pair, e.g. weird_banana, should be in prin-
ciple usable as a substitute for the corresponding corpus-attested AN vector. Moreover, if such a model
performs acceptably, it could even be used predict the compositionality of unattested candidates like
shadowy_banana: this kind of operations is the key to attaining human-like semantic performance.
3.3.1 Correlation analysis between modelled predictions and observations
Let us start the comparative evaluation of the modelled predictions by considering the results of a series
of Mantel correlation tests. First, distance matrices were computed for the observations in the test sets
and then the same was done for each of the prediction models. Then, each of the models? distance
matrices was compared against the distance matrix of the observations trying to determine their degree
of correlation. The null hypothesis in each Mantel test is that the distance matrices being compared are
unrelated. The aim of this task is similar to the evaluation method used by Mitchell and Lapata (2008):
we try to find out which model has the strongest correlation with the original data, with the difference
that in our case no ?external? human ratings are used.
HAL RI
Model Correlation Simul. p-value Correlation Simul. p-value
PLS 0.5438081 0.001 0.4341146 0.001
ADD 0.5344057 0.001 0.3223733 0.001
MUL 0.3297359 0.001 0.1811038 0.002
CON -0.05557023 0.956 -0.02584655 0.727
Table 1: Adjective-Noun pairs. Mantel tests of correlation (max. correlation = 1)
Considering the results for the AN dataset in Table 1, with the PLS and ADD models we can reject
the null hypothesis that the two matrices (distance matrix between the observed AN pairs and distance
matrix between each model?s predictions) are unrelated with p-value = 0.001 in both the semantic spaces
(HAL and RI). MUL also allows the null hypothesis to be rejected, but with a lower correlation (and with
a greater p-value = 0.002 in RI). Having obtained the highest observed correlation in both settings, the
PLS model is highly positively associated with the observed data. Also ADD and MUL have produced
predictions that are positively correlated with the observed AN vectors. CON is not correlated with
the original data. In other words, PLS and ADD seem to be much better that the remaining models in
reproducing unseen AN pairs; overall, however, PLS produces the closest approximation of the corpus-
based test set. Finally, although both semantic spaces (HAL and RI) produce the same ordering among
the models, it seems that the predictions using the HAL space are relatively closer to the observed data.
HAL RI
Model Correlation Simul. p-value Correlation Simul. p-value
PLS 0.2186435 0.003 0.1113741 0.116
ADD 0.4094653 0.001 0.1290508 0.124
MUL 0.1375934 0.042 -0.08865458 0.8
CON 0.05153776 0.174 -0.08186146 0.807
Table 2: Verb-Noun pairs. Mantel tests of correlation (max. correlation = 1)
Turning to the VN dataset, the obtained results are much less promising (see Table 2). As a general
observation, the correlations between each of the models and the observations are very low, except for
ADD in the HAL semantic space. In addition, ADD obtains the best correlation also in the RI space.
PLS comes in second place. Given that PLS is based on the estimation of parameters from training data,
its low performance can be attributed to the size of dataset (only 133 VN examples used for training). On
140
the contrary, ADD, MUL and CON do not have this excuse and their extremely low performance must
be due to other factors. Finally, it is very clear that HAL produces better correlations for all the models.
3.3.2 Observation-based neighbour analysis
For this and for the remaining evaluation protocols, a preliminary step was taken. Since our intention is
to base the evaluation on the analysis of nearest neighbours, we extracted an identical subset of the built
semantic spaces (HAL and RI, which originally had a vocabulary of 23,222 items) in order to compute a
distance matrix of a manageable size.
In the Adjective-Noun dataset, the extracted subset comprises vectors for all the observed AN vectors
in both the training and test sets (1,367 items), all the corresponding predictions, the NOUN- and ADJ-
baseline models, the 2,500 most frequent nouns (not included in the baseline) and the 2,500 most frequent
adjectives (not included in the baseline). The distance matrix for the selected sub-space was then created
by using the Euclidean measure of distance, resulting in a 8,666 ? 8,666 matrix.
The Verb-Noun dataset was treated in the same way, extracting vectors for all the VN observations,
the corresponding predictions from each model, the VERB- and NOUN-baseline models and the 1,000
most frequent nouns and verbs in the space (not overlapping with the baselines); this resulted in a 2,420
? 2,420 distance matrix.
Following Guevara?s (2010) neighbour analysis, for each observed AN pair in the test datasets, the
list of n-top neighbours were extracted from the distance matrix (n=10 and n=20). Then, the resulting
neighbour lists were analysed to see if any of the modelled predictions was to be found in the n-top list.
The ADJ- and NOUN-baselines were introduced in the evaluation to further compare the appropriateness
of each model. Below we only report the results obtained with n=20, but very similar results were
obtained in the 10-top neighbour setting.
As can be observed from Table 3, in the HAL space, PLS obtains the highest score, followed by the
NOUN-baseline at a short distance and then by the ADJ-baseline at a greater distance. The performance
of the remaining models is negligible. A different situation can be seen for the RI space, where the
winner is the NOUN-baseline followed by PLS and ADJ.
HAL RI
Model Predictions found Predictions found
ADD 0 0
CON 0 0
MUL 3 0
PLS 152 112
ADJ 32 53
NOUN 123 144
Table 3: AN pairs. Observation-based neighbour analysis (max. score = 367)
It is interesting to see that PLS is actually competing against the NOUN-baseline alone, being the rival
models almost insensible to the evaluation task. This same pattern will be seen in the other evaluation
tasks. Furthermore, the score differences obtained by PLS and the NOUN-baseline are significant (HAL
p-value = 0.03275, RI p-value = 0.01635, 2-sample test for equality of proportions).
The VN dataset gave much poorer results, once more. In fact, it is almost pointless to comment
anything except that only MUL was able to rank its predictions in top-20 neighbours six times (only in
the HAL space) and that PLS managed to do the same 9 times (only in the RI space). The maximum
score in this setting was 60.
3.3.3 Prediction-based neighbour analysis
Building on the previous neighbour analysis, a new task was set up by changing the starting point for
neighbour extraction. In this case, for each modelled AN pair in the test dataset in each composition
141
model, the list of n-top neighbours were extracted from the distance matrix (n=10 and n=20). Then, the
resulting neighbour lists were analysed to see if the originally observed corresponding AN pair was to
be found in the n-top list. The same procedure was used with the VN dataset.
Below we only report the results obtained with n=20, but very similar results were obtained in the
10-top neighbour setting. This task at first did not seem to be particularly difficult, but the obtained
results were very poor.
HAL RI
Model Predictions found Predictions found
ADD 2 0
CON 0 0
MUL 0 0
PLS 32 25
ADJ 6 2
NOUN 26 16
Table 4: AN pairs. Prediction-based neighbour analysis (max. score = 367)
The winner in this experiment was PLS, once again followed by the NOUN-baseline. However, the score
differences obtained by PLS and the NOUN-baseline are not significant (HAL p-value = 0.4939, RI p-
value = 0.1985, 2-sample test for equality of proportions). The main observation to be made is that the
obtained scores are surprisingly low if compared with the previous evaluation task. The reason for this
difference is to be found in the homogeneity and specialization that characterizes each of the models?
neighbour sets: each model produces predictions that are relatively very close to each other. This has the
consequence that the nearest neighbour lists for each model?s predictions are, by and large, populated
by items generated in the same model, as shown in Table 5. In conclusion, although PLS obtained the
highest score in this task, we cannot be sure that it performed better than the NOUN-baseline. In any
case, the remaining composition models did not reach the performance of PLS.
Model Same-model items
ADD 3626 (98,8 %)
CON 3670 (100 %)
MUL 3670 (100 %)
PLS 2767 (75,4 %)
NOUN 1524 (41,5 %)
ADJ 1382 (36,1 %)
Table 5: AN pairs. Same-model neighbours in each models? top-10 list of neighbours extracted from
HAL semantic space (total items in each list = 3670)
The VN dataset once again did not produce interesting results. As a brief note, ADD won in the
HAL space (but managing to score only two observations in its predictions? top-20 neighbours) while
PLS won in the RI space as before, scoring 5 observations in its predictions? top-20 neighbours (max.
score 60).
3.3.4 Gold-standard comparison of shared neighbours
Our previous evaluation methods targeted the distance between predictions and observations, i.e. the
ability of each model to reproduce unseen AN/VN pairs. Changing perspective, it would be desirable to
test if the models? predictions show a similar distributional behaviour with respect to the corresponding
observed vector and to other words in the semantic space.
To test this idea, the n-top neighbour-lists (n=10 and n=20) for the observed AN/VN pairs were
extracted and taken to be the gold-standard. Then, each prediction?s n-top list of neighbours was analysed
looking for shared neighbours with respect to the corresponding gold-standard list. Each time a shared
neighbour was found, 1 point was awarded to the model.
142
Table 6 summarises the results obtained with n=20 (similar figures obtained with n=10) in the AN
dataset. Although by a small margin, the winner in this task is PLS. Even if the obtained scores are
still rather low (in the best cases, about 17% of all the available points were obtained), this experiment
represents a significant improvement over Guevara?s (2010) reported results, which reached only about
10% of the maximum score. Here again the same ordering of models can be observed: after PLS we find
the NOUN- and ADJ-baselines, leaving the performance of the remaining models at a extremely modest
level. Additionally, the score differences obtained by PLS and the NOUN-baseline are highly significant
(HAL p-value = 2.363e-08, RI p-value = 0.0003983, 2-sample test for equality of proportions).
HAL RI
Model Shared neighbours Shared neighbours
ADD 28 0
CON 0 0
MUL 5 0
PLS 1299 1267
ADJ 259 534
NOU 1050 1108
Total shared: 2641 2909
Table 6: AN pairs. Gold-standard comparison of shared neighbours (max. score = 7340)
Table 7 summarises the results obtained in the VN dataset, which show a considerable improvement
over the preceding evaluation methods. Here we have to clear winners, ADD in the HAL space and PLS
in the RI space. Interestingly, although the numbers are still on the low side, ADD obtained 8.6% of
the total points, with shared neighbours for 35 out of 60 VN pairs; PLS obtained 21% of the total, with
shared neighbours for 40 out of 60 VN pairs. In particular this last score is (21%) is the highest one ever
obtained with gold-standard comparison of shared neighbours (also considering Guevara?s 2010 results).
HAL RI
Model Shared neighbours Shared neighbours
ADD 103 0
CON 0 0
MUL 31 0
PLS 0 253
VERB 0 0
NOUN 0 0
Total shared: 134 253
Table 7: VN pairs. Gold-standard comparison of shared neighbours (max. score = 1200)
4 Conclusions
This paper proposes an improved framework to model the compositionality of meaning in Distributional
Semantics. The method, Partial Least Squares Regression, is well known in other data-intensive fields of
research, but to our knowledge had never been put to work in computational semantics.
PLS outperformed all the competing models in the reported experiments with AN pairs. In particular,
the PLS model generates compositional predictions that are closer to the observed composed vectors than
those of its rivals. This is an extremely promising result, indicating that it is possible to generalize linear
transformation functions beyond single lexical items in Distributional Semantics? spaces.
It is remarkable that PLS did not actually have to compete against any of the previously proposed
approaches to compositionality, but only against the NOUN- and ADJ-baselines, and in particular against
the former. This fact is expected from a theoretical point of view: since the Noun is the head of the AN
pair, it is likely that the complex expression and its head share much of their distributional properties.
PLS nearly always outperformed the NOUN-baseline, but only by small margins, which indicates that
143
there is a still plenty of space for improvement. Our experiments also show that AN compositionality by
regression performs nearly equally well in semantic spaces of very different nature (HAL and RI).
The second dataset used in this paper contained VN pairs. Generally, this dataset did not produce
good results with any of the considered approaches to model compositionality. This rather negative result
may be due to its relatively smaller size, but this excuse may only be applied to PLS, the only model that
relies on parameter estimation. Surprisingly, though, the gold-standard comparison of shared neighbours
gave much better results, with ADD performing well in the HAL space and PLS performing very well
in the RI space. Even if the VN dataset did not produce excellent results, it highlights some interesting
issues. First, not all syntactic relations may be equally "easy" to model. Second, different evaluation
methods may favor competing approaches. Finally, some approaches may be particularly successful
with a specific distributional space architecture (like PLS and RI, and ADD and HAL).
This work has intentionally left the data as raw as possible, in order to keep the noise present in
the models at a realistic level. The combination of Machine Learning and Distributional Semantics here
advocated suggests a very promising perspective: transformation functions corresponding to different
syntactic relations could be learned from suitably processed corpora and then combined to model larger,
more complex structures, probably also recursive phenomena. It remains to prove if this approach is able
to model the symbolic, logic-inspired kind of compositionality that is common in Formal Semantics; be-
ing inherently based on functional items, it is at present time very difficult and computationally intensive
to attain, but hopefully this will change in the near future.
References
Abdi, H. (2007). Partial least squares regression. In N. Salkind (Ed.) Encyclopaedia of Measurement and Statistics.
Thousand Oaks (CA), Sage.
Baroni, M. and A. Lenci. (2009). One semantic memory, many semantic tasks. In Proc. of the EACL Workshop
on Geometrical Models of Natural Language Semantics, 3?11. Athens, ACL.
Baroni, M. and R. Zamparelli. (2010, to appear). Nouns are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In Proceedings of EMNLP 2010.
Burgess, C. and K. Lund. (1997). Modeling parsing constraints with high-dimensional context space. ?Language
and Cognitive Processes?, 12, 177-210.
Clark, S. and S. Pulman. (2007). Combining symbolic and distributional models of meaning. In Proceedings of
the AAAI Spring Symposium on Quantum Interaction, 52?55. Stanford (CA).
Giesbrecht, E. (2009). In Search of Semantic Compositionality in Vector Spaces. In Proceedings of ICCS 2009,
Moscow, Russia, 173?184. Berlin, Springer.
Grefenstette, E. Bob Coecke, S Pulman, S. Clark and M. Sadrzadeh. (2010). Concrete Compositional Sentence
Spaces. Talk presented at ESSLLI 2010, August 16-20, 2010, University of Copenhagen.
Guevara, E. (2010). A Regression Model of Adjective-Noun Compositionality in Distributional Semantics. In
Proc. of the 2010 Workshop on Geometrical Models of Natural Language Semantics, 33-37. Uppsala, ACL.
Jurgens, D. and K. Stevens. (2010). The S-Space Package: An Open Source Package for Word Space Models. In
Proceedings of the ACL 2010 System Demonstrations, 30-35. Uppsala, ACL.
Kintsch, W. 2001. Predication. ?Cognitive Science?, 25 (2), 173?202.
Mevik, B.-H. and R. Wehrens. (2007). The pls package: principal component and partial least squares regression
in R. ?Journal of Statistical Software?, 18 (2), 1?24.
Mitchell, J. and M. Lapata. (2008). Vector- based Models of Semantic Composition. In Proceedings of the 46th
Annual Meeting of the ACL, 236?244. Columbus, OH, ACL.
Pantel, P. (2005). Inducing ontological co-occurrence vectors. In Proceedings of the 43rd Conference of the ACL,
125?132. Morristown, ACL.
Partee, B.H., A. ter Meulen and R.E. Wall. (1990). Mathematical methods in linguistics. Dordrecht, Kluwer.
Plate, T.A. (1991). Holographic reduced representations: Convolution algebra for compositional distributed repre-
sentations. In Proceedings of the 12th International Joint Conference on Artificial Intelligence, 30?35. Sydney.
Sahlgren, M. (2006). The Word Space Model. Ph.D. dissertation. Stockholm University.
Turney, P. and P. Pantel. (2010). From frequency to meaning: Vector space models of semantics. ?Journal of
Artificial Intelligence Research?, 37, 141?188.
Widdows, D. (2004). Geometry and Meaning. Stanford, CSLI publications.
Widdows, D. (2008). Semantic Vector Products: Some Initial Investigations. Paper presented at the Second AAAI
Symposium on Quantum Interaction. Oxford, 26th?28th March 2008.
144
