Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 665?672
Manchester, August 2008
Semantic role assignment for event nominalisations
by leveraging verbal data
Sebastian Pad
?
o
Department of Linguistics
Stanford University
450 Serra Mall
Stanford CA 94305, USA
pado@stanford.edu
Marco Pennacchiotti and Caroline Sporleder
Computational Linguistics
Saarland University
Postfach 15 11 50
66041 Saarbr?ucken, Germany
{pennacchiotti|csporled}@coli.uni-sb.de
Abstract
This paper presents a novel approach to
the task of semantic role labelling for event
nominalisations, which make up a consider-
able fraction of predicates in running text,
but are underrepresented in terms of train-
ing data and difficult to model. We propose
to address this situation by data expansion.
We construct a model for nominal role la-
belling solely from verbal training data. The
best quality results from salvaging gram-
matical features where applicable, and gen-
eralising over lexical heads otherwise.
1 Introduction
The last years have seen a large body of work on
modelling the semantic properties of individual
words, both in the form of hand-built resources
like WordNet and data-driven methods like seman-
tic space models. It is still much less clear how the
combined meaning of phrases can be described.
Semantic roles describe an important aspect of
phrasal meaning by characterising the relationship
between predicates and their arguments on a seman-
tic level (e.g., agent, patient). They generalise over
surface categories (such as subject, object) and vari-
ations (such as diathesis alternations). Two frame-
works for semantic roles have found wide use in
the community, PropBank (Palmer et al, 2005) and
FrameNet (Fillmore et al, 2003). Their corpora are
used to train supervised models for semantic role
labelling (SRL) of new text (Gildea and Jurafsky,
2002; Carreras and M`arquez, 2005). The resulting
analysis can benefit a number of applications, such
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
as Information Extraction (Moschitti et al, 2003)
or Question Answering (Frank et al, 2007).
A commonly encountered criticism of seman-
tic roles, and arguably a major obstacle to their
adoption in NLP, is their limited coverage. Since
manual semantic role tagging is costly, it is hardly
conceivable that gold standard annotation will ulti-
mately be available for every predicate of English.
In addition, the lexically specific nature of the map-
ping between surface syntax and semantic roles
makes it difficult to generalise from seen predicates
to unseen predicates for which no training data is
available. Techniques for extending the coverage of
SRL therefore address an important need.
Unfortunately, pioneering work in unsupervised
SRL (Swier and Stevenson, 2004; Grenager and
Manning, 2006) currently either relies on a small
number of semantic roles, or cannot identify equiva-
lent roles across predicates. A promising alternative
direction is automatic data expansion, i.e., lever-
aging existing annotations to classify unseen, but
similar, predicates. The feasibility of this approach
was demonstrated by Gordon and Swanson (2007)
for syntactically similar verbs. However, their ap-
proach requires at least one annotated instance of
each new predicate, limiting its practicability.
In this paper, we present a pilot study on the
application of automatic data expansion to event
nominalisations of verbs, such as agreement for
agree or destruction for destroy. While event nom-
inalisations often afford the same semantic roles
as verbs, and often replace them in written lan-
guage (Gurevich et al, 2006), they have played a
largely marginal role in annotation. PropBank has
only annotated verbs.1 FrameNet annotates nouns,
but covers far fewer nouns than verbs. The same
1A follow-up project, NomBank (Meyers et al, 2004), has
since provided annotations for nominal instances, too.
665
situation holds in other languages (Erk et al, 2003).
Our fundamental intuition is that it is possible to
increase the annotation coverage of event nominal-
isations by data expansion from verbal instances,
since the verbal and nominal predicates share a
large part of the underlying argument structure. We
assume that annotation is available for verbal in-
stances. Then, for a given instance of a nominal-
isation and its arguments, the aim is to assign se-
mantic role labels to these arguments. We solve this
task by constructing mappings between the argu-
ments of the noun and the semantic roles realised
by the verb?s arguments. Crucially, unlike previous
work (Liu and Ng, 2007), we do not employ a clas-
sical supervised approach, and thus do not require
any nominal annotations.
Structure of the paper. Sec. 2 provides back-
ground on nominalisations and SRL. Sec. 3 pro-
vides concrete details on our expansion-based ap-
proach to SRL for nominalisations. The second part
of the paper (Sec. 4?6) provides a first evaluation
of different mapping strategies based on syntactic,
semantic, and hybrid information. Sec. 8 concludes.
2 Nominalisations
Nominalisations (or deverbal nouns) are commonly
defined as nouns morphologically derived from
verbs, usually by suffixation (Quirk et al, 1985).
They have been classified into at least three cate-
gories in the linguistic literature, event, result, and
agent/patient nominalisations (Grimshaw, 1990).
Event and result nominalisations account for the
bulk of deverbal nouns. The first class refers to an
event/activity/process, with the nominal expressing
this action (e.g. killing, destruction). Nouns in the
second class describe the result or goal of an ac-
tion (e.g. agreement). Many nominals have both
an event and a result reading (e.g., selection can
mean the process of selecting or the selected ob-
ject). Choosing a single reading for an instance is
often difficult; see Nunes (1993); Grimshaw (1990).
A smaller class is agent/patient nominalisations.
Agent nominals are usually identified by suffixes
such as -er, -or, -ant (e.g. speaker, applicant), while
patient nominalisations end with -ee, -ed (e.g. em-
ployee). While these nominalisations can be anal-
ysed as events (the baker?s bread implies that bak-
ing has taken place), they more naturally refer to
participants. In consequence, agent/patient nomi-
nals tend to realise fewer arguments ? the average
in FrameNet is 1.46 arguments, compared to 1.74
PropBank
Verbs (Carreras and M`arquez, 2005) 80%
Nouns (Liu and Ng, 2007) 73%
FrameNet
Verbs (Mihalcea and Edmonds, 2005) 72%
Nouns (Pradhan et al, 2004) 64%
Table 1: F-Scores for supervised SRL (end-to-end)
for events/results. As our goal is nominal SRL, we
concentrate on the event/results class.
SRL for nominalisations. Compared to the wealth
of studies on verbal SRL (e.g., Gildea and Juraf-
sky (2002); Fleischman and Hovy (2003)), there
is relatively little work that specifically addresses
nominal SRL. Nouns are generally treated like
verbs: the task is split into two classification steps,
argument recognition (telling arguments from non-
arguments) and argument labelling (labelling recog-
nised arguments with a role). Nominal SRL also
typically draws on feature sets that are similar to
those for verbs, i.e., comprising mainly syntac-
tic and lexical-semantic information (Liu and Ng,
2007; Jiang and Ng, 2006).
On the other hand, there is converging evidence
that nominal SRL is somewhat more difficult than
verbal SRL. Table 1 shows some results for both
verbal and nominal SRL from the literature. For
both PropBank and for FrameNet, we find a differ-
ence of 7?8% F-Score. Note, however, that these
studies use different datasets and are thus not di-
rectly comparable.
In order to confirm the difference between nouns
and verbs, we modelled a controlled dataset (de-
scribed in detail in Sec. 4) of verbs and corre-
sponding event nominalisations. We used Shal-
maneser (Erk and Pad?o, 2006), to our knowledge
the only freely available SRL system that handles
nouns. SRL models were trained on verbs and
nouns separately, using the same settings and fea-
tures. Table 2 shows the results, averaged over 10
cross-validation (CV) folds. Accuracy was about
equal in the recognition step, and 5% higher for
verbs in the labelling step. We analysed these re-
sults by fitting a logit mixed model. These models
determine which fixed factors are responsible for
differences in a response variable (here: SRL per-
formance) while correcting for imbalances intro-
duced by random factors (see Jaeger (2008)). We
modelled the training and test set sizes and the pred-
icates? parts of speech as fixed effects, and frames
and CV folds as random factors.
For both argument recognition and labelling, the
666
Step Verbs Nouns
Arg recognition (F
1
, class FE) 0.59 0.60
Arg labelling (Accuracy) 0.70 0.65
Table 2: FrameNet SRL on verbs and nouns
amount of training data turned out to be a signifi-
cant factor, i.e., more data leads to higher results.
While the part of speech was not systematically
linked to performance for argument recognition,
it was a highly significant predictor of accuracy
in the labelling step: Even when training set size
was taken into account, verbal arguments were still
significantly easier to label (z=4.5, p<0.001).
In sum, these results lend empirical support to
claims that nominal arguments are less tightly cou-
pled to syntactic realisation than verbal ones (Carl-
son, 1984); their interpretation is harder to capture
with shallow cues.
3 Data Expansion for Nominal SRL
The previous section has established two observa-
tions. First, the argument structures of verbs and
their event nominalisations correspond largely. Sec-
ond, nominal SRL is a difficult task, even given
nominal training data, which is hard to obtain.
Our proposal in this paper is to take advan-
tage of the first observation to address the sec-
ond. We do so by modelling SRL for event nom-
inalisations as a data expansion task ? i.e., us-
ing existing verbal annotations to carry out SRL
for novel nominal instances. In this manner, we
do away completely with the need for manual
annotation of nominal instances that is required
for previous supervised approaches (cf. Sec. 2).
Consider the following examples, given in format
[constituent]
grammatical function/SEMANTIC ROLE
:
(1) a. [Peter]
Subj/COGNIZER
laughs
[about the joke]
PP-about/STIMULUS
b. [Peter]
Subj/COGNIZER
laughs
[at him]
PP-at/STIMULUS
(2) [Peter?s]
Prenom-Gen/?
[hearty]
Prenom-Mod/?
laughter [about the event]
PP-about/?
The sentences with the verbal predicate laugh in
(1) are labelled with semantic roles, while the NP
containing the event nominalisation laughter in (2)
is not. The question we face are what information
from (1) can be re-used to perform argument recog-
nition and labelling on (2), and how.
In this respect, there is a fundamental difference
between lexical-semantic and syntactic information.
Lexical-semantic features, such as the head word,
are basically independent of the predicate?s part of
speech. Thus, the information from (1) that Peter is
a COGNIZER can be used directly for the analysis of
the occurrence of Peter in (2). Unfortunately, pure
lexical features tend to be sparse: the head word
of the last role, event, is unseen in (1), and due to
its abstract nature, also difficult to classify through
semantic similarity. Therefore, it is necessary to
consider syntactic features as well. However, these
vary substantially between verbs and nouns. When
they are applicable to both parts of speech, some
mileage can be apparently gained: the phrase in (2)
headed by event can be classified as STIMULUS
because it is an about-PP like (1a). In contrast, no
direct inferences can be drawn about prenominal
genitives or modifiers which do not exist for verbs.
In the remainder of this paper, we will present
experiments on different ways of combining syn-
tactic and lexical-semantic information to balance
precision and recall in data expansion. We address
argument recognition and labelling separately, since
the two tasks require different kinds of information.
We assume that the frame has been determined be-
forehand with word sense disambiguation methods.
4 Data
The dataset for our study consists of the annotated
FrameNet 1.3 examples. We obtained pairs of verbs
and corresponding event/result nominalisations by
intersecting the FrameNet predicate list with a list
of nominalisations obtained from Celex (Baayen
et al, 1995) and Nomlex (Macleod et al, 1998).
We found 306 nominalisations with correspond-
ing verbs in the same frame, but excluded some
pairs where either the nominalisation was not of the
event/result type, or no annotated FrameNet exam-
ples were available for either verb or noun. The final
dataset, consisting of 265 pairs exemplifying 117
frames, served for both the analysis in Section 2 and
the evaluations in subsequent sections. For the eval-
uations, we used the 26,479 verbal role instances
(2,066 distinct role types) as training data and the
6,502 nominal role instances (993 distinct role
types) as test data. The specification of the dataset
can be downloaded from http://www.coli.
uni-sb.de/
?
pado/nom data.html.
5 Argument Recognition
Argument recognition is usually modelled as a su-
pervised machine learning task. Unfortunately, ar-
667
NP
PP
NP
NP
NP
Peter?s laughter about the joke
Figure 1: Parse tree for example sentence
gument recognition ? at least within predicates ? re-
lies heavily on syntactic features, with the grammat-
ical function (or alternatively syntactic path) feature
as the single most important predictor (Gildea and
Jurafsky, 2002). Since we are bootstrapping from
verbal instances to nominal ones, and since there is
typically considerable variation between nominal
and verbal subcategorisation patterns, we cannot
model argument recognition as a supervised task.
Instead, we follow up on an idea developed by Xue
and Palmer (2004) for verbal SRL, who charac-
terise the set of grammatical functions that could
fill a semantic role in the first place. In our apppli-
cation, we simply extract all syntactic arguments of
the nominalisation, including any premodifiers. We
make no attempt to distinguish between adjuncts
and compulsory arguments. Fig. 1 shows an exam-
ple: in the NP Peter?s laughter about the joke, the
noun laughter has two syntactic arguments: the PP
about the joke and the premodifying NP Peter?s.
Both are extracted as (potential) arguments.
This method cannot identify roles that are syntac-
tically non-local, i.e., those that are not in the max-
imal projection of the frame evoking noun. Such
roles are more common for nouns than for verbs.
Example 3 shows that an ?external? NP like Bill can
be analysed as filling the HELPER role of the noun
help. However, the overall proportion of non-local
roles is still fairly small in our data (around 10%).
(3) [Bill]
HELPER
offered help in case of need.
Table 3 gives the argument recognition results for
our rule-based system on all roles in the gold stan-
dard and on the local roles alone. This simple ap-
proach is surprisingly effective, achieving an over-
all F-Measure of 76.89% on all roles, while on local
roles the F-Measure increases to 82.83% due to the
higher recall. Precision is 82.01%, as not all syn-
tactic arguments do fill a role. For example, modal
modifiers such as hearty in (2) rarely fill a (core)
role in FrameNet. False-negative errors, which af-
fect recall, are partly due to parser errors and partly
Roles Precision Recall F-Measure
all roles 82.01 72.37 76.89
local roles 82.01 83.66 82.83
Table 3: Argument recognition (local / all roles).
to role fillers that do not correspond to constituents
or that are embedded in syntactic arguments. For in-
stance, in (4) the PP in this country, which fills the
PLACE role of cause, is embedded in the PP of suf-
fering in this country, which fills the role EFFECT.
We extract only the larger PP.
(4) the causes [of suffering
[in this country]
PP-in
]
PP-of
6 Argument Labelling
Argument labelling presents a different picture
from argument recognition. Here, both syntactic
and lexical-semantic information contribute to suc-
cess in the task. We present three model families
for nominal argument labelling that take different
stances with respect to this observation.
The first (naive-semantic) and the second (naive-
syntactic) model families represent extreme posi-
tions that attempt to re-use verbal information as
directly as possible. Models from the third fam-
ily, distributional models infer the role of a noun?s
arguments by computing the semantic similarity
between nominal arguments and semantic represen-
tations of the verb roles given by the role fillers?
semantic heads.2 In the lexical-level instantiation,
the mapping is established between individual noun
arguments and roles. In the function-level instantia-
tion, complete nominal grammatical functions are
mapped onto roles.
3
6.1 Naive semantic model
The naive semantic model (naive sem) implements
the assumption that lexical-semantic features pro-
vide the same predictive evidence for verbal and
nominal arguments (cf. Sec. 3). It can be thought of
as constructing the trivial identity mapping between
the values of nominal and verbal semantic features.
To test the usefulness of this model, we train the
Shalmaneser SRL system (Erk and Pad?o, 2006)
on the verbal instances of the dataset described
2Usually, the semantic head of a phrase is its syntactic head.
Exceptions occur e.g. for PPs, where the semantic head is the
syntactic head of the embedded NP.
3We compute grammatical functions as phrase types plus
relative position; for PPs, we add the preposition.
668
in Sec. 4, using only the lexical-semantic features
(head word, first word, last word). We then apply
the resulting models directly to the corresponding
nominal instances.
6.2 Naive syntactic model
The intuition of this model (naive syn) is that gram-
matical functions shared between verbs and nouns
are likely to express the same semantic roles. It
maps all grammatical functions of a verb g
v
onto
the identical functions of the corresponding noun
g
n
and then assigns the most frequent role realised
by g
v
to all arguments with grammatical function
g
n
. For example, if PPs headed by about for the
verb laugh typically realise the STIMULUS role, all
arguments of the noun laughter which are realised
as PP-about are also assigned the STIMULUS role.
We predict that this strategy has a ?high
precision?low recall? profile: It produces reliable
mappings for those grammatical functions that are
preserved across verb and noun, in particular prepo-
sitional phrases; however, it fails for grammatical
functions that only occur for one part of speech.
This problem becomes particular pertinent for
two prominent role types, namely AGENT-style
roles (deep subjects) and PATIENT-style roles (deep
objects). These roles are usually expressed via dif-
ferent and ambiguous noun and verb functions
(Gurevich et al, 2006). For verbs, the AGENT
is typically expressed by the Subject, while for
nouns it is expressed by a Pre-Modifier. The PA-
TIENT is commonly realised as the Object for
verbs, and either as a Pre-Modifier or as a PP-of
for nouns. As the noun?s Pre-Modifier is highly
ambiguous, it is also ineffective to apply a non-
identity mapping such as (subject
v
,Pre-Modifier
n
)
or (object
v
,Pre-Modifier
n
).
4
A final variation of this model is the generalised
naive syntactic model (naive sem-gen), where we
assign the role most frequently realised by a given
function across all verbs in the frame. This method
alleviates data sparseness stemming from functions
never seen with particular verbs and is fairly safe,
since mapping within frames tends to be uniform.
6.3 Distributional models
The distributional models construct mappings be-
tween verbal and nominal semantic heads. In con-
4Lapata (2002) has shown that the mapping can be dis-
ambiguated for individual nominalisations. Her model, using
lexical-semantic, contextual and pragmatic information, is out-
side the scope of the present paper.
trast to the naive semantic model, they make use of
some measure of semantic similarity to find map-
pings, and optionally use syntactic constraints to
guide generalisation. In this manner, distributional
models can deal with unseen feature values more
effectively. In sentences (1) and (2), for example,
an ideal distributional model would find the head
word event in (2) to be more semantically similar
to the head joke in (1a) than to head him in (1b).
The resulting mapping (joke, event) leads to the
assignment of the role STIMULUS to event.
Semantic Similarity. Semantic similarity mea-
sures are commonly used to compute similarity
between two lexemes. There are two main types
of similarity: Ontology-based, computed through
the closeness of two lexemes in a lexical database
(e.g., WordNet); and distributional, given by some
measure of the distance between the lexemes? vec-
tor representations in a semantic co-occurrence
space. We chose the latter approach because it tends
to have a higher coverage and it is knowledge-lean,
requiring just an unannotated corpus.
We compute distributional-similarity with a
semantic space model based on lexical co-
occurrences backed by syntactic relations (Pad?o
and Lapata, 2007).5 The model is constructed from
the British National Corpus (BNC), using the 2.000
most pairs of words and grammatical functions as
dimensions. As similarity measure, we use cosine
distance on log-likelihood transformed counts.
Lexical level model. The lexical level model
(dist-lex) assigns to each nominal argument the verb
role that it is semantically most similar to. Each role
is represented by the semantic heads of its fillers.
For example, suppose that the role STIMULUS of
the verb laugh has been realised by the heads story,
scene, joke, and tale. Then, in ?Peter?s laughter
about the event?, we analyse event as STIMULUS,
since event is similar to these heads.
Formally, each argument head l is represented
by a co-occurrence vector ~l. A verb role r
v
? R
v
is
modelled by the centroid ~r
v
of its instances? heads:
~r
v
=
1
|L
r
v
|
?
l?L
r
v
~
l
Roles are assigned to nominal argument heads l
n
?
L
n
by finding the semantically most similar role r
5We also experimented with bag-of-words based vector
spaces, which showed worse performance throughout.
669
while the grammatical function g
n
is ignored:
r(l
n
, g
n
) = argmax
r
v
?R
v
sim
cos
(
~
l
n
, ~r
v
)
Function level model. The syntactic level model
(dist-fun) generalises the lexical level model by ex-
ploiting the intuition that, within nouns, most se-
mantic roles tend to be consistently realised by one
specific grammatical function. This function can
be identified as the one most semantically similar
to the role?s representation. Following the exam-
ple above, suppose that the grammatical function
PP-about of laughter has as semantic heads the
lexemes: event, story, news. Then, it is likely to
express the role STIMULUS, as its heads are seman-
tically similar to those of the verbal fillers of this
role: story, scene, sentence, tale. For each nomi-
nalisation, this model constructs mappings (r
v
, g
n
)
between a verbal semantic role r
v
and a nominal
grammatical function g
n
. The representations for
roles are computed as described above. We com-
pute the semantic representations for grammatical
functions, in parallel to the roles? definition above,
as the centroid of their fillers? representations L
g
n
:
~g
n
=
1
|L
g
n
|
?
l?L
g
n
~
l
The assignment of a role to a nominal arguments
is now determined by the argument?s grammatical
function g
n
; its lemma l
n
only enters indirectly, via
the similarity computation:
r(l
n
, g
n
) = argmax
r
v
?R
v
sim
cos
(~r
v
, ~g
n
)
This strategy guarantees that each nominal gram-
matical function is mapped to exactly one role. In
the inverse direction, roles can be left unmapped or
mapped to more than one function.
6
6.4 Hybrid models
Our last class combines the naive and distributional
models with a back-off approach. We first attempt
to harness the reliable naive syntactic approach
whenever a mapping for the argument?s grammati-
cal function is available. If this fails, it backs off to a
distributional model. This strategy helps to recover
the frequent AGENT- and PATIENT-style roles that
cannot be recovered on syntactic grounds.
6We also experimented with a global optimisation strategy
where we maximised the overall similarity between roles and
functions subject to different constraints (e.g., perfect match-
ing). Unfortunately, this strategy did not improve results.
System Accuracy
baseline random 17.09
B
L
baseline most common 42.97
naive syn 15.29
naive syn-gen 21.56
N
a
i
v
e
naive sem 24.00
dist-lex 44.57
D
i
s
t
dist-fun 52.00
naive syn + dist-lex 48.22
naive syn-gen + dist-lex 50.54
naive syn + dist-fun 54.39
H
y
b
r
i
d
naive syn-gen + dist-fun 56.42
Table 4: Results for nominal argument labelling
In (2), a hybrid model would assign the
role STIMULUS to the argument headed by
event, using the naive syntactic mapping
(PP-about
v
,PP-about
n
) derived from (1a). For the
prenominal modifier, no syntactic mapping is avail-
able; thus, it backs off to lexical-semantic evidence
from (1a-b) to analyse Peter as COGNIZER.
We experiment with two hybrid models: naive
syntactic plus lexical level distributional (naive syn
+ dist-lex), and naive syntactic plus functional level
distributional (naive syn + dist-fun).
7 Experimental results
The results of our experiments are reported in Ta-
ble 4. The models are compared against two base-
lines: A random baseline which randomly chooses
one of the verb roles for each of the arguments of
the corresponding noun; a most common baseline
which assigns to each nominal argument the most
frequent role of the corresponding verb ? i.e. the
role which has most fillers. All models with the
exception of naive syn significantly outperform the
random baseline, but only dist-fun and all hybrid
models outperform the most common baseline.
In general, the best performing methods are the
hybrid ones, with best accuracy achieved by naive
syn-gen + dist-fun. Non-hybrid approaches always
have lower accuracy. This validates our main hy-
pothesis in this paper, namely that the combination
of syntactic information with distributional seman-
tics is a promising strategy.
Matching our predictions, the low accuracy of
the naive syntactic model is mainly due to a lack
of coverage. In fact, the model leaves 5,010 of the
6,502 gold standard noun fillers unassigned since
they realise syntactic roles that are unseen for the
verbs in question. A large part of these are Pre-
Modifier and PP-of functions, which are central
for nouns, but mostly ungrammatical for verbs. On
670
the 1,492 fillers for which a role was assigned, the
model obtains an accuracy of 67%, indicating a rea-
sonably high, but not perfect, accuracy for shared
grammatical functions. The remaining errors stem
from two sources. First, many grammatical func-
tions are ambiguous, causing wrong assignments
by a ?syntax-only? model. For example, PP-in can
indicate both TIME and PLACE for many nominal-
isations.Second, a certain number of grammatical
functions do not preserve their role between verb to
noun (Hull and Gomez, 1996). For example, PP-to
realises the MESSAGE role of the verb require but
the ADDRESSEE role of the noun request.
Distributional models show in general better per-
formance than the naive syntactic approach (ap-
prox. +25% accuracy). They do not suffer from the
coverage problem, since they assign a role to each
filler. Yet, the accuracy over assigned roles is lower
than for the syntactic approach (52% for dist-fun).
We conclude that in the limited cases where a
pure syntactic mapping is applicable, it is far more
reliable than methods which are mainly based on
lexical-semantic information. The major limitation
of the latter is that lexical-semantics tend to fail
when roles are semantically very similar. For ex-
ample, for the noun announcement, the syntactic-
level distributional model wrongly builds the map-
ping (ADDRESSEE, PP-by) instead of (SPEAKER,
PP-by), because the two roles are very similar se-
mantically (the computed similarities of the PP-by
arguments to ADDRESSEE and SPEAKER in the
semantic space are 0.94 and 0.92, respectively).
The syntactic-level distributional model outper-
forms the lexical-level, suggesting that generalising
the mapping at the argument level offers more sta-
ble statistical evidence to find the correct role, i.e. a
set of noun arguments better defines the seman-
tics of the mapping than a single argument. This
is mostly the case when the context vector of the
argument is not a good representation because the
semantic head is ambiguous, infrequent or atypical.
Consider, for example, the following sentence for
the noun violation:
(5) Sterne?s Tristram Shandy consists
of a series of violations [of literary
conventions]
PP-OF/NORM
The syntactic-level model builds the correct map-
ping (NORM, PP-of ), as the role fillers of the verb
violate (e.g. principle, right, treaty, law) are very
similar to the noun?s category fillers (e.g. conven-
tion, rule, agreement, treaty, norm), causing the cen-
troids of NORM and PP-of to be close in the space.
The lexical-level model, however, builds the incor-
rect mapping (PROTAGONIST, convention). This
happens because convention is ambiguous, and one
of its senses (?a large formal assembly?) is compat-
ible with the PROTAGONIST role, and happens to
have a large influence on the position of the vector
for convention. Unfortunately, this is not the sense
in which the word is used in this sentence.
8 Conclusions
We have presented a data expansion approach to
SRL for event nominalisations. Instead of relying
on manually annotated nominal training data, we
harness annotated data for verbs to bootstrap a se-
mantic role labeller for nouns. For argument recog-
nition, we use a simple rule-based approach. For
argument labelling, we profit from the fact that the
argument structures of event nominalisations and
the corresponding verbs are typically similar. This
allows us to learn a mapping between verbal roles
and nominal arguments, using syntactic features,
lexical-semantic similarity, or both.
We found that our rule-based approach for argu-
ment recognition works fairly well. For argument
labelling, our approach does not yet attain the per-
formance of supervised models, but has the crucial
advantage of not requiring any labelled data for
nominal predicates.
We achieved the highest accuracy with a hybrid
syntactic-semantic model, which indicates that both
types of information need to be combined for op-
timal results. A purely syntactic approach results
in a high precision, but low coverage because fre-
quent grammatical functions in particular cannot be
trivially mapped. Backing off to semantic similarity
provides additional coverage. However, semantic
similarity has to be considered on the level of com-
plete functions rather than individual instances to
promote ?uniformity? in the mappings.
In this paper, we have only considered nominal
SRL by data expansion, i.e. we only applied our
approach to those nominalisations for which we
have annotated data for the corresponding verbs.
However, even if no data is available for the corre-
sponding verb, it might still be possible to bootstrap
from other verbs in the same frame (assuming that
the frame is known for the nominalisation) and we
plan to pursue this idea in furture research. We also
intend to investigate whether a joint optimisation of
671
the mapping constrained by additional syntactic in-
formation such as subcategorisation frames leads to
better results. Finally, we will verify that our meth-
ods, which we have evaluated on English FrameNet
data, carry over to other corpora and languages.
Acknowledgments. Our work was partly funded
by the German Research Foundation DFG (grant
PI 154/9-3).
References
Baayen, R., R. Piepenbrock, and L. Gulikers, 1995. The
CELEX Lexical Database (Release 2). LDC.
Carlson, G. 1984. Thematic roles and their role in se-
mantic interpretation. Linguistics, 22:259?279.
Carreras, X. and L. M`arquez, editors. 2005. Proceed-
ings of the CoNLL shared task: Semantic role la-
belling, Ann Arbor, MI.
Erk, K. and S. Pad?o. 2006. Shalmaneser ? a flexible
toolbox for semantic role assignment. In Proceed-
ings of LREC, Genoa, Italy.
Erk, K., A. Kowalski, S. Pad?o, and M. Pinkal. 2003.
Towards a resource for lexical semantics: A large
German corpus with extensive semantic annotation.
In Proceedings of ACL, pages 537?544, Sapporo,
Japan.
Fillmore, C., C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. International Journal of Lexi-
cography, 16:235?250.
Fleischman, M. and E. Hovy. 2003. Maximum entropy
models for FrameNet classification. In Proceedings
of EMNLP, pages 49?56, Sapporo, Japan.
Frank, A., H.-U. Krieger, F. Xu, H. Uszkoreit, B. Crys-
mann, B. J?org, and U. Sch?afer. 2007. Question an-
swering from structured knowledge sources. Journal
of Applied Logic, 5(1):20?48.
Gildea, D. and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
Gordon, A. and R. Swanson. 2007. Generalizing
semantic role annotations across syntactically simi-
lar verbs. In Proceedings of ACL, pages 192?199,
Prague, Czechia.
Grenager, T. and C. Manning. 2006. Unsupervised dis-
covery of a statistical verb lexicon. In Proceedings
of EMNLP, pages 1?8, Sydney, Australia.
Grimshaw, J. 1990. Argument Structure. MIT Press.
Gurevich, O., R. Crouch, T. King, and V. de Paiva.
2006. Deverbal nouns in knowledge representation.
In Proceedings of FLAIRS, pages 670?675, Mel-
bourne Beach, FL.
Hull, R. and F. Gomez. 1996. Semantic interpretation
of nominalizations. In Proceedings of AAAI, pages
1062?1068, Portland, OR.
Jaeger, T. 2008. Categorical data analysis: Away from
ANOVAs and toward Logit Mixed Models. Journal
of Memory and Language. To appear.
Jiang, Zheng Ping and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proceedings of EMNLP, pages 138?145,
Sydney, Australia.
Lapata, M. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28(3):357?388.
Liu, C. and H. Ng. 2007. Learning predictive structures
for semantic role labeling of NomBank. In Proceed-
ings of ACL, pages 208?215, Prague, Czechia.
Macleod, C., R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998. Nomlex: A lexicon of nominaliza-
tions. In Proceedings of EURALEX, Li`ege, Belgium.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004. An-
notating Noun Argument Structure for NomBank. In
Proceedings of LREC, Lisbon, Portugal.
Mihalcea, Rada and Phil Edmonds, editors. 2005.
Proceedings of Senseval-3: The Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain.
Moschitti, A., P. Morarescu, and S. Harabagiu. 2003.
Open-domain information extraction via automatic
semantic labeling. In Proceedings of FLAIRS, pages
397?401, St. Augustine, FL.
Nunes, M. 1993. Argument linking in English de-
rived nominals. In Valin, Robert D. Van, editor, Ad-
vances in Role and Reference Grammar, pages 372?
432. John Benjamins.
Pad?o, S. and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Pradhan, S., H. Sun, W. Ward, J. Martin, and D. Ju-
rafsky. 2004. Parsing arguments of nominaliza-
tions in English and Chinese. In Proceedings of
HLT/NAACL, pages 141?144, Boston, MA.
Quirk, R., S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman.
Swier, R. and S. Stevenson. 2004. Unsupervised se-
mantic role labelling. In Proceedings of EMNLP,
pages 95?102.
Xue, N. and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP,
pages 88?94, Barcelona, Spain.
672
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 457?465,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic induction of FrameNet lexical units
Marco Pennacchiotti(?), Diego De Cao(?), Roberto Basili(?), Danilo Croce(?), Michael Roth(?)
(?) Computational Linguistics
Saarland University
Saarbru?cken, Germany
{pennacchiotti,mroth}@coli.uni-sb.de
(?) DISP
University of Roma Tor Vergata
Roma, Italy
{decao,basili,croce}@info.uniroma2.it
Abstract
Most attempts to integrate FrameNet in NLP
systems have so far failed because of its lim-
ited coverage. In this paper, we investigate the
applicability of distributional and WordNet-
based models on the task of lexical unit induc-
tion, i.e. the expansion of FrameNet with new
lexical units. Experimental results show that
our distributional and WordNet-based models
achieve good level of accuracy and coverage,
especially when combined.
1 Introduction
Most inference-based NLP tasks require a large
amount of semantic knowledge at the predicate-
argument level. This type of knowledge allows to
identify meaning-preserving transformations, such
as active/passive, verb alternations and nominal-
izations, which are crucial in several linguistic in-
ferences. Recently, the integration of NLP sys-
tems with manually-built resources at the predi-
cate argument-level, such as FrameNet (Baker et
al., 1998) and PropBank (Palmer et al, 2005) has
received growing interest. For example, Shen and
Lapata (2007) show the potential improvement that
FrameNet can bring on the performance of a Ques-
tion Answering (QA) system. Similarly, several
other studies (e.g. (Bar-Haim et al, 2005; Garoufi,
2007)) indicate that frame semantics plays a central
role in Recognizing Textual Entailment (RTE). Un-
fortunately, most attempts to integrate FrameNet or
similar resources in QA and RTE systems have so
far failed, as reviewed respectively in (Shen and La-
pata, 2007) and (Burchardt and Frank, 2006). These
studies indicate limited coverage as the main reason
of insuccess. Indeed, the FrameNet database only
contains 10,000 lexical units (LUs), far less than
the 210,000 entries in WordNet 3.0. Also, frames
are based on more complex information than word
senses, so that their manual development is much
more demanding (Burchardt et al, 2006; Subirats
and Petruck, 2003).
Therefore, there is nowadays a pressing need to
adopt learning approaches to extend the coverage
of the FrameNet lexicon by automatically acquiring
new LUs, a task we call LU induction, as recently
proposed at SemEval-2007 (Baker et al, 2007). Un-
fortunately, research in this area is still somehow
limited and fragmentary. The aim of our study is
to pioneer in this field by proposing two unsuper-
vised models for LU induction, one based on dis-
tributional techniques and one using WordNet as a
support; and a combined model which mixes the
two. The goal is to investigate to what extent distri-
butional and WordNet-based models can be used to
induce frame semantic knowledge in order to safely
extend FrameNet, thus limiting the high costs of
manual annotation.
In Section 2 we introduce the LU induction task
and present related work. In Sections 3, 4 and 5 we
present our distributional, WordNet-based and com-
bined models. Then, in Section 6 we report experi-
mental results and comparative evaluations. Finally,
in Section 7 we draw final conclusions and outline
future work.
2 Task Definition and Related Work
As defined in (Fillmore, 1985), a frame is a con-
ceptual structure modeling a prototypical situation,
evoked in texts through the occurrence of its lex-
ical units. A lexical unit (LU) is a predicate that
linguistically expresses the situation of the frame.
Lexical units of the same frame share semantic ar-
guments. For example the frame KILLING has lex-
ical units such as assassin, assassinate, blood-bath,
fatal, murderer, kill, suicide that share semantic ar-
guments such as KILLER, INSTRUMENT, CAUSE,
VICTIM. Building on this frame-semantic model,
the Berkeley FrameNet project (Baker et al, 1998)
has been developing a frame-semantic lexicon for
457
the core vocabulary of English since 1997. The
current FrameNet release contains 795 frames and
about 10,000 LUs. Part of FrameNet is also a cor-
pus of 135,000 annotated example sentences from
the British National Corpus (BNC).
LU induction is a fairly new task. Formally,
it can be defined as the task of assigning a
generic lexical unit not yet present in the FrameNet
database (hereafter called unknown LU) to the cor-
rect frame(s). As the number of frames is very
large (about 800) the task is intuitively hard to solve.
A further complexity regards multiple assignments.
Lexical units are sometimes ambiguous and can then
be mapped to more than one frame (for example
the word tea could map both to FOOD and SO-
CIAL EVENT). Also, even unambiguous words can
be assigned to more than one frame ? e.g. child maps
to both KINSHIP and PEOPLE BY AGE.
LU induction is relevant to many NLP tasks, such
as the semi-automatic creation of new FrameNets,
and semantic role labelling. LU induction has been
integrated at SemEval-2007 as part of the Frame Se-
mantic Structure Extraction shared task (Baker et
al., 2007), where systems are requested to assign
the correct frame to a given LU, even when the
LU is not yet present in FrameNet. Johansson and
Nugues (2007) approach the task as a machine learn-
ing problem: a Support Vector Machine trained on
existing LUs is applied to assign unknown LUs to
the correct frame, using features derived from the
WordNet hierarchy. Tested on the FrameNet gold
standard, the method achieves an accuracy of 0.78,
at the cost of a low coverage of 31% (i.e. many LUs
are not assigned). Johansson and Nugues (2007)
also experiment with a simple model based on stan-
dard WordNet similarity measures (Pedersen et al,
2004), achieving lower performance. Burchardt and
colleagues (2005) present Detour, a rule-based sys-
tem using words in a WordNet relation with the un-
known LU to find the correct frame. The system
achieves an accuracy of 0.39 and a coverage of 87%.
Unfortunately this algorithm requires the LU to be
previously disambiguated, either by hand or using
contextual information.
In a departure from previous work, our first model
leverages distributional properties to induce LUs, in-
stead of relying on pre-existing lexical resources as
WordNet. This guarantees two main advantages.
First, it can predict a frame for any unknown LU,
while WordNet based approaches can be applied
only to words having a WordNet entry. Second, it
allows to induce LUs in languages for which Word-
Net is not available or has limited coverage. Our
second WordNet-based model uses sense informa-
tion to characterize the frame membership for un-
known LU, by adopting a semantic similarity mea-
sure which is sensitive to all the known LUs of a
frame.
3 Distributional model
The basic idea behind the distributional approach is
to induce new LUs by modelling existing frames and
unknown LUs in a semantic space, where they are
represented as distributional co-occurrence vectors
computed over a corpus.
Semantic spaces are widely used in NLP for rep-
resenting the meaning of words or other lexical en-
tities. They have been successfully applied in sev-
eral tasks, such as information retrieval (Salton et al,
1975) and harvesting thesauri (Lin, 1998). The intu-
ition is that the meaning of a word can be described
by the set of textual contexts in which it appears
(Distributional Hypothesis (Harris, 1964)), and that
words with similar vectors are semantically related.
In our setting, the goal is to find a semantic space
model able to capture the notion of frame ? i.e. the
property of ?being characteristic of a frame?. In
such a model, an unknown LU is induced by first
computing the similarity between its vector and the
vectors of the existing frames, and then assigning the
LU to the frame with the highest similarity.
3.1 Assigning unknown LUs to frames
In our model, a LU l is represented by a vector ~l
whose dimensions represent the set of contexts C
of the semantic space. The value of each dimen-
sion is given by the co-occurrence value of the LU
with a contextual feature c ? C, computed over a
large corpus using an association measure. We ex-
periment with two different association measures:
normalized frequency and pointwise mutual infor-
mation. We approximate these measures by using
Maximum Likelihood Estimation, as follows:
458
F (l, c) =MLE |l, c||?, ?|
MI(l, c) =MLE |l, c||?, ?||?, c||l, ?|
(1)
where |l, c| denotes the co-occurrence counts
of the pair (l, c) in the corpus, |?, c| =?
l?L |l, c|, |l, ?| =
?
c?C |l, c| and finally |?, ?| =?
l?L,c?C |l, c|.
A frame f is modeled by a vector ~f , representing
the distributional profile of the frame in the seman-
tic space. We here assume that a frame can be fully
described by the set of its lexical units F . We imple-
ment this intuition by computing ~f as the weighted
centroid of the set F , as follows:
~f =
?
l?F
wlf ?~l (2)
where wlf is a weighting factor, accounting for
the relevance of a given lexical unit with respect to
the frame, estimated as:
wlf = |l|?
l?F
|l|
(3)
where |l| denotes the counts of l in the corpus.
From a more cognitive perspective, the vector ~f rep-
resents the prototypical lexical unit of the frame.
Given the set of all framesN and an unknown lex-
ical unit ul, we assign ul to the frame fmaxul which
is distributionally most similar ? i.e. we intuitively
map an unknown lexical unit to the frame whose
prototypical lexical unit ~f has the highest similarity
with ~ul:
fmaxul = argmaxf?N simD(~ul, ~f) (4)
In our model, we used the traditional cosine simi-
larity:
simcos(ul, f) =
~ul ? ~f
|~ul| ? |~f |
(5)
3.2 Choosing the space
Different types of contexts C define spaces with dif-
ferent semantic properties. We are here looking for
a space able to capture the properties which charac-
terise a frame. The most relevant of these properties
is that LUs in the same frame tend to be either co-
occurring or substitutional words (e.g. assassin/kill
or assassinate/kill) ? i.e. they are either in paradig-
matic and syntagmatic relation. In an ideal space,
a high similarity value simD would be then given
both to assassinate/kill and to assassin/kill. We ex-
plore three spaces which seem to capture the above
property well:
Word-based space: Contexts are words appear-
ing in a n-window of the lexical unit. Such spaces
model a generic notion of semantic relatedness.
Two LUs close in the space are likely to be re-
lated by some type of generic semantic relation,
either paradigmatic (e.g. synonymy, hyperonymy,
antonymy) or syntagmatic (e.g. meronymy, concep-
tual and phrasal association).1
Syntax-based space: Contexts are syntactic re-
lations (e.g. X-VSubj-man where X is the LU), as
described in (Pado?, 2007). These spaces are good
at modeling semantic similarity. Two LUs close in
the space are likely to be in a paradigmatic relation,
i.e. to be close in a is-a hierarchy (Budanitsky and
Hirst, 2006; Lin, 1998; Pado?, 2007). Indeed, as con-
texts are syntactic relations, targets with the same
part of speech are much closer than targets of differ-
ent types.
Mixed space: In a combination of the two above
spaces, contexts are words connected to the LU by a
dependency path of at most length n. Unlike word-
based spaces, contexts are selected in a more princi-
pled way: only syntactically related words are con-
texts, while other (possibly noisy) material is filtered
out. Unlike syntax-based spaces, the context c does
not explicitly state the type of syntactic relation with
the LU: this usually allows to capture both paradig-
matic and syntagmatic relations.
4 WordNet-based model
In a departure from previous work, our WordNet-
based model does not rely on standard WordNet sim-
ilarity measures (Pedersen et al, 2004), as these
measures can only be applied to pairs of words,
while we here need to capture the meaning of whole
frames, which typically consist of larger sets of LUs.
Our intuition is that senses able to evoke a frame can
be detected via WordNet, by jointly considering the
WordNet synsets activated by all LUs of the frame.
We implement this intuition in a weakly-
supervised model, where each frame f is repre-
sented as a set of specific sub-graphs of the WordNet
1See (Pado?, 2007; Sahlgren, 2006) for an in depth analysis.
459
hyponymy hierarchy. As different parts of speech
have different WordNet hierarchies, we build a sub-
graph for each of them: Snf for nouns, Svf for verbs
and Saf for adjectives.2 These sub-graphs repre-
sent the lexical semantic properties characterizing
the frame. An unknown LU ul of a given part of
speech is assigned to the frame whose correspond-
ing sub-graph is semantically most similar to one of
the senses of ul:
fmaxul = argmaxf?N simWN (ul, f) (6)
where simWN is a WordNet-based similarity
measure. In the following subsections we will de-
scribe how we build sub-graphs and model the sim-
ilarity measure for the different part of speech.
Figure 1 reports an excerpt of the noun sub-
graph for the frame PEOPLE BY AGE, cover-
ing the suitable senses of its nominal LUs
{adult, baby, boy, kid, youngster, youth}. The
relevant senses (e.g. sense 1 of youth out of the 6
potential ones) are generally selected, as they share
the most specific generalizations in WordNet with
the other words.
Nouns. To compute similarity for nouns we adopt
conceptual density (cd) (Agirre and Rigau, 1996),
a semantic similarity model previously applied to
word sense disambiguation tasks.
Given a frame f and its set of nominal lexical
units Fn, the nominal subgraph Snf is built as fol-
lows. All senses of all words in Fn are activated
in WordNet. All hypernyms Hnf of these senses are
then retrieved. Every synset ? ? Hnf is given a cd
score, representing the density of the WordNet sub-
hierarchy rooted at ? in representing the set of nouns
Fn. The intuition behind this model is that the larger
the number of LUs in Fn that are generalized by ? is,
the better it captures the lexical semantics intended
by the frame f . Broader generalizations are penal-
ized as they give rise to bigger hierarchies, not well
correlated with the full set of targets Fn.
To build the final sub-graph Snf , we apply the
greedy algorithm proposed by Basili and colleagues
(2004). It first computes the set of WordNet synsets
that generalize at least two LUs in Fn, and then se-
lects the subset of most dense ones Snf ? Hnf that
2Our WordNet model does not cover the limited number of
LUs which are not nouns, verbs or adjectives.
cover Fn. If a LU has no common hypernym with
other members of Fn, it is not represented in Snf , and
its similarity is set to 0 . Snf disambiguates words in
Fn as only the lexical senses with at least one hyper-
nym in Snf are considered.
Figure 1 shows the nominal sub-graph automati-
cally derived using conceptual density for the frame
PEOPLE BY AGE. The word boy is successfully dis-
ambiguated, as its only hypernym in the sub-graph
refers to its third sense (a male human offspring)
which correctly maps to the given frame. Notice
that this model departs from the first sense heuris-
tics largely successful in word sense disambigua-
tion: most frames in fact are characterized by non
predominant senses. The only questionable disam-
biguation is for the word adult: the wrong sense
(adult mammal) is selected. However, even in these
cases, the cd values are very low (about 10?4), so
that they do not impact much on the quality of the
resulting inference.
Figure 1: The noun sub-graph for the frame PEO-
PLE BY AGE as evoked by a subset of the words. Sense
numbers #n refers to WordNet 2.0.
Using this model, LU induction is performed as
follows. Given an unknown lexical unit ul, for each
frame f ? N we first build the sub-graph Snf from
the set Fn ? {ul}. We then compute simWN (f, ul)
as the maximal cd of any synset ? ? Snf that gener-
alizes one of the lexical senses of ul. In the example
baby would receive a score of 0.117 according to its
first sense in WordNet 2.0 (?baby,babe,infant?). In
a final step, we assign the LU to the most similar
frame, according to Eq. 6
Verbs and Adjectives. As the conceptual density
algorithm can be used only for nouns, we apply dif-
ferent similarity measures for verbs and adjectives.
460
For verbs we exploit the co-hyponymy relation:
the sub-graph Svf is given by all hyponyms of all
verbs Fv in the frame f . Similarity simWN (f, ul)
is computed as follows:
simWN (ul, f) =
?
???
???
1 iff ?K ? F such that
|K| > ? AND
?l ? K, l is a co-hyponym of ul
? otherwise
(7)
As for adjectives, WordNet does not provide a hy-
ponymy hierarchy. We then compute similarity sim-
ply on the basis of the synonymy relation, as fol-
lows:
simWN (ul, f) =
?
?
?
1 iff ?l ? F such that
l is a synonym of ul
? otherwise
(8)
5 Combined model
The methods presented so far use two independent
information sources to induce LUs: distributional
similarity simD and WordNet similarity simWN .
We also build a joint model, leveraging both ap-
proaches: we expect the combination of different
information to raise the overall performance. We
here choose to combine the two approaches using a
simple back-off model, that uses the WordNet-based
model as a default and backs-off to the distributional
one when no frame is proposed by the former. The
intuition is that WordNet should guarantee the high-
est precision in the assignment, while distributional
similarity should recover cases of low coverage.
6 Experiments
In this section we present a comparative evaluation
of our models on the task of inducing LUs, in a
leave-one-out setting over a reference gold standard.
6.1 Experimental Setup
Our gold standard is the FrameNet 1.3 database,
containing 795 frames and a set L of 7,522 unique
LUs (in all there are 10,196 LUs possibly assigned
to more than one frame). Given a lexical unit l ? L,
we simulate the induction task by executing a leave-
one-out procedure, similarly to Burchardt and col-
leagues (2005). First, we remove l from all its origi-
nal frames. Then, we ask our models to reassign it to
the most similar frame(s) f , according to the simi-
larity measure3. We repeat this procedure for all lex-
ical units. Though our experiment is not completely
realistic (we test over LUs already in FrameNet), it
has the advantage of a reliable gold standard pro-
duced by expert annotators. A second, more re-
alistic, small-scale experiment is described in Sec-
tion 6.2.
We compute accuracy as the fraction of LUs in L
that are correctly re-assigned to the original frame.
Accuracy is computed at different levels k: a LU l is
correctly assigned if its gold standard frame appears
among the best-k frames f ranked by the model us-
ing the sim(l, f) measure. As LUs can have more
than one correct frame, we deem as correct an as-
signment for which at least one of the correct frames
is among the best-k.
We also measure coverage, intended as the per-
centage of LUs that have been assigned to at least
one frame by the model. Notice that when no
sense preference can be found above the threshold ?,
the WordNet-based model cannot predict any frame,
thus decreasing coverage.
We present results for the following models and
parametrizations (further parametrizations have re-
vealed comparable performance).
Dist-word : the word-based space described in
Section 3. Contextual features correspond to the
set of the 4,000 most frequent words in the BNC.4
The association measure between LUs and contexts
is the pointwise mutual information. Valid contexts
for LUs are fixed to a 20-window.
Dist-syntax : the syntax-based space described
in Section 3. Context features are the 10,000 most
frequent syntactic relations in the BNC5. As associ-
ation measure we apply log-likelihood ratio (Dun-
ning, 1993) to normalized frequency. Syntactic rela-
tions are extracted using the Minipar parser.
Dist-mixed : the mixed space described in Sec-
3In the distributional model, we recompute the centroids for
each frame f in which the LU appeared, applying Eq. 2 to the
set F ? {l}.
4We didn?t use the FrameNet corpus directly, as it is too
small to obtain reliable statistics.
5Specifically, we use the minimum context selection func-
tion and the plain path value function described in Pado (2007).
461
tion 3. As for the Dist-word model, contextual fea-
tures are 4,000 and pointwise mutual information is
the association measure. The maximal dependency
path length for selecting each context word is 3.
Syntactic relations are extracted using Minipar.
WNet-full : the WordNet based model described
in Section 4.
WNet-bsense : this model is computed as WNet-
full but using only the most frequent sense for each
LU as defined in WordNet.
Combined : the combined method presented in
Section 5. Specifically, it uses WNet-full as a default
and Dist-word as back-off.
Baseline-rnd : a baseline model, randomly as-
signing LUs to frames.
Baseline-mostfreq : a model predicting as best-k
frames the most likely ones in FrameNet ? i.e. those
containing the highest number of LUs.
6.2 Experimental Results
Table 1 reports accuracy and coverage results for the
different models, considering only 6792 LUs with
frequency higher than 5 in the BNC, and frames
with more than 2 lexical units (to allow better gen-
eralizations in all models). Results show that all our
models largely outperform both baselines, achieving
a good level of accuracy and high coverage. In
particular, accuracy for the best-10 frames is high
enoungh to support tasks such as the semi-automatic
creation of new FrameNets. This claim is supported
by a further task-driven experiment, in which we
asked 3 annotators to assign 60 unknown LUs (from
the Detour system log) to frames, with and without
the support of the Dist-word model?s predictions as
suggestions6. We verified that our model guarantee
an annotation speed-up of 25% ? i.e. in average an
annotator saves 25% of annotation time by using
the system?s suggestions.
Distributional vs. WordNet-based models.
WordNet-based models are significantly better than
distributional ones, for several reasons. First, distri-
butional models acquire information only from the
contexts in the corpus. As we do not use a FrameNet
annotated corpus, there is no guarantee that the us-
age of a LU in the texts reflects exactly the semantic
6For this purpose, the dataset is evenly split in two parts.
properties of the LU in FrameNet. In the extreme
cases of polysemous LUs, it may happen that the
textual contexts refer to senses which are not ac-
counted for in FrameNet. In our study, we explicitly
ignore the issue of polisemy, which is a notoriously
hard task to solve in semantics spaces (see (Schu?tze,
1998)), as the occurrences of different word senses
need to be clustered separately. We will approach
the problem in future work. The WordNet-based
model suffers from the problem of polisemy to a
much lesser extent, as all senses are explicitly rep-
resented and separated in WordNet, including those
related to the FrameNet gold standard.
A second issue regards data sparseness. The vec-
torial representation of LUs with few occurrences in
the corpus is likely to be semantically incomplete,
as not enough statistical evidence is available. Par-
ticularly skewed distributions can be found when
some frames are very rarely represented in the cor-
pus. A more in-depth descussion on these two issues
is given later in this section.
Regarding the WordNet-based models, WNet-full
in most cases outperforms WNet-bsense. The first
sense heuristic does not seem to be as effective as
in other tasks, such as Word Sense Disambigua-
tion. Although sense preferences (or predominance)
across two general purpose resources, such as Word-
Net and FrameNet, should be a useful hint, the con-
ceptual density algorithm seems to produce better
distributions (i.e. higher accuracy), especially when
several solutions are considered. Indeed, for many
LUs the first WordNet sense is not the one repre-
sented in the FrameNet database.
As for distributional models, results show that the
Dist-word model performs best. In general, syntac-
tic relations (Dist-syntax model) do not help to cap-
ture frame semantic properties better than a simple
window-based approach. This seems to indicate that
LUs in a same frame are related both by paradig-
matic and syntagmatic relations, in accordance to
the definition given in Section 3.2 ? i.e. they are
mostly semantically related, but not similar.
Coverage. Distributional models show a coverage
15% higher than WordNet-based ones. Indeed, as far
as corpus evidence is available (i.e. the unknown LU
appears in the corpus), distributional methods are al-
ways able to predict a frame. WordNet-based mod-
462
MODEL B-1 B-2 B-3 B-4 B-5 B-6 B-7 B-8 B-9 B-10 COVERAGE
Dist-word 0.27 0.36 0.42 0.46 0.49 0.51 0.53 0.55 0.56 0.57 95%
Dist-syntax 0.22 0.29 0.34 0.38 0.41 0.44 0.46 0.48 0.50 0.51 95%
Dist-mixed 0.25 0.35 0.40 0.44 0.47 0.49 0.51 0.53 0.54 0.56 95%
WNet-full 0.47 0.59 0.65 0.69 0.72 0.73 0.75 0.76 0.77 0.78 80%
WNet-bsense 0.52 0.61 0.64 0.66 0.67 0.68 0.69 0.69 0.70 0.70 72%
Combined 0.43 0.54 0.60 0.64 0.66 0.68 0.70 0.71 0.72 0.73 95%
Baseline-rnd 0.02 0.03 0.05 0.06 0.08 0.10 0.11 0.12 0.14 0.15
Baseline-mostfreq 0.02 0.05 0.07 0.08 0.10 0.11 0.13 0.14 0.15 0.17
Table 1: Accuracy and coverage of different models on best-k ranking with frequency threshold 5 and frame threshold
2
els cannot make predictions in two specific cases.
First, when the LU is not present in WordNet. Sec-
ond, when the function simWN does not has suffi-
cient relational information to find a similar frame.
This second factor is particularly evident for adjec-
tives, as Eq. 8 assigns a frame only when a synonym
of the unknown LU is found. It is then not surpris-
ing that 68% of the missed assignment are indeed
adjectives.
Results for the Combined model suggest that
the integration of distributional and WordNet-based
methods can offer a viable solution to the cover-
age problem, as it achieves an accuracy comparable
to the pure WordNet approaches, while keeping the
coverage high.
Figure 2: Dist-word model accuracy at different LU fre-
quency cuts.
Data Sparseness. A major issue when using dis-
tributional approaches is that words with low fre-
quency tend to have a very sparse non-meaningful
representation in the vector space. This highly im-
pacts on the accuracy of the models. To measure
the impact of data sparseness, we computed the ac-
curacy at different frequency cuts ? i.e. we exclude
LUs below a given frequency threshold from cen-
troid computation and evaluation. Figure 2 reports
the results for best-10 assignment at different cuts,
for the Dist-word model. As expected, accuracy im-
proves by excluding infrequent LUs. Only at a fre-
quency cut of 200 performance becomes stable, as
statistical evidence is enough for a reliable predic-
tion. Yet, in a real setting the improvement in accu-
racy implies a lower coverage, as the system would
not classify LUs below the threshold. For example,
by discarding LUs occurring less than 200 times in
the corpus, we obtain a +0.12 improvement in accu-
racy, but the coverage decreases to 57%. However,
uncovered LUs are also the most rare ones and their
relevance in an application may be negligible.
Lexical Semantics, Ambiguity and Plausible As-
signments. The overall accuracies achieved by
our methods are ?pessimistic?, in the sense that they
should be intended as lower-bounds. Indeed, a qual-
itative analysis of erroneous predictions reveals that
in many cases the frame assignments produced by
the models are semantically plausible, even if they
are considered incorrect in the leave-one-out test.
Consider for example the LU guerrilla, assigned in
FrameNet to the frame PEOPLE BY VOCATION. Our
mixed model proposes as two most similar frames
MILITARY and TERRORISM, which could still be
considered plausible assignment. The same holds
for the LU caravan, for which the most similar
frame is VEHICLE, while in FrameNet the LU is as-
signed only to the frame BUILDINGS. These cases
are due to the low FrameNet coverage, i.e LUs are
not fully annotated and they appear only in a subset
of their potential frames. The real accuracy of our
463
models is therefore expected to be higher.
To explore the issue, we carried out a qualita-
tive analysis of 5 words (i.e. abandon.v, accuse.v,
body.n, charge.v and partner.n). For each of them,
we randomly picked 60 sentences from the BNC
corpus, and asked two human annotators to assign
to the correct frame the occurrence of the word in
the given sentence. For 2 out of 5 words, no frame
could be found for most of the sentences, suggesting
that the most frequent frames for these words were
missing from FrameNet7. We can then conclude that
100% accuracy cannot be considered as the upper-
bound of our experiment, as word usage in texts is
not well reflected in the FrameNet modelling.
Further experiments. We also tested our models
on a realistic gold-standard set of 24 unknown LUs
extracted from the SemEval-2007 corpus (Baker et
al., 2007). These are words not present in FrameNet
1.3 which have been assigned by human annotators
to an existing frame8. WNet-full achieves an accu-
racy of 0.25 for best-1 and 0.69 for best-10, with a
coverage of 67%. A qualitative analysis showed that
the lower performance wrt to our main experiment is
due to higher ambiguity of the LUs (e.g. we assign
tea to SOCIAL EVENT instead of FOOD).
Comparison to other approaches. We compare
our models to the system presented by Johans-
son and Nugues (2007) and Burchardt and col-
leagues (2005). Johansson and Nugues (2007) eval-
uate their machine learning system using 7,000
unique LUs to train the Support Vector Machine, and
the remaining LUs as test. They measure accuracy at
different coverage levels. At 80% coverage accuracy
is about 0.42, 10 points below our best WordNet-
based system. At 90% coverage, the system shows
an accuracy below 0.10 and is significantly out-
performed by both our distributional and combined
methods. These results confirm that WordNet-based
approaches, while being highly accurate wrt dis-
tributional ones, present strong weaknesses as far
as coverage is concerned. Furthermore, Johansson
and Nugues (2007) show that their machine learn-
7Note that the need of new frames to account for seman-
tic phenomena in free texts has been also demonstrated by the
SemEval-2007 competition.
8The set does not contain 4 LUs which have no frame in
FrameNet.
ing approach outperforms a simple approach based
on WordNet similarity: thus, our results indirectly
prove that our WordNet-based method is more ef-
fective than the application of the similarity measure
presented in (Pedersen et al, 2004).
We also compare our results to those reported
by Burchardt and colleagues (2005) for Detour.
Though the experimental setting is slightly different
(LU assignment is done at the text-level), they use
the same gold standard and leave-one-out technique,
reporting a best-1 accuracy of 0.38 and a coverage
of 87%. Our WordNet-based models significantly
outperform Detour on best-1 accuracy, at the cost of
lower coverage. Yet,our combined model is signifi-
cantly better both on accuracy (+5%) and coverage
(+8%). Also, in most cases Detour cannot predict
more than one frame (best-1), while our accuracies
can be improved by relaxing to any best-k level.
7 Conclusions
In this paper we presented an original approach for
FrameNet LU induction. Results show that mod-
els combining distributional and WordNet informa-
tion offer the most viable solution to model the no-
tion of frame, as they allow to achieve a reasonable
trade-off between accuracy and coverage. We also
showed that in contrast to previous work, simple se-
mantic spaces are more helpful than complex syn-
tactic ones. Results are accurate enough to support
the creation and the development of new FrameNets.
As future work, we will evaluate new types of
spaces (e.g. dimensionality reduction methods) to
improve the generalization capabilities of the space
models. We will also address the data sparseness is-
sue, by testing smoothing techniques to better model
low frequency LUs. Finally, we will implement
the presented models in a complex architecture for
semi-supervised FrameNets development, both for
specializing the existing English FrameNet in spe-
cific domains, and for creating new FrameNets in
other languages.
Acknowledgements
This work has partly been funded by the German Re-
search Foundation DFG (grant PI 154/9-3). Thanks
to Richard Johansson and Aljoscha Burchardt for
providing the data of their systems.
464
References
E. Agirre and G. Rigau. 1996. Word Sense Disam-
biguation using Conceptual Density. In Proceedings
of COLING-96, Copenhagen, Denmark.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval-2007 Task 19: Frame Semantic Structure
Extraction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99?104, Prague, Czech Republic, June.
Roy Bar-Haim, Idan Szpektor, and Oren Glickman.
2005. Definition and Analysis of Intermediate Entail-
ment Levels. In ACL-05 Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, Ann
Arbor, Michigan.
R. Basili, M. Cammisa, and F.M. Zanzotto. 2004. A
semantic similarity measure for unsupervised semantic
disambiguation. In Proceedings of LREC-04, Lisbon,
Portugal.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of PASCAL RTE2 Workshop.
Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005.
A WordNet Detour to FrameNet. In Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8 of Computer Studies in Language
and Speech. Peter Lang, Frankfurt/Main.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC, Genova,
Italy.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 18(1):61?74.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222?254.
K. Garoufi. 2007. Towards a better understanding of
applied textual entailment: Annotation and evaluation
of the rte-2 dataset. M.Sc. thesis, saarland university.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Phi-
losophy of Linguistics, New York. Oxford University
Press.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic Re-
sources for Scandinavian and Baltic Languages, at
NODALIDA, Tartu, Estonia, May 24.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Sebastian Pado?. 2007. Cross-Lingual Annotation Projec-
tion Models for Role-Semantic Information. Saarland
University.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1).
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Magnus Sahlgren. 2006. The Word-Space Model. De-
partment of Linguistics, Stockholm University.
G. Salton, A. Wong, and C. Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18:613620.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97?124.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceedings
of EMNLP-CoNLL, pages 12?21, Prague.
C. Subirats and M. Petruck. 2003. Surprise! Spanish
FrameNet! In Proceedings of the Workshop on Frame
Semantics at the XVII. International Congress of Lin-
guists, Prague.
465
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 72?77,
Prague, June 2007. c?2007 Association for Computational Linguistics
Shallow Semantics in Fast Textual Entailment Rule Learners
Fabio Massimo Zanzotto
DISP
University of Rome ?Tor Vergata?
Roma, Italy
zanzotto@info.uniroma2.it
Marco Pennacchiotti
Computerlinguistik
Universita?t des Saarlandes,
Saarbru?cken, Germany
pennacchiotti@coli.uni-sb.de
Alessandro Moschitti
DIT
University of Trento
Povo di Trento, Italy
moschitti@dit.unitn.it
Abstract
In this paper, we briefly describe two
enhancements of the cross-pair similarity
model for learning textual entailment rules:
1) the typed anchors and 2) a faster compu-
tation of the similarity. We will report and
comment on the preliminary experiments
and on the submission results.
1 Introduction
Results of the second RTE challenge (Bar Haim et
al., 2006) have suggested that both deep semantic
models and machine learning approaches can suc-
cessfully be applied to solve textual entailment. The
only problem seems to be the size of the knowledge
bases. The two best systems (Tatu et al, 2005; Hickl
et al, 2005), which are significantly above all the
others (more than +10% accuracy), use implicit or
explicit knowledge bases larger than all the other
systems. In (Tatu et al, 2005), a deep semantic
representation is paired with a large amount of gen-
eral and task specific semantic rules (explicit knowl-
edge). In (Hickl et al, 2005), the machine learning
model is trained over a large amounts of examples
(implicit knowledge).
In contrast, Zanzotto&Moschitti (2006) proposed
a machine-learning based approach which reaches a
high accuracy by only using the available RTE data.
The key idea is the cross-pair similarity, i.e. a simi-
larity applied to two text and hypothesis pairs which
considers the relations between the words in the two
texts and between the words in the two hypotheses.
This is obtained by using placeholders to link the re-
lated words. Results in (Bar Haim et al, 2006) are
comparable with the best machine learning system
when this latter is trained only on the RTE exam-
ples.
Given the high potential of the cross-pair similar-
ity model, for the RTE3 challenge, we built on it by
including some features of the two best systems: 1)
we go towards a deeper semantic representation of
learning pairs including shallow semantic informa-
tion in the syntactic trees using typed placeholders;
2) we reduce the computational cost of the cross-pair
similarity computation algorithm to allow the learn-
ing over larger training sets.
The paper is organized as follows: in Sec. 2 we
review the cross-pair similarity model and its limits;
in Sec. 3, we introduce our model for typed anchors;
in Sec. 4 we describe how we limit the computa-
tional cost of the similarity; in Sec. 5 we present the
two submission experiments, and in Sec. 6 we draw
some conclusions.
2 Cross-pair similarity and its limits
2.1 Learning entailment rules with syntactic
cross-pair similarity
The cross-pair similarity model (Zanzotto and
Moschitti, 2006) proposes a similarity measure
aiming at capturing rewrite rules from train-
ing examples, computing a cross-pair similarity
KS((T ?,H ?), (T ??,H ??)). The rationale is that if two
pairs are similar, it is extremely likely that they have
the same entailment value. The key point is the use
of placeholders to mark the relations between the
sentence words. A placeholder co-indexes two sub-
structures in the parse trees of text and hypothesis,
72
indicating that such substructures are related. For
example, the sentence pair, ?All companies file an-
nual reports? implies ?All insurance companies file
annual reports?, is represented as follows:
T1 (S (NP: 1 (DT All) (NNS: 1 compa-
nies)) (VP: 2 (VBP: 2 file) (NP: 3 (JJ: 3
annual) (NNS: 3 reports))))
H1 (S (NP: 1 (DT All) (NNP Fortune)
(CD 50) (NNS: 1 companies)) (VP: 2
(VBP: 2 file) (NP: 3 (JJ: 3 annual)
(NNS: 3 reports))))
(E1)
where the placeholders 1 , 2 , and 3 indicate the rela-
tions between the structures of T and of H .
Placeholders help to determine if two pairs share
the same rewriting rule by looking at the subtrees
that they have in common. For example, suppose
we have to determine if ?In autumn, all leaves fall?
implies ?In autumn, all maple leaves fall?. The re-
lated co-indexed representation is:
T2 (S (PP (IN In) (NP (NN: a automn)))
(, ,) (NP: b (DT all) (NNS: b leaves))
(VP: c (VBP: c fall)))
H2 (S (PP (IN In) (NP: a (NN: a automn)))
(, ,) (NP: b (DT all) (NN maple)
(NNS: a leaves)) (VP: c (VBP: c fall)))
(E2)
E1 and E2 share the following subtrees:
T3 (S (NP: x (DT all) (NNS: x )) (VP: y
(VBP: y )))
H3 (S (NP: x (DT all) (NN) (NNS: x ))
(VP: x (VBP: x )))
(R3)
This is the rewrite rule they have in common. Then,
E2 can be likely classified as a valid entailment, as
it shares the rule with the valid entailment E1.
The cross-pair similarity model uses: (1) a tree
similarity measure KT (?1, ?2) (Collins and Duffy,
2002) that counts the subtrees that ?1 and ?2 have
in common; (2) a substitution function t(?, c) that
changes names of the placeholders in a tree accord-
ing to a set of correspondences between placehold-
ers c. Given C as the collection of all correspon-
dences between the placeholders of (T ?,H ?) and
(T ??,H ??), the cross-pair similarity is computed as:
KS((T ?, H ?), (T ??, H ??)) =
maxc?C(KT (t(T ?, c), t(T ??, c)) + KT (t(H ?, c), t(H ??, c)))
(1)
The cross-pair similarity KS , used in a kernel-based
learning model as the support vector machines, al-
lows the exploitation of implicit true and false en-
tailment rewrite rules described in the examples.
2.2 Limits of the syntactic cross-pair similarity
Learning from examples using cross-pair similarity
is an attractive and effective approach. However,
the cross-pair strategy, as any machine learning ap-
proach, is highly sensitive on how the examples are
represented in the feature space, as this can strongly
bias the performance of the classifier.
Consider for example the following text-
hypothesis pair, which can lead to an incorrect rule,
if misused.
T4 ?For my younger readers, Chapman
killed John Lennon more than twenty
years ago.?
H4 ?John Lennon died more than twenty
years ago.?
(E4)
In the basic cross-pair similarity model, the learnt
rule would be the following:
T5 (S (NP: x ) (VP: y (VBD: y ) (NP: z )
(ADVP: k )))
H5 (S (NP: z ) (VP: y (VBD: y )
(ADVP: k )))
(R5)
where the verbs kill and die are connected by the y
placeholder. This rule is useful to classify examples
like:
T6 ?Cows are vegetarian but, to save
money on mass-production, farmers fed
cows animal extracts.?
H6 ?Cows have eaten animal extracts.?
(E6)
but it will clearly fail when used for:
T7 ?FDA warns migraine medicine makers
that they are illegally selling migraine
medicines without federal approval.?
H7 ?Migraine medicine makers declared
that their medicines have been ap-
proved.?
(E7)
where warn and declare are connected as generically
similar verbs.
The problem of the basic cross-pair similarity
measure is that placeholders do not convey the
semantic knowledge needed in cases such as the
above, where the semantic relation between con-
nected verbs is essential.
2.3 Computational cost of the cross-similarity
measure
Let us go back to the computational cost of KS (eq.
1). It heavily depends on the size of C. We de-
fine p? and p?? as the placeholders of, respectively,
(T ?,H ?) and (T ??,H ??). As C is combinatorial with
respect to |p?| and |p??|, |C| rapidly grows. Assigning
placeholders only to chunks helps controlling their
73
number. For example, in the RTE data the number
of placeholders hardly goes beyond 7, as hypothe-
ses are generally short sentences. But, even in these
cases, the number of KT computations grows. As
the trees t(?, c) are obtained from a single tree ?
(containing placeholder) applying different c ? C,
it is reasonable to think that they will share com-
mon subparts. Then, during the iterations of c ?
C, KT (t(??, c), t(???, c)) will compute the similarity
between subtrees that have already been evaluated.
The reformulation of the cross-pair similarity func-
tion we present takes advantage of this.
3 Adding semantic information to
cross-pair similarity
The examples in the previous section show that
the cross-pairs approach lacks the lexical-semantic
knowledge connecting the words in a placeholder.
In the examples, the missed knowledge is the type
of semantic relation between the main verbs. The
relation that links kill and die is not a generic sim-
ilarity, as a WordNet based similarity measure can
suggest, but a more specific causal relation. The
learnt rewrite rule R5 holds only for verbs in such
relation. In facts, it is correctly applied in example
E6, as feed causes eat, but it gives a wrong sugges-
tion in example E7, since warn and declare are only
related by a generic similarity relation.
We then need to encode this information in the
syntactic trees in order to learn correct rules.
3.1 Defining anchor types
The idea of introducing anchor types should be in
principle very simple and effective. Yet, this may be
not the case: simpler attempts to introduce semantic
information in RTE systems have often failed. To
investigate the validity of our idea, we then need to
focus on a small set of relevant relation types, and to
carefully control ambiguity for each type.
A valuable source of relation types among words
is WordNet. We choose to integrate in our system
three important relation standing at the word level:
part-of, antinomy, and verb entailment. We also de-
fine two more general anchor types: similarity and
the surface matching. The first type links words
which are similar according to some WordNet simi-
larity measure. Specifically, this type is intended to
Rank Relation Type Symbol
1. antinomy ?
2. part-of ?
3. verb entailment ?
4. similarity ?
5. surface matching =
Table 1: Ranked anchor types
capture the semantic relations of synonymy and hy-
peronymy. The second type is activated when words
or lemmas match: then, it captures cases in which
words are semantically equivalent. The complete set
of relation types used in the experiments is given in
Table 1.
3.2 Type anchors in the syntactic tree
To learn more correct rewrite rules by using the an-
chor types defined in the previous section, we need
to add this information to syntactic trees. The best
position would be in the same nodes of the anchors.
Also, to be more effective, this information should
be inserted in as many subtrees as possible. Thus we
define the typed-anchor climbing-up rules. We then
implement in our model the following climbing up
rule:
if two typed anchors climb up to the same
node, give precedence to that with the high-
est ranking in Tab. 1.
This rule can be easily showed to be consistent with
common sense intuitions. For an example like ?John
is a tall boy? that does not entail ?John is a short
boy?, our strategy will produce these trees:
(E8)
T8 H8
S ? 3
NP = 1
NNP = 1
John
VP ? 2
AUX
is
NP ? 3
DT
a
JJ ? 2
tall
NN = 3
boy
S ? 3
NP = 1
NNP = 1
John
VP ? 2
AUX
is
NP ? 3
DT
a
JJ ? 2
short
NN = 3
boy
This representation can be used to derive a correct
rewrite rule, such as:
if two fragments have the same syntactic struc-
ture S(NP1, V P (AUX,NP2)), and there is an
antonym type (?) on the S and NP2 , then the
74
c1 = {(a, 1), (b, 2), (c, 3)} c2 = {(a, 1), (b, 2), (d, 3)}
?1 t(?1, c1) t(?1, c2)
X1 a
A2 a
B3 a
w1
a
C4 b
w2
b
D5 d
D6 c
w3
c
C7 d
w4
d
X1 a:1
A2 a:1
B3 a:1
w1
a:1
C4 b:2
w2
b:2
D5 d
D6 c:3
w3
c:3
C7 d
w4
d
X1 a:1
A2 a:1
B3 a:1
w1
a:1
C4 b:2
w2
b:2
D5 d:3
D6 c
w3
c
C7 d:3
w4
d:3
?2 t(?2, c1) t(?2, c2)
X1 1
A2 1
B3 1
m1
1
C4 2
m2
2
D5
D6 3
m3
3
C7
m4
X1 a:1
A2 a:1
B3 a:1
m1
a:1
C4 b:2
m2
b:2
D5
D6 c:3
m3
c:3
C7
m4
X1 a:1
A2 a:1
B3 a:1
m1
a:1
C4 b:2
m2
b:2
D5
D6 d:3
m3
d:3
C7
m4
Figure 1: Tree pairs with placeholders and t(T, c) transformation
entailment does not hold.
4 Reducing computational cost of the
cross-pair similarity computation
4.1 The original kernel function
In this section, we describe more in detail the simi-
larity function KS (Eq. 1). To simplify, we focus on
the computation of only one KT of the kernel sum.
KS(??,???) = maxc?C KT (t(?
?, c), t(???, c)), (2)
where the (??,???) pair can be either (T ?, T ??) or
(H ?,H ??). We apply this simplification since we
are interested in optimizing the evaluation of the
KT with respect to different sets of correspondences
c ? C.
To better explain KS , we need to analyze the role
of the substitution function t(?, c) and to review the
tree kernel function KT .
The aim of t(?, c) is to coherently replace place-
holders in two trees ?? and ??? so that these two trees
can be compared. The substitution is carried out
according to the set of correspondences c. Let p?
and p?? be placeholders of ?? and ???, respectively,
if p?? ? p? then c is a bijection between a subset
p?? ? p? and p??. For example (Fig. 1), the trees ?1
has p1 ={ a , b , c , d } as placeholder set and ?2 has
p2 ={ 1 , 2 , 3 }. In this case, a possible set of corre-
spondence is c1 = {(a, 1), (b, 2), (c, 3)}. In Fig. 1
the substitution function replaces each placeholder
a of the tree ?1with the new placeholder a:1 by
t(?, c) obtaining the transformed tree t(?1, c1), and
each placeholder 1 of ?2 with a:1 . After these sub-
stitutions, the labels of the two trees can be matched
and the similarity function KT is applicable.
KT (? ?, ? ??), as defined in (Collins and Duffy,
2002), computes the number of common subtrees
between ? ? and ? ??.
4.2 An observation to reduce the
computational cost
The above section has shown that the similarity
function KS firstly applies the transformation t(?, c)
and then computes the tree kernel KT . The overall
process can be optimized by factorizing redundant
KT computations.
Indeed, two trees, t(?, c?) and t(?, c??), obtained
by applying two sets of correspondences c?, c?? ? C,
may partially overlap since c? and c?? can share a non-
empty set of common elements. Let us consider the
subtree set S shared by t(?, c?) and t(?, c??) such
that they contain placeholders in c? ? c?? = c, then
t(?, c) = t(?, c?) = t(?, c??) ?? ? S. Therefore if
we apply a tree kernel function KT to a pair (??,???),
we can find a c such that subtrees of ?? and subtrees
of ??? are invariant with respect to c? and c??. There-
fore, KT (t(??, c), t(???, c)) = KT (t(??, c?), t(???, c?))
= KT (t(??, c??), t(???, c??)). This implies that it is
possible to refine the dynamic programming algo-
rithm used to compute the ? matrices while com-
75
puting the kernel KS(??,???).
To better explain this idea let us consider
Fig. 1 that represents two trees, ?1 and ?2,
and the application of two different transforma-
tions c1 = {(a, 1), (b, 2), (c, 3)} and c2 =
{(a, 1), (b, 2), (d, 3)}. Nodes are generally in the
form Xi z where X is the original node label, z is
the placeholder, and i is used to index nodes of the
tree. Two nodes are equal if they have the same node
label and the same placeholder. The first column of
the figure represents the original trees ?1 and ?2.
The second and third columns contain respectively
the transformed trees t(?, c1) and t(?, c2)
Since the subtree of ?1 starting from A2 a con-
tains only placeholders that are in c, in the trans-
formed trees, t(?1, c1) and t(?1, c2), the subtrees
rooted in A2 a:1 are identical. The same happens
for ?2 with the subtree rooted in A2 1 . In the trans-
formed trees, t(?2, c1) and t(?2, c2), subtrees rooted
in A2 a:1 are identical. The computation of KT
applied to the above subtrees gives an identical re-
sult. Then, this computation can be avoided. If cor-
rectly used in a dynamic programming algorithm,
the above observation can produce an interesting de-
crease in the time computational cost. More de-
tails on the algorithm and the decrease in computa-
tional cost may be found in (Moschitti and Zanzotto,
2007).
5 Experimental Results
5.1 Experimental Setup
We implemented the novel cross-similarity kernel
in the SVM-light-TK (Moschitti, 2006) that en-
codes the basic syntactic kernel KT in SVM-light
(Joachims, 1999).
To assess the validity of the typed anchor model
(tap), we evaluated two sets of systems: the plain
and lexical-boosted systems. The plain systems are:
-tap: our tree-kernel approach using typed place-
holders with climbing in the syntactic tree;
-tree: the cross-similarity model described in Sec.2.
Its comparison with tap indicates the effectiveness
of our approaches;
The lexical-boosted systems are:
-lex: a standard approach based on lexical over-
lap. The classifier uses as the only feature the lexi-
cal overlap similarity score described in (Corley and
Mihalcea, 2005);
-lex+tap: these configurations mix lexical overlap
and our typed anchor approaches;
-lex+tree: the comparison of this configuration with
lex+tap should further support the validity of our in-
tuition on typed anchors;
Preliminary experiments have been performed us-
ing two datasets: RTE2 (the 1600 entailment pairs
from the RTE-2 challenge) and RTE3d (the devel-
opment dataset of this challenge). We randomly
divided this latter in two halves: RTE3d0 and
RTE3d1.
5.2 Investigatory Results Analysis and
Submission Results
Table 2 reports the results of the experiments. The
first column indicates the training set whereas the
second one specifies the used test set. The third and
the forth columns represent the accuracy of basic
models: the original tree model and the enhanced
tap model. The latter three columns report the basic
lex model and the two combined models, lex+tree
and lex+tap. The second and the third rows repre-
sent the accuracy of the models with respect to the
first randomly selected half of RTE3d whilst the
last two rows are related to the second half.
The experimental results show some interesting
facts. In the case of the plain systems (tree and tap),
we have the following observations:
- The use of the typed anchors in the model seems
to be effective. All the tap model results are higher
than the corresponding tree model results. This sug-
gests that the method used to integrate this kind of
information in the syntactic tree is effective.
- The claim that using more training material helps
seems not to be supported by these experiments. The
gap between tree and tap is higher when learn-
ing with RTE2 + RTE3d0 than when learning
with RTE30. This supports the claim. How-
ever, the result is not kept when learning with
RTE2 + RTE3d1 with respect to when learning
with RTE31. This suggests that adding not very
specific information, i.e. derived from corpora dif-
ferent from the target one (RTE3), may not help the
learning of accurate rules.
On the other hand, in the case of the lexical-
boosted systems (lex, lex+tree, and lex+tap), we
see that:
76
Train Test tree tap lex lex+tree lex+tap
RTE3d0 RTE3d1 62.97 64.23 69.02 68.26 69.02
RTE2 +RTE3d0 RTE3d1 62.22 62.47 71.03 71.28 71.79
RTE3d1 RTE3d0 62.03 62.78 70.22 70.22 71.22
RTE2 +RTE3d0 RTE3d0 63.77 64.76 71.46 71.22 72.95
Table 2: Accuracy of the systems on two folds of RTE3 development
- There is an extremely high accuracy result for the
pure lex model. This result is counterintuitive. A
model like lex has been likely used by QA or IE
systems to extract examples for the RTE3d set. If
this is the case we may expect that positive and
negative examples should have similar values for
this lex distance indicator. It is then not clear why
this model results in so high accuracy.
- Given the high results of the lex model, the model
lex+tree does not increase the performances.
- On the contrary, the model lex+tap is always better
(or equal) than the lex model. This suggests that
for this particular set of examples the typed anchors
are necessary to effectively use the rewriting rules
implicitly encoded in the examples.
- When the tap model is used in combination with
the lex model, it seems that the claim ?the more
training examples the better? is valid. The gaps
between lex and lex+tap are higher when the RTE2
is used in combination with the RTE3d related set.
Given this analysis we submitted two systems
both based on the lex+tap model. We did two differ-
ent training: one using RTE3d and the other using
RTE2 +RTE3d. Results are reported in the Table
below:
Train Accuracy
RTE3d 66.75%
RTE2 +RTE3d 65.75%
Such results seem too low to be statistically consis-
tent with our development outcome. This suggests
that there is a clear difference between the content
of RTE3d and the RTE3 test set. Moreover, in
contrast with what expected, the system trained with
only the RTE3d data is more accurate than the oth-
ers. Again, this suggests that the RTE corpora (from
all the challenges) are most probably very different.
6 Conclusions and final remarks
This paper demonstrates that it is possible to ef-
fectively include shallow semantics in syntax-based
learning approaches. Moreover, as it happened in
RTE2, it is not always true that more learning ex-
amples increase the accuracy of RTE systems. This
claim is still under investigation.
References
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.
The II PASCAL RTE challenge. In PASCAL Challenges
Workshop, Venice, Italy.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of ACL02.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, pages 13?18, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2005. Recognizing textual en-
tailment with LCCs GROUNDHOG system. In Proceedings
of the Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods-Support Vector Learning. MIT
Press.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from texts.
In Proceedings of the International Conference of Machine
Learning (ICML), Corvallis, Oregon.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL?06,
Trento, Italy.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and
Dan Moldovan. 2005. COGEX at the second recognizing
textual entailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy.
Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Au-
tomatic learning of textual entailments with cross-pair sim-
ilarities. In Proceedings of the 21st Coling and 44th ACL,
pages 401?408, Sydney, Australia, July.
77
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94?99,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx? , Su Nam Kim? , Zornitsa Kozareva? , Preslav Nakov? ,
Diarmuid O? Se?aghdha?, Sebastian Pado?? , Marco Pennacchiotti??,
Lorenza Romano??, Stan Szpakowicz??
Abstract
We present a brief overview of the main
challenges in the extraction of semantic
relations from English text, and discuss the
shortcomings of previous data sets and shared
tasks. This leads us to introduce a new
task, which will be part of SemEval-2010:
multi-way classification of mutually exclusive
semantic relations between pairs of common
nominals. The task is designed to compare
different approaches to the problem and to
provide a standard testbed for future research,
which can benefit many applications in
Natural Language Processing.
1 Introduction
The computational linguistics community has a con-
siderable interest in robust knowledge extraction,
both as an end in itself and as an intermediate step
in a variety of Natural Language Processing (NLP)
applications. Semantic relations between pairs of
words are an interesting case of such semantic
knowledge. It can guide the recovery of useful facts
about the world, the interpretation of a sentence, or
even discourse processing. For example, pears and
bowl are connected in a CONTENT-CONTAINER re-
lation in the sentence ?The bowl contained apples,
?University of Antwerp, iris.hendrickx@ua.ac.be
?University of Melbourne, snkim@csse.unimelb.edu.au
?University of Alicante, zkozareva@dlsi.ua.es
?National University of Singapore, nakov@comp.nus.edu.sg
?University of Cambridge, do242@cl.cam.ac.uk
?University of Stuttgart, pado@stanford.edu
??Yahoo! Inc., pennacc@yahoo-inc.com
??Fondazione Bruno Kessler, romano@fbk.eu
??University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
pears, and oranges.?, while ginseng and taste are in
an ENTITY-ORIGIN relation in ?The taste is not from
alcohol, but from the ginseng.?.
The automatic recognition of semantic relations
can have many applications, such as information
extraction (IE), document summarization, machine
translation, or construction of thesauri and seman-
tic networks. It can also facilitate auxiliary tasks
such as word sense disambiguation, language mod-
eling, paraphrasing or recognizing textual entail-
ment. For example, semantic network construction
can benefit from detecting a FUNCTION relation be-
tween airplane and transportation in ?the airplane
is used for transportation? or a PART-WHOLE rela-
tion in ?the car has an engine?. Similarly, all do-
mains that require deep understanding of text rela-
tions can benefit from knowing the relations that de-
scribe events like ACQUISITION between named en-
tities in ?Yahoo has made a definitive agreement to
acquire Flickr?.
In this paper, we focus on the recognition of se-
mantic relations between pairs of common nomi-
nals. We present a task which will be part of the
SemEval-2010 evaluation exercise and for which we
are developing a new benchmark data set. This data
set and the associated task address three significant
problems encountered in previous work: (1) the def-
inition of a suitable set of relations; (2) the incorpo-
ration of context; (3) the desire for a realistic exper-
imental design. We outline these issues in Section
2. Section 3 describes the inventory of relations we
adopted for the task. The annotation process, the
design of the task itself and the evaluation method-
ology are presented in Sections 4-6.
94
2 Semantic Relation Classification: Issues
2.1 Defining the Relation Inventory
A wide variety of relation classification schemes ex-
ist in the literature, reflecting the needs and granular-
ities of various applications. Some researchers only
investigate relations between named entities or in-
ternal to noun-noun compounds, while others have a
more general focus. Some schemes are specific to a
domain such as biomedical text.
Rosario and Hearst (2001) classify noun com-
pounds from the domain of medicine into 13 classes
that describe the semantic relation between the head
noun and the modifier. Rosario et al (2002) classify
noun compounds using the MeSH hierarchy and a
multi-level hierarchy of semantic relations, with 15
classes at the top level. Stephens et al (2001) pro-
pose 17 very specific classes targeting relations be-
tween genes. Nastase and Szpakowicz (2003) ad-
dress the problem of classifying noun-modifier rela-
tions in general text. They propose a two-level hier-
archy, with 5 classes at the first level and 30 classes
at the second one; other researchers (Kim and Bald-
win, 2005; Nakov and Hearst, 2008; Nastase et al,
2006; Turney, 2005; Turney and Littman, 2005)
have used their class scheme and data set. Moldovan
et al (2004) propose a 35-class scheme to classify
relations in various phrases; the same scheme has
been applied to noun compounds and other noun
phrases (Girju et al, 2005). Lapata (2002) presents a
binary classification of relations in nominalizations.
Pantel and Pennacchiotti (2006) concentrate on five
relations in an IE-style setting. In short, there is little
agreement on relation inventories.
2.2 The Role of Context
A fundamental question in relation classification is
whether the relations between nominals should be
considered out of context or in context. When one
looks at real data, it becomes clear that context does
indeed play a role. Consider, for example, the noun
compound wood shed : it may refer either to a shed
made of wood, or to a shed of any material used to
store wood. This ambiguity is likely to be resolved
in particular contexts. In fact, most NLP applica-
tions will want to determine not all possible relations
between two words, but rather the relation between
two instances in a particular context. While the in-
tegration of context is common in the field of IE (cf.
work in the context of ACE1), much of the exist-
ing literature on relation extraction considers word
pairs out of context (thus, types rather than tokens).
A notable exception is SemEval-2007 Task 4 Clas-
sification of Semantic Relations between Nominals
(Girju et al, 2007; Girju et al, 2008), the first to of-
fer a standard benchmark data set for seven semantic
relations between common nouns in context.
2.3 Style of Classification
The design of SemEval-2007 Task 4 had an im-
portant limitation. The data set avoided the chal-
lenge of defining a single unified standard classifi-
cation scheme by creating seven separate training
and test sets, one for each semantic relation. That
made the relation recognition task on each data set
a simple binary (positive / negative) classification
task.2 Clearly, this does not easily transfer to prac-
tical NLP settings, where any relation can hold be-
tween a pair of nominals which occur in a sentence
or a discourse.
2.4 Summary
While there is a substantial amount of work on re-
lation extraction, the lack of standardization makes
it difficult to compare different approaches. It is
known from other fields that the availability of stan-
dard benchmark data sets can provide a boost to the
advancement of a field. As a first step, SemEval-
2007 Task 4 offered many useful insights into the
performance of different approaches to semantic re-
lation classification; it has also motivated follow-
up research (Davidov and Rappoport, 2008; Ka-
trenko and Adriaans, 2008; Nakov and Hearst, 2008;
O? Se?aghdha and Copestake, 2008).
Our objective is to build on the achievements of
SemEval-2007 Task 4 while addressing its short-
comings. In particular, we consider a larger set of
semantic relations (9 instead of 7), we assume a
proper multi-class classification setting, we emulate
the effect of an ?open? relation inventory by means
of a tenth class OTHER, and we will release to the
research community a data set with a considerably
1http://www.itl.nist.gov/iad/mig/tests/
ace/
2Although it was not designed for a multi-class set-up, some
subsequent publications tried to use the data sets in that manner.
95
larger number of examples than SemEval-2007 Task
4 or other comparable data sets. The last point is cru-
cial for ensuring the robustness of the performance
estimates for competing systems.
3 Designing an Inventory of Semantic Re-
lations Between Nominals
We begin by considering the first of the problems
listed above: defining of an inventory of semantic
relations. Ideally, it should be exhaustive (should al-
low the description of relations between any pair of
nominals) and mutually exclusive (each pair of nom-
inals in context should map onto only one relation).
The literature, however, suggests no such inventory
that could satisfy all needs. In practice, one always
must decide on a trade-off between these two prop-
erties. For example, the gene-gene relation inven-
tory of Stephens et al (2001), with relations like X
phosphorylates Y, arguably allows no overlaps, but
is too specific for applications to general text.
On the other hand, schemes aimed at exhaus-
tiveness tend to run into overlap issues, due
to such fundamental linguistic phenomena as
metaphor (Lakoff, 1987). For example, in the sen-
tence Dark clouds gather over Nepal., the relation
between dark clouds and Nepal is literally a type of
ENTITY-DESTINATION, but in fact it refers to the
ethnic unrest in Nepal.
We seek a pragmatic compromise between the
two extremes. We have selected nine relations with
sufficiently broad coverage to be of general and
practical interest. We aim at avoiding ?real? overlap
to the extent that this is possible, but we include two
sets of similar relations (ENTITY-ORIGIN/ENTITY-
DESTINATION and CONTENT-CONTAINER/COM-
PONENT-WHOLE/MEMBER-COLLECTION), which
can help assess the models? ability to make such
fine-grained distinctions.3
As in Semeval-2007 Task 4, we give ordered two-
word names to the relations, where each word de-
scribes the role of the corresponding argument. The
full list of our nine relations follows4 (the definitions
we show here are intended to be indicative rather
than complete):
3COMPONENT-WHOLE and MEMBER-COLLECTION are
proper subsets of PART-WHOLE, one of the relations in
SemEval-2007 Task 4.
4We have taken the first five from SemEval-2007 Task 4.
Cause-Effect. An event or object leads to an effect.
Example: Smoking causes cancer.
Instrument-Agency. An agent uses an instrument.
Example: laser printer
Product-Producer. A producer causes a product to
exist. Example: The farmer grows apples.
Content-Container. An object is physically stored
in a delineated area of space, the container. Ex-
ample: Earth is located in the Milky Way.
Entity-Origin. An entity is coming or is derived
from an origin (e.g., position or material). Ex-
ample: letters from foreign countries
Entity-Destination. An entity is moving towards a
destination. Example: The boy went to bed.
Component-Whole. An object is a component of a
larger whole. Example: My apartment has a
large kitchen.
Member-Collection. A member forms a nonfunc-
tional part of a collection. Example: There are
many trees in the forest.
Communication-Topic. An act of communication,
whether written or spoken, is about a topic. Ex-
ample: The lecture was about semantics.
We add a tenth element to this set, the pseudo-
relation OTHER. It stands for any relation which
is not one of the nine explicitly annotated relations.
This is motivated by modelling considerations. Pre-
sumably, the data for OTHER will be very nonho-
mogeneous. By including it, we force any model of
the complete data set to correctly identify the deci-
sion boundaries between the individual relations and
?everything else?. This encourages good generaliza-
tion behaviour to larger, noisier data sets commonly
seen in real-world applications.
3.1 Semantic Relations versus Semantic Roles
There are three main differences between our task
(classification of semantic relations between nomi-
nals) and the related task of automatic labeling of
semantic roles (Gildea and Jurafsky, 2002).
The first difference is to do with the linguistic
phenomena described. Lexical resources for theo-
ries of semantic roles such as FrameNet (Fillmore et
96
al., 2003) and PropBank (Palmer et al, 2005) have
been developed to describe the linguistic realization
patterns of events and states. Thus, they target pri-
marily verbs (or event nominalizations) and their de-
pendents, which are typically nouns. In contrast,
semantic relations may occur between all parts of
speech, although we limit our attention to nominals
in this task. Also, semantic role descriptions typi-
cally relate an event to a set of multiple participants
and props, while semantic relations are in practice
(although not necessarily) binary.
The second major difference is the syntactic con-
text. Theories of semantic roles usually developed
out of syntactic descriptions of verb valencies, and
thus they focus on describing the linking patterns of
verbs and their direct dependents, phenomena like
raising and noninstantiations notwithstanding (Fill-
more, 2002). Semantic relations are not tied to
predicate-argument structures. They can also be es-
tablished within noun phrases, noun compounds, or
sentences more generally (cf. the examples above).
The third difference is that of the level of gen-
eralization. FrameNet currently contains more than
825 different frames (event classes). Since the se-
mantic roles are designed to be interpreted at the
frame level, there is a priori a very large number
of unrelated semantic roles. There is a rudimen-
tary frame hierarchy that defines mappings between
roles of individual frames,5 but it is far from com-
plete. The situation is similar in PropBank. Prop-
Bank does use a small number of semantic roles, but
these are again to be interpreted at the level of in-
dividual predicates, with little cross-predicate gen-
eralization. In contrast, all of the semantic relation
inventories discussed in Section 1 contain fewer than
50 types of semantic relations. More generally, se-
mantic relation inventories attempt to generalize re-
lations across wide groups of verbs (Chklovski and
Pantel, 2004) and include relations that are not verb-
centered (Nastase and Szpakowicz, 2003; Moldovan
et al, 2004). Using the same labels for similar se-
mantic relations facilitates supervised learning. For
example, a model trained with examples of sell re-
lations should be able to transfer what it has learned
to give relations. This has the potential of adding
5For example, it relates the BUYER role of the COM-
MERCE SELL frame (verb sell ) to the RECIPIENT role of the
GIVING frame (verb give).
1. People in Hawaii might be feeling
<e1>aftershocks</e1> from that power-
ful <e2>earthquake</e2> for weeks.
2. My new <e1>apartment</e1> has a
<e2>large kitchen</e2>.
Figure 1: Two example sentences with annotation
crucial robustness and coverage to analysis tools in
NLP applications based on semantic relations.
4 Annotation
The next step in our study will be the actual annota-
tion of relations between nominals. For the purpose
of annotation, we define a nominal as a noun or a
base noun phrase. A base noun phrase is a noun and
its pre-modifiers (e.g., nouns, adjectives, determin-
ers). We do not include complex noun phrases (e.g.,
noun phrases with attached prepositional phrases or
relative clauses). For example, lawn is a noun, lawn
mower is a base noun phrase, and the engine of the
lawn mower is a complex noun phrase.
We focus on heads that are common nouns. This
emphasis distinguishes our task from much work in
IE, which focuses on named entities and on consid-
erably more fine-grained relations than we do. For
example, Patwardhan and Riloff (2007) identify cat-
egories like Terrorist organization as participants in
terror-related semantic relations, which consists pre-
dominantly of named entities. We feel that named
entities are a specific category of nominal expres-
sions best dealt with using techniques which do not
apply to common nouns; for example, they do not
lend themselves well to semantic generalization.
Figure 1 shows two examples of annotated sen-
tences. The XML tags <e1> and <e2> mark the
target nominals. Since all nine proper semantic re-
lations in this task are asymmetric, the ordering of
the two nominals must be taken into account. In
example 1, CAUSE-EFFECT(e1, e2) does not hold,
although CAUSE-EFFECT(e2, e1) would. In exam-
ple 2, COMPONENT-WHOLE(e2, e1) holds.
We are currently developing annotation guide-
lines for each of the relations. They will give a pre-
cise definition for each relation and some prototypi-
cal examples, similarly to SemEval-2007 Task 4.
The annotation will take place in two rounds. In
the first round, we will do a coarse-grained search
97
for positive examples for each relation. We will
collect data from the Web using a semi-automatic,
pattern-based search procedure. In order to ensure
a wide variety of example sentences, we will use
several dozen patterns per relation. We will also
ensure that patterns retrieve both positive and nega-
tive example sentences; the latter will help populate
the OTHER relation with realistic near-miss negative
examples of the other relations. The patterns will
be manually constructed following the approach of
Hearst (1992) and Nakov and Hearst (2008).6
The example collection for each relation R will
be passed to two independent annotators. In order to
maintain exclusivity of relations, only examples that
are negative for all relations but R will be included
as positive and only examples that are negative for
all nine relations will be included as OTHER. Next,
the annotators will compare their decisions and as-
sess inter-annotator agreement. Consensus will be
sought; if the annotators cannot agree on an exam-
ple it will not be included in the data set, but it will
be recorded for future analysis.
Finally, two other task organizers will look for
overlap across all relations. They will discard any
example marked as positive in two or more relations,
as well as examples in OTHER marked as positive in
any of the other classes. The OTHER relation will,
then, consist of examples that are negatives for all
other relations and near-misses for any relation.
Data sets. The annotated data will be divided into
a training set, a development set and a test set. There
will be 1000 annotated examples for each of the
ten relations: 700 for training, 100 for development
and 200 for testing. All data will be released under
the Creative Commons Attribution 3.0 Unported Li-
cense7. The annotation guidelines will be included
in the distribution.
5 The Classification Task
The actual task that we will run at SemEval-2010
will be a multi-way classification task. Not all pairs
of nominals in each sentence will be labeled, so the
gold-standard boundaries of the nominals to be clas-
sified will be provided as part of the test data.
6Note that, unlike in Semeval 2007 Task 4, we will not re-
lease the patterns to the participants.
7http://creativecommons.org/licenses/by/
3.0/
In contrast with Semeval 2007 Task 4, in which
the ordering of the entities was provided with each
example, we aim at a more realistic scenario in
which the ordering of the labels is not given. Par-
ticipants in the task will be asked to discover both
the relation and the order of the arguments. Thus,
the more challenging task is to identify the most
informative ordering and relation between a pair
of nominals. The stipulation ?most informative?
is necessary since with our current set of asym-
metrical relations that includes OTHER, each pair
of nominals that instantiates a relation in one di-
rection (e.g., REL(e1, e2)), instantiates OTHER in
the inverse direction (OTHER (e2, e1)). Thus, the
correct answers for the two examples in Figure 1
are CAUSE-EFFECT (earthquake, aftershocks) and
COMPONENT-WHOLE (large kitchen, apartment).
Note that unlike in SemEval-2007 Task 4, we will
not provide manually annotated WordNet senses,
thus making the task more realistic. WordNet senses
did, however, serve for disambiguation purposes in
SemEval-2007 Task 4. We will therefore have to
assess the effect of this change on inter-annotator
agreement.
6 Evaluation Methodology
The official ranking of the participating systems will
be based on their macro-averaged F-scores for the
nine proper relations. We will also compute and re-
port their accuracy over all ten relations, including
OTHER. We will further analyze the results quan-
titatively and qualitatively to gauge which relations
are most difficult to classify.
Similarly to SemEval-2007 Task 4, in order to
assess the effect of varying quantities of training
data, we will ask the teams to submit several sets of
guesses for the labels for the test data, using varying
fractions of the training data. We may, for example,
request test results when training on the first 50, 100,
200, 400 and all 700 examples from each relation.
We will provide a Perl-based automatic evalua-
tion tool that the participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
7 Conclusion
We have introduced a new task, which will be part of
SemEval-2010: multi-way classification of semantic
98
relations between pairs of common nominals. The
task will compare different approaches to the prob-
lem and provide a standard testbed for future re-
search, which can benefit many NLP applications.
The description we have presented here should
be considered preliminary. We invite the in-
terested reader to visit the official task web-
site http://semeval2.fbk.eu/semeval2.
php?location=tasks\#T11, where up-to-
date information will be published; there is also a
discussion group and a mailing list.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. EMNLP 2004, pages 33?40.
Dmitry Davidov and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals using
pattern clusters. In Proc. ACL-08: HLT, pages 227?
235.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Charles J. Fillmore. 2002. FrameNet and the linking be-
tween semantic and syntactic relations. In Proc. COL-
ING 2002, pages 28?36.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Roxana Girju, Dan Moldovan, Marta Tatu, , and Dan An-
tohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19:479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. 4th Semantic Eval-
uation Workshop (SemEval-2007).
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2008.
Classification of semantic relations between nominals.
Language Resources and Evaluation. In print.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING
92, pages 539?545.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
types of some generic relation arguments: Detection
and evaluation. In Proc. ACL-08: HLT, Short Papers,
pages 185?188.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCAI, pages 945?956.
George Lakoff. 1987. Women, fire, and dangerous
things. University of Chicago Press, Chicago, IL.
Maria Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28:357?388.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 60?67.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. ACL-08: HLT, pages 452?460.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and WordNet-
based features. In Proc. AAAI, pages 781?787.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. COLING 2008, pages 649?656.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?106.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. COLING/ACL, pages
113?120.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proc. EMNLP-CoNLL), pages
717?727.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proc. EMNLP 2001,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in re-
lational semantics. In Proc. ACL-02, pages 247?254.
Matthew Stephens, Mathew Palakal, Snehasis
Mukhopadhyay, Rajeev Raje, and Javed Mostafa.
2001. Detecting gene relations from Medline ab-
stracts. In Pacific Symposium on Biocomputing, pages
483?495.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Peter D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. IJCAI, pages 1136?
1141.
99
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 238?247,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Entity Extraction via Ensemble Semantics
Marco Pennacchiotti
Yahoo! Labs
Sunnyvale, CA, 94089
pennac@yahoo-inc.com
Patrick Pantel
Yahoo! Labs
Sunnyvale, CA, 94089
ppantel@yahoo-inc.com
Abstract
Combining information extraction sys-
tems yields significantly higher quality re-
sources than each system in isolation. In
this paper, we generalize such a mixing of
sources and features in a framework called
Ensemble Semantics. We show very large
gains in entity extraction by combining
state-of-the-art distributional and pattern-
based systems with a large set of fea-
tures from a webcrawl, query logs, and
Wikipedia. Experimental results on a web-
scale extraction of actors, athletes and mu-
sicians show significantly higher mean av-
erage precision scores (29% gain) com-
pared with the current state of the art.
1 Introduction
Mounting evidence shows that combining infor-
mation sources and information extraction algo-
rithms leads to improvements in several tasks
such as fact extraction (Pas?ca et al, 2006), open-
domain IE (Talukdar et al, 2008), and entailment
rule acquisition (Mirkin et al, 2006). In this paper,
we show large gains in entity extraction by com-
bining state-of-the-art distributional and pattern-
based systems with a large set of features from
a 600 million document webcrawl, one year of
query logs, and a snapshot of Wikipedia. Further,
we generalize such a mixing of sources and fea-
tures in a framework called Ensemble Semantics.
Distributional and pattern-based extraction al-
gorithms capture aspects of paradigmatic and syn-
tagmatic dimensions of semantics, respectively,
and are believed to be quite complementary. Pas?ca
et al (2006) showed that filtering facts, extracted
by a pattern-based system, according to their ar-
guments? distributional similarity with seed facts
yielded large precision gains. Mirkin et al (2006)
showed similar gains on the task of acquiring lex-
ical entailment rules by exploring a supervised
combination of distributional and pattern-based al-
gorithms using an ML-based SVM classifier.
This paper builds on the above work, by study-
ing the impact of various sources of features exter-
nal to distributional and pattern-based algorithms,
on the task of entity extraction. Mirkin et al?s re-
sults are corroborated on this task and large and
significant gains over this baseline are obtained
by incorporating 402 features from a webcrawl,
query logs and Wikipedia. We extracted candidate
entities for the classes Actors, Athletes and Mu-
sicians from a webcrawl using a variant of Pas?ca
et al?s (2006) pattern-based engine and Pantel et
al.?s (2009) distributional extraction system. A
gradient boosted decision tree is used to learn a re-
gression function over the feature space for rank-
ing the candidate entities. Experimental results
show 29% gains (19% nominal) in mean average
precision over Mirkin et al?s method and 34%
gains (22% nominal) in mean average precision
over an unsupervised baseline similar to Pas?ca et
al.?s method. Below we summarize the contribu-
tions of this paper:
? We explore the hypothesis that although dis-
tributional and pattern-based algorithms are
complementary, they do not exhaust the se-
mantic space; other sources of evidence can
be leveraged to better combine them;
? We model the mixing of knowledge sources
and features in a novel and general informa-
tion extraction framework called Ensemble
Semantics; and
? Experiments over an entity extraction task
show that our model achieves large and sig-
nificant gains over state-of-the-art extractors.
A detailed analysis of feature correlations
and interactions shows that query log and we-
bcrawl features yield the highest gains, but
easily accessible Wikipedia features also im-
prove over current state-of-the-art systems.
238
Figure 1: The Ensemble Semantics framework for information extraction.
The remainder of this paper is organized as fol-
lows. In the next section, we present our Ensemble
Semantics framework and outline how various in-
formation extraction systems can be cast into the
framework. Section 3 then presents our entity ex-
traction system as an instance of Ensemble Se-
mantics, comparing and contrasting it with previ-
ous information extraction systems. Our experi-
mental methodology and analysis is described in
Section 4 and shows empirical evidence that our
extractor significantly outperforms prior art. Fi-
nally, Section 5 concludes with a discussion and
future work.
2 Ensemble Semantics
Ensemble Semantics (ES) is a general framework
for modeling information extraction algorithms
that combine multiple sources of information and
multiple extractors. The ES framework allows to:
? Represent multiple sources of knowledge and
multiple extractors of that knowledge;
? Represent multiple sources of features;
? Integrate both rule-based and ML-based
knowledge ranking algorithms; and
? Model previous information extraction sys-
tems (i.e., backwards compatibility).
2.1 ES Framework
ES can be instantiated to extract various types of
knowledge such as entities, facts, and lexical en-
tailment rules. It can also be used to better under-
stand the commonalities and differences between
existing information extraction systems.
After presenting the framework in the next sec-
tion, Section 2.2 shows how previous information
extraction algorithms can be cast into ES. In Sec-
tion 3 we describe our novel entity extraction al-
gorithm based on ES.
The ES framework is illustrated in Figure 1. It
decomposes the process of information extraction
into the following components:
Sources (S): textual repositories of information,
either structured (e.g., a database such as DBpe-
dia), semi-structured (e.g., Wikipedia Infoboxes or
HTML tables) or unstructured (e.g., news articles
or a webcrawl).
Knowledge Extractors (KE): algorithms re-
sponsible for extracting candidate instances such
as entities or facts. Examples include fact extrac-
tion systems such as (Cafarella et al, 2005) and
entity extraction systems such as (Pas?ca, 2007).
Feature Generators (FG): methods that extract
evidence (features) of knowledge in order to de-
cide which candidate instances extracted from
KEs are correct. Examples include capitalization
features for named entity extractors, and the dis-
tributional similarity matrix used in (Pas?ca et al,
2006) for filtering facts.
Aggregator (A). A module collecting and as-
sembling the instances coming from the different
extractors. This module keeps the footprint of
each instance, i.e. the number and the type of the
KEs that extracted the instance. This information
can be used by the Ranker module to build a rank-
ing strategy, as described below.
Ranker (R): a module for ranking the knowl-
edge instances returned from KEs using the fea-
tures generated by FGs. Ranking algorithms may
be rule-based (e.g., the one using a threshold on
distributional similarity in (Pas?ca et al, 2006)) or
ML-based (e.g., the SVM model in (Mirkin et al,
2006) for combining pattern-based and distribu-
tional features).
239
The Ranker is composed of two sub-modules:
the Modeler and the Decoder. The Modeler is re-
sponsible for creating the model which ranks the
candidate instances. The Decoder collects the can-
didate instances from the Aggregator, and applies
the model to produce the final ranking.
In rule-based systems, the Modeler corresponds
to a set of manually crafted or automatically in-
duced rules operating on the features (e.g. a com-
bination of thresholds). In ML-based systems, it is
an actual machine learning algorithm, that takes as
input a set of labeled training instances, and builds
the model according to their features. Training in-
stances can be obtained as a subset of those col-
lected by the Aggregator, or from some exter-
nal resource. In many cases, training instances
are manually labeled by human experts, through
a long and costly editorial process.
Information sources (S) serve as inputs to the
system. Some sources will serve as sources for
knowledge extractors to generate candidate in-
stances, some will serve as sources for feature gen-
erators to generate features or evidence of knowl-
edge, and some will serve as both.
2.2 Related Work
To date, most information extraction systems rely
on a model composed of a single source S, a single
extractor KE and a single feature generator FG.
For example, many classic relation extraction sys-
tems (Hearst, 1992; Riloff and Jones, 1999; Pan-
tel and Pennacchiotti, 2006; Pas?ca et al, 2006)
are based on a single pattern-based extractor KE,
which is seeded with a set of patterns or instances
for a given relation (e.g. the pattern ?X starred in
Y? for the act-in relation). The extractor then itera-
tively extracts new instances until a stop condition
is met. The resulting extractor scores are proposed
by FG as a feature. The Ranker simply consists
of a sorting function on the feature from FG.
Systems such as the above that do not consist
of multiple sources, knowledge extractors or fea-
ture generators are not considered Ensemble Se-
mantics models, even though they can be cast into
the framework. Recently, some researchers have
explored more complex systems, having multiple
sources, extractors and feature generators. Below
we show examples and describe how they map as
Ensemble Semantics systems. We use this charac-
terization to clearly outline how our proposed en-
tity extraction system, proposed in Section 3, dif-
fers from previous work.
Talukdar et al (2008) present a weakly-
supervised system for extracting large sets of
class-instance pairs using two knowledge extrac-
tors: a pattern-based extractor supported by distri-
butional evidence, which harvests candidate pairs
from a Web corpus; and a table extractor that har-
vests candidates from Web tables. The Ranker
uses graph random walks to combine the informa-
tion of the two extractors and output the final list.
The authors show large improvements in coverage
with little precision loss.
Mirkin et al (2006) introduce a machine learn-
ing system for extracting lists of lexical entail-
ments (e.g. ?government?? ?organization?). They
rely on two knowledge extractors, operating on a
same large textual source: a pattern-based extrac-
tor, leveraging the Hearst (1992) is-a patterns; and
a distributional extractor applied to a set of entail-
ment seeds. Candidate instances are passed to an
SVM Ranker, which uses features stemming from
the two extractors, to decide which instances are
output in the final list. The authors report a +9%
increase in F-measure over a rule-based system
that takes the union of the instances extracted by
the two modules.
Other examples include the system for
taxonomic-relation extraction by Cimiano et
al. (2005), using a pool of feature genera-
tors based on pattern-based, distributional
and WordNet techniques; and Pas?ca and Van
Durme?s (2008) system that uses a Web corpus
and query logs to extract semantic classes and
their attributes.
Similarly to these methods, our proposed entity
extractor (Section 3) utilizes multiple sources and
extractors. A key difference of our method lies in
the Feature Generator module. We propose sev-
eral generators resulting in 402 features extracted
from Web pages, query logs and Wikipedia arti-
cles. The use of these features results in dramatic
performance improvements, reported in Section 4.
3 ES for Entity Extraction
Entity extraction is a fundamental task in NLP
responsible for extracting instances of semantic
classes (e.g., ?Brad Pitt? and ?Tom Hanks? are in-
stances of the class Actors). It forms a build-
ing block for various NLP tasks such as on-
tology learning (Cimiano and Staab, 2004) and
co-reference resolution (Mc Carthy and Lehn-
240
Family Type Features
Web (w) Frequency (wF ) term frequency; document frequency; term frequency as noun phrase
Pattern (wP ) confidence score returned by KE
pat
; pmi with the 100 most reliable patterns
used by KE
pat
Distributional (wD) distributional similarity with the centroid in KE
dis
; distributional similarities
with each seed in S
Termness (wT ) ratio between term frequency as noun phrase and term frequency; pmi between
internal tokens of the instance; capitalization ratio
Query log (q) Frequency (qF ) number of queries matching the instance; number of queries containing the in-
stance
Co-occurrence (qC) query log pmi with any seed in S
Pattern (qP ) pmi with a set of trigger words T (i.e., the 10 words in the query logs with
highest pmi with S)
Distributional (qD) distributional similarity with S (vector coordinates consist of the instance?s pmi
with the words in T )
Termness (qT ) ratio between the two frequency features F
Web table (t) Frequency (tF ) table frequency
Co-occurrence (tC) table pmi with S; table pmi with any seed in S
Wikipedia (k) Frequency (kF ) term frequency
Co-occurrence (kC) pmi with any seed in S
Distributional (kD) distributional similarity with S
Table 1: Feature space describing each candidate instance (S indicates the set of seeds for a given class).
ert, 2005). Search engines such as Yahoo, Live,
and Google collect large sets of entities (Pas?ca,
2007; Chaudhuri et al, 2009) to better interpret
queries (Tan and Peng, 2006), to improve query
suggestions (Cao et al, 2008) and to understand
query intents (Hu et al, 2009). Entity extraction
differs from the similar task of named entity ex-
traction, in that classes are more fine-grained and
possibly overlapping.
Below, we propose a new method for entity ex-
traction built on the ES framework (Section 3.1).
Then, we comment on related work in entity ex-
traction (Section 3.2).
3.1 ES Entity Extraction Model
In this section, we propose a novel entity ex-
traction model following the Ensemble Semantics
framework presented in Section 2. The sources of
our systems can come from any textual corpus. In
our experiments (described in Section 4.1), we ex-
tracted entities from a large crawl of the Web, and
generated features from this crawl as well as query
logs and Wikipedia.
3.1.1 Knowledge extractors
Our system relies on two knowledge extractors:
one pattern-based and the other distributional.
Pattern-based extractor (KE
pat
). We reimple-
mented Pas?ca et al?s (2006) state-of-the-art web-
scale fact extractor, which, given seed instances of
a binary relation, finds instances of that relation.
We extract entities of a class, such as Actors, by
instantiating typical relations involving that class
such as act-in(Actor, Movie). We instantiate such
relations instead of the classical is-a patterns since
these have been shown to bring in too many false
positives, see (Pantel and Pennacchiotti, 2006) for
a discussion of such generic patterns. The extrac-
tor?s confidence score for each instance is used by
the Ranker to score the entities being extracted.
Section 4.1 lists the system parameters we used in
our experiments.
Distributional extractor (KE
dis
). We use Pan-
tel et al?s (2009) distributional entity extractor.
For each noun in our source corpus, we build a
context vector consisting of the noun chunks pre-
ceding and following the target noun, scored us-
ing pointwise mutual information (pmi). Given
a small set of seed entities S of a class, the ex-
tractor computes the centroid of the seeds? context
vectors as a geometric mean, and then returns all
nouns whose similarity with the centroid exceeds a
threshold ? (using the cosine measure between the
context vectors). Full algorithmic details are pre-
sented in (Pantel et al, 2009). Section 4.1 lists the
threshold and text preprocessing algorithms used
in our experiments.
The Aggregator simply takes a union of the en-
tities discovered by the two extractors.
3.1.2 Feature generators
Our model includes four feature generators,
which compute a total of 402 features (full set
described in Table 1). Each generator extracts
from a specific source a feature family, as follows:
? Web (w): a body of 600 million documents
241
crawled from the Web at Yahoo! in 2008;
? Query logs (q): one year of web search
queries issued to the Yahoo! search engine;
? Web tables: all HTML inner tables extracted
from the above Web source; and
? Wikipedia: an official Wikipedia dump from
February 2008, consisting of about 2 million
articles.
Feature families are further subclassified into
five types: frequency (F) (frequency-based fea-
tures); co-occurrence (C) (features capturing first
order co-occurrences between an instance and
class seeds); distributional (D) (features based on
the distributional similarity between an instance
and class seeds); pattern (P) (features indicat-
ing class-specific lexical pattern matches); and
termness (T) (features used to distinguish well-
formed terms such as ?Brad Pitt? from ill-formed
ones such as ?with Brad Pitt?). The seeds S used
in many of the feature families are the same seeds
used by the KE
pat
extractor, described in Sec-
tion 3.1.1.
The different seed families are designed to cap-
ture different semantic aspects: paradigmatic (D),
syntagmatic (C and P), popularity (F), and term
cohesiveness (T).
3.1.3 ML-based Ranker
Our Modeler adopts a supervised ML regression
model. Specifically, we use a Gradient Boosted
Decision Tree regression model - GBDT (Fried-
man, 2001), which consists of an ensemble of de-
cision trees, fitted in a forward step-wise manner
to current residuals. Friedman (2001) shows that
by drastically easing the problem of overfitting on
training data (which is common in boosting al-
gorithms), GDBT competes with state-of-the-art
machine learning algorithms such as SVM (Fried-
man, 2006) with much smaller resulting models
and faster decoding time. The model is trained
on a manually annotated random sample of enti-
ties taken from the Aggregator, using the features
generated by the four generators presented in Sec-
tion 3.1.2. The Decoder then ranks each entity ac-
cording to the trained model.
3.2 Related Work
Entity extraction systems follow two main ap-
proaches: pattern-based and distributional. The
pattern-based approach leverages lexico-syntactic
patterns to extract instances of a given class. Most
commonly used are is-a pattern families such as
those first proposed by Hearst (1992) (e.g., ?Y such
as X? for matching ?actors such as Brad Pitt?).
Minimal supervision is used in the form of small
sets of manually provided seed patterns or seed in-
stances. This approach is very common in both
the NLP and Semantic Web communities (Cimi-
ano and Staab, 2004; Cafarella et al, 2005; Pantel
and Pennacchiotti, 2006; Pas?ca et al, 2006).
The distributional approach uses contextual ev-
idence to model the instances of a given class,
following the distributional hypothesis (Harris,
1964). Weakly supervised, these methods take a
small set of seed instances (or the class label) and
extract new instances from noun phrases that are
most similar to the seeds (i.e., that share similar
contexts). Following Lin (1998), example sys-
tems include Fleischman and Hovy (2002), Cimi-
ano and Volker (2005), Tanev and Magnini (2006),
and Pantel et al (2009).
4 Experimental Evaluation
This section reports our experiments, showing the
effectiveness of our entity extraction system and
the importance of our different feature families.
4.1 Experimental Setup
Evaluated classes. We evaluate our system over
three classes: Actors (movie, tv and stage ac-
tors); Athletes (professional and amateur); Musi-
cians (singers, musicians, composers, bands, and
orchestras)
System setup. We instantiated our knowledge
extractors, KE
pat
and KE
dis
from Section 3.1.1,
over our Web crawl of 600 million documents (see
Section 3.1.2). The documents were preprocessed
using Brill?s POS-tagger (Brill, 1995) and the Ab-
ney?s chunker (Abney, 1991). ForKE
dis
, context
vectors are extracted for noun phrases recognized
as NP-chunks with removed modifiers. The vec-
tor space includes the 250M most frequent noun
chunks in the corpus. KE
dis
returns as instances
all noun phrases having a similarity with the seeds?
centroid above ? = 0.005
1
. The sets of seeds S
for KE
dis
include 10, 24 and 10 manually chosen
instances for respectively the Actors, Athletes and
Musicians classes
2
. The sets of seedsP forKE
pat
1
Experimentally set on an independent development set.
2
The higher number of seeds for Athletes is chosen to
cover different sports.
242
Dataset Actors Athletes Musicians
KE
pat
58,005 40,816 125,657
KE
dis
72,659 24,380 24,593
KE
pat
?KE
dis
113,245 61,709 142,694
KE
pat
?KE
dis
17,419 3,487 7,556
R 500 500 500
P=80 P=258 P=134
N=420 N=242 N=366
Table 2: Number of extracted instances and the
sample sizes (P and N indicate positive and neg-
ative annotations).
include 11, 8 and 9 pairs respectively for the Ac-
tors (relation acts-in), Athletes (relation plays-for)
and Musicians (relation part-of-band) classes. Ta-
ble 6 lists all seeds for both KE
dis
and KE
pat
.
The GBDT ranker uses an ensemble of 300 trees.
3
Goldset Preparation. The number of instances
extracted by KE
pat
and KE
dis
for each class
is reported in Table 2. For each class, we ex-
tract a random sample R of 500 instances from
KE
pat
?KE
dis
. A pool of 10 paid expert editors
annotated the instances of each class inR as posi-
tive or negative. Inter-annotator overlap was 0.88.
Uncertain instances were manually adjudicated by
a separate paid expert editor, yielding a gold stan-
dard dataset for each class.
Evaluation Metrics. Entity extraction perfor-
mance is evaluated using the average precision
(AP) statistic, a standard information retrieval
measure for evaluating ranking algorithms, de-
fined as:
AP (L) =
?
|L|
i=1
P (i) ? corr(i)
?
|L|
i=1
corr(i)
(1)
where L is a ranked list produced by an extractor,
P (i) is the precision of L at rank i, and corr(i) is 1
if the instance at rank i is correct, and 0 otherwise.
AP is computed overR for each class.
We also evaluate the coverage, i.e. the percent-
age of instances extracted by a system wrt those
extracted by all systems.
In order to accurately compute statistical signif-
icance, our experiments are performed using 10-
fold cross validation.
Baselines and comparisons. We compare our
proposed ES entity extractor, using different fea-
ture configurations, with state-of-the-art systems
(referred to as baselines B* below):
3
GBDT model parameters were experimentally set on an
independent development set as follows: trees=300, shrink-
age=0.01, max nodes per tree=12, sample rate=0.5.
System Actors Athletes Musicians
AP Cov AP Cov AP Cov
B1 0.729 51.2% 0.616 66.1% 0.570 88.1%
B2 0.618 64.1% 0.687 39.5% 0.681 17.2%
B3 0.676 100% 0.664 100% 0.576 100%
B4 0.715 100% 0.697 100% 0.579 100%
ES-all 0.860? 100% 0.915? 100% 0.788? 100%
Table 3: Average precision (AP) and coverage
(Cov) results for our proposed system ES-all and
the baselines. ? indicates AP statistical signifi-
cance at the 0.95 level wrt all baselines.
ES-all. Our ES system, usingKE
pat
andKE
dis
,
the full set of feature families described in
Section 3.1.2, and the GBDT ranker.
B1. KE
pat
alone, a state-of-the-art pattern-
based extractor reimplementing (Pas?ca et al,
2006), where the Ranker assigns scores to in-
stances using the confidence score returned
by KE
pat
.
B2. KE
dis
alone, a state-of-the-art distributional
system implementing (Pantel et al, 2009),
where the Ranker assigns scores to instances
using the similarity score returned by KE
dis
alone.
B3. A rule-based ES system, combining B1 and
B2. This system uses bothKE
pat
andKE
dis
as extractors, and a Ranker that assigns
scores to instances according to the sum of
their normalized confidence scores.
B4. A state-of-the-art machine learning system
based on (Mirkin et al, 2006). This ES
system uses KE
pat
and KE
dis
as extractors.
The Ranker is a GBDT regression model,
using the full sets of features derived from
the two extractors, i.e., wP and wD (see
Table 1). GBDT parameters are set as for our
proposed ES-all system.
4.2 Experimental Results
Table 3 summarizes the average-precision (AP)
and coverage results for our ES-all system and the
baselines. Figure 2 reports the precision at each
rank for the Athletes class (the other two classes
follow similar trends). Table 6 lists the top-10 en-
tities discovered for each class on one test fold.
ES-all outperforms all baselines in AP (all results
are statistically significant at the 0.95 level), offer-
ing at the same time full coverage
4
.
4
Recall that coverage is reported relative to all instances
retrieved by extractors KE
pat
and KE
dis
.
243
Figure 2: Precision at rank for the different sys-
tems on the Athletes class.
Our simple rule-based combination baseline,
B3, leads to a large increase in coverage wrt the in-
dividual extractors alone (B1 and B2) without sig-
nificant impact on precision. The supervised ML-
based combination baseline (B4) consistently im-
proves AP across classes wrt the rule-based com-
bination (B3), but without statistical significance.
These results corroborate those found in (Mirkin et
al., 2006), where this ML-based combination was
reported to be significantly better than a rule-based
one on the task of lexical entailment acquisition.
The large set of features adopted in ES-all ac-
counts for a dramatic improvement in AP, indicat-
ing that existing state-of-the-art systems for entity
extraction (reflected by our baselines strategies)
are not making use of enough semantic cues. The
adoption of evidence other than distributional and
pattern-based, such as features coming from web
documents, HTML tables and query logs, is here
demonstrated to be highly valuable.
The above empirical claim can be grounded and
corroborated by a deeper semantic analysis. From
a semantic perspective, the above results translate
in the observation that distributional and pattern-
based evidence do not completely capture all se-
mantic aspects of entities. Other evidence, such as
popularity, term cohesiveness and co-occurrences
capture other aspects. For instance, in one of our
Actors folds, the B3 system ranks the incorrect in-
stance ?Tom Sellek? (a misspelling of ?Tom Sel-
leck?) in 9
th
position (out of 142), while ES-all
lowers it to the 33
rd
position, by relying on table-
based features (intuitively, tables contain much
fewer misspelling than running text). Other than
misspellings, ES-all fixes errors that are either typ-
ical of distributional approaches, such as the in-
clusion of instances of other classes (e.g. the
movie ?Someone Like You? often appears in con-
texts similar to those of actors); errors typical
of pattern-based approaches, such as incorrect in-
System AP MAP
Actors Athletes Musicians
B3 0.676 0.664 0.576 0.639
B4 0.715 0.697 0.579 0.664
B4+w 0.813
?
0.908
?
0.724
?
0.815
?
B4+q 0.815
?
0.905
?
0.743
?
0.821
?
B4+t 0.784
?
0.825
?
0.727
?
0.779
?
B4+k 0.776
?
0.825
?
0.624 0.741
?
B4+w+q 0.835
?
0.915
?
0.758
?
0.836
?
B4+w+t 0.840
?
0.906
?
0.774
?
0.840
?
B4+w+k 0.814
?
0.903
?
0.725
?
0.814
?
B4+q+t 0.847
?
0.910
?
0.774
?
0.844
?
B4+q+k 0.832
?
0.906
?
0.748
?
0.829
?
B4+t+k 0.817
?
0.861
?
0.743
?
0.807
?
B4+w+q+t 0.846
?
0.917
?
0.782
?
0.849
?
B4+w+q+k 0.841
?
0.916
?
0.756
?
0.838
?
B4+w+t+k 0.835
?
0.906
?
0.783
?
0.841
?
Es-all 0.860
?
0.915
?
0.788
?
0.854
?
Table 4: Overall AP results of the different feature
configurations, compared to two baselines. ? in-
dicates statistical significance at the 0.95 level wrt
B3. ? indicates statistical significance at 0.95 level
wrt both B3 and B4.
stances highly-associated with an ambiguous pat-
tern (e.g., the pattern ?X of the rock band Y? for
finding Musicians matched an incorrect instance
?song submission?); or errors typical of both, such
as the inclusion of common nouns (e.g. ?country
music hall?) or too generic last names (e.g. ?John-
son?). ES-all successfully recovers all these error
by using termness, co-occurrence and frequency
features.
We also compare ES-all with a state-of-the-art
random walk system (RW) presented by Talukdar
et al (2008) (see Section 2.2 for a description).
As we could not reimplement the system, we re-
port the following indirect comparison. RW was
evaluated on five entity classes, one of which, NFL
players, overlaps with our Athletes class. On this
class, they report 0.95 precision on the top-100
ranked entities. Unfortunately, they do not report
coverage or recall statistics, making the interpre-
tation of this analysis difficult. In an attempt to
compare RW with ES-all, we evaluated the preci-
sion of our top-100 Athletes, obtaining 0.99. Us-
ing a random sample of our extracted Athletes, we
approximate the precision of the top-22,000 Ath-
letes to be 0.97? 0.01 (at the 0.95 level).
4.3 Feature Analysis
Feature family analysis: Table 4 reports the av-
erage precision (AP) for our system using different
feature family combinations (see Table 1). Col-
umn 1 reports the family combinations; columns
244
2-4 report AP for each class; and column 5 reports
the mean-average-precision (MAP) across classes.
In all configurations, except the k family alone,
and along all classes, our system significantly out-
performs (at the 0.95 level) the baselines.
Rows 3-6 report the performance of each fea-
ture family alone. w and t are consistently better
than q and k, across all classes. k is shown to be
the least useful family. This is mostly due to data
sparseness, e.g., in our experiments almost 40%
of the test instances in the Actors sample do not
have any occurrence in Wikipedia. However, with-
out access to richer resources such as a webcrawl
or query logs, the features from k do indeed pro-
vide large gains over current baselines (on average
+10.2% and +7.7% over B3 and B4).
Rows 7-12 report results for combinations of
two feature families. All combinations (except
those with k) appear valuable, substantially in-
creasing the single-family results in rows 3-6, in-
dicating that combining different feature families
(as suggested by the ES paradigm) is helpful. Sec-
ond, it indicates that q, w and t convey comple-
mentary information, thus boosting the regression
model when combined together. It is interesting to
notice that q+t tends to be the best combination,
surprising given that t alone did not show high per-
formance (row 5). One would expect the combina-
tion q+w to outperform q+t, but the good perfor-
mance of q+t is mainly due to the fact that these
two families are more complementary than q+w.
To verify this intuition, we compute the Spearman
correlation coefficient r among the rankings pro-
duced by the different combinations. As expected,
q and w have a higher correlation (r = 0.82) than
q and t (r = 0.67) and w and t (r = 0.66), suggest-
ing that q and w tend to subsume each other (i.e.
no added information for the regression model).
Rows 13-15 report results for combinations of
three feature families. As expected, the best
combination is q+w+t with an average precision
nearly identical to the full ES-all system. If one
has access to Web or query log sources, then the
value of the Wikipedia features tends to be sub-
sumed by our web and query log features.
Feature by feature analysis: The feature fam-
ilies analyzed in the previous section consist of
402 features. For each trained GBDT model,
one can inspect the resulting most important fea-
tures (Friedman, 2001). Consistently, the two
most important features for ES-all are, as ex-
System AP MAP
Actors Athletes Musicians
B4 0.715 0.697 0.579 0.664
B4+w 0.813 0.908 0.724 0.815
B4+wF 0.798 0.865 0.679 0.781
B4+wT 0.806 0.891 0.717 0.805
B4+t 0.784 0.825 0.727 0.779
B4+tF 0.760 0.802 0.701 0.781
B4+tC 0.771 0.815 0.718 0.805
B4+q 0.815 0.905 0.743 0.821
B4+qF 0.786 0.890 0.693 0.790
B4+qC 0.715 0.738 0.581 0.678
B4+qD 0.735 0.709 0.644 0.696
B4+qP 0.779 0.796 0.648 0.741
B4+qT 0.780 0.868 0.725 0.791
B4+qF+qW+qT 0.816 0.906 0.743 0.822
ES-all 0.860 0.915 0.788 0.854
Table 5: Ablation study of the web (w), query-
log (q) and table (t) features (bold letters indicate
whole feature families).
pected, the confidence scores of KE
pat
and
KE
dis
. This suggests that syntagmatic and
paradigmatic information are most important in
defining the semantics of entities. Also very im-
portant, in third position, is a feature from qT ,
namely the ratio between the number of queries
matching the instance and the number of queries
containing it as a substring. This feature is a strong
indicator of termness.
Webcrawl term frequencies and document fre-
quencies (from the wF set) are also important.
Very frequent and infrequent instances were found
to be often incorrect (e.g., respectively ?song? and
?Brad Pitttt?). Table PMI (a feature in the qC fam-
ily) also ranked high in importance: instances that
co-occurr very frequently in the same column/row
with seeds S are often found to be correct (e.g.,
a column containing the seeds ?Brad Pitt? and
?Tom Hanks? will likely contains other actors).
Other termness (T ), frequency-based (F ) and co-
occurrence (C) features also play some role in the
model.
Variable importance is only an intrinsic indi-
cator of feature relevance. In order to better as-
sess the actual impact of the single features on
AP, we ran our system on each feature type: re-
sults for the web (w), query log (q) and table (t)
families are reported in Table 5. For reason of
space constraints, we here only focus on some
high level observations. The set of web termness
features (wT ) and frequency features (wF ) are
alone able to provide a large improvement over B4
(row 1), while their combination (row 2) does not
improve much over the features taken individually.
245
Seed instances forKE
dis
Actors Athletes Musicians
Jodie Foster Bob Gibson Jared Allen Randy Moss Rise Against the Machine
Humphrey Bogart Don Drysdale Andres Romero Peyton Manning Pink Floyd
Anthony Hopkins Albert Pujols Kenny Perry Jerry Rice Spice Girls
Katharine Hepburn Yogi Berra Martin Kaymer Robert Karlsson Pussycat Dolls
Christopher Walken Dejan Bodiroga Alexander Ovechkin Gheorghe Hagi The Beatles
Gene Hackman Allen Iverson Shea Weber Marco Van Basten Iron Maiden
Diane Keaton Yao Ming Patrick Roy Zinedine Zidane John Lennon
Edward Norton Tim Duncan Alexei Kovalev Roberto Baggio Frank Sinatra
Robert Duvall Led Zeppelin
Hilary Swank Freddie Mercury
Seed instances forKE
pat
Actors Athletes Musicians
Dennis Hopper - The Good Life Dallas cowboys - Julius Crosslin Kevin Brown - Corndaddy
Tom Hanks - The Terminal New york Giants - Plaxico Burress Barry Gibb - The Bee Gees
Julia Roberts - Mona Lisa Smile Philadelphia Eagles - Danny Amendola Patty Smyth - Scandal
Kevin Bacon - Footloose Washington Redskins - Rock Cartwright Dave Matthews - Dave Mathews Band
Keanu Reeves - The Lake House New England Patriots - Laurence Maroney Gwen Stefani - No Doubt
Marlon Brando - Don Jaun Demarco Buffalo Bills - Xavier Omon George Michael - Wham
Morgan Freeman - The Shawshank Redemption Miami Dolphins - Ernest Wilford Mark Knopfler - Dire Straits
Nicole Kidman - Eyes Wide Shut New York Jets - Chansi Stuckey Brian Jones - The Rolling Stones
Al Pacino - The Godfather Pete Shelley - Buzzcocks
Johnny Depp - Chocolat
Halle Berry - Monster?s Ball
10-best ranked instances in one test fold
Actors Athletes Musicians
Gordon Tootoosis Ron Randell Rumeal Robinson Todd Warriner Colin Newman Wu-tang Clan
Rosalind Chao Alimi Ballard Jeff Mcinnis Hong-chih Kuo Ghost Circus Tristan Prettyman
John Hawkes Fernando Lamas Ahmad Nivins Leon Clarke Ray Dorset Top Cats
Jeffrey Dean Morgan Bruno Cremer Carlos Marchena Josh Dollard Plastic Tree *Roseanne
George Macready Muhammad Bakri Chad Kreuter Robbie Alomar *Doomwatch John Moen
Table 6: Listing of all seeds used for KE
dis
and KE
pat
, as well as the top-10 entities discovered by
ES-all on one of our test folds.
This suggests that wT and wF capture very simi-
lar information: they are indeed highly correlated
(r = 0.80). Rows 5-7 refer to web table features:
the features tC outperform and subsume the fre-
quency features tF (r = 0.92). For query log
features (rows 8-14), only qF , qP and qT signif-
icantly increase performance. Distributional and
co-occurrence features (qD and qC) have very low
effect, as they are mostly subsumed by the others.
The combination of qF , qP and qT (row 14) per-
forms as well as the whole q (row 8).
Experiment conclusions: From our experi-
ments, we can draw the following conclusions:
1. Wikipedia features taken alone outperform
the baselines, however, web and query log
features, if available, subsume Wikipedia fea-
tures;
2. q, t and w are all important, and should be
used in combination, as they drive mostly in-
dependent information;
3. the syntagmatic and paradigmatic informa-
tion conveyed by the two extractors are most
relevant, and can be significantly boosted by
adding frequency- and termness-based fea-
tures from other sources.
5 Conclusions and Future Work
In this paper, we presented a general informa-
tion extraction framework, called Ensemble Se-
mantics, for combining multiple sources of knowl-
edge, and we instantiated the framework to build
a novel ML-based entity extraction system. The
system significantly outperforms state-of-the-art
ones by up to 22% in mean average precision.
We provided an in-depth analysis of the impact of
our proposed 402 features, their feature families
(Web documents, HTML tables, query logs, and
Wikipedia), and feature types.
There is ample directions for future work. On
entity extraction, exploring more knowledge ex-
tractors from different sources (such as the HTML
tables and query log sources used for our features)
is promising. Other feature types may potentially
capture other aspects of the semantics of entities,
such as WordNet and search engine click logs. For
the ranking system, semi- or weakly-supervised
algorithms may provide competing performance
to our model with reduced manual labor. Finally,
there are many opportunities for applying the gen-
eral Ensemble Semantics framework to other in-
formation extraction tasks such as fact extraction
and event extraction.
246
References
Steven Abney. 1991. Learning taxonomic rela-
tions from heterogeneous sources of evidence. In
Principle-Based Parsing. Kluwer Academic Pub-
lishers.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4).
Michael J. Cafarella, Doug Downey, Stephen Soder-
land, and Oren Etzioni. 2005. KnowItNow: Fast,
scalable information extraction from the web. In
Proceedings of EMNLP-2005.
Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen
Liao, Enhong Chen, and Hang Li. 2008. Context-
aware query suggestion by mining click-through and
session data. In Proceedings of KDD-08, pages
875?883.
Surajit Chaudhuri, Venkatesh Ganti, and Dong Xin.
2009. Exploiting web search to generate synonyms
for entities. In Proceedings of WWW-09, pages 151?
160.
Philipp Cimiano and Steffen Staab. 2004. Learning by
googling. SIGKDD Explorations, 6(2):24?34.
Philipp Cimiano and Johanna Volker. 2005. To-
wards large-scale, open-domain and ontology-based
named entity classification. In Proceedings of
RANLP-2005, pages 166?172.
Philipp Cimiano, Aleksander Pivk, Lars Schmidt-
Thieme, and Steffen Staab. 2005. Learning taxo-
nomic relations from heterogeneous sources of ev-
idence. In Ontology Learning from Text: Meth-
ods, Evaluation and Applications, pages 59?73. IOS
Press.
Michael Fleischman and Eduard Hovy. 2002. Classi-
fication of named entities. In Proceedings of COL-
ING 2002.
Jerome H. Friedman. 2001. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics, 29(5):1189?1232.
Jerome H. Friedman. 2006. Recent advances in pre-
dictive (machine) learning. Journal of Classifica-
tion, 23(2):175?197.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING-92, pages 539?545.
Jian Hu, Gang Wang, Fred Lochovsky, Jian tao Sun,
and Zheng Chen. 2009. Understanding user?s query
intent with Wikipedia. In Proceedings of WWW-09,
pages 471?480.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL-
98.
Joseph F. Mc Carthy and Wendy G Lehnert. 2005. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of IJCAI-1995, pages 1050?1055.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
Proceedings of ACL/COLING-06, pages 579?586.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proceedings of ACL-08, pages 19?27.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Organizing and
searching the world wide web of facts - step one:
The one-million fact extraction challenge. In Pro-
ceedings of AAAI-06, pages 1400?1405.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Pro-
ceedings of CIKM-07, pages 683?690, New York,
NY, USA.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: A Bootstrapping Algorithm for Automat-
ically Harvesting Semantic Relations. In Proceed-
ings of ACL-2006, Sydney, Australia.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP-09, Singapore.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In Proceedings of AAAI-99, pages
474?479.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas?ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of EMNLP-08, pages 582?
590.
Bin Tan and Fuchun Peng. 2006. Unsupervised
query segmentation using generative language mod-
els and wikipedia. In Proceedings of WWW-06,
pages 1400?1405.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proceedings of EACL-2006.
247
Proceedings of NAACL HLT 2007, pages 131?138,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Domain Restriction Hypothesis:
Relating Term Similarity and Semantic Consistency
Alfio Massimiliano Gliozzo
ITC-irst
Trento, Italy
gliozzo@itc.it
Marco Pennacchiotti
University of Rome Tor Vergata
Rome, Italy
pennacchiotti@info.uniroma2.it
Patrick Pantel
USC, Information Sciences Institute
Marina del Rey, CA
pantel@isi.edu
Abstract
In this paper, we empirically demonstrate
what we call the domain restriction hy-
pothesis, claiming that semantically re-
lated terms extracted from a corpus tend
to be semantically coherent. We apply
this hypothesis to define a post-processing
module for the output of Espresso, a state
of the art relation extraction system, show-
ing that irrelevant and erroneous relations
can be filtered out by our module, in-
creasing the precision of the final output.
Results are confirmed by both quantita-
tive and qualitative analyses, showing that
very high precision can be reached.
1 Introduction
Relation extraction is a fundamental step in
many natural language processing applications such
as learning ontologies from texts (Buitelaar et
al., 2005) and Question Answering (Pasca and
Harabagiu, 2001).
The most common approach for acquiring con-
cepts, instances and relations is to harvest semantic
knowledge from texts. These techniques have been
largely explored and today they achieve reasonable
accuracy. Harvested lexical resources, such as con-
cept lists (Pantel and Lin, 2002), facts (Etzioni et
al., 2002) and semantic relations (Pantel and Pen-
nacchiotti, 2006) could be then successfully used in
different frameworks and applications.
The state of the art technology for relation extrac-
tion primarily relies on pattern-based approaches
(Snow et al, 2006). These techniques are based on
the recognition of the typical patterns that express
a particular relation in text (e.g. ?X such as Y?
usually expresses an is-a relation). Yet, text-based
algorithms for relation extraction, in particular
pattern-based algorithms, still suffer from a number
of limitations due to complexities of natural lan-
guage, some of which we describe below.
Irrelevant relations. These are valid relations
that are not of interest in the domain at hand. For
example, in a political domain, ?Condoleezza Rice
is a football fan? is not as relevant as ?Condoleezza
Rice is the Secretary of State of the United States?.
Irrelevant relations are ubiquitous, and affect ontol-
ogy reliability, if used to populate it, as the relation
drives the wrong type of ontological knowledge.
Erroneous or false relations. These are particu-
larly harmful, since they directly affect algorithm
precision. A pattern-based relation extraction
algorithm is particularly likely to extract erroneous
relations if it uses generic patterns, which are
defined in (Pantel and Pennacchiotti, 2006) as
broad coverage, noisy patterns with high recall and
low precision (e.g. ?X of Y? for part-of relation).
Harvesting algorithms either ignore generic patterns
(Hearst, 1992) (affecting system recall) or use man-
ually supervised filtering approaches (Girju et al,
2006) or use completely unsupervised Web-filtering
methods (Pantel and Pennacchiotti, 2006). Yet,
these methods still do not sufficiently mitigate the
problem of erroneous relations.
Background knowledge. Another aspect that
makes relation harvesting difficult is related to the
131
nature of semantic relations: relations among enti-
ties are mostly paradigmatic (de Saussure, 1922),
and are usually established in absentia (i.e., they are
not made explicit in text). According to Eco?s posi-
tion (Eco, 1979), the background knowledge (e.g.
?persons are humans?) is often assumed by the
writer, and thus is not explicitly mentioned in text.
In some cases, such widely-known relations can be
captured by distributional similarity techniques but
not by pattern-based approaches.
Metaphorical language. Even when paradigmatic
relations are explicitly expressed in texts, it can
be very difficult to distinguish between facts and
metaphoric usage (e.g. the expression ?My mind is
a pearl? occurs 17 times on the Web, but it is clear
that mind is not a pearl, at least from an ontological
perspective).
The considerations above outline some of the dif-
ficulties of taking a purely lexico-syntactic approach
to relation extraction. Pragmatic issues (background
knowledge and metaphorical language) and onto-
logical issues (irrelevant relation) can not be solved
at the syntactic level. Also, erroneous relations can
always arise. These considerations lead us to the
intuition that extraction can benefit from imposing
some additional constraints.
In this paper, we integrate Espresso with a lex-
ical distribution technique modeling semantic co-
herence through semantic domains (Magnini et al,
2002). These are defined as common discourse top-
ics which demonstrate lexical coherence, such as
ECONOMICS or POLITICS. We explore whether se-
mantic domains can provide the needed additional
constraints to mitigate the acceptance of erroneous
relations. At the lexical level, semantic domains
identify clusters of (domain) paradigmatically re-
lated terms. We believe that the main advantage of
adopting semantic domains in relation extraction is
that relations are established mainly among terms in
the same Domain, while concepts belonging to dif-
ferent fields are mostly unrelated (Gliozzo, 2005),
as described in Section 2. For example, in a chem-
istry domain, an is-a will tend to relate only terms of
that domain (e.g., nitrogen is-a element), while out-
of-domain relations are likely to be erroneous e.g.,
driver is-a element.
By integrating pattern-based and distributional ap-
proaches we aim to capture the two characteristic
properties of semantic relations:
? Syntagmatic properties: if two terms X and
Y are in a given relation, they tend to co-
occur in texts, and are mostly connected by spe-
cific lexical-syntactic patterns (e.g., the patter
?X is a Y ? connects terms in is-a relations).
This aspect is captured using a pattern-based
approach.
? Domain properties: if a semantic relation
among two terms X and Y holds, both X
and Y should belong to the same semantic
domain (i.e. they are semantically coherent),
where semantic domains are sets of terms
characterized by very similar distributional
properties in a (possibly domain specific)
corpus.
In Section 2, we develop the concept of semantic do-
main and an automatic acquisition procedure based
on Latent Semantic Analysis (LSA) and we provide
empirical evidence of the connection between rela-
tion extraction and domain modelling. Section 3 de-
scribes the Espresso system. Section 4 concerns our
integration of semantic domains and Espresso. In
Section 5, we evaluate the impact of our LSA do-
main restriction module on improving a state of the
art relation extraction system. In Section 6 we draw
some interesting research directions opened by our
work.
2 Semantic Domains
Semantic domains are common areas of human
discussion, which demonstrate lexical coherence,
such as ECONOMICS, POLITICS, LAW, SCIENCE,
(Magnini et al, 2002). At the lexical level, se-
mantic domains identify clusters of (domain) related
lexical-concepts, i.e. sets of highly paradigmatically
related words also known as Semantic Fields.
In the literature, semantic domains have been
inferred from corpora by adopting term clustering
methodologies (Gliozzo, 2005), and have been used
for several NLP tasks, such as Text Categorization
and Ontology Learning (Gliozzo, 2006).
Semantic domains can be described by Domain
Models (DMs) (Gliozzo, 2005). A DM is a com-
132
putational model for semantic domains, that repre-
sents domain information at the term level, by defin-
ing a set of term clusters. Each cluster represents a
Semantic Domain, i.e. a set of terms that often co-
occur in texts having similar topics. A DM is repre-
sented by a k ? k? rectangular matrix D, containing
the domain relevance for each term with respect to
each domain, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of a Domain Model
DMs can be acquired from texts in a completely
unsupervised way by exploiting a lexical coherence
assumption. To this end, term clustering algorithms
can be used with each cluster representing a Se-
mantic Domain. The degree of association among
terms and clusters, estimated by the learning algo-
rithm, provides a domain relevance function. For
our experiments we adopted a clustering strategy
based on LSA (Deerwester et al, 1990), following
the methodology described in (Gliozzo, 2005). The
input of the LSA process is a term-by-document ma-
trix T reporting the term frequencies in the whole
corpus for each term. The matrix is decomposed by
means of a Singular Value Decomposition (SVD),
identifying the principal components of T. This op-
eration is done off-line, and can be efficiently per-
formed on large corpora. SVD decomposes T into
three matrixes T ' V?k?UT where ?k? is the di-
agonal k ? k matrix containing the highest k? ? k
eigenvalues of T on the diagonal, and all the re-
maining elements are 0. The parameter k? is the
dimensionality of the domain and can be fixed in
advance1. Under this setting we define the domain
matrix DLSA2 as
DLSA = INV
?
?k? (1)
where IN is a diagonal matrix such that iNi,i =
1
q
? ~w?i, ~w?i?
and ~w?i is the ith row of the matrix V
??k? .
1It is not clear how to choose the right dimensionality. In
our experiments we used 100 dimensions.
2Details of this operation can be found in (Gliozzo, 2005).
Once a DM has been defined by the matrix D, the
Domain Space is a k? dimensional space, in which
both texts and terms are associated to Domain Vec-
tors (DVs), i.e. vectors representing their domain
relevancies with respect to each domain. The DV
~t?i for the term ti ? V is the ith row of D, where
V = {t1, t2, . . . , tk} is the vocabulary of the corpus.
The domain similarity ?d(ti, tj) among terms is then
estimated by the cosine among their corresponding
DVs in the Domain Space, defined as follows:
?d(ti, tj) = ?
~ti, ~tj??
?~ti, ~ti??~tj , ~tj?
(2)
Figure 1: Probability of finding paradigmatic rela-
tions
The main advantage of adopting semantic do-
mains for relation extraction is that they allow us to
impose a domain restriction on the set of candidate
pairs of related terms. In fact, semantic relations can
be established mainly among terms in the same Se-
mantic Domain, while concepts belonging to differ-
ent fields are mostly unrelated.
To show the validity of the domain restriction we
conducted a preliminary experiment, contrasting the
probability for two words to be related in Word-
Net (Magnini and Cavaglia`, 2000) with their domain
similarity, measured in the Domain Space induced
from the British National Corpus. In particular, for
each couple of words, we estimated the domain sim-
ilarity, and we collected word pairs in sets charac-
terized by different ranges of similarity (e.g. all the
pairs between 0.8 and 0.9). Then we estimated the
133
probability of each couple of words in different sets
to be linked by a semantic relation in WordNet, such
as synonymy, hyperonymy, co-hyponymy and do-
main in WordNet Domains (Magnini et al, 2002).
Results in Figure 1 show a monotonic crescent rela-
tion between these two quantities. In particular the
probability for two words to be related tends to 0
when their similarity is negative (i.e., they are not
domain related), supporting the basic hypothesis of
this work. In Section 4 we will show that this prop-
erty can be used to improve the overall performances
of the relation extraction algorithm.
3 The pattern-based Espresso system
Espresso (Pantel and Pennacchiotti, 2006) is a
corpus-based general purpose, broad, and accurate
relation extraction algorithm requiring minimal su-
pervision, whose core is based on the framework
adopted in (Hearst, 1992). Espresso introduces two
main innovations that guarantee high performance:
(i) a principled measure for estimating the reliabil-
ity of relational patterns and instances; (ii) an algo-
rithm for exploiting generic patterns. Generic pat-
terns are broad coverage noisy patterns (high recall
and low precision), e.g. ?X of Y? for the part-of re-
lation. As underlined in the introduction, previous
algorithms either required significant manual work
to make use of generic patterns, or simply ignore
them. Espresso exploits an unsupervised Web-based
filtering method to detect generic patterns and to dis-
tinguish their correct and incorrect instances.
Given a specific relation (e.g. is-a) and a POS-
tagged corpus, Espresso takes as input few seed
instances (e.g. nitrogen is-a element) or seed surface
patterns (e.g. X/NN such/JJ as/IN Y/NN). It then
incrementally learns new patterns and instances
by iterating on the following three phases, until a
specific stop condition is met (i.e., new patterns are
below a pre-defined threshold of reliability).
Pattern Induction. Given an input set of seed
instances I , Espresso infers new patterns connecting
as many instances as possible in the given corpus.
To do so, Espresso uses a slight modification of the
state of the art algorithm described in (Ravichandran
and Hovy, 2002). For each instance in input, the
sentences containing it are first retrieved and then
generalized, by replacing term expressions with a
terminological label using regular expressions on
the POS-tags. This generalization allows to ease
the problem of data sparseness in small corpora.
Unfortunately, as patterns become more generic,
they are more prone to low precision.
Pattern Ranking and Selection. Espresso ranks
all extracted patterns using a reliability measure rpi
and discards all but the top-k P patterns, where k is
set to the number of patterns from the previous iter-
ation plus one. rpi captures the intuition that a reli-
able pattern is one that is both highly precise and one
that extracts many instances. rpi is formally defined
as the average strength of association between a pat-
tern p and each input instance i in I , weighted by the
reliability r? of the instance i (described later):
rpi(p) =
?
i?I
(
pmi(i,p)
maxpmi ? r?(i)
)
|I|
where pmi(i, p) is the pointwise mutual information
(pmi) between i and p (estimated with Maximum
Likelihood Estimation), and maxpmi is the maxi-
mum pmi between all patterns and all instances.
Instance Extraction, Ranking, Selection.
Espresso extracts from the corpus the set of in-
stances I matching the patterns in P . In this phase
generic patterns are detected, and their instances
are filtered, using a technique described in detail in
(Pantel and Pennacchiotti, 2006). Instances are then
ranked using a reliability measure r?, similar to that
adopted for patterns. A reliable instance should be
highly associated with as many reliable patterns as
possible:
r?(i) =
?
p?P
(
pmi(i,p)
maxpmi ? rpi(i)
)
|P |
Finally, the best scoring instances are selected for
the following iteration. If the number of extracted
instances is too low (as often happens in small
corpora) Espresso enters an expansion phase, in
which instances are expanded by using web based
and syntactic techniques.
134
The output Espresso is a list of instances
i = (X,Y ) ? I , ranked according to r?(i). This
score accounts for the syntagmatic similarity be-
tween X and Y , i.e., how strong is the co-occurrence
of X and Y in texts with a given pattern p.
A key role in the Espresso algorithm is played
by the reliability measures. The accuracy of the
whole extraction process is in fact highly sensitive
to the ranking of patterns and instances because, at
each iteration, only the best scoring entities are re-
tained. For instance, if an erroneous instance is se-
lected after the first iteration, it could in theory af-
fect the following pattern extraction phase and cause
drift in consequent iterations. This issue is criti-
cal for generic patterns (where precision is still a
problem, even with Web-based filtering), and could
sometimes also affect non-generic patterns.
It would be then useful to integrate Espresso with
a technique able to retain only very precise in-
stances, without compromising recall. As syntag-
matic strategies are already in place, another strategy
is needed. In the next Section, we show how this can
be achieved using instance domain information.
4 Integrating syntagmatic and domain
information
The strategy of integrating syntagmatic and do-
main information has demonstrated to be fruitful in
many NLP tasks, such as Word Sense Disambigua-
tion and open domain Ontology Learning (Gliozzo,
2006). According to the structural view (de Saus-
sure, 1922), both aspects contribute to determine
the linguistic value (i.e. the meaning) of words:
the meaning of lexical constituents is determined
by a complex network of semantic relations among
words. This suggests that relation extraction can
benefit from accounting for both syntagmatic and
domain aspects at the same time.
To demonstrate the validity of this claim we can
explore many different integration schemata. For ex-
ample we can restrict the search space (i.e. the set of
candidate instances) to the set of all those terms be-
longing to the same domain. Another possibility is
to exploit a similarity metric for domain relatedness
to re-rank the output instances I of Espresso, hoping
that the top ranked ones will mostly be those which
are correct. One advantage of this latter method-
ology is that it can be applied to the output of any
relation extraction system without any modification
to the system itself. In addition, this methodology
can be evaluated by adopting standard Information
Retrieval (IR) measures, such as mean average pre-
cision (see Section 5). Because of these advantages,
we decided to adopt the re-ranking procedure.
The procedure is defined as follows: each in-
stance extracted by Espresso is assigned a Domain
Similarity score ?d(X,Y ) estimated in the domain
space according to Equation 2; a higher score is
then assigned to the instances that tend to co-occur
in the same documents in the corpus. For exam-
ple, the candidate instances ethanol is-a nonaro-
matic alcohol has a higher score than ethanol is-a
something, as ethanol and alcohol are both from the
chemistry domain, while something is a generic term
and is thus not associated to any domain.
Instances are then re-ranked according to
?d(X,Y ), which is used as the new index of
reliability instead of the original reliability scores
of Espresso. In Subsection 5.2 we will show that
the re-ranking technique improves the original
reliability scores of Espresso.
5 Evaluation
In this Section we evaluate the benefits of applying
the domain information to relation extraction (ESP-
LSA), by measuring the improvements of Espresso
due to domain based re-ranking.
5.1 Experimental Settings
As a baseline system, we used the ESP- implemen-
tation of Espresso described in (Pantel and Pennac-
chiotti, 2006). ESP- is a fully functioning Espresso
system, without the generic pattern filtering module
(ESP+). We decided to use ESP- for two main rea-
sons. First, the manual evaluation process would
have been too time consuming, as ESP+ extracts
thousands of relations. Also, the small scale experi-
ment for EXP- allows us to better analyse and com-
pare the results.
To perform the re-ranking operation, we acquired
a Domain Model from the input corpus itself. To this
aim we performed a SVD of the term by document
matrix T describing the input corpus, indexing all
the candidate terms recognized by Espresso.
135
As an evaluation benchmark, we adopted the
same instance sets extracted by ESP- in the ex-
periment described in (Pantel and Pennacchiotti,
2006). We used an input corpus of 313,590 words,
a college chemistry textbook (Brown et al 2003),
pre-processed using the Alembic Workbench POS-
tagger (Day et al 1997). We considered the fol-
lowing relations: is-a, part-of, reaction (a relation
of chemical reaction among chemical entities) and
production (a process or chemical element/object
producing a result). ESP- extracted 200 is-a, 111
part-of, 40 reaction and 196 production instances.
5.2 Quantitative Analysis
The experimental evaluation compared the accuracy
of the ranked set of instances extracted by ESP- with
the re-ranking produced on these instances by ESP-
LSA. By analogy to IR, we are interested in ex-
tracting positive instances (i.e. semantically related
words). Accordingly, we utilize the standard defi-
nitions of precision and recall typically used in IR .
Table 2 reports the Mean Average Precision obtained
by both ESP- and ESP-LSA on the extracted rela-
tions, showing the substantial improvements on all
the relations due to domain based re-ranking.
ESP- ESP-LSA
is-a 0.54 0.75 (+0.21)
part-of 0.65 0.82 (+0.17)
react 0.75 0.82 (+0.07)
produce 0.55 0.62 (+0.07)
Table 2: Mean Average Precision reported by ESP-
and ESP-LSA
Figures 2, 3, 4 and 5 report the precision/recall
curves obtained for each relation, estimated by mea-
suring the precision / recall at each point of the
ranked list. Results show that precision is very high
especially for the top ranked relations extracted by
ESP-LSA. Precision reaches the upper bound for the
top ranked part of the part-of relation, while it is
close to 0.9 for the is-a relation. In all cases, the
precision reported by the ESP-LSA system surpass
those of the ESP- system at all recall points.
5.3 Qualitative Analysis
Table 3 shows the best scoring instances for ESP-
and ESP-LSA on the evaluated relations. Results
Figure 2: Syntagmatic vs. Domain ranking for the
is-a relation
Figure 3: Syntagmatic vs. Domain ranking for the
produce relation
show that ESP-LSA tends to assign a much lower
score to erroneous instances, as compared to the
original Espresso reliability ranking. For exam-
ple for the part-of relation, the ESP- ranks the er-
roneous instance geometry part-of ion in 23th po-
sition, while ESP-LSA re-ranks it in 92nd. In
this case, a lower score is assigned because ge-
ometry is not particularly tied to the domain of
chemistry. Also, ESP-LSA tends to penalize in-
stances derived from parsing/tokenization errors:
136
Figure 4: Syntagmatic vs. Domain ranking for the
part-of relation
Figure 5: Syntagmatic vs. Domain ranking for the
react relation
] binary hydrogen compounds hydrogen react ele-
ments is 16th for ESP-, while in the last tenth of
the ESP-LSA. In addition, out-of-domain relations
are successfully interpreted by ESP-LSA. For ex-
ample, the instance sentences part-of exceptions is
a possibly correct relation, but unrelated to the do-
main, as an exception in chemistry has nothing to
do with sentences. This instance lies at the bottom
of the ESP-LSA ranking, while is in the middle of
ESP- list. Also, low ranked and correct relations ex-
tracted by ESP- emerge with ESP-LSA. For exam-
ple, magnesium metal react elemental oxygen lies at
the end of ESP- rank, as there are not enough syntag-
matic evidence (co-occurrences) that let the instance
emerge. The domain analysis of ESP-LSA promotes
this instance to the 2nd rank position. However, in
few cases, the strategy adopted by ESP-LSA tends
to promote erroneous instances (e.g. high voltage
produce voltage). Yet, results show that these are
isolated cases.
6 Conclusion and future work
In this paper, we propose the domain restriction hy-
pothesis, claiming that semantically related terms
extracted from a corpus tend to be semantically co-
herent. Applying this hypothesis, we presented a
new method to improve the precision of pattern-
based relation extraction algorithms, where the inte-
gration of domain information allows the system to
filter out many irrelevant relations, erroneous can-
didate pairs and metaphorical language relational
expressions, while capturing the assumed knowl-
edge required to discover paradigmatic associations
among terms. Experimental evidences supports this
claim both qualitatively and quantitatively, opening
a promising research direction, that we plan to ex-
plore much more in depth. In the future, we plan
to compare LSA to other term similarity measures,
to train the LSA model on large open domain cor-
pora and to apply our technique to both generic and
specific corpora in different domains. We want also
to increase the level of integration of the LSA tech-
nique in the Espresso algorithm, by using LSA as an
alternative reliability measure at each iteration. We
will also explore the domain restriction property of
semantic domains to develop open domain ontology
learning systems, as proposed in (Gliozzo, 2006).
The domain restriction hypothesis has potential
to greatly impact many applications where match-
ing textual expressions is a primary component. It is
our hope that by combining existing ranking strate-
gies in applications such as information retrieval,
question answering, information extraction and doc-
ument classification, with knowledge of the coher-
ence of the underlying text, one will see significant
improvements in matching accuracy.
137
Relation ESP- ESP - LSA
X is-a Y Aluminum ; metal F ; electronegative atoms
nitride ion ; strong Br O ; electronegative atoms
heat flow ; calorimeter NaCN ; cyanide salt
complete ionic equation ; spectator NaCN ; cyanide salts
X part-of Y elements ; compound amino acid building blocks ; tripeptide
composition ; substance acid building blocks ; tripeptide
blocks ; tripeptide powdered zinc metal ; battery
elements ; sodium chloride building blocks ; tripeptide
X react Y hydrazine ; water magnesium metal ; elemental oxygen
magnesium metal ; hydrochloric acid nitrogen ; ammonia
magnesium ; oxygen sodium metal ; chloride
magnesium metal ; acid carbon dioxide ; methane
X produce Y bromine ; bromide high voltage ; voltage
oxygen ; oxide reactions ; reactions
common fuels ; dioxide dr jekyll ; hyde
kidneys ; stones yellow pigments ; green pigment
Table 3: Top scoring relations extracted by ESP- and ESP-LSA.
Acknowledgments
Thanks to Roberto Basili for his precious comments,
suggestions and support. Alfio Gliozzo was sup-
ported by the OntoText project, funded by the Au-
tonomous Province of Trento under the FUP-2004,
and the FIRB founded project N.RBIN045PXH.
References
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. On-
tology learning from texts: methods, evaluation and
applications. IOS Press.
F. de Saussure. 1922. Cours de linguistique ge?ne?rale.
Payot, Paris.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
U. Eco. 1979. Lector in fabula. Bompiani.
O. Etzioni, M.J. Cafarella, D. Downey, A.-M
A.M. Popescu, T. Shaked, S. Soderland, D.S.
Weld, and A. Yates. 2002. Unsupervised named-
entity extraction from the web: An experimental
study. Artificial Intelligence, 165(1):91?143.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In Proceedings of HLT/NAACL-
03, pages 80?87, Edmonton, Canada, July.
A. Gliozzo. 2005. Semantic Domains in Computational
Linguistics. Ph.D. thesis, University of Trento.
A. Gliozzo. 2006. The god model. In Proceedings of
EACL.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics.
Nantes, France.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, pages 1413?1418, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of ACM Conference on
Knowledge Discovery and Data Mining, pages 613?
619.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In ACL-COLING-06, pages 113?
120, Sydney, Australia.
M. Pasca and S. Harabagiu. 2001. The informative role
of wordnet in open-domain question answering. In
Proceedings of NAACL-01 Workshop on WordNet and
Other Lexical Resources, pages 138?143, Pittsburgh,
PA.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of ACL-02, pages 41?47, Philadelphia, PA.
R. Snow, D. Jurafsky, and A.Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proceedings of the ACL/COLING-06, pages 801?808,
Sydney, Australia.
138
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 113?120,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Espresso: Leveraging Generic Patterns for  
Automatically Harvesting Semantic Relations 
 
Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
pantel@isi.edu 
Marco Pennacchiotti 
ART Group - DISP 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
  
Abstract 
In this paper, we present Espresso, a 
weakly-supervised, general-purpose, 
and accurate algorithm for harvesting 
semantic relations. The main contribu-
tions are: i) a method for exploiting ge-
neric patterns by filtering incorrect 
instances using the Web; and ii) a prin-
cipled measure of pattern and instance 
reliability enabling the filtering algo-
rithm. We present an empirical com-
parison of Espresso with various state of 
the art systems, on different size and 
genre corpora, on extracting various 
general and specific relations. Experi-
mental results show that our exploita-
tion of generic patterns substantially 
increases system recall with small effect 
on overall precision. 
1 Introduction 
Recent attention to knowledge-rich problems 
such as question answering (Pasca and Harabagiu 
2001) and textual entailment (Geffet and Dagan 
2005) has encouraged natural language process-
ing researchers to develop algorithms for auto-
matically harvesting shallow semantic resources. 
With seemingly endless amounts of textual data 
at our disposal, we have a tremendous opportu-
nity to automatically grow semantic term banks 
and ontological resources. 
To date, researchers have harvested, with 
varying success, several resources, including 
concept lists (Lin and Pantel 2002), topic signa-
tures (Lin and Hovy 2000), facts (Etzioni et al 
2005), and word similarity lists (Hindle 1990). 
Many recent efforts have also focused on extract-
ing semantic relations between entities, such as 
entailments (Szpektor et al 2004), is-a (Ravi-
chandran and Hovy 2002), part-of (Girju et al 
2006), and other relations. 
The following desiderata outline the properties 
of an ideal relation harvesting algorithm: 
? Performance: it must generate both high preci-
sion and high recall relation instances; 
? Minimal supervision: it must require little or no 
human annotation; 
? Breadth: it must be applicable to varying cor-
pus sizes and domains; and 
? Generality: it must be applicable to a wide va-
riety of relations (i.e., not just is-a or part-of). 
To our knowledge, no previous harvesting algo-
rithm addresses all these properties concurrently. 
In this paper, we present Espresso, a general-
purpose, broad, and accurate corpus harvesting 
algorithm requiring minimal supervision. The 
main algorithmic contribution is a novel method 
for exploiting generic patterns, which are broad 
coverage noisy patterns ? i.e., patterns with high 
recall and low precision. Insofar, difficulties in 
using these patterns have been a major impedi-
ment for minimally supervised algorithms result-
ing in either very low precision or recall. We 
propose a method to automatically detect generic 
patterns and to separate their correct and incor-
rect instances. The key intuition behind the algo-
rithm is that given a set of reliable (high 
precision) patterns on a corpus, correct instances 
of a generic pattern will fire more with reliable 
patterns on a very large corpus, like the Web, 
than incorrect ones. Below is a summary of the 
main contributions of this paper: 
? Algorithm for exploiting generic patterns: 
Unlike previous algorithms that require signifi-
cant manual work to make use of generic pat-
terns, we propose an unsupervised Web-
filtering method for using generic patterns; and 
? Principled reliability measure: We propose a 
new measure of pattern and instance reliability 
which enables the use of generic patterns. 
113
Espresso addresses the desiderata as follows: 
? Performance: Espresso generates balanced 
precision and recall relation instances by ex-
ploiting generic patterns; 
? Minimal supervision: Espresso requires as in-
put only a small number of seed instances; 
? Breadth: Espresso works on both small and 
large corpora ? it uses Web and syntactic ex-
pansions to compensate for lacks of redun-
dancy in small corpora; 
? Generality: Espresso is amenable to a wide 
variety of binary relations, from classical is-a 
and part-of to specific ones such as reaction 
and succession. 
Previous work like (Girju et al 2006) that has 
made use of generic patterns through filtering has 
shown both high precision and high recall, at the 
expensive cost of much manual semantic annota-
tion. Minimally supervised algorithms, like 
(Hearst 1992; Pantel et al 2004), typically ignore 
generic patterns since system precision dramati-
cally decreases from the introduced noise and 
bootstrapping quickly spins out of control. 
2 Relevant Work 
To date, most research on relation harvesting has 
focused on is-a and part-of. Approaches fall into 
two categories: pattern- and clustering-based. 
Most common are pattern-based approaches. 
Hearst (1992) pioneered using patterns to extract 
hyponym (is-a) relations. Manually building 
three lexico-syntactic patterns, Hearst sketched a 
bootstrapping algorithm to learn more patterns 
from instances, which has served as the model 
for most subsequent pattern-based algorithms. 
Berland and Charniak (1999) proposed a sys-
tem for part-of relation extraction, based on the 
(Hearst 1992) approach. Seed instances are used 
to infer linguistic patterns that are used to extract 
new instances. While this study introduces statis-
tical measures to evaluate instance quality, it re-
mains vulnerable to data sparseness and has the 
limitation of considering only one-word terms. 
Improving upon (Berland and Charniak 1999), 
Girju et al (2006) employ machine learning al-
gorithms and WordNet (Fellbaum 1998) to dis-
ambiguate part-of generic patterns like ?X?s Y? 
and ?X of Y?. This study is the first extensive at-
tempt to make use of generic patterns. In order to 
discard incorrect instances, they learn WordNet-
based selectional restrictions, like ?X(scene#4)?s 
Y(movie#1)?. While making huge grounds on 
improving precision/recall, heavy supervision is 
required through manual semantic annotations. 
Ravichandran and Hovy (2002) focus on scal-
ing relation extraction to the Web. A simple and 
effective algorithm is proposed to infer surface 
patterns from a small set of instance seeds by 
extracting substrings relating seeds in corpus sen-
tences. The approach gives good results on spe-
cific relations such as birthdates, however it has 
low precision on generic ones like is-a and part-
of. Pantel et al (2004) proposed a similar, highly 
scalable approach, based on an edit-distance 
technique, to learn lexico-POS patterns, showing 
both good performance and efficiency. Espresso 
uses a similar approach to infer patterns, but we 
make use of generic patterns and apply refining 
techniques to deal with wide variety of relations. 
Other pattern-based algorithms include (Riloff 
and Shepherd 1997), who used a semi-automatic 
method for discovering similar words using a 
few seed examples, KnowItAll (Etzioni et al 
2005) that performs large-scale extraction of 
facts from the Web, Mann (2002) who used part 
of speech patterns to extract a subset of is-a rela-
tions involving proper nouns, and (Downey et al 
2005) who formalized the problem of relation 
extraction in a coherent and effective combinato-
rial model that is shown to outperform previous 
probabilistic frameworks. 
Clustering approaches have so far been ap-
plied only to is-a extraction. These methods use 
clustering algorithms to group words according 
to their meanings in text, label the clusters using 
its members? lexical or syntactic dependencies, 
and then extract an is-a relation between each 
cluster member and the cluster label. Caraballo 
(1999) proposed the first attempt, which used 
conjunction and apposition features to build noun 
clusters. Recently, Pantel and Ravichandran 
(2004) extended this approach by making use of 
all syntactic dependency features for each noun. 
The advantage of clustering approaches is that 
they permit algorithms to identify is-a relations 
that do not explicitly appear in text, however 
they generally fail to produce coherent clusters 
from fewer than 100 million words; hence they 
are unreliable for small corpora. 
3 The Espresso Algorithm 
Espresso is based on the framework adopted in 
(Hearst 1992). It is a minimally supervised boot-
strapping algorithm that takes as input a few seed 
instances of a particular relation and iteratively 
learns surface patterns to extract more instances. 
The key to Espresso lies in its use of generic pat-
ters, i.e., those broad coverage noisy patterns that 
114
extract both many correct and incorrect relation 
instances. For example, for part-of relations, the 
pattern ?X of Y? extracts many correct relation 
instances like ?wheel of the car? but also many 
incorrect ones like ?house of representatives?. 
The key assumption behind Espresso is that in 
very large corpora, like the Web, correct in-
stances generated by a generic pattern will be 
instantiated by some reliable patterns, where 
reliable patterns are patterns that have high preci-
sion but often very low recall (e.g., ?X consists of 
Y? for part-of relations). In this section, we de-
scribe the overall architecture of Espresso, pro-
pose a principled measure of reliability, and give 
an algorithm for exploiting generic patterns. 
3.1 System Architecture 
Espresso iterates between the following three 
phases: pattern induction, pattern rank-
ing/selection, and instance extraction. 
The algorithm begins with seed instances of a 
particular binary relation (e.g., is-a) and then it-
erates through the phases until it extracts ?1 pat-
terns or the average pattern score decreases by 
more than ?2 from the previous iteration. In our 
experiments, we set ?1 = 5 and ?2 = 50%. 
For our tokenization, in order to harvest multi-
word terms as relation instances, we adopt a 
slightly modified version of the term definition 
given in (Justeson 1995), as it is one of the most 
commonly used in the NLP literature: 
 ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun 
Pattern Induction 
In the pattern induction phase, Espresso infers a 
set of surface patterns P that connects as many of 
the seed instances as possible in a given corpus. 
Any pattern learning algorithm would do. We 
chose the state of the art algorithm described in 
(Ravichandran and Hovy 2002) with the follow-
ing slight modification. For each input instance 
{x, y}, we first retrieve all sentences containing 
the two terms x and y. The sentences are then 
generalized into a set of new sentences Sx,y by 
replacing all terminological expressions by a 
terminological label, TR. For example: 
 ?Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN 
  and/CC x is/VBZ a/DT y? 
is generalized as: 
 ?Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y? 
Term generalization is useful for small corpora to 
ease data sparseness. Generalized patterns are 
naturally less precise, but this is ameliorated by 
our filtering step described in Section 3.3. 
As in the original algorithm, all substrings 
linking terms x and y are then extracted from Sx,y, 
and overall frequencies are computed to form P. 
Pattern Ranking/Selection 
In (Ravichandran and Hovy 2002), a frequency 
threshold on the patterns in P is set to select the 
final patterns. However, low frequency patterns 
may in fact be very good. In this paper, instead of 
frequency, we propose a novel measure of pat-
tern reliability, r?, which is described in detail in 
Section 3.2. 
Espresso ranks all patterns in P according to 
reliability r? and discards all but the top-k, where 
k is set to the number of patterns from the previ-
ous iteration plus one. In general, we expect that 
the set of patterns is formed by those of the pre-
vious iteration plus a new one. Yet, new statisti-
cal evidence can lead the algorithm to discard a 
pattern that was previously discovered. 
Instance Extraction 
In this phase, Espresso retrieves from the corpus 
the set of instances I that match any of the pat-
terns in P. In Section 3.2, we propose a princi-
pled measure of instance reliability, r?, for 
ranking instances. Next, Espresso filters incor-
rect instances using the algorithm proposed in 
Section 3.3 and then selects the highest scoring m 
instances, according to r?, as input for the subse-
quent iteration. We experimentally set m=200. 
In small corpora, the number of extracted in-
stances can be too low to guarantee sufficient 
statistical evidence for the pattern discovery 
phase of the next iteration. In such cases, the sys-
tem enters an expansion phase, where instances 
are expanded as follows: 
Web expansion: New instances of the patterns 
in P are retrieved from the Web, using the 
Google search engine. Specifically, for each in-
stance {x, y}? I, the system creates a set of que-
ries, using each pattern in P instantiated with y. 
For example, given the instance ?Italy, country? 
and the pattern ?Y such as X?, the resulting 
Google query will be ?country such as *?. New 
instances are then created from the retrieved Web 
results (e.g. ?Canada, country?) and added to I. 
The noise generated from this expansion is at-
tenuated by the filtering algorithm described in 
Section 3.3. 
Syntactic expansion: New instances are cre-
ated from each instance {x, y}? I by extracting 
sub-terminological expressions from x corre-
sponding to the syntactic head of terms. For ex-
115
ample, the relation ?new record of a criminal 
conviction part-of FBI report? expands to: ?new 
record part-of FBI report?, and ?record part-of 
FBI report?. 
3.2 Pattern and Instance Reliability 
Intuitively, a reliable pattern is one that is both 
highly precise and one that extracts many in-
stances. The recall of a pattern p can be approxi-
mated by the fraction of input instances that are 
extracted by p. Since it is non-trivial to estimate 
automatically the precision of a pattern, we are 
wary of keeping patterns that generate many in-
stances (i.e., patterns that generate high recall but 
potentially disastrous precision). Hence, we de-
sire patterns that are highly associated with the 
input instances. Pointwise mutual information 
(Cover and Thomas 1991) is a commonly used 
metric for measuring this strength of association 
between two events x and y: 
 
( ) ( )( ) ( )yPxP
yxP
yxpmi
,
log, =
 
We define the reliability of a pattern p, r?(p), 
as its average strength of association across each 
input instance i in I, weighted by the reliability of 
each instance i: 
 ( )
( )
I
ir
pipmi
pr
Ii pmi
?
? ?
?
?
?
???
? ?
=
?
?
max
),(
 
where r?(i) is the reliability of instance i (defined 
below) and maxpmi is the maximum pointwise 
mutual information between all patterns and all 
instances. r?(p) ranges from [0,1]. The reliability 
of the manually supplied seed instances are r?(i) 
= 1. The pointwise mutual information between 
instance i = {x, y} and pattern p is estimated us-
ing the following formula: 
 ( )
,**,,*,
,,
log,
pyx
ypx
pipmi =  
where |x, p, y| is the frequency of pattern p in-
stantiated with terms x and y and where the aster-
isk (*) represents a wildcard. A well-known 
problem is that pointwise mutual information is 
biased towards infrequent events. We thus multi-
ply pmi(i, p) with the discounting factor sug-
gested in (Pantel and Ravichandran 2004). 
Estimating the reliability of an instance is 
similar to estimating the reliability of a pattern. 
Intuitively, a reliable instance is one that is 
highly associated with as many reliable patterns 
as possible (i.e., we have more confidence in an 
instance when multiple reliable patterns instanti-
ate it.) Hence, analogous to our pattern reliability 
measure, we define the reliability of an instance 
i, r?(i), as: 
 ( )
( )
P
pr
pipmi
ir Pp pmi
?
??
?
=
?
?
max
),(
 
where r?(p) is the reliability of pattern p (defined 
earlier) and maxpmi is as before. Note that r?(i) 
and r?(p) are recursively defined, where r?(i) = 1 
for the manually supplied seed instances. 
3.3 Exploiting Generic Patterns 
Generic patterns are high recall / low precision 
patterns (e.g, the pattern ?X of Y? can ambigu-
ously refer to a part-of, is-a and possession rela-
tions). Using them blindly increases system 
recall while dramatically reducing precision. 
Minimally supervised algorithms have typically 
ignored them for this reason. Only heavily super-
vised approaches, like (Girju et al 2006) have 
successfully exploited them. 
Espresso?s recall can be significantly in-
creased by automatically separating correct in-
stances extracted by generic patterns from 
incorrect ones. The challenge is to harness the 
expressive power of the generic patterns while 
remaining minimally supervised. 
The intuition behind our method is that in a 
very large corpus, like the Web, correct instances 
of a generic pattern will be instantiated by many 
of Espresso?s reliable patterns accepted in P. Re-
call that, by definition, Espresso?s reliable pat-
terns extract instances with high precision (yet 
often low recall). In a very large corpus, like the 
Web, we assume that a correct instance will oc-
cur in at least one of Espresso?s reliable pattern 
even though the patterns? recall is low. Intui-
tively, our confidence in a correct instance in-
creases when, i) the instance is associated with 
many reliable patterns; and ii) its association 
with the reliable patterns is high. At a given Es-
presso iteration, where PR represents the set of 
previously selected reliable patterns, this intui-
tion is captured by the following measure of con-
fidence in an instance i = {x, y}: 
 ( ) ( ) ( )?
?
?=
RPp
p T
pr
iSiS ?   
where T is the sum of the reliability scores r?(p) 
for each pattern p ? PR, and 
 ( ) ( )
,**,,*,
,,
log,
pyx
ypx
pipmiiS p ?==
  
116
where pointwise mutual information between 
instance i and pattern p is estimated with Google 
as follows: 
 ( )
pyx
ypx
iS p ???
,,   
An instance i is rejected if S(i) is smaller than 
some threshold ?. 
Although this filtering may also be applied to 
reliable patterns, we found this to be detrimental 
in our experiments since most instances gener-
ated by reliable patterns are correct. In Espresso, 
we classify a pattern as generic when it generates 
more than 10 times the instances of previously 
accepted reliable patterns. 
4 Experimental Results 
In this section, we present an empirical compari-
son of Espresso with three state of the art sys-
tems on the task of extracting various semantic 
relations. 
4.1 Experimental Setup 
We perform our experiments using the following 
two datasets: 
? TREC: This dataset consists of a sample of 
articles from the Aquaint (TREC-9) newswire 
text collection. The sample consists of 
5,951,432 words extracted from the following 
data files: AP890101 ? AP890131, AP890201 
? AP890228, and AP890310 ? AP890319. 
? CHEM: This small dataset of 313,590 words 
consists of a college level textbook of introduc-
tory chemistry (Brown et al 2003). 
Each corpus is pre-processed using the Alembic 
Workbench POS-tagger (Day et al 1997). 
Below we describe the systems used in our 
empirical evaluation of Espresso. 
? RH02: The algorithm by Ravichandran and 
Hovy (2002) described in Section 2. 
? GI03: The algorithm by Girju et al (2006) de-
scribed in Section 2. 
? PR04: The algorithm by Pantel and Ravi-
chandran (2004) described in Section 2. 
? ESP-: The Espresso algorithm using the pat-
tern and instance reliability measures, but 
without using generic patterns. 
? ESP+: The full Espresso algorithm described 
in this paper exploiting generic patterns. 
For ESP+, we experimentally set ? from Section 
3.3 to ? = 0.4 for TREC and ? = 0.3 for CHEM 
by manually inspecting a small set of instances. 
Espresso is designed to extract various seman-
tic relations exemplified by a given small set of 
seed instances. We consider the standard is-a and 
part-of relations as well as the following more 
specific relations: 
? succession: This relation indicates that a person 
succeeds another in a position or title. For ex-
ample, George Bush succeeded Bill Clinton 
and Pope Benedict XVI succeeded Pope John 
Paul II. We evaluate this relation on the 
TREC-9 corpus. 
? reaction: This relation occurs between chemi-
cal elements/molecules that can be combined 
in a chemical reaction. For example, hydrogen 
gas reacts-with oxygen gas and zinc reacts-with 
hydrochloric acid. We evaluate this relation on 
the CHEM corpus. 
? production: This relation occurs when a proc-
ess or element/object produces a result1. For 
example, ammonia produces nitric oxide. We 
evaluate this relation on the CHEM corpus. 
For each semantic relation, we manually ex-
tracted a small set of seed examples. The seeds 
were used for both Espresso as well as RH02. 
Table 1 lists a sample of the seeds as well as 
sample outputs from Espresso. 
4.2 Precision and Recall 
We implemented the systems outlined in Section 
4.1, except for GI03, and applied them to the 
                                                     
1 Production is an ambiguous relation; it is intended to be 
a causation relation in the context of chemical reactions. 
Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number 
in the parentheses for each relation denotes the total number of seeds used as input for the system. 
 Is-a (12) Part-Of (12) Succession (12) Reaction (13) Production (14) 
Seeds 
wheat :: crop 
George Wendt :: star 
nitrogen :: element 
diborane :: substance 
leader :: panel 
city :: region 
ion :: matter 
oxygen :: water 
Khrushchev :: Stalin 
Carla Hills :: Yeutter 
Bush :: Reagan 
Julio Barbosa :: Mendes 
magnesium :: oxygen 
hydrazine :: water 
aluminum metal :: oxygen 
lithium metal :: fluorine gas 
bright flame :: flares 
hydrogen :: metal hydrides 
ammonia :: nitric oxide 
copper :: brown gas 
Es-
presso 
Picasso :: artist 
tax :: charge 
protein :: biopolymer 
HCl :: strong acid 
trees :: land 
material :: FBI report 
oxygen :: air 
atom :: molecule 
Ford :: Nixon 
Setrakian :: John Griesemer 
Camero Cardiel :: Camacho
Susan Weiss :: editor 
hydrogen :: oxygen 
Ni :: HCl 
carbon dioxide :: methane 
boron :: fluorine 
electron :: ions 
glycerin :: nitroglycerin 
kidneys :: kidney stones 
ions :: charge 
 
117
Table 8. System performance: CHEM/production.
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 197 57.5% 0.80 
ESP- 196 72.5% 1.00 
ESP+ 1676 55.8% 6.58 
 
TREC and CHEM datasets. For each output set, 
per relation, we evaluate the precision of the sys-
tem by extracting a random sample of instances 
(50 for the TREC corpus and 20 for the CHEM 
corpus) and evaluating their quality manually 
using two human judges (a total of 680 instances 
were annotated per judge). For each instance, 
judges may assign a score of 1 for correct, 0 for 
incorrect, and ? for partially correct. Example 
instances that were judged partially correct in-
clude ?analyst is-a manager? and ?pilot is-a 
teacher?. The kappa statistic (Siegel and Castel-
lan Jr. 1988) on this task was ? = 0.692. The pre-
cision for a given set of instances is the sum of 
the judges? scores divided by the total instances. 
Although knowing the total number of correct 
instances of a particular relation in any non-
trivial corpus is impossible, it is possible to com-
pute the recall of a system relative to another sys-
tem?s recall. Following (Pantel et al 2004), we 
define the relative recall of system A given sys-
tem B, RA|B, as: 
 
BP
AP
C
C
R
R
R
B
A
B
A
C
C
C
C
B
A
BA
B
A
?
?====|  
where RA is the recall of A, CA is the number of 
correct instances extracted by A, C is the (un-
known) total number of correct instances in the 
corpus, PA is A?s precision in our experiments, 
                                                     
2 The kappa statistic jumps to ? = 0.79 if we treat partially 
correct classifications as correct. 
and |A| is the total number of instances discov-
ered by A. 
Tables 2 ? 8 report the total number of in-
stances, precision, and relative recall of each sys-
tem on the TREC-9 and CHEM corpora 3 4 . The 
relative recall is always given in relation to the 
ESP- system. For example, in Table 2, RH02 has 
a relative recall of 5.31 with ESP-, which means 
that the RH02 system outputs 5.31 times more 
correct relations than ESP- (at a cost of much 
lower precision). Similarly, PR04 has a relative 
recall of 0.23 with ESP-, which means that PR04 
outputs 4.35 fewer correct relations than ESP- 
(also with a smaller precision). We did not in-
clude the results from GI03 in the tables since the 
system is only applicable to part-of relations and 
we did not reproduce it. However, the authors 
evaluated their system on a sample of the TREC-
9 dataset and reported 83% precision and 72% 
recall (this algorithm is heavily supervised.) 
                                                     
* Because of the small evaluation sets, we estimate the 
95% confidence intervals using bootstrap resampling to be 
in the order of ? 10-15% (absolute numbers). 
? Relative recall is given in relation to ESP-. 
Table 2. System performance: TREC/is-a. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 57,525 28.0% 5.31 
PR04 1,504 47.0% 0.23 
ESP- 4,154 73.0% 1.00 
ESP+ 69,156 36.2% 8.26 
Table 4. System performance: TREC/part-of. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 12,828 35.0% 42.52 
ESP- 132 80.0% 1.00 
ESP+ 87,203 69.9% 577.22 
Table 3. System performance: CHEM/is-a. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 2556 25.0% 3.76 
PR04 108 40.0% 0.25 
ESP- 200 85.0% 1.00 
ESP+ 1490 76.0% 6.66 
Table 5. System performance: CHEM/part-of. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 11,582 33.8% 58.78 
ESP- 111 60.0% 1.00 
ESP+ 5973 50.7% 45.47 
Table 7. System performance: CHEM/reaction. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 6,083 30% 53.67 
ESP- 40 85% 1.00 
ESP+ 3102 91.4% 89.39 
Table 6. System performance: TREC/succession.
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 49,798 2.0% 36.96 
ESP- 55 49.0% 1.00 
ESP+ 55 49.0% 1.00 
118
In all tables, RH02 extracts many more rela-
tions than ESP-, but with a much lower precision, 
because it uses generic patterns without filtering. 
The high precision of ESP- is due to the effective 
reliability measures presented in Section 3.2. 
4.3 Effect of Generic Patterns 
Experimental results, for all relations and the two 
different corpus sizes, show that ESP- greatly 
outperforms the other methods on precision. 
However, without the use of generic patterns, the 
ESP- system shows lower recall in all but the 
production relation. 
As hypothesized, exploiting generic patterns 
using the algorithm from Section 3.3 substan-
tially improves recall without much deterioration 
in precision. ESP+ shows one to two orders of 
magnitude improvement on recall while losing 
on average below 10% precision. The succession 
relation in Table 6 was the only relation where 
Espresso found no generic pattern. For other re-
lations, Espresso found from one to five generic 
patterns. Table 4 shows the power of generic pat-
terns where system recall increases by 577 times 
with only a 10% drop in precision. In Table 7, we 
see a case where the combination of filtering 
with a large increase in retrieved instances re-
sulted in both higher precision and recall. 
In order to better analyze our use of generic 
patterns, we performed the following experiment. 
For each relation, we randomly sampled 100 in-
stances for each generic pattern and built a gold 
standard for them (by manually tagging each in-
stance as correct or incorrect). We then sorted the 
100 instances according to the scoring formula 
S(i) derived in Section 3.3 and computed the av-
erage precision, recall, and F-score of each top-K 
ranked instances for each pattern5. Due to lack of 
space, we only present the graphs for four of the 
22 generic patterns: ?X is a Y? for the is-a rela-
tion of Table 2, ?X in the Y? for the part-of rela-
tion of Table 4, ?X in Y? for the part-of relation 
of Table 5, and ?X and Y? for the reaction rela-
tion of Table 7. Figure 1 illustrates the results. 
In each figure, notice that recall climbs at a 
much faster rate than precision decreases. This 
indicates that the scoring function of Section 3.3 
effectively separates correct and incorrect in-
stances. In Figure 1a), there is a big initial drop 
in precision that accounts for the poor precision 
reported in Table 1. 
Recall that the cutoff points on S(i) were set to 
? = 0.4 for TREC and ? = 0.3 for CHEM. The 
figures show that this cutoff is far from the 
maximum F-score. An interesting avenue of fu-
ture work would be to automatically determine 
the proper threshold for each individual generic 
pattern instead of setting a uniform threshold. 
                                                     
5 We can directly compute recall here since we built a 
gold standard for each set of 100 samples. 
Figure 1. Precision, recall and F-score curves of the Top-K% ranking instances of patterns ?X is a Y? 
(TREC/is-a), ?X in Y? (TREC/part-of), ?X in the Y? (CHEM/part-of), and ?X and Y? (CHEM/reaction). 
a) TREC/is-a: "X is a Y"
0
0.2
0.4
0.6
0.8
1
5 15 25 35 45 55 65 75 85 95
Top-K%
d) CHEM/reaction: "X and Y"
0
0.2
0.4
0.6
0.8
1
5 15 25 35 45 55 65 75 85 95
Top-K%
c) CHEM/part-of: "X in Y"
0
0.2
0.4
0.6
0.8
1
5 15 25 35 45 55 65 75 85 95
Top-K%
b) TREC/part-of: "X in the Y"
0
0.2
0.4
0.6
0.8
1
5 15 25 35 45 55 65 75 85 95
Top-K%
119
5 Conclusions 
We proposed a weakly-supervised, general-
purpose, and accurate algorithm, called Espresso, 
for harvesting binary semantic relations from raw 
text. The main contributions are: i) a method for 
exploiting generic patterns by filtering incorrect 
instances using the Web; and ii) a principled 
measure of pattern and instance reliability ena-
bling the filtering algorithm. 
We have empirically compared Espresso?s 
precision and recall with other systems on both a 
small domain-specific textbook and on a larger 
corpus of general news, and have extracted sev-
eral standard and specific semantic relations: is-
a, part-of, succession, reaction, and production. 
Espresso achieves higher and more balanced per-
formance than other state of the art systems. By 
exploiting generic patterns, system recall sub-
stantially increases with little effect on precision. 
There are many avenues of future work both in 
improving system performance and making use 
of the relations in applications like question an-
swering. For the former, we plan to investigate 
the use of WordNet to automatically learn selec-
tional constraints on generic patterns, as pro-
posed by (Girju et al 2006). We expect here that 
negative instances will play a key role in deter-
mining the selectional restrictions. 
Espresso is the first system, to our knowledge, 
to emphasize concurrently performance, minimal 
supervision, breadth, and generality. It remains 
to be seen whether one could enrich existing on-
tologies with relations harvested by Espresso, 
and it is our hope that these relations will benefit 
NLP applications. 
References 
Berland, M. and E. Charniak, 1999. Finding parts in very 
large corpora. In Proceedings of ACL-1999. pp. 57-64. 
College Park, MD. 
Brown, T.L.; LeMay, H.E.; Bursten, B.E.; and Burdge, J.R. 
2003. Chemistry: The Central Science, Ninth Edition. 
Prentice Hall. 
Caraballo, S. 1999. Automatic acquisition of a hypernym-
labeled noun hierarchy from text. In Proceedings of 
ACL-99. pp 120-126, Baltimore, MD. 
Cover, T.M. and Thomas, J.A. 1991. Elements of 
Information Theory. John Wiley & Sons. 
Day, D.; Aberdeen, J.; Hirschman, L.; Kozierok, R.; 
Robinson, P.; and Vilain, M. 1997. Mixed-initiative 
development of language processing systems. In 
Proceedings of ANLP-97. Washington D.C. 
Downey, D.; Etzioni, O.; and Soderland, S. 2005. A 
Probabilistic model of redundancy in information 
extraction. In Proceedings of IJCAI-05. pp. 1034-1041. 
Edinburgh, Scotland. 
Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-M.; 
Shaked, T.; Soderland, S.; Weld, D.S.; and Yates, A. 
2005. Unsupervised named-entity extraction from the 
Web: An experimental study. Artificial Intelligence, 
165(1): 91-134. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press. 
Geffet, M. and Dagan, I. 2005. The Distributional Inclusion 
Hypotheses and Lexical Entailment. In Proceedings of 
ACL-2005. Ann Arbor, MI. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2006. 
Automatic Discovery of Part-Whole Relations. 
Computational Linguistics, 32(1): 83-135. 
Hearst, M. 1992. Automatic acquisition of hyponyms from 
large text corpora. In Proceedings of COLING-92. pp. 
539-545. Nantes, France. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp. 268?
275. Pittsburgh, PA. 
Justeson J.S. and Katz S.M. 1995. Technical Terminology: 
some linguistic properties and algorithms for 
identification in text. In Proceedings of ICCL-95. 
pp.539-545. Nantes, France. 
Lin, C.-Y. and Hovy, E.H.. 2000. The Automated 
acquisition of topic signatures for text summarization. In 
Proceedings of COLING-00. pp. 495-501. Saarbr?cken, 
Germany. 
Lin, D. and Pantel, P. 2002. Concept discovery from text. In 
Proceedings of COLING-02. pp. 577-583. Taipei, 
Taiwan. 
Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies 
for Question Answering. In Proceedings of SemaNet? 02: 
Building and Using Semantic Networks, Taipei, Taiwan. 
Pantel, P. and Ravichandran, D. 2004. Automatically 
labeling semantic classes. In Proceedings of 
HLT/NAACL-04. pp. 321-328. Boston, MA. 
Pantel, P.; Ravichandran, D.; Hovy, E.H. 2004. Towards 
terascale knowledge acquisition. In Proceedings of 
COLING-04. pp. 771-777. Geneva, Switzerland. 
Pasca, M. and Harabagiu, S. 2001. The informative role of 
WordNet in Open-Domain Question Answering. In 
Proceedings of NAACL-01 Workshop on WordNet and 
Other Lexical Resources. pp. 138-143. Pittsburgh, PA. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface 
text patterns for a question answering system. In 
Proceedings of ACL-2002. pp. 41-47. Philadelphia, PA. 
Riloff, E. and Shepherd, J. 1997. A corpus-based approach 
for building semantic lexicons. In Proceedings of 
EMNLP-97. 
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric 
Statistics for the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. 
Scaling web-based acquisition of entailment relations. In 
Proceedings of EMNLP-04. Barcelona, Spain. 
120
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 793?800,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Ontologizing Semantic Relations 
 
Marco Pennacchiotti 
ART Group - DISP 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA90292 
pantel@isi.edu 
  
Abstract 
Many algorithms have been developed 
to harvest lexical semantic resources, 
however few have linked the mined 
knowledge into formal knowledge re-
positories. In this paper, we propose two 
algorithms for automatically ontologiz-
ing (attaching) semantic relations into 
WordNet. We present an empirical 
evaluation on the task of attaching part-
of and causation relations, showing an 
improvement on F-score over a baseline 
model. 
1 Introduction 
NLP researchers have developed many algo-
rithms for mining knowledge from text and the 
Web, including facts (Etzioni et al 2005), se-
mantic lexicons (Riloff and Shepherd 1997), 
concept lists (Lin and Pantel 2002), and word 
similarity lists (Hindle 1990). Many recent ef-
forts have also focused on extracting binary se-
mantic relations between entities, such as 
entailments (Szpektor et al 2004), is-a (Ravi-
chandran and Hovy 2002), part-of (Girju et al 
2003), and other relations. 
The output of most of these systems is flat lists 
of lexical semantic knowledge such as ?Italy is-a 
country? and ?orange similar-to blue?. However, 
using this knowledge beyond simple keyword 
matching, for example in inferences, requires it 
to be linked into formal semantic repositories 
such as ontologies or term banks like WordNet 
(Fellbaum 1998). 
Pantel (2005) defined the task of ontologizing 
a lexical semantic resource as linking its terms to 
the concepts in a WordNet-like hierarchy. For 
example, ?orange similar-to blue? ontologizes in 
WordNet to ?orange#2 similar-to blue#1? and 
?orange#2 similar-to blue#2?. In his framework, 
Pantel proposed a method of inducing ontologi-
cal co-occurrence vectors 1  which are subse-
quently used to ontologize unknown terms into 
WordNet with 74% accuracy. 
In this paper, we take the next step and explore 
two algorithms for ontologizing binary semantic 
relations into WordNet and we present empirical 
results on the task of attaching part-of and causa-
tion relations. Formally, given an instance  
(x, r, y) of a binary relation r between terms x 
and y, the ontologizing task is to identify the 
WordNet senses of x and y where r holds. For 
example, the instance (proton, PART-OF, element) 
ontologizes into WordNet as (proton#1, PART-OF, 
element#2). 
The first algorithm that we explore, called the 
anchoring approach, was suggested as a promis-
ing avenue of future work in (Pantel 2005). This 
bottom up algorithm is based on the intuition that 
x can be disambiguated by retrieving the set of 
terms that occur in the same relation r with y and 
then finding the senses of x that are most similar 
to this set. The assumption is that terms occur-
ring in the same relation will tend to have similar 
meaning. In this paper, we propose a measure of 
similarity to capture this intuition. 
In contrast to anchoring, our second algorithm, 
called the clustering approach, takes a top-down 
view. Given a relation r, suppose that we are 
given every conceptual instance of r, i.e., in-
stances of r in the upper ontology like (parti-
cles#1, PART-OF, substances#1). An instance  
(x, r, y) can then be ontologized easily by finding 
the senses of x and y that are subsumed by ances-
tors linked by a conceptual instance of r. For ex-
ample, the instance (proton, PART-OF, element) 
ontologizes to (proton#1, PART-OF, element#2) 
since proton#1 is subsumed by particles and 
element#2 is subsumed by substances. The prob-
lem then is to automatically infer the set of con-
                                                     
1 The ontological co-occurrence vector of a concept con-
sists of all lexical co-occurrences with the concept in a 
corpus. 
793
ceptual instances. In this paper, we develop a 
clustering algorithm for generalizing a set of re-
lation instances to conceptual instances by look-
ing up the WordNet hypernymy hierarchy for 
common ancestors, as specific as possible, that 
subsume as many instances as possible. An in-
stance is then attached to its senses that are sub-
sumed by the highest scoring conceptual 
instances. 
2 Relevant Work 
Several researchers have worked on ontologizing 
semantic resources. Most recently, Pantel (2005) 
developed a method to propagate lexical co-
occurrence vectors to WordNet synsets, forming 
ontological co-occurrence vectors. Adopting an 
extension of the distributional hypothesis (Harris 
1985), the co-occurrence vectors are used to 
compute the similarity between synset/synset and 
between lexical term/synset. An unknown term is 
then attached to the WordNet synset whose co-
occurrence vector is most similar to the term?s 
co-occurrence vector. Though the author sug-
gests a method for attaching more complex lexi-
cal structures like binary semantic relations, the 
paper focused only on attaching terms. 
Basili (2000) proposed an unsupervised 
method to infer semantic classes (WordNet syn-
sets) for terms in domain-specific verb relations. 
These relations, such as (x, EXPAND, y) are first 
automatically learnt from a corpus. The semantic 
classes of x and y are then inferred using concep-
tual density (Agirre and Rigau 1996), a Word-
Net-based measure applied to all instantiation of 
x and y in the corpus. Semantic classes represent 
possible common generalizations of the verb ar-
guments. At the end of the process, a set of syn-
tactic-semantic patterns are available for each 
verb, such as: 
(social_group#1, expand, act#2) 
(instrumentality#2, expand, act#2) 
The method is successful on specific relations 
with few instances (such as domain verb rela-
tions) while its value on generic and frequent 
relations, such as part-of, was untested. 
Girju et al (2003) presented a highly super-
vised machine learning algorithm to infer seman-
tic constraints on part-of relations, such as 
(object#1, PART-OF, social_event#1). These con-
straints are then used as selectional restrictions in 
harvesting part-of instances from ambiguous 
lexical patterns, like ?X of Y?. The approach 
shows high performance in terms of precision 
and recall, but, as the authors acknowledge, it 
requires large human effort during the training 
phase. 
Others have also made significant additions to 
WordNet. For example, in eXtended WordNet 
(Harabagiu et al 1999), the glosses in WordNet 
are enriched by disambiguating the nouns, verbs, 
adverbs, and adjectives with synsets. Another 
work has enriched WordNet synsets with topi-
cally related words extracted from the Web 
(Agirre et al 2001). Finally, the general task of 
word sense disambiguation (Gale et al 1991) is 
relevant since there the task is to ontologize each 
term in a passage into a WordNet-like sense in-
ventory. If we had a large collection of sense-
tagged text, then our mining algorithms could 
directly discover WordNet attachment points at 
harvest time. However, since there is little high 
precision sense-tagged corpora, methods are re-
quired to ontologize semantic resources without 
fully disambiguating text. 
3 Ontologizing Semantic Relations 
Given an instance (x, r, y) of a binary relation r 
between terms x and y, the ontologizing task is to 
identify the senses of x and y where r holds. In 
this paper, we focus on WordNet 2.0 senses, 
though any similar term bank would apply. 
Let Sx and Sy be the sets of all WordNet senses 
of x and y. A sense pair, sxy, is defined as any 
pair of senses of x and y: sxy={sx, sy} where sx?Sx 
and sy?Sy. The set of all sense pairs Sxy consists 
of all permutations between senses in Sx and Sy. 
In order to attach a relation instance (x, r, y) 
into WordNet, one must: 
? Disambiguate x and y, that is, find the subsets 
S'x?Sx and S'y?Sy for which the relation r holds; 
and 
? Instantiate the relation in WordNet, using the 
synsets corresponding to all correct permuta-
tions between the senses in S'x and S'y. We de-
note this set of attachment points as S'xy. 
If Sx or Sy is empty, no attachments are produced. 
For example, the instance (study, PART-OF, re-
port) is ontologized into WordNet through the 
senses S'x={survey#1, study#2} and 
S?y={report#1}. The final attachment points S'xy 
are: 
(survey#1, PART-OF, report#1) 
(study#1, PART-OF, report#1) 
Unlike common algorithms for word sense 
disambiguation, here it is important to take into 
consideration the semantic dependency between 
the two terms x and y. For example, an entity that 
is part-of a study has to be some kind of informa-
794
tion. This knowledge about mutual selectional 
preference (the preferred semantic class that fills 
a certain relation role, as x or y) can be exploited 
to ontologize the instance. 
In the following sections, we propose two al-
gorithms for ontologizing binary semantic rela-
tions. 
3.1 Method 1: Anchor Approach 
Given an instance (x, r, y), this approach fixes the 
term y, called the anchor, and then disambiguates 
x by looking at all other terms that occur in the 
relation r with y. Based on the principle of distri-
butional similarity (Harris 1985), the algorithm 
assumes that the words that occur in the same 
relation r with y will be more similar to the cor-
rect sense(s) of x than the incorrect ones. After 
disambiguating x, the process is then inverted 
with x as the anchor to disambiguate y. 
In the first step, y is fixed and the algorithm 
retrieves the set of all other terms X' that occur in 
an instance (x', r, y), x' ? X'2. For example, given 
the instance (reflections, PART-OF, book), and a 
resource containing the following relations: 
 (false allegations, PART-OF, book) 
 (stories, PART-OF, book) 
 (expert analysis, PART-OF, book) 
 (conclusions, PART-OF, book) 
the resulting set X' would be: {allegations, sto-
ries, analysis, conclusions}. 
All possible permutations, Sxx', between the 
senses of x and the senses of each term in X', 
called Sx', are computed. For each sense pair  
{sx, sx'} ? Sxx', a similarity score r(sx, sx') is calcu-
lated using WordNet: 
 )(
1),(
1
),( '
'
' x
xx
xx sfssd
ssr ?+=  
where the distance d(sx, sx') is the length of the 
shortest path connecting the two synsets in the 
hypernymy hierarchy of WordNet, and f(sx') is 
the number of times sense sx' occurs in any of the 
instances of X'. Note that if no connection be-
tween two synsets exists, then r(sx, sx') = 0. 
The overall sense score for each sense sx of x 
is calculated as: 
 ?
?
=
''
),()( '
xx Ss
xxx ssrsr  
Finally, the algorithm inverts the process by 
setting x as the anchor and computes r(sy) for 
                                                     
2 For semantic relations between complex terms, like (ex-
pert analysis, PART-OF, book), only the head noun of terms 
are recorded, like ?analysis?. As a future work, we plan to 
use the whole term if it is present in WordNet. 
each sense of y. All possible permutations of 
senses are computed and scored by averaging 
r(sx) and r(sy). Permutations scoring higher than a 
threshold ?1 are selected as the attachment points 
in WordNet. We experimentally set ?1 = 0.02. 
3.2 Method 2: Clustering Approach 
The main idea of the clustering approach is to 
leverage the lexical behaviors of the two terms in 
an instance as a whole. The assumption is that 
the general meaning of the relation is derived 
from the combination of the two terms. 
The algorithm is divided in two main phases. 
In the first phase, semantic clusters are built us-
ing the WordNet senses of all instances. A se-
mantic cluster is defined by the set of instances 
that have a common semantic generalization. We 
denote the conceptual instance of the semantic 
cluster as the pair of WordNet synsets that repre-
sents this generalization. For example the follow-
ing two part-of instances: 
 (second section, PART-OF, Los Angeles-area news) 
 (Sandag study, PART-OF, report) 
are in a common cluster represented by the fol-
lowing conceptual instance: 
 [writing#2, PART-OF, message#2] 
since writing#2 is a hypernym of both section 
and study, and message#2 is a hypernym of news 
and report3. 
In the second phase, the algorithm attaches an 
instance into WordNet by using WordNet dis-
tance metrics and frequency scores to select the 
best cluster for each instance. A good cluster is 
one that: 
? achieves a good trade-off between generality 
and specificity; and 
? disambiguates among the senses of x and y us-
ing the other instances? senses as support. 
For example, given the instance (second section, 
PART-OF, Los Angeles-area news) and the follow-
ing conceptual instances: 
 [writing#2, PART-OF, message#2] 
 [object#1, PART-OF, message#2] 
 [writing#2, PART-OF, communication#2]  
 [social_group#1, PART-OF, broadcast#2]   
 [organization#, PART-OF, message#2] 
the first conceptual instance should be scored 
highest since it is both not too generic nor too 
specific and is supported by the instance (Sandag 
study, PART-OF, report), i.e., the conceptual in-
stance subsumes both instances. The second and 
                                                     
3 Again, here, we use the syntactic head of each term for 
generalization since we assume that it drives the meaning 
of the term itself. 
795
the third conceptual instances should be scored 
lower since they are too generic, while the last 
two should be scored lower since the sense for 
section and news are not supported by other in-
stances. The system then outputs, for each in-
stance, the set of sense pairs that are subsumed 
by the highest scoring conceptual instance. In the 
previous example: 
(section#1, PART-OF, news#1) 
(section#1, PART-OF, news#2) 
(section#1, PART-OF, news#3) 
are selected, as they are subsumed by [writing#2, 
PART-OF, message#2]. These sense pairs are then 
retained as attachment points into WordNet. 
Below, we describe each phase in more detail. 
Phase 1: Cluster Building 
Given an instance (x, r, y), all sense pair permu-
tations sxy={sx, sy} are retrieved from WordNet. 
A set of candidate conceptual instances, Cxy, is 
formed for each instance from the permutation of 
each WordNet ancestor of sx and sy, following the 
hypernymy link, up to degree ?2. 
Each candidate conceptual instance,  
c={cx, cy}, is scored by its degree of generaliza-
tion as follows: 
 
)1()1(
1
)( +?+= yx nn
cr  
where ni is the number of hypernymy links 
needed to go from si to ci, for i ? {x, y}. r(c) 
ranges from [0, 1] and is highest when little gen-
eralization is needed. 
For example, the instance (Sandag study, 
PART-OF, report) produces 70 sense pairs since 
study has 10 senses and report has 7 senses. As-
suming ?2=1, the instance sense (survey#1, PART-
OF, report#1) has the following set of candidate 
conceptual instances: 
 
Cxy nx ny r(c)
(survey#1, PART-OF,report#1) 0 0 1
(survey#1, PART-OF,document#1) 0 1 0.5
(examination#1, PART-OF,report#1) 1 0 0.5
(examination#1, PART-OF,document#1) 1 1 0.25
 
Finally, each candidate conceptual instance c 
forms a cluster of all instances (x, r, y) that have 
some sense pair sx and sy as hyponyms of c. Note 
also that candidate conceptual instances may be 
subsumed by other candidate conceptual in-
stances. Let Gc refer to the set of all candidate 
conceptual instances subsumed by candidate 
conceptual instance c. 
Intuitively, better candidate conceptual in-
stances are those that subsume both many in-
stances and other candidate conceptual instances, 
but at the same time that have the least distance 
from subsumed instances. We capture this intui-
tion with the following score of c: 
 cc
c
Gg GI
G
gr
cscore c loglog
)(
)( ??=
?
?  
where Ic is the set of instances subsumed by c. 
We experimented with different variations of this 
score and found that it is important to put more 
weight on the distance between subsumed con-
ceptual instances than the actual number of sub-
sumed instances. Without the log terms, the 
highest scoring conceptual instances are too ge-
neric (i.e., they are too high up in the ontology). 
Phase 2: Attachment Points Selection 
In this phase, we utilize the conceptual instances 
of the previous phase to attach each instance  
(x, r, y) into WordNet. 
At the end of Phase 1, an instance can be clus-
tered in different conceptual instances. In order 
to select an attachment, the algorithm selects the 
sense pair of x and y that is subsumed by the 
highest scoring candidate conceptual instance. It 
and all other sense pairs that are subsumed by 
this conceptual instance are then retained as the 
final attachment points. 
As a side effect, a final set of conceptual in-
stances is obtained by deleting from each candi-
date those instances that are subsumed by a 
higher scoring conceptual instance. Remaining 
conceptual instances are then re-scored using 
score(c). The final set of conceptual instances 
thus contains unambiguous sense pairs. 
4 Experimental Results 
In this section we provide an empirical evalua-
tion of our two algorithms. 
4.1 Experimental Setup 
Researchers have developed many algorithms for 
harvesting semantic relations from corpora and 
the Web. For the purposes of this paper, we may 
choose any one of them and manually validate its 
mined relations. We choose Espresso4, a general-
purpose, broad, and accurate corpus harvesting 
algorithm requiring minimal supervision. Adopt-
                                                     
4 Reference suppressed ? the paper introducing Espresso 
has also been submitted to COLING/ACL 2006. 
796
ing a bootstrapping approach, Espresso takes as 
input a few seed instances of a particular relation 
and iteratively learns surface patterns to extract 
more instances. 
Test Sets 
We experiment with two relations: part-of and 
causation. The causation relation occurs when an 
entity produces an effect or is responsible for 
events or results, for example (virus, CAUSE, in-
fluenza) and (burning fuel, CAUSE, pollution). We 
manually built five seed relation instances for 
both relations and apply Espresso to a dataset 
consisting of a sample of articles from the 
Aquaint (TREC-9) newswire text collection. The 
sample consists of 55.7 million words extracted 
from the Los Angeles Times data files. Espresso 
extracted 1,468 part-of instances and 1,129 cau-
sation instances. We manually validated the out-
put and randomly selected 200 correct relation 
instances of each relation for ontologizing into 
WordNet 2.0. 
Gold Standard 
We manually built a gold standard of all correct 
attachments of the test sets in WordNet. For each 
relation instance (x, r, y), two human annotators 
selected from all sense permutations of x and y 
the correct attachment points in WordNet. For 
example, for (synthetic material, PART-OF, filter), 
the judges selected the following attachment 
points: (synthetic material#1, PART-OF, filter#1) 
and (synthetic material#1, PART-OF, filter#2). The 
kappa statistic (Siegel and Castellan Jr. 1988) on 
the two relations together was ? = 0.73. 
Systems 
The following three systems are evaluated: 
? BL: the baseline system that attaches each rela-
tion instance to the first (most common) 
WordNet sense of both terms; 
? AN: the anchor approach described in Section 
3.1. 
? CL: the clustering approach described in Sec-
tion 3.2. 
4.2 Precision, Recall and F-score 
For both the part-of and causation relations, we 
apply the three systems described above and 
compare their attachment performance using pre-
cision, recall, and F-score. Using the manually 
built gold standard, the precision of a system on a 
given relation instance is measured as the per-
centage of correct attachments and recall is 
measured as the percentage of correct attach-
ments retrieved by the system. Overall system 
precision and recall are then computed by aver-
aging the precision and recall of each relation 
instance. 
Table 1 and Table 2 report the results on the 
part-of and causation relations. We experimen-
tally set the CL generalization parameter ?2 to 5 
and the ?1 parameter for AN to 0.02. 
4.3 Discussion 
For both relations, CL and AN outperform the 
baseline in overall F-score. For part-of, Table 1 
shows that CL outperforms BL by 13.6% in F-
score and AN by 9.4%. For causation, Table 2 
shows that AN outperforms BL by 4.4% on F-
score and CL by 0.6%. 
The good results of the CL method on the 
part-of relation suggest that instances of this rela-
tion are particularly amenable to be clustered. 
The generality of the part-of relation in fact al-
lows the creation of fairly natural clusters, corre-
sponding to different sub-types of part-of, as 
those proposed in (Winston 1983). The causation 
relation, however, being more difficult to define 
at a semantic level (Girju 2003), is less easy to 
cluster and thus to disambiguate. 
Both CL and AN have better recall than BL, 
but precision results vary with CL beating BL 
only on the part-of relation. Overall, the system 
performances suggest that ontologizing semantic 
relations into WordNet is in general not easy. 
The better results of CL and AN with respect 
to BL suggest that the use of comparative seman-
tic analysis among corpus instances is a good 
way to carry out disambiguation. Yet, the BL 
SYSTEM PRECISION RECALL F-SCORE 
BL 45.0% 25.0% 32.1% 
AN 41.7% 32.4% 36.5% 
CL 40.0% 32.6% 35.9% 
Table 2. System precision, recall and F-score on 
the causation relation. 
 
SYSTEM PRECISION RECALL F-SCORE 
BL 54.0% 31.3% 39.6% 
AN 40.7% 47.3% 43.8% 
CL 57.4% 49.6% 53.2% 
Table 1. System precision, recall and F-score on 
the part-of relation. 
 
797
method shows surprisingly good results. This 
indicates that also a simple method based on 
word sense usage in language can be valuable. 
An interesting avenue of future work is to better 
combine these two different views in a single 
system. 
The low recall results for CL are mostly at-
tributed to the fact that in Phase 2 only the best 
scoring cluster is retained for each instance. This 
means that instances with multiple senses that do 
not have a common generalization are not cap-
tured. For example the part-of instance (wings, 
PART-OF, chicken) should cluster both in 
[body_part#1, PART-OF, animal#1] and 
[body_part#1, PART-OF, food#2], but only the 
best scoring one is retained. 
5 Conceptual Instances: Other Uses 
Our clustering approach from Section 3.2 is en-
abled by learning conceptual instances ? relations 
between mid-level ontological concepts. Beyond 
the ontologizing task, conceptual instances may 
be useful for several other tasks. In this section, 
we discuss some of these opportunities and pre-
sent small qualitative evaluations. 
Conceptual instances represent common se-
mantic generalizations of a particular relation. 
For example, below are two possible conceptual 
instances for the part-of relation: 
 [person#1, PART-OF, organization#1] 
 [act#1, PART-OF, plan#1] 
The first conceptual instance in the example sub-
sumes all the part-of instances in which one or 
more persons are part of an organization, such as: 
 (president Brown, PART-OF, executive council) 
 (representatives, PART-OF, organization) 
 (students, PART-OF, orchestra) 
 (players, PART-OF, Metro League) 
Below, we present three possible ways of ex-
ploiting these conceptual instances. 
Support to Relation Extraction Tools 
Conceptual instances may be used to support re-
lation extraction algorithms such as Espresso. 
Most minimally supervised harvesting algo-
rithm do not exploit generic patterns, i.e. those 
patterns with high recall but low precision, since 
they cannot separate correct and incorrect rela-
tion instances. For example, the pattern ?X of Y? 
extracts many correct relation instances like 
?wheel of the car? but also many incorrect ones 
like ?house of representatives?. 
Girju et al (2003) described a highly super-
vised algorithm for learning semantic constraints 
on generic patterns, leading to a very significant 
increase in system recall without deteriorating 
precision. Conceptual instances can be used to 
automatically learn such semantic constraints by 
acting as a filter for generic patterns, retaining 
only those instances that are subsumed by high 
scoring conceptual instances. Effectively, con-
ceptual instances are used as selectional restric-
tions for the relation. For example, our system 
discards the following incorrect instances: 
 (week, CAUSE, coalition) 
 (demeanor, CAUSE, vacuum) 
as they are both part of the very low scoring con-
ceptual instance [abstraction#6, CAUSE, state#1]. 
Ontology Learning from Text 
Each conceptual instance can be viewed as a 
formal specification of the relation at hand. For 
example, Winston (1983) manually identified six 
sub-types of the part-of relation: member-
collection, component-integral object, portion-
mass, stuff-object, feature-activity and place-
area. Such classifications are useful in applica-
tions and tasks where a semantically rich organi-
zation of knowledge is required. Conceptual 
instances can be viewed as an automatic deriva-
tion of such a classification based on corpus us-
age. Moreover, conceptual instances can be used 
to improve the ontology learning process itself. 
For example, our clustering approach can be 
seen as an inductive step producing conceptual 
instances that are then used in a deductive step to 
learn new instances. An algorithm could iterate 
between the induction/deduction cycle until no 
new relation instances and conceptual instances 
can be inferred. 
Word Sense Disambiguation 
Word Sense Disambiguation (WSD) systems can 
exploit the selectional restrictions identified by 
conceptual instances to disambiguate ambiguous 
terms occurring in particular contexts. For exam-
ple, given the sentence: 
?the board is composed by members of different countries? 
and a harvesting algorithm that extracts the part-
of relation (members, PART-OF, board), the sys-
tem could infer the correct senses for board and 
members by looking at their closest conceptual 
instance. In our system, we would infer the at-
tachment (member#1, PART-OF, board#1) since it 
is part of the highest scoring conceptual instance 
[person#1, PART-OF, organization#1]. 
798
5.1 Qualitative Evaluation 
Table 3 and Table 4 list samples of the highest 
ranking conceptual instances obtained by our 
system for the part-of and causation relations. 
Below we provide a small evaluation to verify: 
? the correctness of the conceptual instances. 
Incorrect conceptual instances such as [attrib-
ute#2, CAUSE, state#4], discovered by our sys-
tem, can impede WSD and extraction tools 
where precise selectional restrictions are 
needed; and 
? the accuracy of the conceptual instances. 
Sometimes, an instance is incorrectly attached 
to a correct conceptual instance. For example, 
the instance (air mass, PART-OF, cold front) is 
incorrectly clustered in [group#1, PART-OF, 
multitude#3] since mass and front both have a 
sense that is descendant of group#1 and multi-
tude#3. However, these are not the correct 
senses of mass and front for which the part-of 
relation holds. 
For evaluating correctness, we manually ver-
ify how many correct conceptual instances are 
produced by Phase 2 of the clustering approach 
described in Section 3.2. The claim is that a cor-
rect conceptual instance is one for which the re-
lation holds for all possible subsumed senses. For 
example, the conceptual instance [group#1, 
PART-OF, multitude#3] is correct, as the relation 
holds for every semantic subsumption of the two 
senses. An example of an incorrect conceptual 
instance is [state#4, CAUSE, abstraction#6] since 
it subsumes the incorrect instance (audience, 
CAUSE, new context). A manual evaluation of the 
highest scoring 200 conceptual instances, gener-
ated on our test sets described in Section 4.1, 
showed 82% correctness for the part-of relation 
and 86% for causation. 
For estimating the overall clustering accuracy, 
we evaluated the number of correctly clustered 
instances in each conceptual instance. For exam-
ple, the instance (business people, PART-OF, 
committee) is correctly clustered in [multitude#3, 
PART-OF, group#1] and the instance (law, PART-
OF, constitutional pitfalls) is incorrectly clustered 
in [group#1, PART-OF, artifact#1]. We estimated 
the overall accuracy by manually judging the 
instances attached to 10 randomly sampled con-
ceptual instances. The accuracy for part-of is 
84% and for causation it is 76.6%. 
6 Conclusions 
In this paper, we proposed two algorithms for 
automatically ontologizing binary semantic rela-
tions into WordNet: an anchoring approach and 
a clustering approach. Experiments on the part-
of and causation relations showed promising re-
sults. Both algorithms outperformed the baseline 
on F-score. Our best results were on the part-of 
relation where the clustering approach achieved 
13.6% higher F-score than the baseline. 
The induction of conceptual instances has 
opened the way for many avenues of future 
work. We intend to pursue the ideas presented in 
Section 5 for using conceptual instances to:  
i) support knowledge acquisition tools by learn-
ing semantic constraints on extracting patterns; 
ii) support ontology learning from text; and iii) 
improve word sense disambiguation through se-
lectional restrictions. Also, we will try different 
similarity score functions for both the clustering 
and the anchor approaches, as those surveyed in 
Corley and Mihalcea (2005). 
CONCEPTUAL INSTANCE SCORE # INSTANCES INSTANCES 
[multitude#3, PART-OF, group#1] 2.04 10 
(ordinary people, PART-OF, Democratic Revolutionary Party) 
(unlicensed people, PART-OF, underground economy) 
(young people, PART-OF, commission) 
(air mass, PART-OF, cold front) 
[person#1, PART-OF, organization#1] 1.71 43 
(foreign ministers, PART-OF, council) 
(students, PART-OF, orchestra) 
(socialists, PART-OF, Iraqi National Joint Action Committee) 
(players, PART-OF, Metro League) 
[act#2, PART-OF, plan#1] 1.60 16 
(major concessions, PART-OF, new plan) 
(attacks, PART-OF, coordinated terrorist plan) 
(visit, PART-OF, exchange program) 
(survey, PART-OF, project) 
[communication#2, PART-OF, book#1] 1.14 10 
(hints, PART-OF, booklet) 
(soup recipes, PART-OF, book) 
(information, PART-OF, instruction manual) 
(extensive expert analysis, PART-OF, book) 
[compound#2, PART-OF, waste#1] 0.57 3 
(salts, PART-OF, powdery white waste) 
(lime, PART-OF, powdery white waste) 
(resin, PART-OF, waste) 
Table 3. Sample of the highest scoring conceptual instances learned for the part-of relation. For each 
conceptual instance, we report the score(c), the number of instances, and some example instances. 
799
The algorithms described in this paper may be 
applied to ontologize many lexical resources of 
semantic relations, no matter the harvesting algo-
rithm used to mine them. In doing so, we have 
the potential to quickly enrich our ontologies, 
like WordNet, thus reducing the knowledge ac-
quisition bottleneck. It is our hope that we will be 
able to leverage these enriched resources, albeit 
with some noisy additions, to improve perform-
ance on knowledge-rich problems such as ques-
tion answering and textual entailment. 
References 
Agirre, E. and Rigau, G. 1996. Word sense 
disambiguation using conceptual density. In 
Proceedings of COLING-96. pp. 16-22. Copenhagen, 
Danmark. 
Agirre, E.; Ansa, O.; Martinez, D.; and Hovy, E. 2001. 
Enriching WordNet concepts with topic signatures. In 
Proceedings of NAACL Workshop on WordNet and 
Other Lexical Resources: Applications, Extensions 
and Customizations. Pittsburgh, PA. 
Basili, R.; Pazienza, M.T.; and Vindigni, M. 2000. 
Corpus-driven learning of event recognition rules. In 
Proceedings of Workshop on Machine Learning and 
Information Extraction (ECAI-00). 
Corley, C. and Mihalcea, R. 2005. Measuring the 
Semantic Similarity of Texts. In Proceedings of the 
ACL Workshop on Empirical Modelling of Semantic 
Equivalence and Entailment. Ann Arbor, MI. 
Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-
M.; Shaked, T.; Soderland, S.; Weld, D.S.; and Yates, 
A. 2005. Unsupervised named-entity extraction from 
the Web: An experimental study. Artificial 
Intelligence, 165(1): 91-134. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press. 
Gale, W.; Church, K.; and Yarowsky, D. 1992. A 
method for disambiguating word senses in a large 
corpus. Computers and Humanities, 26:415-439. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. 
Learning semantic constraints for the automatic 
discovery of part-whole relations. In Proceedings of 
HLT/NAACL-03. pp. 80-87. Edmonton, Canada. 
Girju, R. 2003. Automatic Detection of Causal Relations 
for Question Answering. In Proceedings of ACL 
Workshop on Multilingual Summarization and 
Question Answering. Sapporo, Japan. 
Harabagiu, S.; Miller, G.; and Moldovan, D. 1999. 
WordNet 2 - A Morphologically and Semantically 
Enhanced Resource. In Proceedings of SIGLEX-99. 
pp.1-8. University of Maryland. 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. 
(ed.) The Philosophy of Linguistics. New York: 
Oxford University Press. pp. 26?47. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp. 
268?275. Pittsburgh, PA. 
Lin, D. and Pantel, P. 2002. Concept discovery from text. 
In Proceedings of COLING-02. pp. 577-583. Taipei, 
Taiwan. 
Pantel, P. 2005. Inducing Ontological Co-occurrence 
Vectors. In Proceedings of ACL-05. pp. 125-132. Ann 
Arbor, MI. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface 
text patterns for a question answering system. In 
Proceedings of ACL-2002. pp. 41-47. Philadelphia, 
PA. 
Riloff, E. and Shepherd, J. 1997. A corpus-based 
approach for building semantic lexicons. In 
Proceedings of EMNLP-97. 
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric 
Statistics for the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. 
Scaling web-based acquisition of entailment relations. 
In Proceedings of EMNLP-04. Barcelona, Spain. 
Winston, M.; Chaffin, R.; and Hermann, D. 1987. A 
taxonomy of part-whole relations. Cognitive Science, 
11:417?444. 
CONCEPTUAL INSTANCE SCORE # INSTANCES INSTANCES 
[change#3, CAUSE, state#4] 1.49 17 
(separation, CAUSE, anxiety) 
(demotion, CAUSE, roster vacancy) 
(budget cuts, CAUSE, enrollment declines) 
(reduced flow, CAUSE, vacuum) 
[act#2, CAUSE, state#3] 0.81 20 
(oil drilling, CAUSE, air pollution) 
(workplace exposure, CAUSE, genetic injury) 
(industrial emissions, CAUSE, air pollution) 
(long recovery, CAUSE, great stress) 
[person#1, CAUSE, act#2] 0.64 12 
(homeowners, CAUSE, water waste) 
(needlelike puncture, CAUSE, physician) 
(group member, CAUSE, controversy) 
(children, CAUSE, property damage) 
[organism#1, CAUSE, disease#1] 0.03 4 
(parasites, CAUSE, pneumonia) 
(virus, CAUSE, influenza) 
(chemical agents, CAUSE, pneumonia) 
(genetic mutation, CAUSE, Dwarfism) 
Table 4. Sample of the highest scoring conceptual instances learned for the causation relation. For 
each conceptual instance, we report score(c) , the number of instances, and some example instances. 
 
800
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 849?856,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discovering asymmetric entailment relations between verbs
using selectional preferences
Fabio Massimo Zanzotto
DISCo
University of Milano-Bicocca
Via Bicocca degli Arcimboldi 8, Milano, Italy
zanzotto@disco.unimib.it
Marco Pennacchiotti, Maria Teresa Pazienza
ART Group - DISP
University of Rome ?Tor Vergata?
Viale del Politecnico 1, Roma, Italy
{pennacchiotti, pazienza}@info.uniroma2.it
Abstract
In this paper we investigate a novel
method to detect asymmetric entailment
relations between verbs. Our starting point
is the idea that some point-wise verb selec-
tional preferences carry relevant seman-
tic information. Experiments using Word-
Net as a gold standard show promising re-
sults. Where applicable, our method, used
in combination with other approaches, sig-
nificantly increases the performance of en-
tailment detection. A combined approach
including our model improves the AROC
of 5% absolute points with respect to stan-
dard models.
1 Introduction
Natural Language Processing applications often
need to rely on large amount of lexical semantic
knowledge to achieve good performances. Asym-
metric verb relations are part of it. Consider for
example the question ?What college did Marcus
Camby play for??. A question answering (QA)
system could find the answer in the snippet ?Mar-
cus Camby won for Massachusetts? as the ques-
tion verb play is related to the verb win. The vice-
versa is not true. If the question is ?What college
did Marcus Camby won for??, the snippet ?Mar-
cus Camby played for Massachusetts? cannot be
used. Winnig entails playing but not vice-versa, as
the relation between win and play is asymmetric.
Recently, many automatically built verb lexical-
semantic resources have been proposed to sup-
port lexical inferences, such as (Resnik and Diab,
2000; Lin and Pantel, 2001; Glickman and Dagan,
2003). All these resources focus on symmetric
semantic relations, such as verb similarity. Yet,
not enough attention has been paid so far to the
study of asymmetric verb relations, that are often
the only way to produce correct inferences, as the
example above shows.
In this paper we propose a novel approach to
identify asymmetric relations between verbs. The
main idea is that asymmetric entailment relations
between verbs can be analysed in the context of
class-level and word-level selectional preferences
(Resnik, 1993). Selectional preferences indicate
an entailment relation between a verb and its ar-
guments. For example, the selectional preference
{human} win may be read as a smooth constraint:
if x is the subject of win then it is likely that x
is a human, i.e. win(x) ? human(x). It fol-
lows that selectional preferences like {player} win
may be read as suggesting the entailment relation
win(x) ? play(x).
Selectional preferences have been often used to
infer semantic relations among verbs and to build
symmetric semantic resources as in (Resnik and
Diab, 2000; Lin and Pantel, 2001; Glickman and
Dagan, 2003). However, in those cases these are
exploited in a different way. The assumption is
that verbs are semantically related if they share
similar selectional preferences. Then, according
to the Distributional Hypothesis (Harris, 1964),
verbs occurring in similar sentences are likely to
be semantically related.
The Distributional Hypothesis suggests a
generic equivalence between words. Related
methods can then only discover symmetric rela-
tions. These methods can incidentally find verb
pairs as (win,play) where an asymmetric entail-
ment relation holds, but they cannot state the di-
rection of entailment (e.g., win?play).
As we investigate the idea that a single rel-
evant verb selectional preference (as {player}
849
win) could produce an entailment relation between
verbs, our starting point can not be the Distribu-
tional Hypothesis. Our assumption is that some
point-wise assertions carry relevant semantic in-
formation (as in (Robison, 1970)). We do not de-
rive a semantic relation between verbs by compar-
ing their selectional preferences, but we use point-
wise corpus-induced selectional preferences.
The rest of the paper is organised as follows.
In Sec. 2 we discuss the intuition behind our re-
search. In Sec. 3 we describe different types of
verb entailment. In Sec. 4 we introduce our model
for detecting entailment relations among verbs . In
Sec. 5 we review related works that are used both
for comparison and for building combined meth-
ods. Finally, in Sec. 6 we present the results of our
experiments.
2 Selectional Preferences and Verb
Entailment
Selectional restrictions are strictly related to en-
tailment. When a verb or a noun expects a modi-
fier having a predefined property it means that the
truth value of the related sentences strongly de-
pends on the satisfiability of these expectations.
For example, ?X is blue? implies the expectation
that X has a colour. This expectation may be seen
as a sort of entailment between ?being a modi-
fier of that verb or noun? and ?having a property?.
If the sentence is ?The number three is blue?,
then the sentence is false as the underlying entail-
ment blue(x) ? has colour(x) does not hold (cf.
(Resnik, 1993)). In particular, this rule applies to
verb logical subjects: if a verb v has a selectional
restriction requiring its logical subjects to satisfy a
property c, it follows that the implication:
v(x) ? c(x)
should be verified for each logical subject x of the
verb v. The implication can also be read as: if x
has the property of doing the action v this implies
that x has the property c. For example, if the verb
is to eat, the selectional restrictions of to eat would
imply that its subjects have the property of being
animate.
Resnik (1993) introduced a smoothed version
of selectional restrictions called selectional pref-
erences. These preferences describe the desired
properties a modifier should have. The claim is
that if a selectional preference holds, it is more
probable that x has the property c given that it
modifies v rather than x has this property in the
general case, i.e.:
p(c(x)|v(x)) > p(c(x)) (1)
The probabilistic setting of selectional prefer-
ences also suggests an entailment: the implica-
tion v(x) ? c(x) holds with a given degree of
certainty. This definition is strictly related to the
probabilistic textual entailment setting in (Glick-
man et al, 2005).
We can use selectional preferences, intended
as probabilistic entailment rules, to induce entail-
ment relations among verbs. In our case, if a verb
vt expects that the subject ?has the property of do-
ing an action vh?, this may be used to induce that
the verb vt probably entails the verb vh, i.e.:
vt(x) ? vh(x) (2)
As for class-based selectional preference ac-
quisition, corpora can be used to estimate
these particular kinds of preferences. For ex-
ample, the sentence ?John McEnroe won the
match...? contributes to probability estimation of
the class-based selectional preference win(x) ?
human(x) (since John McEnroe is a human). In
particular contexts, it contributes also to the induc-
tion of the entailment relation between win and
play, as John McEnroe has the property of play-
ing. However, as the example shows, classes rele-
vant for acquiring selectional preferences (such as
human) are explicit, as they do not depend from
the context. On the contrary, properties such as
?having the property of doing an action? are less
explicit, as they depend more strongly on the con-
text of sentences. Thus, properties useful to derive
entailment relations among verbs are more diffi-
cult to find. For example, it is easier to derive that
John McEnroe is a human (as it is a stable prop-
erty) than that he has the property of playing. In-
deed, this latter property may be relevant only in
the context of the previous sentence.
However, there is a way to overcome this lim-
itation: agentive nouns such as runner make ex-
plicit this kind of property and often play subject
roles in sentences. Agentive nouns usually denote
the ?doer? or ?performer? of some action. This is
exactly what is needed to make clearer the relevant
property vh(x) of the noun playing the logical sub-
ject role. The action vh will be the one entailed by
the verb vt heading the sentence. As an example
in the sentence ?the player wins?, the action play
850
evocated by the agentive noun player is entailed
by win.
3 Verb entailment: a classification
The focus of our study is on verb entailment. A
brief review of the WordNet (Miller, 1995) verb
hierarchy (one of the main existing resources on
verb entailment relations) is useful to better ex-
plain the problem and to better understand the ap-
plicability of our hypothesis.
In WordNet, verbs are organized in synonymy
sets (synsets) and different kinds of seman-
tic relations can hold between two verbs (i.e.
two synsets): troponymy, causation, backward-
presupposition, and temporal inclusion. All these
relations are intended as specific types of lexical
entailment. According to the definition in (Miller,
1995) lexical entailment holds between two verbs
vt and vh when the sentence Someone vt entails
the sentence Someone vh (e.g. ?Someone wins?
entails ?Someone plays?). Lexical entailment is
then an asymmetric relation. The four types of
WordNet lexical entailment can be classified look-
ing at the temporal relation between the entailing
verb vt and the entailed verb vh.
Troponymy represents the hyponymy relation
between verbs. It stands when vt and vh are tem-
porally co-extensive, that is, when the actions de-
scribed by vt and vh begin and end at the same
times (e.g. limp?walk). The relation of temporal
inclusion captures those entailment pairs in which
the action of one verb is temporally included in the
action of the other (e.g. snore?sleep). Backward-
presupposition stands when the entailed verb vh
happens before the entailing verb vt and it is nec-
essary for vt. For example, win entails play via
backward-presupposition as it temporally follows
and presupposes play. Finally, in causation the
entailing verb vt necessarily causes vh. In this
case, the temporal relation is thus inverted with
respect to backward-presupposition, since vt pre-
cedes vh. In causation, vt is always a causative
verb of change, while vh is a resultative stative
verb (e.g. buy?own, and give?have).
As a final note, it is interesting to notice that the
Subject-Verb structure of vt is generally preserved
in vh for all forms of lexical entailment. The two
verbs have the same subject. The only exception is
causation: in this case the subject of the entailed
verb vh is usually the object of vt (e.g., X give Y
? Y have). In most cases the subject of vt carries
out an action that changes the state of the object of
vt, that is then described by vh.
The intuition described in Sec. 2 is then applica-
ble only for some kinds of verb entailments. First,
the causation relation can not be captured since
the two verbs should have the same subject (cf.
eq. (2)). Secondly, troponymy seems to be less
interesting than the other relations, since our fo-
cus is more on a logic type of entailment (i.e., vt
and vh express two different actions one depend-
ing from the other). We then focus our study and
our experiments on backward-presupposition and
temporal inclusion. These two relations are orga-
nized in WordNet in a single set (called ent) parted
from troponymy and causation pairs.
4 The method
Our method needs two steps. Firstly (Sec. 4.1),
we translate the verb selectional expectations
in specific Subject-Verb lexico-syntactic patterns
P(vt, vh). Secondly (Sec. 4.2), we define a statis-
tical measure S(vt, vh) that captures the verb pref-
erences. This measure describes how much the re-
lations between target verbs (vt, vh) are stable and
commonly agreed.
Our method to detect verb entailment relations
is based on the idea that some point-wise asser-
tions carry relevant semantic information. This
idea has been firstly used in (Robison, 1970) and
it has been explored for extracting semantic re-
lations between nouns in (Hearst, 1992), where
lexico-syntactic patterns are induced by corpora.
More recently this method has been applied for
structuring terminology in isa hierarchies (Morin,
1999) and for learning question-answering pat-
terns (Ravichandran and Hovy, 2002).
4.1 Nominalized textual entailment
lexico-syntactic patterns
The idea described in Sec. 2 can be applied to
generate Subject-Verb textual entailment lexico-
syntactic patterns. It often happens that verbs can
undergo an agentive nominalization, e.g., play vs.
player. The overall procedure to verify if an entail-
ment between two verbs (vt, vh) holds in a point-
wise assertion is: whenever it is possible to ap-
ply the agentive nominalization to the hypothesis
vh, scan the corpus to detect those expressions in
which the agentified hypothesis verb is the subject
of a clause governed by the text verb vt.
Given a verb pair (vt, vh) the assertion is for-
851
Lexico-syntactic patterns
nominalization
Pnom(vt, vh) = {?agent(vh)|num:sing vt|person:third,t:pres?,
?agent(vh)|num:plur vt|person:nothird,t:pres?,
?agent(vh)|num:sing vt|t:past?,
?agent(vh)|num:plur vt|t:past?}
happens-before
(Chklovski and Pantel, 2004)
Phb(vt, vh) = {?vh|t:inf and then vt|t:pres?,
?vh|t:inf * and then vt|t:pres?,
?vh|t:past and then vt|t:pres?,
?vh|t:past * and then vt|t:pres?,
?vh|t:inf and later vt|t:pres?,
?vh|t:past and later vt|t:pres?,
?vh|t:inf and subsequently vt|t:pres?,
?vh|t:past and subsequently vt|t:pres?,
?vh|t:inf and eventually vt|t:pres?,
?vh|t:past and eventually vt|t:pres?}
probabilistic entailment
(Glickman et al, 2005)
Ppe(vt, vh) = {?vh|person:third,t:pres? ? ?vt|person:third,t:pres?,
?vh|t:past? ? ?vt|t:past?,
?vh|t:pres cont? ? ?vt|t:pres cont?,
?vh|person:nothird,t:pres? ? ?vt|person:nothird,t:pres?}
additional sets
Fagent(v) = {?agent(v)|num:sing?, ?agent(v)|num:plur?}
F(v) = {?v|person:third,t:present?,
?v|person:nothird,t:present?, ?v|t:past?}
Fall(v) = {?v|person:third,t:pres?, ?v|t:pres cont,
?v|person:nothird,t:present?, ?v|t:past?}
Table 1: Nominalization and related textual entailment lexico-syntactic patterns
malized in a set of textual entailment lexico-
syntactic patterns, that we call nominalized pat-
terns Pnom(vt, vh). This set is described in Tab. 1.
agent(v) is the noun deriving from the agentifi-
cation of the verb v. Elements such as l|f1,...,fN
are the tokens generated from lemmas l by ap-
plying constraints expressed via the feature-value
pairs f1, ..., fN . For example, in the case of the
verbs play and win, the related set of textual en-
tailment expressions derived from the patterns are
Pnom(win, play) = {?player wins?, ?players
win?, ?player won?, ?players won?}. In the ex-
periments hereafter described, the required verbal
forms have been obtained using the publicly avail-
able morphological tools described in (Minnen et
al., 2001). Simple heuristics have been used to
produce the agentive nominalizations of verbs1.
Two more sets of expressions, Fagent(v) and
F(v) representing the single events in the pair,
are needed for the second step (Sec. 4.2).
This two additional sets are described in
Tab. 1. In the example, the derived expressions
are Fagent(play) = {?player?,?players?} and
F(win) = {?wins?,?won?}.
4.2 Measures to estimate the entailment
strength
The above textual entailment patterns define point-
wise entailment assertions. If pattern instances are
found in texts, the related verb-subject pairs sug-
gest but not confirm a verb selectional preference.
1Agentive nominalization has been obtained adding ?-er?
to the verb root taking into account possible special cases
such as verbs ending in ?-y?. A form is retained as a correct
nominalization if it is in WordNet.
The related entailment can not be considered com-
monly agreed. For example, the sentence ?Like a
writer composes a story, an artist must tell a good
story through their work.? suggests that compose
entails write. However, it may happen that these
correctly detected entailments are accidental, that
is, the detected relation is only valid for the given
text. For example, if the text fragment ?The writ-
ers take a simple idea and apply it to this task?
is taken in isolation, it suggests that take entails
write, but this could be questionable.
In order to get rid of these wrong verb pairs,
we perform a statistical analysis of the verb selec-
tional preferences over a corpus. This assessment
will validate point-wise entailment assertions.
Before introducing the statistical entailment in-
dicator, we provide some definitions. Given a cor-
pus C containing samples, we will refer to the ab-
solute frequency of a textual expression t in the
corpus C with fC(t). The definition can be easily
extended to a set of expressions T .
Given a pair vt and vh we define the fol-
lowing entailment strength indicator S(vt, vh).
Specifically, the measure Snom(vt, vh) is derived
from point-wise mutual information (Church and
Hanks, 1989):
Snom(vt, vh) = log
p(vt, vh|nom)
p(vt)p(vh|pers)
(3)
where nom is the event of having a nominalized
textual entailment pattern and pers is the event of
having an agentive nominalization of verbs. Prob-
abilities are estimated using maximum-likelihood:
p(vt, vh|nom) ?
fC(Pnom(vt, vh))
fC(
?
Pnom(v?t, v
?
h))
,
852
p(vt) ? fC(F(vt))/fC(
?
F(v)), and
p(vh|pers) ? fC(Fagent(vh))/fC(
?
Fagent(v)).
Counts are considered useful when they are
greater or equal to 3.
The measure Snom(vt, vh) indicates the relat-
edness between two elements composing a pair,
in line with (Chklovski and Pantel, 2004; Glick-
man et al, 2005) (see Sec. 5). Moreover, if
Snom(vt, vh) > 0 the verb selectional preference
property described in eq. (1) is satisfied.
5 Related ?non-distributional? methods
and integrated approaches
Our method is a ?non-distributional? approach for
detecting semantic relations between verbs. We
are interested in comparing and integrating our
method with similar approaches. We focus on two
methods proposed in (Chklovski and Pantel, 2004)
and (Glickman et al, 2005). We will shortly re-
view these approaches in light of what introduced
in the previous sections. We also present a simple
way to combine these different approaches.
The lexico-syntactic patterns introduced in
(Chklovski and Pantel, 2004) have been devel-
oped to detect six kinds of verb relations: similar-
ity, strength, antonymy, enablement, and happens-
before. Even if, as discussed in (Chklovski and
Pantel, 2004), these patterns are not specifically
defined as entailment detectors, they can be use-
ful for this purpose. In particular, some of these
patterns can be used to investigate the backward-
presupposition entailment. Verb pairs related by
backward-presupposition are not completely tem-
porally included one in the other (cf. Sec. 3):
the entailed verb vh precedes the entailing verb
vt. One set of lexical patterns in (Chklovski and
Pantel, 2004) seems to capture the same idea: the
happens-before (hb) patterns. These patterns are
used to detect not temporally overlapping verbs,
whose relation is semantically very similar to en-
tailment. As we will see in the experimental sec-
tion (Sec. 6), these patterns show a positive re-
lation with the entailment relation. Tab. 1 re-
ports the happens-before lexico-syntactic patterns
(Phb) as proposed in (Chklovski and Pantel, 2004).
In contrast to what is done in (Chklovski and
Pantel, 2004) we decided to directly count pat-
terns derived from different verbal forms and not
to use an estimation factor. As in our work,
also in (Chklovski and Pantel, 2004), a mutual-
information-related measure is used as statistical
indicator. The two methods are then fairly in line.
The other approach we experiment is the
?quasi-pattern? used in (Glickman et al, 2005) to
capture lexical entailment between two sentences.
The pattern has to be discussed in the more gen-
eral setting of the probabilistic entailment between
texts: the text T and the hypothesis H . The idea is
that the implication T ? H holds (with a degree
of truth) if the probability that H holds knowing
that T holds is higher that the probability that H
holds alone, i.e.:
p(H|T ) > p(H) (4)
This equation is similar to equation (1) in Sec. 2.
In (Glickman et al, 2005), words in H and T are
supposed to be mutually independent. The previ-
ous relation between H and T probabilities then
holds also for word pairs. A special case can be
applied to verb pairs:
p(vh|vt) > p(vh) (5)
Equation (5) can be interpreted as the result of
the following ?quasi-pattern?: the verbs vh and
vt should co-occur in the same document. It is
possible to formalize this idea in the probabilistic
entailment ?quasi-patterns? reported in Tab. 1 as
Ppe, where verb form variability is taken into con-
sideration. In (Glickman et al, 2005) point-wise
mutual information is also a relevant statistical in-
dicator for entailment, as it is strictly related to eq.
(5).
For both approaches, the strength indicator
Shb(vt, vh) and Spe(vt, vh) are computed as fol-
lows:
Sy(vt, vh) = log
p(vt, vh|y)
p(vt)p(vh)
(6)
where y is hb for the happens-before patterns and
pe for the probabilistic entailment patterns. Prob-
abilities are estimated as in the previous section.
Considering independent the probability spaces
where the three patterns lay (i.e., the space of
subject-verb pairs for nom, the space of coordi-
nated sentences for hb, and the space of docu-
ments for pe), the combined approaches are ob-
tained summing up Snom, Shb, and Spe. We will
then experiment with these combined approaches:
nom+pe, nom+hb, nom+hb+pe, and hb+pe.
6 Experimental Evaluation
The aim of the experimental evaluation is to es-
tablish if the nominalized pattern is useful to help
853
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Se(t)
1? Sp(t)
(a)
nom
hb
pe
hb + pe
hb + pe + nom
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Se(t)
1? Sp(t)
(b)
hb
hb + pe
hb + pe + n
hb + pe + n
Figure 1: ROC curves of the different methods
in detecting verb entailment. We experiment with
the method by itself or in combination with other
sets of patterns. We are then interested only in
verb pairs where the nominalized pattern is ap-
plicable. The best pattern or the best combined
method should be the one that gives the highest
values of S to verb pairs in entailment relation,
and the lowest value to other pairs.
We need a corpus C over which to estimate
probabilities, and two dataset, one of verb entail-
ment pairs, the True Set (TS), and another with
verbs not in entailment, the Control Set (CS). We
use the web as corpus C where to estimate Smi
and GoogleTM as a count estimator. The web has
been largely employed as a corpus (e.g., (Turney,
2001)). The findings described in (Keller and La-
pata, 2003) suggest that the count estimations we
need in our study over Subject-Verb bigrams are
highly correlated to corpus counts.
6.1 Experimental settings
Since we have a predefined (but not exhaustive)
set of verb pairs in entailment, i.e. ent in Word-
Net, we cannot replicate a natural distribution of
verb pairs that are or are not in entailment. Re-
call and precision lose sense. Then, the best way
to compare the patterns is to use the ROC curve
(Green and Swets, 1996) mixing sensitivity and
specificity. ROC analysis provides a natural means
to check and estimate how a statistical measure
is able to distinguish positive examples, the True
Set (TS), and negative examples, the Control Set
(CS). Given a threshold t, Se(t) is the probability
of a candidate pair (vh, vt) to belong to True Set if
the test is positive, while Sp(t) is the probability
of belonging to ControlSet if the test is negative,
i.e.:
Se(t) = p((vh, vt) ? TS|S(vh, vt) > t)
Sp(t) = p((vh, vt) ? CS|S(vh, vt) < t)
The ROC curve (Se(t) vs. 1 ? Sp(t)) natu-
rally follows (see Fig. 1). Better methods will
have ROC curves more similar to the step func-
tion f(1 ? Sp(t)) = 0 when 1 ? Sp(t) = 0 and
f(1? Sp(t)) = 1 when 0 < 1? Sp(t) ? 1.
The ROC analysis provides another useful eval-
uation tool: the AROC, i.e. the total area under
the ROC curve. Statistically, AROC represents
the probability that the method in evaluation will
rank a chosen positive example higher than a ran-
domly chosen negative instance. AROC is usually
used to better compare two methods that have sim-
ilar ROC curves. Better methods will have higher
AROCs.
As True Set (TS) we use the controlled verb en-
tailment pairs ent contained in WordNet. As de-
scribed in Sec. 3, the entailment relation is a se-
mantic relation defined at the synset level, stand-
ing in the verb sub-hierarchy. That is, each pair
of synsets (St, Sh) is an oriented entailment rela-
tion between St and Sh. WordNet contains 409
entailed synsets. These entailment relations are
consequently stated also at the lexical level. The
pair (St, Sh) naturally implies that vt entails vh
for each possible vt ? St and vh ? Sh. It is pos-
sible to derive from the 409 entailment synset a
test set of 2,233 verb pairs. As Control Set we
use two sets: random and ent. The random set
854
is randomly generated using verb in ent, taking
care of avoiding to capture pairs in entailment re-
lation. A pair is considered a control pair if it is
not in the True Set (the intersection between the
True Set and the Control Set is empty). The ent is
the set of pairs in ent with pairs in the reverse or-
der. These two Control Sets will give two possible
ways of evaluating the methods: a general and a
more complex task.
As a pre-processing step, we have to clean the
two sets from pairs in which the hypotheses can
not be nominalized, as our pattern Pnom is appli-
cable only in these cases. The pre-processing step
retains 1,323 entailment verb pairs. For compara-
tive purposes the random Control Set is kept with
the same cardinality of the True Set (in all, 1400
verb pairs).
S is then evaluated for each pattern over the
True Set and the Control Set, using equation (3)
for Pnom, and equation (6) for Ppe and Phb. The
best pattern or combined method is the one that
is able to most neatly split entailment pairs from
random pairs. That is, it should in average assign
higher S values to pairs in the True Set.
6.2 Results and analysis
In the first experiment we compared the perfor-
mances of the methods in dividing the ent test set
and the random control set. The compared meth-
ods are: (1) the set of patterns taken alone, i.e.
nom, hb, and pe; (2) some combined methods,
i.e. nom + pe, hb + pe, and nom + hb + pe. Re-
sults of this first experiment are reported in Tab. 2
and Fig. 1.(a). As Figure 1.(a) shows, our nom-
inalization pattern Pnom performs better than the
others. Only Phb seems to outperform nominal-
ization in some point of the ROC curve, where
Pnom presents a slight concavity, maybe due to a
consistent overlap between positive and negative
examples at specific values of the S threshold t.
In order to understand which of the two patterns
has the best discrimination power a comparison of
the AROC values is needed. As Table 2 shows,
Pnom has the best AROC value (59.94%) indi-
cating a more interesting behaviour with respect
to Phb and Ppe. It is respectively 2 and 3 abso-
lute percent point higher. Moreover, the combi-
nations nom + hb + pe and nom + pe that in-
cludes the Pnom pattern have a very high perfor-
mance considering the difficulty of the task, i.e.
66% and 64%. If compared with the combina-
AROC best accuracy
hb 56.00 57.11
pe 57.00 55.75
nom 59.94 59.86
nom+ pe 64.40 61.33
hb+ pe 61.44 58.98
hb+ nom+ pe 66.44 63.09
hb 61.64 62.73
hb+ pe 69.03 64.71
hb+ nom+ pe 70.82 66.07
Table 2: Performances in the general case: ent vs.
random
AROC best accuracy
hb 43.82 50.11
nom 54.91 54.94
hb 56.18 57.16
hb+ nom 49.35 51.73
hb+ nom 57.67 57.22
Table 3: Performances in the complex case: ent
vs. ent
tion hb+pe that excludes the Pnom pattern (61%),
the improvement in the AROC is of 5% and 3%.
Moreover, the shape of the nom + hb + pe ROC
curve in Fig. 1.(a) is above all the other in all the
points.
In the second experiment we compared methods
in the more complex task of dividing the ent set
from the ent set. In this case methods are asked
to determine if win ? play is a correct entail-
ment and play ? win is not. Results of these set
of experiments is presented in Tab. 3. The nom-
inalized pattern nom preserves its discriminative
power. Its AROC is over the chance line even
if, as expected, it is worse than the one obtained
in the general case. Surprisingly, the happens-
before (hb) set of patterns seems to be not cor-
related the entailment relation. The temporal re-
lation vh-happens-before-vt does not seem to be
captured by those patterns. But, if this evidence is
seen in a positive way, it seems that the patterns
are better capturing the entailment when used in
the reversed way (hb). This is confirmed by its
AROC value. If we observe for example one of
the implications in the True Set, reach ? go what
is happening may become clearer. Sample sen-
tences respectively for the hb case and the hb case
are ?The group therefore elected to go to Tyso and
then reach Anskaven? and ?striving to reach per-
sonal goals and then go beyond them?. It seems
that in the second case then assumes an enabling
role more than only a temporal role. After this sur-
855
prising result, as we expected, in this experiment
even the combined approach hb + nom behaves
better than hb + nom and better than hb, respec-
tively around 8% and 1.5% absolute points higher
(see Tab. 3).
The above results imposed the running of a third
experiment over the general case. We need to
compare the entailment indicators derived exploit-
ing the new use of hb, i.e. hb, with respect to the
methods used in the first experiment. Results are
reported in Tab. 2 and Fig. 1.(b). As Fig. 1.(b)
shows, the hb has a very interesting behaviour for
small values of 1 ? Sp(t). In this area it be-
haves extremely better than the combined method
nom+hb+pe. This is an advantage and the com-
bined method nom+hb+pe exploit it as both the
AROC and the shape of the ROC curve demon-
strate. Again the method nom + hb + pe that in-
cludes the Pnom pattern has 1,5% absolute points
with respect to the combined method hb + pe that
does not include this information.
7 Conclusions
In this paper we presented a method to discover
asymmetric entailment relations between verbs
and we empirically demonstrated interesting im-
provements when used in combination with simi-
lar approaches. The method is promising and there
is still some space for improvements. As implic-
itly experimented in (Chklovski and Pantel, 2004),
some beneficial effect can be obtained combining
these ?non-distributional? methods with the meth-
ods based on the Distributional Hypothesis.
References
Timoty Chklovski and Patrick Pantel. 2004. VerbO-
CEAN: Mining the web for fine-grained semantic
verb relations. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, Barcellona, Spain.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicog-
raphy. In Proceedings of the 27th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Vancouver, Canada.
Oren Glickman and Ido Dagan. 2003. Identifying lex-
ical paraphrases from a single corpus: A case study
for verbs. In Proceedings of the International Con-
ference Recent Advances of Natural Language Pro-
cessing (RANLP-2003), Borovets, Bulgaria.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
Web based probabilistic textual entailment. In Pro-
ceedings of the 1st Pascal Challenge Workshop,
Southampton, UK.
David M. Green and John A. Swets. 1996. Signal De-
tection Theory and Psychophysics. John Wiley and
Sons, New York, USA.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics (CoLing-92), Nantes, France.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3), September.
Dekan Lin and Patrick Pantel. 2001. DIRT-discovery
of inference rules from text. In Proc. of the ACM
Conference on Knowledge Discovery and Data Min-
ing (KDD-01), San Francisco, CA.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41, November.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of english. Nat-
ural Language Engineering, 7(3):207?223.
Emmanuel Morin. 1999. Extraction de liens
se?mantiques entre termes a` partir de corpus de
textes techniques. Ph.D. thesis, Univesite? de Nantes,
Faculte? des Sciences et de Techniques.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of the 40th ACL Meeting,
Philadelphia, Pennsilvania.
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In Twenty Second Annual Meeting of the
Cognitive Science Society (COGSCI2000), Philadel-
phia.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, Department of Computer and Informa-
tion Science, University of Pennsylvania.
Harold R. Robison. 1970. Computer-detectable se-
mantic structures. Information Storage and Re-
trieval, 6(3):273?288.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proc. of the 12th Eu-
ropean Conference on Machine Learning, Freiburg,
Germany.
856
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 37?42,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Discovering entailment relations using ?textual entailment patterns?
Fabio Massimo Zanzotto
DISCo, University of Milano-Bicocca,
Via Bicocca degli Arcimboldi 8, Milano, Italy,
zanzotto@disco.unimib.it
Maria Teresa Pazienza, Marco Pennacchiotti
DISP, University of Rome ?Tor Vergata?,
Viale del Politecnico 1, Roma, Italy,
{pennacchiotti, pazienza}@info.uniroma2.it
Abstract
In this work we investigate methods to en-
able the detection of a specific type of tex-
tual entailment (strict entailment), start-
ing from the preliminary assumption that
these relations are often clearly expressed
in texts. Our method is a statistical ap-
proach based on what we call textual en-
tailment patterns, prototypical sentences
hiding entailment relations among two ac-
tivities. We experimented the proposed
method using the entailment relations of
WordNet as test case and the web as cor-
pus where to estimate the probabilities;
obtained results will be shown.
1 Introduction
Textual entailment has been recently defined as a
common solution for modelling language variability
in different NLP tasks (Glickman and Dagan, 2004).
Roughly, the problem is to recognise if a given tex-
tual expression, the text (t), entails another expres-
sion, the hypothesis (h). An example is determining
whether or not ?Yahoo acquired Overture (t) entails
Yahoo owns Overture (h)?. More formally, the prob-
lem of determining a textual entailment between t
and h is to find a possibly graded truth value for the
entailment relation t ? h.
Since the task involves natural language expres-
sions, textual entailment has a more difficult nature
with respect to logic entailment, as it hides two dif-
ferent problems: paraphrase detection and what can
be called strict entailment detection. Generally, this
task is faced under the simplifying assumption that
the analysed text fragments represent facts (ft for
the ones in the text and fh for those in the hypothe-
sis) in an assertive or negative way. Paraphrase de-
tection is then needed when the hypothesis h carries
a fact f that is also in the target text t but is described
with different words, e.g., Yahoo acquired Overture
vs. Yahoo bought Overture. On the other hand, strict
entailment emerges when target sentences carry dif-
ferent facts, fh 6= ft. The challenge here is to derive
the truth value of the entailment ft ? fh. For exam-
ple, a strict entailment is ?Yahoo acquired Overture
? Yahoo owns Overture?. In fact, it does not de-
pend on the possible paraphrasing between the two
expressions but on an entailment of the two facts
governed by acquire and own.
Whatever the form of textual entailment is, the
real research challenge consists in finding a rel-
evant number of textual entailment prototype re-
lations such as ?X acquired Y entails X owns Y? or
?X acquired Y entails X bought Y? that can be used
to recognise entailment relations. Methods for ac-
quiring such textual entailment prototype relations
are based on the assumption that specific facts are
often repeated in possibly different linguistic forms.
These forms may be retrieved using their anchors,
generally nouns or noun phrases completely char-
acterising specific facts. The retrieved text frag-
ments are thus considered alternative expressions
for the same fact. This supposed equivalence is
then exploited to derive textual entailment proto-
type relations. For example, the specific fact Yahoo
bought Overture is characterised by the two anchors
37
{Yahoo, Overture}, that are used to retrieve in the
corpus text fragments where they co-occur, e.g. ?Ya-
hoo purchased Overture (July 2003).?, ?Now that
Overture is completely owned by Yahoo!...?. These
retrieved text fragments are then considered good
candidate for paraphrasing X bought Y.
Anchor-based learning methods have been used
to investigate many semantic relations ranging from
very general ones as the isa relation in (Morin, 1999)
to very specific ones as in (Ravichandran and Hovy,
2002) where paraphrases of question-answer pairs
are searched in the web or as in (Szpektor et al,
2004) where a method to scan the web for searching
textual entailment prototype relations is presented.
These methods are mainly devoted to induce entail-
ment pairs related to the first kind of textual entail-
ment, that is, paraphrasing as their target is mainly
to look for the same ?fact? in different textual forms.
Incidentally, these methods can come across strict
entailment relations whenever specific anchors are
used for both a fact ft and a strictly entailed fact fh.
In this work we will investigate specific meth-
ods to induce the second kind of textual entailment
relations, that is, strict entailment. We will focus
on entailment between verbs, due to the fact that
verbs generally govern the meaning of sentences.
The problem we are facing is to look for (or ver-
ify) entailment relations like vt ? vh (where vt is
the text verb and vh the hypothesis verb). Our ap-
proach is based on an intuition: strict entailment re-
lations among verbs are often clearly expressed in
texts. For instance the text fragment ?Player wins
$50K in Montana Cash? hides an entailment rela-
tion between two activities, namely play and win. If
someone wins, he has first of all to play, thus, win ?
play. The idea exploits the existence of what can be
called textual entailment pattern, a prototypical sen-
tence hiding an entailment relation among two activ-
ities. In the abovementioned example the pattern in-
stance player win subsumes the entailment relation
?win ? play?.
In the following we will firstly describe in Sec.
2 our method to recognise entailment relations be-
tween verbs that uses: (1) the prior linguistic knowl-
edge of these textual entailment patterns and (2) sta-
tistical models to assess stability of the implied re-
lations in a corpus. Then, we will experiment our
method by using the WordNet entailment relations
as test cases and the web as corpus where to esti-
mate the probabilities (Sec. 3). Finally we will draw
some conclusions (Sec. 4).
2 The method
Discovering entailment relations within texts im-
plies the understanding of two aspects: firstly, how
these entailment relations are usually expressed and,
secondly, when an entailment relation may be con-
sidered stable and commonly shared. Assessing the
first aspect requires the investigation of which are
the prototypical textual forms that describe entail-
ment relations. We will call them textual entailment
patterns. These patterns (analysed in Sec. 2.2) will
enable the detection of point-wise entailment asser-
tions, that is, candidate verb pairs that still need a
further step of analysis in order to be considered
true entailment expressions. In fact, some of these
candidates may be not enough stable and commonly
shared in the language to be considered true en-
tailments. To better deal with this second aspect,
methods for statistically analysing large corpora are
needed (see later in Sec. 2.3).
The method we propose may be used in either: (1)
recognising if entailment holds between two verbs,
or, (2) extracting from a corpus C all the implied
entailment relations. In recognition, given a verb
pair, the related textual entailment expressions are
derived as instances of the textual entailment pat-
terns and, then, the statistical entailment indicators
on a corpus C are computed to evaluate the stability
of the relation. In extraction, the corpus C should
be scanned to extract textual expressions that are in-
stances of the textual entailment patterns. The re-
sulting pairs are sorted according to the statistical
entailment indicators and only the best ranked are
retained as useful verb entailment pairs.
2.1 An intuition
Our method stems from an observation: verb logical
subjects, as any verb role filler, have to satisfy spe-
cific preconditions as the theory of selectional re-
strictions suggests. Then, if in a given sentence a
verb v has a specific logical subject x, its selectional
restrictions imply that the subject has to satisfy some
preconditions p, that is, v(x) ? p(x). This can be
read also as: if x has the property of doing the action
38
v this implies that x has the property p. For example,
if the verb is to eat, the selectional restrictions of eat
would imply, among other things, that its subject is
an animal. If the precondition p is ?having the prop-
erty of doing an action a?, the constraint may imply
that the action v entails the action a, that is, v ? a.
As for selectional restriction acquisition, the pre-
vious observation can enable the use of corpora as
enormous sources of candidate entailment relations
among verbs. For example ?John McEnroe won the
match...? can contribute to the definition of the selec-
tional restriction win(x) ? human(x) (since John
McEnroe is a human), as well as to the induction (or
verification) of the entailment relation between win
and play, since John McEnroe has the property of
playing. However, as the example shows, classes
relevant for acquiring selectional preferences may
be more explicit than active properties useful to de-
rive entailment relations (i.e., it is easier to derive
that John McEnroe is a human than that he has the
property of playing).
This limitation can be overcome when agentive
nouns such as runner play subject roles in some sen-
tences. Agentive nouns usually denote the ?doer? or
?performer? of some action a. This is exactly what
is needed to make clearer the relevant property of
the noun playing the logical subject role, in order to
discover entailment. The action a will be the one en-
tailed by the verb heading the sentence. For exam-
ple, in ?the player wins?, the action play evocated
by the agentive noun player is entailed by win.
2.2 Textual entailment patterns
As observed for the isa relations in (Hearst, 1992)
local and simple inter-sentential patterns may carry
relevant semantic relations. As we saw in the pre-
vious section, this also happens for entailment re-
lations. Our aim is thus to search for an initial set
of textual patterns that describe possible linguistic
forms expressing entailment relations between two
verbs (vt, vh). By using these patterns, actual point-
wise assertions of entailment can be detected or ver-
ified in texts. We call these prototypical patterns tex-
tual entailment patterns.
The idea described in Sec. 2.1 can be straight-
forwardly applied to generate textual entailment pat-
terns, as it often happens that verbs can undergo an
agentive nominalization (hereafter called personifi-
cation), e.g., play vs. player. Whether or not an
entailment relation between two verbs (vt, vh) holds
according to some writer can be verified looking for
sentences with expressions involving the agentive
nominalization of the hypothesis verb vh. Then, the
procedure to verify if entailment between two verbs
(vt, vh) holds in a point-wise assertion is: whenever
it is possible to personify the hypothesis vh, scan the
corpus to detect the expressions where the personi-
fied hypothesis verb is the subject of a clause gov-
erned by the text verb vt.
Given the two investigated verbs (vt, vh) we will
refer to this first set of textual entailment patterns
as personified patterns Ppers(vt, vh). This set will
contain the following textual patterns:
Ppers(vt, vh) =
{?pers(vh)|number:sing vt|person:third,tense:present?,
?pers(vh)|number:plur vt|person:nothird,tense:present?,
?pers(vh)|number:sing vt|tense:past?,
?pers(vh)|number:plur vt|tense:past?}
where pers(v) is the noun deriving from the person-
ification of the verb v and elements such as l|f1,...,fN
are the tokens generated from lemmas l by apply-
ing constraints expressed via the features f1, ..., fN .
For example, in the case of the verbs play and win,
the related set of textual entailment expressions de-
rived from the patterns will be Ppers(win, play)
= { ?player wins?, ?players win?, ?player won?,
?players won? }. In the experiments hereafter de-
scribed, the required verbal inflections (except per-
sonification) have been obtained using the publicly
available morphological tools described in (Minnen
et al, 2001) whilst simple heuristics have been used
to personify verbs1.
As the statistical measures introduced in the fol-
lowing section are those usually used for study-
ing co-occurrences, two more sets of expressions,
Fpers(v) and F(v), are needed to represent the sin-
gle events in the pair. These are defined as:
Fpers(v) = {?pers(v)|number:sing?, ?pers(v)|number:plur?}
F(v) = {?v|person:third,tense:present?,
?v|person:nothird,tense:present?, ?v|tense:past?}
1Personification, i.e. agentive nominalization, has been ob-
tained adding ?-er? to the verb root taking into account possible
special cases such as verbs ending in ?-y?. A form is retained
as a correct personification if it is in WordNet.
39
2.3 Measures to estimate the entailment
strength
The above textual entailment patterns define point-
wise entailment assertions. In fact, if pattern in-
stances are found in texts, the only conclusion that
may be drawn is that someone (the author of the
text) sustains the related entailment pairs. A sen-
tence like ?Painter draws on old techniques but cre-
ates only decorative objects.? suggests that painting
entails drawing. However, it may happen that these
correctly detected entailments are accidental, that is,
the detected relation is only valid for that given text.
For example, the text fragment ?When a painter dis-
covers this hidden treasure, other people are imme-
diately struck by its beauty.? if taken in insulation
suggests that painting entails discovering, but this is
questionable. Furthermore, it may also happen that
patterns detect wrong cases due to ambiguous ex-
pressions like ?Painter draws inspiration from for-
est, field? where the sense of the verb draw is not
the one expected.
In order to get rid of these wrong verb pairs, an
assessment of point-wise entailment assertions over
a corpus is needed to understand how much the de-
rived entailment relations are shared and commonly
agreed. This validation activity can be obtained by
both analysing large textual collections and applying
statistical measures relevant for the task.
Before introducing the statistical entailment indi-
cators, some definitions are necessary. Given a cor-
pus C containing samples, we will refer to the abso-
lute frequency of a textual expression t in the corpus
C with fC(t). The definition is easily extended to a
set of expressions T as follows:
fC(T ) =
?
t?T
fC(t)
Given a pair vt and vh we may thus define the fol-
lowing entailment strength indicators S(vt, vh), re-
lated to more general statistical measures.
The first relevance indicator, Sf (vt, vh), is related
to the probability of the textual entailment pattern
as it is. This probability may be represented by the
frequency, as the fixed corpus C makes constant the
total number of pairs:
Sf (vt, vh) = log10(fC(Ppers(vt, vh)))
where logarithm is used to contrast the effect of the
Zipf?s law. This measure is often positively used in
terminology extraction (e.g., (Daille, 1994)).
Secondly, another measure Smi(vt, vh) related to
point-wise mutual information (Fano, 1961) may
be also used. Given the possibility of estimating
the probabilities through maximum-likelihood prin-
ciple, the definition is straightforward:
Smi(vt, vh) = log10
p(Ppers(vt, vh))
p(Fpers(vt))p(F(vh))
where p(x) = fC(x)/fC(.). The aim of this mea-
sure is to indicate the relatedness between two el-
ements composing a pair. Mutual information has
been positively used in many NLP tasks such as col-
location analysis (Church and Hanks, 1989), termi-
nology extraction (Damerau, 1993), and word sense
disambiguation (Brown et al, 1991).
3 Experimental Evaluation
As many other corpus linguistic approaches, our en-
tailment detection model relies partially on some lin-
guistic prior knowledge (the expected structure of
the searched collocations, i.e., the textual entailment
patterns) and partially on some probability distribu-
tion estimation. Only a positive combination of both
these two ingredients can give good results when ap-
plying (and evaluating) the model.
The aim of the experimental evaluation is then to
understand, on the one side, if the proposed textual
entailment patterns are useful to detect entailment
between verbs and, on the other, if a statistical mea-
sure is preferable with respect to the other. We will
here evaluate the capability of our method to recog-
nise entailment between given pairs of verbs.
We carried out the experiments using the web as
the corpus C where to estimate our two textual en-
tailment measures (Sf and Smi) and GoogleTM as
a count estimator. The findings described in (Keller
and Lapata, 2003) seem to suggest that count estima-
tions we need in the present study over Subject-Verb
bigrams are highly correlated to corpus counts.
As test bed we used existing resources: a non triv-
ial set of controlled verb entailment pairs is in fact
contained in WordNet (Miller, 1995). There, the en-
tailment relation is a semantic relation defined at the
synset level, standing in the verb subhierarchy. Each
40
Figure 1: ROC curves
pair of synsets (St, Sh) is an oriented entailment re-
lation between St and Sh. WordNet contains 415
entailed synsets. These entailment relations are con-
sequently stated also at the lexical level. The pair
(St, Sh) naturally implies that vt entails vh for each
possible vt ? St and vh ? Sh. It is then possible
to derive from the 415 entailment synset a test set of
2,250 verb pairs. As the proposed model is appli-
cable only when hypotheses can be personified, the
number of the pairs relevant for the experiment is
thus reduced to 856. This set is hereafter called the
True Set (TS).
As the True Set is our starting point for the eval-
uation, it is not possible to produce a natural distri-
bution in the verb pair space between entailed and
not-entailed elements. Then, precision, recall, and
f-measure are not applicable. The only solution is
to use a ROC (Green and Swets, 1996) curve mix-
ing sensitity and specificity. What we then need is a
Control Set (CS) of verb pairs that in principle are
not in entailment relation. The Control Set has been
randomly built on the basis of the True Set: given
the set of all the hypothesis verbs H and the set of
all the text verbs T of the True Set, control pairs are
obtained randomly extracting one element from H
and one element from T . A pair is considered a con-
trol pair if it is not in the True Set. For comparative
purposes the Control Set has the same cardinality
of the True Set. However, even if the intersection
between the True Set and the Control Set is empty,
we are not completely sure that the Control Set does
not contains any pair where the entailment relation
holds. What we may assume is that this last set at
least contains a smaller number of positive pairs.
Sensitivity, i.e. the probability of having positive
answers for positive pairs, and specificity, i.e. the
probability of having negative answers for negative
pairs, are then defined as:
Sensitivity(t) = p((vh, vt) ? TS|S(vh, vt) > t)
Specificity(t) = p((vh, vt) ? CS|S(vh, vt) < t)
where p((vh, vt) ? TS|S(vh, vt) > t) is the prob-
ability of a candidate pair (vh, vt) to belong to TS
if the test is positive, i.e. the value S(vh, vt) of the
entailment detection measure is greater than t, while
p((vh, vt) ? CS|S(vh, vt) < t) is the probability
of belonging to CS if the test is negative. The ROC
curve (Sensitivity vs. 1 ? Specificity) naturally
follows (see Fig. 1).
Results are encouraging as textual entailment pat-
terns show a positive correlation with the entailment
relation. Both ROC curves, the one related to the fre-
quency indicator Sf (f in figure) and the one related
to the mutual information SMI (MI in figure), are
above the Baseline curve. Moreover, both curves
are above the second baseline (Baseline2) applica-
ble when it is really possible to use the indicators. In
fact, textual entailment patterns have a non-zero fre-
quency only for 61.4% of the elements in the True
Set. This is true also for 48.1% of the elements in the
Control Set. The presence-absence in the corpus is
then already an indicator for the entailment relation
of verb pairs, but the application of the two indica-
tors can help in deciding among elements that have
a non-zero frequency in the corpus. Finally, in this
case, mutual information appears to be a better indi-
cator for the entailment relation with respect to the
frequency.
4 Conclusions
We have defined a method to recognise and extract
entailment relations between verb pairs based on
what we call textual entailment pattern. In this work
we defined a first kernel of textual entailment pat-
terns based on subject-verb relations. Potentials of
the method are still high as different kinds of textual
41
entailment patterns may be defined or discovered
investigating relations between sentences and sub-
sentences as done in (Lapata and Lascarides, 2004)
for temporal relations or between near sentences as
done in (Basili et al, 2003) for cause-effect relations
between domain events. Some interesting and sim-
ple inter-sentential patters are defined in (Chklovski
and Pantel, 2004). Moreover, with respect to anchor-
based approaches, the method we presented here
offers a different point of view on the problem of
acquiring textual entailment relation prototypes, as
textual entailment patterns do not depend on the rep-
etition of ?similar? facts. This practically indepen-
dent view may open the possibility to experiment
co-training algorithms (Blum and Mitchell, 1998)
also in this area. Finally, the approach proposed can
be useful to define better probability estimations in
probabilistic entailment detection methods such as
the one described in (Glickman et al, 2005).
References
Roberto Basili, Maria Teresa Pazienza, and Fabio Mas-
simo Zanzotto. 2003. Inducing hyperlinking rules
from text collections. In Proceedings of the Interna-
tional Conference Recent Advances of Natural Lan-
guage Processing (RANLP-2003), Borovets, Bulgaria.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT: Proceedings of the Conference on Computa-
tional Learning Theory. Morgan Kaufmann.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1991. Word-sense
disambiguation using statistical methods. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics (ACL), Berkely, CA.
Timoty Chklovski and Patrick Pantel. 2004. VerbO-
CEAN: Mining the web for fine-grained semantic verb
relations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
Barcellona, Spain.
K.W. Church and P. Hanks. 1989. Word association
norms, mutual information and lexicography. In Pro-
ceedings of the 27th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Vancouver,
Canada.
Beatrice Daille. 1994. Approche mixte pour l?extraction
de terminologie: statistque lexicale et filtres linguis-
tiques. Ph.D. thesis, C2V, TALANA, Universite` Paris
VII.
F.J. Damerau. 1993. Evaluating domain-oriented multi-
word terms from text. Information Processing and
Management, 29(4):433?447.
R.M. Fano. 1961. Transmission of Information: a sta-
tistical theory of communications. MIT Press, Cam-
bridge,MA.
Oren Glickman and Ido Dagan. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In Proceedings of the Workshop on
Learning Methods for Text Understanding and Mining,
Grenoble, France.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
Web based probabilistic textual entailment. In Pro-
ceedings of the 1st Pascal Challenge Workshop,
Southampton, UK.
D.M. Green and J.A. Swets. 1996. Signal Detection The-
ory and Psychophysics. John Wiley and Sons, New
York, USA.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics (CoLing-92), Nantes, France.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3), September.
Mirella Lapata and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In Proceedings
of the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, MA.
George A. Miller. 1995. WordNet: A lexical database for
English. Communications of the ACM, 38(11):39?41,
November.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of english. Natural Lan-
guage Engineering, 7(3):207?223.
Emmanuel Morin. 1999. Extraction de liens
se?mantiques entre termes a` partir de corpus de textes
techniques. Ph.D. thesis, Univesite? de Nantes, Faculte?
des Sciences et de Techniques.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th ACL Meeting, Philadelphia,
Pennsilvania.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura
Coppola. 2004. Scaling web-based acquisition of en-
tailment relations. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Barcellona, Spain.
42
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423?430,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Assessing Review Helpfulness 
 
Soo-Min Kim?, Patrick Pantel?, Tim Chklovski?, Marco Pennacchiotti? 
?Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{skim,pantel,timc}@isi.edu 
?ART Group - DISP 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
 
 
 
Abstract 
User-supplied reviews are widely and 
increasingly used to enhance e-
commerce and other websites. Because 
reviews can be numerous and varying in 
quality, it is important to assess how 
helpful each review is. While review 
helpfulness is currently assessed manu-
ally, in this paper we consider the task 
of automatically assessing it. Experi-
ments using SVM regression on a vari-
ety of features over Amazon.com 
product reviews show promising results, 
with rank correlations of up to 0.66. We 
found that the most useful features in-
clude the length of the review, its uni-
grams, and its product rating. 
1 Introduction 
Unbiased user-supplied reviews are solicited 
ubiquitously by online retailers like Ama-
zon.com, Overstock.com, Apple.com and Epin-
ions.com, movie sites like imdb.com, traveling 
sites like citysearch.com, open source software 
distributors like cpanratings.perl.org, and count-
less others. Because reviews can be numerous 
and varying in quality, it is important to rank 
them to enhance customer experience. 
In contrast with ranking search results, assess-
ing relevance when ranking reviews is of little 
importance because reviews are directly associ-
ated with the relevant product or service. Instead, 
a key challenge when ranking reviews is to de-
termine which reviews the customers will find 
helpful. 
Most websites currently rank reviews by their 
recency or product rating (e.g., number of stars 
in Amazon.com reviews). Recently, more sophis-
ticated ranking schemes measure reviews by their 
helpfulness, which is typically estimated by hav-
ing users manually assess it. For example, on 
Amazon.com, an interface allows customers to 
vote whether a particular review is helpful or not. 
Unfortunately, newly written reviews and re-
views with few votes cannot be ranked as several 
assessments are required in order to properly es-
timate helpfulness. For example, for all MP3 
player products on Amazon.com, 38% of the 
20,919 reviews received three or fewer helpful-
ness votes. Another problem is that low-traffic 
items may never gather enough votes. Among the 
MP3 player reviews that were authored at least 
three months ago on Amazon.com, still only 31% 
had three or fewer helpfulness votes. 
It would be useful to assess review helpfulness 
automatically, as soon as the review is written. 
This would accelerate determining a review?s 
ranking and allow a website to provide rapid 
feedback to review authors. 
In this paper, we investigate the task of auto-
matically predicting review helpfulness using a 
machine learning approach. Our main contribu-
tions are: 
? A system for automatically ranking reviews 
according to helpfulness; using state of the art 
SVM regression, we empirically evaluate our 
system on a real world dataset collected from 
Amazon.com on the task of reconstructing the 
helpfulness ranking; and 
? An analysis of different classes of features 
most important to capture review helpful-
ness; including structural (e.g., html tags, 
punctuation, review length), lexical (e.g., n-
grams), syntactic (e.g., percentage of verbs and 
nouns), semantic (e.g., product feature men-
tions), and meta-data (e.g., star rating). 
2 Relevant Work 
The task of automatically assessing product re-
view helpfulness is related to these broader areas 
423
of research: automatic analysis of product re-
views, opinion and sentiment analysis, and text 
classification. 
In the thriving area of research on automatic 
analysis and processing of product reviews (Hu 
and Liu 2004; Turney 2002; Pang and Lee 2005), 
little attention has been paid to the important task 
studied here ? assessing review helpfulness. Pang 
and Lee (2005) have studied prediction of prod-
uct ratings, which may be particularly relevant 
due to the correlation we find between product 
rating and the helpfulness of the review (dis-
cussed in Section 5). However, a user?s overall 
rating for the product is often already available. 
Helpfulness, on the other hand, is valuable to 
assess because it is not explicitly known in cur-
rent approaches until many users vote on the 
helpfulness of a review.  
In opinion and sentiment analysis, the focus is 
on distinguishing between statements of fact vs. 
opinion, and on detecting the polarity of senti-
ments being expressed. Many researchers have 
worked in various facets of opinion analysis. 
Pang et al (2002) and Turney (2002) classified 
sentiment polarity of reviews at the document 
level.  Wiebe et al (1999) classified sentence 
level subjectivity using syntactic classes such as 
adjectives, pronouns and modal verbs as features.  
Riloff and Wiebe (2003) extracted subjective 
expressions from sentences using a bootstrapping 
pattern learning process. Yu and Hatzivassi-
loglou (2003) identified the polarity of opinion 
sentences using semantically oriented words. 
These techniques were applied and examined in 
different domains, such as customer reviews (Hu 
and Liu 2004) and news articles (TREC novelty 
track 2003 and 2004).  
In text classification, systems typically use 
bag-of-words models, although there is some 
evidence of benefits when introducing relevant 
semantic knowledge (Gabrilovich and Mark-
ovitch, 2005). In this paper, we explore the use of 
some semantic features for review helpfulness 
ranking. Another potential relevant classification 
task is academic and commercial efforts on de-
tecting email spam messages1, which aim to cap-
ture a much broader notion of helpfulness. For an 
SVM-based approach, see (Drucker et al 1999).  
Finally, a related area is work on automatic es-
say scoring, which seeks to rate the quality of an 
essay (Attali and Burstein 2006; Burstein et al 
2004). The task is important for reducing the 
human effort required in scoring large numbers 
                                                     
1 See http://www.ceas.cc/, http://spamconference.org/  
of student essays regularly written for standard 
tests such as the GRE. The exact scoring ap-
proaches developed in commercial systems are 
often not disclosed. However, more recent work 
on one of the major systems, e-rater 2.0, has fo-
cused on systematizing and simplifying the set of 
features used (Attali and Burstein 2006). Our 
choice of features to test was partially influenced 
by the features discussed by Attali and Burstein. 
At the same time, due to differences in the tasks, 
we did not use features aimed at assessing essay 
structure such as discourse structure analysis fea-
tures. Our observations suggest that even helpful 
reviews vary widely in their discourse structure. 
We present the features which we have used be-
low, in Section 3.2. 
3 Modeling Review Helpfulness 
In this section, we formally define the learning 
task and we investigate several features for as-
sessing review helpfulness. 
3.1 Task Definition 
Formally, given a set of reviews R for a particu-
lar product, our task is to rank the reviews ac-
cording to their helpfulness. We define a review 
helpfulness function, h, as: 
 ( ) ( )( ) ( )rratingrrating
rrating
Rrh
?+
+
+=?  (1) 
where rating+(r) is the number of people that will 
find a review helpful and rating-(r) is the number 
of people that will find the review unhelpful. For 
evaluation, we resort to estimates of h from man-
ual review assessments on websites like Ama-
zon.com, as described in Section 4. 
3.2 Features 
One aim of this paper is to investigate how well 
different classes of features capture the helpful-
ness of a review. We experimented with various 
features organized in five classes: Structural, 
Lexical, Syntactic, Semantic, and Meta-data. Be-
low we describe each feature class in turn. 
Structural Features 
Structural features are observations of the docu-
ment structure and formatting. Properties such as 
review length and average sentence length are 
hypothesized to relate structural complexity to 
helpfulness. Also, HTML formatting tags could 
help in making a review more readable, and con-
sequently more helpful. We experimented with 
the following features: 
424
? Length (LEN): The total number of tokens in a 
syntactic analysis2 of the review. 
? Sentential (SEN): Observations of the sen-
tences, including the number of sentences, the 
average sentence length, the percentage of 
question sentences, and the number of excla-
mation marks. 
? HTML (HTM): Two features for the number of 
bold tags <b> and line breaks <br>. 
Lexical Features 
Lexical features capture the words observed in 
the reviews. We experimented with two sets of 
features: 
? Unigram (UGR): The tf-idf statistic of each 
word occurring in a review. 
? Bigram (BGR): The tf-idf statistic of each bi-
gram occurring in a review. 
For both unigrams and bigrams, we used lemma-
tized words from a syntactic analysis of the re-
views and computed the tf-idf statistic (Salton 
and McGill 1983) using the following formula: 
 ( )
N
idftf
idftf
log?=  
where N is the number of tokens in the review. 
Syntactic Features 
Syntactic features aim to capture the linguistic 
properties of the review. We grouped them into 
the following feature set: 
? Syntax (SYN): Includes the percentage of 
parsed tokens that are open-class (i.e., nouns, 
verbs, adjectives and adverbs), the percentage 
of tokens that are nouns, the percentage of to-
kens that are verbs, the percentage of tokens 
that are verbs conjugated in the first person, 
and the percentage of tokens that are adjectives 
or adverbs. 
Semantic Features 
Most online reviews are fairly short; their spar-
sity suggests that bigram features will not per-
form well (which is supported by our 
experiments described in Section 5.3). Although 
semantic features have rarely been effective in 
many text classification problems (Moschitti and 
Basili 2004), there is reason here to hypothesize 
that a specialized vocabulary of important words 
might help with the sparsity. We hypothesized 
                                                     
2  Reviews are analyzed using the Minipar dependency 
parser (Lin 1994). 
that good reviews will often contain: i) refer-
ences to the features of a product (e.g., the LCD 
and resolution of a digital camera), and ii) men-
tions of sentiment words (i.e., words that express 
an opinion such as ?great screen?). Below we 
describe two families of features that capture 
these semantic observations within the reviews: 
? Product-Feature (PRF): The features of prod-
ucts that occur in the review, e.g., capacity of 
MP3 players and zoom of a digital camera. 
This feature counts the number of lexical 
matches that occur in the review for each prod-
uct feature. There is no trivial way of obtaining 
a list of all the features of a product. In Section 
5.1 we describe a method for automatically ex-
tracting product features from Pro/Con listings 
from Epinions.com. Our assumption is that 
pro/cons are the features that are important for 
customers (and hence should be part of a help-
ful review). 
? General-Inquirer (GIW): Positive and negative 
sentiment words describing products or prod-
uct features (e.g., ?amazing sound quality? and 
?weak zoom?). The intuition is that reviews 
that analyze product features are more helpful 
than those that do not. We try to capture this 
analysis by extracting sentiment words using 
the publicly available list of positive and nega-
tive sentiment words from the General Inquirer 
Dictionaries3. 
Meta-Data Features 
Unlike the previous four feature classes, meta-
data features capture observations which are in-
dependent of the text (i.e., unrelated with linguis-
tic features). We consider the following feature: 
? Stars (STR): Most websites require reviewers 
to include an overall rating for the products 
that they review (e.g., star ratings in Ama-
zon.com). This feature set includes the rating 
score (STR1) as well as the absolute value of 
the difference between the rating score and the 
average rating score given by all reviewers 
(STR2). 
We differentiate meta-data features from seman-
tic features since they require external knowl-
edge that may not be available from certain 
review sites. Nowadays, however, most sites that 
collect user reviews also collect some form of 
product rating (e.g., Amazon.com, Over-
stock.com, and Apple.com). 
                                                     
3 http://www.wjh.harvard.edu/~inquirer/homecat.htm 
425
4 Ranking System 
In this paper, we estimate the helpfulness func-
tion in Equation 1 using user ratings extracted 
from Amazon.com, where rating+(r) is the num-
ber of unique users that rated the review r as 
helpful and rating-(r) is the number of unique 
users that rated r as unhelpful. 
Reviews from Amazon.com form a gold stan-
dard labeled dataset of {review, h(review)} pairs 
that can be used to train a supervised machine 
learning algorithm. In this paper, we applied an 
SVM (Vapnik 1995) package on the features ex-
tracted from reviews to learn the function h. 
Two natural options for learning helpfulness 
according to Equation 1 are SVM Regression and 
SVM Ranking (Joachims 2002). Though learning 
to rank according to helpfulness requires only 
SVM Ranking, the helpfulness function provides 
non-uniform differences between ranks in the 
training set. Also, in practice, many products 
have only one review, which can serve as train-
ing data for SVM Regression but not SVM Rank-
ing. Furthermore, in large sites such as 
Amazon.com, when new reviews are written it is 
inefficient to re-rank all previously ranked re-
views. We therefore choose SVM Regression in 
this paper. We describe the exact implementation 
in Section 5.1. 
After the SVM is trained, for a given product 
and its set of reviews R, we rank the reviews of R 
in decreasing order of h(r), r ? R. 
Table 1 shows four sample reviews for the 
iPod Photo 20GB product from Amazon.com, 
their total number of helpful and unhelpful votes, 
as well as their rank according to the helpfulness 
score h from both the gold standard from Ama-
zon.com and using the SVM prediction of our 
best performing system described in Section 5.2. 
5 Experimental Results 
We empirically evaluate our review model and 
ranking system, described in Section 3 and Sec-
tion 4, by comparing the performance of various 
feature combinations on products mined from 
Amazon.com. Below, we describe our experi-
mental setup, present our results, and analyze 
system performance. 
5.1 Experimental Setup 
We describe below the datasets that we extracted 
from Amazon.com, the implementation of our 
SVM system, and the method we used for ex-
tracting features of reviews. 
Extraction and Preprocessing of Datasets 
We focused our experiments on two products 
from Amazon.com: MP3 Players and Digital 
Cameras. 
Using Amazon Web Services API, we col-
lected reviews associated with all products in the 
MP3 Players and Digital Cameras categories. 
For MP3 Players, we collected 821 products and 
33,016 reviews; for Digital Cameras, we col-
lected 1,104 products and 26,189 reviews. 
In most retailer websites like Amazon.com, 
duplicate reviews, which are quite frequent, skew 
statistics and can greatly affect a learning algo-
rithm. Looking for exact string matches between 
reviews is not a sufficient filter since authors of 
duplicated reviews often make small changes to 
the reviews to avoid detection. We built a simple 
filter that compares the distribution of word bi-
grams across each pair of reviews. A pair is 
deemed a duplicate if more than 80% of their 
bigrams match. 
Also, whole products can be duplicated. For 
different product versions, such as iPods that can 
come in black or white models, reviews on Ama-
zon.com are duplicated between them. We filter 
Table 1. Sample of 4 out of 43 reviews for the iPod Photo 20GB product from Ama-
zon.com along with their ratings as well as their helpfulness ranks (from both the gold 
standard from Amazon.com and the SVM prediction of our best performing system de-
scribed in Section 5.2). 
RANK(h) 
REVIEW TITLE 
HELPFUL 
VOTES 
UNHELPFUL 
VOTES GOLD 
STANDARD 
SVM 
PREDICTION 
?iPod Moves to All-color Line-up? 215 11 7 1 
?iPod: It's NOT Music to My Ears? 11 13 25 30 
?The best thing I ever bought? 22 32 26 27 
?VERY disappointing? 1 18 40 40 
 
426
out complete products where each of its reviews 
is detected as a duplicate of another product (i.e., 
only one iPod version is retained). 
The filtering of duplicate products and dupli-
cate reviews discarded 85 products and 12,097 
reviews for MP3 Players and 38 products and 
3,692 reviews for Digital Cameras. 
In order to have accurate estimates for the 
helpfulness function in Equation 1, we filtered 
out any review that did not receive at least five 
user ratings (i.e., reviews where less than five 
users voted it as helpful or unhelpful are filtered 
out). This filtering was performed before dupli-
cate detection and discarded 45.7% of the MP3 
Players reviews and 32.7% of the Digital Cam-
eras reviews. 
Table 2 describes statistics for the final data-
sets after the filtering steps. 10% of products for 
both datasets were withheld as development cor-
pora and the remaining 90% were randomly 
sorted into 10 sets for 10-fold cross validation. 
SVM Regression 
For our regression model, we deployed the state 
of the art SVM regression tool SVMlight 
(Joachims 1999). We tested on the development 
sets various kernels including linear, polynomial 
(degrees 2, 3, and 4), and radial basis function 
(RBF). The best performing kernel was RBF and 
we report only these results in this paper (per-
formance was measured using Spearman?s corre-
lation coefficient, described in Section 5.2). 
We tuned the RBF kernel parameters C (the 
penalty parameter) and ? (the kernel width hy-
perparameter) performing full grid search over 
the 110 combinations of exponentially spaced 
parameter pairs (C,?) following (Hsu et al 2003). 
Feature Extraction 
To extract the features described in Section 3.2, 
we preprocessed each review using the Minipar 
dependency parser (Lin 1994). We used the 
parser tokenization, sentence breaker, and syn-
tactic categorizations to generate the Length, 
Sentential, Unigram, Bigram, and Syntax feature 
sets. 
In order to count the occurrences of product 
features for the Product-Feature set, we devel-
oped an automatic way of mining references to 
product features from Epinions.com. On this 
website, user-generated product reviews include 
explicit lists of pros and cons, describing the best 
and worst aspects of a product. For example, for 
MP3 players, we found the pro ?belt clip? and 
the con ?Useless FM tuner?. Our assumption is 
that the pro/con lists tend to contain references to 
the product features that are important to cus-
tomers, and hence their occurrence in a review 
may correlate with review helpfulness. We fil-
tered out all single-word entries which were in-
frequently seen (e.g., hold, ever). After splitting 
and filtering the pro/con lists, we were left with a 
total of 9,110 unique features for MP3 Players 
and 13,991 unique features for Digital Cameras. 
The Stars feature set was created directly from 
the star ratings given by each author of an Ama-
zon.com review. 
For each feature measurement f, we applied 
the following standard transformation: 
 ( )1ln +f  
and then scaled each feature between [0, 1] as 
suggested in (Hsu et al 2003). 
We experimented with various combinations 
of feature sets. Our results tables use the abbre-
viations presented in Section 3.2. For brevity, we 
report the combinations which contributed to our 
best performing system and those that help assess 
the power of the different feature classes in cap-
turing helpfulness. 
5.2 Ranking Performance 
Evaluating the quality of a particular ranking is 
difficult since certain ranking intervals can be 
more important than others (e.g., top-10 versus 
bottom-10) We adopt the Spearman correlation 
coefficient ? (Spearman 1904) since it is the 
most commonly used measure of correlation be-
tween two sets of ranked data points4. 
For each fold in our 10-fold cross-validation 
experiments, we trained our SVM system using 9 
folds. For the remaining test fold, we ranked each 
product?s reviews according to the SVM predic-
tion (described in Section 4) and computed the ? 
                                                     
4 We used the version of Spearman?s correlation coeffi-
cient that allows for ties in rankings. See Siegel and Cas-
tellan (1988) for more on alternate rank statistics such as 
Kendall?s tau. 
Table 2. Overview of filtered datasets extracted 
from Amazon.com. 
 MP3 PLAYERS 
DIGITAL 
CAMERAS 
Total Products 736 1066 
Total Reviews 11,374 14,467 
Average Reviews/Product 15.4 13.6 
Min/MaxReviews/Product 1 / 375 1 / 168 
 
427
correlation between the ranking and the gold 
standard ranking from the test fold5. 
Although our task definition is to learn review 
rankings according to helpfulness, as an interme-
diate step the SVM system learns to predict the 
absolute helpfulness score for each review. To 
test the correlation of this score against the gold 
standard, we computed the standard Pearson cor-
relation coefficient. 
Results show that the highest performing fea-
ture combination consisted of the Length, the 
Unigram, and the Stars feature sets. Table 3 re-
ports the evaluation results for every combination 
of these features with 95% confidence bounds. 
Of the three features alone, neither was statisti-
cally more significant than the others. Examining 
each pair combination, only the combination of 
length with stars outperformed the others. Sur-
prisingly, adding unigram features to this combi-
nation had little effect for the MP3 Players. 
Given our list of features defined in Section 
3.2, helpfulness of reviews is best captured with 
a combination of the Length and Stars features. 
Training an RBF-kernel SVM regression model 
does not necessarily make clear the exact rela-
tionship between input and output variables. To 
investigate this relationship between length and 
helpfulness, we inspected their Pearson correla-
tion coefficient, which was 0.45. Users indeed 
tend to find short reviews less helpful than longer 
ones: out of the 5,247 reviews for MP3 Players 
that contained more than 1000 characters, the 
average gold standard helpfulness score was 
82%; the 204 reviews with fewer than 100 char-
acters had on average a score of 23%. The ex-
plicit product rating, such as Stars is also an 
                                                     
5 Recall that the gold standard is extracted directly from 
user helpfulness votes on Amazon.com (see Section 4). 
indicator of review helpfulness, with a Pearson 
correlation coefficient of 0.48. 
The low Pearson correlations of Table 3 com-
pared to the Spearman correlations suggest that 
we can learn the ranking without perfectly learn-
ing the function itself. To investigate this, we 
tested the ability of SVM regression to recover 
the target helpfulness score, given the score itself 
as the only feature. The Spearman correlation for 
this test was a perfect 1.0. Interestingly, the Pear-
son correlation was only 0.798, suggesting that 
the RBF kernel does learn the helpfulness rank-
ing without learning the function exactly. 
5.3 Results Analysis 
Table 3 shows only the feature combinations of 
our highest performing system. In Table 4, we 
report several other feature combinations to show 
why we selected certain features and what was 
the effect of our five feature classes presented in 
Section 3.2. 
In the first block of six feature combinations in 
Table 4, we show that the unigram features out-
perform the bigram features, which seem to be 
suffering from the data sparsity of the short re-
views. Also, unigram features seem to subsume 
the information carried in our semantic features 
Product-Feature (PRF) and General-Inquirer 
(GIW). Although both PRF and GIW perform 
well as standalone features, when combined with 
unigrams there is little performance difference 
(for MP3 Players we see a small but insignificant 
decrease in performance whereas for Digital 
Cameras we see a small but insignificant im-
provement). Recall that PRF and GIW are simply 
subsets of review words that are found to be 
product features or sentiment words. The learn-
ing algorithm seems to discover on its own which 
Table 3. Evaluation of the feature combinations that make up our best performing system 
(in bold), for ranking reviews of Amazon.com MP3 Players and Digital Cameras accord-
ing to helpfulness. 
MP3 PLAYERS DIGITAL CAMERAS 
FEATURE COMBINATIONS 
SPEARMAN? PEARSON? SPEARMAN? PEARSON? 
LEN 0.575 ? 0.037 0.391 ? 0.038 0.521 ? 0.029 0.357 ? 0.029 
UGR 0.593 ? 0.036 0.398 ? 0.038 0.499 ? 0.025 0.328 ? 0.029 
STR1 0.589 ? 0.034 0.326 ? 0.038 0.507 ? 0.029 0.266 ? 0.030 
UGR+STR1 0.644 ? 0.033 0.436 ? 0.038 0.490 ? 0.032 0.324 ? 0.032 
LEN+UGR 0.582 ? 0.036 0.401 ? 0.038 0.553 ? 0.028 0.394 ? 0.029 
LEN+STR1 0.652 ? 0.033 0.470 ? 0.038 0.577 ? 0.029 0.423 ? 0.031 
LEN+UGR+STR1 0.656 ? 0.033 0.476 ? 0.038 0.595 ? 0.028 0.442 ? 0.031 
LEN=Length; UGR=Unigram; STR=Stars 
?95% confidence bounds are calculated using 10-fold cross-validation. 
428
words are most important in a review and does 
not use additional knowledge about the meaning 
of the words (at least not the semantics contained 
in PRF and GIW). 
We tested two different versions of the Stars 
feature: i) the number of star ratings, STR1; and 
ii) the difference between the star rating and the 
average rating of the review, STR2. The second 
block of feature combinations in Table 4 shows 
that neither is significantly better than the other 
so we chose STR1 for our best performing sys-
tem. 
Our experiments also revealed that our struc-
tural features Sentential and HTML, as well as 
our syntactic features, Syntax, did not show any 
significant improvement in system performance. 
In the last block of feature combinations in Table 
4, we report the performance of our best per-
forming features (Length, Unigram, and Stars) 
along with these other features. Though none of 
the features cause a performance deterioration, 
neither of them significantly improves perform-
ance. 
5.4 Discussion 
In this section, we discuss the broader implica-
tions and potential impacts of our work, and pos-
sible connections with other research directions. 
The usefulness of the Stars feature for deter-
mining review helpfulness suggests the need for 
developing automatic methods for assessing pro-
duct ratings, e.g., (Pang and Lee 2005).  
Our findings focus on predictors of helpful-
ness of reviews of tangible consumer products 
(consumer electronics). Helpfulness is also solic-
ited and tracked for reviews of many other types 
of entities: restaurants (citysearch.com), films 
(imdb.com), reviews of open-source software 
modules (cpanratings.perl.org), and countless 
others. Our findings of the importance of Length, 
Unigrams, and Stars may provide the basis of 
comparison for assessing helpfulness of reviews 
of other entity types. 
Our work represents an initial step in assessing 
helpfulness. In the future, we plan to investigate 
other possible indicators of helpfulness such as a 
reviewer?s reputation, the use of comparatives 
(e.g., more and better than), and references to 
other products. 
Taken further, this work may have interesting 
connections to work on personalization, social 
networks, and recommender systems, for in-
stance by identifying the reviews that a particular 
user would find helpful.  
Our work on helpfulness of reviews also has 
potential applications to work on automatic gen-
Table 4. Performance evaluation of various feature combinations for ranking reviews of MP3 Players 
and Digital Cameras on Amazon.com according to helpfulness. The first six lines suggest that uni-
grams subsume the semantic features; the next two support the use of the raw counts of product ratings 
(stars) rather than the distance of this count from the average rating; the final six investigate the impor-
tance of auxiliary feature sets.  
MP3 PLAYERS DIGITAL CAMERAS 
FEATURE COMBINATIONS 
SPEARMAN? PEARSON? SPEARMAN? PEARSON? 
UGR 0.593 ? 0.036 0.398 ? 0.038 0.499 ? 0.025 0.328 ? 0.029 
BGR 0.499 ? 0.040 0.293 ? 0.038 0.434 ? 0.032 0.242 ? 0.029 
PRF 0.591? 0.037 0.400 ? 0.039 0.527 ? 0.030 0.316 ? 0.028 
GIW 0.571 ? 0.036 0.381 ? 0.038 0.524 ? 0.030 0.333 ? 0.028 
UGR+PRF 0.570 ? 0.037 0.375 ? 0.038 0.546 ? 0.029 0.348 ? 0.028 
UGR+GIW 0.554 ? 0.037 0.358 ? 0.038 0.568 ? 0.031 0.324 ? 0.029 
STR1 0.589 ? 0.034 0.326 ? 0.038 0.507 ? 0.029 0.266 ? 0.030 
STR2 0.556 ? 0.032 0.303 ? 0.038 0.504 ? 0.027 0.229 ? 0.027 
LEN+UGR+STR1 0.656 ? 0.033 0.476 ? 0.038 0.595 ? 0.028 0.442 ? 0.031 
LEN+UGR+STR1+SEN 0.653 ? 0.033 0.470 ? 0.038 0.599 ? 0.028 0.448 ? 0.030 
LEN+UGR+STR1+HTM 0.640 ? 0.035 0.459 ? 0.039 0.594 ? 0.028 0.442 ? 0.031 
LEN+UGR+STR1+SYN 0.645 ? 0.034 0.469 ? 0.039 0.595 ? 0.028 0.447 ? 0.030 
LEN+UGR+STR1+SEN+HTM+SYN 0.631 ? 0.035 0.453 ? 0.039 0.600 ? 0.028 0.452 ? 0.030 
LEN+UGR+STR1+SEN+HTM+SYN+PRF+GIW 0.601 ? 0.035 0.396 ? 0.038 0.604 ? 0.027 0.460 ? 0.030 
LEN=Length; SEN=Sentential; HTM=HTML; UGR=Unigram; BGR=Bigram; 
SYN=Syntax; PRF=Product-Feature; GIW=General-Inquirer; STR=Stars 
?95% confidence bounds are calculated using 10-fold cross-validation. 
429
eration of review information, by providing a 
way to assess helpfulness of automatically gener-
ated reviews. Work on generation of reviews in-
cludes review summarization and extraction of 
useful reviews from blogs and other mixed texts. 
6 Conclusions 
Ranking reviews according to user helpfulness is 
an important problem for many online sites such 
as Amazon.com and Ebay.com. To date, most 
websites measure helpfulness by having users 
manually assess how helpful each review is to 
them. In this paper, we proposed an algorithm for 
automatically assessing helpfulness and ranking 
reviews according to it. Exploiting the multitude 
of user-rated reviews on Amazon.com, we 
trained an SVM regression system to learn a 
helpfulness function and then applied it to rank 
unlabeled reviews. Our best system achieved 
Spearman correlation coefficient scores of 0.656 
and 0.604 against a gold standard for MP3 play-
ers and digital cameras. 
We also performed a detailed analysis of dif-
ferent features to study the importance of several 
feature classes in capturing helpfulness. We 
found that the most useful features were the 
length of the review, its unigrams, and its product 
rating. Semantic features like mentions of prod-
uct features and sentiment words seemed to be 
subsumed by the simple unigram features. Struc-
tural features (other than length) and syntactic 
features had no significant impact. 
It is our hope through this work to shed some 
light onto what people find helpful in user-
supplied reviews and, by automatically ranking 
them, to ultimately enhance user experience. 
References 
Attali, Y. and Burstein, J. 2006. Automated Essay 
Scoring With e-rater? V.2. Journal of Technology, 
Learning, and Assessment, 4(3). 
Burstein, J., Chodorow, M., and Leacock, C. 2004. 
Automated essay evaluation: the criterion online 
writing service. AI Magazine. 25(3), pp 27?36.  
Drucker,H., Wu,D. and Vapnik,V. 1999. Support vector 
machines for spam categorization. IEEE Trans. 
Neural Netw., 10, 1048?1054. 
Gabrilovich, E. and Markovitch, S. 2005. 
Feature Generation for Text Categorization Using 
World Knowledge. In Proceedings of IJCAI-2005. 
Hsu, C.-W.; Chang, C.-C.; and Lin, C.-J. 2003. A 
practical guide to SVM classification. Technical 
report, Department of Computer Science and 
Information Technology, National Taiwan University. 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. KDD?04. pp.168 ? 177 
Kim, S. and Hovy, E. 2004. Determining the Sentiment 
of Opinions. Proceedings of COLING-04. 
Joachims, T. 1999. Making Large-Scale SVM Learning 
Practical. In B. Sch?lkopf, C. Burges, and A. Smola 
(eds), Advances in Kernel Methods: Support Vector 
Learning. MIT Press. Cambridge, MA. 
Joachims, T. 2002. Optimizing Search Engines Using 
Clickthrough Data. In Proceedings of ACM KDD-02.  
Moschitti, A. and Basili R. 2004. Complex Linguistic 
Features for Text Classification: A Comprehensive 
Study. In Proceedings of ECIR 2004. Sunderland, 
U.K. 
Pang, B, L. Lee, and S. Vaithyanathan. 2001. Thumbs 
up? Sentiment Classification using Machine Learning 
Techniques.  Proceedings of EMNLP 2002. 
Pang, B. and Lee, L. 2005. Seeing stars: Exploiting class 
relationships for sentiment categorization with respect 
to rating scales. In Proceedings of the ACL, 2005. 
Riloff , E. and J. Wiebe. 2003. Learning Extraction 
Patterns for Subjective Expressions. In Proc. of 
EMNLP-03. 
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning 
Subjective Nouns Using Extraction Pattern 
Bootstrapping. Proceedings of CoNLL-03 
Rose, C., Roque, A., Bhembe, D., and Vanlehn, K. 2003. 
A Hybrid Text Classification Approach for Analysis 
of Student Essays. In Proc. of the HLT-NAACL, 2003. 
Salton, G. and McGill, M. J. 1983. Introduction to 
Modern Information Retrieval. McGraw Hill. 
Siegel, S. and Castellan, N.J. Jr. 1988. Nonparametric 
Statistics for the Behavioral Sciences. McGraw-Hill. 
Spearman C. 1904. The Proof and Measurement of 
Association Between Two Things. American Journal 
of Psychology, 15:72?101. 
Turney, P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. Proceedings of the 40th 
Annual Meeting of the ACL, Philadelphia, 417?424. 
Vapnik, V.N. 1995. The Nature of Statistical Learning 
Theory. Springer.  
Wiebe, J, R. Bruce, and T. O?Hara. 1999. Development 
and use of a gold standard data set for subjectivity 
classifications. Proc. of the 37th Annual Meeting of the 
Association for Computational Linguistics(ACL-99), 
246?253. 
Yu, H. and Hatzivassiloglou, V. 2003. Towards 
Answering Opinion Questions: Separating Facts from 
Opinions and Identifying the Polarity of Opinion 
Sentences. Proceedings of EMNLP 2003.
  
430
A Bootstrapping Algorithm for  
Automatically Harvesting Semantic Relations 
Marco Pennacchiotti 
Department of Computer Science 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
pantel@isi.edu 
Abstract 
In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a 
web-based knowledge expansion technique, for extracting binary semantic relations. Given a 
small set of seed instances for a particular relation, the system learns lexical patterns, applies 
them to extract new instances, and then uses the Web to filter and expand the instances. 
Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of 
semantic relations when compared with two state of the art systems. 
1. Introduction 
Recent attention to knowledge-rich problems such as question answering [18] and textual 
entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop 
algorithms for automatically harvesting shallow semantic resources. With seemingly endless 
amounts of textual data at our disposal, we have a tremendous opportunity to automatically 
grow semantic term banks and ontological resources. Methods must be accurate, adaptable 
and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), 
and independent or weakly dependent on human supervision. 
In this paper we present Espresso, a novel bootstrapping algorithm for automatically 
harvesting semantic relations, aiming at effectively supporting NLP applications, 
emphasizing two major points that have been partially neglected by previous systems: 
generality and weak supervision. 
From the one side, Espresso is intended as a general-purpose system able to extract a wide 
variety of binary semantic relations, from the classical is-a and part-of relations, to more 
specific and domain oriented ones like chemical reactants in a chemistry domain and position 
succession in political texts. The system architecture is designed with generality in mind, 
avoiding any relation-specific inference technique. Indeed, for each semantic relation, the 
system builds specific lexical patterns inferred from textual corpora. 
From the other side, Espresso requires only weak human supervision. In order to start the 
extraction process, a user provides only a small set of seed instances of a target relation (e.g. 
Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed 
instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger 
set is required. To guarantee weakest supervision, Espresso combines its bootstrapping 
approach with a web-based knowledge expansion technique and linguistic analysis, 
exploiting the seeds as much as possible. 
2. Relevant Work 
To date, most research on lexical relation harvesting has focused on is-a and part-of relations. 
Approaches fall into two main categories: pattern- and clustering-based. 
Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract 
hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a 
bootstrapping algorithm to learn more patterns from instances, which has served as the model 
for most subsequent pattern-based algorithms. 
Berland and Charniak [1] propose a system for part-of relation extraction, based on the 
Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used 
to extract new instances, ranked according to various statistical measures. While this study 
introduces statistical measures to evaluate instance reliability, it remains vulnerable to data 
sparseness and has the limitation of taking into consideration only one-word terms. 
Improving upon Berland and Charniak [1], Girju et al [11] employ machine learning 
algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP?s part-
NP]. This study is the first extensive attempt to solve the problem of generic relational 
patterns, that is, those expressive patterns that have high recall while suffering low precision, 
as they subsume a large set of instances. In order to discard incorrect instances, Girju et al 
learn WordNet-based selectional restrictions, like [whole-NP(scene#4)?s part-NP(movie#1)]. 
While making huge grounds on improving precision/recall, the system requires heavy 
supervision through manual semantic annotations. 
Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to 
terabytes of data. A simple and effective algorithm is proposed to infer surface patterns from 
a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. 
The frequencies of the substrings in the corpus are then used to retain the best patterns. The 
approach gives good results on specific relations such as birthdates, however it has low 
precision on generic ones like is-a and part-of. Pantel et al [17] proposed a similar, highly 
scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing 
both good performances and efficiency. Espresso uses a similar approach to infer patterns, 
but we then apply refining techniques to deal with various types of relations. 
Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic 
method for discovering similar words using a few seed examples by using pattern-based 
techniques and human supervision, KnowItAll [7] that performs large-scale extraction of 
facts from the Web, Mann [15] and Fleischman et al [9] who used part of speech patterns to 
extract a subset of is-a relations involving proper nouns, and Downey et al [6] who 
formalized the problem of relation extraction in a coherent and effective combinatorial model 
that is shown to outperform previous probabilistic frameworks. 
Clustering approaches to relation extraction are less common and have insofar been applied 
only to is-a extraction. These methods employ clustering algorithms to group words 
according to their meanings in text, label the clusters using its members? lexical or syntactic 
dependencies, and then extract an is-a relation between each cluster member and the cluster 
label. Caraballo [3] proposed the first attempt, which used conjunction and apposition 
features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this 
approach by making use of all syntactic dependency features for each noun. The advantage of 
clustering approaches is that they permit algorithms to identify is-a relations that do not 
explicitly appear in text, however they generally fail to produce coherent clusters from fewer 
than 100 million words; hence they are unreliable for small corpora. 
3. The Espresso Algorithm 
The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a 
specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of 
seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the 
relation at hand (e.g., Pablo Picasso is-a artist). Starting from these seeds, the algorithm 
begins a four-phase loop. In the first phase, the algorithm infers a set of patterns P that 
captures as many of the seed instances as possible in C. In the second phase, we define a 
reliability measure to select the best set of patterns P'?P. In phase three, the patterns in P' are 
used to extract a set of instances I. Finally, in phase four, Espresso scores each instance and 
then selects the best instances I' as input seeds for the next iteration. The algorithm terminates 
when a predefined stopping condition is met (for our preliminary experiments, the stopping 
condition is set according to the size of the corpus). For each induced pattern p and instance i, 
the information theoretic scores, r?(p) and r?(i) respectively, aim to express their reliability. 
Below, Sections 3.2?3.5 describe in detail these different phases of Espresso. 
3.1. Term definition 
Before one can extract relation instances from a corpus, it is necessary to define a 
tokenization procedure for extracting terms. Terms are commonly defined as surface 
representations of stable and key domain concepts [19]. Defining regular expressions over 
POS-tagged corpora is the most commonly used technique to both define and extract terms. 
We adopt a slightly modified version of the term definition given in [13], as it is one of the 
most commonly used in the literature: 
 ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun 
We operationally extend the definition of Adj to include present and past participles as most 
noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus, 
unlike many approaches for automatic relation extraction, we allow complex multi-word 
terms as anchor points. Hence, we can capture relations between complex terms, such as 
?record of a criminal conviction? part-of ?FBI report?. 
3.2. Phase 1: Pattern discovery 
The pattern discovery phase takes as input a set of instances I' and produces as output a set of 
lexical patterns P. For the first iteration I' = Is, the set of initial seeds. In order to induce P, we 
apply a slight modification to the approach presented in [20]. For each input instance i = {x, 
y}, we first retrieve all sentences Sx,y containing the two terms x and y. Sentences are then 
generalized into a set of new sentences SGx,y by replacing all terminological expressions by a 
terminological label (TR). For example: 
 ?Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN and/CC x is/VBZ a/DT y? 
is generalized as: 
 ?Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y? 
All substrings linking terms x and y are then extracted from the set SGx,y, and overall 
frequencies are computed. The most frequent substrings then represent the set of new patterns 
P, where the frequency cutoff is experimentally set. Term generalization is particularly useful 
for small corpora, where generalization is vital to ease the data sparseness. However, the 
generalized patterns are naturally less precise. Hence, when dealing with bigger corpora, the 
system allows the use of Sx,y?SGx,y in order to extract substrings. For our experiments, we 
used the set SGx,y . 
3.3. Phase 2: Pattern filtering 
In this phase, Espresso selects among the patterns P those that are most reliable. Intuitively, a 
reliable pattern is one that is both highly precise and one that extracts many instances. The 
recall of a pattern p can be approximated by the fraction of input instances in I' that are 
extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are 
weary of keeping patterns that generate many instances (i.e., patterns that generate high recall 
but potentially disastrous precision). We thus prefer patterns that are highly associated with 
the input patterns I'. Pointwise mutual information [4] is a commonly used metric for 
measuring the strength of association between two events x and y: 
 ( ) ( )( ) ( )yPxP
yxP
yxpmi
,
log, =  
We define the reliability of a pattern p, r?(p), as its average strength of association across 
each input instance i in I', weighted by the reliability of each instance i: 
 ( )
( )
I
ir
pipmi
pr
Ii pmi
?
???
?
???
? ?
=
?
??
?
?
max
),(
 
where r?(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum 
pointwise mutual information between all patterns and all instances. r?(p) ranges from [0,1]. 
The reliability of the manually supplied seed instances are r?(i) = 1. The pointwise mutual in-
formation between instance i = {x, y} and pattern p is estimated using the following formula: 
 ( )
,**,,*,
,,
log,
pyx
ypx
pipmi =  
where |x, p, y| is the frequency of pattern p instantiated with terms x and y and where the 
asterisk (*) represents a wildcard. A well-known problem is that pointwise mutual 
information is biased towards infrequent events. To address this, we multiply pmi(i, p) with 
the discounting factor suggested in [16]. 
The set of highest n scoring patterns P', according to r?(p), are then selected and retained for 
the next phase, where n is the number of patterns of the previous iteration incremented by 1. 
In general, we expect that the set of patterns is formed by those of the previous iteration plus 
a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was 
previously discovered. 
Moreover, to further discourage too generic patterns that might have low precision, a 
threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than 
t instances are then discarded, no matter what their score is. In this paper, we experimentally 
set t to a value dependent on the size of the corpus. In future work, this parameter can be 
learned using a development corpus. 
Our reliability measure ensures that overly generic patterns, which may potentially have very 
low precision, are discarded. However, we are currently exploring a web-expansion algorithm 
that could both help detect generic patterns and also filter out their incorrect instances. We 
estimate the precision of the instance set generated by a new pattern p by looking at the 
number of these instances that are instantiated on the Web by previously accepted patterns. 
Generic patterns will generate instances with higher Web counts than incorrect patterns. 
Then, the Web counts can also be used to filter out incorrect instances from the generic 
patterns? instantiations. More details are discussed in Section 4.3. 
3.4. Phase 3: Instance discovery 
In this phase, Espresso retrieves from the corpus the set of instances I that match any of the 
lexical patterns in P'. 
In small corpora, the number of extracted instances can be too low to guarantee sufficient 
statistical evidence for the pattern discovery phase of the next iteration. In such cases, the 
system enters a web expansion phase, in which new instances for the given patterns are 
retrieved from the Web, using the Google search engine. Specifically, for each instance i? I, 
the system creates a set of queries, using each pattern in P' with its y term instantiated with i?s 
y term. For example, given the instance ?Italy ; country? and the pattern [Y such as X] , the 
resulting Google query will be ?country such as *?. New instances are then created from the 
retrieved Web results (e.g. ?Canada ; country?) and added to I. We are currently exploring 
filtering mechanisms to avoid retrieving too much noise. 
Moreover, to cope with data sparsity, a syntactic expansion phase is also carried out. A set of 
new instances is created for each instance i? I by extracting sub-terminological expressions 
from x corresponding to the syntactic head of terms. For example, expanding the relation 
?new record of a criminal conviction? part-of ?FBI report?, the following new instances are 
obtained: ?new record? part-of ?FBI report?, and ?record? part-of ?FBI report?. 
3.5. Phase 4: Instance filtering 
Estimating the reliability of an instance is similar to estimating the reliability of a pattern. 
Intuitively, a reliable instance is one that is highly associated with as many reliable patterns 
as possible (i.e., we have more confidence in an instance when multiple reliable patterns 
instantiate it.) Hence, analogous to our pattern reliability measure in Section 3.3, we define 
the reliability of an instance i, r?(i), as: 
 ( )
( )
P
pr
pipmi
ir Pp pmi?
?
=
?
??
?
?
max
),(
 
where r?(p) is the reliability of pattern p (defined in Section 3.3) and maxpmi is the maximum 
pointwise mutual information between all patterns and all instances, as in Section 3.3. 
Espresso finally selects the highest scoring m instances, I', and retains them as input for the 
subsequent iteration. In this paper, we experimentally set m = 200. 
4. Experimental Results 
4.1. Experimental Setup 
In this section, we present a preliminary comparison of Espresso with two state of the art 
systems on the task of extracting various semantic relations. 
4.1.1. Datasets 
We perform our experiments using the following two datasets: 
? TREC-9: This dataset consists of a sample of articles from the Aquaint (TREC-9) 
newswire text collection. The sample consists of 5,951,432 words extracted from the 
following data files: AP890101 ? AP890131, AP890201 ? AP890228, and AP890310 
? AP890319. 
? CHEM: This small dataset of 313,590 words consists of a college level textbook of 
introductory chemistry [2]. 
We preprocess the corpora using the Alembic Workbench POS-tagger [5]. 
4.1.2. Systems 
We compare the results of Espresso with the following two state of the art extraction 
systems: 
? RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction 
patterns from a set of seed instances of a particular relation (see Section 2.) 
? PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first 
automatically induces concepts (clusters) from a raw corpus, names the concepts, and 
then extracts an is-a relation between each cluster member and its cluster label. For 
each cluster member, the system may generate multiple possible is-a relations, but in 
this evaluation we only keep the highest scoring one. To apply this algorithm, both 
datasets were first analyzed using the Minipar parser [14]. 
? ESP: This is the algorithm described in this paper (details in Section 3). 
4.1.3. Semantic Relations 
Espresso is designed to extract various semantic relations exemplified by a given small set of 
seed instances. For our preliminary evaluation, we consider the standard is-a and part-of 
relations as well as three novel relations: 
? succession: This relation indicates that one proper noun succeeds another in a position 
or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI 
succeeded Pope John Paul II. We evaluate this relation on the TREC-9 corpus. 
? reaction: This relation occurs between chemical elements/molecules that can be 
combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas 
and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM 
corpus. 
? production: This relation occurs when a process or element/object produces a result. 
For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM 
corpus. 
For each semantic relation, we manually extracted a set of seed examples. The seeds were 
used for both Espresso as well as RH021. Table 1 lists a sample of the seeds as well as sample 
outputs from Espresso. 
4.2. Precision and Recall 
We implemented each of the three systems outlined in Section 4.1.2 and applied them to the 
TREC and CHEM datasets. For each output set, per relation, we evaluate the precision of the 
system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 
                                                 
1 PR04 does not require any seeds. 
CHEM corpus) and evaluating their quality manually using one human judge2. For each 
instance, the judge may assign a score of 1 for correct, 0 for incorrect, and ? for partially 
correct. Example instances that were judged partially correct include ?analyst is-a manager? 
and ?pilot is-a teacher?. The precision for a given set of relation instances is the sum of the 
judge?s scores divided by the number of instances. 
Although knowing the total number of instances of a particular relation in any non-trivial 
corpus is impossible, it is possible to compute the recall of a system relative to another 
system?s recall. The recall of a system A, RA, is given by the following formula: 
 C
C
R AA =
 
where CA is the number of correct instances of a particular relation extracted by A and C is 
the total number of correct instances in the corpus. Following [17], we define the relative 
recall of system A given system B, RA|B, as: 
 
BP
AP
C
C
R
R
R
B
A
B
A
B
A
BA ?
?===|  
Using the precision estimates, PA, from our precision experiments, we can estimate CA ? PA ? 
|A|, where A is the total number of instances of a particular relation discovered by system A. 
                                                 
2 In future work, we will perform this evaluation using multiple judges in order to obtain confidence bounds and 
agreement scores. 
Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The 
number in the parentheses for each relation denotes the total number of seeds. 
  SEEDS ESP 
Is-a (12) 
wheat :: crop 
George Wendt :: star 
Miami :: city 
shark :: predator 
Picasso :: artist 
tax :: charge 
drug dealers :: felons 
Italy :: country 
Part-Of (12) 
leader :: panel 
city :: region 
plastic :: explosive 
United States :: alliance 
shield :: nuclear missile 
biblical quotations :: book 
trees :: land 
material :: FBI report 
T
R
E
C
9 
Succession (12) 
Khrushchev :: Stalin 
Carla Hills :: Yeutter 
George Bush :: Ronald Reagan 
Julio Barbosa de Aquino :: Mendes 
Ford :: Nixon 
Setrakian :: John Griesemer 
Camero Cardiel :: Camacho 
Susan Weiss :: editor 
Is-a (12) 
NaCl :: ionic compounds 
diborane :: substance 
nitrogen :: element 
gold :: precious metal 
Na :: element 
protein :: biopolymer 
HCl :: strong acid 
electromagnetic radiation :: energy 
Part-Of (12) 
ion :: matter 
oxygen :: water 
light particle :: gas 
element :: substance 
oxygen :: air 
powdered zinc metal :: battery 
atom :: molecule 
ethylene glycol :: automotive antifreeze 
Reaction (13) 
magnesium :: oxygen 
hydrazine :: water 
aluminum metal :: oxygen 
lithium metal :: fluorine gas 
hydrogen :: oxygen 
Ni :: HCl 
carbon dioxide :: methane 
boron :: fluorine 
C
H
E
M 
Production (14) 
bright flame :: flares 
hydrogen :: solid metal hydrides 
ammonia :: nitric oxide 
copper :: brown gas 
electron :: ions 
glycerin :: nitroglycerin 
kidneys :: kidney stones 
ions :: charge 
 
Table 8. System performance on the production
relation on the CHEM dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 197 57.5% 0.80 
ESP 196 72.5% 1.00 
* Precision estimated from 20 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Tables 2 ? 8 reports the total number of 
instances, precision, and relative recall of 
each system on the TREC-9 and CHEM 
corpora. The relative recall is always given in 
relation to the Espresso system. For example, 
in Table 2, RH02 has a relative recall of 5.31 
with Espresso, which means that the RH02 
system output 5.31 times more correct 
relations than Espresso (at a cost of much 
lower precision). Similarly, PR04 has a relative recall of 0.23 with Espresso, which means 
that PR04 outputs 4.35 fewer correct relations than Espresso (also with a smaller precision). 
4.3. Discussion 
Experimental results, for all relations and the two different corpus sizes, show that Espresso 
greatly outperforms the other two methods on precision. However, Espresso fails to match 
the recall level of RH02 in all but the experiment on the production relation. Indeed, the 
filtering of unreliable patterns and instances during the bootstrapping algorithm not only 
discards the patterns that are unrelated to the actual relation, but also patterns that are too 
generic and ambiguous ? hence resulting in a loss of recall. 
As underlined in Section 3.2, the ambiguity of generic patterns often introduces much noise 
in the system (e.g, the pattern [X of Y] can ambiguously refer to a part-of, is-a or possession 
Table 2. System performance on the is-a
relation on the TREC-9 dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 57,525 28.0% 5.31 
PR04 1,504 47.0% 0.23 
ESP 4,154 73.0% 1.00 
* Precision estimated from 50 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 3. System performance on the is-a
relation on the CHEM dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 2556 25.0% 3.76 
PR04 108 40.0% 0.25 
ESP 200 85.0% 1.00 
* Precision estimated from 20 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 4. System performance on the part-of
relation on the TREC-9 dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL?
RH02 12,828 35.0% 42.52 
ESP 132 80.0% 1.00 
* Precision estimated from 50 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 5. System performance on the part-of
relation on the CHEM dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL?
RH02 11,582 33.8% 58.78 
ESP 111 60.0% 1.00 
* Precision estimated from 20 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 6. System performance on the succession
relation on the TREC-9 dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL?
RH02 49,798 2.0% 36.96 
ESP 55 49.0% 1.00 
* Precision estimated from 50 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
Table 7. System performance on the reaction
relation on the CHEM dataset. 
SYSTEM INSTANCES PRECISION* REL RECALL? 
RH02 6,083 30% 53.67 
ESP 40 85% 1.00 
* Precision estimated from 20 randomly sampled instances. 
? Relative recall is given in relation to ESP. 
relation). However, generic patterns, while having low precision, yield a high recall, as also 
reported by [11]. We ran an experiment on the reaction relation, retaining the generic patterns 
produced during Espresso?s selection process. As expected, we obtained 1923 instances 
instead of the 40 reported in Table 7, but precision dropped from 85% to 30%. 
The challenge, then, is to harness the expressive power of the generic patterns whilst 
maintaining the precision of Espresso. We propose the following solution that helps both in 
distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances 
produced by generic patterns. Unlike Girju et al [11] that propose a highly supervised 
machine learning approach based on selectional restriction, ours is an unsupervised method 
based on statistical evidence obtained from the Web. At a given iteration in Espresso, the 
intuition behind our solution is that the Web is large enough that correct instances will be 
instantiated by many of the currently accepted patterns P. Hence, we can distinguish between 
generic patterns and incorrect patterns by inspecting the relative frequency distribution of 
their instances using the patterns in P. More formally, given an instance i produced by a 
generic or incorrect pattern, we count how many times i instantiates on the Web with every 
pattern in P, using Google. The instance i is then considered correct if its web count surpasses 
a given threshold. The pattern in question is accepted as a generic pattern if a sufficient 
number of its instances are considered correct, otherwise it is rejected as an incorrect pattern. 
Although our results in Section 4.2 do not include this algorithm, we performed a small 
experiment by adding an a-posteriori generic pattern recovery phase to Espresso. We tested 
the 7,634 instances extracted by the generic pattern [X of Y] on the CHEM corpus for the 
part-of relation. We randomly sample 200 of these instances and then queried Google for 
these instances using the pattern [X consists of Y]. Manual evaluation of the 25 instances that 
occurred at least once on Google showed 50% precision. Adding these instances to the results 
from Table 5 decreases the system precision from 60% to 51%, but dramatically increases 
Espresso?s recall by a factor of 8.16. Furthermore, it is important to note that there are several 
other generic patterns, like [X?s Y], from which we expect a similar precision of 50% with a 
continual increase of recall. This is a very exciting avenue of further investigation. 
5. Conclusions 
We proposed a weakly supervised bootstrapping algorithm, called Espresso, for 
automatically extracting a wide variety of binary semantic relations from raw text. Given a 
small set of seed instances for a particular relation, the system learns reliable lexical patterns, 
applies them to extract new instances ranked by an information theoretic definition of 
reliability, and then uses the Web to filter and expand the instances. 
There are many avenues of future work. Preliminary results show that Espresso generates 
highly precise relations, but at the expense of lower recall. As mentioned above in Section 
4.3, we are working on improving system recall with a web-based method to identify generic 
patterns and filter their instances. Early results appear very promising. We also plan to 
investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here 
that negative instances will play a key role in determining the selectional restriction on 
generic patterns. 
Espresso is the first system, to our knowledge, to emphasize both minimal supervision and 
generality, both in identification of a wide variety of relations and in extensibility to various 
corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations 
harvested by Espresso, and if these relations can benefit NLP applications such as QA. 
Acknowledgements 
The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for 
evaluating the outputs of the systems. 
References 
[1] Berland, M. and E. Charniak, 1999. Finding parts in very large corpora. In Proceedings of ACL-1999. pp. 
57-64. College Park, MD. 
[2] Brown, T.L.; LeMay, H.E.; Bursten, B.E.; and Burdge, J.R. 2003. Chemistry: The Central Science, Ninth 
Edition. Prentice Hall. 
[3] Caraballo, S. 1999. Automatic acquisition of a hypernym-labeled noun hierarchy from text. In Proceedings 
of ACL-99. pp 120-126, Baltimore, MD. 
[4] Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. John Wiley & Sons. 
[5] Day, D.; Aberdeen, J.; Hirschman, L.; Kozierok, R.; Robinson, P.; and Vilain, M. 1997. Mixed-initiative 
development of language processing systems. In Proceedings of ANLP-1997. Washington D.C. 
[6] Downey, D.; Etzioni, O.; and Soderland, S. 2005. A Probabilistic model of redundancy in information 
extraction. In Proceedings of IJCAI-2005. pp. 1034-1041. Edinburgh, Scotland. 
[7] Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-M.; Shaked, T.; Soderland, S.; Weld, D.S.; and 
Yates, A. 2005. Unsupervised named-entity extraction from the Web: An experimental study. Artificial 
Intelligence, 165(1): 91-134. 
[8] Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press. 
[9] Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline strategies for online question answering: 
Answering questions before they are asked. In Proceedings of ACL-03. pp. 1-7. Sapporo, Japan. 
[10] Geffet, M. and Dagan, I. 2005. The Distributional Inclusion Hypotheses and Lexical Entailment. In 
Proceedings of ACL-2005. Ann Arbor, MI. 
[11] Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning semantic constraints for the automatic 
discovery of part-whole relations. In Proceedings of HLT/NAACL-03. pp. 80-87. Edmonton, Canada. 
[12] Hearst, M. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING-92. pp. 539-545. 
Nantes, France. 
[13] Justeson J.S. and Katz S.M. 1995. Technical Terminology: some linguistic properties and algorithms for 
identification in text. In Proceedings of ICCL-1995. pp.539-545. Nantes, France. 
[14] Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based parser. In Proceedings of COLING-
94. pp. 42-48. Kyoto, Japan. 
[15] Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies for Question Answering. In Proceedings of 
SemaNet? 02: Building and Using Semantic Networks, Taipei, Taiwan. 
[16] Pantel, P. and Ravichandran, D. 2004. Automatically labeling semantic classes. In Proceedings of 
HLT/NAACL-04. pp. 321-328. Boston, MA. 
[17] Pantel, P.; Ravichandran, D.; Hovy, E.H. 2004. Towards terascale knowledge acquisition. In Proceedings of 
COLING-04. pp. 771-777. Geneva, Switzerland. 
[18] Pasca, M. and Harabagiu, S. 2001. The informative role of WordNet in Open-Domain Question Answering. 
In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources. pp. 138-143. Pittsburgh, 
PA. 
[19] Pazienza M.T. 2000. A domain-specific terminology-extraction system. In Terminology, 5:2. 
[20] Ravichandran, D. and Hovy, E.H. 2002. Learning surface text patterns for a question answering system. In 
Proceedings of ACL-2002. pp. 41-47. Philadelphia, PA. 
[21] Riloff, E. and Shepherd, J. 1997. A corpus-based approach for building semantic lexicons. In Proceedings 
of EMNLP-1997. 
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 657?665,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Measuring frame relatedness
Marco Pennacchiotti
Yahoo! Inc.
Santa Clara, CA 95054
pennac@yahoo-inc.com
Michael Wirth
Computational Linguistics
Saarland University, Germany
miwirth@coli.uni-sb.de
Abstract
In this paper we introduce the notion
of ?frame relatedness?, i.e. relatedness
among prototypical situations as repre-
sented in the FrameNet database. We first
demonstrate the cognitive plausibility of
that notion through an annotation experi-
ment, and then propose different types of
computational measures to automatically
assess relatedness. Results show that our
measures provide good performance on
the task of ranking pairs of frames.
1 Introduction
Measuring relatedness among linguistic entities
is a crucial topic in NLP. Automatically assess-
ing the degree of similarity or relatedness be-
tween two words or two expressions, is of great
help in a variety of tasks, such as Question An-
swering, Recognizing Textual Entailment (RTE),
Information Extraction and discourse processing.
Since the very beginning of computational lin-
guistics, many studies have been devoted to the
definition and the implementation of automatic
measures for word relatedness (e.g. (Ruben-
stein and Goodenough, 1965; Resnik, 1995; Lin,
1998; Budanitsky and Hirst, 2006; Mohammad
and Hirst, 2006)). More recently, relatedness
between lexical-syntactic patterns has also been
studied (Lin and Pantel, 2001; Szpektor et al,
2004), to support advanced tasks such as para-
phrasing and RTE. Unfortunately, no attention has
been paid so far to the definition of relatedness at
the more abstract situational level ? i.e. related-
ness between two prototypical actions, events or
state-of-affairs, taken out of context (e.g. the sit-
uations of Killing and Death). A prominent defi-
nition of ?prototypical situation? is given in frame
semantics (Fillmore, 1985), where a situation is
modelled as a conceptual structure (a frame) con-
stituted by the predicates that can evoke the situ-
ation, and the semantic roles expressing the situa-
tion?s participants.
As measures of word relatedness help in discov-
ering if two word occurrences express related con-
cepts, so measures of frame relatedness should
help to discover if two large text fragments are re-
lated or talk about similar situations. Such mea-
sures would be valuable in many tasks. For exam-
ple, consider the following fragment, in the con-
text of discourse processing:
?In the 1950s the Shah initiated Iran ?s nu-
clear research program and developed an ambi-
tious plan to produce 23,000MW from nuclear
power. The program was stopped by the Islamic
Revolution in 1979, but it was revived later in the
decade, when strategic interests began to drive the
nuclear program.?
The underlined words evoke highly related
frames, namely ACTIVITY START, ACTIV-
ITY STOP and CAUSE TO RESUME. This could
suggest to link the three textual fragments associ-
ated to the words, into a single coherent discourse
unit, where the semantic roles of the different
fragments can be easily mapped as co-referential
(e.g. ?Iran?s nuclear research program? - ?The
program? - ?it?). Frame relatedness can also
help in RTE. Consider for example the following
entailment pair:
Text : ?An avalanche has struck a popular skiing
resort in Austria, killing at least 11 people.?
Hypothesis : ?Humans died in an avalanche.?
The frames KILLING and DEATH, respectively
evoked by killing and died, are highly related and
can then be mapped. Leveraging this mapping, an
RTE system could easily discover that the Text en-
tails the Hypothesis, by verifying that the fillers of
the mapped semantic roles of the two frames are
semantically equivalent.
657
In this paper we investigate the notion of re-
latedness in the context of frame semantics, and
propose different types of automatic measures to
compute relatedness between frames. Our main
contributions can be summarized as follows: (1)
We empirically show that the notion of frame re-
latedness is intuitive and principled from a cogni-
tive perspective: to support this claim, we report
agreement results over a pool of human annota-
tors on the task of ranking frame pairs on relat-
edness; (2) We propose a variety of measures for
computing frame relatedness, inspired by differ-
ent approaches and by existing measures for word
relatedness; (3) We show that our measures offer
good performance, thus opening the path to the use
of frame relatedness as a practical tool for NLP,
and showing that measures for word relatedness
can be successfully adapted to frames. The paper
is organized as follows. In Section 2 we summa-
rize related work. In Section 3 we describe the ex-
periment of humans ranking frame pairs, and dis-
cuss the results. In Section 4 and 5 we respectively
introduce our relatedness measures, and test them
over a manual gold standard. In Section 6 we draw
final conclusions and outline future work.
2 Related Work
Much research in NLP has studied similarity and
relatedness between words. Rubenstein and Good-
enough (1965) were the first to propose a pro-
cedure to assess human agreement on ranking
pairs of words on relatedness. Their experi-
ment was later replicated by Resnik (1995) and
Charles (2000). All these studies reported good
levels of agreements among annotators, suggest-
ing that the notion of word relatedness is cogni-
tively principled. In our experiment in Section 3.2
we apply the same procedure to assess agreement
on ranking frames.
Measures for estimating word relatedness have
been systematically proposed since the early 90?s,
and are today widely used in NLP for various
tasks. Most measures can be classified either as
corpus-based or ontology-based. Corpus-based
measures compute relatedness looking at the dis-
tributional properties of the two words: words that
tend to co-occur in the same contexts or having
similar distributional profiles, are deemed to be
highly related. A complete survey on these mea-
sures is reported in (Mohammad and Hirst, 2006).
Ontology-based measures estimate relatedness by
studying the path connecting the two words in an
ontology or a hierarchical lexicon (e.g. WordNet).
The basic idea is that closer words are more related
than distant ones. Budanitsky and Hirst (2006)
provide an extensive survey of these measures.
Budanitsky and Hirst (2006) also point out an
important distinction, between relatedness and
similarity. Two words are related if any type of
relation stands between them, e.g. antonymy or
meronymy; they are similar when related through
an is-a like hierarchy. Similarity is then a spe-
cial case of relatedness. Following Budanitsky and
Hirst (2006), we consider two frames as similar if
they are linked via is-a like relations (e.g. GET-
TING and COMMERCE BUY), while as related if
any relation stands between them (e.g. causation
between KILLING and DEATH). In this paper, we
focus our attention solely on the notion of frame
relatedness.
3 Defining frame relatedness
In this section we check if the notion of frame re-
latedness is intuitive and principled from a cog-
nitive perspective. In Section 3.1 we first intro-
duce the basic concepts or frame semantics; in
Section 3.2 we report the agreement results ob-
tained by human annotators, on the task of ranking
a dataset of frame pairs according to relatedness.
3.1 Frame Semantics and FrameNet
Frame semantics (Fillmore, 1985) seeks to de-
scribe the meaning of a sentence as it is actu-
ally understood by characterizing the background
knowledge necessary to understand the sentence.
Background knowledge is represented in the form
of frames, conceptual structures modelling proto-
typical situations. Linguistically, a frame is a se-
mantic class containing predicates called lexical
units (LU), that can evoke the described situation
(see example in Table 1). Each frame comes with
its own set of semantic roles, called frame ele-
ments (FE). These are the participants and props in
the abstract situation described. Roles are local to
individual frames, thus avoiding the commitment
to a small set of universal roles, whose specifica-
tion has turned out to be unfeasible in the past.
The Berkeley FrameNet project (Baker et al,
1998) has been developing a frame-semantic lexi-
con for the core vocabulary of English since 1997.
The current FrameNet release contains about 800
frames and 10,000 lexical units. Part of FrameNet
658
Frame: STATEMENT
This frame contains verbs and nouns that communicate
the act of a SPEAKER to address a MESSAGE to some
ADDRESSEE using language. A number of the words
can be used performatively, such as declare and insist.
SPEAKER Evelyn said she wanted to leave.
MESSAGE Evelyn announced that she wanted
to leave.
ADDRESSEE Evelyn spoke to me about her past.
TOPIC Evelyn?s statement about her past
F
E
s MEDIUM Evelyn preached to me over the
phone.
L
U
s acknowledge.v, acknowledgment.n, add.v, ad-
dress.v, admission.n, admit.v, affirm.v, affirma-
tion.n, allegation.n, allege.v, announce.v, . . .
Table 1: Example frame from FrameNet.
is also a corpus of annotated example sentences
from the British National Corpus, currently con-
taining 135,000 sentences.
In FrameNet, asymmetric frame relations can
relate two frames, forming a complex hierarchy
(Ruppenhofer et al, 2005): Inheritance: anything
true in the semantics of the parent frame, must
also be true for the other (e.g. KILLING ? EX-
ECUTION). Uses: a part of the situation evoked
by one frame refers to the other. Subframe: one
frame describes a subpart of a complex situation
described in the other (e.g. CRIMINAL-PROCESS
? SENTENCING). Causative of : the action in
one frame causes the event described in the other
(e.g. KILLING ? DEATH). Inchoative of : the
event in one frame ends in the state described in
the other (e.g. DEATH ? DEAD OR ALIVE). Pre-
cedes: one frame temporally proceeds the other
(e.g. FALL ASLEEP ? SLEEP). Perspective on:
one frame describes a specific point-of-view on a
neutral frame.
The first two are is-a like relations, while the
others are non-hierarchical.
3.2 Manually ranking related frames
We asked a pool of human annotators to manually
rank a set of frame pairs according to their relat-
edness. The goal was twofolds. First, we wanted
to check how intuitive the notion of frame related-
ness is, by computing inter-annotator agreement,
and by comparing the agreement results to those
obtained by Rubenstein and Goodenough (1965)
for word relatedness. Second, we planned to use
the produced dataset as a gold standard for test-
ing the relatedness measures, as described in Sec-
tion 5. In the rest of the section we describe the
annotation process in detail.
Dataset creation. We created two different
datasets, a simple and a controlled set, each con-
taining 155 pairs. Frame pairs in the simple
set were randomly selected from the FrameNet
database. Frame pairs in the controlled set were
either composed of two frames belonging to the
same scenario1, or being so that one frame is one
edge from the scenario of the other. This ensured
that all pairs in the controlled set contained seman-
tically related frames. Indeed, we use the con-
trolled set to check if human agreement and au-
tomatic measure accuracy get better when consid-
ering only highly related frames.
Human ranking agreement. A preliminary an-
notation phase involved a group of 15 annotators
consisting of graduate students and researchers,
native or nearly native speakers of English. For
each set, each annotator was given 15 frame pairs
from the original 155 set: 5 of these where shared
with all other annotators. This setting has three
advantages: (1) The set is small enough to obtain
a reliable annotation in a short time; (2) We can
compute the agreement among the 15 annotators
over the shared pairs; (3) We can check the relia-
bility of the final gold standard created in the sec-
ond phase (see following section) by comparing to
the annotations. Each annotator was asked to or-
der a shuffled deck of 15 cards, each one describ-
ing a pair of frames. The card contained the fol-
lowing information about the two frames: names;
definitions; the lists of core FEs; a frame anno-
tated sentence for each frame, randomly chosen
from the FrameNet database. Similarly to Ruben-
stein and Goodenough (1965) we gave the anno-
tators the following instructions: (i) After looking
through the whole deck, order the pairs according
to amount of relatedness; (ii) You may assign the
same rank to pairs having the same degree of re-
latedness (i.e. ties are allowed).
We checked the agreement among the 15 an-
notators in ranking the 5 shared pairs by using
the Kendall?s ? correlation coefficient (Kendall,
1938). Kendall?s ? can be interpreted as the dif-
ference between the probability that in the dataset
two variables are in the same order versus the
probability that they are in different orders (see
(Lapata, 2006) for details). The average corre-
1A scenario frame is a ?hub? frame describing a gen-
eral topic; specific frames modelling situations related to the
topic are linked to it (e.g. COMMERCE BUY and COMMER-
CIAL TRANSACTION are linked to COMMERCE SCENARIO).
FrameNet contains 16 scenarios.
659
lation2 among annotators on the simple and con-
trolled sets was ? = 0.600 and ? = 0.547.
Gold standard ranking. The final dataset was
created by two expert annotators, jointly working
to rank the 155 pairs collected in the data creation
phase. We computed the rank correlation agree-
ment between this annotation and the 15 annota-
tion produced in the first stage. We obtained an av-
erage Kendall?s ? = 0.530 and ? = 0.566 respec-
tively on the simple and controlled sets (Standard
deviations from the average are StdDev = 0.146
and StdDev = 0.173). These results are all statis-
tically significant at the 99% level, indicating that
the notion of ?frame relatedness? is intuitive and
principled for humans, and that the final datasets
are reliable enough to be used as gold standard for
our experiments. Table 2 reports the first and last
5 ranked frame pairs for the two datasets.
We compared the correlation results obtained
above on ?frame relatedness?, to those derived
from previous works on ?word relatedness?. This
comparison should indicate if ranking related
frames (i.e. situations) is more or less complex
and intuitive than ranking words.3 As for words,
we computed the average Kendall?s ? among three
different annotation efforts (namely, (Rubenstein
and Goodenough, 1965; Resnik, 1995; Charles,
2000)) carried out over a same dataset of 28 word
pairs originally created by Rubenstein and Goode-
nough. Note that the annotation schema followed
in the three works is the same as ours. We ob-
tained a Kendall?s ? = 0.775, which is statisti-
cally significant at the 99% level. As expected,
the correlation for word relatedness is higher than
for frames: Humans find it easier to compare two
words than two complex situations, as the former
are less complex linguistic entities than the latter.
4 Measures for frame relatedness
Manually computing relatedness between all pos-
sible frame pairs in FrameNet is an unfeasible
task. The on-going FrameNet project and auto-
matic methods for FrameNet expansion (e.g. (Pen-
2Average correlation is computed by averaging the ? ob-
tained on each pair of annotators, as suggested in (Siegel and
Castellan, 1988); note that the obtained value corresponds
to the Kendall u correlation coefficient. Ties are properly
treated with the correction factor described in (Siegel and
Castellan, 1988).
3The comparison should be taken only as indicative, as
words can be ambiguous while frames are not. A more prin-
cipled comparison should involve word senses, not words.
nacchiotti et al, 2008)) are expected to produce an
ever growing set of frames. The definition of auto-
matic measures for frame relatedness is thus a key
issue. In this section we propose different types of
such measures.
4.1 WordNet-based measures
WordNet-based measures estimate relatedness by
leveraging the WordNet hierarchy. The hypothesis
is that two frames whose sets of LUs are close in
WordNet are likely to be related. We assume that
LUs are sense-tagged, i.e. we know which Word-
Net senses of a LU map to a given frame. For ex-
ample, among the 25 senses of the LU charge.v,
only the sense charge.v#3 (?demand payment?)
maps to the frame COMMERCE COLLECT.
Given a frame F , we define SF as the set
of all WordNet senses that map to any frame?s
LU (e.g. for COMMERCE COLLECT, SF con-
tains charge.v#3, collect.v#4, bill.v#1). A generic
WordNet-based measure is then defined as fol-
lows:
wn(F1, F2) =
?
s1?SF1
?
s2?SF2
wn rel(s1, s2)
|SF1 | ? |SF2 |
(1)
where wn rel(s1, s2) is a sense function estimat-
ing the relatedness between two senses in Word-
Net. Since we focus on frame relatedness, we
are interested in assigning high scores to pairs of
senses which are related by any type of relations
in WordNet (i.e. not limited to is-a). We there-
fore adopt as function wn rel the Hirst-St.Onge
measure (Hirst and St.Onge, 1998) as it accounts
for different relations. We also experiment with
the Jiang and Conrath?s (Jiang and Conrath, 1997)
measure which relies only on the is-a hierarchy,
but proved to be the best WordNet-based mea-
sure in the task of ranking words (Budanitsky
and Hirst, 2006). We call the frame relatedness
measures using the two functions respectively as
wn hso(F1, F2) and wn jcn(F1, F2).
4.2 Corpus-based measures
Corpus-based measures compute relatedness look-
ing at the distributional properties of the two
frames over a corpus. The intuition is that related
frames should occur in the same or similar con-
texts.
660
SIMPLE SET CONTROLLED SET
Measure volume - Measure mass (1) Knot creation - Rope manipulation (1,5)
Communication manner - Statement (2) Shoot projectiles - Use firearm (1,5)
Giving - Sent items (3) Scouring - Scrutiny (3)
Abundance - Measure linear extent (4) Ambient temperature - Temperature (4)
Remembering information - Reporting (5) Fleeing - Escaping (5)
... ...
Research - Immobilization (126) Reason - Taking time (142)
Resurrection - Strictness (126) Rejuvenation - Physical artworks (142)
Social event - Word relations (126) Revenge - Bungling (142)
Social event - Rope manipulation (126) Security - Likelihood (142)
Sole instance - Chatting (126) Sidereal appearance - Aggregate (142)
Table 2: Human gold standard ranking: first and last 5 ranked pairs (in brackets ranks allowing ties).
4.2.1 Co-occurrence measures
Given two frames F1 and F2, the co-occurrence
measure computes relatedness as the pointwise
mutual information (pmi) between them:
pmi(F1, F2) = log2
P (F1, F2)
P (F1)P (F2)
(2)
Given a corpus C consisting of a set of documents
c ? C, we estimate pmi as the number of contexts
in the corpus (either documents or sentences)4 in
which the two frames co-occur:
cr occ(F1, F2) = log2
|CF1,F2 |
|CF1 ||CF2 |
(3)
where CFi is the set of documents in which Fi oc-
curs, and CF1,F2 is the set of documents in which
F1 and F2 co-occur. A frame Fi is said to occur in
a document if at least one of its LUs lFi occurs in
the document, i.e.:
CFi = {c ? C : ?lFi in c} (4)
CF1,F2 = {c ? C : ?lF1 and ?lF2 in c} (5)
A limitation of the above measure is that it does
not treat ambiguity. If a word is a LU of a frame
F , but it occurs in a document with a sense
s /? SF , it still counts as a frame occurrence.
For example, consider the word charge.v, whose
third sense charge.v#3 maps in FrameNet to COM-
MERCE COLLECT. In the sentence: ?Tripp Isen-
hour was charged with killing a hawk on pur-
pose?, charge.v co-occurs with kill.v, which in
FrameNet maps to KILLING. The sentence would
then result as a co-occurrence of the two above
frames. Unfortunately this is not the case, as
the sentence?s sense charge.v#2 does not map to
the frame. Ideally, one could solve the problem
by using a sense-tagged corpus where senses? oc-
currences are mapped to frames. While sense-
to-frame mappings exist (e.g. mapping between
4For sake of simplicity in the rest of the section we refer
to documents, but the same holds for sentences.
frames and WordNet senses in (Shi and Mihal-
cea, 2005)), sense-tagged corpora large enough for
distributional studies are not yet available (e.g.,
the SemCor WordNet-tagged corpus (Miller et al,
1993) consists of only 700,000 words).
We therefore circumvent the problem, by imple-
menting pmi in a weighted co-occurrence mea-
sure, which gives lower weights to co-occurrences
of ambiguous words:
cr wgt(F1, F2) = log2
?
c?CF1,F2
wF1(c) ? wF2(c)
?
c?CF1
wF1(c) ?
?
c?CF2
wF2(c)
(6)
The weighting function wF (c) estimates the
probability that the document c contains a LU
of the frame F in the correct sense. For-
mally, given the set of senses Sl of a LU (e.g.
charge.v#1...charge.v#24), we define SlF as the set
of senses mapping to the frame (e.g. charge.v#3
for COMMERCE COLLECT). The weighting func-
tion is then:
wF (c) = argmax
lF?LF in c
P (SlF |lF ) (7)
where LF is the set of LUs of F . We estimate
P (SlF |lF ) by counting sense occurrences of lF
over the SemCor corpus:
P (SlF |lF ) =
|SlF |
|Sl|
(8)
In other terms, a frame receives a high weight in
a document when the document contains a LU
whose most frequent senses are those mapped to
the frame.5 For example, in the sentence: ?Tripp
Isenhour was charged with killing a hawk on pur-
pose.?, wF (c) = 0.17, as charge.v#3 is not very
frequent in SemCor.
5In Eq.8 we use Lidstone smoothing (Lidstone, 1920) to
account for unseen senses in SemCor. Also, if a LU does not
occur in SemCor, an equal probability (corresponding to the
inverse of the number of word?s senses) is given to all senses.
661
4.2.2 Distributional measure
The previous measures promote (i.e. give a higher
rank to) frames co-occurring in the same con-
texts. The distributional measure promotes frames
occurring in similar contexts. The distributional
hypothesis (Harris, 1964) has been widely and
successfully used in NLP to compute relatedness
among words (Lin, 1998), lexical patterns (Lin
and Pantel, 2001), and other entities. The underly-
ing intuition is that target entities occurring in sim-
ilar contexts are likely to be semantically related.
In our setting, we consider either documents and
sentences as valid contexts.
Each frame F is modelled by a distributional
vector ~F , whose dimensions are documents. The
value of each dimension expresses the association
ratioA(F, c) between a document c and the frame.
We say that a document is highly associated to a
frame when most of the FrameNet LUs it contains,
map to the given frame in the correct senses:
A(F, c) =
?
l?LF in c
P (SlF |lF )
?
Fi?F
?
l?Fi in c
P (SlFi |lFi)
(9)
where F is the set of all FrameNet frames, and
P (SlF |lF ) is as in Eq. 8. We then compute relat-
edness between two frames using cosine similar-
ity:
cr dist(F1, F2) =
~F1 ? ~F2
| ~F1| ? | ~F2|
(10)
When we use sentences as contexts we re-
fer to cr dist sent(F1, F2), otherwise to
cr dist doc(F1, F2)
4.3 Hierarchy-based measures
A third family or relatedness measures leverages
the FrameNet hierarchy. The hierarchy forms a
directed graph of 795 nodes (frames), 1136 edges,
86 roots, 7 islands and 26 independent compo-
nents. Similarly to measures for word related-
ness, we here compute frame relatedness leverag-
ing graph-based measures over the FrameNet hi-
erarchy. The intuition is that the closer in the hier-
archy two frames are, the more related they are6.
We here experiment with the Hirst-St.Onge and
the Wu and Palmer (Wu and Palmer, 1994) mea-
sures, as they are pure taxonomic measures, i.e.
they do not require any corpus statistics.
6The Pathfinder Through FrameNet tool gives a prac-
tical proof of this intuition: http://fnps.coli.
uni-saarland.de/pathsearch.
WU and Palmer: this measure calculates relat-
edness by considering the depths of the two frames
in the hierarchy, along with the depth of their least
common subsumer (LCS):
hr wu(F1, F2) =
2?dp(LCS)
ln(F1, LCS)+ln(F2, LCS)+2?dp(LCS)
(11)
where ln is the length of the path connecting two
frames, and dp is the length of the path between
a frame and a root. If a path does not exist, then
hr wu(F1, F2) = 0.
Hirst-St.Onge: two frames are semantically
close if they are connected in the FrameNet hier-
archy through a ?not too long path which does not
change direction too often?:
hr hso(F1, F2) = M? path length ?k ?d (12)
where M and and k are constants, and d is the
number of changes of direction in the path. If a
path does not exist, hr hso(F1, F2) = 0. For both
measures we consider as valid edges all relations.
The FrameNet hierarchy also provides for each
relation a partial or complete FE mapping between
the two linked frames (for example the role Vic-
tim of KILLING maps to the role Protagonist of
DEATH). We leverage this property implementing
a FE overlap measure, which given the set of FEs
of the two frames, FE1 and FE2 , computes re-
latedness as the percentage of mapped FEs:
hr fe(F1, F2) =
|FE1 ? FE2|
max(|FE1|, |FE2|)
(13)
The intuition is that FE overlap between frames
is a more fine grained and accurate predictor of
relatedness wrt. simple frame relation measures
as those above ? i.e. two frames are highly related
not only if they describe connected situations, but
also if they share many participants.
5 Experiments
We evaluate the relatedness measures by compar-
ing their rankings over the two datasets described
in Section 3.2, using the manual gold standard an-
notation as reference. As evaluation metrics we
use Kendall?s ? . As baselines, we adopt a def-
inition overlap measure that counts the percent-
age of overlapping content words in the definition
of the two frames;7 and a LU overlap baseline
7We use stems of nouns, verbs and adjectives.
662
Measure Simple Set Controlled Set
wn jcn 0.114 0.141
wn hso 0.106 0.141
cr occ sent 0.239 0.340
cr wgt sent 0.281 0.349
cr occ doc 0.143 0.227
cr wgt doc 0.173 0.240
cr dist doc 0.152 0.240
hr wu 0.139 0.286
hr hso 0.134 0.296
hr fe 0.252 0.326
def overlap baseline 0.056 0.210
LU overlap baseline 0.080 0.253
human upper bound 0.530 0.566
Table 3: Kendall?s ? correlation results for differ-
ent measures over the two dataset.
that counts the percentage of overlapping LUs be-
tween the two frames. We also defined as upper-
bound the human agreement over the gold stan-
dard. As regards distributional measures, statis-
tics are drawn from the TREC-2002 Vol.2 cor-
pus, consisting of about 110 million words, orga-
nized in 230,401 news documents and 5,433,048
sentences8. LUs probabilities in Eq. 8 are esti-
mate over the SemCor 2.0 corpus, consisting of
700,000 running words, sense-tagged with Word-
Net 2.0 senses.9. WordNet-based measures are
computed using WordNet 2.0 and implemented
as in (Patwardhan et al, 2003). Mappings be-
tween WordNet senses and FrameNet verbal LUs
are taken from Shi and Mihalcea (2005); as map-
pings for nouns and adjectives are not available,
for the WordNet-based measures we use the first
sense heuristic.
Note that some of the measures we adopt need
some degree of supervision. The WordNet-based
and the cr wgt measures rely on a WordNet-
FrameNet mapping, which has to be created man-
ually or by some reliable automatic technique.
Hierarchy-based measures instead rely on the
FrameNet hierarchy that is also a manual artifact.
5.1 Experimental Results
Table 3 reports the correlation results over the two
datasets. Table 4 reports the best 10 ranks pro-
duced by some of the best performing measures.
Results show that all measures are positively cor-
related with the human gold standard, with a level
8For computational limitations we could not afford exper-
imenting the cr dist sent measure, as the number and size of
the vectors was too big.
9We did not use directly the SemCor for drawing distribu-
tional statistics, because of its small size.
of significance beyond the p < 0.01 level , but
the wn jcn measure which is at p < 0.05. All
measures, but the WordNet-based ones, signifi-
cantly outperform the definition overlap baseline
on both datasets, and most of them also beat the
more informed LU overlap baseline.10 It is in-
teresting to notice that the two best performing
measures, namely cr wgt sent and hr fe, use re-
spectively a distributional and a hierarchy-based
strategy, suggesting that both approaches are valu-
able. WordNet-based measures are less effective,
performing close or below the baselines.
Results obtained on the simple set are in gen-
eral lower than those on the controlled set, sug-
gesting that it is easier to discriminate among pairs
of connected frames than random ones. A possi-
ble explanation is that when frames are connected,
all measures can rely on meaningful evidence for
most of the pairs, while this is not always the case
for random pairs. For example, corpus-based mea-
sures tend to suffer the problem of data sparseness
much more on the simple set, because many of the
pairs are so loosely related that statistical informa-
tion cannot significantly emerge from the corpus.
WordNet-based measures. The low perfor-
mance of these measures is mainly due to the
fact that they fail to predict relatedness for many
pairs, e.g. wn hso assigns zero to 137 and 119
pairs, respectively on the simple and controlled
sets. This is mostly caused by the limited set of
relations of the WordNet database. Most impor-
tantly in our case, WordNet misses the situational
relation (Hirst and St.Onge, 1998), which typi-
cally relates words participating in the same sit-
uation (e.g. child care - school). This is exactly
the relation that would help in mapping frames?
LUs. Another problem relates to adjectives and
adverbs: WordNet measures cannot be trustfully
applied to these part-of-speech, as they are not
hierarchically organized. Unfortunately, 18% of
FrameNet LUs are either adjectives or adverbs,
meaning that such amount of useful information
is lost. Finally, WordNet has in general an incom-
plete lexical coverage: Shi and Mihalcea (2005)
show that 7% of FrameNet verbal LUs do not have
a mapping in WordNet.
Corpus-based measures. Table 3 shows that
co-occurrence measures are effective when using
10The average level of correlation obtained by our mea-
sures is comparable to that obtained in other complex
information-ordering tasks, e.g. measuring compositionality
of verb-noun collations (Venkatapathy and Joshi, 2005)
663
WN JCN CR WGT SENT HR FE
Ambient temperature - Temperature (4) Change of phase - Cause change of phase (7) Shoot projectiles - Use firearm (1,5)
Run risk - Endangering (27) Knot creation - Rope manipulation (1,5) Intentionally affect - Rope manipulation (37,5)
Run risk - Safe situation (51) Ambient temperature - Temperature (4) Knot creation - Rope manipulation (1,5)
Knot creation - Rope manipulation (1,5) Shoot projectiles - Use firearm (1,5) Ambient temperature - Temperature (4)
Endangering - Safe situation (62) Hit target - Use firearm (18) Hit target - Intentionally affect (91,5)
Shoot projectiles - Use firearm (1,5) Run risk - Safe situation (51) Safe situation - Security (28)
Scouring - Scrutiny (3) Safe situation - Security (28) Suspicion - Criminal investigation (40)
Reliance - Contingency (109) Cause impact - Hit target (10) Age - Speed (113)
Safe situation - Security (28) Rape - Arson (22) Motion noise - Motion directional (55)
Change of phase - Cause change of phase (7) Suspicion - Robbery (98) Body movement - Motion (45)
Table 4: First 10 ranked frame pairs for different relatedness measure on the Controlled Set; in brackets,
the rank in the gold standard (full list available at (suppressed)).
sentences as contexts, while correlation decreases
by about 10 points using documents as contexts.
This suggest that sentences are suitable contex-
tual units to model situational relatedness, while
documents (i.e. news) may be so large to include
unrelated situations. It is interesting to notice
that corpus-based measures promote frame pairs
which are in a non-hierarchical relation, more than
other measures do. For example the pair CHANGE
OF PHASE - CAUSE CHANGE OF PHASE score
first, and RAPE - ARSON score ninth, while the
other measures tend to rank them much lower.
By contrast, the two frames SCOURING - IN-
SPECTING which are siblings in the FrameNet hi-
erarchy and rank 17th in the gold standard, are
ranked only 126th by cr wgt sent. This is due
to the fact that hierarchically related frames are
substitutional ? i.e. they tend not to co-occur
in the same documents; while otherwise related
frames are mostly in syntagmatic relation. As for
cr dist doc, it performs in line with cr wgt doc,
but their ranks differ; cr dist doc promotes more
hierarchical relations: distributional methods cap-
ture both paradigmatically and syntagmatically re-
lated entities.
Hierarchy-based measures. As results show,
the FrameNet hierarchy is a good indicator of re-
latedness, especially when considering FE map-
pings. Hierarchy-based measures promote frame
pairs related by diverse relations, with a slight pre-
dominance of is-a like ones (indeed, the FrameNet
hierarchy contains roughly twice as many is-a re-
lations as other ones). These measures are slightly
penalized by the low coverage of the FrameNet
hierarchy. For example, they assign zero to
CHANGE OF PHASE - ALTERED PHASE, as an in-
choative link connecting the frames is missing.
Correlation between measures. We computed
the Kendall?s ? among the experimented mea-
sures, to investigate if they model relatedness in
different or similar ways. As expected, measures
of the same type are highly correlated (e.g. hr fe
and hr wu have ? = 0.52), while those of differ-
ent types seem complementary, showing negative
or non-significant correlation (e.g. cr wgt sent has
? = ?0.034 with hr wu, and ? = 0.078 with
wn jcn). The LU overlap baseline shows signif-
icant correlation only with hr wu (? = 0.284),
suggesting that in the FrameNet hierarchy frames
correlated by some relation do share LUs.
Comparison to word relatedness. The best
performing measures score about 0.200 points be-
low the human upper bound, indicating that rank-
ing frames is much easier for humans than for ma-
chines. A direct comparison to the word ranking
task, suggests that ranking frames is harder than
words, not only for humans (as reported in Sec-
tion 3.2), but also for machines: Budanitsky and
Hirst (2006) show that measures for ranking words
get much closer to the human upper-bound than
our measures do, confirming that frame related-
ness is a fairly complex notion to model.
6 Conclusions
We empirically defined a notion of frame relat-
edness. Experiments suggest that this notion is
cognitively principled, and can be safely used in
NLP tasks. We introduced a variety of measures
for automatically estimating relatedness. Results
show that our measures have good performance,
all statistically significant at the 99% level, though
improvements are expected by using other evi-
dence. As future work, we will build up and refine
these basic measures, and investigate more com-
plex ones. We will also use our measures in appli-
cations, to check their effectiveness in supporting
various tasks, e.g. in mapping frames across Text
and Hypothesis in RTE, in linking related frames
in discourse, or in inducing frames for LU which
are not in FrameNet (Baker et al, 2007).
664
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 19: Frame Semantic
Structure Extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99?104, Prague, Czech Re-
public, June.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Walter Charles. 2000. Contextual correlates of mean-
ing. Applied Psycholinguistics, (21):502?524.
C. J. Fillmore. 1985. Frames and the Semantics of
Understanding. Quaderni di Semantica, IV(2).
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Graeme Hirst and David St.Onge, 1998. Lexical chains
as representations of context for the detection and
correction of malapropisms, pages 305?332. MIT
press.
Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
taxonomy. In Proceedings of International Confer-
ence on Research in Computational Linguistics (RO-
CLING X), Taiwan.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, (30):81?93.
Mirella Lapata. 2006. Automatic evaluation of infor-
mation ordering: Kendall?s tau. Computational Lin-
guistics, 32(4):471?484.
G.J. Lidstone. 1920. Note on the general case of the
Bayes-Laplace formula for inductive or a posteriori
probabilities. Transactions of the Faculty of Actuar-
ies, 8:182?192.
Dekang Lin and Patrick Pantel. 2001. DIRT-discovery
of inference rules from text. In Proceedings of
KDD-01, San Francisco, CA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar word. In Proceedings of COLING-ACL,
Montreal, Canada.
G. A. Miller, C. Leacock, T. Randee, and Bunker R.
1993. A Semantic Concordance. In In Proceedings
of the 3rd DARPA Workshop on Human Language
Technology, Plainsboro, New Jersey.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance. a task-oriented
evaluation. In Proceedings of EMNLP-2006, Syd-
ney,Australia.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of Fourth In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, Mexico City,
Mexico.
Marco Pennacchiotti, Diego De Cao, Paolo Marocco,
and Roberto Basili. 2008. Towards a vector space
model for framenet-like resources. In Proceedings
of LREC, Marrakech, Marocco.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity. In Proceedings of the
14th International Joint Conference on Artificial In-
telligence, Montreal, Canada.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, and Christopher R. Johnson. 2005.
FrameNet II: Extended Theory and Practice. In ICSI
Technical Report.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In In Proceedings
of Cicling, Mexico.
S. Siegel and N. J. Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, Barcellona, Spain.
Sriram Venkatapathy and Aravind K. Joshi. 2005.
Measuring the relative compositionality of verb
noun (V-N) collocations by integrating features. In
Proceedings of HLT/EMNLP, Vancouver, Canad.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In 32nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 133?
138, Las Cruces, New Mexico.
665
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 510?518,
Beijing, August 2010
Open Entity Extraction from Web Search Query Logs
Alpa Jain
Yahoo! Labs
alpa@yahoo-inc.com
Marco Pennacchiotti
Yahoo! Labs
pennac@yahoo-inc.com
Abstract
In this paper we propose a completely un-
supervised method for open-domain en-
tity extraction and clustering over query
logs. The underlying hypothesis is that
classes defined by mining search user activ-
ity may significantly differ from those typ-
ically considered over web documents, in
that they better model the user space, i.e.
users? perception and interests. We show
that our method outperforms state of the art
(semi-)supervised systems based either on
web documents or on query logs (16% gain
on the clustering task). We also report evi-
dence that our method successfully supports
a real world application, namely keyword
generation for sponsored search.
1 Introduction
Search engines are increasingly moving beyond the
traditional keyword-in document-out paradigm, and
are improving user experience by focusing on user-
oriented tasks such as query suggestions and search
personalization. A fundamental building block of
these applications is recognizing structured infor-
mation, such as, entities (e.g., mentions of people,
organizations, or locations) or relations among en-
tities (Cao et al, 2008; Hu et al, 2009). For this,
search engines typically rely on large collections of
entities and relations built using information extrac-
tion (IE) techniques (Chaudhuri et al, 2009).
Commonly used IE techniques follow two main
assumptions: (1) IE focuses on extracting infor-
mation from syntactically and semantically ?well-
formed? pieces of texts, such as, news corpora and
web documents (Pennacchiotti and Pantel, 2009);
(2) extraction processes are bootstrapped with some
pre-existing knowledge of the target domain (e.g
entities are typically extracted for pre-defined cat-
egories, such as Actors, Manufacturers, Persons,
Locations (Grishman and Sundheim, 1996)). Prior
work (Banko et al, 2007), has looked into relax-
ing the second assumption and proposed open in-
formation extraction (OIE), a domain-independent
and scalable extraction paradigm, which however
focuses mostly on web corpora.
In this paper, we argue that for user-oriented ap-
plications discussed earlier, IE techniques should
go beyond the traditional approach of using ?well-
formed? text documents. With this in mind, we ex-
plore the utility of search query logs, a rich source
of user behaviors and perception, and build tech-
niques for open entity extraction and clustering
over query logs. We hypothesize that web docu-
ments and query logs model two different spaces:
web documents model the web space, i.e. general
knowledge about entities and concepts in an objec-
tive and generic way; search query logs model the
user space, i.e. the users? view and perception of
the world in a more specific fashion, where avail-
able information directly expresses users? needs
and intents. For example, in a web space, ?brit-
ney spears? will tend to be similar and be clus-
tered with other singers, such as ?celine dion? and
?bruce springsteen?. On the contrary, in the users?
space, she is highly similar and clustered with other
gossiped celebrities like ?paris hilton? and ?serena
williams?: the users? space better models the users?
perception of that person; such a space is then
highly valuable for all those applications where
users? perceptions matters.
To computationally model our hypothesis for
OIE over search query logs, we present a two phase
approach to OIE for search query logs. The first
phase (entity extraction) extracts entities from the
search query logs using an unsupervised approach,
by applying pattern-based heuristics and statistical
measures. The second phase (entity clustering) in-
duces classes over these entities by applying clus-
tering techniques. In summary, our main contribu-
510
tions are: (1) We propose and instantiate a novel
model for open information extraction over web
search query logs; and we apply it to the task of
entity extraction and clustering. (2) We show how
we characterize each extracted entity to capture the
?user space?, and induce classes over the entities.
(3) We present an extensive evaluation over real-life
datasets showing that query logs is a rich source for
domain-independent user-oriented extraction tasks
(Section 3). We also show the practicality of our
approach by incorporating it into a real-world appli-
cation, namely keyword suggestions for sponsored
search (Section 4).
2 Open Entity Extraction on Query Log
In this section, we present our method for open
entity extraction from query logs. We first de-
scribe our heuristic method for extracting entities
(Section 2.1), and then three different feature ?user
spaces? to cluster the entities (Section 2.2).
2.1 Entity Extraction
In our setting, entities correspond to Named Enti-
ties. i.e. they are defined using the standard named
entity types described in (Sekine et al, 2002)1. In
this paper, we use a set of entities extracted from
query log, obtained by applying a simple algorithm
(any other query log entity extraction method would
apply here, e.g. (Pasca, 2007b)). The algorithm is
based on the observation that oftentimes users con-
struct their search query by copy-pasting phrases
from existing texts. Due to this phenomenon, user
queries often carry over surface-level properties
such as capitalization and tokenization information.
Our approach realizes this observation by iden-
tifying contiguous capitalized words from a user
query. (In our experiments, we observed that 42%
of the queries had at least one upper-case character.)
Specifically, given a query Q = q1 q2 q3 ? ? ? qn,
we define a candidate entity E = e1 e2 ? ? ? em as
the maximal sequence of words (i.e., alpha-numeric
characters) in the query such that each word ei in
the entity begins with an uppercase character. The
set of candidate entities is then cleaned by apply-
ing a set of heuristics, thus producing the final set
of entities. In particular, for each extracted entity,
1We exclude ?Time? and ?Numerical Expressions?, which
are out of the scope of our study.
we assign two confidence scores: a Web-based rep-
resentation score and a query-log-based standalone
score. The representation score checks if the case-
sensitive representation observed for E in Q, is the
most likely representation for E, as observed on
a Web corpus (e.g., ?DOor HANGing TIps? is as-
signed a low representation score). The standalone
score is based on the observation that a candidate
E should often occur in a standalone form among
the search query logs, in order to get the status of
a proper named entity as defined in (Sekine et al,
2002; Grishman and Sundheim, 1996). In practice,
among the query logs we must find queries of the
form Q == E, capturing the fact that users are
looking to learn more about the given entity2.
2.2 Entity Clustering
The clustering phase takes as input any of the fea-
ture spaces presented in the rest of this section, and
groups the entities according to the similarity of
their vectors in the space. The desiderata for a clus-
tering algorithm for the task of open-domain infor-
mation extraction are the following: (1) The algo-
rithm must be highly scalable, efficient, and able
to handle high dimensionality, since the number of
queries and the size of the feature vectors can be
large; (2) We do not know in advance the number
of clusters; therefore, the algorithm needs not to re-
quire a pre-defined number of clusters.
Any clustering algorithm fulfilling the above re-
quirements would fit here. In our experiments, we
adopt a highly scalable Map-Reduce implementa-
tion of the hard-clustering version of Clustering by
Committee (CBC), a state-of-the-art clustering al-
gorithm presented in (Pantel and Lin, 2002).
Context Feature Space. The basic hypothesis for
the context feature space, is that an entity can be ef-
fectively represented by the set of contexts in which
it appears in queries. This allows to capture the
users? view of the entity, i.e. what people query,
and want to know about the entity. This is similar
to that proposed by Pasca (2007b; 2007a), i.e. that
queries provide good semantics cues for modeling
named entities.
Our query log feature space may significantly
differ from a classical contextual feature space com-
2We refer the readers to (Jain and Pennacchiotti, 2010) for
details on the entity extraction algorithms.
511
puted over a Web corpus, since the same entity can
be differently perceived and described in the two
corpora (query log and Web). Consider for exam-
ple the entity ?galapagos islands?. Typical contexts
on the Web and query log for this entity are:
web: endemic birds
web: big turtles
web: charles darwin foundation
web: sensitive water
qlog : trip to
qlog : diving
qlog : where are the
qlog : travel package
The difference between the two representations
implies that entities that are similar on the Web, are
not necessarily similar on query logs. For exam-
ple, on the Web ?galapagos islands? is very simi-
lar to other countries such as ?tasmania?, ?guinea?
and ?luxemburg?; while on query log is similar to
other sea-side travel destination and related con-
cepts, such as ?greek isle?, ?kauai snorkeling? and
?south america cruise?. Our new similarity com-
puted over query log, is potentially useful for those
applications in which is more important to represent
users? intents, than an objective description of enti-
ties (e.g. in query suggestion and intent modeling).
To obtain our contextual representation we pro-
ceed as follows. For each entity e, we identify
all queries in the query log, in which e appears.
Then, we collect the set of all suffixes and postfixes
of the entity in those queries. For example, given
the entity ?galapagos islands? and the query ?sum-
mer 2008 galapagos islands tour?, the contexts are:
?summer 2008? and ?tour?.
Once the set of all contexts of all entities has been
collected, we discard contexts appearing less than
? -times in the query log, so to avoid statistical bi-
ases due to data sparseness (in the reported experi-
ments we set ? = 200). We then compute the cor-
rected pointwise mutual information (cpmi) (Pan-
tel and Ravichandran, 2004) between each instance
and each context c as:
cpmi(e, c) = log2
f(e, c) ? f(?, ?)
f(e) ? f(c) ?M (1)
where f(e, c) is the number of times e and c
occur in the same query; f(e) and f(c) is the
count of the entity and the context in the query
log; f(?, ?) the overall count of all co-occurrences
between contexts and entities; and M is the correc-
tion factor presented in (Pantel and Ravichandran,
2004), that eases the pmi?s bias towards infrequent
entities/features. Each instance is then represented
in the feature space of all contexts, by the computed
pmi values. Note that our method does not use any
NLP parsing, since queries rarely present syntactic
structure. This guarantees the method to be com-
putationally inexpensive and easily adaptable to
languages other than English.
Clickthrough Feature Space. During a search
session, users issue a search query for which the
search engine presents a list of result urls. Of the
search results, users choose those urls that are rep-
resentative of their intent. This interaction is cap-
tured by means of a click, which is logged by most
search engines as click-through data. For instance,
a search log may contain the following clicked urls
for a query ?flv converter?, for different users:
user1: www.flv-converter.com
user2: www.videoconverterdownload.com/flv/
user3: www.ripzor.com/flv.html
Our main motivation behind clustering entities
based on past user click behavior is that non-
identical queries that generate clicks on the same
urls capture similar user intent. Thus, grouping en-
tities that were issued as a query and generated user
clicks on the same url may be considered similar.
For instance, the query ?convert flv? may also gen-
erate clicks on one of the above urls, thus hinting
that the two entities are similar. We observed that
websites tend to dedicate a url per entity. There-
fore, grouping by click urls can lead to clusters with
synonyms (i.e., different ways of representing the
same entity) or variants (e.g., spelling errors). To
get more relevant clusters, instead of grouping en-
tities by the click urls, we use the base urls. For
instance, the url www.ripzor.com/flv.html
is generalized to www.ripzor.com.
With the advent of encyclopedic web-
sites such as, www.wikipedia.org and
wwww.youtube.com, naively clustering entities
by the clickthrough data can led to non-similar
entities to be placed in the same cluster. For
instance, we observed the most frequently clicked
base url for both ?gold retriever? and ?abraham
lincoln? is www.wikipedia.org. To address
this issue, in our experiments we employed a
512
stop-list by eliminating top-5 urls based on their
inverse document frequency, where an entity is
intended as the ?document?.
In practice, each extracted entity e is represented
by a feature vector of size equal to the number of
distinct base urls in the click-through data, across
all users. Each dimension in the vector represents a
url in the click-through information. The value f of
an entity e for the dimension associated with url j
is computed as:
f(e, j) =
?
?
?
w(e,j)??|U|
i w(e,i)
2
if url j clicked for query e;
0 otherwise.
where U is the set of base urls found in click-
through data when entity e was issued as a query;
and w(e, i) is the number of time the base url i was
clicked when e was a query.
Hybrid Feature Space. We also experiment a hy-
brid feature space, which is composed by the nor-
malized union of the two feature spaces above (i.e.
context and clickthrough). Though more complex
hybrid models could be applied, such as one based
on ensemble clustering, we here opt for a simple
solution which allows to better read and compare to
other methods.
3 Experimental Evaluation
In this section, we report experiments on our clus-
tering method. The goal of the experiment is two-
fold: (1) evaluate the intrinsic quality of the cluster-
ing methods, i.e. if two entities in the same cluster
are similar or related from a web user?s perspec-
tive; (2) verify if our initial hypothesis holds, i.e.
if query log based features spaces capture different
properties than Web based feature spaces (i.e. the
?user space?). In Section 3.1 we describe our ex-
perimental setup; and, in 3.2 we provide the results.
We couple this intrinsic evaluation with an extrinsic
application-driven one in Section 4.
3.1 Experimental Settings
In the experiments we use the following datasets:
Query log: A random sample of 100 million, fully
anonymized queries collected by the Yahoo! search
engine in the first 3 months of 2009, along with their
frequency. This dataset is used to generate both the
context and the clickthrough feature spaces for the
clustering step.
Web documents: A collection of 500 million web
pages crawled by a Yahoo! search engine crawl.
This data set is used to implement a web-based fea-
ture space that we will compare to in Section 3.2.
Entity set: A collection of 2,067,385 entities, ex-
tracted with the method described in 2.1, which
shows a precision of 0.705 ?0.044. Details on
the evaluation of such method are available in (Jain
and Pennacchiotti, 2010), where a full comparison
with state-of-the-art systems such as (Pasca, 2007b)
and (Banko et al, 2007) are also reported.
Evaluation methodology: Many clustering evalu-
ation metrics have been proposed, ranging from Pu-
rity to Rand-statistics and F-Measure. We first se-
lect from the original 2M entity set, a random set of
n entities biased by their frequency in query logs,
so to keep the experiment more realistic (more fre-
quent entities have more chances to be picked in
the sample). For each entity e in the sample set,
we derived a random list of k entities that are clus-
tered with e. In our experiments, we set n = 10
and k = 20. We then present to a pool of paid edi-
tors, each entity e along with the list of co-clustered
entities. Editors are requested to classify each co-
clustered entity ei as correct or incorrect. An entity
ei is deemed as correct, if it is similar or related to e
from a web user?s perspective: to capture this intu-
ition, the editor is asked the question: ?If you were
interested in e, would you be also interested in ei
in any intent??.3 Annotators? agreement over a ran-
dom set of 30 entities is kappa = 0.64 (Marques
De Sa?, 2003), corresponding to substantial agree-
ment. Additionally, we ask editors to indicate the
relation type between e and ei (synonyms, siblings,
parent-child, topically related).
Compared methods:
CL-CTX: A CBC run, based on the query log con-
text feature space (Section 2.2).
CL-CLK: A CBC run, based on the clickthrough
feature space (Section 2.2).
3For example, if someone is interested in ?hasbro?, he could
be probably also be interested in ?lego?, when the intent is buy-
ing a toy. The complete set of annotation guidelines is reported
in (Jain and Pennacchiotti, 2010).
513
method # cluster avg cluster size
CL-Web 1,601 240
CL-CTX 875 1,182
CL-CLK 4,385 173
CL-HYB 1,580 478
Table 1: Statistics on the clustering results.
CL-HYB: A CBC run, based on the hybrid space
that combines CL-CTXand CL-CLK(Section 2.2).
CL-Web: A state-of-the-art open domain method
based on features extracted from the Web docu-
ments data set (Pantel et al, 2009). This method
runs CBC over a space where features are the con-
texts in which an entity appears (noun chunks pre-
ceding and following the target entity); and feature
value is the pmi between the entity and the chunks.
Evaluation metrics: We evaluate each method us-
ing accuracy, intended as the percentage of correct
judgments.
3.2 Experimental Results
Table 3 reports accuracy results. CL-HYB is the
best performing method, achieving 0.85 accuracy,
respectively +4% and +11% above CL-CLK and
CL-Web. CL-CTX shows the lowest performance.
Our results suggest that query log spaces are more
suitable to model the ?user space? wrt web features.
Specifically, clickthrough information are most use-
ful confirming our hypothesis that queries that gen-
erate clicks on the same urls capture similar user
intents.
To have an anecdotal and practical intuition on
the results, in Table 2 we report some entities and
examples of other entities from the same clusters, as
obtained from the CL-HYB and CL-Web methods.
The examples show that CL-HYB builds clusters
according to a variety of relations, while CL-Web
mostly capture sibling-like relations.
One relevant of such relations is topicality. For
example, for ?aaa insurance? the CL-HYB cluster
mostly contains entities that are topically related to
the American Automobile Association, while the
CL-Web cluster contains generic business compa-
nies. In this case, the CL-HYB approach sim-
ply chose to group together entities having clicks
to ?aaa.com? and appearing in contexts as ?auto
club?. On the contrary, CL-Web grouped accord-
ing to contexts such as ?selling? and ?company?.
The entity ?hip osteoarthritis? shows a similar be-
entity CL-HYB CL-Web
aaa insurance roadside assistance loanmax
personal liability insurance pilot car service
international driving permits localnet
aaa minnesota fibermark
travelers checks country companies
insurance
paris hilton brenda costa julia roberts
adriana sklenarikova brad pitt
kelly clarkson nicole kidman
anja rubik al pacino
federica ridolfi tom hanks
goldie hawn bonnie hunt julia roberts
brad pitt brad pitt
tony curtis nicole kidman
nicole kidman al pacino
nicholas cage tom hanks
basic algebra numerical analysis math tables
discrete math trigonometry help
lattice theory mathtutor
nonlinear physics surface area formula
ramsey theory multiplying fractions
hip osteoarthritis atherosclerosis wrist arthritis
pneumonia disc replacement
hip fracture rotator cuff tears
breast cancer shoulder replacement
anorexia nervosa american orthopedic
society
acer america acer aspire accessories microsoft
aspireone casio computer
acer monitors borland software
acer customer service sony
acer usa nortel networks
Table 2: Sample of the generated entity clusters.
havior: CL-HYB groups entities topically related
to orthopedic issues, since most of the entities are
sharing contexts such as ?treatment? and ?recovery?
and, at the same time, clicks to urls such as ?or-
thoinfo.aaos.org? and ?arthirtis.about.com?.
Another interesting observation regards entities
referring to people. The ?paris hilton? and ?goldie
hawn? examples show that the CL-Web approach
groups famous people according to their category
? i.e. profession in most cases. On the contrary,
query log approaches tend to group people accord-
ing to their social attitude, when this prevails over
the profession. In the example, CL-HYB clusters
the actress ?goldie hawn? with other actors, while
?paris hilton? is grouped with an heterogeneous set
of celebrities that web users tend to query and click
in a same manner: In this case, the social per-
sona of ?paris hilton? prevails over its profession
(actress/singer). This aspect is important in many
applications, e.g. in query suggestion, where one
wants to propose to the user entities that have been
similarly queried and clicked.
In order to check if the above observations are
not anecdotal, we studied the relation type annota-
tion provided by the editors (Table 4). Table shows
514
method Precision
CL-Web 0.735
CL-CTX 0.460
CL-CLK 0.815 ?
CL-HYB 0.850 ?
Table 3: Precision of various clustering methods
(? indicates statistical-significant better than the
CL-Web method, using t-test).
that query log based methods are more varied in the
type of clusters they build. Table 5 shows the dif-
ference between the clustering obtained using the
different methods and the overlap between the pro-
duced clusters. For example, 40% of the relations
for the CL-HYB system are topical, while 32% are
sibiling ones. On the contrary, the CL-Web method
is highly biased towards sibling relations.
As regard a more attentive analysis of the dif-
ferent query log based methods, CL-CTX has the
lowest performance. This is mainly due to the fact
that contextual data are sometimes too sparse and
generic. For example ?mozilla firefox? is clustered
with ?movie program? and ?astro reading? because
they share only some very generic contexts such as
?free downloads?. In order to get more data, one op-
tion is to relax the ? threshold (see Section 2) so to
include more contexts in the semantic space. Unfor-
tunately, this would have a strong drawback, in that
low-frequency context tend to be idiosyncratic and
spurious. A typical case regards recurring queries
submitted by robots for research purposes, such as
?who is X?, ?biography of X?, or ?how to X?. These
queries tend to build too generic clusters containing
people or objects. Another relevant problem of the
CL-CTX method is that even when using a high ?
cut, clusters still tend to be too big and generic, as
statistics in Table 4 shows.
CL-CTX, despite the low performance, is very
useful when combined with CL-CLK. Indeed the
CL-HYB system improves +4% over the CL-CLK
system alone. This is because the CL-HYB method
is able to recover some misleading or incomplete
evidence coming from the CL-CLK using features
provided by CL-CLK. For example, editors judged
as incorrect 11 out of 20 entities co-clustered with
the entity ?goldie hawn? by CL-CLK. Most of these
errors are movies (e.g. ?beverly hills cops?) soap
operas (e.g. ?sortilegio?) and directors, because all
have clicks to ?imdb.com? and ?movies.yahoo.com?.
class method
CL-Web CL-CTX CL-CLK CL-HYB
topic 0.27 0.46 0.46 0.40
sibling 0.72 0.43 0.29 0.32
parent - 0.09 0.13 0.09
child 0.01 - 0.01 0.02
synonym 0.01 0.03 0.12 0.16
Table 4: Fraction of entities that have been classi-
fied by editors in the different relation types.
method labelled clusters
CL-CTX CL-CLK CL-HYB CL-Web
CL-CTX - 0.2 0.53 0.29
CL-CLK 0.21 - 0.54 0.34
CL-HYB 0.53 0.51 - 0.31
CL-Web 0.33 0.35 0.41 -
Table 5: Purity of clusters for each method using
clusters from other methods as ?labelled? data.
CL-HYB recovers these errors by including features
coming from CL-CTX such as ?actress?.
In summary, query log spaces group together en-
tities that are similar by web users (this being topi-
cal similarity or social attitude), thus constituting a
practical model of the ?user space? to be leveraged
by web applications.
4 Keywords for Sponsored Search
In this section we explore the use of our methods for
keyword generation for sponsored search. In spon-
sored search, a search company opens an auction,
where on-line advertisers bid on specific keywords
(called bidterms). The winner is allowed to put its
ad and link on the search result page of the search
company, when the bidterm is queried. Compa-
nies such as Google and Yahoo are investing efforts
for improving their bidding platforms, so to attract
more advertisers in the auctions. Bidterm sugges-
tion tools (adWords, 2009; yahooTool, 2009) are
used to help advertiser in selecting bidterms: the
advertisers enters a seed keyword (seed) express-
ing the intent of its ad, and the tool returns a list
of suggested keywords (suggestions) that it can use
for bidding ? e.g for the seed ?mp3 player?, a sug-
gestion could be ?ipod nano?. The task of gen-
erating bid suggestions (i.e. keyword generation)
is typically automatic, and has received a grow-
ing attention in the search community for its im-
pact on search company revenue. The main prob-
lem of existing methods for suggestion (adWords,
2009; yahooTool, 2009; wordTracker, 2009) is that
515
they produce only suggestions that contain the ini-
tial seed (e.g. ?belkin mp3 player? for the seed ?mp3
player?), while nonobvious (and potentially less ex-
pensive) suggestions not containing the seed are ne-
glected (e.g. ?ipod nano? for ?mp3 player?). For
example for ?galapagos islands?, a typical produc-
tion system suggests ?galapagos islands tour? which
cost almost 5$ per click; while the less obvious ?isla
santa cruz? would cost only 0.35$. Below we show
our method to discover such nonobvious sugges-
tions, by retrieving entities in the same cluster of
a given seed.
4.1 Experimental Setting
We evaluate the quality of the suggestions proposed
by different methods for a set of seed bidterms.,
adopting the evaluation schema in (Joshi and Mot-
wani, 2006)
Dataset Creation. To create the set of seeds, we
use Google skTool4. The tool provides a list of
popular bid terms, organized in a taxonomy of ad-
vertisement topics. We select 3 common topics:
tourism, vehicles and consumer-electronics. For
each topic, we randomly pick 5 seeds among the
800 most popular bid terms, which also appear in
our entity set described in Section 3.1.5. We evalu-
ate a system by collecting all its suggestions for the
15 seeds, and then extracting a random sample of
20 suggestions per seed.
Evaluation and Metrics. We use precision and
Nonobviousness. Precision is computed by ask-
ing two experienced human experts to classify each
suggestion of a given seed, as relevant or irrelevant.
A suggestion is deemed as relevant if any advertiser
would likely choose to bid for the suggestion, hav-
ing as intent the seed. Annotator agreement, evalu-
ated on a subset of 120 suggestions is kappa = 0.72
(substantial agreement). Precision is computed as
the percentage of suggestions judged as relevant.
Nonobviousness is a metric introduced in (Joshi
and Motwani, 2006), capturing how nonobvious the
suggestions are. It simply counts how many sug-
4http://www.google.com/sktool
5The final set of 15 bid terms is: tourism:galapagos
islands,holiday insurance,hotel booking,obertauern,wagrain;
vehicles:audi q7,bmw z4,bmw dealers,suzuki grand vi-
tara,yamaha banshee; consumer electr:canon rebel xti,divx
converter,gtalk,pdf reader,flv converter.
gestions for a given seed do not contain the seed it-
self (or any of its variants): this metric is computed
automatically using string matching and a simple
stemmer.
Comparisons. We compare the suggestions pro-
posed by CL-CTX, CL-CLK, and CL-HYB, against
Web and two reference state-of-the-art produc-
tion systems: Google AdWords (GOO) and Yahoo
Search Marketing Tool (YAH). As concerns our
methods, we extract as suggestions the entities that
occur in the same cluster of a given seed. For the
production systems, we rely on the suggestions pro-
posed on the website of the tools.
4.2 Experimental Results
Precision results are reported in the second column
of Table 6. Both CL-CLK and CL-HYB outper-
form Web in precision, CL-HYB being close to the
upper-bound of the two production systems. As ex-
pected, production systems show a very high pre-
cision but their suggestions are very obvious. Our
results are fairly in line with those obtained on a
similar dataset, by Joshi and Motwani (2006).
A closer look at the results shows that most of the
errors for CL-CTX are caused by the same problem
outlined in Section 3.2: Some entities are wrongly
assigned to a cluster, because they have some high
cpmi context feature which is shared with the clus-
ter centroid, but which is not very characteristic
for the entity itself. This is particularly evident for
some of the low frequency entities, where cpmi val-
ues could not reflect the actual semantics of the en-
tity. For example the entity ?nickelodeon? (a kids tv
channel in UK) is assigned to the cluster of ?galapa-
gos islands?, because of the feature ?cruise?: indeed,
some people query about ?nickelodeon cruise? be-
cause the tv channel organizes some kids cruises.
Other mistakes are due to feature ambiguity. For
example, the entity ?centurion boats? is assigned
to the cluster of ?obertauern? (a ski resort in Aus-
tria), because they share the ambiguous feature ?ski?
(meaning either winter-ski or water-ski). As for the
CL-CLK system, some of the errors are caused by
the fact that some base url can refer to very differ-
ent types of entities. For example the entity ?color
copier? is suggested for the the camera ?canon rebel
xti?, since they both share clicks to the Canon web-
site. The CL-HYB system achieves a higher preci-
516
method Precision Nonobviousness
GOO 0.982 0.174
YAH 0.966 0.195
Web 0.814 0.827
CL-CTX 0.547 0.963
CL-CLK 0.827 0.630
CL-HYB 0.946 0.567
Table 6: Results for keyword generation.
sion wrt CL-CTX and CL-CLK: the combination of
the two spaces decreases the impact of misleading
features ?e.g. for ?yamaha bunshee?, all CL-HYB ?s
suggestions are correct, while almost all CL-CLK ?s
suggestions are incorrect: the hybrid system recov-
ered the negative effect of the misleading feature
ebay.com, by backing up on features from the
contextual subspace (e.g. ?custom?, ?specs?, ?used
parts?).
Nonobviousness results are reported in column
three of Table 6. All our systems return a high num-
ber of nonobvious suggestions (all above 50%).6
On the contrary, GOO and YAH show low perfor-
mance, as both systems are heavily based on the
substring matching technique. This strongly moti-
vates the use of semantic approaches as those we
propose, that guarantee at the same time both a
higher linguistic variety and an equally high preci-
sion wrt the production systems. For example, for
the seeds ?galapagos islands?, GOO returns simple
suggestions such as ?galapagos islands vacations?
and ?galapagos islands map?; while CL-HYB re-
turns ?caribbean mexico? and ?pacific dawn?, two
terms that are semantically related but dissimilar
from the seed. Remember that these letter terms are
related to the seed because they are similar in the
user space, i.e. users looking at ?galapagos islands?
tend to similarly look for ?caribbean mexico? and
?pacific dawn?. These suggestions would then be
very valuable for tourism advertisers willing to im-
prove their visibility through a non-trivial and pos-
sibly less expensive set of bid terms.
5 Related Work
While literature abounds with works on entity ex-
traction from web documents (e.g. (Banko et al,
2007; Chaudhuri et al, 2009; Pennacchiotti and
Pantel, 2009)), the extraction of classes of entities
6Note that very high values for CL-CTX may be mislead-
ing, as many of the suggestions proposed by this system are
incorrect (see precision results) and hence non-obvious (e.g.,
?derek lewis? for ?galapagos islands?).
over query logs is a pretty new task, recently intro-
duced in (Pasca, 2007b). Pasca?s system extracts
entities of pre-defined classes in a semi-supervised
fashion, starting with an input class represented by a
set of seeds, which are used to induce typical query-
contexts for the class. Contexts are then used to
extract and select new candidate instances for the
class. A similar approach is also adopted in (Sekine
and Suzuki, 2007). Pasca shows an improvement
of about 20% accuracy, compared to existing Web-
based systems. Our extraction algorithm differs
from Pasca?s work in that it is completely unsuper-
vised. Also, Pasca?s cannot be applied to OIE, i.e.
it only works for pre-defined classes. Our cluster-
ing approach is related to Lin and Wu?s work (Lin
and Wu, 2009). Authors propose a semi-supervised
algorithm for query classification. First, they ex-
tract a large set of 20M phrases from a query log, as
those unique queries appearing more than 100 times
in a Web corpus. Then, they cluster the phrases
using the K-means algorithm, where features are
the phrases? bag-of-words contexts computed over
a web corpus. Finally, they classify queries using
a logistic regression algorithm. Our work differs
from Lin and Wu, as we focus on entities instead of
phrases. Also, the features we use for clustering are
from query logs and click data, not web contexts.
6 Conclusions
We presented an open entity extraction approach
over query logs that goes beyond the traditional web
corpus, with the goal of modeling a ?user-space? as
opposed to an established ?web-space?. We showed
that the clusters generated by query logs substan-
tially differ from those by a Web corpus; and that
our method is able to induce state-of-the-art qual-
ity classes on a user-oriented evaluation on the real
world task of keyword generation for sponsored
search. As future work we plan to: (i) experiment
different clustering algorithms and feature models,
e.g. soft-clustering for handling ambiguous enti-
ties; (ii) integrate the Web space and the query log
spaces; (iii) embed our methods in in existing tools
for intent modeling, query suggestion and similia,
to check its impact in production systems.
517
References
adWords. 2009. Google adwords. ad-
words.google.com/select/keywordtoolexternal.
Banko, Michele, Michael Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of IJCAI.
Cao, Huanhuan, Daxin Jiang, Jian Pei, Qi He, Zhen
Liao, Enhong Chen, and Hang Li. 2008. Context-
aware query suggestion by mining click-through and
session data. In Proceedings of KDD-08.
Chaudhuri, Surajit, Venkatesh Ganti, and Dong Xin.
2009. Exploiting web search to generate synonyms
for entities. In Proceedings of WWW-09.
Grishman, R. and B. Sundheim. 1996. Message under-
standing conference- 6: A brief history. In Proceed-
ings of COLING.
Hu, Jian, Gang Wang, Fred Lochovsky, Jian tao Sun,
and Zheng Chen. 2009. Understanding user?s query
intent with Wikipedia. In Proceedings of WWW-09.
Jain, Alpa and Marco Pennacchiotti. 2010. Open In-
formation Extraction from Web Search Query Logs.
Technical Report YL-2010-003, Yahoo! Labs.
Joshi, Amruta and Rajeev Motwani. 2006. Keyword
generation for search engine advertising. In Proceed-
ings of Sixth IEEE-ICDM.
Lin, Dekang and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP-2009.
Marques De Sa?, Joaquim P. 2003. Applied Statistics.
Springer Verlag.
Pantel, Patrick and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings KDD-02.
Pantel, Patrick and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceeding of
HLT-NAACL-2004.
Pantel, Patrick, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP-09.
Pasca, Marius. 2007a. Organizing and searching the
world wide web of facts - step two: Harnessing the
wisdom of the crowds. In Proceedings of the WWW-
2007.
Pasca, Marius. 2007b. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of CIKM-2007.
Pennacchiotti, Marco and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
EMNLP-2009.
Sekine, Satoshi and Hisami Suzuki. 2007. Acquiring
ontological knowledge from query logs. In Proceed-
ings of WWW-07.
Sekine, Satoshi, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of LREC-2002.
wordTracker. 2009. Word tracker.
www.wordtracker.com.
yahooTool. 2009. Yahoo search marketing. searchmar-
keting.yahoo.com.
518
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx
?
, Su Nam Kim
?
, Zornitsa Kozareva
?
, Preslav Nakov
?
,
Diarmuid
?
O S
?
eaghdha
?
, Sebastian Pad
?
o
?
, Marco Pennacchiotti
??
,
Lorenza Romano
??
, Stan Szpakowicz
??
Abstract
SemEval-2 Task 8 focuses on Multi-way
classification of semantic relations between
pairs of nominals. The task was designed
to compare different approaches to seman-
tic relation classification and to provide a
standard testbed for future research. This
paper defines the task, describes the train-
ing and test data and the process of their
creation, lists the participating systems (10
teams, 28 runs), and discusses their results.
1 Introduction
SemEval-2010 Task 8 focused on semantic rela-
tions between pairs of nominals. For example, tea
and ginseng are in an ENTITY-ORIGIN relation in
?The cup contained tea from dried ginseng.?. The
automatic recognition of semantic relations has
many applications, such as information extraction,
document summarization, machine translation, or
construction of thesauri and semantic networks.
It can also facilitate auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing, and recognizing textual entailment.
Our goal was to create a testbed for automatic
classification of semantic relations. In developing
the task we met several challenges: selecting a
suitable set of relations, specifying the annotation
procedure, and deciding on the details of the task
itself. They are discussed briefly in Section 2; see
also Hendrickx et al (2009), which includes a sur-
vey of related work. The direct predecessor of Task
8 was Classification of semantic relations between
nominals, Task 4 at SemEval-1 (Girju et al, 2009),
?
University of Lisbon, iris@clul.ul.pt
?
University of Melbourne, snkim@csse.unimelb.edu.au
?
Information Sciences Institute/University of Southern
California, kozareva@isi.edu
?
National University of Singapore, nakov@comp.nus.edu.sg
?
University of Cambridge, do242@cl.cam.ac.uk
?
University of Stuttgart, pado@ims.uni-stuttgart.de
??
Yahoo! Inc., pennacc@yahoo-inc.com
??
Fondazione Bruno Kessler, romano@fbk.eu
??
University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
which had a separate binary-labeled dataset for
each of seven relations. We have defined SemEval-
2010 Task 8 as a multi-way classification task in
which the label for each example must be chosen
from the complete set of ten relations and the map-
ping from nouns to argument slots is not provided
in advance. We also provide more data: 10,717 an-
notated examples, compared to 1,529 in SemEval-1
Task 4.
2 Dataset Creation
2.1 The Inventory of Semantic Relations
We first decided on an inventory of semantic rela-
tions. Ideally, it should be exhaustive (enable the
description of relations between any pair of nomi-
nals) and mutually exclusive (each pair of nominals
in context should map onto only one relation). The
literature, however, suggests that no relation inven-
tory satisfies both needs, and, in practice, some
trade-off between them must be accepted.
As a pragmatic compromise, we selected nine
relations with coverage sufficiently broad to be of
general and practical interest. We aimed at avoid-
ing semantic overlap as much as possible. We
included, however, two groups of strongly related
relations (ENTITY-ORIGIN / ENTITY-DESTINA-
TION and CONTENT-CONTAINER / COMPONENT-
WHOLE / MEMBER-COLLECTION) to assess mod-
els? ability to make such fine-grained distinctions.
Our inventory is given below. The first four were
also used in SemEval-1 Task 4, but the annotation
guidelines have been revised, and thus no complete
continuity should be assumed.
Cause-Effect (CE). An event or object leads to an
effect. Example: those cancers were caused
by radiation exposures
Instrument-Agency (IA). An agent uses an in-
strument. Example: phone operator
Product-Producer (PP). A producer causes a
product to exist. Example: a factory manu-
factures suits
33
Content-Container (CC). An object is physically
stored in a delineated area of space. Example:
a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is de-
rived from an origin (e.g., position or mate-
rial). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving to-
wards a destination. Example: the boy went
to bed
Component-Whole (CW). An object is a com-
ponent of a larger whole. Example: my
apartment has a large kitchen
Member-Collection (MC). A member forms a
nonfunctional part of a collection. Example:
there are many trees in the forest
Message-Topic (MT). A message, written or spo-
ken, is about a topic. Example: the lecture
was about semantics
2.2 Annotation Guidelines
We defined a set of general annotation guidelines
as well as detailed guidelines for each semantic
relation. Here, we describe the general guidelines,
which delineate the scope of the data to be col-
lected and state general principles relevant to the
annotation of all relations.
1
Our objective is to annotate instances of seman-
tic relations which are true in the sense of hold-
ing in the most plausible truth-conditional inter-
pretation of the sentence. This is in the tradition
of the Textual Entailment or Information Valida-
tion paradigm (Dagan et al, 2009), and in con-
trast to ?aboutness? annotation such as semantic
roles (Carreras and M`arquez, 2004) or the BioNLP
2009 task (Kim et al, 2009) where negated rela-
tions are also labelled as positive. Similarly, we
exclude instances of semantic relations which hold
only in speculative or counterfactural scenarios. In
practice, this means disallowing annotations within
the scope of modals or negations, e.g., ?Smoking
may/may not have caused cancer in this case.?
We accept as relation arguments only noun
phrases with common-noun heads. This distin-
guishes our task from much work in Information
Extraction, which tends to focus on specific classes
of named entities and on considerably more fine-
grained relations than we do. Named entities are a
specific category of nominal expressions best dealt
1
The full task guidelines are available at http://docs.
google.com/View?id=dfhkmm46_0f63mfvf7
with using techniques which do not apply to com-
mon nouns. We only mark up the semantic heads of
nominals, which usually span a single word, except
for lexicalized terms such as science fiction.
We also impose a syntactic locality requirement
on example candidates, thus excluding instances
where the relation arguments occur in separate sen-
tential clauses. Permissible syntactic patterns in-
clude simple and relative clauses, compounds, and
pre- and post-nominal modification. In addition,
we did not annotate examples whose interpretation
relied on discourse knowledge, which led to the
exclusion of pronouns as arguments. Please see
the guidelines for details on other issues, includ-
ing noun compounds, aspectual phenomena and
temporal relations.
2.3 The Annotation Process
The annotation took place in three rounds. First,
we manually collected around 1,200 sentences for
each relation through pattern-based Web search. In
order to ensure a wide variety of example sentences,
we used a substantial number of patterns for each
relation, typically between one hundred and several
hundred. Importantly, in the first round, the relation
itself was not annotated: the goal was merely to
collect positive and near-miss candidate instances.
A rough aim was to have 90% of candidates which
instantiate the target relation (?positive instances?).
In the second round, the collected candidates for
each relation went to two independent annotators
for labeling. Since we have a multi-way classifi-
cation task, the annotators used the full inventory
of nine relations plus OTHER. The annotation was
made easier by the fact that the cases of overlap
were largely systematic, arising from general phe-
nomena like metaphorical use and situations where
more than one relation holds. For example, there is
a systematic potential overlap between CONTENT-
CONTAINER and ENTITY-DESTINATION depend-
ing on whether the situation described in the sen-
tence is static or dynamic, e.g., ?When I came,
the <e1>apples</e1> were already put in the
<e2>basket</e2>.? is CC(e1, e2), while ?Then,
the <e1>apples</e1> were quickly put in the
<e2>basket</e2>.? is ED(e1, e2).
In the third round, the remaining disagreements
were resolved, and, if no consensus could be
achieved, the examples were removed. Finally, we
merged all nine datasets to create a set of 10,717
instances. We released 8,000 for training and kept
34
the rest for testing.
2
Table 1 shows some statistics about the dataset.
The first column (Freq) shows the absolute and rel-
ative frequencies of each relation. The second col-
umn (Pos) shows that the average share of positive
instances was closer to 75% than to 90%, indicating
that the patterns catch a substantial amount of ?near-
miss? cases. However, this effect varies a lot across
relations, causing the non-uniform relation distribu-
tion in the dataset (first column).
3
After the second
round, we also computed inter-annotator agreement
(third column, IAA). Inter-annotator agreement
was computed on the sentence level, as the per-
centage of sentences for which the two annotations
were identical. That is, these figures can be inter-
preted as exact-match accuracies. We do not report
Kappa, since chance agreement on preselected can-
didates is difficult to estimate.
4
IAA is between
60% and 95%, again with large relation-dependent
variation. Some of the relations were particularly
easy to annotate, notably CONTENT-CONTAINER,
which can be resolved through relatively clear cri-
teria, despite the systematic ambiguity mentioned
above. ENTITY-ORIGIN was the hardest relation to
annotate. We encountered ontological difficulties
in defining both Entity (e.g., in contrast to Effect)
and Origin (as opposed to Cause). Our numbers
are on average around 10% higher than those re-
ported by Girju et al (2009). This may be a side
effect of our data collection method. To gather
1,200 examples in realistic time, we had to seek
productive search query patterns, which invited
certain homogeneity. For example, many queries
for CONTENT-CONTAINER centered on ?usual sus-
pect? such as box or suitcase. Many instances of
MEMBER-COLLECTION were collected on the ba-
sis of from available lists of collective names.
3 The Task
The participating systems had to solve the follow-
ing task: given a sentence and two tagged nominals,
predict the relation between those nominals and the
direction of the relation.
We released a detailed scorer which outputs (1) a
confusion matrix, (2) accuracy and coverage, (3)
2
This set includes 891 examples from SemEval-1 Task 4.
We re-annotated them and assigned them as the last examples
of our training dataset to ensure that the test set was unseen.
3
To what extent our candidate selection produces a biased
sample is a question that we cannot address within this paper.
4
We do not report Pos or IAA for OTHER, since OTHER is
a pseudo-relation that was not annotated in its own right. The
numbers would therefore not be comparable to other relations.
Relation Freq Pos IAA
Cause-Effect 1331 (12.4%) 91.2% 79.0%
Component-Whole 1253 (11.7%) 84.3% 70.0%
Entity-Destination 1137 (10.6%) 80.1% 75.2%
Entity-Origin 974 (9.1%) 69.2% 58.2%
Product-Producer 948 (8.8%) 66.3% 84.8%
Member-Collection 923 (8.6%) 74.7% 68.2%
Message-Topic 895 (8.4%) 74.4% 72.4%
Content-Container 732 (6.8%) 59.3% 95.8%
Instrument-Agency 660 (6.2%) 60.8% 65.0%
Other 1864 (17.4%) N/A
4
N/A
4
Total 10717 (100%)
Table 1: Annotation Statistics. Freq: Absolute and
relative frequency in the dataset; Pos: percentage
of ?positive? relation instances in the candidate set;
IAA: inter-annotator agreement
precision (P), recall (R), and F
1
-Score for each
relation, (4) micro-averaged P, R, F
1
, (5) macro-
averaged P, R, F
1
. For (4) and (5), the calculations
ignored the OTHER relation. Our official scoring
metric is macro-averaged F
1
-Score for (9+1)-way
classification, taking directionality into account.
The teams were asked to submit test data pre-
dictions for varying fractions of the training data.
Specifically, we requested results for the first 1000,
2000, 4000, and 8000 training instances, called
TD1 through TD4. TD4 was the full training set.
4 Participants and Results
Table 2 lists the participants and provides a rough
overview of the system features. Table 3 shows the
results. Unless noted otherwise, all quoted numbers
are F
1
-Scores.
Overall Ranking and Training Data. We rank
the teams by the performance of their best system
on TD4, since a per-system ranking would favor
teams with many submitted runs. UTD submit-
ted the best system, with a performance of over
82%, more than 4% better than the second-best
system. FBK IRST places second, with 77.62%,
a tiny margin ahead of ISI (77.57%). Notably, the
ISI system outperforms the FBK IRST system for
TD1 to TD3, where it was second-best. The accu-
racy numbers for TD4 (Acc TD4) lead to the same
overall ranking: micro- versus macro-averaging
does not appear to make much difference either.
A random baseline gives an uninteresting score of
6%. Our competitive baseline system is a simple
Naive Bayes classifier which relies on words in the
sentential context only; two systems scored below
this baseline.
35
System Institution Team Description Res. Class.
Baseline Task organizers local context of 2 words only BN
ECNU-SR-1 East China Normal
University
Man Lan, Yuan
Chen, Zhimin
Zhou, Yu Xu
stem, POS, syntactic patterns S SVM
(multi)
ECNU-SR-2,3 features like ECNU-SR-1, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-4 stem, POS, syntactic patterns,
hyponymy and meronymy rela-
tions
WN,
S
SVM
(multi)
ECNU-SR-5,6 features like ECNU-SR-4, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-7 majority vote of ECNU-1,2,4,5
FBK IRST-6C32 Fondazione Bruno
Kessler
Claudio Giu-
liano, Kateryna
Tymoshenko
3-word window context features
(word form, part of speech, or-
thography) + Cyc; parameter
estimation by optimization on
training set
Cyc SVM
FBK IRST-12C32 FBK IRST-6C32 + distance fea-
tures
FBK IRST-12VBC32 FBK IRST-12C32 + verbs
FBK IRST-6CA,
-12CA, -12VBCA
features as above, parameter es-
timation by cross-validation
FBK NK-RES1 Fondazione Bruno
Kessler
Matteo Negri,
Milen Kouylekov
collocations, glosses, semantic
relations of nominals + context
features
WN BN
FBK NK-RES 2,3,4 like FBK NK-RES1 with differ-
ent context windows and collo-
cation cutoffs
ISI Information Sci-
ences Institute,
University of
Southern Califor-
nia
Stephen Tratz features from different re-
sources, a noun compound
relation system, and various
feature related to capitalization,
affixes, closed-class words
WN,
RT, G
ME
ISTI-1,2 Istituto di sci-
enca e tecnologie
dell?informazione
?A. Faedo?
Andrea Esuli,
Diego Marcheg-
giani, Fabrizio
Sebastiani
Boosting-based classification.
Runs differ in their initializa-
tion.
WN 2S
JU Jadavpur Univer-
sity
Santanu Pal, Partha
Pakray, Dipankar
Das, Sivaji Bandy-
opadhyay
Verbs, nouns, and prepositions;
seed lists for semantic relations;
parse features and NEs
WN,
S
CRF
SEKA Hungarian
Academy of
Sciences
Eszter Simon, An-
dras Kornai
Levin and Roget classes, n-
grams; other grammatical and
formal features
RT,
LC
ME
TUD-base Technische Univer-
sit?at Darmstadt
Gy?orgy Szarvas,
Iryna Gurevych
word, POS n-grams, depen-
dency path, distance
S ME
TUD-wp TUD-base + ESA semantic re-
latedness scores
+WP
TUD-comb TUD-base + own semantic relat-
edness scores
+WP,WN
TUD-comb-threshold TUD-comb with higher thresh-
old for OTHER
UNITN University of
Trento
Fabio Celli punctuation, context words,
prepositional patterns, estima-
tion of semantic relation
? DR
UTD University of Texas
at Dallas
Bryan Rink, Sanda
Harabagiu
context wods, hypernyms, POS,
dependencies, distance, seman-
tic roles, Levin classes, para-
phrases
WN,
S, G,
PB/NB,
LC
SVM,
2S
Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP:
Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:
PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision
Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)
36
System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9
Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)
ECNU-SR-1 52.13 56.58 58.16 60.08 57.1
4
CE (79.7) IA (32.2)
ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)
ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)
ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)
ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)
ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)
ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)
FBK IRST-6C32 60.19 67.31 71.78 76.81 72.4
2
ED (82.6) IA (69.4)
FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)
FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)
FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)
FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)
FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)
FBK NK-RES1 55.71
?
64.06
?
67.80
?
68.02 62.1
7
ED (77.6) IA (52.9)
FBK NK-RES2 54.27
?
63.68
?
67.08
?
67.48 61.4 ED (77.4) PP (55.2)
FBK NK-RES3 54.25
?
62.73
?
66.11
?
66.90 60.5 MC (76.7) IA (56.3)
FBK NK-RES4 44.11
?
58.85
?
63.06
?
65.84 59.4 MC (76.1) IA/PP (58.0)
ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)
ISTI-1 50.49
?
55.80
?
61.14
?
68.42 63.2
6
ED (80.7) PP (53.8)
ISTI-2 50.69
?
54.29
?
59.77
?
66.65 61.5 ED (80.2) IA (48.9)
JU 41.62
?
44.98
?
47.81
?
52.16 50.2 9 CE (75.6) IA (27.8)
SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)
TUD-base 50.81 54.61 56.98 60.50 56.1
5
CE (80.7) IA (31.1)
TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)
TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)
TUD-comb-? 58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)
UNITN 16.57
?
18.56
?
22.45
?
26.67 27.4 10 ED (46.4) PP (0)
UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)
Table 3: F
1
-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,
TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results
marked with
?
were submitted after the deadline. The best-performing run for each participant is italicized.
As for the amount of training data, we see a sub-
stantial improvement for all systems between TD1
and TD4, with diminishing returns for the transi-
tion between TD3 and TD4 for many, but not all,
systems. Overall, the differences between systems
are smaller for TD4 than they are for TD1. The
spread between the top three systems is around 10%
at TD1, but below 5% at TD4. Still, there are clear
differences in the influence of training data size
even among systems with the same overall archi-
tecture. Notably, ECNU-SR-4 is the second-best
system at TD1 (67.95%), but gains only 7% from
the eightfold increase of the size of the training data.
At the same time, ECNU-SR-3 improves from less
than 40% to almost 69%. The difference between
the systems is that ECNU-SR-4 uses a multi-way
classifier including the class OTHER, while ECNU-
SR-3 uses binary classifiers and assigns OTHER
if no other relation was assigned with p>0.5. It
appears that these probability estimates for classes
are only reliable enough for TD3 and TD4.
The Influence of System Architecture. Almost
all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two
systems, UTD and ISTI (rank 1 and 6) split the task
into two classification steps (relation and direction),
but the 2nd- and 3rd-ranked systems do not. The
use of a sequence model such as a CRF did not
show a benefit either.
The systems use a variety of resources. Gener-
ally, richer feature sets lead to better performance
(although the differences are often small ? compare
the different FBK IRST systems). This improve-
ment can be explained by the need for semantic
generalization from training to test data. This need
can be addressed using WordNet (contrast ECNU-1
to -3 with ECNU-4 to -6), the Google n-gram col-
lection (see ISI and UTD), or a ?deep? semantic
resource (FBK IRST uses Cyc). Yet, most of these
resources are also included in the less successful
systems, so beneficial integration of knowledge
sources into semantic relation classification seems
to be difficult.
System Combination. The differences between
the systems suggest that it might be possible to
achieve improvements by building an ensemble
37
system. When we combine the top three systems
(UTD, FBK IRST-12VBCA, and ISI) by predict-
ing their majority vote, or OTHER if there was none,
we obtain a small improvement over the UTD sys-
tem with an F
1
-Score of 82.79%. A combination of
the top five systems using the same method shows
a worse performance, however (80.42%). This sug-
gests that the best system outperforms the rest by
a margin that cannot be compensated with system
combination, at least not with a crude majority vote.
We see a similar pattern among the ECNU systems,
where the ECNU-SR-7 combination system is out-
performed by ECNU-SR-5, presumably since it
incorporates the inferior ECNU-SR-1 system.
Relation-specific Analysis. We also analyze the
performance on individual relations, especially the
extremes. There are very stable patterns across all
systems. The best relation (presumably the eas-
iest to classify) is CE, far ahead of ED and MC.
Notably, the performance for the best relation is
75% or above for almost all systems, with compar-
atively small differences between the systems. The
hardest relation is generally IA, followed by PP.
5
Here, the spread among the systems is much larger:
the highest-ranking systems outperform others on
the difficult relations. Recall was the main prob-
lem for both IA and PP: many examples of these
two relations are misclassified, most frequently as
OTHER. Even at TD4, these datasets seem to be
less homogeneous than the others. Intriguingly, PP
shows a very high inter-annotator agreement (Ta-
ble 1). Its difficulty may therefore be due not to
questionable annotation, but to genuine variability,
or at least the selection of difficult patterns by the
dataset creator. Conversely, MC, among the easiest
relations to model, shows only a modest IAA.
Difficult Instances. There were 152 examples
that are classified incorrectly by all systems. We
analyze them, looking for sources of errors. In ad-
dition to a handful of annotation errors and some
borderline cases, they are made up of instances
which illustrate the limits of current shallow mod-
eling approaches in that they require more lexical
knowledge and complex reasoning. A case in point:
The bottle carrier converts your <e1>bottle</e1>
into a <e2>canteen</e2>. This instance of
OTHER is misclassified either as CC (due to the
5
The relation OTHER, which we ignore in the overall F
1
-
score, does even worse, often below 40%. This is to be ex-
pected, since the OTHER examples in our datasets are near
misses for other relations, thus making a very incoherent class.
nominals) or as ED (because of the preposition
into). Another example: [...] <e1>Rudders</e1>
are used by <e2>towboats</e2> and other ves-
sels that require a high degree of manoeuvrability.
This is an instance of CW misclassified as IA, prob-
ably on account of the verb use which is a frequent
indicator of an agentive relation.
5 Discussion and Conclusion
There is little doubt that 19-way classification is a
non-trivial challenge. It is even harder when the
domain is lexical semantics, with its idiosyncrasies,
and when the classes are not necessarily disjoint,
despite our best intentions. It speaks to the success
of the exercise that the participating systems? per-
formance was generally high, well over an order
of magnitude above random guessing. This may
be due to the impressive array of tools and lexical-
semantic resources deployed by the participants.
Section 4 suggests a few ways of interpreting
and analyzing the results. Long-term lessons will
undoubtedly emerge from the workshop discussion.
One optimistic-pessimistic conclusion concerns the
size of the training data. The notable gain TD3?
TD4 suggests that even more data would be helpful,
but that is so much easier said than done: it took
the organizers well in excess of 1000 person-hours
to pin down the problem, hone the guidelines and
relation definitions, construct sufficient amounts of
trustworthy training data, and run the task.
References
X. Carreras and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(4):i?xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2009. SemEval-2010 Task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. NAACL Workshop
on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of BioNLP?09 shared task on event
extraction. In Proc. BioNLP-09, Boulder, CO.
38
Combining Word Sense and
Usage for Modeling Frame
Semantics
Diego De Cao1
Danilo Croce1
Marco Pennacchiotti2
Roberto Basili1
1University of Rome Tor Vergata (Italy)
2University of the Saarland (Germany)
email: decao@info.uniroma2.it
Abstract
Models of lexical semantics are core paradigms in most NLP applica-
tions, such as dialogue, information extraction and document understand-
ing. Unfortunately, the coverage of currently available resources (e.g.
FrameNet) is still unsatisfactory. This paper presents a largely applicable
approach for extending frame semantic resources, combining word sense
information derived fromWordNet and corpus-based distributional infor-
mation. We report a large scale evaluation over the English FrameNet,
and results on extending FrameNet to the Italian language, as the basis of
the development of a full FrameNet for Italian.
85
86 De Cao, Croce, Pennacchiotti, and Basili
1 Introduction and Related Work
Models of lexical meaning are explicit or implicit basic components of any text pro-
cessing system devoted to information extraction, question answering or dialogue.
Several paradigms proposed for a variety of notions, such as word sense (Miller et al,
1990) or frame semantics (Baker et al, 1998), have given rise to large scale resources,
respectively WordNet and FrameNet. Recent studies (e.g. Shen and Lapata (2007))
show that the use of FrameNet can potentially improve the performance of Question
Answering systems. Yet, Shen and Lapata (2007) also point out that the low cov-
erage of the current version of FrameNet significantly limits the expected boost in
performance. Other studies have shown similar evidences for Recognizing Textual
Entailment (RTE) (Clark et al, 2007; Burchardt et al, 2008): most examples of the
RTE challenges corpora can be solved at the predicate-argument structure level, but
FrameNet coverage is still a major problem.
Approaches to (semi-)automatically acquire frame information are then today a
priority to solve these problems. Despite this, not many efforts have been paid so far
in this direction. Burchardt et al (2005) presented Detour, a system for predicting
frame assignment of potential lexical units not covered by FrameNet, by using the
paradigmatic information enclosed in WordNet. Although the authors do not fully
solve the problem related to the fuzzy relationships between senses and frames, they
propose an empirical association measure for ranking frame candidates according to
sense information as stored in WordNet. To our knowledge, this is the only work
trying to bridge frame membership to referential properties of lexical senses. Pitel
(2006) presents a preliminary study on the applicability of semantic spaces and space
geometrical transformations (namely, Latent Semantic Analysis) to expand FrameNet,
but the investigation is too limited in scope to draw relevant conclusions. Finally, Pad?
et al (2008) propose a method to automatically label unknown semantic roles of event
nominalizations in FrameNet, but their method needs a large amount of annotated
verbal data.
Another important limitation of FrameNet is the limited support to multilingual-
ity, which is becoming a critical issue in real NLP applications. In recent years,
some efforts have focused on the manual adaptation of the English FrameNet to other
languages (e.g., German (Burchardt et al, 2006) and Spanish (Subirats and Petruck,
2003)). Unlike PropBank, FrameNet is in fact suitable to cross-lingual induction, as
frames are mostly defined at the conceptual level, thus allowing cross-lingual interpre-
tation. Yet, all these efforts consist in manually defining frame linguistic knowledge
(e.g. lexical units) in the specific language, and in annotating a large corpus, thus
requiring a large human effort. While attempts to automate the annotation process
are quite promising (Pado and Lapata, 2007), they require the availability of a par-
allel corpus, and leave open the issue of inducing the resource as a whole in a new
language.
In this work, we investigate novel methods for automatically expanding the English
FrameNet, and supporting the creation of new ones in other languages (namely Ital-
ian), thus tackling the abovementioned problems of coverage and multilinguality. The
proposed methods are inspired by the basic hypothesis that FrameNet can be automat-
ically modeled by a fruitful interaction between advanced distributional techniques,
and paradigmatic properties derived from WordNet.
Combining Word Sense and Usage for Modeling Frame Semantics 87
In particular, in this paper we focus on the application of such methods to study
the semantics of the core elements of FrameNet, i.e. the lexical units (hereafter LUs).
Lexical units are predicates (nouns, verbs, adjectives, etc.) that linguistically express
the situation described by a frame. Lexical units of the same frame share semantic
arguments. For example the frame KILLING has lexical units such as: assassin, blood-
bath, fatal, massacre, kill, suicide. These predicates share semantic arguments such
as KILLER, INSTRUMENT and VICTIM. Our goal is to combine corpus distributional
evidence with WordNet information to supply three tasks: the induction of new LUs
not already in FrameNet (unknown LUs); the reduction of LUs polysemy by mapping
them to WordNet synsets; the translation of English LUs into Italian.
The paper is organized as follows. In Section 2 we describe our FrameNet paradig-
matic and distributional model, and we discuss how these two models can be com-
bined in a single framework. In Section 3 we analyze the applicability of these models
to the three proposed experimental tasks, and discuss the results. Finally, in Section 4
we draw final conclusions and outline future work.
2 Paradigmatic and distributional models of frame semantics
In this section we describe our paradigmatic, distributional and combined models for
representing FrameNet. The general goal of each of these three methods is to offer
a computational model of FrameNet. In such a model, frames and LUs should have
a specific computational representation (e.g. vectors), and allow the computation of
similarity either among different LUs or between a frame and a LU. Such model thus
offers explicit means to use FrameNet in a NLP task or to expand FrameNet, e.g. by
assigning unknown LUs to its most similar frame, or by mapping a LU to its proper
WordNet synset(s). A key notion for these tasks is the definition of a principled and
reliable semantic similarity measure sim to be applied to frames and LUs.
2.1 Paradigmatic model
The basic intuition behind our paradigmatic model is that knowledge about predicates
of a frame, through a (possibly limited) set of LUs, allows to detect the set of the
suitable WordNet senses able to evoke the same frame. These senses are topologically
related to (one or more) sub-hierarchies capturing the lexical semantics implicit in the
frame. We propose a weakly-supervised approach to discover these structures. The
main idea is that frames correspond to specific sub-graphs of the WordNet hyponymy
hierarchy, so that these latter can be used to predict frames valid for other LUs, not
yet coded in FrameNet. Figure 1 reports the WordNet sub-hierarchy covering the
frame PEOPLE_BY_AGE: here, the frame?s nominal LUs {adult, adolescent, baby,
boy, infant, kid, geezer, teenager, youngster, youth} are all represented with the senses
correctly referring to the frame. The correct senses (e.g. sense 1 of youth out of its
6 potential senses) are selected as they share most specific generalizations with the
other LUs. This graph can be intended as an ?explanation? of the lexical semantic
properties characterizing the frame: future predictions about new LUs can be done on
the basis of the graph as a paradigmatic model for PEOPLE_BY_AGE. We call such a
graph the WordNet model of the frame. As WordNet organizes nouns, verbs and other
parts-of-speech in different hierarchies, three independent WordNet models (one for
each part-of-speech) are created for each frame.
88 De Cao, Croce, Pennacchiotti, and Basili
Formally, given the set F of the LUs of a frame, a WordNet model is built around
the subset SF of WordNet synsets able to generalize the largest number of words in
F1.
Figure 1: The WordNet model for the frame People_by_Age as evoked by the set of
its nouns. Sense numbers #n refer to WordNet 2.0
The WordNet modelWNF(?,W ) of a frame F , is a graph
(3) WNF(?,W ) =<W,SF ,LF ,h,simWN ,m >
where: W ? F are the subset of all LUs in F having the same part-of-speech ? ?
{verb,noun,ad jective}, SF is the subset of synsets in WN needed to generalize words
w ?W ; LF ? SF are the lexical senses of w ?W subsumed by SF ; h ? SF ?SF is the
projection of the hyponymy relation of WN in SF ; m?W ?2LF is the lexical relation
between words w ?W and synsets in LF ; simWN : SF ? ? is a weighting function that
expresses the relevance of each sense ? ? SF for the frame, as it is represented by its
words in F .
The model exemplified in Figure 1 is
WNPeople_by_Age(noun, {adult, ..., youth}), where LF = {adult#1, adolescent#1, baby#1,
boy#1, boy#2, boy#3, ..., youth#1} and the set SF corresponds to the sub-hierarchies dom-
inated by the synsets #6026, #9622621, #9285271 and #9015843.
The overall goal of computing the WordNet model is to determine the similarity
function simWN : SF ? ?, expressing the relevance of a synset ? ? SF as a good
representative of a frame F . This is what is hereafter referred to as the paradigmatic
similarity model between words senses and frames.
Paradigmatic Similarity measures
Given the WordNet hierarchy separation on part-of-speaches, the similarity function
simWN is independently defined for verbs, nouns and adjectives.
1In the following, we will use the same notation for a frame F and for the set of its known lexical units,
as in our approach we use LU membership as a basic definition of a frame.
Combining Word Sense and Usage for Modeling Frame Semantics 89
For nouns we adopt conceptual density (cd) (Agirre and Rigau, 1996; Basili et al,
2004), a semantic similarity measure defined for word sense disambiguation tasks.
The cd score for a sense ? ? SF is the density of the WordNet sub-hierarchy rooted
at ? in representing the set of nouns in F . The intuition behind this model is that the
larger is the number of all and only LUs in F that are generalized by ?, the better
it captures the lexical semantics intended by the frame. Coarse generalizations (i.e.
synsets higher in the hierarchy) are penalized, as they give rise to bushy hierarchies,
covering too many words not in the target F . The greedy algorithm proposed in Basili
et al (2004) selects the subset of synsets able to ?cover? (i.e. generalize) all the input
words and characterized by the highest cd values. The set of such synsets and their
corresponding sub-hierarchies forms a graph derived from a set of LUs F . The result
is the WordNet model WNF for F , i.e. the minimal subset of WordNet that explains
all (the possible) LUs in F with the maximally similar senses.
Figure 1 shows that correct senses (e.g. the sense 1 of youth out of the 6 potential
senses) are generally detected and preserved in the model. Irrelevant senses that do
not share any common hypernym with other words in F are neglected. Conceptual
density scores can be used to rank individual senses as in the case of boy.
Given a frame F , the above model can be naturally used to compute the similarity
between a noun n /? F and F . This is particularly useful in LU induction task, as de-
scribed in Section 3.1. To do so, the similarity simWN(F,n) between n and F is derived
by computing the cd scores over the set F ?{n}. The simWN(F,n) is the maximal cd
of any synset ?n ? SF that is also hypernym of a lexical sense of n. In the exam-
ple, the noun boy would receive a score of 0.117 through the hypernym {child,kid},
according to its third sense in WordNet 2.0 (?{son,boy}?).
As conceptual density can be only applied to nouns, when verbs v are consid-
ered, we exploit the synonymy and co-hyponymy relations. The following similarity
simWN(F,v) is computed:
(4) simWN(F,v) =
?
?
?
1 iff ?K ? F such that |K| > ? AND
?w ? K w is a co-hyponym of v
? otherwise
For adjectives, the similarity simWN(F,a), is computed on the basis of the syn-
onymy relation, as follows:
(5) simWN(F,a) =
?
?
?
1 iff ?w ? F such that
w is a synonym of tw
? otherwise
The overall model WNF is used to predict if a frame F is a correct situation for a
given unknownLU ul /?F (a noun, a verb or an adjective), whenever simWN(F,ul) > ?.
This can be used as a frame predictor for a ul currently not foreseen in the Berkley
database but possibly very frequent in a specific corpus, as described in Section 3.1.
2.2 Distributional model
The distributional model is based on the intuition that FrameNet frames and LUs can
be modelled in a semantic space, where they are represented as distributional co-
occurrence vectors computed over a corpus. Such framework, it is possible to compute
90 De Cao, Croce, Pennacchiotti, and Basili
the similarity between a LU and a frame, by evaluating the distance of their vectors in
the space.
Semantic spaces have been widely applied in several NLP tasks, ranging from in-
formation retrieval to paraphrase rules extraction (Lin and Pantel, 2001). The intuition
is that the meaning of a word can be described by the set of textual contexts in which
it appears (Distributional Hypothesis (Harris, 1964)), and that words with similar vec-
tors are semantically related. This distributional approach has been often claimed to
support the language in use view on meaning. Word space models (Sch?tze, 1993)
have been shown to emphasize different aspects of lexical semantics: associative (i.e.
topical) information between words, as well as paradigmatic information (i.e. in ab-
sentia) or syntagmatic information (i.e. in presentia).
In our setting, the goal is to leverage semantic spaces to capture the notion of frame
? i.e. the property of ?being characteristic of a frame?. To do so, we model a lexical
unit l as a vector ~l, whose dimensions represent the set of contexts of the semantic
space. In our space, contexts are words appearing in a n-window of the lexical unit:
such a space models a generic notion of semantic relatedness ? i.e. two LUs close in
the space are likely to be either in paradigmatic or syntagmatic relation (Pado, 2007;
Sahlgren, 2006). The overall semantic space is then represented by a matrix M, whose
rows describe LUs and whose columns describe contexts.
We reduce in dimensionality the matrix M by applying Singular Value Decom-
position (SVD) (Landauer and Dumais, 1997), a decomposition process that creates
an approximation of the original matrix, aiming to capture semantic dependencies
between source vectors, i.e. contexts. The original space is replaced by a lower di-
mensional space Mk, called k-space in which each dimension is a derived concept.
The matrix M is transformed in the product of three new matrices: U , S, and V such
that M = USVT . Truncating M to its first k dimensions means neglecting the least
meaningful dimensions according to the original distribution. Mk captures the same
statistical information in a new k-dimensional space, where each dimension is a linear
combination of some original features. These newly derived features may be thought
of as artificial concepts, each one representing an emerging meaning component as a
linear combination of many different words (or contexts).
The SVD reduction has two main advantages. First, the overall computational cost
of the model is reduced, as similarities are computed on a space with much fewer
dimensions. Secondly, it allows to capture second-order relations among LUs, thus
improving the quality of the similarity measure.
Once the vectors~l for all FrameNet LUs are available in the reduced space, it is
also possible to derive a vectorial representation ~F of a whole frame F . Intuitively,
~F should be computed as the geometric centroid of the vectors of its lexical units.
Unfortunately, such a simple approach is prone to errors due to the semantic nature
of frames. Indeed, even if the LUs of a given frame describe the same particular
situation, they can typically do that in different type of contexts. For example, the
LUs assassinate and holocaust evoke the KILLING frame, but are likely to appear in
very different linguistic contexts. Then, the vectors of the two words are likely to be
distant in the space. Consequently, different regions of the semantic space may act
as good representations for the same frame: these regions corresponds to clusters of
LUs which appear in similar contexts (e.g. {holocaust,extermination,genocide} and
Combining Word Sense and Usage for Modeling Frame Semantics 91
{suicide,euthanasia}).
We then adopt a clustering approach to model frames: each frame is represented
by the set of clustersCF of its lexical units. ClustersCF are composed by lexical units
close in the space and can have different size. They are computed by using an adaptive
(unsupervised) algorithm, based on k-means (Heyer et al, 1999; Basili et al, 2007),
applied to all known LUs of F . Each cluster c ? CF is represented in the space by a
vector~c, computed as the geometric centroid of all its lexical units.
In this framework, it is then possible to compute the similarity between an unknown
LU ul and a frame F , as the the cosine distance between the vector ~ul and the closest
centroid ~c ?CF :
(6) simLSF (F,ul) = argmaxc?CF simcos(~ul, ~CF )
Given this measure, it is finally possible to assign an unknown ul to one or more of
the most similar frames.
2.3 Combining paradigmatic and distributional models
In order to be effective in a NLP task, a model of lexical meaning should typically ac-
count for both the paradigmatic and distributional similarity. The following definition
thus hold:
(7) ?(F,w) = ?(D(w,F),P(w,F))
where ?(F,w) is the association between a word w and a frame F , ? is a com-
position operator applied to the corpus-driven distributional measure D(F,w) and
to the paradigmatic similarity P(F,w). Notice that in this work simLSF (F,w) and
simWN(F,w) are used as models of D(F,w) and P(F,W ), respectively. Different com-
binations ? are here possible, from simple algebraic operations (e.g. linear combina-
tions) to more complex algorithmics. We will explore this latter issue in section 3.1
where the evaluation of a combined model for LU induction is reported.
3 Experiments
In this section we experiment our proposed models on three different tasks: induc-
tion of new LUs (Section 3.1), mapping LUs to WordNet synsets (Section 3.2), and
automatic acquisition of LUs in Italian (Section 3.3). In all the experiments we use
FrameNet 1.3, consisting of 795 frames and about 10,196 LUs (7,522 unique LUs), as
source information and as a gold standard. As regardsWordNet, we adopt version 2.0,
with all mappings from 1.6 applied through the Italian component of MultiWordNet
(Pianta et al, 2008)2
For computing vectors in the distributional model, we use the TREC-2002 Vol.2
corpus, consisting of about 110 million words for English. The contexts for the de-
scription of LUs are obtained as ?5 windows around each individual LU occurrence:
each word occurring in this windows is retained as a potential context 3. A resulting
set of about 30,000 contexts (i.e. individual words) has been obtained. The vector~l
2http://www.lsi.upc.es/~nlp/tools/download-map.php
3For all occurrences of feature words, the POS tag has been neglected in order to verify the applicability
of the model even with a shallow preprocessing. Words occurring less than 10 times in all windows are
neglected in our experiments.
92 De Cao, Croce, Pennacchiotti, and Basili
representing an individual LU is derived by computing pointwise mutual information
between the LU and each context. The SVD reduction has been run over the result-
ing 7.522? 30,000 matrix, with a dimension cut of k = 50, other values resulting in
non-statistically different outcomes.
Experiments for Italian are run against the italian component of the Europarliament
corpus (Koehn, 2002), made of about 1 million sentences for about 36 millions tokens,
for which about 87,000 contexts are used for the targeted LUs. Also for Italian a
dimension cut of k = 50 has been applied.
3.1 Lexical Unit induction
The goal of this experiment is to tackle the FrameNet low coverage problem, by check-
ing if our models are good in expanding FrameNet with new LUs. Formally, we de-
fine LU induction as the task of assigning a generic unknown lexical unit ul not yet
present in the FrameNet database to the correct frame(s). As the number of frames
is very large, the task is intuitively hard to solve. A further complexity regards mul-
tiple assignments. Lexical units are sometimes ambiguous and can then be mapped
to more than one frame (for example the word tea could map both to FOOD and SO-
CIAL_EVENT). Also, even unambiguous words can be assigned to more than one
frame ? e.g. child maps to both KINSHIP and PEOPLE_BY_AGE.
In the experiment, we simulate the induction task by executing a leave-one-out
procedure over the set of existing FrameNet LUs, as follows. First, we remove a LU
from all its original frames. Then, we ask our model to reassign it to the most similar
frame(s), according to its similarity measure. We repeat this procedure for all lexical
units and compute the accuracy in the assignment.
We experiment all three models: distributional, paradigmatic and the combined
one. In particular, the combined model is applied as follows. First, for each frame
F we create its cluster set CF in the LSA space. Then, at each iteration of the leave-
one-out a different LU ul is removed from FrameNet, and the following steps are
performed:
? We recompute the clusters for all frames Ful which contain ul, by neglecting
ul.4
? We compute the similarity simLFS(F,ul) between ul and all frames. During
the computation we empirically impose a threshold: if a cluster c ? C has a
similarity cos(c,ul) < 0.1 (i.e. poorly similar to ul), it is neglected. Finally, all
suggested frames are ranked according to simLFS(F,ul).
? For each frame F we also compute the similarity simWN(F,ul) according to the
paradigmatic model, by neglecting ul in the computation of the WordNet model
of each frame.
? We combine the distributional and paradigmatic similarities following the gen-
eral Equation 7, by applying the following specific equation:
(8) sim(F,ul) = simLFS(F,ul) ? simWN(F,ul)
4Note that all appearances of ul in the database are neglected (irrespectively from its POS tag, e.g.
march as a verb vs. march as a noun)
Combining Word Sense and Usage for Modeling Frame Semantics 93
Table 1: The Gold Standard for the test over English and Italian
English Number of frames: 220
Number of LUs: 5042
Most likely frames: Self_Motion (p=0.015), Clothing (p=0.014)
Italian Number of frames: 10
Number of LUs: 112
Frames: Buildings, Clothing, Killing, Kinship, Make_noise
Medical_conditions, Natural_Features, Possession, Self_Motion, Text
Note, that in practice sim(F,ul) acts as a re-ranking function of the previously
obtained clusters CF
? We execute LU induction, by mapping ul to the most similar k frames according
to sim(F,ul).
Evaluation
We evaluate the model by computing the accuracy over the FrameNet gold standard.
Accuracy is defined as the fraction of LUs that are correctly re-assigned to the original
frame during the leave-one-out. Accuracy is computed at different levels k: a LU is
correctly assigned if its gold standard frame appears among the best-k frames ranked
by the model. We experimented both on English (using FrameNet version 1.3), and on
Italian. Since an Italian FrameNet is not available, we manually created a gold stan-
dard of 11 frames. Overall statistics on the data are reported in Table 1: the number of
frames and LUs analysed is slightly reduced wrt FrameNet as we ignored the predicate
words absent from the targeted corpus (e.g. moo in MAKE_NOISE) and multiwords
expressions as it was not possible to locate them unambiguously in the corpus (e.g.
shell out in COMMERCE_PAY). Also, in order to get reliable distributional statistics,
we filter out LUs occurring less than 50 times in the corpus, and frames with less than
10 LUs.
For all the experiments, the parameter ? in the Equation 4 of the paradigmaticmodel
has been set to 2. As a baseline, we adopt a model predicting as best-k frames the most
likely ones in FrameNet ? i.e. those containing the highest number of LUs.
Results for English are reported in Figure 2.
As shown, all methods improve significantly the baseline whereas accuracy values
naturally improve along increasing values for k. The performance of the paradigmatic
model are significantly high even for very small k. The best model is given by the
combination of distributional and paradigmatic similarity, producing significant im-
provements wrt the paradigmatic model alone.
Results for Italian are reported in Figure 3. The leave-one-out test has been applied
as for English, but over a manually compiled set of 527 LUs for the 11 frames used
as gold standard. These LUs have been obtained via direct translation of the English
Framenet LUs. In order to evaluate LUs for which a consistent distributional model
was available, only those occurring at least 50 times in the Europarliament corpus have
been selected: this amounts to a total number of 112 Italian LUs. The paradigmatic
94 De Cao, Croce, Pennacchiotti, and Basili
Figure 2: Accuracy of the leave-one-out over the English FrameNet 1.3
model for the test has been obtained using as source the LUs in the English FrameNet.
As the computation of the simWN(F,w) depends only on the hyponymy hierarchy, for
each Italian noun n the conceptual density computation over the set {n}?F is applied,
where F is given by the LUs in English. The interlingual index is here used to map
every n to its lexical senses (i.e. synsets) in the English WN. Then, the computation
of the greedy algorithm is applied exactly as in the monolingual process. The same
approach has been used for verbs (Equation 4) and adjectives (Equation 5).
Although the limited scale of the experiment (only 11 frames are targeted), the ev-
idence are similar as for the test over English: the combined model is always superior
to the individual ones. High levels of accuracy are achieved, although the ?most likely
frame? baseline is much higher than for the English test. Similar trends are also ob-
served for the paradigmatic model, reaching a plateau for smaller values of k. Overall
results indicate that reliable predictions can be obtained for unknown LUs also when
a whole Italian FrameNet is not yet available. Our method can then be used to support
lexicographers in the task of building a new FrameNet, in the specific stage of adding
LUs to frames.
Results suggest that the WordNet models derived from the English LUs are valid
predictors also for Italian words, as confirmed by the experiments in the next sections.
3.2 Assessing WordNet models of Frames
The goal of the experiment is to validate the notion of WordNet model of a frame as
derived through the method discussed in Section 2.1. Formally, given the set of all
possible WordNet senses Sl of a given LU l, we aim at mapping each sense s ? Sl to
the correct frame f ? Fl , where Fl is the set of frames in which l appears. If a frame
cannot be found for a given sense, the sense is simply neglected.
For example, the LU burn has 15 senses in WordNet and it belongs to 3 frames:
EMOTION_HEAT, EXPERIENCE_BODILY_HARM and PERCEPTION_BODY. Figure 2
Combining Word Sense and Usage for Modeling Frame Semantics 95
Figure 3: Accuracy of the leave-one-out tests over 11 frames in Italian
reports some of the possible correct mapping between senses and frames. Other
senses, such as ?destroy by fire? cannot be mapped to any existing frame.
By creating such an automatic mapping we achieve three goals. First, we disam-
biguate FrameNet lexical units. Second, we enrich WordNet synsets with new infor-
mation ? i.e. a computational description of the situations they refer to, as repre-
sented in FrameNet. Third, we derive a language independent model of frames based
on WordNet synsets.
The mapping targeted by the experiment is carried out according to the discussion
in Section 2.1. The WordNet model of a frame F for nouns is the outcome of the
greedy cd computation over the set F of all frame?s LUs: given a LU, a sense is
accepted if it is a member of the set LF in the model. For verbs and adjectives all co-
hyponyms and synonyms used in Equations 4 and 5 are included in LF . The procedure
for developing a WordNet model is completely automatic, this avoiding the costs of
manual annotation.5
Note that our approach is easily portable to languages different from English. In-
deed, the WordNet hierarchy is the backbone of sense repositories in other languages
(as for example in MultiWordNet (Pianta et al, 2008)). The English modelsWNF can
be then interpreted in a different language, by applying the interlingual indexes to all
synsets LF in WNF . The corresponding sets of synonyms are natural candidates as
LUs in the target language.
Evaluation
In this experiment, in order to account for data sparseness we reduce the dataset in
two ways. First, we neglect low frequency lexical units: LUs occurring less than 50
times in the corpus are not considered. Second, we exclude frames that have less than
5In this experiment we focus on verbs and nouns, since they are core predicates expressing the targeted
situation in sentences.
96 De Cao, Croce, Pennacchiotti, and Basili
Table 2: Mapping between WordNet senses and frames for verb burn, as induced by
the paradigmatic method
Synset Evoked FRAME Co-Hyponyms WordNet Definition
1775952 EMOTION_HEAT chafe, fume, smolder Feel strong emotion, especially anger or
passion; ?She was burning with anger?;
?He was burning to try out his new
skies?
189569 EXPERIENCE_BODILY_HARM break, bruise, hurt,
injure
Burn with heat, fire, or radiation; ?The
iron burnt a hole in my dress?
2059143 PERCEPTION_BODY itch, sting Cause a sharp or stinging pain or dis-
comfort; ?The sun burned his face?
10 LUs. This leaves us with 220 frames, involving 2,200 nominal LUs and 2,180
verbal LUs. Table 3 reports overall statistics. Over the 2,200 nouns and 2,180 verbs
examined, the vast majority is covered by WordNet (fourth row). For these words, a
large set of lexical senses exist in WordNet giving an average polysemy between 3 and
6 senses per word (sixth row). Our paradigmatic method is able to significantly reduce
the average polysemy: only 1.79 senses per verb survive among the initial 5.29, while
only 1.29 among the 3.62 are retained for nouns. Moreover, the number of senses
used to entirely represent a frame in a paradigmatic model (i.e. SF ) is about 3,512 and
2,718 respectively for nouns and verbs, as averaged across all frames. An example of
the mapping produced by our method is reported in Table 2.
The above statistics suggest that a consistent reduction in average polysemy can be
obtained when the context of a frame is used to model semantic similarity among LUs
in WordNet.
Table 3: Statistics on nominal and verbal senses in the paradigmatic model of the
English FrameNet
Nouns Verbs
Targeted Frames 220 220
Involved LUs 2,200 2,180
Average LUs per frame 10.0 9.91
LUs covered by WordNet 2,187 2,169
Number of Evoked Senses 7,443 11,489
Average Polysemy 3.62 5.97
Represented words (i.e. ?FWF ) 2,145 1,270
Average represented LUs 9.94 9.85
Active Lexical Senses (LF ) 3,095 2,282
Average Active Lexical Senses (|LF |/|WF |) per word over frames 1.27 1.79
Active synsets (SF ) 3,512 2,718
Average Active synsets (|SF |/|WF |) per word over frames 1.51 2.19
We evaluated the quality of the above process through manual validation. Given a
frame, for each LU we provided two annotators with the list of all its WordNet senses,
and asked to select those that correctly map to FrameNet. Then, we evaluated our
automatic mapping method by computing standard Precision and Recall. In all, we
Combining Word Sense and Usage for Modeling Frame Semantics 97
analysed all 786 senses of 306 LUs in 4 frames (i.e. KILLING, PEOPLE_BY_AGE,
STATEMENT and CLOTHING). The Cohen?s kappa, computed over two frames (i.e.
KILLING and PEOPLE_BY_AGE for 192 senses of 77 words) results in a 0.90 inter-
annotator agreement: this indicates that senses and frames are highly correlated and
their mapping is consistent and motivated, as Table 2 suggests.
The system is considered to accept a sense ? for a given frame F iff the conceptual
density score characterizing such a sense is positive, i.e. the ? ? LF . Our method
obtained a Precision of 0.803 and a Recall of 0.79 (F-measure=0.796). Among the 786
senses tested, 85 false positives and 92 false negatives have been found: 346 senses
have been correctly accepted and 263 true negatives have been rejected by the sytem.
It must be also noticed that the conceptual density scores obtained are well correlated
with correct senses. If senses of a word with significantly lower cd scores than others
are removed from the set LF of a frame, a significant improvement in precision can
be obtained. For example, tie in CLOTHING has 9 senses, of which 3 are proposed by
the system, corresponding to 1 true positives, 2 false positives and 6 true negatives.
It is interesting to note that the true positive sense (i.e. {necktie, tie} as ?a neckwear
consisting of a long narrow piece of material worn ...?) has a cd score of 0.492, while
0.018 is the score of all the three false positives (i.e. sense #5 {link,linkup tie, tie-in}
as ?a fastener that serves to join or link?; sense #8 {tie, railroad tie, crosstie, sleeper}
as ?one of the cross braces that support the rails on a railway track?; sense #9 {tie} as
?a cord with which something is tied?). A careful selection policy can be thus easily
devised to deal with such skewed preference distributions and achieve higher values
of precision by neglecting lower preferences.
These results show that the proposed frame WordNet model is not only effective
in reducing the average lexical polysemy (as shown in Table 3), but it is also a rather
accurate method to capture the lexical semantics implied by frames. The achieved
level of accuracy justifies the adoption of the model defined in (3) for the development
of FrameNets in languages other than English.
3.3 Development of an Italian FrameNet
In this section we explore the use of our English paradigmatic model to automatically
support the building process of a FrameNet in a different language, namely Italian. In
particular, we leverage the model WNF for the English language to induce new LUs
for F in the new language. To do so, we proceed as follows.
For each frame F in FrameNet we first generate the WordNet model for English
WNF using all the LUs available in the database, as discussed in Section 2.1. Then,
we use an interlingual index (e.g. MultiWordNet) to obtain words in the new language
corresponding to lexical senses LF in the model WNF . Each of these translated LU
l is a cross-lingual synonym of at least a sense in SF and is a candidate LUs for the
frame in the new language, since it satisfies simWN(F, l) > ?.
Evaluation
In the experiment we focus on Italian, for which a full FrameNet is not yet available,
though a manual building process is currently underway (Tonelli and Pianta, 2008).
As interlingual index we adopt the Italian component of MultiWordNet (Pianta et al,
2008). As shown in Table 4, the WordNet model allows to generate approximately
15,000 Italian LUs, partitioned in 6,600 nouns, 8,300 verbs and 130 adjectives.
98 De Cao, Croce, Pennacchiotti, and Basili
Table 4: Number of generated Lexical Units
Number of LUs
Nouns 6611
Verbs 8332
Adjectives 129
Total 15072
To evaluate the quality of the translated LUs we performed two different tests. In
the first test, we collected the 776 most frequent words in the Europarliament corpus,
including many generic nouns and verbs, such as produrre (to_produce/make), fare
(to_make/fabricate), avere (to_have). Then we manually validated all the 1,500 sys-
tem decisions regarding these words. A decision is accepted if the frame suggested
for the word is correct for at least one of its senses. Accuracy is computed as the per-
centage of the correct system decisions over the number of validated cases. For some
words no frame was predicted, as they were not in Wordnet or as no Wordnet model
was able to correctly generalize them. The percentage of words receiving at least one
correct prediction, i.e. assigned to at least one frame accepted by the annotators, is
here called Coverage, and reported with the accuracy scores in the second line of Ta-
ble 5. The above test was repeated also for more specific words, with a number of
occurrences in the corpus ranging from 100 to 200. Results are reported in the third
line of Table 5. These outcomes are surprisingly good especially considering that the
computation of the individual simWN(F, l) scores is fully automatic.
Table 5: Manual validation of the italian LUs generated through WordNet
Frequency Numb. of Numb. of
Range Test pairs Words Acc. Cov.
[722;55,000] 1,500 776 0.79 93.0%
[100;200] 558 357 0.87 94.3%
In a second test, the results generated by our method were compared against the
words of the oracle manually developed for the experiment in Section 3.1. In this
case, the predictions of the method about frames and words are compared with the
oracle. As no filter has been here applied with respect to the corpus, all the 527
< LU,Frame > pairs in the manually created oracle have been used6, although only
437 pairs were represented through the MultiWordNet resources. The system was
not able to decide for 71 words, and produced wrong guesses for 49 words: 317
correct guesses are thus produced. The results is a Precision of 0.87 and a Recall of
0.72. The recall value, lower than the coverage observed in the previous test, is also
by a significant generative effect: the method discovers a number of new entries not
accounted for in the oracle.
6Notice that no LU was multiply assigned to different frames in the oracle, so that the number of predi-
cate words here is exactly the number of different pairs.
Combining Word Sense and Usage for Modeling Frame Semantics 99
Table 6: Excerpt of the Italian LUs in four different frames
FRAME FRAME definition Italian LUs
BUILDINGS Words which name permanent
fixed structures forming an
enclosure and providing pro-
tection from the elements
autorimessa, cuccia, casolare,
casotto, dependance, masseria,
palazzina, ...
CLOTHING Anything that people conven-
tionally wear
cappotto, calzetta, cami-
cia_da_notte, duepezzi, ...
SELF_MOTION The Self mover, a living being,
moves under its own power in a
directed fashion
annaspare, arrancare, buttarsi,
claudicare, giro, ...
TEXT An entity that contains linguis-
tic, symbolic information on a
Topic, created by an Author at
the Time of creation
arringa, articolo_di_fondo,
canzonetta, conto, polizza,
vademecum, ...
Typical new LUs introduced in the oracle are words not accounted for in the En-
glish Framenet, as reported in Table 6. The table shows that most of the new guesses
of the system are indeed highly plausible. They represent widely used dialectal forms
(e.g. masseria in the frame BUILDING), jargon (e.g. duepezzi in CLOTHING), techni-
cal terms (e.g. polizza in TEXT) and specific nouns (e.g. autorimessa in BUILDINGS).
Although it is certainly questionable if words like articolo_di_fondo (i.e. main article
in a newspaper) are worth to be considering as LUs for the frame TEXT, it is clear that
if the application domain requires frame-like information, the presented model (even
only the paradigmatic association here discussed) provides an effective tool for fast
and robust prototyping. Notice that the low Recall (only 0.60 of the oracle words are
correctly addressed), can be compensated by combining paradigmatic and distribu-
tional similarity (Equation 8), as experiments reported in Figure 3 suggest. We leave
this last point as a future work.
4 Conclusions
We presented a combined model for representing frame semantics through paradig-
matic and distributional evidence. We reported three experiments, which indicate pos-
sible application scenarios of these models. First, the combination of the presented
models has been applied to extend FrameNet in a LU induction task, for English and
Italian. In both cases the evaluation has shown that the combination of the two models
achieves better performance against their independent uses, and that the level of accu-
racy is high enough to support lexicographers in the task of building FrameNets. In a
second experiment, we showed that a strong association exists between lexical senses,
as defined by WordNet, and the frame?s lexical units in FrameNet. Its automatic de-
tection, as proposed in this paper, results in a significant reduction of the polysemy of
LUs and in a highly accurate selection of those lexical senses semantically related to
the situations represented by a frame. Finally, we demonstrated that this paradigmatic
information can be used to develop a FrameNet resource in another language. For
100 De Cao, Croce, Pennacchiotti, and Basili
Italian, we automatically generated a very large and accurate set of 15,000 LUs.
The overall framework has encouraged us to develop a robust toolbox for the large
scale acquisition of FrameNet-like lexicons in different domains and languages. The
tool will be made publicly available for research studies in this area. Future work is on
going on the adoption of richer models for Framenet, able to take into account more
evidence than LUs, such as frame elements and syntagmatic information. Moreover,
the use of the derived space as a model for the recognition of frames in free-texts is
expected to speed-up the development of a large collection of annotated sentences for
the Italian language.
References
Agirre, E. and G. Rigau (1996). Word sense disambiguation using conceptual density.
In Proceedings of COLING-96, Copenhagen, Denmark.
Baker, C. F., C. J. Fillmore, and J. B. Lowe (1998). The Berkeley FrameNet project.
In Proceedings of COLING-ACL, Montreal, Canada.
Basili, R., M. Cammisa, and F. Zanzotto (2004). A semantic similarity measure for
unsupervised semantic disambiguation. In Proceedings of LREC-04, Lisbon, Por-
tugal.
Basili, R., D. D. Cao, P. Marocco, and M. Pennacchiotti (2007). Learning selectional
preferences for entailment or paraphrasing rules. In Proceedings of RANLP 07.
Burchardt, A., K. Erk, and A. Frank (2005). A wordnet detour to framenet. In Pro-
ceedings of the GLDV 2005 GermaNet II Workshop, Bonn, Germany.
Burchardt, A., K. Erk, A. Frank, A. Kowalski, S. Pado, and M. Pinkal (2006). The
salsa corpus: a german corpus resource for lexical semantics. In Proceedings of
the 5th International Conference on Language Resources and Evaluation, Genova,
Italy.
Burchardt, A., M. Pennacchiotti, S. Thater, and M. Pinkal (2008). Assessing the
impact of frame semantics on textual entailment. Journal of Natural Language
Engineering (to appear).
Clark, P., P. Harrison, J. Thompson, W. Murray, J. Hobbs, and C. Fellbaum (2007,
June). On the Role of Lexical and World Knowledge in RTE3. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, Prague, pp.
54?59. Association for Computational Linguistics.
Harris, Z. (1964). Distributional structure. In J. J. Katz and J. A. Fodor (Eds.), The
Philosophy of Linguistics, New York. Oxford University Press.
Heyer, L., S. Kruglyak, and S. Yooseph (1999). Exploring expression data: Identifi-
cation and analysis of coexpressed genes. Genome Research 9, 1106?1115.
Koehn, P. (2002). Europarl: A multilingual corpus for evaluation of machine transla-
tion. Draft.
Combining Word Sense and Usage for Modeling Frame Semantics 101
Landauer, T. and S. Dumais (1997). A solution to plato?s problem: The latent se-
mantic analysis theory of acquisition, induction and representation of knowledge.
Psychological Review 104, 211?240.
Lin, D. and P. Pantel (2001). DIRT-discovery of inference rules from text. In Proceed-
ings of the ACM Conference on Knowledge Discovery and Data Mining (KDD-01),
San Francisco, CA.
Miller, G., R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. (1990). An on-line
lexical database. International Journal of Lexicography 13(4), 235?312.
Pado, S. (2007). Cross-Lingual Annotation Projection Models for Role-Semantic In-
formation, Volume 21 of Saarbr?cken Dissertations in Computational Linguistics
and Language Technology. Saarland University.
Pado, S. and M. Lapata (2007). Dependency-based construction of semantic space
models. Computational Linguistics 33(2), 161?199.
Pad?, S., M. Pennacchiotti, and C. Sporleder (2008). Semantic role assignment for
event nominalisations by leveraging verbal data. In Proceedings of COLING 2008,
Manchester, UK.
Pianta, E., L. Bentivogli, and C. Girardi (2008). MultiWordNet: Developing an
aligned multilingual database. In Proceedings of the 1st International Global
WordNet Conference, Marrakech, Morocco, pp. 293?302.
Pitel, G. (2006). Using bilingual lsa for framenet annotation of french text from
generic resources. In Workshop on Multilingual Semantic Annotation: Theory and
Applications, Saarbr?ijcken, Germany.
Sahlgren, M. (2006). The Word-Space Model. Ph. D. thesis, Department of Linguis-
tics, Stockholm University.
Sch?tze, H. (1993). Word space. In S. Hanson, J. Cowan, and C. Giles (Eds.), Ad-
vances in Neural Information Processing Systems 5. MorganKaufmann Publishers.
Shen, D. and M. Lapata (2007). Using semantic roles to improve question answer-
ing. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing and on Computational Natural Language Learning, Prague, pp. 12?21.
Subirats, C. and M. Petruck (2003). Surprise! Spanish FrameNet! In Proceedings of
the Workshop on Frame Semantics at the XVII. International Congress of Linguists,
Prague.
Tonelli, S. and E. Pianta (2008). Frame Information Transfer from English to Italian.
In Proceedings of LREC 2008, Marrakech, Morocco.
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 31?32,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Detecting controversies in Twitter: a first study
Marco Pennacchiotti
Yahoo! Labs
Sunnyvale, CA.
pennac@yahoo-inc.com
Ana-Maria Popescu
Yahoo! Labs
Sunnyvale, CA.
amp@yahoo-inc.com
Social media gives researchers a great opportunity
to understand how the public feels and thinks about
a variety of topics, from political issues to entertain-
ment choices. While previous research has explored
the likes and dislikes of audiences, we focus on a
related but different task of detecting controversies
involving popular entities, and understanding their
causes. Intuitively, if people hotly debate an entity
in a given period of time, there is a good chance of a
controversy occurring. Consequently, we use Twit-
ter data, boosted with knowledge extracted from the
Web, as a starting approach: This paper introduces
our task, an initial method and encouraging early re-
sults.
Controversy Detection. We focus on detect-
ing controversies involving known entities in Twit-
ter data. Let a snapshot denote a triple s =
(e,?t, tweets), where e is an entity, ?t is a time
period and tweets is the set of tweets from the tar-
get time period which refer to the target entity.1. Let
cont(s) denote the level of controversy associated
with entity e in the context of the snapshot s. Our
task is as follows:
Task. Given an entity set E and a snapshot set
S = {(e,?t, tweets)|e ? E}, compute the con-
troversy level cont(s) for each snapshot s in S and
rank S with respect to the resulting scores.
Overall Solution. Figure 1 gives an overview of
our solution. We first select the set B ? S, consist-
ing of candidate snapshots that are likely to be con-
troversial (buzzy snapshots). Then, for each snap-
shot in B, we compute the controversy score cont,
by combining a timely controversy score (tcont) and
a historical controversy score (hcont).
Resources. Our method uses a sentiment lexi-
con SL (7590 terms) and a controversy lexicon CL
1We use 1-day as the time period ?t. E.g. s=(?Brad
Pitt?,12/11/2009,tweets)
Algorithm 0.1: CONTROVERSYDETECTION(S, Twitter)
select buzzy snapshots B ? S
for s ? B{
tcont(s) = ? ?MixSent(s) + (1? ?) ? Controv(s))
cont(s) = ? ? tcont(s) + (1? ?) ? hcont(s)
rank B on scores
return (B)
Figure 1: Controversy Detection: Overview
(750 terms). The sentiment lexicon is composed by
augmenting the set of positive and negative polarity
terms in OpinionFinder 1.5 2 (e.g. ?love?,?wrong?)
with terms bootstrapped from a large set of user
reviews. The controversy lexicon is compiled by
mining controversial terms (e.g. ?trial?, ?apology?)
from Wikipedia pages of people included in the
Wikipedia controversial topic list.
Selecting buzzy snapshots. We make the simple
assumption that if in a given time period, an entity is
discussed more than in the recent past, then a contro-
versy involving the entity is likely to occurr in that
period. We model the intuition with the score:
b(s) =
|tweetss|
(
?
i?prev(s,N)
|tweetsi|)/N
where tweetss is the set of tweets in the snapshot
s; and prev(s,N) is the set of snapshots referring to
the same entity of s, in N time periods previous to
s. In our experiment, we use N = 2, i.e. we focus
on two days before s. We retain as buzzy snapshots
only those with b(s) > 3.0.
Historical controversy score. The hcont score
estimates the overall controversy level of an entity
in Web data, independently of time. We consider
hcont our baseline system, to which we compare
the Twitter-based models. The score is estimated
on Web document data using the CL lexicon as fol-
2J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. In Language
Resources and Evaluation.
31
lows: hcont(e) = k/|CL|, where k is the number of
controversy terms t? s.t. PMI(e, t?) > A3.
Timely controversy score. tcont estimates the
controversy of an entity by analyzing the discussion
among Twitter?s users in a given time period, i.e. in
a given snapshot. It is a linear combination (tuned
with ? ? [0, 1]) of two scores:
MixSent(s): reflects the relative disagreement
about the entity in the Twitter data from snapshot
s. First, each of the N tweets in s is placed in one of
the following sets: Positive (Pos), Negative (Neg),
Neutral (Neu), based on the number of positive and
negative SL terms in the tweet. MixSent is com-
puted as:
MixSent(s) =
Min(|Pos|, |Neg|)
Max(|Pos)|, |Neg|)
?
|Pos|+ |Neg|
N
Controv(s): this score reflects the presence of
explicit controversy terms in tweets. It is computed
as: Controv(s) = |ctv|/N , where ctv is the set of
tweets in s which contain at least one controversy
term from CL.
Overall controversy score. The overall score
is a linear combination of the timely and historical
scores: cont(s) = ??tcont(s)+(1??)?hcont(s),
where ? ? [0, 1] is a parameter.
Experimental Results
We evaluate our model on the task of ranking snap-
shots according to their controversy level. Our cor-
pus is a large set of Twitter data from Jul-2009 to
Feb-2010. The set of entities E is composed of
104,713 celebrity names scraped from Wikipedia
for the Actor, Athlete, Politician and Musician cat-
egories. The overall size of S amounts to 661,226
(we consider only snapshots with a minimum of 10
tweets). The number of buzzy snapshots in B is
30,451. For evaluation, we use a gold standard of
120 snapshots randomly sampled from B, and man-
ually annotated as controversial or not-controversial
by two expert annotators (detailed guidelines will be
presented at the workshop). Kappa-agreement be-
tween the annotators, estimated on a subset of 20
snapshots, is 0.89 (?almost perfect? agreement). We
experiment with different ? and ? values, as re-
ported in Table 1, in order to discern the value of
final score components. We use Average Precision
3PMI is computed based on the co-occurrences of entities
and terms in Web documents; here we use A = 2.
Model ? ? AP AROC
hcont (baseline) 0.0 0.0 0.614 0.581
tcont-MixSent 1.0 1.0 0.651 0.642
tcont-Controv 0.0 1.0 0.614 0.611
tcont-combined 0.5 1.0 0.637 0.642
cont 0.5 0.5 0.628 0.646
cont 0.8 0.8 0.643 0.642
cont 1.0 0.5 0.660 0.662
Table 1: Controversial Snapshot Detection: results over
different model parametrizations
(AP), and the area under the ROC curve (AROC) as
our evaluation measures.
The results in Table 1 show that all Twitter-based
models perform better than the Web-based baseline.
The most effective basic model is MixSent, sug-
gesting that the presence of mixed polarity sentiment
terms in a snapshot is a good indicator of contro-
versy. For example, ?Claudia Jordan? appears in a
snapshot with a mix of positive and negative terms
-in a debate about a red carpet appearance- but the
hcont and Controv scores are low as there is no
record of historical controversy or explicit contro-
versy terms in the target tweets. Best overall per-
formance is achieved by a mixed model combining
the hcont and theMixSent score (last row in Table
label 1). There are indeed cases in which the evi-
dence from MixSent is not enough - e.g., a snap-
shot discussing ?Jesse Jackson? ?s appearance on a
tv show lacks common positive or negative terms,
but reflects users? confusion nevertheless; however,
?Jesse Jackson? has a high historical controversy
score, which leads our combined model to correctly
assign a high controversy score to the snapshot. In-
terestingly, most controversies in the gold standard
refer to micro-events (e.g., tv show, award show or
athletic event appearances), rather than more tradi-
tional controversial events found in news streams
(e.g., speeches about climate change, controversial
movie releases, etc.); this further strengthens the
case that Twitter is a complementary information
source wrt news corpora.
We plan to follow up on this very preliminary
investigation by improving our Twitter-based sen-
timent detection, incorporating blog and news data
and generalizing our controversy model (e.g., dis-
covering the ?what? and the ?why? of a controversy,
and tracking common controversial behaviors of en-
tities over time).
32
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 28?36,
Beijing, August 2010
Expanding textual entailment corpora from Wikipedia using co-training
Fabio Massimo Zanzotto
University of Rome ?Tor Vergata?
zanzotto@info.uniroma2.it
Marco Pennacchiotti
Yahoo! Lab
pennac@yahoo-inc.com
Abstract
In this paper we propose a novel method
to automatically extract large textual en-
tailment datasets homogeneous to existing
ones. The key idea is the combination of
two intuitions: (1) the use of Wikipedia
to extract a large set of textual entail-
ment pairs; (2) the application of semi-
supervised machine learning methods to
make the extracted dataset homogeneous
to the existing ones. We report empirical
evidence that our method successfully ex-
pands existing textual entailment corpora.
1 Introduction
Despite the growing success of the Recognizing
Textual Entailment (RTE) challenges (Dagan et
al., 2006; Bar-Haim et al, 2006; Giampiccolo et
al., 2007), the accuracy of most textual entailment
recognition systems are still below 60%. An in-
tuitive way to improve performance is to provide
systems with larger annotated datasets. This is es-
pecially true for machine learning systems, where
the size of the training corpus is an important fac-
tor. As a consequence, several attempts have been
made to train systems using larger datasets ob-
tained by merging RTE corpora of different chal-
lenges. Unfortunately, experimental results show
a significant decrease in accuracy (de Marneffe et
al., 2006). There are two major reasons for this
counter-intuitive result:
Homogeneity. As indicated by many studies (e.g.
(Siefkes, 2008)), homogeneity of the training cor-
pus is an important factor for the applicability of
supervised machine learning models, since exam-
ples with similar properties often imply more ef-
fective models. Unfortunately, the corpora of the
four RTE challenges are not homogenous. Indeed,
they model different properties of the textual en-
tailment phenomenon, as they have been created
using slightly (but significantly) different method-
ologies. For example, part of the RTE-1 dataset
(Dagan et al, 2006) was created using compara-
ble documents, where positive entailments have a
lexical overlap higher than negative ones (Nichol-
son et al, 2006; Dagan et al, 2006). Comparable
documents have not been used as a source of later
RTE corpora, making RTE-1 odd with respect to
other datasets.
Corpus size. RTE corpora are relatively small
in size (typically 800 pairs). The increase in
size obtained by merging corpora from different
challenges is not a viable solution. Much larger
datasets, of one or more order of magnitude, are
needed to capture the complex properties charac-
terizing entailment.
A key issue for the future development of RTE
is then the creation of datasets fulfilling two prop-
erties: (1) large size; (2) homogeneity wrt. ex-
isting RTE corpora. The task of creating large
datasets is unfeasible for human annotators. Col-
laborative annotation environments such as the
Amazon Mechanical Turk1 can help to annotate
pairs of sentences in positive or negative entail-
ment (Zaenen, submitted; Snow et al, 2008). Yet,
these environments can hardly solve the problem
of finding relevant pairs of sentences. Completely
automatic processes of dataset creation have been
proposed (Burger and Ferro, 2005; Hickl et al,
2006). Unfortunately, these datasets are not ho-
mogeneous wrt. to the RTE datasets, as they are
1http://mturk.com
28
created using different methodologies. In this pa-
per we propose a novel method to automatically
extract entailment datasets which are guaranteed
to be large and homogeneous to RTE ones. The
key idea is the combination of two factors: (1) the
use of Wikipedia as source of a large set of tex-
tual entailment pairs; (2) the application of semi-
supervised machine learning methods, namely co-
training, to make corpora homogeneous to RTE.
The paper is organized as follows. In Section 2
we report on previous attempts in automatically
creating RTE corpora. In Section 3 we outline im-
portant properties that these corpora should have,
and introduce our methodology to extract an RTE
corpus from Wikipedia (the WIKI corpus), con-
forming to these properties. In Section 4 we de-
scribe how co-training techniques can be lever-
aged to make the WIKI corpus homogeneous to
existing RTE corpora. In Section 5 we report em-
pirical evidence that the combination of the WIKI
corpus and co-training is successful. Finally, in
Section 6 we draw final conclusions and outline
future work.
2 Related Work
The first attempt to automatically create large
RTE corpora was proposed by Burger and
Ferro (Burger and Ferro, 2005), with the MITRE
corpus, a corpus of positive entailment examples
extracted from the XIE section of the Gigaword
news collection (Graff, 2003). The idea of the ap-
proach is that the headline and the first paragraph
of a news article should be (near-)paraphrase. Au-
thors then collect paragraph-headline pairs as Text
(T ) - Hypothesis (H) examples, where the head-
lines plays the role of H . The final corpus con-
sists of 100,000 pairs, with an estimated accuracy
of 70% ? i.e. two annotators checked a sample
of about 500 pairs, and verified that 30% of these
were either false entailments or noisy pairs. The
major limitation of the Burger and Ferro (Burger
and Ferro, 2005)?s approach is that the final cor-
pus consist only of positive examples. Because
of this imbalance, the corpus cannot be positively
used by RTE learning systems.
Hickl et al (2006) propose a solution to the
problem, providing a methodology to extract both
positive and negative pairs (the LCC corpus). A
positive corpus consisting of 101,000 pairs is ex-
tracted similarly to (Burger and Ferro, 2005). Cor-
pus accuracy is estimated on a sample of 2,500 ex-
amples, achieving 92% (i.e. almost all examples
are positives), 22 points higher than Burger and
Ferro. A negative corpus of 119,000 is extracted
either: (1) selecting sequential sentences includ-
ing mentions of a same named entity (98.000
pairs); (2) selecting pairs of sentences connected
by words such as even though, although, other-
wise, but (21,000 pairs). Estimated accuracy for
the two techniques is respectively 97% and 94%.
Hickl and colleagues show that expanding the
RTE-2 training set with the LCC corpus (the ex-
pansion factor is 125), their RTE system im-
proves 10% accuracy. This suggests that by ex-
panding with a large and balanced corpus, en-
tailment recognition performance drastically im-
proves. This intuition is later contradicted in a
second experiment by Hickl and Bensley (2007).
Authors use the LCC corpus with the RTE-3 train-
ing set to train a new RTE system, showing an im-
provement in accuracy of less than 1% wrt. the
RTE-3 training alone.
Overall, evidence suggests that automatic ex-
pansion of the RTE corpora do not always lead to
performance improvement. This highly depends
on how balanced the corpus is, on the RTE system
adopted, and on the specific RTE dataset that is
expanded.
3 Extracting the WIKI corpus
In this section we outline some of the properties
that a reliable corpus for RTE should have (Sec-
tion 3.1), and show that a corpus extracted from
Wikipedia conforms to these properties (Sec-
tion 3.2).
3.1 Good practices in building RTE corpora
Previous work in Section 2 and the vast literature
on RTE suggest that a ?reliable? corpus for RTE
should have, among others, the following proper-
ties:
(1) Not artificial. Textual entailment is a complex
phenomenon which encompasses different lin-
guistic levels. Entailment types range from very
simple polarity mismatches and syntactic alterna-
tions, to very complex semantic and knowledge-
29
S?1 In this regard, some have charged the New World Translation Committee with being inconsistent.
S?2 In this regard, some have charged the New World Translation Committee with not be consistent.
S??1 The ?Stockholm Network? is Europe?s only dedicated service organisation for market-oriented think tanks and
thinkers.
S??2 The ?Stockholm Network? is, according to its own site, Europe?s only dedicated service organisation for market-
oriented think tanks and thinkers.
Figure 1: Sentence pairs from the Wikipedia revision corpus
based inferences. These different types of en-
tailments are naturally distributed in texts, such
as news and every day conversations. A reliable
RTE corpus should preserve this important prop-
erty, i.e. it should be rich in entailment types
whose distribution in the corpus is similar to that
in real texts; and should not include unrepresenta-
tive hand-crafted prototypical examples.
(2) Balanced and consistent. A reliable corpus
should be balanced, i.e. composed by an equal or
comparable number of positive and negative ex-
amples. This is particularly critical for RTE sys-
tems based on machine learning: highly imbal-
anced class distributions often result in poor learn-
ing performance (Japkowicz and Stephen, 2002;
Kubat and Matwin, 1997). Also, the positive and
negative subsets of the corpus should be consis-
tent, i.e. created using the same methodology. If
this property is not preserved, the risk is a learning
system building a model which separates positive
and negatives according to the properties charac-
terizing the two methodologies, instead of those
of the entailment phenomenon.
(3) Not biased on lexical overlap. A major criti-
cism on the RTE-1 dataset was that it contained
too many positive examples with high lexical
overlap wrt. negative examples (Nicholson et al,
2006). Glickman et al (2005) show that an RTE
system using word overlap to decide entailment,
surprisingly achieves an accuracy of 0.57 on RTE-
1 test set. These performances are comparable to
those obtained on the same dataset by more so-
phisticated and principled systems. Learning from
this experience, a good corpus for RTE should
avoid imbalances on lexical overlap.
(4) Homogeneous to existing RTE corpora.
Corpus homogeneity is a key property for any ma-
chine learning approach (Siefkes, 2008). A new
corpus for RTE should then model the same or
similar entailments types of the reliable existing
ones (e.g., those of the RTE challenges). If this is
not the case, RTE system will be unable to learn
a coherent model, thus resulting in a decrease in
performance.
The MITRE corpus satisfies property (1), but
does not (2) and (3), as it is highly imbalanced
(it contains mostly positive examples), and is
fairly biased on lexical overlap, as most examples
of headline-paragraph pairs have many words in
common. The LCC corpus suffers the problem of
inconsistency, as positive and negative examples
are derived with radically different methodolo-
gies. Both the MITRE and the LCC corpora are
difficult to merge with the RTE challenge datasets,
as they are not homogeneous ? i.e. they have been
built using very different methodologies.
3.2 Extracting the corpus from Wikipedia
revisions
Our main intuition in using Wikipedia to build
an entailment corpus is that the wiki framework
should provide a natural source of non-artificial
examples of true and false entailments, through
its revision system. Wikipedia is an open ency-
clopedia, where every person can behave as an
author, inserting new entries or modifying exist-
ing ones. We call original entry S1 a piece of
text in Wikipedia before it is modified by an au-
thor, and revision S2 the modified text. The pri-
mary concern of Wikipedia authors is to reshape
a document according to their intent, by adding
or replacing pieces of text. Excluding vandalism,
there are several reasons for making a revision:
missing information, misspelling, syntactic errors,
and, more importantly, disagreement on the con-
tent. For example, in Fig. 1, S??1 is revised to S??2 ,
as the author disagrees on the content of S??1 .
Our hypothesis is that (S1, S2) pairs represent
good candidates of both true and false entailment
pairs (T,H), as they represent semantically close
30
pieces of texts. Also, Wikipedia pairs conform to
the properties listed in the previous section, as de-
scribed in the following.
(S1, S2) pairs are not artificial, as we extract
them from pieces of original texts, without any
modification or post-processing. Also, pairs are
rich of different entailment types, whose distribu-
tion is a reliable sample of language in use2. As
shown later in the paper, a collection of (S1, S2)
pairs is likely balanced on positive and negative
examples, as authors either contradict the content
of the original entry (false entailment) or add new
information to the existing content (true entail-
ment). Positive and negative pairs are guaranteed
to be consistent, as they are drawn from the same
Wikipedia source. Finally, the Wikipedia is not
biased in lexical overlap: A sentence S2 replac-
ing S1, usually changes only a few words. Yet,
the meaning of S2 may or may not change wrt.
the meaning of S1 ? i.e. the lexical overlap of
the two sentences is very high, but the entailment
relation between S1 and S2 may be either posi-
tive or negative. For example, in Fig. 1 both pairs
have high overlap, but the first is a positive en-
tailment (S?1 ? S?2), while the second is negative
(S??1 ? S??2 ).
An additional interesting property of Wikipedia
revisions is that the transition from S1 to S2 is
commented by the author. The comment is a
piece of text where authors explain and motivate
the change (e.g. ?general cleanup of spelling and
grammar?, ?revision: Eysenck died in 1997!!?).
Even if very small, the comment can be used to
determine if S1 and S2 are in entailment or not.
In the following section we show how we lever-
age comments to make the WIKI corpus homoge-
neous to those of the RTE challenges.
4 Expanding the RTE corpus with WIKI
using co-training
Unlike the LCC corpus where negative and posi-
tive examples are clearly separated, the WIKI cor-
pus mixes the two sets ? i.e. it is unlabelled. In
order to exploit the WIKI corpus in the RTE task,
one should either manually annotate the corpus,
2It has been shown that web documents (as Wikipedia)
are reliable samples of language (Keller and Lapata, 2003).
CO-TRAINING ALGORITHM(L,U ,k)
returns h1,h2,L1,L2
set L1 = L2 = L
while stopping condition is not met
? learn h1 on F1 from L1, and learn h2 on F1 from
L2,
? classify U with h1 obtaining U1, and classify U
with h2 obtaining U2
? select and remove k-best classified examples u1
and u2 from respectively U1 and U2
? add u1 to L2 and u2 to L1
Figure 2: General co-training algorithm
or find an alternative strategy to leverage the cor-
pus even if unlabelled. As manual annotation is
unfeasible, we choose the second solution. The
goal is then to expand a labelled RTE challenge
training set with the unlabelled WIKI, so that the
performance of an RTE system can increase over
an RTE test set.
In the literature, several techniques have been
proposed to use unlabelled data to expand a
training labelled corpus, e.g. Expectation-
Maximization (Dempster et al, 1977). We here
apply the co-training technique, first proposed by
(Blum and Mitchell, 1998) and then successfully
leveraged and analyzed in different settings (Ab-
ney, 2002). Co-training can be applied when the
unlabelled dataset alows two independent views
on its instances (applicability condition).
In this section, we first provide a short descrip-
tion of the co-training algorithm (Section 4.1). We
then investigate if different RTE corpora conform
to the applicability condition (Section 4.2). Fi-
nally, we show that our WIKI corpus conforms to
the condition, and then apply co-training by creat-
ing two independent views (Section 4.3).
4.1 Co-training
The co-training algorithm uses unlabelled data to
increase classification performance, and to indi-
rectly increasing the size of labelled corpora. The
algorithm can be applied only under a specific ap-
plicability condition: corpus? instances must have
two independent views, i.e. they can be modeled
by two independent feature sets.
We here adopt a slightly modified version of the
31
cotraining algorithm, as described in Fig.2. Under
the applicability condition, instances are modeled
on a feature space F = F1 ? F2 ? C , where F1
and F2 are the two independent views and C is
the set of the target classes (in our case, true and
false entailment). The algorithm starts with an ini-
tial set of training labelled examples L and a set
of unlabelled examples U . The set L is copied
in two sets L1 and L2, used to train two differ-
ent classifiers h1 and h2, respectively using views
F1 and F2. The two classifiers are used to clas-
sify the unlabelled set U , obtaining two different
classifications, U1 and U2. Then comes the co-
training step: the k-best classified instances in U1
are added to L2 and feed the learning of a new
classifier h2 on the feature space F2. Similarly, the
k-best instances in U2 are added to L1 and train a
new classifier h1 on F1.
The procedure repeats until a stopping condi-
tion is met. This can be either a fixed number of
added unlabelled examples (Blum and Mitchell,
1998), the performance drop on a control set of
labelled instances, or a filter on the disagreement
of h1 and h2 in classifying U (Collins and Singer,
1999). The final outcome of co-training is the new
set of labelled examples L1?L2 and the two clas-
sifier h1 and h2, obtained from the last iteration.
4.2 Applicability condition on RTE corpora
In order to leverage co-training for homoge-
neously expanding an RTE corpus, it is neces-
sary to have a large unlabelled corpus which sat-
isfies the applicability condition. Unfortunately,
existing methodologies cannot guarantee the con-
dition.
For example, the corpora from which the
datasets of the RTE challenges were derived, were
created from the output of applications perform-
ing specific tasks (e.g., Question&Answering, In-
formation Extraction, Machine Translation, etc.).
These corpora do not offer the possibility to cre-
ate two completely independent views. Indeed,
each extracted pair is composed only by the tex-
tual fragments of T and H , i.e. the only infor-
mation available are the two pieces of texts, from
which it is difficult to extract completely indepen-
dent sets of features, as linguistic features tend to
be dependent.
The MITRE corpus is extracted using two sub-
sequent sentences, the title and the first paragraph.
The LCC negative corpus is extracted using two
correlated sentences or subsentences. Also in
these two cases, it is very hard to find a view that is
independent from the space of the sentence pairs.
None of the existing RTE corpora can then be
used for co-training. In the next section we show
that this is not the case for the WIKI corpus.
4.3 Creating independent views on the WIKI
corpus
The WIKI corpus is naturally suited for co-
training, as for each (S1, S2) pair, it is possible
to clearly define two independent views:
? content-pair view: a set of features modeling
the actual textual content of S1 and S2. This
view is typically available also in any other
RTE corpus.
? comment view: a set of features regarding the
revision comment inserted by an author. This
view represents ?external? information (wrt.
to the text fragments) which are peculiar of
the WIKI corpus.
These two views are most likely independent.
Indeed, the content-pair view deals with the con-
tent of the Wikipedia revision, while the com-
ment view describes the reason why a revision
has been made. This setting is very similar to
the original one proposed for co-training by Blum
and Mitchell (Blum and Mitchell, 1998), where
the target problem was the classification of web
pages, and the two independent views on a page
were (1) its content and (2) its hyperlinks.
In the rest of this section we describe the feature
spaces we adopt for the two independent views.
4.3.1 Content-pair view
The content-pair view is the classical view used
in RTE. The original entry S1 represents the Text
T , while the revision S2 is the Hypothesis H .
Any feature space of those reported in the textual
entailment literature could be applied. We here
adopt the space that represents first-order syntac-
tic rewrite rules (FOSR), as described in (Zan-
zotto and Moschitti, 2006). In this feature space,
each feature represents a syntactic first-order or
32
grounded rewrite rule. For example, the rule:
? = l ? r=
S
NP X VP
VBP
bought
NP Y
?
S
NP Y VP
VBP
owns
NP X
is represented by the feature < l, r >. A (T,H)
pair activates a feature if it unifies with the related
rule. A detailed discussion of the FOSR feature
space is given in (Zanzotto et al, 2009) and ef-
ficient algorithms for the computation of the re-
lated kernel functions can be found in (Moschitti
and Zanzotto, 2007; Zanzotto and Dell?Arciprete,
2009).
4.4 Comment view
A review comment is typically a textual fragment
describing the reason why an author has decided
to make a revision. In most cases the comment is
not a well-formed sentence, as authors tend to use
informal slang expressions and abbreviations (e.g.
?details: Trelew Massacre; cat: Dirty War, copy-
edit?, ?removed a POV vandalism by Spylab?,
?dab ba:clean up using Project:AWB?). In these
cases, where syntactic analysis would mostly fail,
it is advisable to use simpler surface approaches
to build the feature space. We then use a stan-
dard bag-of-word space, combined with a bag-of-
2-grams space. For the first space we keep only
meaningful content words, by using a standard
stop-list including articles, prepositions, and very
frequent words such as be and have. The sec-
ond space should help in capturing small text frag-
ments containing functional words: we then keep
all words without using any stop-list.
5 Experiments
The goals of our experiments are the following:
(1) check the quality of the WIKI corpus, i.e. if
positive and negative examples well represent the
entailment phenomenon; (2) check if WIKI con-
tains examples similar to those of the RTE chal-
lenges, i.e. if the corpus is homogeneous to RTE;
(3) check if the WIKI corpus improves classifica-
tion performance when used to expand the RTE
datasets using the co-training technique described
in Section 4.
5.1 Experimental Setup
In order to check the above claims, we need
to experiment with both manually labelled and
unlabelled corpora. As unlabelled corpora we
adopt:
wiki unlabelled: An unlabelled WIKI corpus of
about 3,000 examples. The corpus has been built
by downloading 40,000 Wikipedia pages dealing
with 800 entries about politics, scientific theories,
and religion issues. We extracted original entries
and revisions from the XML and wiki code,
collecting an overall corpus of 20,000 (S1, S2)
pairs. We then randomly selected the final 3,000
pairs.
news: A corpus of 1,600 examples obtained using
the methods adopted for the LCC corpus, both
for negative and positive examples (Hickl et al,
2006).3 We randomly divided the corpus in two
parts: 800 training and 800 testing examples.
Each set contains an equal number of 400 positive
and negative pairs.
As labelled corpora we use:
RTE-1,RTE-2, and RTE-3: The corpora from
the first three RTE challenges (Dagan et al, 2006;
Bar-Haim et al, 2006; Giampiccolo et al, 2007).
We use the standard split between training and
testing.
wiki: A manually annotated corpus of 2,000
examples from the WIKI corpus. Pairs have been
annotated considering the original entry as the
H and the revision as T . Noisy pairs containing
vandalism or grammatical errors were removed
(these accounts for about 19% of the examples).
In all, the annotation produced 945 positive
examples (strict entailments and paraphrases) and
669 negative examples (reverse strict entailments
and contradictions). The annotation was carried
out by two experienced researchers, each one
annotating half of the corpus. Annotation guide-
lines follow those used for the RTE challenges.4
3For negative examples, we adopt the headline - first para-
graph extraction methodology.
4Annotators were initially trained on a small development
corpus of 200 pairs. The inter-annotator agreement on this
set, computed using the Kappa-statistics (Siegel and Castel-
lan, 1988), was 0.60 corresponding to substantial agreement,
33
The corpus has been randomly split in three
equally numerous parts: development, training,
and testing. We kept aside the development to
design the features, while we used training and
testing for the experiments.
We use the Charniak Parser (Charniak, 2000)
for parsing sentences, and SVM-light (Joachims,
1999) extended with the syntactic first-order rule
kernels described in (Zanzotto and Moschitti,
2006; Moschitti and Zanzotto, 2007) for creating
the FOSR feature space.
5.2 Experimental Results
The first experiment aims at checking the qual-
ity of the WIKI corpus, by comparing the perfor-
mance obtained by a standard RTE system over
the corpus in exam with those obtained over any
RTE challenge corpus. The hypothesis is that if
performance is comparable, then the corpus in
exam has the same complexity (and quality) as
the RTE challenge corpora. We then indepen-
dently experiment with the wiki and the news
corpora with the training-test splits reported in
Section 5.1. As RTE system we adopt an SVM
model learnt on the FOSR feature space described
in Section 4.3.1.
The accuracies of the system on the wiki
and news corpora are respectively 70.73% and
94.87%. The performance of the system on the
wiki corpus are in line with those obtained over
the RTE-2 dataset (60.62%). This suggests that
the WIKI corpus is at least as complex as the RTE
corpora (i.e. positive and negatives are not triv-
ially separable). On the contrary, the news cor-
pus is much easier to separate. Pilot experiments
show that increasing the size of the news corpus,
accuracy reaches nearly 100%. This indicates that
positive and negative examples in the news cor-
pus are extremely different. Indeed, as mentioned
in Section 3.1, news is not consistent ? i.e. the
extraction methods for the positives and the neg-
atives are so different that the examples can be
easily recognized using evidence not representa-
tive of the entailment phenomenon (e.g. for nega-
tive examples, the lexical overlap is extremely low
wrt. positives).
in line with the RTE challenge annotation efforts.
Training Corpus Accuracy
RTE-2 60.62
RTE-1 51.25
RTE-3 57.25
wiki 56.00
news 53.25
RTE-2+RTE-1 58.5
RTE-2+RTE-3 59.62
RTE-2+news 56.75
RTE-2+wiki 59.25
RTE-1+wiki 53.37
RTE-3+wiki 59.00
Table 1: Accuracy of different training corpora over RTE-2
test.
In a second experiment we aim at checking if
WIKI is homogeneous to the RTE challenge cor-
pora ? i.e. if it contains (T,H) pairs similar to
those of the RTE corpora. If this holds, we would
expect the performance of the RTE system to im-
prove (or at least not decrease) when expanding a
given RTE challenge corpus with WIKI. de Marn-
effe et al (2006) already showed in their experi-
ment that it is extremely difficult to obtain better
performance by simply expanding an RTE chal-
lenge training corpus with corpora of other chal-
lenges, since different corpora are usually not ho-
mogeneous.
We here repeat a similar experiment: we ex-
periment with different combinations of training
sets, over the same test set (namely, RTE-2 test).
Results are reported in Table 1. The higher per-
formance is the one of the system when trained on
RTE-2 training set (second row) ? i.e. a corpus
completely homogeneous to RTE-2 would pro-
duce the same performance as RTE-2 training.
As expected, the models learnt on RTE-1 and
RTE-3 perform worse (third and fourth rows): in
particular, RTE-1 seems extremely different from
RTE-2, as results show. The wiki corpus is more
similar to RTE-2 than the news corpus, i.e. per-
formance are higher. Yet, it is quite surprising that
the news corpus yields to a performance drop as
in (Hickl et al, 2006) it shows a high performance
increase.
The expansion of RTE-2 with the above cor-
pora (seventh-tenth rows) lead to a drop in per-
formance, suggesting that none of the corpora
is completely homogeneous to RTE-2. Yet, the
performance drop of the wiki corpus (RTE-2 +
34
60
60.5
61
61.5
62
0 20 40 60 80 100
accuracy
unlabelled examples
RTE2
RTE3
Figure 3: Co-training accuracy curve on the two corpora.
wiki) is comparable to the performance drop ob-
tained using the other two RTE corpora (RTE-2 +
RTE-1 and RTE-2 + RTE-3). This indicates that
wiki is more homogeneous to RTE than news
? i.e. it contains (T,H) pairs that are similar to
the RTE examples. Interestingly, wiki combined
with other RTE corpora (RTE-1 + wiki and RTE-
3 + wiki) increases performance wrt. the models
obtained with RTE-1 and RTE-3 alone (last two
rows).
In a final experiment, we check if the WIKI
corpus improves the performance when combined
with the RTE-2 training in a co-training setting, as
described in Section 4. This would confirm that
WIKI is homogeneous to the RTE-2 corpus, and
could then be successfully adopted in future RTE
competitions. As test sets, we experiment both
with RTE-2 and RTE-3 test. In the co-training,
we use the RTE-2 training set as initial set L, and
wiki unlabelled as the unlabelled set U .5
Figure 3 reports the accuracy curves obtained
by the classifier h1 learnt on the content view, at
each co-training iteration, both on the RTE-2 and
RTE-3 test sets. As the comment view is not avail-
able in the RTE sets, the comment-view classi-
fier become active only after the first 10 examples
are fed as training from the content view classi-
5Note that only wiki unlabelled allows both views de-
scribed in Section 4.3.
fier. As expected, performance increase for some
steps and then become stable for RTE-3 and de-
crease for RTE-2. This is the only case in which
we verified an increase in performance using cor-
pora other than the official ones from RTE chal-
lenges. This result suggests that the WIKI corpus
can successfully contribute to learn better textual
entailment models for RTE.
6 Conclusions
In this paper we proposed a method for expanding
existing textual entailment corpora that leverages
Wikipedia. The method is extremely promising
as it allows building corpora homogeneous to ex-
isting ones. The model we have presented is not
strictly related to the RTE corpora. This method
can then be used to expand corpora such as the
Fracas test-suite (Cooper et al, 1996) which is
more oriented to specific semantic phenomena.
Even if the performance increase of the com-
pletely unsupervised cotraining method is not ex-
tremely high, this model can be used to semi-
automatically expanding corpora by using active
learning techniques (Cohn et al, 1996). The
initial increase of performances is an interesting
starting point.
In the future, we aim at releasing the annotated
portion of the WIKI corpus to the community; we
will also carry out further experiments and refine
the feature spaces. Finally, as Wikipedia is a mul-
tilingual resource, we will use the WIKI method-
ology to semi-automatically build RTE corpora
for other languages.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 360?367, Philadelphia, Pennsyl-
vania, USA, July. Association for Computational Linguis-
tics.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, and Idan Magnini, Bernardo Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT: Proceed-
ings of the Conference on Computational Learning The-
ory. Morgan Kaufmann.
35
John Burger and Lisa Ferro. 2005. Generating an entailment
corpus from news headlines. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment, pages 49?54, Ann Arbor, Michi-
gan, June. Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the 1st NAACL, pages 132?139, Seat-
tle, Washington.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan.
1996. Active learning with statistical models. Journal of
Artificial Intelligence Research, 4:129?145.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In In Proceedings
of the Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora,
pages 100?110.
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Jo-
han Van Genabith, Jan Jaspars, Hans Kamp, David Mil-
ward, Manfred Pinkal, Massimo Poesio, and Steve Pul-
man. 1996. Using the framework. Technical Report LRE
62-051 D-16, The FraCaS Consortium. Technical report.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006.
The pascal recognising textual entailment challenge. In
Quionero-Candela et al, editor, LNAI 3944: MLCW 2005,
pages 177?190, Milan, Italy. Springer-Verlag.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christopher
D. Manning. 2006. Learning to distinguish valid tex-
tual entailments. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment,
Venice, Italy.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em algo-
rithm. Journal of the Royal Statistical Society, Series B,
39(1):1?38.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill
Dolan. 2007. The third pascal recognizing textual en-
tailment challenge. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing, pages
1?9, Prague, June. Association for Computational Lin-
guistics.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings of
the 1st Pascal Challenge Workshop, Southampton, UK.
David Graff. 2003. English gigaword.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing textual en-
tailment. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing, pages 171?176,
Prague, June. ACL.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing
textual entailment with LCCs GROUNDHOG system. In
Proceedings of the 2nd PASCAL Challenge Workshop on
RTE, Venice, Italy.
N. Japkowicz and S. Stephen. 2002. The class imbalance
problem: A systematic study. Intelligent Data Analysis,
6(5).
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods-Support Vector Learn-
ing. MIT Press.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3), September.
M. Kubat and S. Matwin. 1997. Addressing the curse of in-
balanced data sets: One-side sampleing. In Proceedings
of the 14th International Conference on Machine Learn-
ing, pages 179?186. Morgan Kaufmann.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from
texts. In Proceedings of the International Conference of
Machine Learning (ICML), Corvallis, Oregon.
Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin.
2006. Detecting entailment using an extended imple-
mentation of the basic elements overlap metric. In Pro-
ceedings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Christian Siefkes. 2008. An Incrementally Trainable Statis-
tical Approach to Information Extraction. VDM Verlag,
Saarbrucken, Germany.
S. Siegel and Jr. N. J. Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good? eval-
uating non-expert annotations for natural language tasks.
In Proceedings of the 2008 Conference on EmNLP, pages
254?263, Honolulu, Hawaii. ACL.
Annie Zaenen. submitted. Do give a penny for their
thoughts. Journal of Natural Language Engineering.
Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete. 2009.
Efficient kernels for sentence pair classification. In Con-
ference on Empirical Methods on Natural Language Pro-
cessing, pages 91?100, 6-7 August.
Fabio Massimo Zanzotto and Alessandro Moschitti. 2006.
Automatic learning of textual entailments with cross-pair
similarities. In Proceedings of the 21st Coling and 44th
ACL, pages 401?408, Sydney, Australia, July.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessan-
dro Moschitti. 2009. A machine learning approach to
textual entailment recognition. NATURAL LANGUAGE
ENGINEERING, 15-04:551?582. Accepted for pubblica-
tion.
36
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 163?171,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Automatically Building Training Examples for Entity Extraction
Marco Pennacchiotti
Yahoo! Labs
Sunnyvale, CA, USA
pennac@yahoo-inc.com
Patrick Pantel
Microsoft Research
Redmond, WA, USA
ppantel@microsoft.com
Abstract
In this paper we present methods for automat-
ically acquiring training examples for the task
of entity extraction. Experimental evidence
show that: (1) our methods compete with a
current heavily supervised state-of-the-art sys-
tem, within 0.04 absolute mean average pre-
cision; and (2) our model significantly out-
performs other supervised and unsupervised
baselines by between 0.15 and 0.30 in abso-
lute mean average precision.
1 Introduction
Entity extraction is a fundamental task in NLP and
related applications. It is broadly defined as the task
of extracting entities of a given semantic class from
texts (e.g., lists of actors, musicians, cities). Search
engines such as Bing, Yahoo, and Google collect
large sets of entities to better interpret queries (Tan
and Peng, 2006), to improve query suggestions (Cao
et al, 2008) and to understand query intents (Hu et
al., 2009). In response, automated techniques for
entity extraction have been proposed (Pas?ca, 2007;
Wang and Cohen, 2008; Chaudhuri et al, 2009; Pan-
tel et al, 2009).
There is mounting evidence that combining
knowledge sources and information extraction sys-
tems yield significant improvements over applying
each in isolation (Pas?ca et al, 2006; Mirkin et al,
2006). This intuition is explored by the Ensem-
ble Semantics (ES) framework proposed by Pennac-
chiotti and Pantel (2009), which outperforms pre-
vious state-of-the-art systems. A severe limitation
of this type of extraction system is its reliance on
editorial judgments for building large training sets
for each semantic class to be extracted. This is
particularly troublesome for applications such as
web search that require large numbers of semantic
classes in order to have a sufficient coverage of facts
and objects (Tan and Peng, 2006). Hand-crafting
training sets across international markets is often in-
feasible. In an exploratory study we estimated that
a pool of editors would need roughly 300 working
days to complete a basic set of 100 English classes
using the ES framework. Critically needed are meth-
ods for automatically building training sets that pre-
serve the extraction quality.
In this paper, we propose simple and intuitively
appealing solutions to automatically build training
sets. Positive and negative training sets for a tar-
get semantic class are acquired by leveraging: i)
?trusted? sources such as structured databases (e.g.,
IMDB or Wikipedia for acquiring a list of Actors);
ii) automatically constructed semantic lexicons; and
iii) instances of semantic classes other than the tar-
get class. Our models focus on extracting training
sets that are large, balanced, and representative of
the unlabeled data. These models can be used in any
extraction setting, where ?trusted? sources of knowl-
edge are available: Today, the popularity of struc-
tured and semi-structured sources such as Wikipedia
and internet databases, makes this approach widely
applicable. As an example, in this paper we show
that our methods can be successfully adapted and
used in the ES framework. This gives us the possi-
bility to test the methods on a large-scale entity ex-
traction task. We replace the manually built training
data in the the ES model with the training data built
163
by our algorithms. We show by means of a large em-
pirical study that our algorithms perform nearly as
good as the fully supervised ES model, within 4% in
absolute mean average precision. Further, we com-
pare the performance of our method against both
Pas?ca et al (2006) and Mirkin et al (2006), show-
ing 17% and 15% improvements in absolute mean
average precision, respectively.
The main contributions of this paper are:
? We propose several general methods for
automatically acquiring labeled training data;
we show that they can be used in a large-scale
extraction framework, namely ES; and
? We show empirical evidence on a large-scale
entity extraction task that our system using
automatically labeled training data performs
nearly as well as the fully-supervised ES
model, and that it significantly outperforms
state-of-the-art systems.
2 Automatic Acquisition of Training Data
Supervised machine learning algorithms require
training data that is: (1) balanced and large enough
to correctly model the problem at hand (Kubat and
Matwin, 1997; Japkowicz and Stephen, 2002); and
(2) representative of the unlabeled data to decode,
i.e., training and unlabeled instances should be ide-
ally drawn from the same distribution (Blumer et al,
1989; Blum and Langley, 1997). If these two prop-
erties are not met, various learning problems, such
as overfitting, can drastically impair predictive ac-
curacy. To address the above properties, a common
approach is to select a subset of the unlabeled data
(i.e., the instances to be decoded), and manually la-
bel them to build the training set.
In this section we propose methods to automate
this task by leveraging the multitude of structured
knowledge bases available on the Web.
Formally, given a target class c, our goal is
to implement methods to automatically build a
training set T (c), composed of both positive and
negative examples, respectively P (c) and N(c);
and to apply T (c) to classify (or rank) a set
of unlabeled data U(c), by using a learning
algorithm. For example, in entity extraction,
given the class Actors, we might have P (c) =
{Brad Pitt, Robert De Niro} and N(c) =
{Serena Williams, Rome, Robert Demiro}.
Below, we define the components of a typical
knowledge acquisition system as in the ES frame-
work, where our methods can be applied :
Sources. Textual repositories of information, ei-
ther structured (e.g., Freebase), semi-structured
(e.g., HTML tables) or unstructured (e.g., a we-
bcrawl). Information sources serve as inputs to the
extraction system, either for the Knowledge Extrac-
tors to generate candidate instances, or for the Fea-
ture Generators to generate features (see below).
Knowledge Extractors (KE). Algorithms re-
sponsible for extracting candidate instances such as
entities or facts. Extractors fall into two categories:
trusted and untrusted. Trusted extractors execute on
structured sources where the contents are deemed to
be highly accurate. Untrusted extractors execute on
unstructured or semi-structured sources and gener-
ally generate high coverage but noisy knowledge.
Feature Generators. Methods that extract evi-
dence (features) of knowledge in order to decide
which extracted candidate instances are correct.
Ranker. A module for ranking the extracted in-
stances using the features generated by the feature
generators. In supervised ML-based rankers, labeled
training instances are required to train the model.
Our goal here is to automatically label training in-
stances thus avoiding the editorial costs.
2.1 Acquiring Positive Examples
Trusted positives: Candidate instances for a class
c that are extracted by a trusted Knowledge Extrac-
tor (e.g., a wrapper induction system over IMDB),
tend to be mostly positive examples. A basic ap-
proach to acquiring a set of positive examples is then
to sample from the unlabeled set U(c) as follows:
P (c) = {i ? U(c) : (?KEi|KEi is trusted} (1)
where KEi is a knowledge extractor that extracted
instance i.
The main advantage of this method is that P (c) is
guaranteed to be highly accurate, i.e., most instances
are true positives. On the downside, instances in
P (c) are not necessarily representative of the un-
trusted KEs. This can highly impact the perfor-
mance of the learning algorithm, which could over-
fit the training data on properties that are specific to
164
the trusted KEs, but that are not representative of the
true population to be decoded (which is largely com-
ing from untrusted KEs).
We therefore enforce that the instances in P (c)
are extracted not only from a trusted KE, but also
from any of the untrusted extractors:
P (c) = {i ? U(c) :?KEi|KEi is trusted ?
?KEj |KEj is untrusted}
(2)
External positives: This method selects the set of
positive examples P (c) from an external repository,
such as an ontology, a database, or an automati-
cally harvested source. The main advantage of this
method is that such resources are widely available
for many knowledge extraction tasks. Yet, the risk
is that P (c) is not representative of the unlabeled in-
stances U(c), as they are drawn from different pop-
ulations.
2.1.1 Acquiring Negative Examples
Acquiring negative training examples is a much
more daunting task (Fagni and Sebastiani, 2007).
The main challenge is to select a set which is a good
representative of the unlabeled negatives in U(c).
Various strategies can be adopted, ranging from
selecting near-miss examples to acquiring generic
ones, each having its own pros and cons. Below
we propose our methods, some building on previous
work described in Section 5.
Near-class negatives: This method selects N(c)
from the population U(C) of the set of classes C
which are semantically similar to c. For example, in
entity extraction, the classes Athletes, Directors
and Musicians are semantically similar to the class
Actors, while Manufacturers and Products are
dissimilar. Similar classes allow us to select negative
examples that are semantic near-misses for the class
c. The hypothesis is the following:
A positive instance extracted for a class similar
to the target class c, is likely to be a near-miss
incorrect instance for c.
To model this hypothesis, we acquire N(c) from
the set of instances having the following two restric-
tions:
1. The instance is most likely correct for C
2. The instance is most likely incorrect for c
Note that restriction (1) alone is not sufficient, as an
instance of C can be at the same time also instance
of c. For example, given the target class Actors, the
instance ?Woody Allen? ? Directors, is not a good
negative example for Actors, since Woody Allen is
both a director and an actor.
In order to enforce restriction (1), we select only
instances that have been extracted by a trusted KE
of C, i.e., the confidence of them being positive is
very high. To enforce (2), we select instances that
have never been extracted by any KE of c. More
formally, we define N(c) as follows:
N(c) =
?
ci?C
P (ci) \ U(c) (3)
The main advantage of this method is that it acquires
negatives that are semantic near-misses of the tar-
get class, thus allowing the learning algorithm to fo-
cus on these borderline cases (Fagni and Sebastiani,
2007). This is a very important property, as most
incorrect instances extracted by unsupervised KEs
are indeed semantic near-misses. On the downside,
the extracted examples are not representative of the
negative examples of the target class c, since they
are drawn from two different distributions.
Generic negatives: This method selects N(c)
from the population U(C) of all classes C different
from the target class c, i.e., both classes semantically
similar and dissimilar to c. The method is very sim-
ilar to the one above, apart from the selection of C,
which now includes any class different from c. The
underlying hypothesis is the following:
A positive instance extracted for a class different
from the target class c, is likely to be an incorrect
instance for c.
This method acquires negatives that are both seman-
tic near-misses and far-misses of the target class.
The learning algorithm is then able to focus both on
borderline cases and on clear-cut incorrect cases, i.e.
the hypothesis space is potentially larger than for the
near-class method. On the downside, the distribu-
tion of c and C are very different. By enlarging the
potential hypothesis space, the risk is then again to
capture hypotheses that overfit the training data on
properties which are not representative of the true
population to be decoded.
165
Same-class negatives: This method selects the
set of negative examples N(c) from the population
U(c). The driving hypothesis is the following:
If a candidate instance for a class c has been ex-
tracted by only one KE and this KE is untrusted,
then the instance is likely to be incorrect, i.e., a
negative example for c.
The above hypothesis stems from an intuitive obser-
vation common to many ensemble-based paradigms
(e.g., ensemble learning in Machine Learning): the
more evidence you have of a given fact, the higher is
the probability of it being actually true. In our case,
the fact that an instance has been extracted by only
one untrusted KE, provides weak evidence that the
instance is correct. N(c) is defined as follows:
N(c) = {i ? U(c) : ?! KEi ?KEi is untrusted}
(4)
The main advantage of this method is that the ac-
quired instances in N(c) are good representatives of
the negatives that will have to be decoded, i.e., they
are drawn from the same distribution U(c). This al-
lows the learning algorithm to focus on the typical
properties of the incorrect examples extracted by the
pool of KEs.
A drawback of this method is that instances in
N(c) are not guaranteed to be true negatives. It fol-
lows that the final training set may be noisy. Two
main strategies can be applied to mitigate this prob-
lem: (1) Use a learning algorithm which is robust to
noise in the training data; and (2) Adopt techniques
to automatically reduce or eliminate noise. We here
adopt the first solution, and leave the second as a
possible avenue for future work, as described in Sec-
tion 6. In Section 4 we demonstrate the amount of
noise in our training data, and show that its impact
is not detrimental for the overall performance of the
system.
3 A Use Case: Entity Extraction
Entity extraction is a fundamental task in NLP
(Cimiano and Staab, 2004; McCarthy and Lehnert,
2005) and web search (Chaudhuri et al, 2009; Hu
et al, 2009; Tan and Peng, 2006), responsible for
extracting instances of semantic classes (e.g., ?Brad
Pitt? and ?Tom Hanks? are instances of the class Ac-
tors). In this section we apply our methods for auto-
matically acquiring training data to the ES entity ex-
traction system described in Pennacchiotti and Pan-
tel (2009).1
The system relies on the following three knowl-
edge extractors. KEtrs: a ?trusted? database
wrapper extractor acquiring entities from sources
such as Yahoo! Movies, Yahoo! Music and Yahoo!
Sports, for extracting respectively Actors, Musicians
and Athletes. KEpat: an ?untrusted? pattern-based
extractor reimplementing Pas?ca et al?s (2006) state-
of-the-art web-scale fact extractor. KEdis: an ?un-
trusted? distributional extractor implementing a vari-
ant of Pantel et al?s (2009).
The system includes four feature generators,
which compute a total of 402 features of various
types extracted from the following sources: (1) a
body of 600 million documents crawled from the
Web at Yahoo! in 2008; (2) one year of web search
queries issued to Yahoo! Search; (3) all HTML inner
tables extracted from the above web crawl; (4) an
official Wikipedia dump from February 2008, con-
sisting of about 2 million articles.
The system adopts as a ranker a supervised
Gradient Boosted Decision Tree regression model
(GBDT) (Friedman, 2001). GBDT is generally con-
sidered robust to noisy training data, and hence is a
good choice given the errors introduced by our auto-
matic training set construction algorithms.
3.1 Training Data Acquisition
The positive and negative components of the training
set for GBDT are built using the methods presented
in Section 2, as follows:
Trusted positives (Ptrs and Pcls): According to
Eq. 2, we acquire a set of positive instances Pcls
as a random sample of the instances extracted by
both KEtrs and either: KEdis, KEpat or both of
them. As a basic variant, we also experiment with
the simpler definition in Eq. 1, i.e. we acquire a set
of positive instances Ptrs as a random sample of the
instances extracted by the trusted extractor KEtrs,
irrespective of KEdis and KEpat.
External positives (Pcbc): Any external repository
of positive examples would serve here. In our spe-
1We here give a summary description of our implementation
of that system. Refer to the original paper for more details.
166
cific implementation, we select a set of positive ex-
amples from the CBC repository (Pantel and Lin,
2002). CBC is a word clustering algorithm that
groups instances appearing in similar textual con-
texts. By manually analyzing the cluster members
in the repository created by CBC, it is easy to pick-
up the cluster(s) representing a target class.
Same-class negatives (Ncls): We select a set of
negative instances as a random sample of the in-
stances extracted by only one extractor, which can
be either of the two untrusted ones, KEdis or
KEpat.
Near-class negatives (Noth): We select a set of
negative instances, as a random sample of the in-
stances extracted by any of our three extractors for a
class different than the one at hand. We also enforce
the condition that instances in Noth must not have
been extracted for the class at hand.
Generic negatives (Ncbc): We automatically se-
lect as generic negatives a random sample of in-
stances appearing in any CBC cluster, except those
containing at least one member of the class at hand
(i.e., containing at least one instance extracted by
one of our KEs for the given class).
4 Experimental Evaluation
In this section, we report experiments comparing
the ranking performance of our different methods
for acquiring training data presented in Section 3,
to three different baselines and a fully supervised
upper-bound.
4.1 Experimental Setup
We evaluate over three semantic classes: Actors
(movie, tv and stage actors); Athletes (profes-
sional and amateur); Musicians (singers, musicians,
composers, bands, and orchestras), so to compare
with (Pennacchiotti and Pantel, 2009). Ranking per-
formance is tested over the test set described in the
above paper, composed of 500 instances, randomly
selected from the instances extracted by KEpat and
KEdis for each of the classes2.
We experiment with various instantiations of the
ES system, each trained on a different training set
2We do not test over instances extracted by KEtrs, as they
do not go though the decoding phase
obtained from our methods. The different system in-
stantiations (i.e., different training sets) are reported
in Table 1 (Columns 1-3). Each training set consists
of 500 positive examples, and 500 negative exam-
ples.
As an upper bound, we use the ES system, where
the training consists of 500 manually annotated in-
stances (Pman and Nman), randomly selected from
those extracted by the KEs. This allows us to di-
rectly check if our automatically acquired training
sets can compete to the human upper-bound. We
also compare to the following baselines.
Baseline 1: An unsupervised rule-based ES sys-
tem, assigning the lowest score to instances ex-
tracted by only one KE, when the KE is untrusted;
and assigning the highest score to any other instance.
Baseline 2: An unsupervised rule-based ES sys-
tem, adopting as KEs the two untrusted extractors
KEpat and KEdis, and a rule-based Ranker that as-
signs scores to instances according to the sum of
their normalized confidence scores.
Baseline 3: An instantiation of our ES system,
trained on Pman and Nman. The only differ-
ence with the upper-bound is that it uses only two
features, namely the confidence score returned by
KEdis and KEpat. This instantiation implements
the system presented in (Mirkin et al, 2006).
For evaluation, we use average precision (AP), a
standard information retrieval measure for evaluat-
ing ranking algorithms:
AP (L) =
?|L|
i=1 P (i) ? corr(i)
?|L|
i=1 corr(i)
(5)
where L is a ranked list produced by a system, P (i)
is the precision of L at rank i, and corr(i) is 1 if the
instance at rank i is correct, and 0 otherwise.
In order to accurately compute statistical signifi-
cance, we divide the test set in 10-folds, and com-
pute the AP mean and variance obtained over the
10-folds. For each configuration, we perform the
random sampling of the training set five times, re-
building the model each time, to estimate the vari-
ance when varying the training sampling.
4.2 Experimental Results
Table 1 reports average precision (AP) results for
different ES instantiations, separately on the three
167
System Training Set AP MAP
Positives Negatives Actors Athletes Musicians
Baseline1 (unsup.) - - 0.562 0.535 0.437 0.511
Baseline2 (unsup.) - - 0.676 0.664 0.576 0.639
Baseline3 (sup.) Pman Nman 0.715 0.697 0.576 0.664
Upper-bound (full-sup.) Pman Nman 0.860? 0.901? 0.786? 0.849?
S1. Pcls Noth 0.751? 0.880? 0.642 0.758?
S2. Pcls Ncbc 0.734? 0.854? 0.644 0.744?
S3. Pcls Ncls 0.842? 0.806? 0.770? 0.806?
S4. Pcls Noth + Ncbc 0.756? 0.853? 0.693? 0.767?
S5. Pcls Ncls + Noth 0.835? 0.807? 0.763? 0.802?
S6. Pcls Ncls + Ncbc 0.838? 0.822? 0.768? 0.809?
S7. Pcls Ncls + Noth + Ncbc 0.838? 0.818? 0.764? 0.807?
Table 1: Average precision (AP) results of systems using different training sets, compared to two usupervised Base-
lines, a supervised Baseline, and a fully supervised upper-bound system. ? indicates statistical significance at the 0.95
level wrt all Baselines. ? indicates statistical significance at the 0.95 level wrt Baseline1 and Baseline 2. ? indicates
statistical significance at the 0.95 level wrt Baseline1.
classes; and the mean average precision (MAP)
computed across the classes. We report results us-
ing Pcls as positive training, and varying the neg-
ative training composition3. Systems S1-S3 use a
single method to build the negatives. Systems S4-
S6 combine two methods (250 examples from one
method, 250 from the other), and S7 combines all
three methods. Table 3 reports additional basic re-
sults when varying the positive training set compo-
sition, and fixing the best performing negative set
(namely Ncls).
Table 1 shows that all systems outperform the
baselines in MAP, with 0.95 statistical significance,
but S2 which is not significant wrt Baseline 3. S6 is
the best performing system, achieving 0.809 MAP,
only 4% below the supervised upper-bound (statis-
tically insignificant at the 0.95 level). These results
indicate that our methods for automatically acquir-
ing training data are highly effective and competitive
with manually crafted training sets.
A class-by-class analysis reveals similar behav-
ior for Actors and Musicians. For these two classes,
the best negative set is Ncls (system S3), achieving
alone the best AP (respectively 0.842 and 0.770 for
Actors and Musicians, 2.1% and 1.6% points below
the upper-bound). Noth and Ncbc show a lower ac-
curacy, more than 10% below Ncls. This suggest
that the most promising strategy for automatically
3For space limitation we cannot report exhaustively all com-
binations.
Negative set False Negatives
Actors Athletes Musicians
Ncls 5% 45% 30%
Noth 0% 10% 10%
Ncbc 0% 0% 15%
Table 2: Percentage of false negatives in different types of
negative sets, across the three experimented classes (esti-
mations over a random sample of 20 examples per class).
acquiring negative training data is to collect exam-
ples from the target class, as they guarantee to be
drawn from the same distribution as the instances to
be decoded. The use of near- and far-misses is still
valuable (AP results are still better than the base-
lines), but less effective.
Results for Athletes give different evidence: the
best performing negative set is Noth, performing
significantly better than Ncls. To investigate this
contrasting result, we manually picked 20 exam-
ples from Ncls, Noth and Ncbc for each class, and
checked their degree of noise, i.e., how many false
negatives they contain. Table 2 reports the results:
these numbers indicate that the Ncls is very noisy
for the Athletes class, while it is more clean for the
other two classes. This suggests that the learning
algorithm, while being robust enough to cope with
the small noise in Ncls for Actors and Musicians, it
starts to diverge when too many false negatives are
presented for training, as it happens for Athletes.
False negatives in Ncls are correct instances ex-
tracted by one untrusted KE alone. The results in
168
Table 2 indicates that our untrusted KEs are more
accurate in extracting instances for Athletes than for
the other classes: accurate enough to make our train-
ing set too noisy, thus decreasing the performance
of S3 wrt S1 and S2. This indicates that the effec-
tiveness of Ncls decreases when the accuracy of the
untrusted KEs is higher.
A good strategy to avoid the above problem is to
pair Ncls with another negative set, either Ncbc or
Noth, as in S5 and S6, respectively. Then, when
the above problem is presented, the learning algo-
rithm can rely on the other negative set to com-
pensate some for the noise. Indeed, when adding
Ncbc to Ncls (system S6) the accuracy over Athletes
improves, while the overall performance across all
classes (MAP) is kept constant wrt the system using
Ncls (S3).
It is interesting that in Table 2, Ncbc and Noth also
have a few false negatives. An intrinsic analysis re-
veals that these are either: (1) Incorrect instances
of the other classes that are actual instances of the
target class; (2) Correct instances of other classes
that are also instances of the target class. Case (1) is
caused by errors of KEs for the other classes (e.g.,
erroneously extracting ?Matthew Flynt? as a Musi-
cian). Case (2) covers cases in which instances are
ambiguous across classes, for example ?Kerry Tay-
lor? is both an Actor and a Musician. This observa-
tion is still surprising, since Eq. 3 explicitly removes
from Ncbc and Noth any correct instance of the tar-
get class extracted by the KEs. The presence of false
negatives is then due to the low coverage of the KEs
for the target class, e.g. the KEs were not able to ex-
tract ?Matthew Flynt? and ?Kerry Taylor? as actors.
Correlations. We computed the Spearman corre-
lation coefficient r among the rankings produced
by the different system instantiations, to verify
how complementary the information enclosed in the
training sets are for building the learning model.
Among the basic systems S1? S3, the highest cor-
relation is between S1 and S2 (r = 0.66 in aver-
age across all classes), which is expected, since they
both apply the principle of acquiring negative ex-
amples from classes other than the target one. S3
exhibits lower correlation with both S1 and S2, re-
spectively r = 0.57 and r = 0.53, suggesting that it
is complementary to them. Also, the best system S6
System Training Set AP MAP
Pos. Neg. Act. Ath. Mus.
S3. Pcls Ncls 0.842 0.806 0.770 0.806
S8. Ptrs Ncls 0.556 0.779 0.557 0.631
S9. Pcbc Ncls 0.633 0.521 0.561 0.571
Table 3: Comparative average precision (AP) results for
systems using different positive sets as training data.
Figure 1: Average precision of system S6 with different
training sizes.
has higher correlation with S3 (r = 0.94) than with
S2 (r = 0.62), indicating that in the combination of
Ncls and Ncbc, most of the model is built on Ncls.
Varying the positive training. Table 3 reports re-
sults when fixing the negative set to the best per-
forming Ncls, and exploring the use of other posi-
tive sets. As expected Pcls largely outperforms Ptrs,
confirming that removing the constraint in Eq. 2 and
using the simpler Eq. 1 makes the training set unrep-
resentative of the unlabeled population. A similar
observation stands for Pcbc. These results indicate
that having a good trusted KE, or even an external
resource of positives, is effective only when select-
ing from the training set examples that are also ex-
tracted by the untrusted KEs.
Varying the training size. In Figure 1 we report
an analysis of the AP achieved by the best perform-
ing System (S6), when varying the training size, i.e.,
changing the cardinality of Pcls and Ncls + Ncbc.
The results show that a relatively small-sized train-
ing set offers good performance, the plateau being
reached already with 500 training examples. This
is an encouraging result, showing that our methods
can potentially be applied also in cases where few
examples are available, e.g., for rare or not well-
represented classes.
169
5 Related Work
Most relevant are efforts in semi-supervised learn-
ing. Semi-supervised systems use both labeled and
unlabeled data to train a machine learning system.
Most common techniques are based on co-training
and self-training. Co-training uses a small set of la-
beled examples to train two classifiers at the same
time. The classifiers use independent views (i.e.
?conditionally independent? feature sets) to repre-
sent the labeled examples. After the learning phase,
the most confident predictions of each classifier
on the unlabeled data are used to increase the la-
beled set of the other. These two phases are re-
peated until a stop condition is met. Co-training
has been successfully applied to various applica-
tions, such as statistical parsing (Sarkar, 2001) and
web pages classification (Yarowsky, 1998). Self-
training techniques (or bootsrapping) (Yarowsky,
1995) start with a small set of labeled data, and it-
eratively classify unlabeled data, selecting the most
confident predictions as additional training. Self-
training has been applied in many NLP tasks, such
as word sense disambiguation (Yarowsky, 1995) and
relation extraction (Hearst, 1992). Unlike typical
semi-supervised approaches, our approach reduces
the needed amount of labeled data not by acting on
the learning algorithm itself (any algorithm can be
used in our approach), but on the method to acquire
the labeled training data.
Our work also relates to the automatic acquisi-
tion of labeled negative training data. Yangarber et
al. (2002) propose a pattern-based bootstrapping ap-
proach for harvesting generalized names (e.g., dis-
eases, locations), where labeled negative examples
for a given class are taken from positive seed exam-
ples of ?competing? classes (e.g. examples of dis-
eases are used as negatives for locations). The ap-
proach is semi-supervised, in that it requires some
manually annotated seeds. The study shows that
using competing categories improves the accuracy
of the system, by avoiding sematic drift, which is
a common cause of divergence in boostrapping ap-
proaches. Similar approaches are used among others
in (Thelen and Riloff, 2002) for learning semantic
lexicons, in (Collins and Singer, 1999) for named-
entity recognition, and in (Fagni and Sebastiani,
2007) for hierarchical text categorization. Some of
our methods rely on the same intuition described
above, i.e., using instances of other classes as nega-
tive training examples. Yet, the ES framework al-
lows us to add further restrictions to improve the
quality of the data.
6 Conclusion
We presented simple and general techniques for au-
tomatically acquiring training data, and then tested
them in the context of the Ensemble Semantics
framework. Experimental results show that our
methods can compete with supervised systems us-
ing manually crafted training data. It is our hope that
these simple and easy-to-implement methods can al-
leviate some of the cost of building machine learn-
ing architectures for supporting open-domain infor-
mation extraction, where the potentially very large
number of classes to be extracted makes infeasible
the use of manually labeled training data.
There are many avenues for future work. Al-
though our reliance on high-quality knowledge
sources is not an issue for many head classes, it
poses a challenge for tail classes such as ?wine con-
noisseurs?, where finding alternative sources of high
precision samples is important. We also plan to ex-
plore techniques to automatically identify and elim-
inate mislabeled examples in the training data as
in (Rebbapragada and Brodley, 2007), and relax the
boolean assumption of trusted/untrusted extractors
into a graded one. Another important issue regards
the discovery of ?near-classes? for collecting near-
classes negatives: we plan to automate this step by
adapting existing techniques as in (McIntosh, 2010).
Finally, we plan to experiment on a larger set of
classes, to show the generalizability of the approach.
Our current work focuses on leveraging auto-
learning to create an extensive taxonomy of classes,
which will constitute the foundation of a very large
knowledge-base for supporting web search.
References
Avrim L. Blum and Pat Langley. 1997. Selection of rel-
evant features and examples in machine learning. Ar-
tificial Intelligence, 97:245?271.
A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. War-
muth. 1989. Proceedings of ltc-07. Journal of ACM,
36:929?965.
170
Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao,
Enhong Chen, and Hang Li. 2008. Context-aware
query suggestion by mining click-through and session
data. In Proceedings of KDD-08, pages 875?883.
Surajit Chaudhuri, Venkatesh Ganti, and Dong Xin.
2009. Exploiting web search to generate synonyms for
entities. In Proceedings of WWW-09, pages 151?160.
Philipp Cimiano and Steffen Staab. 2004. Learning by
googling. SIGKDD Explorations, 6(2):24?34.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings of
WVLC/EMNLP-99, pages 100?110.
Tiziano Fagni and Fabrizio Sebastiani. 2007. On the se-
lection of negative examples for hierarchical text cate-
gorization. In Proceedings of LTC-07, pages 24?28.
Jerome H. Friedman. 2001. Greedy function approxima-
tion: A gradient boosting machine. Annals of Statis-
tics, 29(5):1189?1232.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING-92, pages 539?545.
Jian Hu, Gang Wang, Fred Lochovsky, Jian tao Sun, and
Zheng Chen. 2009. Understanding user?s query intent
with Wikipedia. In Proceedings of WWW-09, pages
471?480.
N. Japkowicz and S. Stephen. 2002. The class imbalance
problem: A systematic study. Intelligent Data Analy-
sis, 6(5).
M. Kubat and S. Matwin. 1997. Addressing the curse
of inbalanced data sets: One-side sampleing. In Pro-
ceedings of the ICML-1997, pages 179?186. Morgan
Kaufmann.
Joseph F. McCarthy and Wendy G Lehnert. 2005. Using
decision trees for coreference resolution. In Proceed-
ings of IJCAI-1995, pages 1050?1055.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 356?365, Mas-
sachusetts, USA. Association for Computational Lin-
guistics.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In Pro-
ceedings of ACL/COLING-06, pages 579?586.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and search-
ing the world wide web of facts - step one: The one-
million fact extraction challenge. In Proceedings of
AAAI-06, pages 1400?1405.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of CIKM-07, pages 683?690, New York, NY,
USA.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of KDD-02, pages
613?619.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP-09.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 238?247, Singapore.
Association for Computational Linguistics.
Umaa Rebbapragada and Carla E. Brodley. 2007. Class
noise mitigation through instance weighting. In Pro-
ceedings of the 18th European Conference on Machine
Learning.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In NAACL-2001.
Bin Tan and Fuchun Peng. 2006. Unsupervised query
segmentation using generative language models and
wikipedia. In Proceedings of WWW-06, pages 1400?
1405.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 214?221, Philadelphia, PA, USA. As-
sociation for Computational Linguistics.
Richard C. Wang and William W. Cohen. 2008. Itera-
tive set expansion of named entities using the web. In
ICDM ?08: Proceedings of the 2008 Eighth IEEE In-
ternational Conference on Data Mining, pages 1091?
1096, Washington, DC, USA. IEEE Computer Society.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names.
In COLING-2002.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL-1996, pages 189?196.
David Yarowsky. 1998. Combining labeled and unla-
beled data with co-training. In Proceedings of the
Workshop on Computational Learning Theory, pages
92?100.
171
