Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 315?322, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Enhanced Answer Type Inference from Questions using Sequential Models
Vijay Krishnan and Sujatha Das and Soumen Chakrabarti?
Computer Science and Engineering Department, IIT Bombay, India
Abstract
Question classification is an important
step in factual question answering (QA)
and other dialog systems. Several at-
tempts have been made to apply statistical
machine learning approaches, including
Support Vector Machines (SVMs) with
sophisticated features and kernels. Curi-
ously, the payoff beyond a simple bag-of-
words representation has been small. We
show that most questions reveal their class
through a short contiguous token subse-
quence, which we call its informer span.
Perfect knowledge of informer spans can
enhance accuracy from 79.4% to 88%
using linear SVMs on standard bench-
marks. In contrast, standard heuristics
based on shallow pattern-matching give
only a 3% improvement, showing that the
notion of an informer is non-trivial. Us-
ing a novel multi-resolution encoding of
the question?s parse tree, we induce a Con-
ditional Random Field (CRF) to identify
informer spans with about 85% accuracy.
Then we build a meta-classifier using a
linear SVM on the CRF output, enhancing
accuracy to 86.2%, which is better than all
published numbers.
1 Introduction
An important step in factual question answering
(QA) and other dialog systems is to classify the
question (e.g., Who painted Olympia?) to the antic-
ipated type of the answer (e.g., person). This step
is called ?question classification? or ?answer type
identification?.
The answer type is picked from a hand-built tax-
onomy having dozens to hundreds of answer types
(Harabagiu et al, 2000; Hovy et al, 2001; Kwok et
al., 2001; Zheng, 2002; Dumais et al, 2002). QA
? soumen@cse.iitb.ac.in
systems can use the answer type to short-list answer
tokens from passages retrieved by an information re-
trieval (IR) subsystem, or use the type together with
other question words to inject IR queries.
Early successful QA systems used manually-
constructed sets of rules to map a question to a
type, exploiting clues such as the wh-word (who,
where, when, how many) and the head of noun
phrases associated with the main verb (what is the
tallest mountain in . . .).
With the increasing popularity of statistical NLP,
Li and Roth (2002), Hacioglu and Ward (2003) and
Zhang and Lee (2003) used supervised learning for
question classification on a data set from UIUC that
is now standard1. It has 6 coarse and 50 fine answer
types in a two-level taxonomy, together with 5500
training and 500 test questions. Webclopedia (Hovy
et al, 2001) has also published its taxonomy with
over 140 types.
The promise of a machine learning approach is
that the QA system builder can now focus on de-
signing features and providing labeled data, rather
than coding and maintaining complex heuristic rule-
bases. The data sets and learning systems quoted
above have made question classification a well-
defined and non-trivial subtask of QA for which al-
gorithms can be evaluated precisely, isolating more
complex factors at work in a complete QA system.
Prior work: Compared to human performance,
the accuracy of question classifiers is not high. In all
studies, surprisingly slim gains have resulted from
sophisticated design of features and kernels.
Li and Roth (2002) used a Sparse Network of
Winnows (SNoW) (Khardon et al, 1999). Their fea-
tures included tokens, parts of speech (POS), chunks
(non-overlapping phrases) and named entity (NE)
tags. They achieved 78.8% accuracy for 50 classes,
which improved to 84.2% on using an (unpublished,
to our knowledge) hand-built dictionary of ?seman-
tically related words?.
1http://l2r.cs.uiuc.edu/?cogcomp/Data/
QA/QC/
315
Hacioglu and Ward (2003) used linear support
vector machines (SVMs) with question word 2-
grams and error-correcting output codes (ECOC)?
but no NE tagger or related word dictionary?to get
80.2?82% accuracy.
Zhang and Lee (2003) used linear SVMs with
all possible question word q-grams, and obtained
79.2% accuracy. They went on to design an inge-
nious kernel on question parse trees, which yielded
visible gains for the 6 coarse labels, but only ?slight?
gains for the 50 fine classes, because ?the syntactic
tree does not normally contain the information re-
quired to distinguish between the various fine cate-
gories within a coarse category?.
Algorithm 6-class 50-class
Li and Roth, SNoW (1) 78.8(2)
Hacioglu et al, SVM+ECOC ? 80.2?82
Zhang & Lee, LinearSVMq 87.4 79.2
Zhang & Lee, TreeSVM 90 ?
SVM, ?perfect? informer 94.2 88
SVM, CRF-informer 93.4 86.2
Table 1: Summary of % accuracy for UIUC data.
(1) SNoW accuracy without the related word dictio-
nary was not reported. With the related-word dic-
tionary, it achieved 91%. (2) SNoW with a related-
word dictionary achieved 84.2% but the other algo-
rithms did not use it. Our results are summarized in
the last two rows, see text for details.
Our contributions: We introduce the notion of
the answer type informer span of the question (in
?2): a short (typically 1?3 word) subsequence of
question tokens that are adequate clues for question
classification; e.g.: How much does an adult ele-
phant weigh?
We show (in ?3.2) that a simple linear SVM us-
ing features derived from human-annotated informer
spans beats all known learning approaches. This
confirms our suspicion that the earlier approaches
suffered from a feature localization problem.
Of course, informers are useful only if we can find
ways to automatically identify informer spans. Sur-
prisingly, syntactic pattern-matching and heuristics
widely used in QA systems are not very good at cap-
turing informer spans (?3.3). Therefore, the notion
of an informer is non-trivial.
Using a parse of the question sentence, we derive
a novel set of multi-resolution features suitable for
training a conditional random field (CRF) (Lafferty
et al, 2001; Sha and Pereira, 2003). Our feature de-
sign paradigm may be of independent interest (?4).
Our informer tagger is about 85?87% accurate.
We use a meta-learning framework (Chan and
Stolfo, 1993) in which a linear SVM predicts the an-
swer type based on features derived from the origi-
nal question as well as the output of the CRF. This
meta-classifier beats all published numbers on stan-
dard question classification benchmarks (?4.4). Ta-
ble 1 (last two rows) summarizes our main results.
2 Informer overview
Our key insight is that a human can classify a ques-
tion based on very few tokens gleaned from skeletal
syntactic information. This is certainly true of the
most trivial classes (Who wrote Hamlet? or How
many dogs pull a sled at Iditarod?) but is also true of
more subtle clues (How much does a rhino weigh?).
In fact, informal experiments revealed the surpris-
ing property that only one contiguous span of tokens
is adequate for a human to classify a question. E.g.,
in the above question, a human does not even need
the how much clue once the word weigh is avail-
able. In fact, ?How much does a rhino cost?? has an
identical syntax but a completely different answer
type, not revealed by how much alone. The only
exceptions to the single-span hypothesis are multi-
function questions like ?What is the name and age
of . . .?, which should be assigned to multiple answer
types. In this paper we consider questions where one
type suffices.
Consider another question with multiple clues:
Who is the CEO of IBM? In isolation, the clue who
merely tells us that the answer might be a person or
country or organization, while CEO is perfectly pre-
cise, rendering who unnecessary. All of the above
applies a forteriori to what and which clues, which
are essentially uninformative on their own, as in
?What is the distance between Pisa and Rome??
Conventional QA systems use mild analysis on
the wh-clues, and need much more sophistication on
the rest of the question (e.g. inferring author from
wrote, and even verb subcategorization). We submit
that a single, minimal, suitably-chosen contiguous
316
span of question token/s, defined as the informer
span of the question, is adequate for question clas-
sification.
The informer span is very sensitive to the struc-
ture of clauses, phrases and possessives in the ques-
tion, as is clear from these examples (informers ital-
icized): ?What is Bill Clinton?s wife?s profession?,
and ?What country?s president was shot at Ford?s
Theater?. The choice of informer spans also de-
pends on the target classification system. Initially
we wished to handle definition questions separately,
and marked no informer tokens in ?What is digi-
talis?. However, what is is an excellent informer
for the UIUC class DESC:def (description, defi-
nition).
3 The meta-learning approach
We propose a meta-learning approach (?3.1) in
which the SVM can use features from the original
question as well as its informer span. We show
(?3.2) that human-annotated informer spans lead to
large improvements in accuracy. However, we show
(?3.3) that simple heuristic extraction rules com-
monly used in QA systems (e.g. head of noun phrase
following wh-word) cannot provide informers that
are nearly as useful. This naturally leads us to de-
signing an informer tagger in ?4.
Figure 1 shows our meta-learning (Chan and
Stolfo, 1993) framework. The combiner is a linear
multi-class one-vs-one SVM2, as in the Zhang and
Lee (2003) baseline. We did not use ECOC (Ha-
cioglu and Ward, 2003) because the reported gain is
less than 1%.
The word feature extractor selects unigrams and
q-grams from the question. In our experience, q =
1 or q = 2 were best; if unspecified, all possible
qgrams were used. Through tuning, we also found
that the SVM ?C? parameter (used to trade between
training data fit and model complexity) must be set
to 300 to achieve their published baseline numbers.
3.1 Adding informer features
We propose two very simple ways to derive features
from informers for use with SVMs. Initially, assume
that perfect informers are known for all questions;
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
question CRF Informer
span tagger
Word and qgram
feature extractor
Informer
feature extractor
Combined feature vector
class
SV
M
 
M
et
a 
Le
ar
n
er
Figure 1: The meta-learning approach.
later (?4) we study how to predict informers.
Informer q-grams: This comprises of all word q-
grams within the informer span, for all possible q.
E.g., such features enable effective exploitation of
informers like length or height to classify to the
NUMBER:distance class in the UIUC data.
Informer q-gram hypernyms: For each word or
compound within the informer span that is a Word-
Net noun, we add all hypernyms of all senses. The
intuition is that the informer (e.g. author, crick-
eter, CEO) is often narrower than a broad ques-
tion class (HUMAN:individual). Following hy-
pernym links up to person via WordNet produces a
more reliably correlated feature.
Given informers, other question words might
seem useless to the classifier. However, retaining
regular features from other question words is an ex-
cellent idea for the following reasons.
First, we kept word sense disambiguation (WSD)
outside the scope of this work because WSD en-
tails computation costs, and is unlikely to be reliable
on short single-sentence questions. Questions like
How long . . . or Which bank . . . can thus become
ambiguous and corrupt the informer hypernym fea-
tures. Additional question words can often help nail
the correct class despite the feature corruption.
Second, while our CRF-based approach to in-
former span tagging is better than obvious alterna-
tives, it still has a 15% error rate. For the questions
where the CRF prediction is wrong, features from
non-informer words give the SVM an opportunity to
still pick the correct question class.
Word features: Based on the above discussion,
one boolean SVM feature is created for every word
q-gram over all question tokens. In experiments, we
found bigrams (q = 2) to be most effective, closely
followed by unigrams (q = 1). As with informers,
we can also use hypernyms of regular words as SVM
317
features (marked ?Question bigrams + hypernyms?
in Table 2).
3.2 Benefits from ?perfect? informers
We first wished to test the hypothesis that identi-
fying informer spans to an SVM learner can im-
prove classification accuracy. Over and above the
class labels, we had two volunteers tag the 6000
UIUC questions with informer spans (which we call
?perfect??agreement was near-perfect).
Features Coarse Fine
Question trigrams 91.2 77.6
All question qgrams 87.2 71.8
All question unigrams 88.4 78.2
Question bigrams 91.6 79.4
+informer q-grams 94.0 82.4
+informer hypernyms 94.2 88.0
Question unigrams + all informer 93.4 88.0
Only informer 92.2 85.0
Question bigrams + hypernyms 91.6 79.4
Table 2: Percent accuracy with linear SVMs, ?per-
fect? informer spans, and various feature encodings.
Observe in Table 2 that the unigram baseline is
already quite competitive with the best prior num-
bers, and exploiting perfect informer spans beats all
known numbers. It is clear that both informer q-
grams and informer hypernyms are very valuable
features for question classification. The fact that no
improvement was obtained with over Question bi-
grams using Question hypernyms highlights the im-
portance of choosing a few relevant tokens as in-
formers and designing suitable features on them.
Table 3 (columns b and e) shows the benefits from
perfect informers broken down into broad question
types. Questions with what as the trigger are the
biggest beneficiaries, and they also form by far the
most frequent category.
The remaining question, one that we address in
the rest of the paper, is whether we can effectively
and accurately automate the process of providing in-
former spans to the question classifier.
3.3 Informers provided by heuristics
In ?4 we will propose a non-trivial solution to the
informer-tagging problem. Before that, we must jus-
tify that such machinery is indeed required.
Some leading QA systems extract words very
similar in function to informers from the parse tree
of the question. Some (Singhal et al, 2000) pick
the head of the first noun phrase detected by a shal-
low parser, while others use the head of the noun
phrase adjoining the main verb (Ramakrishnan et al,
2004). Yet others (Harabagiu et al, 2000; Hovy
et al, 2001) use hundreds of (unpublished to our
knowledge) hand-built pattern-matching rules on the
output of a full-scale parser.
A natural baseline is to use these extracted words,
which we call ?heuristic informers?, with an SVM
just like we used ?perfect? informers. All that re-
mains is to make the heuristics precise.
How: For questions starting with how, we use the
bigram starting with how unless the next word
is a verb.
Wh: If the wh-word is not how, what or which, use
the wh-word in the question as a separate fea-
ture.
WhNP: For questions having what and which, use
the WHNP if it encloses a noun. WHNP is the
Noun Phrase corresponding to the Wh-word,
given by a sentence parser (see ?4.2).
NP1: Otherwise, for what and which questions, the
first (leftmost) noun phrase is added to yet an-
other feature subspace.
Table 3 (columns c and f) shows that these
already-messy heuristic informers do not capture the
same signal quality as ?perfect? informers. Our find-
ings corroborate Li and Roth (2002), who report lit-
tle benefit from adding head chunk features for the
fine classification task.
Moreover, observe that using heuristic informer
features without any word features leads to rather
poor performance (column c), unlike using perfect
informers (column b) or even CRF-predicted in-
former (column d, see ?4). These clearly establish
that the notion of an informer is nontrivial.
4 Using CRFs to label informers
Given informers are useful but nontrivial to recog-
nize, the next natural question is, how can we learn
to identify them automatically? From earlier sec-
tions, it is clear (and we give evidence later, see Ta-
ble 5) that sequence and syntax information will be
318
6 coarse classes
B Only Informers B+ B+ B+
Type #Quest. (Bigrams) Perf.Inf H.Inf CRF.Inf Perf.Inf H.Inf CRF.Inf
what 349 88.8 89.4 69.6 79.3 91.7 87.4 91.4
which 11 72.7 100.0 45.4 81.8 100.0 63.6 81.8
when 28 100.0 100.0 100.0 100.0 100.0 100.0 100.0
where 27 100.0 96.3 100.0 96.3 100.0 100.0 100.0
who 47 100.0 100.0 100.0 100.0 100.0 100.0 100.0
how * 32 100.0 96.9 100.0 100.0 100.0 100.0 100.0
rest 6 100.0 100.0 100.0 66.7 100.0 66.7 66.7
Total 500 91.6 92.2 77.2 84.6 94.2 90.0 93.4
50 fine classes
what 349 73.6 82.2 61.9 78.0 85.1 79.1 83.1
which 11 81.8 90.9 45.4 73.1 90.9 54.5 81.8
when 28 100.0 100.0 100.0 100.0 100.0 100.0 100.0
where 27 92.6 85.2 92.6 88.9 88.9 92.5 88.9
who 47 97.9 93.6 93.6 93.6 100.0 100.0 97.9
how * 32 87.5 84.3 81.2 78.1 87.5 90.6 90.6
rest 6 66.7 66.7 66.7 66.7 100.0 66.7 66.7
Total 500 79.4 85.0 69.6 78.0 88.0 82.6 86.2
a b c d e f g
Table 3: Summary of % accuracy broken down by question type (referred from ?3.2, ?3.3 and ?4.4). a:
question bigrams, b: perfect informers only, c: heuristic informers only, d: CRF informers only, e?g:
bigrams plus perfect, heuristic and CRF informers.
important.
We will model informer span identification as a
sequence tagging problem. An automaton makes
probabilistic transitions between hidden states y,
one of which is an ?informer generating state?, and
emits tokens x. We observe the tokens and have to
guess which were produced from the ?informer gen-
erating state?.
Hidden Markov models are extremely popular for
such applications, but recent work has shown that
conditional random fields (CRFs) (Lafferty et al,
2001; Sha and Pereira, 2003) have a consistent ad-
vantage over traditional HMMs in the face of many
redundant features. We refer the reader to the above
references for a detailed treatment of CRFs. Here
we will regard a CRF as largely a black box3.
To train a CRF, we need a set of state nodes, a
transition graph on these nodes, and tokenized text
where each token is assigned a state. Once the CRF
is trained, it can be applied to a token sequence, pro-
3We used http://crf.sourceforge.net/
ducing a predicted state sequence.
4.1 State transition models
We started with the common 2-state ?in/out? model
used in information extraction, shown in the left half
of Figure 2. State ?1? is the informer-generating
state. Either state can be initial and final (double
circle) states.
0 1 0 1 2
What kind of an animal is Winnie the Pooh
What, kind,
of, an, is,
Winnie, the,
Pooh 
animal
What, kind,
of, an 
is, Winnie,
the, Pooh 
animal
start start
Figure 2: 2- and 3-state transition models.
The 2-state model can be myopic. Consider the
question pair
319
A: What country is the largest producer of wheat?
B: Name the largest producer of wheat
The i?1 context of producer is identical in A and
B. In B, for want of a better informer, we would want
producer to be flagged as the informer, although it
might refer to a country, person, animal, company,
etc. But in A, country is far more precise.
Any 2-state model that depends on positions i?1
to define features will fail to distinguish between A
and B, and might select both country and producer
in A. As we have seen with heuristic informers, pol-
luting the informer pool can significantly hurt SVM
accuracy.
Therefore we also use the 3-state ?begin/in/out?
(BIO) model. The initial state cannot be ?2? in the
3-state model; all states can be final. The 3-state
model allows at most one informer span. Once the
3-state model chooses country as the informer, it is
unlikely to stretch state 1 up to producer.
There is no natural significance to using four or
more states. Besides, longer range syntax dependen-
cies are already largely captured by the parser.
What is the capital city of Japan
WP VBZ DT NN NN IN NNP
NP NP
PP
NP
VP
SQ
SBARQ
WHNP
0
1
2
3
4
5
6

Le
v
e
l
Figure 3: Stanford Parser output example.
4.2 Features from a parse of the question
Sentences with similar parse trees are likely to have
the informer in similar positions. This was the in-
tuition behind Zhang et al?s tree kernel, and is also
our starting point. We used the Stanford Lexicalized
Parser (Klein and Manning, 2003) to parse the ques-
tion. (We assume familiarity with parse tree notation
for lack of space.) Figure 3 shows a sample parse
tree organized in levels. Our first step was to trans-
i 1 2 3 4 5 6 7
yi 0 0 0 1 1 2 2
xi What is the capital city of Japan
` ? Features for xis
1 WP,1 VBZ,1 DT,1 NN,1 NN,1 IN,1 NNP,1
2 WHNP,1 VP,1 NP,1 NP,1 NP,1 Null,1 NP,2
3 Null,1 Null,1 Null,1 Null,1 Null,1 PP,1 PP,1
4 Null,1 Null,1 NP,1 NP,1 NP,1 NP,1 NP,1
5 Null,1 SQ,1 SQ,1 SQ,1 SQ,1 SQ,1 SQ,1
6 SBARQ SBARQSBARQSBARQSBARQSBARQSBARQ
Table 4: A multi-resolution tabular view of the ques-
tion parse showing tag and num attributes. capital
city is the informer span with y = 1.
late the parse tree into an equivalent multi-resolution
tabular format shown in Table 4.
Cells and attributes: A labeled question com-
prises the token sequence xi; i = 1, . . . and the label
sequence yi, i = 1, . . . Each xi leads to a column
vector of observations. Therefore we use matrix no-
tation to write down x: A table cell is addressed as
x[i, `] where i is the token position (column index)
and ` is the level or row index, 1?6 in this example.
(Although the parse tree can be arbitrarily deep, we
found that using features from up to level ` = 2 was
adequate.)
Intuitively, much of the information required for
spotting an informer can be obtained from the part
of speech of the tokens and phrase/clause attachment
information. Conversely, specific word information
is generally sparse and misleading; the same word
may or may not be an informer depending on its po-
sition. E.g., ?What birds eat snakes?? and ?What
snakes eat birds?? have the same words but different
informers. Accordingly, we observe two properties
at each cell:
tag: The syntactic class assigned to the cell by
the parser, e.g. x[4, 2].tag = NP. It is well-known
that POS and chunk information are major clues to
informer-tagging, specifically, informers are often
nouns or noun phrases.
num: Many heuristics exploit the fact that the first
NP is known to have a higher chance of containing
informers than subsequent NPs. To capture this po-
sitional information, we define num of a cell at [i, `]
as one plus the number of distinct contiguous chunks
to the left of [i, `] with tags equal to x[4, 2].tag.
E.g., at level 2 in the table above, the capital city
320
forms the first NP, while Japan forms the second NP.
Therefore x[7, 2].num = 2.
In conditional models, it is notationally conve-
nient to express features as functions on (xi, yi). To
one unfamiliar with CRFs, it may seem strange that
yi is passed as an argument to features. At training
time, yi is indeed known, and at testing time, the
CRF algorithm efficiently finds the most probable
sequence of yis using a Viterbi search. True labels
are not revealed to the CRF at testing time.
Cell features IsTag and IsNum: E.g., the ob-
servation ?y4 = 1 and x[4, 2].tag = NP? is cap-
tured by the statement that ?position 4 fires the fea-
ture IsTag1,NP,2? (which has a boolean value).
There is an IsTagy,t,` feature for each (y, t, `)
triplet. Similarly, for every possible state y, ev-
ery possible num value n (up to some maximum
horizon), and every level `, we define boolean fea-
tures IsNumy,n,`. E.g., position 7 fires the feature
IsNum2,2,2 in the 3-state model, capturing the state-
ment ?x[7, 2].num = 2 and y7 = 2?.
Adjacent cell features IsPrevTag and
IsNextTag: Context can be exploited by a
CRF by coupling the state at position i with
observations at positions adjacent to position i
(extending to larger windows did not help). To
capture this, we use more boolean features: posi-
tion 4 fires the feature IsPrevTag1,DT,1 because
x[3, 1].tag = DT and y4 = 1. Position 4 also fires
IsPrevTag1,NP,2 because x[3, 2].tag = NP and
y4 = 1. Similarly we define a IsNextTagy,t,`
feature for each possible (y, t, `) triple.
State transition features IsEdge: Position i
fires feature IsEdgeu,v if yi?1 = u and yi = v.
There is one such feature for each state-pair (u, v)
allowed by the transition graph. In addition we have
sentinel features IsBeginu and IsEndu marking
the beginning and end of the token sequence.
4.3 Informer-tagging accuracy
We study the accuracy of our CRF-based informer
tagger wrt human informer annotations. In the next
section we will see the effect of CRF tagging on
question classification.
There are at least two useful measures of
informer-tagging accuracy. Each question has a
known set Ik of informer tokens, and gets a set
of tokens Ic flagged as informers by the CRF. For
each question, we can grant ourself a reward of 1 if
Ic = Ik, and 0 otherwise. In ?3.1, informers were
regarded as a separate (high-value) bag of words.
Therefore, overlap between Ic and Ik would be a
reasonable predictor of question classification accu-
racy. We use the Jaccard similarity |Ik?Ic|/|Ik?Ic|.
Table 5 shows the effect of using diverse feature sets.
Fraction Jaccard
Features used Ic = Ik overlap
IsTag 0.368 0.396
+IsNum 0.474 0.542
+IsPrevTag+IsNextTag 0.692 0.751
+IsEdge+IsBegin+IsEnd 0.848 0.867
Table 5: Effect of feature choices.
? IsTag features are not adequate.
? IsNum features improve accuracy 10?20%.
? IsPrevTag and IsNextTag (?+Prev
+Next?) add over 20% of accuracy.
? IsEdge transition features help exploit
Markovian dependencies and adds another
10?15% accuracy, showing that sequential
models are indeed required.
Type #Quest. Heuristic 2-state 3-state
Informers CRF CRF
what 349 57.3 68.2 83.4
which 11 77.3 83.3 77.2
when 28 75.0 98.8 100.0
where 27 84.3 100.0 96.3
who 47 55.0 47.2 96.8
how * 32 90.6 88.5 93.8
rest 6 66.7 66.7 77.8
Total 500 62.4 71.2 86.7
Table 6: Effect of number of CRF states, and com-
parison with the heuristic baseline (Jaccard accuracy
expressed as %).
Table 6 shows that the 3-state CRF performs
much better than the 2-state CRF, especially on diffi-
cult questions with what and which. It also compares
the Jaccard accuracy of informers found by the CRF
vs. informers found by the heuristics described in
?3.3. Again we see a clear superiority of the CRF
321
approach.
Unlike the heuristic approach, the CRF approach
is relatively robust to the parser emitting a somewhat
incorrect parse tree, which is not uncommon. The
heuristic approach picks the ?easy? informer, who,
over the better one, CEO, in ?Who is the CEO of
IBM?. Its bias toward the NP-head can also be a
problem, as in ?What country?s president . . .?.
4.4 Question classification accuracy
We have already seen in ?3.2 that perfect knowledge
of informers can be a big help. Because the CRF
can make mistakes, the margin may decrease. In this
section we study this issue.
We used questions with human-tagged informers
(?3.2) to train a CRF. The CRF was applied back
on the training questions to get informer predictions,
which were used to train the 1-vs-1 SVM meta-
learner (?3). Using CRF-tagged and not human-
tagged informers may seem odd, but this lets the
SVM learn and work around systematic errors in
CRF outputs.
Results are shown in columns d and g of Table 3.
Despite the CRF tagger having about 15% error, we
obtained 86.2% SVM accuracy which is rather close
to the the SVM accuracy of 88% with perfect in-
formers.
The CRF-generated tags, being on the training
data, might be more accurate that would be for un-
seen test cases, potentially misleading the SVM.
This turns out not to be a problem: clearly we are
very close to the upper bound of 88%. In fact, anec-
dotal evidence suggests that using CRF-assigned
tags actually helped the SVM.
5 Conclusion
We presented a new approach to inferring the type
of the answer sought by a well-formed natural lan-
guage question. We introduced the notion of a span
of informer tokens and extract it using a sequential
graphical model with a novel feature representation
derived from the parse tree of the question. Our ap-
proach beats the accuracy of recent algorithms, even
ones that used max-margin methods with sophisti-
cated kernels defined on parse trees.
An intriguing feature of our approach is that
when an informer (actor) is narrower than the ques-
tion class (person), we can exploit direct hyper-
nymy connections like actor to Tom Hanks, if avail-
able. Existing knowledge bases like WordNet and
Wikipedia, combined with intense recent work (Et-
zioni et al, 2004) on bootstrapping is-a hierarchies,
can thus lead to potentially large benefits.
Acknowledgments: Thanks to Sunita Sarawagi
for help with CRFs, and the reviewers for improv-
ing the presentation.
References
P. K Chan and S. J Stolfo. 1993. Experiments in mul-
tistrategy learning by meta-learning. In CIKM, pages
314?323, Washington, DC.
S Dumais, M Banko, E Brill, J Lin, and A Ng. 2002.
Web question answering: Is more always better? In
SIGIR, pages 291?298.
O Etzioni, M Cafarella, et al 2004. Web-scale informa-
tion extraction in KnowItAll. In WWW Conference,
New York. ACM.
K Hacioglu and W Ward. 2003. Question classifica-
tion with support vector machines and error correcting
codes. In HLT, pages 28?30.
S Harabagiu, D Moldovan, M Pasca, R Mihalcea, M Sur-
deanu, R Bunescu, R Girju, V Rus, and P Morarescu.
2000. FALCON: Boosting knowledge for answer en-
gines. In TREC 9, pages 479?488. NIST.
E Hovy, L Gerber, U Hermjakob, M Junk, and C.-Y
Lin. 2001. Question answering in Webclopedia. In
TREC 9. NIST.
R Khardon, D Roth, and L. G Valiant. 1999. Relational
learning for NLP using linear threshold elements. In
IJCAI.
D Klein and C. D Manning. 2003. Accurate unlexical-
ized parsing. In ACL, volume 41, pages 423?430.
C Kwok, O Etzioni, and D. S Weld. 2001. Scaling ques-
tion answering to the Web. In WWW Conference, vol-
ume 10, pages 150?161, Hong Kong.
J Lafferty, A McCallum, and F Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
X Li and D Roth. 2002. Learning question classifiers. In
COLING, pages 556?562.
G Ramakrishnan, S Chakrabarti, D. A Paranjpe, and
P Bhattacharyya. 2004. Is question answering an ac-
quired skill? In WWW Conference, pages 111?120,
New York.
F Sha and F Pereira. 2003. Shallow parsing with condi-
tional random fields. In HLT-NAACL, pages 134?141.
A Singhal, S Abney, M Bacchiani, M Collins, D Hindle,
and F Pereira. 2000. AT&T at TREC-8. In TREC 8,
pages 317?330. NIST.
D Zhang and W Lee. 2003. Question classification using
support vector machines. In SIGIR, pages 26?32.
Z Zheng. 2002. AnswerBus question answering system.
In HLT.
322
Question answering via Bayesian inference on lexical relations
Ganesh Ramakrishnan, Apurva Jadhav, Ashutosh Joshi, Soumen Chakrabarti, Pushpak Bhattacharyya
 
hare,apurvaj,ashuj,soumen,pb  @cse.iitb.ac.in
Dept. of Computer Science and Engg.,
Indian Institute of Technology, Mumbai, India
Abstract
Many researchers have used lexical networks
and ontologies to mitigate synonymy and polysemy
problems in Question Answering (QA), systems
coupled with taggers, query classifiers, and answer
extractors in complex and ad-hoc ways. We seek
to make QA systems reproducible with shared and
modest human effort, carefully separating knowl-
edge from algorithms. To this end, we propose
an aesthetically ?clean? Bayesian inference scheme
for exploiting lexical relations for passage-scoring
for QA . The factors which contribute to the effi-
cacy of Bayesian Inferencing on lexical relations are
soft word sense disambiguation, parameter smooth-
ing which ameliorates the data sparsity problem and
estimation of joint probability over words which
overcomes the deficiency of naive-bayes-like ap-
proaches. Our system is superior to vector-space
ranking techniques from IR, and its accuracy ap-
proaches that of the top contenders at the TREC QA
tasks in recent years.
1 Introduction
This paper describes an approach to probabilistic in-
ference using lexical relations, such as expressed by
a WordNet, an ontology, or a combination, with ap-
plications to passage-scoring for open-domain ques-
tion answering (QA).
The use of lexical resources in Information Re-
trieval (IR) is not new; for almost a decade, the
IR community has considered the use of natural
language processing techniques (Lewis and Jones,
1996) to circumvent synonymy, polysemy, and other
barriers to purely string-matching search engines. In
particular, a number of researchers have attempted
to use the English WordNet to ?bridge the gap? be-
tween query and response. Interestingly, the results
have mostly been inconclusive or negative (Fell-
baum, 1998a). A number of explanations have been
offered for this lack of success, some of which are
 presence of unnecessary links and absence of
necessary links in the WordNet (Fellbaum,
1998b),
 hurdle of Word Sense Disambiguation (WSD)
(Sanderson, 1994)
 ad-hocness in the distance and scoring func-
tions (Abe et al, 1996).
1.1 Question answering (QA)
Unlike IR systems which return a list of documents
in response to a query, from which the user must
extract the answer manually, the goal of QA is to
extract from the corpus direct answers to questions
posed in a natural language.
An important step before answer extraction is
to identify and rate candidate passages from docu-
ments which might contain the answer. The notion
of a passage is somewhat arbitrary: various notions
of a passage have emerged (Vorhees, 2000); For our
purposes, a passage comprises  consecutive sen-
tences, or  consecutive words.
In contrast to IR, where linguistic resources have
not been found very useful, QA has always de-
pended on a mixture of stock lexical networks and
custom ontologies (language-independent concep-
tual hierarchies) crafted through human understand-
ing of the task at hand (Harabagiu et al, 2000;
Clarke et al, 2001). Ontologies, hand-crafted and
customized, sometimes from the WordNet itself, are
employed for question type classification, relation-
ships between places, measures, etc.
The scoring (and thereby, ranking) of passages
through lexical networks or ontologies is more suc-
cessful in QA than in classic IR because of the na-
ture of the QA task. Passage-scoring in QA benefits
from indirect matches through an ontology.
By separating the passage-scoring algorithm from
the knowledge base, we can keep improving our sys-
tem by continually upgrading the lexical relations in
the knowledge base and retraining our inference al-
gorithm.
Map:  2 describes the related work.  3 gives the
motivation behind our approach and the background
information (WordNet and Bayesian inferencing).
 4 describes our QA system. Results are presented
in  5, and concluding remarks made in  6.
1
2 Related work
Information Retrieval (IR) systems such as
SMART (Buckley, 1985) rank documents for
relevance w.r.t. to a user query, based on keyword
match between the query and a document, each rep-
resented in the well-known ?vector space model?.
The degree of match is measured as the cosine of
the angle between query and document vectors.
In QA, an IR subsystem is typically used to short-
list passages which are likely to embed the answer.
Usually, several enhancements are made to stock IR
systems to meet this task.
First, the cosine measure used in stock vector-
space systems will be biased against long docu-
ments even if they embed the answer in a narrow
zone. This problem can be ameliorated by repre-
senting suitably-sized passage windows (rather than
whole documents) as vectors. While scoring pas-
sages using the cosine measure, we can also ignore
passage terms which do not occur in the query.
The second issue is one of proximity. A passage
is likely to be promising if query words occur close
to one another. Commercial search engines reward
proximity of matched query terms, but in undocu-
mented ways. Clarke et al (Clarke et al, 2001) ex-
ploit term proximity within documents for passage
scoring.
The third and most important limitation of stock
IR systems is the inability to bridge the lexical
chasm between question and potential answer via
lexical networks. One query from TREC (Vorhees,
2000) asks, ?Who painted Olympia?? The answer
is in the passage: ?Manet, who, after all, created
Olympia, gets no credit.?
QA systems use a gamut of techniques to deal
with this problem. FALCON (Harabagiu et al,
2000) (one of the best QA systems in recent TREC
competitions) integrates syntactic, semantic and
pragmatic knowledge for QA. It uses WordNet-
based query expansion to try to bridge the lexical
chasm. WordNet is customized into a answer-type
taxonomy to infer the expected answer type for a
question. Named-entity recognition techniques are
also employed to improve quality of passages re-
trieved. The answers are finally filtered by justifying
them using abductive reasoning. Mulder (Kwok et
al., 2001) uses a similar approach to perform QA on
Web scale. The well-known START system (Katz, )
goes even further in this direction.
Discussion: In general, the TREC QA systems di-
vide QA into two tasks: identifying relevant doc-
uments and extracting answer passages from them.
For the former task, most systems use traditional IR
engines coupled with ad-hoc query expansion based
on WordNet. Handcrafted knowledge bases, ques-
tion/answer type classifiers and a variety of heuris-
tics are used for the latter task. Success in QA
comes at the cost of great effort in custom-designed
wordnets and ontologies, and expansion, matching
and scoring heuristics which need to be upgraded
as the knowledge bases are enhanced. Ideally, we
should use a knowledge base which can be readily
extended, and a core scoring algorithm which is ele-
gant and ?universal?.
3 Proposed approach
3.1 An inferencing approach to QA
Given a question and a passage that contains the an-
swer, how do we correlate the two ? Take for exam-
ple, the following question
What type of animal is Winnie the Pooh?
and the answer passage is
A Canadian town that claims to be the birthplace
of Winnie the Pooh wants to erect a giant statue of
the famous bear; but Walt Disney Studios will not
permit it.
It is clear that there is a linkage between the ques-
tion word animal and the answer word bear. That
the word bear occurred in the answer, in the context
of Winnie, means that there was a hidden ?cause?
for the occurrence of bear, and that was the concept
of  animal  .
In general, there could be multiple words in the
question and answer that are connected by many hid-
den causes. This scenario is depicted in figure  1.
The causes themselves may have hidden causes as-
sociated with them.
QUESTION ANSWER
NODESNODES
Hidden Causes that are switched on
Observed nodes(WORDS) 
Hidden Causes that are switched off(CONCEPTS)
(CONCEPTS)
Figure 1: Motivation
2
These causal relationships are represented in on-
tologies and WordNets. The familiar English Word-
Net, in particular, encodes relations between words
and concepts. For instance WordNet gives the hy-
pernymy relation between the concepts  animal 
and  bear  .
3.2 WordNet
WordNet (Fellbaum, 1998b) is an online lexical ref-
erence system in which English nouns, verbs, ad-
jectives and adverbs are organized into synonym
sets or synsets, each representing one underly-
ing lexical concept. Noun synsets are related to
each other through hypernymy (generalization), hy-
ponymy (specialization), holonymy (whole of) and
meronymy (part of) relations. Of these, (hypernymy,
hyponymy) and (meronymy,holonymy) are comple-
mentary pairs.
The verb and adjective synsets are very sparsely
connected with each other. No relation is available
between noun and verb synsets. However, 4500 ad-
jective synsets are related to noun synsets with per-
tainyms (pertaining to) and attra (attributed with) re-
lations.
DOG, DOMESTIC_DOG, CANIS_FAMILIARIS 
CORGI, WELSH_CORGIFLAG
meronymy
(from CANIS, GENUS_CANIS)
hyponymy
Figure 2: Illustration of WordNet relations.
Figure  2 shows that the synset  dog, domes-
tic dog, canis familiaris  has a hyponymy link to
 corgi, welshcorgi  and meronymy link to  flag 
(?a conspicuously marked or shaped tail?). While
the hyponymy link helps us answer the question
(TREC#371) ?A corgi is a kind of what??, the
meronymy connection here is perhaps more confus-
ing than useful: this sense of flag is rare.
3.3 Inferencing on lexical relations
It is surprisingly difficult to make the simple idea
of bridging passage to query through lexical net-
works perform well in practice. Continuing the ex-
ample of Winnie the bear (section  3.1), the En-
glish WordNet has five synsets on the path from bear
to animal:  carnivore...  ,  placental mammal...  ,
 mammal...  ,  vertebrate..  ,  chordate...  .
Some of these intervening synsets would be ex-
tremely unlikely to be associated with a corpus that
is not about zoology; a common person would more
naturally think of a bear as a kind of animal, skip-
ping through the intervening nodes.
It is, however, dangerous to design an algorithm
which is generally eager to skip across links in a lex-
ical network. E.g., few QA applications are expected
to need an expansion of ?bottle? beyond ?vessel?
and ?container? to ?instrumentality? and beyond.
Another example would be the shallow verb hierar-
chy in the English WordNet, with completely dis-
similar verbs within very few links of each other.
There is also the problem of missing links.
Another important issue is which ?hidden causes?
(synsets) should be inferred to have caused words
in the text. This is a classical problem called
word sense disambiguation (WSD). For instance,
the word dog belongs to 6 noun synsets in Word-
Net. Which of the  synsets should be treated as the
?hidden cause? that generated the word dog in the
passage could be inferred from the fact that collie is
related to dog only through one of the latter?s senses
- it?s sense as  dog, domestic dog, Canis familiaris  .
But this problem of finding the ?appropriate? hidden
causes, in general, in non-trivial. Given that state-of-
the-art WSD systems perform not better than 74%
(Sanderson, 1994) (Lewis and Jones, 1996) (Fell-
baum, 1998b), in this paper, we use a probabilistic
approach to WSD - called ?soft WSD? (Pushpak, )
; hidden nodes are considered to have probabilisti-
cally ?caused? words in the question and answer or in
other words, causes are probabilistically ?switched
on?.
Clearly, any scoring algorithm that seeks to uti-
lize WordNet link information must also discrimi-
nate between them based (at least) on usage statis-
tics of the connected synsets. Also required is an
estimate of the likelihood of instantiating a synset
into a token because it was ?activated? by a closely
related synset. We find a Bayesian belief network
(BBN) a natural structure to encode such combined
knowledge from WordNet and corpus.
3.4 Bayesian Belief Network
A Bayesian Network (Heckerman, 1995) for a set of
random variables 	
			Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 436?446,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Bootstrapping of Corpus Annotations and Entity Types
Hrushikesh Mohapatra Siddhanth Jain
IIT Bombay
Soumen Chakrabarti?
Abstract
Web search can be enhanced in powerful ways if to-
ken spans in Web text are annotated with disambiguated
entities from large catalogs like Freebase. Entity anno-
tators need to be trained on sample mention snippets.
Wikipedia entities and annotated pages offer high-quality
labeled data for training and evaluation. Unfortunately,
Wikipedia features only one-ninth the number of enti-
ties as Freebase, and these are a highly biased sample
of well-connected, frequently mentioned ?head? entities.
To bring hope to ?tail? entities, we broaden our goal to a
second task: assigning types to entities in Freebase but
not Wikipedia. The two tasks are synergistic: know-
ing the types of unfamiliar entities helps disambiguate
mentions, and words in mention contexts help assign
types to entities. We present TMI, a bipartite graphical
model for joint type-mention inference. TMI attempts
no schema integration or entity resolution, but exploits
the above-mentioned synergy. In experiments involving
780,000 people in Wikipedia, 2.3 million people in Free-
base, 700 million Web pages, and over 20 professional
editors, TMI shows considerable annotation accuracy im-
provement (e.g., 70%) compared to baselines (e.g., 46%),
especially for ?tail? and emerging entities. We also com-
pare with Google?s recent annotations of the same corpus
with Freebase entities, and report considerable improve-
ments within the people domain.
1 Introduction
Thanks to automatic information extraction and se-
mantic Web efforts, keyword search over unstruc-
tured Web text is rapidly evolving toward entity-
and type-oriented queries (Guo et al, 2009; Pan-
tel et al, 2012) over semi-structured databases such
as Wikipedia, Freebase, and other forms of Linked
Data.
A key enabling component for such enhanced
search capability is a type and entity catalog. This
includes a directed acyclic graph of types under the
subTypeOf relation between types, and entities at-
tached to one or more types via instanceOf edges.
?soumen@cse.iitb.ac.in
YAGO (Suchanek et al, 2007) provides such a cat-
alog by unifying Wikipedia and WordNet, followed
by some cleanup.
Another enabling component is an annotated cor-
pus in which token spans (e.g., the word ?Albert?)
are identified as a mention of an entity (e.g., the
Physicist Einstein). Equipped with suitable indices,
a catalog and an annotated corpus let us find ?sci-
entists who played some musical instrument?, and
answer many other powerful classes of queries (Li
et al, 2010; Sawant and Chakrabarti, 2013).
Consequently, accurate corpus annotation has
been intensely investigated (Mihalcea and Csomai,
2007; Cucerzan, 2007; Milne and Witten, 2008;
Kulkarni et al, 2009; Han et al, 2011; Ratinov et
al., 2011; Hoffart et al, 2011). With two exceptions
(Zheng et al, 2012; Gabrilovich et al, 2013) that we
discuss later, public-domain corpus annotation work
has almost exclusively used Wikipedia and deriva-
tives, partly because Wikipedia provides not only a
standardized space of entities, but also reliably la-
beled mention text within its own documents, which
can be used to train machine learning algorithms for
entity disambiguation.
However, the high quality of Wikipedia comes
at the cost of low entity coverage (4.2 million)
and bias toward often-mentioned, richly-connected
?head? entities. Hereafter, Wikipedia entities are
called W . Freebase has fewer editorial controls, but
has at least nine times as many entities. This is par-
ticularly perceptible for people entities: one needs
to be relatively famous to be featured on Wikipedia,
but Freebase is less selective. Hereafter, Freebase
entities are called F .
As in any heavy-tailed distribution, even rela-
tively obsecure entities from F \W are collectively
mentioned a great many times on the Web, and in-
cluding them in Web annotation is critical, if entity-
oriented search is to impact the vast number of tail
436
queries submitted to Web search engines.
Primary goal ? corpus annotation: We have
thus established a pressing need to bootstrap from a
small entity catalog W (such as Wikipedia entities),
and a small reference corpus CW (e.g., Wikipedia
text) reliably annotated with entities from W , to a
much larger catalog F (e.g., Freebase), and an open-
domain large payload corpus C (e.g., the Web).
We can and will use entities in F ? W 1 in the
bootstrapping process, but the real challenge is to
annotate C with mentions m of entities in F \W .
Unlike for F ?W , we have no training mentions for
F \W . Therefore, the main disambiguation signal
is from the immediate entity neighborhood N(e) of
the candidate entity e in the Freebase graph. I.e., if
m also reliably mentions some entity in N(e), then
e becomes a stronger candidate. Unfortunately, for
many ?tail? entities e ? F \W , N(e) is sparse. Is
there hope for annotating the Web with tail entities?
Here, we achieve enhanced accuracy for the primary
annotation goal by extending it with a related sec-
ondary goal.
Secondary goal ? entity typing: If we had avail-
able a suitable type catalog T with associated enti-
ties in W , which in turn have known textual men-
tions, we can build models of contexts referring to
types like chemists, sports people and politicans.
When faced with people called John Williams in
F \W , we may first try to associate them with types.
This can then help disambiguate mentions to specific
instances of John Williams in F \W . In principle,
useful information may also flow in the reverse di-
rection: words in mention contexts may help assign
types to entities in F \W . For reasons to be made
clear, we choose YAGO (Suchanek et al, 2007) as
the type catalog T accompanying entities in W .
Our contributions: We present TMI, a bootstrap-
ping system for solving the two tasks jointly. Apart
from matches between the context of m and entity
names in N(e), TMI combines and balances evi-
dence from two other sources to decide if e is men-
tioned at token span m, and has type t:
? a language model for the context in which enti-
ties of type t are usually mentioned
1With F=Freebase and W=Wikipedia, F ?W ?W but not
quite; W \ F is small but non-empty.
? correlations between t and certain path features
generated from N(e).
TMI uses a novel probabilistic graphical model for-
mulation to integrate these signals. We give a de-
tailed account of our design of node and edge po-
tentials, and a natural reject option (recall/precision
tradeoff).
We report on extensive experiments using YAGO
types, Wikipedia entities and text, Freebase en-
tities, and text from ClueWeb122, a 700-million-
page Web corpus. We focus on all people enti-
ties in Wikipedia and Freebase, and provide three
kinds of evaluation. First, we evaluate TMI on over
1100 entities in F ? W and 5500 snippets from
Wikipedia text, where it visibly improves upon base-
lines and a recently proposed alternative method
(Zheng et al, 2012). Second, we resort to exten-
sive manual evaluation of annotation on ClueWeb12
Web text with Freebase entities, by professional ed-
itors at a commercial search company. TMI again
clearly outperforms strong baselines, doing partic-
ularly well for nascent or tail entities. TMI im-
proves per-snippet accuracy, for some classes of
entities, from 46% to 70%, and pooled F1 score
from 66% to 73%. Third, we compare TMI an-
notations with Google?s FACC1 (Gabrilovich et al,
2013) annotations restricted to people; TMI is sig-
nificantly better. Our annotations and related data
can be downloaded from http://www.cse.iit
b.ac.in/?soumen/doc/CSAW/. To our knowl-
edge, this is among the first reports on extensive hu-
man evaluation of machine annotation for F \W on
a large Web corpus.
2 Related work
The vast majority of entity annotation work (Mi-
halcea and Csomai, 2007; Cucerzan, 2007; Milne
and Witten, 2008; Kulkarni et al, 2009; Han et al,
2011; Ratinov et al, 2011; Hoffart et al, 2011) use
Wikipedia or derivative knowledge bases. (Ritter et
al., 2011) and (Zheng et al, 2012) are notable ex-
ceptions. (Ritter et al, 2011) use entity names for
distant supervision in POS tagging, chunking and
broad named entity typing in short tweets, which are
different from our goals.
Recently, others have investigated inferring types
2http://lemurproject.org/clueweb12/
437
Antony John
Williams
0g
gb
n
2k John
Williams
John 
Williams03
n
m
yf
z
0b
hb
qm
m
UK
St Aspah
chemist
academic
ChemSpider
type
type
founder
nationality
place of birth
Muskegon
Wisconsin
Madison
Athlete
American
football
player
place of birth
type
type
education education
??? type
Eunice 
Kanenstenhawi
Williams
children
? Antony John Williams, VP for 
Strategic Development and Head of 
the Cheminformatics group for the 
Royal Society of Chemistry has 
been honoured by Microsoft 
Research for his ?
? to a 20-0 lead by the second 
quarter with running back John 
Williams?s 1 yard touchdown 
quarterback Neil O Donnell?s ?
? Massachusetts, on 17 September 
1696, the daughter of Puritan 
minister Rev. John Williams and his 
wife Eunice Mather Williams ?
ChemSpider, Chemistry
Steelers, Packers, touchdown, 
quarterback, Colts, quarter, yard, 
goal, Indianapolis, field, receiver
Massachusetts, Eunice Williams, 
minister, Puritan, kanenstenhawi, 
Rev.
Mention contexts:
Salient words from page containing contexts:
(a) (b) (c)
Harvard University
Fig. 1: Signal synergies. Three of the many people mentioned as ?John Williams? on the Web are shown, with
Freebase MIDs. (a) Easy case where Freebase neighbors of 0ggbn2k match snippet and salient text, and type links are
also available. (b) No match exists between Freebase neighborhood and snippet, but type links help attach the snippet
to 03nmvfz. (c) Freebase provides no types, but we can provide types from YAGO based on snippet and salient text,
which also match neighbors of 0bhbqmm.
of emerging entities (related to our secondary goal).
In concurrent work, (Nakashole et al, 2013) pro-
pose integer linear program formulations for infer-
ring types of emerging entities from the way their
mentions are embedded in curated relation-revealing
phrases. (Lin et al, 2012) earlier approached the
problem using weight propagation in a bipartite
graph connecting unknown to known entities via
textual relation patterns. Both note that this can
boost mention disambiguation accuracy.
The closest work to ours is by (Zheng et al,
2012): they use semisupervised learning to anno-
tate a corpus with Freebase entities. Like (Milne
and Witten, 2008), they depend on unambiguous
entity-mention pairs to bootstrap a classifier, then
apply it to unlabeled, ambiguous mentions, creating
more training data. They use a per-type language
model like us (?3.3), but this is used as a secondary
cause for (word) feature generation, supplementing
and smoothing entity-specific language models. In
contrast, we use a rigorous graphical model to com-
bine new signals, not depending on naturally unam-
biguous mentions. Finally, in the interest of fully
automated evaluation, they limit their experiments
to F ?W and Wikipedia corpus, thus differing crit-
ically from our human evaluation on F and a Web
corpus.
(Gabrilovich et al, 2013) have recently released
FACC1: annotations of ClueWeb09 and ClueWeb12
with Freebase entities. Their algorithm is not yet
public. They report: ?Due to the sheer size of the
data, it was not possible to verify all the automatic
annotations manually. Based on a small-scale hu-
man evaluation, the precision . . . is believed to be
around 80?85%. Estimating the recall is of course
difficult; however, it is believed to be around 70-
85%.? In ?5, we will see that, for people entities,
TMI greatly increases recall beyond FACC1, keep-
ing precision unimpaired.
3 The three signals
Fig. 1 shows three Freebase entities mentioned as
?John Williams? in Web text, represented as nodes
with Freebase ?MID?s e = 0ggbn2k, 03nmvfz,
and 0bhbqmm, embedded in their Freebase graph
neighborhoods. Owing to larger size and higher
flux, Freebase shows less editorial uniformity than
Wikipedia. This shows up in missing or non-
standard relation edges. Unlike YAGO, where
each entity is attached to one or more types, e =
0bhbqmm does not have a type link. Many people
have a link labeled profession, which is a second
438
kind of type link. Entities like e = 0bhbqmm also
have small, uninformative graph neighborhoods.
Also shown are three mention contexts, each rep-
resented by the snippet immediately surrounding the
mention, and salient words from the documents con-
taining each snippet. (Salient words may be ex-
tracted as words that contribute the largest compo-
nents to the document represented as a TFIDF vec-
tor.) (a) shows a favorable but relatively rare case
where e = 0ggbn2k has reliable type links. We can-
not assume there will be a 1-to-1 correspondence be-
tween Freebase and YAGO types, but in ?3.2 we will
describe how to learn associations between Freebase
paths around entities and their YAGO types. The
snippet and salient words show reasonable overlap
with N(e). In ?3.4 we will describe features that
characterize such overlap. In (b), e = 03nmvfz
is reliably typed, but there is no direct match be-
tween N(e) and the snippet. Nevertheless, the snip-
pet can be reliably annotated with 03nmvfz if we
can learn associations between types American foot-
ball player and Athlete (or their approximate YAGO
target types) and several context/salient words (see
?3.3). In (c), e = 0bhbqmm is not reliably typed.
However, there are matches between N(e) and con-
text/salient words. Once the snippet-to-entity asso-
ciation is established, it is easier to assign 0bhbqmm
to suitable types in YAGO.
In this section we will first describe our design of
the target type space, and then the tree signals that
will be used in our joint inference.
3.1 Designing the target type space
By typing two people called John Williams as ac-
tor and footballer, we may also disambiguate their
mentions accurately. Therefore, we need a well-
organized type space where the types
? collectively cover most entities of interest,
? offer reasonable type prediction accuracy, and
? can be selected algorithmically, for any do-
main.
Wikipedia and Freebase have many obscure types
like ?people born in 1937? or ?artists from On-
tario? which satisfy none of the above requirements.
YAGO, on the other hand, has a clean hierarchy of
over 200,000 types with broad coverage and fine dif-
ferentiation. Most entities in F \ W can be accu-
if t has < Nlow = 5000 member entities then
reject t from our type space
return
if t has > Nhigh = 25000 member entities then
for each immediate child t? ? t do
call ChooseTypes(t?)
else
accept t into our type space (but do not recurse)
Fig. 2: Procedure ChooseTypes(t).
rately attached to one or more YAGO types.
YAGO lists around 37,000 subtypes of person.
To satisfy the three requirements above, we called
ChooseTypes(person) (Fig. 2); this resulted in
130 suitable types being selected. These directly
covered 80% of Freebase people; the rest could
mostly be attached to slightly over-generic types
within our selection.
3.2 Predicting types from entity neighborhood
There will generally not be a simple mapping be-
tween Freebase and target types. E.g., entity e
may be known as a Mayor in Freebase, but the
closest YAGO type may be Politician. Edge and
node labels from the Freebase graph neighborhood
N(e) can embed many clues for assigning e a target
type. E.g., e = 03nmvfz may have an edge labeled
playedFor to a node representing the Wikipedia en-
tity Pittsburgh Steelers, which has a type
link to NFL team. This two-hop link label sequence
would repeat for a large number of players, and can
be used as a feature in a classifier.
0
0.2
0.4
0.6
0.8
1
1 2 3 4Hops
Re
ac
ha
bl
e
Whole
Prune1
Prune2
Prune3
Fig. 3: Freebase has small diameter despite graph thin-
ning. Prune1 removes paths from the whole Free-
base graph that pass through nodes /user/root and /com-
mon/topic, Prune2 also removes node /people/person,
and Prune3 removes several other high degree hubs.
Two further refinements are needed to make this
work. First, we have to collect path labels around
negative instances we well, and submit positive and
negative path labels to a binary classifier to can-
439
cel the effect of frequent but non-informative path
types. Second, indiscriminate expansion around e is
infeasible because the Freebase graph has very small
diameter. Even after substantial pruning, paths of
length 3 and 4 reach over 40% and 96% of all nodes
(Fig. 3). This increases computational burden and
floods us with noisy and spurious paths. We rem-
edy the problem using an idea from PathRank (Lao
and Cohen, 2010). Instead of trying to explore all
paths originating (or terminating) at e, where e may
or may not belong to a target type, we focus on paths
between e and other known members of the target
type.
3.3 Type ?language model?
To exploit the second signal, shown in Fig. 1(b), we
need to model the association between target YAGO
types and the mention contexts of Wikipedia entities
known to belong to those types. This model compo-
nent is in the same spirit as (Zheng et al, 2012).
For each target YAGO type t, we sample positive
entities e ? F ?W , and for each e, we collect, from
Wikipedia annotated text, a corpus of snippets men-
tioning e. We remove the mention words and retain
the rest. We also collect salient words from the en-
tire Wikipedia document containing the snippet, as
shown in Fig. 1.
At this point each target type is associated with
a ?corpus? of contexts, each represented by snippet
words. We compute the IDF of all words in this
corpus3, and then represent each type as a TFIDF
vector (Salton and McGill, 1983). A test context is
turned into a similar vector, and its score with re-
spect to t is the cosine between these two vectors.
This simple approach was found superior to building
a more traditional smoothed multinomial unigram
model (Zhai, 2008) for each type. Given the output
of this component feeds into an outer discriminative
inference mechanism, a strict probabilistic model is
not necessary.
3.4 Entity neighborhood match with snippet
The third signal is a staple of any disambiguation
work: match the occurrence context against the
neighborhood in the structured representation. In
word sense disambiguation (WSD), support for as-
signing a word in context to a synset comes from
3Generic IDF from Wiki text does not work.
matches between, say, other words in the context and
the WordNet neighborhood of the proposed synset.
As in WSD, many approaches to Wikification mea-
sure some local consistency between a mention m
and the neighborhood N(e) of a candidate entity e.
N(e) is again limited by a maximum path length `.
From snippetmwe extract all phrases P (m) exclud-
ing the mention words. For each phrase p ? P (m),
if p occurs at least once4 in any node of N(e), then
we accumulate a credit of |p|
?
w?p IDF(w), where
w ranges over words in p, IDF(w) is its inverse
document frequency (Salton and McGill, 1983) in
Wikipedia, and |p| is the length of the phrase. This
rewards exact phrase matches.
e
t t?
m2m1
e?
Mentions in context
Candidate
entities
Candidate types
Fig. 4: Tripartite assignment problem.
4 Unified model
Figure 4 abstracts out the signals shown in Figure 1
into a tripartite assignment problem. Each mention
likem1 has to choose at most one entity from among
candidate aliasing entities like e and e?. Each entity
e ? F \W has to choose one type (for simplicity we
ignore zero or more than one types as possibilities)
from candidates like t and t?.
The configuration of thick (green) edges should
be preferred to alternative dotted (red) edges under
these considerations:
? There is high local affinity or compatibility be-
tween e and t, based on associations between t
and N(e) as discussed in ?3.2.
? There are better textual matches between N(e)
and m1, as compared to N(e?) and m1.
? In aggregate, the non-mention tokens in the
context of m1,m2 (shown as gray horizontal
lines) match well the language model associ-
ated with mentions of entities of type t (rather
than t?).
4Incorporating term frequency often polluted the score.
440
0?
0t1
0t0
?t2
? (t1)t1
? (t0)t0
?
 
? (e1)e1
? (e0)e0
?
 
? (e1)e1
? (e5)e5
Snippet node
potential
Snippet node
potential
Entity node
potential
Uniform node potential
for dummy entity
???
???
e1
e0
t0
0t0
0t0
Potential of edge
connecting snippet
to dummy entity 
node
???
?
e1
e0
0...
0t0
t0
Potential of edge
connecting snippet
node to entity node
Fig. 5: Illustration of the proposed bipartite graphical model, with tables for node and edge potentials and synthetic
?-entity nodes to implement the reject option.
We now present a unified model that combines the
signals and solves the two proposed tasks jointly.
We model two kinds of decision variables, which
will be represented by nodes in a graphical model.
Associated with each entity e ? F \ W there is a
hidden type variable (node) Te, which can take on a
value from (some subset of) the type catalog T . As-
sociated with each mention m (along with its snip-
pet context and all its observable features) there is
a hidden entity variable Em, which can take on val-
ues from some subset of entities. (For simplicity, we
assume that entities in F ?W have already been an-
notated in the corpus, and no m of interest mentions
such entities.)
We will model the probability of a joint assign-
ment of values to Te, Em as
log Pr(~t,~e) = ?
?
e
?e(te) + ?
?
m
?m(em)
+
?
e,m
?e,m(te, em)? const., (1)
where node log potentials are called ?e, ?m, edge
log potentials are called ?e,m, and ?, ? are tuned
constants. The log partition function, written
?const.? above, will not be of interest in infer-
ence, where we will seek to choose ~t,~e to maximize
Pr(~t,~e). In this section, we will design the node and
edge log potentials.
4.1 Node log potentials
Each node Te is associated with a node potential ta-
ble, mapping from possible types in Te to nonneg-
ative potentials. The potential values are supplied
from ?3.2 as the classifier output scores.
Each node Em is associated with a node poten-
tial table, mapping from possible entities in Em to
nonnegative potentials. The potential values are sup-
plied from ?3.4.
4.2 Edge log potentials
Suppose we assign Em = e, and Te = t. Then
we would like the non-mention context words of
m to be highly compatible with the type ?language
model? developed in ?3.3.
If e is among the set of values Em that Em can
take, then nodes Te and Em are joined by an edge.
This edge is associated with an edge potential table
?e,m : Te?Em ? R+. ?e,m(?, e?) will be set to zero
(cells shaded gray in Fig. 5) when e 6= e?. ?e,m(t, e)
is set to the cosine match score described in ?3.3.
4.3 The reject (a.k.a. null, nil, NA, ?) option
An algorithm may reject many snippets, i.e., refrain
from annotating them. This could be because the
snippet mentions an entity outside F (and outside
W ), or the system wishes to ensure high precision at
some cost to recall.
Rejection is modeled by adding, for each snippet
m, a pseudo or ?null? entity?m (also called ?no an-
441
notation? NA, null or nil in the TAC-KBP5 commu-
nity). For simplicity, we assume that ?m and ?m?
are incomparable or distinct for m 6= m?. I.e., we
do not offer to cluster mentions of unknown enti-
ties. These remain separate, unconnected nodes in
the (augmented) Freebase graph.
If we choose Em = ?m, we get zero credit for
matching non-mention text inm toN(?m), because
N(?m) = ? and ?3.4 has no information to con-
tribute. I.e., we set ?m(?m) = 0. In general,
?m(?) ? 0, so ?m gets the lowest possible credit
(but this will be modified shortly).
There is also a type variable T?m . To what type
should we assign ?entity? ?m? Because ?m has no
connections in the Freebase graph, no hint can come
from ?3.2. Put differently, ??m(t) will be constant
(say, zero) for all snippets m and types t.
Even if we do not know the entity mentioned in
m, the non-mention text in m will have differential
affinity to different types, obtained from ?3.3. This
means that, if Em = ?m is chosen, T?m will be
argmaxt ??m,m(t,?m), which explains the non-
mention words in m using the best available lan-
guage model associated with some type. For a dif-
ferent entity Em = e0 and type Te0 = t assign-
ment to win, ??m(e0)+??e0(t)+?e0,m(t, e0) must
exceed the null score above. This provides a us-
able recall/precision handle: we modify ?m(?m) to
a tuned number; making it smaller generally gives
higher recall and lower precision.
4.4 Inference and training
The goal of collective inference will be to assign a
type value to each Te and an entity value to each
Em. We seek the maximum a-posteriori (MAP) la-
bel, for which we use tree-reweighted message pass-
ing (TRW-S) (Kolmogorov, 2006). Our graph has
plenty of bipartite cycles, so inference is approxi-
mate. Given the sparsity of data, we preferred to
delexicalize our objective (1), i.e., avoid word-level
features and pre-aggregate their signals via time-
tested aggregators (such as TFIDF cosine). As a re-
sult we have only two free parameters ?, ? in (1),
which we tune via grid search. A more principled
training regimen is left for future work.
5http://www.nist.gov/tac/2013/KBP/
5 Experiments
We focus our experiments on one broad type of en-
tities, people, that is more challenging for disam-
biguators than typical showcase examples of distin-
guishing (Steve) Jobs from employment and Apple
Inc. from fruit.
We report on three sets of experiments. In ?5.1,
we restrict to entities from F ? W and Wikipedia
text, for which ground truth annotation is avail-
able. In ?5.2, we evaluate TMI and baselines on
ClueWeb12 and entities from Freebase, not lim-
ited to Wikipedia. In ?5.3, we compare TMI
with Google?s recently published FACC1 annota-
tions (Gabrilovich et al, 2013).
5.1 Reference corpus CW with F ?W entities
Limited to people, |F | = 2323792, |W | = 807381,
|F \ W | = 1544942, and |F ? W | = 778850. It
is easiest to evaluate TMI and others on Wikipedia
entities. They have known YAGO types. Wikipedia
text has explicit (disambiguated) entity annotations.
For these reasons, the few known systems for
Freebase-based annotation (Zheng et al, 2012) are
evaluated exclusively on F ?W .
5.1.1 Seeding and setup
People in F ?W are known by one or more men-
tion words/phrases. From these, we collect mention
phrases along with the candidate entity set for each
phrase. The number of candidates is the phrase?s de-
gree of ambiguity. We sort phrases by ambiguity and
draw a sample over the ambiguity range. This gives
us seed phrases with representative ambiguity. Then
we collect all entities mentioned by these phrases.
Overall we collect about 1100 entities and 5500 dis-
tinct mentions.
Contrast this with (Zheng et al, 2012), who sam-
ple entities from much fewer than 130, and largely
well-separated types: professional athletes, aca-
demics, actors, films, books, hotels, and tourist at-
tractions. If there were only two namesakes, an ac-
tor and a politician, the politician disappears, leav-
ing a naturally unambiguous alias. I.e., (Zheng et
al., 2012) did not ?complete? their entity sets with
aliased entities. For all these reasons, Z0 numbers
here are not directly comparable to those in their pa-
per.
442
5.1.2 Tasks and baselines
The structure of TMI suggests two natural base-
lines to compare against it. TMI solves two tasks si-
multaneously: assign types to entities and entities to
snippets. So the first baseline, T0, is one that solves
the typing task separately, and the second, A0, does
snippet annotation separately. A third baseline, Z0,
from (Zheng et al, 2012) does only snippet annota-
tion; they do not consider typing entities.
While evaluating types output by TMI and T0
against ground truth, we may wish to assign partial
credit for overlapping types, e.g., athlete vs. soccer
player, because our types form an incomplete hierar-
chy. We use the standard ?M&W? score of semantic
similarity between types (Milne and Witten, 2008)
for this.
As regards snippet annotation, Z0 (Zheng et al,
2012) does not specify any mechanism for han-
dling ?. Therefore we run two sets of experiments.
In one we eliminate all snippets with ground truth
?. A0, and TMI are also debarred from returning ?
for any snippet. In the other, snippets marked ? in
ground truth are included. A0 and TMI are enabled
to return ?, but Z0 cannot.
TMI A0 Z0
0/1 snippet accuracy 0.827 0.699 0.627
Fig. 6: Snippet annotation onCW corpus, F ?W entities,
? not allowed.
TMI A0 Z0
0/1 snippet accuracy 0.7307 0.651 0.622
Snippet precision 0.858 0.843 0.622
Snippet recall 0.777 0.692 0.639
Snippet F1 0.815 0.760 0.630
Fig. 7: Snippet annotation onCW corpus, F ?W entities,
? allowed.
5.1.3 Snippet annotation results
Fig. 6 shows snippet annotation accuracy (frac-
tion of snippets labeled with the correct entity) when
? is not allowed as an entity. As two uninformed
refernces, uniform random choice gives an accuracy
of 0.423 and choosing the entity with the largest
prior gives an accuracy of 0.767. TMI is consid-
erably better than A0, which is better than Z0 and
the uninformed references. This is despite training
Z0?s per-type topic models not only on unambiguous
0.
73
0.
93
0.
85
0.
52
0.
74 0.
81
0.
42
0.
75 0
.8
5
0
0.2
0.4
0.6
0.8
1
0?19 20?39 >=40Degree-->
Ac
cu
ra
cy
-->
TMI A0 Z0
0.
73
0.
98
0.
38
0.
75
0.
24
0.
34
0.
18
0.
98
0.
13
0
0.5
0.4
0.7
0.2
6
0?69 50?89 >=40Degree-->
Ac
cu
ea
yT
6-
->
MIZ ? ?
Fig. 8: Bucketed comparison between TMI and baselines,
F ?W , ? allowed.
TMI T0
0/1 type accuracy 0.80 0.81
M&W type accuracy 0.82 0.83
Fig. 9: Type inference, CW corpus, F ?W entities.
snippets, but also on a disjoint fraction ofF?W , as a
surrogate for Wikipedia?s containment in Freebase.
Fig. 7 repeats the experiment while allowing ?.
Here, in 0/1 accuracy, ? is regarded as just another
entity. Again, we see that TMI has a clear advantage.
Z0?s performance here is worse than in (Zheng et
al., 2012). This is explained by our much larger and
difficult-to-separate type system.
We disaggregate the summary results into buck-
ets, shown in Fig. 8. Each bucket covers a range of
degrees of entity nodes in Freebase, while roughly
balacing the number of snippets in each bucket. TMI
generally shows larger gains for low-degree buckets.
5.1.4 Type prediction results
We also compared the type inference accuracy of
TMI and T0; (Zheng et al, 2012) do not infer types.
The summary is in Fig. 9. Two uninformed baselines
are worth mentioning. Uniform random choice over
130 types gave only 2% accuracy. Chossing the type
with largest prior probability gave 28.2% accuracy.
TMI is much better, but offers no significant ben-
efit (or degradation) compared to T0. We verified,
partly by way of debugging, that there do exist enti-
ties e with small degree but a modest number of as-
signed snippets, for which snippet-to-N(e) matches
443
angus mcdonald, chris robinson, christopher henry, elizabeth
cameron, george woods , henry barnes, jack scott, jeremy
robert, john sherman, leonard thomas, marc anthony, mitchell
donald, morrison mark, parker edward, richard andrew , simon
scott, stephen ross, stuart baron, tom clark, whitney john, austin
scott, barbara johnson, brian peterson, carlos rivero, david
berman, david johns, donald fraser, george davies, george fisher,
graham smith, john pepper, jonathan edwards, kevin brown,
kevin hughes, matt johnson, michael davidson, nancy johnson,
paul holmes, pedro martins, peter frank, peter mitchell, peter
mullen, robert stern, roger edwards, stuart walker, terry evans,
tony angelo, tony ward, william jarvis, william sampson
Fig. 10: Seed mentions for confusion clusters for Web
corpus C and entities in F .
provide a boost to type prediction accuracy (about
50%), as compared to T0 (about 20% for these in-
stances). Therefore, the flow of information between
type and entity assignments is, in principle, bidirec-
tional, in the regime of such entities.
5.2 Payload corpus C with entities in F
Recall our main goal is to annotate payload cor-
pus C with entities in all of F . Experience with
F ?W and CW may not be representative of F and
C. Entities in F \W may not come with reference
mentions, and their type and entity neighborhoods
may be sparse. Furthermore, compared to the closed
world of F ?W , evaluating TMI and baselines over
C and F \ W is challenging. Entities in F \ W
do not have ground truth types in the type catalog
T (here, YAGO), nor snippets labeled by humans as
mentioning them. Therefore, we need human edi-
torial judgment, which is scarce. Even though TMI
can be applied at Web scale, the scale if evaluation
is limited by editorial input.
5.2.1 Seeding and data setup
There are about 2.3 million Freebase entities con-
nected to /people/person via type links. Similar to
?5.1.1, we chose phrases (Fig. 10) with diverse de-
gree of ambiguity (Fig. 11), to seed confusion clus-
ters. Then we completed the clusters by including
aliased entities, as before, so as not to artifically re-
duce the degree of ambiguity. Note that entities in
W can and do contend with entities in F \W . The
cluster size distribution is shown in Fig. 12. Limited
by editorial budget, we finished with 634 entities,
238 distinct aliases and 4,500 snippets.
We used the 700-million-page ClueWeb12 Web
corpus. All phrases in the expanded clusters are
0
.0
70
30
90
85
24
85
16
?
>=
54
D5
e?
g1
r4
2-
g
A2
c1
ua
y4
D
ua
y4
D1
A2
c
-=
?
Ag
5T
1M
2=
g
I5
A5
41
c?
Au
=5
aa
A=
2c
y>
1a
52
gy
4Z
u=
4?
>1
42
r?
g>
2g
5Z
-y
4Z
1I
y4
D5
4
>u
2A
A1
y?
>A
?
g
-y
4Z
1A
2g
T
c?
u=
y5
a1
Zy
e?
Z>
2g
5e
yg
>1
A5
44
T
42
85
41
5Z
-y
4Z
>
uy
4a
2>
14
?
e5
42
A=
2c
y>
1u
1a
52
gy
4Z
5Z
-y
4Z
1=
y4
I5
41
Iy
4D
54
y?
>A
?
g1
-y
D5
cy
g1
>u
2A
A
e?
e?
yg
1a
52
gy
4Z
1A
=2
cy
>
>A
?y
4A
1=
1-
ya
D5
4
>u
45
yc
?
g1
>u
2A
A1
>?
c2
g
42
r5
4A
1y
4A
=?
41
c2
4A
2g
I5
A5
41
Z1
c?
Au
=5
aa
gy
gu
T1
M
2=
g>
2g
1>
45
r4
2
cy
4?
gZ
y1
gy
gu
T1
M
2=
g>
2g
cy
4u
1y
gA
=2
gT
1Z
yg
?y
D5
e?
g1
M
1r
42
-g
M
2=
g1
-?
aa
?
yc
1>
=5
4c
yg
M
2=
g1
>=
54
cy
g1
M
4
M
2=
g1
=5
g4
T1
ry
4g
5>
M
2=
g1
51
I5
II5
4
M
54
5c
T1
42
r5
4A
1M
2=
g>
2g
M
yu
D1
Z5
gA
2g
1>
u2
AA
=5
g4
T1
u=
4?
>A
2I
=5
4
84
y=
yc
15
1>
c?
A=
85
24
85
1-
5>
u2
AA
16
?
>=
54
85
24
85
1a
5c
?5
a1
-2
2Z
>
85
24
85
1Z
1-
22
Z>
85
24
85
1r
yr
T1
-2
2Z
>
64
yg
D1
I5
A5
41
a5
=c
yg
g
5Z
-y
4Z
1M
2=
g1
ua
?
6A
2g
Z2
gy
aZ
1c
16
4y
>5
4
Z2
gy
aZ
1Z
1c
?
Au
=5
aa
Zy
4?
5g
18
4y
=y
c1
>c
?
A=
u=
y4
a5
>1
-?
aa
?
yc
1M
y4
e?
>
ry
4r
y4
y1
a1
M
2=
g>
2g
y4
A=
?4
1-
?
aa
?
yc
1>
yc
I>
2g
?gA?2g>?
cr
?
8?
?
AT
??
? 0
.0
70
30
90
??
g?
II5
A>
1?
.0
00
??
??
?
?g?II5A>
?r?8?AT
Fig. 11: Ambiguity distribution for Web corpus C. Un-
ambiguous names are usually fully expanded and very
rare, if at all present, in evaluated snippets.
0
.0
70
30
90
. 8 .. .8 7. 78 3. 38 9. 98
52416?>6=Degr-Accu
ay
4r
Tr
T-
gc
cu
Fig. 12: Confusion cluster size distribution for Web cor-
pus C.
loaded into a trie used by a map-reduce job to ex-
tract documents, then snippets, from the corpus.
Some phrases in Figs. 10 and 11 have overwhelm-
ing numbers of pages with matches. In produc-
tion, we naturally want all of them to be annotated.
But human editorial judgment being the bottleneck,
we sampled 50% or 50,000 snippets, whichever was
smaller. Starting with about 752,450 pages, we ran
the Stanford NER (Finkel et al, 2005) to mark per-
son spans. Pages with fewer than five non-person to-
kens per person were discarded; this effectively dis-
carded long list pages without any informative text
to disambiguate anyone, and left us with 574,135
pages. From these we collected 304,309 snippets
where the mention phrase is marked by the NER as
a person. Each seed phrase leads to one cluster on
which TMI and A0 are run. Note that ? must be
allowed on the open Web.
5.2.2 Editorial judgment
Finally, for each algorithm, about 634 entity-type
and about 4500 snippet-entity assignments are ran-
domly sampled and sent to 20 editors in a commer-
cial search engine company, who judged each as-
signment as correct or incorrect, without knowing
which algorithm produced the annotation, to avoid
444
TMI A0
entity
0/1 accuracy .714 .562
Pooled recall .764 .869
Precision .714 .562
F1 .738 .683
e
?
W
(0
/1
ac
c.
)
e
?
F
\
W
(0
/1
)
TMI .75 .62
A0 .65 .42
Fig. 13: Snippet summary for F and payload corpus C.
bias. Because the editors are trained professionals
(unlike Mechanical Turks), we increased our evalu-
ation coverage by having each type or entity assign-
ment reviewed by one editor.
Pooling: Ideally, editors can be asked to find the
best type or entity for each entity or snippet, but,
given the size and diversity of Freebase, the cogni-
tive burden would be unacceptable. In the Wikipedia
corpus CW , a snippet marked? (no entity) by an al-
gorithm can be judged a loss of recall if Wikipedia
ground truth annotates it with an entity. Unfortu-
nately, this is no longer practical for Web corpus C,
because 8,217 snippets marked ? would have to be
manually inspected and compared with a large num-
ber of candidate entities in Freebase. Therefore, we
adopt pooling as in TREC. (Although the pool is
small, A0 has very high recall.) Recall is evaluated
with respect to the union of snippets annoted with a
non-? entity by at least one competing algorithm,
with agreement in case of more than one.
0/1 Type accuracy: Editors judged each pro-
posed type as correct or not. Unlike in ?5.1, where
the true and proposed types could be compared via
M&W (Milne and Witten, 2008), they could not be
asked to objectively estimate relatedness between
types. Therefore we present only their reported post-
hoc 0/1 accuracy for types: T0 and TMI have 0/1
type accuracy of 0.828 and 0.818.
5.2.3 Snippet annotation results
Given the large gap between TMI and Z0 in the
easier setup in ?5.1, we no longer consider Z0, and
instead focus on TMI vs. A0. The summary com-
parison of A0 vs. TMI is shown in Fig. 13. Here
TMI?s absolute gains in 0/1 accuracy and F1 are
even larger than in ?5.1. To understand TMI?s per-
formance across a diversity of Freebase entity nodes
e, as a function of 1. the size and richness of N(e),
and 2. the number of snippets claimed to mention e,
we disaggregate the data of Fig. 13 into buckets of
0.73
0.98 0.90 0.93
0.52 0.84 0.57
0.75
0.4
0.5
0.8
0.7
0.9
0.1
063 467 962 ?>=0Degree--?
Ac
cu
ra
cy
--? TMI
A0
0.73
0.98
0.75
0.92
0.42
0.77
0.71
0.94
0.4
0.7
0.9
0.5
063 867 962 ?>10=DegDDrr?
-A
Ac
Du
ay
1r
r? TMI
Z0
Fig. 14: 0/1 accuracy and F1 for snippets, payload corpus
C and entities in F .
Snippet label judgements %
TMI ok, FACC1 ok, neither ? 22
TMI ok, FACC1 wrong, neither ? 6
TMI6= ? ok, FACC1=? wrong 40
TMI6= ? wrong, FACC1=? 23
TMI wrong, FACC1 wrong, neither ? 2
TMI= ? wrong, FACC16= ? correct 4
TMI= ?, FACC16= ?, wrong 3
(TMI= ?, FACC1= ?, not judged) -
Fig. 15: TMI vs. FACC1 comparison.
consecutive degrees, roughly balancing the number
of snippets per bucket, as shown in Fig. 14. At the
very low end of almost disconnected entity nodes,
no algorithm does very well, because these entities
are also hardly ever mentioned. When the entity is
popular and well-connected, TMI?s benefits are rela-
tively modest. TMI?s gains are best in the mid-range
of degrees. The gap narrows for large-degree nodes,
which is expected.
5.3 Comparison with FACC1
After collecting our pool of snippets as in ?5.2.2,
we consulted FACC1 (Gabrilovich et al, 2013), and
passed on FACC1 annotations to our editors. As be-
fore, the identity of the algorithm was concealed.
Results are shown in Fig. 15. In a large 40% of
cases, TMI labels correctly while FACC1 backs off.
The converse, where FACC1 backs off and TMI
makes a mistake, is about half as frequent. These
preliminary numbers suggest that TMI is able to
push recall beyond FACC1 while also giving better
precision.
445
6 Conclusion
We presented a formal model for bootstrapping from
YAGO types and entities annotated in Wikipedia to
two tasks, 1. annotating Web snippets with Freebase
entities, and 2. associating Freebase entities with
YAGO types. We presented TMI, a system to solve
the two tasks jointly. Experiments show that TMI?s
snippet annotation accuracy, especially for relatively
weakly-connected Freebase entities, is superior to
baselines. We aim to extend from people to all major
Freebase categories, and larger Web crawls.
Acknowledgment: We are grateful to Shrikant
Naidu, Muthusamy Chelliah, and the editors from
Yahoo! for their generous support. Shashank Gupta
helped process FACC1 data.
References
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In EMNLP Con-
ference, pages 708?716.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In ACL Conference, pages 363?370.
Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag
Subramanya. 2013. FACC1: Freebase annotation
of ClueWeb corpora. http://lemurproject.or
g/clueweb12/, June. Version 1 (Release date 2013-
06-26, Format version 1, Correction level 0).
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In SIGIR Confer-
ence, pages 267?274. ACM.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in Web text: A graph-based method. In
SIGIR Conference, pages 765?774.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen Fu?rstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard Weikum.
2011. Robust disambiguation of named entities in text.
In EMNLP Conference, pages 782?792, Edinburgh,
Scotland, UK, July. SIGDAT.
Vladimir Kolmogorov. 2006. Convergent tree-
reweighted message passing for energy minimization.
IEEE PAMI, 28(10):1568?1583, October.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
Wikipedia entities in Web text. In SIGKDD Confer-
ence, pages 457?466.
Ni Lao and William W. Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine Learning, 81(1):53?67, October.
Xiaonan Li, Chengkai Li, and Cong Yu. 2010. Enti-
tyEngine: Answering entity-relationship queries using
shallow semantics. In CIKM, October. (demo).
Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun
phrase left behind: detecting and typing unlinkable en-
tities. In EMNLP Conference, pages 893?903.
R Mihalcea and A Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM, pages
233?242.
David Milne and Ian H Witten. 2008. Learning to link
with Wikipedia. In CIKM, pages 509?518.
Ndapandula Nakashole, Tomasz Tylenda, and Gerhard
Weikum. 2013. Fine-grained semantic typing of
emerging entities. In ACL Conference.
Patrick Pantel, Thomas Lin, and Michael Gamon. 2012.
Mining entity types from query logs via user intent
modeling. In ACL Conference, pages 563?571, Jeju
Island, Korea, July.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for
disambiguation to Wikipedia. In ACL Conference,
ACL/HLT, pages 1375?1384, Portland, Oregon.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In EMNLP Conference, pages 1524?
1534, Edinburgh, UK. ACL.
G Salton and M J McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill.
Uma Sawant and Soumen Chakrabarti. 2013. Learn-
ing joint query interpretation and response ranking. In
WWW Conference, Brazil.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge unifying WordNet and Wikipedia. In WWWCon-
ference, pages 697?706. ACM Press.
ChengXiang Zhai. 2008. Statistical language models
for information retrieval: A critical review. Founda-
tions and Trends in Information Retrieval, 2(3):137?
213, March.
Zhicheng Zheng, Xiance Si, Fangtao Li, Edward Y.
Chang, and Xiaoyan Zhu. 2012. Entity disambigua-
tion with Freebase. In Web Intelligence Conference,
WI-IAT ?12, pages 82?89.
446
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104?1114,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Knowledge Graph and Corpus Driven Segmentation and
Answer Inference for Telegraphic Entity-seeking Queries
Mandar Joshi
?
IBM Research
mandarj90@in.ibm.com
Uma Sawant
IIT Bombay, Yahoo Labs
uma@cse.iitb.ac.in
Soumen Chakrabarti
IIT Bombay
soumen@cse.iitb.ac.in
Abstract
Much recent work focuses on formal in-
terpretation of natural question utterances,
with the goal of executing the resulting
structured queries on knowledge graphs
(KGs) such as Freebase. Here we address
two limitations of this approach when ap-
plied to open-domain, entity-oriented Web
queries. First, Web queries are rarely well-
formed questions. They are ?telegraphic?,
with missing verbs, prepositions, clauses,
case and phrase clues. Second, the KG is
always incomplete, unable to directly an-
swer many queries. We propose a novel
technique to segment a telegraphic query
and assign a coarse-grained purpose to
each segment: a base entity e
1
, a rela-
tion type r, a target entity type t
2
, and
contextual words s. The query seeks en-
tity e
2
? t
2
where r(e
1
, e
2
) holds, fur-
ther evidenced by schema-agnostic words
s. Query segmentation is integrated with
the KG and an unstructured corpus where
mentions of entities have been linked to
the KG. We do not trust the best or any
specific query segmentation. Instead, evi-
dence in favor of candidate e
2
s are aggre-
gated across several segmentations. Ex-
tensive experiments on the ClueWeb cor-
pus and parts of Freebase as our KG, us-
ing over a thousand telegraphic queries
adapted from TREC, INEX, and Web-
Questions, show the efficacy of our ap-
proach. For one benchmark, MAP im-
proves from 0.2?0.29 (competitive base-
lines) to 0.42 (our system). NDCG@10
improves from 0.29?0.36 to 0.54.
?
Work done as Masters student at IIT Bombay
1 Introduction
A majority of Web queries mention an entity or
type (Lin et al., 2012), as users increasingly ex-
plore the Web of objects using Web search. To
better support entity-oriented queries, commercial
Web search engines are rapidly building up large
catalogs of types, entities and relations, popu-
larly called a ?knowledge graph? (KG) (Gallagher,
2012). Despite these advances, robust, Web-scale,
open-domain, entity-oriented search faces many
challenges. Here, we focus on two.
1.1 ?Telegraphic? queries
First, the surface utterances of entity-oriented Web
queries are dramatically different from TREC-
or Watson-style factoid question answering (QA),
where questions are grammatically well-formed.
Web queries are usually ?telegraphic?: they are
short, rarely use function words, punctuations
or clausal structure, and use relatively flexible
word orders. E.g., the natural utterance ?on the
bank of which river is the Hermitage Museum lo-
cated? may be translated to the telegraphic Web
query hermitage museum river bank. Even
on well-formed question utterances, 50% of in-
terpretation failures are contributed by parsing or
structural matching failures (Kwiatkowski et al.,
2013). Telegraphic utterances will generally be
even more challenging.
Consequently, whereas TREC-QA/NLP-style
research has focused on parsing and precise in-
terpretation of a well-formed query sentence to
a strongly structured (typically graph-oriented)
query language (Kasneci et al., 2008; Pound et
al., 2012; Yahya et al., 2012; Berant et al., 2013;
Kwiatkowski et al., 2013), the Web search and in-
formation retrieval (IR) community has focused
on telegraphic queries (Guo et al., 2009; Sarkas
et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin
et al., 2012; Sawant and Chakrabarti, 2013). In
terms of target schema richness, these efforts may
1104
appear more modest. The act of query ?interpre-
tation? is mainly a segmentation of query tokens
by purpose. In the example above, one may re-
port segments ?Hermitage Museum? (a located ar-
tifact or named entity), and ?river bank? (the target
type). This is reminiscent of record segmentation
in information extraction (IE). Over well-formed
utterances, IE baselines are quite competitive (Yao
and Van Durme, 2014). But here, we are interested
exclusively in telegraphic queries.
1.2 Incomplete knowledge graph
The second problem is that the KG is always
work in progress (Pereira, 2013), and connec-
tions found within nodes of the KG, between the
KG and the query, or the KG and unstructured
text, are often incomplete or erroneous. E.g.,
Wikipedia is considered tiny, and Freebase rather
small, compared to what is needed to answer all
but the ?head? queries. Google?s Freebase an-
notations (Gabrilovich et al., 2013) on ClueWeb
(ClueWeb09, 2009) number fewer than 15 per
page to ensure precision. Fewer than 2% are to
entities in Freebase but not in Wikipedia.
It may also be difficult to harness the KG for
answering certain queries. E.g., answering the
query fastest odi century batsman, the intent of
which is to find the batsman holding the record for
the fastest century in One Day International (ODI)
cricket, may be too difficult for most KG-only sys-
tems, but may be answered quite effectively by a
system that also utilizes evidence from unstruc-
tured text.
There is a clear need for a ?pay-as-you-go? ar-
chitecture that involves both the corpus and KG. A
query easily served by a curated KG should give
accurate results, but it is desirable to have a grace-
ful interpolation supported by the corpus: e.g., if
the relation r(e
1
, e
2
) is not directly evidenced in
the KG, but strongly hinted in the corpus, we still
want to use this for ranking.
1.3 Our contributions
Here, we make progress beyond the above frontier
of prior work in the following significant ways.
We present a new architecture for structural in-
terpretation of a telegraphic query into these seg-
ments (some may be empty):
? Mention/s e?
1
of an entity e
1
,
? Mention r? of a relation type r,
? Mention
?
t
2
of a target type t
2
, and
? Other contextual matching words s (some-
times called selectors),
with the simultaneous intent of finding and rank-
ing entities e
2
? t
2
, such that r(e
1
, e
2
) is likely
to hold, evidenced near the matching words in un-
structured text.
Given the short, telegraphic query utterances,
we limit our scope to at most one relation mention,
unlike the complex mapping of clauses in well-
formed questions to twig and join style queries
(e.g., ?find an actor whose spouse was an Italian
bookwriter?). On the other hand, we need to deal
with the unhelpful input, as well as consolidate
the KG with the corpus for ranking candidate e
2
s.
Despite the modest specification, our query tem-
plate is quite expressive, covering a wide range of
entity-oriented queries (Yih et al., 2014).
We present a novel discriminative graphical
model to capture the entity ranking inference task,
with query segmentation as a by-product. Ex-
tensive experiments with over a thousand entity-
seeking telegraphic queries using the ClueWeb09
corpus and a subset of Freebase show that we can
accurately predict the segmentation and intent of
telegraphic relational queries, and simultaneously
rank candidate responses with high accuracy. We
also present evidence that the KG and corpus have
synergistic salutary effects on accuracy.
?2 explores related work in more detail. ?3
gives some examples fitting our query template,
explains why interpreting some of them is nontriv-
ial, and sets up notation. ?4 presents our core tech-
nical contributions. ?5 presents experiments. Data
can be accessed at http://bit.ly/Spva49
and http://bit.ly/WSpxvr.
2 Related work
The NLP/QA community has traditionally as-
sumed that question utterances are grammatically
well-formed, from which precise clause structure,
ground constants, variables, and connective rela-
tions can be inferred via semantic parsing (Kas-
neci et al., 2008; Pound et al., 2012; Yahya et
al., 2012; Berant et al., 2013; Kwiatkowski et
al., 2013) and translated to lambda expressions
(Liang, 2013) or SPARQL style queries (Kasneci
et al., 2008), with elaborate schema knowledge.
Such approaches are often correlated with the as-
sumption that all usable knowledge has been cu-
rated into a KG. The query is first translated to a
structured form and then ?executed? on the KG. A
1105
Telegraphic query e?
1
r? t?
2
s
first african american nobel prize winner
nobel prize winner african american first
nobel prize - winner first african american
- - winner first african american nobel prize
dave navarro first band
dave navarro band - first
dave navarro band band first
merril lynch headquarters
merril lynch headquarters - -
merril lynch - headquarters -
spanish poet died in civil war
spanish died in poet civil war
civil war died - spanish poet
spanish in poet died civil war
first american in space
- - - first american in space
- - american first, in space
Figure 1: Example queries and some potential segmentations.
large corpus may be used to build relation expres-
sion models (Yao and Van Durme, 2014), but not
as supporting evidence for target entities.
In contrast, the Web and IR community gener-
ally assumes a free-form query that is often tele-
graphic (Guo et al., 2009; Sarkas et al., 2010; Li
et al., 2011). Queries being far more noisy, the
goal of structure discovery is more modest, and of-
ten takes the form of a segmentation of the query
regarded as a token sequence, assigning a broad
purpose (Pantel et al., 2012; Lin et al., 2012) to
each segment, mapping them probabilistically to
a relatively loose schema, and ranking responses
in conjunction with segmentations (Sawant and
Chakrabarti, 2013). To maintain quality in the face
of noisy input, these approaches often additionally
exploit clicks (Li et al., 2011) or a corpus that has
been annotated with entity mentions (Cheng and
Chang, 2010; Li et al., 2010). The corpus provides
contextual snippets for queries where the KG fails,
preventing the systems from falling off the ?struc-
ture cliff? (Pereira, 2013).
Our work advances the capabilities of the lat-
ter class of approaches, bringing them closer to
the depth of the former, while handling telegraphic
queries and retaining the advantage of corpus evi-
dence over and above the KG. Very recently, (Yao
et al., 2014) have concluded that for current bench-
marks, deep parsing and shallow information ex-
traction give comparable interpretation accuracy.
The very recent work of (Yih et al., 2014) is simi-
lar in spirit to ours, but they do not unify segmen-
tation and answer inference, along with corpus ev-
idence, like we do.
3 Notation and examples
We use e
1
, r, t
2
, e
2
to represent abstract nodes and
edges (MIDs in case of Freebase) from the KG,
and e?
1
, r?,
?
t
2
to represent their textual mentions or
hints, if any, in the query. s is a set of uninterpreted
textual tokens in the query that are used to match
and collect corpus contexts that lend evidence to
candidate entities.
Figure 1 shows some telegraphic queries
with possible segmentation into the above
parts. Consider another example: dave
navarro first band. ?Band? is a hint for
type /music/musical group, so it com-
prises
?
t
2
. Dave Navarro is an entity, with men-
tion words ?dave navarro? comprising e?
1
. r? is
made up of ?band?, and represents the relation
/music/group member/membership. Fi-
nally, the word first cannot be mapped to any sim-
ple KG artifact, so are relegated to s (which makes
the corpus a critical part of answer inference). We
use s and s? interchangeably.
Generally, there will be enough noise and uncer-
tainty that the search system should try out several
of the most promising segmentations as shown in
Figure 1. The accuracy of any specific segmenta-
tion is expected to be low in such adversarial set-
tings. Therefore, support for an answer entity is
aggregated over several segmentations. The ex-
pectation is that by considering multiple interpre-
tations, the system will choose the entity with best
supporting evidence from corpus and knowledge
base.
4 Our Approach
Telegraphic queries are usually short, so we enu-
merate query token spans (with some restrictions,
similar to beam search) to propose segmentations
(?4.1). Candidate response entities are lined up
for each interpretation, and then scored in a global
model along with query segmentations (?4.2).
?4.3 describes how model parameters are trained.
1106
1: input: query token sequence q
2: initialize segmentations I = ?
3: E
1
= (entity, mention) pairs from linker
4: for all (e
1
, e?
1
) ? E
1
do
5: assign label E
1
to mention tokens e?
1
6: for all contiguous span v ? q \ e?
1
do
7: label each word w ? v as T
2
R
8: label other words w ? q \ e?
1
\ v as S
9: add segments (E
1
, T
2
R,S) to I
10: end for
11: end for
12: return candidate segmentations I
Figure 2: Generating candidate query segmenta-
tions.
4.1 Generating candidate query
segmentations
Each query token can have four labels,
E
1
, T
2
, R, S, corresponding to the mentions
of the base entity, target type, connecting relation,
and context words. We found that segments
hinting at T
2
and R frequently overlapped
(e.g., ?author? in the query zhivago author).
In our implementation, we simplified to three
labels, E
1
, T
2
R,S, where tokens labeled T
2
R
are involved with both t
2
and r, the proposed
structured target type and connecting relation.
Another reasonable assumption was that the base
entity mention and type/relation mentions are
contiguous token spans, whereas context words
can be scattered in multiple segments.
Figure 2 shows how candidate segmentations
are generated. For step 3, we use TagMe (Ferrag-
ina and Scaiella, 2010), an entity linker backed by
an entity gazette derived from our KG.
4.2 Graphical model
Based on the previous discussion, we assume that
an entity-seeking query q is a sequence of tokens
q
1
, q
2
, . . ., and this can be partitioned into different
kinds of subsequences, corresponding to e
1
, r, t
2
and s, and denoted by a structured (vector) label-
ing z = z
1
, z
2
, . . .. Given sequences q and z, we
can separate out (possibly empty) token segments
e?
1
(q, z),
?
t
2
(q, z), r?(q, z), and s?(q, z).
A query segmentation z becomes plausible in
conjunction with proposals for e
1
, r, t
2
and e
2
from the KG. The probability Pr(z, e
1
, r, t
2
, e
2
|q)
is modeled as proportional to the product of sev-
eral potentials (Koller and Friedman, 2009) in a
graphical model. In subsequent subsections, we
will present the design of specific potentials.
? ?
R
(q, z, r) denotes the compatibility be-
tween the relation hint segment r?(q, z) and
a proposed relation type r in the KG (?4.2.1).
? ?
T
2
(q, z, t
2
) denotes the compatibility be-
tween the type hint segment
?
t
2
(q, z) and a
proposed target entity type t
2
in the KG
(?4.2.2).
? ?
E
1
,R,E
2
,S
(q, z, e
1
, r, e
2
) is a novel corpus-
based evidence potential that measures how
strongly e
1
and e
2
appear in corpus snippets
in the proximity of words in s?(q, z), and ap-
parently related by relation type r (?4.2.3).
? ?
E
1
(q, z, e
1
) denotes the compatibility be-
tween the query segment e?
1
(q, z) and entity
e
1
that it purportedly mentions (?4.2.4).
? ?
S
(q, z) denotes selector compatibility. Se-
lectors are a fallback label, so this is pinned
arbitrarily to 1; other potentials are balanced
against this base value.
? ?
E
1
,R,E
2
(e
1
, r, e
2
) is A if the relation
r(e
1
, e
2
) exists in the KG, and is B > 0 oth-
erwise, for tuned/learnt constants A > B >
0. Note that this is a soft constraint (B > 0);
if the KG is incomplete, the corpus may be
able to supplement the required information.
? ?
E
2
,T
2
(e
2
, t
2
) is 1 if e
2
belongs to t
2
and
zero otherwise. In other words, candidate e
2
s
must be proposed to be instances of the pro-
posed t
2
? this is a hard constraint, but can
be softened if desired, like ?
E
1
,R,E
2
.
Figure 3 shows the relevant variable states as
circled nodes, and the potentials as square factor
nodes. To rank candidate entities e
2
, we pin the
node E
2
to each entity in turn. With E
2
pinned,
we perform a MAP inference over all other hidden
variables and note the score of e
2
as the product of
the above potentials maximized over choices of all
other variables: score(e
2
) =
max
z,t
2
,r,e
1
?
T
2
(q, z, t
2
)?
R
(q, z, r)
?
E
1
(q, z, e
1
)?
S
(q, z)
?
E
2
,T
2
(e
2
, t
2
)?
E
1
,R,E
2
(e
1
, r, e
2
)
?
E
1
,R,E
2
,S
(q, z, e
1
, r, e
2
). (1)
We rank candidate e
2
s by decreasing score, which
is estimated by max-product message-passing
(Koller and Friedman, 2009).
As noted earlier, any of the relation/type, or
query entity partitions may be empty. To handle
1107
Target
type
Connecting
relation
Query
entity Selectors
Type
language
model
Relation
language
model
Entity
language
model
Segmentation
Candidate
entity
Corpus-assisted
entity-relation
evidence potential
Figure 3: Graphical model for query segmentation and entity scoring. Factors/potentials are shown as
squares. A candidate e
2
is observed and scored using equation (1). Query q is also observed but not
shown to reduce clutter; most potentials depend on it.
this case, we allow each of the entity, relation or
target type nodes in the graphical to take the value
? or ?null?. To support this, the value of the fac-
tor between the query segmentation node Z and
?
E1
(q, z, e
1
), ?
T2
(q, z, t
2
), and ?
R
(q, z, r)) are
set to suitable low values.
Next, we will describe the detailed design of
some of the key potentials introduced above.
4.2.1 Relation language model for ?
R
Potential ?
R
(q, z, r) captures the compatibility
between r?(q, z) and the proposed relation r.
E.g., if the query is steve jobs death rea-
son, and r? is (correctly chosen as) death rea-
son, then the correct candidate r is /people/
deceased_person/cause_of_death. An
incorrect r is /people/deceased_person/
place_of_death. An incorrect z may lead to
r?(q, z) being jobs death.
Using corpus: Considerable variation may exist
in how r is represented textually in a query. The
relation language model needs to build a bridge
between the formal r and the textual r?, so that
(un)likely r?s have (small) large potential. Many
approaches (Berant et al., 2013; Berant and Liang,
2014; Kwiatkowski et al., 2013; Yih et al., 2014)
to this problem have been intensely studied re-
cently. Given our need to process billions of Web
pages efficiently, we chose a pattern-based ap-
proach (Nakashole et al., 2012): with each r, dis-
cover the most strongly associated phrase patterns
from a reference corpus, then mark these patterns
into much larger payload corpus.
We started with the 2000 (out of approximately
14000) most frequent relation types in Freebase,
and the ClueWeb09 corpus annotated with Free-
base entities (Gabrilovich et al., 2013). For each
triple instance of each relation type, we located all
corpus sentences that mentioned both participat-
ing entities. We made the crude assumption that
if r(e
1
, e
2
) holds and e
1
, e
2
co-occur in a sen-
tence then this sentence is evidence of the rela-
tionship. Each such sentence is parsed to obtain
a dependency graph using the Malt Parser (Hall
et al., 2014). Words in the path connecting the
entities are joined together and added to a candi-
date phrase dictionary, provided the path is at most
three hops. (Inspection suggested that longer de-
pendency paths mostly arise out of noisy sentences
or botched parses.) 30% of the sentences were
thus retained. Finally, we defined
?
R
(q, z, r) =
n(r, r?(q, z))
?
p
?
n(r, p
?
)
, (2)
where p
?
ranges over all phrases that are known to
hint at r, and n(r, p) denotes the number of sen-
tences where the phrase p occurred in the depen-
dency path between the entities participating in re-
lation r.
Assuming entity co-occurrence implies evi-
dence is admittedly simplistic. However, the pri-
mary function of the relation model is to retrieve
top-k relations that are compatible with the type/s
1108
of e
1
and the given relation hint. Moreover, the
remaining noise is further mitigated by the collec-
tive scoring in the graphical model. While we may
miss relations if they are expressed in the query
through obscure hints, allowing the relation to be
? acts as a safety net.
Using Freebase relation names: As mentioned
earlier, queries may express relations differently as
compared to the corpus. A relation model based
solely on corpus annotations may not be able to
bridge that gap effectively, particularly so, because
of sparsity of corpus annotations or the rarity of
Freebase triples in ClueWeb. E.g., for the Freebase
relation /people/person/profession, we
found very few annotated sentences. One way
to address this problem is to utilize relation type
names in Freebase to map hints to relation types.
Thus, in addition to the corpus-derived relation
model, we also built a language model that used
Freebase relation type names as lemmas. E.g., the
word ?profession? would contribute to the relation
type /people/person/profession.
Our relation models are admittedly simple. This
is mainly because telegraphic queries may ex-
press relations very differently from natural lan-
guage text. As it is difficult to ensure precision of
query interpretation stage, our models are geared
towards recall. The system generates a large num-
ber of interpretations and relies on signals from
the corpus and KG to bring forth correct interpre-
tations.
4.2.2 Type language model for ?
T
2
Similar to the relation language model, we need
a type language model to measure compatibil-
ity between t
2
and
?
t
2
(q, z). Estimating the tar-
get entity type, without over-generalizing or over-
specifying it, has always been important for QA.
E.g., when
?
t
2
is ?city?, a good type language model
should prefer t
2
as /location/citytown
over /location/location while avoiding
/location/es_autonomous_city.
A catalog like Freebase suggests a straight-
forward method to collect a type language model.
Each type is described by one or more phrases
through the link /common/topic/alias. We
can collect these into a micro-?document? and
use a standard Dirichlet-smoothed language model
from IR (Zhai, 2008). In Freebase, an entity
node (e.g., Einstein, /m/0jcx) may be linked
to a type node (e.g. /base/scientist/
physicist) using an edge with label /type/
object/type.
But relation types provide additional clues to
types of the endpoint entities. Freebase relation
types have the form /x/y/z, where x is the
domain of the relation, and y and z are string
representations of the type of the entities partic-
ipating in the relation. E.g., the (directed) re-
lation type /location/country/capital
connects from from /location/country to
/location/citytown. Therefore, ?capital?
can be added to the set of descriptive phrases of
entity type /location/citytown.
It is important to note that while we use Free-
base link nomenclature for relation and type lan-
guage models, our models are not incompati-
ble with other catalogs. Indeed, most catalogs
have established ways of deriving language mod-
els that describe their various structures. For ex-
ample, most YAGO types are derived from Word-
Net synsets with associated phrasal descriptions
(lemmas). YAGO relations also have readable
names such as actedIn, isMarriedTo, etc. which
can be used to estimate language models. DB-
Pedia relations are mostly derived from (mean-
ingfully) named attributes taken from infoboxes,
hence they can be used directly. Furthermore, oth-
ers (Wu and Weld, 2007) have shown how to asso-
ciate language models with such relations.
4.2.3 Snippet scoring
The factor ?
E
1
,R,E
2
,S
(q, z, e
1
, r, e
2
) should be
large if many snippets contain a mention of e
1
and
e
2
, relation r, and many high-signal words from s.
Recall that we begin with a corpus annotated with
entity mentions. Our corpus is not directly anno-
tated with relation mentions. Therefore, we get
from relations to documents via high-confidence
phrases. Snippets are retrieved using a combined
entity + word index, and scored for a given e
1
, r,
e
2
, and selectors s?(q, z).
Given that relation phrases may be noisy and
that their occurrence in the snippet may not nec-
essarily mean that the given relation is being ex-
pressed, we need a scoring function that is cog-
nizant of the roles of relation phrases and enti-
ties occurring in the snippets. In a basic ver-
sion, e
1
, p, e
2
, s? are used to probe a combined en-
tity+word index to collect high scoring snippets,
with the score being adapted from BM25. The sec-
ond, refined scoring function used a RankSVM-
1109
style (Joachims, 2002) optimization.
min
?,?
???
2
+ C
?
e
+
,e
?
?
e
+
,e
? s.t.
?e
+
, e
?
: ? ? f(q,D
e
+ , e
+
) + ?
e
+
,e
? (3)
? ? ? f(q,D
e
? , e
?
) + 1.
where e
+
and e
?
are positive and negative enti-
ties for the query q and f(q,D
e
, e) represents the
feature map for the set of snippets D
e
belonging
to entity e. The assumption here is that all snip-
pets containing e
+
are ?positive? snippets for the
query. f consolidates various signals like the num-
ber of snippets where e occurs near query entity
e
1
and a relation phrase, or the number of snippets
with high proportion of query IDF, hinting that e
is a positive entity for the given query. A partial
list of features used for snippet scoring is given in
Figure 4.
Number of snippets with distance(e
2
, e?
1
) < k
1
(k
1
= 5, 10)
Number of snippets with distance(e
2
, relation phrase) < k
2
(k
2
= 3, 6)
Number of snippets with relation r = ?
Number of snippets with relation phrases as prepositions
Number of snippets covering fraction of query IDF > k
3
(k
3
= 0.2, 0.4, 0.6, 0.8)
Figure 4: Sample features used for learning
weights ? to score snippets.
4.2.4 Query entity model
Potential ?
E
1
(q, z, e
1
) captures the compatibil-
ity between e?
1
(q, z) (i.e., the words that mention
e
1
) and the claimed entity e
1
mentioned in the
query. We used the TagMe entity linker (Fer-
ragina and Scaiella, 2010) for annotating enti-
ties in queries. TagMe annotates the query with
Wikipedia entities, which we map to Freebase, and
use the annotation confidence scores as the poten-
tial ?
E
1
(q, z, e
1
).
4.3 Discriminative parameter training with
latent variables
We first set the potentials in (1) as explained in
?4.2 (henceforth called ?Unoptimized?), and got
encouraging accuracy. Then we rewrote each po-
tential as
?
?
(? ? ? ) = exp
(
w
?
? ?
?
(? ? ? )
)
(4)
or log
?
?
?
?
(? ? ? ) =
?
?
w
?
? ?
?
(? ? ? ),
with w
?
being a weight vector for a specific poten-
tial ?, and ?
?
being a corresponding feature vector.
During inference, we seek to maximize
max
q,z,e
1
,t
2
,r
w ? ?(q, z, e
1
, t
2
, r, e
2
), (5)
for a fixed w, to find the score of each candidate
entity e
2
. Here all w
?
and ?
?
have been collected
into unified weight and feature vectors w, ?. Dur-
ing training of w, we are given pairs of correct and
incorrect answer entities e
+
2
, e
?
2
, and we wish to
satisfy constraints of the form
max
q,z,e
1
,t
2
,r
w ? ?(q, z, e
1
, t
2
, r, e
+
2
) + ? (6)
? 1 + max
q,z,e
1
,t
2
,r
w ? ?(q, z, e
1
, t
2
, r, e
?
2
),
because collecting e
+
2
, e
?
2
pairs is less work than
supervising with values of z, e
1
, t
2
, r, e
2
for each
query. Similar distant supervision problems were
posed via bundle method by (Bergeron et al.,
2008), and (Yu and Joachims, 2009), who used
CCCP (Yuille and Rangarajan, 2006). These are
equivalent in our setting. We use the CCCP style,
and augment the objective with an additional en-
tropy term as in (Sawant and Chakrabarti, 2013).
We call this LVDT (latent variable discriminative
training) in ?5.
5 Experiments
5.1 Testbed
Corpus and knowledge graph: We used the
ClueWeb09B (ClueWeb09, 2009) corpus contain-
ing 50 million Web documents. This corpus
was annotated by Google with Freebase enti-
ties (Gabrilovich et al., 2013). The average page
contains 15 entity annotations from Freebase. We
used the Freebase KG and its links to Wikipedia.
Queries: We report on two sets of entity-seeking
queries. A sample of about 800 well-formed
queries from WebQuestions (Berant et al., 2013)
were converted to telegraphic utterances (such as
would be typed into commercial search engines)
by volunteers familiar with Web search. We call
this WQT (WebQuestions, telegraphic). Queries
are accompanied by ground truth entities. The
second data set, TREC-INEX, from (Sawant and
Chakrabarti, 2013) has about 700 queries sam-
pled from TREC and INEX, available at http:
//bit.ly/WSpxvr. These come with well-
formed and telegraphic utterances, as well as
ground truth entities.
1110
There are some notable differences between
these query sets. For WQT, queries were gener-
ated by using Google?s query suggestions inter-
face. Volunteers were asked to find answers using
single Freebase pages. Therefore, by construction,
queries retained can be answered using the Free-
base KG alone, with a simple r(e
1
, ?) form. In
contrast, TREC-INEX queries provide a balanced
mix of t
2
and r hints in the queries, and direct an-
swers from triples is relatively less available.
5.2 Implementation details
On an average, the pseudocode in Figure 2
generated 13 segmentations per query, with
longer queries generating more segmentations
than shorter ones.
We used an MG4J (Boldi and Vigna, 2005)
based query processor, written in Java, over en-
tity and word indices on ClueWeb09B. The in-
dex supplies snippets with a specified maximum
width, containing a mention of some entity and
satisfying a WAND (Broder et al., 2003) predi-
cate over words in s?. In case of phrases in the
query, the WAND threshold was computed by
adding the IDF of constituent words. The index
returned about 330,000 snippets on average for
WAND threshold of 0.6.
We retained the top 200 candidate entities from
the corpus; increasing this horizon did not give
benefits. We also considered as candidates for e
2
those entities that are adjacent to e
1
in the KG
via top-scoring r candidates. In order to gener-
ate supporting snippets for an interpretation con-
taining entity annotation e, we need to match e
with Google?s corpus annotations. However, re-
lying solely on corpus annotations fails to retrieve
many potential evidence snippets, because entity
annotations are sparse. Therefore we probed the
token index with the textual mention of e
1
in the
query; this improved recall.
We also investigated the feasibility of our pro-
posals for interactive search. There are three major
processes involved in answering a query - gener-
ating potential interpretations, collecting/scoring
snippets, and inference (MAP for Unoptimized
and w?(?) for LVDT). For the WQT dataset, av-
erage time per query for each stage was approx-
imately - 0.2, 16.6 and 1.3 seconds respectively.
Our (Java) code did not optimize the bottleneck
at all; only 10 hosts and no clever load balancing
were used. We believe commercial search engines
can cut this down to less than a second.
5.3 Research questions
In the rest of this section we will address these
questions:
? For telegraphic queries, is our entity-relation-
type-selector segmentation better than the
type-selector segmentation of (Sawant and
Chakrabarti, 2013)?
? When semantic parsers (Berant et al., 2013;
Kwiatkowski et al., 2013) are subjected to
telegraphic queries, how do they perform
compared to our proposal?
? Are the KG and corpus really complementary
as regards their support of accurate ranking of
candidate entities?
? Is the prediction of r and t
2
from our ap-
proach better than a greedy assignment based
on local language models?
We also discuss anecdotes of successes and fail-
ures of various systems.
5.4 Benefits of relation in addition to type
Figure 5 shows entity-ranking MAP, MRR, and
NDCG@10 (n@10) for two data sets and vari-
ous systems. ?No interpretation? is an IR baseline
without any KG. Type+selector is our implemen-
tation of (Sawant and Chakrabarti, 2013). Unopti-
mized and LVDT both beat ?no interpretation? and
?type+selector? by wide margins. (Boldface im-
plies best performing formulation.) There are two
notable differences between S&C and our work.
First, S&C do not use the knowledge graph (KG)
and rely on a noisy corpus. This means S&C fails
to answer queries whose answers are found only
in KG. This can be seen from WQT results; they
perform only slightly better than the baseline. Sec-
ond, even for queries that can be answered through
the corpus alone, S&C miss out on two important
signals that the query may provide - namely the
query entity and the relation. Our framework not
only provides a way to use a curated and high pre-
cision knowledge graph but also attempts to pro-
vide more reachability to corpus by the use of re-
lational phrases.
In case of TREC-INEX, LVDT improves upon
the unoptimized graphical model, where for WQT,
it does not. Preliminary inspection suggests this is
because WQT has noisy and incomplete ground
truth, and LVDT trains to the noise; a non-convex
1111
Dataset Formulation map mrr n@10
No interpretation .205 .215 .292
TREC Type+selector .292 .306 .356
-INEX Unoptimized .409 .419 .502
LVDT .419 .436 .541
No interpretation .080 .095 .131
WQT Type+selector .116 .152 .201
Unoptimized .377 .401 .474
LVDT .295 .323 .406
Figure 5: ?Entity-relation-type-selector? segmen-
tation yields better accuracy than ?type-selector?
segmentation.
objective makes matters worse. The bias in our
unoptimized model circumvents training noise.
5.5 Comparison with semantic parsers
For TREC-INEX, both unoptimized and LVDT
beat SEMPRE (Berant et al., 2013) convinc-
ingly, whether it is trained with Free917 or Web-
Questions (Figure 6).
SEMPRE?s relatively poor performance, in this
case, is explained by its complete reliance on the
knowledge graph. As discussed previously, the
TREC-INEX dataset contains a sizable proportion
of queries that may be difficult to answer using
a KG alone. When SEMPRE is compared with
our systems with a telegraphic sample of Web-
Questions (WQT), results are mixed. Our Unop-
timized model still compares favorably to SEM-
PRE, but with slimmer gains. As before, LVDT
falls behind.
Dataset Formulation map mrr n@10
SEMPRE(Free917) .154 .159 .186
TREC SEMPRE(WQ) .197 .208 .247
-INEX Unoptimized .409 .419 .502
LVDT .419 .436 .541
SEMPRE(Free917) .229 .255 .285
WQT SEMPRE(WQ) .374 .406 .449
Unoptimized .377 .401 .474
Jacana .239 .256 .329
LVDT .295 .323 .406
Figure 6: Comparison with semantic parsers.
Our smaller gains over SEMPRE in case of
WebQuestions is explained by how WebQuestions
was assembled (Berant et al., 2013). Although
Google?s query suggestions gave an eclectic pool,
only those queries survived that could be answered
using a single Freebase page, which effectively re-
duced the role of a corpus. In fact, a large frac-
tion of WQT queries cannot be answered well us-
ing the corpus alone, because FACC1 annotations
are too sparse and rarely cover common nouns and
phrases such as ?democracy? or ?drug overdose?
which are needed for some WQT queries.
For WQT, our system also compares favorably
with Jacana (Yao and Van Durme, 2014). Given
that they subject their input to natural langauge
parsing, their relatively poor performance is not
unsurprsing.
5.6 Complementary benefits of KG & corpus
Figure 7 shows the synergy between the corpus
and the KG. In all cases and for all metrics, using
the corpus and KG together gives superior perfor-
mance to using any of them alone. However, it
is instructive that in case of TREC-INEX, corpus-
only is better than KG-only, whereas this is re-
versed for WQT, which also supports the above
argument.
Data Formulation map mrr n@10
T
R
E
C
-
I
N
E
X
Unoptimized (KG) .201 .209 .241
Unoptimized (Corpus) .381 .388 .471
Unoptimized (Both) .409 .419 .502
LVDT (KG only) .255 .264 .293
LVDT (Corpus) .267 .272 .315
LVDT (Both) .419 .436 .541
W
Q
T
Unoptimized (KG) .329 .343 .394
Unoptimized (Corpus) .188 .228 .291
Unoptimized (Both) .377 .401 .474
LVDT (KG only) .257 .281 .345
LVDT (Corpus only) .170 .210 .280
LVDT (Both) .295 .323 .406
Figure 7: Synergy between KB and corpus.
5.7 Collective vs. greedy segmentation
To judge the quality of interpretations, we asked
paid volunteers to annotate queries with an appro-
priate relation and type, and compared them with
the interpretations associated with top-ranked en-
tities. Results in Figure 8 indicate that in spite
of noisy relation and type language models, our
formulations produce high quality interpretations
through collective inference.
Figure 9 demonstrates the benefit of collective
inference over greedy segmentation followed by
1112
Formulation Type Relation Type/Rel
Unoptimized (top 1) 23 49 60
Unoptimized (top 5) 29 57 68
LVDT (top 1) 25 52 61
LVDT (top 5) 33 61 69
Figure 8: Fraction of queries (%) with correct in-
terpretations of t
2
, r, and t
2
or r, on TREC-INEX.
evaluation. Collective inference boosts absolute
MAP by as much as 0.2.
Dataset Formulation map mrr n@10
Unoptimized (greedy) .343 .347 .432
TREC Unoptimized .409 .419 .502
-INEX LVDT (greedy) 205 .214 .259
LVDT .419 .436 .541
Unoptimized (greedy) .246 .271 .335
Unoptimized .377 .401 .474
WQT LVDT (greedy) .212 .246 .317
LVDT .295 .323 .406
Figure 9: Collective vs. greedy segmentation
5.8 Discussion
Closer scrutiny revealed that collective infer-
ence often overcame errors in earlier stages
to produce a correct ranking over answer en-
tities. E.g., for the query automobile com-
pany makes spider the entity disambiguation
stage fails to identify the car Alfa Romeo Spi-
der (/m/08ys39). However, the interpretation
stage recovers from the error and segments the
query with Automobile (/m/0k4j) as the query
entity e
1
, /organization/organization
and /business/industry/companies as
target type t
2
and relation r respectively (from the
relation/type hint ?company?), and spider as se-
Figure 10: Comparison of various approaches for
NDCG at rank 1 to 10, TREC-INEX dataset
Figure 11: Comparison of various approaches for
NDCG at rank 1 to 10, WQT dataset
lector to arrive at the correct answer Alfa Romeo
(/m/09c50). The corpus features also play a cru-
cial role for queries which may not be accurately
represented with an appropriate logical formula.
For the query meg ryan bookstore movie, the
textual patterns for the relation ActedIn in con-
junction with the selector word ?bookstore? cor-
rectly identifies the answer entity You?ve Got Mail
(/m/014zwb).
We also analyzed samples of queries where
our system did not perform particularly well.
We observed that one of the recurring themes
of these queries was that their answer enti-
ties had very little corpus support, and the
type/relation hint mapped to too many or no
candidate type/relations. For example, in the
query south africa political system, the rel-
evant type/relation hint ?political system? could
not be mapped to /government/form_of_
government and /location/country/
form_of_government respectively.
6 Conclusion and future work
We presented a technique to partition telegraphic
entity-seeking queries into functional segments
and to rank answer entities accordingly. While
our results are favorable compared to strong prior
art, further improvements may result from relax-
ing our model to recognize multiple e
1
s and rs. It
may also help to deploy more sophisticated para-
phrasing models (Berant and Liang, 2014) or word
embeddings (Yih et al., 2014) for relation hints.
It would also be interesting to supplement entity-
linked corpora and curated KGs with extracted
triples (Fader et al., 2014). Another possibility is
to apply the ideas presented here to well-formed
questions.
1113
References
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In ACL Conference.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Empirical Methods in
Natural Language Processing (EMNLP).
Charles Bergeron, Jed Zaretzki, Curt Breneman, and
Kristin P. Bennett. 2008. Multiple instance ranking.
In ICML, pages 48?55. ACM.
Paolo Boldi and Sebastiano Vigna. 2005. MG4J at
TREC 2005. In Ellen M. Voorhees and Lori P. Buck-
land, editors, TREC, number SP 500-266 in Special
Publications. NIST.
Andrei Z. Broder, David Carmel, Michael Herscovici,
Aya Soffer, and Jason Zien. 2003. Efficient query
evaluation using a two-level retrieval process. In
CIKM, pages 426?434. ACM.
Tao Cheng and Kevin Chen-Chuan Chang. 2010. Be-
yond pages: supporting efficient, scalable entity
search with dual-inversion index. In EDBT. ACM.
ClueWeb09. 2009. http://www.
lemurproject.org/clueweb09.php/.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In SIGKDD Confer-
ence.
Paolo Ferragina and Ugo Scaiella. 2010. TAGME:
on-the-fly annotation of short text fragments (by
wikipedia entities). CoRR/arXiv, abs/1006.3498.
http://arxiv.org/abs/1006.3498.
Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. FACC1: Free-
base annotation of ClueWeb corpora. http://
lemurproject.org/clueweb12/, June. Ver-
sion 1 (Release date 2013-06-26, Format version 1,
Correction level 0).
Sean Gallagher. 2012. How Google and Microsoft
taught search to ?understand? the Web. ArsTechnica
article. http://goo.gl/NWs0zT.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In SIGIR Con-
ference, pages 267?274. ACM.
Johan Hall, Jens Nilsson, and Joakim Nivre. 2014.
Maltparser. http://www.maltparser.org/.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In SIGKDD Conference,
pages 133?142. ACM.
Gjergji Kasneci, Fabian M. Suchanek, Georgiana Ifrim,
Maya Ramanath, and Gerhard Weikum. 2008.
NAGA: Searching and ranking knowledge. In
ICDE. IEEE.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling seman-
tic parsers with on-the-fly ontology matching. In
EMNLP Conference, pages 1545?1556.
Xiaonan Li, Chengkai Li, and Cong Yu. 2010. Enti-
tyEngine: Answering entity-relationship queries us-
ing shallow semantics. In CIKM, October. (demo).
Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, and
Kuansan Wang. 2011. Unsupervised query segmen-
tation using clickthrough for information retrieval.
In SIGIR Conference, pages 285?294. ACM.
Percy Liang. 2013. Lambda dependency-
based compositional semantics. Technical Report
arXiv:1309.4408, Stanford University. http://
arxiv.org/abs/1309.4408.
Thomas Lin, Patrick Pantel, Michael Gamon, Anitha
Kannan, and Ariel Fuxman. 2012. Active objects:
Actions for entity-centric search. In WWW Confer-
ence, pages 589?598. ACM.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. PATTY: A taxonomy of relational
patterns with semantic types. In EMNLP Confer-
ence, EMNLP-CoNLL ?12, pages 1135?1145. ACL.
Patrick Pantel, Thomas Lin, and Michael Gamon.
2012. Mining entity types from query logs via user
intent modeling. In ACL Conference, pages 563?
571, Jeju Island, Korea, July.
Fernando Pereira. 2013. Meaning in the
wild. Invited talk at EMNLP Conference.
http://hum.csse.unimelb.edu.au/
emnlp2013/invited-talks.html.
Jeffrey Pound, Alexander K. Hudek, Ihab F. Ilyas, and
Grant Weddell. 2012. Interpreting keyword queries
over Web knowledge bases. In CIKM.
Nikos Sarkas, Stelios Paparizos, and Panayiotis
Tsaparas. 2010. Structured annotations of Web
queries. In SIGMOD Conference.
Uma Sawant and Soumen Chakrabarti. 2013. Learn-
ing joint query interpretation and response ranking.
In WWW Conference, Brazil.
Fei Wu and Daniel S Weld. 2007. Automatically se-
mantifying Wikipedia. In CIKM, pages 41?50.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
Web of data. In EMNLP Conference, pages 379?
390, Jeju Island, Korea, July.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with Freebase. In ACL Conference. ACL.
Xuchen Yao, Jonathan Berant, and Benjamin Van
Durme. 2014. Freebase QA: Information extrac-
tion or semantic parsing? In ACL 2014 Workshop
on Semantic Parsing (SP14).
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In ACL Conference. ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
ICML, pages 1169?1176. ACM.
A. L. Yuille and Anand Rangarajan. 2006. The
concave-convex procedure. Neural Computation,
15(4):915?936.
ChengXiang Zhai. 2008. Statistical language models
for information retrieval: A critical review. Founda-
tions and Trends in Information Retrieval, 2(3):137?
213, March.
1114
