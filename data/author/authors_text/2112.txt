Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 402?405,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNN-WePS: Web Person Search using co-Present Names 
and Lexical Chains 
Jeremy Ellman 
Northumbria University  
Pandon Building  
Newcastle upon Tyne 
UK 
Jeremy. Ellman @unn.ac.uk 
Gary Emery 
Northumbria University  
Pandon Building  
Newcastle upon Tyne 
UK 
Gary.Emery@unn.ac.uk 
 
 
Abstract 
We describe a system, UNN-WePS for 
identifying individuals from web pages us-
ing data from Semeval Task 13. Our sys-
tem is based on using co-presence of per-
son names to form seed clusters. These are 
then extended with pages that are deemed 
conceptually similar based on a lexical 
chaining analysis computed using Roget?s 
thesaurus. Finally, a single link hierarchical 
agglomerative clustering algorithm merges 
the enhanced clusters for individual entity 
recognition. UNN-WePS achieved an aver-
age purity of 0.6, and inverse purity of 0.73. 
1 Introduction 
Guha and Garg (2004) report that approximately 
4% of internet searches are to locate named indi-
viduals. Yet, many people share the same name 
with for example 157630 individuals in the UK 
sharing the most common name ?David Jones? 
(UK statistics cited by Ash 2006). Consequently 
identifying web pages on specific individuals is a 
significant problem that will grow as everyone ac-
quires a web presence.  
There are several proposed approaches to identi-
fying which individuals correspond to which web 
pages. For example, Bollegala et al (2007) pro-
pose augmenting queries in the style of relevance 
feedback (Salton and Buckley 1990), Kalashnikov 
(2007) treat Web Person Search (WePS) as a dis-
ambiguation problem whose objective is to distin-
guish individuals, whilst Wan et al (2005) see 
WePS as a clustering problem. 
WePS has both similarities and differences to 
word sense disambiguation (WSD). Both seek to 
classify instances of usage, but in WSD the sense 
inventory is fixed. WSD then is more amenable to 
a classification solution where a system can be ef-
fectively trained using learning algorithms. In 
WePS we do not know from the outset how many 
individuals our pages correspond to. Consequently 
we took the view that WePS is better seen as a 
clustering rather than a classification problem. 
1.1 Ambiguity 
Ambiguity is a common feature of WePS and 
WSD.  There are multiple types of ambiguity in the 
relation between person names and entities that 
confound overly simple approaches. Firstly, note 
that some first names are also last names (Les Paul, 
Michael Howard), and that some last names also 
occur as given names (Woodrow Wilson Guthrie, 
Martin Luther King). Consequently, an overly 
simple name parser will easily be confused. Sec-
ondly many last names are also place names (Jack 
London, William Manchester). Thus, if a last name 
is not found in the names database, but is found in 
the gazetteer, a name can be confused with a loca-
tion. Finally, we come to toponym ambiguity, 
where the name of a place may correspond to sev-
eral locations. (For example, there are thirteen 
places called Manchester, multiple Londons, 
Washingtons etc.) Resolving toponyms is a re-
search problem itself (Leidner, 2004). 
1.2 Statistics 
Statistics are a further relation between WePS and 
WSD. We expect Zipf?s law (e.g. Adamic and 
Huberman 2002) to apply to the relation between 
402
web pages and individuals, meaning that relative 
frequency and rank form a harmonic series. In 
other words some people will be associated with 
many pages and increasingly more will be linked 
to fewer. This has a strong link to disambiguation, 
where an inaccurate algorithm may give inferior 
performance to the strategy of always selecting the 
most frequent sense. 
Now if we consider the types of data that distin-
guish individuals, we might find colleagues, 
friends, and family mentioned in web pages, in 
addition to locations, dates, and topics of interest. 
Of these, names are particularly useful, and we 
define co-present names as names found in a web 
page in addition to the name for which we are 
searching.  
Names are statistically useful, even though 
many people share the same name. For example 
there are 7640 individuals in the UK (for example) 
that share the most popular female name ?Margaret 
Smith?. Given the population of the UK is ap-
proximately 60 million, the probability of even the 
most common female name in the UK occurring 
randomly is 1.27+10-4 (of course not all the indi-
viduals have web pages).  
Now, Semeval WePS pages (Artiles 2007) have 
been retrieved in response to a search for one 
name. Often such web pages will contain addi-
tional names. The probability that a web page will 
contain two names corresponding to two different 
individuals is quite low (~ca 7x10-8). Conse-
quently co-present names form indicators of an 
individual?s identity. These give accurate seed 
points, which are critical to the success of many 
clustering algorithms such as k-means (Jain et al 
1999) 
1.3 Lexical Chain Text Similarity 
Not all WePS pages contain multiple names, or 
even content in any form. Consequently we need to 
distinguish between pages that are similar in mean-
ing to a page already in a seed cluster, those that 
refer to separate entities, and those to be discarded 
This was done by comparing the conceptual 
similarity of the WePS pages using Roget?s thesau-
rus as the conceptual inventory. The approach was 
described in Ellman (2000), where lexical chains 
are identified from each document using Roget's 
thesaurus. These chains are then unrolled to yield 
an attribute value vector of concepts where the 
values are given by repetition, type of thesaural 
relation found, and textual cohesion. Thus, we are 
not simply indexing by thesaural categories. 
Vectors corresponding to different documents 
can be compared to give a measure of conceptual 
similarity. Roget?s thesaurus typically contains one 
thousand sense entries divided by part of speech 
usage, giving a total of 6400 entries. Such vectors 
may be compared using many algorithms, although 
a nearest neighbor algorithm was implemented in 
Ellman (2000).  
1.4 One Sense Per Discourse 
UNN-WePS was based on a deliberate strategy 
that the success of an active disambiguation 
method needed to exceed its overall error rate in 
order to improve baseline performance. As such, 
simple methods that improved overall success 
modestly were preferred to complex ones that did 
not. Consequently, to reduce the search space, we 
used the ?one sense per discourse? heuristic (Gale 
et al 1992). This assumes that one web page 
would not refer to two different individuals that 
share a name. 
2 System Description 
UNN-WePS was made up of three components, 
comprising modules to: 
1. Create seed clusters that associated files 
with person names other than those being 
searched for. 
2. Match similarity of unallocated documents 
to micro clusters using lexical chains de-
rived from Roget?s thesaurus. 
3. Identify entities using single link agglom-
erative clustering algorithm. 
In detail, a part of speech tagger (Coburn et al 
2007) was used to identify sequences of proper 
nouns. Person names were identified from these 
sequences using the following simple names 
?grammar? coupled with data from the US Census 
(1990). 
 
Name = [Title*][Initials | 1st name]+[2nd name]+ 
Figure 1: Regular Expression Name Syntax  
We also used a gazetteer to forms seed clusters 
using data from the World Gazetteer (2007). This 
did not form part of the submitted system. 
 
403
In the second step, conceptual similarity was de-
termined using the method and tool described in 
Ellman (2000). Documents not allocated to seed 
clusters, were compared for conceptual similarity 
to all other documents. If similar to a document in 
a seed cluster, the unallocated document was in-
serted into the seed cluster. If neither document nor 
one to which it was similar too were in a seed clus-
ter, they were formed into a new seed cluster. Fi-
nally if document has 'meaningful' content, but is 
not conceptually similar to any other it is stored in 
a singleton seed cluster otherwise, it is discarded.  
In the final step, seed clusters were sorted by 
size and merged using a single link hierarchical 
agglomerative clustering algorithm to identify enti-
ties (Jain et al 1999). The use of a single link 
means that a document can only be associated with 
one entity, which conforms to the ?one sense per 
discourse? heuristic. 
Further details of the UNN-WePS algorithm are 
given in figure 2 below. 
 
3 Results 
UNN-WePS achieved an average purity of 0.6, and 
inverse purity of 0.73 in Semeval Task 13, achiev-
ing seventh position out of sixteen competing sys-
tems (Artiles et al 2007). However there was con-
siderable variance in UNN-WePS results as shown 
in graph 1 below. 
 
Purity Vs Name
0
0.2
0.4
0.6
0.8
1
M
ar
th
a_
Ed
w
ar
ds
Ja
m
es
_M
or
eh
ea
d
Vi
ol
et
_H
ow
ar
d
Ja
m
es
_C
ur
ra
n
Ka
re
n_
Pe
te
rs
on
Th
om
as
_K
irk
M
ar
k_
Jo
hn
so
n
St
ep
he
n_
Cl
ar
k
Al
vi
n_
Co
op
er
Ha
rry
_H
ug
he
s
Ar
th
ur
_M
or
ga
n
Ju
de
_B
ro
wn
Je
rr
y_
Ho
bb
s
Ch
ris
_B
ro
ck
et
t
Sh
ar
on
_G
ol
dw
at
er
P(submitted) P(no-chain) P (places-chainer)
 FOREACH Person_Name 
1. TAG raw html Files with Part of Speech.  
2. IDENTIFY Generic Document Profiles using 
lexical chains in html Files. 
3. CONSTRUCT table T to associate person 
names  with Files. 
a. FOREACH File in Person_Name 
i. IDENTIFY Names in File 
ii. FOREACH Name in Names 
IF Name ? Person_Name  
STORE Name, File in T 
4. CREATE Seed clusters by inverting T to 
give files that are associated by co-
present names 
5. MATCH Similarity of unallocated docu-
ments to seed clusters  
a. FOREACH unallocated document D 
IF similar to a document in cluster C 
  INSERT D into C 
ELSE IF similar to a non-clustered   
document D?                                        
CREATE  D, D? as new cluster C?  
ELSE IF CONTAINS D > 200 words 
      CREATE D as new cluster C?? 
      ELSE DISCARD D 
6. IDENTIFY entities using single link ag-
glomerative clustering algorithm over 
seed clusters. 
Graph 1: UNN-WePS purity performance 
 
Graph 1 shows the purity scores for UNN-WePS 
on the Semeval 13 test data on three conditions: (1) 
as submitted (solid line), (2) using the gazetteer 
(dashed line), and (3) without the lexical chain 
based similarity matching (dotted line).  
Note although the purity is lower when similar-
ity matching is included the number of discarded 
documents is approximately halved. 
An examination of the data suggests that where 
performance was especially poor it was due to ge-
nealogical data. Firstly this contains multiple indi-
viduals sharing the same name violating the ?one 
sense per discourse? heuristic. Secondly genealogi-
cal data includes birth and death information which 
was outside the scope of UNN-WePS. Further-
more, the large number of names confounds the 
statistical utility of co-present names. 
4 Conclusion and Future Work 
We have described a system, UNN-WePS that dis-
ambiguates individuals in web pages as required 
for Semeval task 13 (Artiles et al 2007).  
UNN-WePS was composed of three modules. 
The first formed seed clusters based on names pre-
sent in web pages other than the individual for 
whom we are searching. The second used a lexical 
Figure 2: UNN-WePS Algorithm 
404
chain based similarity measure to associates re-
maining files with clusters, whilst the third joined 
the clusters to identify identities using a single link 
hierarchical algorithm. 
UNN-WePS performed surprisingly well con-
sidering the simplicity of its basic seeding algo-
rithm. The use however of the ?one sense per dis-
course? heuristic was flawed. Names do re-occur 
across generations in families.  
Genealogy is a popular Internet pastime, and 
web pages containing genealogy data frequently 
refer to multiple individuals that share a name at 
different time periods. As UNN-WePS did not ac-
count for time, this could not be detected. Further-
more, the large number of names in on-line genea-
logical data does lead to spurious associations. 
As WePS was time limited, several extensions 
and refinements were envisaged, but not executed. 
Firstly, as described, the world gazetteer (2007) 
did not lead to performance improvements. We 
speculate therefore the disambiguation effect from 
using place names was exceeded by the ambiguity 
introduced by using them blindly. We note espe-
cially the inference between unidentified names (or 
street names, or building names) being interpreted 
as place data. 
A further system deficiency was the lack of rec-
ognition of date data. This is essential to differenti-
ate between identically named individuals in ge-
nealogical data. 
Finally, we are currently experimenting with dif-
ferent clustering algorithms using the CLUTO 
toolkit (Karypis 2002) to improve on UNN-WePS 
baseline performance. 
References 
Adamic L.A. and Huberman B.A., 2002 Zipf's law and 
the Internet, Glottometrics 3, 2002, 143-150 
Artiles, J., Gonzalo, J. and Sekine, S. (2007).  
The SemEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task. In Pro-
ceedings of Semeval 2007, Association for Computa-
tional Linguistics. 
Ash, Russell 2006 The top 10 of Everything Hamlyn, 
Palazzo Bath UK 
Bollegala, Danushka, Matsuo  Yutaka Ishizuka Mitsuru 
Disambiguating Personal Names on the Web using 
Automatically Extracted Key Phrases Proc. ECAI 
2006, pp.553-557, Trento, Italy (2006.8) 
Coburn A, Ceglowski M, and Cuadrado J 2007  
Lingua::EN::Tagger, a Perl part-of-speech tagger 
for English text. http://search.cpan.org/~acoburn/ 
Lingua-EN-Tagger-0.13/ 
Ellman, Jeremy. 2000 Using Roget?s Thesaurus to De-
termine the Similarity of Texts. PhD thesis, Univer-
sity of Sunderland [Available at 
http://citeseer.ist.psu.edu/ellman00using.html ] 
Gale, W., Church, K., and Yarowsky, D. (1992). One 
sense per discourse. In Proceedings of the Fourth 
DARPA Speech and Natural Language Workshop, 
pages 233--237. 
Guha R. & Garg A. Disambiguating People in Search. 
Stanford University, 2004 
Jain, A. K., Murty, M. N., and Flynn, P. J. 1999. Data 
clustering: a review. ACM Comput. Surv. 31, 3 (Sep. 
1999), 264-323  
Karypis G.  2002. CLUTO: A clustering toolkit. Techni-
cal Report 02-017, University of Minnesota. Avail-
able at: http://wwwusers.cs.umn.edu/~karypis/cluto/. 
Leidner, Jochen L. (2004). Toponym Resolution in Text: 
"Which Sheffield is it?" in proc. 27th Annual Interna-
tional ACM SIGIR Conference (SIGIR 2004), Shef-
field, UK. 
Navigli, Roberto 2006. Meaningful clustering of senses 
helps boost word sense disambiguation performance. 
In Proc. ACL (Sydney, Australia, July 17 - 18, 2006).  
Salton and Buckley 1990 Improving Retrieval Perform-
ance by Relevance Feedback JASIS 41(4) pp288-297  
US Census 1990 http://www.census.gov/ 
genealogy/names/names_files.html accessed 17th 
April 2007 
Wan, X., Gao, J., Li, M., and Ding, B. 2005. Person 
resolution in person search results: WebHawk. in 
Proc. CIKM '05. ACM Press, New York, NY 
World Gazetteer 2007 http://world-gazetteer.com/ ac-
cessed 17th April 2007 
 
405
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 375?379, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
TJP: Using Twitter to Analyze the Polarity of Contexts 
 
 
Tawunrat Chalothorn Jeremy Ellman 
University of Northumbria at Newcastle University of Northumbria at Newcastle 
Pandon Building, Camden Street Pandon Building, Camden Street 
Newcastle Upon Tyne, NE2 1XE, UK Newcastle Upon Tyne, NE2 1XE, UK 
Tawunrat.chalothorn@unn.ac.uk Jeremy.ellman@unn.ac.uk 
 
 
 
 
 
 
Abstract 
This paper presents our system, TJP, which 
participated in SemEval 2013 Task 2 part A: 
Contextual Polarity Disambiguation. The goal 
of this task is to predict whether marked con-
texts are positive, neutral or negative. Howev-
er, only the scores of positive and negative 
class will be used to calculate the evaluation 
result using F-score. We chose to work as 
?constrained?, which used only the provided 
training and development data without addi-
tional sentiment annotated resources. Our ap-
proach considered unigram, bigram and 
trigram using Na?ve Bayes training model 
with the objective of establishing a simple-
approach baseline. Our system achieved F-
score 81.23% and F-score 78.16% in the re-
sults for SMS messages and Tweets respec-
tively. 
1 Introduction 
Natural language processing (NLP) is a research 
area comprising various tasks; one of which is sen-
timent analysis. The main goal of sentiment analy-
sis is to identify the polarity of natural language 
text (Shaikh et al, 2007). Sentiment analysis can 
be referred to as opinion mining, as study peoples? 
opinions, appraisals and emotions towards entities 
and events and their attributes (Pang and Lee, 
2008). Sentiment analysis has become a popular 
research area in NLP with the purpose of identify-
ing opinions or attitudes in terms of polarity.  
This paper presents TJP, a system submitted to 
SemEval 2013 for Task 2 part A: Contextual Polar-
ity Disambiguation (Wilson et al, 2013). TJP was 
focused on the ?constrained? task, which used only 
training and development data provided. This 
avoided both resource implications and potential 
advantages implied by the use of additional data 
containing sentiment annotations. The objective 
was to explore the relative success of a simple ap-
proach that could be implemented easily with 
open-source software.  
The TJP system was implemented using the Py-
thon Natural Language Toolkit (NLTK, Bird et al, 
2009). We considered several basic approaches. 
These used a preprocessing phase to expand con-
tractions, eliminate stopwords, and identify emoti-
cons. The next phase used supervised machine 
learning and n-gram features. Although we had 
two approaches that both used n-gram features, we 
were limited to submitting just one result. Conse-
quently, we chose to submit a unigram based ap-
proach followed by naive Bayes since this 
performed better on the data.  
The remainder of this paper is structured as fol-
lows: section 2 provides some discussion on the 
related work. The methodology of corpus collec-
tion and data classification are provided in section 
3. Section 4 outlines details of the experiment and 
results, followed by the conclusion and ideas for 
future work in section 5. 
2 Related Work  
The micro-blogging tool Twitter is well-known 
and increasingly popular. Twitter allows its users 
to post messages, or ?Tweets? of up to 140 charac-
ters each time, which are available for immediate 
375
download over the Internet. Tweets are extremely 
interesting to marketing since their rapid public 
interaction can either indicate customer success or 
presage public relations disasters far more quickly 
than web pages or traditional media. Consequently, 
the content of tweets and identifying their senti-
ment polarity as positive or negative is a current 
active research topic. 
Emoticons are features of both SMS texts, and 
tweets. Emoticons such as :) to represent a smile, 
allow emotions to augment the limited text in SMS 
messages using few characters. Read (2005) used 
emoticons from a training set that was downloaded 
from Usenet newsgroups as annotations (positive 
and negative). Using the machine learning tech-
niques of Na?ve Bayes and Support Vector Ma-
chines Read (2005) achieved up to 70 % accuracy 
in determining text polarity from the emoticons 
used. 
Go et al (2009) used distant supervision to clas-
sify sentiment of Twitter, as similar as in (Read, 
2005). Emoticons have been used as noisy labels in 
training data to perform distant supervised learning 
(positive and negative). Three classifiers were 
used: Na?ve Bayes, Maximum Entropy and Sup-
port Vector Machine, and they were able to obtain 
more than 80% accuracy on their testing data.  
Aisopos et al (2011) divided tweets in to three 
groups using emoticons for classification. If tweets 
contain positive emoticons, they will be classified 
as positive and vice versa. Tweets without posi-
tive/negative emoticons will be classified as neu-
tral. However, tweets that contain both positive 
and negative emoticons are ignored in their study. 
Their task focused on analyzing the contents of 
social media by using n-gram graphs, and the re-
sults showed that n-gram yielded high accuracy 
when tested with C4.5, but low accuracy with Na-
?ve Bayes Multinomial (NBM). 
3 Methodology  
3.1 Corpus 
The training data set for SemEval was built using 
Twitter messages training and development data.  
There are more than 7000 pieces of context. Users 
usually use emoticons in their tweets; therefore, 
emoticons have been manually collected and la-
beled as positive and negative to provide some 
context (Table 1), which is the same idea as in Ai-
sopos et al (2011).  
 
Negative emoticons :( :-( :d :< D: :\ /: etc. 
Positive emoticons 
:) ;) :-) ;-) :P ;P (: (; :D 
;D etc. 
 
Table 1: Emoticon labels as negative and positive 
 
Furthermore, there are often features that have 
been used in tweets, such as hashtags, URL links, 
etc. To extract those features, the following pro-
cesses have been applied to the data. 
 
1. Retweet (RT), twitter username (@panda), 
URL links (e.g. y2u.be/fiKKzdLQvFo), 
and special punctuation were removed. 
2. Hashtags have been replaced by the fol-
lowing word (e.g. # love was replaced by 
love, # exciting was replaced by exciting). 
3. English contraction of ?not? was converted 
to full form (e.g. don?t -> do not). 
4. Repeated letters have been reduced and re-
placed by 2 of the same character (e.g. 
happpppppy will be replaced by happy, 
coollllll will be replaced by cooll). 
3.2 Classifier 
Our system used the NLTK Na?ve Bayes classifier 
module. This is a classification based on Bayes?s 
rule and also known as the state-of-art of the Bayes 
rules (Cufoglu et al, 2008). The Na?ve Bayes 
model follows the assumption that attributes within 
the same case are independent given the class label 
(Hope and Korb, 2004).  
Tang et al (2009) considered that Na?ve Bayes 
assigns a context   (represented by a vector   
 ) to 
the class    that maximizes        
   by applying 
Bayes?s rule, as in (1). 
 
 (  |  
 )   
         
     
    
  
 (1) 
 
 
 
where     
   is a randomly selected context  . The 
representation of vector is   
 .      is the random 
select context that is assigned to class  . 
To classify the term     
     , features in   
  
were assumed as    from         as in (2). 
376
  (  |  
 )   
     ?         
 
   
    
  
 (2) 
 
There are many different approaches to lan-
guage analysis using syntax, semantics, and se-
mantic resources such as WordNet. That may be 
exploited using the NLTK (Bird et al 2009). How-
ever, for simplicity we opted here for the n-gram 
approach where texts are decomposed into term 
sequences. A set of single sequences is a unigram. 
The set of two word sequences (with overlapping) 
are bigrams, whilst the set of overlapping three 
term sequences are trigrams. The relative ad-
vantage of the bi-and trigram approaches are that 
coordinates terms effectively disambiguate senses 
and focus content retrieval and recognition. 
N-grams have been used many times in contents 
classification. For example, Pang et al (2002) used 
unigram and bigram to classify movie reviews. The 
results showed that unigram gave better results 
than bigram. Conversely, Dave et al (2003) re-
ported gaining better results from trigrams rather 
than bigram in classifying product reviews. Conse-
quently, we chose to evaluate unigrams, bigrams 
and trigrams to see which will give the best results 
 
 
 
 
Figure 1: Comparison of Twitter messages from two approaches 
 
 
 
 
Figure 2: Comparison of SMS messages from two approaches 
Unigram Bigram Trigram
Pos 1 84.46 82.09 80.8
Neg 1 71.08 59.53 52.91
Pos 2 84.62 83.31 83.25
Neg 2 71.70 65.00 64.34
50
55
60
65
70
75
80
85
90
F
-s
co
re
 (
%
) Pos 1
Neg 1
Pos 2
Neg 2
Unigram Bigram Trigram
Pos 1 76.23 73.89 72.02
Neg 1 82.61 76.04 71.19
Pos 2 77.81 75.69 75.42
Neg 2 84.66 79.94 79.37
50
55
60
65
70
75
80
85
90
F
-s
co
re
 (
%
) 
Pos 1
Neg 1
Pos 2
Neg 2
377
in the polarity classification. Our results are de-
scribed in the next section. 
4 Experiment and Results  
In this experiment, we used the distributed data 
from Twitter messages and the F-measure for sys-
tem evaluation. As at first approach, the corpora 
were trained directly in the system, while stop-
words (e.g. a, an, the) were removed before train-
ing using the python NLTK for the second 
approach. The approaches are demonstrated on a 
sample context in Table 2 and 3. 
After comparing both approaches (Figure 1), we 
were able to obtain an F-score 84.62% of positive 
and 71.70% of negative after removing stopwords. 
Then, the average F-score is 78.16%, which was 
increased from the first approach by 0.50%. The 
results from both approaches showed that, unigram 
achieved higher scores than either bigrams or tri-
grams.  
Moreover, these experiments have been tested 
with a set of SMS messages to assess how well our 
system trained on Twitter data can be generalized 
to other types of message data. The second ap-
proach still achieved the better scores (Figure 2), 
where we were able to obtain an F-score of 77.81% 
of positive and 84.66% of negative; thus, the aver-
age F-score is 81.23%. 
The results of unigram from the second ap-
proach submitted to SemEval 2013 can be found in 
Figure 3. After comparing them using the average 
F-score from positive and negative class, the re-
sults showed that our system works better for SMS 
messaging than for Twitter. 
 
gonna miss some of my classes. 
Unigram Bigram Trigram 
gonna 
miss 
some 
of 
my 
classes 
gonna miss 
miss some 
some of 
of my 
my classes 
gonna miss some 
miss some of 
some of my 
of my classes 
 
Table 2: Example of context from first approach 
 
 
 
 
 
 
gonna miss (some of) my classes. 
Unigram Bigram Trigram 
gonna 
miss 
my 
classes 
gonna miss 
miss my 
my classes 
gonna miss my 
miss my classes 
 
Table 3: Example of context from second approach. 
Note ?some? and ?of? are listed in NLTK stopwords. 
 
 
 
 
Figure 3: Results of unigram of Twitter and SMS in the 
second approach 
5 Conclusion and Future Work 
A system, TJP, has been described that participated 
in SemEval 2013 Task 2 part A: Contextual Polari-
ty Disambiguation (Wilson et al, 2013). The sys-
tem used the Python NLTK (Bird et al2009) Naive 
Bayes classifier trained on Twitter data. Further-
more, emoticons were collected and labeled as pos-
itive and negative in order to classify contexts with 
emoticons. After analyzing the Twitter message 
and SMS messages, we were able to obtain an av-
erage F-score of 78.16% and 81.23% respectively 
during the SemEval 2013 task. The reason that, our 
system achieved better scores with SMS message 
then Twitter message might be due to our use of 
Twitter messages as training data. However this is 
still to be verified experimentally. 
The experimental performance on the tasks 
demonstrates the advantages of simple approaches. 
This provides a baseline performance set to which 
more sophisticated or resource intensive tech-
niques may be compared. 
 
Pos Neg Average
Twitter 84.62 71.70 78.16
SMS 77.81 84.66 81.23
65
70
75
80
85
90
F
-s
co
re
 (
%
) 
378
For future work, we intend to trace back to the 
root words and work with the suffix and prefix that 
imply negative semantics, such as ?dis-?, ?un-?, ?-
ness? and ?-less?. Moreover, we would like to col-
lect more shorthand texts than that used commonly 
in microblogs, such as gr8 (great), btw (by the 
way), pov (point of view), gd (good) and ne1 (any-
one). We believe these could help to improve our 
system and achieve better accuracy when classify-
ing the sentiment of context from microblogs. 
References  
Alec Go, Richa Bhayani and Lei Huang. 2009. Twitter 
sentiment classification using distant supervision. 
CS224N Project Report, Stanford, 1-12. 
Ayse Cufoglu, Mahi Lohi and Kambiz Madani. 2008. 
Classification accuracy performance of Naive Bayes-
ian (NB), Bayesian Networks (BN), Lazy Learning of 
Bayesian Rules (LBR) and Instance-Based Learner 
(IB1) - comparative study. Paper presented at the 
Computer Engineering & Systems, 2008. ICCES 
2008. International Conference on. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up?: sentiment classification using 
machine learning techniques. Paper presented at the 
Proceedings of the ACL-02 conference on Empirical 
methods in natural language processing - Volume 10. 
Fotis Aisopos, George Papadakis and Theodora 
Varvarigou. 2011. Sentiment analysis of social media 
content using N-Gram graphs. Paper presented at the 
Proceedings of the 3rd ACM SIGMM international 
workshop on Social media, Scottsdale, Arizona, 
USA. 
Huifeng Tang, Songbo Tan and Xueqi Cheng. 2009. A 
survey on sentiment detection of reviews. Expert Sys-
tems with Applications, 36(7), 10760-10773. 
Jonathon. Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. Paper presented at the 
Proceedings of the ACL Student Research Work-
shop, Ann Arbor, Michigan. 
Kushal Dave, Steve Lawrence and David M. Pennock. 
2003. Mining the peanut gallery: opinion extraction 
and semantic classification of product reviews. Paper 
presented at the Proceedings of the 12th international 
conference on World Wide Web, Budapest, Hungary. 
Lucas R. Hope and Kevin B. Korb. 2004. A bayesian 
metric for evaluating machine learning algorithms. 
Paper presented at the Proceedings of the 17th Aus-
tralian joint conference on Advances in Artificial In-
telligence, Cairns, Australia.  
Mostafa Al Shaikh, Helmut Prendinger and Ishizuka 
Mitsuru. 2007. Assessing Sentiment of Text by Se-
mantic Dependency and Contextual Valence Analy-
sis. Paper presented at the Proceedings of the 2nd in-
ternational conference on Affective Computing and 
Intelligent Interaction, Lisbon, Portugal. 
Pang Bo and Lillian Lee. 2008. Opinion Mining and 
Sentiment Analysis. Found. Trends Inf. Retr., 2(1-2), 
1-135. 
Steven Bird, Ewan Klein and Edward Loper. 2009. Nat-
ural language processing with Python: O'Reilly.  
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, 
Sara Rosenthal, Veselin Stoyanov and Alan Ritter. 
2013. SemEval-2013 Task 2: Sentiment Analysis in 
Twitter Proceedings of the 7th International Work-
shop on Semantic Evaluation: Association for Com-
putational Linguistics. 
 
 
379
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 657?662,
Dublin, Ireland, August 23-24, 2014.
TJP: Identifying the Polarity of Tweets from Context 
 
Tawunrat Chalothorn Jeremy Ellman 
Department of Computer Science and 
Digital Technologies  
University of Northumbria at Newcas-
tle, Pandon Building, Camden Street 
Newcastle Upon Tyne, NE2 1XE, UK 
Department of Computer Science and 
Digital Technologies  
University of Northumbria at Newcas-
tle, Pandon Building, Camden Street 
Newcastle Upon Tyne, NE2 1XE, UK 
tawunrat.chalothorn 
@northumbria.ac.uk 
jeremy.ellman 
@northumbria.ac.uk 
 
 
Abstract 
The TJP system is presented, which partici-
pated in SemEval 2014 Task 9, Part A: 
Contextual Polarity Disambiguation. Our 
system is ?constrained?, using only data 
provided by the organizers. The goal of this 
task is to identify whether marking contexts 
are positive, negative or neutral. Our system 
uses a support vector machine, with exten-
sive pre-processing and achieved an overall 
F-score of 81.96%. 
1 Introduction 
The aim of sentiment analysis is to identify 
whether the subject of a text is intended to be 
viewed positively of negatively by a reader. Such 
emotions are sometimes hidden in long sentences 
and are difficult to identify. Consequently senti-
ment analysis is an active research area in natural 
language processing. * 
Sentiment is currently conceived terms of po-
larity. This has numerous interesting applica-
tions. For example, Grabner et al. (2012) used  
sentiment analysis to classify customers? reviews 
of hotels by using a star rating to categorize the  
                                                          
*     This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details: 
http://creativecommons.org/licenses/by/4.0/ 
 
reviews as bad, neutral and good. Similarly, 
Tumasjan et al. (2010) tried to predict the out-
come of the German federal election through the 
analysis more than 100,000 tweets posted in the 
lead up. Sentiment analysis has also used to 
classify whether dreams are positive or nega-
tive (Nadeau et al. 2006). 
This paper presents the TJP system which 
was  submitted to SemEval 2014 Task 9, Part A: 
Contextual Polarity Disambiguation (Rosenthal 
et al., 2014). TJP focused on the ?Constrained? 
task.  
The ?Constrained? task only uses data provid-
ed by the organizers. That is, external resources 
such as sentiment inventories (e.g. Sentiwordnet 
(Esuli, and Sebastiani 2006) are excluded. The 
objective of the TJP system was to use the results 
for comparison with our previous experiment 
(Chalothorn and Ellman, 2013). More details of 
these can be found in section 5.  
The TJP system was implemented using a 
support vector machine (SVM, e.g. Joachims, 
1999) with the addition of extensive pre-
processing such as stopword removal, negation, 
slang, contraction,  and emoticon expansions. 
The remainder of this paper is constructed as 
follows: firstly, related work is discussed in sec-
tion 2; the methodology, the experiment and re-
sults are presented in sections 3 and 4, 
657
respectively. Finally a discussion and future 
work are given in section 5. 
2 Related Work  
Twitter is a popular social networking and mi-
croblogging site that allows users to post mes-
sages of up to 140 characters; known as 
?Tweets?. Tweets are extremely attractive to the 
marketing sector, since tweets may be searched 
in real-time. This means marketing can find cus-
tomer sentiment (both positive and negative) far 
more quickly than through the use of web pages 
or traditional media. Consequently analyzing the 
sentiment of tweets is currently active research 
task. 
The word 'emoticon' is a neologistic contrac-
tion of 'emotional icon'.  It refers specifically to 
the use of combinations of punctuation charac-
ters to indicate sentiment in a text. Well known 
emoticons include :) to represent a happy face, 
and :( a sad one. Emoticons allow writers to 
augment the impact of limited texts (such as in 
SMS messages or tweets) using few characters.  
Read (2005) used emoticons from a training 
set downloaded from Usenet newsgroups as an-
notations (positive and negative). Using the ma-
chine learning techniques of Na?ve Bayes and 
SVM, Read (2005) achieved up to 61.50 % and 
70.10%, accuracy respectively in determining 
text polarity from the emoticons used.  
Go et al. (2009) used distant supervision to   
classify sentiment of Twitter, similar to Read 
(2005). Emoticons were used as noisy labels in 
training data. This allowed the performance of  
supervised learning (positive and negative) at a 
distance. Three classifiers were used: Na?ve 
Bayes, Maximum   Entropy and SVM. These 
classifiers were able to   obtain more than 
81.30%, 80.50% and 82.20%, respectively accu-
racy on their unigram testing data . 
Aramaki et al. (2011) classified contexts on 
Twitter related to influenza using a SVM. The 
training data was annotated with the polarity la-
bel by humans, whether they are positive or neg-
ative. The contexts will be labelled as positive if 
the contexts mention the user or someone close 
to them has the flu, or if they mention a time 
when they caught the flu. The results demon-
strated that they obtained a 0.89 correction ratio 
for their testing data against a gold standard. 
Finally, a well known paper by Bollen and 
Mao (2011) identified a correlation between the 
movements of the Dow Jones stock market    
index, and prevailing sentiment as determined 
from twitter's live feed. This application has 
prompted considerable work such as Makrehchi 
et al (2013) that has attempted to create success-
ful trading strategies from sentiment analysis of 
tweets.  
These work both the wide ranging applica-
tions of analysing twitter data, and the             
importance of Sentiment Analysis. We now 
move on to look at our approach to SemEval 
2014 task 9. 
3 Methodology  
3.1 Corpus 
The training and development dataset of 
SemEval was built using Tweets from more than 
one thousand pieces of context. The contexts 
have various features often used in Tweets, such 
as emoticons, tags, usernames etc. These features 
were extracted from the datasets before training 
for the  supervised machine learning model. 
During initial pre-processing of the datasets, 
emoticons were labelled by matching with the 
emoticons that have been collect manually from 
the dataset. Those labelled were matched against 
a well-known collection of emoticons?. 
Subsequently, negative contractions? were 
expanded in place and converted to full form 
(e.g. don?t -> do not). Moreover, the features of 
                                                          
?http://en.wikipedia.org/wiki/List_of_emoticons 
?http://en.wikipedia.org/wiki/English_auxiliaries_and_contr
actions#Negative_contractions 
658
twitters were also removed or replaced by words 
such as twitter usernames, URLs and hashtags. 
A Twitter username is a unique name that 
shows in the user's profile and may be used for 
both authentication and identification.  This is 
shown by prefacing the username with an @ 
symbol. When a tweet is directed at an individual 
or particular entity this can be shown in the tweet 
by including @username. For example a tweet 
directed at ?tawunrat? would include the text  
@tawunrat.  Before URLs are posted in twitter 
they are shortened automatically to use the t.co 
domain whose modified URLs are at most 22 
characters. However, both features have been 
removed from the datasets. For the hashtags, 
they are used for represent keyword and topics in 
twitter by using # follow by words or phrase 
such as #newcastleuk.  This feature has been re-
placed with the following word after # symbol. 
For example, #newcastleuk was replaced by 
newcastleuk. 
Frequently repeated letters are used in tweets 
for emphasis. These were reduced and replaced 
using a simple regular expression by two of the 
same character. For example, happpppppy will 
be replaced with happy, and coollllll will be re-
placed by cooll. Next, special character such as 
[,],{,},?,and ! were also removed. Slang and con-
tracted words were converted to their full form. 
E.g. ?fyi? was converted to ?for your infor-
mation?. Finally, NLTK (Bird et al. 2009) stop-
words such as ?a?, ?the?, etc., were removed from 
the datasets. 
3.2 Classifier 
Our system uses the SVM classifier model 
(Hearst et al., 1998, Cristianini and Shawe-
Taylor, 2000), which is based on SVM-light (Jo-
achims, 1999). SVM is a binary linear classifica-
tion model with the learning algorithm for 
classification and regression analyzing the data 
and recognizing the pattern. 
Training SVMLight requires data to be for-
mulated into vectors of attribute value pairs pre-
ceded by a numeric value. For example, 
 
<target>  <feature>:<value> <feature>:<value> ... <feature>:<value> # 
<info> 
 
Here, ?target? represents the polarity of a sen-
tence or tweet; ?feature? refers to a term in the 
document, and ?value? refers to a feature weight. 
This could be used as the relative frequency of a 
term in the set of documents, or Tf-Idf. Tf-idf is 
the combination of term frequency (tf) and in-
verse document frequency (idf), is a weight value 
often used in text mining and information re-
trieval. This weight is a statistical measure used 
to evaluate the relative important of word in a 
document in the collection (Manning et al., 
2008).  
 
                   (1) 
where           is the weighting the scheme assigns to 
term   in document   
 
Term frequency (tf) is used to measure how fre-
quent the term appears in the document. 
 
      
   
?     
 (2) 
where     is the number of term   appears in a document    
?      is the total number of terms   in the document  . 
 
Inverse document frequency (idf) is used to 
measure how important the term is ? i.e. whether 
the term is common or rare in the collection.  
 
        
 
  
 
(3) 
where   is the total number of documents in the collection 
in corpus.    is the number of documents   which term   
appears. 
 
Therefore, we chose to work with both of these 
to observe which yielded the best results in the 
polarity classification.  
659
The default settings of SVMLight were used 
throughout. This meant that we used a linear 
kernel that did not require any parameters.? 
4 Experiment and Results  
In our experiment, we used the datasets and 
evaluated the system using the F-score measure-
ment. During pre-processing features were ex-
tracted from both datasets. First, we used a 
frequency of word as a featured weight by calcu-
lating the frequency of word in the dataset and, 
during pre-processing, we labelled the emotions 
in both datasets. The results revealed a lower 
than average F-score at 34.80%.  As this was 
quite low we disregarded further use of term fre-
quency as a feature weight. We moved on to use 
Tf-Idf as the feature weight and, again, emoti-
cons in both datasets were labelled. The score of 
78.10% was achieved. Then, we kept the pre-
possessing of the training set stable by combin-
ing the features to extract from the testing data. 
These results are presented in Table 1**.  
The highest score of 81.96% was recorded 
when all the features were combined and extract-
ed from both datasets.  
The lowest score of 36.48% was recorded 
when emoticons were extracted from testing data 
and all features were extracted from training da-
tasets. The results of the highest scoring experi-
ment were submitted to the task organizers. 
Following solution submissions, the task or-
ganizers announced the scores by separating the 
data into the following five groups: LiveJour-
nal2014; SMS2013; Twitter2013; Twitter2014; 
and Twitter2014 Sarcasm. This would allow the 
identification of any domain dependent effects. 
However, the results showed that we achieved 
above average in all the datasets, as illustrated in 
Figure 1. 
                                                          
?Based on SVMLight 
**The results in the table are from the test set 2014 in task 
2A. 
5 Conclusion and Future Work 
The TJP system participated in SemEval 2014 
Task 9, Part A: Contextual Polarity Disambigua-
tion. The system exploited considerable pre-
processing, before using the well known, 
SVMLight machine learning algorithm (Joa-
chims. 1999). The pre-processing used several 
twitter specific features, such as hashtags and 
ids, in addition to more traditional Information 
Retrieval concepts such as the Tf-Idf heuristic 
(Manning et al., 2008). The results showed that 
the combination of all features in both datasets 
achieved the best results, at 81.96%. 
An aspect of this contribution is the compara-
tive analysis of feature effectiveness. That is, we 
attempted to identify which factor(s) made the 
most significant improvement to system perfor-
mance. It is clear the pre-processing had a con-
siderable effect on system performance. The use 
of a different learning algorithm also contributed 
to performance since, on this task, SVMLight 
performed better than the Naive Bayes algorithm 
that was used by our team in 2013. 
Sentiment resources was not been used in our 
system in SemEval 2014 as same as in SemEval 
2013 whilst other user groups have employed a 
variety of resources of different sizes, and accu-
racy (Wilson et al., 2013). These points lead to 
the following plan for future activities. 
Our future work is to rigorously investigate 
the success factors for sentiment analysis, espe-
cially in the twitter domain. More specifically, 
we have formulated the following research ques-
tions as a result of our participation in SemEval 
? Are Sentiment resources essential for the 
Sentiment Analysis task? 
? Can the accuracy and effectiveness of 
sentiment lexicons be measured? If so, 
which feature of the resource (accuracy 
vs. coverage) is the most effective met-
ric. 
? Might it be more effective to use a range 
of sentiments (e.g. [-1.0 .. 1.0]), rather 
660
than binary approach(e.g. positive and 
negative) taken in SemEval 2013, and 
2014? 
? Is one machine learning algorithm suffi-
cient, and if so which is it? Or, alternate-
ly would an ensemble approach (Rokach, 
2005) significantly improve perfor-
mance?
 
 
Table 1: The results of each feature analyzed in the approach of TF-IDF 
 
Figure 1: The comparison of TJP and average scores 
 
References 
Alec Go, Richa Bhayani and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervi-
sion. CS224N Project Report, Stanford, 1-12. 
Andrea Esuli, Fabrizio Sebastiani 2006 
"SENTIWORDNET: A Publicly Available Lexi-
cal Resource for Opinion Mining" in Proceedings 
of the 5th Conference on Language Resources and 
Evaluation, LREC (2006), pp. 417-422 
Andranik Tumasjan, Timm O. Sprenger, Philipp G. 
Sandner and Isabell M. Welpe. 2010. "Predicting 
elections with twitter: What 140 characters reveal 
about political sentiment," in Proceedings of the 
Fourth International AAAI Conference on Web-
logs and Social Media, pp. 178-185. 
Christopher D. Manning, Prabhakar Raghavan and 
Hinrich Sch?tze, Introduction to Information Re-
trieval, Cambridge University Press. 2008.  ISBN: 
0521865719.  
David Nadeau, Catherine Sabourin, Joseph De Kon-
inck, Stan Matwin and Peter D. Turney. 2006. 
661
"Automatic dream sentiment analysis," presented 
at the In: Proceedings of the Workshop on Compu-
tational Aesthetics at the Twenty-First National 
Conference on Artificial Intelligence, Boston, 
Massachussetts, USA. 
Dietmar Grabner, Markus Zanker, Gunther Fliedl and 
Matthias Fuchs. 2012."Classification of customer 
reviews based on sentiment analysis," presented at 
the 19th Conference on Information and Commu-
nication Technologies in Tourism (ENTER), Hel-
singborg, Sweden. 
Eiji Aramaki, Sachiko Maskawa and Mizuki Morita. 
2011. Twitter catches the flu: detecting influenza 
epidemics using Twitter. Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Edinburgh, United Kingdom: 
Association for Computational Linguistics. 
Johan. Bollen and Huina. Mao. Twitter mood as a 
stock market predictor. IEEE Computer, 
44(10):91?94. 
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for sen-
timent classification. Proceedings of the ACL 
Student Research Workshop. Ann Arbor, Michi-
gan: Association for Computational Linguistics. 
Lior Rokach. 2005. Chapter 45 Ensemble Methods for 
Classifiers. In: Oded Maimon and Lior Rokach 
(eds.) Data Mining and Knowledge Discovery 
Handbook. Springer US. 
Marti A. Hearst, Susan T. Dumais, Edgar Osman, 
John Platt and Bernhard Scholkopf . 1998. Sup-
port vector machines. IEEE, Intelligent Systems 
and their Applications, 13, 18-28.  
Masoud Makrehchi, Sameena Shah and Wenhui Liao. 
2013. Stock Prediction Using Event-Based Senti-
ment Analysis.  Web Intelligence (WI) and Intelli-
gent Agent Technologies (IAT), 2013 
IEEE/WIC/ACM International Joint Conferences 
on,. 337-342. 
Nello Cristianini and John Shawe-Taylor. 2000. An 
introduction to support vector machines and other 
kernel-based learning methods, Cambridge uni-
versity press. 
Sara Rosenthal, Preslav Nakov, Alan Ritter and 
Veselin Stoyanov. 2014. SemEval-2014 Task 9: 
Sentiment Analysis in Twitter. International 
Workshop on Semantic Evaluation (SemEval-
2014). Dublin, Ireland. 
Steven Bird, Ewan Klein and Edward Loper. 2009. 
NLTK: Natural language processing with Python, 
O'Reilly. 
Takeshi Sakaki, Makoto Okazaki and Yutaka Matsuo. 
2010. Earthquake shakes Twitter users: real-time 
event detection by social sensors. Proceedings of 
the 19th international conference on World wide 
web. Raleigh, North Carolina, USA: ACM. 
Tawunrat Chalothorn and Jeremy Ellman. 2013. TJP: 
Using Twitter to Analyze the Polarity of Contexts. 
Second Joint Conference on Lexical and Compu-
tational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on 
Semantic Evaluation (SemEval 2013). Atlanta, 
Georgia, USA: Association for Computational 
Linguistics. 
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, 
Alan Ritter, Sara Rosenthal and Veselin Stoyanov. 
2013. SemEval-2013 Task 2: Sentiment Analysis 
in Twitter. Proceedings of the 7th International 
Workshop on Semantic Evaluation. Association 
for Computational Linguistics. 
Thorsten Joachims. 1999. Making large-scale support 
vector machine learning practical. Advances in 
kernel methods. MIT Press. 
662
