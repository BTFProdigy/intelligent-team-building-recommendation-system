Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 933?939,
Prague, June 2007. c?2007 Association for Computational Linguistics
Single Malt or Blended? A Study in Multilingual Parser Optimization
Johan Hall? Jens Nilsson? Joakim Nivre??
Gu?ls?en Eryig?it? Bea?ta Megyesi? Mattias Nilsson? Markus Saers?
?Va?xjo? University, School of Mathematics and Systems Engineering
E-mail: firstname.lastname@vxu.se
?Uppsala University, Dept. of Linguistics and Philology
E-mail: firstname.lastname@lingfil.uu.se
?Istanbul Technical University, Computer Engineering Dept.
E-mail: gulsen.cebiroglu@itu.edu.tr
Abstract
We describe a two-stage optimization of the
MaltParser system for the ten languages in
the multilingual track of the CoNLL 2007
shared task on dependency parsing. The
first stage consists in tuning a single-parser
system for each language by optimizing pa-
rameters of the parsing algorithm, the fea-
ture model, and the learning algorithm. The
second stage consists in building an ensem-
ble system that combines six different pars-
ing strategies, extrapolating from the opti-
mal parameters settings for each language.
When evaluated on the official test sets, the
ensemble system significantly outperforms
the single-parser system and achieves the
highest average labeled attachment score.
1 Introduction
In the multilingual track of the CoNLL 2007 shared
task on dependency parsing, a single parser must be
trained to handle data from ten different languages:
Arabic (Hajic? et al, 2004), Basque (Aduriz et al,
2003), Catalan, (Mart?? et al, 2007), Chinese (Chen
et al, 2003), Czech (Bo?hmova? et al, 2003), English
(Marcus et al, 1993; Johansson and Nugues, 2007),
Greek (Prokopidis et al, 2005), Hungarian (Csendes
et al, 2005), Italian (Montemagni et al, 2003), and
Turkish (Oflazer et al, 2003).1 Our contribution is
a study in multilingual parser optimization using the
freely available MaltParser system, which performs
1For more information about the task and the data sets, see
Nivre et al (2007).
deterministic, classifier-based parsing with history-
based feature models and discriminative learning,
and which was one of the top performing systems
in the CoNLL 2006 shared task (Nivre et al, 2006).
In order to maximize parsing accuracy, optimiza-
tion has been carried out in two stages, leading to
two different, but related parsers. The first of these is
a single-parser system, similar to the one described
in Nivre et al (2006), which parses a sentence deter-
ministically in a single left-to-right pass, with post-
processing to recover non-projective dependencies,
and where the parameters of the MaltParser system
have been tuned for each language separately. We
call this system Single Malt, to emphasize the fact
that it consists of a single instance of MaltParser.
The second parser is an ensemble system, which
combines the output of six deterministic parsers,
each of which is a variation of the Single Malt parser
with parameter settings extrapolated from the first
stage of optimization. It seems very natural to call
this system Blended.
Section 2 summarizes the work done to optimize
the Single Malt parser, while section 3 explains how
the Blended parser was constructed from the Single
Malt parser. Section 4 gives a brief analysis of the
experimental results, and section 5 concludes.
2 The Single Malt Parser
The parameters available in the MaltParser system
can be divided into three groups: parsing algorithm
parameters, feature model parameters, and learn-
ing algorithm parameters.2 Our overall optimization
2For a complete documentation of these parameters, see
http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
933
strategy for the Single Malt parser was as follows:
1. Define a good baseline system with the same
parameter settings for all languages.
2. Tune parsing algorithm parameters once and
for all for each language (with baseline settings
for feature model and learning algorithm pa-
rameters).
3. Optimize feature model and learning algorithm
parameters in an interleaved fashion for each
language.
We used nine-fold cross-validation on 90% of the
training data for all languages with a training set size
smaller than 300,000 tokens and an 80%?10% train-
devtest split for the remaining languages (Catalan,
Chinese, Czech, English). The remaining 10% of
the data was in both cases saved for a final dry run,
where the parser was trained on 90% of the data for
each language and tested on the remaining (fresh)
10%. We consistently used the labeled attachment
score (LAS) as the single optimization criterion.
Below we describe the most important parameters
in each group, define baseline settings, and report
notable improvements for different languages during
development. The improvements for each language
from step 1 (baseline) to step 2 (parsing algorithm)
and step 3 (feature model and learning algorithm)
can be tracked in table 1.3
2.1 Parsing Algorithm
MaltParser implements several parsing algorithms,
but for the Single Malt system we stick to the one
used by Nivre et al (2006), which performs labeled
projective dependency parsing in linear time, using a
stack to store partially processed tokens and an input
queue of remaining tokens. There are three basic
parameters that can be varied for this algorithm:
1. Arc order: The baseline algorithm is arc-
eager, in the sense that right dependents are
attached to their head as soon as possible, but
there is also an arc-standard version, where the
attachment of right dependents has to be post-
poned until they have found all their own de-
pendents. The arc-standard order was found
3Complete specifications of all parameter settings for all
languages, for both Single Malt and Blended, are available at
http://w3.msi.vxu.se/users/jha/conll07/.
to improve parsing accuracy for Chinese, while
the arc-eager order was maintained for all other
languages.
2. Stack initialization: In the baseline version
the parser is initialized with an artificial root
node (with token id 0) on the stack, so that arcs
originating from the root can be added explic-
itly during parsing. But it is also possible to ini-
tialize the parser with an empty stack, in which
case arcs from the root are only added implic-
itly (to any token that remains a root after pars-
ing is completed). Empty stack initialization
(which reduces the amount of nondeterminism
in parsing) led to improved accuracy for Cata-
lan, Chinese, Hungarian, Italian and Turkish.4
3. Post-processing: The baseline parser performs
a single left-to-right pass over the input, but it
is possible to allow a second pass where only
unattached tokens are processed.5 Such post-
processing was found to improve results for
Basque, Catalan, Czech, Greek and Hungarian.
Since the parsing algorithm only produces projective
dependency graphs, we may use pseudo-projective
parsing to recover non-projective dependencies, i.e.,
projectivize training data and encode information
about these transformations in extended arc labels
to support deprojectivization of the parser output
(Nivre and Nilsson, 2005). Pseudo-projective pars-
ing was found to have a positive effect on over-
all parsing accuracy only for Basque, Czech, Greek
and Turkish. This result can probably be explained
in terms of the frequency of non-projective depen-
dencies in the different languages. For Basque,
Czech, Greek and Turkish, more than 20% of the
sentences have non-projective dependency graphs;
for all the remaining languages the corresponding
4For Arabic, Basque, Czech, and Greek, the lack of im-
provement can be explained by the fact that these data sets allow
more than one label for dependencies from the artificial root.
With empty stack initialization all such dependencies are as-
signed a default label, which leads to a drop in labeled attach-
ment score. For English, however, empty stack initialization did
not improve accuracy despite the fact that dependencies from
the artificial root have a unique label.
5This technique is similar to the one used by Yamada and
Matsumoto (2003), but with only a single post-processing pass
parsing complexity remains linear in string length.
934
Attributes
Tokens FORM LEMMA CPOSTAG POSTAG FEATS DEPREL
S: Top + + + + + +
S: Top?1 +
I: Next + + + + +
I: Next+1 + +
I: Next+2 +
I: Next+3 +
G: Head of Top +
G: Leftmost dependent of Top +
G: Rightmost dependent of Top +
G: Leftmost dependent of Next +
Figure 1: Baseline feature model (S = Stack, I = Input, G = Graph).
figure is 10% or less.6
The cumulative improvement after optimization
of parsing algorithm parameters was a modest 0.32
percentage points on average over all ten languages,
with a minimum of 0.00 (Arabic, English) and a
maximum of 0.83 (Czech) (cf. table 1).
2.2 Feature Model
MaltParser uses a history-based feature model for
predicting the next parsing action. Each feature of
this model is an attribute of a token defined relative
to the current stack S, input queue I, or partially built
dependency graph G, where the attribute can be any
of the symbolic input attributes in the CoNLL for-
mat: FORM, LEMMA, CPOSTAG, POSTAG and
FEATS (split into atomic attributes), as well as the
DEPREL attribute of tokens in the graph G. The
baseline feature model is depicted in figure 1, where
rows denote tokens, columns denote attributes, and
each cell containing a plus sign represents a model
feature.7 This model is an extrapolation from many
previous experiments on different languages and
usually represents a good starting point for further
optimization.
The baseline model was tuned for each of the ten
languages using both forward and backward feature
6In fact, for Arabic, which has about 10% sentences with
non-projective dependencies, it was later found that, with an
optimized feature model, it is beneficial to projectivize the train-
ing data without trying to recover non-projective dependencies
in the parser output. This was also the setting that was used for
Arabic in the dry run and final test.
7The names Top and Next refer to the token on top of the
stack S and the first token in the remaining input I, respectively.
selection. The total number of features in the tuned
models varies from 18 (Turkish) to 56 (Hungarian)
but is typically between 20 and 30. This feature se-
lection process constituted the major development
effort for the Single Malt parser and also gave the
greatest improvements in parsing accuracy, but since
feature selection was to some extent interleaved with
learning algorithm optimization, we only report the
cumulative effect of both together in table 1.
2.3 Learning Algorithm
MaltParser supports several learning algorithms but
the best results have so far been obtained with sup-
port vector machines, using the LIBSVM package
(Chang and Lin, 2001). We use a quadratic kernel
K(xi, xj) = (?xTi xj + r)
2 and LIBSVM?s built-
in one-versus-one strategy for multi-class classifica-
tion, converting symbolic features to numerical ones
using the standard technique of binarization. As our
baseline settings, we used ? = 0.2 and r = 0 for
the kernel parameters, C = 0.5 for the penalty para-
meter, and ? = 1.0 for the termination criterion. In
order to reduce training times during development,
we also split the training data for each language into
smaller sets and trained separate multi-class classi-
fiers for each set, using the POSTAG of Next as the
defining feature for the split.
The time spent on optimizing learning algorithm
parameters varies between languages, mainly due
to lack of time. For Arabic, Basque, and Catalan,
the baseline settings were used also in the dry run
and final test. For Chinese, Greek and Hungarian,
935
Development Dry Run Test Test: UAS
Language Base PA F+L SM B SM B SM B
Arabic 70.31 70.31 71.67 70.93 73.09 74.75 76.52 84.21 85.81
Basque 73.86 74.44 76.99 77.18 80.12 74.97 76.92 80.61 82.84
Catalan 85.43 85.51 86.88 86.65 88.00 87.74 88.70 92.20 93.12
Chinese 83.85 84.39 87.64 87.61 88.61 83.51 84.67 87.60 88.70
Czech 75.00 75.83 77.74 77.91 82.17 77.22 77.98 82.35 83.59
English 85.44 85.44 86.35 86.35 88.74 85.81 88.11 86.77 88.93
Greek 72.67 73.04 74.42 74.89 78.17 74.21 74.65 80.66 81.22
Hungarian 74.62 74.64 77.40 77.81 80.04 78.09 80.27 81.71 83.55
Italian 81.42 81.64 82.50 83.37 85.16 82.48 84.40 86.26 87.77
Turkish 75.12 75.80 76.49 75.87 77.09 79.24 79.79 85.04 85.77
Average 77.78 78.10 79.81 79.86 82.12 79.80 81.20 84.74 86.13
Table 1: Development results for Single Malt (Base = baseline, PA = parsing algorithm, F+L = feature model
and learning algorithm); dry run and test results for Single Malt (SM) and Blended (B) (with corrected test
scores for Blended on Chinese). All scores are labeled attachment scores (LAS) except the last two columns,
which report unlabeled attachment scores (UAS) on the test sets.
slightly better results were obtained by not splitting
the training data into smaller sets; for the remain-
ing languages, accuracy was improved by using the
CPOSTAG of Next as the defining feature for the
split (instead of POSTAG). With respect to the SVM
parameters (?, r, C, and ?), Arabic, Basque, Cata-
lan, Greek and Hungarian retain the baseline set-
tings, while the other languages have slightly dif-
ferent values for some parameters.
The cumulative improvement after optimization
of feature model and learning algorithm parameters
was 1.71 percentage points on average over all ten
languages, with a minimum of 0.69 (Turkish) and a
maximum of 3.25 (Chinese) (cf. table 1).
3 The Blended Parser
The Blended parser is an ensemble system based
on the methodology proposed by Sagae and Lavie
(2006). Given the output dependency graphs Gi
(1 ? i ? m) of m different parsers for an input sen-
tence x, we construct a new graph containing all the
labeled dependency arcs proposed by some parser
and weight each arc a by a score s(a) reflecting its
popularity among the m parsers. The output of the
ensemble system for x is the maximum spanning
tree of this graph (rooted at the node 0), which can
be extracted using the Chu-Liu-Edmonds algorithm,
as shown by McDonald et al (2005). Following
Sagae and Lavie (2006), we let s(a) =
?m
i=1 w
c
iai,
where wci is the average labeled attachment score of
parser i for the word class c8 of the dependent of a,
and ai is 1 if a ? Gi and 0 otherwise.
The Blended parser uses six component parsers,
with three different parsing algorithms, each of
which is used to construct one left-to-right parser
and one right-to-left parser. The parsing algorithms
used are the arc-eager baseline algorithm, the arc-
standard variant of the baseline algorithm, and the
incremental, non-projective parsing algorithm first
described by Covington (2001) and recently used
for deterministic classifier-based parsing by Nivre
(2007), all of which are available in MaltParser.
Thus, the six component parsers for each language
were instances of the following:
1. Arc-eager projective left-to-right
2. Arc-eager projective right-to-left
3. Arc-standard projective left-to-right
4. Arc-standard projective right-to-left
5. Covington non-projective left-to-right
6. Covington non-projective right-to-left
8We use CPOSTAG to determine the part of speech.
936
root 1 2 3?6 7+
Parser R P R P R P R P R P
Single Malt 87.01 80.36 95.08 94.87 86.28 86.67 77.97 80.23 68.98 71.06
Blended 92.09 74.20 95.71 94.92 87.55 88.12 78.66 83.02 65.29 78.14
Table 2: Recall (R) and precision (P) of Single Malt and Blended for dependencies of different length,
averaged over all languages (root = dependents of root node, regardless of length).
The final Blended parser was constructed by reusing
the tuned Single Malt parser for each language (arc-
standard left-to-right for Chinese, arc-eager left-to-
right for the remaining languages) and training five
additional parsers with the same parameter settings
except for the following mechanical adjustments:
1. Pseudo-projective parsing was not used for the
two non-projective parsers.
2. Feature models were adjusted with respect to
the most obvious differences in parsing strategy
(e.g., by deleting features that could never be
informative for a given parser).
3. Learning algorithm parameters were adjusted
to speed up training (e.g., by always splitting
the training data into smaller sets).
Having trained all parsers on 90% of the training
data for each language, the weights wci for each
parser i and coarse part of speech c was determined
by the labeled attachment score on the remaining
10% of the data. This means that the results obtained
in the dry run were bound to be overly optimistic for
the Blended parser, since it was then evaluated on
the same data set that was used to tune the weights.
Finally, we want to emphasize that the time for
developing the Blended parser was severely limited,
which means that several shortcuts had to be taken,
such as optimizing learning algorithm parameters
for speed rather than accuracy and using extrapo-
lation, rather than proper tuning, for other impor-
tant parameters. This probably means that the per-
formance of the Blended system can be improved
considerably by optimizing parameters for all six
parsers separately.
4 Results and Discussion
Table 1 shows the labeled attachment score results
from our internal dry run (training on 90% of the
training data, testing on the remaining 10%) and the
official test runs for both of our systems. It should
be pointed out that the test score for the Blended
parser on Chinese is different from the official one
(75.82), which was much lower than expected due
to a corrupted specification file required by Malt-
Parser. Restoring this file and rerunning the parser
on the Chinese test set, without retraining the parser
or changing any parameter settings, resulted in the
score reported here. This also improved the aver-
age score from 80.32 to 81.20, the former being the
highest reported official score.
For the Single Malt parser, the test results are on
average very close to the dry run results, indicating
that models have not been overfitted (although there
is considerably variation between languages). For
the Blended parser, there is a drop of almost one
percentage point, which can be explained by the fact
that weights could not be tuned on held-out data for
the dry run (as explained in section 3).
Comparing the results for different languages, we
see a tendency that languages with rich morphology,
usually accompanied by flexible word order, get
lower scores. Thus, the labeled attachment score is
below 80% for Arabic, Basque, Czech, Greek, Hun-
garian, and Turkish. By comparison, the more con-
figurational languages (Catalan, Chinese, English,
and Italian) all have scores above 80%. Linguis-
tic properties thus seem to be more important than,
for example, training set size, which can be seen by
comparing the results for Italian, with one of the
smallest training sets, and Czech, with one of the
largest. The development of parsing methods that
are better suited for morphologically rich languages
with flexible word order appears as one of the most
important goals for future research in this area.
Comparing the results of our two systems, we
see that the Blended parser outperforms the Single
Malt parser for all languages, with an average im-
937
provement of 1.40 percentage points, a minimum of
0.44 (Greek) and a maximum of 2.40 (English). As
shown by McDonald and Nivre (2007), the Single
Malt parser tends to suffer from two problems: error
propagation due to the deterministic parsing strat-
egy, typically affecting long dependencies more than
short ones, and low precision on dependencies orig-
inating in the artificial root node due to fragmented
parses.9 The question is which of these problems is
alleviated by the multiple views given by the compo-
nent parsers in the Blended system. Table 2 throws
some light on this by giving the precision and re-
call for dependencies of different length, treating de-
pendents of the artificial root node as a special case.
As expected, the Single Malt parser has lower preci-
sion than recall for root dependents, but the Blended
parser has even lower precision (and somewhat bet-
ter recall), indicating that the fragmentation is even
more severe in this case.10 By contrast, we see that
precision and recall for other dependencies improve
across the board, especially for longer dependencies,
which probably means that the effect of error propa-
gation is mitigated by the use of an ensemble system,
even if each of the component parsers is determinis-
tic in itself.
5 Conclusion
We have shown that deterministic, classifier-based
dependency parsing, with careful optimization, can
give highly accurate dependency parsing for a wide
range of languages, as illustrated by the performance
of the Single Malt parser. We have also demon-
strated that an ensemble of deterministic, classifier-
based dependency parsers, built on top of a tuned
single-parser system, can give even higher accuracy,
as shown by the results of the Blended parser, which
has the highest labeled attachment score for five lan-
guages (Arabic, Basque, Catalan, Hungarian, and
9A fragmented parse is a dependency forest, rather than a
tree, and is automatically converted to a tree by attaching all
(other) roots to the artificial root node. Hence, children of the
root node in the final output may not have been predicted as
such by the treebank-induced classifier.
10This conclusion is further supported by the observation
that the single most frequent ?frame confusion? of the Blended
parser, over all languages, is to attach two dependents with the
label ROOT to the root node, instead of only one. The frequency
of this error is more than twice as high for the Blended parser
(180) as for the Single Malt parser (83).
Italian), as well as the highest multilingual average
score.
Acknowledgements
We want to thank all treebank providers for making
the data available for the shared task and the (other)
organizers for their efforts in organizing it. Special
thanks to Ryan McDonald, for fruitful discussions
and assistance with the error analysis, and to Kenji
Sagae, for showing us how to produce a good blend.
Thanks also to two reviewers for useful comments.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf., pages 95?102.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
938
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523?530.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency. In Proc. of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
99?106.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221?225.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2007. Incremental non-projective dependency
parsing. In Human Language Technologies: The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT),
pages 396?403.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 129?132.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195?206.
939
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 28?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Phrase-Based Translation via Word Alignments from Stochastic Inversion Transduction Grammars  Markus SAERS   Dept. of Linguistics and Philology  Uppsala University Sweden   markus.saers@lingfil.uu.se Dekai WU   Human Language Technology Center Dept. of Computer Science & Engineering HKUST Hong Kong dekai@cs.ust.hk  Abstract We argue that learning word alignments through a compositionally-structured, joint process yields higher phrase-based transla-tion accuracy than the conventional heuris-tic of intersecting conditional models. Flawed word alignments can lead to flawed phrase translations that damage translation accuracy. Yet the IBM word alignments usually used today are known to be flawed, in large part because IBM models (1) model reordering by allowing unrestricted movement of words, rather than con-strained movement of compositional units, and therefore must (2) attempt to compen-sate via directed, asymmetric distortion and fertility models. The conventional heuris-tics for attempting to recover from the re-sulting alignment errors involve estimating two directed models in opposite directions and then intersecting their alignments ? to make up for the fact that, in reality, word alignment is an inherently joint relation. A natural alternative is provided by Inversion Transduction Grammars, which estimate the joint word alignment relation directly, eliminating the need for any of the conven-tional heuristics. We show that this align-ment ultimately produces superior translation accuracy on BLEU, NIST, and METEOR metrics over three distinct lan-guage pairs. 1 Introduction In this paper we argue that word alignments learned through a compositionally-structured, joint 
process are able to significantly improve the train-ing of phrase-based translation systems, leading to higher translation accuracy than the conventional heuristic of intersecting conditional models. To-day, statistical machine translation (SMT) systems perform at state-of-the-art levels; their ability to weigh different translation hypotheses against each other to find an optimal solution has proven to be a great asset. What sets various SMT systems apart are the models employed to determine what to con-sider optimal. The most common systems today consist of phrase-based models, where chunks of texts are substituted and rearranged to produce the output sentence. Our premise is that certain flawed word align-ments can lead to flawed phrase translations that in turn damage translation accuracy, since word alignment is the basis for learning phrase transla-tions in phrase-based SMT systems. A critical part of such systems is the word-level translation model, which is estimated from aligned data. Cur-rently, the standard way of computing a word alignment is to estimate a function linking words in one of the languages to words in the other. Func-tions can only define many-to-one relations, but word alignment is a many-to-many relation. The solution is to combine two functions, one in each direction, and harmonize them by means of some heuristic. After that, phrases can be extracted from the word alignments. The problem is that the starting point for word alignments is usually the IBM models (Brown et al, 1993), which are known to produce flawed alignments, in large part because they (1) model reordering by allowing unrestricted movement of words, rather than constrained movement of com-positional units, and therefore must (2) attempt to compensate via directed, asymmetric distortion and fertility models. 
28
The conventional heuristics for attempting to re-cover from the resulting alignment errors is to es-timate two directed models in opposite directions and then intersect their alignments ? to make up for the fact that, in reality, word alignment is an inherently joint relation. It is unfortunate that such a critical stage in the training process of an SMT system relies on inaccurate heuristics, which have been largely motivated by historical implementa-tion factors, rather than principles explaining lan-guage phenomena. Inversion Transduction Grammar (ITG) models provide a natural, alternative approach, by estimat-ing the joint word alignment relation directly, eliminating the need for any of the conventional heuristics. A transduction grammar is a grammar that generates sentences in two languages (L0 and L1) simultaneously; i.e., one start symbol expands into two strings, as for example in Figure 1(b).  A transduction grammar explains two languages si-multaneously.  ITGs model a class of transductions (sets of sentence translations) with expressive power and computational complexity falling be-tween (a) finite-state transducers or FSTs and (b) syntax-directed transduction grammars1 or SDTGs.  An ITG produces both a common structural form for a sentence pairs, as well as relating the words ? aligning them.  This could actually work as the joint word alignment that is usually constructed by heuristic function combination. Yet despite the substantial body of literature on word alignment, ITG based models, and phrase-based SMT, the existing work has not assessed the potential for improving phrase-based translation quality by using joint ITG based word alignments to replace the error-prone conditional IBM model based word alignments and associated heuristics for intersecting bidirectional IBM alignments. On one hand, word alignment work is usually evaluated not on actual translation quality, but rather on artificial metrics like alignment error rate (AER, Och & Ney, 2003), which relies on a manu-ally annotated gold standard word alignment. There are some indications that ITG produces bet-ter alignment then the standard method (Zhao & Vogel, 2003, Zhang & Gildea 2005, Chao & Li, 2007). There is, however, little inherent utility in alignments ? their value is determined by the SMT systems one can build from them. In fact, recent                                                            1 Which ?synchronous CFGs? are essentially identical to. 
studies have discredited the earlier assumption that lower AER is correlated with improved translation quality ? the opposite can very well occur (Ayan & Dorr, 2006). Therefore it is essential to evaluate the quality of the word alignment not in terms of AER, but rather in terms of actual translation qual-ity in a system built from it. On the other hand, ITG models have been em-ployed to improve translation quality as measured by BLEU (Papineni et al, 2002), but still without directly addressing the problem of dependence on inaccurate IBM alignments. S?nchez & Bened? (2006) construct an ITG from word alignments computed by the conventional IBM model, which does little to alleviate the problems. Sima?an & Mylonakis (2008) use an ITG to structure a prior distribution to a phrase extraction system, which is an altogether different approach. Cherry & Lin (2007) do use ITG to build word alignments, but blur the lines by still mixing in the conventional IBM method, and focus on phrase extraction. The present work clearly demonstrates, for the first time to our knowledge, that replacing the widely-used heuristic of intersecting IBM word alignments from two directed conditional models instead with a single ITG alignment from a joint model produces superior translation accuracy.  The experiments are performed on three distinct lan-guage pairs: German?English, Spanish?English, and French?English. Translation accuracy is re-ported in terms of BLEU, NIST, and METEOR metrics. 2 Background Statistical Machine Translation is a paradigm where translation is considered as a code-breaking problem. The goal is to find the most likely output sentence (clear text message) given the supplied input sentence (coded message), according to some model. To get a probabilistic model, large amounts of training data are used. These data have to be aligned so that an understanding of correspon-dences between the languages is there to be learnt from. Even if the data is assumed to be aligned at sentence level, sub-sentence alignment is also needed. This is usually carried out by training some statistical model of a word-to-word function (Brown et al, 1993), or a hidden Markov model consuming input words and emitting output words 
29
(Vogel, Ney & Tillmann, 1996). The toolkit GIZA++ (Och & Ney, 2000) is freely available and widely used to compute such word alignments. All these models learn a directed translation function that maps input words to output words. Since these functions focus solely on surface phe-nomena, they have no mechanisms for dealing with the kind of structured reordering between lan-guages that could account for, e.g., the difference between SVO languages and SOV languages. What emerges is in fact a rather flawed model of how one language is rewritten into another. The conventional way to alleviate this flaw is to train an equally flawed model in the other direction, and then intersect the two. This practice certainly alle-viates some of the problems, but far from all. To build a phrase-based SMT system, the word alignment is used as a starting point to try to ac-count for the entire sentence. This means that the word alignment is gradually expanded, so that all words in both sentences are accounted for, either by words in the other language, or by the null empty word ?. This process is called grow-diag-final (Koehn, Och & Marcu, 2003). The grow-diag-final process does smooth over some of the flaws still left in the word alignment, but error analysis gives reason to doubt that it re-pairs enough of the errors to avoid damaging trans-lation accuracy. Thus, we are motivated to investigate a completely different approach that attempts to avoid the noisy directed alignments in the first place. 2.1 Inversion Transduction Grammars A transduction is a set of sentence translation pairs ? just as a language is a set of sentences.  The set defines a relation between the input and output languages. In the generative view, a transduction gram-mar generates a transduction, i.e., a set of sentence translation pairs or bisentences ? just as an ordi-nary (monolingual) language grammar generates a language, i.e., a set of sentences.  In the recogni-tion view, alternatively, a transduction grammar biparses or accepts all sentence pairs of a trans-duction ? just as a language grammar parses or accepts all sentences of a language.  And in the transduction view, a transduction grammar trans-duces (translates) input sentences to output sen-tences. 
Two familiar classes of transductions have been in widespread use for decades in many areas of computer science and linguistics:  A syntax-directed transduction is a set of bisen-tences generated by some syntax-directed transduc-tion grammar or SDTG (Lewis & Stearns, 1968; Aho & Ullman, 1969, 1972).  A ?synchronous CFG? is equivalent to an SDTG.  A finite-state transduction is a set of bisentences generated by some finite-state transducer or FST.  It is possible to describe finite-state transductions us-ing SDTGs (or synchronous CFGs) by restricting them alternatively to the special cases of either ?right regular SDTGs? or ?left regular SDTGs?.  However, such characterizations rather misleadingly overlook the key point ? by severely limiting expressive power, finite-state transductions are orders of magni-tude cheaper to biparse, train, and induce than syn-tax-directed transductions ? and are often even more accurate to induce.  More recently, an intermediate equivalence class of transductions whose generative capacity and computational complexity falls in between these two has become widely used in state-of-the-art MT systems ? due to numerous empirical results indi-cating significantly better fit to modeling transla-tion between many human language pairs:  An inversion transduction is a set of bisentences generated by some inversion transduction gram-mar or ITG (Wu, 1995a, 1995b, 1997).  As above with finite-state transductions, it is possible to de-scribe inversion transductions using SDTGs (or syn-chronous CFGs) by restricting them alternatively to the special cases of ?binary SDTGs?, ?ternary SDTGs?, or ?SDTGs whose transduction rules are restricted to straight and inverted permutations only?.  Again however, as above, such characterizations rather misleadingly overlook the key point ? by se-verely limiting expressive power, inversion transduc-tions are orders of magnitude cheaper to biparse, train, and induce than syntax-directed transductions ? and are often even more accurate to induce.  Any SDTG (or synchronous CFG) of binary rank ? i.e., that has at most two nonterminals on the right-hand-side of any rule ? is an ITG.  (Simi-larly, any SDTG (or synchronous CFG) that is right regular is a finite-state transduction gram-mar.)  Thus, for example, any grammar computed by the binarization algorithm of Zhang et al 
30
monolingual bilingual  regular or finite-state languages FSA or CFG that is right regular or left regular      O(n2)  regular or finite-state transductions FST or  SDTG (or syn-chronous CFG) that is right regular or left regular  
    O(n4) context-free languages CFG   O(n3) inversion transductions ITG or  SDTG (or syn-chronous CFG) that is binary or ternary or inverting   O(n6)    syntax-directed transductions SDTG (or synchro-nous CFG)     O(n2n+2)  Table 1: Summary comparison of computational complexity for Viterbi and chart (bi)parsing, and EM training algorithms for both monolingual and bilingual hierarchies.   
(2006) is an ITG.  Similarly, any grammar induced following the hierarchical phrase-based translation method, which always yields a binary transduction grammar (Chiang 2005), is an ITG. Moreover, any SDTG (or synchronous CFG) of ternary rank ? i.e., that has at most three nontermi-nals on the right-hand-side of any rule ? is still equivalent to an ITG.  Of course, this does not hold for SDTGs (or synchronous CFGs) in general, which allow arbitrary rank (possibly exceeding three) at the price of exponential complexity, as summarized in Table 1. 
Without loss of generality, any ITG can be con-veniently written in a 2-normal form (Wu, 1995a, 1997).  This cannot be done for SDTGs (or syn-chronous CFGs) ? unlike the monolingual case of CFGs, which form an equivalence class of context-free languages that can all be written in Chomsky?s 2-normal form.  In the bilingual case, only ITGs 
form an equivalence class of inversion transduc-tions that can all be written in a 2-normal form. Formally, an ITG in this 2-normal form, which segregates syntactic versus lexical rules, consists of a tuple  where N is a set of non-terminal symbols, V0 and V1 are the vocabularies of L0 and L1 respectively, R is a set of transduction rules, and  is the start symbol.  Each trans-duction rule takes one of the following forms:  S ? X X ? [Y Z] X ? <Y Z> X ? segmentL0/? X ? ?/segmentL1 X ? segmentL0/segmentL1  where X, Y and Z may be any nonterminal. Aside from the start rule, there are two kinds of syntactic transduction rules, namely straight and inverted.  In the above notation, straight transduc-tion rules X ? [Y Z] use square brackets, whereas inverted rules X ? <Y Z> use angled brackets.  The transductions generated by straight nodes have the same order in both languages, whereas the transduction generated by the inverted nodes are inverted in one of the languages, mean-ing that the children are read left-to-right in L0 and right-to-left in L1. In Figure 1(b) for example, the parse tree node instantiating an inverted transduc-tion rule is marked with a horizontal bar.  This mechanism allows for a minimal amount of reor-dering, while keeping the complexity down. The last three forms are for lexical transduction rules.  Each segment comes from the vocabulary of one of the languages, indicated by the subscript.  In the simplest case, the two ?-rule forms define singletons, which insert ?spurious? segments into either language.  Spurious segments lack any cor-respondence in the other language ? they are ?aligned to null? ? and singletons are lexical rules that associate a null-aligned segment in one of the languages with an empty segment (?) in the other. On the other hand, the last rule form defines a lexical translation pair that aligns the word/phrase segmentL0 to its translation segmentL1.  Such rules can also be written com-positionally as a pair of singletons, although it reads less transparently:  X ? segmentL0/? ?/segmentL1 
31
Note that segments typically consist of multiple tokens. Common examples include: ? Chinese word/phrase segments consisting of multiple unsegmented character tokens ? Chinese word/phrase segments consisting of multiple smaller, presegmented multi-character word/phrase tokens ? English phrase/collocation segments consist-ing of multiple word tokens (roller coaster) ITGs inherently model phrasal translation ? lin-guistically speaking, ITGs assume the set of lexical translation pairs constitutes a phrasal lexicon (just as lexicographers assume in building ordinary eve-ryday dictionaries). An advantage of this is that the ITG biparsing and decoding algorithms perform integrated translation-driven segmentation si-multaneously with optimizing the parse (Wu, 1997; Wu & Wong, 1998). These properties allow an ITG to (1) insert and delete words/phrases, which matches the ability of the conventional methods for word alignment as well as phrase alignment, and (2) account for the reordering in a more principled and restricted way than conventional alignment methods. A stochastic ITG or SITG is an ITG where every rule is associated with a probability. As with a stochastic CFG (SCFG), the probabilities are conditioned on the left-hand-side symbol, so that the probability of rule X ? ? is p(?|X). A bracketing ITG or BITG or BTG (Wu, 1995a) contains only one nonterminal symbol, with syntactic transduction rules X ? [X X] and X ? <X X>, which means that it produces a bracketing rather than a labeled tree. With a sto-chastic BITG (SBITG or SBTG) it is still possible to determine an optimal tree, since inversion and alignment are coupled: where inversions are needed is decided by the translations, and vice versa. In Wu (1995b) algorithms for training a SITG using expectation maximization, as well as finding the optimal parse of a sentence pair given a SITG are presented. These are polynomial time O(n6), as seen in Table 1. Further pruning methods can also be added, especially for longer sentences. 2.2 Previous uses of ITG in alignment There have been several attempts to use various forms of ITGs in an alignment setting. 
Zhao & Vogel (2003) and S?nchez & Bened? (2006) both use GIZA++ to establish their SITG. Since they use GIZA++ to create their ITG, little light is shed on the question of whether an ITG produces better alignments than GIZA++. Zhang & Gildea (2005) compare lexicalized and standard ITGs on an alignment task, and conclude that both are superior to IBM models 1 and 4, and that lexicalization helps. They also employ some pruning techniques to speed up training. Chao & Li (2007) incorporate the reordering constraints im-posed by an ITG to their discriminative word aligner, and also note a lower alignment error rate in their system. Since neither work evaluates re-sults on a translation task, it is hard to know whether better AER would translate into improved translation quality, in light of Ayan & Dorr (2006). Sima?an & Mylonakis (2008) use an ITG as the basis of a prior distribution in their system that ex-tracts all possible phrases rather than employing a length cut-off, and report an increase in translation quality as measured by the BLEU score (Papineni et al, 2002). In this paper, it is not primarily pure ITG that is being evaluated, but it lends some credibility to our assumption that the ITG structure helps when aligning. Cherry & Lin (2007) use an ITG to produce phrase tables that are then used in a translation sys-tem. However, to make their system outperform GIZA++, they blend in a non-compositionality constraint that is still based on GIZA++ word alignments. We would very much like to clearly see and understand the difference between ITG and GIZA++ alignments, and the lines are somewhat blurred in their work. 3 Model First, the lexicon of the SBITG is initialized, by extracting lexical transduction rules from cooccur-rence data from the corpus. Each pair of tokens in each sentence pair is initially considered equally likely to be a lexical translation pair. Each token is also considered to be a possible singleton. The two syntactic transduction rules X ? [X X] and X ? <X X> are initially assumed to be equally likely. Then full expectation-maximization training (Wu, 1995b) is carried out on the training data. Instead of waiting for full convergence, the process is halted when the increase in the training data?s probability starts to decline. 
32
 sentence pairs tokens de-en 115,323 1,602,781 es-en 108,073 1,466,132 fr-en 95,990 1,340,718 Table 2: Summary of training data.   
  (a)   (a)   (a) 
  (b)   (b)   (b)  Figure 1: (a) Bidirectional IBM alignments and their intersection and (b) ITG alignments.  Figure 2: (a) Bidirectional IBM alignments and their intersection and (b) ITG alignments.  Figure 3: (a) Bidirectional IBM alignments and their intersec-tion and (b) ITG alignments.    At this point, we extract the optimal parses from the training data, and use the word alignment im-posed by the ITG instead of the one computed by GIZA++ (Och & Ney, 2000). Training after this point is carried out according to the guidelines for the WSMT08 baseline system (see section 4.2). In Figure 1(a) is an example of a sentence aligned with GIZA++, and in Figure 1(b) is the same sen-tence, aligned with ITG. In this case it is clearly visible how the structured reordering constraints that the ITG enforces results in a clear alignment, whereas GIZA++ is unable to sort it out. 
4 Experimental setup 4.1 Data We used a subset of the data provided for the Sec-ond Workshop on Statistical Machine Translation2, which consists mainly of texts from the Europarl corpus (Koehn, 2005). We used the Europarl part for the translation tasks: German?English (de-en), Spanish?English (es-en), and French?English (fr-en). Table 2 summarizes the datasets used for training. For tuning and testing, the tuning and de-velopment test sets provided for the workshop were used ? each measuring 2,000 sentence pairs. 4.2 Baseline system For baseline system we trained phrase-based SMT models with GIZA++ (Och & Ney, 2000), the training scripts supplied with Moses (Koehn et al,                                                            2 www.statmt.org/wmt08 
33
2007), and minimum error rate training (MERT, Och, 2003), all according to the WSMT08-guidelines for baseline systems. This means that 5 iterations are carried out with IBM model 1 train-ing, 5 iterations with HMM training, 3 iterations of IBM model 3 training, and finally 3 iterations of IBM model 4 training. After GIZA++ training, the Moses training script extracts and scores phrases, and establishes a lexicalized reordering model. The WSMT08 guidelines call for the combina-tion heuristic ?grow-diag-final-and? (GDFA). We also tried the ?intersect? combination heuristic, which simply calculates the intersection of align-ment points in the two directed alignments pro-vided by GIZA++. 4.3 SBITG system Since imposing an SBITG biparse on a sentence pair forces a word alignment on the sentence pair, word alignment under SBITG models is identical to biparsing. Expectation-maximization training was used to induce a SBITG from the training data. Training is halted when the EM-process started to converge. In our experience, convergence typically requires no more than 3 iterations or so. When EM training is finished, we extracted the optimal biparses from the training data, which then constitute the optimal alignment given the grammar. This alignment was then output in GIZA++ format. All singletons from the SBITG alignment were converted to be null-alignments in the GIZA++ formatted file. These files could then be used instead of GIZA++ in the remainder of the training process for the phrase-based translation system. Although the results from the ITG are inter-preted as two directed alignments, they are identi-cal, both with each other and the intersection. Trying different combination heuristics for these results always yields the same results. The training process was identical save for the fact that the word alignments were produced by SBITGs rather than by GIZA++. 5 Experimental results We trained a total of nine systems (three tasks and three different alignments), which we evaluated with three different measures: BLEU (Papineni et al, 2002), NIST (Doddington, 2002), and METEOR (Lavie & Agarwal 2007). 
Figure 2 shows a sentence pair as it was aligned with the two different models. Figure 2(a) shows the GIZA++ alignment in both directions, and the intersection between them, whereas Figure 2(b) shows the SBITG alignment with its common structure. The asymmetric reordering mechanism of the IBM models is simply unable to relate the two halves to one another. The segment zur kenntnis genommen could certainly be said to mean note, but as a verb, and not as a noun, which is the current usage of the word. This is an inherent problem of the asymmetry of the IBM models, which is rectified by simultaneous alignment. Figure 3 shows another sentence pair. Again, Figure 3(a) was aligned with GIZA++ and Figure 3(b) with the SITG model. This shows a case with perhaps even more structured reordering, where a notion of constituency is definitely needed to get it right. SITG handles constituency, and gets this is-sue right. The IBM models do not, resulting in the error of aligning either to aufgerufen. As mentioned before, the GDFA heuristic is ap-plied after the word alignment process, and it does fix some of these problems. Therefore we opted to evaluate this, not on alignments, but rather on translation quality of phrase based SMT systems derived from the alignments. Our empirical results confirm that SBITG alignments do indeed lead to better translation quality, as shown in Table 2. We also tried the intersect combination heuris-tic, and depending on language pair and evaluation metric, the GDFA and intersect heuristics come out on top. The ITG approach is, however, consistently better than either of the heuristics applied to GIZA++ output. 6 Discussion There are of course fundamental differences be-tween ITG and IBM models. The main difference is that IBM models are directed and surface ori-ented, whereas the ITG model is joint and struc-tured. The directedness means that the IBM models are unable to produce a word alignment that is op-timal for a sentence pair; they can only produce word alignments that are optimal when translating from one language into the other. An ITG on the other hand is capable of producing the optimal alignment that explains both sentences in the pair. We see this phenomenon clearly in Figures 1?3. 
34
BLEU NIST METEOR GIZA++ GIZA++ GIZA++  GDFA inters. SBITG GDFA inters. SBITG GDFA inters. SBITG de-en 20.59 20.69 21.13 5.8668 5.8623 5.9380 0.4969 0.4953 0.5029 es-en 25.97 26.33 26.63 6.6352 6.6793 6.7407 0.5599 0.5582 0.5612 fr-en 26.03 26.17 26.63 6.6907 6.7071 6.8151 0.5544 0.5560 0.5635  Table 2: Results. The best result on each task/metric combination is in bold digits. (The identical results for SBITG on Spanish?English and French?English are not typos.)  IBM models are also built to allow for fairly ?whimsical? reorderings, which are not modeled very well to begin with. This allows for far too many degrees of freedom to fit the model to the data. Because natural languages are inherently structural, this excess degree of freedom could hurt performance. Some restraints are needed. ITGs on the other hand only allow for compositionally structured reordering, which corresponds better to the reorderings between natural languages. There are some issues with ITG as well, one of them be-ing that all permutations are actually not allowed, even if structured. This has led to some problems when an a prior alignment or structure is forced upon a sentence pair, but using unrestricted expec-tation-maximization means that the sentence pair is fitted to the grammar, and what the grammar can-not express is not applied to the data. Even if ITG proves to be too restrictive in the future, the fact that it bases reordering on structure, rather than unrestricted lexical movement, gives it an edge over the IBM models. The benefits of structured reordering as opposed to unrestricted are clearly visible in Figures 1?3. An argument to continue using IBM models is that two directed alignments can be intersected and heuristically grown to build a joint alignment, thus compensating for the flaws in the original models. But as we have seen in Figure 3, even the combi-nation of two models contains errors that should have been avoided. This approach is not able to smooth over the flaws of the IBM models. The results in this paper give credibility to the claim that these limitations of the IBM models are so serious that they hurt translation quality of sys-tems built upon them; even after the phrase build-ing heuristic has been applied. Systems built on ITG alignment on the other hand fare better, on all three evaluation metrics. 
There is still more to be done. So far we have only employed bracketing SITGs, which are not able to distinguish one structure form another. The structural changes that the SBITG is capable of are dictated by the alignment of the leaves in the tree. This seems impressive, given the information at hand, but is really a logical conclusion of the fact that the grammar can leverage different alignment probabilities against each other, and as the align-ment is coupled to the structure of the ITG parse, the structure is constrained to the alignment. The reverse is also true: the alignment is constrained by the structure. This coupling is essential to the train-ing of SITGs. For a SBITG, there is very little in-formation in the structure, only the decision to read the node as straight or inverted. This is not an in-herent property of ITGs in general; more informa-tion can be carried higher up in the tree by labeling the nonterminals. There is great hope that adding more information to the structuring, even better alignments could be gained. In this paper we have extracted the word align-ments from ITG biparses, and inserted them into the conventional phrase-based SMT pipeline. It is feasible to extract phrases directly from the gram-mar, as demonstrated by Cherry & Lin (2007). Our results suggest that augmenting other portions of the phrase-based SMT framework with ITG struc-tures might also be worth exploring, in particular decoding. Recall that in the transduction view of transduction grammars (as opposed to generative or recognition views), an output translation can be determined by parsing an input sentence with a transduction grammar (Wu 1996; Wu & Wong 1998). This kind of translation would also entail the notion of structure that we have just witnessed helping alignment. Phrase-based SMT currently relies on unrestricted phrasal movement, which is a lot better than unrestricted lexical movement, but could probably use some structure as well. 
35
7 Conclusion We have shown that learning word alignments through a compositionally-structured, joint process yields higher phrase-based translation accuracy than the conventional heuristic of intersecting con-ditional models. The conventional method with IBM-models suf-fers from their directionality. The asymmetry causes bad alignments. We have instead introduced an automatically induced ITG alignment that does not suffer from this asymmetry, and is able to ex-plain the two sentences simultaneously rather than one in terms of the other. The IBM-models also suffers from a simplified reordering model, which relies on moving individual words. The hierarchi-cal structure of ITGs means that even a BITG has enough structural information to outperform the IBM models. Previous work shows that these ad-vantages translate into better alignments as meas-ured against a manually annotated gold standard using alignment error rate (AER). Previous work also shows that AER is a poor indicator of whether translation quality is increased. We have showed that the increase in alignment quality actually translates into an increase in translation quality in this case, as measured by BLEU, NIST and METEOR across three different language pairs. Acknowledgments This material is based upon work supported in part by the Swedish National Graduate School of Language Technology, the Olof Gjerd-mans Travel Grant, the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and the Hong Kong Research Grants Council (RGC) under research grants GRF621008, DAG03/04.EG09, RGC6256/00E, and RGC6083/99E. Any opinions, findings and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. References AHO, Alfred V. & Jeffrey D. ULLMAN (1969) ?Syntax-directed trans-lations and the pushdown assembler? in Journal of Computer and System Sciences 3: 37?56. AHO, Alfred V. & Jeffrey D. ULLMAN (1972) The Theory of Parsing, Translation, and Compiling (Volumes 1 and 2). Englewood Cliffs, NJ: Prentice-Hall. AYAN, Necip Fazil & Bonnie J. DORR (2006) ?Going Beyond AER: An Extensive Analysis of Word Alignments and Their Impact on MT? in COLING-ACL?06, pp. 9?16, Sydney, Australia, July 2006. BROWN, Peter F., Stephen A. DELLA PIETRA, Vincent J. DELLA PIETRA & Robert L. MERCER (1993) ?The Mathematics of Statis-tical Machine Translation? in Computational Linguistics 19(2): 263?311.  CHAO, Wen-Han & Zhou-Jun LI (2007) ?Incorporating Constituent Structure Constraint into Discriminative Word Alignment? in MT Summit XI, pp. 97?103, Copenhagen, Denmark. 
CHERRY, Colin & Dekang LIN (2007) ?Inversion Transduction Grammar for Joint Phrasal Translation Modeling? in Proceedings of SSST, pp. 17?24, Rochester, New York, April 2007. CHIANG, David (2005) ?A Hierarchical Phrase-Based Model for Sta-tistical Machine Translation? in ACL-2005, pp. 263?270, Ann Ar-bor, MI, June 2005. KOEHN, Philipp, Franz Josef OCH & Daniel MARCU (2003) ?Statisti-cal Phrase-based Translation? in HLT-NAACL?03, pp. 127?133. KOEHN, Philipp (2005) ?Europarl: A Parallel Corpus for Statistical Machine Translation? in MT Summit X, Phuket, Thailand, Septem-ber 2005. DODDINGTON, George (2002) ?Automatic Evaluation of Machine Translation Quality using n-gram Co-occurrence Statistics? in HLT-2002. San Diego, California. KOEHN, Philipp, Hieu HOANG, Alexandra BIRCH, Chris CALLISON-BURCH, Marcello FEDERICO, Nicola BERTOLDI, Brooke COWAN, Wade SHEN, Christine MORAN, Richard ZENS, Chris DYER, On-drej BOJAR, Alexandra CONSTANTIN & Evan HERBST (2007) ?Moses: Open Source Toolkit for Statistical Machine Translation? in ACL?07, Prague, Czech Republic, June 2007.  LAVIE, Alon & Abhaya AGARWAL (2007) ?METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgment? in WSMT. Prague, Czech Republic, June 2007. LEWIS, Philip M. & Richard E. STEARNS. (1968) ?Syntax-directed transduction? in Journal of the ACM 15: 465?488. OCH, Franz Josef & Hermann NEY (2000) ?Improved Statistical Alignment Models? in ACL-2000, pp. 440?447, Hong Kong, Oc-tober 2000. OCH, Franz Josef (2003) ?Minimum error rate training in statistical machine translation? in ACL?03. OCH, Franz Josef & Hermann NEY (2003) ?A Systematic Comparison of Various Statistical Alignment Models? in Computational Lin-guistics 29(1), pp. 19?52. PAPINENI, Kishore, Salim ROUKOS, Todd WARD & Wei-Jing ZHU (2002) ?BLEU: a Method for Automatic Evaluation of Machine Translation? in ACL?02, pp 311-318. Philadelphia, Pennsylvania. S?NCHEZ, J. A., J.M. BENED? (2006) ?Stochastic Inversion Transduc-tion Grammars for Obtaining Word Phrases for Phrase-based Sta-tistical Machine Translation? in WSMT, pp. 130?133, New York City, June 2006. SIMA?AN, Khalil & Markos MYLONAKIS (2008) ?Better Statistical Estimation Can Benefit all Phrases in Phrase-based Statistical Ma-chine Translation? in SLT 2008, pp. 237?240, Goa, India, Decem-ber 2008. VOGEL, Stephan, Hermann NEY & Christoph TILLMANN (1996) ?HMM-based Word Alignment in Statistical Translation? in COLING?96, pp. 836?841. WU, Dekai (1995a) ?An Algorithm for Simultaneously Bracketing Parallel Texts by Aligning Words? in ACL?95, pp. 244?251, Cam-bridge, Massachusetts, June 1995. WU, Dekai (1995b) ?Trainable Coarse Bilingual Grammars for Paral-lel Text Bracketing? in WVLC-3, pp. 69?82, Cambridge, Massa-chusetts, June 1995. WU, Dekai (1996) ?A polynomial-time algorithm for statistical ma-chine translation? in ACL-96, Santa Cruz, CA: June 1996. WU, Dekai (1997) ?Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora? in Computational Linguis-tics 23(3), pp 377?403. WU, Dekai & Hongsing WONG (1998) ?Machine Translation with a Stochastic Grammatical Channel? in COLING-ACL'98, Montreal, August 1998.  ZHANG, Hao & Daniel GILDEA (2005) ?Stochastic Lexicalized Inver-sion Transduction Grammar for Alignment? in ACL?05, pp. 475?482, Ann Arbor, June 2005. ZHANG, Hao, Liang HUANG, Dan GILDEA & Kevin KNIGHT (2006) ?Synchronous Binarization for Machine Translation? in HLT/NAACL-2006, pp. 256?263, New York, June 2006. ZHAO, Bing & Stephan VOGEL (2003) ?Word Alignment Based on Bilingual Bracketing? in HLT-NAACL Workshop: Building and Using Parallel Texts, pp. 15?18, Edmonton, May?June 2003. 
36
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 29?32,
Paris, October 2009. c?2009 Association for Computational Linguistics
Learning Stochastic Bracketing Inversion Transduction Grammars
with a Cubic Time Biparsing Algorithm
Markus SAERS Joakim NIVRE
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
We present a biparsing algorithm for
Stochastic Bracketing Inversion Transduc-
tion Grammars that runs in O(bn3) time
instead of O(n6). Transduction gram-
mars learned via an EM estimation proce-
dure based on this biparsing algorithm are
evaluated directly on the translation task,
by building a phrase-based statistical MT
system on top of the alignments dictated
by Viterbi parses under the induced bi-
grammars. Translation quality at different
levels of pruning are compared, showing
improvements over a conventional word
aligner even at heavy pruning levels.
1 Introduction
As demonstrated by Saers & Wu (2009) there
is something to be gained by applying structural
models such as Inversion Transduction Grammars
(ITG) to the problem of word alignment. One is-
sue is that na??ve methods for inducing ITGs from
parallel data can be very time consuming. We in-
troduce a parsing algorithm for inducing Stochas-
tic Bracketing ITGs from parallel data in O(bn3)
time instead ofO(n6), where b is a pruning param-
eter (lower = tighter pruning). We try out different
values for b, and evaluate the results on a transla-
tion tasks.
In section 2 we summarize the ITG framework;
in section 3 we present our algorithm, whose time
complexity is analyzed in section 4. In section 5
we describe how the algorithm is evaluated, and in
section 6, the empirical results are given.
2 Inversion Transduction Grammars
Inversion transductions are a theoretically inter-
esting and empirically useful equivalence class of
transductions, with expressiveness and computa-
tional complexity characteristics lying intermedi-
ate between finite-state transductions and syntax-
directed transductions. An Inversion Transduc-
tion Grammar (ITG) can be used to synchronously
generate sentence pairs, synchronously parse sen-
tence pairs, or transduce from a sentence in one
language to a sentence in another.1
The equivalence class of inversion transduc-
tions can be described by restricting Syntax-
Directed Transduction Grammars (SDTG)2 in var-
ious equivalent ways to the special cases of (a) bi-
nary SDTGs, (b) ternary SDTGs, or (c) SDTGs
whose transduction rules are restricted to straight
and inverted permutations only.
Thus on one hand, any binary or ternary SDTG
is an ITG. Conversely, any ITG can be stated in
binary two-normal form (Wu, 1997). Only three
kinds of rules are present in the normal form:
A? [BC]
A? ?BC?
A? e/f
On the other hand, under characterization (c),
what distinguishes ITGs is that the permutation of
constituents is restricted in such a way that all chil-
dren of a node must be read either left-to-right, or
right-to-left. The movement only applies to one of
the languages, the other is fixed. Formally, an ITG
is a tuple ?N,V,?, S?, where N is a set of nonter-
minal symbols, ? is a set of rewrite rules, S ? N
is the start symbol and V ? VE ? VF is a set of
biterminal symbols, where VE is the vocabulary of
E and VF is the vocabulary of F . We will write a
biterminal as e/f , where e ? VE and f ? VF . A
sentence pair will be written as e/f , and a bispan
as es..t/fu..v.
Each rule ? ? ? is a tuple ?X, ?, ?? where
X ? N is the right hand side of the rule, ? ?
1All transduction grammars (a.k.a. synchronous gram-
mars, or simply bigrammars) can be interpreted as models
for generation, recognition, or transduction.
2SDTGs (Lewis & Stearns (1968); Aho & Ullman (1969),
(1972)) are also recently called synchronous CFGs.
29
{N ? V }? is a series of nonterminal and biter-
minal symbols representing the production of the
rule and ? ? {?, [], ??} denotes the orientation (ax-
iomatic, straight or inverted) of the rule. Straight
rules are read left-to-right in both languages, while
inverted rules are read left-to-right in E and right-
to-left in F . The direction of the axiomatic rules is
undefined, as they must be completely made up of
terminals. For notational convenience, the orien-
tation of the rule is written as surrounding the pro-
duction, like so: X ? ?, X ? [?] and X ? ???.
The vocabularies of the languages may both in-
clude the empty token , allowing for deletions
and insertions. The empty biterminal, / is not
allowed.
2.1 Stochastic ITGs
In a Stochastic ITG (SITG), each rule is also asso-
ciated with a probability, such that
?
?
Pr(X ? ?) = 1
for all X ? N . The probability of a deriva-
tion S ?? e/f is defined as the production of
the probabilities of all rules used. As shown by
Wu (1995), it is possible to fit the parameters of
a SITG to a parallel corpus via EM (expectation-
maximization) estimation.
2.2 Bracketing ITGs
An ITG where there is only one nonterminal (other
than the start symbol) is called a bracketing ITG
(BITG). Since the one nonterminal is devoid of
information, it can only be used to group its chil-
dren together, imposing a bracketing on the sen-
tence pairs.
3 Parsing SBITGs
In this section we present a biparsing algorithm
for Stochastic Bracketing Inversion Transduction
Grammars (SBITGs) in normal form which incor-
porates a pruning parameter b. The algorithm is
basically an agenda-based bottom-up chart parser,
where the pruning parameter controls the number
of active items of a given length.
To parse a sentence pair e/f , the parser needs
a chart C and a series of T + V agendas
A1, A2, . . . , AT+V , where T = |e| and V = |f |.
An item is defined as a nonterminal symbol (we
use X to denote the anonymous nonterminal sym-
bol of the bracketing ITG) and one span in each
language, written as Xstuv where 0 ? s ? t ? T
corresponds to the span es..t and 0 ? u ? v ? V
corresponds to the span fu..v. The length of an
item is defined as |Xstuv| = (t?s)+(v?u). Since
items are grouped by their length, highly skewed
links (eg. 6:1) will be competing with very even
links (eg. 4:3). Skewed links are generally bad
(and should be pruned), or have a high probability
(which means they are likely to survive pruning).
An item may be active or passive, the active items
are present in the agendas and the chart, whereas
the passive items are only present in the chart.
The parser starts by asserting items from all lex-
ical rules (X ? e/f ), and placing them on their
respective agendas. After the initial seeding, the
agendas are processed in order. When an agenda
is processed, it is first pruned, so that only the b
best items are kept active. After pruning, the re-
maining active items are allowed to be extended.
When extended, the item combines with an adja-
cent item in the chart to form a larger item. The
newly created item is considered active, and added
to both the chart and the appropriate agenda. Once
an item has been processed it goes from being ac-
tive to being passive. The process is halted when
the goal item S0T0V is reached, or when no active
items remain. To build the forest corresponding to
the parse process, back-pointers are used.
3.1 Initialization
In the initial step, the set of lexical items L is built.
All lexical items i ? L are then activated by plac-
ing them on their corresponding agenda A|i|.
L =
?
?
?Xstuv
??????
0?s? t?T,
0?u?v?V,
X ? es..t/fu..v ? ?
?
?
?
By limiting the length of phrasal terminals to some
threshold ?, the variables t and v can be limited to
s+? and u+? respectively, limiting the complexity
of the initialization step from O(n4) to O(n2).
3.2 Recursion
In the recursive step we build a set of extensions
E(i) for all active items i. All items in E(i)
are then activated by placing them on their cor-
responding agenda (i ? A|i|).
E(Xstuv) =
{XStUv|0?S?s, 0?U?u,XSsUu ? C} ?
{XsSuU |t?S?T, v?U?V,XtSvU ? C} ?
{XsSUv|t?S?T, 0?U?u,XtSUu ? C} ?
{XStuU |0?S?s, v?U?V,XSsvU ? C}
30
Since we are processing the agendas in order, any
item in the chart will be as long as or shorter than
the item being extended. This fact can be exploited
to limit the number of possible siblings explored,
but has no impact on time complexity.
3.3 Viterbi parsing
When doing Viterbi parsing, all derivations but
the most probable are discarded. This gives an
unambiguous parse, which dictates exactly one
alignment between e and f . The alignment of
the Viterbi parse can be used to substitute that of
other word aligners (Saers and Wu, 2009) such as
GIZA++ (Och and Ney, 2003).
4 Analysis
Looking at the algorithm, it is clear that there will
be a total of T + V = O(n) agendas, each con-
taining items of a certain length. The items in an
agenda can start anywhere in the alignment space:
O(n2) possible starting points, but once the end
point in one language is set, the end point in the
other follows from that, adding a factor O(n).
This means that each agenda contains O(n3) ac-
tive items. Each active item has to go through all
possible siblings in the recursive step. Since the
start point of the sibling is determined by the item
itself (it has to be adjacent), only the O(n2) pos-
sible end points have to be explored. This means
that each active item takes O(n2) time to process.
The total time is thus O(n6): O(n) agendas,
containing O(n3) active items, requiring O(n2)
time to process. This is also the time complex-
ity reported for ITGs in previous work (Wu, 1995;
Wu, 1997).
The pruning works by limiting the number of
active items in an agenda to a constant b, meaning
that there are O(n) agendas, containing O(b) ac-
tive items, requiring O(n2) time to process. This
gives a total time complexity of O(bn3).
5 Evaluation
We evaluate the parser on a translation task
(WMT?08 shared task3). In order to evaluate on
a translation task, a translation system has to be
built. We use the alignments from the Viterbi
parses of the training corpus to substitute the
alignments of GIZA++. This is the same approach
as taken in Saers & Wu (2009). We will evalu-
ate the resulting translations with two automatic
3http://www.statmt.org/wmt08/
metrics: BLEU (Papineni et al, 2002) and NIST
(Doddington, 2002).
6 Empirical results
In this section we describe the experimental setup
as well as the outcomes.
6.1 Setup
We use the Moses Toolkit (Koehn et al, 2007) to
train our phrase-based SMT models. The toolkit
also includes scripts for applying GIZA++ (Och
and Ney, 2003) as a word aligner. We have
trained several systems, one using GIZA++ (our
baseline system), one with no pruning at all, and
6 different values of b (1, 10, 25, 50, 75 and
100). We used the grow-diag-final-and
method to extract phrases from the word align-
ment, and MERT (Och, 2003) to optimize the re-
sulting model. We trained a 5-gram SRI language
model (Stolcke, 2002) using the corpus supplied
for this purpose by the shared task organizers. All
of the above is consistent with the guidelines for
building a baseline system for the WMT?08 shared
task.
The translation tasks we applied the above
procedure to are all taken from the Europarl
corpus (Koehn, 2005). We selected the tasks
German-English, French-English and Spanish-
English. Furthermore, we restricted the training
sentence pairs so that none of the sentences ex-
ceeded length 10. This was necessary to be able to
carry out exhaustive search. The total amount of
training data was roughly 100,000 sentence pairs
in each language pair, which is a relatively small
corpus, but by no means a toy example.
6.2 Grammar induction
It is possible to set the parameters of a SBITG
by applying EM to an initial guess (Wu, 1995).
As our initial guess, we used word co-occurrence
counts, assuming that there was one empty token
in each sentence. This gave an estimate of the lex-
ical rules. The probability mass was divided so
that the lexical rules could share half of it, while
the other half was shared equally by the two struc-
tural rules (X ? [XX] and X ? ?XX?).
Several training runs were made with different
pruning parameters. The EM process was halted
when a relative improvement in log-likelihood of
10?3 was no longer achieved over the previous it-
eration.
31
Baseline Different values of b for SBITGs
Metric (GIZA++) ? 100 75 50 25 10 1
Spanish-English
BLEU 0.2597 0.2663 0.2671 0.2661 0.2653 0.2655 0.2608 0.1234
NIST 6.6352 6.7407 6.7445 6.7329 6.7101 6.7312 6.6439 3.9705
time 03:20:00 02:40:00 02:00:00 01:20:00 00:38:00 00:17:00 00:03:10
German-English
BLEU 0.2059 0.2113 0.2094 0.2091 0.2090 0.2091 0.2050 0.0926
NIST 5.8668 5.9380 5.9086 5.8955 5.8947 5.9292 5.8743 3.4297
time 03:40:00 02:45:00 02:10:00 01:25:00 00:41:00 00:17:00 00:03:20
French-English
BLEU 0.2603 0.2663 0.2655 0.2668 0.2669 0.2654 0.2632 0.1268
NIST 6.6907 6.8151 6.8068 6.8068 6.8065 6.7013 6.7136 4.0849
time 03:10:00 02:45:00 02:10:00 01:25:00 00:42:00 00:17:00 00:03:25
Table 1: Results. Time measures are approximate time per iteration.
Once the EM process terminated, Viterbi parses
were calculated for the training corpus, and the
alignments from them outputted in the same for-
mat produced by GIZA++.
6.3 Results
The results are presented in Table 1. GIZA++
generally terminates within minutes (6?7) on the
training corpora used, making it faster than any
of the SBITGs (they generally required 4?6 iter-
ations to terminate, making even the fastest ones
slower than GIZA++). To put the times in per-
spective, about 6 iterations were needed to get
the ITGs to converge, making the longest training
time about 16?17 hours. The time it takes to ex-
tract the phrases and tune the model using MERT
is about 14 hours for these data sets.
Looking at translation quality, we see a sharp
initial rise as b grows to 10. At this point the
SBITG system is on par with GIZA++. It con-
tinues to rise up to b = 25, but after that is more or
less levels out. From this we conclude that the pos-
itive results reported in Saers & Wu (2009) hold
under harsh pruning.
7 Conclusions
We have presented a SBITG biparsing algorithm
that uses a novel form of pruning to cut the com-
plexity of EM-estimation from O(n6) to O(bn3).
Translation quality using the resulting learned
SBITG models is improved over using conven-
tional word alignments, even under harsh levels of
pruning.
Acknowledgments
The authors are grateful for the comments made by the two anonymous review-
ers. This work was funded by the Swedish National Graduate School of Lan-
guage Technology, the Defense Advanced Research Projects Agency (DARPA)
under GALE Contract No. HR0011-06-C-0023, and the Hong Kong Research
Grants Council (RGC) under research grants GRF621008, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the authors and do
not necessarily reflect the views of the Defense Advanced Research Projects
Agency.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax-directed translations
and the pushdown assembler. Journal of Computer and System Sciences,
3(1):37?56.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Transla-
tion, and Compiling (Volumes 1 and 2). Prentice-Halll, Englewood Cliffs,
NJ.
George Doddington. 2002. Automatic evaluation of machine translation qual-
ity using n-gram co-occurrence statistics. In Human Language Technology
conference (HLT-2002), San Diego, CA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine trans-
lation. In ACL-2007 Demo and Poster Sessions, pages 177?180, Prague,
Jun.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine trans-
lation. In Machine Translation Summit X, Phuket, Thailand, September.
Philip M. Lewis and Richard E. Stearns. 1968. Syntax-directed transduction.
Journal of the Association for Computing Machinery, 15(3):465?488.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum error rate training in statistical machine
translation. In 41st Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of machine translations. In 40th Annual
Meeting of the Association for Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, Jul.
Markus Saers and Dekai Wu. 2009. Improving phrase-based translation via
word alignments from Stochastic Inversion Transduction Grammars. In
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statis-
tical Translation (at NAACL HLT 2009), pages 28?36, Boulder, CO, Jun.
Andreas Stolcke. 2002. SRILM ? an extensible language modeling toolkit.
In International Conference on Spoken Language Processing, Denver, CO,
Sep.
Dekai Wu. 1995. Trainable coarse bilingual grammars for parallel text brack-
eting. In Third Annual Workshop on Very Large Corpora (WVLC-3), pages
69?81, Cambridge, MA, Jun.
Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and bilingual
parsing of parallel corpora. Computational Linguistics, 23(3):377?404,
Sep.
32
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 102?112,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning to Freestyle: Hip Hop Challenge-Response Induction via
Transduction Rule Segmentation
Dekai Wu Karteek Addanki Markus Saers Meriem Beloucif
Human Language Technology Center
Department of Computer Science
HKUST, Clear Water Bay, Hong Kong
{dekai|vskaddanki|masaers|mbeloucif}@cs.ust.hk
Abstract
We present a novel model, Freestyle, that
learns to improvise rhyming and fluent re-
sponses upon being challenged with a line of
hip hop lyrics, by combining both bottom-
up token based rule induction and top-down
rule segmentation strategies to learn a stochas-
tic transduction grammar that simultaneously
learns both phrasing and rhyming associations.
In this attack on the woefully under-explored
natural language genre of music lyrics, we
exploit a strictly unsupervised transduction
grammar induction approach. Our task is par-
ticularly ambitious in that no use of any a pri-
ori linguistic or phonetic information is al-
lowed, even though the domain of hip hop
lyrics is particularly noisy and unstructured.
We evaluate the performance of the learned
model against a model learned only using
the more conventional bottom-up token based
rule induction, and demonstrate the superi-
ority of our combined token based and rule
segmentation induction method toward gen-
erating higher quality improvised responses,
measured on fluency and rhyming criteria as
judged by human evaluators. To highlight
some of the inherent challenges in adapting
other algorithms to this novel task, we also
compare the quality of the responses generated
by our model to those generated by an out-of-
the-box phrase based SMT system. We tackle
the challenge of selecting appropriate training
data for our task via a dedicated rhyme scheme
detection module, which is also acquired via
unsupervised learning and report improved
quality of the generated responses. Finally,
we report results with Maghrebi French hip
hop lyrics indicating that our model performs
surprisingly well with no special adaptation to
other languages.
1 Introduction
The genre of lyrics in music has been severely under-
studied from the perspective of computational lin-
guistics despite being a form of language that has
perhaps had the most impact across almost all human
cultures. With the motivation of spurring further re-
search in this genre, we apply stochastic transduc-
tion grammar induction algorithms to address some
of the modeling issues in song lyrics. An ideal start-
ing point for this investigation is hip hop, a genre
that emphasizes rapping, spoken or chanted rhyming
lyrics against strong beats or simple melodies. Hip
hop lyrics, in contrast to poetry and other genres of
music, present a significant number of challenges for
learning as it lacks well-defined structure in terms of
rhyme scheme, meter, or overall meaning making it
an interesting genre to bring to light some of the less
studied modeling issues.
The domain of hip hop lyrics is particularly un-
structured when compared to classical poetry, a do-
main on which statistical methods have been applied
in the past. Hip hop lyrics are unstructured in the
sense that a very high degree of variation is permit-
ted in the meter of the lyrics, and large amounts of
colloquial vocabulary and slang from the subculture
are employed. The variance in the permitted me-
ter makes it hard to make any assumptions about
the stress patterns of verses in order to identify the
rhyming words used when generating output. The
broad range of unorthodox vocabulary used in hip
hop make it difficult to use off-the-shelf NLP tools
for doing phonological and/or morphological analy-
sis. These problems are further exacerbated by dif-
ferences in intonation of the same word and lack of
robust transcription (Liberman, 2010).
102
We argue that stochastic transduction grammars,1
given their success in the area of machine transla-
tion and efficient unsupervised learning algorithms,
are ideal for capturing the structural relationship be-
tween lyrics. Hence, our Freestyle system mod-
els the problem of improvising a rhyming response
given any hip hop lyric challenge as transducing
a challenge line into a rhyming response. We
use a stochastic transduction grammar induced in
a completely unsupervised fashion using a combi-
nation of token based rule induction and segment-
ing (Saers et al, 2013) as the underlying model to
fully-automatically learn a challenge-response sys-
tem and compare its performance against a simpler
token based transduction grammar model. Both our
models are completely unsupervised and use no prior
phonetic or linguistic knowledge whatsoever despite
the highly unstructured and noisy domain.
We believe that the challenge-response system
based on an interpolated combination of token based
rule induction and rule segmenting transduction
grammars will generate more fluent and rhyming re-
sponses compared to one based on token based trans-
duction grammars models. This is based on the ob-
servation that token based transduction grammars
suffer from a lack of fluency; a consequence of the
degree of expressivity they permit. Therefore, as a
principal part of our investigation we compare the
quality of responses generated using a combination
of token based rule induction and top-down rule seg-
menting transduction grammars to those generated
by pure token based transduction grammars.
We also hypothesize that in order to generate flu-
ent and rhyming responses, it is not sufficient to train
the transduction grammars on all adjacent lines of a
hip hop verse. Therefore, we propose a data selec-
tion scheme using a rhyme scheme detector acquired
through unsupervised learning to generate the train-
ing data for the challenge-response systems. The
rhyme scheme detector segments each verse of a hip
hop song into stanzas and identifies the lines in each
stanza that rhyme with each other which are then
added as training instances. We demonstrate the su-
periority of our training data selection method by
comparing the quality of the responses generated by
the models trained on data selected with and without
1Also known in SMT as ?synchronous grammars?.
using the rhyme scheme detector.
Unlike conventional spoken and written language,
disfluencies and backing vocals2 occur very fre-
quently in the domain of hip hop lyrics which af-
fect the performance of NLP models designed for
processing well-formed sentences. We propose two
strategies to mitigate the effect of disfluencies on our
model performance and compare their efficacy using
human evaluations. Finally, in order to illustrate the
challenges faced by other NLP algorithms, we con-
trast the performance of our model against a conven-
tional, widely used phrase-based SMT model.
A brief terminological note: ?stanza? and ?verse?
are frequently confused and sometimes conflated.
Worse yet, their usage for song lyrics is often con-
tradictory to that for poetry. To avoid ambiguity
we consistently follow these technical definitions for
segments in decreasing size of granularity:
verse a large unit of a song?s lyrics. A song typi-
cally contains several verses interspersed with
choruses. In the present work, we do not differ-
entiate choruses from verses. In song lyrics, a
verse is most commonly represented as a sepa-
rate paragraph.
stanza a segment within a verse which has a me-
ter and rhyme scheme. Stanzas often consist of
2, 3, or 4 lines, but stanzas of more lines are
also common. Particularly in hip hop, a single
verse often contains many stanzas with differ-
ent rhyme schemes and meters.
line a segment within a stanza consisting of a single
line. In poetry, strictly speaking this would be
called a ?verse?, which however conflicts with
the conventional use of ?verse? in song lyrics.
In Section 2, we discuss some of the previous
work that applies statistical NLP methods to less
conventional domains and problems. We describe
our experimental conditions in Section 3. We com-
pare the performance of token and segment based
transduction grammar models in Section 4. We com-
pare our data selection schemes and disfluency han-
dling strategies in Sections 5 and 6. Finally, in
2Particularly the repetitive chants, exclamations, and inter-
jections in hip hop ?hype man? style backing vocals.
103
Section 7 we describe some preliminary results ob-
tained using our approach on improvising hip hop
responses in French and conclude in Section 8.
2 Related work
Although a few attempts have been made to apply
statistical NLP learning methods to unconventional
domains, Freestyle is among the first to tackle the
genre of hip hop lyrics (Addanki and Wu, 2013; Wu
et al, 2013a,b). Our preliminary work suggested the
need for further research to identify models that cap-
ture the correct generalizations to be able to gener-
ate fluent and rhyming responses. As a step towards
this direction, we contrast the performance of inter-
polated bottom-up token based rule induction and
top-down segmenting transduction grammar models
and token based transduction grammar models. We
briefly describe some of the past work in statistical
NLP on unconventional domains below.
Most of the past work either uses some form of
prior linguistic knowledge or enforces harsher con-
straints such as set number of words in a line, or a set
meter which are warranted by more structured do-
mains such as poetry. However, in hip hop lyrics it
is hard to make any linguistic or structural assump-
tions. For example, words such as sho, flo, holla
which frequently appear in the lyrics are not part of
any standard lexicon and hip hop does not require a
set number of syllables in a line, unlike poems. Also,
surprising and unlikely rhymes in hip hop are fre-
quently achieved via intonation and assonance, mak-
ing it hard to apply prior phonological constraints.
A phrase based SMT systemwas trained to ?trans-
late? the first line of a Chinese couplet or duilian
into the second by Jiang and Zhou (2008). The most
suitable next line was selected by applying linguistic
constraints to the n best output of the SMT system.
However in contrast to Chinese couplets, which ad-
here to strict rules requiring, for example, an identi-
cal number of characters in each line and one-to-one
correspondence in their metrical length, the domain
of hip hop lyrics is far more unstructured and there
exists no clear constraint that would ensure fluent
and rhyming responses to hip hop challenge lyrics.
Barbieri et al (2012) use controlled Markov pro-
cesses to semi-automatically generate lyrics that sat-
isfy the structural constraints of rhyme and meter.
Tamil lyrics were automatically generated given a
melody using conditional random fields by A. et al
(2009). The lyrics were represented as a sequence
of labels using the KNM system where K, N and M
represented the long vowels, short vowels and con-
sonants respectively.
Genzel et al (2010) used SMT in conjunction
with stress patterns and rhymes found in a pronun-
ciation dictionary to produce translations of poems.
Although many constraints were applied in translat-
ing full verses of poems, it was challenging to sat-
isfy all the constraints. Stress patterns were assigned
to words given the meter of a line in Shakespeare?s
sonnets by Greene et al (2010), which were then
combined with a language model to generate poems.
Sonderegger (2011) attempted to infer the pronun-
ciation of words in old English by identifying the
rhyming patterns using graph theory. However, their
heuristic of clustering words with similar IPA end-
ings resulted in large clusters of false positives such
as bloom and numb. A language-independent gener-
ative model for stanzas in poetry was proposed by
Reddy and Knight (2011) via which they could dis-
cover rhyme schemes in French and English poetry.
3 Experimental conditions
Before introducing our Freestyle models, we first
detail our experimental assumptions and the evalua-
tion scheme under which the responses generated by
different models are compared against one another.
We describe our training data as well as a phrase-
based SMT (PBSMT) contrastive baseline. We also
define the evaluation scheme used to compare the re-
sponses of different systems on criteria of fluency
and rhyming.
3.1 Training data
We used freely available user generated hip hop
lyrics on the Internet to provide training data for our
experiments. We collected approximately 52,000
English hip hop song lyrics amounting to approxi-
mately 800Mb of raw HTML content. The data was
cleaned by stripping HTML tags, metadata and nor-
malized for special characters and case differences.
The processed corpus contained 22 million tokens
with 260,000 verses and 2.7 million lines of hip hop
lyrics. As human evaluation using expert hip hop
104
listeners is expensive, a small subset of 85 lines was
chosen as the test set to provide challenges for com-
paring the quality of responses generated by different
systems.
3.2 Evaluation scheme
The performance of various Freestyle versions
was evaluated on the task of generating a improvised
fluent and rhyming response given a single line of a
hip hop verse as a challenge. The output of all the
systems on the test set was given to three indepen-
dent frequent hip hop listeners for manual evalua-
tion. They were asked to evaluate the system out-
puts according to fluency and the degree of rhyming.
They were free to choose the tune to make the lyrics
rhyme as the beats of the song were not used in the
training data. Each evaluator was asked to score the
response of each system on the criterion of fluency
and rhyming as being good, acceptable or bad.
3.3 Phrase-based SMT baseline
In order to evaluate the performance of an out-of-
the-box phrase-based SMT (PBSMT) system toward
this novel task of generating rhyming and fluent re-
sponses, a standard Moses baseline (Koehn et al,
2007) was also trained in order to compare its per-
formance with our transduction grammar induction
model. A 4-gram language model which was trained
on the entire training corpus using SRILM (Stolcke,
2002) was used to generate responses in conjunction
with the phrase-based translation model. As no au-
tomatic quality evaluation metrics exist for hip hop
responses analogous to BLEU for SMT, the model
weights cannot be tuned in conventional ways such
asMERT (Och, 2003). Instead, a slightly higher than
typical language model weight was empirically cho-
sen using a small development set to produce fluent
outputs.
4 Interpolated segmenting model vs. token
based model
We compare the performance of transduction gram-
mars induced via interpolated token based and rule
segmenting (ISTG) versus token based transduction
grammars (TG) on the task of generating a rhyming
and fluent response to hip hop challenges. We use
the framework of stochastic transduction grammars,
specifically bracketing ITGs (inversion transduction
grammars) (Wu, 1997), as our translation model for
?transducing? any given challenge into a rhyming
and fluent response. Our choice is motivated by
the significant amount of empirical evidence for the
representational capacity of transduction grammars
across a spectrum of natural language tasks such as
textual entailment (Wu, 2006), mining parallel sen-
tences (Wu and Fung, 2005) and machine translation
(Zens and Ney, 2003). Further, existence of effi-
cient learning algorithms (Saers et al, 2012; Saers
and Wu, 2011) that make no language specific as-
sumptions, make inversion transduction grammars a
suitable framework for our modeling needs. Exam-
ples of lexical transduction rules can be seen in Ta-
bles 3 and 5. In addition, the grammar also includes
structural transduction rules for the straight case
A? [A A] and also the inverted case A? <A A>.
4.1 Token based vs. segmental ITGs
The degenerate case of ITGs are token based ITGs
wherein each translation rule contains at most one
token in input and output languages. Efficient induc-
tion algorithmswith polynomial run time exist for to-
ken based ITGs and the expressivity they permit has
been empirically determined to capture most of the
word alignments that occur across natural languages.
The parameters of the token based ITGs can be es-
timated using expectation maximization through an
efficient dynamic programming algorithm in con-
junction with beam pruning (Saers and Wu, 2011).
In contrast to token based ITGs, each rule in a seg-
mental ITG grammar can contain more than one to-
ken in both input and output languages. In machine
translation applications, segmental models produce
translations that are more fluent as they can capture
lexical knowledge at a phrasal level. However, only
a handful of purely unsupervised algorithms exist
for learning segmental ITGs under matched training
and testing assumptions. Most other approaches in
SMT use a variety of ad hoc heuristics for extracting
segments from token alignments, justified purely by
short term improvements in automatic MT evalua-
tion metrics such as BLEU (Papineni et al, 2002)
which cannot be transferred to our current task. In-
stead, we use a completely unsupervised learning al-
gorithm for segmental ITGs that stays strictly within
the transduction grammar optimization framework
for both training and testing as proposed in Saers
105
et al (2013).
Saers et al (2013) induce a phrasal inversion
transduction grammar via interpolating the bottom-
up rule chunking approach proposed in Saers et al
(2012) with a top-down rule segmenting approach
driven by a minimum description length objective
function (Solomonoff, 1959; Rissanen, 1983) that
trades off the maximum likelihood against model
size. Saers et al (2013) report improvements in
BLEU score (Papineni et al, 2002) on their transla-
tion task. In our current approach instead of using a
bottom-up rule chunking approach we use a simpler
token based grammar instead. Given two grammars
(Ga and Gb) and an interpolation parameter ? the
probability function of the interpolated grammar is
given by:
pa+b (r) = ?pa (r) + (1? ?)pb (r)
for all rules r in the union of the two rule sets, and
where pa+b is the rule probability function of the
combined grammar and pa and pb are the rule prob-
ability functions of Ga and Gb respectively. The
pseudocode for the top-down rule segmenting algo-
rithm is shown in 1. The algorithm uses the methods
collect_biaffixes, eval_dl, sort_by_delta and
make_segmentations. These methods collect all the
biaffixes in an ITG, evaluate the difference in de-
scription length, sort candidates by these differences,
and commit to a given set of candidates, respectively.
The suitable interpolation parameter is chosen em-
pirically based on the responses generated on a small
development set.
We compare the performance of inducing a token
based ITG versus inducing a segmental ITG using in-
terpolated bottom-up token based rule induction and
top-down rule segmentation. To highlight some of
the inherent challenges in adapting other algorithms
to this novel task, we also compare the quality of the
responses generated by our model to those generated
by an off-the-shelf phrase based SMT system.
4.2 Decoding heuristics
We use our in-house ITG decoder implemented ac-
cording to the algorithm mentioned in Wu (1996)
for the generating responses to challenges by decod-
ing with the trained transduction grammars. The de-
coder uses a CKY-style parsing algorithm (Cocke,
Algorithm 1 Iterative rule segmenting learning
driven by minimum description length.
1: ? ? The ITG being induced
2: repeat
3: ?sum ? 0
4: bs? collect_biaffixes(?)
5: b? ? []
6: for all b ? bs do
7: ? ? eval_dl(b,?)
8: if ? < 0 then
9: b? ? [b?, ?b, ??]
10: sort_by_delta(b?)
11: for all ?b, ?? ? b? do
12: ?? ? eval_dl(b,?)
13: if ?? < 0 then
14: ?? make_segmentations(b,?)
15: ?sum ? ?sum + ??
16: until ?sum ? 0
17: return ?
1969) with cube pruning (Chiang, 2007). The de-
coder builds an efficient hypergraph structure which
is then scored using the induced grammar. The
trained transduction grammar model was decoded
using the 4-gram language model and the model
weights determined as described in 3.3.
In our decoding algorithm, we restrict the reorder-
ing to only be monotonic as we want to produce out-
put that follows the same rhyming order of the chal-
lenge. Interleaved rhyming order is harder to evalu-
ate without the larger context of the song and we do
not address that problem in our current model. We
also penalize singleton rules to produce responses of
similar length as successive lines in a stanza are typ-
ically of similar length. Finally, we add a penalty to
reflexive translation rules that map the same surface
form to itself such as A ? yo/yo. We obtain these
rules with a high probability due to the presence of
sentence pairs where both the input and output are
identical strings as many stanzas in our data contain
repeated chorus lines.
4.3 Results: Rule segmentation improves
responses
Results in Table 1 indicate that the ISTG outperforms
the TG model towards the task of generating fluent
and rhyming responses. On the criterion of fluency,
106
Table 1: Percentage of ?good and ?acceptable (i.e., either good or acceptable) responses on fluency and rhyming
criteria. PBSMT, TG and ISTG models trained using corpus generated from all adjacent lines in a verse. PBSMT+RS,
TG+RS, ISTG+RS are models trained on rhyme scheme based corpus selection strategy. Disfluency correction strategy
was used in all cases.
model fluency (?good) fluency (?acceptable) rhyming (?good) rhyming (?acceptable)
PBSMT 3.14% 4.70% 1.57% 4.31%
TG 21.18% 54.51% 23.53% 39.21%
ISTG 26.27% 57.64% 27.45% 48.23%
PBSMT+RS 30.59% 43.53% 1.96% 9.02%
TG+RS 34.12% 60.39% 20.00% 42.74%
ISTG+RS 30.98% 61.18% 30.98% 53.72%
Table 2: Transduction rules learned by ISTG model.
transduction grammar rule log prob.
A? long/wrong -11.6747
A? rhyme/time -11.6604
A? felt bad/couldn't see what i really had -11.3196
A? matter what you say/leaving anyway -11.8792
A? arhythamatic/this rhythm is sick -12.3492
the ISTGmodel produces a significantly higher frac-
tion of sentences rated good (26.27% vs. 21.18%)
and ?acceptable (57.64% vs. 54.51%). Higher frac-
tion of responses generated by the ISTG model are
rated as good (27.45% vs. 23.53%) and ?acceptable
(57.64% vs. 54.51%) compared to the TG model.
Both TG and ISTG model perform significantly bet-
ter than the PBSMT baseline. Upon inspecting the
learned rules, we noticed that the ISTG models cap-
ture rhyming correspondences both at the token and
segmental levels. Table 2 shows some examples
of the transduction rules learned by ISTG grammar
trained using rhyme scheme detection.
5 Data selection via rhyme scheme
detection vs. adjacent lines
We now compare two data selection approaches
for generating the training data for transduction
grammar induction via a rhyme scheme detection
module and choosing all adjacent lines in a verse.
We also briefly describe the training of the rhyme
scheme detection module and determine the efficacy
of our data selection scheme by training the ISTG
model, TG model and the PBSMT baseline on train-
ing data generated with and without employing the
rhyme scheme detection module. As the rule seg-
menting approach was intended to improve the flu-
ency as opposed to the rhyming nature of the re-
sponses, we only train the rule segmenting model
on the randomly chosen subset of all adjacent lines
in the verse. Further, adding adjacent lines as the
training data to the segmenting model maintains the
context of the responses generated thereby produc-
ing higher quality responses. The segmental trans-
duction grammar model was combined with the to-
ken based transduction grammar model trained on
data selected with and without using rhyme scheme
detection model.
5.1 Rhyme scheme detection
Although our approach adapts a transduction gram-
mar induction model toward the problem of generat-
ing fluent and rhyming hip hop responses, it would
be undesirable to train the model directly on all the
successive lines of the verses?as done by Jiang and
Zhou (2008)?due to variance in hip hop rhyming
patterns. For example, adding successive lines of a
stanza which follows ABAB rhyme scheme as train-
ing instances to the transduction grammar causes in-
correct rhyme correspondences to be learned. The
fact that a verse (which is usually represented as
a separate paragraph) may contain multiple stanzas
of varying length and rhyme schemes worsens this
problem. Adding all possible pairs of lines in a verse
as training examples not only introduces a lot of
noise but also explodes the size of the training data
due to the large size of the verse.
We employ a rhyme scheme detection model (Ad-
danki and Wu, 2013) in order to select training in-
stances that are likely to rhyme. Lines belonging to
107
the same stanza and marked as rhyming according
to the rhyme scheme detection model are added to
the training corpus. We believe that this data selec-
tion scheme will improve the rhyming associations
learned during the transduction grammar induction
thereby biasing the model towards producing fluent
and rhyming output.
The rhyme scheme detection model proposes a
HMM based generative model for a verse of hip hop
lyrics similar to Reddy and Knight (2011). However,
owing to the lack of well-defined verse structure in
hip hop, a number of hidden states corresponding to
stanzas of varying length are used to automatically
obtain a soft-segmentation of the verse. Each state
in the HMM corresponds to a stanza with a particu-
lar rhyme scheme such asAA,ABAB,AAAAwhile
the emissions correspond to the final words in the
stanza. We restrict the maximum length of a stanza
to be four to maintain a tractable number of states
and further only use states to represent stanzas whose
rhyme schemes could not be partitioned into smaller
schemes without losing a rhyme correspondence.
The parameters of the HMM are estimated using
the EM algorithm (Devijer, 1985) using the corpus
generated by taking the final word of each line in the
hip hop lyrics. The lines from each stanza that rhyme
with each other according to the Viterbi parse using
the trained model are added as training instances for
transduction grammar induction. As the source and
target languages are identical, each selected pair gen-
erates two training instances: a challenge-response
and a response-challenge pair.
The training data for the rhyme scheme detector
was obtained by extracting the end-of-line tokens
from each verse. However, upon data inspection we
noticed that shorter lines in hip hop stanzas are typi-
cally joined with a comma and represented as a sin-
gle line of text and hence all the tokens before the
commas were also added to the training corpus. We
obtained a corpus containing 4.2 million tokens cor-
responding to potential rhyming candidates compris-
ing of around 153,000 unique token types.
We evaluated the performance of our rhyme
scheme detector on the task of correctly labeling a
given verse with rhyme schemes. As our model is
completely unsupervised, we chose a random sam-
ple of 75 verses from our training data as our test set.
Two native English speakers who were frequent hip
hop listeners were asked to partition the verse into
stanzas and assign them with a gold standard rhyme
scheme. Precision and recall were aggregated for the
Viterbi parse of each verse against this gold standard
and f-score was calculated. The rhyme scheme de-
tection module employed in our data selection ob-
tained a precision of 35.81% and a recall of 57.25%,
giving an f-score of 44.06%.
5.2 Training data selection via rhyme scheme
detection
We obtained around 600,000 training instances upon
extracting a training corpus using rhyme scheme de-
tection module as described in Section 5.1. We
added those lines that were adjacent and labeled as
rhyming by the rhyme scheme detector as training in-
stances resulting in a training corpus of size 200,000.
5.3 Training data selection via adjacent lines
Considering all adjacent lines in a verse resulted in
a corpus with over 5 million training instances. In
order to ensure fair comparison of models trained
on data selected using rhyme scheme detection, we
randomly chose 200,000 training instances from the
generated corpus. The training corpus thus gener-
ated shared around 15% of training instances with
the corpus generated through our proposed data se-
lection scheme.
5.4 Results: Rhyme scheme detection helps
Results in Table 1 indicate that using the rhyme
scheme detector for training data selection helps
produce significantly more fluent responses com-
pared to using adjacent lines. A possible explana-
tion for this could be that adding all adjacent lines as
training instances introduces a lot of noise into the
model which hurts the fluency of the responses gen-
erated. Also, the cumulative fraction of sentences
that were labeled good or ?acceptable on the crite-
rion of rhyming is larger when rhyme scheme detec-
tion was used to generate the training data (although
the TG model trained on the corpus generated us-
ing adjacent lines produces a higher percentage of
rhyming responses that were rated good). Given the
significantly higher rate of response fluency when
using rhyme scheme detection, we argue that using
rhyme scheme detector for data selection is benefi-
cial.
108
Table 3: English hip hop challenge-response examples.
challenge man i die to see em all thun i just don't care
tg+rs in the sky and me the in polla and the you there
tg and the ride the me the and white the i the air
pbsmt+rs man i live to see em all i just don't care
challenge did a twelve year bid in the streets and held it down
tg+rs to the girls here kid the and to the thought the now
tg to the p's here did the a the i was the the
pbsmt+rs did a year in the streets and it down
challenge oh i believe in yesterday
tg+rs can you see the day
tg now you see the way
pbsmt+rs oh i believe in tomorrow
challenge what would i do
tg+rs just me and you
tg and you and you
pbsmt+rs what would you do
challenge cause you ain't going home till the early morn
tg+rs and the you this alone i i gotta on
tg and i you my on the a home we
pbsmt+rs cause you and your friends aint nothing but
It is also interesting to note from Table 1 that
ISTG+RS performs better than TG+RS indicating
that transduction grammar induced via interpolating
token based grammar and rule segmenting produces
better responses than token based transduction gram-
mar on both data selection schemes. Although the
average fraction of responses rated good on fluency
are slightly lower for ISTG+RS compared to TG+RS
(34.12% vs. 30.98%), the fraction of responses rated
?acceptable are higher (61.18% vs. 57.64%). It is
important to note that the fraction of sentences rated
good and ?acceptable on rhyming are much larger
for ISTG+RS model. Although the fluency of the
responses generated by PBSMT+RS drastically im-
proves compared to PBSMT it still lags behind the
TG+RS and ISTG+RS models on both fluency and
rhyming. The results in Table 1 confirm our hypoth-
esis that off-the-shelf SMT systems are not guaran-
teed to be effective on our novel task.
5.5 Challenge-response examples
Table 3 shows some of the challenges and the cor-
responding responses of PBSMT+RS, TG+RS and
TG model. While PBSMT+RS and TG+RS mod-
els generate responses reflecting a high degree of
fluency, the output of the TG contains a lot of ar-
ticles. It is interesting to note that TG+RS produces
responses comparable to PBSMT+RS despite being
a token based transduction grammar. However, PB-
SMT tends to produce responses that are too simi-
lar to the challenge. Moreover, TG models produce
responses that indeed rhyme better (shown in bold-
face). In fact, TG tries to rhymewords not only at the
end but also in middle of the lines, as our transduc-
tion grammar model captures structural associations
more effectively than the phrase-based model.
6 Disfluency handling via disfluency
correction and filtering
In this section, we compare the effect of two dis-
fluency mitigating strategies on the quality of the re-
sponses generated by the PBSMT baseline and token
based transduction grammar model with and without
using rhyme scheme detection.
6.1 Correction vs. filtering
Error analysis of our initial runs showed a dis-
turbingly high proportion of responses generated by
our system that contained disfluencies with succes-
sive repetitions of words such as the and I. Upon in-
spection of data we noticed that the training lyrics
actually did contain such disfluencies and backing
vocal lines, amounting to 10% of our training data.
We therefore compared two alternative strategies to
tackle this problem. The first strategy involved fil-
tering out all lines from our training corpus which
contained such disfluencies. In the second strategy,
we implemented a disfluency detection and correc-
tion algorithm (for example, the the the, which fre-
quently occurred in the training corpus, was cor-
rected to simply the). The PBSMT baseline and the
TG model were trained on both the filtered and cor-
rected versions of the training corpus and the quality
of the responses were compared.
6.2 Results: Disfluency correction helps
The results in Table 4 indicate that the disfluency
correction strategy outperforms the filtering strategy
for both TG and TG+RS models. For the model
TG+RS, disfluency correction generated 34.12%
good responses in terms of fluency, while the filter-
ing strategy produced only 28.63% good responses.
Similarly for the model TG, disfluency correction
produced 21.8% of responses with good fluency and
the filtering strategy produced only 17.25%. Dis-
fluency correction strategy produces higher fraction
of responses with ?acceptable fluency compared to
the filtering strategy for both TG and TG+RS mod-
els. This result is not surprising, as harshly pruning
109
Table 4: Effect of the disfluency correction strategies on fluency of the responses generated for the TG induction
models vs PBSMT baselines using both rhyme scheme detection and adjacent lines as the corpus selection method.
model+disfluency strat. fluency (good) fluency (?acceptable) rhyming (good) rhyming (?acceptable)
PBSMT+filtering 4.3% 13.72% 3.53% 7.06%
PBSMT+correction 3.14% 4.70% 1.57% 4.31%
PBSMT+RS+filtering 31.76% 43.91% 12.15% 21.17%
PBSMT+RS+correction 30.59% 43.53% 1.96% 9.02%
TG+filtering 17.25% 46.27% 18.04% 33.33%
TG+correction 21.18% 54.51% 23.53% 39.21%
TG+RS+filtering 28.63% 56.86% 14.90% 34.51%
TG+RS+correction 34.12% 60.39% 20.00% 42.74%
the training corpus causes useful word association
information necessary for rhyming to be lost. Sur-
prisingly, for both PBSMT and PBSMT+RSmodels,
the disfluency correction has a negative effect on the
fluency level of the response but still falls behind TG
and TG+RS models. As disfluency correction yields
more fluent responses for TG and TG+RS models,
the results for ISTG and ISTG+RS models in Table
1 were obtained using disfluency correction strategy.
7 Maghrebi French hip hop
We have begun to apply Freestyle to rap in lan-
guages other than English, taking advantage of
the language independence and linguistics-light ap-
proach of our unsupervised transduction grammar
induction methods. With no special adaption our
transduction grammar based model performs sur-
prisingly well, even with significantly smaller train-
ing data size and noisier data. These results across
different languages are encouraging as they can be
used to discover truly language independence as-
sumptions. We briefly describe our initial experi-
ments on Maghrebi French hip hop lyrics below.
7.1 Dataset
We collected freely available French hip hop lyrics
of approximately 1300 songs. About 85% of the
songs were by Maghrebi French artists of Alge-
rian, Moroccan, or Tunisian cultural backgrounds,
while the remaining were by artists from the rest
of Francophonie. As the large majority of songs
are in Maghrebi French, the lyrics are sometimes
interspersed with romanized Arabic such as ?De la
travers?e du d?sert au bon couscous de Y?ma? (Y?ma
meansMy mother). Some songs also contain Berber
phrases, for instance ?a yemmi ino, a thizizwith?
(which means my son, a bee). Furthermore, some
songs also contained English phrases in the style of
gangster rap such as ?T'es game over, game over... Le
son de Chicken wings?. As mentioned earlier, it is
complexity like this which dissuaded us from mak-
ing language specific assumptions in our model.
We extracted the end-of-line words and obtained
a corpus containing 120,000 tokens corresponding
to potential rhyming candidates with around 29,000
unique token types which was used as the training
data for the rhyme scheme detector module. For the
transduction grammar induction, the training data
contained about 47,000 sentence pairs selected us-
ing rhyme scheme detection.
7.2 Results
After human evaluation by native French speak-
ers and frequent hip hop listeners, our transduction
grammar based model generates about 9.2% and
14.5% of the responses that are rated good by the
human evaluators on the criterion of fluency and
rhyming respectively. About 30.2% and 38% of
the responses are rated as ?acceptable. These num-
bers are encouraging given the noisy lyrics andmuch
smaller amount of training data. Some examples of
the challenge-response pairs and learned transduc-
tion rules in French are shown in Tables 5 and 6.
From Table 5, we can see that responses gener-
ated by the system rhyme with the challenges. The
first response is fluent and not only perfectly rhymes
with the challenge but also semantically valid. In the
second example, the model realizes a less common
AABA rhyme scheme through the response. The re-
110
Table 5: French hip hop challenge-response examples.
challenge Si je me trompe
response faut que je raconte
challenge Un jour je suis un livre
response et ce que je de vivre
challenge Pacha mama ils ne voient pas ta souffrance
response Combat ni leur de voulait de la d?cadence
challenge la palestine n'etait pas une terre sans peuple.
response le darfour d'autre de la guerre on est
challenge Une banlieue qui meut
response les yeux et
Table 6: Transduction rules for Maghrebi French hip hop.
transduction grammar rule log prob.
A? terre/la guerre -9.4837
A? haine/peine -9.77056
A? mal/pays natal -10.6877
A? je frissonne/mi corazon -11.0931
A? gratteurs/rappeurs -11.7306
sponse in the third example, exhibits strong rhyming
with the challenge and both the challenge and the
response contain words like souffrance, combat and
d?cadence which are related. Similarly in the fourth
example, the challenge and response also contain se-
mantically related tokens which also rhyme. These
examples illustrate that our transduction grammar
formalism coupled with our rhyme scheme detection
module does capture the necessary correspondences
between lines of hip hop lyrics without assuming any
language specific resources.
8 Conclusion
We presented a new machine learning approach for
improvising hip hop responses to challenge lyrics
by inducing stochastic transduction grammars, and
demonstrated that inducing the transduction rules by
interpolating bottom-up token based rule induction
and rule segmentation strategies outperforms a token
based baseline. We compared the performance of
our Freestylemodel against a widely used off-the-
shelf phrase-based SMT model, showing that PB-
SMT falls short in tackling the noisy and highly un-
structured domain of hip hop lyrics. We showed that
the quality of responses improves when the training
data for the transduction grammar induction is se-
lected using a rhyme scheme detector. Several do-
main related oddities such as disfluencies and back-
ing vocals have been identified and some strategies
for alleviating their effects have been compared. We
also reported results on Maghrebi French hip hop
lyrics which indicate that our model works surpris-
ingly well with no special adaptation for languages
other than English. In the future, we plan to inves-
tigate alternative training data selection techniques,
disfluency handling strategies, search heuristics, and
novel transduction grammar induction models.
Acknowledgements
This material is based upon work supported in part by
the Hong Kong Research Grants Council (RGC) research
grants GRF620811, GRF621008, GRF612806; by the
Defense Advanced Research Projects Agency (DARPA)
under BOLT contract no. HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-
0023; and by the European Union under the FP7 grant
agreement no. 287658. Any opinions, findings and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the
views of the RGC, EU, or DARPA.
References
Ananth Ramakrishnan A., Sankar Kuppan, and
Lalitha Devi Sobha. ?Automatic generation of
Tamil lyrics for melodies.? Workshop on Computa-
tional Approaches to Linguistic Creativity (CALC-09).
2009.
Karteek Addanki and DekaiWu. ?Unsupervised rhyme
scheme identification in hip hop lyrics using hidden
Markov models.? 1st International Conference on Sta-
tistical Language and Speech Processing (SLSP 2013).
2013.
Gabriele Barbieri, Fran?ois Pachet, Pierre Roy, and
Mirko Degli Esposti. ?Markov constraints for gen-
erating lyrics with style.? 20th European Conference
on Artificial Intelligence, (ECAI 2012). 2012.
David Chiang. ?Hierarchical phrase-based translation.?
Computational Linguistics, 33(2), 2007.
John Cocke. Programming languages and their compil-
ers: Preliminary notes. Courant Institute ofMathemat-
ical Sciences, New York University, 1969.
P.A. Devijer. ?Baum?s forward-backward algorithm re-
visited.? Pattern Recognition Letters, 3(6), 1985.
D. Genzel, J. Uszkoreit, and F. Och. ?Poetic statisti-
cal machine translation: rhyme and meter.? 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010). Association for Computa-
tional Linguistics, 2010.
E. Greene, T. Bodrumlu, and K. Knight. ?Auto-
matic analysis of rhythmic poetry with applications
111
to generation and translation.? 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010). Association for Computational Lin-
guistics, 2010.
Long Jiang and Ming Zhou. ?Generating Chinese
couplets using a statistical MT approach.? 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008). 2008.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. ?Moses:
Open source toolkit for statistical machine translation.?
Interactive Poster and Demonstration Sessions of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2007). June 2007.
MarkLiberman. ?Rap scholarship, rapmeter, and the an-
thology of mondegreens.? http://languagelog.ldc.
upenn.edu/nll/?p=2824, December 2010. Accessed:
2013-06-30.
Franz Josef Och. ?Minimum error rate training in sta-
tistical machine translation.? 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
2003). July 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. ?BLEU: a method for automatic evalu-
ation of machine translation.? 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02). July 2002.
S. Reddy and K. Knight. ?Unsupervised discovery of
rhyme schemes.? 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL HLT 2011), vol. 2. Association for
Computational Linguistics, 2011.
Jorma Rissanen. ?A universal prior for integers and es-
timation by minimum description length.? The Annals
of Statistics, 11(2), June 1983.
Markus Saers, KarteekAddanki, andDekaiWu. ?From
finite-state to inversion transductions: Toward un-
supervised bilingual grammar induction.? 24th In-
ternational Conference on Computational Linguistics
(COLING 2012). December 2012.
Markus Saers, Karteek Addanki, and Dekai Wu.
?Combining top-down and bottom-up search for un-
supervised induction of transduction grammars.? Sev-
enth Workshop on Syntax, Semantics and Structure in
Statistical Translation (SSST-7). June 2013.
Markus Saers and Dekai Wu. ?Reestimation of reified
rules in semiring parsing and biparsing.? Fifth Work-
shop on Syntax, Semantics and Structure in Statistical
Translation (SSST-5). Association for Computational
Linguistics, June 2011.
Ray J. Solomonoff. ?A new method for discov-
ering the grammars of phrase structure languages.?
International Federation for Information Processing
Congress (IFIP). 1959.
M. Sonderegger. ?Applications of graph theory to an
English rhyming corpus.? Computer Speech & Lan-
guage, 25(3), 2011.
Andreas Stolcke. ?SRILM ? an extensible language
modeling toolkit.? 7th International Conference on
Spoken Language Processing (ICSLP2002 - INTER-
SPEECH 2002). September 2002.
Dekai Wu. ?A polynomial-time algorithm for statisti-
cal machine translation.? 34th Annual Meeting of the
Association for Computational Linguistics (ACL96).
1996.
DekaiWu. ?Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora.? Computa-
tional Linguistics, 23(3), 1997.
Dekai Wu. ?Textual entailment recognition using inver-
sion transduction grammars.? Joaquin Qui?onero-
Candela, Ido Dagan, Bernardo Magnini, and Flo-
rence d?Alch? Buc (eds.),Machine Learning Chal-
lenges, Evaluating Predictive Uncertainty, Visual Ob-
ject Classification and Recognizing Textual Entail-
ment, First PASCAL Machine Learning Challenges
Workshop (MLCW 2005), vol. 3944 of Lecture Notes
in Computer Science. Springer, 2006.
Dekai Wu, Karteek Addanki, and Markus Saers.
?FREESTYLE: A challenge-response system for hip
hop lyrics via unsupervised induction of stochastic
transduction grammars.? 14th Annual Conference of
the International Speech Communication Association
(Interspeech 2013). 2013a.
Dekai Wu, Karteek Addanki, and Markus Saers.
?Modeling hip hop challenge-response lyrics as ma-
chine translation.? 14th Machine Translation Summit
(MT Summit XIV). 2013b.
Dekai Wu and Pascale Fung. ?Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora.? Second Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP 2005). Springer, 2005.
Richard Zens and HermannNey. ?A comparative study
on reordering constraints in statistical machine trans-
lation.? 41st Annual Meeting of the Association for
Computational Linguistics (ACL-2003). Association
for Computational Linguistics, 2003.
112
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341?344,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Word Alignment with
Stochastic Bracketing Linear Inversion Transduction Grammar
Markus SAERS and Joakim NIVRE
Computational Linguistics Group
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
The class of Linear Inversion Transduction
Grammars (LITGs) is introduced, and used to
induce a word alignment over a parallel cor-
pus. We show that alignment via Stochas-
tic Bracketing LITGs is considerably faster
than Stochastic Bracketing ITGs, while still
yielding alignments superior to the widely-
used heuristic of intersecting bidirectional
IBM alignments. Performance is measured as
the translation quality of a phrase-based ma-
chine translation system built upon the word
alignments, and an improvement of 2.85 BLEU
points over baseline is noted for French?
English.
1 Introduction
Machine translation relies heavily on word align-
ments, which are usually produced by training IBM-
models (Brown et al, 1993) in both directions and
combining the resulting alignments via some heuris-
tic. Automatically training an Inversion Transduc-
tion Grammar (ITG) has been suggested as a viable
way of producing superior alignments (Saers and
Wu, 2009). The main problem of using Bracket-
ing ITGs for alignment is that exhaustive biparsing
runs in O(n6) time. Several ways to lower the com-
plexity of ITGs has been suggested, but in this paper,
a different approach is taken. Instead of using full
ITGs, we explore the possibility of subjecting the
grammar to a linear constraint, making exhaustive
biparsing of a sentence pair in O(n4) time possible.
This can be further improved by applying pruning.
2 Background
A transduction is the bilingual version of a language.
A language (Ll) can be formally viewed as a set of
sentences, sequences of tokens taken from a speci-
fied vocabulary (Vl). A transduction (Te,f ) between
two languages (Le and Lf ) is then a set of sentence
pairs, sequences of bitokens from the cross produc-
tion of the vocabularies of the two languages being
transduced (Ve,f = Ve ? Vf ). This adds an extra
layer of complexity to finding transductions from
raw bitexts, as an alignment has to be imposed.
Simple (STG) and Syntax Directed (SDTG) Trans-
duction Grammars (Aho and Ullman, 1972) can
be used to parse transductions between context-free
languages. Both work fine as long as a grammar is
given and parsing is done as transduction, that is: a
sentence in one language is rewritten into the other
language. In NLP, interest has shifted away from
hand-crafted grammars, towards stochastic gram-
mars induced from corpora. To induce a stochas-
tic grammar from a parallel corpus, expectations of
all possible parses over a sentence pair are typically
needed. STGs can biparse sentence pairs in polyno-
mial time, but are unable to account for the complex-
ities typically found in natural languages. SDTGs do
account for the complexities in natural languages,
but are intractable for biparsing.
Inversion transductions (Wu, 1995; Wu, 1997) are
a special case of transductions that are not mono-
tone, but where permutations are severely limited.
By limiting the possible permutations, biparsing be-
comes tractable. This in turn means that ITGs can be
induced from parallel corpora in polynomial time,
341
as well as account for most of the reorderings found
between natural languages.
An Inversion transduction is limited so that it
must be expressible as non-overlapping groups, in-
ternally permuted either by the identity permuta-
tion or the inversion permutation (hence the name).
This requirement also means that the grammar is bi-
narizable, yielding a two-normal form. A produc-
tion with the identity permutation is written inside
square brackets, while productions with the inver-
sion permutation is written inside angled brackets.
This gives us a two-normal form that looks like this
(where e/f is a biterminal):
A ? [B C]
A ? ?B C?
A ? e/f
The time complexity for exhaustive ITG biparsing is
O(Gn6), which is typically too large to be applica-
ble to large grammars and long sentence. The gram-
mar constant G can be eliminated by limiting the
grammar to a bracketing ITG (BITG), which only has
one nonterminal symbol. Saers & Wu (2009) show
that it is possible to apply exhaustive biparsing to a
large parallel corpus (? 100, 000 sentence pairs) of
short sentences (?10 tokens in both language). The
word alignments read off the Viterbi parse also in-
creased translation quality when used instead of the
alignments from bidirectional IBM alignments.
The O(n6) time complexity is somewhat pro-
hibitive for large corpora, so pruning in some form is
needed. Saers, Nivre & Wu (2009) introduce a beam
pruning scheme, which reduces time complexity to
O(bn3). They also show that severe pruning is pos-
sible without significant deterioration in alignment
quality. Haghighi et. al (2009) use a simpler aligner
as guidance for pruning, which reduce the time com-
plexity by two orders of magnitude, and also intro-
duce block ITG, which gives many-to-one instead of
one-to-one alignments. Zhang et. al (2008) present
a method for evaluating spans in the sentence pair to
determine whether they should be excluded or not.
The algorithm has a best case time complexity of
O(n3).
In this paper we introduce Linear ITG (LITG), and
apply it to a word-alignment task which is evaluated
by the phrase-based statistical machine translation
(PBSMT) system that can be built from that.
3 Stochastic Bracketing Linear Inversion
Transduction Grammar
A Bracketing Linear Inversion Transduction Gram-
mar (BLITG) is a BITG where rules may have at most
one nonterminal symbol in their production. This
gives us a normal form that is somewhat different
from the usual ITG:
X ? [Xe/f ]
X ? [e/fX]
X ? ?Xe/f?
X ? ?e/fX?
X ? ?/?
where one but not both of the tokens in the bitermi-
nal may be the empty string ?, if a nonterminal is
produced. By associating each rule with a probabil-
ity, we get a Stochastic BLITG (SBLITG).
3.1 Biparsing Algorithm
The sentence pair to be biparsed consists of two vec-
tors of tokens (e and f ). An item is represented
as a nonterminal (X), and one span in each of the
languages (es..t and fu..v). For notational conve-
nience, an item will be written as the nonterminal
with the spans as subscripts (Xs,t,u,v). The length of
an item is defined as the sum of the length of the two
spans: |Xs,t,u,v| = t ? s + v ? u. Items are gath-
ered in buckets, Bn, according to their length so that
Xs,t,u,v ? B|Xs,t,u,v|. The algorithm is initialized
with the item spanning the entire sentence pair:
X0,|e|,0,|f | ? B|X0,|e|,0,|f||
Starting from this top bucket, buckets are processed
in larger to smaller order: Bn, Bn?1, . . . , B1. While
processing a bucket, only smaller items are added,
meaning that B0 is fully constructed by the time B1
has been processed. Each item in B0 can have the
rule X ? ?/? applied to it, eliminating the nonter-
minal and halting processing. If there are no items
in B0, parsing has failed.
To process a bucket, each item is extended by all
applicable rules, and the nonterminals in the produc-
tions are added to their respective buckets.
342
System BLEU NIST Phrases
GIZA++ (intersect) 0.2629 6.7968 146,581,109
GIZA++ (grow-diag-final) 0.2632 6.7410 1,298,566
GIZA++ (grow-diag-final-and) 0.2742 6.9228 7,340,369
SBLITG (b = 25) 0.3027 7.3664 13,551,915
SBLITG (b = ?) 0.3008 7.3303 12,673,361
Table 1: Results for French?English.
Xs,t,u,v ?
[es,s+1/fu,u+1 Xs+1,t,u+1,v]
| [Xs,t?1,u,v?1 et?1,t/fv?1,v ]
| ?es,s+1/fv?1,v Xs+1,t,u,v?1?
| ?Xs,t?1,u+1,v et?1,t/fu,u+1?
| [es,s+1/? Xs+1,t,u,v] | ?es,s+1/? Xs+1,t,u,v?
| [?/fu,u+1 Xs,t,u+1,v] | ?Xs,t,u+1,v ?/fu,u+1?
| [Xs,t?1,u,v et?1,t/?] | ?Xs,t?1,u,v et?1,t/??
| [Xs,t,u,v?1 ?/fv?1,v] | ??/fv?1,v Xs,t,u,v?1?
Note that there are two productions on each of the
four last rows. These are distinct rules, but the
symbols in the productions are identical. This phe-
nomenon is due to the fact that the empty sym-
bols can be ?read? off either end of the span. In
our experiments, such rules were merged into their
non-inverting form, effectively eliminating the last
four inverted rules (productions enclosed in angled
brackets) above.
3.2 Analysis
Let n be the length of the longer sentence in the
pair. The number of buckets will be O(n), since
the longest item will be at most 2n long. Within a
bucket, there can be O(n2) starting points for items,
but once the length of one of the spans is fixed, the
length of the other follows, adding a factor O(n),
making the total number of items in a bucket O(n3).
Each item in a bucket can be analyzed in 8 possible
ways, requiring O(1) time. In summary, we have:
O(n)?O(n3)?O(1) = O(n4)
The pruning scheme works by limiting the num-
ber of items that are processed from each bucket, re-
ducing the cost of processing a bucket from O(n3)
to O(b), where b is the beam width. This gives time
complexity O(n) ?O(b) ?O(1) = O(bn).
4 Experiments
We used the guidelines of the shared task of
WMT?081 to train our baseline system as well as
our experimental system. This includes induction of
word alignments with GIZA++ (Och and Ney, 2003),
induction of a Phrase-based SMT system (Koehn et
al., 2007), and tuning with minimum error rate train-
ing (Och, 2003), as well as applying some utility
scripts provided for the workshop. The translation
model is combined with a 5-gram language model
(Stolcke, 2002).
Our experimental system uses alignments from
the Viterbi parses, extracted during EM training of an
SBLITG on the training corpus, instead of GIZA++.
Since EM will converge fairly slowly, it was limited
to 10 iterations, after which it was halted.
We used the French?English part of the WMT?08
shared task, but limited the training set to sentence
pairs where both sentences were of length 20 or less.
This was necessary in order to carry out exhaustive
search in the SBLITG algorithm. In total, we had
381,780 sentence pairs for training, and 2,000 sen-
tence pairs each for tuning and testing. The language
model was trained with the entire training set.
To evaluate the systems we used BLEU (Papineni
et al, 2002) and NIST (Doddington, 2002)
Results are presented in Table 1. It is interesting
to note that there is no correlation between the num-
ber of phrases extracted and translation quality. The
only explanation for the results we are seeing is that
the SBLITGs find better phrases. Since the only dif-
ference is the word alignment strategy, this suggests
that the word alignments from SBLITGs are better
suited for phrase extraction than those from bidirec-
tional IBM-models. The fact that SBLITGs extract
more phrases than bidirectional IBM-models under
1http://www.statmt.org/wmt08/
343
the grow-diag-x heuristics is significant, since
more phrases means that more translation possibil-
ities are extracted. The fact that SBLITGs extract
fewer phrases than bidirectional IBM-models under
the intersect heuristic is also significant, since
it implies that simply adding more phrases is a bad
strategy. Combined, the two observations leads us
to believe that there are some alignments missed by
the bidirectional IBM-models that are found by the
SBLITG-models. It is also interesting to see that the
pruned version outperforms the exhaustive version.
We believe this to be because the pruned version ap-
proaches the correct grammar faster than the exhaus-
tive. That would mean that the exhaustive SBLITG
would be better in the limit, but the experiment was
limited to 10 iterations.
5 Conclusion
In this paper we have focused on the benefits of ap-
plying SBLITGs to the task of inducing word align-
ments, which leads to a 2.85 BLEU points improve-
ment compared to the standard model (heuristically
combined bidirectional IBM-models). In the future,
we hope that LITGs will be a spring board towards
full ITGs, with more interesting nonterminals than
the BITGs seen in the literature so far. With the pos-
sibility of inducing full ITG from parallel corpora it
becomes viable to use ITG decoders directly as ma-
chine translation systems.
Acknowledgments
This work was funded by the Swedish National Gradu-
ate School of Language Technology (GSLT), the Defense
Advanced Research Projects Agency (DARPA) under
GALE Contract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under research
grants GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of DARPA.The computations were performed on
UPPMAX resources under project p2007020.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice-Halll, En-
glewood Cliffs, New Jersey.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology
conference (HLT-2002), San Diego, California.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings of ACL/IJCNLP 2009, pages 923?931,
Singapore.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
ACL 2007 Demo and Poster Session, pages 177?180,
Prague, Czech Republic.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL 2003,
pages 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, pages 311?
318, Philadelphia, Pennsylvania.
M. Saers and D. Wu. 2009. Improving phrase-based
translation via word alignments from Stochastic In-
version Transduction Grammars. In Proceedings of
SSST-3 at NAACL HLT 2009, pages 28?36, Boulder,
Colorado.
M. Saers, J. Nivre, and D. Wu. 2009. Learning stochas-
tic bracketing inversion transduction grammars with
a cubic time biparsing algorithm. In Proceedings of
IWPT?09, pages 29?32, Paris, France.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In International Conference on Spoken
Language Processing, Denver, Colorado.
D. Wu. 1995. An algorithm for simultaneously bracket-
ing parallel texts by aligning words. In Proceedings of
WVLC-3, pages 69?82, Cambridge, Massachusetts.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL/HLT
2008, pages 97?105, Columbus, Ohio.
344
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375?381,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving machine translation by training against
an automatic semantic frame based evaluation metric
Chi-kiu Lo and Karteek Addanki and Markus Saers and Dekai Wu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|vskaddanki|masaers|dekai}@cs.ust.hk
Abstract
We present the first ever results show-
ing that tuning a machine translation sys-
tem against a semantic frame based ob-
jective function, MEANT, produces more
robustly adequate translations than tun-
ing against BLEU or TER as measured
across commonly used metrics and human
subjective evaluation. Moreover, for in-
formal web forum data, human evalua-
tors preferredMEANT-tuned systems over
BLEU- or TER-tuned systems by a sig-
nificantly wider margin than that for for-
mal newswire?even though automatic se-
mantic parsing might be expected to fare
worse on informal language. We argue
that by preserving themeaning of the trans-
lations as captured by semantic frames
right in the training process, an MT sys-
tem is constrained to make more accu-
rate choices of both lexical and reorder-
ing rules. As a result, MT systems tuned
against semantic frame based MT evalu-
ation metrics produce output that is more
adequate. Tuning a machine translation
system against a semantic frame based ob-
jective function is independent of the trans-
lation model paradigm, so, any transla-
tion model can benefit from the semantic
knowledge incorporated to improve trans-
lation adequacy through our approach.
1 Introduction
We present the first ever results of tuning a statis-
tical machine translation (SMT) system against a
semantic frame based objective function in order
to produce a more adequate output. We compare
the performance of our system with that of two
baseline SMT systems tuned against BLEU and
TER, the commonly used n-gram and edit distance
based metrics. Our system performs better than
the baseline across seven commonly used evalu-
ation metrics and subjective human evaluation on
adequacy. Surprisingly, tuning against a seman-
tic MT evaluation metric also significantly out-
performs the baseline on the domain of informal
web forum data wherein automatic semantic pars-
ing might be expected to fare worse. These results
strongly indicate that using a semantic frame based
objective function for tuning would drive develop-
ment of MT towards direction of higher utility.
Glaring errors caused by semantic role confu-
sion that plague the state-of-the-art MT systems
are a consequence of using fast and cheap lexi-
cal n-gram based objective functions like BLEU
to drive their development. Despite enforcing flu-
ency it has been established that these metrics do
not enforce translation utility adequately and often
fail to preservemeaning closely (Callison-Burch et
al., 2006; Koehn and Monz, 2006).
We argue that instead of BLEU, a metric that fo-
cuses on getting the meaning right should be used
as an objective function for tuning SMT so as to
drive continuing progress towards higher utility.
MEANT (Lo et al, 2012), is an automatic seman-
tic MT evaluation metric that measures similarity
between the MT output and the reference transla-
tion via semantic frames. It correlates better with
human adequacy judgment than other automatic
MT evaluation metrics. Since a high MEANT
score is contingent on correct lexical choices as
well as syntactic and semantic structures, we be-
lieve that tuning against MEANT would improve
both translation adequacy and fluency.
Incorporating semantic structures into SMT by
tuning against a semantic frame based evaluation
metric is independent of the MT paradigm. There-
fore, systems from different MT paradigms (such
as hierarchical, phrase based, transduction gram-
mar based) can benefit from the semantic informa-
tion incorporated through our approach.
375
2 Related Work
Relatively little work has been done towards bi-
asing the translation decisions of an SMT system
to produce adequate translations that correctly pre-
servewho did what to whom, when, where and why
(Pradhan et al, 2004). This is because the devel-
opment of SMT systemswas predominantly driven
by tuning against n-gram based evaluation met-
rics such as BLEU or edit distance based metrics
such as TER which do not sufficiently bias SMT
system?s decisions to produce adequate transla-
tions. Although there has been a recent surge of
work aimed towards incorporating semantics into
the SMT pipeline, none attempt to tune against a
semantic objective function. Below, we describe
some of the attempts to incorporate semantic in-
formation into the SMT and present a brief survey
on evaluation metrics that focus on rewarding se-
mantically valid translations.
Utilizing semantics in SMT In the past few
years, there has been a surge of work aimed at in-
corporating semantics into various stages of the
SMT. Wu and Fung (2009) propose a two-pass
model that reorders the MT output to match the
SRL of the input, which is too late to affect the
translation decisions made by the MT system dur-
ing decoding. In contrast, training against a se-
mantic objective function attempts to improve the
decoding search strategy by incorporating a bias
towards meaningful translations into the model in-
stead of postprocessing its results.
Komachi et al (2006) and Wu et al (2011) pre-
process the input sentence to match the verb frame
alternations in the output side. Liu and Gildea
(2010) and Aziz et al (2011) use input side SRL
to train a tree-to-string SMT system. Xiong et al
(2012) trained a discriminative model to predict
the position of the semantic roles in the output.
All these approaches are orthogonal to the present
question of whether to train toward a semantic ob-
jective function. Any of the above models could
potentially benefit from tuning with semantic met-
rics.
MT evaluation metrics As mentioned previ-
ously, tuning against n-gram based metrics such
as BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie,
2005) does not sufficiently drive SMT into mak-
ing decisions to produce adequate translations
that correctly preserve ?who did what to whom,
when, where and why?. In fact, a number of
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) report cases
where BLEU strongly disagrees with human judg-
ments of translation accuracy. Tuning against edit
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al, 2000), and TER
(Snover et al, 2006) also fails to sufficiently bias
SMT systems towards producing translations that
preserve semantic information.
We argue that an SMT system tuned against an
adequacy-oriented metric that correlates well with
human adequacy judgement produces more ade-
quate translations. For this purpose, we choose
MEANT, an automatic semantic MT evaluation
metric that focuses on getting the meaning right by
comparing the semantic structures of the MT out-
put and the reference. We briefly describe some
of the alternative semantic metrics below to justify
our choice.
ULC (Gim?nez and M?rquez, 2007, 2008) is
an aggregated metric that incorporates several se-
mantic similarity features and shows improved
correlation with human judgement on translation
quality (Callison-Burch et al, 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al, 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an MT system against
ULC perhaps due to its expensive running time.
Lambert et al (2006) did tune on QUEEN, a sim-
plified version of ULC that discards the seman-
tic features and is based on pure lexical features.
Although tuning on QUEEN produced slightly
more preferable translations than solely tuning on
BLEU, themetric does not make use of any seman-
tic features and thus fails to exploit any potential
gains from tuning to semantic objectives.
Although TINE (Rios et al, 2011) is an recall-
oriented automatic evaluation metric which aims
to preserve the basic event structure, no work has
been done towards tuning an SMT system against
it. TINE performs comparably to BLEU andworse
than METEOR on correlation with human ade-
quacy judgment.
In contrast to TINE, MEANT (Lo et al, 2012),
which is the weighted f-score over the matched se-
mantic role labels of the automatically aligned se-
mantic frames and role fillers, outperforms BLEU,
NIST, METEOR, WER, CDER and TER. This
makes it more suitable for tuning SMT systems to
produce much adequate translations.
376
newswire BLEU NIST METEOR no_syn METEOR WER CDER TER MEANT
BLEU-tuned 29.85 8.84 52.10 55.42 67.88 55.67 58.40 0.1667
TER-tuned 25.37 6.56 48.26 51.24 66.18 52.58 56.96 0.1578
MEANT-tuned 25.91 7.81 50.15 53.60 67.76 54.56 58.61 0.1676
Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
forum BLEU NIST METEOR no_syn METEOR WER CDER TER MEANT
BLEU-tuned 9.58 4.10 31.77 34.63 80.09 64.54 76.12 0.1711
TER-tuned 6.94 2.21 28.55 30.85 76.15 57.96 74.73 0.1539
MEANT-tuned 7.92 3.11 30.40 33.08 77.32 61.01 74.64 0.1727
Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
3 Tuning SMT against MEANT
We now show that using MEANT as an objec-
tive function to drive minimum error rate training
(MERT) of state-of-the-art MT systems improves
MT utility not only on formal newswire text, but
even on informal forum text, where automatic se-
mantic parsing is difficult.
Toward improving translation utility of state-of-
the-art MT systems, we chose to use a strong and
competitive system in the DARPA BOLT program
as our baseline. The baseline system is a Moses
hierarchical model trained on a collection of LDC
newswire and a small portion of Chinese-English
parallel web forum data, together with a 5-gram
language model. For the newswire experiment, we
used a collection of NIST 02-06 test sets as our de-
velopment set and NIST 08 test set for evaluation.
The development and test sets contain 6,331 and
1,357 sentences respectively with four references.
For the forum data experiment, the development
and test sets were a held-out subset of the BOLT
phase 1 training data. The development and test
sets contain 2,000 sentences and 1,697 sentences
with one reference.
We use ZMERT (Zaidan, 2009) to tune the base-
line because it is a widely used, highly competi-
tive, robust, and reliable implementation of MERT
that is also fully configurable and extensible with
regard to incorporating new evaluation metrics. In
this experiment, we use aMEANT implementation
along the lines described in Lo et al (2012).
In each experiment, we tune two contrastive
conventional 100-best MERT tuned baseline sys-
tems on both newswire and forum data genres; one
tuned against BLEU, an n-gram based evaluation
metric and the other using TER, an edit distance
based metric. As semantic role labeling is expen-
sive we only tuned using 10-best list for MEANT-
tuned system. Tuning against BLEU and TER took
around 1.5 hours and 5 hours per iteration respec-
tively whereas tuning against MEANT took about
1.6 hours per iteration.
4 Results
Of course, tuning against any metric would maxi-
mize the performance of the SMT system on that
particular metric, but would be overfitting. For
example, something would be seriously wrong
if tuning against BLEU did not yield the best
BLEU scores. A far more worthwhile goal would
be to bias the SMT system to produce adequate
translations while achieving the best scores across
all the metrics. With this as our objective, we
present the results of comparing MEANT-tuned
systems against the baselines as evaluated on com-
monly used automatic metrics and human ade-
quacy judgement.
Cross-evaluation using automatic metrics Ta-
bles 1 and 2 show that MEANT-tuned systems
achieve the best scores across all other metrics in
both newswire and forum data genres, when avoid-
ing comparison of the overfit metrics too similar to
the one the system was tuned on (the cells shaded
in grey in the table: NIST and METEOR are n-
gram based metrics, similar to BLEU while WER
and CDER are edit distance based metrics, similar
to TER). In the newswire domain, however, our
system achieves marginally lower TER score than
BLEU-tuned system.
Figure 1 shows an example where the MEANT-
tuned system produced a more adequate transla-
tion that accurately preserves the semantic struc-
ture of the input sentence than the two baseline
systems. The MEANT scores for the MT output
from the BLEU-, TER- and MEANT-tuned sys-
tems are 0.0635, 0.1131 and 0.2426 respectively.
Both the MEANT score and the human evaluators
rank the MT output from the MEANT-tuned sys-
377
Figure 1: Examples of machine translation output and the corresponding semantic parses from the [B]
BLEU-, [T] TER-and [M]MEANT-tuned systems together with [IN] the input sentence and [REF] the
reference translation. Note that the MT output of the BLEU-tuned system has no semantic parse output
by the automatic shallow semantic parser.
tem as the most adequate translation. In this exam-
ple, the MEANT-tuned system has translated the
two predicates ???? and ???? in the input sen-
tence into the correct form of the predicates ?at-
tack? and ?adopted? in theMT output, whereas the
BLEU-tuned system has translated both of them
incorrectly (translates the predicates into nouns)
and the TER-tuned system has correctly translated
only the first predicate (into ?seized?) and dropped
the second predicate. Moreover, for the frame ??
?? in the input sentence, the MEANT-tuned sys-
tem has correctly translated the ARG0 ??????
??? into ?Hamas militants? and the ARG1 ??
???? into ?Gaza?. However, the TER-tuned
system has dropped the predicate ???? so that
the corresponding arguments ?The Palestinian Au-
thority? and ?into a state of emergency? have all
been incorrectly associated with the predicate ??
? /seized?. This example shows that the transla-
tion adequacy of SMT has been improved by tun-
ing against MEANT because the MEANT-tuned
system is more accurately preserving the semantic
structure of the input sentence.
Our results show that MEANT-tuned system
maintains a balance between lexical choices and
word order because it performs well on n-gram
based metrics that reward lexical matching and
edit distance metrics that penalize incorrect word
order. This is not surprising as a high MEANT
score relies on a high degree of semantic structure
matching, which is contingent upon correct lexi-
cal choices as well as syntactic and semantic struc-
tures.
Human subjective evaluation In line with our
original objective of biasing SMT systems towards
producing adequate translations, we conduct a hu-
man evaluation to judge the translation utility of
the outputs produced by MEANT-, BLEU- and
TER-tuned systems. Following the manual eval-
uation protocol of Lambert et al (2006), we ran-
domly draw 150 sentences from the test set in each
domain to form the manual evaluation set. Table
3 shows the MEANT scores of the two manual
evaluation sets. In both evaluation sets, like in the
test sets, the output from the MEANT-tuned sys-
tem score slightly higher inMEANT than that from
the BLEU-tuned system and significantly higher
than that from the TER-tuned system. The output
of each tuned MT system along the input sentence
and the reference were presented to human evalu-
ators. Each evaluation set is ranked by two evalu-
ators for measuring inter-evaluator agreement.
Table 4 indicates that output of the MEANT-
tuned system is ranked adequate more frequently
compared to BLEU- and TER-tuned baselines for
both newswire and web forum genres. The inter-
378
newswire forum
BLEU-tuned 0.1564 0.1663
TER-tuned 0.1203 0.1453
MEANT-tuned 0.1633 0.1737
Table 3: MEANT scores of each system in the 150-
sentence manual evaluation set.
newswire forum
Eval 1 Eval 2 Eval 1 Eval 2
BLEU-tuned (B) 37 42 47 42
TER-tuned (T) 22 24 28 23
MEANT-tuned (M) 55 56 59 68
B=T 14 12 0 0
M=B 5 4 8 9
M=T 4 4 4 4
M=B=T 13 9 4 4
Table 4: No. of sentences ranked the most ade-
quate by human evaluators for each system.
H1 newswire forum
MEANT-tuned > BLEU-tuned 80% 95%
MEANT-tuned > TER-tuned 99% 99%
Table 5: Significance level of accepting the alter-
native hypothesis.
evaluator agreement is 84% and 70% for newswire
and forum data genres respectively.
We performed the right-tailed two proportion
significance test on human evaluation of the SMT
system outputs for both the genres. Table 5 shows
that the MEANT-tuned system generates more ad-
equate translations than the TER-tuned system at
the 99% significance level for both newswire and
web forum genres. The MEANT-tuned system is
ranked more adequate than the BLEU-tuned sys-
tem at the 95% significance level on the web fo-
rum genre and for the newswire genre the hypoth-
esis is accepted at a significance level of 80%.
The high inter-evaluator agreement and the signif-
icance tests confirm that MEANT-tuned system is
better at producing adequate translations compared
to BLEU- or TER-tuned systems.
Informal vs. formal text The results of table
4 and 5 also show that?surprisingly?the human
evaluators preferred MEANT-tuned system out-
put over BLEU-tuned and TER-tuned system out-
put by a far wider margin on the informal forum
text compared to the formal newswire text. The
MEANT-tuned system is better than both base-
lines at the 80% significance level for the formal
text genre. For the informal text genre, it per-
forms the two baselines at the 95% significance
level. Although one might expect an semantic
frame dependent metric such as MEANT to per-
form poorly on the domain of informal text, sur-
prisingly, it nonetheless significantly outperforms
the baselines at the task of generating adequate out-
put. This indicates that the design of the MEANT
evaluation metric is robust enough to tune an SMT
system towards adequate output on informal text
domains despite the shortcomings of automatic
shallow semantic parsing.
5 Conclusion
We presented the first ever results to demon-
strate that tuning an SMT system against MEANT
produces much adequate translation than tuning
against BLEU or TER, as measured across all
other commonly used metrics and human subjec-
tive evaluation. We also observed that tuning
against MEANT succeeds in producing adequate
output significantly more frequently even on the
informal text such as web forum data. By pre-
serving the meaning of the translations as captured
by semantic frames right in the training process,
an MT system is constrained to make more accu-
rate choices of both lexical and reordering rules.
The performance of our system as measured across
all commonly used metrics indicate that tuning
against a semantic MT evaluation metric does pro-
duce output which is adequate and fluent.
We believe that tuning onMEANTwould prove
equally useful for MT systems based on any
paradigm, especially where the model does not
incorporate semantic information to improve the
adequacy of the translations produced and using
MEANT as an objective function to tune SMT
would drive sustainable development of MT to-
wards the direction of higher utility.
Acknowledgment
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
ment no. 287658; and by the Hong Kong
Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
379
References
Wilker Aziz, Miguel Rios, and Lucia Specia. Shal-
low semantic trees for SMT. In Proceedings
of the Sixth Workshop on Statistical Machine
Translation (WMT2011), 2011.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?
72, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational Linguis-
tics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation. In
Proceedings of the 2nd Workshop on Statistical
Machine Translation, pages 136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further Meta-evaluation of Machine Transla-
tion. In Proceedings of the 3rd Workshop on
Statistical Machine Translation, pages 70?106,
2008.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the
2nd International Conference on Human Lan-
guage Technology Research, pages 138?145,
San Diego, California, 2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic
features for automatic evaluation of heteroge-
nous MT systems. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 256?264, Prague, Czech Republic,
June 2007.
Jes?s Gim?nez and Llu?sM?rquez. A smorgasbord
of features for automaticMT evaluation. In Pro-
ceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198, Colum-
bus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation
between European Languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation (WMT-06), pages 102?121, 2006.
Mamoru Komachi, Yuji Matsumoto, and Masaaki
Nagata. Phrase reordering for statistical ma-
chine translation based on predicate-argument
structure. In Proceedings of the 3rd Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2006), 2006.
Patrik Lambert, Jes?s Gim?nez, Marta R Costa-
juss?, Enrique Amig?, Rafael E Banchs, Llu?s
M?rquez, and JAR Fonollosa. Machine Transla-
tion system development based on human like-
ness. In Spoken Language Technology Work-
shop, 2006. IEEE, pages 246?249. IEEE, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT Evaluation Using
Block Movements. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
06), 2006.
Ding Liu and Daniel Gildea. Semantic role fea-
tures for machine translation. In Proceedings of
the 23rd international conference on Computa-
tional Linguistics (COLING-10), 2010.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully Automatic Semantic MT Evaluation.
In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation (WMT2012), 2012.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International
Conference on Language Resources and Evalu-
ation (LREC-2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H.Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
In Proceedings of the 2004 Conference on Hu-
man Language Technology and the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL-04), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine:
A metric to assess mt adequacy. In Proceed-
380
ings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 116?122. Association
for Computational Linguistics, 2011.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In Proceedings of the 7th Conference
of the Association for Machine Translation in
the Americas (AMTA-06), pages 223?231, Cam-
bridge, Massachusetts, August 2006.
Dekai Wu and Pascale Fung. Semantic Roles for
SMT: A Hybrid Two-Pass Model. In Proceed-
ings of the 2009 Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics - Human Language Technolo-
gies (NAACL-HLT-09), pages 13?16, 2009.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Ha-
jime Tsukada, and Masaaki Nagata. Extract-
ing preordering rules from predicate-argument
structures. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language
Processing (IJCNLP-11), 2011.
Deyi Xiong, Min Zhang, and Haizhou Li. Mod-
eling the Translation of Predicate-Argument
Structure for SMT. In Proceedings of the Joint
conference of the 50th AnnualMeeting of the As-
sociation for Computational Linguistics (ACL-
12), 2012.
Omar F. Zaidan. Z-MERT: A Fully Config-
urable Open Source Tool for Minimum Error
Rate Training of Machine Translation Systems.
The Prague Bulletin of Mathematical Linguis-
tics, 91:79?88, 2009.
381
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 765?771,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
XMEANT: Better semantic MT evaluation without reference translations
Lo, Chi-kiu Beloucif, Meriem Saers, Markus Wu, Dekai
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
Abstract
We introduce XMEANT?a new cross-lingual
version of the semantic frame based MT
evaluation metric MEANT?which can cor-
relate even more closely with human ade-
quacy judgments than monolingual MEANT
and eliminates the need for expensive hu-
man references. Previous work established
that MEANT reflects translation adequacy
with state-of-the-art accuracy, and optimiz-
ing MT systems against MEANT robustly im-
proves translation quality. However, to go
beyond tuning weights in the loglinear SMT
model, a cross-lingual objective function that
can deeply integrate semantic frame crite-
ria into the MT training pipeline is needed.
We show that cross-lingual XMEANT out-
performs monolingual MEANT by (1) replac-
ing the monolingual context vector model in
MEANT with simple translation probabilities,
and (2) incorporating bracketing ITG con-
straints.
1 Introduction
We show that XMEANT, a new cross-lingual ver-
sion of MEANT (Lo et al, 2012), correlates with
human judgment even more closely than MEANT
for evaluating MT adequacy via semantic frames,
despite discarding the need for expensive human
reference translations. XMEANT is obtained by
(1) using simple lexical translation probabilities,
instead of the monolingual context vector model
used in MEANT for computing the semantic role
fillers similarities, and (2) incorporating bracket-
ing ITG constrains for word alignment within the
semantic role fillers. We conjecture that the rea-
son that XMEANT correlates more closely with
human adequacy judgement than MEANT is that
on the one hand, the semantic structure of the
MT output is closer to that of the input sentence
than that of the reference translation, and on the
other hand, the BITG constraints the word align-
ment more accurately than the heuristic bag-of-
word aggregation used in MEANT. Our results
suggest that MT translation adequacy is more ac-
curately evaluated via the cross-lingual semantic
frame similarities of the input and the MT output
which may obviate the need for expensive human
reference translations.
The MEANT family of metrics (Lo and Wu,
2011a, 2012; Lo et al, 2012) adopt the princi-
ple that a good translation is one where a human
can successfully understand the central meaning
of the foreign sentence as captured by the basic
event structure: ?who did what to whom, when,
where and why? (Pradhan et al, 2004). MEANT
measures similarity between the MT output and
the reference translations by comparing the simi-
larities between the semantic frame structures of
output and reference translations. It is well estab-
lished that the MEANT family of metrics corre-
lates better with human adequacy judgments than
commonly used MT evaluation metrics (Lo and
Wu, 2011a, 2012; Lo et al, 2012; Lo and Wu,
2013b; Mach?a?cek and Bojar, 2013). In addition,
the translation adequacy across different genres
(ranging from formal news to informal web fo-
rum and public speech) and different languages
(English and Chinese) is improved by replacing
BLEU or TER with MEANT during parameter
tuning (Lo et al, 2013a; Lo and Wu, 2013a; Lo
et al, 2013b).
In order to continue driving MT towards better
translation adequacy by deeply integrating seman-
tic frame criteria into theMT training pipeline, it is
necessary to have a cross-lingual semantic objec-
tive function that assesses the semantic frame sim-
ilarities of input and output sentences. We there-
fore propose XMEANT, a cross-lingual MT evalu-
ation metric, that modifies MEANT using (1) sim-
ple translation probabilities (in our experiments,
765
from quick IBM-1 training), to replace the mono-
lingual context vector model in MEANT, and (2)
constraints from BITGs (bracketing ITGs). We
show that XMEANT assesses MT adequacy more
accurately than MEANT (as measured by correla-
tion with human adequacy judgement) without the
need for expensive human reference translations in
the output language.
2 Related Work
2.1 MT evaluation metrics
Surface-form oriented metrics such as BLEU (Pa-
pineni et al, 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), CDER
(Leusch et al, 2006), WER (Nie?en et al, 2000),
and TER (Snover et al, 2006) do not correctly re-
flect the meaning similarities of the input sentence.
In fact, a number of large scale meta-evaluations
(Callison-Burch et al, 2006; Koehn and Monz,
2006) report cases where BLEU strongly dis-
agrees with human judgments of translation ade-
quacy.
This has caused a recent surge of work to de-
velop better ways to automatically measure MT
adequacy. Owczarzak et al (2007a,b) improved
correlation with human fluency judgments by us-
ing LFG to extend the approach of evaluating syn-
tactic dependency structure similarity proposed by
Liu and Gildea (2005), but did not achieve higher
correlation with human adequacy judgments than
metrics like METEOR. TINE (Rios et al, 2011) is
a recall-oriented metric which aims to preserve the
basic event structure but it performs comparably
to BLEU and worse than METEOR on correlation
with human adequacy judgments. ULC (Gim?enez
and M`arquez, 2007, 2008) incorporates several
semantic features and shows improved correla-
tion with human judgement on translation quality
(Callison-Burch et al, 2007, 2008) but no work
has been done towards tuning an SMT system us-
ing a pure form of ULC perhaps due to its expen-
sive run time. Similarly, SPEDE (Wang and Man-
ning, 2012) predicts the edit sequence for match-
ing the MT output to the reference via an inte-
grated probabilistic FSM and PDA model. Sagan
(Castillo and Estrella, 2012) is a semantic textual
similarity metric based on a complex textual en-
tailment pipeline. These aggregated metrics re-
quire sophisticated feature extraction steps, con-
tain several dozens of parameters to tune, and em-
ploy expensive linguistic resources like WordNet
Figure 1: Monolingual MEANT algorithm.
or paraphrase tables; the expensive training, tun-
ing, and/or running time makes them hard to in-
corporate into the MT development cycle.
2.2 The MEANT family of metrics
MEANT (Lo et al, 2012), which is the weighted f-
score over the matched semantic role labels of the
automatically aligned semantic frames and role
fillers, that outperforms BLEU, NIST, METEOR,
WER, CDER and TER in correlation with human
adequacy judgments. MEANT is easily portable
to other languages, requiring only an automatic se-
mantic parser and a large monolingual corpus in
the output language for identifying the semantic
structures and the lexical similarity between the
semantic role fillers of the reference and transla-
tion.
Figure 1 shows the algorithm and equations for
computing MEANT. q
0
i,j
and q
1
i,j
are the argument
of type j in frame i in MT and REF respectively.
w
0
i
and w
1
i
are the weights for frame i in MT/REF
respectively. These weights estimate the degree of
contribution of each frame to the overall meaning
of the sentence. w
pred
and w
j
are the weights of
the lexical similarities of the predicates and role
fillers of the arguments of type j of all frame be-
tween the reference translations and the MT out-
766
Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic
shallow semantic parser. The reference and MT output are parsed by an English automatic shallow
semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate.
put. There is a total of 12 weights for the set
of semantic role labels in MEANT as defined in
Lo and Wu (2011b). For MEANT, they are de-
termined using supervised estimation via a sim-
ple grid search to optimize the correlation with
human adequacy judgments (Lo and Wu, 2011a).
For UMEANT (Lo and Wu, 2012), they are es-
timated in an unsupervised manner using relative
frequency of each semantic role label in the refer-
ences and thus UMEANT is useful when human
judgments on adequacy of the development set are
unavailable.
s
i,pred
and s
i,j
are the lexical similarities based
on a context vector model of the predicates and
role fillers of the arguments of type j between the
reference translations and the MT output. Lo et al
(2012) and Tumuluru et al (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al, 2013a; Lo
and Wu, 2013a; Lo et al, 2013b). In this paper,
we employ a newer version of MEANT that uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of seman-
tic role fillers, as our experiments indicate this is
more accurate than the previously used aggrega-
tion functions.
Recent studies (Lo et al, 2013a; Lo and Wu,
2013a; Lo et al, 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
2.3 MT quality estimation
Evaluating cross-lingual MT quality is similar to
the work of MT quality estimation (QE). Broadly
speaking, there are two different approaches to
QE: surface-based and feature-based.
Token-based QE models, such as those in Gan-
drabur et al (2006) and Ueffing and Ney (2005)
fail to assess the overall MT quality because trans-
lation goodness is not a compositional property. In
contrast, Blatz et al (2004) introduced a sentence-
level QE system where an arbitrary threshold is
used to classify the MT output as good or bad.
The fundamental problem of this approach is that
it defines QE as a binary classification task rather
than attempting to measure the degree of goodness
of the MT output. To address this problem, Quirk
(2004) related the sentence-level correctness of the
QE model to human judgment and achieved a high
correlation with human judgement for a small an-
notated corpus; however, the proposed model does
not scale well to larger data sets.
Feature-based QE models (Xiong et al, 2010;
He et al, 2011; Ma et al, 2011; Specia, 2011;
Avramidis, 2012; Mehdad et al, 2012; Almaghout
and Specia, 2013; Avramidis and Popovi?c, 2013;
Shah et al, 2013) throw a wide range of linguis-
tic and non-linguistic features into machine learn-
767
Figure 3: Cross-lingual XMEANT algorithm.
ing algorithms for predicting MT quality. Al-
though the feature-based QE system of Avramidis
and Popovi?c (2013) slightly outperformed ME-
TEOR on correlation with human adequacy judg-
ment, these ?black box? approaches typically lack
representational transparency, require expensive
running time, and/or must be discriminatively re-
trained for each language and text type.
3 XMEANT: a cross-lingual MEANT
Like MEANT, XMEANT aims to evaluate how
well MT preserves the core semantics, while
maintaining full representational transparency.
But whereas MEANT measures lexical similar-
ity using a monolingual context vector model,
XMEANT instead substitutes simple cross-lingual
lexical translation probabilities.
XMEANT differs only minimally from
MEANT, as underlined in figure 3. The same
weights obtained by optimizing MEANT against
human adequacy judgement were used for
XMEANT. The weights can also be estimated in
unsupervised fashion using the relative frequency
of each semantic role label in the foreign input, as
in UMEANT.
To aggregate individual lexical translation prob-
abilities into phrasal similarities between cross-
lingual semantic role fillers, we compared two nat-
ural approaches to generalizing MEANT?s method
of comparing semantic parses, as described below.
3.1 Applying MEANT?s f-score within
semantic role fillers
The first natural approach is to extend MEANT?s
f-score based method of aggregating semantic
parse accuracy, so as to also apply to aggregat-
ing lexical translation probabilities within seman-
tic role filler phrases. However, since we are miss-
ing structure information within the flat role filler
phrases, we can no longer assume an injective
mapping for aligning the tokens of the role fillers
between the foreign input and the MT output. We
therefore relax the assumption and thus for cross-
lingual phrasal precision/recall, we align each to-
ken of the role fillers in the output/input string
to the token of the role fillers in the input/output
string that has the maximum lexical translation
probability. The precise definition of the cross-
lingual phrasal similarities is as follows:
e
i,pred
? the output side of the pred of aligned frame i
f
i,pred
? the input side of the pred of aligned frame i
e
i,j
? the output side of the ARG j of aligned frame i
f
i,j
? the input side of the ARG j of aligned frame i
p(e, f) =
?
t (e|f) t (f |e)
prec
e,f
=
?
e?e
max
f?f
p(e, f)
|e|
rec
e,f
=
?
f?f
max
e?e
p(e, f)
|f|
s
i,pred
=
2 ? prec
e
i,pred
,f
i,pred
? rec
e
i,pred
,f
i,pred
prec
e
i,pred
,f
i,pred
+ rec
e
i,pred
,f
i,pred
s
i,j
=
2 ? prec
e
i,j
,f
i,j
? rec
e
i,j
,f
i,j
prec
e
i,j
,f
i,j
+ rec
e
i,j
,f
i,j
where the joint probability p is defined as the har-
monized the two directions of the translation table
t trained using IBM model 1 (Brown et al, 1993).
prec
e,f
is the precision and rec
e,f
is the recall of
the phrasal similarities of the role fillers. s
i,pred
and s
i,j
are the f-scores of the phrasal similarities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
3.2 Applying MEANT?s ITG bias within
semantic role fillers
The second natural approach is to extend
MEANT?s ITG bias on compositional reorder-
ing, so as to also apply to aggregating lexical
translation probabilities within semantic role filler
phrases. Addanki et al (2012) showed empiri-
cally that cross-lingual semantic role reordering of
the kind that MEANT is based upon is fully cov-
ered within ITG constraints. In Wu et al (2014),
we extend ITG constraints into aligning the tokens
within the semantic role fillers within monolingual
MEANT, thus replacing its previous monolingual
phrasal aggregation heuristic. Here we borrow the
768
idea for the cross-lingual case, using the length-
normalized inside probability at the root of a BITG
biparse (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009) as follows:
G ? ?{A} ,W
0
,W
1
,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 0.25
p (e/f |A) =
1
2
?
t (e|f) t (f |e)
s
i,pred
=
1
1?
ln
(
P
(
A
?
?e
i,pred
/f
i,pred
|G
))
max(|e
i,pred
|,|f
i,pred
|)
s
i,j
=
1
1?
ln
(
P
(
A
?
?e
i,j
/f
i,j
|G
))
max(|e
i,j
|,|f
i,j
|)
where G is a bracketing ITG, whose only nonter-
minal is A, and where R is a set of transduction
rules where e ? W
0
? {?} is an output token
(or the null token), and f ? W
1
? {?} is an in-
put token (or the null token). The rule probabil-
ity function p is defined using fixed probabilities
for the structural rules, and a translation table t
trained using IBM model 1 in both directions. To
calculate the inside probability of a pair of seg-
ments, P
(
A
?
? e/f|G
)
, we use the algorithm de-
scribed in Saers et al (2009). s
i,pred
and s
i,j
are
the length normalized BITG parsing probabilities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
4 Results
Table 1 shows that for human adequacy judgments
at the sentence level, the f-score based XMEANT
(1) correlates significantly more closely than other
commonly used monolingual automatic MT eval-
uation metrics, and (2) even correlates nearly as
well as monolingual MEANT. This suggests that
the semantic structure of the MT output is indeed
closer to that of the input sentence than that of the
reference translation.
Furthermore, the ITG-based XMEANT (1) sig-
nificantly outperforms MEANT, and (2) is an au-
tomatic metric that is nearly as accurate as the
HMEANT human subjective version. This indi-
cates that BITG constraints indeed provide a more
robust token alignment compared to the heuris-
tics previously employed in MEANT. It is also
consistent with results observed while estimating
word alignment probabilities, where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Table 1: Sentence-level correlation with HAJ
(GALE phase 2.5 evaluation data)
Metric Kendall
HMEANT 0.53
XMEANT (BITG) 0.51
MEANT (f-score) 0.48
XMEANT (f-score) 0.46
MEANT (2013) 0.46
NIST 0.29
BLEU/METEOR/TER/PER 0.20
CDER 0.12
WER 0.10
5 Conclusion
We have presented XMEANT, a new cross-lingual
variant of MEANT, that correlates even more
closely with human translation adequacy judg-
ments thanMEANT, without the expensive human
references. This is (1) accomplished by replacing
monolingual MEANT?s context vector model with
simple translation probabilities when computing
similarities of semantic role fillers, and (2) fur-
ther improved by incorporating BITG constraints
for aligning the tokens in semantic role fillers.
While monolingual MEANT alone accurately re-
flects adequacy via semantic frames and optimiz-
ing SMT against MEANT improves translation,
the new cross-lingual XMEANT semantic objec-
tive function moves closer toward deep integration
of semantics into the MT training pipeline.
The phrasal similarity scoring has only been
minimally adapted to cross-lingual semantic role
fillers in this first study of XMEANT. We expect
further improvements to XMEANT, but these first
results already demonstrate XMEANT?s potential
to drive research progress toward semantic SMT.
6 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT con-
tract nos. HR0011-12-C-0014 and HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-
0022 and HR0011-06-C-0023; by the European
Union under the FP7 grant agreement no. 287658;
and by the Hong Kong Research Grants Council
(RGC) research grants GRF620811, GRF621008,
and GRF612806. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of DARPA, the EU, or
RGC.
769
References
Karteek Addanki, Chi-Kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-lingual
verb frame alternations. In 16th Annual Conference
of the European Association for Machine Transla-
tion (EAMT-2012), Trento, Italy, May 2012.
Hala Almaghout and Lucia Specia. A CCG-based qual-
ity estimation metric for statistical machine transla-
tion. In Machine Translation Summit XIV (MT Sum-
mit 2013), Nice, France, 2013.
Eleftherios Avramidis and Maja Popovi?c. Machine
learning methods for comparative and time-oriented
quality estimation of machine translation output. In
8th Workshop on Statistical Machine Translation
(WMT 2013), 2013.
Eleftherios Avramidis. Quality estimation for machine
translation output using linguistic analysis and de-
coding features. In 7th Workshop on Statistical Ma-
chine Translation (WMT 2012), 2012.
Satanjeev Banerjee and Alon Lavie. METEOR: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. Confidence estimation
for machine translation. In 20th international con-
ference on Computational Linguistics, 2004.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, 1993.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. (meta-)
evaluation of machine translation. In Second Work-
shop on Statistical Machine Translation (WMT-07),
2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third
Workshop on Statistical Machine Translation (WMT-
08), 2008.
Julio Castillo and Paula Estrella. Semantic textual sim-
ilarity for MT evaluation. In 7th Workshop on Sta-
tistical Machine Translation (WMT 2012), 2012.
George Doddington. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. In The second international conference on
Human Language Technology Research (HLT ?02),
San Diego, California, 2002.
Simona Gandrabur, George Foster, and Guy Lapalme.
Confidence estimation for nlp applications. ACM
Transactions on Speech and Language Processing,
2006.
Jes?us Gim?enez and Llu??s M`arquez. Linguistic features
for automatic evaluation of heterogenous MT sys-
tems. In Second Workshop on Statistical Machine
Translation (WMT-07), pages 256?264, Prague,
Czech Republic, June 2007.
Jes?us Gim?enez and Llu??s M`arquez. A smorgasbord
of features for automatic MT evaluation. In Third
Workshop on Statistical Machine Translation (WMT-
08), Columbus, Ohio, June 2008.
Yifan He, Yanjun Ma, Andy Way, and Josef van
Genabith. Rich linguistic features for translation
memory-inspired consistent translation. In 13th Ma-
chine Translation Summit (MT Summit XIII), 2011.
Philipp Koehn and Christof Monz. Manual and auto-
matic evaluation of machine translation between eu-
ropean languages. In Workshop on Statistical Ma-
chine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), 2006.
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. In 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on
Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. super-
vised weight estimation for semantic MT evalua-
tion metrics. In Sixth Workshop on Syntax, Seman-
tics and Structure in Statistical Translation (SSST-
6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be
better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT
Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013:
A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Sta-
tistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT
2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by train-
ing against an automatic semantic frame based eval-
770
uation metric. In 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2013),
2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by tun-
ing against Chinese MEANT. In International
Workshop on Spoken Language Translation (IWSLT
2013), 2013.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. Consistent translation using discriminative
learning: a translation memory-inspired approach.
In 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL HLT 2011). Association for Computa-
tional Linguistics, 2011.
Matou?s Mach?a?cek and Ond?rej Bojar. Results of the
WMT13 metrics shared task. In Eighth Workshop on
Statistical Machine Translation (WMT 2013), Sofia,
Bulgaria, August 2013.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
Match without a referee: evaluating mt adequacy
without reference translations. In 7th Workshop on
Statistical Machine Translation (WMT 2012), 2012.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first Na-
tional Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. A evaluation tool for machine trans-
lation: Fast evaluation for MT research. In The
Second International Conference on Language Re-
sources and Evaluation (LREC 2000), 2000.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Dependency-based automatic evaluation for
machine translation. In Syntax and Structure in Sta-
tistical Translation (SSST), 2007.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Evaluating machine translation with LFG de-
pendencies. Machine Translation, 21:95?119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-02), pages 311?318, Philadelphia, Pennsylva-
nia, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow seman-
tic parsing using support vector machines. In Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2004), 2004.
Christopher Quirk. Training a sentence-level machine
translation confidence measure. In Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), Lisbon, Portugal, May 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine: A
metric to assess MT adequacy. In Sixth Workshop on
Statistical Machine Translation (WMT 2011), 2011.
Markus Saers and Dekai Wu. Improving phrase-based
translation via word alignments from stochastic in-
version transduction grammars. In Third Workshop
on Syntax and Structure in Statistical Translation
(SSST-3), Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu. Learning
stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th
International Conference on Parsing Technologies
(IWPT?09), Paris, France, October 2009.
Kashif Shah, Trevor Cohn, and Lucia Specia. An in-
vestigation on the effectiveness of features for trans-
lation quality estimation. In Machine Translation
Summit XIV (MT Summit 2013), Nice, France, 2013.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A study of trans-
lation edit rate with targeted human annotation. In
7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages
223?231, Cambridge, Massachusetts, August 2006.
Lucia Specia. Exploiting objective annotations for
measuring translation post-editing effort. In 15th
Conference of the European Association for Ma-
chine Translation, pages 73?80, 2011.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu.
Accuracy and robustness in measuring the lexical
similarity of semantic role fillers for automatic se-
mantic MT evaluation. In 26th Pacific Asia Confer-
ence on Language, Information, and Computation
(PACLIC 26), 2012.
Nicola Ueffing and Hermann Ney. Word-level con-
fidence estimation for machine translation using
phrase-based translation models. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 763?770, 2005.
Mengqiu Wang and Christopher D. Manning. SPEDE:
Probabilistic edit distance metrics for MT evalua-
tion. In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Dekai Wu, Chi-kiu Lo, Meriem Beloucif, and Markus
Saers. IMEANT: Improving semantic frame based
MT evaluation via inversion transduction grammars.
Forthcoming, 2014.
Dekai Wu. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403, 1997.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detec-
tion for statistical machine translation using linguis-
tic features. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2010),
2010.
Richard Zens and Hermann Ney. A comparative
study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2003),
pages 144?151, Stroudsburg, Pennsylvania, 2003.
771
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 248?252
Manchester, August 2008
Mixing and Blending Syntactic and Semantic Dependencies
Yvonne Samuelsson
Dept. of Linguistics
Stockholm University
yvonne.samuelsson@ling.su.se
Johan Eklund
SSLIS
University College of Bor?as
johan.eklund@hb.se
Oscar T
?
ackstr
?
om
Dept. of Linguistics and Philology
SICS / Uppsala University
oscar@sics.se
Mark Fi
?
sel
Dept. of Computer Science
University of Tartu
fishel@ut.ee
Sumithra Velupillai
Dept. of Computer and Systems Sciences
Stockholm University / KTH
sumithra@dsv.su.se
Markus Saers
Dept. of Linguistics and Philology
Uppsala University
markus.saers@lingfil.uu.se
Abstract
Our system for the CoNLL 2008 shared
task uses a set of individual parsers, a set of
stand-alone semantic role labellers, and a
joint system for parsing and semantic role
labelling, all blended together. The system
achieved a macro averaged labelled F
1
-
score of 79.79 (WSJ 80.92, Brown 70.49)
for the overall task. The labelled attach-
ment score for syntactic dependencies was
86.63 (WSJ 87.36, Brown 80.77) and the
labelled F
1
-score for semantic dependen-
cies was 72.94 (WSJ 74.47, Brown 60.18).
1 Introduction
This paper presents a system for the CoNLL 2008
shared task on joint learning of syntactic and se-
mantic dependencies (Surdeanu et al, 2008), com-
bining a two-step pipelined approach with a joint
approach.
In the pipelined system, eight different syntac-
tic parses were blended, yielding the input for two
variants of a semantic role labelling (SRL) system.
Furthermore, one of the syntactic parses was used
with an early version of the SRL system, to pro-
vide predicate predictions for a joint syntactic and
semantic parser. For the final submission, all nine
syntactic parses and all three semantic parses were
blended.
The system is outlined in Figure 1; the dashed
arrow indicates the potential for using the predi-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Process
Parse + SRL
Process
8 MaltParsers
Parser Blender
Process
2 Pipelined SRLs
SRL Blender
Joint Parser/SRL
Possible
Iteration
Figure 1: Overview of the submitted system.
cate prediction to improve the joint syntactic and
semantic system.
2 Dependency Parsing
The initial parsing system was created using Malt-
Parser (Nivre et al, 2007) by blending eight dif-
ferent parsers. To further advance the syntactic ac-
curacy, we added the syntactic structure predicted
by a joint system for syntactic and semantic depen-
dencies (see Section 3.4) in the blending process.
2.1 Parsers
The MaltParser is a dependency parser genera-
tor, with three parsing algorithms: Nivre?s arc
standard, Nivre?s arc eager (see Nivre (2004)
for a comparison between the two Nivre algo-
rithms), and Covington?s (Covington, 2001). Both
of Nivre?s algorithms assume projectivity, but
the MaltParser supports pseudo-projective parsing
(Nilsson et al, 2007), for projectivization and de-
projectivization.
248
WSJ Brown
Best single parse 85.22% 78.37%
LAS weights 87.00% 80.60%
Learned weights 87.36% 80.77%
Table 1: Labelled attachment score on the two test
sets of the best single parse, blended with weights
set to PoS labelled attachment score (LAS) and
blended with learned weights.
Four parsing algorithms (the two Nivre al-
gorithms, and Covington?s projective and non-
projective version) were used, creating eight
parsers by varying the parsing direction, left-to-
right and right-to-left. The latter was achieved by
reversing the word order in a pre-processing step
and then restoring it in post-processing. For the fi-
nal system, feature models and training parameters
were adapted from Hall et al (2007).
2.2 Blender
The single parses were blended following the pro-
cedure of Hall et al (2007). The parses of each
sentence were combined into a weighted directed
graph. The Chu-Liu-Edmonds algorithm (Chu and
Liu, 1965; Edmonds, 1967) was then used to find
the maximum spanning tree (MST) of the graph,
which was considered the final parse of the sen-
tence. The weight of each graph edge was calcu-
lated as the sum of the weights of the correspond-
ing edges in each single parse tree.
We used a simple iterative weight updating algo-
rithm to learn the individual weights of each single
parser output and part-of-speech (PoS) using the
development set. To construct an initial MST, the
labelled attachment score was used. Each single
weight, corresponding to an edge of the hypoth-
esis tree, was then iteratively updated by slightly
increasing or decreasing the weight, depending on
whether it belonged to a correct or incorrect edge
as compared to the reference tree.
2.3 Results
The results are summarized in Table 1; the parse
with LAS weights and the best single parse
(Nivre?s arc eager algorithm with left-to-right pars-
ing direction) are also included for comparison.
3 Semantic Role Labelling
The SRL system is a pipeline with three chained
stages: predicate identification, argument identifi-
cation, and argument classification. Predicate and
argument identification are treated as binary clas-
sification problems. In a simple post-processing
predicate classification step, a predicted predicate
is assigned the most frequent sense from the train-
ing data. Argument classification is treated as a
multi-class learning problem, where the classes
correspond to the argument types.
3.1 Learning and Parameter Optimization
For learning and prediction we used the freely
available support vector machine (SVM) imple-
mentation LIBSVM (version 2.86) (Chang and
Lin, 2001). The choice of cost and kernel parame-
ter values will often significantly influence the per-
formance of the SVM classifier. We therefore im-
plemented a parameter optimizer based on the DI-
RECT optimization algorithm (Gablonsky, 2001).
It iteratively divides the search space into smaller
hyperrectangles, sampling the objective function
in the centroid of each hyperrectangle, and select-
ing those hyperrectangles that are potentially opti-
mal for further processing. The search space con-
sisted of the SVM parameters to optimize and the
objective function was the cross-validation accu-
racy reported by LIBSVM.
Tests performed during training for predicate
identification showed that the use of runtime opti-
mization of the SVM parameters for nonlinear ker-
nels yielded a higher average F
1
-score effective-
ness. Surprisingly, the best nonlinear kernels were
always outperformed by the linear kernel with de-
fault settings, which indicates that the data is ap-
proximately linearly separable.
3.2 Filtering and Data Set Splitting
To decrease the number of instances during train-
ing, all predicate and argument candidates with
PoS-tags that occur very infrequently in the
training set were filtered out. Some PoS-tags
were filtered out for all three stages, e.g. non-
alphanumerics, HYPH, SYM, and LS. This ap-
proach was effective, e.g. removing more than half
of the total number of instances for predicate pre-
diction.
To speed up the SVM training and allow for
parallelization, each data set was split into several
bins. However, there is a trade-off between speed
and accuracy. Performance consistently deterio-
rated when splitting into smaller bins. The final
system contained two variants, one with more bins
based on a combination of PoS-tags and lemma
frequency information, and one with fewer bins
249
based only on PoS-tag information. The three
learning tasks used different splits. In general, the
argument identification step was the most difficult
and therefore required a larger number of bins.
3.3 Features
We implemented a large number of features (over
50)
1
for the SRL system. Many of them can be
found in the literature, starting from Gildea and
Jurafsky (2002) and onward. All features, except
bag-of-words, take nominal values, which are bi-
narized for the vectors used as input to the SVM
classifier. Low-frequency feature values (except
for Voice, Initial Letter, Number of Words, Rela-
tive Position, and the Distance features), below a
threshold of 20 occurrences, were given a default
value.
We distinguish between single node and node
pair features. The following single node features
were used for all three learning tasks and for both
the predicate and argument node:
2
? Lemma, PoS, and Dependency relation (DepRel) for
the node itself, the parent, and the left and right sibling
? Initial Letter (upper-case/lower-case), Number of
Words, and Voice (based on simple heuristics, only for
the predicate node during argument classification)
? PoS Sequence and PoS bag-of-words (BoW) for the
node itself with children and for the parent with chil-
dren
? Lemma and PoS for the first and last child of the node
? Sequence and BoW of Lemma and PoS for content
words
? Sequence and BoW of PoS for the immediate children?s
content words
? Sequence and BoW of PoS for the parent?s content
words and for the parent?s immediate children
? Sequence and BoW of DepRels for the node itself, for
the immediate children, and for the parent?s immediate
children
All extractors of node pair features, where the pair
consists of the predicate and the argument node,
can be used both for argument identification and
argument classification. We used the following
node pair features:
? Relative Position (the argument is before/after the pred-
icate), Distance in Words, Middle Distance in DepRels
? PoS Full Path, PoS Middle Path, PoS Short Path
1
Some features were discarded for the final system based
on Information Gain, calculated using Weka (Witten and
Frank, 2005).
2
For all features using lemma or PoS the (predicted) split
value is used.
The full path feature contains the PoS-tag of the ar-
gument node, all dependency relations between the
argument node and the predicate node and finally
the PoS-tag of the predicate node. The middle path
goes to the lowest common ancestor for argument
and predicate (this is also the distance calculated
by Middle Distance in DepRels) and the short path
only contains the dependency relation of the argu-
ment and predicate nodes.
3.4 Joint Syntactic and Semantic Parsing
When considering one predicate at a time, SRL be-
comes a regular labelling problem. Given a pre-
dicted predicate, joint learning of syntactic and se-
mantic dependencies can be carried out by simulta-
neously assigning an argument label and a depen-
dency relation. This is possible because we know
a priori where to attach the argument, since there
is only one predicate candidate
3
. The MaltParser
system for English described in Hall et al (2007)
was used as a baseline, and then optimized for this
new task, focusing on feature selection.
A large feature model was constructed, and
backward selection was carried out until no fur-
ther gain could be observed. The feature model of
MaltParser consists of a number of feature types,
each describing a starting point, a path through the
structure so far, and a column of the node arrived
at. The number of feature types was reduced from
37 to 35 based on the labelled F
1
-score.
As parsing is done at the same time as argu-
ment labelling, different syntactic structures risk
being assigned to the same sentence, depending
on which predicate is currently processed. This
means that several, possibly different, parses have
to be combined into one. In this experiment, the
head and the dependency label were concatenated,
and the most frequent one was used. In case of
a tie, the first one to appear was used. The like-
lihood of the chosen labelling was also used as a
confidence measure for the syntactic blender.
3.5 Blending and Post-Processing
Combining the output from several different sys-
tems has been shown to be beneficial (Koomen
et al, 2005). For the final submission, we com-
bined the output of two variants of the pipelined
SRL system, each using different data splits, with
3
The version of the joint system used in the submission
was based on an early predicate prediction. More accurate
predicates would give a major improvement for the results.
250
Test set Pred PoS Labelled F
1
Unlabelled F
1
WSJ All 82.90 90.90
NN* 81.12 86.39
VB* 85.52 96.49
Brown All 67.48 85.49
NN* 58.34 75.35
VB* 73.24 91.97
Table 2: Semantic predicate results on the test sets.
the SRL output of the joint system. A simple uni-
form weight majority vote heuristic was used, with
no combinatorial constraints on the selected argu-
ments. For each sentence, all predicates that were
identified by a majority of the systems were se-
lected. Then, for each selected predicate, its ar-
guments were picked by majority vote (ignoring
the systems not voting for the predicate). The best
single SRL system achieved a labelled F
1
-score
of 71.34 on the WSJ test set and 57.73 on the
Brown test set, compared to 74.47 and 60.18 for
the blended system.
As a final step, we filtered out all verbal and
nominal predicates not in PropBank or NomBank,
respectively, based on the predicted PoS-tag and
lemma. Each lexicon was expanded with lemmas
from the training set, due to predicted lemma er-
rors in the training data. This turned out to be a
successful strategy for the individual systems, but
slightly detrimental for the blended system.
3.6 Results
Semantic predicate results for WSJ and Brown can
be found in Table 2. Table 4 shows the results for
identification and classification of arguments.
4 Analysis and Conclusions
In general, the mixed and blended system performs
well on all tasks, rendering a sixth place in the
CoNLL 2008 shared task. The overall scores for
the submitted system can be seen in Table 3.
4.1 Parsing
For the blended parsing system, the labelled at-
tachment score drops from 87.36 for the WSJ test
set to 80.77 for the Brown test set, while the unla-
belled attachment score only drops from 89.88 to
86.28. This shows that the system is robust with
regards to the overall syntactic structure, even if
picking the correct label is more difficult for the
out-of-domain text.
The parser has difficulties finding the right head
for punctuation and symbols. Apart from errors re-
WSJ + Brown WSJ Brown
Syn + Sem 79.79 80.92 70.49
Syn 86.63 87.36 80.77
Sem 72.94 74.47 60.18
Table 3: Syntactic and semantic scores on the test
sets for the submitted system. The scores, from top
to bottom, are labelled macro F
1
, labelled attach-
ment score and labelled F
1
.
garding punctuation, most errors occur for IN and
TO. A majority of these problems are related to as-
signing the correct dependency. This is not surpris-
ing, since these are categories that focus on form
rather than function.
There is no significant difference in score for left
and right dependencies, presumably because of the
bi-directional parsing. However, the system over-
predicts dependencies to the root. This is mainly
due to the way MaltParser handles tokens not be-
ing attached anywhere during parsing. These to-
kens are by default assigned to the root.
4.2 SRL
Similarly to the parsing results, the blended SRL
system is less robust with respect to labelled F
1
-
score, dropping from 74.47 on the WSJ test set to
60.18 on the Brown test set. The corresponding
drop in unlabelled F
1
-score is from 82.90 to 75.49.
The simple method of picking the most com-
mon sense from the training data works quite well,
but the difference in domain makes it more diffi-
cult to find the correct sense for the Brown corpus.
In the future, a predicate classification module is
needed. For the WSJ corpus, assigning the most
common predicate sense works better with nomi-
nal than with verbal predicates, while verbal pred-
icates are handled better for the Brown corpus.
In general, verbal predicate-argument structures
are handled better than nominal ones, for both
test sets. This is not surprising, since nominal
predicate-argument structures tend to vary more in
their composition.
Since we do not use global constraints for the
argument labelling (looking at the whole argument
structure for each predicate), the system can out-
put the same argument label for a predicate several
times. For the WSJ test set, for instance, the ra-
tio of repeated argument labels is 5.4% in the sys-
tem output, compared to 0.3% in the gold standard.
However, since there are no confidence scores for
predictions it is difficult to handle this in the cur-
rent system.
251
PPOSS(pred) + ARG WSJ F
1
Brown F
1
NN* + A0 61.42 38.99
NN* + A1 67.07 53.10
NN* + A2 57.02 26.19
NN* + A3 63.08 (16.67)
NN* + AM-ADV 4.65 (-)
NN* + AM-EXT 44.78 (40.00)
NN* + AM-LOC 49.45 (-)
NN* + AM-MNR 53.51 21.82
NN* + AM-NEG 79.37 (46.15)
NN* + AM-TMP 67.23 (25.00)
VB* + A0 81.72 73.58
VB* + A1 81.77 67.99
VB* + A2 60.91 50.67
VB* + A3 61.49 (14.28)
VB* + A4 77.84 (40.00)
VB* + AM-ADV 47.49 30.33
VB* + AM-CAU 55.12 (35.29)
VB* + AM-DIR 41.86 37.14
VB* + AM-DIS 71.91 37.04
VB* + AM-EXT 60.38 (-)
VB* + AM-LOC 55.69 37.50
VB* + AM-MNR 49.54 36.25
VB* + AM-MOD 94.85 82.42
VB* + AM-NEG 93.45 77.08
VB* + AM-PNC 50.00 (62.50)
VB* + AM-TMP 69.59 49.07
VB* + C-A1 70.76 55.32
VB* + R-A0 83.68 70.83
VB* + R-A1 68.87 51.43
VB* + R-AM-LOC 38.46 (25.00)
VB* + R-AM-TMP 56.82 (58.82)
Table 4: Semantic argument results on the two
test sets, showing arguments with more than 20
instances in the gold test set (fewer instances for
Brown are given in parentheses).
Acknowledgements
This project was carried out within the course Ma-
chine Learning 2, organized by GSLT (Swedish
National Graduate School of Language Tech-
nology), with additional support from NGSLT
(Nordic Graduate School of Language Technol-
ogy). We thank our supervisors Joakim Nivre,
Bj?orn Gamb?ack and Pierre Nugues for advice and
support. Computations were performed on the
BalticGrid and UPPMAX (projects p2005008 and
p2005028) resources. We thank Tore Sundqvist at
UPPMAX for technical assistance.
References
Chang, Chih-Chung and Chih-Jen Lin, 2001. LIBSVM:
A library for support vector machines.
Chu, Y. J. and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual Association for Computing Machinery
Southeast Conference, Athens, Georgia.
Edmonds, Jack. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233?240.
Gablonsky, J?org M. 2001. Modifications of the DI-
RECT algorithm. Ph.D. thesis, North Carolina State
University, Raleigh, North Carolina.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Hall, Johan, Jens Nilsson, Joakim Nivre, G?uls?en
Eryi?git, Be?ata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, Prague, Czech Republic.
Koomen, Peter, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), Ann Arbor,
Michigan.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, Prague, Czech Republic.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
2(13):95?135.
Nivre, Joakim. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the ACL?04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, Barcelona, Spain.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), Manchester, Great
Britain.
Witten, Ian H. and Eibe Frank. 2005. Data mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, Amsterdam, 2nd edition.
252
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167?171,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Linear Inversion Transduction Grammar Alignments
as a Second Translation Path
Markus SAERS and Joakim NIVRE
Computational Linguistics Group
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
We explore the possibility of using
Stochastic Bracketing Linear Inversion
Transduction Grammars for a full-scale
German?English translation task, both on
their own and in conjunction with align-
ments induced with GIZA++. The ratio-
nale for transduction grammars, the details
of the system and some results are pre-
sented.
1 Introduction
Lately, there has been some interest in using In-
version Transduction Grammars (ITGs) for align-
ment purposes. The main problem with ITGs is the
time complexity, O(Gn6) doesn?t scale well. By
limiting the grammar to a bracketing ITG (BITG),
the grammar constant (G) can be eliminated, but
O(n6) is still prohibitive for large data sets.
There has been some work on approximate in-
ference of ITGs. Zhang et al (2008) present a
method for evaluating spans in the sentence pair
to determine whether they should be excluded or
not. The algorithm has a best case time com-
plexity of O(n3). Saers, Nivre & Wu (2009) in-
troduce a beam pruning scheme, which reduces
time complexity to O(bn3). They also show
that severe pruning is possible without significant
deterioration in alignment quality (as measured
by downstream translation quality). Haghighi et
al. (2009) use a simpler aligner as guidance for
pruning, which reduces the time complexity by
two orders of magnitude. Their work also par-
tially implements the phrasal ITGs for translation-
driven segmentation introduced in Wu (1997), al-
though they only allow for one-to-many align-
ments, rather than many-to-many alignments. A
more extreme approach is taken in Saers, Nivre
& Wu (2010). Not only is the search severely
pruned, but the grammar itself is limited to a lin-
earized form, getting rid of branching within a sin-
gle parse. Although a small deterioration in down-
stream translation quality is noted (compared to
harshly pruned SBITGs), the grammar can be in-
duced in linear time.
In this paper we apply SBLITGs to a full size
German?English WMT?10 translation task. We
also use differentiated translation paths to com-
bine SBLITG translation models with a standard
GIZA++ translation model.
2 Background
A transduction grammar is a grammar that gener-
ates a pair of languages. In a transduction gram-
mar, the terminal symbols consist of pairs of to-
kens where the first is taken from the vocabulary
of one of the languages, and the second from the
vocabulary of the other. Transduction grammars
have to our knowledge been restricted to trans-
duce between languages no more complex than
context-free languages (CFLs). Transduction be-
tween CFLs was first described in Lewis & Stearns
(1968), and then further explored in Aho & Ull-
man (1972). The main motivation for explor-
ing this was to build programming language com-
pilers, which essentially translate between source
code and machine code. There are two types of
transduction grammars between CFLs described in
the computer science literature: simple transduc-
tion grammars (STGs) and syntax-directed trans-
duction grammars (SDTGs). The difference be-
tween them is that STGs are monotone, whereas
SDTGs allow unlimited reordering in rule produc-
tions. Both allow the use of singletons to insert
and delete tokens from either language. A sin-
gleton is a biterminal where one of the tokens is
the empty string (). Neither STGs nor SDTGs
are intuitively useful in translating natural lan-
guages, since STGs have no way to model reorder-
ing, and SDTGs require exponential time to be in-
duced from examples (parallel corpora). Since
167
compilers in general work on well defined, manu-
ally specified programming languages, there is no
need to induce them from examples, so the expo-
nential complexity is not a problem in this setting
? SDTGs can transduce in O(n3) time, so once the
grammar is known they can be used to translate
efficiently.
In natural language translation, the grammar is
generally not known, in fact, state-of-the art trans-
lation systems rely heavily on machine learning.
For transduction grammars, this means that they
have to be induced from parallel corpora.
An inversion transduction grammar (ITG)
strikes a good balance between STGs and SDTGs,
as it allows some reordering, while requiring only
polynomial time to be induced from parallel cor-
pora. The allowed reordering is either the iden-
tity permutation of the production, or the inver-
sion permutation. Restricting the permutations in
this way ensures that an ITG can be expressed in
two-normal form, which is the key property for
avoiding exponential time complexity in biparsing
(parsing of a sentence pair).
An ITG in two-normal form (representing the
transduction between L1 and L2) is written with
identity productions in square brackets, and in-
verted productions in angle brackets. Each such
rule can be construed to represent two (one L1 and
one L2) synchronized CFG rules:
ITGL1,L2 CFGL1 CFGL2
A? [ B C ] A? B C A? B C
A? ? B C ? A? B C A? C B
A? e/f A? e A? f
Inducing an ITG from a parallel corpus is still slow,
as the time complexity is O(Gn6). Several ways
to get around this has been proposed (Zhang et al,
2008; Haghighi et al, 2009; Saers et al, 2009;
Saers et al, 2010).
Taking a closer look at the linear ITGs (Saers et
al., 2010), there are five rules in normal form. De-
composing these five rule types into monolingual
rule types reveals that the monolingual grammars
are linear grammars (LGs):
LITGL1,L2 LGL1 LGL2
A? [ e/f C ] A? e C A? f C
A? [ B e/f ] A? B e A? B f
A? ? e/f C ? A? e C A? C f
A? ? B e/f ? A? B e A? f B
A? / A?  A? 
This means that LITGs are transduction grammars
that transduce between linear languages.
There is also a nice parallel in search time com-
plexities between CFGs and ITGs on the one hand,
and LGs and LITGs on the other. Searching for
all possible parses given a sentence is O(n3) for
CFGs, and O(n2) for LGs. Searching for all possi-
ble biparses given a bisentence is O(n6) for ITGs,
and O(n4) for LITGs. This is consistent with
thinking of biparsing as finding every L2 parse for
every L1 parse. Biparsing consists of assigning a
joint structure to a sentence pair, rather than as-
signing a structure to a sentence.
In this paper, only stochastic bracketing gram-
mars (SBITGs and SBLITGs) were used. A brack-
eting grammar has only one nonterminal symbol,
denoted X . A stochastic grammar is one where
each rule is associated with a probability, such that
?X
?
?
?
?
p(X ? ?) = 1
?
?
While training a Stochastic Bracketing ITG
(SBITG) or LITG (SBLITG) with EM, expectations
of probabilities over the biparse-forest are calcu-
lated. These expectations approach the true prob-
abilities, and can be used as approximations. The
probabilities over the biparse-forest can be used
to select the one-best parse-tree, which in turn
forces an alignment over the sentence pair. The
alignments given by SBITGs and SBLITGs has been
shown to give better translation quality than bidi-
rectional IBM-models, when applied to short sen-
tence corpora (Saers and Wu, 2009; Saers et al,
2009; Saers et al, 2010). In this paper we ex-
plore whether this hold for SBLITGs on standard
sentence corpora.
3 Setup
The baseline system for the shared task was a
phrase based translation model based on bidi-
rectional IBM- (Brown et al, 1993) and HMM-
models (Vogel et al, 1996) combined with the
grow-diag-final-and heuristic. This is
computed with the GIZA++ tool (Och and Ney,
2003) and the Moses toolkit (Koehn et al, 2007).
The language model was a 5-gram SRILM (Stol-
cke, 2002). Parameters in the final translation sys-
tem were determined with Minimum Error-Rate
Training (Och, 2003), and translation quality was
assessed with the automatic measures BLEU (Pap-
ineni et al, 2002) and NIST (Doddington, 2002).
168
Corpus Type Size
German?English Europarl out of domain 1,219,343 sentence pairs
German?English news commentary in-domain 86,941 sentence pairs
English news commentary in-domain 48,653,884 sentences
German?English news commentary in-domain tuning data 2,051 sentence pairs
German?English news commentary in-domain test data 2,489 sentence pairs
Table 1: Corpora available for the German?English translation task after baseline cleaning.
System BLEU NIST
GIZA++ 17.88 5.9748
SBLITG 17.61 5.8846
SBLITG (only Europarl) 17.46 5.8491
SBLITG (only news) 15.49 5.4987
GIZA++ and SBLITG 17.66 5.9650
GIZA++ and SBLITG (only Europarl) 17.58 5.9819
GIZA++ and SBLITG (only news) 17.48 5.9693
Table 2: Results for the German?English translation task.
We chose to focus on the German?English
translation task. The corpora resources available
for that task is summarized in Table 1. We used the
entire news commentary monolingual data con-
catenated with the English side of the Europarl
bilingual data to train the language model. In ret-
rospect, this was probably a bad choice, as others
seem to prefer the use of two language models in-
stead.
We contrasted the baseline system with pure
SBLITG systems trained on different parts of the
training data, as well as combined systems, where
the SBLITG systems were combined with the base-
line system. The combination was done by adding
the SBLITG translation model as a second transla-
tion path to the base line system.
To train our SBLITG systems, we used the algo-
rithm described in Saers et al (2010). We set the
beam size parameter to 50, and ran expectation-
maximization for 10 iterations or until the log-
probability of the training corpus started deterio-
rating. After the grammar was induced we ob-
tained the one-best parse for each sentence pair,
which also dictates a word alignment over that
sentence pair, which we used instead of the word
alignments provided by GIZA++. From that point,
training did not differ from the baseline procedure.
We trained a total of three pure SBLITG system,
one with only the news commentary part of the
corpus, one with only the Europarl part, and one
with both. We also combined all three SBLITG
systems with the baseline system to see whether
the additional translation paths would help.
The system we submitted corresponds to the
?GIZA++ and SBLITG (only news)? system, but
with RandLM (Talbot and Osborne, 2007) as lan-
guage model rather than SRILM. This was because
we lacked the necessary RAM resources to calcu-
late the full SRILM model before the system sub-
mission deadline.
4 Results
The results for the development test set are sum-
marized in Table 2. The submitted system
achieved a BLEU score of 0.1759 and a NIST
score of 5.9579 for cased output on this year?s test
set (these numbers are not comparable to those
in Table 2). To our surprise, adding the addi-
tional phrases as a second translation path does
not seem to help. Instead a small deterioration
in BLEU is noted (0.22?0.40 points), whereas the
differences in NIST are mixed (-0.0098?+0.0071
points). Over all the variations were very small.
The pure SBLITG systems perform consistently
below baseline, which could indicate that the
grammar class is unable to capture the reorderings
found in longer sentence pairs adequately in one
parse. The variation between the pure SBLITG sys-
tems can be explained by the size of the training
data: more data ? better quality.
169
5 Conclusions
We tried to use SBLITGs as word aligners on full
size sentences, which has not been done to date,
and noted that the formalism seems unable to ac-
count for the full complexity of longer sentence
pairs. We also tried combining the translation
models acquired with SBLITG alignments to the
baseline system, and noted very small differences,
tending to a deterioration in quality. The fact that
SBLITGs seem unable to capture the complex re-
lationship between an English and a German sen-
tence in one parse means that we need to find ei-
ther some more complex model or some way to
use the entire parse forest to arrive at the align-
ment.
Acknowledgments
This work was funded by the Swedish Na-
tional Graduate School of Language Technol-
ogy (GSLT), the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under
research grants GRF621008, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency. The com-
putations were performed on UPPMAX resources
under project p2007020.
References
Jeffrey D. Aho, Alfred V. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-
Halll, Englewood Cliffs, NJ.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of Human
Language Technology conference (HLT-2002), San
Diego, California.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 923?
931, Suntec, Singapore, August.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June.
Philip M. Lewis and Richard E. Stearns. 1968. Syntax-
directed transduction. Journal of the Association for
Computing Machinery, 15(3):465?488.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 160?167, Sapporo, Japan, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, July.
Markus Saers and Dekai Wu. 2009. Improving phrase-
based translation via word alignments from Stochas-
tic Inversion Transduction Grammars. In Proceed-
ings of the Third Workshop on Syntax and Structure
in Statistical Translation (SSST-3) at NAACL HLT
2009, pages 28?36, Boulder, Colorado, June.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning Stochastic Bracketing Inversion Transduc-
tion Grammars with a cubic time biparsing algo-
rithm. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT?09), pages
29?32, Paris, France, October.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with Stochastic Bracketing Linear
Inversion Transduction Grammar. In Proceedings of
Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Los An-
geles, California, June.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
512?519, Prague, Czech Republic, June.
170
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics, pages 836?841, Mor-
ristown, New Jersey.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June.
171
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10?18,
COLING 2010, Beijing, August 2010.
A Systematic Comparison between Inversion Transduction Grammar
and Linear Transduction Grammar for Word Alignment
Markus Saers and Joakim Nivre
Dept. of Linguistics & Philology
Uppsala University
first.last@lingfil.uu.se
Dekai Wu
HKUST
Human Language Technology Center
Dept. of Computer Science & Engineering
Hong Kong Univ. of Science & Technology
dekai@cs.ust.hk
Abstract
We present two contributions to gram-
mar driven translation. First, since both
Inversion Transduction Grammar and
Linear Inversion Transduction Gram-
mars have been shown to produce bet-
ter alignments then the standard word
alignment tool, we investigate how the
trade-off between speed and end-to-end
translation quality extends to the choice
of grammar formalism. Second, we
prove that Linear Transduction Gram-
mars (LTGs) generate the same transduc-
tions as Linear Inversion Transduction
Grammars, and present a scheme for ar-
riving at LTGs by bilingualizing Linear
Grammars. We also present a method for
obtaining Inversion Transduction Gram-
mars from Linear (Inversion) Transduc-
tion Grammars, which can speed up
grammar induction from parallel corpora
dramatically.
1 Introduction
In this paper we introduce Linear Transduction
Grammars (LTGs), which are the bilingual case
of Linear Grammars (LGs). We also show that
LTGs are equal to Linear Inversion Transduction
Grammars (Saers et al, 2010). To be able to in-
duce transduction grammars directly from par-
allel corpora an approximate search for parses is
needed. The trade-off between speed and end-to-
end translation quality is investigated and com-
pared to Inversion Transduction Grammars (Wu,
1997) and the standard tool for word alignment,
GIZA++ (Brown et al, 1993; Vogel et al, 1996;
Och and Ney, 2003). A heuristic for converting
stochastic bracketing LTGs into stochastic brack-
eting ITGs is presented, and fitted into the speed?
quality trade-off.
In section 3 we give an overview of transduc-
tion grammars, introduce LTGs and show that
they are equal to LITGs. In section 4 we give
a short description of the rational for the trans-
duction grammar pruning used. In section 5 we
describe a way of seeding a stochastic bracketing
ITG with the rules and probabilities of a stochas-
tic bracketing LTG. Section 6 describes the setup,
and results are given in section 7. Finally, some
conclusions are offered in section 8
2 Background
Any form of automatic translation that relies on
generalizations of observed translations needs to
align these translations on a sub-sentential level.
The standard way of doing this is by aligning
words, which works well for languages that use
white space separators between words. The stan-
dard method is a combination of the family of
IBM-models (Brown et al, 1993) and Hidden
Markov Models (Vogel et al, 1996). These
methods all arrive at a function (A) from lan-
guage 1 (F ) to language 2 (E). By running the
process in both directions, two functions can be
estimated and then combined to form an align-
ment. The simplest of these combinations are in-
tersection and union, but usually, the intersection
is heuristically extended. Transduction gram-
mars on the other hand, impose a shared struc-
ture on the sentence pairs, thus forcing a consis-
tent alignment in both directions. This method
10
has proved successful in the settings it has been
tried (Zhang et al, 2008; Saers and Wu, 2009;
Haghighi et al, 2009; Saers et al, 2009; Saers
et al, 2010). Most efforts focus on cutting down
time complexity so that larger data sets than toy-
examples can be processed.
3 Transduction Grammars
Transduction grammars were first introduced in
Lewis and Stearns (1968), and further devel-
oped in Aho and Ullman (1972). The origi-
nal notation called for regular CFG-rules in lan-
guage F with rephrased E productions, either in
curly brackets, or comma separated. The bilin-
gual version of CFGs is called Syntax-Directed
Transduction Grammars (SDTGs). To differenti-
ate identical nonterminal symbols, indices were
used (the bag of nonterminals for the two pro-
ductions are equal by definition).
A ? B(1) a B(2) {x B(1) B(2)}
= A ? B(1) a B(2), x B(1) B(2)
The semantics of the rules is that one nontermi-
nal rewrites into a bag of nonterminals that is dis-
tributed independently in the two languages, and
interspersed with any number of terminal sym-
bols in the respective languages. As with CFGs,
the terminal symbols can be factored out into
preterminals with the added twist that they are
shared between the two languages, since preter-
minals are formally nonterminals. The above
rule can thus be rephrased as
A ? B(1) Xa/x B(2), Xa/x B(1) B(2)
Xa/x ? a, x
In this way, rules producing nonterminals and
rules producing terminals can be separated.
Since only nonterminals are allowed to move,
their movement can be represented as the orig-
inal sequence of nonterminals and a permutation
vector as follows:
A ? B Xa/x B ; 1, 0, 2
Xa/x ? a, x
To keep the reordering as monotone as possible,
the terminals a and x can be produced separately,
but doing so eliminates any possibility of param-
eterizing their lexical relationship. Instead, the
individual terminals are pair up with the empty
string (?).
A ? Xx B Xa B ; 0, 1, 2, 3
Xa ? a, ?
Xx ? ?, x
Lexical rules involving the empty string are re-
ferred to as singletons. Whenever a preterminal
is used to pair up two terminal symbols, we refer
to that pair of terminals as a biterminal, which
will be written as e/f .
Any SDTG can be rephrased to contain per-
muted nonterminal productions and biterminal
productions only, and we will call this the nor-
mal form of SDTGs. Note that it is not possi-
ble to produce a two-normal form for SDTGs,
as there are some rules that are not binarizable
(Wu, 1997; Huang et al, 2009). This is an
important point to make, since efficient parsing
for CFGs is based on either restricting parsing
to only handle binary grammars (Cocke, 1969;
Kasami, 1965; Younger, 1967), or rely on on-
the-fly binarization (Earley, 1970). When trans-
lating with a grammar, parsing only has to be
done in F , which is binarizable (since it is a
CFG), and can therefor be computed in polyno-
mial time (O(n3)). Once there is a parse tree
for F , the corresponding tree for E can be eas-
ily constructed. When inducing a grammar from
examples, however, biparsing (finding an anal-
ysis that is consistent across a sentence pair) is
needed. The time complexity for biparsing with
SDTGs is O(n2n+2), which is clearly intractable.
Inversion Transduction Grammars or ITGs
(Wu, 1997) are transduction grammars that have
a two-normal form, thus guaranteeing binariz-
ability. Defining the rank of a rule as the number
of nonterminals in the production, and the rank
of a grammar as the highest ranking rule in the
rule set, ITGs are a) any SDTG of rank two, b)
any SDTG of rank three or c) any SDTG where no
rule has a permutation vector other than identity
permutation or inversion permutation. It follows
from this definition that ITGs have a two-normal
form, which is usually expressed as SDTG rules,
11
with brackets around the production to distin-
guish the different kinds of rules from each other.
A ? B C ; 0, 1 = A ? [ B C ]
A ? B C ; 1, 0 = A ? ? B C ?
A ? e/f = A ? e/f
By guaranteeing binarizability, biparsing time
complexity becomes O(n6).
There is an even more restricted version of
SDTGs called Simple Transduction Grammar
(STG), where no permutation at all is allowed,
which can also biparse a sentence pair in O(n6)
time.
A Linear Transduction Grammar (LTG) is a
bilingual version of a Linear Grammar (LG).
Definition 1. An LG in normal form is a tuple
GL = ?N,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols, R is a finite
set of rules and S ? N is the designated start
symbol. The rule set is constrained so that
R ? N ? (? ? {?})N(? ? {?}) ? {?}
Where ? is the empty string.
To bilingualize a linear grammar, we will take
the same approach as taken when a finite-state
automaton is bilingualized into a finite-state
transducer. That is: to replace all terminal sym-
bols with biterminal symbols.
Definition 2. An LTG in normal form is a tuple
T GL = ?N,?,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols in language
E, ? is a finite set of terminal symbols in lan-
guage F , R is a finite set of linear transduction
rules and S ? N is the designated start symbol.
The rule set is constrained so that
R ? N ??N? ? {??, ??}
Where ? = ??{?}???{?} and ? is the empty
string.
Graphically, we will represent LTG rules as pro-
duction rules with biterminals:
?A, ?x, p?B?y, q?? = A ? x/p B y/q
?A, ??, ??? = B ? ?/?
Like STGs, LTGs do not allow any reordering,
and are monotone, but because they are linear,
this has no impact on expressiveness, as we shall
see later.
Linear Inversion Transduction Grammars
(LITGs) were introduced in Saers et al (2010),
and represent ITGs that are allowed to have at
most one nonterminal symbol in each produc-
tion. These are attractive because they can bi-
parse a sentence pair in O(n4) time, which can
be further reduced to linear time by severely
pruning the search space. This makes them
tractable for large parallel corpora, and a viable
way to induce transduction grammars from large
parallel corpora.
Definition 3. An LITG in normal form is a tuple
T GLI = ?N,?,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols from lan-
guage E, ? is a finite set of terminal symbols
from language F , R is a set of rules and S ? N
is the designated start symbol. The rule set is
constrained so that
R ? N ? {[], ??} ??N ?N? ? {??, ??}
Where [] represents identity permutation and ??
represents inversion permutation, ? = ??{?}?
? ? {?} is a possibly empty biterminal, and ? is
the empty string.
Graphically, a rule will be represented as an ITG
rule:
?A, [], B?e, f?? = A ? [ B e/f ]
?A, ??, ?e, f?B? = A ? ? e/f B ?
?A, [], ??, ??? = A ? ?/?
As with ITGs, productions with only biterminals
will be represented without their permutation, as
any such rule can be trivially rewritten into in-
verted or identity form.
12
Definition 4. An ?-free LITG is an LITG where
no rule may rewrite one nonterminal into another
nonterminal only. Formally, the rule set is con-
strained so that
R ?N ? {[], ??} ? ({??, ??}B ?B{??, ??}) = ?
The LITG presented in Saers et al (2010) is
thus an ?-free LITG in normal form, since it has
the following thirteen rule forms (of which 8 are
meaningful, 1 is only used to terminate genera-
tion and 4 are redundant):
A ? [ e/f B ]
A ? ? e/f B ?
A ? [ B e/f ]
A ? ? B e/f ?
A ? [ e/? B ] | A ? ? e/? B ?
A ? [ B e/? ] | A ? ? B e/? ?
A ? [ ?/f B ] | A ? ? B ?/f ?
A ? [ B ?/f ] | A ? ? ?/f B ?
A ? ?/?
All the singleton rules can be expressed either in
straight or inverted form, but the result of apply-
ing the two rules are the same.
Lemma 1. Any LITG in normal form can be ex-
pressed as an LTG in normal form.
Proof. The above LITG can be rewritten in LTG
form as follows:
A ? [ e/f B ] = A ? e/f B
A ? ? e/f B ? = A ? e/? B ?/f
A ? [ B e/f ] = A ? B e/f
A ? ? B e/f ? = A ? ?/f B e/?
A ? [ e/? B ] = A ? e/? B
A ? [ B e/? ] = A ? B e/?
A ? [ ?/f B ] = A ? ?/f B
A ? [ B ?/f ] = A ? B ?/f
A ? ?/? = A ? ?/?
To account for all LITGs in normal form, the fol-
lowing two non-?-free rules also needs to be ac-
counted for:
A ? [ B ] = A ? B
A ? ? B ? = A ? B
Lemma 2. Any LTG in normal form can be ex-
pressed as an LITG in normal form.
Proof. An LTG in normal form has two rules,
which can be rewritten in LITG form, either as
straight or inverted rules as follows
A ? x/p B y/q = A ? [ x/p B? ]
B? ? [ B y/q ]
= A ? ? x/q B? ?
B? ? ? B y/p ?
A ? ?/? = A ? ?/?
Theorem 1. LTGs in normal form and LITGs in
normal form express the same class of transduc-
tions.
Proof. Follows from lemmas 1 and 2.
By theorem 1 everything concerning LTGs is also
applicable to LITGs, and an LTG can be expressed
in LITG form when convenient, and vice versa.
4 Pruning the Alignment Space
The alignment space for a transduction grammar
is the combinations of the parse spaces of the
sentence pair. Let e be the E sentence, and f
be the F sentence. The parse spaces would be
O(|e|2) and O(|f |2) respectively, and the com-
bination of these spaces would be O(|e|2?|f |2),
or O(n4) if we assume n to be proportional
to the sentence lengths. In the case of LTGs,
this space is searched linearly, giving time com-
plexity O(n4), and in the case of ITGs there
is branching within both parse spaces, adding
an order of magnitude each, giving a total time
complexity of O(n6). There is, in other words,
a tight connection between the alignment space
and the time complexity of the biparsing al-
gorithm. Furthermore, most of this alignment
space is clearly useless. Consider the case where
the entire F sentence is deleted, and the entire E
sentence is simply inserted. Although it is pos-
sible that it is allowed by the grammar, it should
have a negligible probability (since it is clearly a
translation strategy that generalize poorly), and
could, for all practical reasons, be ignored.
13
Language pair Bisentences Tokens
Spanish?English 108,073 1,466,132
French?English 95,990 1,340,718
German?English 115,323 1,602,781
Table 1: Size of training data.
Saers et al (2009) present a scheme for prun-
ing away most of the points in the alignment
space. Parse items are binned according to cov-
erage (the total number of words covered), and
each bin is restricted to carry a maximum of b
items. Any items that do not fit in the bins are
excluded from further analysis. To decide which
items to keep, inside probability is used. This
pruning scheme effectively linearizes the align-
ment space, as is will be of size O(nb), regard-
less of what type grammar is used. An ITG can
thus be biparsed in cubic time, and an LTG in lin-
ear time.
5 Seeding an ITG with an LTG
Since LTGs are a subclass of ITGs, it would be
possible to convert an LTG to a ITG. This could
save a lot of time, since LTGs are much faster to
induce from corpora than ITGs.
Converting a BLTG to a BITG is fairly straight
forward. Consider the BLTG rule
X ? [ e/f X ]
To convert it to BITG in two-normal form, the
biterminal has to be factored out. Replacing
the biterminal with a temporary symbol X? , and
introducing a rule that rewrites this temporary
symbol to the replaced biterminal produces two
rules:
X ? [ X? X ]
X? ? e/f
This is no longer a bracketing grammar since
there are two nonterminals, but equating X? to X
restores this property. An analogous procedure
can be applied in the case where the nonterminal
comes before the biterminal, as well as for the
inverting cases.
When converting stochastic LTGs, the proba-
bility mass of the SLTG rule has to be distributed
to two SITG rules. The fact that the LTG rule
X ? ?/? lacks correspondence in ITGs has to
be weighted in as well. In this paper we took the
maximum entropy approach and distributed the
probability mass uniformly. This means defin-
ing the probability mass function p? for the new
SBITG from the probability mass function p of
the original SBLTG such that:
p?(X ? [ X X ]) =
?
e/f
?
???
?
p(X?[ e/f X ])
1?p(X??/?)
+?
p(X?[ X e/f ])
1?p(X??/?)
?
???
p?(X ? ? X X ?) =
?
e/f
?
???
?
p(X?? e/f X ?)
1?p(X??/?)
+?
p(X?? X e/f ?)
1?p(X??/?)
?
???
p?(X ? e/f) =
?
?????????????
?
p(X?[ e/f X ])
1?p(X??/?)
+?
p(X?[ X e/f ])
1?p(X??/?)
+?
p(X?? e/f X ?)
1?p(X??/?)
+?
p(X?? X e/f ?)
1?p(X??/?)
?
?????????????
6 Setup
The aim of this paper is to compare the align-
ments from SBITG and SBLTG to those from
GIZA++, and to study the impact of pruning
on efficiency and translation quality. Initial
grammars will be estimated by counting cooc-
currences in the training corpus, after which
expectation-maximization (EM) will be used to
refine the initial estimate. At the last iteration,
the one-best parse of each sentence will be con-
sidered as the word alignment of that sentence.
In order to keep the experiments comparable,
relatively small corpora will be used. If larger
corpora were used, it would not be possible to get
any results for unpruned SBITGs because of the
prohibitive time complexity. The Europarl cor-
pus (Koehn, 2005) was used as a starting point,
and then all sentence pairs where one of the sen-
tences were longer than 10 tokens were filtered
14
Figure 1: Trade-offs between translation quality (as measured by BLEU) and biparsing time (in
seconds plotted on a logarithmic scale) for SBLTGs, SBITGs and the combination.
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.1234 0.2608 0.2655 0.2653 0.2661 0.2671 0.2663
SBLTG 0.2574 0.2645 0.2631 0.2624 0.2625 0.2633 0.2628
GIZA++ 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597
NIST
SBITG 3.9705 6.6439 6.7312 6.7101 6.7329 6.7445 6.6793
SBLTG 6.6023 6.6800 6.6657 6.6637 6.6714 6.6863 6.6765
GIZA++ 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464
Training times
SBITG 03:10 17:00 38:00 1:20:00 2:00:00 2:40:00 3:20:00
SBLTG 35 1:49 3:40 7:33 9:44 12:13 11:59
Table 2: Results for the Spanish?English translation task.
out (see table 1). The GIZA++ system was built
according to the instructions for creating a base-
line system for the Fifth Workshop on Statistical
Machine Translation (WMT?10),1 but the above
corpora were used instead of those supplied by
the workshop. This includes word alignment
with GIZA++, a 5-gram language model built
with SRILM (Stolcke, 2002) and parameter tun-
ing with MERT (Och, 2003). To carry out the ac-
tual translations, Moses (Koehn et al, 2007) was
used. The SBITG and SBLTG systems were built
in exactly the same way, except that the align-
ments from GIZA++ were replaced by those from
the respective grammars.
In addition to trying out exhaustive biparsing
1http://www.statmt.org/wmt10/
for SBITGs and SBLTGs on three different trans-
lation tasks, several different levels of pruning
were tried (1, 10, 25, 50, 75 and 100). We also
used the grammar induced from SBLTGs with a
beam size of 25 to seed SBITGs (see section 5),
which were then run for an additional iteration
of EM, also with beam size 25.
All systems are evaluated with BLEU (Pap-
ineni et al, 2002) and NIST (Doddington, 2002).
7 Results
The results for the three different translation
tasks are presented in Tables 2, 3 and 4. It is
interesting to note that the trend they portray is
quite similar. When the beam is very narrow,
GIZA++ is better, but already at beam size 10,
both transduction grammars are superior. Con-
15
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.1268 0.2632 0.2654 0.2669 0.2668 0.2655 0.2663
SBLTG 0.2600 0.2638 0.2651 0.2668 0.2672 0.2662 0.2649
GIZA++ 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603
NIST
SBITG 4.0849 6.7136 6.7913 6.8065 6.8068 6.8088 6.8151
SBLTG 6.6814 6.7608 6.7656 6.7992 6.8020 6.7925 6.7784
GIZA++ 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907
Training times
SBITG 03:25 17:00 42:00 1:25:00 2:10:00 2:45:00 3:10:00
SBLTG 31 1:41 3:25 7:06 9:35 13:56 10:52
Table 3: Results for the French?English translation task.
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.0926 0.2050 0.2091 0.2090 0.2091 0.2094 0.2113
SBLTG 0.2015 0.2067 0.2066 0.2073 0.2080 0.2066 0.2088
GIZA++ 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059
NIST
SBITG 3.4297 5.8743 5.9292 5.8947 5.8955 5.9086 5.9380
SBLTG 5.7799 5.8819 5.8882 5.8963 5.9252 5.8757 5.9311
GIZA++ 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668
Training times
SBITG 03:20 17:00 41:00 1:25:00 2:10:00 2:45:00 3:40:00
SBLTG 38 1:58 4:52 8:08 11:42 16:05 13:32
Table 4: Results for the German?English translation task.
sistent with Saers et al (2009), SBITG has a sharp
rise in quality going from beam size 1 to 10,
and then a gentle slope up to beam size 25, af-
ter which it levels out. SBLTG, on the other hand
start out at a respectable level, and goes up a gen-
tle slope from beam size 1 to 10, after which is
level out. This is an interesting observation, as it
suggests that SBLTG reaches its optimum with a
lower beam size (although that optimum is lower
than that of SBITG). The trade-off between qual-
ity and time can now be extended beyond beam
size to include grammar choice. In Figure 1, run
times are plotted against BLEU scores to illus-
trate this trade-off. It is clear that SBLTGs are
indeed much faster than SBITGs, the only excep-
tion is when SBITGs are run with b = 1, but then
the BLEU score is so low that is is not worth con-
sidering.
The time may seem inconsistent between b =
100 and b = ? for SBLTG, but the extra time
for the tighter beam is because of beam manage-
ment, which the exhaustive search doesn?t bother
with.
In table 5 we compare the pure approaches
to one where an LTG was trained during 10 it-
erations of EM and then used to seed (see sec-
16
Translation task System BLEU NIST Total time
SBLTG 0.2631 6.6657 36:40
Spanish?English SBITG 0.2655 6.7312 6:20:00
Both 0.2660 6.7124 1:14:40
SBLTG 0.2651 6.7656 34:10
French?English SBITG 0.2654 6.7913 7:00:00
Both 0.2625 6.7609 1:16:10
SBLTG 0.2066 5.8882 48:52
German?English SBITG 0.2091 5.9292 6:50:00
Both 0.2095 5.9224 1:29:40
Table 5: Results for seeding an SBITG with an SBLTG (Both) compared to the pure approach. Total
time refers to 10 iterations of EM training for SBITG and SBLTG respectively, and 10 iterations of
SBLTG and one iteration of SBITG training for the combined system.
tion 5) an SBITG, which was then trained for
one iteration of EM. Although the differences
are fairly small, German?English and Spanish?
English seem to reach the level of SBITG,
whereas French?English is actually hurt. The
big difference is in time, since the combined sys-
tem needs about a fifth of the time the SBITG-
based system needs. This phenomenon needs to
be more thoroughly examined.
It is also worth noting that GIZA++ was beaten
by an aligner that used less than 20 minutes (less
than 2 minutes per iteration and at most 10 itera-
tions) to align the corpus.
8 Conclusions
In this paper we have introduced the bilingual
version of linear grammar: Linear Transduc-
tion Grammars, and found that they generate the
same class of transductions as Linear Inversion
Transduction Grammars. We have also com-
pared Stochastic Bracketing versions of ITGs and
LTGs to GIZA++ on three word alignment tasks.
The efficiency issues with transduction gram-
mars have been addressed by pruning, and the
conclusion is that there is a trade-off between
run time and translation quality. A part of the
trade-off is choosing which grammar framework
to use, as LTGs are faster but not as good as ITGs.
It also seems possible to take a short-cut in this
trade-off by starting out with an LTG and convert-
ing it to an ITG. We have also showed that it is
possible to beat the translation quality of GIZA++
with a quite fast transduction grammar.
Acknowledgments
This work was funded by the Swedish Na-
tional Graduate School of Language Technol-
ogy (GSLT), the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tracts No. HR0011-06-C-0022 and No. HR0011-
06-C-0023, and the Hong Kong Research
Grants Council (RGC) under research grants
GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views ofthe Defense Ad-
vanced Research Projects Agency. The computa-
tions were performed on UPPMAX resources un-
der project p2007020.
References
Aho, Alfred V. Ullman, Jeffrey D. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-
Halll, Inc., Upper Saddle River, NJ.
Brown, Peter F., Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Cocke, John. 1969. Programming languages and
their compilers: Preliminary notes. Courant Insti-
17
tute of Mathematical Sciences, New York Univer-
sity.
Doddington, George. 2002. Automatic eval-
uation of machine translation quality using n-
gram co-occurrence statistics. In Proceedings of
Human Language Technology conference (HLT-
2002), San Diego, California.
Earley, Jay. 1970. An efficient context-free parsing
algorithm. Communications of the Association for
Comuter Machinery, 13(2):94?102.
Haghighi, Aria, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
923?931, Suntec, Singapore, August.
Huang, Liang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguis-
tics, 35(4):559?595.
Kasami, Tadao. 1965. An efficient recognition
and syntax analysis algorithm for context-free lan-
guages. Technical Report AFCRL-65-00143, Air
Force Cambridge Research Laboratory.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic, June.
Koehn, Philipp. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Machine
Translation Summit X, Phuket, Thailand, Septem-
ber.
Lewis, Philip M. and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the Asso-
ciation for Computing Machinery, 15(3):465?488.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 160?167, Sapporo, Japan, July.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July.
Saers, Markus and Dekai Wu. 2009. Improving
phrase-based translation via word alignments from
Stochastic Inversion Transduction Grammars. In
Proceedings of the Third Workshop on Syntax
and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009, pages 28?36, Boulder, Col-
orado, June.
Saers, Markus, Joakim Nivre, and Dekai Wu. 2009.
Learning Stochastic Bracketing Inversion Trans-
duction Grammars with a cubic time biparsing al-
gorithm. In Proceedings of the 11th International
Conference on Parsing Technologies (IWPT?09),
pages 29?32, Paris, France, October.
Saers, Markus, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with Stochastic Bracketing Linear
Inversion Transduction Grammar. In Proceedings
of Human Language Technologies: The 11th An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
Los Angeles, California, June.
Stolcke, Andreas. 2002. SRILM ? an extensible
language modeling toolkit. In International Con-
ference on Spoken Language Processing, Denver,
Colorado, September.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in sta-
tistical translation. In Proceedings of the 16th con-
ference on Computational linguistics, pages 836?
841, Morristown, New Jersey.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?403.
Younger, Daniel H. 1967. Recognition and parsing
of context-free languages in time n3. Information
and Control, 10(2):189?208.
Zhang, Hao, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June.
18
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 70?78,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Reestimation of Reified Rules in Semiring Parsing and Biparsing
Markus Saers and Dekai Wu
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|dekai}@cs.ust.hk
Abstract
We show that reifying the rules from hyper-
edge weights to first-class graph nodes au-
tomatically gives us rule expectations in any
kind of grammar expressible as a deductive
system, without any explicit algorithm for cal-
culating rule expectations (such as the inside-
outside algorithm). This gives us expecta-
tion maximization training for any grammar
class with a parsing algorithm that can be
stated as a deductive system, for free. Having
such a framework in place accelerates turn-
over time for experimenting with new gram-
mar classes and parsing algorithms?to imple-
ment a grammar learner, only the parse forest
construction has to be implemented.
1 Introduction
We propose contextual probability as a quantity that
measures how often something has been used in
a corpus, and when calculated for rules, it gives
us everything needed to calculate rule expectations
for expectation maximization. For labeled spans in
context-free parses, this quantity is called outside
probability, and in semiring (bi-) parsing, it is called
reverse value. The inside-outside algorithm for rees-
timating context-free grammar rules uses this quan-
tity for the symbols occurring in the parse forest.
Generally, the contextual probability is:
The contextual probability of something
is the sum of the probabilities
of all contexts where it was used.
For symbols participating in a parse, we could state
it like this:
The contextual probability of an item
is the sum of the probabilities
of all contexts where it was used.
. . . which is exactly what we mean with outside
probability. In semiring (bi-) parsing, this quantity
is called reverse value, but in this framework it is
also defined for rules, which means that we could
restate our boxed statement as:
The contextual probability of a rule
is the sum of the probabilities
of all contexts where it was used.
This opens up an interesting line of inquiry into what
this quantity might represent. In this paper we show
that the contextual probabilities of the rules contain
precisely the new information needed in order to cal-
culate the expectations needed to reestimate the rule
probabilities. This line of inquiry was discovered
while working on a preterminalized version of lin-
ear inversion transduction grammars (LITGs), so we
will use these preterminalized LITGs (Saers and Wu,
2011) as an example throughout this paper.
We will start by examining semiring parsing
(parsing as deductive systems over semirings, Sec-
tion 3), followed by a section on how this relates to
weighted hypergraphs, a common representation of
parse forests (Section 4). This reveals a disparity be-
tween weighted hypergraphs and semiring parsing.
It seems like we are forced to choose between the
inside-outside algorithm for context-free grammars
70
on the one side, and the flexibility of grammar for-
malism and parsing algorithm development afforded
by semiring (bi-) parsing. It is, however, possible to
have both, which we will show in Section 5. An
integral part of this unification is the concept of con-
textual probability. Finally, we will offer some con-
clusions in Section 6.
2 Background
A common view on probabilistic parsing?be it
bilingual or monolingual?is that it involves the
construction of a weighted hypergraph (Billot and
Lang, 1989; Manning and Klein, 2001; Huang,
2008). This is an appealing conceptualization, as it
separates the construction of the parse forest (the ac-
tual hypergraph) from the probabilistic calculations
that need to be carried out. The calculations are,
in fact, given by the hypergraph itself. To get the
probability of the sentence (pair) being parsed, one
simply have to query the hypergraph for the value
of the goal node. It is furthermore possible to ab-
stract away the calculations themselves, by defining
the hypergraph over an arbitrary semiring. When the
Boolean semiring is used, the value of the goal node
will be true if the sentence (pair) is a member of the
language (or transduction) defined by the grammar,
and false otherwise. When the probabilistic semir-
ing is used, the probability of the sentence (pair) is
attained, and with the tropical semiring, the proba-
bility of the most likely tree is attained. To further
generalize the building of the hypergraph?the pars-
ing algorithm?a deductive system can be used. By
defining a hand-full of deductive rules that describe
how items can be constructed, the full complexi-
ties of a parsing algorithm can be very succinctly
summarized. Deductive systems to represent parsers
and semirings to calculate the desired values for the
parses were introduced in Goodman (1999).
In this paper we will reify the grammar rules
by moving them from the meta level to the object
level?effectively making them first-class citizens of
the parse trees, which are no longer weighted hyper-
graphs, but mul/add-graphs. This move allows us
to calculate rule expectations for expectation maxi-
mization (Dempster et al, 1977) as part of the pars-
ing process, which significantly shortens turn-over
time for experimenting with different grammar for-
malisms.
Another approach which achieve a similar goal is
to use a expectation semiring (Eisner, 2001; Eisner,
2002; Li and Eisner, 2009). In this semiring, all val-
ues are pairs of probabilities and expectations. The
inside-outside algorithm with the expectation semir-
ing requires the usual inside and outside calcula-
tions over the probability part of the semiring val-
ues, followed by a third traversal over the parse for-
est to populate the expectation part of the semiring
values. The approach taken in this paper also re-
quires the usual inside and outside calculations, but
o third traversal of the parse forest. Instead, the pro-
posed approach requires two passes over the rules
of the grammar per EM iteration. The asymptotic
time complexities are thus equivalent for the two ap-
proaches.
2.1 Notation
We will use w to mean a monolingual sentence,
and index the individual tokens from 0 to |w| ? 1.
This means that w = w0, . . . , w|w|?1. We will fre-
quently use spans from this sentence, and denote
them wi..j , which is to be interpreted as array slices,
that is: including the token at position i, but ex-
cluding the token at position j (the interval [i, j)
over w, or wi, . . . , wj?1). A sentence w thus cor-
responds to the span w0..|w|. We will also assume
that there exists a grammar G = ?N,?, S,R? or a
transduction grammar (over languages L0 and L1)
G = ?N,?,?, S,R? (depending on the context),
where N is the set of nonterminal symbols, ? is a
set of (L0) terminal symbols, ? is a set of (L1) ter-
minal symbols, S ? N is the dedicated start symbol
and R is a set of rules appropriate to the grammar.
A stochastic grammar is further assumed to have a
parameterization function ?, that assigns probabili-
ties to all the rules in R. For general L0 tokens we
will use lower case letters from the beginning of the
alphabet, and for L1 from the end of the alphabet.
For specific sentences we will use e = e0..|e| to rep-
resent an L0 sentence and f = f0..|f | to represent an
L1 sentence.
3 Semiring parsing
Semiring parsing was introduced in Goodman
(1999), as a unifying approach to parsing. The gen-
71
eral idea is that any parsing algorithm can be ex-
pressed as a deductive system. The same algorithm
can then be used for both traditional grammars and
stochastic grammars by changing the semiring used
in the deductive system. This approach thus sepa-
rates the algorithm from the specific calculations it
is used for.
Definition 1. A semiring is a tuple ?A,?,?,0,1?,
where A is the set the semiring is defined over, ? is
an associative, commutative operator over A, with
identity element 0 and ? is an associative operator
over A distributed over ?, with identity element 1.
Semirings can be intuitively understood by consid-
ering the probabilistic semiring: ?R+,+,?, 0, 1?,
that is: the common meaning of addition and
multiplication over the positive real numbers (in-
cluding zero). Although this paper will have a
heavy focus on the probabilistic semiring, sev-
eral other exists. Among the more popular are
the Boolean semiring ?{>,?},?,?,?,>? and the
tropical semiring ?R+ ? {?},min,+,?, 0? (or
?R? ? {??},max,+,??, 0? which can be used
for probabilities in the logarithmic domain).
The deductive systems used in semiring parsing
have three components: an item representation, a
goal item and a set of deductive rules. Taking
CKY parsing (Cocke, 1969; Kasami and Torii, 1969;
Younger, 1967) as an example, the items would have
the form Ai,j , which is to be interpreted as the span
wi..j of the sentence being parsed, labeled with the
nonterminal symbol A. The goal item would be
S0,|w|: the whole sentence labeled with the start
symbol of the grammar. Since the CKY algorithm
is a very simple parsing algorithm, it only has two
deductive rules:
A? a, Ia(wi..j)
Ai,j
0?i?j?|w| (1)
Bi,k, Ck,j , A? BC
Ai,j
(2)
Where Ia(?) is the terminal indicator function for the
semiring. The general form of a deductive rule is
that the conditions (entities over the line) yield the
consequence (the entity under the line) given that
the side conditions (to the right of the line) are satis-
fied. We will make a distinction between conditions
that are themselves items, and conditions that are
not. The non-item conditions will be called axioms,
and are exemplified above by the indicator function
(Ia(wi..j) which has a value that depends only on the
sentence) and the rules (A? a andA? BC which
have values that depends only on the grammar).
The indicator function might seem unnecessary,
but allows us to reason under uncertainty regarding
the input. In this paper, we will assume that we have
perfect knowledge of the input (but for generality,
we will not place it as a side condition). The func-
tion is defined such that:
?a ? ?? : Ia(w) =
{
1 if a = w
0 otherwise
An important concept of semiring parsing is that
the deductive rules also specify how to arrive at the
value of the consequence. Since it is the first value
computed for a node, we will call it ?, and the gen-
eral way to calculate it given a deductive rule and the
?-values of the conditions is:
?(b) =
n?
i=1
?(ai) iff
a1, . . . , an
b
c1,...,cm
If the same consequence can be produced in several
ways, the values are summed using the ? operator:
?(b) =
?
n,a1,...,an
such that
a1,...,an
b
n?
i=1
?(ai)
The ?-values of axioms depend on what kind of ax-
iom it is. For the indicator function, the ?-value is
the value of the function, and for grammar rules, the
?-value is the value assigned to the rule by the pa-
rameterization function ? of the grammar.
The ?-value of a consequence corresponds to the
value of everything leading up to that consequence.
If we are parsing with a context-free grammar and
the probabilistic semiring, this corresponds to the in-
side probability.
3.1 Reverse values
When we want to reestimate rule probabilities, it is
not enough to know the probabilities of arriving at
different consequences, we also need to know how
likely we are to need the consequences as a condi-
tion for other deductions. These values are called
72
S ? A
A0,|e|,0,|f |
,
As,s,u,u, A? /
G
,
Bs?,t,u?,v, B ? [XA], X ? a/x , Ia/x (
es..s?/fu..u? )
As,t,u,v
0?s?s?,
0?u?u?,
Bs,t?,u,v? , B ? [AX], X ? a/x , Ia/x (
et?..t/fv?..v )
As,t,u,v
t??t?|e|,
v??v?|f |,
Bs?,t,u,v? , B ? ?XA?, X ? a/x , Ia/x (
es..s?/fv?..v )
As,t,u,v
0?s?s?,
v??v?|f |,
Bs,t?,u?,v, B ? ?AX?, X ? a/x , Ia/x (
et?..t/fu..u? )
As,t,u,v
t??t?|e|,
0?u?u?
Figure 2: Deductive system describing a PLITG parser. The symbols A, B and S are nonterminal symbols, while X
represents a preterminal symbol.
S ? A
A0,|e|,0,|f |
,
As,s,u,u, A? /
G
,
Bs?,t,u?,v, B ? [a/x A], Ia/x (
es..s?/fu..u? )
As,t,u,v
0?s?s?,
0?u?u?,
Bs,t?,u,v? , B ? [A a/x ], Ia/x (
et?..t/fv?..v )
As,t,u,v
t??t?|e|,
v??v?|f |,
Bs?,t,u,v? , B ? ?a/x A?, Ia/x (
es..s?/fv?..v )
As,t,u,v
0?s?s?,
v??v?|f |,
Bs,t?,u?,v, B ? ?A a/x ?, Ia/x (
et?..t/fu..u? )
As,t,u,v
t??t?|e|,
0?u?u?
Figure 1: Deductive system describing an LITG parser.
reverse values in Goodman (1999), and outside
probabilities in the inside-outside algorithm (Baker,
1979). In this paper we will call them contextual
values, or ?-values (since they are the second value
we calculate).
The way to calculate the reverse values is to start
with the goal node and work your way back to the
axioms. The reverse value is calculated to be:
?(x) =
?
n,i,b,a1,...,an
such that
a1,...,an
b ?x=ai
?(b)?
?
{j|1?j?n,j 6=i}
?(aj)
That is: the reverse value of the consequence com-
bined with the values of all sibling conditions is cal-
culated and summed for all deductive rules where
the item is a condition.
3.2 SPLITG
After we introduced stochastic preterminalized
LITGs (Saers, 2011, SPLITG), the idea of express-
ing them in term of semiring parsing occurred. This
is relatively straight forward, producing a compact
set of deductive rules similar to that of LITGs. For
LITGs, the items take the form of bispans labeled
with a symbol. We will represent these bispans as
As,t,u,v, where A is the label, and the two spans be-
ing labeled are es..t and fu..v. Since we usually do
top-down parsing, the goal item is a virtual item (G)
than can only be reached by rewriting a nontermi-
nal to the empty bistring ( / ). Figure 1 shows the
deductive rules for LITG parsing.
A preterminalized LITG promote preterminal
symbols to a distinct class of symbols in the gram-
mar, which is only allowed to rewrite into bitermi-
nals. Factoring out the terminal productions in this
fashion allows the grammar to define one probability
distribution over all the biterminals, which is useful
for bilexica induction. It also means that the LITG
rules that produce biterminals have to be replaced
by two rules in a PLITG, resulting in the deductive
rules in Figure 2.
4 Weighted hypergraphs
A hypergraph is a graph where the nodes are con-
nected with hyperedges. A hyperedge is an edge
that can connect several nodes with one node?it has
73
Figure 3: A weighted hyperedge between three nodes,
based on the rule A ? BC. The tip of the arrow points
to the head of the edge, and the two ends are the tails. The
dashed line idicates where the weight of the edge comes
from.
one head, but may have any number of tails. Intu-
itively, this is a good match to context-free gram-
mars, since each rule connects one symbol on the
left hand side (the head of the hyperedge) with any
number of symbols on the right hand side (the tails
of the hyperedge). During parsing, one node is con-
structed for each labeled (bi-) span, and the nodes
are connected with hyperedges based on the valid
applications of rules. A hyperedge will be repre-
sented as [h : t1, . . . , tn] where h is the head and ti
are the tails.
When this is applied to weighted grammar, each
hyperedge can be associated with a weight, making
the hypergraph weighted. Every time an edge is tra-
versed, its weight is combined with the value travel-
ling through the edge. Weights are assigned to hy-
peredges via a weighting function w(?).
Figure 3 contains an illustration of a weighted
hyperedge. The arrow indicates the edge itself,
whereas the dotted line indicates where the weight
comes from. Since each hyperedge corresponds to
exactly one rule from a stochastic context-free gram-
mar, we can use the inside-outside algorithm (Baker,
1979) to calculate inside and outside probabilities as
well as to reestimate the probabilities of the rules.
What we cannot easily do, however, is to change the
parsing algorithm or grammar formalism.
If the weighted hyperedge approach was a one-to-
one mapping to the semiring parsing approach, we
could, but it is not. The main difference is that rules
are part of the object level in semiring parsing, but
Figure 4: The same hyperedge as in Figure 3, where the
rule has been promoted to first-class citizen. The hyper-
edge is no longer weighted.
part of the meta level in weighted hypergraphs. To
address this disparity, we will reify the rules in the
weighted hypergraph to make them nodes. Figure 4
shows the same hyperedge as Figure 3, but with the
rule as a proper node rather than a weight associ-
ated with the hyperedge. These hyperedges are ag-
nostic to what the tail nodes represent, so we can no
longer use the inside-outside algorithm to reestimate
the rule probabilities. We can, however, still calcu-
late inside probabilities. In the weighted hyperedge
approach, the inside probability of a node is:
?(p) =
?
n,q1,...,qn
such that
[p:q1,...qn]
w([p : q1, . . . , qn])?
n?
i=1
?(qi)
Whereas with the rules reified, the weight simply
moved into the tail product:
?(p)
?
n,q1,...,qn
such that
[p:q1,...qn]
n?
i=1
?(qi)
By virtue of the deductive system used to build the
hypergraph, we also have the reverse values, which
correspond to outside probability:
?(x) =
?
i,p,n,q1,...,qn
such that
[p:q1,...qn]?x=qi
?(p)?
?
{j|1?j?n,j 6=i}
?(qj)
This means that we have the inside and outside prob-
abilities of the nodes, and we could shoe-horn it into
the reestimation part of the inside-outside algorithm.
74
It also means that we have ?-values for the rules,
which we are calculating as a side-effect of moving
them into the object level. In Section 5, we will take
a closer look at the semantics of the contextual prob-
abilities that we are in fact calculating for the reified
rules, and see how they can be used in reestimation
of the rules.
4.1 SPLITG
Using the hypergraph parsing framework for
SPLITGs turns out to be non-trivial. Where the stan-
dard LITG uses one rule to rewrite a nonterminal into
another nonterminal and a biterminal, the SPLITG
rewrites a nonterminal to a preterminal and a non-
terminal, and rewrites the preterminal into a biter-
minal. This causes problems within the hypergraph
framework, where each rule application should cor-
respond to one hyperedge. As it stands we have two
options:
1. Let each rule correspond to one hyperedge,
which means that we need to introduce preter-
minal nodes into the hypergraph. This has
a clear drawback for bracketing grammars,1
since it is now necessary to keep different sym-
bols apart. It also produces larger hypergraphs,
since the number of nodes is inflated.
2. Let hypergraphs be associated with one or two
rules, which means that we need to redefine hy-
peredges so that there are two different weight-
ing functions: one for the nonterminal weight
and one for the preterminal weight. Although
all hyperedges are associated with one nonter-
minal rule, some hyperedges are not associated
with any preterminal rule, making the pretermi-
nal weighting function partly defined.
Both of these approaches work in practice, but nei-
ther is completely satisfactory since they both rep-
resent work-arounds to shoe-horn the parsing algo-
rithm (as stated in the deductive system) into a for-
malism that is not completely compatible. By reify-
ing the rules into the object level, we rid ourselves
of this inconvenience, as we no longer differentiate
between different types of conditions.
1A bracketing grammar is a grammar where |N | = 1.
5 Reestimation of reified rules
As has been amply hinted at, the contextual prob-
abilities (outside probabilities, reverse values or ?-
values) contain all new information we need about
the rules to reestimate their probability in an expec-
tation maximization (Dempster et al, 1977) frame-
work. To show that this is indeed the case, we
will rewrite the reestimation formulas of the inside-
outside algorithm (Baker, 1979) so that they are
stated in terms of contextual probability for the
rules.
In general, a stochastic context-free grammar can
be estimated from examples of trees generated by
the grammar by means of relative frequency. This
is also true for expectation maximization with the
caveat that we have multiple hypotheses over each
sentence (pair), and therefore calculate expectations
rather than discrete frequency counts. We thus com-
pute the updated parameterization function ?? based
on expectations from the current parameterization
function:
?? (?|p) =
E? [p? ?]
E? [p]
Where p ? N and ? ? {? ? N}+ (or ? ?
{(?????)?N}+ for transduction grammars). The
expectations are calculated from the sentences in a
corpus C:
E? [x] =
?
w?C
E? [x|w]
The exact way of calculating the expectation on x
given a sentence depends on what x is. For nonter-
minal symbols, the expectations are given by:
E? [p|w] =
E? [p,w]
E? [w]
=
?
0?i?j?|w| Pr (pi,j ,w|G)
Pr (w|G)
=
?
0?i?j?|w| ?(pi,j)?(pi,j)
?(S0,|w|)?(S0,|w|)
For nonterminal rules, the expectations are shown in
Figure 5. The most noteworthy step is the last one,
where we use the fact that the summation is over
the equivalence of the rule?s reverse value. Each
75
E? [p? qr|w] =
E? [p? qr,w]
E? [w]
=
?
0?i?k?j?|w| Pr
(
w0..i, pi,j , wj..|w|
?
?G
)
Pr (wi..k|qi,k, G) Pr (wk..j |rk,j , G) ? (qr|p)
Pr (w|G)
=
?
0?i?k?j?|w| ?(pi,j)?(qi,k)?(rk,j)? (qr|p)
?(S0,|w|)?(S0,|w|)
=
? (qr|p)
?
0?i?k?j?|w| ?(pi,j)?(qi,k)?(rk,j)
?(S0,|w|)?(S0,|w|)
=
?(p? qr)?(p? qr)
?(S0,|w|)?(S0,|w|)
Figure 5: Expected values for nonterminal rules in a specific sentence.
E? [p? a|w] =
E? [p? a,w]
E? [w]
=
?
0?i?j?|w| Pr
(
w0..i, pi,j , wj..|w|
?
?G
)
Ia(wi..j)? (a|p)
Pr (w|G)
=
?
0?i?j?|w| ?(pi,j)Ia(wi..j)? (a|p)
?(S0,|w|)?(S0,|w|)
=
? (a|p)
?
0?i?j?|w| ?(pi,j)Ia(wi..j)
?(S0,|w|)?(S0,|w|)
=
?(p? a)?(p? a)
?(S0,|w|)?(S0,|w|)
Figure 6: Expected values of terminal rules in a specific sentence.
?(pi,j)?(qi,k)?(rk,j) term of the summation corre-
sponds to one instance where the rule was used in
the parse. Furthermore, the ? value is the outside
probability of the consequence of the deductive rule
applied, and the two ? values are the inside prob-
abilities of the sibling conditions of that deductive
rule. The entire summation thus corresponds to our
definition of the reverse value of a rule, or its outside
probability.
In Figure 6, the same process is carried out for ter-
minal rules. Again, the summation is over all possi-
ble ways that we can combine the inside probability
of the sibling conditions of the rule with the outside
probability of the consequence.
Since the expected values of both terminal and
nonterminal rules have the same form, we can gen-
eralize the formula for any production ?:
E? [p? ?|w] =
?(p? ?)?(p? ?)
?(S0,|w|)?(S0,|w|)
Finally, plugging it all into the original rule estima-
tion formula, we have:
?? (?|p) =
E? [p? ?]
E? [p]
=
?
w?C
?(p??)?(p??)
?(S0,|w|)?(S0,|w|)
?
w?C
?
0?i?j?|w|
?(pi,j)?(pi,j)
?(S0,|w|)?(S0,|w|)
= ?(p? ?)
?
w?C
?(p??)
?(S0,|w|)?(S0,|w|)
?
w?C
?
0?i?j?|w|
?(pi,j)?(pi,j)
?(S0,|w|)?(S0,|w|)
Rather than keeping track of the expectations of non-
terminals, they can be calculated from the rule ex-
pectations by marginalizing the productions:
E? [p] =
?
?
E? [p? ?]
76
Figure 7: The same hyperedge as in Figures 3 and 4, rep-
resented as a mul/add-subgraph.
5.1 SPLITG
Since this view of EM and parsing generalizes to de-
ductive systems with multiple rules as conditions,
we can apply it to the deductive system of SPLITGs.
It is, however, also interesting to note how the hy-
pergraph view of parsing is changed by this. We
effectively removed the weights from the edges, but
kept the feature that values of nodes depend entirely
on the values connected by incoming hyperedges. If
we assume the values to be from the Boolean semir-
ing, the hypergraphs we ended up with are in fact
and/or-graphs. That is: each node in the hypergraph
corresponds to an or-node, and each hyperedge cor-
responds to an and-node. We note that this can be
generalized to any semiring, since or is equivalent to
? and and is equivalent to ? for the Boolean semir-
ing, we can express a hypergraph over an arbitrary
semiring as a mul/add-graph.2 Figure 7 shows how
a hyperedge looks in this new graph form. The ?-
value of a node is calculated by combining the val-
ues of all incoming edges using the operator of the
node. The ?-values are also calculated using the op-
erator of the node, but with the edges reversed. For
this to work properly, the mul-nodes need to behave
somewhat different from add-nodes: each incoming
edge has to be reversed one at a time, as illustrated
in Figure 8.
6 Conclusions
We have shown that the reification of rules into the
parse forest graphs allows for a unified framework
where all calculations are performed the same way,
2Because it is much easier to pronounce than ?/?-graph.
Figure 8: Reverse values (?) are calculated by track-
ing backwards through all possible paths. This produces
three different paths for the mul/add-subgraph from Fig-
ure 7. Arrows pointing downward propagate ?-values
while arrows pointing upward propagate ?-values.
and where the calculations for the rules encompass
all information needed to reestimate them using ex-
pectation maximization. The contextual probability
of a rule?its outside probability?holds all infor-
mation needed to calculate expectations, which can
be exploited by promoting the rules to first-class cit-
izens of the parse forest. We have also seen how this
reification of the rules helped solve a real transla-
tion problem?induction of stochastic preterminal-
ized linear inversion transduction grammars using
expectation maximization.
Acknowledgments
This work was funded by the Defense Advanced
Research Projects Agency (DARPA) under GALE
Contract Nos. HR0011-06-C-0023 and HR0011-
06-C-0023, and the Hong Kong Research Grants
Council (RGC) under research grants GRF621008,
GRF612806, DAG03/04.EG09, RGC6256/00E, and
RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily re-
flect the views of the Defense Advanced Research
Projects Agency. We would also like to thank the
three anonymous reviewers, whose feedback made
this a better paper.
References
James K. Baker. 1979. Trainable grammars for speech
recognition. In Speech Communication Papers for the
97th Meeting of the Acoustical Society of America,
pages 547?550, Cambridge, Massachusetts.
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
77
of the 27th annual meeting on Association for Compu-
tational Linguistics, ACL?89, pages 143?151, Strouds-
burg, Pennsylvania, USA.
John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Courant Institute of
Mathematical Sciences, New York University.
Arthur Pentland Dempster, Nan M. Laird, and Don-
ald Bruce Rubin. 1977. Maximum likelihood from
incomplete data via the em algorithm. Journal of the
Royal Statistical Society. Series B (Methodological),
39(1):1?38.
Jason Eisner. 2001. Expectation semirings: Flexible
EM for finite-state transducers. In Gertjan van No-
ord, editor, Proceedings of the ESSLLI Workshop on
Finite-State Methods in Natural Language Processing
(FSMNLP). Extended abstract (5 pages).
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 1?8, Philadelphia, July.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Liang Huang. 2008. Forest-based Algorithms in Natu-
ral Language Processing. Ph.D. thesis, University of
Pennsylvania.
Tadao Kasami and Koji Torii. 1969. A syntax-analysis
procedure for unambiguous context-free grammars.
Journal of the Association for Computing Machinery,
16(3):423?431.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 40?51, Singapore,
August.
Christopher D. Manning and Dan Klein. 2001. Parsing
and hypergraphs. In Proceedings of the 2001 Interna-
tional Workshop on Parsing Technologies.
Markus Saers and Dekai Wu. 2011. Principled induction
of phrasal bilexica. In Proceedings of the 15th Annual
Conference of the European Association for Machine
Translation, Leuven, Belgium, May.
Markus Saers. 2011. Translation as Linear Transduc-
tion: Models and Algorithms for Efficient Learning in
Statistical Machine Translation. Ph.D. thesis, Uppsala
University, Department of Linguistics and Philology.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
78
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 372?378,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Uppsala-FBK systems at WMT 2011
Christian Hardmeier
Jo?rg Tiedemann
Uppsala universitet
Inst. fo?r lingvistik och filologi
Uppsala, Sweden
first.last@lingfil.uu.se
Markus Saers
Human Language
Technology Center
Hong Kong Univ. of
Science & Technology
masaers@cs.ust.hk
Marcello Federico
Mathur Prashant
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
lastname@fbk.eu
Abstract
This paper presents our submissions to the
shared translation task at WMT 2011. We
created two largely independent systems
for English-to-French and Haitian Creole-to-
English translation to evaluate different fea-
tures and components from our ongoing re-
search on these language pairs. Key features
of our systems include anaphora resolution,
hierarchical lexical reordering, data selection
for language modelling, linear transduction
grammars for word alignment and syntax-
based decoding with monolingual dependency
information.
1 English to French
Our submission to the English-French task was a
phrase-based Statistical Machine Translation based
on the Moses decoder (Koehn et al, 2007). Phrase
tables were separately trained on Europarl, news
commentary and UN data and then linearly inter-
polated with uniform weights. For language mod-
elling, we used 5-gram models trained with the
IRSTLM toolkit (Federico et al, 2008) on the mono-
lingual News corpus and parts of the English-French
109 corpus. More unusual features of our system
included a special component to handle pronomi-
nal anaphora and the hierarchical lexical reordering
model by Galley and Manning (2008). Selected fea-
tures of our system will be discussed in depth in the
following sections.
1.1 Handling pronominal anaphora
Pronominal anaphora is the use of pronominal ex-
pressions to refer to ?something previously men-
tioned in the discourse? (Strube, 2006). It is a very
common phenomenon found in almost all kinds of
texts. Anaphora can be local to a sentence, or it can
cross sentence boundaries. Standard SMT methods
do not handle this phenomenon in a satisfactory way
at present: For sentence-internal anaphora, they de-
pend on the n-gram language model with its lim-
ited history, while cross-sentence anaphora is left
to chance. We therefore added a word-dependency
model (Hardmeier and Federico, 2010) to our sys-
tem to handle anaphora explicitly.
Our processing of anaphoric pronouns follows
the procedure outlined by Hardmeier and Federico
(2010). We use the open-source coreference resolu-
tion system BART (Broscheit et al, 2010) to link
pronouns to their antecedents in the text. Coref-
erence links are handled differently depending on
whether or not they cross sentence boundaries. If
a coreference link points to a previous sentence, we
process the sentence containing the antecedent with
the SMT system and look up the translation of the
antecedent in the translated output. If the corefer-
ence link is sentence-internal, the translation lookup
is done dynamically by the decoder during search.
In either case, the word-dependency model adds a
feature function to the decoder score representing
the probability of a particular pronoun choice given
the translation of the antecedent.
In our English-French system, this model was
only applied to the inanimate pronouns it and they,
which seemed to be the most promising candidates
for improvement since their French equivalents re-
quire gender marking. It was trained on data au-
tomatically annotated for anaphora taken from the
news-commentary corpus, and the vocabulary of the
predicted pronouns was limited to words recognised
as pronouns by the POS tagger.
372
1.2 Hierarchical lexical reordering
The basic word order model of SMT penalises any
divergence between the order of the words in the in-
put sentence and the order of their translation equiv-
alents in the MT output. All reordering must thus be
driven by the language model when no other reorder-
ing model is present. Lexical reordering models
making certain word order choices in the MT out-
put conditional on the identity of the words involved
have been a standard component in SMT for some
years. The lexical reordering model usually em-
ployed in the Moses decoder was implemented by
Koehn et al (2005). Adopting the perspective of the
SMT decoder, which produces the target sentence
from left to right while covering source phrases in
free order, the model distinguishes between three or-
dering classes, monotone, swap and discontinuous,
depending on whether the source phrases giving rise
to the two last target phrases emitted were adjacent
in the same order, adjacent in swapped order or sep-
arated by other source words. Probabilities for each
ordering class given source and target phrase are
estimated from a word-aligned training corpus and
integrated into MT decoding as extra feature func-
tions.
In our submission, we used the hierarchical lexi-
cal reordering model proposed by Galley and Man-
ning (2008) and recently implemented in the Moses
decoder.1 This model uses the same approach of
classifying movements as monotone, swap or dis-
continuous, but unlike the phrase-based model, it
does not require the source language phrases to be
strictly adjacent in order to be counted as monotone
or swap. Instead, a phrase can be recognised as ad-
jacent to, or swapped with, a contiguous block of
source words that has been segmented into multi-
ple phrases. Contiguous phrase blocks are recog-
nised by the decoder with a shift-reduce parsing al-
gorithm. As a result, fewer jumps are labelled with
the uninformative discontinuous class.
1.3 Data selection from the WMT Giga corpus
One of the supplied language resources for this eval-
uation is the French-English WMT Giga corpus,
1The hierarchical lexical reordering model was imple-
mented in Moses during MT Marathon 2010 by Christian Hard-
meier, Gabriele Musillo, Nadi Tomeh, Ankit Srivastava, Sara
Stymne and Marcello Federico.
 60 80 100 120 140 160 180 200 220 240 260 280
 100  150  200  250  300  350  400 60 80 100 120 140 160 180 200 220 240 260 280LM Perplexity LM size (million 5-grams)Data Selection ThresholdThreshold vs PerplexityThreshold vs LM Size
Figure 1: Perplexity and size of language models trained
on data of the WMT Giga corpus that were selected using
different perplexity thresholds.
aka 109 corpus, a large collection of parallel sen-
tences crawled from Canadian and European Union
sources. While this corpus was too large to be used
for model training with the means at our disposal,
we exploited it as a source of parallel data for trans-
lation model training as well as monolingual French
data for the language model by filtering it down to a
manageable size. In order to extract sentences close
to the news translation task, we applied a simple
data selection procedure based on perplexity. Sen-
tence pairs were selected from the WMT Giga cor-
pus if the perplexity of their French part with respect
to a language model (LM) trained on French news
data was below a given threshold. The rationale is
that text sentences which are better predictable by
the LM should be closer to the news domain. The
threshold was set in a way to capture enough novel
n-grams, from one side, but also to avoid adding too
many irrelevant n-grams. It was tuned by training
a 5-gram LM on the selected data and checking its
size and its perplexity on a development set. In fig-
ure 1 we plot perplexity and size of the WMT Giga
LM for different values of the data-selection thresh-
old. Perplexities are computed on the newstest2009
set. As a good perplexity-size trade-off, the thresh-
old 250 was chosen to estimate an additional 5-gram
LM (WMT Giga 250) that was interpolated with
the original News LM. The resulting improvement
in perplexity is reported in table 1. For translation
model data, a perplexity threshold of 159 was ap-
plied.
373
LM Perplexity OOV rate
News 146.84 0.82
News + WMT Giga 250 130.23 0.71
Table 1: Perplexity reduction after interpolating the News
LM with data selected from the 109 corpus.
newstest
2009 2010 2011
Primary submission 0.246 0.286 0.284
w/o Anaphora handling 0.246 0.286 0.284
WMT Giga data
w/o LM 0.244 0.289 0.280
w/o TM 0.247 0.286 0.282
w/o LM and TM 0.247 0.289 0.278
Lexical reordering
phrase-based reo 0.239 0.281 0.275
no lexical reo 0.239 0.281 0.275
with LDC data 0.254 0.293 0.291
Table 2: Ablation test results (case-sensitive BLEU)
1.4 Results and Ablation tests
Owing to time constraints, we were not able to run
thorough tests on our system before submitting it to
the evaluation campaign. We therefore evaluated the
various components included in a post hoc fashion
by running ablation tests. In each test, we left out
one of the system components to identify its effect
on the overall performance. The results of these tests
are reported in table 2.
Performance-wise, the most important particular-
ity of our SMT system was the hierarchical lexical
reordering model, which led to a sizeable improve-
ment of 0.7, 0.5 and 0.9 BLEU points for the 2009,
2010 and 2011 test sets, respectively. We had previ-
ously seen negative results when trying to apply the
same model to English-German SMT, so its perfor-
mance seems to be strongly dependent on the lan-
guage pair it is used with.
Compared to the scores obtained using the full
system, the anaphora handling system did not have
any effect on the BLEU scores. This result is
similar to our result for English-German transla-
tion (Hardmeier and Federico, 2010). Unfortu-
nately, for English-French, the negative results ex-
tends to the pronoun translation scores (not reported
here), where slightly higher recall with the word-
dependency model was overcompensated by de-
graded precision, so the outcome of the experiments
clearly suggests that the anaphora handling proce-
dure is in need of improvement.
The effect of the WMT Giga language model dif-
fers among the test sets. For the 2009 and 2011
test sets, it results in an improvement of 0.2 and 0.4
BLEU points, respectively, while the 2010 test set
fares better without this additional language model.
However, it should be noted that there may be a
problem with the 2010 test set and the News lan-
guage model, which was used as a component in all
our systems. In particular, upgrading the News LM
data from last year?s to this year?s release led to an
improvement of 4 BLEU points on the 2010 test set
and an unrealistically low perplexity of 73 as com-
pared to 130 for the 2009 test set, which makes us
suspect that the latest News LM data may be tainted
with data from the 2010 test corpus. If this is the
case, the 2010 test set should be considered unreli-
able for LM evaluation. The benefit of adding WMT
Giga data to the translation model is less clear. For
the 2009 and 2010 test sets, this leads to a slight
degradation, but for the 2011 corpus, we obtained
a small improvement.
Our shared task submission did not use the French
Gigaword corpus from the Linguistic Data Consor-
tium (LDC2009T28), which is not freely available
to sites without LDC membership. After the sub-
mission, we ran a contrastive experiment including
a 5-gram model trained on this corpus, which led
to a sizeable improvement of 0.7?0.8 BLEU points
across all test sets.
2 Haitian Creole to English
Our experiments with the Haitian Creole-English
data are independent of the system presented for the
English to French task above. We experimented with
both phrase-based SMT and syntax-based SMT. The
main questions we investigated were i) whether we
can improve word alignment and phrase extraction
for phrase-based SMT and ii) whether we can in-
tegrate dependency parsing into a syntax-based ap-
proach. All our experiments were conducted on the
clean data set using Moses for training and decod-
ing. In the following we will first describe the exper-
iments with phrase-based models and linear trans-
374
duction grammars for word alignment and, there-
after, our findings from integrating English depen-
dency parses into a syntax-based approach.
2.1 Phrase-based SMT
The phrase-based system that we used in this series
of experiments uses a rather traditional setup. For
the translations into English we used the news data
provided for the other translations tasks in WMT
2011 to build a large scale-background language
model. The English data from the Haitian Creole
task were used as a separate domain-specific lan-
guage model. For the other translation direction we
only used the in-domain data provided. We used
standard 5-gram models with Witten-Bell discount-
ing and backoff interpolation for all language mod-
els. For the translation model we applied standard
techniques and settings for phrase extraction and
score estimations. However, we applied two differ-
ent systems for word alignment: One is the standard
GIZA++ toolbox implementing the IBM alignment
models (Och and Ney, 2003) and extensions and the
other is based on transduction grammars which will
briefly be introduced in the next section.
2.1.1 Alignment with PLITGs
By making the assumption that the parallel cor-
pus constitutes a linear transduction (Saers, 2011)2
we can induce a grammar that is the most likely to
have generated the observed corpus. The grammar
induced will generate a parse forest for each sen-
tence pair in the corpus, and each parse tree in that
forest will correspond to an alignment between the
two sentences. Following Saers et al (2010), the
alignment corresponding to the best parse can be ex-
tracted and used instead of other word alignment ap-
proaches such as GIZA++. There are several gram-
mar types that generate linear transductions, and in
this work, stochastic bracketing preterminalized lin-
ear inversion transduction grammars (PLITG) were
used (Saers and Wu, 2011). Since we were mainly
interested in the word alignments, we did not induce
phrasal grammars.
Although alignments from PLITGs may not reach
the same level of translation quality as GIZA++,
they make different mistakes, so both complement
2A transduction is a set of pairs of strings, and thus repre-
sents a relation between two languages.
each other. By duplicating the training corpus and
aligning each copy of the corpus with a different
alignment tool, the phrase extractor seems to be able
to pick the best of both worlds, producing a phrase
table that is superior to one produced with either of
the alignments tools used in isolation.
2.1.2 Results
In the following we present our results on the pro-
vided test set3 for translating into both languages
with phrase-based systems trained on different word
alignments. Table 3 summarises the BLEU scores
obtained.
English-Haitian BLEU phrase-table
GIZA++ 0.2567 3,060,486
PLITG 0.2407 5,007,254
GIZA++ & PLITG 0.2572 7,521,754
Haitian-English BLEU phrase-table
GIZA++ 0.3045 3,060,486
PLITG 0.2922 5,049,280
GIZA++ & PLITG 0.3105 7,561,043
Table 3: Phrase-based SMT (pbsmt) on the Haitian
Creole-English test set with different word alignments.
From the table we can see that phrase-based sys-
tems trained on PLITG alignments performs slightly
worse than the ones trained on GIZA++. However
combining both alignments with the simple data du-
plication technique mentioned earlier produces the
overall best scores in both translation directions.
The fact that both alignments lead to complemen-
tary information can be seen in the size of the phrase
tables extracted (see table 3).
2.2 Syntax-based SMT
We used Moses and its syntax-mode for our exper-
iments with hierarchical phrase-based and syntax-
augmented models. Our main interest was to in-
vestigate the influence of monolingual parsing on
the translation performance. In particular, we tried
to integrate English dependency parses created by
MaltParser (Nivre et al, 2007) trained on the Wall
Street Journal section of the Penn Treebank (Mar-
cus et al, 1993) extended with about 4000 questions
3We actually swapped the development set and the test set
by mistake. But, of course, we never mixed development and
test data in any result reported.
375
from the Question Bank (Judge et al, 2006). The
conversion to dependency trees was done using the
Stanford Parser (de Marneffe et al, 2006). Again,
we ran both translation directions to test our settings
in more than just one task. Interesting here is also
the question whether there are significant differences
when integrating monolingual parses on the source
or on the target side.
The motivation for applying dependency parsing
in our experiments is to use the specific information
carried by dependency relations. Dependency struc-
tures encode functional relations between words that
can be seen as an interface to the semantics of a
sentence. This information is usually not avail-
able in phrase-structure representations. We believe
that this type of information can be beneficial for
machine translation. For example, knowing that a
noun acts as the subject of a sentence is more in-
formative than just marking it as part of a noun
phrase. Whether or not this information can be ex-
plored by current syntax-based machine translation
approaches that are optimised for phrase-structure
representations is a question that we liked to inves-
tigate. For comparison we also trained hierarchical
phrase-based models without any additional annota-
tion.
2.2.1 Converting projective dependency trees
First we needed to convert dependency parses to
a tree representation in order to use our data in
the standard models of syntax-based models imple-
mented in Moses. In our experiments, we used
a parser model that creates projective dependency
graphs that can be converted into tree structures of
nested segments. We used the yield of each word
(referring to that word and its transitive dependents)
to define spans of phrases and their dependency rela-
tions are used as span labels. Furthermore, we also
defined pre-terminal nodes that encode the part-of-
speech information of each word. These tags were
obtained using the HunPos tagger (Hala?csy et al,
2007) trained on the Wall Street Journal section of
the Penn Treebank. Figure 2 illustrates the conver-
sion process. Tagging and parsing is done for all En-
glish data without any manual corrections or optimi-
sation of parameters. After the conversion, we were
able to use the standard training procedures imple-
mented in Moses.
-ROOT- andCC howWRB oldJJ isVBZ yourPRP$ nephewNN ?.
advmoddep possnsubjcc
punctnull
<tree label="null">
<tree label="cc">
<tree label="CC">and</tree>
</tree>
<tree label="dep">
<tree label="advmod">
<tree label="WRB">how</tree>
</tree>
<tree label="JJ">old</tree>
</tree>
<tree label="VBZ">is</tree>
<tree label="nsubj">
<tree label="poss">
<tree label="PRP$">your</tree>
</tree>
<tree label="NN">nephew</tree>
</tree>
<tree label="punct">
<tree label=".">?</tree>
</tree>
</tree>
Figure 2: A dependency graph from the training corpus
and its conversion to a nested tree structure. The yield of
each word in the sentence defines a span with the label
taken from the relation of that word to its head. Part-of-
speech tags are used as additional pre-terminal nodes.
2.2.2 Experimental Results
We ran several experiments with slightly differ-
ent settings. We used the same basic setup for
all of them including the same language models
and GIZA++ word alignments that we have used
for the phrase-based models already. Further, we
used Moses for extracting rules of the syntax-based
translation model. We use standard settings for
the baseline system (=hiero) that does not employ
any linguistic markup. For the models that include
dependency-based trees we changed the maximum
span threshold to a high value of 999 (default: 15)
in order to extract as many rules as possible. This
large degree of freedom is possible due to the oth-
erwise strong constraints on rule flexibility imposed
by the monolingual syntactic markup. Rule tables
are dramatically smaller than for the unrestricted hi-
erarchical models (see table 4).
However, rule restriction by linguistic constraints
usually hurts performance due to the decreased cov-
erage of the rule set. One common way of improving
376
reference Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes. I am ...
pbsmt Do you are letting us die in Ilavach island?s on in Les Cayes. I am ...
hiero do you will let us die in the island Ilavach on the in Les Cayes . I am ...
samt2 Are you going to let us die in the island Ilavach the which is on the Les. My name is ...
reference I?m begging you please help me my situation is very critical.
pbsmt Please help me please. Because my critical situation very much.
hiero please , please help me because my critical situation very much .
samt2 Please help me because my situation very critical.
reference I don?t have money to go and give blood in Port au Prince from La Gonave.
pbsmt I don?t have money, so that I go to give blood Port-au-Prince since lagonave.
hiero I don ?t have any money , for me to go to give blood Port-au-Prince since lagonave .
samt2 I don?t have any money, to be able to go to give blood Port-au-Prince since Gona?ve Island.
Figure 3: Example translations for various models.
English-Haitian BLEU number of rules
hiero 0.2549 34,118,622
malt (source) 0.2180 1,628,496
- binarised 0.2327 9,063,933
- samt1 0.2311 11,691,279
- samt2 0.2366 29,783,694
Haitian-English BLEU number of rules
hiero 0.3034 33,231,535
malt (target) 0.2739 1,922,688
- binarised 0.2857 8,922,343
- samt1 0.2952 11,073,764
- samt2 0.2954 24,554,317
Table 4: Syntax-based SMT on the Haitian Creole-
English test set with (=malt) or without (=hiero) English
parse trees and various parse relaxation strategies. The
final system submitted to WMT11 is malt(target)-samt2.
rule extraction is based on tree manipulation and re-
laxed extraction algorithms. Moses implements sev-
eral algorithms that have been proposed in the lit-
erature. Tree binarisation is one of them. This can
be done in a left-branching and in a right-branching
mode. We used a combination of both in the set-
tings denoted as binarised. The other relaxation al-
gorithms are based on methods proposed for syntax-
augmented machine translation (Zollmann et al,
2008). We used two of them: samt1 combines pairs
of neighbouring children nodes into combined com-
plex nodes and creates additional complex nodes of
all children nodes except the first child and similar
complex nodes for all but the last child. samt2 com-
bines any pair of neighbouring nodes even if they are
not children of the same parent. All of these relax-
ation algorithms lead to increased rule sets (table 4).
In terms of translation performance there seems to
be a strong correlation between rule table size and
translation quality as measured by BLEU. None of
the dependency-based models beats the unrestricted
hierarchical model. Both translation directions be-
have similar with slightly worse performances of
the dependency-based models (relative to the base-
line) when syntax is used on the source language
side. Note also that all syntax-based models (includ-
ing hiero) are below the corresponding phrase-based
SMT systems. Of course, automatic evaluation has
its limits and interesting qualitative differences may
be more visible in manual assessments. The use of
linguistic information certainly has an impact on the
translation hypotheses produced as we can see in the
examples in figure 3. In the future, we plan to inves-
tigate the effect of dependency information on gram-
maticality of translated sentences in more detail.
3 Conclusions
In our English-French and Haitian Creole-English
shared task submissions, we investigated the use
of anaphora resolution, hierarchical lexical reorder-
ing and data selection for language modelling
(English-French) as well as LTG word alignment
and syntax-based decoding with dependency infor-
mation (Haitian Creole-English). While the re-
sults for the systems with anaphora handling were
somewhat disappointing and the effect of data fil-
tering was inconsistent, hierarchical lexical reorder-
ing brought substantial improvements. We also ob-
tained consistent gains by combining information
from different word aligners, and we presented a
simple way of including dependency parses in stan-
dard tree-based decoding.
377
Acknowledgements
Most of the features used in our English-French sys-
tem were originally developed while Christian Hard-
meier was at FBK. Activities at FBK were supported
by the EuroMatrixPlus project (IST-231720) and the
T4ME network of excellence (IST-249119), both
funded by the DG INFSO of the European Commis-
sion through the Seventh Framework Programme.
References
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A multilingual anaphora resolution sys-
tem. In Proceedings of the 5th International Workshop
on Semantic Evaluations (SemEval-2010), Uppsala.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618?1621. ISCA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847?855, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz. 2007.
Hunpos: an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 209?
212.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Marcello Federico, Ian Lane, Michael
Paul, and Franc?ois Yvon, editors, Proceedings of the
seventh International Workshop on Spoken Language
Translation (IWSLT), pages 283?289.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 497?504.
Philipp Koehn, Amittai Axelrod, Alexandra
Birch Mayne, et al 2005. Edinburgh system
description for the 2005 iwslt speech translation
evaluation. In International workshop on spoken
language translation, Pittsburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The Penn Treebank. Computational
Linguistics, 19:313?330, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29:19?51.
Markus Saers and Dekai Wu. 2011. Principled induction
of phrasal bilexica. In Proceedings of the 15th Annual
Conference of the European Association for Machine
Translation, Leuven, Belgium, May.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010. Word
alignment with stochastic bracketing linear inversion
transduction grammar. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 341?344, Los Angeles, California,
June.
Markus Saers. 2011. Translation as Linear Transduc-
tion: Models and Algorithms for Efficient Learning in
Statistical Machine Translation. Ph.D. thesis, Uppsala
University, Department of Linguistics and Philology.
M. Strube. 2006. Anaphora and coreference resolution,
Statistical. In Encyclopedia of language and linguis-
tics, pages 216?222. Elsevier.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics - Volume 1, pages
1145?1152.
378
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48?57,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Combining Top-down and Bottom-up Search
for Unsupervised Induction of Transduction Grammars
Markus SAERS and Karteek ADDANKI and Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|vskaddanki|dekai}@cs.ust.hk
Abstract
We show that combining both bottom-up rule
chunking and top-down rule segmentation
search strategies in purely unsupervised learn-
ing of phrasal inversion transduction gram-
mars yields significantly better translation ac-
curacy than either strategy alone. Previous ap-
proaches have relied on incrementally building
larger rules by chunking smaller rules bottom-
up; we introduce a complementary top-down
model that incrementally builds shorter rules
by segmenting larger rules. Specifically, we
combine iteratively chunked rules from Saers
et al (2012) with our new iteratively seg-
mented rules. These integrate seamlessly be-
cause both stay strictly within a pure trans-
duction grammar framework inducing under
matching models during both training and
testing?instead of decoding under a com-
pletely different model architecture than what
is assumed during the training phases, which
violates an elementary principle of machine
learning and statistics. To be able to drive in-
duction top-down, we introduce a minimum
description length objective that trades off
maximum likelihood against model size. We
show empirically that combining the more lib-
eral rule chunking model with a more conser-
vative rule segmentation model results in sig-
nificantly better translations than either strat-
egy in isolation.
1 Introduction
In this paper we combine both bottom-up chunking
and top-down segmentation as search directions in
the unsupervised pursuit of an inversion transduc-
tion grammar (ITG); we also show that the combi-
nation of the resulting grammars is superior to ei-
ther of them in isolation. For the bottom-up chunk-
ing approach we use the method reported in Saers
et al (2012), and for the top-down segmentation ap-
proach, we introduce a minimum description length
(MDL) learning objective. The new learning objec-
tive is similar to the Bayesian maximum a poste-
riori objective, and makes it possible to learn top-
down, which is impossible using maximum likeli-
hood, as the initial grammar that rewrites the start
symbol to all sentence pairs in the training data al-
ready maximizes the likelihood of the training data.
Since both approaches result in stochastic ITGs, they
can be easily combined into a single stochastic ITG
which allows for seamless combination. The point
of our present work is that the two different search
strategies result in very different grammars so that
the combination of them is superior in terms of trans-
lation accuracy to either of them in isolation.
The transduction grammar approach has the ad-
vantage that induction, tuning and testing are op-
timized on the exact same underlying model?this
used to be a given in machine learning and statistical
prediction, but has been largely ignored in the statis-
tical machine translation (SMT) community, where
most current SMT approaches to learning phrase
translations that (a) require enormous amounts of
run-time memory, and (b) contain a high degree of
redundancy. In particular, phrase-based SMT mod-
els such as Koehn et al (2003) and Chiang (2007)
often search for candidate translation segments and
transduction rules by committing to a word align-
ment that is completely alien to the grammar, as it
is learned with very different models (Brown et al
(1993), Vogel et al (1996)), whose output is then
combined heuristically to form the alignment actu-
ally used to extract lexical segment translations (Och
48
and Ney, 2003). The fact that it is even possible
to improve the performance of a phrase-based di-
rect translation system by tossing away most of the
learned segmental translations (Johnson et al, 2007)
illustrates the above points well.
Transduction grammars can also be induced from
treebanks instead of unannotated corpora, which cuts
down the vast search space by enforcing additional,
external constraints. This approach was pioneered
by Galley et al (2006), and there has been a lot of re-
search since, usually referred to as tree-to-tree, tree-
to-string and string-to-tree, depending on where
the analyses are found in the training data. This com-
plicates the learning process by adding external con-
straints that are bound to match the translation model
poorly; grammarians of English should not be ex-
pected to care about its relationship to Chinese. It
does, however, constitute a way to borrow nonter-
minal categories that help the translation model.
It is also possible for the word alignments leading
to phrase-based SMT models to be learned through
transduction grammars (see for example Cherry and
Lin (2007), Zhang et al (2008), Blunsom et al
(2008), Saers andWu (2009), Haghighi et al (2009),
Blunsom et al (2009), Saers et al (2010), Blunsom
and Cohn (2010), Saers and Wu (2011), Neubig et
al. (2011), Neubig et al (2012)). Even when the
SMT model is hierarchical, most of the information
encoded in the grammar is tossed away, when the
learned model is reduced to a word alignment. A
word alignment can only encode the lexical relation-
ships that exist between a sentence pair according to
a single parse tree, which means that the rest of the
model: the alternative parses and the syntactic struc-
ture, is ignored.
Theminimumdescription length (MDL) objective
that we will be using to drive the learning will pro-
vide a way to escape the maximum-likelihood-of-
the-data-given-the-model optimum that we start out
with. However, going only by MDL will also lead to
a degenerate case, where the size of the grammar is
allowed to shrink regardless of how unlikely the cor-
pus becomes. Instead, we will balance the length of
the grammar with the probability of the corpus given
the grammar. This has a natural Bayesian interpreta-
tion where the length of the grammar acts as a prior
over the structure of the grammar.
Similar approaches have been used before, but to
induce monolingual grammars. Stolcke and Omo-
hundro (1994) use a method similar to MDL called
Bayesianmodel merging to learn the structure of hid-
den Markov models as well as stochastic context-
free grammars. The SCFGs are induced by allowing
sequences of nonterminals to be replaced with a sin-
gle nonterminal (chunking) as well as allowing two
nonterminals to merge into one. Gr?nwald (1996)
uses it to learn nonterminal categories in a context-
free grammar. It has also been used to interpret vi-
sual scenes by classifying the activity that goes on in
a video sequences (Si et al, 2011). Our work in this
paper is markedly different to even the previous NLP
work in that (a) we induce an inversion transduc-
tion grammar (Wu, 1997) rather than a monolingual
grammar, and (b) we focus on learning the terminal
segments rather than the nonterminal categories.
The similar Bayesian approaches to finding the
model structure of ITGs have been tried before, but
only to generate alignments that mismatched trans-
lation models are then trained on, rather than using
the ITG directly as translation model, which we do.
Zhang et al (2008) use variational Bayes with a spar-
sity prior over the parameters to prevent the size of
the grammar to explode when allowing for adjacent
terminals in the Viterbi biparses to chunk together.
Blunsom et al (2008), Blunsom et al (2009) and
Blunsom and Cohn (2010) use Gibbs sampling to
find good phrasal translations. Neubig et al (2011)
and Neubig et al (2012) use a method more similar
to ours, but with a Pitman-Yor process as prior over
the structures.
The idea of iteratively segmenting the existing
sentence pairs to find good phrasal translations has
also been tried before; Vilar and Vidal (2005) intro-
duces the Recursive Alignment Model, which recur-
sively determines whether a bispan is a good enough
translation on its own (using IBM model 1), or if it
should be split into two bispans (either in straight or
inverted order). The model uses length of the input
sentence to determine whether to split or not, and
uses very limited local information about the split
point to determine where to split. Training the pa-
rameters is done with a maximum likelihood objec-
tive. In contrast, our model is one single genera-
tive model (as opposed to an ad hoc model), trained
with a minimum description length objective (rather
than trying to maximize the probability of the train-
49
ing data).
The rest of the paper is structured so that we first
take a closer look at the minimum description length
principle that will be used to drive the top-down
search (Section 2). We then show how the top-down
grammar is learned (Sections 3 and 4), before show-
ing how we combine the new grammar with that of
Saers et al (2012) (Section 5). We then detail the
experimental setup that will substantiate our claims
empirically (Section 6) before interpreting the results
of those experiments (Section 7). Finally, we offer
some conclusions (Section 8).
2 Minimum description length
The minimum description length principle is about
finding the optimal balance between the size of a
model and the size of some data given the model
(Solomonoff (1959), Rissanen (1983)). Consider the
information theoretical problem of encoding some
datawith amodel, and then sending both the encoded
data and the information needed to decode the data
(the model) over a channel; the minimum descrip-
tion length would be the minimum number of bits
sent over the channel. The encoded data can be inter-
preted as carrying the information necessary to dis-
ambiguate the ambiguities or uncertainties that the
model has about the data. Theoretically, the model
can grow in size and become more certain about the
data, and it can shrink in size and become more un-
certain about the data. An intuitive interpretation of
this is that the exceptions, which are a part of the en-
coded data, can be moved into the model itself. By
doing so, the size of the model increases, but there
is no longer an exception that needs to be conveyed
about the data. Some ?exceptions? occur frequently
enough that it is a good idea to incorporate them into
the model, and some do not; finding the optimal bal-
ance minimizes the total description length.
Formally, the description length (DL) is:
DL (M,D) = DL (D|M) + DL (M) (1)
Where M is the model and D is the data. Note the
clear parallel to probabilities that have been moved
into the logarithmic domain.
In natural language processing, we never have
complete data to train on, so we need our models to
generalize to unseen data. A model that is very cer-
tain about the training data runs the risk of not being
able to generalize to new data: it is over-fitting. It
is bad enough when estimating the parameters of a
transduction grammar, and catastrophic when induc-
ing the structure of the grammar. The key concept
that we want to capture when learning the structure
of a transduction grammar is generalization. This is
the property that allow it to translate new, unseen,
input. The challenge is to pin down what general-
ization actually is, and how to measure it.
One property of generalization for grammars is
that it will lower the probability of the training data.
This may seem counterintuitive, but can be under-
stood as moving some of the probability mass away
from the training data and putting it in unseen data.
A second property is that rules that are specific to
the training data can be eliminated from the gram-
mar (or replaced with less specific rules that generate
the same thing). The second property would shorten
the description of the grammar, and the first would
make the description of the corpus given the gram-
mar longer. That is: generalization raises the first
term and lowers the second in Equation 1. A good
generalization will lower the total MDL, whereas a
poor onewill raise it; a good generalizationwill trade
a little data certainty for more model parsimony.
2.1 Measuring the length of a corpus
The information-theoretic view of the problem also
gives a hint at the operationalization of length. Shan-
non (1948) stipulates that the number of bits it takes
to encode that a probabilistic variable has taken a cer-
tain value can be encoded using as little as the nega-
tive logarithmic probability of that outcome.
Following this, the parallel corpus given the trans-
duction grammar gives the number of bits required
to encode it: DL (C|G) = ?log2 (P (C|G)), where
C is the corpus and G is the grammar.
2.2 Measuring the length of an ITG
Since information theory deals with encoding se-
quences of symbols, we need some way to serialize
an inversion transduction grammar (ITG) into a mes-
sage whose length can be measured.
To serialize an ITG, we first need to determine
the alphabet that the message will be written in. We
need one symbol for every nonterminal, L0-terminal
and L1-terminal. We will also make the assump-
tion that all these symbols are used in at least one
50
rule, so that it is sufficient to serialize the rules in
order to express the entire grammar. To serialize
the rules, we need some kind of delimiter to know
where one rule starts and the next ends; we will ex-
ploit the fact that we also need to specify whether the
rule is straight or inverted (unary rules are assumed
to be straight), and merge these two functions into
one symbol. This gives the union of the symbols of
the grammar and the set {[], ??}, where [] signals the
beginning of a straight rule, and ?? signals the be-
ginning of an inverted rule. The serialized format
of a rule will be: rule type/start marker, followed by
the left-hand side nonterminal, followed by all right-
hand side symbols. The symbols on the right-hand
sides are either nonterminals or biterminals?pairs
ofL0-terminals andL1-terminals that model transla-
tion equivalences. The serialized form of a grammar
is the serialized form of all rules concatenated.
Consider the following toy grammar:
S ? A, A ? ?AA?, A ? [AA] ,
A ? have/?, A ? yes/?, A ? yes/?
Its serialized form would be:
[]SA??AAA[]AAA[]Ahave?[]Ayes?[]Ayes?
Now we can, again turn to information theory to ar-
rive at an encoding for this message. Assuming a
uniform distribution over the symbols, each symbol
will require ?log2
(
1
N
)
bits to encode (where N is
the number of different symbols?the type count).
The above example has 8 symbols, meaning that
each symbol requires 3 bits. The entire message is
23 symbols long, which means that we need 69 bits
to encode it.
3 Model initialization
Rather than starting out with a general transduction
grammar and fitting it to the training data, we do the
exact opposite: we start with a transduction gram-
mar that fits the training data as well as possible, and
generalize from there. The transduction grammar
that fits the training data the best is the one where
the start symbol rewrites to the full sentence pairs
that it has to generate. It is also possible to add any
number of nonterminal symbols in the layer between
the start symbol and the bisentences without altering
the probability of the training data. We take advan-
tage of this by allowing for one intermediate sym-
bol so that the start symbol conforms to the normal
form and always rewrites to precisely one nontermi-
nal symbol. This violate the MDL principle, as the
introduction of new symbols, by definition, makes
the description of the model longer, but conforming
to the normal form of ITGs was deemedmore impor-
tant than strictly minimizing the description length.
Our initial grammar thus looks like this:
S ? A,
A ? e0..T0/f0..V0 ,
A ? e0..T1/f0..V1 ,
...,
A ? e0..TN /f0..VN
Where S is the start symbol, A is the nonterminal,
N is the number of sentence pairs in the training cor-
pus, Ti is the length of the ith output sentence (which
makes e0..Ti the ith output sentence), and Vi is the
length of the ith input sentence (which makes f0..Vi
the ith input sentence).
4 Model generalization
To generalize the initial inversion transduction gram-
mar we need to identify parts of the existing biter-
minals that could be validly used in isolation, and
allow them to combine with other segments. This
is the very feature that allows a finite transduction
grammar to generate an infinite set of sentence pairs.
Doing this moves some of the probability mass,
which was concentrated in the training data, to un-
seen data?the very definition of generalization. Our
general strategy is to propose a number of sets of
biterminal rules and a place to segment them, eval-
uate how the description length would change if we
were to apply one of these sets of segmentations to
the grammar, and commit to the best set. That is:
we do a greedy search over the power set of possi-
ble segmentations of the rule set. As we will see, this
intractable problem can be reasonable efficiently ap-
proximated, which is what we have implemented and
tested.
The key component in the approach is the ability
to evaluate how the description length would change
if a specific segmentation was made in the grammar.
51
This can then be extended to a set of segmentations,
which only leaves the problem of generating suitable
sets of segmentations.
The key to a successful segmentation is to maxi-
mize the potential for reuse. Any segment that can
be reused saves model size. Consider the terminal
rule:
A ? five thousand yen is my limit/
????????
(Chinese gloss: ?w? z?i d?o ch? w? q?an r? y?an?).
This rule can be split into three rules:
A ? ?AA?,
A ? five thousand yen/????,
A ? is my limit/????
Note that the original rule consists of 16 symbols (in
our encoding scheme), whereas the new three rules
consists of 4 + 9 + 9 = 22 symbols. It is reason-
able to believe that the bracketing inverted rule is in
the grammar already, but this still leaves 18 symbols,
which is decidedly longer than 16 symbols?and we
need to get the length to be shorter if we want to see
a net gain, since the length of the corpus given the
grammar is likely to be longer with the segmented
rules. What we really need to do is find a way to
reuse the lexical rules that came out of the segmen-
tation. Now suppose the grammar also contained this
terminal rule:
A ? the total fare is five thousand yen/
??????????
(Chinese gloss: ?z?ng g?ng de f?i y?ng sh? w? q?an
r? y?an?). This rule can also be split into three rules:
A ? [AA] ,
A ? the total fare is/??????,
A ? five thousand yen/????
Again, we will assume that the structural rule is al-
ready present in the grammar, the old rule was 19
symbols long, and the two new terminal rules are
12+9 = 21 symbols long. Again we are out of luck,
as the new rules are longer than the old one, and three
rules are likely to be less probable than one rule dur-
ing parsing. The way to make this work is to realize
that the two existing rules share a bilingual affix?a
biaffix: ?five thousand dollars? translating into ??
????. If we make the two changes at the same
time, we get rid of 16 + 19 = 35 symbols worth of
rules, and introduce a mere 9 + 9 + 12 = 30 sym-
bols worth of rules (assuming the structural rules are
already in the grammar). Making these two changes
at the same time is essential, as the length of the five
saved symbols can be used to offset the likely in-
crease in the length of the corpus given the data. And
of course: the more rules we can find with shared bi-
affixes, the more likely we are to find a good set of
segmentations.
Our algorithm takes advantage of the above obser-
vation by focusing on the biaffixes found in the train-
ing data. Each biaffix defines a set of lexical rules
paired up with a possible segmentation. We evaluate
the biaffixes by estimating the change in description
length associated with committing to all the segmen-
tations defined by a biaffix. This allows us to find
the best set of segmentations, but rather than com-
mitting only to the one best set of segmentations, we
will collect all sets which would improve descrip-
tion length, and try to commit to as many of them
as possible. The pseudocode for our algorithm is as
follows:
G // The grammar
biaffixes_to_rules // Maps biaffixes to the
// rules they occur in
biaffixes_delta = [] // A list of biaffixes and
// their DL impact on G
for each biaffix b :
delta = eval_dl(b, biaffixes_to_rules[b], G)
if (delta < 0)
biaffixes_delta.push(b, delta)
sort_by_delta(biaffixes_delta)
for each b:delta pair in biaffixes_delta :
real_delta = eval_dl(b, biaffixes_to_rules[b], G)
if (real_delta < 0)
G = make_segmentations(b, biaffixes_to_rules[b], G)
The methods eval_dl, sort_by_delta and
make_segmentations evaluates the impact on de-
scription length that committing to a biaffix would
cause, sorts a list of biaffixes according to this delta,
and applies all the changes associated with a biaffix
to the grammar, respectively.
Evaluating the impact on description length
breaks down into two parts: the difference in de-
scription length of the grammar DL (G?) ? DL (G)
(where G? is the grammar that results from applying
all the changes that committing to a biaffix dictates),
52
and the difference in description length of the corpus
given the grammar DL (C|G?) ? DL (C|G). These
two quantities are simply added up to get the total
change in description length.
The difference in grammar length is calculated
as described in Section 2.2. The difference in de-
scription length of the corpus given the grammar
can be calculated by biparsing the corpus, since
DL (C|G?) = ?log2 (P (C|p?)) and DL (C|G) =
?log2 (P (C|p)) where p? and p are the rule prob-
ability functions of G? and G respectively. Bipars-
ing is, however, a very costly process that we do not
want to have inside a loop. Instead, we assume that
we have the original corpus probability (through bi-
parsing outside the loop), and estimate the new cor-
pus probability from it (in closed form). Given that
we are splitting the rule r0 into the three rules r1,
r2 and r3, and that the probability mass of r0 is dis-
tributed uniformly over the new rules, the new rule
probability function p? will be identical to p, except
that:
p? (r0) = 0,
p? (r1) = p (r1) +
1
3
p (r0) ,
p? (r2) = p (r2) +
1
3
p (r0) ,
p? (r3) = p (r3) +
1
3
p (r0)
Since we have eliminated all the occurrences of r0
and replaced them with combinations of r1, r2 and
r3, the probability of the corpus given this new rule
probability function will be:
P
(
C|p?
)
= P (C|p) p
? (r1) p? (r2) p? (r3)
p (r0)
To make this into a description length, we need to
take the negative logarithm of the above, which re-
sults in:
DL
(
C|G?
)
=
DL (C|G) ? log2
(p? (r1) p? (r2) p? (r3)
p (r0)
)
The difference in description length of the corpus
given the grammar can now be expressed as:
DL (C|G?) ? DL (C|G) =
?log2
(
p?(r1)p?(r2)p?(r3)
p(r0)
)
To calculate the impact of a set of segmentations, we
need to take all the changes into account in one go.
We do this in a two-pass fashion, first calculating
the new probability function (p?) and the change in
grammar description length (taking care not to count
the same rule twice), and then, in the second pass,
calculating the change in corpus description length.
5 Model combination
Themodel we learn by iteratively subsegmenting the
training data is guaranteed to be parsimonious while
retaining a decent fit to the training data; these are
desirable qualities, but there is a real risk that we
failed to make some generalization that we should
have made; to counter this risk, we can use a model
trained under more liberal conditions. We chose the
approach taken by Saers et al (2012) for two rea-
sons: (a) the model has the same form as our model,
which means that we can integrate it seamlessly, and
(b) their aims are similar to ours but their method
differs significantly; specifically, they let the model
grow in size as long as the data reduces in size. Both
these qualities make it a suitable complement for our
model.
Assuming we have two grammars (Ga and Gb)
that we want to combine, the interpolation param-
eter ? will determine the probability function of the
combined grammar such that:
pa+b (r) = ?pa (r) + (1 ? ?)pb (r)
for all rules r in the union of the two rule sets, and
where pa+b is the rule probability function of the
combined grammar and pa and pb are the rule prob-
ability functions of Ga and Gb respectively. Some
initial experiments indicated that an ? value of about
0.4 was reasonable (when Ga was the grammar ob-
tained through the training scheme outlined above,
andGb was the grammar obtained through the train-
ing scheme outlined in Saers et al (2012)), so we
used 0.4 in this paper.
6 Experimental setup
We have made the claim that iterative top-down seg-
mentation guided by the objective of minimizing the
description length gives a better precision grammar
than iterative bottom-up chunking, and that the com-
bination of the two gives superior results to either
53
0
2
4
6
8
10
12
14
 0  1  2  3  4  5  6  7Pro
bab
ility
 in 
log
 do
ma
in (M
bit)
Iterations
Figure 1: Description length in bits over the different it-
erations of top-down search. The lower portion represents
DL (G) and the upper portion represents DL (C|G).
approach in isolation. We have outlined how this
can be done in practice, and we now substantiate that
claim empirically.
We will initialize a stochastic bracketing inver-
sion transduction grammar (BITG) to rewrite it?s
one nonterminal symbol directly into all the sen-
tence pairs of the training data (iteration 0). We will
then segment the grammar iteratively a total of seven
times (iterations 1?7). For each iteration we will
record the change in description length and test the
grammar. Each iteration requires us to biparse the
training data, which we do with the cubic time algo-
rithm described in Saers et al (2009), with a beam
width of 100.
As training data, we use the IWSLT07 Chinese?
English data set (Fordyce, 2007), which contains
46,867 sentence pairs of training data, 506 Chinese
sentences of development data with 16 English ref-
erence translations, and 489 Chinese sentences with
6 English reference translations each as test data; all
the sentences are taken from the traveling domain.
Since the Chinese is written without whitespace, we
use a tool that tries to clump characters together into
more ?word like? sequences (Wu, 1999).
As the bottom-up grammar, we will reuse the
grammar learned in Saers et al (2012), specifically,
we will use the BITG that was bootstrapped from
a bracketing finite-state transduction grammar (BF-
STG) that has been chunked twice, giving bitermi-
nals where the monolingual segments are 0?4 tokens
long. The bottom-up grammar is trained on the same
0
10
20
30
40
50
60
 0  1  2  3  4  5  6  7N
um
ber
 of 
rule
s (th
ous
and
s)
Iterations
Figure 2: Number of rules learned during top-down
search over the different iterations.
data as our model.
To test the learned grammars as translation mod-
els, we first tune the grammar parameters to the train-
ing data using expectation maximization (Dempster
et al, 1977) and parse forests acquired with the
above mentioned biparser, again with a beam width
of 100. To do the actual decoding, we use our
in-house ITG decoder. The decoder uses a CKY-
style parsing algorithm (Cocke, 1969; Kasami, 1965;
Younger, 1967) and cube pruning (Chiang, 2007) to
integrate the language model scores. The decoder
builds an efficient hypergraph structure which is then
scored using both the induced grammar and the lan-
guage model. The weights for the language model
and the grammar, are tuned towards BLEU (Papineni
et al, 2002) using MERT (Och, 2003). We use the
ZMERT (Zaidan, 2009) implementation ofMERT as
it is a robust and flexible implementation of MERT,
while being loosely coupled with the decoder. We
use SRILM (Stolcke, 2002) for training a trigram
language model on the English side of the training
data. To evaluate the quality of the resulting transla-
tions, we use BLEU, and NIST (Doddington, 2002).
7 Experimental results
The results from running the experiments detailed
in the previous section can be summarized in four
graphs. Figures 1 and 2 show the size of our new,
segmenting model during induction, in terms of de-
scription length and in terms of rule count. The ini-
tial ITG is at iteration 0, where the vast majority
54
0.00
0.05
0.10
0.15
0.20
 0  1  2  3  4  5  6  7
BL
EU
Iterations
Figure 3: Variations in BLEU score over different iter-
ations. The thin line represents the baseline bottom-up
search (Saers et al, 2012), the dotted line represents the
top-down search, and the thick line represents the com-
bined results.
of the size is taken up by the model (DL (G)), and
very little by the data (DL (C|G))?just as we pre-
dicted. The trend over the induction phase is a sharp
decrease in model size, and a moderate increase in
data size, with the overall size constantly decreas-
ing. Note that, although the number of rules rises,
the total description length decreases. Again, this is
precisely what we expected. The size of the model
learned according to Saers et al (2012) is close to 30
Mbits?far off the chart. This shows that our new
top-down approach is indeed learning a more parsi-
monious grammar than the bottom-up approach.
Figures 3 and 4 shows the translation quality of
the learned model. The thin flat lines show the qual-
ity of the bottom-up approach (Saers et al, 2012),
whereas the thick curves shows the quality of the
new, top-down model presented in this paper with-
out (dotted line), and without the bottom-up model
(solid line). Although the MDL-based model is bet-
ter than the old model, the combination of the two
is still superior. It is particularly encouraging to see
that the over-fitting that seems to take place after iter-
ation 3 with the MDL-based approach is ameliorated
with the bottom-up model.
8 Conclusions
We have introduced a purely unsupervised learning
scheme for phrasal stochastic inversion transduction
grammars that is the first to combine two oppos-
0.0
1.0
2.0
3.0
4.0
5.0
 0  1  2  3  4  5  6  7
NIS
T
Iterations
Figure 4: Variations in NIST score over different iter-
ations. The thin line represents the baseline bottom-up
search (Saers et al, 2012), the dotted line represents the
top-down search, and the thick line represents the com-
bined results.
ing ways of searching for the phrasal translations: a
bottom-up rule chunking approach driven by a maxi-
mum likelihood (ML) objective and a top-down rule
segmenting approach driven by a minimum descrip-
tion length (MDL) objective. The combination ap-
proach takes advantage of the fact that the conser-
vative top-down MDL-driven rule segmenting ap-
proach learns a very parsimonious, yet competitive,
model when compared to a liberal bottom-up ML-
driven approach. Results show that the combination
of the two opposing approaches is significantly su-
perior to either of them in isolation.
9 Acknowledgements
This material is based upon work supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under BOLT contract no. HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-
0022 and HR0011-06-C-0023; by the European
Union under the FP7 grant agreement no. 287658;
and by the Hong Kong Research Grants Council
(RGC) research grants GRF620811, GRF621008,
and GRF612806. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.
55
References
P. Blunsom and T. Cohn. Inducing syn-
chronous grammars with slice sampling. In
HLT/NAACL2010, pages 238?241, Los Angeles,
California, June 2010.
P. Blunsom, T. Cohn, and M. Osborne. Bayesian
synchronous grammar induction. In Proceedings
of NIPS 21, Vancouver, Canada, December 2008.
P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. A
gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of ACL/IJCNLP, pages
782?790, Suntec, Singapore, August 2009.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L.Mercer. TheMathematics ofMachine Trans-
lation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311, 1993.
C. Cherry and D. Lin. Inversion transduction gram-
mar for joint phrasal translation modeling. In Pro-
ceedings of SSST, pages 17?24, Rochester, New
York, April 2007.
D. Chiang. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228, 2007.
J. Cocke. Programming languages and their compil-
ers: Preliminary notes. Courant Institute ofMath-
ematical Sciences, New York University, 1969.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38, 1977.
G. Doddington. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. In Proceedings of the 2nd International
Conference on Human Language Technology Re-
search, pages 138?145, San Diego, California,
2002.
C. S. Fordyce. Overview of the IWSLT 2007 evalu-
ation campaign. In Proceedings of IWSLT, pages
1?12, 2007.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thayer. Scalable infer-
ence and training of context-rich syntactic trans-
lation models. In Proceedings of COLING/ACL-
2006, pages 961?968, Sydney, Australia, July
2006.
Peter Gr?nwald. A minimum description length ap-
proach to grammar inference in symbolic. Lecture
Notes in Artificial Intelligence, (1040):203?216,
1996.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein.
Better word alignments with supervised itg mod-
els. In Proceedings of ACL/IJCNLP-2009, pages
923?931, Suntec, Singapore, August 2009.
H. Johnson, J. Martin, G. Foster, and R. Kuhn.
Improving translation quality by discarding most
of the phrasetable. In Proceedings of EMNLP-
CoNLL-2007, pages 967?975, Prague, Czech Re-
public, June 2007.
T. Kasami. An efficient recognition and syntax anal-
ysis algorithm for context-free languages. Tech-
nical Report AFCRL-65-00143, Air Force Cam-
bridge Research Laboratory, 1965.
P. Koehn, F. J. Och, and D. Marcu. Statistical
Phrase-Based Translation. In Proceedings of
HLT/NAACL-2003, volume 1, pages 48?54, Ed-
monton, Canada, May/June 2003.
G. Neubig, T. Watanabe, E. Sumita, S. Mori, and
T. Kawahara. An unsupervised model for joint
phrase alignment and extraction. In Proceedings
of ACL/HLT-2011, pages 632?641, Portland, Ore-
gon, June 2011.
G. Neubig, T. Watanabe, S. Mori, and T. Kawahara.
Machine translation without words through sub-
string alignment. In Proceedings of ACL-2012,
pages 165?174, Jeju Island, Korea, July 2012.
F. J. Och and H. Ney. A Systematic Comparison of
Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51, 2003.
F. J. Och. Minimum error rate training in statistical
machine translation. InProceedings of ACL-2003,
pages 160?167, Sapporo, Japan, July 2003.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL-2002,
pages 311?318, Philadelphia, Pennsylvania, July
2002.
J. Rissanen. A universal prior for integers and esti-
mation by minimum description length. The An-
nals of Statistics, 11(2):416?431, June 1983.
56
M. Saers and D. Wu. Improving phrase-based
translation via word alignments from Stochastic
Inversion Transduction Grammars. In Proceed-
ings of SSST-3, pages 28?36, Boulder, Colorado,
June 2009.
M. Saers and D. Wu. Principled induction of phrasal
bilexica. In Proceedings of EAMT-2011, pages
313?320, Leuven, Belgium, May 2011.
M. Saers, J. Nivre, and D. Wu. Learning stochastic
bracketing inversion transduction grammars with
a cubic time biparsing algorithm. In Proceedings
of IWPT?09, pages 29?32, Paris, France, October
2009.
M. Saers, J. Nivre, and D. Wu. Word alignment with
stochastic bracketing linear inversion transduc-
tion grammar. In Proceedings of HLT/NAACL-
2010, pages 341?344, Los Angeles, California,
June 2010.
M. Saers, K. Addanki, and D. Wu. From finite-state
to inversion transductions: Toward unsupervised
bilingual grammar induction. In Proceedings of
COLING 2012: Technical Papers, pages 2325?
2340, Mumbai, India, December 2012.
C. E. Shannon. A mathematical theory of com-
munication. The Bell System Technical Journal,
27:379?423, 623?, July, October 1948.
Z. Si, M. Pei, B. Yao, and S. Zhu. Unsuper-
vised learning of event and-or grammar and se-
mantics from video. In Proceedings of the 2011
IEEE International Conference on Computer Vi-
sion (ICCV), pages 41?48, November 2011.
R. J. Solomonoff. A new method for discovering the
grammars of phrase structure languages. In IFIP
Congress, pages 285?289, 1959.
A. Stolcke and S. Omohundro. Inducing proba-
bilistic grammars by bayesian model merging. In
R. C. Carrasco and J. Oncina, editors, Grammat-
ical Inference and Applications, pages 106?118.
Springer, 1994.
A. Stolcke. SRILM ? an extensible language model-
ing toolkit. In Proceedings of ICSLP-2002, pages
901?904, Denver, Colorado, September 2002.
J. M. Vilar and E. Vidal. A recursive statistical trans-
lation model. In ACL-2005 Workshop on Building
andUsing Parallel Texts, pages 199?207, AnnAr-
bor, Jun 2005.
S. Vogel, H. Ney, and C. Tillmann. HMM-based
Word Alignment in Statistical Translation. In Pro-
ceedings of COLING-96, volume 2, pages 836?
841, 1996.
D. Wu. Stochastic Inversion Transduction Gram-
mars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403, 1997.
Z. Wu. LDC Chinese segmenter, 1999.
D. H. Younger. Recognition and parsing of context-
free languages in time n3. Information and Con-
trol, 10(2):189?208, 1967.
O. F. Zaidan. Z-MERT: A Fully Configurable Open
Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague
Bulletin of Mathematical Linguistics, 91:79?88,
2009.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea.
Bayesian learning of non-compositional phrases
with synchronous parsing. In Proceedings of
ACL-08: HLT, pages 97?105, Columbus, Ohio,
June 2008.
57
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 67?73,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Unsupervised Transduction Grammar Induction
via Minimum Description Length
Markus Saers and Karteek Addanki and Dekai Wu
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|vskaddanki|dekai}@cs.ust.hk
Abstract
We present a minimalist, unsupervised
learning model that induces relatively
clean phrasal inversion transduction gram-
mars by employing the minimum descrip-
tion length principle to drive search over
a space defined by two opposing ex-
treme types of ITGs. In comparison to
most current SMT approaches, the model
learns a very parsimonious phrase trans-
lation lexicons that provide an obvious
basis for generalization to abstract trans-
lation schemas. To do this, the model
maintains internal consistency by avoid-
ing use of mismatched or unrelated mod-
els, such as word alignments or probabil-
ities from IBM models. The model in-
troduces a novel strategy for avoiding the
pitfalls of premature pruning in chunking
approaches, by incrementally splitting an
ITGwhile using a second ITG to guide this
search.
1 Introduction
We introduce an unsupervised approach to induc-
ing parsimonious, relatively clean phrasal inver-
sion transduction grammars or ITGs (Wu, 1997)
that employs a theoretically well-founded mini-
mum description length (MDL) objective to ex-
plicitly drive two opposing, extreme ITGs to-
wards one minimal ITG. This represents a new
attack on the problem suffered by most current
SMT approaches of learning phrase translations
that require enormous amounts of run-time mem-
ory, contain a high degree of redundancy, and fails
to provide an obvious basis for generalization to
abstract translation schemas. In particular, phrasal
SMT models such as Koehn et al (2003) and Chi-
ang (2005) often search for candidate translation
segments and transduction rules by committing
to a word alignment based on very different as-
sumptions (Brown et al, 1993; Vogel et al, 1996),
and heuristically derive lexical segment transla-
tions (Och and Ney, 2003). In fact, it is possible
to improve the performance by tossing away most
of the learned segmental translations (Johnson et
al., 2007). In addition to preventing such waste-
fulness, our work aims to also provide an obvi-
ous basis for generalization to abstract translation
schemas by driving the search for phrasal rules by
simultaneously using two opposing types of ITG
constraints that have both individually been empir-
ically proven to match phrase reordering patterns
across translations well.
We adopt a more ?pure? methodology for eval-
uating transduction grammar induction than typ-
ical system building papers. Instead of embed-
ding our learned ITG in the midst of many other
heuristic components for the sake of a short term
boost in BLEU, we focus on scientifically under-
standing the behavior of pure MDL-based search
for phrasal translations, divorced from the effect
of other variables, even though BLEU is naturally
much lower this way. The common practice of
plugging some aspect of a learned ITG into ei-
ther (a) a long pipeline of training heuristics and/or
(b) an existing decoder that has been patched up
to compensate for earlier modeling mistakes, as
we and others have done before?see for example
Cherry and Lin (2007); Zhang et al (2008); Blun-
som et al (2008, 2009); Haghighi et al (2009);
Saers and Wu (2009, 2011); Blunsom and Cohn
(2010); Burkett et al (2010); Riesa and Marcu
(2010); Saers et al (2010); Neubig et al (2011,
2012)?obscures the specific traits of the induced
grammar. Instead, we directly use our learned
ITG in translation mode (any transduction gram-
mar also represents a decoder when parsing with
the input sentence as a hard constraint) which al-
lows us to see exactly which aspects of correct
translation the transduction rules have captured.
67
When the structure of an ITG is induced without
supervision, it has so far been assumed that smaller
rules get clumped together into larger rules. This
is a natural way to search, since maximum like-
lihood (ML) tends to improve with longer rules,
which is typically balanced with Bayesian priors
(Zhang et al, 2008). Bayesian priors are also used
in Gibbs sampling (Blunsom et al, 2008, 2009;
Blunsom and Cohn, 2010), as well as other non-
parametric learning methods (Neubig et al, 2011,
2012). All of the above evaluate their models by
feeding them into mismatched decoders, making it
hard to evaluate how accurate the learned models
themselves were. In this work we take a radically
different approach, and start with the longest rules
possible and attempt to segment them into shorter
rules iteratively. This makes ML useless, since our
initial model maximizes it. Instead, we balance the
ML objective with a minimum description length
(MDL) objective, which let us escape the initial
ML optimum by rewarding model parsimony.
Transduction grammars can also be induced
with supervision from treebanks, which cuts down
the search space by enforcing external constraints
(Galley et al, 2006). This complicates the learn-
ing process by adding external constraints that are
bound to match the translation model poorly. It
does, however, constitute a way to borrow nonter-
minal categories that help the translation model.
MDL has been used before in monolingual
grammar induction (Gr?nwald, 1996; Stolcke and
Omohundro, 1994), as well as to interpret visual
scenes (Si et al, 2011). Our work is markedly dif-
ferent in that we (a) induce an ITG rather than a
monolingual grammar, and (b) focus on learning
the terminal segments rather than the nonterminal
categories. Iterative segmentation has also been
used before, but only to derive a word alignment
as part of a larger pipeline (Vilar and Vidal, 2005).
The paper is structured as follows: we start by
describing theMDL principle (Section 2). We then
describe the initial ITGs (Section 3), followed by
the algorithm that induces an MDL-optimal ITG
from them (Section 4). After that we describe the
experiments (Section 5), and the results (Section
6). Finally, we offer some conclusions (Section 7).
2 Minimum description length
The minimum description length principle is about
finding the optimal balance between the size of a
model and the size of some data given the model
(Solomonoff, 1959; Rissanen, 1983). Consider the
information theoretical problem of encoding some
data with a model, and then sending both the en-
coded data and the information needed to decode
the data (the model) over a channel; the minimum
description length is the minimum number of bits
sent over the channel. The encoded data can be in-
terpreted as carrying the information necessary to
disambiguate the uncertainties that the model has
about the data. The model can grow in size and be-
comemore certain about the data, and it can shrink
in size and become more uncertain about the data.
Formally, description length (DL) is:
DL (?, D) = DL (D|?) + DL (?)
where ? is the model and D is the data.
In practice, we rarely have complete data to train
on, so we need our models to generalize to unseen
data. Amodel that is very certain about the training
data runs the risk of not being able to generalize to
new data: it is over-fitting. It is bad enough when
estimating the parameters of a transduction gram-
mar, and catastrophic when inducing the structure.
The information-theoretic view of the problem
gives a hint at the operationalization of descrip-
tion length of a corpus given a grammar. Shannon
(1948) stipulates that we can get a lower bound on
the number of bits required to encode a specific
outcome of a random variable. We thus define de-
scription length of the corpus given the grammar
to be: DL (D|?) = ?lgP (D|?)
Information theory is also useful for the descrip-
tion length of the grammar: if we can find a way
to serialize the grammar into a sequence of tokens,
we can figure out how that sequence can be opti-
mally encoded. To serialize an ITG, we first need
to determine the alphabet that the message will be
written in. We need one symbol for every nonter-
minal, L0- and L1-terminal. We will also make
the assumption that all these symbols are used in
at least one rule, so that it is sufficient to serial-
ize the rules in order to express the entire ITG.
We serialize a rule with a type marker, followed
by the left-hand side nonterminal, followed by all
the right-hand side symbols. The type marker is
either [] denoting the start of a straight rule, or
?? denoting the start of an inverted rule. Unary
rules are considered to be straight. We serialize
the ITG by concatenating the serialized form of all
the rules, assuming that each symbol can be serial-
ized into?lgc bits where c is the symbol?s relative
frequency in the serialized form of the ITG.
68
3 Initial ITGs
To tackle the exponential problem of searching for
an ITG that minimizes description length, it is use-
ful to contrast two extreme forms of ITGs. De-
scription length has two components, model length
and data length. We call an ITG that minimizes
the data at the expense of the model a long ITG;
we call an ITG that minimizes the model at the ex-
pense of the data a short ITG.1 The long ITG sim-
ply has all the sentence pairs as biterminals:
S ? A
A ? e0..T0/f0..V0
A ? e0..T1/f0..V1
...
A ? e0..TN /f0..VN
where S is the start symbol, A is the nonterminal,
N is the number of sentence pairs, Ti is the length
of the ith output sentence (making e0..Ti the ith out-
put sentence), and Vi is the length of the ith input
sentence (making f0..Vi the ith input sentence). The
short ITG is a token-based bracketing ITG:
S ? A, A? [AA] , A? ?AA?,
A? e/f, A? e/?, A? ?/f
where, S is the start symbol, A is the nonterminal
symbol, e is an L0-token, f is an L1-token, and ?
is the empty sequence of tokens.
4 Shortening the long ITG
To shorten the long ITG,wewill identify good split
candidates in the terminal rules by parsing them
with the short ITG, and commit to split candidates
that give a net gain. A split candidate is an exist-
ing long terminal rule, information about where to
split its right-hand side, and whether to invert the
resulting two rules or not. Consider the terminal
rule A ? es..t/fu..v; it can be split at any point S
in L0 and any point U in L1, giving the three rules
A ? [AA], A ? es..S/fu..U and A ? eS..t/fU..v
when it is split in straight order, and the three rules
A? ?AA?, A? es..S/fU..v and A? eS..t/fu..U
when it is split in inverted order. We will refer to
the original long rule as r0, and the resulting three
rules as r1, r2 and r3.
To identify the split candidates and to figure out
how the probability mass of r0 is to be distributed
1Long and short ITGs correspond well to ad-hoc and
promiscuous grammars in Gr?nwald (1996).
Algorithm 1 Rule shortening.
Gl ? The long ITG
Gs ? The short ITG
repeat
cands? collect_candidates(Gl, Gs)
? ? 0
removed? {}
repeat
score(cands)
sort_by_delta(cands)
for all c ? cands do
r ? original_rule(c)
if r /? removed and ?c ? 0 then
Gl ? update_grammar(Gl, c)
removed? {r} ? removed
? ? ? + ?c
end if
end for
until ? ? 0
until ? ? 0
return Gl
to the new rules, we use the short ITG to biparse the
right-hand side of r0. The distribution is derived
from the inside probability of the bispans that the
new rules are covering in the chart, and we refer
to them as ?1, ?2 and ?3, where the index indi-
cates which new rule they apply to. This has the
effect of preferring to split a rule into parts that are
roughly equally probable, as the size of the data is
minimized when the weights are equal.
To choose which split candidates to commit to,
we need a way to estimate their impact on the to-
tal MDL score of the model. This breaks down
into two parts: the difference in description length
of the grammar: DL (??) ? DL (?) (where ?? is
? after committing to the split candidate), and the
difference in description length of the corpus given
the grammar: DL (D|??) ? DL (D|?). The two
are added up to get the total change in description
length.The difference in grammar length is calcu-
lated as described in Section 2. The difference in
description length of the corpus given the grammar
can be calculated by biparsing the corpus, since
DL (D|??) = ?lgP (D|p?) and DL (D|?) =
?lgP (D|p) where p? and p are the rule probabil-
ity functions of ?? and ? respectively. Bipars-
ing is, however, a very costly process that we do
not want to carry out for every candidate. Instead,
we assume that we have the original corpus proba-
bility (through biparsing when generating the can-
69
Table 1: The results of decoding. NIST and BLEU are the translation scores at each iteration, followed
by the number of rules in the grammar, followed by the average (as measured by mean and mode) number
of English tokens in the rules.
Iteration NIST BLEU Rules Mean Mode
1 2.7015 11.97 43,704 7.20 6
2 4.0116 14.04 42,823 6.30 6
3 4.1654 16.58 41,867 5.68 2
4 4.3723 17.43 40,953 5.23 1
5 4.2032 18.78 40,217 4.97 1
6 4.1329 17.28 39,799 4.84 1
7 4.0710 17.31 39,587 4.79 1
8 4.0437 17.10 39,470 4.75 1
didates), and estimate the new corpus probability
from it (in closed form). The new rule probability
function p? is identical to p, except that:
p? (r0) = 0
p? (r1) = p (r1) + ?1p (r0)
p? (r2) = p (r2) + ?2p (r0)
p? (r3) = p (r3) + ?3p (r0)
We assume the probability of the corpus given this
new rule probability function to be:
P
(
D|p?
)
= P (D|p) p
? (r1) p? (r2) p? (r3)
p (r0)
This gives the following description length differ-
ence:
DL (D|??)? DL (D|?) =
?lgp
?(r1)p?(r2)p?(r3)
p(r0)
We will commit to all split candidates that are es-
timated to lower the DL, restricting it so that any
original rule is split only in the best way (Algo-
rithm 1).
5 Experimental setup
To test whether minimum description length is a
good driver for unsupervised inversion transduc-
tion induction, we implemented and executed the
method described above. We start by initializing
one long and one short ITG. The parameters of the
long ITG cannot be adjusted to fit the data better,
but the parameters of the short ITG can be tuned to
the right-hand sides of the long ITG.We do so with
an implementation of the cubic time algorithm de-
scribed in Saers et al (2009), with a beam width
of 100. We then run the introduced algorithm.
As training data, we use the IWSLT07 Chinese?
English data set (Fordyce, 2007), which contains
46,867 sentence pairs of training data, and 489
Chinese sentences with 6 English reference trans-
lations each as test data; all the sentences are taken
from the traveling domain. Since the Chinese is
written without whitespace, we use a tool that tries
to clump characters together into more ?word like?
sequences (Wu, 1999).
After each iteration, we use the long ITG to
translate the held out test set with our in-house ITG
decoder. The decoder uses a CKY-style parsing
algorithm (Cocke, 1969; Kasami, 1965; Younger,
1967) and cube pruning (Chiang, 2007) to inte-
grate the language model scores. The decoder
builds an efficient hypergraph structure which is
scored using both the induced grammar and a lan-
guage model. We use SRILM (Stolcke, 2002) for
training a trigram language model on the English
side of the training corpus. To evaluate the re-
sulting translations, we use BLEU (Papineni et al,
2002) and NIST (Doddington, 2002).
We also perform a combination experiment,
where the grammar at different stages of the learn-
ing process (iterations) are interpolated with each
other. This is a straight-forward linear interpola-
tion, where the probabilities of the rules are added
up and the grammar is renormalized. Although
it makes little sense from an MDL point of view
to increase the size of the grammar so indiscrim-
inately, it does make sense from an engineering
point of view, since more rules typically means
better coverage, which in turn typically means bet-
ter translations of unknown data.
6 Results
As discussed at the outset, rather than burying our
learned ITG in many layers of unrelated heuristics
just to push up the BLEU score, we think it is more
70
Table 2: The results of decoding with combined grammars. NIST and BLEU are the translation scores for
each combination, followed by the number of rules in the grammar, followed by the average (as measured
by mean and mode) number of English tokens in the rules.
Combination NIST BLEU Rules Mean Mode
1?2 (2 grammars) 4.2426 15.28 74,969 6.69 6
3?4 (2 grammars) 4.5087 18.75 54,533 5.41 3
5?6 (2 grammars) 4.1897 18.19 44,264 4.86 1
7?8 (2 grammars) 4.0953 17.40 40,785 4.79 1
1?4 (4 grammars) 4.9234 19.98 109,183 6.19 5
5?8 (4 grammars) 4.1089 17.86 47,504 4.84 1
1?8 (8 grammars) 4.8649 20.41 124,423 5.92 3
important to illuminate scientific understanding of
the behavior of pure MDL-driven rule induction
without interference from other variables. Directly
evaluating solely the ITG in translation mode?
instead of (a) deriving word alignments from it by
committing to only the one-best parse, but then dis-
carding any trace of structure and/or (b) evaluating
it through a decoder that has been patched up to
compensate for deficiencies in disparate aspects of
translation?allows us to see exactly how accurate
the learned transduction rules are.
The results from the individual iterations (Table
1) show that we learn very parsimonious models
that far outperforms the only other result we are
aware of where an ITG is tested exactly as it was
learned without altering the model itself: Saers et
al. (2012) induce a pure ITG by iteratively chunk-
ing rules, but they report significantly lower trans-
lation quality (8.30 BLEU and 0.8554 NIST) de-
spite a significantly larger ITG (251,947 rules).
The average rule length also decreases as smaller
reusable spans are found. The English side of the
training data has amean of 8.45 and amode of 7 to-
kens per sentence, and these averages drop steadily
during training. It is very encouraging to see the
mode drop to one so quickly, as this indicates that
the learning algorithm finds translations of individ-
ual English words. Not only are the rules getting
fewer, but they are also getting shorter.
The results from the combination experiments
(Table 2) corroborate the engineering intuition that
more rules give better translations at the expense of
a larger model. Using all eight grammars gives a
BLEU score of 20.41, at the expense of approxi-
mately tripling the size of the grammar. All indi-
vidual iterations benefit from being combined with
other iterations?but for the very best iterations
more additional data is needed to get this improve-
ment; the fifth iteration, which excelled at BLEU
score needs to be combinedwith all other iterations
to see an improvement, whereas the first and sec-
ond iterations only need each other to see an im-
provement.
7 Conclusions
We have presented a minimalist, unsupervised
learning model that induces relatively clean
phrasal ITGs by iteratively splitting existing rules
into smaller rules using a theoretically well-
founded minimum description length objective.
The resulting translation model is very parsimo-
nious and provide an obvious foundation for gen-
eralization tomore abstract transduction grammars
with informative nonterminals.
8 Acknowledgements
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
ment no. 287658; and by the Hong Kong
Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
References
Phil Blunsom and Trevor Cohn. Inducing syn-
chronous grammars with slice sampling. In
HLT/NAACL2010, pages 238?241, Los Ange-
les, California, June 2010.
71
Phil Blunsom, Trevor Cohn, and Miles Osborne.
Bayesian synchronous grammar induction. In
Proceedings of NIPS 21, Vancouver, Canada,
December 2008.
Phil Blunsom, Trevor Cohn, Chris Dyer, andMiles
Osborne. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of
the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP, pages 782?790, Suntec, Singapore,
August 2009.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The Mathe-
matics of Machine Translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, 1993.
David Burkett, John Blitzer, and Dan Klein. Joint
parsing and alignment with weakly synchro-
nized grammars. In HLT/NAACL?10, pages
127?135, Los Angeles, California, June 2010.
Colin Cherry and Dekang Lin. Inversion transduc-
tion grammar for joint phrasal translation mod-
eling. In Proceedings of SSST?07, pages 17?24,
Rochester, New York, April 2007.
David Chiang. A hierarchical phrase-based model
for statistical machine translation. In Proceed-
ings of ACL?05, pages 263?270, Ann Arbor,
Michigan, June 2005.
David Chiang. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?
228, 2007.
John Cocke. Programming languages and their
compilers: Preliminary notes. Courant Institute
of Mathematical Sciences, New York Univer-
sity, 1969.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. InProceedings of HLT?02,
pages 138?145, San Diego, California, 2002.
C. S. Fordyce. Overview of the IWSLT 2007 eval-
uation campaign. In Proceedings IWSLT?07,
pages 1?12, 2007.
Michel Galley, Jonathan Graehl, Kevin Knight,
Daniel Marcu, Steve DeNeefe, Wei Wang, and
Ignacio Thayer. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings COLING/ACL?06, pages 961?968,
Sydney, Australia, July 2006.
Peter Gr?nwald. A minimum description
length approach to grammar inference in sym-
bolic. Lecture Notes in Artificial Intelligence,
(1040):203?216, 1996.
Aria Haghighi, John Blitzer, John DeNero, and
Dan Klein. Better word alignments with
supervised itg models. In Proceedings of
ACL/IJCNLP?09, pages 923?931, Suntec, Sin-
gapore, August 2009.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. Improving translation quality
by discarding most of the phrasetable. In Pro-
ceedings EMNLP/CoNLL?07, pages 967?975,
Prague, Czech Republic, June 2007.
TadaoKasami. An efficient recognition and syntax
analysis algorithm for context-free languages.
Technical Report AFCRL-65-00143, Air Force
Cambridge Research Laboratory, 1965.
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. Statistical Phrase-Based Translation.
In Proceedings of HLT/NAACL?03, volume 1,
pages 48?54, Edmonton, Canada, May/June
2003.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. An
unsupervised model for joint phrase alignment
and extraction. In Proceedings of ACL/HLT?11,
pages 632?641, Portland, Oregon, June 2011.
Graham Neubig, Taro Watanabe, Shinsuke Mori,
and Tatsuya Kawahara. Machine translation
without words through substring alignment. In
Proceedings of ACL?12, pages 165?174, Jeju Is-
land, Korea, July 2012.
Franz Josef Och and Hermann Ney. A Systematic
Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?
51, 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of ACL?02, pages 311?318, Philadelphia,
Pennsylvania, July 2002.
Jason Riesa andDanielMarcu. Hierarchical search
for word alignment. In Proceedings of ACL?10,
pages 157?166, Uppsala, Sweden, July 2010.
Jorma Rissanen. A universal prior for integers and
estimation by minimum description length. The
Annals of Statistics, 11(2):416?431, June 1983.
72
Markus Saers and Dekai Wu. Improving phrase-
based translation via word alignments from
Stochastic Inversion Transduction Grammars.
In Proceedings of SSST?09, pages 28?36, Boul-
der, Colorado, June 2009.
Markus Saers and Dekai Wu. Principled induction
of phrasal bilexica. In Proceedings of EAMT?11,
pages 313?320, Leuven, Belgium, May 2011.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In Proceedings of IWPT?09, pages
29?32, Paris, France, October 2009.
Markus Saers, JoakimNivre, and DekaiWu. Word
alignment with stochastic bracketing linear in-
version transduction grammar. In Proceedings
of HLT/NAACL?10, pages 341?344, Los Ange-
les, California, June 2010.
Markus Saers, Karteek Addanki, and Dekai Wu.
From finite-state to inversion transductions: To-
ward unsupervised bilingual grammar induc-
tion. In Proceedings of COLING 2012: Techni-
cal Papers, pages 2325?2340, Mumbai, India,
December 2012.
Claude Elwood Shannon. A mathematical theory
of communication. The Bell System Technical
Journal, 27:379?423, 623?656, July, October
1948.
Zhangzhang Si, Mingtao Pei, Benjamin Yao,
and Song-Chun Zhu. Unsupervised learning
of event and-or grammar and semantics from
video. In Proceedings of the 2011 IEEE ICCV,
pages 41?48, November 2011.
Ray J. Solomonoff. A new method for discovering
the grammars of phrase structure languages. In
IFIP Congress, pages 285?289, 1959.
Andreas Stolcke and Stephen Omohundro. Induc-
ing probabilistic grammars by bayesian model
merging. In R. C. Carrasco and J. Oncina, ed-
itors, Grammatical Inference and Applications,
pages 106?118. Springer, 1994.
Andreas Stolcke. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the
International Conference on Spoken Language
Processing, pages 901?904, Denver, Colorado,
September 2002.
Juan Miguel Vilar and Enrique Vidal. A recur-
sive statistical translation model. In ACL-2005
Workshop on Building and Using Parallel Texts,
pages 199?207, Ann Arbor, Jun 2005.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. HMM-basedWord Alignment in Statisti-
cal Translation. In Proceedings of COLING-96,
volume 2, pages 836?841, 1996.
Dekai Wu. Stochastic Inversion Transduc-
tion Grammars and Bilingual Parsing of Par-
allel Corpora. Computational Linguistics,
23(3):377?403, 1997.
Zhibiao Wu. LDC Chinese segmenter, 1999.
Daniel H. Younger. Recognition and parsing of
context-free languages in time n3. Information
and Control, 10(2):189?208, 1967.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In Proceedings of ACL/HLT?08, pages 97?
105, Columbus, Ohio, June 2008.
73
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22?33,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Better Semantic Frame Based MT Evaluation
via Inversion Transduction Grammars
Dekai Wu Lo Chi-kiu Meriem Beloucif Markus Saers
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
Abstract
We introduce an inversion transduc-
tion grammar based restructuring of
the MEANT automatic semantic frame
based MT evaluation metric, which,
by leveraging ITG language biases, is
able to further improve upon MEANT?s
already-high correlation with human
adequacy judgments. The new metric,
called IMEANT, uses bracketing ITGs to
biparse the reference and machine transla-
tions, but subject to obeying the semantic
frames in both. Resulting improvements
support the presumption that ITGs, which
constrain the allowable permutations
between compositional segments across
the reference and MT output, score the
phrasal similarity of the semantic role
fillers more accurately than the simple
word alignment heuristics (bag-of-word
alignment or maximum alignment) used
in previous version of MEANT. The
approach successfully integrates (1) the
previously demonstrated extremely high
coverage of cross-lingual semantic frame
alternations by ITGs, with (2) the high
accuracy of evaluating MT via weighted
f-scores on the degree of semantic frame
preservation.
1 Introduction
There has been to date relatively little use of in-
version transduction grammars (Wu, 1997) to im-
prove the accuracy of MT evaluation metrics, de-
spite long empirical evidence the vast majority of
translation patterns between human languages can
be accommodated within ITG constraints (and the
observation that most current state-of-the-art SMT
systems employ ITG decoders). We show that
ITGs can be used to redesign the MEANT seman-
tic frame based MT evaluation metric (Lo et al.,
2012) to produce improvements in accuracy and
reliability. This work is driven by the motiva-
tion that especially when considering semanticMT
metrics, ITGs would be seem to be a natural basis
for several reasons.
To begin with, it is quite natural to think of
sentences as having been generated from an ab-
stract concept using a rewriting system: a stochas-
tic grammar predicts how frequently any particu-
lar realization of the abstract concept will be gen-
erated. The bilingual analogy is a transduction
grammar generating a pair of possible realizations
of the same underlying concept. Stochastic trans-
duction grammars predict how frequently a partic-
ular pair of realizations will be generated, and thus
represent a good way to evaluate how well a pair
of sentences correspond to each other.
The particular class of transduction gram-
mars known as ITGs tackle the problem that
the (bi)parsing complexity for general syntax-
directed transductions (Aho and Ullman, 1972)
is exponential. By constraining a syntax-directed
transduction grammar to allow only monotonic
straight and inverted reorderings, or equivalently
permitting only binary or ternary rank rules, it is
possible to isolate the low end of that hierarchy into
a single equivalence class of inversion transduc-
tions. ITGs are guaranteed to have a two-normal
form similar to context-free grammars, and can
be biparsed in polynomial time and space (O
(
n
6
)
time andO
(
n
4
)
space). It is also possible to do ap-
proximate biparsing in O
(
n
3
)
time (Saers et al.,
2009). These polynomial complexities makes it
feasible to estimate the parameters of an ITG us-
ing standard machine learning techniques such as
expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have
also been directly shown to be more than sufficient
to account for the reordering that occur within se-
mantic frame alternations (Addanki et al., 2012).
This makes ITGs an appealing alternative for eval-
22
uating the possible links between both semantic
role fillers in different languages as well as the
predicates, and how these parts fit together to form
entire semantic frames. We believe that ITGs are
not only capable of generating the desired struc-
tural correspondences between the semantic struc-
tures of two languages, but also provide meaning-
ful constraints to prevent alignments from wander-
ing off in the wrong direction.
In this paper we show that IMEANT, a newmet-
ric drawing from the strengths of both MEANT
and inversion transduction grammars, is able to
exploit bracketing ITGs (also known as BITGs
or BTGs) which are ITGs containing only a sin-
gle non-differentiated non terminal category (Wu,
1995a), so as to produce even higher correlation
with human adequacy judgments than any auto-
matic MEANT variants, or other common auto-
matic metrics. We argue that the constraints pro-
vided by BITGs over the semantic frames and ar-
guments of the reference and MT output sentences
are essential for accurate evaluation of the phrasal
similarity of the semantic role fillers.
In common with the various MEANT semantic
MT evaluation metrics (Lo and Wu, 2011a, 2012;
Lo et al., 2012; Lo and Wu, 2013b), our proposed
IMEANT metric measures the degree to which
the basic semantic event structure is preserved
by translation?the ?who did what to whom, for
whom, when, where, how and why? (Pradhan et
al., 2004)?emphasizing that a good translation
is one that can successfully be understood by a
human. In the other versions of MEANT, sim-
ilarity between the MT output and the reference
translations is computed as a modified weighted f-
score over the semantic predicates and role fillers.
Across a variety of language pairs and genres, it
has been shown thatMEANT correlates better with
human adequacy judgment than both n-gram based
MT evaluation metrics such as BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Banerjee and Lavie, 2005), as well as edit-
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al., 2000), and TER
(Snover et al., 2006) when evaluating MT output
(Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and
Wu, 2013b; Mach??ek and Bojar, 2013). Further-
more, tuning the parameters of MT systems with
MEANT instead of BLEU or TER robustly im-
proves translation adequacy across different gen-
res and different languages (English and Chinese)
(Lo et al., 2013a; Lo and Wu, 2013a; Lo et al.,
2013b). This has motivated our choice of MEANT
as the basis on which to experiment with deploying
ITGs into semantic MT evaluation.
2 Related Work
2.1 ITGs and MT evaluation
Relatively little investigation into the potential
benefits of ITGs is found in previous MT eval-
uation work. One exception is invWER, pro-
posed by Leusch et al. (2003) and Leusch and Ney
(2008). The invWER metric interprets weighted
BITGs as a generalization of the Levenshtein edit
distance, in which entire segments (blocks) can be
inverted, as long as this is done strictly compo-
sitionally so as not to violate legal ITG biparse
tree structures. The input and output languages
are considered to be those of the reference and ma-
chine translations, and thus are over the same vo-
cabulary (say,English). At the sentence level, cor-
relation of invWER with human adequacy judg-
ments was found to be among the best.
Our current approach differs in several key
respects from invWER. First,invWER operates
purely at the surface level of exact token match,
IMEANT mediates between segments of refer-
ence translation andMT output using lexical BITG
probabilities.
Secondly, there is no explicit semantic model-
ing in invWER. Providing they meet the BITG
constraints, the biparse trees in invWER are com-
pletely unconstrained. In contrast, IMEANT em-
ploys the same explicit, strong semantic frame
modeling as MEANT, on both the reference and
machine translations. In IMEANT, the semantic
frames always take precedence over pure BITG
biases. Compared to invWER, this strongly con-
strains the space of biparses that IMEANT permits
to be considered.
2.2 MT evaluation metrics
Like invWER, other common surface-form ori-
ented metrics like BLEU (Papineni et al., 2002),
NIST (Doddington, 2002), METEOR (Banerjee
and Lavie, 2005; Denkowski and Lavie, 2014),
CDER (Leusch et al., 2006), WER (Nie?en et
al., 2000), and TER (Snover et al., 2006) do
not correctly reflect the meaning similarities of
the input sentence. There are in fact several
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) reporting cases
23
where BLEU strongly disagrees with human judg-
ments of translation adequacy.
Such observations have generated a recent surge
of work on developing MT evaluation metrics that
would outperform BLEU in correlation with hu-
man adequacy judgment (HAJ). Like MEANT, the
TINE automatic recall-oriented evaluation metric
(Rios et al., 2011) aims to preserve basic event
structure. However, its correlation with human ad-
equacy judgment is comparable to that of BLEU
and not as high as that of METEOR. Owczarzak
et al. (2007a,b) improved correlation with human
fluency judgments by using LFG to extend the ap-
proach of evaluating syntactic dependency struc-
ture similarity proposed by Liu and Gildea (2005),
but did not achieve higher correlation with hu-
man adequacy judgments than metrics like ME-
TEOR. Another automatic metric, ULC (Gim?nez
and M?rquez, 2007, 2008), incorporates several
semantic similarity features and shows improved
correlation with human judgement of translation
quality (Callison-Burch et al., 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al., 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an SMT system using
a pure form of ULC perhaps due to its expensive
run time. Likewise, SPEDE (Wang and Manning,
2012) predicts the edit sequence needed to match
the machine translation to the reference translation
via an integrated probabilistic FSM and probabilis-
tic PDA model. The semantic textual similarity
metric Sagan (Castillo and Estrella, 2012) is based
on a complex textual entailment pipeline. These
aggregated metrics require sophisticated feature
extraction steps, contain many parameters that
need to be tuned, and employ expensive linguis-
tic resources such asWordNet or paraphrase tables.
The expensive training, tuning and/or running time
renders these metrics difficult to use in the SMT
training cycle.
3 IMEANT
In this section we give a contrastive description
of IMEANT: we first summarize the MEANT ap-
proach, and then explain how IMEANT differs.
3.1 Variants of MEANT
MEANT and its variants (Lo et al., 2012) measure
weighted f-scores over corresponding semantic
frames and role fillers in the reference andmachine
translations. The automatic versions of MEANT
replace humans with automatic SRL and align-
ment algorithms. MEANT typically outperforms
BLEU, NIST, METEOR, WER, CDER and TER
in correlation with human adequacy judgment, and
is relatively easy to port to other languages, re-
quiring only an automatic semantic parser and a
monolingual corpus of the output language, which
is used to gauge lexical similarity between the se-
mantic role fillers of the reference and translation.
MEANT is computed as follows:
1. Apply an automatic shallow semantic parser
to both the reference and machine transla-
tions. (Figure 1 shows examples of auto-
matic shallow semantic parses on both refer-
ence and MT.)
2. Apply the maximum weighted bipartite
matching algorithm to align the semantic
frames between the reference and machine
translations according to the lexical similari-
ties of the predicates. (Lo and Wu (2013a)
proposed a backoff algorithm that evaluates
the entire sentence of theMT output using the
lexical similarity based on the context vector
model, if the automatic shallow semantic
parser fails to parse the reference or machine
translations.)
3. For each pair of the aligned frames, apply the
maximum weighted bipartite matching algo-
rithm to align the arguments between the ref-
erence andMT output according to the lexical
similarity of role fillers.
4. Compute the weighted f-score over the
matching role labels of these aligned predi-
cates and role fillers according to the follow-
ing definitions:
q
0
i,j
? ARG j of aligned frame i in MT
q
1
i,j
? ARG j of aligned frame i in REF
w
0
i
?
#tokens filled in aligned frame i of MT
total #tokens in MT
w
1
i
?
#tokens filled in aligned frame i of REF
total #tokens in REF
wpred ? weight of similarity of predicates
w
j
? weight of similarity of ARG j
e
i,pred ? the pred string of the aligned frame i of MT
f
i,pred ? the pred string of the aligned frame i of REF
e
i,j
? the role fillers of ARG j of the aligned frame i of MT
f
i,j
? the role fillers of ARG j of the aligned frame i of REF
s(e, f) = lexical similarity of token e and f
24
[IN] ?? ? ? ?? ?? ?? ? ? ? ? ? ? ????? ?? ?? ?? ?? ?  
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ? II products have now been resumed . 
ARGM-TMP PRED ARGM-LOC PRED ARG1 
ARGM-LOC PRED ARG1 PRED ARG1 
ARG0 ARGM-TMP 
[MT1] So far , nearly two months sk - ii the sale of products in the mainland of China to resume sales .  
PRED ARG0 ARG1 
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .  
ARGM-TMP ARG1 PRED PRED ARG1 
[MT3] So far , the sale in the mainland of China for nearly two months of SK - II line of products .  
PRED 
PRED ARG0 
ARG1 ARGM-TMP 
ARGM-ADV 
ARG0 
ARGM-EXT 
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
are parsed using automatic English SRL. There are no semantic frames forMT3 since there is no predicate
in the MT output.
prece,f =
?
e?e max
f?f
s(e, f)
| e |
rece,f =
?
f?f max
e?e
s(e, f)
| f |
s
i,pred =
2 ? prece
i,pred,fi,pred ? recei,pred,fi,pred
prece
i,pred,fi,pred + recei,pred,fi,pred
s
i,j
=
2 ? prece
i,j
,f
i,j
? rece
i,j
,f
i,j
prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
precision =
?
i
w
0
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
0
i,j
|
?
i
w
0
i
recall =
?
i
w
1
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
1
i,j
|
?
i
w
1
i
MEANT = 2 ? precision ? recallprecision + recall
where q0
i,j
and q1
i,j
are the argument of type j in
frame i inMT andREF respectively.w0
i
andw1
i
are
the weights for frame i in MT/REF respectively.
These weights estimate the degree of contribution
of each frame to the overall meaning of the sen-
tence. wpred and wj are the weights of the lexical
similarities of the predicates and role fillers of the
arguments of type j of all frame between the ref-
erence translations and the MT output.There is a
total of 12 weights for the set of semantic role la-
bels in MEANT as defined in Lo and Wu (2011b).
For MEANT, they are determined using super-
vised estimation via a simple grid search to opti-
mize the correlation with human adequacy judg-
ments (Lo andWu, 2011a). For UMEANT (Lo and
Wu, 2012), they are estimated in an unsupervised
manner using relative frequency of each semantic
role label in the references and thus UMEANT is
useful when human judgments on adequacy of the
development set are unavailable.
s
i,pred and si,j are the lexical similarities based
on a context vectormodel of the predicates and role
fillers of the arguments of type j between the ref-
erence translations and the MT output. Lo et al.
(2012) and Tumuluru et al. (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al., 2013a; Lo and
Wu, 2013a; Lo et al., 2013b). In this paper, we
will assess IMEANT against the latest version of
MEANT (Lo et al., 2014) which, as shown, uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of semantic
role fillers,since this has been shown to bemore ac-
curate than the previously used aggregation func-
tions.
Recent studies (Lo et al., 2013a; Lo and Wu,
2013a; Lo et al., 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
25
In an alternative quality-estimation oriented line
of research, Lo et al. (2014) describe a cross-
lingual variant called XMEANT capable of eval-
uating translation quality without the need for ex-
pensive human reference translations, by utiliz-
ing semantic parses of the original foreign in-
put sentence instead of a reference translation.
Since XMEANT?s results could have been due
to either (1) more accurate evaluation of phrasal
similarity via cross-lingual translation probabili-
ties, or (2) better match of semantic frames with-
out reference translations, there is no direct evi-
dencewhether ITGs contribute to the improvement
in MEANT?s correlation with human adequacy
judgment. For the sake of better understanding
whether ITGs improve semantic MT evaluation,
we will also assess IMEANT against cross-lingual
XMEANT.
3.2 The IMEANT metric
Although MEANT was previously shown to pro-
duce higher correlation with human adequacy
judgments compared to other automatic metrics,
our error analyses suggest that it still suffers from a
common weakness among metrics employing lex-
ical similarity, namely that word/token alignments
between the reference and machine translations
are severely under constrained. No bijectivity or
permutation restrictions are applied, even between
compositional segments where this should be nat-
ural. This can cause role fillers to be aligned even
when they should not be. IMEANT, in contrast,
uses a bracketing inversion transduction grammar
to constrain permissible token alignment patterns
between aligned role filler phrases. The semantic
frames above the token level also fits ITG com-
positional structure, consistent with the aforemen-
tioned semantic frame alternation coverage study
of Addanki et al. (2012). Figure 2 illustrates how
the ITG constraints are consistent with the needed
permutations between semantic role fillers across
the reference and machine translations for a sam-
ple sentence from our evaluation data, which as
we will see leads to higher HAJ correlations than
MEANT.
Subject to the structural ITG constraints,
IMEANT scores sentence translations in a spirit
similar to the way MEANT scores them: it utilizes
an aggregated score over the matched semantic
role labels of the automatically aligned semantic
frames and their role fillers between the reference
and machine translations. Despite the structural
differences, like MEANT, at the conceptual level
IMEANT still aims to evaluate MT output in
terms of the degree to which the translation has
preserved the essential ?who did what to whom,for
whom, when, where, how and why? of the foreign
input sentence.
Unlike MEANT, however, IMEANT aligns and
scores under ITG assumptions. MEANT uses a
maximum alignment algorithm to align the tokens
in the role fillers between the reference and ma-
chine translations, and then scores by aggregating
the lexical similarities into a phrasal similarity us-
ing an f-measure. In contrast, IMEANT aligns and
scores by utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009; Addanki et al., 2012). To be precise in
this regard, we can see IMEANT as differing from
the foregoing description of MEANT in the defi-
nition of s
i,pred and si,j , as follows.
G ? ?{A} ,W0,W1,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 1
p (e/f |A) = s(e, f)
s
i,pred = lg?1
?
?
lg
(
P
(
A ?? e
i,pred/fi,pred|G
))
max(| e
i,pred |, | fi,pred |)
?
?
s
i,j
= lg?1
?
?
lg
(
P
(
A ?? e
i,j
/f
i,j
|G
))
max(| e
i,j
|, | f
i,j
|)
?
?
where G is a bracketing ITG whose only non ter-
minal is A, andR is a set of transduction rules with
e ? W
0
?{?} denoting a token in theMToutput (or
the null token) and f ? W1?{?} denoting a token
in the reference translation (or the null token). The
rule probability (or more accurately, rule weight)
function p is set to be 1 for structural transduction
rules, and for lexical transduction rules it is de-
fined using MEANT?s context vector model based
lexical similarity measure. To calculate the inside
probability (or more accurately, inside score) of a
pair of segments, P
(
A ?? e/f|G
)
, we use the al-
gorithm described in Saers et al. (2009). Given
this, s
i,pred and si,j now represent the length nor-
malized BITG parse scores of the predicates and
role fillers of the arguments of type j between the
reference and machine translations.
4 Experiments
In this section we discuss experiments indicating
that IMEANT further improves upon MEANT?s
26
[REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work .  
[MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency .  
ARG0 ARG1 PRED 
ARG0 PRED ARG1 
The 
level 
of 
reduction 
is 
conducive 
to 
raising 
the 
inspection 
and 
supervision 
work 
efficiency 
. 
Th
e 
red
uct
ion
 in 
hie
rar
chy
 
hel
ps rais
e the
 
effi
cie
ncy
 of 
ins
pec
tion
 
and
 
sup
erv
iso
ry . 
wo
rk 
pred 
ARG0 
ARG1 
pr
ed
 
AR
G0
 
AR
G1
 
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
biparse tree and alignment matrix depictions, for the Chinese input sentence ????????????
????????. Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
already-high correlation with human adequacy
judgments.
4.1 Experimental setup
We perform the meta-evaluation upon two differ-
ent partitions of the DARPA GALE P2.5 Chinese-
English translation test set. The corpus includes
the Chinese input sentences, each accompanied by
one English reference translation and three partic-
ipating state-of-the-art MT systems? output.
For the sake of consistent comparison, the first
evaluation partition, GALE-A, is the same as the
one used in Lo and Wu (2011a), and the second
evaluation partition, GALE-B, is the same as the
one used in Lo and Wu (2011b).
For both reference and machine translations, the
ASSERT (Pradhan et al., 2004) semantic role la-
beler was used to automatically predict semantic
parses.
27
Table 1: Sentence-level correlation with human
adequacy judgements on different partitions of
GALE P2.5 data. IMEANT always yields top
correlations, and is more consistent than either
MEANT or its recent cross-lingual XMEANT
quality estimation variant. For reference, the hu-
man HMEANT upper bound is 0.53 for GALE-A
and 0.37 for GALE-B?thus, the fully automated
IMEANT approximation is not far from closing the
gap.
metric GALE-A GALE-B
IMEANT 0.51 0.33
XMEANT 0.51 0.20
MEANT 0.48 0.33
METEOR 1.5 (2014) 0.43 0.10
NIST 0.29 0.16
METEOR 0.4.3 (2005) 0.20 0.29
BLEU 0.20 0.27
TER 0.20 0.19
PER 0.20 0.18
CDER 0.12 0.16
WER 0.10 0.26
4.2 Results
The sentence-level correlations in Table 1 show
that IMEANT outperforms other automatic met-
rics in correlation with human adequacy judgment.
Note that this was achieved with no tuning what-
soever of the default rule weights (suggesting that
the performance of IMEANT could be further im-
proved in the future by slightly optimizing the ITG
weights).
On the GALE-A partition, IMEANT shows 3
points improvement over MEANT, and is tied
with the cross-lingual XMEANT quality estimator
discussed earlier.IMEANT produces much higher
HAJ correlations than any of the other metrics.
On the GALE-B partition, IMEANT is tied with
MEANT, and is significantly better correlated with
HAJ than the XMEANT quality estimator. Again,
IMEANT produces much higher HAJ correlations
than any of the other metrics.
We note that we have also observed this pattern
consistently in smaller-scale experiments?while
the monolingual MEANT metric and its cross-
lingual XMEANT cousin vie with each other on
different data sets, IMEANT robustly and consis-
tently produces top HAJ correlations.
In both the GALE-A and GALE-B partitions,
IMEANT comes within a few points of the human
upper bound benchmark HAJ correlations com-
puted using the human labeled semantic frames
and alignments used in the HMEANT.
Data analysis reveals two reasons that IMEANT
correlates with human adequacy judgement more
closely than MEANT. First, BITG constraints in-
deed provide more accurate phrasal similarity ag-
gregation, compared to the naive bag-of-words
based heuristics employed in MEANT. Similar re-
sults have been observed while trying to estimate
word alignment probabilities where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Secondly, the permutation and bijectivity con-
straints enforced by the ITG provide better lever-
age to reject token alignments when they are not
appropriate, compared with the maximal align-
ment approach which tends to be rather promiscu-
ous. A case of this can be seen in Figure 3, which
shows the result on the same example sentence as
in Figure 1. Disregarding the semantic parsing er-
rors arising from the current limitations of auto-
matic SRL tools, the ITG tends to provide clean,
sparse alignments for role fillers like the ARG1
of the resumed PRED, preferring to leave tokens
like complete and range unaligned instead of aligning
them anyway as MEANT?s maximal alignment al-
gorithm tends to do. Note that it is not simply a
matter of lowering thresholds for accepting token
alignments: Tumuluru et al. (2012) showed that
the competitive linking approach (Melamed, 1996)
which also generally produces sparser alignments
does not work as well inMEANT, whereas the ITG
appears to be selective about the token alignments
in a manner that better fits the semantic structure.
For contrast, Figure 4 shows a case where
IMEANT appropriately accepts dense alignments.
5 Conclusion
We have presented IMEANT, an inversion trans-
duction grammar based rethinking of the MEANT
semantic frame based MT evaluation approach,
that achieves higher correlation with human ad-
equacy judgments of MT output quality than
MEANT and its variants, as well as other com-
mon evaluation metrics. Our results improve upon
previous research showing that MEANT?s explicit
use of semantic frames leads to state-of-the-art au-
tomatic MT evaluation. IMEANT achieves this
by aligning and scoring semantic frames under a
simple, consistent ITG that provides empirically
28
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ? II products have now been resumed .  
ARG0 PRED ARGM-LOC PRED ARG1 
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .  
ARGM-TMP ARG1 PRED PRED ARG1 PRED 
ARGM-TMP ARGM-TMP 
So 
far 
, 
in 
the 
mainland 
of 
China 
to 
stop 
selling 
nearly 
two 
months 
of 
SK 
- 
2 
products 
sales 
resumed 
. 
Un
til 
afte
r 
the
ir 
sal
e had
 
cea
sed
 in 
ma
inla
nd 
Ch
ina
 for 
alm
ost
 
two
 , 
sal
es of the
 
com
ple
te 
PRED 
PRED 
ARG1 
ARGM-TMP 
ARG1 
PRED 
PR
ED
 
PR
ED
 
AR
G1
 
AR
GM
-L
OC
 
AR
G0
 
ran
ge of SK
 - II 
pro
duc
ts 
hav
e 
now
 
bee
n 
res
um
ed . 
mo
nth
s 
AR
GM
-T
MP
 
AR
GM
-T
MP
 
Figure 3: An example where the ITG helps produce correctly sparse alignments by rejecting inappro-
priate token alignments in the ARG1 of the resumed PRED, instead of wrongly aligning tokens like the,
complete, and range as MEANT tends to do. (The semantic parse errors are due to limitations of automatic
SRL.)
informative permutation and bijectivity biases, in-
stead of the maximal alignment and bag-of-words
assumptions used by MEANT. At the same time,
IMEANT retains the Occam?s Razor style simplic-
ity and representational transparency characteris-
tics of MEANT.
Given the absence of any tuning of ITG weights
in this first version of IMEANT, we speculate that
29
[REF] Australian Prime Minister Howard said the government could cancel AWB 's monopoly in the wheat business next week . 
[MT2] Australian Prime Minister John Howard said that the Government might cancel the AWB company wheat monopoly next week . 
ARG0 
ARG0 
PRED 
ARG0 PRED ARG1 PRED 
ARG0 
ARGM-MOD ARGM-TMP 
ARG1 
PRED ARGM-MOD ARG1 ARGM-TMP 
ARG1 
Australian 
Prime 
Minister 
John 
Howard 
said 
the 
Government 
might 
cancel 
the 
AWB 
company 
wheat 
monopoly 
Au
str
alia
n 
Pri
me
 
Min
iste
r 
Ho
wa
rd sai
d the
 
go
ver
nm
en
t 
cou
ld 
can
cel
 
AW
B 's 
mo
no
po
ly the
 in 
next 
week 
that 
. 
wh
ea
t 
bu
sin
ess
 
ne
xt 
we
ek . 
pr
ed
 
pr
ed
 
AR
G0
 
AR
G0
 
AR
GM
-M
OD
 
AR
G1
 
AR
GM
-T
MP
 
AR
G1
 
pred 
pred 
ARG0 
ARG0 
ARGM-MOD 
ARGM-TMP 
ARG1 
ARG1 
Figure 4: An example of dense alignments in IMEANT, for the Chinese input sentence ???????
?????????????? AWB?????????? (The semantic parse errors are due to limitations
of automatic SRL.)
IMEANT could perform even better than it already
does here.We plan to investigate simple hyperpa-
rameter optimizations in the near future.
30
6 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessar-
ily reflect the views of DARPA, the EU, or RGC.
Thanks to Karteek Addanki for supporting work,
and to Pascale Fung, Yongsheng Yang and Zhao-
jun Wu for sharing the maximum entropy Chinese
segmenter and C-ASSERT, the Chinese semantic
parser.
References
Karteek Addanki, Chi-kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-
lingual verb frame alternations. In 16th An-
nual Conference of the European Association
for Machine Translation (EAMT-2012), Trento,
Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Sum-
marization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in ma-
chine translation research. In 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(meta-) evaluation of machine translation. In
Second Workshop on Statistical Machine Trans-
lation (WMT-07), 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further meta-evaluation of machine transla-
tion. In Third Workshop on Statistical Machine
Translation (WMT-08), 2008.
Julio Castillo and Paula Estrella. Semantic tex-
tual similarity for MT evaluation. In 7th Work-
shop on Statistical Machine Translation (WMT
2012), 2012.
Michael Denkowski and Alon Lavie. METEOR
universal: Language specific translation eval-
uation for any target language. In 9th Work-
shop on Statistical Machine Translation (WMT
2014), 2014.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In The second interna-
tional conference on Human Language Technol-
ogy Research (HLT ?02), San Diego, California,
2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic fea-
tures for automatic evaluation of heterogenous
MT systems. In Second Workshop on Statisti-
cal Machine Translation (WMT-07), pages 256?
264, Prague, Czech Republic, June 2007.
Jes?s Gim?nez and Llu?s M?rquez. A smorgas-
bord of features for automaticMT evaluation. In
Third Workshop on Statistical Machine Transla-
tion (WMT-08), Columbus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
automatic evaluation of machine translation be-
tween european languages. InWorkshop on Sta-
tistical Machine Translation (WMT-06), 2006.
Gregor Leusch and Hermann Ney. Bleusp, invwer,
cder: Three improved mt evaluation measures.
In NIST Metrics for Machine Translation Chal-
lenge (MetricsMATR), at Eighth Conference of
the Association for Machine Translation in the
Americas (AMTA 2008), Waikiki, Hawaii, Oct
2008.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. A novel string-to-string distance measure
with applications to machine translation evalu-
ation. In Machine Translation Summit IX (MT
Summit IX), New Orleans, Sep 2003.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT evaluation using
block movements. In 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL-2006), 2006.
31
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. InWorkshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization,
Ann Arbor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpen-
sive, high-accuracy, semi-automatic metric for
evaluating translation utility based on seman-
tic roles. In 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux:
How semantic frames evaluate MT more ac-
curately. In Twenty-second International Joint
Conference on Artificial Intelligence (IJCAI-
11), 2011.
Chi-kiu Lo andDekaiWu. Unsupervised vs. super-
vised weight estimation for semantic MT evalu-
ation metrics. In Sixth Workshop on Syntax, Se-
mantics and Structure in Statistical Translation
(SSST-6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres
be better translated by tuning on automatic se-
mantic metrics? In 14th Machine Translation
Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT
2013: A tunable, accurate yet inexpensive se-
mantic frame based mt evaluation metric. In
8th Workshop on Statistical Machine Transla-
tion (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully automatic semantic MT evaluation.
In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by
training against an automatic semantic frame
based evaluation metric. In 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by
tuning against Chinese MEANT. In Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. XMEANT: Better semantic MT
evaluation without reference translations. In
52nd Annual Meeting of the Association for
Computational Linguistics (ACL 2014), 2014.
Matou?Mach??ek andOnd?ej Bojar. Results of the
WMT13 metrics shared task. In Eighth Work-
shop on Statistical Machine Translation (WMT
2013), Sofia, Bulgaria, August 2013.
I. Dan Melamed. Automatic construction of
clean broad-coverage translation lexicons. In
2nd Conference of the Association for Ma-
chine Translation in the Americas (AMTA-
1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In The
Twenty-first National Conference on Artificial
Intelligence (AAAI-06), volume 21, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A evaluation tool for ma-
chine translation: Fast evaluation for MT re-
search. In The Second International Conference
on Language Resources and Evaluation (LREC
2000), 2000.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Dependency-based automatic eval-
uation for machine translation. In Syntax
and Structure in Statistical Translation (SSST),
2007.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Evaluating machine translation
with LFG dependencies. Machine Translation,
21:95?119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In 40th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL-02), pages 311?318,
Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow se-
mantic parsing using support vector machines.
In Human Language Technology Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-
NAACL 2004), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia.
TINE: A metric to assess MT adequacy. In
Sixth Workshop on Statistical Machine Transla-
tion (WMT 2011), 2011.
32
Markus Saers and Dekai Wu. Improving phrase-
based translation via word alignments from
stochastic inversion transduction grammars. In
Third Workshop on Syntax and Structure in
Statistical Translation (SSST-3), pages 28?36,
Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT?09), pages 29?32,
Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In 7th Biennial Conference Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 2006), pages 223?231, Cambridge,
Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai
Wu. Accuracy and robustness in measuring the
lexical similarity of semantic role fillers for au-
tomatic semantic MT evaluation. In 26th Pa-
cific Asia Conference on Language, Informa-
tion, and Computation (PACLIC 26), 2012.
Mengqiu Wang and Christopher D. Manning.
SPEDE: Probabilistic edit distance metrics for
MT evaluation. In 7th Workshop on Statistical
Machine Translation (WMT 2012), 2012.
DekaiWu. An algorithm for simultaneously brack-
eting parallel texts by aligning words. In 33rd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL 95), pages 244?251,
Cambridge, Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars
for parallel text bracketing. In Third Annual
Workshop on Very Large Corpora (WVLC-3),
pages 69?81, Cambridge, Massachusetts, June
1995.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, 1997.
Richard Zens and Hermann Ney. A compara-
tive study on reordering constraints in statisti-
cal machine translation. In 41st Annual Meeting
of the Association for Computational Linguis-
tics (ACL-2003), pages 144?151, Stroudsburg,
Pennsylvania, 2003.
33
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86?93,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Ternary Segmentation for Improving Search
in Top-down Induction of Segmental ITGs
Markus Saers Dekai Wu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|dekai}@cs.ust.hk
Abstract
We show that there are situations where
iteratively segmenting sentence pairs top-
down will fail to reach valid segments and
propose a method for alleviating the prob-
lem. Due to the enormity of the search
space, error analysis has indicated that it is
often impossible to get to a desired embed-
ded segment purely through binary seg-
mentation that divides existing segmental
rules in half ? the strategy typically em-
ployed by existing search strategies ? as
it requires two steps. We propose a new
method to hypothesize ternary segmenta-
tions in a single step, making the embed-
ded segments immediately discoverable.
1 Introduction
One of the most important improvements to sta-
tistical machine translation to date was the move
from token-basedmodel to segmental models (also
called phrasal). This move accomplishes two
things: it allows a flat surface-based model to
memorize some relationships between word real-
izations, but more importantly, it allows the model
to capture multi-word concepts or chunks. These
chunks are necessary in order to translate fixed ex-
pressions, or other multi-word units that do not
have a compositional meaning. If a sequence in
one language can be broken down into smaller
pieces which are then translated individually and
reassembled in another language, the meaning of
the sequence is compositional; if not, the only way
to translate it accurately is to treat it as a single unit
? a chunk. Existing surface-based models (Och et
al., 1999) have high recall in capturing the chunks,
but tend to over-generate, which leads to big mod-
els and low precision. Surface-based models have
no concept of hierarchical composition, instead
they make the assumption that a sentence consists
of a sequence of segments that can be individually
translated and reordered to form the translation.
This is counter-intuitive, as the who-did-what-to-
whoms of a sentence tends to be translated and re-
ordered as units, rather than have their components
mixed together. Transduction grammars (Aho and
Ullman, 1972; Wu, 1997), also called hierarchical
translation models (Chiang, 2007) or synchronous
grammars, address this through a mechanism sim-
ilar to context-free grammars. Inducing a segmen-
tal transduction grammar is hard, so the standard
practice is to use a similar method as the surface-
based models use to learn the chunks, which is
problematic, since that method mostly relies on
memorizing the relationships that the mechanics
of a compositional model is designed to general-
ize. A compositional translation model would be
able to translate lexical chunks, as well as gener-
alize different kinds of compositions; a segmen-
tal transduction grammar captures this by having
segmental lexical rules and different nonterminal
symbols for different categories of compositions.
In this paper, we focus on inducing the former:
segmental lexical rules in inversion transduction
grammars (ITGs).
One natural way would be to start with a token-
based grammar and chunk adjacent tokens to form
segments. The main problemwith chunking is that
the data becomes more and more likely as the seg-
ments get larger, with the degenerate end point of
all sentence pairs being memorized lexical items.
Zhang et al. (2008) combat this tendency by intro-
ducing a sparsity prior over the rule probabilities,
and variational Bayes to maximize the posterior
probability of the data subject to this symmetric
Dirichlet prior. To hypothesize possible chunks,
they examine the Viterbi biparse of the existing
model. Saers et al. (2012) use the entire parse for-
est to generate the hypotheses. They also boot-
strap the ITG from linear and finite-state transduc-
tion grammars (LTGs, Saers (2011), and FSTGs),
86
rather than initialize the lexical probabilities from
IBM models.
Another way to arrive at a segmental ITG is to
start with the degenerate chunking case: each sen-
tence pair as a lexical item, and segment the exist-
ing lexical rules into shorter rules. Since the start
point is the degenerate case when optimizing for
data likelihood, this approach requires a different
objective function to optimize against. Saers et al.
(2013c) proposes to use description length of the
model and the data given the model, which is sub-
sequently expressed in a Bayesian form with the
addition of a prior over the rule probabilities (Saers
andWu, 2013). The way they generate hypotheses
is restricted to segmenting an existing lexical item
into two parts, which is problematic, because em-
bedded lexical items are potentially overlooked.
There is also the option of implicitly defining
all possible grammars, and sample from that dis-
tribution. Blunsom et al. (2009) do exactly that;
they induce with collapsed Gibbs sampling which
keeps one derivation for each training sentence
that is altered and then resampled. The operations
to change the derivations are split, join, delete
and insert. The split-operator corresponds to bi-
nary segmentation, the join-operator corresponds
to chunking; the delete-operator removes an inter-
nal node, resulting in its parent having three chil-
dren, and the insert-operator allows a parent with
three children to be normalized to have only two.
The existence of ternary nodes in the derivation
means that the learned grammar contains ternary
rules. Note that it still takes three operations: two
split-operations and one delete-operation for their
model to do what we propose to do in a single
ternary segmentation. Also, although we allow for
single-step ternary segmentations, our grammar
does not contain ternary rules; instead the results of
a ternary segmentation is immediately normalized
to the 2-normal form. Although their model can
theoretically sample from the entire model space,
the split-operation alone is enough to do so; the
other operations were added to get the model to do
so in practice. Similarly, we propose ternary seg-
mentation to be able to reach areas of the model
space that we failed to reach with binary segmen-
tation.
To illustrate the problem with embedded lexi-
cal items, we will introduce a small example cor-
pus. Although Swedish and English are relatively
similar, with the structure of basic sentences being
identical, they already illustrate the common prob-
lem of rare embedded correspondences. Imagine
a really simple corpus of three sentence pairs with
identical structure:
he has a red book / han har en r?d bok
she has a biology book / hon har en biologibok
it has begun / det har b?rjat
The main difference is that Swedish concate-
nates rather than juxtaposes compounds such as
biologibok instead of biology book. A bilingual
person looking at this corpus would produce bilin-
gual parse trees like those in Figure 1. Inducing
this relatively simple segmental ITG from the data
is, however, quite a challenge.
The example above illustrates a problem with
the chunking approach, as one of the most com-
mon chunks is has a/har en, whereas the linguis-
tically motivated chunk biology book/biologibok
occurs only once. There is very little in this data
that would lead the chunking approach towards the
desired ITG. It also illustrates a problem with the
binary segmentation approach, as all the bilingual
prefixes and suffixes, the biaffixes, are unique;
there is no way of discovering that all the above
sentences have the exact same verb.
In this paper, we propose a method to al-
low bilingual infixes to be hypothesized and used
to drive the minimization of description length,
which would be able to induce the desired ITG
from the above corpus.
The paper is structured so that we start by giv-
ing a definition of the grammar formalism we use:
ITGs (Section 2). We then describe the notion
of description length that we use (Section 3), and
how ternary segmentation differs from and com-
plements binary segmentation (Section 4). We
then present our induction algorithm (Section 5)
and give an example of a run through (Section 6).
Finally we offer some concluding remarks (Sec-
tion 7).
2 Inversion transduction grammars
Inversion transduction grammars, or ITGs (Wu,
1997), are an expressive yet efficient way to
model translation. Much like context-free gram-
mars (CFGs), they allow for sentences to be ex-
plained through composition of smaller units into
larger units, but where CFGs are restricted to gen-
erate monolingual sentences, ITGs generate sets
of sentence pairs ? transductions ? rather than
languages. Naturally, the components of differ-
87
hasshe a biology book
har en biologibokhon
hasit begun
har b?rjatdet
hashe a red
har en r?dhan
book
bok
Figure 1: Possible inversion transduction trees over the example sentence pairs.
ent languages may have to be ordered differently,
which means that transduction grammars need to
handle these differences in order. Rather than al-
lowing arbitrary reordering and pay the price of ex-
ponential time complexity, ITGs allow only mono-
tonically straight or inverted order of the produc-
tions, which cuts the time complexity down to a
manageable polynomial.
Formally, an ITG is a tuple ?N,?,?, R, S?,
where N is a finite nonempty set of nonterminal
symbols, ? is a finite set of terminal symbols in
L
0
, ? is a finite set of terminal symbols in L
1
, R
is a finite nonempty set of inversion transduction
rules and S ? N is a designated start symbol. An
inversion transduction rule is restricted to take one
of the following forms:
S ? [A] , A?
[
?
+
]
, A? ??
+
?
where S ? N is the start symbol, A ? N is a non-
terminal symbol, and ?+ is a nonempty sequence
of nonterminals and biterminals. A biterminal is
a pair of symbol strings: ?? ???, where at least
one of the strings have to be nonempty. The square
and angled brackets signal straight and inverted or-
der respectively. With straight order, both the L
0
and the L
1
productions are generated left-to-right,
but with inverted order, theL
1
production is gener-
ated right-to-left. The brackets are frequently left
out when there is only one element on the right-
hand side, which means that S ? [A] is shortened
to S ? A.
Like CFGs, ITGs also have a 2-normal form,
analogous to the Chomsky normal form for CFGs,
where the rules are further restricted to only the
following four forms:
S ? A, A? [BC] , A? ?BC?, A? e/f
where S ? N is the start symbol, A,B,C ? N
are nonterminal symbols and e/f is a biterminal
string.
A bracketing ITG, or BITG, has only one non-
terminal symbol (other than the dedicated start
symbol), which means that the nonterminals carry
no information at all other than the fact that their
yields are discrete unit. Rather than make a proper
analysis of the sentence pair they only produce a
bracketing, hence the name.
A transduction grammar such as ITG can be
used in three modes: generation, transduction
and biparsing. Generation derives a bisentence, a
sentence pair, from the start symbol. Transduction
derives a sentence in one language from a sentence
in the other language and the start symbol. Bipars-
ing verifies that a given bisentence can be derived
from the start symbol. Biparsing is an integral part
of any learning that requires expected counts such
as expectation maximization, and transduction is
the actual translation process.
3 Description length
We follow the definition of description length from
Saers et al. (2013b,c,d,a); Saers and Wu (2013),
that is: the size of the model is determined by
counting the number of symbols needed to encode
the rules, and the size of the data given the model
is determined by biparsing the data with the model.
Formally, given a grammar? its description length
DL (?) is the sum of the length of the symbols
needed to serialize the rule set. For convenience
later on, the symbols are assumed to be uniformly
distributed with a length of?lg 1
N
bits each (where
N is the number of different symbols). The de-
scription length of the data D given the model is
defined as DL (D|?) = ?lgP (D|?).
88
Figure 2: The four different kinds of binary seg-
mentation hypotheses.
Figure 3: The two different hypotheses that can
be made from an infix-to-infix link.
4 Segmenting lexical items
With a background in computer science it is tempt-
ing to draw the conclusion that any segmentation
can be made as a sequence of binary segmenta-
tions. This is true, but only relevant if the entire
search space can be exhaustively explored. When
inducing transduction grammars, the search space
is prohibitively large; in fact, we are typically af-
forded only an estimate of a single step forward
in the search process. In such circumstances, the
kinds of steps you can take start to matter greatly,
and adding ternary segmentation to the typically
used binary segmentation adds expressive power.
Figure 2 contains a schematic illustration of bi-
nary segmentation: To the left is a lexical item
where a good biaffix (anL
0
prefix or suffix associ-
ated with anL
1
prefix or suffix) has been found, as
illustrated with the solid connectors. To the right is
the segmentation that can be inferred. For binary
segmentation, there is no uncertainty in this step.
When adding ternary segmentation, there are
five more situations: one situation where an in-
Figure 4: The eight different hypotheses that can
bemade from the four different infix-to-affix links.
fix is linked to an infix, and four situations where
an infix is linked to an affix. Figure 3 shows the
infix-to-infix situation, where there is one addi-
tional piece of information to be decided: are the
surroundings linked straight or inverted? Figure 4
shows the situations where one infix is linked to an
affix. In these situations, there are twomore pieces
of information that needs to be inferred: (a) where
the sibling of the affix needs to be segmented, and
(b) how the two pieces of the sibling of the affix
link to the siblings of the infix. The infix-to-affix
situations require a second monolingual segmen-
tations decision to be made. As this is beyond the
scope of this paper, we will limit ourselves to the
infix-to-infix situation.
5 Finding segmentation hypotheses
Previous work on binary hypothesis generation
makes assumptions that do not hold with ternary
segmentation; this section explains why that is and
how we get around it. The basic problem with bi-
nary segmentation is that any bisegment hypothe-
sized to be good on its own has to be anchored to
either the beginning or the end of an existing biseg-
ment. An infix, by definition, does not.
While recording all affixes is possible, even for
non-toy corpora (Saers and Wu, 2013; Saers et al.,
2013b,c), recording all bilingual infixes is not, so
collecting them all is not an option (while there are
89
Algorithm 1 Pseudo code for segmenting an ITG.
? ? The ITG being induced.
? ? The token-based ITG used to evaluate lexical rules.
h
max
? The maximum number of hypotheses to keep from a single lexical rule.
repeat
? ? 0
H
?
? Initial hypotheses
for all lexical rules A? e/f do
p? parse(?, e/f)
c ? Fractional counts of bispans
for all bispans s, t, u, v ? e/f do
c(s, t, u, v)? 0
H
??
? []
for all items B
s,t,u,v
? p do
c(s, t, u, v)? c(s, t, u, v) + ?(B
s,t,u,v
)?(B
s,t,u,v
)/?(S
0,T,0,V
)
H
??
? [H
??
, ?s, t, u, v, c(s, t, u, v)?]
sort H ?? on c(s, t, u, v)
for all ?s, t, u, v, c(s, t, u, v)? ? H ??[0..h
max
] do
H
?
(e
s..t
/f
u..v
)? [H
?
(e
s..t
/f
u..v
), ?s, t, u, v, A? e/f?]
H ? Evaluated hypotheses
for all bisegments e
s..t
/f
u..v
? keys(H
?
) do
?
?
? ?
R? []
for all bispan-rule pairs ?s, t, u, v, A? e/f? ? H ?(e
s..t
/f
u..v
) do
?
?
? make_grammar_change(??, e/f, s, t, u, v)
R? [R,A? e/f ]
?
?
? DL(?
?
)?DL(?) +DL(D|?
?
)?DL(D|?)
if ?? < 0 then
H ? [H, ?e
s..t
/f
u..v
, R, ?
?
?]
sort H on ??
for all ?e
s..t
/f
u..v
, R, ?
?
? ? H do
?
?
? ?
for all rules A? e/f ? R ?R
?
? do
?
?
? make_grammar_change(??, e/f, s, t, u, v)
?
?
? DL(?
?
)?DL(?) +DL(D|?
?
)?DL(D|?)
if ?? < 0 then
?? ?
?
? ? ? + ?
?
until ? ? 0
return ?
only O
(
n
2
)
possible biaffixes for a parallel sen-
tence of average length n, there are O
(
n
4
)
possi-
ble bilingual infixes). A way to prioritize, within
the scope of a single bisegment, which infixes and
affixes to consider as hypotheses is crucial. In this
paper we use an approach similar to Saers et al.
(2013d), in which we use a token-based ITG to
evaluate the lexical rules in the ITG that is be-
ing induced. Using a transduction grammar has
the advantage of calculating fractional counts for
hypotheses, which allows both long and short hy-
potheses to compete on a level playing field.
In Algorithm 1, we start by parsing all the lex-
ical rules in the grammar ? being learned using a
token-based ITG ?. For each rule, we only keep
the best hmax bispans. In the second part, all col-
lected bispans are evaluated as if they were the
only hypothesis being considered for changing ?.
Any hypothesis with a positive effect is kept for
further processing. These hypotheses are sorted
90
and applied. Since the grammarmay have changed
since the effect of the hypothesis was estimated,
we have to check that the hypothesis would have
a positive effect on the updated grammar before
committing to it. All this is repeated as long as
there are improvements that can be made.
Themake_grammar_changemethod deletes the
old rule, and distributes its probability mass to
the rules replacing it. For ternary segmentation,
this will be three lexical rules, and two structural
rules (which happens to be identical in a bracket-
ing grammar, giving that one rule two shares of
the probability mass being distributed). For binary
segmentation it is two lexical rules and one struc-
tural rule.
Rather than calculating DL (D|?)? DL (D|?)
explicitly by biparsing the entire corpus, we es-
timate the change. For binary rules, we use the
same estimate as Saers and Wu (2013): multiply-
ing in the new rule probabilities and dividing out
the old. For ternary rules, we make the assump-
tion that the three new lexical rules are combined
using structural rules the way they would during
parsing, which means two binary structural rules
being applied. The infix-to-infix situation must be
generated either by two straight combinations or
by two inverted combinations, so for a bracketing
grammar it is always two applications of a single
structural rule. We thus multiply in the three new
lexical rules and the structural rule twice, and di-
vide out the old rule. In essence, both these meth-
ods are recreating the situations in which the parser
would have used the old rule, but now uses the new
rules.
Having exhausted all the hypotheses, we also
run expectation maximization to stabilize the pa-
rameters. This step is not shown in the pseudo
code.
Examining the pseudocode closer reveals that
the outer loop will continue as long as the grammar
changes; since the only way the grammar changes
is by making lexical rules shorter, this loop is guar-
anteed to terminate. Inside the outer loop there are
three inner loops: one over the rule set, one over
the set of initial hypothesesH ? and one over the set
of evaluated hypothesesH . The sets of hypotheses
are related such that |H| ? |H ?|, which means that
the size of the initial set of hypotheses will dom-
inate the time complexity. The size of this initial
set of hypotheses is itself limited so that it cannot
contain more than hmax hypotheses from any one
rule. The dominating factor is thus the size of the
rule set, which we will further analyze.
The first thing we do is to parse the right-hand
side of the rule, which requires O
(
n
3
)
with the
Saers et al. (2009) algorithm, where n is the av-
erage length of the lexical items. We then initial-
ize the counts, which does not actually require a
specific step in implementation. We then iterate
over all bispans in the parse, which has the same
upper bound as the parsing process, since the ap-
proximate parsing algorithm avoids exploring the
entire search space. We then sort the set of hy-
potheses derived from the current rule only, which
is asymptotically bound byO
(
n
3lgn
)
, since there
is exactly one hypothesis per parse item. Finally,
there is a selection being made from the set of hy-
potheses derived from the current rule. In prac-
tice, the parsing is more complicated than the sort-
ing, making the time complexity of the whole inner
loop be dominated by the time it takes to parse the
rules.
6 Example
In this section we will trace through how the ex-
ample from the introduction fails to go through
binary segmentation, but succeeds when infix-to-
infix segmentations are an option.
The initial grammar consists of all the sentence
pairs as segmental lexical rules:
S ? A 1
A?
he has a red book
han har en r?d bok 0.3
A?
she has a biology book
hon har en biologibok 0.3
A?
it has begun
det har b?rjat 0.3
As noted before, there are no shared biaffixes
among the three lexical rules, so binary segmen-
tation cannot break this grammar down further.
There are, however, three shared bisegments rep-
resenting three different segmentation hypotheses:
has a/har en, has/har and a/en. In this example it
does not matter which hypothesis you choose, so
we will go with the first one, since that is the one
our implementation chose. Breaking out all occur-
rences of has a/har en gives the following gram-
91
mar:
S ? A 1
A? [AA] 0.36
A? it has begun/det har b?rjat 0.09
A? has a/har en 0.18
A? he/han 0.09
A? red book/r?d bok 0.09
A? she/hon 0.09
A? biology book/biologibok 0.09
At this point there are two bisegments that occur
in more than one rule: has/har and a/en. Again,
it does not matter for the final outcome which of
the hypotheses we choose, so we will chose the
first one, again because that is the one our imple-
mentation chose. Breaking out all occurrences of
has/har gives the following grammar:
S ? A 1
A? [AA] 0.421
A? he/han 0.053
A? red book/r?d bok 0.053
A? she/hon 0.053
A? biology book/biologibok 0.053
A? has/har 0.158
A? it/det 0.053
A? begun/b?rjat 0.053
A? a/en 0.105
There are no shared bisegments left in the gram-
mar now, so no more segmentations can be done.
Obviously, the probability of the data given this
new grammar is much smaller, but the grammar
itself has generalized far beyond the training data,
to the point where it largely agrees with the pro-
posed trees in Figure 1 (except that this grammar
binarizes the constituents, and treats red book/r?d
bok as a segment).
7 Conclusions
We have shown that there are situations in which
a top-down segmenting approach that relies solely
on binary segmentation will fail to generalize, de-
spite there being ample evidence to a human that
a generalization is warranted. We have proposed
ternary segmentation as a solution to provide hy-
potheses that are considered good under a mini-
mum description length objective. And we have
shown that the proposed method could indeed per-
form generalizations that are clear to the human
eye, but not discoverable through binary segmen-
tation. The algorithm is comparable to previ-
ous segmentation approaches in terms of time and
space complexity, so scaling up to non-toy training
corpora is likely to work when the time comes.
Acknowledgements
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.
References
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Phil Blunsom, Trevor Cohn, Chris Dyer, andMiles
Osborne. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Joint Confer-
ence of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and 4th
International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP
2009), pp. 782?790, Suntec, Singapore, August
2009.
David Chiang. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?
228, 2007.
Frans Josef Och, Christoph Tillmann, and Her-
mann Ney. Improved alignment models for sta-
tistical machine translation. In 1999 Joint SIG-
DAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Cor-
pora, pp. 20?28, University of Maryland, Col-
lege Park, Maryland, June 1999.
Markus Saers and Dekai Wu. Bayesian induc-
tion of bracketing inversion transduction gram-
mars. In Sixth International Joint Conference on
Natural Language Processing (IJCNLP2013),
pp. 1158?1166, Nagoya, Japan, October 2013.
Asian Federation of Natural Language Process-
ing.
92
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference
on Parsing Technologies (IWPT?09), pp. 29?32,
Paris, France, October 2009.
Markus Saers, Karteek Addanki, and Dekai Wu.
From finite-state to inversion transductions: To-
ward unsupervised bilingual grammar induc-
tion. In 24th International Conference on
Computational Linguistics (COLING 2012), pp.
2325?2340, Mumbai, India, December 2012.
Markus Saers, Karteek Addanki, and Dekai Wu.
Augmenting a bottom-up ITG with top-down
rules by minimizing conditional description
length. In Recent Advances in Natural Lan-
guage Processing (RANLP 2013), Hissar, Bul-
garia, September 2013.
Markus Saers, Karteek Addanki, and Dekai Wu.
Combining top-down and bottom-up search for
unsupervised induction of transduction gram-
mars. In Seventh Workshop on Syntax, Se-
mantics and Structure in Statistical Translation
(SSST-7), pp. 48?57, Atlanta, Georgia, June
2013.
Markus Saers, Karteek Addanki, and Dekai Wu.
Iterative rule segmentation under minimum
description length for unsupervised transduc-
tion grammar induction. In Adrian-Horia
Dediu, Carlos Mart?n-Vide, Ruslan Mitkov, and
Bianca Truthe, editors, Statistical Language and
Speech Processing, First International Confer-
ence, SLSP 2013, Lecture Notes in Artificial In-
telligence (LNAI). Springer, Tarragona, Spain,
July 2013.
Markus Saers, Karteek Addanki, and Dekai Wu.
Unsupervised transduction grammar induction
via minimum description length. In Second
Workshop on Hybrid Approaches to Transla-
tion (HyTra), pp. 67?73, Sofia, Bulgaria, Au-
gust 2013.
Markus Saers. Translation as Linear Trans-
duction: Models and Algorithms for Efficient
Learning in Statistical Machine Translation.
PhD thesis, Uppsala University, Department of
Linguistics and Philology, 2011.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, 1997.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In 46th Annual Meeting of the Association
for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pp. 97?
105, Columbus, Ohio, June 2008.
93
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144?153,
Dublin, Ireland, August 23, 2014.
Lexical Access Preference and Constraint Strategies for Improving
Multiword Expression Association within Semantic MT Evaluation
Dekai Wu Lo Chi-kiu Markus Saers
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{dekai|jackielo|masaers|dekai}@cs.ust.hk
Abstract
We examine lexical access preferences and constraints in computing multiword expression asso-
ciations from the standpoint of a high-impact extrinsic task-based performance measure, namely
semantic machine translation evaluation. In automated MT evaluation metrics, machine transla-
tions are compared against human reference translations, which are almost never worded exactly
the sameway except in the most trivial of cases. Because of this, one of the most important factors
in correctly predicting semantic translation adequacy is the accuracy of recognizing alternative
lexical realizations of the same multiword expressions in semantic role fillers. Our results com-
paring bag-of-words, maximum alignment, and inversion transduction grammars indicate that
cognitively motivated ITGs provide superior lexical access characteristics for multiword expres-
sion associations, leading to state-of-the-art improvements in correlation with human adequacy
judgments.
1 Introduction
We investigate lexical access strategies in the context of computing multiword expression associations
within automatic semantic MT evaluation metrics?a high-impact real-world extrinsic task-based per-
formance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious
issues in machine translation and automatic MT evaluation; there are simply too many forms to enumer-
ate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a
decade and a half, but until recently little has been done to use lexical semantics as the main foundation
for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nie?en et al.,
2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference
and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and
Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation
adequacy.
Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et
al., 2012; Lo andWu, 2013b), have instead directly couchedMT evaluation in the more cognitive terms of
semantic frames, by measuring the degree to which the basic event structure is preserved by translation?
the ?who did what to whom, for whom, when, where, how and why? (Pradhan et al., 2004)?emphasizing
that a good translation is one that can successfully be understood by a human. Across a variety of language
pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both n-
gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and
METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al., 2000), and TER (Snover et al., 2006) when evaluatingMT output (Lo and
Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach??ek and Bojar, 2013). Furthermore, tuning
the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/
144
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
are parsed using automatic English SRL. There are no semantic frames for MT3 since automatic SRL
decided to drop the predicate.
adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) across different languages (English
and Chinese) and different genres (formal newswire text, informal web forum text and informal public
speech).
Because of this, we have chosen to run our lexical association experiments in the context of the neces-
sity of recognizingmatching semantic role fillers, approximately 85%ofwhich aremultiword expressions
in our data, the overwhelming majority of which would not be enumerated within conventional lexicons.
We compare four common lexical access approaches to aggregation, preferences, and constraints: bag-
of-words, two different types of maximal alignment, and inversion transduction grammar based methods.
2 Background
The MEANT metric measures weighted f-scores over corresponding semantic frames and role fillers
in the reference and machine translations. Whereas HMEANT uses human annotation, the automatic
versions of MEANT instead replace humans with automatic SRL and alignment algorithms. MEANT
typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human ade-
quacy judgment, and is relatively easy to port to other languages, requiring only an automatic semantic
parser and a monolingual corpus of the output language, which is used to gauge lexical similarity between
the semantic role fillers of the reference and translation. More precisely, MEANT computes scores as
follows:
1. Apply an automatic shallow semantic parser to both the references and MT output. (Figure 1 shows
examples of automatic shallow semantic parses on both reference and MT.)
2. Apply the maximum weighted bipartite matching algorithm to align the semantic frames between
the references and MT output according to the lexical similarities of the predicates.
3. For each pair of the aligned frames, apply the maximum weighted bipartite matching algorithm to
align the arguments between the reference and MT output according to the lexical similarity of role
fillers.
4. Compute the weighted f-score over the matching role labels of these aligned predicates and role
fillers according to the following definitions:
145
q0
i,j
? ARG j of aligned frame i in MT
q
1
i,j
? ARG j of aligned frame i in REF
w
0
i
?
#tokens filled in aligned frame i of MT
total #tokens in MT
w
1
i
?
#tokens filled in aligned frame i of REF
total #tokens in REF
wpred ? weight of similarity of predicates
w
j
? weight of similarity of ARG j
e
i,pred ? the pred of the aligned frame i of the machine translation
f
i,pred ? the pred of the aligned frame i of the reference translation
e
i,j
? the ARG j of the aligned frame i of the machine translation
f
i,j
? the ARG j of the aligned frame i of the reference translation
s(e, f) = lexical similarity of token e and f
prece,f =
?
e?e max
f?f
s(e, f)
| e |
rece,f =
?
f?f max
e?e
s(e, f)
| f |
precision =
?
i
w
0
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
0
i,j
|
?
i
w
0
i
recall =
?
i
w
1
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
1
i,j
|
?
i
w
1
i
MEANT = 2 ? precision ? recallprecision + recall
where the possible approaches to defining the lexical associations s
i,pred and si,j are discussed in the
following section. q0
i,j
and q1
i,j
are the argument of type j in frame i in MT and REF, respectively. w0
i
and w1
i
are the weights for frame i in MT and REF, respectively. These weights estimate the degree
of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of
the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between
the reference translations and the MT output. There is a total of 12 weights for the set of semantic role
labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised
estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and
Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using
relative frequency of each semantic role label in the references and thus UMEANT is useful when human
judgments on adequacy of the development set are unavailable.
3 Comparison of multiword expression association approaches
To assess alternative lexical access preferences and constraints for computing multiword expression
associations, we now consider four alternative approaches to defining the lexical similarities s
i,pred and
s
i,j
, all of which employ a standard context vector model of the individual words/tokens in the multiword
expression arguments between the reference and machine translations, as descibed by Lo et al. (2012)
and Tumuluru et al. (2012).
3.1 Bag of words (geometric mean)
The original MEANT approaches employed standard a bag-of-words strategy for lexical association.
This baseline approach applies no alignment constraints on multiword expressions:
s
i,pred = e
?
e?e
i,pred
?
f?f
i,pred
lg(s(e,f))
|e
i,pred|?|fi,pred|
s
i,j
= e
?
e?e
i,j
?
f?f
i,j
lg(s(e,f))
|e
i,j
|?|f
i,j
|
146
3.2 Maximum alignment (precision-recall average)
In the first maximum alignment based approach we will consider, the definitions of s
i,pred and si,j are
inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length.
s
i,pred =
1
2
(prece
i,pred,fi,pred + recei,pred,fi,pred)
s
i,j
=
1
2
(prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
)
3.3 Maximum alignment (f-score)
The second of the maximum alignment based approaches replaces the above linear averaging of pre-
cision and recall with a proper f-score. Although this is less consistent with the previous literature, such
as Mihalcea et al. (2006), it seems more consistent with the overall f-score based approach of MEANT,
and thus we include it in our comparison as a variant of the maximum alignment strategy.
s
i,pred =
2 ? prece
i,pred,fi,pred ? recei,pred,fi,pred
prece
i,pred,fi,pred + recei,pred,fi,pred
s
i,j
=
2 ? prece
i,j
,f
i,j
? rece
i,j
,f
i,j
prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
3.4 Inversion transduction grammar based
There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve
the accuracy of MT evaluation metrics?despite (1) long empirical evidence the vast majority of transla-
tion patterns between human languages can be accommodated within ITG constraints, and (2) the obser-
vation thatmost current state-of-the-art SMT systems employ ITG decoders. Especially when considering
semanticMTmetrics, ITGs would seem to be a natural strategy for multiword expression association for
several cognitively motivated reasons, having to do with language universal properties of cross-linguistic
semantic frame structure.
To begin with, it is quite natural to think of sentences as having been generated from an abstract concept
using a rewriting system: a stochastic grammar predicts how frequently any particular realization of the
abstract concept will be generated. The bilingual analogy is a transduction grammar generating a pair
of possible realizations of the same underlying concept. Stochastic transduction grammars predict how
frequently a particular pair of realizations will be generated, and thus represent a good way to evaluate
how well a pair of sentences correspond to each other.
The particular class of transduction grammars known as ITGs tackle the problem that the (bi)parsing
complexity for general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By
constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted
reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low
end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to
have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and
space (O
(
n
6
)
time and O
(
n
4
)
space). It is also possible to do approximate biparsing in O
(
n
3
)
time
(Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an
ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have also been directly shown to be more than sufficient
to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This
language universal property has an evolutionary explanation in terms of computational efficiency and
cognitive load for language learnability and interpretability (Wu, 2014).
ITGs are thus an appealing alternative for evaluating the possible links between both semantic role
fillers in different languages as well as the predicates, and how these parts fit together to form entire
semantic frames. We believe that ITGs are not only capable of generating the desired structural corre-
spondences between the semantic structures of two languages, but also provide meaningful constraints
to prevent alignments from wandering off in the wrong direction.
Following this reasoning, alternate definitions of s
i,pred and si,j can be constructed in terms of brack-
eting ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated
147
nonterminal category (Wu, 1995a). The idea is to attack a potential weakness of the foregoing three
lexical association strategies, namely that word/token alignments between the reference and machine
translations are severely underconstrained. No bijectivity or permutation restrictions are applied, even
between compositional segments where this should be natural. This can cause multiword expressions of
semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inver-
sion transduction grammar can potentially better constrain permissible token alignment patterns between
aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed
permutations between semantic role fillers across the reference and machine translations for a sample
sentence from the evaluation data.
In this approach, both alignment and scoring are performed utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define s
i,pred and
s
i,j
as follows.
s
i,pred = lg?1
?
?
lg
(
P
(
A ?? e
i,pred/fi,pred|G
))
max(| e
i,pred |, | fi,pred |)
?
?
s
i,j
= lg?1
?
?
lg
(
P
(
A ?? e
i,j
/f
i,j
|G
))
max(| e
i,j
|, | f
i,j
|)
?
?
where
G ? ?{A} ,W0,W1,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 1
p (e/f |A) = s(e, f)
Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with
e ? W
0
? {?} denoting a token in the MT output (or the null token) and f ? W1 ? {?} denoting
a token in the reference translation (or the null token). The rule probability (or more accurately, rule
weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is
defined by MEANT?s lexical similarity measure on English Gigaword context vectors. To calculate the
inside probability (or more accurately, inside score) of a pair of segments, P
(
A ?? e/f|G
)
, we use the
algorithm described in Saers et al. (2009). Given this, s
i,pred and si,j now represent the length normalized
BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and
machine translations.
4 Experiments
In this section we discuss experiments comparing the four alternative lexical access preference and
constraint strategies.
4.1 Experimental setup
We compared using the DARPA GALE P2.5 Chinese-English translation test set, as used in Lo and
Wu (2011a). The corpus includes the Chinese input sentences, each accompanied by an English reference
translation and three participating state-of-the-art MT systems? output.
We computed sentence-level correlations following the benchmark assessment procedure used by
WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; Mach??ek and Bojar,
2013), which use Kendall?s ? correlation coefficient, to evaluate the correlation of evaluation metrics
against human judgment on ranking the translation adequacy of the three systems? output. A higher
value for Kendall?s ? indicates more similarity to the human adequacy rankings by the evaluation met-
rics. The range of possible values of Kendall?s ? correlation coefficient is [-1, 1], where 1 means the
148
Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE
P2.5 data. For reference, the human HMEANT upper bound is 0.53?so the fully automatic ITG based
MEANT approximation is not far from closing the gap.
Kendall correlation
MEANT + ITG based 0.51
MEANT + maximum alignment (f-score) 0.48
MEANT + maximum alignment (average of precision & recall) 0.46
MEANT + bag of words (geometric mean) 0.38
NIST 0.29
METEOR 0.20
BLEU 0.20
TER 0.20
PER 0.20
CDER 0.12
WER 0.10
systems are ranked in the same order as the human judgment by the evaluation metric; and -1 means the
systems are ranked in the reverse order as human judgment by the evaluation metric.
For both reference and machine translations, the ASSERT (Pradhan et al., 2004) semantic role labeler
was used to automatically predict semantic parses.
4.2 Results and discussion
The sentence-level correlations in Table 1 show that the ITG based strategy outperforms other auto-
matic metrics in correlation with human adequacy judgment. Note that this was achieved with no tuning
whatsoever of the rule weights (suggesting that the performance could be further improved in the future
by slightly optimizing the ITG weights).
The ITG based strategy shows 3 points improvement over the next best strategy, which is maximal
alignment under f-score aggregation. The ITG based approach produces much higher HAJ correlations
than any of the other metrics.
In fact, the ITG based strategy even comes within a few points of the human upper bound bench-
mark HAJ correlations computed using the human labeled semantic frames and alignments used in the
HMEANT.
Data analysis reveals two reasons that the ITG based strategy correlates with human adequacy judge-
ment more closely than the other approaches. First, BITG constraints indeed provide more accurate
phrasal similarity aggregation, compared to the naive bag-of-words based heuristics. Similar results
have been observed while trying to estimate word alignment probabilities where BITG constraints out-
performed alignments from GIZA++ (Saers and Wu, 2009). Secondly, the permutation and bijectivity
constraints enforced by the ITG provide better leverage to reject token alignments when they are not
appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The
ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave
tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not
simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed
that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered
in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better
fits the semantic structure.
5 Conclusion
We have compared four alternative lexical access strategies for aggregation, preferences, and con-
straints in scoringmultiword expression associations that are far too numerous to be explicitly enumerated
in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words,
149
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
biparse tree and alignment matrix depictions, for the Chinese input sentence ????????????
???????? Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
150
two maximum alignment based approaches, and an inversion transduction grammar based approach.
Controlled experiments within the MEANT semantic MT evaluation framework shows that the cog-
nitively motivated ITG based strategy achieves significantly higher correlation with human adequacy
judgments of MT output quality than the more typically used lexical association approaches. The results
show how to improve upon previous research showing that MEANT?s explicit use of semantic frames
leads to state-of-the-art automatic MT evaluation, by aligning and scoring semantic frames under a sim-
ple, consistent ITG that provides empirically informative permutation and bijectivity biases, instead of
more naive maximal alignment or bag-of-words assumptions.
Cognitive studies of the lexicon are often described using intrinsic measures of quality. Our exper-
iments complement this by situating the empirical comparisons within extrinsic real-world task-based
performance measures. We believe that progress can be accelerated via a combination of intrinsic and
extrinsic measures of lexicon acquisition and access models.
Acknowledgments
This material is based upon work supported in part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract nos. HR0011-12-C-0014 and HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-0023; by the European Union under the FP7
grant agreement no. 287658; and by the Hong Kong Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA,
the EU, or RGC.
References
Karteek Addanki, Chi-kiu Lo, Markus Saers, and Dekai Wu. LTG vs. ITG coverage of cross-lingual verb
frame alternations. In 16th Annual Conference of the European Association for Machine Translation
(EAMT-2012), Trento, Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation, and Compiling. Prentice-Halll,
Englewood Cliffs, New Jersey, 1972.
Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the European Chapter of the Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third Workshop on Statistical Machine Translation (WMT-
08), 2008.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Pryzbocki, and Omar Zaidan.
Findings of the 2010 joint workshop on statistical machine translation and metrics for machine trans-
lation. In Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR (WMT10), pages
17?53, Uppsala, Sweden, 15-16 July 2010.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F. Zaidan. Findings of the 2011
Workshop on Statistical Machine Translation. In 6th Workshop on Statistical Machine Translation
(WMT 2011), 2011.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. Find-
ings of the 2012 workshop on statistical machine translation. In 7th Workshop on Statistical Machine
Translation (WMT 2012), pages 10?51, 2012.
151
George Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence
statistics. In The second international conference on Human Language Technology Research
(HLT ?02), San Diego, California, 2002.
Philipp Koehn and Christof Monz. Manual and automatic evaluation of machine translation between
european languages. InWorkshop on Statistical Machine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chapter of the Association for Computational Linguistics
(EACL-2006), 2006.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluat-
ing translation utility based on semantic roles. In 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. supervised weight estimation for semantic MT evaluation
metrics. In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6),
2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Statistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and Dekai Wu. Improving machine translation by training
against an automatic semantic frame based evaluationmetric. In 51st AnnualMeeting of the Association
for Computational Linguistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Improving machine translation into Chinese by tuning
against Chinese MEANT. In International Workshop on Spoken Language Translation (IWSLT 2013),
2013.
Matou? Mach??ek and Ond?ej Bojar. Results of the WMT13 metrics shared task. In Eighth Workshop
on Statistical Machine Translation (WMT 2013), Sofia, Bulgaria, August 2013.
I. DanMelamed. Automatic construction of clean broad-coverage translation lexicons. In 2nd Conference
of the Association for Machine Translation in the Americas (AMTA-1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first National Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Hermann Ney. A evaluation tool for machine transla-
tion: Fast evaluation forMT research. In The Second International Conference on Language Resources
and Evaluation (LREC 2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics
(ACL-02), pages 311?318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow semantic
parsing using support vector machines. In Human Language Technology Conference of the North
American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004), 2004.
Markus Saers and Dekai Wu. Improving phrase-based translation via word alignments from stochastic
inversion transduction grammars. In Third Workshop on Syntax and Structure in Statistical Translation
(SSST-3), pages 28?36, Boulder, Colorado, June 2009.
152
Markus Saers, JoakimNivre, and DekaiWu. Learning stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th International Conference on Parsing Technologies
(IWPT?09), pages 29?32, Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and JohnMakhoul. A study of trans-
lation edit rate with targeted human annotation. In 7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages 223?231, Cambridge, Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu. Accuracy and robustness in measuring the lex-
ical similarity of semantic role fillers for automatic semantic MT evaluation. In 26th Pacific Asia
Conference on Language, Information, and Computation (PACLIC 26), 2012.
Dekai Wu. An algorithm for simultaneously bracketing parallel texts by aligning words. In 33rd An-
nual Meeting of the Association for Computational Linguistics (ACL 95), pages 244?251, Cambridge,
Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars for parallel text bracketing. In Third Annual Workshop
on Very Large Corpora (WVLC-3), pages 69?81, Cambridge, Massachusetts, June 1995.
Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, 1997.
Dekai Wu. The magic number 4: Evolutionary pressures on semantic frame structure. In 10th Interna-
tional Conference on the Evolution of Language (Evolang X), Vienna, Apr 2014.
Richard Zens and Hermann Ney. A comparative study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),
pages 144?151, Stroudsburg, Pennsylvania, 2003.
153
