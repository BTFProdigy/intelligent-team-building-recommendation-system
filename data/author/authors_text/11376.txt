Proceedings of NAACL HLT 2009: Short Papers, pages 41?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Automatic Image Region Annotation - Image Region Textual
Coreference Resolution
Emilia Apostolova
College of Computing and Digital Media
DePaul University
Chicago, IL 60604, USA
emilia.aposto@gmail.com
Dina Demner-Fushman
Communications Engineering Branch
National Library of Medicine
Bethesda, MD 20894, USA
ddemner@mail.nih.gov
Abstract
Detailed image annotation necessary for reli-
able image retrieval involves not only annotat-
ing the image as a single artifact, but also an-
notating specific objects or regions within the
image. Such detailed annotation is a costly en-
deavor and the available annotated image data
are quite limited. This paper explores the fea-
sibility of using image captions from scientific
journals for the purpose of automatically an-
notating image regions. Salient image clues,
such as an object location within the image or
an object color, together with the associated
explicit object mention, are extracted and clas-
sified using rule-based and SVM learners.
1 Introduction
The profusion of digitally available images has nat-
urally led to an interest in the field of automatic im-
age annotation and retrieval. A number of studies
attempt to associate image regions with the corre-
sponding concepts. In (Duygulu et al, 2002), for
example, the problem of annotation is treated as a
translation from a set of image segments (or blobs)
to a set of words. Modeling the association between
blobs and words for the purpose of automated an-
notation has also been proposed by (Barnard et al,
2003; Jeon et al, 2003).
A recurring hindrance that appears in studies aim-
ing at automatic image region annotation is the lack
of an appropriate dataset. All of the above studies
use the Corel image dataset that consists of 60,000
images annotated with 3 to 5 keywords. The need
for an image dataset with annotated image regions
has been recognized by many researchers. For ex-
ample, Russell et al(2008) have developed a tool
and a general purpose image database designed to
delineate and annotate objects within image scenes.
The need for an image dataset with annotated ob-
ject boundaries appears to be especially pertinent in
the biomedical field. Organizing and using for re-
search the available medical imaging data proved to
be a challenge and a goal of the ongoing research.
Rubin et al(2008), for example, propose an ontol-
ogy and annotation tool for semantic annotation of
image regions in radiology.
However, creating a dataset of image regions
manually annotated and delineated by domain ex-
perts, is a costly enterprise. Any attempts to auto-
mate or semi-automate the process would be of a
substantial value.
This work proposes an approach towards auto-
matic annotation of regions of interest in images
used in scientific publications. Publications abun-
dant in image data are an untapped source of an-
notated image data. Due to publication standards,
meaningful image captions are almost always pro-
vided within scientific articles. In addition, image
Regions of Interest (ROIs) are commonly referred to
within the image caption. Such ROIs are also com-
monly delineated with some kind of an overlay that
helps locating the ROI. This is especially true for
hard to interpret scientific images such as radiology
images. ROIs are also described in terms of location
within the image, or by the presence of a particular
color. Identifying ROI mentions within image cap-
tions and visual clues pinpointing the ROI within the
image would be the first step in building an object
41
1. Object Location - explicit ROI location, e.g. front row, back-
ground, top, bottom, left, right.
Shells of planktonic animals called formainifera record cli-
matic conditions as they are formed. This one, Globigeri-
noides ruber, lives year-round at the surface of the Sargasso Sea.
The form of the live animal is shown at right, and its shell, which
is actually about the size of a fine grain of sand, at left.
2. Object Color - presence of a distinct color that identifies a
ROI.
Anterior SSD image shows an elongated splenorenal varix (blue
area). The varix travels from the splenic hilar region inferiorly
along the left flank, down into the pelvis, and eventually back up to
the left renal vein via the left gonadal vein. The kidney is encoded
yellow, the portal system is encoded magenta, and the spleen is
encoded tan.
3. Overlay Marker - an overlay marker used to pinpoint the loca-
tion of the ROI, e.g. arrows, asterisks, bounding boxes, or circles.
Transverse sonograms obtained with a 7.5-MHz linear trans-
ducer in the subareolar region. The straight arrows
show a dilated tubular structure. The curved arrow indicates
an intraluminal solid mass.
4. Overlay Label - an overlay label used to pinpoint the location
of the ROI, e.g. numbers, letters, words, abbreviations.
Location of the calf veins. Transverse US image just
above ankle demonstrates the paired posterior tibial veins (V)
and posterior tibial artery (A) imaged from a posteromedial ap-
proach. Note there is inadequate venous flow velocity to visualize
with color Doppler without flow augmentation.
Table 1: Image Markers divided into four categories, followed by
a sample image caption1 in which Image Markers are marked in bold,
Image Marker Referents are underlined.
delineated and annotated image dataset.
2 Problem Definition
The goal of this research is to locate visually salient
image region characteristics in the text surrounding
scientific images that could be used to facilitate the
delineation of the image object boundaries. This
task could be broken down into two related subtasks
- 1) locating and classifying textual clues for visu-
ally salient ROI features (Image Markers), and 2) lo-
cating the corresponding ROI text mentions (Image
Marker Referents). Table 1 gives a classification of
Image Markers including examples of Image Mark-
ers and Image Marker Referents. Figure 1 shows the
frequency of Image Marker occurrences.
1The captions were extracted from Radiology and Ra-
diographics c? Radiological Society of North America and
Oceanus c?Woods Hole Oceanographic Institution.
3 Related Work
Cohen et al(2003) attempt to identify what they
refer to as ?image pointers? within captions in
biomedical publications. The image pointers of in-
terest are, for example, image panel labels, or letters
and abbreviations used as an overlay within the im-
age, similar to the Overlay Labels described in Table
1. They developed a set of hand-crafted rules, and a
learning method involving Boosted Wrapper Induc-
tion on a dataset consisting of biomedical articles
related to fluorescence microscope images.
Deschacht and Moens (2007) analyze text sur-
rounding images in news articles trying to identify
persons and objects in the text that appear in the
corresponding image. They start by extracting per-
sons? names and visual objects using Named Entity
Recognition (NER) tools. Next, they measure the
?salience? of the extracted named entities within the
text with the assumption that more salient named en-
tities in the text will also be present in the accompa-
nying image.
Davis et al(2003) develop a NER tool to iden-
tify references to a single art object (for example a
specific building within an image) in text related to
art images for the purpose of automatic cataloging
of images. They take a semi-supervised approach to
locating the named entities of interest by first provid-
ing an authoritative list of art objects of interest and
then seeking to match variants of the seed named en-
tities in related text.
4 Experimental Methods and Results
4.1 Dataset
Figure 1: Distribution of Image
Marker types across 400 annotated
image captions.
The chosen date-
set contains more
than 60,000 images
together with their as-
sociated captions from
three online life and
earth sciences jour-
nals1. 400 randomly
selected image cap-
tions were manually
annotated by a single
annotator with their
Image Markers and Image Marker Referents and
used for testing and for cross-validation respectively
42
in the two methods described below.
4.2 Rule Based Approach
First, we developed a two-stage rule-based, boot-
strapping algorithm for locating the image markers
and their coreferents from unannotated data. The al-
gorithm is based on the observation that textual im-
age markers commonly appear in parentheses and
are usually closely related semantic concepts. Thus
the seed for the algorithm consists of:
1. The predominant syntactic pattern - parenthe-
ses, as in ?hooking of the soft palate (arrow)?. This
pattern could easily be captured by a regular expres-
sion and doesn?t require sentence parsing.
2. A dozen seed phrases (e.g ?left?, ?circle?, ?as-
terisk?, ?blue?) identified by initially annotating a
small subset of the data (20 captions). Wordnet was
used to look up and prepare a list of their corre-
sponding inherited hypernyms. This hypernym list
contains concepts such as ?a spatially limited lo-
cation?, ?a two-dimensional shape?, ?a written or
printed symbol?, ?a visual attribute of things that
results from the light they emit or transmit or re-
flect?. Best results were achieved when inherited hy-
pernyms up to the third parent were used.
In the first stage of the algorithm, all image cap-
tions were searched for parenthesized expressions
that share the seed hypernyms. This step of the al-
gorithm will result in high precision, but a low re-
call since image markers do not necessarily appear
in parentheses. To increase recall, in stage 2 a full
text search was performed for the stemmed versions
of the expressions identified in stage 1.
A baseline measure was also computed for the
identification of the Image Marker Referents using a
simple heuristic - the coreferent of the Image Marker
is usually the closest Noun Phrase (NP). In the case
of parenthesized image markers, it is the closest NP
to the left of the image marker; in the case of non-
parenthesized image markers, the referent is usually
the complement of the verb; and in the case of pas-
sive voice, the NP preceding the verb phrase. The
Stanford parser was used to parse the sentences.
Table 2 summarizes the results validated against
the annotated dataset (excluding the 20 captions
used to identify the seed phrases). It appears that the
relatively low accuracy for Image Marker Referent
identification was mostly due to parsing errors since
Precision Recall F1-score
Image Marker 87.70 68.10 76.66
Image Marker Referent Accuracy 59.10
Table 2: Rule-based approach results for Image Marker and Im-
age Marker Referent identification. Image Marker Referent results are
reported as accuracy because the algorithm involves locating an Image
Marker Referent for each Image Marker. Referent identification accu-
racy was computed for all annotated Image Markers.
Kind k-5 . . . k0 . . . k+5
Orth o-5 . . . o0 . . . o+5
Stem s-5 . . . s0 . . . s+5
Hypernym h-5 . . . h0 . . . h+5
Dep Path d-5 . . . d0 . . . d+5
Category [c0]
Table 3: Features from a surrounding token window are used to
classify the current token into category [c0]. Best results were achieved
with a five-token window.
the syntactic structure of the image caption texts is
quite distinct from the Penn Treebank dataset used
for training the Stanford parser.
4.3 Support Vector Machines
Next we explored the possibility of improving the
rule-based method results by applying a machine
learning technique on the set of annotated data. Sup-
port Vector Machines (SVM) (Vapnik, 2000) was
the approach taken because it is a state-of-the-art
classification approach proven to perform well on
many NLP tasks.
In our approach, each sentence was tokenized,
and tokens were classified as Beginning, Inside, or
Outside an Image Marker type or Image Marker Ref-
erent. Image Marker Referents are not related to Im-
age Markers and creating a classifier trained on this
task is planned as future work. SVM classifiers were
trained for each of these categories, and combined
via ?one-vs-all? classification (the category of the
classifier with the largest output was selected). Fea-
tures of the surrounding context are used as shown
in Table 3 and Table 4.
Table 5 summarizes the results of a 10-fold cross-
validation. SVM performed well overall for iden-
tifying Image Markers, Location being the hardest
because of higher variability of expressing ROI posi-
tion. Image Marker Referents are harder to classify,
43
Token Kind The general type of the sentence to-
ken (Word, Number, Symbol, Punctuation,
White space).
Orthography Orthographic categorization of the token
(Upper initial, All capitals, Lower case,
Mixed case).
Stem The stem of the token, extracted with the
Porter stemmer.
Wordnet Super-
class
Wordnet hypernyms (nouns, verbs); the hy-
pernym of the derivationally related form
(adjectives); the superclass of the pertanym
(adverbs).
POS Category POS categories extracted using Brill?s tag-
ger.
Dependency
Path*
The smallest sentence parse subtree includ-
ing both the current token and the anno-
tated image marker(s), encoded as an undi-
rected path across POS categories.
Table 4: Orthographic, semantic, and grammatical classification
features computed for each token (*Dependency Path is used only for
classifying Image Marker Referents).
as deeper syntactic knowledge is necessary. Idiosyn-
cratic syntactic structures in image captions pose
a problem for the general-purpose trained Stanford
parser and performance is hindered by the accuracy
of computing Dependency Path feature.
5 Conclusion and Future Work
We explored the feasibility of determining the con-
tent of ROIs in images from scientific publications
using image captions. We developed a two-stage
rule-based approach that utilizes WordNet to find
ROI pointers (Image Markers) and their referents.
We also explored a supervised machine learning ap-
proach. Both approaches are promising. The rule-
based approach seeded with a small manually an-
notated set resulted in 78.7% precision and 68.1%
recall for Image Markers recognition. The SVM ap-
proach (which requires a greater annotation effort)
outperformed the rule based approach (p=93.6%,
r=87.7%). Future plans include training SVMs on
the results of the rule-based annotation. Further
work is also needed in improving Image Marker
Referent identification and co-reference resolution.
We also plan to involve two annotators in order
to collect a more robust dataset based on inter-
annotator agreement.
References
K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D.M.
Blei, and M.I. Jordan. 2003. Matching words and
Precision Recall F1-score
Location 60.93 45.15 51.86
Color 100.00 51.32 67.82
Overlay Marker 97.43 95.39 96.39
Overlay Label 85.74 87.69 86.70
Overall 93.64 87.69 90.56
Image Marker Referent Accuracy 61.15
Table 5: SVM classification results for the four types of Image
Markers, and for Image Marker Referents. LibSVM software was used
(3-degree polynomial kernel, cost parameter = 1, ? = 0.6 empirically
determined).
pictures. The Journal of Machine Learning Research,
3:1107?1135.
W.W. Cohen, R. Wang, and R.F. Murphy. 2003. Un-
derstanding captions in biomedical publications. In
Proceedings of the 9th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 499?504. ACM New York, NY, USA.
P.T. Davis, D.K. Elson, and J.L. Klavans. 2003. Methods
for precise named entity matching in digital collec-
tions. In Proceedings of the 3rd ACM/IEEE-CS joint
conference on Digital libraries, pages 125?127. IEEE
Computer Society Washington, DC, USA.
K. Deschacht and M. Moens. 2007. Text analysis for au-
tomatic image annotation. In Proceedings of the 45th
Annual ACL Meeting, pages 1000?1007. ACL.
P. Duygulu, K. Barnard, JFG de Freitas, and D.A.
Forsyth. 2002. Object Recognition as Machine Trans-
lation: Learning a Lexicon for a Fixed Image Vocab-
ulary. LECTURE NOTES IN COMPUTER SCIENCE,
pages 97?112.
J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Au-
tomatic image annotation and retrieval using cross-
media relevance models. In Proceedings of the 26th
annual international ACM SIGIR conference on Re-
search and development in informaion retrieval, pages
119?126. ACM New York, NY, USA.
D. Rubin, P. Mongkolwat, V. Kleper, K. Supekar, and
D. Channin. 2008. Medical imaging on the Semantic
Web: Annotation and image markup. In AAAI Spring
Symposium Series, Semantic Scientific Knowledge In-
tegration.
B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Free-
man. 2008. LabelMe: A Database and Web-Based
Tool for Image Annotation. International Journal of
Computer Vision, 77(1):157?173.
V.N. Vapnik. 2000. The Nature of Statistical Learning
Theory. Springer.
44
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1924?1929,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining Visual and Textual Features for Information Extraction from
Online Flyers
Emilia Apostolova
BrokerSavant Inc
2506 N. Clark St.
Chicago, IL 60614
emilia@brokersavant.com
Noriko Tomuro
DePaul University
243 S. Wabash Ave.
Chicago, IL 60604
tomuro@cs.depaul.edu
Abstract
Information in visually rich formats such
as PDF and HTML is often conveyed
by a combination of textual and visual
features. In particular, genres such as
marketing flyers and info-graphics often
augment textual information by its color,
size, positioning, etc. As a result, tradi-
tional text-based approaches to informa-
tion extraction (IE) could underperform.
In this study, we present a supervised ma-
chine learning approach to IE from on-
line commercial real estate flyers. We
evaluated the performance of SVM clas-
sifiers on the task of identifying 12 types
of named entities using a combination of
textual and visual features. Results show
that the addition of visual features such
as color, size, and positioning significantly
increased classifier performance.
1 Introduction
Since the Message Understanding Conferences in
the 1990s (Grishman and Sundheim, 1996; Chin-
chor and Robinson, 1997), Information Extraction
(IE) and Named Entity Recognition (NER) ap-
proaches have been applied and evaluated on a va-
riety of domains and textual genres. The majority
of the work, however, focuses on the journalistic,
scientific, and informal genres (newswires, scien-
tific publications, blogs, tweets, and other social
media texts) (Nadeau and Sekine, 2007) and deals
with purely textual corpora. As a result, the fea-
ture space of NER systems involves purely tex-
tual features, typically word attributes and char-
acteristics (orthography, morphology, dictionary
lookup, etc.), their contexts and document features
(surrounding word window, local syntax, docu-
ment/corpus word frequencies, etc.) (Nadeau and
Sekine, 2007).
At the same time, textual information is often
presented in visually rich formats, e.g. HTML and
PDF. In addition to text, these formats use a vari-
ety of visually salient characteristics, (e.g. color,
font size, positioning) to either highlight or aug-
ment textual information. In some genres and do-
mains, a textual representation of the data, exclud-
ing visual features is often not enough to accu-
rately identify named entities of interest or extract
relevant information. Marketing materials, such
as online flyers or HTML emails, often contain
a plethora of visual features and text-based NER
approaches lead to poor results. In this paper, we
present a supervised approach that uses a combi-
nation of textual and visual features to recognize
named entities in online marketing materials.
2 Motivation and Problem Definition
A number of broker-based industries (e.g. com-
mercial real estate, heavy equipment machinery,
etc.) lack a centralized searchable database with
industry offerings. In particular, the commercial
real estate industry (unlike residential real estate)
does not have a centralized database or an estab-
lished source of information. Commercial real
estate brokers often need to rely on networking,
chance, and waste time with a variety of commer-
cial real estate databases that often present out-
dated information. While brokers do not often up-
date third party inventory databases, they do create
marketing materials (usually PDF flyers) that con-
tain all relevant listing information. Virtually all
commercial real estate offerings come with pub-
licly available marketing material that contains all
relevant listing information. Our goal is to harness
this source of information (the marketing flyer)
and use it to extract structured listing information.
Figure 1 shows an example of a commercial
real estate flyer. The commercial real estate fly-
ers are often distributed as PDF documents, links
to HTML pages, or visually rich HTML-based
1924
Figure 1: An example of a commercial real estate
flyer
c
? Kudan Group Real Estate.
emails. They typically contain all relevant listing
information such as the address and neighborhood
of the offering, the names and contact information
of the brokers, the type of space offered (build-
ing, land, unit(s) within a building), etc. Similar to
other info-graphics, relevant information could be
easily pinpointed by visual clues. For example, the
listing street address in Figure 1 (1629 N. Halsted
St., upper left corner) can be quickly identified and
distinguished from the brokerage firm street ad-
dress (156 N. Jefferson St., upper right corner) due
to its visual prominence (font color, size, position-
ing).
In this study we explored a supervised machine
learning approach to the task of identifying list-
ing information from commercial real estate fly-
ers. In particular, we focused on the recognition
of 12 types of named entities as described in Table
1 below.
3 Related Work
Nadeau and Satoshi (2007) present a survey of
NER and describe the feature space of NER re-
search. While they mention multi-media NER in
the context of video/text processing, all described
features/approaches focus only on textual repre-
sentation.
Broker Name The contact information of all
Broker Email listing brokers, including full name,
Broker Phone email address, phone number.
Company Phone The brokerage company phone
number.
Street The address information of the
City listing address including street or
Neighborhood intersection, city, neighborhood,
State state, and zip code.
Zip
Space Size Size and attributes of relevant spaces
Space Type (e.g. 27,042 SF building, 4.44 acres
site, etc.); Mentions of space type
descriptors, e.g. building, land/lot,
floor, unit. This excludes space type
and size information of non-essential
listing attributes (e.g. basement size
or parking lot size).
Confidential Any mentions of confidentiality.
Table 1: Types and descriptions of named enti-
ties relevant to extracting listing information from
commercial real estate flyers.
The literature on Information Extraction from
HTML resources is dominated by various ap-
proaches based on wrapper induction (Kushmer-
ick, 1997; Kushmerick, 2000). Wrapper induc-
tions rely on common HTML structure (based on
the HTML DOM) and formatting features to ex-
tract structured information from similarly format-
ted HTML pages. This approach, however, is not
applicable to the genres of marketing materials
(PDF and HTML) since they typically do not share
any common structure that can be used to iden-
tify relevant named entities. Laender et al. (2002)
present a survey of data extraction techniques and
tools from structured or semi-structured web re-
sources.
Cai et al. (2003) present a vision-based segmen-
tation algorithm of web pages that uses HTML
layout features and attempts to partition the page
at the semantic level. In (Burget and Rudolfova,
2009) authors propose web-page block classifica-
tion based on visual features. Yang and Zhang
(2001) build a content tree of HTML documents
based on visual consistency inferred semantics.
Burget (2007) proposes a layout based informa-
tion extraction from HTML documents and states
that this visual approach is more robust than tradi-
tional DOM-based methods.
Changuel et al.(2009a) describe a system for
automatically extracting author information from
web-pages. They use spatial information based on
the depth of the text node in the HTML DOM tree.
In (Changuel et al., 2009b) and (Hu et al., 2006),
1925
the authors proposed a machine learning method
for title extraction and utilize format information
such as font size, position, and font weight. In
(Zhu et al., 2007) authors use layout information
based on font size and weight for NER for auto-
mated expense reimbursement.
While the idea of utilizing visual features based
on HTML style has been previously suggested,
this study tackles a non-trivial visually rich dataset
that prevents the use of previously suggested sim-
plistic approaches to computing HTML features
(such as relying on the HTML DOM tree or sim-
plistic HTML style rendering). In addition, we in-
troduce the use of RGB color as a feature and nor-
malize it approximating human perception.
4 Dataset and Method
The dataset consists of 800 randomly selected
commercial real estate flyers spanning 315 US
locations, 75 companies, and 730 brokers. The
flyers were collected from various online sources
and were originally generated using a variety of
HTML and PDF creator tools. The collection rep-
resents numerous flyer formats and layouts, com-
mercial real estate property types (industrial, re-
tail, office, land, etc.), and transactions (invest-
ment, sale, lease).
All flyers were converted to a common format
(HTML)
1
. The HTML versions of all documents
were then annotated by 2 annotators. Figure 2
shows an example of an annotated flyer. Annota-
tion guidelines were developed and the 2 annota-
tors were able to achieve an inter-annotator agree-
ment of 91%
2
. The named entities with lowest
inter-annotator agreement were entities describ-
ing Space Size and Type because of the some-
what complex rules for determining essential list-
ing space information. For example, one of the
space size/type rules reads as follows: If the list-
ing refers to a building and mentions the lot size, include
both the land size, the building size, and corresponding space
types. Do not include individual parts of the building (e.g.
office/basement) as separate spaces. If the listing refers to a
UNIT within the building, not the whole building, then DO
NOT include the land site as a separate space.
A supervised machine learning approach was
1
PDFs were converted to HTML using the PDFTO-
HTML conversion program http://pdftohtml.
sourceforge.net/.
2
The inter-annotator agreement was measured as F1-score
using one of the annotator?s named entities as the gold stan-
dard set and the other as a comparison set.
Figure 2: The HTML versions of the flyers were
annotated by 2 annotators using a custom web-
based annotation tool.
then applied to the task of identifying the 12
named entities shown in Table 1. Flyers were con-
verted to text using an HTML parser while pre-
serving some of the white space formatting. The
text was tokenized and the task was then modeled
as a BIO classification task, classifiers identify the
Beginning, the Inside, and Outside of the text seg-
ments. We first used a traditional set of text-based
features for the classification task. Table 2 lists
the various text-based features used. In all cases,
a sliding window including the 5 preceding and 5
following tokens was used as features.
Feature Name Description
Token A normalized string representation of
the token. All tokens were converted
to lower case and all digits were
converted to a common format.
Token Orth The token orthography. Possible values
are lowercase (all token characters are
lower case), all capitals (all token
characters are upper case), upper initial
(the first token character is upper case,
the rest are lower case), mixed (any
mixture of upper and lower case letters
not included in the previous categories).
Token Kind Possible values are word, number,
symbol, punctuation.
Regex type Regex-based rules were used to mark
chunks as one of 3 regex types:
email, phone number, zip code.
Gazetteer Text chunks were marked as possible
US cities or states based on US Census
Bureau city and state data.
www.census.gov/geo/maps-data/data/gazetteer2013.html.
Table 2: List of text-based features used for the
NER task. A sliding window of the 5 preceding
and 5 following tokens was used for all features.
1926
As noted previously, human annotators were
able to quickly spot named entities of interest
solely because of their visual characteristics. For
example, a text-only version of the flyer shown in
Figure 1, stripped of all rich formatting, will make
it quite difficult to distinguish the listing address
(shown in prominent size, position, and color)
from the brokerage company address, which is
rarely prominent as it is not considered important
information in the context of the flyer. Similarly,
the essential size information for the listing shown
on Figure 2 appears prominently on the first page
(square footage of the offered restaurant), while
non-essential size information, such as the size of
the adjacent parking lot or basement, tend to ap-
pear in smaller font on subsequent flyer pages.
To account for such visual characteristics we at-
tempted to also include visual features associated
with text chunks. We used the computed HTML
style attributes for each DOM element containing
text. Table 3 lists the computed visual features.
Feature Name Description
Font Size The computed font-size attribute of
the surrounding HTML DOM element,
normalized to 7 basic sizes (xx-small,
x-small, small, medium, large, x-large,
xx-large).
Color The computed color attribute of the
surrounding HTML DOM element.
The RGB values were normalized
to a set of 100 basic colors. We
converted the RGB values to the
YUV color space, and then used
Euclidian distance to find the
most similar basic color
approximating human perception.
Y Coordinate The computed top attribute of the
surrounding HTML DOM element, i.e.
the y-coordinate in pixels. The pixel
locations was normalized to 150 pixel
increments (roughly 1/5th of the
visible screen for the most common
screen resolution.)
Table 3: List of visual features used for the NER
task. A sliding window of 5 preceding and 5 fol-
lowing DOM elements were used for all features.
Computing the HTML style attributes is a com-
plex task since they are typically defined by a
combination of CSS files, in-lined HTML style
attributes, and browser defaults. The complex-
ities of style definition, inheritance, and over-
writing are handled by browsers
3
. We used the
3
We attempted to use an HTML renderer from the Cobra
java toolkit http://lobobrowser.org/cobra.jsp
to compute HTML style attributes. However, this renderer
Chrome browser to compute dynamically the style
of each DOM element and output it as inline
style attributes. To achieve this we program-
matically inserted a javascript snippet that inlines
the computed style and saves the new version of
the HTML on the local file system utilizing the
HTML5 saveAs interface
4
. Details on how we
normalized the style attribute values for font size,
RGB color, and Y coordinate are shown in Table
3.
We then applied Support Vector Machines
(SVM) (Vapnik, 2000) on the NER task using the
LibSVM library (Chang and Lin, 2011). We chose
SVMs as they have been shown to perform well
on a variety of NER tasks, for example (Isozaki
and Kazawa, 2002; Takeuchi and Collier, 2002;
Mayfield et al., 2003; Ekbal and Bandyopadhyay,
2008). We used a linear kernel model with the
default parameters. The multi-class problem was
converted to binary problems using the one-vs-
others scheme. 80% of the documents were used
for training, and the remaining 20% for testing.
5 Results
Results are shown in Table 4. We compared clas-
sifier performance using only textual features (first
3 columns), versus performance using both textual
and visual features (next 3 columns). Results were
averaged over 2 runs of randomly selected train-
ing/test documents with 80%/20% ratio. We used
an exact measure which considers an answer to be
correct only if both the entity boundaries and en-
tity type are accurately predicted.
The addition of visual features significantly
5
increased the overall F1-score from 83 to 87%.
As expected, performance gains are more signif-
icant for named entities that are typically visu-
ally salient and are otherwise difficult (or impossi-
ble) to identify in a text-only version of the fly-
ers. Named Entities referring to listing address
information showed the most significant improve-
ments. In particular, the F1-score for mentions of
Neighborhoods (typically prominently shown on
the first page of the flyers) improved by 19%; F1-
score for mentions of the listing State improved by
9%; and Street, City, Zip by roughly 4% each, all
produced poor results on our dataset and failed to accurately
compute the pixel location of text elements.
4
https://github.com/eligrey/FileSaver.
js
5
The difference is statistically significant with p value <
0.0001% using Z-test on two proportions.
1927
Named Entity Pt Rt Ft Pv+t Rv+t Fv+t S
Broker Name 82.7 91.7 87.0 95.0 91.6 93.2 Y
Broker Email 92.3 92.8 92.6 97.2 90.2 93.6 N
Broker Phone 90.2 86.1 88.1 94.7 85.2 89.7 N
Company Ph. 95.2 67.4 78.9 89.8 65.4 75.7 N
Street 87.4 70.5 78.1 87.3 77.3 82.0 Y
City 92.5 88.5 90.5 94.9 92.8 93.8 Y
Neighborhood 68.2 52.8 59.5 85.3 72.9 78.6 Y
State 77.4 97.5 86.3 95.8 95.0 95.4 Y
Zip 89.7 94.5 92.1 96.1 97.1 96.6 Y
Space Size 80.2 65.0 71.8 87.0 70.6 77.9 Y
Space Type 76.0 74.7 75.3 78.6 72.2 75.3 N
Confidential 100 60.0 75.0 75.0 85.7 79.9 N
OVERALL 84.8 81.3 83.0 91.2 83.2 87.0 Y
Table 4: Results from applying SVM using the
textual features described in Table 2, as well as
both the textual and visual features described in
Tables 2 and 3. t=textual features only, v+t=visual
+ textual features, P=Precision, R=Recall, F=F1-
score, S=Significant Difference
statistically significant. Visual clues are also typi-
cally used when identifying relevant size informa-
tion and, as expected, performance improved sig-
nificantly by roughly 6%. The difference in per-
formance for mentions used to describe confiden-
tial information is not statistically significant
6
be-
cause such mentions rarely occurred in the dataset.
Similarly, performance differences for Company
Phone, Broker Phone, Broker Email, and Space
Type are not statistically significant. In all of
these cases, visual features did not influence per-
formance and text-based features proved adequate
predictors.
6 Conclusion
We have shown that information extraction in cer-
tain genres and domains spans different media -
textual and visual. Ubiquitous online and dig-
ital formats such as PDF and HTML often ex-
ploit the interaction of textual and visual elements.
Information is often augmented or conveyed by
non-textual features such as positioning, font size,
color, and images. However, traditionally, NER
approaches rely exclusively on textual features
and as a result could perform poorly in visually
rich genres such as online marketing flyers or info-
graphics. We have evaluated the performance gain
on the task of NER from commercial real estate
flyers by adding visual features to a set of tradi-
tional text-based features. We used SVM classi-
fiers for the task of identifying 12 types of named
entities. Results show that overall visual features
improved performance significantly.
6
p value = 0.7323% using Z-test on two proportions.
References
Radek Burget and Ivana Rudolfova. 2009. Web
page element classification based on visual features.
In Intelligent Information and Database Systems,
2009. ACIIDS 2009. First Asian Conference on,
pages 67?72. IEEE.
Radek Burget. 2007. Layout based information extrac-
tion from html documents. In Document Analysis
and Recognition, 2007. ICDAR 2007. Ninth Inter-
national Conference on, volume 2, pages 624?628.
IEEE.
Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying
Ma. 2003. Extracting content structure for web
pages based on visual representation. In Web Tech-
nologies and Applications, pages 406?417. Springer.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Sahar Changuel, Nicolas Labroche, and Bernadette
Bouchon-Meunier. 2009a. Automatic web pages
author extraction. In Flexible Query Answering Sys-
tems, pages 300?311. Springer.
Sahar Changuel, Nicolas Labroche, and Bernadette
Bouchon-Meunier. 2009b. A general learning
method for automatic title extraction from html
pages. In Machine Learning and Data Mining in
Pattern Recognition, pages 704?718. Springer.
Nancy Chinchor and Patricia Robinson. 1997. Muc-7
named entity task definition. In Proceedings of the
7th Conference on Message Understanding.
Asif Ekbal and Sivaji Bandyopadhyay. 2008. Named
entity recognition using support vector machine: A
language independent approach. International Jour-
nal of Computer Systems Science & Engineering,
4(2).
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference-6: A brief history. In
COLING, volume 96, pages 466?471.
Yunhua Hu, Hang Li, Yunbo Cao, Li Teng, Dmitriy
Meyerzon, and Qinghua Zheng. 2006. Automatic
extraction of titles from general documents using
machine learning. Information processing & man-
agement, 42(5):1276?1293.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recog-
nition. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1?7. Association for Computational Linguis-
tics.
Nicholas Kushmerick. 1997. Wrapper induction for
information extraction. Ph.D. thesis, University of
Washington.
1928
Nicholas Kushmerick. 2000. Wrapper induction: Ef-
ficiency and expressiveness. Artificial Intelligence,
118(1):15?68.
Alberto HF Laender, Berthier A Ribeiro-Neto, Alti-
gran S da Silva, and Juliana S Teixeira. 2002. A
brief survey of web data extraction tools. ACM Sig-
mod Record, 31(2):84?93.
James Mayfield, Paul McNamee, and Christine Piatko.
2003. Named entity recognition using hundreds of
thousands of features. In Proceedings of the seventh
conference on Natural language learning at HLT-
NAACL 2003-Volume 4, pages 184?187. Association
for Computational Linguistics.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Koichi Takeuchi and Nigel Collier. 2002. Use of
support vector machines in extended named entity
recognition. In proceedings of the 6th conference
on Natural language learning-Volume 20, pages 1?
7. Association for Computational Linguistics.
Vladimir Vapnik. 2000. The nature of statistical learn-
ing theory. springer.
Yudong Yang and HongJiang Zhang. 2001. Html page
analysis based on visual cues. In Document Analysis
and Recognition, 2001. Proceedings. Sixth Interna-
tional Conference on, pages 859?864. IEEE.
Guangyu Zhu, Timothy J Bethea, and Vikas Krishna.
2007. Extracting relevant named entities for auto-
mated expense reimbursement. In Proceedings of
the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1004?
1012. ACM.
1929
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 283?287,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation
and Speculation Scopes
Emilia Apostolova
DePaul University
Chicago, IL USA
emilia.aposto@gmail.com
Noriko Tomuro
DePaul University
Chicago, IL USA
tomuro@cs.depaul.edu
Dina Demner-Fushman
National Library of Medicine
Bethesda, MD USA
ddemner@mail.nih.gov
Abstract
Detecting the linguistic scope of negated and
speculated information in text is an impor-
tant Information Extraction task. This paper
presents ScopeFinder, a linguistically moti-
vated rule-based system for the detection of
negation and speculation scopes. The system
rule set consists of lexico-syntactic patterns
automatically extracted from a corpus anno-
tated with negation/speculation cues and their
scopes (the BioScope corpus). The system
performs on par with state-of-the-art machine
learning systems. Additionally, the intuitive
and linguistically motivated rules will allow
for manual adaptation of the rule set to new
domains and corpora.
1 Motivation
Information Extraction (IE) systems often face
the problem of distinguishing between affirmed,
negated, and speculative information in text. For
example, sentiment analysis systems need to detect
negation for accurate polarity classification. Simi-
larly, medical IE systems need to differentiate be-
tween affirmed, negated, and speculated (possible)
medical conditions.
The importance of the task of negation and spec-
ulation (a.k.a. hedge) detection is attested by a num-
ber of research initiatives. The creation of the Bio-
Scope corpus (Vincze et al, 2008) assisted in the de-
velopment and evaluation of several negation/hedge
scope detection systems. The corpus consists of
medical and biological texts annotated for negation,
speculation, and their linguistic scope. The 2010
i2b2 NLP Shared Task1 included a track for detec-
tion of the assertion status of medical problems (e.g.
affirmed, negated, hypothesized, etc.). The CoNLL-
2010 Shared Task (Farkas et al, 2010) focused on
detecting hedges and their scopes in Wikipedia arti-
cles and biomedical texts.
In this paper, we present a linguistically moti-
vated rule-based system for the detection of nega-
tion and speculation scopes that performs on par
with state-of-the-art machine learning systems. The
rules used by the ScopeFinder system are automat-
ically extracted from the BioScope corpus and en-
code lexico-syntactic patterns in a user-friendly for-
mat. While the system was developed and tested us-
ing a biomedical corpus, the rule extraction mech-
anism is not domain-specific. In addition, the lin-
guistically motivated rule encoding allows for man-
ual adaptation to new domains and corpora.
2 Task Definition
Negation/Speculation detection is typically broken
down into two sub-tasks - discovering a nega-
tion/speculation cue and establishing its scope. The
following example from the BioScope corpus shows
the annotated hedging cue (in bold) together with its
associated scope (surrounded by curly brackets):
Finally, we explored the {possible role of 5-
hydroxyeicosatetraenoic acid as a regulator of arachi-
donic acid liberation}.
Typically, systems first identify nega-
tion/speculation cues and subsequently try to
identify their associated cue scope. However,
the two tasks are interrelated and both require
1https://www.i2b2.org/NLP/Relations/
283
syntactic understanding. Consider the following
two sentences from the BioScope corpus:
1) By contrast, {D-mib appears to be uniformly ex-
pressed in imaginal discs }.
2) Differentiation assays using water soluble phor-
bol esters reveal that differentiation becomes irreversible
soon after AP-1 appears.
Both sentences contain the word form appears,
however in the first sentence the word marks a hedg-
ing cue, while in the second sentence the word does
not suggest speculation.
Unlike previous work, we do not attempt to iden-
tify negation/speculation cues independently of their
scopes. Instead, we concentrate on scope detection,
simultaneously detecting corresponding cues.
3 Dataset
We used the BioScope corpus (Vincze et al, 2008)
to develop our system and evaluate its performance.
To our knowledge, the BioScope corpus is the
only publicly available dataset annotated with nega-
tion/speculation cues and their scopes. It consists
of biomedical papers, abstracts, and clinical reports
(corpus statistics are shown in Tables 1 and 2).
Corpus Type Sentences Documents Mean Document Size
Clinical 7520 1954 3.85
Full Papers 3352 9 372.44
Paper Abstracts 14565 1273 11.44
Table 1: Statistics of the BioScope corpus. Document sizes
represent number of sentences.
Corpus Type Negation Cues Speculation Cues Negation Speculation
Clinical 872 1137 6.6% 13.4%
Full Papers 378 682 13.76% 22.29%
Paper Abstracts 1757 2694 13.45% 17.69%
Table 2: Statistics of the BioScope corpus. The 2nd and 3d
columns show the total number of cues within the datasets; the
4th and 5th columns show the percentage of negated and spec-
ulative sentences.
70% of the corpus documents (randomly selected)
were used to develop the ScopeFinder system (i.e.
extract lexico-syntactic rules) and the remaining
30% were used to evaluate system performance.
While the corpus focuses on the biomedical domain,
our rule extraction method is not domain specific
and in future work we are planning to apply our
method on different types of corpora.
4 Method
Intuitively, rules for detecting both speculation and
negation scopes could be concisely expressed as a
Figure 1: Parse tree of the sentence ?T cells {lack active NF-
kappa B } but express Sp1 as expected? generated by the Stan-
ford parser. Speculation scope words are shown in ellipsis. The
cue word is shown in grey. The nearest common ancestor of all
cue and scope leaf nodes is shown in a box.
combination of lexical and syntactic patterns. For
example, O?zgu?r and Radev (2009) examined sample
BioScope sentences and developed hedging scope
rules such as:
The scope of a modal verb cue (e.g. may, might, could)
is the verb phrase to which it is attached;
The scope of a verb cue (e.g. appears, seems) followed
by an infinitival clause extends to the whole sentence.
Similar lexico-syntactic rules have been also man-
ually compiled and used in a number of hedge scope
detection systems, e.g. (Kilicoglu and Bergler,
2008), (Rei and Briscoe, 2010), (Velldal et al,
2010), (Kilicoglu and Bergler, 2010), (Zhou et al,
2010).
However, manually creating a comprehensive set
of such lexico-syntactic scope rules is a laborious
and time-consuming process. In addition, such an
approach relies heavily on the availability of accu-
rately parsed sentences, which could be problem-
atic for domains such as biomedical texts (Clegg and
Shepherd, 2007; McClosky and Charniak, 2008).
Instead, we attempted to automatically extract
lexico-syntactic scope rules from the BioScope cor-
pus, relying only on consistent (but not necessarily
accurate) parse tree representations.
We first parsed each sentence in the training
dataset which contained a negation or speculation
cue using the Stanford parser (Klein and Manning,
2003; De Marneffe et al, 2006). Figure 1 shows the
parse tree of a sample sentence containing a nega-
tion cue and its scope.
Next, for each cue-scope instance within the sen-
tence, we identified the nearest common ancestor
284
Figure 2: Lexico-syntactic pattern extracted from the sentence
from Figure 1. The rule is equivalent to the following string
representation: (VP (VBP lack) (NP (JJ *scope*) (NN *scope*)
(NN *scope*))).
which encompassed the cue word(s) and all words in
the scope (shown in a box on Figure 1). The subtree
rooted by this ancestor is the basis for the resulting
lexico-syntactic rule. The leaf nodes of the resulting
subtree were converted to a generalized representa-
tion: scope words were converted to *scope*; non-
cue and non-scope words were converted to *; cue
words were converted to lower case. Figure 2 shows
the resulting rule.
This rule generation approach resulted in a large
number of very specific rule patterns - 1,681 nega-
tion scope rules and 3,043 speculation scope rules
were extracted from the training dataset.
To identify a more general set of rules (and in-
crease recall) we next performed a simple transfor-
mation of the derived rule set. If all children of a
rule tree node are of type *scope* or * (i.e. non-
cue words), the node label is replaced by *scope*
or * respectively, and the node?s children are pruned
from the rule tree; neighboring identical siblings of
type *scope* or * are replaced by a single node of
the corresponding type. Figure 3 shows an example
of this transformation.
(a) The children of nodes JJ/NN/NN are
pruned and their labels are replaced by
*scope*.
(b) The children
of node NP are
pruned and its la-
bel is replaced by
*scope*.
Figure 3: Transformation of the tree shown in Figure 2. The
final rule is equivalent to the following string representation:
(VP (VBP lack) *scope* )
The rule tree pruning described above reduced the
negation scope rule patterns to 439 and the specula-
tion rule patterns to 1,000.
In addition to generating a set of scope finding
rules, we also implemented a module that parses
string representations of the lexico-syntactic rules
and performs subtree matching. The ScopeFinder
module2 identifies negation and speculation scopes
in sentence parse trees using string-encoded lexico-
syntactic patterns. Candidate sentence parse sub-
trees are first identified by matching the path of cue
leaf nodes to the root of the rule subtree pattern. If an
identical path exists in the sentence, the root of the
candidate subtree is thus also identified. The candi-
date subtree is evaluated for a match by recursively
comparing all node children (starting from the root
of the subtree) to the rule pattern subtree. Nodes
of type *scope* and * match any number of nodes,
similar to the semantics of Regex Kleene star (*).
5 Results
As an informed baseline, we used a previously de-
veloped rule-based system for negation and spec-
ulation scope discovery (Apostolova and Tomuro,
2010). The system, inspired by the NegEx algorithm
(Chapman et al, 2001), uses a list of phrases split
into subsets (preceding vs. following their scope) to
identify cues using string matching. The cue scopes
extend from the cue to the beginning or end of the
sentence, depending on the cue type. Table 3 shows
the baseline results.
Correctly Predicted Cues All Predicted Cues
Negation P R F F
Clinical 94.12 97.61 95.18 85.66
Full Papers 54.45 80.12 64.01 51.78
Paper Abstracts 63.04 85.13 72.31 59.86
Speculation
Clinical 65.87 53.27 58.90 50.84
Full Papers 58.27 52.83 55.41 29.06
Paper Abstracts 73.12 64.50 68.54 38.21
Table 3: Baseline system performance. P (Precision), R (Re-
call), and F (F1-score) are computed based on the sentence to-
kens of correctly predicted cues. The last column shows the
F1-score for sentence tokens of all predicted cues (including er-
roneous ones).
We used only the scopes of predicted cues (cor-
rectly predicted cues vs. all predicted cues) to mea-
2The rule sets and source code are publicly available at
http://scopefinder.sourceforge.net/.
285
sure the baseline system performance. The base-
line system heuristics did not contain all phrase cues
present in the dataset. The scopes of cues that are
missing from the baseline system were not included
in the results. As the baseline system was not penal-
ized for missing cue phrases, the results represent
the upper bound of the system.
Table 4 shows the results from applying the full
extracted rule set (1,681 negation scope rules and
3,043 speculation scope rules) on the test data. As
expected, this rule set consisting of very specific
scope matching rules resulted in very high precision
and very low recall.
Negation P R F A
Clinical 99.47 34.30 51.01 17.58
Full Papers 95.23 25.89 40.72 28.00
Paper Abstracts 87.33 05.78 10.84 07.85
Speculation
Clinical 96.50 20.12 33.30 22.90
Full Papers 88.72 15.89 26.95 10.13
Paper Abstracts 77.50 11.89 20.62 10.00
Table 4: Results from applying the full extracted rule set on the
test data. Precision (P), Recall (R), and F1-score (F) are com-
puted based the number of correctly identified scope tokens in
each sentence. Accuracy (A) is computed for correctly identi-
fied full scopes (exact match).
Table 5 shows the results from applying the rule
set consisting of pruned pattern trees (439 negation
scope rules and 1,000 speculation scope rules) on the
test data. As shown, overall results improved signif-
icantly, both over the baseline and over the unpruned
set of rules. Comparable results are shown in bold
in Tables 3, 4, and 5.
Negation P R F A
Clinical 85.59 92.15 88.75 85.56
Full Papers 49.17 94.82 64.76 71.26
Paper Abstracts 61.48 92.64 73.91 80.63
Speculation
Clinical 67.25 86.24 75.57 71.35
Full Papers 65.96 98.43 78.99 52.63
Paper Abstracts 60.24 95.48 73.87 65.28
Table 5: Results from applying the pruned rule set on the test
data. Precision (P), Recall (R), and F1-score (F) are computed
based on the number of correctly identified scope tokens in each
sentence. Accuracy (A) is computed for correctly identified full
scopes (exact match).
6 Related Work
Interest in the task of identifying negation and spec-
ulation scopes has developed in recent years. Rele-
vant research was facilitated by the appearance of a
publicly available annotated corpus. All systems de-
scribed below were developed and evaluated against
the BioScope corpus (Vincze et al, 2008).
O?zgu?r and Radev (2009) have developed a super-
vised classifier for identifying speculation cues and
a manually compiled list of lexico-syntactic rules for
identifying their scopes. For the performance of the
rule based system on identifying speculation scopes,
they report 61.13 and 79.89 accuracy for BioScope
full papers and abstracts respectively.
Similarly, Morante and Daelemans (2009b) de-
veloped a machine learning system for identifying
hedging cues and their scopes. They modeled the
scope finding problem as a classification task that
determines if a sentence token is the first token in
a scope sequence, the last one, or neither. Results
of the scope finding system with predicted hedge
signals were reported as F1-scores of 38.16, 59.66,
78.54 and for clinical texts, full papers, and abstracts
respectively3. Accuracy (computed for correctly
identified scopes) was reported as 26.21, 35.92, and
65.55 for clinical texts, papers, and abstracts respec-
tively.
Morante and Daelemans have also developed a
metalearner for identifying the scope of negation
(2009a). Results of the negation scope finding sys-
tem with predicted cues are reported as F1-scores
(computed on scope tokens) of 84.20, 70.94, and
82.60 for clinical texts, papers, and abstracts respec-
tively. Accuracy (the percent of correctly identified
exact scopes) is reported as 70.75, 41.00, and 66.07
for clinical texts, papers, and abstracts respectively.
The top three best performers on the CoNLL-
2010 shared task on hedge scope detection (Farkas
et al, 2010) report an F1-score for correctly identi-
fied hedge cues and their scopes ranging from 55.3
to 57.3. The shared task evaluation metrics used
stricter matching criteria based on exact match of
both cues and their corresponding scopes4.
CoNLL-2010 shared task participants applied a
variety of rule-based and machine learning methods
3F1-scores are computed based on scope tokens. Unlike our
evaluation metric, scope token matches are computed for each
cue within a sentence, i.e. a token is evaluated multiple times if
it belongs to more than one cue scope.
4Our system does not focus on individual cue-scope pair de-
tection (we instead optimized scope detection) and as a result
performance metrics are not directly comparable.
286
on the task - Morante et al (2010) used a memory-
based classifier based on the k-nearest neighbor rule
to determine if a token is the first token in a scope se-
quence, the last, or neither; Rei and Briscoe (2010)
used a combination of manually compiled rules, a
CRF classifier, and a sequence of post-processing
steps on the same task; Velldal et al(2010) manu-
ally compiled a set of heuristics based on syntactic
information taken from dependency structures.
7 Discussion
We presented a method for automatic extraction
of lexico-syntactic rules for negation/speculation
scopes from an annotated corpus. The devel-
oped ScopeFinder system, based on the automati-
cally extracted rule sets, was compared to a base-
line rule-based system that does not use syntac-
tic information. The ScopeFinder system outper-
formed the baseline system in all cases and exhib-
ited results comparable to complex feature-based,
machine-learning systems.
In future work, we will explore the use of statisti-
cally based methods for the creation of an optimum
set of lexico-syntactic tree patterns and will evalu-
ate the system performance on texts from different
domains.
References
E. Apostolova and N. Tomuro. 2010. Exploring surface-
level heuristics for negation and speculation discovery
in clinical texts. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
81?82. Association for Computational Linguistics.
W.W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm
for identifying negated findings and diseases in dis-
charge summaries. Journal of biomedical informatics,
34(5):301?310.
A.B. Clegg and A.J. Shepherd. 2007. Benchmark-
ing natural-language parsers for biological applica-
tions using dependency graphs. BMC bioinformatics,
8(1):24.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006. Citeseer.
R. Farkas, V. Vincze, G. Mo?ra, J. Csirik, and G. Szarvas.
2010. The CoNLL-2010 Shared Task: Learning to
Detect Hedges and their Scope in Natural Language
Text. In Proceedings of the Fourteenth Conference on
Computational Natural Language Learning (CoNLL-
2010): Shared Task, pages 1?12.
H. Kilicoglu and S. Bergler. 2008. Recognizing specu-
lative language in biomedical research articles: a lin-
guistically motivated perspective. BMC bioinformat-
ics, 9(Suppl 11):S10.
H. Kilicoglu and S. Bergler. 2010. A High-Precision
Approach to Detecting Hedges and Their Scopes.
CoNLL-2010: Shared Task, page 70.
D. Klein and C.D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. Advances in neural information processing sys-
tems, pages 3?10.
D. McClosky and E. Charniak. 2008. Self-training for
biomedical parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics on Human Language Technologies: Short Papers,
pages 101?104. Association for Computational Lin-
guistics.
R. Morante and W. Daelemans. 2009a. A metalearning
approach to processing the scope of negation. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, pages 21?29. As-
sociation for Computational Linguistics.
R. Morante and W. Daelemans. 2009b. Learning the
scope of hedge cues in biomedical texts. In Proceed-
ings of the Workshop on BioNLP, pages 28?36. Asso-
ciation for Computational Linguistics.
R. Morante, V. Van Asch, and W. Daelemans. 2010.
Memory-based resolution of in-sentence scopes of
hedge cues. CoNLL-2010: Shared Task, page 40.
A. O?zgu?r and D.R. Radev. 2009. Detecting speculations
and their scopes in scientific text. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3-Volume 3, pages
1398?1407. Association for Computational Linguis-
tics.
M. Rei and T. Briscoe. 2010. Combining manual rules
and supervised learning for hedge cue and scope detec-
tion. In Proceedings of the 14th Conference on Natu-
ral Language Learning, pages 56?63.
E. Velldal, L. ?vrelid, and S. Oepen. 2010. Re-
solving Speculation: MaxEnt Cue Classification and
Dependency-Based Scope Rules. CoNLL-2010:
Shared Task, page 48.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The BioScope corpus: biomedical texts anno-
tated for uncertainty, negation and their scopes. BMC
bioinformatics, 9(Suppl 11):S9.
H. Zhou, X. Li, D. Huang, Z. Li, and Y. Yang. 2010.
Exploiting Multi-Features to Detect Hedges and Their
Scope in Biomedical Texts. CoNLL-2010: Shared
Task, page 106.
287
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 81?82,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Exploring Surface-level Heuristics for Negation and Speculation
Discovery in Clinical Texts
Emilia Apostolova
DePaul University
Chicago, IL USA
emilia.aposto@gmail.com
Noriko Tomuro
DePaul University
Chicago, IL USA
tomuro@cs.depaul.edu
Abstract
We investigate the automatic identification
of negated and speculative statements in
biomedical texts, focusing on the clinical
domain. Our goal is to evaluate the perfor-
mance of simple, Regex-based algorithms
that have the advantage of low compu-
tational cost, simple implementation, and
do not rely on the accurate computation
of deep linguistic features of idiosyncratic
clinical texts. The performance of the
NegEx algorithm with an additional set of
Regex-based rules reveals promising re-
sults (evaluated on the BioScope corpus).
Current and future work focuses on a boot-
strapping algorithm for the discovery of
new rules from unannotated clinical texts.
1 Motivation
Finding negated and speculative (hedging) state-
ments is an important subtask for biomedical In-
formation Extraction (IE) systems. The task of
hedge detection is of particular importance in the
sub-genre of clinical texts which tend to avoid un-
qualified negations or assertions.
Negation/Speculation discovery is typically
broken down into two subtasks - discovering the
negation/speculation cue (a phrase or a syntactic
pattern) and establishing its scope. While a num-
ber of cue and scope discovery algorithms have
been developed, high performing systems typi-
cally rely on machine learning and more involved
feature creation. Deep linguistic feature creation
could pose problems, as the idiosyncrasies of clin-
ical texts often confuse off-the-shelf NLP feature
generation tools (e.g. relying on proper punctu-
ation and grammaticality). In addition, computa-
tionally expensive algorithms could pose problems
for high-volume IE systems.
In contrast, simple Regex-based algorithms
have demonstrated larger practical significance as
they offer reasonable performance at a low devel-
opment and computational cost. NegEx1 (Chap-
man et al, 2001), a simple rule-based algorithm
developed for the discovery of negation of findings
and diseases in discharge summaries, has been im-
plemented in a number of BioNLP systems, in-
cluding Metamap2, CaTIES3, and Mayo Clinic?s
Clinical IE System (Savova et al, 2008). In
NegEx, a list of phrases split into subsets are used
to identify cues and their corresponding scopes
(token widows preceding or following the cues).
2 Method
Negation/Speculation in general English could be
expressed by almost any combination of mor-
phologic, syntactic, semantic, and discourse-level
means. However, the scientific ?dryness? of the
biomedical genre and clinical texts in particular,
limits language variability and simplifies the task.
We evaluated the performance of the NegEx al-
gorithm on the BioScope corpus (Szarvas et al,
2008). BioScope corpus statistics are shown in Ta-
bles 1 and 2.
Corpus Type Sentences Documents Mean Document Size
Radiology Reports 7520 1954 3.85
Biological Full Papers 3352 9 372.44
Biological Paper Abstracts 14565 1273 11.44
Table 1: Statistics of the BioScope corpus. Document sizes
represent number of sentences.
Corpus Type Negation Cues Speculation Cues Negation Speculation
Rad Reports 872 1137 6.6% 13.4%
Full Papers 378 682 13.76% 22.29%
Paper Abstracts 1757 2694 13.45% 17.69%
Table 2: The percentage of speculative sentences (last col-
umn) is larger than the percentage of negated sentences.
We first evaluated the performance of an un-
modified version of the NegEx algorithm on the
task of cue detection (Table 3). Without any tuning
or modifications, NegEx performed well on identi-
fying negation cues across all documents, achiev-
1
http://code.google.com/p/negex/
2
c?The National Library of Medicine
3
http://caties.cabig.upmc.edu/Wiki.jsp?page=Home
81
ing an F-score of 90% on the clinical texts. For
the task of identifying speculation cues, we sim-
ply used the NegEx Conditional Possibility Phrase
list (35 speculative cue phrases). The overall per-
formance of this simplistic approach revealed poor
results.
TP FP FN Precision Recall F-score
Negation
Rad Reports 836 131 36 86.45 95.87 90.92
Full Papers 307 74 71 80.58 81.22 80.9
Paper Abstracts 1390 211 367 86.82 79.11 82.79
Speculation
Rad Reports 62 1 1075 98.41 5.45 10.33
Full Papers 1 0 681 100.0 0.15 0.3
Paper Abstracts 0 5 2694 0.0 0.0 0
Table 3: NegEx performance on identifying Negation and
Speculation Cues (non-exact boundary). (TP=true positive,
FP=false positive, FN=false negative)
As shown in Figure 1, speculation cues ex-
hibit wider variability and a rule matching only
35 phrases proved inefficient. To enrich the list of
speculation cues, we used hedging cues from the
FlySlip corpus of speculative sentences4. Without
any synonym expansion or fine-tuning, the per-
formance of speculation cue detection improved
significantly as shown in Table 4, achieving an F-
score of 86% on the clinical dataset5.
Figure 1: The number of occurrences (Y axis) of the 228
unique speculation cues and the 45 unique negation cues of
the BioScope corpus (X axis).
Corpus TP FP FN Precision Recall F-score
Rad Reports 903 52 234 94.55 79.42 86.33
Full Papers 439 553 243 44.25 64.37 52.45
Paper Abstracts 1741 1811 953 49.01 64.63 55.75
Table 4: NegEx performance on identifying speculation
cues (non-exact boundary) with the addition of the FlySlip
hedging cues.
We next measured the performance of NegEx
on scope detection. Newly introduced speculation
cues from the FlySlip corpus were automatically
classified into preceding or following their scope
based the position of of their annotated ?topic?. Ta-
ble 5 shows the results of scope identification.
3 Discussion
Our results show that a simple, surface-level algo-
rithm could be sufficient for the task of negation
4
http://www.wiki.cl.cam.ac.uk/rowiki/NaturalLanguage/FlySlip/Flyslip-resources
5
To avoid fine-tuning cues on the corpus we did not set aside a training subset of
the BioScope corpus for speculation cue enhancements and instead used an independent
hedging corpus (FlySlip).
TP FP FN Precision Recall F-score
Negation
Rad Reports 4003 267 140 94.12 97.61 95.18
Full Papers 2129 1835 525 54.45 80.12 64.01
Paper Abstracts 10049 6023 1728 63.04 85.13 72.31
Speculation
Rad Reports 2817 1459 2471 65.87 53.27 58.90
Full Papers 3313 2372 2958 58.27 52.83 55.41
Paper Abstracts 17219 6329 9477 73.12 64.50 68.54
Table 5: NegEx performance on identifying scopes of cor-
rectly identified cues. Precision and recall are computed
based on the number of correctly identified scope tokens
excluding punctuation (i.e. number of tokens within cue
scopes). Best results were achieved with no scope window
size (i.e. using sentence boundaries).
and hedge detection in clinical texts. Using the
NegEx algorithm and the FlySlip hedging corpus,
without any modifications or additions, we were
able to achieve an impressive F-score of 90.92%
and 86.33% for negation and speculation cue dis-
covery respectively6. We are currently expand-
ing the set of speculation cues using an unan-
notated dataset of clinical texts and a bootstrap-
ping algorithm (Medlock, 2008). The algorithm
is based on the intuition that speculative cues tend
to co-occur and this redundancy could be explored
to probabilistically discover new cues from high-
confidence existing ones. We are also exploring
the discovery of degree of speculativeness (e.g.
very unlikely vs very likely).
While NegEx performed well on the task of
identifying negation scope (F-score 95.18), further
work is needed on the discovery of speculation
scopes (F-score 58.90). As hedging cues require
a more fine-tuned set of rules, in future work we
will evaluate linguistically motivated approaches
(Kilicoglu and Bergler, 2008) for the creation of a
set of surface-level speculation scope rules.
References
W.W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper, and B.G. Buchanan.
2001. A simple algorithm for identifying negated findings and diseases in
discharge summaries. Journal of biomedical informatics, 34(5):301?310.
H. Kilicoglu and S. Bergler. 2008. Recognizing speculative language in
biomedical research articles: a linguistically motivated perspective. BMC
bioinformatics, 9(Suppl 11):S10.
B. Medlock. 2008. Exploring hedge identification in biomedical literature.
Journal of Biomedical Informatics, 41(4):636?654.
G.K. Savova, K. Kipper-Schuler, J.D. Buntrock, and C.G. Chute. 2008.
UIMA-based Clinical Information Extraction System. In Proc. UIMA for
NLP Workshop. LREC.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008. The BioScope corpus:
annotation for negation, uncertainty and their scope in biomedical texts.
In Proceedings of the Workshop on Current Trends in Biomedical Natural
Language Processing, pages 38?45. Association for Computational Lin-
guistics.
6
The enhanced speculation cue phrase lists and a UIMA-based NegEx implementation
are available upon request.
82
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 118?121,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Domain Adaptation of Coreference Resolution for Radiology Reports
Emilia Apostolova, Noriko Tomuro, Pattanasak Mongkolwat*, Dina Demner-Fushman?
College of Computing and Digital Media, DePaul University, Chicago, IL
*Department of Radiology, Northwestern University Medical School, Chicago, IL
?Communications Engineering Branch, National Library of Medicine, Bethesda, MD
emilia.aposto@gmail.com, tomuro@cs.depaul.edu,
p-mongkolwat@northwestern.edu, ddemner@mail.nih.gov
Abstract
In this paper we explore the applicability of
existing coreference resolution systems to a
biomedical genre: radiology reports. Analysis
revealed that, due to the idiosyncrasies of the
domain, both the formulation of the problem
of coreference resolution and its solution need
significant domain adaptation work. We refor-
mulated the task and developed an unsuper-
vised algorithm based on heuristics for coref-
erence resolution in radiology reports. The
algorithm is shown to perform well on a test
dataset of 150 manually annotated radiology
reports.
1 Introduction
Coreference resolution is the process of determin-
ing whether two expressions in natural language re-
fer to the same entity in the world. General purpose
coreference resolution systems typically cluster all
mentions (usually noun phrases) in a document into
coreference chains according to the underlying ref-
erence entity. A number of coreference resolution
algorithms have been developed for general texts. To
name a few, Soon et al (2001) employed machine
learning on the task and achieved an F-score of 62.6
and 60.4 on the MUC-6 (1995) and MUC-7 (1997)
coreference corpora respectively. Ng et al (2002)
improved this learning framework and achieved F-
scores of 70.4 and 63.4 respectively on the same
datasets.
There are also a number of freely available off-
the-shelf coreference resolution modules developed
for the general domain. For example, BART (Vers-
ley et al, 2008) is an open source coreference reso-
lution system which provides an implementation of
the Soon et al algorithm (2001). The Stanford De-
terministic Coreference Resolution System (Raghu-
nathan et al, 2010) uses an unsupervised sieve-like
approach to coreference resolution. Similarly, the
GATE Information Extraction system (Cunningham
et al, 2002) includes a rule-based coreference reso-
lution module consisting of orthography-based pat-
terns and a pronominal coreferencer (matching pro-
nouns to the most recent referent).
While coreference resolution is a universal dis-
course problem, both the scope of the problem and
its solution could vary significantly across domains
and text genres. Newswire coreference resolution
corpora (such as the MUC corpus) and general pur-
pose tools do not always fit the needs of specific do-
mains such as the biomedical domain well.
The importance and distinctive characteristics of
coreference resolution for biomedical articles has
been recognized, for example (Castano et al, 2002;
Gasperin, 2006; Gasperin et al, 2007; Su et al,
2008). Within the biomedical field, clinical texts
have been noted as a genre that needs specialized
coreference corpora and methodologies (Zheng et
al., 2011). The importance of the task for the clini-
cal domain has been attested by the 2011 i2b2 NLP
shared task (Informatics for Integrating Biology and
the Bedside1) which provided an evaluation plat-
form for coreference resolution for clinical texts.
However, even within the clinical domain, coref-
erence in different sub-genres could vary signifi-
1https://www.i2b2.org/NLP/Coreference/
118
cantly. In this paper we demonstrate the idiosyn-
crasies of the task of coreference resolution in a
clinical domain sub-genre, radiology reports, and
describe an unsupervised system developed for the
task.
2 Coreference Resolution for Radiology
Reports
Radiology reports have some unique characteristics
that preclude the use of coreference resolution mod-
ules or algorithms developed for the general biomed-
ical domain or even for other types of clinical texts.
The radiology report is a clinical text used to com-
municate medical image findings and observations
to referring physicians. Typically, radiology reports
are produced by radiologists after examining medi-
cal images and are used to describe the findings and
observations present in the accompanied images.
The radiology report accompanies an imaging
study and frequently refers to artifacts present in
the image. In radiology reports, artifacts present
in the image exhibit discourse salience, and as a
result are often introduced with definite pronouns
and articles. For example, consider the sentence
The pericardial space is clear. The definite noun
phrase the pericardial space does not represent an
anaphoric (or cataphoric) discourse entity and has
no antecedent. In contrast, coreference resolution
in general texts typically considers definite noun
phrases to be anaphoric discourse entities and at-
tempts to find their antecedents.
Another important distinction between general
purpose coreference resolution and the coreference
resolution module needed by an NLP system for
clinical texts is the scope of the task. General pur-
pose coreference resolution systems typically cluster
all mentions in a document into coreference chains.
Such comprehensive mention clustering is often not
necessary for the purposes of clinical text NLP sys-
tems. Biomedical Information Extraction systems
typically first identify named entities (medical con-
cepts) and map them to unambiguous biomedical
standard vocabularies (e.g. UMLS2 or RadLex3 in
the radiological domain). While multiple mentions
of the same named entity could exist in a document,
2http://www.nlm.nih.gov/research/umls/
3http://www.radlex.org/
in most cases these mentions were previously as-
signed to the same medical concept. For example,
multiple report mentions of ?the heart? or ?the lung?
will normally be mapped to the same medical con-
cept and clustering of these mentions into corefer-
ence chains is typically not needed.
3 Task Definition
Analysis revealed that the coreference resolution
task could be simplified and still meet the needs of
most Information Extraction tasks relevant to the ra-
diological domain. Due to their nature, texts de-
scribing medical image finding and observations do
not contain most pronominal references typically
targeted by coreference resolution systems. For ex-
ample, no occurrence of personal pronouns (e.g. he,
I), possessive pronouns (e.g. his, my), and indefi-
nite pronouns (e.g. anyone, nobody) was found in
the validation dataset. Demonstrative pronouns and
non-pleonastic ?it? mentions were the only pronom-
inal references observed in the dataset4. The fol-
lowing examples demonstrate the use of demonstra-
tive pronouns and the non-pleonastic ?it? pronoun
(shown in bold):
There is prominent soft tissue swelling involving
the premaxillary tissues. This measures approxi-
mately 15 mm in thickness and extends to the infe-
rior aspect of the nose.
There is a foreign object in the proximal left main-
stem bronchus on series 11 image 17 that was not
present on the prior study. It has a somewhat ovoid
to linear configuration.
Following these observations, the coreference res-
olution task has been simplified as follows. Corefer-
ence chains are assigned only for demonstrative pro-
nouns and ?it? noun phrases. The coreference reso-
lution task then involves selecting for each mention
a single best antecedent among previously annotated
named entities (medical concepts) or the NULL an-
tecedent.
4 Dataset
A total of 300 radiology reports were set aside for
validation and testing purposes. The dataset consists
4Pleonastic ?it? refers to its use as a ?dummy? pronoun, e.g.
It is raining, while non-pleonastic use of the pronoun refers to
a specific entity.
119
Figure 1: A sample DICOM image from an imaging
study described by the following radiology report snip-
pet: . . . FINDINGS: Targeted sonography of the upper in-
ner left breast was performed. At the site of palpable ab-
normality, at the 11 o?clock position 3 cm from the nipple,
there is an oval circumscribed, benign-appearing hypoe-
choic mass measuring 2.0 x 1.6 x 1.4 cm. There is mild
internal blood flow. It is surrounded by normal appearing
glandular breast tissue.. . .
of 100 Computed Tomography Chest reports, 100
Ultrasound Breast reports, and 100 Magnetic Res-
onance Brain reports, all randomly selected based
on their report types from a dataset of more than
100,000 de-identified reports spanning a period of
9 years5. These three types of reports represent
a diverse dataset covering representative imaging
modalities and body regions. Figure 1 shows a sam-
ple Breast Ultrasound DICOM6 image and its asso-
ciated radiology report.
The reports were previously tagged (using an au-
tomated system) with medical concepts and their
semantic types (e.g. anatomical entity, disorder,
imaging observation, etc.). Half of the dataset (150
reports) was manually annotated with coreference
chains using the simplified task definition described
above. The other half of the dataset was used for
validation of the system described next.
5 Method and Results
The coreference resolution task involves selecting
for each mention a single best antecedent among
previously annotated named entities or the NULL
antecedent. Mentions are demonstrative pronoun
phrases or definite noun phrases containing previ-
ously annotated named entities.
5The collection is a proprietary dataset belonging to North-
western University Medical School.
6Digital Imaging and Communications in Medicine, c? The
National Electrical Manufacturers Association.
We implemented an algorithm for the task de-
scribed above which was inspired by the work of
Haghighi and Klein (2009). The algorithm first iden-
tifies mentions within each report and orders them
linearly according to the position of the mention
head. Then it selects the antecedent (or the NULL
antecedent) for each mention as follows:
1. The possible antecedent candidates are first fil-
tered based on a distance constraint. Only mentions
of interest belonging to the preceding two sentences
are considered. The rationale for this filtering step is
that radiology reports are typically very concise and
less cohesive than general texts. Paragraphs often
describe multiple observations and anatomical enti-
ties sequentially and rarely refer to mentions more
distant than the preceding two sentences.
2. The remaining antecedent candidates are then
filtered based on a syntactic constraint: the co-
referent mentions must agree in number (singular or
plural based on the noun phrase head).
3. The remaining antecedent candidates are then
filtered based on a semantic constraint. If the two
mentions refer to named entities, the named entities
need to have the same semantic category7.
4. After filtering, the closest mention from the set
of remaining possible antecedents is selected. If the
set is empty, the NULL antecedent is selected.
Pairwise coreference decisions are considered
transitive and antecedent matches are propagated
transitively to all paired co-referents.
The algorithm was evaluated on the manually an-
notated test dataset. Results (Table 1) were com-
puted using the pairwise F1-score measure: preci-
sion, recall, and F1-score were computed over all
pairs of mentions in the same coreference cluster.
Precision Recall F1-score
74.90 48.22 58.66
Table 1: Pairwise coreference resolution results.
The system performance is within the range of
state-of-the-art supervised and unsupervised coref-
erence resolution systems8. F1-scores could range
7The same semantic type in the case of UMLS concepts or
the same parent in the case of RadLex concepts.
8Source code for the described system will be made avail-
able upon request.
120
between 39.8 and 67.3 for various methods and
test sets (Haghighi and Klein, 2009). The simpli-
fication of the coreference resolution problem de-
scribed above allowed us to focus only on corefer-
ence chains of interest to clinical text Information
Extraction tasks and positively influenced the out-
come. In addition, our goal was to focus on high
precision results as opposed to optimizing the over-
all F1-score. This guarantees that coreference reso-
lution errors will result in mostly omissions of coref-
erence pairs and will not introduce information ex-
traction inaccuracies.
6 Conclusion
In this paper, we presented some of the challenges
involved in the task of adapting coreference resolu-
tion for the domain of clinical radiology. We pre-
sented a domain-specific definition of the corefer-
ence resolution task. The task was reformulated and
simplified in a practical manner that ensures that the
needs of biomedical information extraction systems
are still met. We developed an unsupervised ap-
proach to the task of coreference resolution of radi-
ology reports and demonstrate state-of-the-art preci-
sion and reasonable recall results. The developed
system is made publicly available to the NLP re-
search community.
References
J. Castano, J. Zhang, and J. Pustejovsky. 2002. Anaphora
resolution in biomedical literature. In International
Symposium on Reference Resolution. Citeseer.
D.H. Cunningham, D.D. Maynard, D.K. Bontcheva, and
M.V. Tablan. 2002. GATE: A Framework and Graph-
ical Development Environment for Robust NLP Tools
and Applications.
C. Gasperin, N. Karamanis, and R. Seal. 2007. Annota-
tion of anaphoric relations in biomedical full-text arti-
cles using a domain-relevant scheme. In Proceedings
of DAARC, volume 2007. Citeseer.
C. Gasperin. 2006. Semi-supervised anaphora resolution
in biomedical texts. In Proceedings of the Workshop
on Linking Natural Language Processing and Biology:
Towards Deeper Biological Literature Analysis, pages
96?103. Association for Computational Linguistics.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1152?1161. Association for Compu-
tational Linguistics.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104?111.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010. A
multi-pass sieve for coreference resolution. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 492?501.
Association for Computational Linguistics.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
J. Su, X. Yang, H. Hong, Y. Tateisi, J. Tsujii, M. Ash-
burner, U. Leser, and D. Rebholz-Schuhmann. 2008.
Coreference resolution in biomedical texts: a machine
learning approach. Ontologies and Text Mining for
Life Sciences 08.
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman,
A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
Bart: A modular toolkit for coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies: Demo Session, pages 9?12. As-
sociation for Computational Linguistics.
J. Zheng, W.W. Chapman, R.S. Crowley, and G.K.
Savova. 2011. Coreference resolution: A review of
general methodologies and applications in the clinical
domain. Journal of biomedical informatics.
121
