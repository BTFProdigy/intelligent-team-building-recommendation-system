Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 841?848
Manchester, August 2008
Modeling Latent-Dynamic in Shallow Parsing:
A Latent Conditional Model with Improved Inference
Xu Sun? Louis-Philippe Morency? Daisuke Okanohara? Jun?ichi Tsujii??
?Department of Computer Science, The University of Tokyo, Hongo 7-3-1, Tokyo, Japan
?USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, USA
?School of Computer Science, The University of Manchester, 131 Princess St, Manchester, UK
?{sunxu, hillbig, tsujii}@is.s.u-tokyo.ac.jp ?morency@ict.usc.edu
Abstract
Shallow parsing is one of many NLP tasks
that can be reduced to a sequence la-
beling problem. In this paper we show
that the latent-dynamics (i.e., hidden sub-
structure of shallow phrases) constitutes a
problem in shallow parsing, and we show
that modeling this intermediate structure
is useful. By analyzing the automatically
learned hidden states, we show how the
latent conditional model explicitly learn
latent-dynamics. We propose in this paper
the Best Label Path (BLP) inference algo-
rithm, which is able to produce the most
probable label sequence on latent condi-
tional models. It outperforms two existing
inference algorithms. With the BLP infer-
ence, the LDCRF model significantly out-
performs CRF models on word features,
and achieves comparable performance of
the most successful shallow parsers on the
CoNLL data when further using part-of-
speech features.
1 Introduction
Shallow parsing identifies the non-recursive cores
of various phrase types in text. The paradigmatic
shallow parsing problem is noun phrase chunking,
in which the non-recursive cores of noun phrases,
called base NPs, are identified. As the represen-
tative problem in shallow parsing, noun phrase
chunking has received much attention, with the de-
velopment of standard evaluation datasets and with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
extensive comparisons among methods (McDon-
ald 2005; Sha & Pereira 2003; Kudo & Matsumoto
2001).
Syntactic contexts often have a complex under-
lying structure. Chunk labels are usually far too
general to fully encapsulate the syntactic behavior
of word sequences. In practice, and given the lim-
ited data, the relationship between specific words
and their syntactic contexts may be best modeled
at a level finer than chunk tags but coarser than
lexical identities. For example, in the noun phrase
(NP) chunking task, suppose that there are two lex-
ical sequences, ?He is her ?? and ?He gave her
? ?. The observed sequences, ?He is her? and
?He gave her?, would both be conventionally la-
beled by ?BOB?, where B signifies the ?beginning
NP?, and O the ?outside NP?. However, this label-
ing may be too general to encapsulate their respec-
tive syntactic dynamics. In actuality, they have dif-
ferent latent-structures, crucial in labeling the next
word. For ?He is her ??, the NP started by ?her? is
still incomplete, so the label for ? is likely to be I,
which conveys the continuation of the phrase, e.g.,
?[He] is [her brother]?. In contrast, for ?He gave
her ??, the phrase started by ?her? is normally self-
complete, and makes the next label more likely to
be B, e.g., ?[He] gave [her] [flowers]?.
In other words, latent-dynamics is an interme-
diate representation between input features and la-
bels, and explicitly modeling this can simplify the
problem. In particular, in many real-world cases,
when the part-of-speech tags are not available, the
modeling on latent-dynamics would be particu-
larly important.
In this paper, we model latent-dynamics in
shallow parsing by extending the Latent-Dynamic
Conditional Random Fields (LDCRFs) (Morency
et al 2007), which offer advantages over previ-
841
y y y1 2 m h h h1 2 m
y y y1 2 m
CRF LDCRF
x x x1 2 mx x x1 2 m
Figure 1: Comparison between CRF and LDCRF.
In these graphical models, x represents the obser-
vation sequence, y represents labels and h repre-
sents hidden states assigned to labels. Note that
only gray circles are observed variables. Also,
only the links with the current observation are
shown, but for both models, long range dependen-
cies are possible.
ous learning methods by explicitly modeling hid-
den state variables (see Figure 1). We expect LD-
CRFs to be particularly useful in those cases with-
out POS tags, though this paper is not limited to
this.
The inference technique is one of the most im-
portant components for a structured classification
model. In conventional models like CRFs, the op-
timal label path can be directly searched by using
dynamic programming. However, for latent condi-
tional models like LDCRFs, the inference is kind
of tricky, because of hidden state variables. In this
paper, we propose an exact inference algorithm,
the Best Label Path inference, to efficiently pro-
duce the optimal label sequence on LDCRFs.
The following section describes the related
work. We then review LDCRFs, and propose the
BLP inference. We further present a statistical
interpretation on learned hidden states. Finally,
we show that LDCRF-BLP is particularly effective
when pure word features are used, and when POS
tags are added, as existing systems did, it achieves
comparable results to the best reported systems.
2 Related Work
There is a wide range of related work on shallow
parsing. Shallow parsing is frequently reduced to
sequence labeling problems, and a large part of
previous work uses machine learning approaches.
Some approaches rely on k-order generative proba-
bilistic models of paired input sequences and label
sequences, such as HMMs (Freitag & McCallum
2000; Kupiec 1992) or multilevel Markov mod-
els (Bikel et al 1999). The generative model
provides well-understood training and inference
but requires stringent conditional independence as-
sumptions.
To accommodate multiple overlapping features
on observations, some other approaches view the
sequence labeling problem as a sequence of clas-
sification problems, including support vector ma-
chines (SVMs) (Kudo & Matsumoto 2001) and a
variety of other classifiers (Punyakanok & Roth
2001; Abney et al 1999; Ratnaparkhi 1996).
Since these classifiers cannot trade off decisions at
different positions against each other (Lafferty et
al. 2001), the best classifier based shallow parsers
are forced to resort to heuristic combinations of
multiple classifiers.
A significant amount of recent work has shown
the power of CRFs for sequence labeling tasks.
CRFs use an exponential distribution to model the
entire sequence, allowing for non-local dependen-
cies between states and observations (Lafferty et
al. 2001). Lafferty et al (2001) showed that CRFs
outperform classification models as well as HMMs
on synthetic data and on POS tagging tasks. As for
the task of shallow parsing, CRFs also outperform
many other state-of-the-art models (Sha & Pereira
2003; McDonald et al 2005).
When the data has distinct sub-structures, mod-
els that exploit hidden state variables are advanta-
geous in learning (Matsuzaki et al 2005; Petrov
et al 2007). Sutton et al (2004) presented an
extension to CRF called dynamic conditional ran-
dom field (DCRF) model. As stated by the authors,
training a DCRF model with unobserved nodes
(hidden variables) makes their approach difficult
to optimize. In the vision community, the LD-
CRF model was recently proposed by Morency et
al. (2007), and shown to outperform CRFs, SVMs,
and HMMs for visual sequence labeling.
In this paper, we introduce the concept of latent-
dynamics for shallow parsing, showing how hid-
den states automatically learned by the model
present similar characteristics. We will also pro-
pose an improved inference technique, the BLP,
for producing the most probable label sequence in
LDCRFs.
3 Latent-Dynamic Conditional Random
Fields
The task is to learn a mapping between a sequence
of observations x = x
1
, x
2
, . . . , x
m
and a sequence
of labels y = y
1
, y
2
, . . . , y
m
. Each y
j
is a class la-
842
bel for the j?th token of a word sequence and is a
member of a set Y of possible class labels. For
each sequence, the model also assumes a vector of
hidden state variables h = {h
1
, h
2
, . . . , h
m
}, which
are not observable in the training examples.
Given the above definitions, we define a latent
conditional model as follows:
P(y|x,?) =
?
h
P(y|h, x,?)P(h|x,?), (1)
where ? are the parameters of the model. The LD-
CRF model can seem as a natural extension of the
CRF model, and the CRF model can seem as a spe-
cial case of LDCRFs employing one hidden state
for each label.
To keep training and inference efficient, we re-
strict the model to have disjointed sets of hidden
states associated with each class label. Each h
j
is
a member of a set H
y
j
of possible hidden states for
the class label y
j
. We define H, the set of all pos-
sible hidden states to be the union of all H
y
j
sets.
Since sequences which have any h
j
< H
y
j
will by
definition have P(y|x,?) = 0, we can express our
model as:
P(y|x,?) =
?
h?H
y
1
?...?H
y
m
P(h|x,?), (2)
where P(h|x,?) is defined using the usual con-
ditional random field formulation: P(h|x,?) =
exp ??f(h|x)/
?
?h
exp ??f(h|x), in which f(h|x) is
the feature vector. Given a training set consisting
of n labeled sequences (x
i
, y
i
) for i = 1 . . . n, train-
ing is performed by optimizing the objective func-
tion to learn the parameter ?
?
:
L(?) =
n
?
i=1
log P(y
i
|x
i
,?) ? R(?). (3)
The first term of this equation is the conditional
log-likelihood of the training data. The second
term is the regularizer.
4 BLP Inference on Latent Conditional
Models
For testing, given a new test sequence x, we want
to estimate the most probable label sequence (Best
Label Path), y
?
, that maximizes our conditional
model:
y
?
= argmax
y
P(y|x,?
?
). (4)
In the CRF model, y
?
can be simply searched by
using the Viterbi algorithm. However, for latent
conditional models like LDCRF, the Best Label
Path y
?
cannot directly be produced by the Viterbi
algorithm because of the incorporation of hidden
states.
In this paper, we propose an exact inference al-
gorithm, the Best Label Path inference (BLP), for
producing the most probable label sequence y
?
on
LDCRF. In the BLP schema, top-n hidden paths
HP
n
= {h
1
,h
2
. . . h
n
} over hidden states are effi-
ciently produced by using A
?
search (Hart et al,
1968), and the corresponding probabilities of hid-
den paths P(h
i
|x,?) are gained. Thereafter, based
on HP
n
, the estimated probabilities of various la-
bel paths, P(y|x,?), can be computed by summing
the probabilities of hidden paths, P(h|x,?), con-
cerning the association between hidden states and
each class label:
P(y|x,?) =
?
h: h?H
y
1
?...?H
y
m
?h?HP
n
P(h|x,?). (5)
By using the A
?
search, HP
n
can be extended in-
crementally in an efficient manner, until the algo-
rithm finds that the Best Label Path is ready, and
then the search stops and ends the BLP inference
with success. The algorithm judges that y
?
is ready
when the following condition is achieved:
P(y
1
|x,?) ? P(y
2
|x,?) +
?
h<H
y
1
?...?H
y
m
P(h|x,?), (6)
where y
1
is the most probable label sequence, and
y
2
is the second ranked label sequence estimated
by using HP
n
. It would be straightforward to prove
that y
?
= y
1
, and further search is unnecessary, be-
cause in this case, the unknown probability mass
can not change the optimal label path. The un-
known probability mass can be computed by using
?
h<H
y
1
?...?H
y
m
P(h|x,?) = 1 ?
?
h?H
y
1
?...?H
y
m
P(h|x,?). (7)
The top-n hidden paths of HP
n
produced by the
A
?
-search are exact, and the BLP inference is ex-
act. To guarantee HP
n
is exact in our BLP in-
ference, an admissible heuristic function should
be used in A
?
search (Hart et al, 1968). We use
a backward Viterbi algorithm (Viterbi, 1967) to
compute the heuristic function of the forward A
?
search:
Heu
i
(h
j
) = max
h
?
i
=h
j
?h
?
i
?HP
|h|
i
P
?
(h
?
|x,?
?
), (8)
843
where h
?
i
= h
j
represents a partial hidden path
started from the hidden state h
j
, and HP
|h|
i
rep-
resents all possible partial hidden paths from the
position i to the ending position |h| . Heu
i
(h
j
) is
an admissible heuristic function for the A
?
search
over hidden paths, therefore HP
n
is exact and BLP
inference is exact.
The BLP inference is efficient when the prob-
ability distribution among the hidden paths is in-
tensive. By combining the forward A
?
with the
backward Viterbi algorithm, the time complexity
of producing HP
n
is roughly a linear complexity
concerning its size. In practice, on the CoNLL test
data containing 2,012 sentences, the BLP infer-
ence finished in five minutes when using the fea-
ture set based on both word and POS information
(see Table 3). The memory consumption is also
relatively small, because it is an online style algo-
rithm and it is not necessary to preserve HP
n
.
In this paper, to make a comparison, we also
study the Best Hidden Path inference (BHP):
y
BHP
= argmax
y
P(h
y
|x,?
?
), (9)
where h
y
? H
y
1
? . . . ?H
y
m
. In other words, the
Best Hidden Path is the label sequence that is di-
rectly projected from the most probable hidden
path h
?
.
In (Morency et al 2007), y
?
is estimated by us-
ing the Best Point-wise Marginal Path (BMP). To
estimate the label y
j
of token j, the marginal prob-
abilities P(h
j
= a|x,?) are computed for possible
hidden states a ? H. Then, the marginal probabili-
ties are summed and the optimal label is estimated
by using the marginal probabilities.
The BLP produces y
?
while the BHP and the
BMP perform an estimation on y
?
. We will make
an experimental comparison in Section 6.
5 Analyzing Latent-Dynamics
The chunks in shallow parsing are represented with
the three labels shown in Table 1, and shallow pars-
ing is treated as a sequence labeling task with those
three labels. A challenge for most shallow parsing
approaches is to determine the concepts learned by
the model. In this section, we show how we can
analyze the latent-dynamics.
5.1 Analyzing Latent-Dynamics
In this section, we show how to analyze the charac-
teristics of the hidden states. Our goal is to find the
words characterizing a specific hidden state, and
B words beginning a chunk
I words continuing a chunk
O words being outside a chunk
Table 1: Shallow parsing labels.
then look at the selected words with their associ-
ated POS tags to determine if the LDCRF model
has learned meaningful latent-dynamics.
In the experiments reported in this section, we
did not use the features on POS tags in order to
isolate the model?s capability of learning latent dy-
namics. In other words, the model could simply
learn the dynamics of POS tags as the latent dy-
namics if the model is given the information about
POS tags. The features used in the experiments are
listed on the left side (Word Features) in Table 3.
The main idea is to look at the marginal proba-
bilities P(h
j
= a|x,?) for each word j, and select
the hidden state a
?
with the highest probability. By
counting how often a specific word selected a as
the optimal hidden state, i.e., ?(w, a), we can cre-
ate statistics about the relationship between hidden
states and words. We define relative frequency as
the number of times a specific word selected a hid-
den state while normalized by the global frequency
of this word:
RltFreq(w, h
j
) =
Freq( ?(w, h
j
) )
Freq(w)
. (10)
5.2 Learned Latent-Dynamics from CoNLL
In this subsection, we show the latent-dynamics
learned automatically from the CoNLL dataset.
The details of these experiments are presented in
the following section.
The most frequent three words corresponding to
the individual hidden states of the labels, B and O,
are shown in Table 2. As shown, the automati-
cally learned hidden states demonstrate prominent
characteristics. The extrinsic label B, which begins
a noun phrase, is automatically split into 4 sub-
categories: wh-determiners (WDT, such as ?that?)
together with wh-pronouns (WP, such as ?who?),
the determiners (DT, such as ?any, an, a?), the per-
sonal pronouns (PRP, such as ?they, we, he?), and
the singular proper nouns (NNP, such as ?Nasdaq,
Florida?) together with the plural nouns (NNS,
such as ?cities?). The results of B1 suggests that
the wh-determiners represented by ?that?, and the
wh-pronouns represented by ?who?, perform simi-
844
Labels HidStat Words POS RltFreq
B
That WDT 0.85
B1 who WP 0.49
Who WP 0.33
any DT 1.00
B2 an DT 1.00
a DT 0.98
They PRP 1.00
B3 we PRP 1.00
he PRP 1.00
Nasdaq NNP 1.00
B4 Florida NNP 0.99
cities NNS 0.99
O
But CC 0.88
O1 by IN 0.73
or IN 0.67
4.6 CD 1.00
O2 1 CD 1.00
11 CD 0.62
were VBD 0.94
O3 rose VBD 0.93
have VBP 0.92
been VBN 0.97
O4 be VB 0.94
to TO 0.92
Table 2: Latent-dynamics learned automatically by
the LDCRF model. This table shows the top three
words and their gold-standard POS tags for each
hidden states.
lar roles in modeling the dynamics in shallow pars-
ing. Further, the singular proper nouns and the
plural nouns are grouped together, suggesting that
they may perform similar roles. Moreover, we can
notice that B2 and B3 are highly consistent.
The label O is automatically split into the coordi-
nating conjunctions (CC) together with the prepo-
sitions (IN) indexed by O1, the cardinal numbers
(CD) indexed by O2, the past tense verbs (VBD)
together with the personal verbs (VBP) indexed by
O3, and another sub-category, O4. From the results
we can find that gold-standard POS tags may not
be adequate in modeling latent-dynamics in shal-
low parsing, as we can notice that three hidden
states out of four (O1, O3 and O4) contains relat-
ing but different gold-standard POS tags.
6 Experiments
Following previous studies on shallow parsing, our
experiments are performed on the CoNLL 2000
Word Features:
{w
i?2
, w
i?1
, w
i
, w
i+1
, w
i+2
, w
i?1
w
i
, w
i
w
i+1
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
POS Features:
{t
i?1
, t
i
, t
i+1
, t
i?2
t
i?1
, t
i?1
t
i
, t
i
t
i+1
, t
i+1
t
i+2
,
t
i?2
t
i?1
t
i
, t
i?1
t
i
t
i+1
, t
i
t
i+1
t
i+2
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
Table 3: Feature templates used in the experi-
ments. w
i
is the current word; t
i
is current POS
tag; and h
i
is the current hidden state (for the case
of latent models) or the current label (for the case
of conventional models).
data set (Sang & Buchholz 2000; Ramshow &
Marcus 1995). The training set consists of 8,936
sentences, and the test set consists of 2,012 sen-
tences. The standard evaluation metrics for this
task are precision p (the fraction of output chunks
matching the reference chunks), recall r (the frac-
tion of reference chunks returned), and the F-
measure given by F = 2pr/(p + r).
6.1 LDCRF for Shallow Parsing
We implemented LDCRFs in C++, and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions.
We employ similar predicate sets defined in Sha
& Pereira (2003). We follow them in using predi-
cates that depend on words as well as POS tags in
the neighborhood of a given position, taking into
account only those 417,835 features which occur
at least once in the training data. The features are
listed in Table 3.
As for numerical optimization (Malouf 2002;
Wallach 2002), we performed gradient decent with
the Limited-Memory BFGS (L-BFGS) optimiza-
tion technique (Nocedal & Wright 1999). L-BFGS
is a second-order Quasi-Newton method that nu-
merically estimates the curvature from previous
gradients and updates. With no requirement on
specialized Hessian approximation, L-BFGS can
handle large-scale problems in an efficient manner.
We implemented an L-BFGS optimizer in C++ by
modifying the OWLQN package (Andrew & Gao
2007) developed by Galen Andrew. In our exper-
iments, storing 10 pairs of previous gradients for
the approximation of the function?s inverse Hes-
sian worked well, making the amount of the ex-
tra memory required modest. Using more pre-
vious gradients will probably decrease the num-
845
ber of iterations required to reach convergence,
but would increase memory requirements signifi-
cantly. To make a comparison, we also employed
the Conjugate-Gradient (CG) optimization algo-
rithm. For details of CG, see Shewchuk (1994).
Since the objective function of the LDCRF
model is non-convex, it is suggested to use the ran-
dom initialization of parameters for the training.
To reduce overfitting, we employed an L
2
Gaus-
sian weight prior (Chen & Rosenfeld 1999). Dur-
ing training and validation, we varied the number
of hidden states per label (from 2 to 6 states per
label), and also varied the L
2
-regularization term
(with values 10
k
, k from -3 to 3). Our experiments
suggested that using 4 or 5 hidden states per label
for the shallow parser is a viable compromise be-
tween accuracy and efficiency.
7 Results and Discussion
7.1 Performance on Word Features
As discussed in Section 4, it is preferred to not
use the features on POS tags in order to isolate
the model?s capability of learning latent dynam-
ics. In this sub-section, we use pure word fea-
tures with their counts above 10 in the training data
to perform experimental comparisons among dif-
ferent inference algorithms on LDCRFs, including
BLP, BHP, and existing BMP.
Since the CRF model is one of the success-
ful models in sequential labeling tasks (Lafferty et
al. 2001; Sha & Pereira 2003; McDonald et al
2005), in this section, we also compare LDCRFs
with CRFs. We tried to make experimental results
more comparable between LDCRF and CRF mod-
els, and have therefore employed the same fea-
tures set, optimizer and fine-tuning strategy be-
tween LDCRF and CRF models.
The experimental results are shown in Table 4.
In the table, Acc. signifies ?label accuracy?, which
is useful for the significance test in the follow-
ing sub-section. As shown, LDCRF-BLP outper-
forms LDCRF-BHP and LDCRF-BMP, suggesting
that BLP inference
1
is superior. The superiority
of BLP is statistically significant, which will be
shown in next sub-section. On the other side, all
the LDCRF models outperform the CRF model. In
particular, the gap between LDCRF-BLP and CRF
is 1.53 percent.
1
In practice, for efficiency, we approximated the BLP on a
few sentences by limiting the number of search steps.
Models: WF Acc. Pre. Rec. F
1
LDCRF-BLP 97.01 90.33 88.91 89.61
LDCRF-BHP 96.52 90.26 88.21 89.22
LDCRF-BMP 97.26 89.83 89.06 89.44
CRF 96.11 88.12 88.03 88.08
Table 4: Experimental comparisons among differ-
ent inference algorithms on LDCRFs, and the per-
formance of CRFs using the same feature set on
pure word features. The BLP inference outper-
forms the BHP and BMP inference. LDCRFs out-
perform CRFs.
Models F
1
Gap Acc. Gap Sig.
BLP vs. BHP 0.39 0.49 1e-10
BLP vs. CRF 1.53 0.90 5e-13
Table 5: The significance tests. LDCRF-BLP is
significantly more accurate than LDCRF-BHP and
CRFs.
7.2 Labeling Accuracy and Significance Test
As shown in Table 4, the accuracy rate for individ-
ual labeling decisions is over-optimistic as a mea-
sure for shallow parsing. Nevertheless, since test-
ing the significance of shallow parsers? F-measures
is tricky, individual labeling accuracy provides a
more convenient basis for statistical significance
tests (Sha & Pereira 2003). One such test is the
McNemar test on paired observations (Gillick &
Cox 1989). As shown in Table 5, for the LD-
CRF model, the BLP inference schema is sta-
tistically more accurate than the BHP inference
schema. Also, Evaluations show that the McNe-
mar?s value on labeling disagreement between the
LDCRF-BLP and CRF models is 5e-13, suggest-
ing that LDCRF-BLP is significantly more accu-
rate than CRFs.
On the other hand, the accuracy rate of BMP in-
ference is a special case. Since the BMP inference
is essentially an accuracy-first inference schema,
the accuracy rate and the F-measure have a differ-
ent relation in BMP. As we can see, the individual
labeling accuracy achieved by the LDCRF-BMP
model is as high as 97.26%, but its F-measure is
still lower than LDCRF-BLP.
7.3 Convergence Speed
It would be interesting to compare the convergence
speed between the objective loss function of LD-
CRFs and CRFs. We apply the L-BFGS optimiza-
846
 150
 200
 250
 300
 350
 400
 450
 500
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
LDCRF
CRF
Figure 2: The value of the penalized loss based on
the number of iterations: LDCRFs vs. CRFs.
 160
 180
 200
 220
 240
 260
 280
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
L-BFGS
CG
Figure 3: Training the LDCRF model: L-BFGS
vs. CG.
tion algorithm to optimize the loss function of LD-
CRF and CRF models, making a comparison be-
tween them. We find that the iterations required
for the convergence of LDCRFs is less than for
CRFs (see Figure 2). Normally, the LDCRF model
arrives at the plateau of convergence in 120-150
iterations, while CRFs require 210-240 iterations.
When we replace the L-BFGS optimizer by the CG
optimization algorithm, we observed as well that
LDCRF converges faster on iteration numbers than
CRF does.
On the contrary, however, the time cost of the
LDCRF model in each iteration is higher than the
CRF model, because of the incorporation of hid-
den states. The time cost of the LDCRF model
in each iteration is roughly a quadratic increase
concerning the increase of the number of hidden
states. Therefore, though the LDCRF model re-
quires less passes for the convergence, it is practi-
cally slower than the CRF model. Improving the
scalability of the LDCRF model would be a inter-
esting topic in the future.
Furthermore, we make a comparison between
Models: WF+POS Pre. Rec. F
1
LDCRF-BLP 94.65 94.03 94.34
CRF
N/A N/A 93.6
(Vishwanathan et al 06)
CRF
94.57 94.00 94.29
(McDonald et al 05)
Voted perceptron
N/A N/A 93.53
(Collins 02)
Generalized Winnow
93.80 93.99 93.89
(Zhang et al 02)
SVM combination
94.15 94.29 94.22
(Kudo & Matsumoto 01)
Memo. classifier
93.63 92.89 93.26
(Sang 00)
Table 6: Performance of the LDCRF-BLP model,
and the comparison with CRFs and other success-
ful approaches. In this table, all the systems have
employed POS features.
the L-BFGS and the CG optimizer for LDCRFs.
We observe that the L-BFGS optimizer is slightly
faster than CG on LDCRFs (see Figure 3), which
echoes the comparison between the L-BFGS and
the CG optimizing technique on the CRF model
(Sha & Pereira 2003).
7.4 Comparisons to Other Systems with POS
Features
Performance of the LDCRF-BLP model and some
of the best results reported previously are summa-
rized in Table 6. Our LDCRF model achieved
comparable performance to those best reported
systems in terms of the F-measure.
McDonald et al (2005) achieved an F-measure
of 94.29% by using a CRF model. By employing a
multi-model combination approach, Kudo & Mat-
sumoto (2001) also achieved a good performance.
They use a combination of 8 kernel SVMs with
a heuristic voting strategy. An advantage of LD-
CRFs over max-margin based approaches is that
LDCRFs can output N-best label sequences and
their probabilities using efficient marginalization
operations, which can be used for other compo-
nents in an information extraction system.
8 Conclusions and Future Work
In this paper, we have shown that automatic model-
ing on ?latent-dynamics? can be useful in shallow
parsing. By analyzing the automatically learned
847
hidden states, we showed how LDCRFs can natu-
rally learn latent-dynamics in shallow parsing.
We proposed an improved inference algorithm,
the BLP, for LDCRFs. We performed experiments
using the CoNLL data, and showed how the BLP
inference outperforms existing inference engines.
When further employing POS features as other
systems did, the performance of the LDCRF-BLP
model is comparable to those best reported results.
The LDCRF model demonstrates a significant ad-
vantage over other models on pure word features
in this paper. We expect it to be particularly useful
in the real-world tasks without rich features.
The latent conditional model handles latent-
dynamics naturally, and can be easily extended to
other labeling tasks. Also, the BLP inference algo-
rithm can be extended to other latent conditional
models for producing optimal label sequences. As
a future work, we plan to further speed up the BLP
algorithm.
Acknowledgments
Many thanks to Yoshimasa Tsuruoka for helpful
discussions on the experiments and paper-writing.
This research was partially supported by Grant-
in-Aid for Specially Promoted Research 18002007
(MEXT, Japan). The work at the USC Institute for
Creative Technology was sponsored by the U.S.
Army Research, Development, and Engineering
Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
References
Abney, S. 1991. Parsing by chunks. In R. Berwick, S. Ab-
ney, and C. Tenny, editors, Principle-based Parsing. Kluwer
Academic Publishers.
Abney, S.; Schapire, R. E. and Singer, Y. 1999. Boosting
applied to tagging and PP attachment. In Proc. EMNLP/VLC-
99.
Andrew, G. and Gao, J. 2007. Scalable training of L1-
regularized log-linear models. In Proc. ICML-07.
Bikel, D. M.; Schwartz, R. L. and Weischedel, R. M. 1999.
An algorithm that learns what?s in a name. Machine Learning,
34: 211-231.
Chen, S. F. and Rosenfeld, R. 1999. A Gaussian prior
for smooth-ing maximum entropy models. Technical Report
CMU-CS-99-108, CMU.
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with perceptron
algorithms. In Proc. EMNLP-02.
Freitag, D. and McCallum, A. 2000. Information extrac-
tion with HMM structures learned by stochastic optimization.
In Proc. AAAI-00.
Gillick, L. and Cox, S. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Interna-
tional Conference on Acoustics Speech and Signal Process-
ing, v1, pages 532-535.
Hart, P.E.; Nilsson, N.J.; and Raphael, B. 1968. A formal
basis for the heuristic determination of minimum cost path.
IEEE Trans. On System Science and Cybernetics, SSC-4(2):
100-107.
Kudo, T. and Matsumoto, Y. 2001. Chunking with support
vector machines. In Proc. NAACL-01.
Kupiec, J. 1992. Robust part-of-speech tagging using a
hidden Markov model. Computer Speech and Language.
6:225-242.
Lafferty, J.; McCallum, A. and Pereira, F. 2001. Condi-
tional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML-01, pages 282-289.
Malouf, R. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proc. CoNLL-02.
Matsuzaki, T.; Miyao Y. and Tsujii, J. 2005. Probabilistic
CFG with Latent Annotations. In Proc. ACL-05.
McDonald, R.; Crammer, K. and Pereira, F. 2005. Flexible
Text Segmentation with Structured Multilabel Classification.
In Proc. HLT/EMNLP-05, pages 987- 994.
Morency, L.P.; Quattoni, A. and Darrell, T. 2007. Latent-
Dynamic Discriminative Models for Continuous Gesture
Recognition. In Proc. CVPR-07, pages 1- 8.
Nocedal, J. and Wright, S. J. 1999. Numerical Optimiza-
tion. Springer.
Petrov, S.; Pauls, A.; and Klein, D. 2007. Discriminative
log-linear grammars with latent variables. In Proc. NIPS-07.
Punyakanok, V. and Roth, D. 2001. The use of classifiers
in sequential inference. In Proc. NIPS-01, pages 995-1001.
MIT Press.
Ramshaw, L. A. and Marcus, M. P. 1995. Text chunking
using transformation-based learning. In Proc. Third Work-
shop on Very Large Corpora. In Proc. ACL-95.
Ratnaparkhi, A. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP-96.
Sang, E.F.T.K. 2000. Noun Phrase Representation by Sys-
tem Combination. In Proc. ANLP/NAACL-00.
Sang, E.F.T.K and Buchholz, S. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proc. CoNLL-00,
pages 127-132.
Sha, F. and Pereira, F. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proc. HLT/NAACL-03.
Shewchuk, J. R. 1994. An introduction to the
conjugate gradient method without the agonizing pain.
http://www.2.cs.cmu.edu/jrs/jrspapers.html/#cg.
Sutton, C.; Rohanimanesh, K. and McCallum, A. 2004.
Dynamic conditional random fields: Factorized probabilistic
models for labeling and segmenting sequence data. In Proc.
ICML-04.
Viterbi, A.J. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. IEEE
Transactions on Information Theory. 13(2):260-269.
Vishwanathan, S.; Schraudolph, N. N.; Schmidt, M.W. and
Murphy, K. 2006. Accelerated training of conditional random
fields with stochastic meta-descent. In Proc. ICML-06.
Wallach, H. 2002. Efficient training of conditional random
fields. In Proc. 6th Annual CLUK Research Colloquium.
Zhang, T.; Damerau, F. and Johnson, D. 2002. Text chunk-
ing based on a generalization of winnow. Journal of Machine
Learning Research, 2:615-637.
848
Assigning Polarity Scores to Reviews
Using Machine Learning Techniques
Daisuke Okanohara1 and Jun?ichi Tsujii1,2,3
1 Department of Computer Science, University of Tokyo,
Hongo, 7-3-1, Bunkyo-ku, Tokyo 113-0013
2 CREST, JST, Honcho, 4-1-8, Kawaguchi-shi, Saitama 332-0012
3 School of Informatics, University of Manchester,
POBox 88, Sackville St, Manchester, M60 1QD, UK
{hillbig, tsujii}@is.s.u-tokyo.ac.jp
Abstract. We propose a novel type of document classification task that
quantifies how much a given document (review) appreciates the target
object using not binary polarity (good or bad) but a continuous mea-
sure called sentiment polarity score (sp-score). An sp-score gives a very
concise summary of a review and provides more information than binary
classification. The difficulty of this task lies in the quantification of po-
larity. In this paper we use support vector regression (SVR) to tackle
the problem. Experiments on book reviews with five-point scales show
that SVR outperforms a multi-class classification method using support
vector machines and the results are close to human performance.
1 Introduction
In recent years, discussion groups, online shops, and blog systems on the Internet
have gained popularity and the number of documents, such as reviews, is growing
dramatically. Sentiment classification refers to classifying reviews not by their
topics but by the polarity of their sentiment (e.g, positive or negative). It is
useful for recommendation systems, fine-grained information retrieval systems,
and business applications that collect opinions about a commercial product.
Recently, sentiment classification has been actively studied and experimental
results have shown that machine learning approaches perform well [13,11,10,20].
We argue, however, that we can estimate the polarity of a review more finely. For
example, both reviews A and B in Table 1 would be classified simply as positive
in binary classification. Obviously, this classification loses the information about
the difference in the degree of polarity apparent in the review text.
We propose a novel type of document classification task where we evaluate
reviews with scores like five stars. We call this score the sentiment polarity score
(sp-score). If, for example, the range of the score is from one to five, we could
give five to review A and four to review B. This task, namely, ordered multi-class
classification, is considered as an extension of binary sentiment classification.
In this paper, we describe a machine learning method for this task. Our
system uses support vector regression (SVR) [21] to determine the sp-scores of
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 314?325, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Assigning Polarity Scores to Reviews 315
Table 1. Examples of book reviews
Example of Review binary sp-score
(1,...,5)
Review A I believe this is very good and a ?must read? plus 5
I can?t wait to read the next book in the series.
Review B This book is not so bad. plus 4
You may find some interesting points in the book.
Table 2. Corpus A: reviews for Harry Potter series book. Corpus B: reviews for all
kinds of books. The column of word shows the average number of words in a review,
and the column of sentences shows the average number of sentences in a review.
sp-score Corpus A Corpus B
review words sentences review words sentences
1 330 160.0 9.1 250 91.9 5.1
2 330 196.0 11.0 250 105.2 5.2
3 330 169.1 9.2 250 118.6 6.0
4 330 150.2 8.6 250 123.2 6.1
5 330 153.8 8.9 250 124.8 6.1
reviews. This method enables us to annotate sp-scores for arbitrary reviews such
as comments in bulletin board systems or blog systems. We explore several types
of features beyond a bag-of-words to capture key phrases to determine sp-scores:
n-grams and references (the words around the reviewed object).
We conducted experiments with book reviews from amazon.com each of
which had a five-point scale rating along with text. We compared pairwise sup-
port vector machines (pSVMs) and SVR and found that SVR outperformed
better than pSVMs by about 30% in terms of the squared error, which is close
to human performance.
2 Related Work
Recent studies on sentiment classification focused on machine learning ap-
proaches. Pang [13] represents a review as a feature vector and estimates the
polarity with SVM, which is almost the same method as those for topic classifi-
cation [1]. This paper basically follows this work, but we extend this task to a
multi-order classification task.
There have been many attempts to analyze reviews deeply to improve ac-
curacy. Mullen [10] used features from various information sources such as ref-
erences to the ?work? or ?artist?, which were annotated by hand, and showed
that these features have the potential to improve the accuracy. We use reference
features, which are the words around the fixed review target word (book), while
Mullen annotated the references by hand.
316 D. Okanohara and J. Tsujii
Turney [20] used semantic orientation, which measures the distance from
phrases to ?excellent? or ?poor? by using search engine results and gives the word
polarity. Kudo [8] developed decision stumps, which can capture substructures
embedded in text (such as word-based dependency), and suggested that subtree
features are important for opinion/modality classification.
Independently of and in parallel with our work, two other papers consider
the degree of polarity for sentiment classification. Koppel [6] exploited a neu-
tral class and applied a regression method as ours. Pang [12] applied a metric
labeling method for the task. Our work is different from their works in several
respects. We exploited square errors instead of precision for the evaluation and
used five distinct scores in our experiments while Koppel used three and Pang
used three/four distinct scores in their experiments.
3 Analyzing Reviews with Polarity Scores
In this section we present a novel task setting where we predict the degree of
sentiment polarity of a review. We first present the definition of sp-scores and
the task of assigning them to review documents. We then explain an evaluation
data set. Using this data set, we examined the human performance for this task
to clarify the difficulty of quantifying polarity.
3.1 Sentiment Polarity Scores
We extend the sentiment classification task to the more challenging task of as-
signing rating scores to reviews. We call this score the sp-score. Examples of
sp-scores include five-star and scores out of 100. Let sp-scores take discrete val-
ues1 in a closed interval [min...max]. The task is to assign correct sp-scores to
unseen reviews as accurately as possible. Let y? be the predicted sp-score and
y be the sp-score assigned by the reviewer. We measure the performance of an
estimator with the mean square error:
1
n
?n
i=1(y?i ? yi)2, (1)
where (x1, y1), ..., (xn, yn) is the test set of reviews. This measure gives a large
penalty for large mistakes, while ordered multi-class classification gives equal
penalties to any types of mistakes.
3.2 Evaluation Data
We used book reviews on amazon.com for evaluation data2 3. Each review has
stars assigned by the reviewer. The number of stars ranges from one to five:
1 We could allow sp-scores to have continuous values. However, in this paper we assume
sp-scores take only discrete values since the evaluation data set was annotated by
only discrete values.
2 http://www.amazon.com
3 These data were gathered from google cache using google API.
Assigning Polarity Scores to Reviews 317
one indicates the worst and five indicates the best. We converted the number
of stars into sp-scores {1, 2, 3, 4, 5} 4. Although each review may include several
paragraphs, we did not exploit paragraph information.
From these data, we made two data sets. The first was a set of reviews for
books in the Harry Potter series (Corpus A). The second was a set of reviews for
books of arbitrary kinds (Corpus B). It was easier to predict sp-scores for Corpus
A than Corpus B because Corpus A books have a smaller vocabulary and each
review was about twice as large. To create a data set with a uniform score distri-
bution (the effect of skewed class distributions is out of the scope of this paper),
we selected 330 reviews per sp-score for Corpus A and 280 reviews per sp-score
for Corpus B 5. Table 2 shows the number of words and sentences in the cor-
pora. There is no significant difference in the average number of words/sentences
among different sp-scores.
Table 3. Human performance of sp-score estimation. Test data: 100 reviews of Corpus
A with 1,2,3,4,5 sp-score.
Square error
Human 1 0.77
Human 2 0.79
Human average 0.78
cf. Random 3.20
All3 2.00
Table 4. Results of sp-score estimation: Human 1 (left) and Human 2 (right)
Assigned
1 2 3 4 5 Total
Correct
1 12 7 0 1 0 20
2 7 8 4 1 0 20
3 1 1 13 5 0 20
4 0 0 4 10 6 20
5 0 1 2 7 10 20
Total 20 17 23 24 16 100
Assigned
1 2 3 4 5 total
Correct
1 16 3 0 1 0 20
2 11 5 3 1 0 20
3 2 5 7 4 2 20
4 0 1 2 1 16 20
5 0 0 0 2 18 20
Total 29 14 12 9 36 100
3.3 Preliminary Experiments: Human Performance for Assigning
Sp-scores
We treat the sp-scores assigned by the reviewers as correct answers. However, the
content of a review and its sp-score may not be related. Moreover, sp-scores may
vary depending on the reviewers. We examined the universality of the sp-score.
4 One must be aware that different scales may reflect the different reactions than just
scales as Keller indicated [17].
5 We actually corrected 25000 reviews. However, we used only 2900 reviews since the
number of reviews with 1 star is very small. We examined the effect of the number
of training data is discussed in 5.3.
318 D. Okanohara and J. Tsujii
We asked two researchers of computational linguistics independently to assign
an sp-score to each review from Corpus A. We first had them learn the relation-
ship between reviews and sp-scores using 20 reviews. We then gave them 100
reviews with uniform sp-score distribution as test data. Table 3 shows the results
in terms of the square error. The Random row shows the performance achieved
by random assignment, and the All3 row shows the performance achieved by
assigning 3 to all the reviews. These results suggest that sp-scores would be
estimated with 0.78 square error from only the contents of reviews.
Table 4 shows the distribution of the estimated sp-scores and correct sp-
scores. In the table we can observe the difficulty of this task: the precise quantifi-
cation of sp-scores. For example, human B tended to overestimate the sp-score
as 1 or 5. We should note that if we consider this task as binary classifica-
tion by treating the reviews whose sp-scores are 4 and 5 as positive examples
and those with 1 and 2 as negative examples (ignoring the reviews whose sp-
scores are 3), the classification precisions by humans A and B are 95% and 96%
respectively.
4 Assigning Sp-scores to Reviews
This section describes a machine learning approach to predict the sp-scores of
review documents. Our method consists of the following two steps: extraction of
feature vectors from reviews and estimation of sp-scores by the feature vectors.
The first step basically uses existing techniques for document classification. On
the other hand, the prediction of sp-scores is different from previous studies
because we consider ordered multi-class classification, that is, each sp-score has
its own class and the classes are ordered. Unlike usual multi-class classification,
large mistakes in terms of the order should have large penalties. In this paper,
we discuss two methods of estimating sp-scores: pSVMs and SVR.
4.1 Review Representation
We represent a review as a feature vector. Although this representation ignores
the syntactic structure, word positions, and the order of words, it is known to
work reasonably well for many tasks such as information retrieval and document
classification. We use binary, tf, and tf-idf as feature weighting methods [15].
The feature vectors are normalized to have L2 norm 1.
4.2 Support Vector Regression
Support vector regression (SVR) is a method of regression that shares the un-
derlying idea with SVM [3,16]. SVR predicts the sp-score of a review by the
following regression:
f : Rn ? R, y = f(x) = ?w ? x? + b. (2)
Assigning Polarity Scores to Reviews 319
SVR uses an -insensitive loss function. This loss function means that all errors
inside the  cube are ignored. This allows SVR to use few support vectors and
gives generalization ability. Given a training set, (x1, y1), ...., (xn, yn), parame-
ters w and b are determined by:
minimize 12 ?w ? w? + C
?n
i=1(?i + ?
?
i )
subject to (?w ? x
i
? + b) ? yi ?  + ?i
yi ? (?w ? xi? + b) ?  + ??i
?(?)i ? 0 i = 1, ..., n. (3)
The factor C > 0 is a parameter that controls the trade-off between training
error minimization and margin maximization. The loss in training data increases
as C becomes smaller, and the generalization is lost as C becomes larger. More-
over, we can apply a kernel-trick to SVR as in the case with SVMs by using a
kernel function.
This approach captures the order of classes and does not suffer from data
sparseness. We could use conventional linear regression instead of SVR [4]. But
we use SVR because it can exploit the kernel-trick and avoid over-training.
Another good characteristic of SVR is that we can identify the features con-
tributing to determining the sp-scores by examining the coefficients (w in (2)),
while pSVMs does not give such information because multiple classifiers are in-
volved in determining final results. A problem in this approach is that SVR
cannot learn non-linear regression. For example, when given training data are
(x = 1, y = 1), (x = 2, y = 2), (x = 3, y = 8), SVR cannot perform regression
correctly without adjusting the feature values.
4.3 Pairwise Support Vector Machines
We apply a multi-class classification approach to estimating sp-scores. pSVMs
[7] considers each sp-score as a unique class and ignores the order among the
classes. Given reviews with sp-scores {1, 2, .., m}, we construct m ? (m ? 1)/2
SVM classifiers for all the pairs of the possible values of sp-scores. The classifier
for a sp-score pair (avsb) assigns the sp-score to a review with a or b. The class
label of a document is determined by majority voting of the classifiers. Ties in
the voting are broken by choosing the class that is closest to the neutral sp-score
(i.e, (1 + m)/2).
This approach ignores the fact that sp-scores are ordered, which causes the
following two problems. First, it allows large mistakes. Second, when the number
of possible values of the sp-score is large (e.g, n > 100), this approach suffers
from the data sparseness problem. Because pSVMs cannot employ examples that
have close sp-scores (e.g, sp-score = 50) for the classification of other sp-scores
(e.g, the classifier for a sp-score pair (51vs100)).
4.4 Features Beyond Bag-of-Words
Previous studies [9,2] suggested that complex features do not work as expected
because data become sparse when such features are used and a bag-of-words
320 D. Okanohara and J. Tsujii
Table 5. Feature list for experiments
Features Description Example in Fig.1 review 1
unigram single word (I) (believe) .. (series)
bigram pair of two adjacent words (I believe) ... (the series)
trigram adjacent three words (I believe this) ... (in the series)
inbook words in a sentence including ?book? (I) (can?t) ... (series)
aroundbook words near ?book? within two words. (the) (next) (in) (the)
approach is enough to capture the information in most reviews. Nevertheless,
we observed that reviews include many chunks of words such as ?very good? or
?must buy? that are useful for estimating the degree of polarity. We confirmed
this observation by using n-grams. Since the words around the review target
might be expected to influence the whole sp-score more than other words, we use
these words as features. We call these features reference. We assume the review
target is only the word ?book?, and we use ?inbook? and ?aroundbook? features.
The ?inbook? features are the words appear in the sentences which include the
word ?book?. The ?around book? are the words around the word ?book? within
two words. Table 5 summarizes the feature list for the experiments.
5 Experiments
We performed two series of experiments. First, we compared pSVMs and SVR.
Second, we examined the performance of various features and weighting methods.
We used Corpus A/B introduced in Sec. 3.2 for experiment data. We removed
all HTML tags and punctuation marks beforehand. We also applied the Porter
stemming method [14] to the reviews.
We divided these data into ten disjoint subsets, maintaining the uniform
class distribution. All the results reported below are the average of ten-fold cross-
validation. In SVMs and SVR, we used SVMlight6 with the quadratic polynomial
kernel K(x, z) = (?x ? z? + 1)2 and set the control parameter C to 100 in all the
experiments.
5.1 Comparison of pSVMs and SVR
We compared pSVMs and SVR to see differences in the properties of the regres-
sion approach compared with those of the classification approach. Both pSVMs
and SVR used unigram/tf-idf to represent reviews. Table 6 shows the square
error results for SVM, SVR and a simple regression (least square error) method
for Corpus A/B. These results indicate that SVR outperformed SVM in terms
of the square error and suggests that regression methods avoid large mistakes
by taking account of the fact that sp-scores are ordered, while pSVMs does not.
We also note that the result of a simple regression method is close to the result
of SVR with a linear kernel.
6 http://svmlight.joachims.org/
Assigning Polarity Scores to Reviews 321
Table 6. Comparison of multi-class SVM and SVR. Both use unigram/tf-idf.
Square error
Methods Corpus A Corpus B
pSVMs 1.32 2.13
simple regression 1.05 1.49
SVR (linear kernel) 1.01 1.46
SVR (polynomial kernel (?x ? z? + 1)2) 0.94 1.38
Figure 1 shows the distribution of estimation results for humans (top left:
human 1, top right: human 2), pSVMs (below left), and SVR (below right). The
horizontal axis shows the estimated sp-scores and the vertical axis shows the
correct sp-scores. Color density indicates the number of reviews. These figures
suggest that pSVMs and SVR could capture the gradualness of sp-scores better
than humans could. They also show that pSVMs cannot predict neutral sp-scores
well, while SVR can do so well.
5.2 Comparison of Different Features
We compared the different features presented in Section 4.4 and feature weight-
ing methods. First we compared different weighting methods. We used only
unigram features for this comparison. We then compared different features. We
used only tf-idf weighting methods for this comparison.
Table 7 summarizes the comparison results of different feature weighting
methods. The results show that tf-idf performed well on both test corpora.
We should note that simple representation methods, such as binary or tf, give
comparable results to tf-idf, which indicates that we can add more complex
features without considering the scale of feature values. For example, when we
add word-based dependency features, we have some difficulty in adjusting these
feature values to those of unigrams. But we could use these features together in
binary weighting methods.
Table 8 summarizes the comparison results for different features. For Corpus
A, unigram + bigram and unigram + trigram achieved high performance. The per-
formance of unigram + inbook was not good, which is contrary to our intuition that
the words that appear around the target object are more important than others.
For Corpus B, the results was different, that is, n-gram features could not predict
the sp-scores well. This is because the variety of words/phrases was much larger
than in Corpus A and n-gram features may have suffered from the data sparseness
problem. We should note that these feature settings are too simple, and we cannot
accept the result of reference or target object (inbook/aroundbook) directly.
Note that the data used in the preliminary experiments described in Section
3.3 are a part of Corpus A. Therefore we can compare the results for humans
with those for Corpus A in this experiment. The best result by the machine
learning approach (0.89) was close to the human results (0.78).
To analyze the influence of n-gram features, we used the linear kernel
k(x, z):= ?x ? z? in SVR training. We used tf-idf as feature weighting. We then
322 D. Okanohara and J. Tsujii
1 2 3 4 5
1
2
3
4
5
estimated sp-score
c o
rr
e c
t s
p-
sc
o
re
10-12
8-10
6-8
4-6
2-4
0-2
1 2 3 4 5
1
2
3
4
5
estimated sp-score
co
rr
ec
t s
p-
sc
or
e
16-18
14-16
12-14
10-1
8-10
6-8
4-6
2-4
0-2
1 2 3 4 5
1
2
3
4
5
estimated sp-score
c o
rr
e c
t s
p-
sc
o
re
14.0 -16.0 
12.0 -14.0 
10.0 -12.0 
8.0 -10.0 
6.0 -8.0 
4.0 -6.0 
2.0 -4.0 
0.0 -2.0 
1 2 3 4 5
1
2
3
4
5
estimated sp-score
co
rr
ec
t s
p-
sc
o
re
14.0 -16.0 
12.0 -14.0 
10.0 -12.0 
8.0 -10.0 
6.0 -8.0 
4.0 -6.0 
2.0 -4.0 
0.0 -2.0 
Fig. 1. Distribution of estimation results. Color density indicates the number of re-
views. Top left: Human A, top right: Human B, below left: pSVMs, below right: SVR.
examined each coefficient of regression. Since we used the linear kernel, the co-
efficient value of SVR showed the polarity of a single feature, that is, this value
expressed how much the occurrence of a feature affected the sp-score. Tables 9
shows the coefficients resulting from the training of SVR. These results show
that neutral polarity words themselves, such as ?all? and ?age?, will affect the
overall sp-scores of reviews with other neutral polarity words, such as, ?all ag
(age)?, ?can?t wait?, ?on (one) star?, and ?not interest?.
5.3 Learning Curve
We generated learning curves to examine the effect of the size of training data on
the performance. Figure 2 shows the results of a classification task using unigram
/tf-idf to represent reviews. The results suggest that the performance can still be
improved by increasing the training data.
Assigning Polarity Scores to Reviews 323
Table 7. Comparison results of different feature weighting methods. We used unigrams
as features of reviews.
Square error
Weighting methods (unigram) Corpus A Corpus B
tf 1.03 1.49
tf-idf 0.94 1.38
binary 1.04 1.47
Table 8. Comparison results of different features. For comparison of different features
we tf-idf as weighting methods.
Square error
Feature (tf-idf) Corpus A Corpus B
unigram (baseline) 0.94 1.38
unigram + bigram 0.89 1.41
unigram + trigram 0.90 1.42
unigram + inbook 0.97 1.36
unigram + aroundbook 0.93 1.37
Table 9. List of bigram features that have ten best/worst polarity values estimated by
SVR in Corpus A/B. The column of pol expresses the estimated sp-score of a feature,
i.e., only this feature is fired in a feature vector. (word stemming was applied)
Corpus A (best) Corpus B (best)
pol bigram pol bigram
1.73 best book 1.64 the best
1.69 is a 1.60 read it
1.49 read it 1.37 a great
1.44 all ag 1.34 on of
1.30 can?t wait 1.31 fast food
1.20 it is 1.22 harri potter
1.14 the sorcer?s 1.19 highli recommend
1.14 great ! 1.14 an excel
1.13 sorcer?s stone 1.12 to read
1.11 come out 1.01 in the
Corpus A (worst) Corpus B (worst)
pol bigram pol bigram
-1.61 at all -1.19 veri disappoint
-1.50 wast of -1.13 wast of
-1.38 potter book -0.98 the worst
-1.36 out of -0.97 is veri
-1.28 not interest -0.96 ! !
-1.18 on star -0.85 i am
-1.14 the worst -0.81 the exampl
-1.13 first four -0.79 bui it
-1.11 a wast -0.76 veri littl
-1.08 no on -0.74 onli to
6 Conclusion
In this paper, we described a novel task setting in which we predicted sp-scores
- degree of polarity - of reviews. We proposed a machine learning method using
SVR to predict sp-scores.
We compared two methods for estimating sp-scores: pSVMs and SVR. Exper-
imental results with book reviews showed that SVR performed better in terms
of the square error than pSVMs by about 30%. This result agrees with our
324 D. Okanohara and J. Tsujii
0
0.5
1
1.5
2
2.5
3
0 50 100 150 200 250 300 350
A number of training reviews per sp-score
S q
u
a r
e  
e r
ro
r
Corpus A
Corpus B
Fig. 2. Learning curve for our task setting for Corpus A and Corpus B. We used SVR
as the classifier and unigram/tf-idf to represent of reviews.
intuition that pSVMs does not consider the order of sp-scores, while SVR cap-
tures the order of sp-scores and avoids high penalty mistakes. With SVR, sp-
scores can be estimated with a square error of 0.89, which is very close to the
square error achieved by human (0.78).
We examined the effectiveness of features beyond a bag-of-words and refer-
ence features (the words around the reviewed objects.) The results suggest that
n-gram features and reference features contribute to improve the accuracy.
As the next step in our research, we plan to exploit parsing results such as
predicate argument structures for detecting precise reference information. We
will also capture other types of polarity than attitude, such as modality and
writing position [8], and we will consider estimating these types of polarity.
We plan to develop a classifier specialized for ordered multi-class classifica-
tion using recent studies on machine learning for structured output space [19,18]
or ordinal regression [5] because our experiments suggest that both pSVMs and
SVR have advantages and disadvantages. We will develop a more efficient clas-
sifier that outperforms pSVMs and SVR by combining these ideas.
References
1. T . Joachims. Learning to Classify Text Using Support Vector Machines. Kluwer,
2002.
2. C. Apte, F. Damerau, and S. Weiss. Automated learning of decision rules for text
categorization. Information Systems, 12(3):233?251, 1994.
3. N. Cristianini and J. S. Taylor. An Introduction to Support Vector Machines and
other Kernel-based Learning Methods. Cambridge University Press, 2000.
4. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning.
Springer, 2001.
5. Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin rank bound-
aries for ordinal regression. In Advances in Large Margin Classifiers, pages 115?132.
MIT press, 2000.
Assigning Polarity Scores to Reviews 325
6. Moshe Koppel and Jonathan Schler. The importance of neutral examples for learn-
ing sentiment. In In Workshop on the Analysis of Informal and Formal Information
Exchange during Negotiations (FINEXIN), 2005.
7. U. Kresel. Pairwise Classification and Support Vector Machines Methods. MIT
Press, 1999.
8. T. Kudo and Y. Matsumoto. A boosting algorithm for classification of semi-
structured text. In Proceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 301?308, 2004.
9. D. Lewis. An evaluation of phrasal and clustered representations on a text cate-
gorization task. In Proceedings of SIGIR-92, 15th ACM International Conference
on Research and Development in Information Retrieval, pages 37?50, 1992.
10. A. Mullen and N. Collier. Sentiment analysis using Support Vector Machines with
diverse information sources. In Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL), 2004.
11. B. Pang and L. Lee. A sentimental education: Sentiment analysis using subjectivity
summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL), pages 271?278, 2004.
12. B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment
categorization with respect to rating scales. In Proceedings of the 43nd Meeting of
the Association for Computational Linguistics (ACL), 2005.
13. B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 79?86, 2002.
14. M.F. Porter. An algorithm for suffix stripping, program. Program, 14(3):130?137,
1980.
15. F. Sebastiani. Machine learning in automated text categorization. ACM Computing
Surveys, 34(1):1?47, 2002.
16. A. Smola and B. Sch. A tutorial on Support Vector Regression. Technical report,
NeuroCOLT2 Technical Report NC2-TR-1998-030, 1998.
17. Antonella Sorace and Frank Keller. Gradience in linguistic data. Lingua,
115(11):1497?1524, 2005.
18. B. Taskar. Learning Structured Prediction Models: A Large Margin Approach. PhD
thesis, Stanford University, 2004.
19. I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine
learning for interdependent and structured output spaces. In Machine Learning,
Proceedings of the Twenty-first International Conference (ICML), 2004.
20. P. D. Turney. Thumbs up or thumbs down? semantic orientation applied to un-
supervised classification of reviews. In Proceedings of the 40th Meeting of the
Association for Computational Linguistics (ACL), pages 417?424, 2002.
21. V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.
Proceedings of NAACL HLT 2009: Short Papers, pages 97?100,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Combination Features with L1 Regularization
Daisuke Okanohara? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
?School of Informatics, University of Manchester
?NaCTeM (National Center for Text Mining)
{hillbig,tsujii}@is.s.u-tokyo.ac.jp
Abstract
When linear classifiers cannot successfully
classify data, we often add combination fea-
tures, which are products of several original
features. The searching for effective combi-
nation features, namely feature engineering,
requires domain-specific knowledge and hard
work. We present herein an efficient algorithm
for learning an L1 regularized logistic regres-
sion model with combination features. We
propose to use the grafting algorithm with ef-
ficient computation of gradients. This enables
us to find optimal weights efficiently without
enumerating all combination features. By us-
ing L1 regularization, the result we obtain is
very compact and achieves very efficient in-
ference. In experiments with NLP tasks, we
show that the proposed method can extract ef-
fective combination features, and achieve high
performance with very few features.
1 Introduction
A linear classifier is a fundamental tool for many
NLP applications, including logistic regression
models (LR), in that its score is based on a lin-
ear combination of features and their weights,. Al-
though a linear classifier is very simple, it can
achieve high performance on many NLP tasks,
partly because many problems are described with
very high-dimensional data, and high dimensional
weight vectors are effective in discriminating among
examples.
However, when an original problem cannot be
handled linearly, combination features are often
added to the feature set, where combination features
are products of several original features. Examples
of combination features are, word pairs in docu-
ment classification, or part-of-speech pairs of head
and modifier words in a dependency analysis task.
However, the task of determining effective combina-
tion features, namely feature engineering, requires
domain-specific knowledge and hard work.
Such a non-linear phenomenon can be implic-
itly captured by using the kernel trick. However,
its computational cost is very high, not only during
training but also at inference time. Moreover, the
model is not interpretable, in that effective features
are not represented explicitly. Many kernels meth-
ods assume an L2 regularizer, in that many features
are equally relevant to the tasks (Ng, 2004).
There have been several studies to find efficient
ways to obtain (combination) features. In the con-
text of boosting, Kudo (2004) have proposed a
method to extract complex features that is similar
to the item set mining algorithm. In the context of
L1 regularization. Dud??k (2007), Gao (2006), and
Tsuda (2007) have also proposed methods by which
effective features are extracted from huge sets of fea-
ture candidates. However, their methods are still
very computationally expensive, and we cannot di-
rectly apply this kind of method to a large-scale NLP
problem.
In the present paper, we propose a novel algorithm
for learning of an L1 regularized LR with combina-
tion features. In our algorithm, we can exclusively
extract effective combination features without enu-
merating all of the candidate features. Our method
relies on a grafting algorithm (Perkins and Theeiler,
2003), which incrementally adds features like boost-
ing, but it can converge to the global optimum.
We use L1 regularization because we can obtain
a sparse parameter vector, for which many of the
parameter values are exactly zero. In other words,
learning with L1 regularization naturally has an in-
trinsic effect of feature selection, which results in an
97
efficient and interpretable inference with almost the
same performance as L2 regularization (Gao et al,
2007).
The heart of our algorithm is a way to find a
feature that has the largest gradient value of likeli-
hood from among the huge set of candidates. To
solve this problem, we propose an example-wise al-
gorithm with filtering. This algorithm is very simple
and easy to implement, but effective in practice.
We applied the proposed methods to NLP tasks,
and found that our methods can achieve the same
high performance as kernel methods, whereas the
number of active combination features is relatively
small, such as several thousands.
2 Preliminaries
2.1 Logistic Regression Model
In this paper, we consider a multi-class logistic re-
gression model (LR). For an input x, and an output
label y ? Y , we define a feature vector ?(x, y) ?
Rm.
Then in LR, the probability for a label y, given an
input x, is defined as follows:
p(y|x;w) = 1Z(x,w) exp
(
wT?(x, y)) , (1)
where w ? Rm is a weight vector1 correspond-
ing to each input dimension, and Z(x,w) =?
y exp(wT?(x, y)) is the partition function.We estimate the parameter w by a maximum like-
lihood estimation (MLE) with L1 regularization us-
ing training examples {(x1, y1), . . . , (xn, yn)}:
w? = argmin
w
? L(w) + C?
i
|wi| (2)
L(w) = ?
i=1...n
log p(yi|xi;w)
where C > 0 is the trade-off parameter between the
likelihood term and the regularization term. This es-
timation is a convex optimization problem.
2.2 Grafting
To maximize the effect of L1 regularization, we use
the grafting algorithm (Perkins and Theeiler, 2003);
namely, we begin with the empty feature set, and
incrementally add effective features to the current
problem. Note that although this is similar to the
1A bias term b is often considered by adding an additional
dimension to ?(x, y)
boosting algorithm for learning, the obtained result
is always optimal. We explain the grafting algorithm
here again for the sake of clarity.
The grafting algorithm is summarized in Algo-
rithm 1.
In this algorithm we retain two variables; w stores
the current weight vector, and H stores the set of
features with a non-zero weight. Initially, we set
w = 0, and H = {}. At each iteration, the fea-
ture is selected that has the largest absolute value of
the gradient of the likelihood. Let vk = ?L(w)?wk bethe gradient value of the likelihood of a feature k.
By following the definition, the value vk can be cal-
culated as follows,
vk =
?
i,y
?i,y?k(xi, y), (3)
where ?i,y = I(yi = y)? p(yi|xi;w) and I(a) is 1
if a is true and 0 otherwise.
Then, we add k? = argmaxk |vk| to H and opti-
mize (2) with regard to H only. The solution w that
is obtained is used in the next search. The iteration
is continued until |v?k| < C.We briefly explain why we can find the optimal
weight by this algorithm. Suppose that we optimize
(2) with all features, and initialize the weights us-
ing the results obtained from the grafting algorithm.
Since all gradients of likelihoods satisfy |vk| ? C,
and the regularization term pushes the weight toward
0 by C, any changes of the weight vector cannot in-
crease the objective value in (2). Since (2) is the
convex optimization problem, the local optimum is
always the global optimum, and therefore this is the
global optimum for (2)
The point is that, given an efficient method to esti-
mate v?k without the enumeration of all features, wecan solve the optimization in time proportional to the
active feature, regardless of the number of candidate
features. We will discuss this in the next section.
3 Extraction of Combination Features
This section presents an algorithm to compute, for
combination features, the feature v?k that has thelargest absolute value of the gradient.
We propose an element-wise extraction method,
where we make use of the sparseness of the training
data.
In this paper, we assume that the values of the
combination features are less than or equal to the
original ones. This assumption is typical; for exam-
ple, it is made in the case where we use binary values
for original and combination features.
98
Algorithm 1 Grafting
Input: training data (xi, yi) (i = 1, ? ? ? , n) and
parameter C
H = {},w = 0
loop
v = ?L(w)?w (L(w) is the log likelihood term)
k? = argmax
k
|vk| (The result of Algorithm 2)
if |vk? | < C then break
H = H ? k?
Optimize w with regards to H
end loop
Output w and H
First, we sort the examples in the order of their?
y |?i,y| values. Then, we look at the examples oneby one. Let us assume that r examples have been
examined so far. Let us define
t = ?
i?r,y
?i,y?(xi, y) (4)
t? = ?
i>r,y
??i,y?(xi, y) t+ =
?
i>r,y
?+i,y?(xi, y)
where ??i,y = min(?i,y, 0) and ?+i,y = max(?i,y, 0).Then, simple calculus shows that the gradient
value for a combination feature k, vk, for which
the original features are k1 and k2, is bounded be-
low/above thus;
tk + t?k < vk < tk + t+k (5)
tk + max(t?k1, t?k2) < vk < tk + min(t+k1, t+k2).
Intuitively, the upper bound of (5) is the case where
the combination feature fires only for the examples
with ?i,y ? 0, and the lower bound of (5is the case
where the combination feature fires only for the ex-
amples with ?i,y ? 0. The second inequality arises
from the fact that the value of a combination feature
is equal to or less than the values of its original fea-
tures. Therefore, we examine (5) and check whether
or not |vk| will be larger than C. If not, we can re-
move the feature safely.
Since the examples are sorted in the order of their?
y |?i,y|, the bound will become tighter quickly.Therefore, many combination features are filtered
out in the early steps. In experiments, the weights
for the original features are optimized first, and then
the weights for combination features are optimized.
This significantly reduces the number of candidates
for combination features.
Algorithm 2 Algorithm to return the feature that has
the largest gradient value.
Input: training data (xi, yi) and its ?i,y value
(i = 1, . . . , n, y = 1, . . . , |Y |), and the param-
eter C. Examples are sorted with respect to their?
y |?i,y| values.
t+ =?ni=1
?
y max(?i,y, 0)?(x, y)
t? =?ni=1
?
y min(?i,y, 0)?(x, y)
t = 0, H = {} // Active Combination Feature
for i = 1 to n and y ? Y do
for all combination features k in xi do
if |vk| > C (Check by using Eq.(5) ) then
vk := vk + ?i,y?k(xi, y)
H = H ? k
end if
end for
t+ := t+ ?max(?i,y, 0)?(xi, y)
t? := t? ?min(?i,y, 0)?(xi, y)
end for
Output: argmaxk?H vk
Algorithm 2 presents the details of the overall al-
gorithm for the extraction of effective combination
features. Note that many candidate features will be
removed just before adding.
4 Experiments
To measure the effectiveness of the proposed
method (called L1-Comb), we conducted experi-
ments on the dependency analysis task, and the doc-
ument classification task. In all experiments, the pa-
rameterC was tuned using the development data set.
In the first experiment, we performed Japanese
dependency analysis. We used the Kyoto Text Cor-
pus (Version 3.0), Jan. 1, 3-8 as the training data,
Jan. 10 as the development data, and Jan. 9 as the
test data so that the result could be compared to those
from previous studies (Sassano, 2004)2. We used the
shift-reduce dependency algorithm (Sassano, 2004).
The number of training events was 11, 3332, each of
which consisted of two word positions as inputs, and
y = {0, 1} as an output indicating the dependency
relation. For the training data, the number of orig-
inal features was 78570, and the number of combi-
nation features of degrees 2 and 3 was 5787361, and
169430335, respectively. Note that we need not see
all of them using our algorithm.
2The data set is different from that in the CoNLL shared
task. This data set is more difficult.
99
Table 1: The performance of the Japanese dependency
task on the Test set. The active features column shows
the number of nonzero weight features.
DEP. TRAIN ACTIVE
ACC. (%) TIME (S) FEAT.
L1-COMB 89.03 605 78002
L1-ORIG 88.50 35 29166
SVM 3-POLY 88.72 35720 (KERNEL)
L2-COMB3 89.52 22197 91477782
AVE. PERCE. 87.23 5 45089
In all experiments, combination features of de-
grees 2 and 3 (the products of two or three original
features) were used.
We compared our methods using LR with L1
regularization using original features (L1-Original),
SVM with a 3rd-polynomial Kernel, LR with L2
regularization using combination features with up to
3 combinations (L2-Comb3), and an averaged per-
ceptron with original features (Ave. Perceptron).
Table 1 shows the result of the Japanese depen-
dency task. The accuracy result indicates that the
accuracy was improved with automatically extracted
combination features. In the column of active fea-
tures, the number of active features is listed. This
indicates thatL1 regularization automatically selects
very few effective features. Note that, in training,
L1-Comb used around 100 MB, while L2-Comb3
used more than 30 GB. The most time consuming
part for L1-Comb was the optimization of the L1-
LR problem.
Examples of extracted combination features in-
clude POS pairs of head and modifiers, such as
Head/Noun-Modifier/Noun, and combinations of
distance features with the POS of head.
For the second experiment, we performed the
document classification task using the Tech-TC-300
data set (Davidov et al, 2004)3. We used the tf-idf
scores as feature values. We did not filter out any
words beforehand. The Tech-TC-300 data set con-
sists of 295 binary classification tasks. We divided
each document set into a training and a test set. The
ratio of the test set to the training set was 1 : 4. The
average number of features for tasks was 25, 389.
Table 2 shows the results for L1-LR with combi-
nation features and SVM with linear kernel4. The
results indicate that the combination features are ef-
fective.
3http://techtc.cs.technion.ac.il/techtc300/techtc300.html
4SVM with polynomial kernel did not achieve significant
improvement
Table 2: Document classification results for the Tech-TC-
300 data set. The column F2 shows the average of F2
scores for each method of classification.
F2
L1-COMB 0.949
L1-ORIG 0.917
SVM (LINEAR KERNEL) 0.896
5 Conclusion
We have presented a method to extract effective
combination features for the L1 regularized logis-
tic regression model. We have shown that a simple
filtering technique is effective for enumerating effec-
tive combination features in the grafting algorithm,
even for large-scale problems. Experimental results
show that a L1 regularized logistic regression model
with combination features can achieve comparable
or better results than those from other methods, and
its result is very compact and easy to interpret. We
plan to extend our method to include more complex
features, and apply it to structured output learning.
References
Davidov, D., E. Gabrilovich, and S. Markovitch. 2004.
Parameterized generation of labeled datasets for text
categorization based on a hierarchical directory. In
Proc. of SIGIR.
Dud??k, Miroslav, Steven J. Phillips, and Robert E.
Schapire. 2007. Maximum entropy density estima-
tion with generalized regularization and an application
to species distribution modeling. JMLR, 8:1217?1260.
Gao, J., H. Suzuki, and B. Yu. 2006. Approximation
lasso methods for language modeling. In Proc. of
ACL/COLING.
Gao, J., G. Andrew, M. Johnson, and K. Toutanova.
2007. A comparative study of parameter estimation
methods for statistical natural language processing. In
Proc. of ACL, pages 824?831.
Kudo, T. and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proc. of
EMNLP.
Ng, A. 2004. Feature selection, l1 vs. l2 regularization,
and rotational invariance. In NIPS.
Perkins, S. and J. Theeiler. 2003. Online feature selec-
tion using grafting. ICML.
Saigo, H., T. Uno, and K. Tsuda. 2007. Mining com-
plex genotypic features for predicting HIV-1 drug re-
sistance. Bioinformatics, 23:2455?2462.
Sassano, Manabu. 2004. Linear-time dependency analy-
sis for japanese. In Proc. of COLING.
100
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 465?472,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving the Scalability of Semi-Markov Conditional
Random Fields for Named Entity Recognition
Daisuke Okanohara? Yusuke Miyao? Yoshimasa Tsuruoka ? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
?School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
?SORST, Solution Oriented Research for Science and Technology
Honcho 4-1-8, Kawaguchi-shi, Saitama, Japan
{hillbig,yusuke,tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents techniques to apply
semi-CRFs to Named Entity Recognition
tasks with a tractable computational cost.
Our framework can handle an NER task
that has long named entities and many
labels which increase the computational
cost. To reduce the computational cost,
we propose two techniques: the first is the
use of feature forests, which enables us to
pack feature-equivalent states, and the sec-
ond is the introduction of a filtering pro-
cess which significantly reduces the num-
ber of candidate states. This framework
allows us to use a rich set of features ex-
tracted from the chunk-based representa-
tion that can capture informative charac-
teristics of entities. We also introduce a
simple trick to transfer information about
distant entities by embedding label infor-
mation into non-entity labels. Experimen-
tal results show that our model achieves an
F-score of 71.48% on the JNLPBA 2004
shared task without using any external re-
sources or post-processing techniques.
1 Introduction
The rapid increase of information in the biomedi-
cal domain has emphasized the need for automated
information extraction techniques. In this paper
we focus on the Named Entity Recognition (NER)
task, which is the first step in tackling more com-
plex tasks such as relation extraction and knowl-
edge mining.
Biomedical NER (Bio-NER) tasks are, in gen-
eral, more difficult than ones in the news domain.
For example, the best F-score in the shared task of
Bio-NER in COLING 2004 JNLPBA (Kim et al,
2004) was 72.55% (Zhou and Su, 2004) 1, whereas
the best performance at MUC-6, in which systems
tried to identify general named entities such as
person or organization names, was an accuracy of
95% (Sundheim, 1995).
Many of the previous studies of Bio-NER tasks
have been based on machine learning techniques
including Hidden Markov Models (HMMs) (Bikel
et al, 1997), the dictionary HMM model (Kou et
al., 2005) and Maximum Entropy Markov Mod-
els (MEMMs) (Finkel et al, 2004). Among these
methods, conditional random fields (CRFs) (Laf-
ferty et al, 2001) have achieved good results (Kim
et al, 2005; Settles, 2004), presumably because
they are free from the so-called label bias problem
by using a global normalization.
Sarawagi and Cohen (2004) have recently in-
troduced semi-Markov conditional random fields
(semi-CRFs). They are defined on semi-Markov
chains and attach labels to the subsequences of a
sentence, rather than to the tokens2. The semi-
Markov formulation allows one to easily construct
entity-level features. Since the features can cap-
ture all the characteristics of a subsequence, we
can use, for example, a dictionary feature which
measures the similarity between a candidate seg-
ment and the closest element in the dictionary.
Kou et al (2005) have recently showed that semi-
CRFs perform better than CRFs in the task of
recognition of protein entities.
The main difficulty of applying semi-CRFs to
Bio-NER lies in the computational cost at training
1Krauthammer (2004) reported that the inter-annotator
agreement rate of human experts was 77.6% for bio-NLP,
which suggests that the upper bound of the F-score in a Bio-
NER task may be around 80%.
2Assuming that non-entity words are placed in unit-length
segments.
465
Table 1: Length distribution of entities in the train-
ing set of the shared task in 2004 JNLPBA
Length # entity Ratio
1 21646 42.19
2 15442 30.10
3 7530 14.68
4 3505 6.83
5 1379 2.69
6 732 1.43
7 409 0.80
8 252 0.49
>8 406 0.79
total 51301 100.00
because the number of named entity classes tends
to be large, and the training data typically contain
many long entities, which makes it difficult to enu-
merate all the entity candidates in training. Table
1 shows the length distribution of entities in the
training set of the shared task in 2004 JNLPBA.
Formally, the computational cost of training semi-
CRFs is O(KLN), where L is the upper bound
length of entities, N is the length of sentence and
K is the size of label set. And that of training in
first order semi-CRFs is O(K2LN). The increase
of the cost is used to transfer non-adjacent entity
information.
To improve the scalability of semi-CRFs, we
propose two techniques: the first is to intro-
duce a filtering process that significantly re-
duces the number of candidate entities by using
a ?lightweight? classifier, and the second is to
use feature forest (Miyao and Tsujii, 2002), with
which we pack the feature equivalent states. These
enable us to construct semi-CRF models for the
tasks where entity names may be long and many
class-labels exist at the same time. We also present
an extended version of semi-CRFs in which we
can make use of information about a preceding
named entity in defining features within the frame-
work of first order semi-CRFs. Since the preced-
ing entity is not necessarily adjacent to the current
entity, we achieve this by embedding the informa-
tion on preceding labels for named entities into the
labels for non-named entities.
2 CRFs and Semi-CRFs
CRFs are undirected graphical models that encode
a conditional probability distribution using a given
set of features. CRFs allow both discriminative
training and bi-directional flow of probabilistic in-
formation along the sequence. In NER, we of-
ten use linear-chain CRFs, which define the con-
ditional probability of a state sequence y = y1, ...,
yn given the observed sequence x = x1,...,xn by:
p(y|x, ?) = 1
Z(x) exp(?
n
i=1?j?jfj(yi?1, yi, x, i)),
(1)
where fj(yi?1, yi,x, i) is a feature function and
Z(x) is the normalization factor over all the state
sequences for the sequence x. The model parame-
ters are a set of real-valued weights ? = {?j}, each
of which represents the weight of a feature. All the
feature functions are real-valued and can use adja-
cent label information.
Semi-CRFs are actually a restricted version of
order-L CRFs in which all the labels in a chunk are
the same. We follow the definitions in (Sarawagi
and Cohen, 2004). Let s = ?s1, ..., sp? denote a
segmentation of x, where a segment sj = ?tj , uj ,
yj? consists of a start position tj , an end position
uj , and a label yj . We assume that segments have a
positive length bounded above by the pre-defined
upper bound L (tj ? uj , uj ? tj + 1 ? L) and
completely cover the sequence x without overlap-
ping, that is, s satisfies t1 = 1, up = |x|, and
tj+1 = uj + 1 for j = 1, ..., p ? 1. Semi-CRFs
define a conditional probability of a state sequence
y given an observed sequence x by:
p(y|x, ?) = 1
Z(x) exp(?j?i?ifi(sj)), (2)
where fi(sj) := fi(yj?1, yj ,x, tj , uj) is a fea-
ture function and Z(x) is the normalization factor
as defined for CRFs. The inference problem for
semi-CRFs can be solved by using a semi-Markov
analog of the usual Viterbi algorithm. The com-
putational cost for semi-CRFs is O(KLN) where
L is the upper bound length of entities, N is the
length of sentence and K is the number of label
set. If we use previous label information, the cost
becomes O(K2LN).
3 Using Non-Local Information in
Semi-CRFs
In conventional CRFs and semi-CRFs, one can
only use the information on the adjacent previ-
ous label when defining the features on a certain
state or entity. In NER tasks, however, informa-
tion about a distant entity is often more useful than
466
O protein O O DNA
O protein O-protein O-protein DNA
Figure 1: Modification of ?O? (other labels) to
transfer information on a preceding named entity.
information about the previous state (Finkel et al,
2005). For example, consider the sentence ?... in-
cluding Sp1 and CP1.? where the correct labels of
?Sp1? and ?CP1? are both ?protein?. It would be
useful if the model could utilize the (non-adjacent)
information about ?Sp1? being ?protein? to clas-
sify ?CP1? as ?protein?. On the other hand, in-
formation about adjacent labels does not necessar-
ily provide useful information because, in many
cases, the previous label of a named entity is ?O?,
which indicates a non-named entity. For 98.0% of
the named entities in the training data of the shared
task in the 2004 JNLPBA, the label of the preced-
ing entity was ?O?.
In order to incorporate such non-local informa-
tion into semi-CRFs, we take a simple approach.
We divide the label of ?O? into ?O-protein? and
?O? so that they convey the information on the
preceding named entity. Figure 1 shows an ex-
ample of this conversion, in which the two labels
for the third and fourth states are converted from
?O? to ?O-protein?. When we define the fea-
tures for the fifth state, we can use the informa-
tion on the preceding entity ?protein? by look-
ing at the fourth state. Since this modification
changes only the label set, we can do this within
the framework of semi-CRF models. This idea is
originally proposed in (Peshkin and Pfeffer, 2003).
However, they used a dynamic Bayesian network
(DBNs) rather than a semi-CRF, and semi-CRFs
are likely to have significantly better performance
than DBNs.
In previous work, such non-local information
has usually been employed at a post-processing
stage. This is because the use of long distance
dependency violates the locality of the model and
prevents us from using dynamic programming
techniques in training and inference. Skip-CRFs
(Sutton and McCallum, 2004) are a direct imple-
mentation of long distance effects to the model.
However, they need to determine the structure
for propagating non-local information in advance.
In a recent study by Finkel et al, (2005), non-
local information is encoded using an indepen-
dence model, and the inference is performed by
Gibbs sampling, which enables us to use a state-
of-the-art factored model and carry out training ef-
ficiently, but inference still incurs a considerable
computational cost. Since our model handles lim-
ited type of non-local information, i.e. the label
of the preceding entity, the model can be solved
without approximation.
4 Reduction of Training/Inference Cost
The straightforward implementation of this mod-
eling in semi-CRFs often results in a prohibitive
computational cost.
In biomedical documents, there are quite a few
entity names which consist of many words (names
of 8 words in length are not rare). This makes
it difficult for us to use semi-CRFs for biomedi-
cal NER, because we have to set L to be eight or
larger, where L is the upper bound of the length of
possible chunks in semi-CRFs. Moreover, in or-
der to take into account the dependency between
named entities of different classes appearing in a
sentence, we need to incorporate multiple labels
into a single probabilistic model. For example, in
the shared task in COLING 2004 JNLPBA (Kim
et al, 2004) the number of labels is six (?pro-
tein?, ?DNA?, ?RNA?, ?cell line?, ?cell type?
and ?other?). This also increases the computa-
tional cost of a semi-CRF model.
To reduce the computational cost, we propose
two methods (see Figure 2). The first is employing
a filtering process using a lightweight classifier to
remove unnecessary state candidates beforehand
(Figure 2 (2)), and the second is the using the fea-
ture forest model (Miyao and Tsujii, 2002) (Fig-
ure 2 (3)), which employs dynamic programming
at training ?as much as possible?.
4.1 Filtering with a naive Bayes classifier
We introduce a filtering process to remove low
probability candidate states. This is the first step
of our NER system. After this filtering step, we
construct semi-CRFs on the remaining candidate
states using a feature forest. Therefore the aim of
this filtering is to reduce the number of candidate
states, without removing correct entities. This idea
467
(1) Enumerate
Candidate States
(2) Filtering by
Na?ve Bayes
(3) Construct feature forest
Training/
Inference
: other : entity
: other with preceding entity information
Figure 2: The framework of our system. We first enumerate all possible candidate states, and then filter
out low probability states by using a light-weight classifier, and represent them by using feature forest.
Table 2: Features used in the naive Bayes Classi-
fier for the entity candidate: ws, ws+1, ..., we. spi
is the result of shallow parsing at wi.
Feature Name Example of Features
Start/End Word ws, we
Inside Word ws, ws+1, ... , we
Context Word ws?1, we+1
Start/End SP sps, spe
Inside SP sps, sps+1, ..., spe
Context SP sps?1, spe+1
is similar to the method proposed by Tsuruoka and
Tsujii (2005) for chunk parsing, in which implau-
sible phrase candidates are removed beforehand.
We construct a binary naive Bayes classifier us-
ing the same training data as those for semi-CRFs.
In training and inference, we enumerate all possi-
ble chunks (the max length of a chunk is L as for
semi-CRFs) and then classify those into ?entity?
or ?other?. Table 2 lists the features used in the
naive Bayes classifier. This process can be per-
formed independently of semi-CRFs
Since the purpose of the filtering is to reduce the
computational cost, rather than to achieve a good
F-score by itself, we chose the threshold probabil-
ity of filtering so that the recall of filtering results
would be near 100 %.
4.2 Feature Forest
In estimating semi-CRFs, we can use an efficient
dynamic programming algorithm, which is simi-
lar to the forward-backward algorithm (Sarawagi
and Cohen, 2004). The proposal here is a more
general framework for estimating sequential con-
ditional random fields.
This framework is based on the feature forest
DNA
protein
Other
DNA
protein
Other
: or node (disjunctive node)
: and node (conjunctive node)
pos i i+1
??
Figure 3: Example of feature forest representation
of linear chain CRFs. Feature functions are as-
signed to ?and? nodes.
protein
O-protein
protein
u
j
=8 
prev-entity:protein
u
j
=  8
prev-entity: protein
packed
pos
87 9
Figure 4: Example of packed representation of
semi-CRFs. The states that have the same end po-
sition and prev-entity label are packed.
model, which was originally proposed for disam-
biguation models for parsing (Miyao and Tsujii,
2002). A feature forest model is a maximum en-
tropy model defined over feature forests, which are
abstract representations of an exponential number
of sequence/tree structures. A feature forest is
an ?and/or? graph: in Figure 3, circles represent
468
?and? nodes (conjunctive nodes), while boxes de-
note ?or? nodes (disjunctive nodes). Feature func-
tions are assigned to ?and? nodes. We can use
the information of the previous ?and? node for de-
signing the feature functions through the previous
?or? node. Each sequence in a feature forest is
obtained by choosing a conjunctive node for each
disjunctive node. For example, Figure 3 represents
3 ? 3 = 9 sequences, since each disjunctive node
has three candidates. It should be noted that fea-
ture forests can represent an exponential number
of sequences with a polynomial number of con-
junctive/disjunctive nodes.
One can estimate a maximum entropy model for
the whole sequence with dynamic programming
by representing the probabilistic events, i.e. se-
quence of named entity tags, by feature forests
(Miyao and Tsujii, 2002).
In the previous work (Lafferty et al, 2001;
Sarawagi and Cohen, 2004), ?or? nodes are con-
sidered implicitly in the dynamic programming
framework. In feature forest models, ?or? nodes
are packed when they have same conditions. For
example, ?or? nodes are packed when they have
same end positions and same labels in the first or-
der semi-CRFs,
In general, we can pack different ?or? nodes that
yield equivalent feature functions in the follow-
ing nodes. In other words, ?or? nodes are packed
when the following states use partial information
on the preceding states. Consider the task of tag-
ging entity and O-entity, where the latter tag is ac-
tually O tags that distinguish the preceding named
entity tags. When we simply apply first-order
semi-CRFs, we must distinguish states that have
different previous states. However, when we want
to distinguish only the preceding named entity tags
rather than the immediate previous states, feature
forests can represent these events more compactly
(Figure 4). We can implement this as follows. In
each ?or? node, we generate the following ?and?
nodes and their feature functions. Then we check
whether there exist ?or? node which has same con-
ditions by using its information about ?end posi-
tion? and ?previous entity?. If so, we connect the
?and? node to the corresponding ?or? node. If not,
we generate a new ?or? node and continue the pro-
cess.
Since the states with label O-entity and entity
are packed, the computational cost of training in
our model (First order semi-CRFs) becomes the
half of the original one.
5 Experiments
5.1 Experimental Setting
Our experiments were performed on the training
and evaluation set provided by the shared task in
COLING 2004 JNLPBA (Kim et al, 2004). The
training data used in this shared task came from
the GENIA version 3.02 corpus. In the task there
are five semantic labels: protein, DNA, RNA,
cell line and cell type. The training set consists
of 2000 abstracts from MEDLINE, and the evalu-
ation set consists of 404 abstracts. We divided the
original training set into 1800 abstracts and 200
abstracts, and the former was used as the training
data and the latter as the development data. For
semi-CRFs, we used amis3 for training the semi-
CRF with feature-forest. We used GENIA taggar4
for POS-tagging and shallow parsing.
We set L = 10 for training and evaluation when
we do not state L explicitly , where L is the upper
bound of the length of possible chunks in semi-
CRFs.
5.2 Features
Table 3 lists the features used in our semi-CRFs.
We describe the chunk-dependent features in de-
tail, which cannot be encoded in token-level fea-
tures.
?Whole chunk? is the normalized names at-
tached to a chunk, which performs like the closed
dictionary. ?Length? and ?Length and End-
Word? capture the tendency of the length of a
named entity. ?Count feature? captures the ten-
dency for named entities to appear repeatedly in
the same sentence.
?Preceding Entity and Prev Word? are fea-
tures that capture specifically words for conjunc-
tions such as ?and? or ?, (comma)?, e.g., for the
phrase ?OCIM1 and K562?, both ?OCIM1? and
?K562? are assigned cell line labels. Even if
the model can determine only that ?OCIM1? is a
cell line , this feature helps ?K562? to be assigned
the label cell line.
5.3 Results
We first evaluated the filtering performance. Table
4 shows the result of the filtering on the training
3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/
4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
Note that the evaluation data are not used for training the GE-
NIA tagger.
469
Table 3: Feature templates used for the chunk s := ws ws+1 ... we where ws and we represent the words
at the beginning and ending of the target chunk respectively. pi is the part of speech tag of wi and sci is
the shallow parse result of wi.
Feature Name description of features
Non-Chunk Features
Word/POS/SC with Position BEGIN + ws, END + we, IN + ws+1, ..., IN + we?1, BEGIN + ps,...
Context Uni-gram/Bi-gram ws?1, we+1, ws?2 + ws?1, we+1 + we+2, ws?1 + we+1
Prefix/Suffix of Chunk 2/3-gram character prefix of ws, 2/3/4-gram character suffix of we
Orthography capitalization and word formation of ws...we
Chunk Features
Whole chunk ws + ws+1 + ... + we
Word/POS/SC End Bi-grams we?1 + we, pe?1 + pe, sce?1 + sce
Length, Length and End Word |s|, |s|+we
Count Feature the frequency of wsws+1..we in a sentence is greater than one
Preceding Entity Features
Preceding Entity /and Prev Word PrevState, PrevState + ws?1
Table 4: Filtering results using the naive Bayes
classifier. The number of entity candidates for the
training set was 4179662, and that of the develop-
ment set was 418628.
Training set
Threshold probability reduction ratio recall
1.0 ? 10?12 0.14 0.984
1.0 ? 10?15 0.20 0.993
Development set
Threshold probability reduction ratio recall
1.0 ? 10?12 0.14 0.985
1.0 ? 10?15 0.20 0.994
and evaluation data. The naive Bayes classifiers
effectively reduced the number of candidate states
with very few falsely removed correct entities.
We then examined the effect of filtering on the
final performance. In this experiment, we could
not examine the performance without filtering us-
ing all the training data, because training on all
the training data without filtering required much
larger memory resources (estimated to be about
80G Byte) than was possible for our experimental
setup. We thus compared the result of the recog-
nizers with and without filtering using only 2000
sentences as the training data. Table 5 shows the
result of the total system with different filtering
thresholds. The result indicates that the filtering
method achieved very well without decreasing the
overall performance.
We next evaluate the effect of filtering, chunk
information and non-local information on final
performance. Table 6 shows the performance re-
sult for the recognition task. L means the upper
bound of the length of possible chunks in semi-
CRFs. We note that we cannot examine the re-
sult of L = 10 without filtering because of the in-
tractable computational cost. The row ?w/o Chunk
Feature? shows the result of the system which does
not employ Chunk-Features in Table 3 at training
and inference. The row ?Preceding Entity? shows
the result of a system which uses Preceding En-
tity and Preceding Entity and Prev Word fea-
tures. The results indicate that the chunk features
contributed to the performance, and the filtering
process enables us to use full chunk representation
(L = 10). The results of McNemar?s test suggest
that the system with chunk features is significantly
better than the system without it (the p-value is
less than 1.0 < 10?4). The result of the preceding
entity information improves the performance. On
the other hand, the system with preceding infor-
mation is not significantly better than the system
without it5. Other non-local information may im-
prove performance with our framework and this is
a topic for future work.
Table 7 shows the result of the overall perfor-
mance in our best setting, which uses the infor-
mation about the preceding entity and 1.0?10?15
threshold probability for filtering. We note that the
result of our system is similar to those of other sys-
5The result of the classifier on development data is 74.64
(without preceding information) and 75.14 (with preceding
information).
470
Table 5: Performance with filtering on the development data. (< 1.0 ? 10?12) means the threshold
probability of the filtering is 1.0 ? 10?12.
Recall Precision F-score Memory Usage (MB) Training Time (s)
Small Training Data = 2000 sentences
Without filtering 65.77 72.80 69.10 4238 7463
Filtering (< 1.0 ? 10.0?12) 64.22 70.62 67.27 600 1080
Filtering (< 1.0 ? 10.0?15) 65.34 72.52 68.74 870 2154
All Training Data = 16713 sentences
Without filtering Not available Not available
Filtering (< 1.0 ? 10.0?12) 70.05 76.06 72.93 10444 14661
Filtering (< 1.0 ? 10.0?15) 72.09 78.47 75.14 15257 31636
Table 6: Overall performance on the evaluation set. L is the upper bound of the length of possible chunks
in semi-CRFs.
Recall Precision F-score
L < 5 64.33 65.51 64.92
L = 10 + Filtering (< 1.0 ? 10.0?12) 70.87 68.33 69.58
L = 10 + Filtering (< 1.0 ? 10.0?15) 72.59 70.16 71.36
w/o Chunk Feature 70.53 69.92 70.22
+ Preceding Entity 72.65 70.35 71.48
tems in several respects, that is, the performance of
cell line is not good, and the performance of the
right boundary identification (78.91% in F-score)
is better than that of the left boundary identifica-
tion (75.19% in F-score).
Table 8 shows a comparison between our sys-
tem and other state-of-the-art systems. Our sys-
tem has achieved a comparable performance to
these systems and would be still improved by us-
ing external resources or conducting pre/post pro-
cessing. For example, Zhou et. al (2004) used
post processing, abbreviation resolution and exter-
nal dictionary, and reported that they improved F-
score by 3.1%, 2.1% and 1.2% respectively. Kim
et. al (2005) used the original GENIA corpus
to employ the information about other semantic
classes for identifying term boundaries. Finkel
et. al (2004) used gazetteers, web-querying, sur-
rounding abstracts, and frequency counts from
the BNC corpus. Settles (2004) used seman-
tic domain knowledge of 17 types of lexicon.
Since our approach and the use of external re-
sources/knowledge do not conflict but are com-
plementary, examining the combination of those
techniques should be an interesting research topic.
Table 7: Performance of our system on the evalu-
ation set
Class Recall Precision F-score
protein 77.74 68.92 73.07
DNA 69.03 70.16 69.59
RNA 69.49 67.21 68.33
cell type 65.33 82.19 72.80
cell line 57.60 53.14 55.28
overall 72.65 70.35 71.48
Table 8: Comparison with other systems
System Recall Precision F-score
Zhou et. al (2004) 75.99 69.42 72.55
Our system 72.65 70.35 71.48
Kim et.al (2005) 72.77 69.68 71.19
Finkel et. al (2004) 68.56 71.62 70.06
Settles (2004) 70.3 69.3 69.8
471
6 Conclusion
In this paper, we have proposed a single proba-
bilistic model that can capture important charac-
teristics of biomedical named entities. To over-
come the prohibitive computational cost, we have
presented an efficient training framework and a fil-
tering method which enabled us to apply first or-
der semi-CRF models to sentences having many
labels and entities with long names. Our results
showed that our filtering method works very well
without decreasing the overall performance. Our
system achieved an F-score of 71.48% without the
use of gazetteers, post-processing or external re-
sources. The performance of our system came
close to that of the current best performing system
which makes extensive use of external resources
and rule based post-processing.
The contribution of the non-local information
introduced by our method was not significant in
the experiments. However, other types of non-
local information have also been shown to be ef-
fective (Finkel et al, 2005) and we will examine
the effectiveness of other non-local information
which can be embedded into label information.
As the next stage of our research, we hope to ap-
ply our method to shallow parsing, in which seg-
ments tend to be long and non-local information is
important.
References
Daniel M. Bikel, Richard Schwartz, and Ralph
Weischedel. 1997. Nymble: a high-performance
learning name-finder. In Proc. of the Fifth Confer-
ence on Applied Natural Language Processing.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malv-
ina Nissim, Gail Sinclair, and Christopher Man-
ning. 2004. Exploiting context for biomedical en-
tity recognition: From syntax to the web. In Proc. of
JNLPBA-04.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proc. of ACL 2005, pages 363?370.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proc. of JNLPBA-04, pages 70?75.
Seonho Kim, Juntae Yoon, Kyung-Mi Park, and Hae-
Chang Rim. 2005. Two-phase biomedical named
entity recognition using a hybrid method. In Proc. of
the Second International Joint Conference on Natu-
ral Language Processing (IJCNLP-05).
Zhenzhen Kou, William W. Cohen, and Robert F. Mur-
phy. 2005. High-recall protein entity recognition
using a dictionary. Bioinformatics 2005 21.
Micahel Krauthammer and Goran Nenadic. 2004.
Term identification in the biomedical literature. Jor-
nal of Biomedical Informatics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML 2001.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002.
Peshkin and Pfeffer. 2003. Bayesian information ex-
traction network. In IJCAI.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In NIPS 2004.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Proc. of JNLPBA-04.
Beth M. Sundheim. 1995. Overview of results of the
MUC-6 evaluation. In Sixth Message Understand-
ing Conference (MUC-6), pages 13?32.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. In ICML workshop on Sta-
tistical Relational Learning.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Chunk
parsing revisited. In Proceedings of the 9th Inter-
national Workshop on Parsing Technologies (IWPT
2005).
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recogni-
tion. In Proc. of JNLPBA-04.
472
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 73?80,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Discriminative Language Model with Pseudo-Negative Samples
Daisuke Okanohara  Jun?ichi Tsujii 
 Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
School of Informatics, University of Manchester
NaCTeM (National Center for Text Mining)
hillbig,tsujii@is.s.u-tokyo.ac.jp
Abstract
In this paper, we propose a novel discrim-
inative language model, which can be ap-
plied quite generally. Compared to the
well known N-gram language models, dis-
criminative language models can achieve
more accurate discrimination because they
can employ overlapping features and non-
local information. However, discriminative
language models have been used only for
re-ranking in specific applications because
negative examples are not available. We
propose sampling pseudo-negative examples
taken from probabilistic language models.
However, this approach requires prohibitive
computational cost if we are dealing with
quite a few features and training samples.
We tackle the problem by estimating the la-
tent information in sentences using a semi-
Markov class model, and then extracting
features from them. We also use an on-
line margin-based algorithm with efficient
kernel computation. Experimental results
show that pseudo-negative examples can be
treated as real negative examples and our
model can classify these sentences correctly.
1 Introduction
Language models (LMs) are fundamental tools for
many applications, such as speech recognition, ma-
chine translation and spelling correction. The goal
of LMs is to determine whether a sentence is correct
or incorrect in terms of grammars and pragmatics.
The most widely used LM is a probabilistic lan-
guage model (PLM), which assigns a probability to
a sentence or a word sequence. In particular, N-
grams with maximum likelihood estimation (NLMs)
are often used. Although NLMs are simple, they are
effective for many applications.
However, NLMs cannot determine correctness
of a sentence independently because the probabil-
ity depends on the length of the sentence and the
global frequencies of each word in it. For exam-
ple,   
 
    

, where    is the probability
of a sentence  given by an NLM, does not always
mean that 

is more correct, but instead could occur
when 

is shorter than 
 
, or if 

has more com-
mon words than 
 
. Another problem is that NLMs
cannot handle overlapping information or non-local
information easily, which is important for more ac-
curate sentence classification. For example, a NLM
could assign a high probability to a sentence even if
it does not have a verb.
Discriminative language models (DLMs) have
been proposed to classify sentences directly as cor-
rect or incorrect (Gao et al, 2005; Roark et al,
2007), and these models can handle both non-local
and overlapping information. However DLMs in
previous studies have been restricted to specific ap-
plications. Therefore the model cannot be used for
other applications. If we had negative examples
available, the models could be trained directly by
discriminating between correct and incorrect sen-
tences.
In this paper, we propose a generic DLM, which
can be used not only for specific applications, but
also more generally, similar to PLMs. To achieve
73
this goal, we need to solve two problems. The first
is that since we cannot obtain negative examples (in-
correct sentences), we need to generate them. The
second is the prohibitive computational cost because
the number of features and examples is very large. In
previous studies this problem did not arise because
the amount of training data was limited and they did
not use a combination of features, and thus the com-
putational cost was negligible.
To solve the first problem, we propose sampling
incorrect sentences taken from a PLM and then
training a model to discriminate between correct and
incorrect sentences. We call these examples Pseudo-
Negative because they are not actually negative sen-
tences. We call this method DLM-PN (DLM with
Pseudo-Negative samples).
To deal with the second problem, we employ an
online margin-based learning algorithm with fast
kernel computation. This enables us to employ com-
binations of features, which are important for dis-
crimination between correct and incorrect sentences.
We also estimate the latent information in sentences
by using a semi-Markov class model to extract fea-
tures. Although there are substantially fewer la-
tent features than explicit features such as words or
phrases, latent features contain essential information
for sentence classification.
Experimental results show that these pseudo-
negative samples can be treated as incorrect exam-
ples, and that DLM-PN can learn to correctly dis-
criminate between correct and incorrect sentences
and can therefore classify these sentences correctly.
2 Previous work
Probabilistic language models (PLMs) estimate the
probability of word strings or sentences. Among
these models, N-gram language models (NLMs) are
widely used. NLMs approximate the probability by
conditioning only on the preceding     words.
For example, let  denote a sentence of  words,
  
 
 

     
 
. Then, by the chain rule of
probability and the approximation, we have
      
 
 

     
 


 
  
  


  
     
  
 (1)
The parameters can be estimated using the maxi-
mum likelihood method.
Since the number of parameters in NLM is still
large, several smoothing methods are used (Chen
and Goodman, 1998) to produce more accurate
probabilities, and to assign nonzero probabilities to
any word string.
However, since the probabilities in NLMs depend
on the length of the sentence, two sentences of dif-
ferent length cannot be compared directly.
Recently, Whole Sentence Maximum Entropy
Models (Rosenfeld et al, 2001) (WSMEs) have
been introduced. They assign a probability to
each sentence using a maximum entropy model.
Although WSMEs can encode all features of a
sentence including non-local ones, they are only
slightly superior to NLMs, in that they have the dis-
advantage of being computationally expensive, and
not all relevant features can be included.
A discriminative language model (DLM) assigns
a score 	  to a sentence , measuring the correct-
ness of a sentence in terms of grammar and prag-
matics, so that 	  
  implies  is correct and
	    implies  is incorrect. A PLM can be
considered as a special case of a DLM by defining
	 using   . For example, we can take 	  
     , where  is some threshold, and 
is the length of .
Given a sentence , we extract a feature vector
( ) from it using a pre-defined set of feature
functions 



 
. The form of the function 	 we
use is
	        (2)
where   is a feature weighting vector.
Since there is no restriction in designing  ,
DLMs can make use of both over-lapping and non-
local information in . We estimate  using training
samples  

 

 for   , where 

  if 

is correct and 

   if 

is incorrect.
However, it is hard to obtain incorrect sentences
because only correct sentences are available from
the corpus. This problem was not an issue for previ-
ous studies because they were concerned with spe-
cific applications and therefore were able to obtain
real negative examples easily. For example, Roark
(2007) proposed a discriminative language model, in
which a model is trained so that a correct sentence
should have higher score than others. The differ-
ence between their approach and ours is that we do
not assume just one application. Moreover, they had
74
For i=1,2,...
Choose a word 

at random
according to the distribution
  


  
     
  

If 

 "end of a sentence"
Break
End End
Figure 1: Sample procedure for pseudo-negative ex-
amples taken from N-gram language models.
training sets consisting of one correct sentence and
many incorrect sentences, which were very similar
because they were generated by the same input. Our
framework does not assume any such training sets,
and we treat correct or incorrect examples indepen-
dently in training.
3 Discriminative Language Model with
Pseudo-Negative samples
We propose a novel discriminative language model;
a Discriminative Language Model with Pseudo-
Negative samples (DLM-PN). In this model,
pseudo-negative examples, which are all assumed to
be incorrect, are sampled from PLMs.
First a PLM is built using training data and then
examples, which are almost all negative, are sam-
pled independently from PLMs. DLMs are trained
using correct sentences from a corpus and negative
examples from a Pseudo-Negative generator.
An advantage of sampling is that as many nega-
tive examples can be collected as correct ones, and
a distinction can be clearly made between truly cor-
rect sentences and incorrect sentences, even though
the latter might be correct in a local sense.
For sampling, any PLMs can be used as long
as the model supports a sentence sampling proce-
dure. In this research we used NLMs with interpo-
lated smoothing because such models support effi-
cient sentence sampling. Figure 1 describes the sam-
pling procedure and figure 2 shows an example of a
pseudo-negative sentence.
Since the focus is on discriminating between cor-
rect sentences from a corpus and incorrect sentences
sampled from the NLM, DLM-PN may not able to
classify incorrect sentences that are not generated
from the NLM. However, this does not result in a se-
We know of no program, and animated
discussions about prospects for trade
barriers or regulations on the rules
of the game as a whole, and elements
of decoration of this peanut-shaped
to priorities tasks across both target
countries
Figure 2: Example of a sentence sampled by PLMs
(Trigram).
Corpus
Build a probabilistic language model
Sample sentences
Positive (Pseudo-) Negative
Binary Classifier
test sentences
Return positive/negative label or score (margin)
Input training examples
Probabilistic LM
(e.g. N-gram LM)
Figure 3: Framework of our classification process.
rious problem, because these sentences, if they exist,
can be filtered out by NLMs.
4 Online margin-based learning with fast
kernel computation
The DLM-PN can be trained by using any binary
classification learning methods. However, since the
number of training examples is very large, batch
training has suffered from prohibitively large com-
putational cost in terms of time and memory. There-
fore we make use of an online learning algorithm
proposed by (Crammer et al, 2006), which has a
much smaller computational cost. We follow the
definition in (Crammer et al, 2006).
The initiation vector  
 
is initialized to  and for
each round the algorithm observes a training exam-
ple 

  

 and predicts its label 

to be either
 or  . After the prediction is made, the true la-
bel 

is revealed and the algorithm suffers an instan-
taneous hinge-loss     

 

     

  

 


which reflects the degree to which its prediction was
wrong. If the prediction was wrong, the parameter
75
  is updated as
 
 
 
 


    



  (3)
subject to     

 

   and    (4)
where  is a slack term and  is a positive parameter
which controls the influence of the slack term on the
objective function. A large value of will result in a
more aggressive update step. This has a closed form
solution as
 
 
  

 





(5)
where 

 	


 

 

 
. As in SVMs, a fi-
nal weight vector can be represented as a kernel-
dependent combination of the stored training exam-
ples.
    








  (6)
Using this formulation the inner product can be re-
placed with a general Mercer kernel  

 such
as a polynomial kernel or a Gaussian kernel.
The combination of features, which can capture
correlation information, is important in DLMs. If
the kernel-trick (Taylor and Cristianini, 2004) is ap-
plied to online margin-based learning, a subset of
the observed examples, called the active set, needs
to be stored. However in contrast to the support set
in SVMs, an example is added to the active set every
time the online algorithm makes a prediction mis-
take or when its confidence in a prediction is inad-
equately low. Therefore the active set can increase
in size significantly and thus the total computational
cost becomes proportional to the square of the num-
ber of training examples. Since the number of train-
ing examples is very large, the computational cost is
prohibitive even if we apply the kernel trick.
The calculation of the inner product between two
examples can be done by intersection of the acti-
vated features in each example. This is similar to
a merge sort and can be executed in 