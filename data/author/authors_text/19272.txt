Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 551?556,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Optimizing Segmentation Strategies for Simultaneous Speech Translation
Yusuke Oda Graham Neubig Sakriani Sakti Tomoki Toda Satoshi Nakamura
Graduate School of Information Science
Nara Institute of Science and Technology
Takayama, Ikoma, Nara 630-0192, Japan
{oda.yusuke.on9, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp
Abstract
In this paper, we propose new algorithms
for learning segmentation strategies for si-
multaneous speech translation. In contrast
to previously proposed heuristic methods,
our method finds a segmentation that di-
rectly maximizes the performance of the
machine translation system. We describe
two methods based on greedy search and
dynamic programming that search for the
optimal segmentation strategy. An experi-
mental evaluation finds that our algorithm
is able to segment the input two to three
times more frequently than conventional
methods in terms of number of words,
while maintaining the same score of auto-
matic evaluation.
1
1 Introduction
The performance of speech translation systems
has greatly improved in the past several years,
and these systems are starting to find wide use in
a number of applications. Simultaneous speech
translation, which translates speech from the
source language into the target language in real
time, is one example of such an application. When
translating dialogue, the length of each utterance
will usually be short, so the system can simply
start the translation process when it detects the end
of an utterance. However, in the case of lectures,
for example, there is often no obvious boundary
between utterances. Thus, translation systems re-
quire a method of deciding the timing at which
to start the translation process. Using estimated
ends of sentences as the timing with which to start
translation, in the same way as a normal text trans-
lation, is a straightforward solution to this problem
(Matusov et al, 2006). However, this approach
1
The implementation is available at
http://odaemon.com/docs/codes/greedyseg.html.
impairs the simultaneity of translation because the
system needs to wait too long until the appearance
of a estimated sentence boundary. For this reason,
segmentation strategies, which separate the input
at appropriate positions other than end of the sen-
tence, have been studied.
A number of segmentation strategies for simul-
taneous speech translation have been proposed in
recent years. F?ugen et al (2007) and Bangalore et
al. (2012) propose using prosodic pauses in speech
recognition to denote segmentation boundaries,
but this method strongly depends on characteris-
tics of the speech, such as the speed of speaking.
There is also research on methods that depend on
linguistic or non-linguistic heuristics over recog-
nized text (Rangarajan Sridhar et al, 2013), and it
was found that a method that predicts the location
of commas or periods achieves the highest perfor-
mance. Methods have also been proposed using
the phrase table (Yarmohammadi et al, 2013) or
the right probability (RP) of phrases (Fujita et al,
2013), which indicates whether a phrase reorder-
ing occurs or not.
However, each of the previously mentioned
methods decides the segmentation on the basis
of heuristics, so the impact of each segmenta-
tion strategy on translation performance is not di-
rectly considered. In addition, the mean number
of words in the translation unit, which strongly af-
fects the delay of translation, cannot be directly
controlled by these methods.
2
In this paper, we propose new segmentation al-
gorithms that directly optimize translation perfor-
mance given the mean number of words in the
translation unit. Our approaches find appropri-
ate segmentation boundaries incrementally using
greedy search and dynamic programming. Each
boundary is selected to explicitly maximize trans-
2
The method using RP can decide relative frequency of
segmentation by changing a parameter, but guessing the
length of a translation unit from this parameter is not trivial.
551
lation accuracy as measured by BLEU or another
evaluation measure.
We evaluate our methods on a speech transla-
tion task, and we confirm that our approaches can
achieve translation units two to three times as fine-
grained as other methods, while maintaining the
same accuracy.
2 Optimization Framework
Our methods use the outputs of an existing ma-
chine translation system to learn a segmentation
strategy. We define F = {f
j
: 1 ? j ? N},
E = {e
j
: 1 ? j ? N} as a parallel corpus
of source and target language sentences used to
train the segmentation strategy. N represents the
number of sentences in the corpus. In this work,
we consider sub-sentential segmentation, where
the input is already separated into sentences, and
we want to further segment these sentences into
shorter units. In an actual speech translation sys-
tem, these sentence boundaries can be estimated
automatically using a method like the period es-
timation mentioned in Rangarajan Sridhar et al
(2013). We also assume the machine translation
system is defined by a function MT (f) that takes
a string of source words f as an argument and re-
turns the translation result
?
e.
3
We will introduce individual methods in the fol-
lowing sections, but all follow the general frame-
work shown below:
1. Decide the mean number of words ? and the
machine translation evaluation measure EV
as parameters of algorithm. We can use an
automatic evaluation measure such as BLEU
(Papineni et al, 2002) as EV . Then, we cal-
culate the number of sub-sentential segmen-
tation boundaries K that we will need to in-
sert into F to achieve an average segment
length ?:
K := max
(
0,
?
?
f?F |f |
?
?
?N
)
. (1)
2. Define S as a set of positions in F in which
we will insert segmentation boundaries. For
example, if we will segment the first sentence
after the third word and the third sentence af-
ter the fifth word, then S = {?1, 3? , ?3, 5?}.
3
In this work, we do not use the history of the language
model mentioned in Bangalore et al (2012). Considering this
information improves the MT performance and we plan to
include this in our approach in future work.
Figure 1: Concatenated translation MT (f ,S).
Based on this representation, choose K seg-
mentation boundaries in F to make the set
S
?
that maximizes an evaluation function ?
as below:
S
?
:= arg max
S?{S
?
:|S
?
|=K}
?(S;F , E , EV,MT ).
(2)
In this work, we define ? as the sum of the
evaluation measure for each parallel sentence
pair ?f
j
,e
j
?:
?(S) :=
N
?
j=1
EV (MT (f
j
,S), e
j
), (3)
where MT (f ,S) represents the concatena-
tion of all partial translations {MT (f
(n)
)}
given the segments S as shown in Figure 1.
Equation (3) indicates that we assume all
parallel sentences to be independent of each
other, and the evaluation measure is calcu-
lated for each sentence separately. This lo-
cality assumption eases efficient implementa-
tion of our algorithm, and can be realized us-
ing a sentence-level evaluation measure such
as BLEU+1 (Lin and Och, 2004).
3. Make a segmentation model M
S
?
by treating
the obtained segmentation boundaries S
?
as
positive labels, all other positions as negative
labels, and training a classifier to distinguish
between them. This classifier is used to de-
tect segmentation boundaries at test time.
Steps 1. and 3. of the above procedure are triv-
ial. In contrast, choosing a good segmentation ac-
cording to Equation (2) is difficult and the focus
of the rest of this paper. In order to exactly solve
Equation (2), we must perform brute-force search
over all possible segmentations unless we make
some assumptions about the relation between the
? yielded by different segmentations. However,
the number of possible segmentations is exponen-
tially large, so brute-force search is obviously in-
tractable. In the following sections, we propose 2
552
I ate lunch but she left
Segments already selected at the k-th iteration
? = 0.5 ? = 0.8
(k+1)-th segment
? = 0.7
Figure 2: Example of greedy search.
Algorithm 1 Greedy segmentation search
S
?
? ?
for k = 1 to K do
S
?
? S
?
?
{
arg max
s

?S
?
?(S
?
? {s})
}
end for
return S
?
methods that approximately search for a solution
to Equation (2).
2.1 Greedy Search
Our first approximation is a greedy algorithm that
selects segmentation boundaries one-by-one. In
this method, k already-selected boundaries are left
unchanged when deciding the (k+1)-th boundary.
We find the unselected boundary that maximizes ?
and add it to S:
S
k+1
= S
k
?
{
arg max
s

?S
k
?(S
k
? {s})
}
. (4)
Figure 2 shows an example of this process for a
single sentence, and Algorithm 1 shows the algo-
rithm for calculating K boundaries.
2.2 Greedy Search with Feature Grouping
and Dynamic Programming
The method described in the previous section
finds segments that achieve high translation per-
formance for the training data. However, because
the translation system MT and evaluation mea-
sureEV are both complex, the evaluation function
? includes a certain amount of noise. As a result,
the greedy algorithm that uses only ? may find a
segmentation that achieves high translation perfor-
mance in the training data by chance. However,
these segmentations will not generalize, reducing
the performance for other data.
We can assume that this problem can be solved
by selecting more consistent segmentations of the
training data. To achieve this, we introduce a con-
straint that all positions that have similar charac-
teristics must be selected at the same time. Specif-
ically, we first group all positions in the source
I ate lunch but she left
PRP VBD NN CC PRP VBD
I ate an apple and an orange
PRP VBD DT NN CC DT NN
WORD:
 POS:
WORD:
 POS:
Group
PRP+VBD
Group
NN+CC
Group
DT+NN
Figure 3: Grouping segments by POS bigrams.
sentences using features of the position, and intro-
duce a constraint that all positions with identical
features must be selected at the same time. Figure
3 shows an example of how this grouping works
when we use the POS bigram surrounding each
potential boundary as our feature set.
By introducing this constraint, we can expect
that features which have good performance over-
all will be selected, while features that have rela-
tively bad performance will not be selected even if
good performance is obtained when segmenting at
a specific location. In addition, because all posi-
tions can be classified as either segmented or not
by evaluating whether the corresponding feature is
in the learned feature set or not, it is not necessary
to train an additional classifier for the segmenta-
tion model when using this algorithm. In other
words, this constraint conducts a kind of feature
selection for greedy search.
In contrast to Algorithm 1, which only selected
one segmentation boundary at once, in our new
setting there are multiple positions selected at one
time. Thus, we need to update our search algo-
rithm to handle this setting. To do so, we use
dynamic programming (DP) together with greedy
search. Algorithm 2 shows ourGreedy+DP search
algorithm. Here, c(?;F) represents the number
of appearances of ? in the set of source sentences
F , and S(F ,?) represents the set of segments de-
fined by both F and the set of features ?.
The outer loop of the algorithm, like Greedy,
iterates over all S of size 1 to K. The inner loop
examines all features that appear exactly j times
in F , and measures the effect of adding them to
the best segmentation with (k ? j) boundaries.
2.3 Regularization by Feature Count
Even after we apply grouping by features, it
is likely that noise will still remain in the less
frequently-seen features. To avoid this problem,
we introduce regularization into the Greedy+DP
algorithm, with the evaluation function ? rewrit-
553
Algorithm 2 Greedy+DP segmentation search
?
0
? ?
for k = 1 to K do
for j = 0 to k ? 1 do
?
?
? {? : c(?;F) = k ? j ? ?
