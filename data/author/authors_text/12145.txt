Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697?704
Manchester, August 2008
Exploiting Constituent Dependencies for Tree Kernel-based Semantic 
Relation Extraction 
Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu   Peide Qian 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,gdzhou,kongfang,qmzhu,pdqian}@suda.edu.cn
Abstract
This paper proposes a new approach to 
dynamically determine the tree span for 
tree kernel-based semantic relation ex-
traction. It exploits constituent dependen-
cies to keep the nodes and their head 
children along the path connecting the 
two entities, while removing the noisy in-
formation from the syntactic parse tree, 
eventually leading to a dynamic syntactic 
parse tree. This paper also explores entity 
features and their combined features in a 
unified parse and semantic tree, which in-
tegrates both structured syntactic parse 
information and entity-related semantic 
information. Evaluation on the ACE 
RDC 2004 corpus shows that our dy-
namic syntactic parse tree outperforms all 
previous tree spans, and the composite 
kernel combining this tree kernel with a 
linear state-of-the-art feature-based ker-
nel, achieves the so far best performance. 
1 Introduction 
Information extraction is one of the key tasks in 
natural language processing. It attempts to iden-
tify relevant information from a large amount of 
natural language text documents. Of three sub-
tasks defined by the ACE program1, this paper 
focuses exclusively on Relation Detection and 
Characterization (RDC) task, which detects and 
classifies semantic relationships between prede-
fined types of entities in the ACE corpus. For 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
1 http://www.ldc.upenn.edu/Projects/ACE/ 
example, the sentence ?Microsoft Corp. is based 
in Redmond, WA? conveys the relation ?GPE-
AFF.Based? between ?Microsoft Corp.? [ORG] 
and ?Redmond? [GPE]. Due to limited accuracy 
in state-of-the-art syntactic and semantic parsing, 
reliably extracting semantic relationships be-
tween named entities in natural language docu-
ments is still a difficult, unresolved problem. 
In the literature, feature-based methods have 
dominated the research in semantic relation ex-
traction. Featured-based methods achieve prom-
ising performance and competitive efficiency by 
transforming a relation example into a set of syn-
tactic and semantic features, such as lexical 
knowledge, entity-related information, syntactic 
parse trees and deep semantic information. How-
ever, detailed research (Zhou et al, 2005) shows 
that it?s difficult to extract new effective features 
to further improve the extraction accuracy. 
Therefore, researchers turn to kernel-based 
methods, which avoids the burden of feature en-
gineering through computing the similarity of 
two discrete objects (e.g. parse trees) directly. 
From prior work (Zelenko et al, 2003; Culotta 
and Sorensen, 2004; Bunescu and Mooney, 2005) 
to current research (Zhang et al, 2006; Zhou et 
al., 2007), kernel methods have been showing 
more and more potential in relation extraction. 
The key problem for kernel methods on rela-
tion extraction is how to represent and capture 
the structured syntactic information inherent in 
relation instances. While kernel methods using 
the dependency tree (Culotta and Sorensen, 2004) 
and the shortest dependency path (Bunescu and 
Mooney, 2005) suffer from low recall perform-
ance, convolution tree kernels (Zhang et al, 2006; 
Zhou et al, 2007) over syntactic parse trees 
achieve comparable or even better performance 
than feature-based methods. 
However, there still exist two problems re-
garding currently widely used tree spans. Zhang 
et al (2006) discover that the Shortest Path-
697
enclosed Tree (SPT) achieves the best perform-
ance. Zhou et al (2007) further extend it to Con-
text-Sensitive Shortest Path-enclosed Tree (CS-
SPT), which dynamically includes necessary 
predicate-linked path information. One problem 
with both SPT and CS-SPT is that they may still 
contain unnecessary information. The other prob-
lem is that a considerable number of useful con-
text-sensitive information is also missing from 
SPT/CS-SPT, although CS-SPT includes some 
contextual information relating to predicate-
linked path. 
This paper proposes a new approach to dy-
namically determine the tree span for relation 
extraction by exploiting constituent dependencies 
to remove the noisy information, as well as keep 
the necessary information in the parse tree. Our 
motivation is to integrate dependency informa-
tion, which has been proven very useful to rela-
tion extraction, with the structured syntactic in-
formation to construct a concise and effective 
tree span specifically targeted for relation extrac-
tion. Moreover, we also explore interesting com-
bined entity features for relation extraction via a 
unified parse and semantic tree. 
The other sections in this paper are organized 
as follows. Previous work is first reviewed in 
Section 2. Then, Section 3 proposes a dynamic 
syntactic parse tree while the entity-related se-
mantic tree is described in Section 4. Evaluation 
on the ACE RDC corpus is given in Section 5. 
Finally, we conclude our work in Section 6. 
2 Related Work 
Due to space limitation, here we only review 
kernel-based methods used in relation extraction. 
For those interested in feature-based methods, 
please refer to Zhou et al (2005) for more details. 
Zelenko et al (2003) described a kernel be-
tween shallow parse trees to extract semantic 
relations, where a relation instance is trans-
formed into the least common sub-tree connect-
ing the two entity nodes. The kernel matches the 
nodes of two corresponding sub-trees from roots 
to leaf nodes recursively layer by layer in a top-
down manner. Their method shows successful 
results on two simple extraction tasks. Culotta 
and Sorensen (2004) proposed a slightly general-
ized version of this kernel between dependency 
trees, in which a successful match of two relation 
instances requires the nodes to be at the same 
layer and in the identical path starting from the 
roots to the current nodes. These strong con-
straints make their kernel yield high precision but 
very low recall on the ACE RDC 2003 corpus. 
Bunescu and Mooney (2005) develop a shortest 
path dependency tree kernel, which simply 
counts the number of common word classes at 
each node in the shortest paths between two enti-
ties in dependency trees. Similar to Culotta and 
Sorensen (2004), this method also suffers from 
high precision but low recall.
Zhang et al (2006) describe a convolution tree 
kernel (CTK, Collins and Duffy, 2001) to inves-
tigate various structured information for relation 
extraction and find that the Shortest Path-
enclosed Tree (SPT) achieves the F-measure of 
67.7 on the 7 relation types of the ACE RDC 
2004 corpus. One problem with SPT is that it 
loses the contextual information outside SPT, 
which is usually critical for relation extraction. 
Zhou et al (2007) point out that both SPT and 
the convolution tree kernel are context-free. They 
expand SPT to CS-SPT by dynamically includ-
ing necessary predicate-linked path information 
and extending the standard CTK to context-
sensitive CTK, obtaining the F-measure of 73.2 
on the 7 relation types of the ACE RDC 2004 
corpus. However, the CS-SPT only recovers part 
of contextual information and may contain noisy 
information as much as SPT. 
In order to fully utilize the advantages of fea-
ture-based methods and kernel-based methods, 
researchers turn to composite kernel methods. 
Zhao and Grishman (2005) define several fea-
ture-based composite kernels to capture diverse 
linguistic knowledge and achieve the F-measure 
of 70.4 on the 7 relation types in the ACE RDC 
2004 corpus. Zhang et al (2006) design a com-
posite kernel consisting of an entity linear kernel 
and a standard CTK, obtaining the F-measure of 
72.1 on the 7 relation types in the ACE RDC 
2004 corpus. Zhou et al (2007) describe a com-
posite kernel to integrate a context-sensitive 
CTK and a state-of-the-art linear kernel. It 
achieves the so far best F-measure of 75.8 on the 
7 relation types in the ACE RDC 2004 corpus. 
In this paper, we will further study how to dy-
namically determine a concise and effective tree 
span for a relation instance by exploiting con-
stituent dependencies inherent in the parse tree 
derivation. We also attempt to fully capture both 
the structured syntactic parse information and 
entity-related semantic information, especially 
combined entity features, via a unified parse and 
semantic tree. Finally, we validate the effective-
ness of a composite kernel for relation extraction, 
which combines a tree kernel and a linear kernel. 
698
3 Dynamic Syntactic Parse Tree 
This section discusses how to generate dynamic 
syntactic parse tree by employing constituent 
dependencies to overcome the problems existing 
in currently used tree spans. 
3.1 Constituent Dependencies in Parse Tree 
Zhang et al (2006) explore five kinds of tree 
spans and find that the Shortest Path-enclosed 
Tree (SPT) achieves the best performance. Zhou 
et al (2007) further propose Context-Sensitive 
SPT (CS-SPT), which can dynamically deter-
mine the tree span by extending the necessary 
predicate-linked path information outside SPT. 
However, the key problem of how to represent 
the structured syntactic parse tree is still partially 
resolved. As we indicate as follows, current tree 
spans suffer from two problems: 
(1) Both SPT and CS-SPT still contain unnec-
essary information. For example, in the sentence 
??bought one of town?s two meat-packing
plants?, the condensed information ?one of 
plants? is sufficient to determine ?DISC? rela-
tionship between the entities ?one? [FAC] and 
?plants? [FAC], while SPT/CS-SPT include the 
redundant underlined part. Therefore more un-
necessary information can be safely removed 
from SPT/CS-SPT. 
(2) CS-SPT only captures part of context-
sensitive information relating to predicate-linked 
structure (Zhou et al, 2007) and still loses much 
context-sensitive information. Let?s take the 
same example sentence ??bought one of town?s
two meat-packing plants?, where indeed there is 
no relationship between the entities ?one? [FAC] 
and ?town? [GPE]. Nevertheless, the information 
contained in SPT/CS-SPT (?one of town?) may 
easily lead to their relationship being misclassi-
fied as ?DISC?, which is beyond our expectation. 
Therefore the underlined part outside SPT/CS-
SPT should be recovered so as to differentiate it 
from positive instances. 
Since dependency plays a key role in many 
NLP problems such as syntactic parsing, seman-
tic role labeling as well as semantic relation ex-
traction, our motivation is to exploit dependency 
knowledge to distinguish the necessary evidence 
from the unnecessary information in the struc-
tured syntactic parse tree.  
On one hand, lexical or word-word depend-
ency indicates the relationship among words 
occurring in the same sentence, e.g. predicate-
argument dependency means that arguments are 
dependent on their target predicates, modifier-
head dependency means that modifiers are de-
pendent on their head words. This dependency 
relationship offers a very condensed representa-
tion of the information needed to assess the rela-
tionship in the forms of the dependency tree (Cu-
lotta and Sorensen, 2004) or the shortest depend-
ency path (Bunescu and Mooney, 2005) that in-
cludes both entities.
On the other hand, when the parse tree corre-
sponding to the sentence is derived using deriva-
tion rules from the bottom to the top, the word-
word dependencies extend upward, making a 
unique head child containing the head word for 
every non-terminal constituent. As indicated as 
follows, each CFG rule has the form: 
P? Ln?L1H R1?Rm
Here, P is the parent node, H is the head child of 
the rule, Ln?L1 and R1?Rm are left and right 
modifiers of H respectively, and both n and m
may be zero. In other words, the parent node P
depends on the head child H, this is what we call 
constituent dependency. Vice versa, we can also 
determine the head child of a constituent in terms 
of constituent dependency. Our hypothesis stipu-
lates that the contribution of the parse tree to es-
tablishing a relationship is almost exclusively 
concentrated in the path connecting the two enti-
ties, as well as the head children of constituent 
nodes along this path. 
3.2 Generation of Dynamic Syntactic Parse 
Tree
Starting from the Minimum Complete Tree 
(MCT, the complete sub-tree rooted by the near-
est common ancestor of the two entities under 
consideration) as the representation of each rela-
tion instance, along the path connecting two enti-
ties, the head child of every node is found ac-
cording to various constituent dependencies. 
Then the path nodes and their head children are 
kept while any other nodes are removed from the 
tree. Eventually we arrive at a tree called Dy-
namic Syntactic Parse Tree (DSPT), which is 
dynamically determined by constituent depend-
encies and only contains necessary information 
as expected. 
There exist a considerable number of constitu-
ent dependencies in CFG as described by Collins 
(2003). However, since our task is to extract the 
relationship between two named entities, our fo-
cus is on how to condense Noun-Phrases (NPs) 
and other useful constituents for relation extrac-
tion. Therefore constituent dependencies can be 
classified according to constituent types of the 
CFG rules: 
699
(1) Modification within base-NPs: base-NPs 
mean that they do not directly dominate an NP
themselves, unless the dominated NP is a posses-
sive NP. The noun phrase right above the entity
headword, whose mention type is nominal or 
name, can be categorized into this type. In this 
case, the entity headword is also the headword of 
the noun phrase, thus all the constituents before 
the headword are dependent on the headword,
and may be removed from the parse tree, while 
the headword and the constituents right after the 
headword remain unchanged. For example, in the 
sentence ??bought one of town?s two meat-
packing plants? as illustrated in Figure 1(a), the
constituents before the headword  ?plants? can 
be removed from the parse tree. In this way the
parse tree ?one of plants? could capture the
?DISC? relationship more concisely and pre-
cisely. Another interesting example is shown in 
Figure 1(b), where the base-NP of the second
entity ?town? is a possessive NP and there is no 
relationship between the entities ?one? and
?town? defined in the ACE corpus. For both SPT
and CS-SPT, this example would be condensed 
to ?one of town? and therefore easily misclassi-
fied as the ?DISC? relationship between the two 
entities. In the contrast, our DSPT can avoid this 
problem by keeping the constituent ??s? and the 
headword ?plants?.
(2) Modification to NPs: except base-NPs,
other modification to NPs can be classified into 
this type. Usually these NPs are recursive, mean-
ing that they contain another NP as their child. 
The CFG rules corresponding to these modifica-
tions may have the following forms:
NP? NP SBAR [relative clause]
NP? NP VP [reduced relative]
NP? NP PP [PP attachment]
Here, the NPs in bold mean that the path con-
necting the two entities passes through them. For
every right hand side, the NP in bold is modified
by the constituent following them. That is, the 
latter is dependent on the former, and may be 
reduced to a single NP. In Figure 1(c) we show a
sentence ?one of about 500 people nominated
for ??, where there exists a ?DISC? relationship
between the entities ?one? and ?people?. Since 
the reduced relative ?nominated for ?? modifies
and is therefore dependent on the ?people?, they 
can be removed from the parse tree, that is, the 
right side (?NP VP?) can be reduced to the left 
hand side, which is exactly a single NP. 
(a) Removal of constituents before the headword in base-NP
(b) Keeping of constituents after the headword in base-NP
NN
one
IN
of
DT
the
NN
town
POS
's
E-FAC
NN
plantstwo
CD NN
one
IN
of
NN
town
POS
's
E-FAC
NN
plantsmeat-packing
JJ
NN
one
PP
IN
of
NP
DT
the
NN
town
POS
's
NN
plantstwo
CD NN
one
IN
of
NN
plantsmeat-packing
JJ NN
one
IN
of
RB
about
QP
CD
500
NNS
people
...
nominated
VBN
for
IN
VP
PP
...
E2-PER
NN
one
IN
of
NNS
people
NN
property
PRP
he
VP
VBZ IN
in
NP
PP
state
NNS
the
NP
JJ
rental
S
owns
DT NN
property
PRP
he
VP
VBZ
owns
governors from connecticut
NNS IN
NP
E-GPE
NNP
,
,
south
NP
E-GPE
NNP
dakota
NNP
,
,
and
CC
montana
NNP
governors from
NNS IN
montana
NNP
(c) Reduction of modification to NP
(d) Removal of arguments to verb
(e) Reduction of conjuncts for NP coordination
E-GPE
NPPP
E1-FAC
NP
E2-FAC
NP
E1-FAC
NP
NP
NP
NP
E2-FAC E1-PER
NP
NP
PP
NP
NP
NP
E1-PER
PP
NP
E2-PER
NP
SBAR
E2-PER
S
NPNP
E1-FAC
PP
NP
NP
E1-PER
NP
E2-GPE
NP
E1-PER
PP
NP
NP
NP
E2-GPE
NP
E1-FAC E2-PER
NP
NP
SBAR
NP
NP
E1-FAC
PP
NP
NP
E2-GPE
NP
NP
E1-PER
PP
NP
NP
E2-GPE
Figure 1. Removal and reduction of constituents using dependencies 
700
(3) Arguments/adjuncts to verbs: this type 
includes the CFG rules in which the left side in-
cludes S, SBAR or VP. An argument represents
the subject or object of a verb, while an adjunct
indicates the location, date/time or way of the
action corresponding to the verb. They depend
on the verb and can be removed if they are not
included in the path connecting the two entities.
However, when the parent tag is S or SBAR, and
its child VP is not included in the path, this VP
should be recovered to indicate the predicate
verb. Figure 1(d) shows a sentence ?? maintain
rental property he owns in the state?, where the
?ART.User-or-Owner? relation holds between 
the entities ?property? and ?he?. While PP can be
removed from the rule  (?VP? VBZ PP?), the 
VP should be kept in the rule (?S? NP VP?).
Consequently, the tree span looks more concise 
and precise for relation extraction. 
(4) Coordination conjunctions: In coordina-
tion constructions, several peer conjuncts may be 
reduced into a single constituent. Although the
first conjunct is always considered as the head-
word (Collins, 2003), actually all the conjuncts
play an equal role in relation extraction. As illus-
trated in Figure 1(e), the NP coordination in the 
sentence (?governors from connecticut, south
dakota, and montana?) can be reduced to a single 
NP (?governors from montana?) by keeping the
conjunct in the path while removing the other 
conjuncts.
(5) Modification to other constituents: ex-
cept for the above four types, other CFG rules 
fall into this type, such as modification to PP,
ADVP and PRN etc. These cases are similar to 
arguments/adjuncts to verbs, but less frequent 
than them, so we will not detail this scenario. 
In fact, SPT (Zhang et al, 2006) can be ar-
rived at by carrying out part of the above re-
moval operations using a single rule (i.e. all the 
constituents outside the linking path should be
removed) and CS-CSPT (Zhou et al, 2007) fur-
ther recovers part of necessary context-sensitive 
information outside SPT, this justifies that SPT
performs well, while CS-SPT outperforms SPT. 
4 Entity-related Semantic Tree 
Entity semantic features, such as entity headword, 
entity type and subtype etc., impose a strong
constraint on relation types in terms of relation
definition by the ACE RDC task. Experiments by
Zhang et al (2006) show that linear kernel using 
only entity features contributes much when com-
bined with the convolution parse tree kernel. 
Qian et al (2007) further indicates that among
these entity features, entity type, subtype, and 
mention type, as well as the base form of predi-
cate verb, contribute most while the contribution
of other features, such as entity class, headword 
and GPE role, can be ignored. 
In order to effectively capture entity-related
semantic features, and their combined features as
well, especially bi-gram or tri-gram features, we 
build an Entity-related Semantic Tree (EST) in 
three ways as illustrated in Figure 2. In the ex-
ample sentence ?they ?re here?, which is ex-
cerpted from the ACE RDC 2004 corpus, there 
exists a relationship ?Physical.Located? between
the entities ?they? [PER] and ?here?
[GPE.Population-Center]. The features are en-
coded as ?TP?, ?ST?, ?MT? and ?PVB?, which
denote type, subtype, mention-type of the two 
entities, and the base form of predicate verb if 
existing (nearest to the 2nd entity along the path 
connecting the two entities) respectively. For 
example, the tag ?TP1? represents the type of the 
1st entity, and the tag ?ST2? represents the sub-
type of the 2nd entity. The three entity-related
semantic tree setups are depicted as follows: 
TP2TP1
(a) Bag Of Features(BOF)
ENT
ST2ST1 MT2MT1 PVB
(c) Entity-Paired Tree(EPT)
ENT
E1 E2
(b) Feature Paired Tree(FPT)
ENT
TP ST MT
ST1TP1 MT1 TP2 ST2 MT2
PVB
TP1 TP2 ST1 ST2 MT1 MT2
PVB
PER null PRO GPE Pop. PRO be
PER null PRO GPE Pop. PRO
be
PER GPE null Pop. PRO PRO
be
Figure 2. Different setups for entity-related se-
mantic tree (EST) 
(a) Bag of Features (BOF, e.g. Fig. 2(a)): all 
feature nodes uniformly hang under the root node,
so the tree kernel simply counts the number of 
common features between two relation instances.
This tree setup is similar to linear entity kernel
explored by Zhang et al (2006). 
(b) Feature-Paired Tree (FPT, e.g. Fig. 2(b)): 
the features of two entities are grouped into dif-
ferent types according to their feature names, e.g.
?TP1? and ?TP2? are grouped to ?TP?. This tree 
setup is aimed to capture the additional similarity
701
of the single feature combined from different 
entities, i.e., the first and the second entities. 
(c) Entity-Paired Tree (EPT, e.g. Fig. 2(c)): all 
the features relating to an entity are grouped to 
nodes ?E1? or ?E2?, thus this tree kernel can fur-
ther explore the equivalence of combined entity 
features only relating to one of the entities be-
tween two relation instances. 
In fact, the BOF only captures the individual 
entity features, while the FPT/EPT can addition-
ally capture the bi-gram/tri-gram features respec-
tively. 
Rather than constructing a composite kernel, 
we incorporate the EST into the DSPT to pro-
duce a Unified Parse and Semantic Tree (UPST) 
to investigate the contribution of the EST to rela-
tion extraction. The entity features can be at-
tached under the top node, the entity nodes, or 
directly combined with the entity nodes as in 
Figure 1. However, detailed evaluation (Qian et 
al., 2007) indicates that the UPST achieves the 
best performance when the feature nodes are at-
tached under the top node. Hence, we also attach 
three kinds of entity-related semantic trees (i.e. 
BOF, FPT and EPT) under the top node of the 
DSPT right after its original children. Thereafter, 
we employ the standard CTK (Collins and Duffy, 
2001) to compute the similarity between two 
UPSTs, since this CTK and its variations are 
successfully applied in syntactic parsing, seman-
tic role labeling (Moschitti, 2004) and relation 
extraction (Zhang et al, 2006; Zhou et al, 2007) 
as well. 
5 Experimentation 
This section will evaluate the effectiveness of the 
DSPT and the contribution of entity-related se-
mantic information through experiments. 
5.1 Experimental Setting  
For evaluation, we use the ACE RDC 2004 cor-
pus as the benchmark data. This data set contains 
451 documents and 5702 relation instances. It 
defines 7 entity types, 7 major relation types and 
23 subtypes. For comparison with previous work, 
evaluation is done on 347 (nwire/bnews) docu-
ments and 4307 relation instances using 5-fold 
cross-validation. Here, the corpus is parsed using 
Charniak?s parser (Charniak, 2001) and relation 
instances are generated by iterating over all pairs 
of entity mentions occurring in the same sentence 
with given ?true? mentions and coreferential in-
formation. In our experimentations, SVMlight
(Joachims, 1998) with the tree kernel function
(Moschitti, 2004) 2  is selected as our classifier. 
For efficiency, we apply the one vs. others
strategy, which builds K classifiers so as to 
separate one class from all others. For 
comparison purposes, the training parameters C 
(SVM) and ? (tree kernel) are also set to 2.4 and 
0.4 respectively. 
5.2 Experimental Results 
Table 1 evaluates the contributions of different 
kinds of constituent dependencies to extraction 
performance on the 7 relation types of the ACE 
RDC 2004 corpus using the convolution parse 
tree kernel as depicted in Figure 1. The MCT 
with only entity-type information is first used as 
the baseline, and various constituent dependen-
cies are then applied sequentially to dynamically 
reshaping the tree in two different modes: 
--[M1] Respective:  every constituent depend-
ency is individually applied on MCT. 
--[M2] Accumulative: every constituent de-
pendency is incrementally applied on the previ-
ously derived tree span, which begins with the 
MCT and eventually gives rise to a Dynamic 
Syntactic Parse Tree (DSPT).  
Dependency types P(%) R(%) F
MCT (baseline) 75.1 53.8 62.7
Modification within 
base-NPs
76.5
(59.8)
59.8
(59.8)
67.1
(67.1)
Modification to NPs 
77.0
(76.2)
63.2
(56.9)
69.4
(65.1)
Arguments/adjuncts to verb 
77.1
(76.1)
63.9
(57.5)
69.9
(65.5)
Coordination conjunctions 
77.3
(77.3)
65.2
(55.1)
70.8
(63.8)
Other modifications 
77.4
(75.0)
65.4
(53.7)
70.9
(62.6)
Table 1. Contribution of constituent dependen-
cies in respective mode (inside parentheses) and 
accumulative mode (outside parentheses) 
The table shows that the final DSPT achieves 
the best performance of 77.4%/65.4%/70.9 in
precision/recall/F-measure respectively after ap-
plying all the dependencies, with the increase of 
F-measure by 8.2 units compared to the baseline 
MCT. This indicates that reshaping the tree by 
exploiting constituent dependencies may signifi-
cantly improve extraction accuracy largely due to 
the increase in recall. It further suggests that con-
stituent dependencies knowledge is very effec-
2 http://ai-nlp.info.uniroma2.it/moschitti/ 
702
tive and can be fully utilized in tree kernel-based 
relation extraction. This table also shows that: 
(1) Both modification within base-NPs and 
modification to NPs contribute much to perform-
ance improvement, acquiring the increase of F-
measure by 4.4/2.4 units in mode M1 and 4.4/2.3 
units in mode M2 respectively. This indicates the 
local characteristic of semantic relations, which 
can be effectively captured by NPs near the two 
involved entities in the DSPT. 
(2) All the other three dependencies show mi-
nor contribution to performance enhancement, 
they improve the F-measure only by 2.8/0.9/-0.1 
units in mode M1 and 0.5/0.9/0.1 units in mode 
M2. This may be due to the reason that these de-
pendencies only remove the nodes far from the 
two entities. 
We compare in Table 2 the performance of 
Unified Parse and Semantic Trees with different 
kinds of Entity Semantic Tree setups using stan-
dard convolution tree kernel, while the SPT and 
DSPT with only entity-type information are 
listed for reference. It shows that: 
(1) All the three unified parse and semantic 
tree kernels significantly outperform the DSPT 
kernel, obtaining an average increase of ~4 units 
in F-measure. This means that they can effec-
tively capture both the structured syntactic in-
formation and the entity-related semantic fea-
tures.
(2) The Unified Parse and Semantic Tree with 
Feature-Paired Tree achieves the best perform-
ance of 80.1/70.7/75.1 in P/R/F respectively, 
with an increase of F-measure by 0.4/0.3 units 
over BOF and EPT respectively. This suggests 
that additional bi-gram entity features captured 
by FPT are more useful than tri-gram entity fea-
tures captured by EPT. 
Tree setups P(%) R(%) F
SPT 76.3 59.8 67.1
DSPT 77.4 65.4 70.9
UPST (BOF) 80.4 69.7 74.7
UPST (FPT) 80.1 70.7 75.1
UPST (EPT) 79.9 70.2 74.8
Table 2. Performance of Unified Parse and 
Semantic Trees (UPSTs) on the 7 relation types 
of the ACE RDC 2004 corpus 
In Table 3 we summarize the improvements of 
different tree setups over SPT. It shows that in a 
similar setting, our DSPT outperforms SPT by 
3.8 units in F-measure, while CS-SPT outper-
forms SPT by 1.3 units in F-measure. This sug-
gests that the DSPT performs best among these 
tree spans. It also shows that the Unified Parse 
and Semantic Tree with Feature-Paired Tree per-
form significantly better than the other two tree 
setups (i.e., CS-SPT and DSPT) by 6.7/4.2 units 
in F-measure respectively. This implies that the 
entity-related semantic information is very useful 
and contributes much when they are incorporated 
into the parse tree for relation extraction. 
Tree setups P(%) R(%) F
CS-SPT over SPT3 1.5   1.1 1.3
DSPT over SPT 1.1   5.6 3.8
UPST (FPT) over SPT 3.8 10.9 8.0
Table 3. Improvements of different tree setups 
over SPT on the ACE RDC 2004 corpus 
Finally, Table 4 compares our system with 
other state-of-the-art kernel-based systems on the 
7 relation types of the ACE RDC 2004 corpus. It 
shows that our UPST outperforms all previous 
tree setups using one single kernel, and even bet-
ter than two previous composite kernels (Zhang 
et al, 2006; Zhao and Grishman, 2005). Fur-
thermore, when the UPST (FPT) kernel is com-
bined with a linear state-of-the-state feature-
based kernel (Zhou et al, 2005) into a composite 
one via polynomial interpolation in a setting 
similar to Zhou et al (2007) (i.e. polynomial de-
gree d=2 and coefficient ?=0.3), we get the so far 
best performance of 77.1 in F-measure for 7 rela-
tion types on the ACE RDC 2004 data set. 
Systems P(%) R(%) F
Ours:
composite kernel 
83.0 72.0 77.1
Zhou et al, (2007):
composite kernel 
82.2 70.2 75.8
Zhang et al, (2006):
composite kernel 
76.1 68.4 72.1
Zhao and Grishman, (2005):4
composite kernel 
69.2 70.5 70.4
Ours:
CTK with UPST 
80.1 70.7 75.1
Zhou et al, (2007): context-
sensitive CTK with CS-SPT 
81.1 66.7 73.2
Zhang et al, (2006):
CTK with SPT 
74.1 62.4 67.7
Table 4. Comparison of different systems on 
the ACE RDC 2004 corpus 
3  We arrive at these values by subtracting P/R/F 
(79.6/5.6/71.9) of Shortest-enclosed Path Tree from P/R/F  
(81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest-
enclosed Path Tree according to Table 2 (Zhou et al, 2007) 
4 There might be some typing errors for the performance 
reported in Zhao and Grishman (2005) since P, R and F do 
not match. 
703
6 Conclusion
This paper further explores the potential of struc-
tured syntactic information for tree kernel-based 
relation extraction, and proposes a new approach 
to dynamically determine the tree span (DSPT) 
for relation instances by exploiting constituent 
dependencies. We also investigate different ways 
of how entity-related semantic features and their 
combined features can be effectively captured in 
a Unified Parse and Semantic Tree (UPST). 
Evaluation on the ACE RDC 2004 corpus shows 
that our DSPT is appropriate for structured repre-
sentation of relation instances. We also find that, 
in addition to individual entity features, com-
bined entity features (especially bi-gram) con-
tribute much when they are combined with a 
DPST into a UPST. And the composite kernel, 
combining the UPST kernel and a linear state-of-
the-art kernel, yields the so far best performance. 
For the future work, we will focus on improv-
ing performance of complex structured parse 
trees, where the path connecting the two entities 
involved in a relationship is too long for current 
kernel methods to take effect. Our preliminary 
experiment of applying certain discourse theory 
exhibits certain positive results.
Acknowledgements 
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of China, Project 2006AA01Z147 under the 
?863? National High-Tech Research and Devel-
opment of China, and the National Research 
Foundation for the Doctoral Program of Higher 
Education of China under Grant No. 
20060285008. We would also like to thank the 
excellent and insightful comments from the three 
anonymous reviewers. 
References
Bunescu, Razvan C. and Raymond J. Mooney. 2005. 
A Shortest Path Dependency Kernel for Relation 
Extraction. In Proceedings of the Human Language 
Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP-2005), pages 724-731. Vancover, B.C. 
Charniak, Eugene. 2001. Intermediate-head Parsing 
for Language Models. In Proceedings of the 39th 
Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2001), pages 116-123. 
Collins, Michael. 2003. Head-Driven Statistics Mod-
els for Natural Language Parsing. Computational 
linguistics, 29(4): 589-617. 
Collins, Michael and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
Neural Information Processing Systems (NIPS-
2001), pages 625-632. Cambridge, MA. 
Culotta, Aron and Jeffrey Sorensen. 2004. Depend-
ency tree kernels for relation extraction. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL-2004),
pages 423-439. Barcelona, Spain. 
Joachims, Thorsten. 1998. Text Categorization with 
Support Vector Machine: learning with many rele-
vant features. In Proceedings of the 10th European 
Conference on Machine Learning (ECML-1998),
pages 137-142. Chemnitz, Germany. 
Moschitti, Alessandro. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Proceed-
ings of the 42nd Annual Meeting of the Association 
of Computational Linguistics (ACL-2004). Barce-
lona, Spain. 
Qian, Longhua, Guodong Zhou, Qiaoming Zhu and 
Peide Qian. 2007. Relation Extraction using Con-
volution Tree Kernel Expanded with Entity Fea-
tures. In Proceedings of the 21st Pacific Asian 
Conference on Language, Information and Compu-
tation (PACLIC-21), pages 415-421. Seoul, Korea. 
Zelenko, Dmitry, Chinatsu Aone and Anthony Rich-
ardella. 2003. Kernel Methods for Relation Extrac-
tion. Journal of Machine Learning Research, 
3(2003): 1083-1106. 
Zhang, Min, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th Annual Meeting of the Association of Compu-
tational Linguistics (COLING/ACL-2006), pages 
825-832. Sydney, Australia. 
Zhao, Shubin and Ralph Grishman. 2005. Extracting 
relations with integrated information using kernel 
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-2005), pages 419-426. Ann Arbor, USA. 
Zhou, Guodong, Jian Su, Jie Zhang and Min Zhang. 
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-2005), pages 427-434. Ann Arbor, USA. 
Zhou, Guodong, Min Zhang, Donghong Ji and 
Qiaoming Zhu. 2007. Tree Kernel-based Relation 
Extraction with Context-Sensitive Structured Parse 
Tree Information. In Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural 
Language Learning (EMNLP/CoNLL-2007), pages 
728-736. Prague, Czech. 
704
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437?1445,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Semi-Supervised Learning for Semantic Relation Classification using 
Stratified Sampling Strategy 
 
Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,gdzhou,kongfang,qmzhu}@suda.edu.cn 
 
 
 
 
Abstract 
 
This paper presents a new approach to 
selecting the initial seed set using stratified 
sampling strategy in bootstrapping-based 
semi-supervised learning for semantic relation 
classification.  First, the training data is 
partitioned into several strata according to 
relation types/subtypes, then relation instances 
are randomly sampled from each stratum to 
form the initial seed set. We also investigate 
different augmentation strategies in iteratively 
adding reliable instances to the labeled set, and 
find that the bootstrapping procedure may stop 
at a reasonable point to significantly decrease 
the training time without degrading too much 
in performance. Experiments on the ACE 
RDC 2003 and 2004 corpora show the 
stratified sampling strategy contributes more 
than the bootstrapping procedure itself. This 
suggests that a proper sampling strategy is 
critical in semi-supervised learning. 
1 Introduction 
With the dramatic increase in the amount of 
textual information available in digital archives 
and the WWW, there has been growing interest 
in techniques for automatically extracting 
information from text documents. Information 
Extraction (IE) is such a technology that IE 
systems are expected to identify relevant 
information (usually of pre-defined types) from 
text documents in a certain domain and put them 
in a structured format. 
According to the scope of the NIST Automatic 
Content Extraction (ACE) program (ACE, 2000-
2007), current research in IE has three main 
objectives: Entity Detection and Tracking (EDT), 
Relation Detection and Characterization (RDC), 
and Event Detection and Characterization (EDC). 
This paper focuses on the ACE RDC subtask, 
where many machine learning methods have 
been proposed, including supervised methods 
(Miller et al, 2000; Zelenko et al, 2002; Culotta 
and Soresen, 2004; Kambhatla, 2004; Zhou et al, 
2005; Zhang et al, 2006; Qian et al, 2008), 
semi-supervised methods (Brin, 1998; Agichtein 
and Gravano, 2000; Zhang, 2004; Chen et al, 
2006; Zhou et al, 2008), and unsupervised 
methods (Hasegawa et al, 2004; Zhang et al, 
2005).  
Current work on semantic relation extraction 
task mainly uses supervised learning methods, 
since it achieves relatively better performance. 
However this method requires a large amount of 
manually labeled relation instances, which is 
both time-consuming and laborious. In the 
contrast, unsupervised methods do not need 
definitions of relation types and hand-tagged data, 
but it is difficult to evaluate their performance 
since there are no criteria for evaluation. 
Therefore, semi-supervised learning has received 
more and more attention, as it can balance the 
advantages and disadvantages between 
supervised and unsupervised methods. With the 
plenitude of unlabeled natural language data at 
hand, semi-supervised learning can significantly 
reduce the need for labeled data with only 
limited sacrifice in performance. Specifically, a 
bootstrapping algorithm chooses the unlabeled 
instances with the highest probability of being 
correctly labeled and use them to augment 
labeled training data iteratively.  
Although previous work (Yarowsky, 1995; 
Blum and Mitchell, 1998; Abney, 2000; Zhang, 
2004) has tackled the bootstrapping approach 
from both the theoretical and practical point of 
view, many key problems still remain unresolved, 
such as the selection of initial seed set. Since the 
size of the initial seed set is usually small (e.g. 
1437
100 instances), the imbalance of relation types or 
manifold structure (cluster structure) in it will 
severely weaken the strength of bootstrapping. 
Therefore, it is critical for a bootstrapping 
approach to select the most appropriate initial 
seed set. However, current systems (Zhang, 2004; 
Chen et al, 2006) use a randomly sampling 
strategy, which fails to explore the affinity nature 
among the training instances. Alternatively, 
Zhou et al (2008) bootstrap a set of weighted 
support vectors from both labeled and unlabeled 
data using SVM. Nevertheless, the initial labeled 
data is still randomly generated only to ensure 
that there are at least 5 instances for every 
relation subtype. 
This paper presents a new approach to 
selecting the initial seed set based on stratified 
sampling strategy in the bootstrapping procedure 
for semi-supervised semantic relation 
classification. The motivation behind the 
stratified sampling is that every relation type 
should be as much as possible represented in the 
initial seed set, thus leading to more instances 
with diverse structures being added to the labeled 
set. In addition, we also explore different 
strategies to augment reliably classified instances 
to the labeled data iteratively, and attempt to find 
a stoppage criterion for the iteration procedure to 
greatly decrease the training time, other than 
using up all the unlabeled set. 
The rest of this paper is organized as follows. 
First, Section 2 reviews related work on semi-
supervised relation extraction. Then we present 
an underlying supervised learner in Section 3. 
Section 4 details various key aspects of the 
bootstrapping procedure, including the stratified 
sampling strategy. Experimental results are 
reported in Section 5. Finally we conclude our 
work in Section 6. 
2 Related Work 
Within the realm of information extraction, 
currently there are several representative semi-
supervised learning systems for extracting 
relations between named entities. 
DIPRE (Dual Iterative Pattern Relation 
Expansion) (Brin, 1998) is a system based on 
bootstrapping that exploits the duality between 
patterns and relations to augment the target 
relation starting from a small sample. However, 
it only extracts simple relations such as (author, 
title) pairs from the WWW. Snowball (Agichtein 
and Gravano, 2000) is another bootstrapping-
based system that extracts relations from 
unstructured text. Snowball shares much in 
common with DIPRE, including the use of both 
the bootstrapping framework and the pattern 
matching approach to extract new unlabeled 
instances. Due to pattern matching techniques, 
their systems are hard to be adapted to the 
general problem of relation extraction. 
Zhang (2004) approaches the relation 
classification problem with bootstrapping on top 
of SVM. He uses various lexical and syntactic 
features in the BootProject algorithm based on 
random feature projection to extract top-level 
relation types in the ACE corpus. Evaluation 
shows that bootstrapping can alleviate the burden 
of hand annotations for supervised learning 
methods to a certain extent.  
Chen et al (2006) investigate a semi-
supervised learning algorithm based on label 
propagation for relation extraction, where labeled 
and unlabeled examples and their distances are 
represented as the nodes and the weights of 
edges respectively in a connected graph, then the 
label information is propagated from any vertex 
to nearby vertices through weighted edges 
iteratively, finally the labels of unlabeled 
examples are inferred after the propagation 
process converges.  
Zhou et al (2008) integrate the advantages of 
SVM bootstrapping in learning critical instances 
and label propagation in capturing the manifold 
structure in both the labeled and unlabeled data, 
by first bootstrapping a moderate number of 
weighted support vectors through a co-training 
procedure from all the available data, and then 
applying label propagation algorithm via the 
bootstrapped support vectors. 
However, in most current systems, the initial 
seed set is selected randomly such that they may 
not adequately represent the inherent structure of 
unseen examples, hence the power of 
bootstrapping may be severely weakened. 
This paper presents a simple yet effective 
approach to generate the initial seed set by 
applying the stratified sampling strategy, 
originated from statistics theory. Furthermore, 
we try to employ the same stratified strategy to 
augment the labeled set. Finally, we attempt to 
find a reasonable criterion to terminate the 
iteration process. 
3 Underlying Supervised Learning 
A semi-supervised learning system usually 
consists of two relevant components: an 
underlying supervised learner and a 
1438
bootstrapping algorithm on top of it. In this 
section we discuss the former, while the latter 
will be described in the following section.  
In this paper, we select Support Vector 
Machines (SVMs) as the underlying supervised 
classifier since it represents the state-of-the-art in 
the machine learning research community, and 
there are good implementations of the algorithm 
available. Specifically, we use LIBSVM (Chang 
et al, 2001), an effective tool for support vector 
classification, since it supports multi-class 
classification and provides probability estimation 
as well. 
For each pair of entity mentions, we extract 
and compute various lexical and syntactic 
features, as employed in a state-of-the-art 
relation extraction system (Zhou et al, 2005). 
(1) Words: According to their positions, four 
categories of words are considered: a) the words 
of both the mentions; b) the words between the 
two mentions; c) the words before M1; and d) 
the words after M2.  
(2) Entity type: This category of features 
concerns about the entity types of both the 
mentions. 
(3) Mention Level: This category of features 
considers the entity level of both the mentions. 
(4) Overlap: This category of features includes 
the number of other mentions and words between 
two mentions. Typically, the overlap features are 
usually combined with other features such as 
entity type and mention level. 
(5) Base phrase chunking: The base phrase 
chunking is proved to play an important role in 
semantic relation extraction. Most of the 
chunking features concern about the headwords 
of the phrases between the two mentions.  
In this paper, we do not employ any deep 
syntactic or semantic features (such as 
dependency tree, full parse tree etc.), since they 
contribute quite limited in relation extraction. 
4 Bootstrapping & Stratified Sampling 
We first present the self-bootstrapping algorithm, 
and then discuss several key problems on 
bootstrapping in the order of initial seed 
selection, augmentation of labeled data and 
stoppage criterion for iteration. 
4.1 Bootstrapping Algorithm 
Following Zhang (2004), we define a basic self-
bootstrapping strategy, which keeps augmenting 
the labeled data set with the models 
straightforwardly trained from previously 
available labeled data as follows: 
Require: labeled seed set L
Require: unlabeled data set U
Require: batch size S
Repeat
    Train a single classifier on L
    Run the classifier on U
    Find at most S instances in U that the classifier has
the highest prediction confidence
    Add them into L
Until: no data points available or the stoppage
condition is reached
Algorithm self-bootstrapping
Figure 1. Self-bootstrapping algorithm 
In order to measure the confidence of the 
classifier?s prediction, we compute the entropy 
of the label probability distribution that the 
classifier assigns to the class label on an example 
(the lower the entropy, the higher the confidence): 
log
n
i i
i
H p p= ??      (1) 
Where n denotes the total number of relation 
classes, and pi denotes the probability of current 
example being classified as the ith class.  
4.2 Stratified Sampling for Initial Seeds  
Normally, the number of available labeled 
instances is quite limited (usually less than 100 
instances) when the iterative bootstrapping 
procedure begins. If the distribution of the initial 
seed set fails to approximate the distribution of 
the test data, the augmented data generated from 
bootstrapping would not capture the essence of 
relation types, and the performance on the test 
set will significantly decrease even only after one 
or two rounds of iterations. Therefore, the 
selection of initial seed set plays an important 
role in bootstrapping-based semantic relation 
extraction. 
Sampling is a part of statistical practice 
concerned with the selection of individual 
observations, which is intended to yield some 
knowledge about a population of interest. When 
dealing with the task of semi-supervised 
semantic relation classification, the population is 
the training set of relation instances from the 
ACE RDC corpora. We compare two practical 
sampling strategies as follows: 
(1) Randomly sampling, which picks the initial 
seeds from the training data using a random 
scheme. Each element thus has an equal 
probability of selection, and the population is not 
1439
subdivided or partitioned. Currently, most work 
on semi-supervised relation extraction employs 
this method. However, since the size of the initial 
seed set is very small, they are not guaranteed to 
capture the statistical properties of the whole 
training data, let alne of the test data. 
(2) Stratified sampling. When the population 
embraces a number of distinct categories, 
stratified sampling (Neyman, 1934) can be 
applied to this case. First, the population can be 
organized by these categories into separate 
"strata", then a sample is selected within each 
"stratum" separately, and randomly. Generally, 
the sample size is normally proportional to the 
relative size of the strata. The main motivation 
for using a stratified sampling design is to ensure 
that particular groups within a population are 
adequately represented in the sample. 
It is well known that the number of the 
instances for each relation type in the ACE RDC 
corpora is greatly unbalanced  (Zhou et al, 2005) 
as shown in Table 1 for the ACE RDC 2004 
corpus. When the relation instances for a specific 
relation type occurs frequently in the initial seed 
set, the classifier will achieve good performance 
on this type, otherwise the classifier can hardly 
recognize them from the test set. In order for 
every type of relations to be properly represented, 
the stratified sampling strategy is applied to the 
seed selection procedure. 
Types Subtypes Train Test
Located 593 145
Near 70 17
PHYS 
Part-Whole 299 79
Business 134 39
Family 101 20
PER-SOC 
Other 44 11
Employ-Executive 388 101
Employ-Staff 427 112
Employ-Undetermined 66 12
Member-of-Group 152 39
Subsidiary 169 37
Partner 10 2
EMP-ORG 
Other 64 16
User-or-Owner 160 40
Inventor-or-Man. 8 1
ART 
Other 1 1
Ethnic 31 8
Ideology 39 9
OTHER-
AFF 
Other 43 11
Citizen-or-Resid. 226 47
Based-In 165 50
GPE-AFF 
Other 31 8
DISC  224 55
Total  3445 860
Table 1. Numbers of relations on the ACE RDC 
2004: break down by relation types and subtypes 
Figure 2 illustrates the stratified sampling 
strategy we use in bootstrapping, where RSET 
denotes the training set, V is the stratification 
variable, and SeedSET denotes the initial seed set. 
First, we divide the relation instances into 
different strata according to available properties, 
such as major relation type (considering reverse 
relations or not) and relation subtype 
(considering reverse relations or not). Then 
within every stratum, a certain number of 
instances are sampled randomly, and this number 
is normally proportional to the size of that 
stratum in the whole population. However, when 
this number is 0 due to the rounding of real 
numbers, it is set to 1. Also it must be ensured 
that the total number of instances being sampled 
is NS. Finally, these instances form the initial 
seed set and can be used as the input to the 
underlying supervised learning for the 
bootstrapping procedure. 
 
Require: RSET ={R1,R2,?,RN} 
Require: V = {v1, v2,?,vK} 
Require: SeedSET with the size of NS (100) 
Initialization: 
SeedSET = NULL 
Steps: 
z Group RSET into K strata according to the 
stratified variable V, i.e.:  
RSET={RSET1,RSET2,?,RSETK} 
z Calculate the class prior probability for each 
stratum i={1,2,?,K} 
)(/)( RSETNUMRSETNUMP ii =  
z Caculate the number of intances being sampled 
for each stratum 
NPN ii ?=  
If Ni =0 then Ni=1 
z Calculate the difference of numbers as follows: 
?
=
? ?=
K
i
iS NNN
1
 
z If N?>0 then add Ni (i=1,2,?,|N?|) by 1 
If N?<0 then subtract 1 from Ni (i=1,2,...,|N?|) 
z For each i from 1 to K 
Select Ni instances from RESTi randomly 
Add them into SeedSET 
 
Figure 2. Stratefied Sampling for initial seeds 
4.3 Augmentation of labeled data 
After each round of iteration, some newly 
classified instances with the highest confidence 
can be augmented to the labeled training data. 
Nevertheless, just like the selection of initial seed 
set, we still wish that every stratum would be 
represented as appropriately as possible in the 
1440
instances added to the labeled set. In this paper, 
we compare two kinds of augmentation strategies 
available: 
(1) Top n method: the classified instances are 
first sorted in the ascending order by their 
entropies (i.e. decreasing confidence), and then 
the top n (usually 100) instances are chosen to be 
added.  
(2) Stratified method: in order to make the 
added instances representative for their stratum, 
we first select m (usually greater than n) 
instances with the highest confidence, then we 
choose n instances from them using the stratified 
strategy. 
4.4 Stoppage of Iterations 
In a self-bootstrapping procedure, as the 
iterations go on, both the reliable and unreliable 
instances are added to the labeled data 
continuously, hence the performance will 
fluctuate in a relatively small range. The key 
question here is how we can know when the 
bootstrapping procedure reaches its best 
performance on the test data. The bootstrapping 
algorithm by Zhang (2004) stops after it runs out 
of all the training instances, which may take a 
relatively long time. In this paper, we present a 
method to determine the stoppage criterion based 
on the mean entropy as follows: 
Hi <= p    (2) 
Where Hi denotes the mean entropy of the 
confidently classified instances being augmented 
to the labeled data in each iteration, and p 
denotes a threshold for the mean entropy, which 
will be fixed through empirical experiments. 
This criterion is based on the assumption that 
when the mean entropy becomes less than or 
equal to a certain threshold, the classifier would 
achieve the most reliable confidence on the 
instances being added to the labeled set, and it 
may be impossible to yield better performance 
since then. Therefore, the iteration may stop at 
that reasonable point.  
5 Experimentation 
This section aims to empirically investigate the 
effectiveness of the bootstrapping-based semi-
supervised learning we discussed above for 
semantic relation classification. In particular, 
different methods for selecting the initial seed set 
and augmenting the labeled data are evaluated. 
5.1 Experimental Setting 
We use the ACE corpora as the benchmark data, 
which are gathered from various newspapers, 
newswire and broadcasts. The ACE 2004 corpus 
contains 451 documents and 5702 positive 
relation instances. It defines 7 relation types and 
23 subtypes between 7 entity types. For easy 
reference with related work in the literature, 
evaluation is also done on 347 documents 
(including nwire and bnews domains) and 4305 
relation instances using 5-fold cross-validation. 
That is, these relation instances are first divided 
into 5 sets, then, one of them (about 860 
instances) is used as the test data set, while the 
others are regarded as the training data set, from 
which the initial seed set is sampled. In the ACE 
2003 corpus, the training set consists of 674 
documents and 9683 positive relation instances 
while the test data consists of 97 documents and 
1386 positive relation instances. The ACE RDC 
2003 task defines 5 relation types and 24 
subtypes between 5 entity types. 
The corpora are first parsed using Collins?s 
parser (Collins, 2003) with the boundaries of all 
the entity mentions kept. Then, the parse trees 
are converted into chunklink format using 
chunklink.pl 1. Finally, various useful lexical and 
syntactic features, as described in Subsection 3.1, 
are extracted and computed accordingly. For the 
purpose of comparison, we define our task as the 
classification of the 5 or 7 major relation types in 
the ACE RDC 2003 and 2004 corpora. 
For LIBSVM parameters, we adopted the 
polynomial kernel, and c is set to 10, g is set to 
0.15. Under this setting, we achieved the best 
classification performance. 
5.2 Experimental Results 
In this subsection, we compare and discuss the 
experimental results using various sampling 
strategies, different augmentation methods, and 
iteration stoppage criterion. 
 
Comparison of sampling strategies in selecting 
the initial seed set 
Table 2 and Table 3 show the initial and the 
highest classification performance of 
Precision/Recall/F-measure for various sampling 
strategies of the initial seed set on 7 major 
relation types of the ACE RDC 2004 corpus 
respectively when the size of initial seed set L is 
100, the batch size S is 100, and the top 100 
                                                 
1 http://ilk.kub.nl/~sabine/chunklink/ 
1441
instances with the highest confidence are added 
at each iteration. Table 2 also lists the number of 
strata for stratified sampling methods from which 
the initial seeds are randomly chosen 
respectively. Table 3 additionally lists the time 
needed to complete the bootstrapping process (on 
a PC with a Pentium IV 3.0G CPU and 1G 
memory). In this paper, we consider the 
following five experimental settings when 
sampling the initial seeds: 
z Randomly Sampling: as described in 
Subsection 4.2. 
z Stratified-M Sampling: the strata are 
grouped in terms of major relation types 
without considering reverse relations. 
z Stratified-MR Sampling: the strata are 
grouped in terms of major relation types, 
including reverse relations. 
z Stratified-S Sampling: the strata are 
grouped in terms of relation subtypes 
without considering reverse subtypes. 
z Stratified-SR Sampling: the strata are 
grouped in terms of relation subtypes, 
including reverse subtypes. 
For each sampling strategies, we performed 20 
trials and computed average scores and the total 
time on the test set over these 20 trials. 
Sampling strategies 
for initial seeds 
# of 
strat. P(%) R(%) F 
Randomly 1 66.1 65.9 65.9
Stratified-M 7 69.1 66.5 67.7
Stratified-MR 13 69.3 67.3 68.2
Stratified-S 30 69.8 67.7 68.7
Stratified-SR 39 69.9 68.5 69.2
Table 2. The initial performance of applying 
various sampling strategies to selecting the initial 
seed set on the ACE RDC 2004 corpus 
Sampling strategies 
for initial seeds 
Time 
(min) P(%) R(%) F 
Randomly 52 68.6 66.2 67.3
Stratified-M 65 71.0 66.9 68.8
Stratified-MR 65 71.6 67.0 69.2
Stratified-S 71 72.7 67.8 70.1
Stratified-SR 77 72.9 68.4 70.6
Table 3. The highest performance of applying 
various sampling strategies in selecting the initial 
seed set on the ACE RDC 2004 corpus 
 
These two tables jointly indicate that the self-
bootstrapping procedure for all sampling 
strategies can moderately improve the 
classification performance by ~1.2 units in F-
score, which is also verified by Zhang (2004). 
Furthermore, they show that: 
z The most improvements in performance 
come from improvements in precision. Actually, 
for some settings the recalls even decrease 
slightly. The reason may be that due to the nature 
of self-bootstrapping, the instances augmented at 
each iteration are always those which are the 
most similar to the initial seed instances, 
therefore the models trained from them would 
exhibit higher precision on the test set, while it 
virtually does no help for recall. 
z All of the four stratified sampling methods 
outperform the randomly sampling method to 
various degrees, both in the initial performance 
and the highest performance. This means that 
sampling of the initial seed set based on 
stratification by major/sub relation types can be 
helpful to relation classification, largely due to 
the performance improvement of the initial seed 
set, which is caused by adequate representation 
of instances for every relation type. 
z Of all the four stratified sampling methods, 
the Stratified-SR sampling achieves the best 
performance of 72.9/68.4/70.6 in P/R/F. 
Moreover, the more the number of strata 
generated by the sampling strategy, the more 
appropriately they would be represented in the 
initial seed set, and the better performance it will 
yield. This also implies that the hierarchy of 
relation types/subtypes in the ACE RDC 2004 
corpus is fairly reasonably defined. 
z An important conclusion, which can be 
draw accordingly, is that the F-score 
improvement of Stratified-SR sampling over 
Randomly sampling in initial performance (3.3 
units) is significantly greater than the F-score 
improvement gained by bootstrapping itself 
using Randomly sampling (1.4 units). This means 
that the sampling strategy of the initial seed set is 
even more important than the bootstrapping 
algorithm itself for relation classification. 
z It is interesting to note that the time needed 
to bootstrap increases with the number of strata. 
The reason may be that due to more diverse 
structures in the labeled data for stratified 
sampling, the SVM needs more time to 
differentiate between instances, i.e. more time to 
learn the models. 
 
Comparison of different augmentation 
strategies of training data 
Figure 3 compares the performance of F-score 
for two augmentation strategies: the Top n 
method and the stratified method, over various 
initial seed sampling strategies on the ACE RDC 
2004 corpus. For each iteration, a variable 
1442
number (m is ranged from 100 to 500) of 
classified instances in the decreasing order of 
confidence are first chosen as the base examples, 
then at most 100 examples are selected from the 
base examples to be augmented to the labeled set. 
Specifically, when m is equal to 100, the whole 
set of the base example is added to the labeled 
data, i.e. degenerated to the Top n augmentation 
strategy. On the other hand, when m is greater 
than 100, we wish we would select examples of 
different major relation types from the base 
examples according to their distribution in the 
training set, in order to achieve the performance 
improvement as much as the stratified sampling 
does in the selection of the initial seed set. 
64
65
66
67
68
69
70
71
72
100 200 300 400 500
# Base examples
F-
sc
or
e
Randomly
Stratified-M 
Stratified-MR
Stratified-S
Stratified-SR
Figure 3. Comparison of two augmentation 
strategies over different sampling strategies in 
selecting the initial seed set. 
This figure shows that, except for randomly 
sampling strategy, the stratified augmentation 
strategies improve the performance. Nevertheless, 
this result is far from our expectation in two 
ways: 
z The performance improvement in F-score is 
trivial, at most 0.4 units on average. The reason 
may be that, although we try to add as many as 
100 classified instances to the labeled data 
according to the distribution of every major 
relation type in the training set, the top m 
instances with the highest confidence are usually 
focused on certain relation types (e.g. PHSY and 
PER-SOC), this leads to the stratified 
augmentation failing to function effectively. 
Hence, all the following experiments will only 
adopt Top n method for augmenting the labeled 
data. 
z With the increase of the number of the base 
examples, the performance fluctuates slightly, 
thus it is relatively difficult to recognize where 
the optima is. We think there are two 
contradictory factors that affect the performance. 
While the reliability of the instances extracted 
from the base examples decreases with the 
increase of the number of base examples, the 
probability of extracting instances of more 
relation types increases with the increase of the 
number of the base examples. These two factors 
inversely interact with each other, leading to the 
fluctuation in performance. 
 
Comparison of different threshold values for 
stoppage criterion 
We compare the performance and 
bootstrapping time (20 trials with the same initial 
seed set) when applying stoppage criterion in 
Formula (2) with different threshold p over 
various sampling strategies on the ACE RDC 
2004 corpus in Figure 4 and Figure 5 
respectively. These two figures jointly show that: 
64
65
66
67
68
69
70
71
0 0.2 0.22 0.24 0.26 0.28 0.3
p
F-
sc
or
e
Randomly
Stratified-M
Stratified-MR
Stratified-S
Stratified-SR
Figure 4. Performance for different p values 
0
10
20
30
40
50
60
70
80
90
0 0.2 0.22 0.24 0.26 0.28 0.3
p
Ti
m
e(
mi
n)
Randomly
Stratified-M
Stratified-MR
Stratified-S
Stratified-SR
Figure 5. Bootstrapping time for different p 
values 
z The performance decreases slowly while the 
bootstrapping time decreases dramatically with 
the increase of p from 0 to 0.3. Specifically, 
when the p equals to 0.3, the bootstrapping time 
tends to be neglected, while the performance is 
almost similar to the initial performance. It 
implies that we can find a reasonable point for 
each sampling strategy, at which the time falls 
greatly while the performance nearly does not 
degrade.  
1443
Bootproject LP-js Stratified Bootstrapping Relation types 
P R F P R F P R F 
ROLE 78.5 69.7 73.8 81.0 74.7 77.7 74.7 86.3 80.1
PART 65.6 34.1 44.9 70.1 41.6 52.2 66.4 47.0 55.0
AT 61.0 84.8 70.9 74.2 79.1 76.6 74.9 66.1 70.2
NEAR - - - 13.7 12.5 13.0 100.0 2.9 5.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0 65.2 79.0 71.4
Average 67.9 67.4 67.6 73.6 69.4 70.9 73.8 73.3 73.5
Table 4. Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpus 
 
z Clearly, if the performance is the primary 
concern, then p=0.2 may be the best choice in 
that we can get ~30% saving on the time at the 
cost of only ~0.08 loss in F-score on average. If 
the time is a primary concern, then p=0.22 is a 
reasonable threshold in that we get ~50% saving 
on the time at the cost of ~0.25 units loss in F-
score on average. This suggests that our 
proposed stoppage criterion is effective to 
terminate the bootstrapping procedure with 
minor performance loss. 
 
Comparison of Stratified Bootstrapping with 
Bootproject and Label propagation  
Table 4 compares Bootproject (Zhang, 2004), 
Label propagation (Chen et al, 2006) with our 
Stratified Bootstrapping on the 5 major types of 
the ACE RDC 2003 corpus. 
Both Bootproject and Label propagation 
select 100 initial instances randomly, and at each 
iteration, the top 100 instances with the highest 
confidence are added to the labeled data. 
Differently, we choose 100 initial seeds using 
stratified sampling strategy; similarly, the top 
100 instances with the highest confidence are 
augmented to the labeled data at each iteration. 
Due to the lack of comparability followed from 
the different size of the labeled data used in 
(Zhou et al, 2008), we omit their results here. 
This table shows that our stratified 
bootstrapping procedure significantly 
outperforms both Bootproject and Label 
Propagation methods on the ACE RDC corpus, 
with the increase of 5.9/4.1 units in F-score on 
average respectively. Stratified bootstrapping 
consistently outperforms Bootproject in every 
major relation type, while it outperforms Label 
Propagation in three of the major relation types, 
especially SOC type, with the exception of AT 
and NEAR types. The reasons may be follows. 
Although there are many AT relation instances in 
the corpus, they are scattered divergently in 
multi-dimension space so that they tend to be 
relatively difficult to be recognized via SVM. 
For the NEAR relation instances, they occur least 
frequently in the whole corpus, so it is very hard 
for them to be identified via SVM. By contrast, 
even small size of labeled instances can be fully 
utilized to correctly induce the unlabeled 
instances via LP algorithm due to its ability to 
exploit manifold structures of both labeled and 
unlabeled instances (Chen et al, 2006). 
In general, these results again suggest that the 
sampling strategy in selecting the initial seed set 
plays a critical role for relation classification, and 
stratified sampling can significantly improve the 
performance due to proper selection of the initial 
seed set. 
6 Conclusion 
This paper explores several key issues in semi-
supervised learning based on bootstrapping for 
semantic relation classification. The application 
of stratified sampling originated from statistics 
theory to the selection of the initial seed set 
contributes most to the performance 
improvement in the bootstrapping procedure. In 
addition, the more strata the training data is 
divided into, the better performance will be 
achieved. However, the augmentation of the 
labeled data using the stratified strategy fails to 
function effectively largely due to the 
unbalanced distribution of the confidently 
classified instances, rather than the stratified 
sampling strategy itself. Furthermore, we also 
propose a mean entropy-based stoppage criterion 
in the bootstrapping procedure, which can 
significantly decrease the training time with little 
loss in performance. Finally, it also shows that 
our method outperforms other state-of-the-art 
semi-supervised ones. 
 
Acknowledgments 
This research is supported by Project 60673041 
and 60873150 under the National Natural 
Science Foundation of China, Project 
2006AA01Z147 under the ?863? National High-
Tech Research and Development of China, 
1444
Project BK2008160 under the Jiangsu Natural 
Science Foundation of China, and the National 
Research Foundation for the Doctoral Program 
of Higher Education of China under Grant No. 
20060285008. We would also like to thank the 
excellent and insightful comments from the three 
anonymous reviewers. 
References  
S. Abney. Bootstrapping. 2002. In Proceedings of the 
40th Annual Meeting of the Association for 
Computational  Linguistics (ACL 2002). 
ACE 2002-2007. The Automatic Content Extraction 
(ACE) Projects. 2007. http//www.ldc.upenn.edu/ 
Projects/ACE/. 
E. Agichtein and L. Gravano. 2000. Snowball: 
Extracting relations from large plain-text 
collections. In Proceedings of the 5th ACM 
international Conference on Digital Libraries 
(ACMDL 2000). 
A. Blum and T. Mitchell. 1996. Combining labeled 
and unlabeled data with co-training. In COLT: 
Proceedings of the workshop on Computational 
Learning Theory. Morgan Kaufmann Publishers. 
S. Brin. 1998. Extracting patterns and relations from 
the world wide web. In WebDB Workshop at 6th 
International Conference on Extending Database 
Technology (EDBT 98). 
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library 
for support vector machines. http:// 
www.csie.ntu.edu.tw/~cjlin/libsvm. 
M. Collins. 2003. Head-Driven Statistics Models for 
Natural Language Parsing. Computational 
linguistics, 29(4): 589-617. 
J.X. Chen, D.H. Ji, and L.T. Chew. 2006. Relation 
Extraction using Label Propagation Based Semi 
supervised Learning. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th Annual Meeting of the 
Association of Computational Linguistics 
(COLING/ACL 2006), pages 129-136. July 2006, 
Sydney, Australia.  
A. Culotta and J. Sorensen. 2004. Dependency tree 
kernels for relation extraction. In Proceedings of 
the 42nd Annual Meeting of the Association of 
Computational Linguistics (ACL 2004), pages 423-
439. 21-26 July 2004, Barcelona, Spain. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. 
Discovering Relations among Named Entities from 
Large Corpora. In Proceedings of the 42nd Annual 
Meeting of the Association of Computational 
Linguistics (ACL 2004). 21-26 July 2004, 
Barcelona, Spain. 
N. Kambhatla. Combining lexical, syntactic and 
semantic features with Maximum Entropy models 
for extracting relations. In Proceedings of the 42nd 
Annual Meeting of the Association of 
Computational Linguistics (ACL 2004)(posters), 
pages 178-181. 21-26 July 2004, Barcelona, Spain. 
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 
2000. A novel use of statistical parsing to extract 
information from text. In Proceedings of the 6th 
Applied Natural Language Processing Conference. 
29 April-4 May 2000, Seattle, USA. 
J. Neyman. 1934. On the Two Different Aspects of 
the Representative Method: The Method of 
Stratified Sampling and the Method of Purposive 
Selection. Journal of the Royal Statistical Society, 
97(4): 558-625. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D Qian. 2008. 
Exploiting constituent dependencies for tree 
kernel-based semantic relation extraction. In 
Proceedings of The 22nd International Conference 
on Computational Linguistics (COLING 2008), 
pages 697-704. 18-22 August 2008, Manchester, 
UK. 
D. Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In the 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL 
95), pages 189-196. 26-30 June 1995, MIT, 
Cambridge, Massachusetts, USA. 
D. Zelenko, C. Aone, and A. Richardella. 2003. 
Kernel Methods for Relation Extraction. Journal of 
Machine Learning Research, (2): 1083-1106. 
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th Annual 
Meeting of the Association of Computational 
Linguistics (COLING/ACL 2006), pages 825-832. 
Sydney, Australia. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree 
Similarity-Based Clustering. In Proceedings of the 
2nd international Joint Conference on Natural 
Language Processing (IJCNLP-2005), pages 378-
389. Jeju Island, Korea.  
Z. Zhang. 2004. Weakly-supervised relation 
classification for Information Extraction. In 
Proceedings of ACM 13th conference on 
Information and Knowledge Management (CIKM 
2004). 8-13 Nov 2004, Washington D.C., USA. 
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. 
Exploring various knowledge in relation extraction. 
In Proceedings of the 43rd Annual Meeting of the 
Association of Computational Linguistics (ACL 
2005), pages 427-434. Ann Arbor, USA. 
G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008. 
Semi-Supervised Learning for Relation Extraction. 
In Proceedings of the 3rd International Joint 
Conference on Natural Language Processing 
(IJCNLP-2008), page 32-38. 7-12 January 2008, 
Hyderabad, India. 
 
1445
Semi-Supervised Learning for Relation Extraction  
 
ZHOU GuoDong    LI JunHui    QIAN LongHua    ZHU Qiaoming 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow Univ., Suzhou, China 215006 
Email : {gdzhou, lijunhui, qianlonghua, qmzhu}@suda.edu.cn 
 
Abstract 
This paper proposes a semi-supervised learn-
ing method for relation extraction. Given a 
small amount of labeled data and a large 
amount of unlabeled data, it first bootstraps a 
moderate number of weighted support vectors 
via SVM through a co-training procedure with 
random feature projection and then applies a 
label propagation (LP) algorithm via the boot-
strapped support vectors. Evaluation on the 
ACE RDC 2003 corpus shows that our method 
outperforms the normal LP algorithm via all 
the available labeled data without SVM boot-
strapping. Moreover, our method can largely 
reduce the computational burden. This sug-
gests that our proposed method can integrate 
the advantages of both SVM bootstrapping 
and label propagation.  
1 Introduction 
Relation extraction is to detect and classify various 
predefined semantic relations between two entities 
from text and can be very useful in many NLP ap-
plications such as question answering, e.g. to an-
swer the query ?Who is the president of the United 
States??, and information retrieval, e.g. to expand 
the query ?George W. Bush? with ?the president of 
the United States? via his relationship with ?the 
United States?. 
During the last decade, many methods have 
been proposed in relation extraction, such as su-
pervised learning (Miller et al2000; Zelenko et al
2003; Culota and Sorensen 2004; Zhao and Grish-
man 2005; Zhang et al2006; Zhou et al2005, 
2006), semi-supervised learning (Brin 1998; 
Agichtein and Gravano 2000; Zhang 2004; Chen et 
al 2006), and unsupervised learning (Hasegawa et 
al 2004; Zhang et al2005). Among these methods, 
supervised learning-based methods perform much 
better than the other two alternatives. However, 
their performance much depends on the availability 
of a large amount of manually labeled data and it is 
normally difficult to adapt an existing system to 
other applications and domains. On the other hand, 
unsupervised learning-based methods do not need 
the definition of relation types and the availability 
of manually labeled data. However, they fail to 
classify exact relation types between two entities 
and their performance is normally very low. To 
achieve better portability and balance between hu-
man efforts and performance, semi-supervised 
learning has drawn more and more attention re-
cently in relation extraction and other NLP appli-
cations. 
This paper proposes a semi-supervised learning 
method for relation extraction. Given a small 
amount of labeled data and a large amount of unla-
beled data, our proposed method first bootstraps a 
moderate number of weighted support vectors from 
all the available data via SVM using a co-training 
procedure with random feature projection and then 
applies a label propagation (LP) algorithm to cap-
ture the manifold structure in both the labeled and 
unlabeled data via the bootstrapped support vectors. 
Compared with previous methods, our method can 
integrate the advantages of both SVM bootstrap-
ping in learning critical instances for the labeling 
function and label propagation in capturing the 
manifold structure in both the labeled and unla-
beled data to smooth the labeling function. 
The rest of this paper is as follows. In Section 2, 
we review related semi-supervised learning work 
in relation extraction. Then, the LP algorithm via 
bootstrapped support vectors is proposed in Sec-
tion 3 while Section 4 shows the experimental re-
sults. Finally, we conclude our work in Section 5.  
2 Related Work 
Generally, supervised learning is preferable to un-
supervised learning due to prior knowledge in the 
32
annotated training data and better performance. 
However, the annotated data is usually expensive 
to obtain. Hence, there has been growing interest in 
semi-supervised learning, aiming at inducing clas-
sifiers by leveraging a small amount of labeled 
data and a large amount of unlabeled data. Related 
work in relation extraction using semi-supervised 
learning can be classified into two categories: 
bootstrapping-based (Brin 1998; Agichtein and 
Gravano 2000; Zhang 2004) and label propaga-
tion(LP)-based (Chen et al2006).  
Currently, bootstrapping-based methods domi-
nate semi-supervised learning in relation extraction. 
Bootstrapping works by iteratively classifying 
unlabeled instances and adding confidently classi-
fied ones into labeled data using a model learned 
from augmented labeled data in previous iteration. 
Brin (1998) proposed a bootstrapping-based 
method on the top of a self-developed pattern 
matching-based classifier to exploit the duality 
between patterns and relations. Agichtein and Gra-
vano (2000) shared much in common with Brin 
(1998). They employed an existing pattern match-
ing-based classifier (i.e. SNoW) instead. Zhang 
(2004) approached the much simpler relation clas-
sification sub-task by bootstrapping on the top of 
SVM. Although bootstrapping-based methods have 
achieved certain success, one problem is that they 
may not be able to well capture the manifold struc-
ture among unlabeled data. 
As an alternative to the bootstrapping-based 
methods, Chen et al(2006) employed a LP-based 
method in relation extraction. Compared with 
bootstrapping, the LP algorithm can effectively 
combine labeled data with unlabeled data in the 
learning process by exploiting the manifold struc-
ture (e.g. the natural clustering structure) in both 
the labeled and unlabeled data. The rationale be-
hind this algorithm is that the instances in high-
density areas tend to carry the same labels. The LP 
algorithm has also been successfully applied in 
other NLP applications, such as word sense disam-
biguation (Niu et al2005), text classification 
(Szummer and Jaakkola 2001; Blum and Chawla 
2001; Belkin and Niyogi 2002; Zhu and Ghahra-
mani 2002; Zhu et al2003; Blum et al2004), and 
information retrieval (Yang et al2006). However, 
one problem is its computational burden, espe-
cially when a large amount of labeled and unla-
beled data is taken into consideration. 
In order to take the advantages of both boot-
strapping and label propagation, our proposed 
method propagates labels via bootstrapped support 
vectors. On the one hand, our method can well 
capture the manifold structure in both the labeled 
and unlabeled data. On the other hand, our method 
can largely reduce the computational burden in the 
normal LP algorithm via all the available data. 
3 Label Propagation via Bootstrapped 
Support Vectors 
The idea behind our LP algorithm via bootstrapped 
support vectors is that, instead of propagating la-
bels through all the available labeled data, our 
method propagates labels through critical instances 
in both the labeled and unlabeled data. In this pa-
per, we use SVM as the underlying classifier to 
bootstrap a moderate number of weighted support 
vectors for this purpose. This is based on an as-
sumption that the manifold structure in both the 
labeled and unlabeled data can be well preserved 
through the critical instances (i.e. the weighted 
support vectors bootstrapped from all the available 
labeled and unlabeled data). The reason why we 
choose SVM is that it represents the state-of-the-
art in machine learning research and there are good 
implementations of the algorithm available. In par-
ticular, SVMLight (Joachims 1998) is selected as 
our classifier. For efficiency, we apply the one vs. 
others strategy, which builds K classifiers so as to 
separate one class from all others. Another reason 
is that we can adopt the weighted support vectors 
returned by the bootstrapped SVMs as the critical 
instances, via which label propagation is done.  
3.1 Bootstrapping Support Vectors 
This paper modifies the SVM bootstrapping algo-
rithm BootProject(Zhang 2004) to bootstrap sup-
port vectors. Given a small amount of labeled data 
and a large amount of unlabeled data, the modified 
BootProject algorithm bootstraps on the top of  
SVM by iteratively classifying  unlabeled  in-
stances  and moving   confidently  classified  ones  
into  labeled data using a model learned from the 
augmented labeled data in previous  iteration,  until 
not enough unlabeled instances can be classified 
confidently. Figure 1 shows the modified BootPro-
ject algorithm for bootstrapping support vectors.  
 
33
_________________________________________ 
Assume: 
L :  the labeled data; 
U :  the unlabeled data; 
S :  the batch size (100 in our experiments); 
P :  the number of views(feature projections); 
r :   the number of classes (including all the rela-
tion (sub)types and the non-relation)  
 
BEGIN 
REPEAT 
FOR i = 1 to P DO 
Generate projected feature space iF  from 
the original feature space F ; 
Project both L  and U  onto iF , thus gener-
ate iL  and iU ; 
Train SVM classifier ijSVM  on iL  for each 
class )1( rjr j K= ; 
Run ijSVM  on iU  for each class 
)1( rjr j K=  
END FOR 
Find (at most) S instances in U  with the 
highest agreement (with threshold 70% in 
our experiments) and the highest average 
SVM-returned confidence value (with 
threshold 1.0 in our experiments); 
Move them from U to L; 
UNTIL not enough unlabeled instances (less 
than 10 in our experiments) can be confidently 
classified; 
Return all the (positive and negative) support 
vectors  included in all the latest SVM classifi-
ers ijSVM  with their collective weight (abso-
lute alpha*y) information as the set of 
bootstrapped support vectors to act as the la-
beled data in the LP algorithm; 
Return U (those hard cases which can not be 
confidently classified) to act as the unlabeled 
data in the LP algorithm; 
END 
_________________________________________ 
Figure 1: The algorithm  
for bootstrapping support vectors 
 
In particular, this algorithm generates multiple 
overlapping ?views? by projecting from the origi-
nal feature space. In this paper, feature views with 
random feature projection, as proposed in Zhang 
(2004), are explored. Section 4 will discuss this 
issue in more details. During the iterative training 
process, classifiers trained on the augmented la-
beled data using the projected views are then asked 
to vote on the remaining unlabeled instances and 
those with the highest probability of being cor-
rectly labeled are chosen to augment the labeled 
data.  
During the bootstrapping process, the support 
vectors included in all the trained SVM classifiers 
(for all the relation (sub)types and the non-relation) 
are bootstrapped (i.e. updated) at each iteration. 
When the bootstrapping process stops, all the 
(positive and negative) support vectors included in 
the SVM classifiers are returned as bootstrapped 
support vectors with their collective weights (abso-
lute a*y) to act as the labeled data in the LP algo-
rithm and all the remaining unlabeled instances (i.e. 
those hard cases which can not be confidently clas-
sified in the bootstrapping process) in the unla-
beled data are returned to act as the unlabeled data 
in the LP algorithm. Through SVM bootstrapping, 
our LP algorithm will only depend on the critical 
instances (i.e. support vectors with their weight 
information bootstrapped from all the available 
labeled and unlabeled data) and those hard in-
stances, instead of all the available labeled and 
unlabeled data.  
3.2 Label Propagation 
In the LP algorithm (Zhu and Ghahramani 2002), 
the manifold structure in data is represented as a 
connected graph. Given the labeled data (the above 
bootstrapped support vectors with their weights) 
and unlabeled data (the remaining hard instances in 
the unlabeled data after bootstrapping, including 
all the test instances for evaluation), the LP algo-
rithm first represents labeled and unlabeled in-
stances as vertices in a connected graph, then 
propagates the label information from any vertex 
to nearby vertex through weighted edges and fi-
nally infers the labels of unlabeled instances until a 
global stable stage is achieved. Figure 2 presents 
the label propagation algorithm on bootstrapped 
support vectors in details. 
 
34
_________________________________________
Assume:  
Y : the rn * labeling matrix, where ijy  repre-
sents the probability of vertex )1( nixi K=  
with label )1( rjr j K=  (including the non-
relation label); 
LY : the top l  rows of 
0Y . LY corresponds to the 
l  labeled instances; 
UY : the bottom u  rows of 
0Y . UY corresponds 
to the u  unlabeled instances; 
T : a nn *  matrix, with ijt  is the probability 
jumping from vertex ix to vertex jx ; 
 
BEGIN (the algorithm) 
Initialization:  
1) Set the iteration index 0=t ;  
2) Let 0Y  be the initial soft labels attached to 
each vertex;  
3) Let 0LY  be consistent with the labeling in 
the labeled (including all the relation 
(sub)types and the non-relation) data, where 
0
ijy = the weight of the bootstrapped support 
vector if ix  has label jr  (Please note that 
jr  can be the non-relation label) and 0 oth-
erwise;  
4) Initialize 0UY ; 
REPEAT 
Propagate the labels of any vertex to nearby 
vertices by tt YTY =+1 ; 
Clamp the labeled data, that is, replace 1+tLY  
with 0LY ; 
UNTIL Y converges(e.g. 1+tLY  converges to 
0
LY ); 
Assign each unlabeled instance with a label: for 
)( nilxi ?p , find its label with 
j
ijymaxarg ; 
END (the algorithm) 
_________________________________________ 
Figure 2: The LP algorithm 
 
 
Here, each vertex corresponds to an instance, 
and the edge between any two instances ix  and jx  
is weighted by ijw  to measure their similarity. In 
principle, larger edge weights allow labels to travel 
through easier. Thus the closer the instances are, 
the more likely they have similar labels. The algo-
rithm first calculates the weight ijw  using a kernel, 
then transforms it to ?
=
=?=
n
k
kjijij wwijpt
1
/)( , 
which measures the probability of propagating a 
label from instance jx to instance ix , and finally 
normalizes ijt row by row using ?
=
=
n
k
ikijij ttt
1
/  to 
maintain the class probability interpretation of the 
labeling matrix Y .  
During the label propagation process, the label 
distribution of the labeled data is clamped in each 
loop using the weights of the bootstrapped support 
vectors and acts like forces to push out labels 
through the unlabeled data. With this push origi-
nates from the labeled data, the label boundaries 
will be pushed much faster along edges with larger 
weights and settle in gaps along those with lower 
weights. Ideally, we can expect that ijw  across 
different classes should be as small as possible and 
ijw  within the same class as big as possible. In this 
way, label propagation happens within the same 
class most likely. 
This algorithm has been shown to converge to 
a unique solution (Zhu and Ghahramani 2002), 
which can be obtained without iteration in theory, 
and the initialization of YU0 (the unlabeled data) is 
not important since YU0 does not affect its estima-
tion. However, proper initialization of YU0 actually 
helps the algorithm converge more rapidly in prac-
tice. In this paper, each row in YU0 is initialized to 
the average similarity with the labeled instances. 
4 Experimentation 
This paper uses the ACE RDC 2003 corpus pro-
vided by LDC for evaluation. This corpus is gath-
ered from various newspapers, newswires and 
broadcasts.  
 
35
Method 
LP via bootstrapped 
(weighted) SVs 
LP via bootstrapped  
(un-weighted) SVs 
LP w/o SVM  
bootstrapping 
SVM 
(BootProject) SVM  
Bootstrapping 
5% 46.5 (+1.4) 44.5 (+1.7) 43.1 (+1.0) 35.4 (-) 40.6 (+0.9) 
10% 48.6 (+1.7) 46.5 (+2.1) 45.2 (+1.5) 38.6 (-) 43.1 (+1.4) 
25% 51.7 (+1.9) 50.4 (+2.3) 49.6 (+1.8) 43.9 (-) 47.8 (+1.7) 
50% 53.6 (+1.8) 52.6 (+2.2) 52.1 (+1.7) 47.2 (-) 50.5 (+1.6) 
75% 55.2 (+1.3) 54.5 (+1.8) 54.2 (+1.2) 53.1 (-) 53.9 (+1.2) 
100% 56.2 (+1.0) 55.8 (+1.3) 55.6 (+0.8) 55.5 (-) 55.8 (+0.7) 
Table 1: Comparison of different methods using a state-of-the-art linear kernel on the ACE RDC 2003 
corpus (The numbers inside the parentheses indicate the increases in F-measure if we add the ACE RDC 
2004 corpus as the unlabeled data) 
4.1 Experimental Setting 
In the ACE RDC 2003 corpus, the training data 
consists of 674 annotated text documents (~300k 
words) and 9683 instances of relations. During 
development, 155 of 674 documents in the training 
set are set aside for fine-tuning. The test set is held 
out only for final evaluation. It consists of 97 
documents (~50k words) and 1386 instances of 
relations. The ACE RDC 2003 task defines 5 rela-
tion types and 24 subtypes between 5 entity types, 
i.e. person, organization, location, facility and GPE. 
All the evaluations are measured on the 24 sub-
types including relation identification and classifi-
cation. 
In all our experiments, we iterate over all pairs 
of entity mentions occurring in the same sentence 
to generate potential relation instances1. For better 
evaluation, we have adopted a state-of-the-art lin-
ear kernel as similarity measurements. In our linear 
kernel, we apply the same feature set as described 
in a state-of-the-art feature-based system (Zhou et 
al 2005): word, entity type, mention level, overlap, 
base phrase chunking, dependency tree, parse tree 
and semantic information. Given above various 
lexical, syntactic and semantic features, multiple 
overlapping feature views are generated in the 
bootstrapping process using random feature projec-
tion (Zhang 2004). For each feature projection in 
bootstrapping support vectors, a feature is ran-
domly selected with probability p and therefore the 
eventually projected feature space has p*F features 
                                                           
1  In this paper, we only measure the performance of 
relation extraction on ?true? mentions with ?true? 
chaining of co-reference (i.e. as annotated by the cor-
pus annotators) in the ACE corpora. We also explic-
itly model the argument order of the two mentions 
involved and only model explicit relations because of 
poor inter-annotator agreement in the annotation of 
implicit relations and their limited number. 
on average, where F is the size of the original fea-
ture space. In this paper, p and the number of dif-
ferent views are fine-tuned to 0.5 and 10 2 
respectively using 5-fold cross validation on the 
training data of the ACE RDC 2003 corpus. 
4.2 Experimental Results 
Table 1 presents the F-measures 3  (the numbers 
outside the parentheses) of our algorithm using the 
state-of-the-art linear kernel on different sizes of 
the ACE RDC training data with all the remaining 
training data and the test data4  as the unlabeled 
data on the ACE RDC 2003 corpus. In this paper, 
we only report the performance (averaged over 5 
trials) with the percentages of 5%, 10%, 25%, 50%, 
75% and 100%5. For example, our LP algorithm 
via bootstrapped (weighted) support vectors 
achieves the F-measure of 46.5 if using only 5% of 
the ACE RDC 2003 training data as the labeled 
data and the remaining training data and the test 
data in this corpus as the unlabeled data. Table 1 
                                                           
2 This suggests that the modified BootProject algorithm 
in the bootstrapping phase outperforms the SelfBoot 
algorithm (with p=1.0 and m=1) which uses all the 
features as the only view. In the related NLP literature, 
co-training has also shown to typically outperform 
self-bootstrapping. 
3 Our experimentation also shows that most of perform-
ance improvement with either bootstrapping or label 
propagation comes from gain in recall. Due to space 
limitation, this paper only reports the overall F-
measure. 
4  In our label propagation algorithm via bootstrapped 
support vectors, the test data is only included in the 
second phase (i.e. the label propagation phase) and not 
used in the first phase (i.e. bootstrapping support vec-
tors). This is to fairly compare different semi-
supervised learning methods. 
5 We have tried less percentage than 5%. However, our 
experiments show that using much less data will suffer 
from performance un-stability. Therefore, we only re-
port the performance with percentage not less than 5%. 
36
also compares our method with SVM and the 
original SVM bootstrapping algorithm BootPro-
ject(i.e. bootstrapping on the top of SVM with fea-
ture projection, as proposed in Zhang (2004)). 
Finally, Table 1 compares our LP algorithm via 
bootstrapped (weighted by default) support vectors 
with other possibilities, such as the scheme via 
bootstrapped (un-weighted, i.e. the importance of 
support vectors is not differentiated) support vec-
tors and the scheme via all the available labeled 
data (i.e. without SVM bootstrapping). Table 1 
shows that: 
1) Inclusion of unlabeled data using semi-
supervised learning, including the SVM boot-
strapping algorithm BootProject, the normal 
LP algorithm via all the available labeled and 
unlabeled data without SVM bootstrapping, 
and our LP algorithms via bootstrapped (either 
weighted or un-weighted) support vectors, 
consistently improves the performance, al-
though semi-supervised learning has shown to 
typically decrease the performance when a lot 
of (enough) labeled data is available (Nigam 
2001).  This may be due to the insufficiency of 
labeled data in the ACE RDC 2003 corpus. 
Actually, most of relation subtypes in the two 
corpora much suffer from the data sparseness 
problem (Zhou et al2006).  
2) All the three LP algorithms outperform the 
state-of-the-art SVM classifier and the SVM 
bootstrapping algorithm BootProject. Espe-
cially, when a small amount of labeled data is 
available, the performance improvements by 
the LP algorithms are significant. This indi-
cates the usefulness of the manifold structure 
in both labeled and unlabeled data and the 
powerfulness of the LP algorithm in modeling 
such information.  
3) Our LP algorithms via bootstrapped (either 
weighted or un-weighted) support vectors out-
performs the normal LP algorithm via all the 
available labeled data w/o SVM bootstrapping. 
For example, our LP algorithm via boot-
strapped (weighted) support vectors outper-
forms the normal LP algorithm from 0.6 to 3.4 
in F-measure on the ACE RDC 2003 corpus 
respectively when the labeled data ranges from 
100% to 5%. This suggests that the manifold 
structure in both the labeled and unlabeled data 
can be well preserved via bootstrapped support 
vectors, especially when only a small amount 
of labeled data is available. This implies that 
weighted support vectors may represent the 
manifold structure (e.g. the decision boundary 
from where label propagation is done) better 
than the full set of data ? an interesting result 
worthy more quantitative and qualitative justi-
fication in the future work.   
4) Our LP algorithms via bootstrapped (weighted) 
support vectors perform better than LP algo-
rithms via bootstrapped (un-weighted) support 
vectors by ~1.0 in F-measure on average. This 
suggests that bootstrapped support vectors with 
their weights can better represent the manifold 
structure in all the available labeled and unla-
beled data than bootstrapped support vectors 
without their weights. 
5) Comparison of SVM, SVM bootstrapping and 
label propagation with bootstrapped (weighted) 
support vectors shows that both bootstrapping 
and label propagation contribute much to the 
performance improvement. 
Table 1 also shows the increases in F-measure 
(the numbers inside the parentheses) if we add all 
the instances in the ACE RDC 20046 corpus into 
the ACE RDC 2003 corpus in consideration as 
unlabeled data in all the four semi-supervised 
learning methods. It shows that adding more unla-
beled data can consistently improve the perform-
ance. For example, compared with using only 5% 
of the ACE RDC 2003 training data as the labeled 
data and the remaining training data and the test 
data in this corpus as the unlabeled data, including 
the ACE RDC 2004 corpus as the unlabeled data 
increases the F-measures of 1.4 and 1.0 in our LP 
algorithm and the normal LP algorithm respec-
tively. Table 1 shows that the contribution grows 
first when the labeled data begins to increase and 
reaches a maximum of ~2.0 in F-measure at a cer-
tain point. 
Finally, it is found in our experiments that 
critical and hard instances normally occupy only 
15~20% (~18% on average) of all the available 
labeled and unlabeled data. This suggests that, 
through bootstrapped support vectors, our LP algo-
                                                           
6  Compared with the ACE RDC 2003 task, the ACE 
RDC 2004 task defines two more entity types, i.e. 
weapon and vehicle, much more entity subtypes, and 
different 7 relation types and 23 subtypes between 7 
entity types. The ACE RDC 2004 corpus from LDC 
contains 451 documents and 5702 relation instances. 
37
rithm can largely reduce the computational burden 
since it only depends on the critical instances (i.e. 
bootstrapped support vectors with their weights) 
and those hard instances.   
5 Conclusion 
This paper proposes a new effective and efficient 
semi-supervised learning method in relation ex-
traction. First, a moderate number of weighted 
support vectors are bootstrapped from all the avail-
able labeled and unlabeled data via SVM through a 
co-training procedure with feature projection. Here, 
a random feature projection technique is used to 
generate multiple overlapping feature views in 
bootstrapping using a state-of-the-art linear kernel. 
Then, a LP algorithm is applied to propagate labels 
via the bootstrapped support vectors, which, to-
gether with those hard unlabeled instances and the 
test instances, are represented as vertices in a con-
nected graph. During the classification process, the 
label information is propagated from any vertex to 
nearby vertex through weighted edges and finally 
the labels of unlabeled instances are inferred until a 
global stable stage is achieved.  In this way, the 
manifold structure in both the labeled and unla-
beled data can be well captured by label propaga-
tion via bootstrapped support vectors. Evaluation 
on the ACE RDC 2004 corpus suggests that our LP 
algorithm via bootstrapped support vectors can 
take the advantages of both SVM bootstrapping 
and label propagation.  
For the future work, we will systematically 
evaluate our proposed method on more corpora 
and explore better metrics of measuring the simi-
larity between two instances. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation of 
China and Project 2006AA01Z147 under the ?863? 
National High-Tech Research and Development of 
China. 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Agichtein E. and Gravano L. (2000). Snowball: 
Extracting relations from large plain-text collec-
tions. Proceedings of the 5th ACM International 
Conference on Digital Libraries 
(ACMDL?2000). 
Belkin, M. and Niyogi, P. (2002). Using Manifold 
Structure for Partially Labeled Classification. 
NIPS 15. 
Blum A. and Chawla S. (2001). Learning from la-
beled and unlabeled data using graph mincuts. 
ICML?2001. 
Blum A., Lafferty J., Rwebangira R and Reddy R. 
(2004). Semi-supervised learning using random-
ized mincuts. ICML?2004. 
Brin S. (1998). Extracting patterns and relations 
from world wide web. Proceedings of WebDB 
Workshop at 6th International Conference on 
Extending Database Technology:172-183. 
Charniak E. (2001). Immediate-head Parsing for 
Language Models. ACL?2001: 129-137. Tou-
louse, France 
Chen J.X., Ji D.H., Tan C.L. and Niu Z.Y. (2006). 
Relation extraction using label propagation 
based semi-supervised learning. COLING-
ACL?2006: 129-136. July 2006. Sydney, Austra-
lia. 
Culotta A. and Sorensen J. (2004). Dependency 
tree kernels for relation extraction. ACL?2004. 
423-429. 21-26 July 2004. Barcelona, Spain. 
Hasegawa T., Sekine S. and Grishman R. (2004). 
Discovering relations among named entities 
form large corpora. ACL?2004. Barcelona, Spain. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to ex-
tract information from text. ANLP?2000. 226-
233. 29 April  - 4 May 2000, Seattle, USA 
Moschitti A. (2004). A study on convolution ker-
nels for shallow semantic parsing. 
ACL?2004:335-342. 
Nigam K.P. (2001). Using unlabeled data to im-
prove text classification. Technical Report 
CMU-CS-01-126. 
Niu Z.Y., Ji D.H., and Tan C.L. (2005). Word 
Sense Disambiguation Using Label Propagation 
Based Semi-supervised Learning. 
ACL?2005:395-402., Ann Arbor, Michigan, 
USA. 
Szummer, M., & Jaakkola, T. (2001). Partially La-
beled Classification with Markov Random 
Walks. NIPS 14. 
38
Yang L.P., Ji D.H., Zhou G.D. and Nie Y. (2006). 
Document Re-ranking using cluster validation 
and label propagation. CIKM?2006. 5-11 Nov 
2006. Arlington, Virginia, USA. 
Zelenko D., Aone C. and Richardella. (2003). Ker-
nel methods for relation extraction. Journal of 
Machine Learning Research. 3(Feb):1083-1106. 
Zhang M., Su J., Wang D.M., Zhou G.D. and Tan 
C.L. (2005). Discovering Relations from a 
Large Raw Corpus Using Tree Similarity-based 
Clustering, IJCNLP?2005, Lecture Notes in Arti-
ficial Intelligence (LNAI 3651). 378-389. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). 
A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured 
Features. COLING-ACL-2006: 825-832. Sydney, 
Australia 
Zhang Z. (2004). Weakly supervised relation clas-
sification for information extraction. 
CIKM?2004. 8-13 Nov 2004. Washington D.C. 
USA. 
Zhao S.B. and Grishman R. (2005). Extracting re-
lations with integrated information using kernel 
methods. ACL?2005: 419-426. Univ of Michi-
gan-Ann Arbor,  USA,  25-30 June 2005. 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). 
Exploring various knowledge in relation extrac-
tion. ACL?2005. 427-434. 25-30 June, Ann Ar-
bor, Michgan, USA.  
Zhou G.D., Su J. and Zhang M. (2006). Modeling 
commonality among related classes in relation 
extraction, COLING-ACL?2006: 121-128. Syd-
ney, Australia. 
Zhu, X. and Ghahramani, Z. (2002). Learning from 
Labeled and Unlabeled Data with Label 
Propagation. CMU CALD Technical Report. 
CMU-CALD-02-107. 
Zhu, X., Ghahramani, Z. and Lafferty, J. (2003). 
Semi-Supervised Learning Using Gaussian 
Fields and Harmonic Functions. ICML?2003. 
39
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599?607,
Beijing, August 2010
Dependency-driven Anaphoricity Determination for Coreference 
Resolution
Fang Kong  Guodong Zhou  Longhua Qian  Qiaoming Zhu*
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology Soochow University 
{kongfang, gdzhou, qianlonghua, qmzhu}@suda.edu.cn 
                                                          
* Corresponding author 
Abstract
This paper proposes a dependency-driven 
scheme to dynamically determine the syn-
tactic parse tree structure for tree ker-
nel-based anaphoricity determination in 
coreference resolution. Given a full syntactic 
parse tree, it keeps the nodes and the paths 
related with current mention based on con-
stituent dependencies from both syntactic 
and semantic perspectives, while removing 
the noisy information, eventually leading to 
a dependency-driven dynamic syntactic 
parse tree (D-DSPT). Evaluation on the ACE 
2003 corpus shows that the D-DSPT out-
performs all previous parse tree structures on 
anaphoricity determination, and that apply-
ing our anaphoricity determination module 
in coreference resolution achieves the so far 
best performance. 
1 Introduction 
Coreference resolution aims to identify which 
noun phrases (NPs, or mentions) refer to the 
same real-world entity in a text. According to 
Webber (1979), coreference resolution can be 
decomposed into two complementary sub-tasks: 
(1) anaphoricity determination, determining 
whether a given NP is anaphoric or not; and (2) 
anaphor resolution, linking together multiple 
mentions of a given entity in the world. Al-
though machine learning approaches have per-
formed reasonably well in coreference resolu-
tion without explicit anaphoricity determina-
tion (e.g. Soon et al 2001; Ng and Cardie 
2002b; Yang et al 2003, 2008; Kong et al 
2009), knowledge of NP anaphoricity is ex-
pected to much improve the performance of a 
coreference resolution system, since a 
non-anaphoric NP does not have an antecedent 
and therefore does not need to be resolved. 
Recently, anaphoricity determination has 
been drawing more and more attention. One 
common approach involves the design of some 
heuristic rules to identify specific types of 
non-anaphoric NPs, such as pleonastic it (e.g. 
Paice and Husk 1987; Lappin and Leass 1994, 
Kennedy and Boguraev 1996; Denber 1998) 
and definite descriptions (e.g. Vieira and Poe-
sio 2000). Alternatively, some studies focus on 
using statistics to tackle this problem (e.g., 
Bean and Riloff 1999; Bergsma et al 2008) 
and others apply machine learning approaches 
(e.g. Evans 2001;Ng and Cardie 2002a, 
2004,2009; Yang et al 2005; Denis and Bal-
bridge 2007; Luo 2007; Finkel and Manning 
2008; Zhou and Kong 2009).  
As a representative, Zhou and Kong (2009) 
directly employ a tree kernel-based method to 
automatically mine the non-anaphoric informa-
tion embedded in the syntactic parse tree. One 
main advantage of the kernel-based methods is 
that they are very effective at reducing the 
burden of feature engineering for structured 
objects. Indeed, the kernel-based methods have 
been successfully applied to mine structured 
information in various NLP applications like 
syntactic parsing (Collins and Duffy, 2001; 
Moschitti, 2004), semantic relation extraction 
(Zelenko et al, 2003; Zhao and Grishman, 
2005; Zhou et al 2007; Qian et al, 2008), se-
mantic role labeling (Moschitti, 2004); corefer-
ence resolution (Yang et al, 2006; Zhou et al, 
2008). One of the key problems for the ker-
nel-based methods is how to effectively capture 
the structured information according to the na-
ture of the structured object in the specific task. 
This paper advances the state-of-the-art per-
formance in anaphoricity determination by ef-
599
fectively capturing the structured syntactic in-
formation via a tree kernel-based method. In 
particular, a dependency-driven scheme is 
proposed to dynamically determine the syntac-
tic parse tree structure for tree kernel-based 
anaphoricity determination by exploiting con-
stituent dependencies from both the syntactic 
and semantic perspectives to keep the neces-
sary information in the parse tree as well as 
remove the noisy information. Our motivation 
is to employ critical dependency information in 
constructing a concise and effective syntactic 
parse tree structure, specifically targeted for 
tree kernel-based anaphoricity determination.  
The rest of this paper is organized as follows. 
Section 2 briefly describes the related work on 
both anaphoricity determination and exploring 
syntactic parse tree structures in related tasks. 
Section 3 presents our dependency-driven 
scheme to determine the syntactic parse tree 
structure. Section 4 reports the experimental 
results. Finally, we conclude our work in Sec-
tion 5. 
2 Related Work 
This section briefly overviews the related work 
on both anaphoricity determination and ex-
ploring syntactic parse tree structures. 
2.1 Anaphoricity Determination 
Previous work on anaphoricity determination 
can be broadly divided into three categories: 
heuristic rule-based (e.g. Paice and Husk 
1987;Lappin and Leass 1994; Kennedy and 
Boguraev 1996; Denber 1998; Vieira and Poe-
sio 2000; Cherry and Bergsma 2005), statis-
tics-based (e.g. Bean and Riloff 1999; Cherry 
and Bergsma 2005; Bergsma et al 2008) and 
learning-based methods (e.g. Evans 2001; Ng 
and Cardie 2002a; Ng 2004; Yang et al 2005; 
Denis and Balbridge 2007; Luo 2007; Finkel 
and Manning 2008; Zhou and Kong 2009; Ng 
2009).  
The heuristic rule-based methods focus on 
designing some heuristic rules to identify spe-
cific types of non-anaphoric NPs. Representa-
tive work includes: Paice and Husk (1987), 
Lappin and Leass (1994) and Kennedy and 
Boguraev (1996). For example, Kennedy and 
Boguraev (1996) looked for modal adjectives 
(e.g. ?necessary?) or cognitive verbs (e.g. ?It is 
thought that?? in a set of patterned construc-
tions) in identifying pleonastic it.
Among the statistics-based methods, Bean 
and Riloff (1999) automatically identified ex-
istential definite NPs which are non-anaphoric.  
The intuition behind is that many definite NPs 
are not anaphoric since their meanings can be 
understood from general world knowledge, e.g. 
?the FBI?. They found that existential NPs ac-
count for 63% of all definite NPs and 76% of 
them could be identified by syntactic or lexical 
means. Cherry and Bergsma (2005) extended 
the work of Lappin and Leass (1994) for 
large-scale anaphoricity determination by addi-
tionally detecting pleonastic it. Bergsma et al 
(2008) proposed a distributional method in de-
tecting non-anaphoric pronouns. They first ex-
tracted the surrounding context of the pronoun 
and gathered the distribution of words that oc-
curred within the context from a large corpus, 
and then identified the pronoun either ana-
phoric or non-anaphoric based on the word dis-
tribution.
Among the learning-based methods, Evans 
(2001) automatically identified the 
non-anaphoricity of pronoun it using various 
kinds of lexical and syntactic features. Ng and 
Cardie (2002a) employed various do-
main-independent features in identifying ana-
phoric NPs. They trained an anaphoricity clas-
sifier to determine whether a NP was anaphoric 
or not, and employed an independently-trained 
coreference resolution system to only resolve 
those mentions which were classified as ana-
phoric. Experiments showed that their method 
improved the performance of coreference 
resolution by 2.0 and 2.6 to 65.8 and 64.2 in 
F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively. Ng (2004) examined the 
representation and optimization issues in com-
puting and using anaphoricity information to 
improve learning-based coreference resolution. 
On the basis, he presented a corpus-based ap-
proach (Ng, 2009) for achieving global opti-
mization by representing anaphoricity as a fea-
ture in coreference resolution. Experiments on 
the ACE 2003 corpus showed that their method 
improved the overall performance by 2.8, 2.2 
and 4.5 to 54.5, 64.0 and 60.8 in F1-measure 
on the NWIRE, NPAPER and BNEWS do-
mains, respectively. However, he did not look 
into the contribution of anaphoricity determi-
600
nation on coreference resolution of different 
NP types. Yang et al (2005) made use of 
non-anaphors to create a special class of train-
ing instances in the twin-candidate model 
(Yang et al 2003) and improved the perform-
ance by 2.9 and 1.6 to 67.3 and 67.2 in 
F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively. However, their experiments 
show that eliminating non-anaphors using an 
anaphoricity determination module in advance 
harms the performance. Denis and Balbridge 
(2007) employed an integer linear program-
ming (ILP) formulation for coreference resolu-
tion which modeled anaphoricity and corefer-
ence as a joint task, such that each local model 
informed the other for the final assignments. 
Experiments on the ACE 2003 corpus showed 
that this joint anaphoricity-coreference ILP 
formulation improved the F1-measure by 
3.7-5.3 on various domains. However, their 
experiments assume true ACE mentions (i.e. all 
the ACE mentions are already known from the 
annotated corpus). Therefore, the actual effect 
of this joint anaphoricity-coreference ILP for-
mulation on fully automatic coreference reso-
lution is still unclear. Luo (2007) proposed a 
twin-model for coreference resolution: a link 
component, which models the coreferential 
relationship between an anaphor and a candi-
date antecedent, and a creation component, 
which models the possibility that a NP was not 
coreferential with any candidate antecedent. 
This method combined the probabilities re-
turned by the creation component (an ana-
phoricity model) with the link component (a 
coreference model) to score a coreference par-
tition, such that a partition was penalized 
whenever an anaphoric mention was resolved. 
Finkel and Manning (2008) showed that transi-
tivity constraints could be incorporated into an 
ILP-based coreference resolution system and 
much improved the performance. Zhou and 
Kong (2009) employed a global learning 
method in determining the anaphoricity of NPs 
via a label propagation algorithm to improve 
learning-based coreference resolution. Experi-
ments on the ACE 2003 corpus demonstrated 
that this method was very effective. It could 
improve the F1-measure by 2.4, 3.1 and 4.1 on 
the NWIRE, NPAPER and BNEWS domains, 
respectively. Ng (2009) presented a novel ap-
proach to the task of anaphoricity determina-
tion based on graph minimum cuts and demon-
strated the effectiveness in improving a learn-
ing-based coreference resolution system. 
In summary, although anaphoricity determi-
nation plays an important role in coreference 
resolution and achieves certain success in im-
proving the overall performance of coreference 
resolution, its contribution is still far from ex-
pectation.
2.2 Syntactic Parse Tree Structures 
For a tree kernel-based method, one key prob-
lem is how to represent and capture the struc-
tured syntactic information. During recent 
years, various tree kernels, such as the convo-
lution tree kernel (Collins and Duffy, 2001), 
the shallow parse tree kernel (Zelenko et al
2003) and the dependency tree kernel (Culota 
and Sorensen, 2004), have been proposed in the 
literature. Among these tree kernels, the con-
volution tree kernel represents the state-of-the 
art and has been successfully applied by 
Collins and Duffy (2002) on syntactic parsing, 
Zhang et al (2006) on semantic relation extrac-
tion and Yang et al (2006) on pronoun resolu-
tion.
Given a tree kernel, the key issue is how to 
generate a syntactic parse tree structure for ef-
fectively capturing the structured syntactic in-
formation. In the literature, various parse tree 
structures have been proposed and successfully 
applied in some NLP applications. As a repre-
sentative, Zhang et al (2006) investigated five 
parse tree structures for semantic relation ex-
traction and found that the Shortest 
Path-enclosed Tree (SPT) achieves the best 
performance on the 7 relation types of the ACE 
RDC 2004 corpus. Yang et al (2006) con-
structed a document-level syntactic parse tree 
for an entire text by attaching the parse trees of 
all its sentences to a new-added upper node and 
examined three possible parse tree structures 
(Min-Expansion, Simple-Expansion and 
Full-Expansion) that contain different sub-
structures of the parse tree for pronoun resolu-
tion. Experiments showed that their method 
achieved certain success on the ACE 2003 
corpus and the simple-expansion scheme per-
forms best. However, among the three explored 
schemes, there exists no obvious overwhelming 
one, which can well cover structured syntactic 
information. One problem of Zhang et al (2006) 
601
and Yang et al (2006) is that their parse tree 
structures are context-free and do not consider 
the information outside the sub-trees. Hence, 
their ability of exploring structured syntactic 
information is much limited. Motivated by 
Zhang et al (2006) and Yang et al (2006), 
Zhou et al (2007) extended the SPT to become 
context-sensitive (CS-SPT) by dynamically 
including necessary predicate-linked path in-
formation. Zhou et al (2008) further proposed 
a dynamic-expansion scheme to automatically 
determine a proper parse tree structure for 
pronoun resolution by taking predicate- and 
antecedent competitor-related information in 
consideration. Evaluation on the ACE 2003 
corpus showed that the dynamic-expansion 
scheme can well cover necessary structured 
information in the parse tree for pronoun reso-
lution. One problem with the above parse tree 
structures is that they may still contain unnec-
essary information and also miss some useful 
context-sensitive information. Qian et al (2008) 
dynamically determined the parse tree structure 
for semantic relation extraction by exploiting 
constituent dependencies to keep the necessary 
information in the parse tree as well as remove 
the noisy information. Evaluation on the ACE 
RDC 2004 corpus showed that their dynamic 
syntactic parse tree structure outperforms all 
previous parse tree structures. However, their 
solution has the limitation in that the depend-
encies were found according to some manu-
ally-written ad-hoc rules and thus may not be 
easily applicable to new domains and applica-
tions.
This paper proposes a new scheme to dy-
namically determine the syntactic parse tree 
structure for anaphoricity determination and 
systematically studies the application of an ex-
plicit anaphoricity determination module in 
improving coreference resolution. 
3 Dependency-driven Dynamic Syn-
tactic Parse Tree 
Given a full syntactic parse tree and a NP in 
consideration, one key issue is how to choose a 
proper syntactic parse tree structure to well 
cover structured syntactic information in the 
tree kernel computation. Generally, the more a 
syntactic parse tree structure includes, the more 
structured syntactic information would be 
available, at the expense of more noisy (or un-
necessary) information.  
It is well known that dependency informa-
tion plays a key role in many NLP problems, 
such as syntactic parsing, semantic role label-
ing as well as semantic relation extraction. Mo-
tivated by Qian et al (2008) and Zhou et al 
(2008), we propose a new scheme to dynami-
cally determine the syntactic parse tree struc-
ture for anaphoricity determination by exploit-
ing constituent dependencies from both the 
syntactic and semantic perspectives to distin-
guish the necessary evidence from the unnec-
essary information in the syntactic parse tree. 
That is, constituent dependencies are explored 
from two aspects: syntactic dependencies and 
semantic dependencies.  
1) Syntactic Dependencies: The Stanford de-
pendency parser1 is employed as our syn-
tactic dependency parser to automatically 
extract various syntactic (i.e. grammatical) 
dependencies between individual words. In 
this paper, only immediate syntactic de-
pendencies with current mention are con-
sidered. The intuition behind is that the im-
mediate syntactic dependencies carry the 
major contextual information of current 
mention.
2) Semantic Dependencies: A state-of-the-art 
semantic role labeling (SRL) toolkit (Li et 
al. 2009) is employed for extracting various 
semantic dependencies related with current 
mention. In this paper, semantic dependen-
cies include all the predicates heading any 
node in the root path from current mention 
to the root node and their compatible argu-
ments (except those overlapping with cur-
rent mention). 
We name our parse tree structure as a depend-
ency-driven dynamic syntactic parse tree 
(D-DSPT). The intuition behind is that the de-
pendency information related with current 
mention in the same sentence plays a critical 
role in anaphoricity determination. Given the 
sentence enclosing the mention under consid-
eration, we can get the D-DSPT as follows: 
(Figure 1 illustrates an example of the D-DSPT 
generation given the sentence ?Mary said the 
woman in the room bit her? with ?woman? as 
current mention.) 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
602
           
Figure 1:  An example of generating the dependency-driven dynamic syntactic parse tree  
1) Generating the full syntactic parse tree of 
the given sentence using a full syntactic parser. 
In this paper, the Charniak parser (Charniak 
2001) is employed and Figure 1 (a) shows the 
resulting full parse tree. 
2) Keeping only the root path from current 
mention to the root node of the full parse tree. 
Figure 1(b) shows the root path corresponding 
to the current mention ?woman?. In the fol-
lowing steps, we attach the above two types of 
dependency information to the root path.  
3) Extracting all the syntactic dependencies 
in the sentence using a syntactic dependency 
parser, and attaching all the nodes, which have 
immediate dependency relationship with cur-
rent mention, and their corresponding paths to 
the root path. Figure 1(c) illustrates the syntac-
tic dependences extracted from the sentence, 
where the ones in italic mean immediate de-
pendencies with current mention. Figure 1(d) 
shows the parse tree structure after considering 
syntactic dependencies. 
4) Attaching all the predicates heading any 
node in the root path from current mention to 
the root node and their corresponding paths to 
the root path. For the example sentence, there 
are two predicates ?said? and ?bit?, which head 
the ?VP? and ?S? nodes in the root path re-
spectively. Therefore, these two predicates and 
their corresponding paths should be attached to 
the root path as shown in Figure 1(e). Note that 
the predicate ?bit? and its corresponding path 
has already been attached in Stop (3). As a re-
sult, the predicate-related information can be 
attached. According to Zhou and Kong (2009), 
such information is important to definite NP 
resolution.
5) Extracting the semantic dependencies re-
lated with those attached predicates using a 
(shallow) semantic parser, and attaching all the 
compatible arguments (except those overlap-
ping with current mention) and their corre-
sponding paths to the root path. For example, 
as shown in Figure 1(e), since the arguments 
?Mary? and ?her? are compatible with current 
mention ?woman?, these two nodes and their 
corresponding paths are attached while the ar-
gument ?room? is not since its gender does not 
agree with current mention. 
In this paper, the similarity between two 
parse trees is measured using a convolution tree 
kernel, which counts the number of common 
sub-tree as the syntactic structure similarity 
between two parse trees. For details, please 
refer to Collins and Duffy (2001). 
603
4 Experimentation and Discussion 
This section evaluates the performance of de-
pendency-driven anaphoricity determination 
and its application in coreference resolution on 
the ACE 2003 corpus. 
4.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), 
and broadcast news (BNEWS). For each do-
main, there exist two data sets, training and 
devtest, which are used for training and testing.  
For preparation, all the documents in the 
corpus are preprocessed automatically using a 
pipeline of NLP components, including to-
kenization and sentence segmentation, named 
entity recognition, part-of-speech tagging and 
noun phrase chunking. Among them, named 
entity recognition, part-of-speech tagging and 
noun phrase chunking apply the same 
state-of-the-art HMM-based engine with er-
ror-driven learning capability (Zhou and Su, 
2000 & 2002). Our statistics finds that 62.0%, 
58.5% and 61.4% of entity mentions are pre-
served after preprocessing on the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 training data respectively while only 
89.5%, 89.2% and 94% of entity mentions are 
preserved after preprocessing on  the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 devtest data. This indicates the difficulty 
of coreference resolution. In addition, the cor-
pus is parsed using the Charniak parser for 
syntactic parsing and the Stanford dependency 
parser for syntactic dependencies while corre-
sponding semantic dependencies are extracted 
using a state-of-the-art semantic role labeling 
toolkit (Li et al 2009). Finally, we use the 
SVM-light2 toolkit with the tree kernel func-
tion as the classifier. For comparison purpose, 
the training parameters C (SVM) and ?(tree
kernel) are set to 2.4 and 0.4 respectively, as 
done in Zhou and Kong (2009).  
For anaphoricity determination, we report 
the performance in Acc+ and Acc-, which 
measure the accuracies of identifying anaphoric 
NPs and non-anaphoric NPs, respectively. Ob-
viously, higher Acc+ means that more ana-
phoric NPs would be identified correctly, while 
                                                          
2 http://svmlight.joachims.org/ 
higher Acc- means that more non-anaphoric 
NPs would be filtered out. For coreference 
resolution, we report the performance in terms 
of recall, precision, and F1-measure using the 
commonly-used model theoretic MUC scoring 
program (Vilain et al 1995). To see whether an 
improvement is significant, we also conduct 
significance testing using paired t-test. In this 
paper, ?***?, ?**? and ?*? denote p-values of an 
improvement smaller than 0.01, in-between 
(0.01, 0,05] and bigger than 0.05, which mean 
significantly better, moderately better and 
slightly better, respectively. 
4.2 Experimental Results 
Performance of anaphoricity determination 
Table 1 presents the performance of anaphoric-
ity determination using the convolution tree 
kernel on D-DSPT. It shows that our method 
achieves the accuracies of 83.27/77.13, 
86.77/80.25 and 90.02/64.24 on identifying 
anaphoric/non-anaphoric NPs in the NWIRE, 
NPAPER and BNEWS domains, respectively.  
This suggests that our approach can effectively 
filter out about 75% of non-anaphoric NPs and 
keep about 85% of anaphoric NPs. In com-
parison, in the three domains Zhou and Kong 
(2009) achieve the accuracies of 76.5/82.3, 
78.9/81.6 and 74.3/83.2, respectively, using the 
tree kernel on a dynamically-extended tree 
(DET). This suggests that their method can fil-
ter out about 82% of non-anaphoric NPs and 
only keep about 76% of anaphoric NPs. In 
comparison, their method outperforms our 
method on filtering out more non-anaphoric 
NPs while our method outperforms their 
method on keeping more anaphoric NPs in 
coreference resolution. While a coreference 
resolution system can detect some 
non-anaphoric NPs (when failing to find the 
antecedent candidate), filtering out anaphoric 
NPs in anaphoricity determination would defi-
nitely cause errors and it is almost impossible 
to recover. Therefore, it is normally more im-
portant to keeping more anaphoric NPs than 
filtering out more non-anaphoric NPs. Table 1 
further presents the performance of anaphoric-
ity determination on different NP types. It 
shows that our method performs best at keep-
ing pronominal NPs and filtering out proper 
NPs.
604
NWIRE NPAPER BNEWS NP Type 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
Pronoun 95.07 50.36 96.40 56.44 98.26 54.03 
Proper NP 84.61 83.17 83.78 79.62 87.61 71.77 
Definite NP 87.17 46.74 82.24 49.18 86.87 53.65 
Indefinite NP 86.01 47.52 80.63 48.45 89.71 47.32 
Over all 83.27 77.13 86.77 80.25 90.02 64.24 
Table 1: Performance of anaphoricity determination using the D-DSPT  
NWIRE NPAPER BNEWS Performance Change 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
D-DSPT 83.27 77.13 86.77 80.25 90.02 64.24 
-Syntactic Dependencies 78.67 72.56 80.14 73.74 87.05 60.20 
-Semantic Dependencies 81.67 76.74 83.47 77.93 89.58 60.67 
Table 2: Contribution of including syntactic and semantic dependencies  
in D-DSPT on anaphoricity determination  
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Pronoun 70.8 57.9 63.7 76.5 63.5 69.4 70.0 60.3 64.8
Proper NP 80.3 80.1 80.2 81.8 83.6 82.7 76.3 76.8 76.6
Definite NP 35.9 43.4 39.2 43.1 48.5 45.6 47.9 51.9 49.8
Indefinite NP 40.3 26.3 31.8 39.7 22.9 29.0 23.6 10.7 14.7
Without ana-
phoricity de-
termination 
(Baseline)
Over all 55.0 63.8 59.1 62.1 65.0 63.5 53.2 60.5 56.6
Pronoun 65.9 70.2 68.0 72.6 78.7 75.5 67.7 75.8 71.5
Proper NP 80.3 81.0 80.6 81.2 85.1 83.1 76.3 84.4 80.1
Definite NP 32.3  63.1 42.7 38.4 61.7 47.3 42.5 66.4 51.8
Indefinite NP 36.4 55.3 43.9 34.7 50.7 41.2 20.3 45.4 28.1
With D-DSPT 
-based ana-
phoricity de-
termination 
Over all 52.4 79.6 63.2 58.1 80.3 67.4 50.1 79.8 61.6
Pronoun 68.6 71.5 70.1 75.2 80.4 77.7 69.1 77.8 73.5
Proper NP 81.7 89.3 85.3 82.6 90.1 86.2 78.6 88.7 83.3
Definite NP 41.8 85.9 56.2 44.9 85.2 58.8 45.2 87.9 59.7
Indefinite NP 40.3 67.6 50.5 41.2 65.1 50.5 40.9 50.1 45.1
With golden 
anaphoricity
determination 
Over all 54.6 81.7 65.5 60.4 82.1 69.6 51.9 82.1 63.6
Table 3: Performance of anaphoricity determination on coreference resolution 
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Without anaphoricity determina-
tion (Baseline) 53.1 67.4 59.4 57.7 67.0 62.1 48.0 65.9 55.5Zhou and 
Kong (2009) With Dynamically Extended 
Tree-based anaphoricity determi-
nation
51.6 77.2 61.8 55.2 78.6 65.2 47.5 80.3 59.6
Without anaphoricity determina-
tion (Baseline)
59.1 58. 58.6 60.8 62.6 61.7 57.7 52.6 55.0
Ng (2009) 
With Graph Minimum Cut-based 
anaphoricity determination
54.1 69.0 60.6 57.9 71.2 63.9 53.1 67.5 59.4
Table 4: Performance comparison with other systems 
Table 2 further presents the contribution of 
including syntactic and semantic dependencies 
in the D-DSPT on anaphoricity determination 
by excluding one or both of them. It shows that 
both syntactic dependencies and semantic de-
pendencies contribute significantly (***). 
Performance of coreference resolution 
We have evaluated the effect of our 
D-DSPT-based anaphoricity determination 
module on coreference resolution by including 
it as a preprocessing step to a baseline corefer-
ence resolution system without explicit ana-
phoricity determination, by filtering our those 
non-anaphoric NPs according to the anaphoric-
ity determination module. Here, the baseline 
system employs the same set of features, as 
adopted in the single-candidate model of Yang 
et al (2003) and uses a SVM-based classifier 
with the feature-based RBF kernel. Table 3 
presents the detailed performance of the 
coreference resolution system without ana-
605
phoricity determination, with D-DSPT-based 
anaphoricity determination and. with golden 
anaphoricity determination. Table 3 shows that: 
1) There is a performance gap of 6.4, 6.1 and 
7.0 in F1-measure on the NWIRE, NPAPER 
and BNEWS domain, respectively, between the 
coreference resolution system with golden 
anaphoricity determination and the baseline 
system without anaphoricity determination. 
This suggests the usefulness of proper ana-
phoricity determination in coreference resolu-
tion. This also agrees with Stoyanov et al 
(2009) which measured the impact of golden 
anaphoricity determination on coreference 
resolution using only the annotated anaphors in 
both training and testing.  
2) Compared to the baseline system without 
anaphoricity determination, the D-DSPT-based 
anaphoricity determination module improves 
the performance by 4.1(***), 3.9(***) and 
5.0(***) to 63.2, 67.4 and 61.6 in F1-measure 
on the NWIRE, NPAPER and BNEWS do-
mains, respectively, due to a large gain in pre-
cision and a much smaller drop in recall. In 
addition, D-DSPT-based anaphoricity determi-
nation can not only much improve the per-
formance of coreference resolution on pro-
nominal NPs (***) but also on definite 
NPs(***) and indefinite NPs(***) while the 
improvement on proper NPs can be ignored 
due to the fact that proper NPs can be well ad-
dressed by the simple abbreviation feature in 
the baseline system. 
3) D-DSPT-based anaphoricity determination 
still lags (2.3, 2.2 and 2.0 on the NWIRE, 
NPAPER and BNEWS domains, respectively) 
behind golden anaphoricity determination in 
improving the overall performance of corefer-
ence resolution. This suggests that there exists 
some room in the performance improvement 
for anaphoricity determination. 
Performance comparison with other systems 
Table 4 compares the performance of our sys-
tem with other systems. Here, Zhou and Kong 
(2009) use the same set of features with ours in 
the baseline system and a dynami-
cally-extended tree structure in anaphoricity 
determination. Ng (2009) uses 33 features as 
described in Ng (2007) and a graph minimum 
cut algorithm in anaphoricity determination. It 
shows that the overall performance of our 
baseline system is almost as good as that of 
Zhou and Kong (2009) and a bit better than 
Ng?s (2009).  
For overall performance, our coreference 
resolution system with D-DSPT-based ana-
phoricity determination much outperforms 
Zhou and Kong (2009) in F1-measure by 1.4, 
2.2 and 2.0 on the NWIRE, NPAPER and 
BNEWS domains, respectively, due to the bet-
ter inclusion of dependency information. De-
tailed evaluation shows that such improvement 
comes from coreference resolution on both 
pronominal and definite NPs (Please refer to 
Table 6 in Zhou and Kong, 2009). Compared 
with Zhou and Kong (2009) and Ng (2009), our 
approach achieves the best F1-measure so far 
for each dataset. 
5 Conclusion and Further Work 
This paper systematically studies a depend-
ency-driven dynamic syntactic parse tree 
(DDST) for anaphoricity determination and the 
application of an explicit anaphoricity deter-
mination module in improving learning-based 
coreference resolution. Evaluation on the ACE 
2003 corpus indicates that D-DSPT-based 
anaphoricity determination much improves the 
performance of coreference resolution. 
To our best knowledge, this paper is the first 
research which directly explores constituent 
dependencies in tree kernel-based anaphoricty 
determination from both syntactic and semantic 
perspectives. 
For further work, we will explore more 
structured syntactic information in coreference 
resolution. In addition, we will study the inter-
action between anaphoricity determination and 
coreference resolution and better integrate 
anaphoricity determination with coreference 
resolution.
Acknowledgments 
This research was supported by Projects 
60873150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project 200802850006 and 20093201110006 
under the Specialized Research Fund for the 
Doctoral Program of Higher Education of 
China.
606
References 
D. Bean and E. Riloff 1999. Corpus-based Identifi-
cation of Non-Anaphoric Noun Phrases. ACL? 
1999 
S. Bergsma, D. Lin and R. Goebel 2008. Distribu-
tional Identification of Non-referential Pronouns. 
ACL?2008 
C. Cherry and S. Bergsma. 2005. An expectation 
maximization approach to pronoun resolution. 
CoNLL?2005 
M. Collins and N. Duffy. 2001. Covolution kernels 
for natural language. NIPS?2001  
M. Denber 1998. Automatic Resolution of Anapho-
ria in English. Technical Report, Eastman Ko-
dakCo. 
P. Denis and J. Baldridge. 2007. Global, joint de-
termination of anaphoricity and coreference 
resolution using integer programming. 
NAACL/HLT?2007
R. Evans 2001. Applying machine learning toward 
an automatic classification of it. Literary and 
Linguistic Computing, 16(1):45-57 
F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employ-
ing the Centering Theory in Pronoun Resolution 
from the Semantic Perspective. EMNLP?2009 
F. Kong, Y.C. Li, G.D. Zhou and Q.M. Zhu. 2009. 
Exploring Syntactic Features for Pronoun Reso-
lution Using Context-Sensitive Convolution Tree 
Kernel. IALP?2009 
S. Lappin and J. L. Herbert. 1994. An algorithm for 
pronominal anaphora resolution. Computational 
Linguistics, 20(4) 
J.H. Li. G.D. Zhou, H. Zhao, Q.M. Zhu and P.D. 
Qian. Improving nominal SRL in Chinese lan-
guage with verbal SRL information and auto-
matic predicate recognition. EMNLP '2009 
X. Luo. 2007. Coreference or not: A twin model for 
coreference resolution.  NAACL-HLT?2007 
V. Ng and C. Cardie 2002. Identify Anaphoric and 
Non-Anaphoric Noun Phrases to Improve 
Coreference Resolution. COLING?2002 
V. Ng and C. Cardie 2002. Improving machine 
learning approaches to coreference resolution. 
ACL?2002 
V. Ng 2004. Learning Noun Phrase Anaphoricity to 
Improve Coreference Resolution: Issues in Rep-
resentation and Optimization. ACL? 2004 
V. Ng 2009. Graph-cut based anaphoricity determi-
nation for coreference resolution. NAACL?2009 
L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D. 
Qian. 2008. Exploiting constituent dependencies 
for  tree kernel-based semantic relation extrac-
tion. COLING?2008 
W. M. Soon, H. T. Ng and D. Lim  2001. A ma-
chine learning approach to coreference resolution 
of noun phrase. Computational Linguistics, 
27(4):521-544. 
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 
2009. Conundrums in Noun Phrase Coreference 
Resolution: Making Sense of the State-of-the Art. 
ACL?2009 
B. L. Webber. 1979. A Formal Approach to Dis-
course Anaphora. Garland Publishing, Inc. 
X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003. 
Coreference Resolution Using Competition 
Learning Approach. ACL?2003 
X.F. Yang, J. Su and C.L. Chew. 2005. A Twin 
Candidate Model of Coreference Resolution with 
Non-Anaphor Identification Capability. 
IJCNLP?2005 
X.F. Yang, J. Su and C.L. Chew. 2006. Ker-
nel-based pronoun resolution with structured 
syntactic knowledge. COLING-ACL?2006 
X.F. Yang, J. Su and C.L. Tan 2008. A 
Twin-Candidate Model for Learning-Based 
Anaphora Resolution. Computational Linguistics 
34(3):327-356 
M. Zhang, J. Zhang, J. Su and G.D. Zhou. 2006. A 
composite kernel to extract relations between en-
tities with both flat and structured features. 
COLING/ACL?2006 
S. Zhao and R. Grishman. 2005. Extracting relations 
with integered information using kernel methods. 
ACL?2005 
D. Zelenko, A. Chinatsu and R. Anthony. 2003. 
Kernel methods for relation extraction. Machine 
Learning Researching 3(2003):1083-1106 
G.D. Zhou, F. Kong and Q.M. Zhu. 2008. Con-
text-sensitive convolution tree kernel for pronoun 
resolution. IJCNLP?2008 
G.D. Zhou and F. Kong. 2009. Global Learning of 
Noun Phrase Anaphoricity in Coreference Reso-
lution via Label Propagetion. EMNLP?2009 
G.D. Zhou and J. Su. 2002. Named Entity recogni-
tion using a HMM-based chunk tagger. 
ACL?2002 
G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007. 
Tree kernel-based relation extraction with con-
text-sensitive structured parse tree information. 
EMNLP/CoNLL?2007
607
Coling 2010: Poster Volume, pages 757?765,
Beijing, August 2010
Dependency-Driven Feature-based Learning for Extracting 
Protein-Protein Interactions from Biomedical Text 
Bing Liu   Longhua Qian   Hongling Wang   Guodong Zhou 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University 
Email: liubingnlp@gmail.com 
{qianlonghua,redleaf,gdzhou}@suda.edu.cn
                                                          
 Corresponding author 
Abstract
Recent kernel-based PPI extraction 
systems achieve promising perform-
ance because of their capability to 
capture structural syntactic informa-
tion, but at the expense of computa-
tional complexity. This paper incorpo-
rates dependency information as well 
as other lexical and syntactic knowl-
edge in a feature-based framework. 
Our motivation is that, considering the 
large amount of biomedical literature 
being archived daily, feature-based 
methods with comparable performance 
are more suitable for practical applica-
tions. Additionally, we explore the 
difference of lexical characteristics be-
tween biomedical and newswire do-
mains. Experimental evaluation on the 
AIMed corpus shows that our system 
achieves comparable performance of 
54.7 in F1-Score with other 
state-of-the-art PPI extraction systems, 
yet the best performance among all the 
feature-based ones.  
1 Introduction 
In recent years, automatically extracting 
biomedical information has been the subject of 
significant research efforts due to the rapid 
growth in biomedical development and 
discovery. A wide concern is how to 
characterize protein interaction partners since 
it is crucial to understand not only the 
functional role of individual proteins but also 
the organization of the entire biological 
process. However, manual collection of 
relevant Protein-Protein Interaction (PPI) 
information from thousands of research papers 
published every day is so time-consuming that 
automatic extraction approaches with the help 
of Natural Language Processing (NLP) 
techniques become necessary.  
Various machine learning approaches for 
relation extraction have been applied to the 
biomedical domain, which can be classified 
into two categories: feature-based methods 
(Mitsumori et al, 2006; Giuliano et al, 2006; 
S?tre et al, 2007) and kernel-based methods 
(Bunescu et al, 2005; Erkan et al, 2007; 
Airola et al, 2008; Kim et al, 2010). 
Provided a large-scale manually annotated 
corpus, the task of PPI extraction can be 
formulated as a classification problem. 
Typically, for featured-based learning each 
protein pair is represented as a vector whose 
features are extracted from the sentence 
involving two protein names. Early studies 
identify the existence of protein interactions 
by using ?bag-of-words? features (usually 
uni-gram or bi-gram) around the protein 
names as well as various kinds of shallow 
linguistic information, such as POS tag, 
lemma and orthographical features. However, 
these systems do not achieve promising results 
since they disregard any syntactic or semantic 
information altogether, which are very useful 
for the task of relation extraction in the 
newswire domain (Zhao and Grishman, 2005; 
Zhou et al, 2005). Furthermore, feature-based 
methods fail to effectively capture the 
structural information, which is essential to 
757
identify the relationship between two proteins 
in a syntactic representation. 
With the wide application of kernel-based 
methods to many NLP tasks, various kernels 
such as subsequence kernels (Bunescu and 
Mooney, 2005) and tree kernels (Li et al, 
2008), are also applied to PPI detection.. 
Particularly, dependency-based kernels such 
as edit distance kernels (Erkan et al, 2007) 
and graph kernels (Airola et al, 2008; Kim et 
al., 2010) show some promising results for PPI 
extraction. This suggests that dependency 
information play a critical role in PPI 
extraction as well as in relation extraction 
from newswire stories (Culotta and Sorensen, 
2004). In order to appreciate the advantages of 
both feature-based methods and kernel-based 
methods, composite kernels (Miyao et al, 
2008; Miwa et al, 2009a; Miwa et al, 2009b) 
are further employed to combine structural 
syntactic information with flat word features 
and significantly improve the performance of 
PPI extraction. However, one critical 
challenge for kernel-based methods is their 
computation complexity, which prevents them 
from being widely deployed in real-world 
applications regarding the large amount of 
biomedical literature being archived everyday.  
Considering the potential of dependency in-
formation for PPI extraction and the challenge 
of computation complexity of kernel-based 
methods, one may naturally ask the question: 
?Can the essential dependency information be 
maximally exploited in featured-based PPI 
extraction so as to enhance the performance 
without loss of efficiency?? ?If the answer is 
Yes, then How?? 
This paper addresses these problems, focus-
ing on the application of dependency informa-
tion to feature-based PPI extraction. Starting 
from a baseline system in which common 
lexical and syntactic features are incorporated 
using Support Vector Machines (SVM), we 
further augment the baseline with various fea-
tures related to dependency information, 
including predicates in the dependency tree. 
Moreover, in order to reveal the linguistic 
difference between distinct domains we also 
compare the effects of various features on PPI 
extraction from biomedical texts with those on 
relation extraction from newswire narratives. 
Evaluation on the AIMed and other PPI cor-
pora shows that our method achieves the best 
performance among all feature-based systems. 
The rest of the paper is organized as follows. 
A feature-based PPI extraction baseline system 
is given in Section 2 while Section 3 describes 
our dependency-driven method. We report our 
experiments in Section 4, and compare our 
work with the related ones in Section 5.  
Section 6 concludes this paper and gives some 
future directions. 
2 Feature-based PPI extraction: 
Baseline
For feature-based methods, PPI extraction task 
is re-cast as a classification problem by first 
transforming PPI instances into 
multi-dimensional vectors with various fea-
tures, and then applying machine learning ap-
proaches to detect whether the potential 
relationship exists for a particular protein pair. 
In training, a feature-based classifier learning 
algorithm, such as SVM or MaxEnt, uses the 
annotated PPI instances to learn a classifier 
while, in testing, the learnt classifier is in turn 
applied to new instances to determine their PPI 
binary classes and thus candidate PPI instances 
are extracted. 
As a baseline, various linguistic features, 
such as words, overlap, chunks, parse tree fea-
tures as well as their combined ones are ex-
tracted from a sentence and formed as a vector 
into the feature-based learner. 
1) Words 
Four sets of word features are used in our sys-
tem: 1) the words of both the proteins; 2) the 
words between the two proteins; 3) the words 
before M1 (the 1st protein); and 4) the words 
after M2 (the 2nd protein). Both the words be-
fore M1 and after M2 are classified into two 
bins: the first word next to the proteins and the 
second word next to the proteins. This means 
that we only consider the two words before M1 
and after M2. Words features include: 
x MW1: bag-of-words in M1 
x MW2: bag-of-words in M2 
x BWNULL: when no word in between 
x BWO: other words in between except 
first and last words when at least three 
words in between 
x BWM1FL: the only word before M1 
758
x BWM1F: first word before M1 
x BWM1L: second word before M1 
x BWM1: first and second word before 
M1
x BWM2FL: the only word after M2 
x BWM2F: first word after M2 
x BWM2L: second word after M2 
x BWM2: first and second word after M2 
2) Overlap 
The numbers of other protein names as well as 
the words that appear between two protein 
names are included in the overlap features. 
This category of features includes: 
x #MB: number of other proteins in be-
tween
x #WB: number of words in between 
x E-Flag: flag indicating whether the two 
proteins are embedded or not 
3) Chunks
It is well known that chunking plays an 
important role in the task of relation extraction 
in the ACE program (Zhou et al, 2005). How-
ever, its significance in PPI extraction has not 
fully investigated. Here, the Stanford Parser1
is first employed for full parsing, and then 
base phrase chunks are derived from full parse 
trees using the Perl script2. The chunking fea-
tures usually concern about the head words of 
the phrases between the two proteins, which 
are further classified into three bins: the first 
phrase head in between, the last phrase head in 
between and other phrase heads in between. In 
addition, the path of phrasal labels connecting 
two proteins is also a common syntactic 
indicator of the polarity of the PPI instance, 
just as the path NP_VP_PP_NP in the sen-
tence ?The ability of PROT1 to interact with 
the PROT2 was investigated.? is likely to sug-
gest the positive interaction between two pro-
teins. These base phrase chunking features 
contain:
x CPHBNULL: when no phrase in be-
tween.
x CPHBFL: the only phrase head when 
only one phrase in between 
x CPHBF: the first phrase head in between 
when at least two phrases in between. 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
2 http://ilk.kub.nl/~sabine/chunklink/ 
x CPHBL: the last phrase head in between 
when at least two phrase heads in be-
tween.
x CPHBO: other phrase heads in between 
except first and last phrase heads when 
at least three phrases in between. 
x CPP: path of phrase labels connecting 
the two entities in the chunking 
Furthermore, we also generate a set of 
bi-gram features which combine the above 
chunk features except CPP with their corre-
sponding chunk types.  
4) Parse Tree 
It is obvious that full pares trees encompass 
rich structural information of a sentence. 
Nevertheless, it is much harder to explore 
such information in featured-based methods 
than in kernel-based ones. Thus so far only 
the path connecting two protein names in the 
full-parse tree is considered as a parse tree 
feature.
x PTP: the path connecting two protein 
names in the full-parse tree. 
 Again, take the sentence ?The ability of 
PROT1 to interact with the PROT2 was 
investigated.? as an example, the parse path 
between PROT1 and PROT2 is 
NP_S_VP_PP_NP, which is slightly different 
from the CPP feature in the chunking feature 
set.
3 Dependency-Driven PPI Extraction 
The potential of dependency information for 
PPI extraction lies in the fact that the depend-
ency tree may well reveal non-local or 
long-range dependencies between the words 
within a sentence. In order to capture the 
necessary information inherent in the 
depedency tree for identifying their 
relationship, various kernels, such as edit 
distance kernel based on dependency path 
(Erkan et al, 2007), all-dependency-path 
graph kernel (Airola et al, 2008), and 
walk-weighted subsequence kernels (Kim et 
al., 2010) as well as other composite kernels 
(Miyao et al, 2008; Miwa et al, 2009a; Miwa 
et al, 2009b), have been proposed to address 
this problem. It?s true that these methods 
achieve encouraging results, neverthless, they 
suffer from prohibitive computation burden. 
759
Thus, our solution is to fold the structural 
dependency information back into flat 
features in a feature-based framework so as to 
speed up the learning process while retaining 
comparable performance. This is what we 
refer to as dependency-driven PPI extraction. 
 First, we construct dependency trees from 
grammatical relations generated by the Stan-
ford Parser. Every grammatical relation has the 
form of dependent-type (word1, word2),
Where word1 is the head word, word2 is de-
pendent on word1, and dependent-type denotes 
the pre-defined type of dependency. Then, 
from these grammatical relations the following 
features called DependenecySet1 are taken 
into consideration as illustrated in Figure 1: 
x DP1TR: a list of words connecting 
PROT1 and the dependency tree root. 
x DP2TR: a list of words connecting 
PROT2 and the dependency tree root. 
x DP12DT: a list of dependency types 
connecting the two proteins in the 
dependency tree. 
x DP12: a list of dependent words com-
bined with their dependency types con-
necting the two proteins in the depend-
ency tree. 
x DP12S: the tuple of every word com-
bined with its dependent type in DP12. 
x DPFLAG: a boolean value indicating 
whether the two proteins are directly 
dependent on each other. 
The typed dependencies produced by the 
Stanford Parser for the sentence ?PROT1 
contains a sequence motif binds to PROT2.? 
are listed as follows: 
nsubj(contains-2,PROT1-1)
det(motif-5, a-3) 
nn(motif-5, sequence-4) 
nsubj(binds-6, motif-5) 
ccomp(contains-2, binds-6) 
prep_to(binds-6, PROT2-8) 
Each word in a dependency tuple is fol-
lowed by its index in the original sentence, 
ensuring accurate positioning of the head 
word and dependent word. Figure 1 shows the 
dependency tree we construct from the above 
grammatical relations.  
contains 
PROT1 
motif 
binds 
PROT2
a sequence 
nsubj ccomp 
prep_to
nsubj 
det nn 
Figure 1: Dependency tree for the sentence 
?PROT1 contains a sequence motif binds to 
PROT2.? 
Erkan et al (2007) extract the path 
information between PROT1 and PROT2 in 
the dependency tree for kernel-based PPI 
extraction and report promising results, 
neverthless, such path is so specific for 
feature-based methods that it may incure 
higher precision but lower recall. Thus we 
alleviate this problem by collapsing the feature 
into multiple ones with finer granularity, 
leading to the features such as DP12S. 
It is widely acknowledged that predicates 
play an important role in PPI extraction. For 
example, the change of a pivot predicate 
between two proteins may easily lead to the 
polarity reversal of a PPI instance. Therefore, 
we extract the predicates and their positions in 
the dependency tree as predicate features 
called DependencySet2:  
x FVW: the predicates in the DP12 feature 
occurring prior to the first protein. 
x LVW: the predicates in the DP12 feature 
occurring next to the second entity. 
x MVW: other predicates in the DP12 
features. 
x #FVW: the number of FVW 
x #LVW: the number of LVW 
x #MVW: the number of MVW 
4 Experimentation
This section systematically evaluates our fea-
ture-based method on the AIMed corpus as 
well as other commonly used corpus and re-
ports our experimental results. 
760
4.1 Data Sets 
We use five corpora3 with the AIMed corpus 
as the main experimental data, which contains 
177 Medline abstracts with interactions be-
tween two interactions, and 48 abstracts with-
out any PPI within single sentences. There are 
4,084 protein references and around 1,000 
annotated interactions in this data set.  
For corpus pre-procession, we first rename 
two proteins of a pair as PROT1 and PROT2 
respectively in order to blind the learner for 
fair comparison with other work.  Then, all 
the instances are generated from the sentences 
which contain at least two proteins,  that is, if 
a sentence contains n different proteins, there 
are n2 different pairs of proteins and these 
pairs are considered untyped and undirected. 
For the purpose of comparison with previous 
work, all the self-interactions (59 instances) 
are removed, while all the PPI instances with 
nested protein names are retained (154 in-
stances). Finally, 1002 positive instances and 
4794 negative instances are generated and 
their corresponding features are extracted.  
We select Support Vector Machines (SVM) 
as the classifier since SVM represents the 
state-of-the-art in the machine learning re-
search community. In particular, we use the 
binary-class SVMLigh 4 developed by 
Joachims (1998) since it satisfies our require-
ment of detecting potential PPI instances. 
Evaluation is done using 10-fold docu-
ment-level cross-validation. Particularly, we 
apply the extract same 10-fold split that was 
used by Bunescu et al (2005) and Giuliano et 
al. (2006). Furthermore, OAOD (One Answer 
per Occurrence in the Document) strategy is 
adopted, which means that the correct interac-
tion must be extracted for each occurrence. 
This guarantees the maximal use of the avail-
able data, and more important, allows fair 
comparison with earlier relevant work.  
The evaluation metrics are commonly used 
Precision (P), Recall (R) and harmonic 
F1-score (F1). As an alternative to F1-score, 
the AUC (area under the receiver operating 
characteristics curve) measure is proved to be 
invariant to the class distribution of the train-
ing dataset. Thus we also provide AUC scores 
                                                          
3 http://mars.cs.utu.fi/PPICorpora/GraphKernel.html 
4 http://svmlight.joachims.org/
for our system as Airola et al (2008) and 
Miwa et al (2009a). 
4.2 Results and Discussion 
Features P(%) R(%) F1 
Baseline features 
Words 59.4 40.6 47.6
+Overlap 60.4 39.9 47.4
+Chunk 59.2 44.5 50.6
+Parse 60.9 44.8 51.4
Dependency-driven features 
+Dependency Set1 62.9 48.0 53.9
+Dependency Set2 63.4 48.8 54.7
Table 1: Performance of PPI extraction with vari-
ous features in the AIMed corpus 
We present in Table 1 the performance of our 
system using document-wise evaluation 
strategies and 10-fold cross-validation with 
different features in the AIMed corpus, where 
the plus sign before a feature means it is 
incrementally added to the feature set. Table 1 
reports that our system achieves the best per-
formance of 63.4/48.8/54.7 in P/R/F scores. It 
also shows that: 
x Words features alone achieve a relatively 
low performance of 59.4/40.9/47.6 in 
P/R/F, particularly with fairly low recall 
score. This suggests the difficulty of PPI 
extraction and words features alone can?t 
effectively capture the nature of protein 
interactions.
x Overlap features slightly decrease the per-
formance. Statistics show that both the 
distributions of #MB and #WB between 
positives and negatives are so similar that 
they are by no means the discriminators for 
PPI extraction. Hence, we exclude the 
overlap features in the succeeding experi-
ments.
x Chunk features significantly improves the 
F-measure by 3 units largely due to the in-
crease of recall by 3.9%, though at the 
slight expense of precision. This suggests 
the effectiveness of shallow parsing infor-
mation in the form of headwords captured 
by chunking on PPI extraction.  
x The usefulness of the parse tree features is 
quite limited. It only improves the 
F-measure by 0.8 units. The main reason 
may be that these paths are usually long 
761
and specific, thus they suffer from the 
problem of data sparsity. Furthermore, 
some of the parse tree features are already 
involved in the chunk features.  
x The DependencySet1 features are very 
effective in that it can increase the preci-
sion and recall by 2.0 and 3.2 units 
respectively, leading to the increase of F1 
score by 2.5 units. This means that the de-
pendency-related features can effectively 
retrieve more PPI instances without intro-
ducing noise that will severely harm the 
precision. According to our statistics, there 
are over 60% sentences with more than 5 
words between their protein entities in the 
AIMed corpus. Therefore, dependency in-
formation exhibit great potential to PPI 
extraction since they can capture 
long-range dependencies within sentences. 
Take the aforementioned sentence 
?PROT1 contains a sequence motif binds 
to PROT2.? as an example, although the 
two proteins step over a relatively long 
distance, the dependency path between 
them is concise and accurate, reflecting the 
essence of the interaction. 
x The predicate features also contribute to 
the F1-score gain of 0.8 units. It is not 
surprising since some predicates, such as 
?interact?, ?activate? and ?inhibit? etc, are 
strongly suggestive of the interaction 
polarity between two proteins. 
We compare in Table 2 the performance of 
our system with other systems in the AIMed 
corpus using the same 10-fold cross validation 
strategy. These systems are grouped into three 
distinct classes: feature-based, kernel-based 
and composite kernels. Except for Airola et al 
(2008) Miwa et al (2009a) and Kim et al 
(2010), which adopt graph kernels, our system 
performs comparably with other systems. In 
particular, our dependency-driven system 
achieves the best F1-score of 54.7 among all 
feature-based systems. 
In order to measure the generalization abil-
ity of our dependency-driven PPI extraction 
system across different corpora, we further 
apply our method to other four publicly avail-
able PPI corpora: BioInfer, HPRD50, IEPA 
and LLL.  
Table 2: Comparison with other PPI extraction 
systems in the AIMed corpus 
The corresponding performance of 
F1-score and AUC metrics as well as their 
standard deviations is present in Table 3.  
Comparative available results from Airola et 
al. (2008) and Miwa et al (2009a) are also 
included in Table 3 for comparison. This table 
shows that our system performs almost 
consistently with the other two systems, that is, 
the LLL corpus gets the best performance yet 
with the greatest variation, while the AIMed 
corpus achieves the lowest performance with 
reasonable variation. 
It is well known that biomedical texts ex-
hibit distinct linguistic characteristics from 
newswire narratives, leading to dramatic per-
formance gap between PPI extraction and 
relation detection in the ACE corpora. How-
ever, no previous work has ever addressed this 
problem and empirically characterized this 
difference. In this paper, we devise a series of 
experiments over the ACE RDC corpora using 
our dependency-driven feature-based method 
as a touchstone task. In order to do that, a sub-
                                                          
5 Airola et al (2008) repeat the method published by 
Giuliano et al (2006) with a correctly preprocessed 
AIMed and reported an F1-score of 52.4%. 
6 The results from Table 1 (Miyao et al, 2009) with the 
most similar settings to ours (Stanford Parser with SD 
representation) are reported. 
Systems P(%) R(%) F1
Feature-based methods 
Our system 63.4 48.8 54.7
Giuliano et al, 20065 60.9 57.2 59.0
S?tre et al, 2007 64.3 44.1 52.0
Mitsumori et al, 2006 54.2 42.6 47.7
Yakushiji et al, 2005 33.7 33.1 33.4
Kernel-based methods 
Kim et al, 2010 61.4 53.3 56.7
Airola et al, 2008 52.9 61.8 56.4
Bunescu et al, 2006  65.0 46.4 54.2
Composite kernels 
Miwa et al, 2009a - - 62.0
Miyao et al, 20086 51.8 58.1 54.5
762
set of 5796 relation instances is randomly 
sampled from the ACE 2003 and 2004 cor-
pora respectively.  The same cross-validation 
and evaluation metrics are applied to these 
two sets as PPI extraction in the AIMed cor-
pus.
Our system Airola et al (2008) 7 Miwa et al (2009a) 
Corpus F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC
AIMed 54.7 4.5 82.4 3.5 56.4 5.0 84.8 2.3 60.8 6.6 86.8 3.3
BioInfer 59.8 3.5 80.9 3.3 61.3 5.3 81.9 6.5 68.1 3.2 85.9 4.4
HPRD50 64.9 13.4 79.8 8.5 63.4 11.4 79.7 6.3 70.9 10.3 82.2 6.3
IEPA 62.1 6.2 74.8 6.6 75.1 7.0 85.1 5.1 71.7 7.8 84.4 4.2
LLL 78.1 15.8 85.1 8.3 76.8 17.8 83.4 12.2 80.1 14.1 86.3 10.8
Table 3: Comparison of performance across the five PPI corpora 
AIMed ACE2003 ACE2004 
Features
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 
Words 59.4 40.6 47.6 66.5 51.6 57.9 68.1 59.6 63.4
+Overlap +1.0 -0.7 -0.2 +5.4 +1.8 +3.2 +4.6 +1.2 +2.7
+Chunk -1.7 +4.6 +3.2 +2.3 +5.1 +4.0 +1.5 +1.9 +1.7
+Parse +1.7 +0.3 +0.8 +0.3 +0.6 +0.5 +0.6 +0.4 +0.5
+Dependency Set1 +2.0 +3.2 +2.5 +0.8 +0.7 +0.7 +0.5 +0.9 +0.7
+Dependency Set2 +0.5 +0.8 +0.8 +0.3 +0.2 +0.3 +0.2 +0.4 +0.3
Table 4: Comparison of contributions of different features to relation detection across multiple domains 
Table 4 compares the performance of our 
method over different domains. The table re-
ports that the words features alone achieve the 
best F1-score of 63.4 in ACE2004 but the low-
est F1-score of 47.6 in AIMed. This suggests 
the wide difference of lexical distribution be-
tween these domains. We extract the words 
appearing before the 1st mention, between the 
two mentions and after the 2nd mention from 
the training sets of these corpora respectively, 
and summarize the statistics (the number of 
tokens, the number of occurrences) in Table 5, 
where the KL divergence between positives 
and negatives is summed over the distribution 
of the 500 most frequently occurring words. 
                                                          
7 The performance results of F1 and AUC on the BioInfer corpus are slightly adjusted according to Table 3 in Miwa et 
al. (2009b) 
Table 5: Lexical statistics on three corpora 
The table shows that AIMed uses the most 
kinds of words and the most words around the 
two mentions than the other two. More impor-
tant, AIMed has the least distribution differ-
ence between the words appearing in positives 
and negatives, as indicated by its least KL 
divergence. Therefore, the lexical words in 
AIMed are less discriminative for relation 
detection than they do in the other two. This 
naturally explains the reason why the perform-
ance by words feature alone is 
AIMed<ACE2003<ACE2004. In addition, 
Table 4 also shows that: 
x The overlap features significantly improve 
the performance in ACE while slightly 
deteriorating that in AIMed. The reason is 
that, as indicated in Zhou et al (2005), most 
of the positive relation instances in ACE 
exist in local contexts, while the positive 
interactions in AIMed occur in relative 
long-range just as the negatives, therefore 
these features are not discriminative for 
AIMed.
Statistics AIMed ACE2003 ACE2004
# of tokens 2,340 2,064 2,099
# of occurrences 69,976 53,744 49,570
KL divergence  0.22 0.28 0.33 
x The chunk features consistently greatly 
boost the performance across multiple cor-
pora. This implies that the headwords in 
chunk phrases can well capture the partial 
nature of relation instances regardless of 
their genre. 
x It?s not surprising that the parse feature 
attain moderate performance gain in all do-
mains since these parse paths are usually 
763
long and specificity, leading to data 
sparseness problem. 
x It is interesting to note that the depend-
ency-related features exhibit more signifi-
cant improvement in AIMed than that in 
ACE. The reason may be that, these 
dependency features can effectively cap-
ture long-range relationships prevailing in 
AIMed, while in ACE a large number of 
local relationships dominate the corpora. 
5 Related Work 
Among feature-based methods, the PreBIND 
system (Donaldson et al, 2003) uses words and 
word bi-grams features to identify the existence 
of protein interactions in abstracts and such 
information is used to enhance manual expert 
reviewing for the BIND database. Mitsumori et 
al. (2006) use SVM to extract protein-protein 
interactions, where bag-of-words features, spe-
cifically the words around the protein names, 
are employed. Sugiyama et al (2003) extract 
various features from the sentences based on 
the verbs and nouns in the sentences such as the 
verbal forms, and the part-of-speech tags of the 
20 words surrounding the verb. In addition to 
word features, Giuliano et al (2006) extract 
shallow linguistic information such as POS tag, 
lemma, and orthographic features of tokens for 
PPI extraction. Unlike our dependency-driven 
method, these systems do not consider any 
syntactic information.  
For kernel-based methods, there are several 
systems which utilize dependency information. 
Erkan et al (2007) defines similarity functions 
based on cosine similarity and edit distance 
between dependency paths, and then incorpo-
rate them in SVM and KNN learning for PPI 
extraction. Airola et al (2008) introduce 
all-dependency-paths graph kernel to capture 
the complex dependency relationships between 
lexical words and attain significant perform-
ance boost at the expense of computational 
complexity. Kim et al (2010) adopt 
walk-weighted subsequence kernel based on 
dependency paths to explore various substruc-
tures such as e-walks, partial match, and 
non-contiguous paths. Essentially, their kernel 
is also a graph-based one. 
For composite kernel methods, S?tre et al 
(2007) combine a ?bag-of-words? kernel with 
dependency and PAS (Predicate Argument 
Structure) tree kernels to exploit both the words 
features and the structural syntactic information. 
Hereafter, Miyao et al (2008) investigate the 
contribution of various syntactic features using 
different representations from dependency 
parsing, phrase structure parsing and deep 
parsing by different parsers. Miwa et al 
(2009a) integrate ?bag-of-words? kernel, PAS 
tree kernel and all-dependency-paths graph 
kernel to achieve the higher performance. They 
(Miwa et al, 2009b) also use similar compos-
ite kernels for corpus weighting learning 
across multiple PPI corpora.  
6 Conclusion and Future Work 
In this paper, we have combined various lexical 
and syntactic features, particularly dependency 
information, into a feature-based PPI extraction 
system. We find that the dependency informa-
tion as well as the chunk features contributes 
most to the performance improvement.  The 
predicate features involved in the dependency 
tree can also moderately enhance the perform-
ance. Furthermore, comparative study between 
biomedical domain and the ACE newswire 
domain shows that these domains exhibit 
different lexical characteristics, rendering the 
task of PPI extraction much more difficult than 
that of relation detection from the ACE cor-
pora.
In future work, we will explore more syntac-
tic features such as PAS information for fea-
ture-based PPI extraction to further boost the 
performance. 
Acknowledgment 
This research is supported by Projects 
60873150 and 60970056 under the National 
Natural Science Foundation of China and Pro-
ject BK2008160 under the Natural Science 
Foundation of Jiangsu, China. We are also very 
grateful to Dr. Antti Airola from Truku 
University for providing partial experimental 
materials. 
References 
A. Airola, S. Pyysalo, J. Bj?rne, T. Pahikkala, F. 
Ginter, and T. Salakoski. 2008. All-paths graph 
kernel for protein-protein interaction extraction 
764
with evaluation of cross corpus learning. BMC
Bioinformatics.
R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney, 
A. Ramani, and Y. Wong. 2005. Comparative 
Experiments on learning information extractors 
for Proteins and their interactions. Journal of 
Artificial Intelligence In Medicine, 33(2).  
R. Bunescu and R. Mooney. 2005. Subsequence 
kernels for relation extraction. In Proceedings of  
NIPS?05, pages 171?178. 
A. Culotta and J. Sorensen. 2004.  Dependency 
Tree Kernels for Relation Extraction. In 
Proceedings of ACL?04.
I. Donaldson, J. Martin, B. de Bruijn, C. Wolting, V. 
Lay, B. Tuekam, S. Zhang, B. Baskin, G. D. 
Bader, K. Michalockova, T. Pawson, and C. W. V. 
Hogue. 2003. Prebind and textomy - mining the 
biomedical literature for protein-protein 
interactions using a support vector machine. 
Journal of BMC Bioinformatics, 4(11). 
G. Erkan, A. ?zg?r, and D.R. Radev. 2007. 
Semi-Supervised Classification for Extracting 
Protein Interaction Sentences using Dependency 
Parsing, In Proceedings of EMNLP-CoNLL?07,
pages 228?237. 
C. Giuliano, A. Lavelli, and L. Romano. 2006. 
Exploiting Shallow Linguistic Information for 
Relation Extraction from Biomedical Literature. 
In Proceedings of EACL?06, pages 401?408. 
 S. Kim, J. Yoon, J. Yang, and S. Park. 2010. 
Walk-weighted subsequence kernels for 
protein-protein interaction extraction. Journal of 
BMC Bioinformatics, 11(107). 
J. Li, Z. Zhang, X. Li, and H. Chen. 2008. 
Kernel-Based Learning for Biomedical Relation 
extraction. Journal of the American Society for 
Information Science and Technology, 59(5). 
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. 
Doi. 2006. Extracting protein-protein interaction 
information from biomedical text with SVM. 
IEICE Transactions on Information and System, 
E89-D (8).
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009a. 
Protein-Protein Interaction Extraction by 
Leveraging Multiple Kernels and Parsers. 
Journal of Medical Informatics, 78(2009). 
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009b.  
A Rich Feature Vector for Protein-Protein 
Interaction Extraction from Multiple Corpora. In 
Proceedings of EMNLP?09, pages 121?130.  
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and 
J.Tsujii. 2008. Task-oriented evaluation of 
syntactic parsers and their representations. In 
Proceedings of ACL?08, pages 46?54. 
T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi. 
2001. Automated extraction of information on 
protein-protein interactions from the biological 
literature. Journal of Bioinformatics, 17(2). 
 K. Sugiyama, K. Hatano, M. Yoshikawa, and S. 
Uemura. 2003. Extracting information on 
protein-protein interactions from biological 
literature based on machine learning approaches. 
Journal of Genome Informatics, (14): 699?700. 
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic 
features for protein-protein interaction extraction. 
In Proceedings of LBM?07, pages 6.1?6.14. 
A. Yakushiji, M. Yusuke, T. Ohta, Y. Tateishi, J. 
Tsujii. 2006. Automatic construction of 
predicate-argument structure patterns for 
biomedical information extraction. In 
Proceedings of EMNLP?06, pages 284?292. 
S.B. Zhao and R. Grishman. 2005. Extracting 
Relations with Integrated Information Using 
Kernel Methods. In Proceedings of ACL?05,
pages 419-426.  
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. 
Exploring various knowledge in relation 
extraction.  In Proceedings of ACL?05, pages 
427-434.  
765
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 346?355,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Clustering-based Stratified Seed Sampling for Semi-Supervised Relation 
Classification 
 
 
Longhua Qian Guodong Zhou 
Natural Language Processing Lab Natural Language Processing Lab 
School of Computer Science and Technology School of Computer Science and Technology 
Soochow University Soochow University 
1 Shizi Street, Suzhou, China 215006 1 Shizi Street, Suzhou, China 215006 
qianlonghua@suda.edu.cn gdzhou@suda.edu.cn 
 
 
 
Abstract 
Seed sampling is critical in semi-supervised 
learning. This paper proposes a clustering-
based stratified seed sampling approach to 
semi-supervised learning.  First, various clus-
tering algorithms are explored to partition the 
unlabeled instances into different strata with 
each stratum represented by a center. Then, 
diversity-motivated intra-stratum sampling is 
adopted to choose the center and additional 
instances from each stratum to form the unla-
beled seed set for an oracle to annotate. Fi-
nally, the labeled seed set is fed into a 
bootstrapping procedure as the initial labeled 
data. We systematically evaluate our stratified 
bootstrapping approach in the semantic rela-
tion classification subtask of the ACE RDC 
(Relation Detection and Classification) task. 
In particular, we compare various clustering 
algorithms on the stratified bootstrapping per-
formance. Experimental results on the ACE 
RDC 2004 corpus show that our clustering-
based stratified bootstrapping approach 
achieves the best F1-score of 75.9 on the sub-
task of semantic relation classification, ap-
proaching the one with golden clustering. 
1 Introduction 
Semantic relation extraction aims to detect and 
classify semantic relationships between a pair of 
named entities occurring in a natural language text. 
Many machine learning approaches have been pro-
posed to attack this problem, including supervised 
(Miller et al, 2000; Zelenko et al, 2003; Culotta 
and Soresen, 2004; Kambhatla, 2004; Zhao and 
Grishman, 2005; Zhou et al, 2005; Zhang et al, 
2006; Zhou and Zhang, 2007; Zhou et al, 2007; 
Qian et al, 2008; Zhou et al, 2010), semi-
supervised (Brin, 1998; Agichtein and Gravano, 
2000; Zhang, 2004; Chen et al, 2006; Qian et al, 
2009; Zhou et al, 2009), and unsupervised meth-
ods (Hasegawa et al, 2004; Zhang et al, 2005; 
Chen et al, 2005). 
Current work on relation extraction mainly 
adopts supervised learning methods, since they 
achieve much better performance. However, they 
normally require a large number of manually la-
beled relation instances, whose acquisition is both 
time consuming and labor intensive. In contrast, 
unsupervised methods do not need any manually 
labeled instances. Nevertheless, it is difficult to 
assess their performance due to the lack of evalua-
tion criteria. As something between them, semi-
supervised learning has received more and more 
attention recently. With the plenitude of unlabeled 
natural language text at hand, semi-supervised 
learning can significantly reduce the need for la-
beled data with only limited sacrifice in perform-
ance. For example, Abney (2002) proposes a 
bootstrapping algorithm which chooses the unla-
beled instances with the highest probability of be-
ing correctly labeled and add them in turn into the 
labeled training data iteratively. 
This paper focuses on bootstrapping-based semi-
supervised learning in relation extraction. Since the 
performance of bootstrapping depends much on the 
quality and quantity of the seed set and researchers 
tend to employ as few seeds as possible (e.g. 100 
instances) to save time and labor, the quality of the 
seed set plays a critical role in bootstrapping. Fur-
thermore, the imbalance of different classes and 
346
the inherent structural complexity of instances will 
severely weaken the strength of bootstrapping and 
semi-supervised learning as well. Therefore, it is 
critical for a bootstrapping procedure to select an 
appropriate seed set, which should be representa-
tive and diverse. However, most of current semi-
supervised relation extraction systems (Zhang, 
2004; Chen et al, 2006) use a random seed sam-
pling strategy, which fails to fully exploit the affin-
ity nature in the training data to derive the seed set. 
Alternatively, Zhou et al (2009) bootstrap a set of 
weighted support vectors from both labeled and 
unlabeled data using SVM and feed these instances 
into semi-supervised relation extraction. However, 
their seed set is sequentially generated only to en-
sure that there are at least 5 instances for each rela-
tion class. Our previous work (Qian et al, 2009) 
attempts to solve this problem via a simple strati-
fied sampling strategy for selecting the seed set. 
Experimentation on the ACE RDC 2004 corpus 
shows that the stratified sampling strategy achieves 
promising results for semi-supervised learning. 
Nevertheless, the success of the strategy relies on 
the assumption that the true distribution of all rela-
tion types is already known, which is impractical 
for real NLP applications. 
This paper presents a clustering-based stratified 
seed sampling approach for semi-supervised rela-
tion extraction, without the assumption on the true 
distribution of different relation types. The motiva-
tions behind our approach are that the unlabeled 
data can be partitioned into a number of strata us-
ing a clustering algorithm and that representative 
and diverse seeds can be derived from such strata 
in the framework of stratified sampling (Neyman, 
1934) for an oracle to annotate. Particularly, we 
employ a diversity-motivated intra-stratum sam-
pling scheme to pick a center and additional in-
stances as seeds from each stratum. Experimental 
results show the effectiveness of the clustering-
based stratified seed sampling for semi-supervised 
relation classification. 
The rest of this paper is organized as follows. 
First an overview of the related work is given in 
Section 2. Then, Section 3 introduces the stratified 
bootstrapping framework including an intra-
stratum sampling scheme while Section 4 describes 
various clustering algorithms. The experimental 
results on the ACE RDC 2004 corpus are reported 
in Section 5. Finally we conclude our work and 
indicate some future directions in Section 6. 
2 Related Work 
In semi-supervised learning for relation extraction, 
most of previous work construct the seed set either 
randomly (Zhang, 2004; Chen et al, 2006) or se-
quentially (Zhou et al, 2009). Qian et al (2009) 
adopt a stratified sampling strategy to select the 
seed set. However, their method needs a stratifica-
tion variable such as the known distribution of the 
relation types, while our method uses clustering to 
divide relation instances into different strata. 
In the literature, clustering techniques have been 
employed in active learning to sample representa-
tive seeds in a certain extent (Nguyen and 
Smeulders, 2004; Tang et al, 2002; Shen et al, 
2004). Our work is similar to the formal frame-
work, as proposed in Nguyen and Smeulders 
(2004), in which K-medoids clustering is incorpo-
rated into active learning. The cluster centers are 
used to construct a classifier and which in turn 
propagates classification decision to other exam-
ples via a local noise model. Unlike their probabil-
istic models, we apply various clustering 
algorithms together with intra-stratum sampling to 
select a seed set in discriminative models like 
SVMs. In active learning for syntactic parsing, 
Tang et al (2002) employ a sampling strategy of 
?most uncertain per cluster? to select representa-
tive examples and weight them using their cluster 
density, while we pick a few seeds (the number of 
the sampled seeds is proportional to the cluster 
density) from a cluster in addition to its center. 
Shen et al (2004) combine multiple criteria to 
measure the informativeness, representativeness, 
and diversity of examples in active learning for 
named entity recognition. Unlike our sampling 
strategy of clustering for representativeness and 
stratified sampling for diversity, they either select 
cluster centroids or diverse examples from a pre-
chosen set in terms of some combined metrics. To 
the best of our knowledge, this is the first work to 
address the issue of seed selection using clustering 
techniques for semi-supervised learning with dis-
criminative models. 
3 Stratified Bootstrapping Framework 
The stratified bootstrapping framework consists of 
three major components: an underlying supervised 
learner and a bootstrapping algorithm on top of it 
347
as usual, plus a clustering-based stratified seed 
sampler. 
3.1 Underlying Supervised Learner 
Due to recent success in tree kernel-based relation 
extraction, this paper adopts a tree kernel-based 
method in the underlying supervised learner. Fol-
lowing the previous work in relation extraction 
(Zhang et al, 2006; Zhou et al, 2007; Qian et al, 
2008), we use the standard convolution tree kernel 
(Collins and Duffy, 2001) to count the number of 
common sub-trees as the structural similarity be-
tween two parse trees. Besides, to properly repre-
sent a relation instance, this paper adopts the 
Unified Parse and Semantic Tree (UPST), as pro-
posed in Qian et al (2008). To our knowledge, the 
USPT has achieved the best performance in rela-
tion extraction so far on the ACE RDC 2004 cor-
pus. 
In particular, we use the SVMlight-TK1 package 
as our classifier. Since the package is a binary clas-
sifier, we adapt it to the multi-class tasks of rela-
tion extraction by applying the one vs. others 
strategy, which builds K binary classifiers so as to 
separate one class from all others. The final classi-
fication decision of an instance is determined by 
the class that has the maximal SVM output margin. 
3.2 Bootstrapping Algorithm 
Following Zhang (2004), we have developed a 
baseline self-bootstrapping procedure, which keeps 
augmenting the labeled data by employing the 
models trained from previously available labeled 
data, as shown in Figure 1. 
Since the SVMlight-TK package doesn?t output 
any probability that it assigns to the class label on 
an instance, we devise a metric to measure the con-
fidence with regard to the classifier?s prediction. 
Given a sequence of output margins of all K binary 
classifiers at some iteration, denoted as 
{m1,m2,?mK} with mi the margin for the i-th clas-
sifier, we compute the margin gap between the 
largest and the mean of the others, i.e. 
)1/()max(max
11
1
???=
===
? KmmmH K
i
i
K
i
ii
K
i
  (1) 
Where K denotes the total number of relation 
classes, and mi denotes the output margin of the i-
                                                          
1 http://ai-nlp.info.uniroma2.it/moschitti/ 
Require: labeled seed set L
Require: unlabeled data set U
Require: batch size S
Repeat
    Train a single classifier on L
    Run the classifier on U
    Find at most S instances in U that the classifier has
the highest prediction confidence
    Add them into L
Until: no data points available or the stoppage
condition is reached
Algorithm self-bootstrapping
 
Figure 1: Self-bootstrapping algorithm 
th classifier. Intuitively, the bigger the H, the 
greater the difference between the maximal margin 
and all others, and thus the more reliably the classi-
fier makes the prediction on the instance.  
3.3 Clustering-based Stratified Seed Sampler 
Stratified sampling is a method of sampling in 
statistics, in which the members of a population are 
grouped into relatively homogeneous subgroups 
(i.e. strata) according to one certain property, and 
then a sample is selected from each stratum. This 
process of grouping is called stratification, and the 
property on which the stratification is performed is 
called the stratification variable. Previous work 
justifies theoretically and practically that stratified 
sampling is more appropriate than random sam-
pling for general use (Neyman, 1934) as well as for 
relation extraction (Qian et al, 2009). However, 
the difficulty lies in how to find the appropriate 
stratification variable for complicated tasks, such 
as relation extraction. 
The idea of clustering-based stratification cir-
cumvents this problem by clustering the unlabeled 
data into a number of strata without the need to 
explicitly specify a stratification variable. Figure 2 
illustrates the clustering-based stratified seed sam-
pling strategy employed in the bootstrapping pro-
cedure, where RSET denotes the whole unlabeled 
data, SeedSET the seed set to be labeled and 
|RSETi| the number of instances in the i-th cluster2 
RSETi. Here, a relation instance is represented us-
ing USPT and the similarity between two instances 
is computed using the standard convolution tree 
                                                          
2 Hereafter, when we refer to clusters from the viewpoint of 
stratified sampling, they are often called ?strata?. 
348
kernel, as described in Section 3.1 (i.e., both the 
clustering and the classification adopt the same 
structural representation, since we want the repre-
sentative seeds in the clustering space to be also 
representative in the classification space). After 
clustering, a certain number of instances from 
every stratum are sampled using an intra-stratum 
scheme (c.f. Subsection 3.4). Normally, this num-
ber is proportional to the size of that stratum in the 
whole data set. However, in case this number is 0 
due to the rounding of real numbers, it is set to 1 to 
ensure the existence of at least one seed from that 
stratum. Furthermore, to ensure that the total num-
ber of instances being sampled equals the pre-
scribed NS, the number of seeds from dominant 
strata may be slightly adjusted accordingly. Finally, 
these instances form the unlabeled seed set for an 
oracle to annotate as the input to the underlying 
supervised learner in the bootstrapping procedure. 
3.4 Intra-stratum sampling 
Given the distribution of clusters, a simple way to 
select the most representative instances is to 
choose the center of each cluster with the cluster 
prior as the weight of the center (Tang et al, 2002; 
Nguyen and Smeulders, 2004). Nevertheless, for 
the complicated task of relation extraction on the 
ACE RDC corpora, which is highly skewed across 
different relation classes, only considering the cen-
ter of each cluster would severely under-represent 
the high-density data. To overcome this problem, 
we adopt a sampling approach, in particular strati-
fied sampling, which takes the size of each stratum 
into consideration. 
Given the size of the seed set NS and the number 
of strata K, a natural question will arise as how to 
select the remaining (NS-K) seeds after we have 
extracted the K centers from the K strata. We view 
this problem as intra-stratum sampling, which is 
required to choose the remaining number of seeds 
from inside individual stratum (excluding the cen-
ters themselves).  
At the first glance, sampling a certain number of 
seeds from one particular stratum (e.g., RSETi), 
seems to be the same sampling problem as we have 
encountered before, which aims to select the most 
representative and diverse seeds. This will natu-
rally lead to another application of a clustering al-
gorithm to the stratification of the stratum RSETi.  
Require: RSET ={R1,R2,?,RN}, the set of unlabeled 
relation instances and K, the number of strata being 
clustered 
Output: SeedSET with the size of NS (100) 
Procedure 
Initialize SeedSET = NULL 
Cluster RSET into K strata using a clustering 
algorithm and perform stratum pruning if 
necessary. 
Calculate the number of instances being sampled 
for each stratum i={1,2,?,K} 
S
i
i NN
RSET
N ?= ||    (2) 
and adjust this number if necessary. 
Perform intra-strata sampling to form SeedSETi 
from each stratum RSETi, by selecting the center 
Ci and (Ni-1) additional instances 
Generate SeedSET by summating RSETi from each 
stratum 
 
Figure 2: Clustering-based stratified seed sampling  
Nevertheless, remember the fact that, this time for 
the stratum RSETi, the center Ci has been chosen, 
so it may not be reasonable to extract additional 
centers in this way. Therefore, in order to avoid 
recursion and over-complexity, we employ a diver-
sity-motivated intra-stratum sampling scheme 
(Shen et al, 2004), called KDN (K-diverse 
neighbors), which aims to maximize the training 
utility of all seeds from a stratum. The motivation 
is that we prefer the seeds with high variance to 
each other, thus avoiding repetitious seeds from a 
single stratum. The basic idea is to add a candidate 
instance to the seed set only if it is sufficiently dif-
ferent from any previously selected seeds, i.e., the 
similarity between the candidate instance and any 
of the current seeds is less than a threshold ?. In 
this paper, the threshold ? is set to the average 
pair-wise similarity between any two instances in a 
stratum.  
4 Clustering Algorithms 
This section describes several typical clustering 
algorithms in the literature, such as K-means, HAC, 
spectral clustering and affinity propagation, as well 
as their application in this paper. 
4.1 K-medoids (KM) 
As a simple yet effective clustering method, the K-
means algorithm assigns each instance to the clus-
ter whose center (also called centroid) is nearest. In 
349
particular, the center is the average of all the in-
stances in the cluster, i.e., with its coordinates the 
arithmetic means for each dimension separately 
over all the instances in the cluster. 
One problem with K-means is that it does not 
yield the same result with each run while the other 
problem is the requirement for the concept of a 
mean to be definable, which is unfortunately not 
available in our setting (we employ a parse tree 
representation for a relation instance). Hence, we 
adopt a variant of K-means, namely, K-medoids, 
where a medoid, rather than a centroid, is defined 
as a representative of a cluster. Besides, K-
medoids has proved to be more robust to noise and 
outliers in comparison with K-means. 
4.2 Hierarchical Agglomerative Clustering 
(HAC) 
Different from K-medoids, hierarchical clustering 
creates a hierarchy of clusters which can be 
represented in a tree structure called a dendrogram. 
The root of the tree consists of a single cluster 
containing all objects, and the leaves correspond to 
individual object.  
Typically, hierarchical agglomerative clustering 
(HAC) starts at the leaves and successively merges 
two clusters together as long as they have the 
shortest distance among all the pair-wise distances 
between any two clusters.  
Given a specified number of clusters, the key 
problem is to determine where to cut the hierarchi-
cal tree into clusters. In this paper, we generate the 
final flat cluster structures greedily by maximizing 
the equal distribution of instances among different 
clusters. 
4.3 Spectral Clustering (SC) 
Spectral clustering has become more and more 
popular recently. Taking as input a similarity 
matrix between any two instances, spectral 
clustering makes use of the spectrum of the 
similarity matrix of the data to perform 
dimensionality reduction for clustering in fewer 
dimensions.  
Compared to the ?traditional algorithms? such 
as K-means or HAC, spectral clustering has many 
fundamental advantages. Results obtained by 
spectral clustering often outperform the traditional 
approaches. Furthermore, spectral clustering is 
very simple to implement and can be solved 
efficiently using standard linear algebra methods 
(von Luxburg, 2006). 
4.4 Affinity Propagation (AP) 
As a new emerging clustering algorithm, affinity 
propagation (AP) (Frey and Dueck, 2007) is basi-
cally an iterative message-passing procedure in 
which the instances being clustered compete to 
serve as cluster exemplars by exchanging two 
types of messages, namely, ?responsibility? and 
?availability?.  After the procedure converges or 
has repeated a finite number of iterations, each 
cluster is represented by an exemplar. AP was re-
ported to find clusters with much lower error than 
those found by other methods. 
For our application, affinity propagation takes as 
input a similarity matrix, whose elements represent 
either the similarity between two different in-
stances or the preference (a real number p) for an 
instance when two instances are the same. One 
problem with AP is that the number of clusters 
cannot be pre-defined, which is indirectly deter-
mined by the preference as well as the convergence 
procedure itself. 
5 Experimentation 
This section systematically evaluates the boot-
strapping approach using clustering-based strati-
fied seed sampling, in the relation classification 
(i.e., given the relationship already detected) sub-
task of relation extraction on the ACE RDC 2004 
corpus. 
5.1 Experimental Setting 
The ACE RDC 2004 corpus 3  is gathered from 
various newspapers, newswire and broadcasts. It 
contains 451 documents and 5702 positive relation 
instances of 7 relation types and 23 subtypes be-
tween 7 entity types. For easy reference with re-
lated work in the literature, evaluation is done on 
347 documents (from nwire and bnews domains), 
which include 4305 relation instances. Table 1 lists 
the major relation types and subtypes, including 
their corresponding instance numbers and ratios in 
our evaluation set. One obvious observation from 
the table is that the numbers of different relation 
types is highly imbalanced. These 347 documents 
are then divided into 3 disjoint sets randomly, with 
                                                          
3 http//www.ldc.upenn.edu/ Projects/ACE/ 
350
Types Subtypes # % 
Located 738 17.1 
Near 87 2.0 PHYS 
Part-Whole 378 8.8 
Business 173 4.0 
Family 121 2.8 PER-SOC 
Other 55 1.3 
Employ-Executive 489 11.4 
Employ-Staff 539 12.5 
Employ-Undeter. 78 1.8 
Member-of-Group 191 4.4 
Subsidiary 206 4.8 
Partner 12 0.3 
EMP-
ORG 
Other 80 1.9 
User-or-Owner 200 4.6 
Inventor-or-Man. 9 0.2 ART 
Other 2 0.0 
Ethnic 39 0.9 
Ideology 48 1.1 OTHER-AFF 
Other 54 1.3 
Citizen-or-Resid. 273 6.3 
Based-In 215 5.0 GPE-AFF 
Other 39 0.9 
DISC   279 6.5 
Total   4305 100.0 
Table 1: Relation types and their corresponding instance 
numbers and ratios in the ACE RDC 2004 corpus 
 
10% of them (35 files, around 400 instances) held 
out as the test data set, 10% of them (35 files, 
around 400 instances) used as the development 
data set to fine-tune various settings and parame-
ters, while the remaining 277 files (over 3400 in-
stances) used as the training data set, from which 
the seed set will be sampled. 
The corpus is parsed using Charniak?s parser 
(Charniak, 2001) and relation instances are gener-
ated by extracting all pairs of entity mentions oc-
curring in the same sentence with positive 
relationships. For easy comparison with related 
work, we only evaluate the relation classification 
task on the 7 major relation types of the ACE RDC 
2004 corpus. For the SVMlight-TK classifier, the 
training parameters C (SVM) and ? (tree kernel) 
are fine-tuned to 2.4 and 0.4 respectively.  
The performance is measured using the standard 
P/R/F1 (Precision/Recall/F1-measure). For each 
relation type, P is the ratio of the true relation in-
stances in all the relation instances being identified, 
R is the ratio of the true relation instances being 
identified in all the true relation instances in the 
corpus, and F1 is the harmonic mean of P and R. 
The overall performance P/R/F1 is then calculated 
using the micro-average measure over all major 
class types. 
5.2 Experimental Results 
Comparison of various seed sampling strategies 
without intra-stratum sampling on the devel-
opment data 
Table 2 compares the performance of bootstrap-
ping-based relation classification using various 
seed sampling strategies without intra-stratum 
sampling on the development data. Here, the size 
of the seed set L is set to 100, and the top 100 in-
stances with the highest confidence (c.f. Formula 1) 
are augmented at each iteration. For sampling 
strategies marked with an asterisk, we performed 
10 trials and calculated their averages. Since for 
these strategies the seed sets sampled from differ-
ent trials may be quite different, their performance 
scores vary in a great degree accordingly. This ex-
perimental setting and notation are also used in all 
the subsequent experiments unless specified. Be-
sides, two additional baseline sampling strategies 
are included for comparison: sequential sampling 
(SEQ), which selects a sequentially-occurring L 
instances as the seed set, and random sampling 
(RAND), which randomly selects L instances as 
the seed set. 
Table 2 shows that 
1) RAND outperforms SEQ by 1.2 units in F1-
score. This is due to the fact that the seed set 
via RAND may better reflect the distribution of 
the whole training data than that via SEQ, nev-
ertheless at the expense of collecting the whole 
training data in advance. 
2) While HAC performs moderately better than 
RAND, it is surprising that both KM and AP 
perform even worse than SEQ, and that SC per-
forms worse than RAND. Furthermore, all the 
four clustering-based seed sampling strategies 
achieve much smaller performance improve-
ment in F1-score than RAND, among which 
KM performs worst with performance im-
provement of only 0.1 in F1-score. 
 
351
Sampling 
strategies P(?P) R(?R) F1(?F1) 
RAND* 69.1(3.1) 66.4(0.2) 67.8(2.0) 
SEQ* 65.8(2.6) 68.0(0.1) 66.6(1.3) 
KM* 62.0(0.9) 61.0(-0.5) 61.3(0.1) 
HAC 69.9(1.3) 70.4(0.4) 70.1(0.8) 
SC* 67.1(1.5) 68.1(0.0) 67.5(0.8) 
AP 66.6(2.0) 66.2(0.1) 66.4(1.1) 
Table 2: Comparison of various seed sampling strate-
gies without intra-stratum sampling on the development 
data 
3) All the performance improvements from boot-
strapping largely come from the improvements 
in precision. While the bootstrapping procedure 
makes the SVM classifier more accurate, it 
lacks enough generalization ability.  
To explain above special phenomena, we have a 
look at the clustering results. Our inspection re-
veals that most of them are severely imbalanced, 
i.e., some clusters are highly dense while others are 
extremely sparse. This indicates that merely select-
ing the centers from each cluster cannot properly 
represent the overall distribution. Moreover, the 
centers with high density lack the generalization 
ability due to its solitude in the cluster, leading to 
less performance enhancement than expected. 
The only exception is HAC, which much outper-
forms RAND by 2.3 in F1-score, although HAC is 
usually not considered as an effective clustering 
algorithm. The reason may be that HAC creates a 
hierarchy of clusters in the top-down manner by 
cutting a cluster into two. Therefore, the centers in 
the two sibling clusters will be closer to each other 
than they are to the centers in other clusters. Be-
sides, the final flat cluster structures given a spe-
cial number of clusters are generated greedily from 
the cluster hierarchy by maximizing the equal dis-
tribution of instances among different clusters. In 
other words, when the cluster number reaches a 
certain threshold, the dense area will get more 
seeds represented in the seed set. As a consequence, 
the distribution of all the seeds sampled by HAC 
will approximate the distribution of the whole 
training data in some degree, while the seeds sam-
pled by other clustering algorithm are kept as far as 
possible due to the objective of clustering and the 
lack of intra-stratum sampling. 
These observations also justify the application 
of the stratified seed sampling to the bootstrapping 
procedure, which enforces the number of seeds 
sampled from a cluster to be proportional to its 
density, presumably approximated by its size in 
this paper. 
 
Comparison of different cluster numbers with 
intra-stratum sampling on the development 
data 
In order to fine-tune the optimal cluster numbers 
for seed sampling, we compare the performance of 
different numbers of clusters for each clustering 
algorithm on the development data set and report 
their F-scores in Table 3. For reference, we also 
list the F-score for golden clustering (GOLD), in 
which all instances are grouped in terms of their 
annotated ground relation major types (7), major 
types considering relation direction (13), subtypes 
(23), and subtypes considering direction (38). Be-
sides, the performance of clustering-based semi-
supervised relation classification is also measured 
over other typical cluster numbers (i.e., 1, 50, 60, 
80, 100). Particularly, when the cluster number 
equals 1, it means that only diversity other than 
representativeness is considered in the seed sam-
pling. Among these clustering algorithms, one of 
the distinct characteristics with the AP algorithm is 
that the number of clusters cannot be specified in 
advance, rather, it is determined by the pre-defined 
preference parameter (c.f. Subsection 4.4). There-
fore, we should tune the preference parameter so as 
to get the pre-defined cluster number. However, 
sometimes we still couldn?t get the exact number 
of clusters as we expected. In these cases, we use 
the approximate cluster numbers for AP instead.  
Table 3 shows that 
1) The performance for all the clustering algo-
rithms varies in some degree with the number 
of clusters being grouped. Interestingly, the 
performance with only one cluster is better 
than those of clustering-based strategies with 
100 clusters, at most cases. This implies that 
the diversity of the seeds is at least as impor-
tant as their representativeness. And this could 
be further explained by our observation that, 
with the increase of cluster numbers, the clus-
ters get smaller and denser while their centers 
also come closer to each other. Therefore, the 
representativeness and diversity as well as the 
distribution of the seeds sampled from them 
may vary accordingly, leading to different per-
formance. 
352
# of  
 Clusters GOLD KM* HAC SC* AP 
1 -  68.7  68.7  - - 
7 73.9 70.3  73.3 72.1 - 
13 70.2 68.9  70.3 67.3 - 
23 64.9 72.3  72.9 68.9 71.1 
38 60.8 69.9  71.6 68.0 71.6 
50 - 68.5  69.9 68.5 70.4 
60 - 66.3  68.5 68.6 69.7 
80 - 64.2  65.9 68.0 68.1 
100 - 61.3  70.1 67.5 66.4 
Table 3: Performance in F1-score over different cluster 
numbers with intra-stratum sampling on the develop-
ment data 
2) Golden clustering achieves the best performance 
of 73.9 in F1-score when the cluster number is 
set to 7, significantly higher than the perform-
ance using other cluster numbers. Interestingly, 
this number coincides with the number of major 
relation types needed to be classified in our task. 
This is reasonable since the instances with the 
same relation type should be much more similar 
than those with different relation types and it is 
easy to discriminate the seed set of one relation 
type from that of other relation types. 
3) Among the four clustering algorithms, HAC 
achieves best performance over most of cluster 
numbers. This further verifies the aforemen-
tioned analysis. That is, as a hierarchical clus-
tering algorithm, HAC can sample seeds that 
better capture the distribution of the training 
data. 
4) For KM, the best performance is achieved 
around the number of 23 while for both HAC 
and SC, the optimal cluster number is consis-
tent with GOLD clustering, namely, 7. For AP, 
the optimal cluster number for AP is 38. This is 
largely due to that we fail to cluster the training 
data into about 7 and 13 groups no matter how 
we vary the preference parameter.  
 
Final comparison of different clustering algo-
rithms on the held-out test data  
After the optimal cluster numbers are determined 
for each clustering algorithm, we apply these num-
bers on the held-out test data and report the per-
formance results (P/R/F1 and their respective 
improvements) in Table 4. For easy reference, we 
also include the performance for GOLD, RAND, 
and SEQ sampling strategies.  
 
Sampling 
strategies P(?P) R(?R) F1(?F1) 
GOLD 79.5(7.8) 72.7(2.1) 76.0(4.8) 
RAND* 71.9(3.7) 69.7(0.1) 70.8(1.8) 
SEQ* 71.9(2.6) 65.2(0.1) 69.3(1.3) 
KM* 73.6(2.1) 72.3(0.3) 72.9(1.2) 
HAC 79.0(10.2) 73.0(1.1) 75.9(5.6) 
SC* 72.3(2.1) 72.1(0.4) 72.2(1.2) 
AP 75.7(2.5) 72.0(0.4) 73.7(1.4)
Table 4: Performance of various clustering-based seed 
sampling strategies on the held-out test data with the 
optimal cluster number for each clustering algorithm 
 
Table 4 shows that 
1) Among all the clustering algorithms, HAC 
achieves the best F1-score of 75.9, significantly 
higher than RAND and SEQ by 5.1 and 6.6 re-
spectively. The improvement comes not only 
from significant precision boost, but also from 
moderate recall increase. This further justifies 
the merits of HAC as a clustering algorithm for 
stratified seed sampling in semi-supervised re-
lation classification.  
2) HAC approaches the best F1-score of 76.0 for 
golden clustering. Obviously, this doesn?t mean 
HAC performs as well as golden clustering in 
terms of clustering quality measures, rather it 
does imply that HAC achieves the performance 
improvement by making the seed set better rep-
resent the overall distribution over inherent 
structure of relation instances, while golden 
clustering accomplishes this using the distribu-
tion over relation types. Since the distribution 
over relation types doesn?t always conform to 
that over instance structures, and for a statistical 
discriminative classifier, often the latter is more 
important than the former, it will be no surprise 
if HAC outperforms golden clustering in some 
real applications, e.g. clustering-based stratified 
sampling. 
6 Conclusion and Future Work 
This paper presents a stratified seed sampling 
strategy based on clustering algorithms for semi-
supervised learning. Our strategy does not rely on 
any stratification variable to divide the training 
instances into a number of strata. Instead, the strata 
are formed via clustering, given a metric measur-
ing the similarity between any two instances. Fur-
ther, diversity-motivated intra-strata sampling is 
353
employed to sample additional instances from 
within each stratum besides its center. We compare 
the effect of various clustering algorithms on the 
performance of semi-supervised learning and find 
that HAC achieves the best performance since the 
distribution of its seed set better approximates that 
of the whole training data. Extensive evaluation on 
the ACE RDC 2004 benchmark corpus shows that 
our clustering-based stratified seed sampling strat-
egy significantly improves the performance of 
semi-supervised relation classification. 
We believe that our clustering-based stratified 
seed sampling strategy can not only be applied to 
other semi-supervised learning tasks, but also can 
be incorporated into active learning, where the in-
stances to be labeled at each iteration as well as the 
seed set could be selected using clustering tech-
niques, thus further reducing the amount of in-
stances needed to be annotated.  
For the future work, it is possible to adapt our 
one-level clustering-based sampling to the multi-
level one, where for every stratum it is still possi-
ble to divide it into lower sub-strata for further 
stratified sampling in order to make the seeds bet-
ter represent the true distribution of the data. 
Acknowledgments 
This research is supported by Projects 60873150, 
60970056, and 90920004 under the National Natu-
ral Science Foundation of China. 
References  
S. Abney. 2002. Bootstrapping. ACL-2002. 
E. Agichtein and L. Gravano. 2000. Snowball: Extract-
ing relations from large plain-text collections. In 
Proceedings of the 5th ACM international Conference 
on Digital Libraries (ACMDL 2000).  
S. Brin. 1998. Extracting patterns and relations from the 
world wide web. In WebDB Workshop at 6th Interna-
tional Conference on Extending Database Technol-
ogy (EDBT 98). 
E. Charniak. 2001. Intermediate-head Parsing for Lan-
guage Models. ACL-2001: 116-123. 
M. Collins and N. Duffy. 2001. Convolution Kernels for 
Natural Language. NIPS 2001: 625-632. 
J.X. Chen, D.H. Ji, C.L. Tan, and Z.Y. Niu. 2005. Un-
supervised Feature Selection for Relation Extraction. 
CIKM-2005: 411-418. 
J.X. Chen, D.H. Ji, and C. L. Tan. 2006. Relation Ex-
traction using Label Propagation Based Semi super-
vised Learning. ACL/COLING-2006: 129-136. 
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. ACL-2004: 423-439.  
B.J. Frey and D. Dueck. 2007. Clustering by Passing 
Messages between Data Points. Science, 315: 972-
976. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from 
Large Corpora. ACL-2004. 
N. Kambhatla. 2004. Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. ACL-2004(posters): 178-181.  
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 
2000. A novel use of statistical parsing to extract in-
formation from text. In Proceedings of the 6th Ap-
plied Natural Language Processing Conference. 
J. Neyman. 1934. On the Two Different Aspects of the 
Representative Method: The Method of Stratified 
Sampling and the Method of Purposive Selection. 
Journal of the Royal Statistical Society, 97(4): 558-
625. 
H.T. Nguyen and A. Smeulders. 2004. Active Learning 
Using Pre-clustering, ICML-2004. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D. Qian. 2008. 
Exploiting constituent dependencies for tree kernel-
based semantic relation extraction. COLING-2008: 
697-704. 
L.H. Qian, G.D. Zhou, F. Kong, and Q.M. Zhu. 2009. 
Semi-Supervised Learning for Semantic Relation 
Classification using Stratified Sampling Strategy. 
EMNLP-2009: 1437-1445.  
D. Shen, J. Zhang, J. Su, G. Zhou and C. Tan. 2004. 
Multi-criteria-based active learning for named entity 
recognition. ACL-2004. 
M. Tang, X. Luo and S. Roukos. 2002. Active Learning 
for Statistical Natural Language Parsing. ACL-2002. 
U. von Luxburg. 2006. A tutorial on spectral clustering. 
Technical report, Max Planck Institute for Biological 
Cybernetics. 
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel 
Methods for Relation Extraction. Journal of Machine 
Learning Research, (2): 1083-1106. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree Simi-
larity-Based Clustering. IJCNLP-2005: 378-389.  
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between Enti-
ties with both Flat and Structured Features. 
ACL/COLING-2006: 825-832. 
Z. Zhang. 2004. Weakly-supervised relation classifica-
tion for Information Extraction. CIKM-2004. 
S.B. Zhao and R. Grishman. 2005. Extracting relations 
with integrated information using kernel methods. 
ACL-2005: 419-426. 
354
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring various knowledge in relation extraction. 
ACL-2005: 427-434. 
G.D. Zhou, L.H. Qian, and J.X. Fan. 2010. Tree kernel-
based semantic relation extraction with rich syntactic 
and semantic information. Information Sciences, 
(179): 1785-1791. 
G.D. Zhou, L.H. Qian, and Q.M. Zhu. 2009. Label 
propagation via bootstrapped support vectors for se-
mantic relation extraction between named entities. 
Computer Speech and Language, 23(4): 464-478. 
G.D. Zhou and M. Zhang. 2007. Extraction relation 
information from text documents by exploring vari-
ous types of knowledge. Information Processing and 
Management, (42):969-982. 
G.D. Zhou, M. Zhang, D.H. Ji, and Q.M. Zhu. 2007. 
Tree Kernel-based Relation Extraction with Context-
Sensitive Structured Parse Tree Information. 
EMNLP/CoNLL-2007: 728-736.  
 
355
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 582?592,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Bilingual Active Learning for Relation Classification via Pseudo Paral-
lel Corpora 
 
Longhua Qian    Haotian Hui   Ya?nan Hu   Guodong Zhou*   Qiaoming Zhu 
Natural Language Processing Lab 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,20134227019,20114227025,gdzhou,qmzhu}@suda.edu.cn 
 
  
 
Abstract 
Active learning (AL) has been proven ef-
fective to reduce human annotation ef-
forts in NLP. However, previous studies 
on AL are limited to applications in a 
single language. This paper proposes a 
bilingual active learning paradigm for re-
lation classification, where the unlabeled 
instances are first jointly chosen in terms 
of their prediction uncertainty scores in 
two languages and then manually labeled 
by an oracle. Instead of using a parallel 
corpus, labeled and unlabeled instances 
in one language are translated into ones 
in the other language and all instances in 
both languages are then fed into a bilin-
gual active learning engine as pseudo 
parallel corpora. Experimental results on 
the ACE RDC 2005 Chinese and English 
corpora show that bilingual active learn-
ing for relation classification signifi-
cantly outperforms monolingual active 
learning. 
1 Introduction 
Semantic relation extraction between named en-
tities (aka. entity relation extraction or more con-
cisely relation extraction) is an important subtask 
of Information Extraction (IE) as well as Natural 
Language Processing (NLP). With its aim to 
identify and classify the semantic relationship 
between two entities (ACE 2002-2007), relation 
extraction is of great significance to many NLP 
applications, such as question answering, infor-
mation fusion, social network construction, and 
knowledge mining and population etc. 
                                                 
* Corresponding author 
In the literature, the mainstream research on 
relation extraction adopts statistical machine 
learning methods, which can be grouped into 
supervised learning (Zelenko et al, 2003; Culotta 
and Soresen, 2004; Zhou et al, 2005; Zhang et 
al., 2006; Qian et al, 2008; Chan and Roth, 
2011), semi-supervised learning (Zhang et al, 
2004; Chen et al, 2006; Zhou et al, 2008; Qian 
et al, 2010) and unsupervised learning (Hase-
gawa et al, 2004; Zhang et al, 2005) in terms of 
the amount of labeled training data they need. 
Usually the extraction performance depends 
heavily on the quality and quantity of the labeled 
data, however, the manual annotation of a large-
scale corpus is labor-intensive and time-
consuming. In the last decade researchers have 
turned to another effective learning paradigm--
active learning (AL), which, given a small num-
ber of labeled instances and a large number of 
unlabeled instances, selects the most informative 
unlabeled instances to be manually annotated and 
add them into the training data in an iterative 
fashion. Essentially active learning attempts to 
decrease the quantity of labeled instances by en-
hancing their quality, gauged by their informa-
tiveness to the learner. Since its emergence, ac-
tive learning has been successfully applied to 
many tasks in NLP (Engelson and Dagan, 1996; 
Hwa, 2004; Tomanek et al, 2007; Settles and 
Craven, 2008).  
It is trivial to validate, as we will do later in 
this paper, that active learning can also alleviate 
the annotation burden for relation extraction in 
one language while retaining the extraction per-
formance. However, there are cases when we 
may exploit relation extraction in multiple lan-
guages and there are corpora with relation in-
stances annotated for more than one language, 
such as the ACE RDC 2005 English and Chinese 
corpora. Hu et al (2013) shows that supervised 
relation extraction in one language (e.g. Chinese) 
582
can be enhanced by relation instances translated 
from another language (e.g. English). This dem-
onstrates that there is some complementariness 
between relation instances in two languages, par-
ticularly when the training data is scarce. One 
natural question is: Can this characteristic be 
made full use of so that active learning can 
maximally benefit relation extraction in two lan-
guages? To the best of our knowledge, so far the 
issue of joint active learning in two languages 
has yet been addressed. Moreover, the success of 
joint bilingual learning may lend itself to many 
inherent multilingual NLP tasks such as POS 
tagging (Yarowsky and Ngai, 2001), name entity 
recognition (Yarowsky et al, 2001), sentiment 
analysis (Wan, 2009), and semantic role labeling 
(Sebastian and Lapata, 2009) etc. 
This paper proposes a bilingual active learn-
ing (BAL) paradigm to relation classification 
with a small number of labeled relation instances 
and a large number of unlabeled instances in two 
languages (non-parallel). Instead of using a par-
allel corpus which should have entity/relation 
alignment information and is thus difficult to 
obtain, this paper employs an off-the-shelf ma-
chine translator to translate both labeled and 
unlabeled instances from one language into the 
other language, forming pseudo parallel corpora. 
These translated instances along with the original 
instances are then fed into a bilingual active 
learning engine. Findings obtained from experi-
ments with relation classification on the ACE 
2005 corpora show that this kind of pseudo-
parallel corpora can significantly improve the 
classification performance for both languages in 
a BAL framework. 
The rest of the paper is organized as follows. 
Section 2 reviews the previous work on relation 
extraction while Section 3 describes our baseline 
systems. Section 4 elaborates on the bilingual 
active learning paradigm and Section 5 discusses 
the experimental results. Finally conclusions and 
directions for future work are presented in Sec-
tion 6. 
2 Related Work 
While there are many studies in monolingual 
relation extraction, there are only a few on multi-
lingual relation extraction in the literature. 
Monolingual relation extraction: A wide 
range of studies on relation extraction focus on 
monolingual resources. As far as representation 
of relation instances is concerned, there are fea-
ture-based methods (Zhao et al, 2004; Zhou et 
al., 2005; Chan and Roth, 2011) and kernel-
based methods (Zelenko et al, 2003; Zhang et al, 
2006; Qian et al, 2008), mainly for the English 
language. Both methods are also widely used in 
relation extraction in other languages, such as 
those in Chinese relation extraction (Che et al, 
2005; Li et al, 2008; Yu et al, 2010). 
Multilingual relation extraction: There are 
only two studies related to multilingual relation 
extraction. Kim et al (2010) propose a cross-
lingual annotation projection approach which 
uses parallel corpora to acquire a relation detec-
tor on the target language. However, the map-
ping of two entities involved in a relation in-
stance may leads to errors. Therefore, Kim and 
Lee (2012) further employ a graph-based semi-
supervised learning method, namely Label 
Propagation (LP), to indirectly propagate labels 
from the source language to the target language 
in an iterative fashion. Both studies transfer rela-
tion annotations via parallel corpora from the 
resource-rich language (English) to the resource-
poor language (Korean), but not vice versa. 
Based on a small number of labeled instances 
and a large number of unlabeled instances in 
both languages, our method differs from theirs in 
that we adopt a bilingual active learning para-
digm via machine translation and improve the 
performance for both languages simultaneously. 
Active Learning in NLP: Active learning 
has become an active research topic due to its 
potential to significantly reduce the amount of 
labeled training data while achieving comparable 
performance with supervised learning. It has 
been successfully applied to many NLP applica-
tions, such as POS tagging (Engelson and Dagan, 
1996; Ringger et al, 2007), word sense disam-
biguation (Chan and Ng, 2007; Zhu and Hovy, 
2007), sentiment detection (Brew et al, 2010; Li 
et al, 2012), syntactical parsing (Hwa, 2004; 
Osborne and Baldridge, 2004), and named entity 
recognition (Shen et al, 2004; Tomanek et al, 
2007; Tomanek and Hahn, 2009) etc.  
Different from these AL studies on a single 
task, Reichart et al (2008) introduce a multi-task 
active learning (MTAL) paradigm, where unla-
beled instances are selected for two annotation 
tasks (i.e. named entity and syntactic parse tree). 
They demonstrate that MTAL in the same lan-
guage outperforms one-sided and random selec-
tion AL. From a different perspective, we pro-
pose an active learning framework for the same 
task, but across two different languages. 
Another related study (Haffari and Sarkar, 
2009) deals with active learning for multilingual 
583
machine translation, which make use of multilin-
gual corpora to decrease human annotation ef-
forts by selecting highly informative sentences 
for a newly added language in multilingual paral-
lel corpora. While machine translation inherently 
deals with multilingual parallel corpora, our task 
focuses on relation extraction by pseudo parallel 
corpora in two languages. 
3 Baseline Systems 
This section first introduces the fundamental su-
pervised learning method, and then describes a 
baseline active learning algorithm. 
3.1 Supervised Learning 
We adopt the feature-based method for funda-
mental supervised relation classification, rather 
than the tree kernel-based method, since active 
learning needs a large number of iterations and 
the kernel-based method usually performs much 
slower than the feature-based one. Following is a 
list of our used features, much similar to Zhou et 
al. (2005): 
a) Lexical features of entities and their contexts 
WM1: bag-of-words in the 1st entity mention 
HM1: headword of M1 
WM2: bag-of-words in the 2nd entity mention 
HM2: headword of M2 
HM12: combination of HM1 and HM2 
WBNULL: when no word in between 
WBFL: the only one word in between 
WBF: the first word in between when at least 
two words in between 
WBL: the last word in between when at least 
two words in between 
WBO: other words in between except the first 
and last words when at least three words in 
between 
b) Entity type 
ET12: combination of entity types 
EST12: combination of entity subtypes 
EC12: combination of entity classes 
c) Mention level 
ML12: combination of entity mention levels 
MT12: combination of LDC mention types 
d) Overlap 
#WB: number of other mentions in between 
#MB: number of words in between 
M1>M2 or M1<M2: flag indicating whether 
M2/M1 is included in M1/M2. 
3.2 Active Learning Algorithm 
We use a pool-based active learning procedure 
with uncertainty sampling (Scheffer et al, 2001; 
Culotta and McCallum, 2005; Kim et al, 2006) 
for both Chinese and English relation classifica-
tion as illustrated in Fig. 1. During iterations a 
batch of unlabeled instances are chosen in terms 
of their informativeness to the current classifier, 
labeled by an oracle and in turn added into the 
labeled data to retrain the classifier. Due to our 
focus on the effectiveness of bilingual active 
learning on relation classification, we only use 
uncertainty sampling without incorporating more 
complex measures, such as diversity and repre-
sentativeness (Settles and Craven, 2008), and 
leave them for future work. 
Input: 
- L, labeled data set 
- U, unlabeled data set 
- n, batch size
Output:
- SVM, classifier 
Repeat:
    1. Train a single classifier SVM on L
2. Run the classifier on U
3. Find at most n instances in U that the classifier 
has the highest prediction uncertainty
    4. Have these instances labeled by an oracle
5. Add them into L
Until: certain number of instances are labeled or 
certain performance is reached
Algorithm uncertainty-based active learning
Figure 1. Pool-based active learning with uncer-
tainty sampling 
Since the SVMLIB package used in this paper 
can output probabilities assigned to the class la-
bels on an instance, we have three uncertainty 
metrics readily available, i.e., least confidence 
(LC), margin (M) and entropy (E). The NER 
experimental results on multiple corpora (Settles 
and Craven, 2008) show that there is no single 
clear winner among these three metrics. This 
conclusion is also validated by our preliminary 
experiments on the task of active learning rela-
tion extraction, thus we adopt the LC metric for 
simplicity. Specifically, with a sequence of K 
probabilities for a relation instance at some itera-
tion, denoted as {p1,p2,?pK} in the descending 
order, the LC metric of the relation instance can 
be simply picked as the first one, i.e. 
1pH
LC =     (1) 
Where K denotes the total number of relation 
classes. Note that this metric actually reflects 
prediction reliability (i.e. reverse uncertainty) 
rather than uncertainty in order to facilitate joint 
584
confidence calculation for two languages (cf. 
?4.4). Intuitively, the smaller the HLC is, the less 
confident the prediction is. 
4 Bilingual Active Learning for Rela-
tion Classification 
In this section, we elaborate on the bilingual ac-
tive learning for relation extraction. 
4.1 Problem Definition 
With Chinese and English (designated as c and e) 
as two languages used in our study, this paper 
intends to address the task of bilingual relation 
classification, i.e., assigning relation labels to 
candidate instances that have semantic relation-
ships. Suppose we have a small number of la-
beled instances in both languages, denoted as Lc 
and Le (non-parallel) respectively, and a large 
number of unlabeled instances in both languages, 
denoted as Uc and Ue (non-parallel). The test in-
stances in both languages are represented as Tc 
and Te. In order to take full advantage of bilin-
gual resources, we translate both labeled and 
unlabeled instances in one language to ones in 
the other language as follows: 
Lc ? Let 
Uc ? Uet 
Le ? Lct 
Ue ? Lct 
The objective is to learn SVM classifiers in 
both languages, denoted as SVMc and SVMe re-
spectively, in a BAL fashion to improve their 
classification performance. 
4.2 Bilingual Active Learning Framework 
Currently, AL is widely used in NLP tasks in a 
single language, i.e., during iterations unlabeled 
instances least confident only in one language 
are picked and manually labeled to augment the 
training data. The only exception is AL for ma-
chine translation (Haffari et al, 2009; Haffari 
and Sarkar, 2009), whose purpose is to select the 
most informative sentences in the source lan-
guage to be manually translated into the target 
language. Previous studies (Reichart et al, 2008; 
Haffari and Sarkar, 2009) show that multi-task 
active learning (MTAL) can yield promising 
overall results, no matter whether they are two 
different tasks or the task of machine translation 
on multiple language pairs. If a specific NLP 
task on two languages, such as relation classifi-
cation, can be regarded as two tasks, it is reason-
able to argue that these two tasks can benefit 
each other when jointly performed in the BAL 
framework. Yet, to our knowledge, this issue 
remains unexplored. 
An important issue for bilingual learning is 
how to obtain two language views for relation 
instances from multilingual resources. There are 
three solutions to this problem, i.e. parallel cor-
pora (Lu et al, 2011), translated corpora (aka. 
pseudo parallel corpora) (Wan 2009), and bilin-
gual lexicons (Oh et al, 2009). We adopt the one 
with pseudo parallel corpora, using the machine 
translation method to generate instances from 
one language to the other in the BAL paradigm, 
as depicted in Fig. 2. 
English View
Labeled 
Chinese Instances 
(Lc)
Labeled Translated 
English Instances 
(Let)
Labeled 
English Instances (Le)
Labeled Translated 
Chinese Instances 
(Lct)
Machine 
Translation
Machine 
Translation
Unlabeled 
Chinese Instances 
(Uc)
Unlabeled 
Translated Chinese 
Instances (Uct)
Unlabeled Translated
 English Instances (Uet)
Unlabeled 
English Instances 
(Ue)
Machine 
Translation
Machine 
Translation
Chinese View
Bilingual 
active learning
Test
Chinese Instances 
(Tc)
Test
English Instances 
(Te)
 
Figure 2. Framework of bilingual active learning 
In order to make full use of pseudo parallel 
corpora, translated labeled and unlabeled in-
stances are augmented in the following two ways: 
z For labeled Chinese instances (Lc) and Eng-
lish instances (Le), their translated counter-
parts (Let and Lct), along with their labels, are 
directly added into the labeled instances in the 
other language; 
z For unlabeled Chinese instances (Uc) and 
English instances (Ue), during an active learn-
ing iteration the top n unlabeled instances in 
Uc and Uet which are least confidently jointly 
585
predicted by SVMc and SVMe are labeled by 
an oracle and added to Lc and Le respectively. 
(cf. ?4.4) 
4.3 Instance Projection via MT 
Among the several off-the-shelf machine transla-
tion services, we select the Google Translator1 
because of its high quality and easy accessibility. 
Both the mentions of relation instances and the 
mentions of two involved entities are first trans-
lated into the other language via machine transla-
tion. Then, two entities in the original instance 
are aligned with their counterparts in the trans-
lated instance in order to form an aligned bilin-
gual relation instance pair. 
Instance translation 
All the positive instances in the ACE 2005 Chi-
nese and English corpora are translated to an-
other language respectively, i.e. Chinese to Eng-
lish and vice versa. The relation instance is rep-
resented as the word sequence between two enti-
ties. This word sequence, rather than the whole 
sentence, is then translated to another language 
by the Google Translator. The reason is that, al-
though this sequence loses partial contextual in-
formation of the relation instance, its translation 
quality is supposed to be better. Our preliminary 
experiments indicate that the addition of contex-
tual information fail to benefit the task. After 
translation, word segmentation is performed on 
Chinese instances translated from English while 
tokenization is needed for translated English in-
stances. 
Entity alignment 
The objective of entity alignment is to build a 
mapping from the entities in the original in-
stances to the entities in the translated instances. 
Put in another way, entity alignment automati-
cally marks the entity mentions in the translated 
instance, thereby the feature vector correspond-
ing to the translated instance can be constructed. 
Entity alignment is vital in cross-language rela-
tion extraction whose difficulty lies in the fact 
that the same entity mention as an isolated phrase 
and as an integral phrase in the relation instance 
can be translated to different phrases. For exam-
ple, the Chinese entity mention ???? (officer) 
is translated to ?officer? in isolation, it is, how-
ever, translated to ?officials? when in the relation 
instance ???? ??? (Syrian officials). 
                                                 
1 http://translate.google.com 
Input:
- Me, entity mention in English
- Re, relation instance in English
- Mct, translation of Me in Chinese
- Rc, translation of Re in Chinese
- L, a lexicon consisting of entries like (ei, ci, pi), 
where pi is the translation probability from ei to ci
- ?, probability threshold
Output:
- Mc, the counterpart of Me in Rc
Steps:
1. If Mct can be exactly found in Rc, then return 
Mct
2. If the rightmost part of Mct can be found in Rc, 
then this part can be returned
3. For very word we in Me,
a) If there exists a word wc in Rc and (we, wc, p) 
in L and p>?, then (we, wc) is a match of two words
b) Return a successive sequence of matching 
words wc
4. Return null
Algorithm entity alignment
 Figure 3. Entity alignment algorithm 
Therefore, we devise some heuristics to align 
entity mentions between Chinese and English. 
The basic idea is that the word sequence in one 
mention successively matches the word sequence 
in the other mention. Take entity alignment from 
English to Chinese as an example, given entity 
mention Me in relation instance Re in English and 
their respective translations Mct and Rc in Chi-
nese, the objective of entity alignment is to find 
Mc, the counterpart of Me in Rc. The procedure of 
entity alignment algorithm can be described in 
Fig. 3. 
In the algorithm, the probability threshold ? is 
empirically set to 0.002 where the precision and 
recall of entity alignment are balanced. Our lexi-
con is derived from the FBIS parallel corpus 
(#LDC2003E14), which is widely used in ma-
chine translation between English and Chinese. It 
should be noted that the process of relation trans-
lation and entity alignment are far from perfec-
tion, leading to reduction in the number of in-
stances being mapping to the other language, i.e. 
|Lc| > |Let| 
|Uc| > |Uet| 
|Le| > |Lct| 
|Ue| > |Lct| 
4.4 Bilingual Active Learning Algorithm 
The basic idea of our BAL paradigm is that, 
while unlabeled instances uncertain in one lan-
586
guage are informative to the learner in that lan-
guage, unlabeled instances jointly uncertain in 
both languages are informative to the learners in 
both languages, thus potentially improving clas-
sification performance for both languages more 
than their individual active learners do.  This 
idea is embodied in the BAL algorithm in Fig. 4, 
where n is the batch size, i.e., the number of in-
stances selected, labeled and augmented at each 
iteration. 
Figure 4. Bilingual active learning algorithm 
The key point of this algorithm lies in Step 5 
and Step 6, where unlabeled instances from Uc 
and Ue are selected and labeled respectively. 
Take Chinese for an example, when gauging the 
prediction uncertainty for an unlabeled instance 
in Uc, not only its own uncertainty measure Hc 
predicted by SVMc is considered, but also the 
uncertainty measure Het for its translation coun-
terpart in Uet, which is predicted by SVMe, is con-
sidered. Generally, in order to jointly consider 
these two measures, there are three methods to 
compute their means, namely, arithmetic mean, 
geometric mean and harmonic mean. Preliminary 
experiments show that among these three means, 
there is no single winner, so we simply take the 
geometric mean defined as follows:  
etcg HHH *=    (2) 
Considering that we adopt the LC measure as 
the uncertainty score, when an instance in Uc 
can?t find its translation counterpart in Uet due to 
translation error or entity alignment failure, Het is 
set to 1, i.e. the maximum. Since the bigger H is, 
the more confident the prediction is, the less 
likely the instance will be chosen, in this way we 
discourage the unlabeled instances without trans-
lation counterparts. 
5 Experimentation 
We have systematically evaluated our BAL para-
digm on the relation classification task using 
ACE RDC 2005 RDC Chinese and English cor-
pora. 
5.1 Experimental Settings 
Corpora and Preprocessing 
We use the ACE 2005 RDC Chinese and English 
corpora as the benchmark data (hereafter we re-
fer to them as the Chinese corpus (ACE2005c) 
and the English corpus (ACE2005e) respec-
tively). Both corpora have the same en-
tity/relation hierarchies, which define 7 entity 
types, 6 major relation types. However, the Chi-
nese corpus contains 633 documents and 9,147 
positive relation instances while the English cor-
pus only contains 498 files and 6,253 positive 
instances. Therefore, in order to balance the cor-
pus scale to fairly evaluate bilingual active learn-
ing impact on relation classification, we ran-
domly select 458 Chinese files and thus get 
6,268 positive instances, comparable to the Eng-
lish corpus. 
Preprocessing steps for both corpora include 
sentence splitting and tokenization (word seg-
mentation for Chinese using ICTCLAS2). Then, 
positive relation mentions with word sequences 
between two entities and their feature vectors are 
extracted from sentences while negative relation 
mentions are simply discarded because we focus 
on the task of relation classification. After entity 
and relation mentions in one language are trans-
                                                 
2 http://ictclas.org/ 
587
lated into the other language using the Google 
translator, entity alignment is performed between 
relation mentions and their translations. Finally 
4,747 Chinese relation mentions are successfully 
translated and aligned from English and vice 
versa, 4,936 English relation mentions are trans-
lated and aligned from Chinese. 
SVMLIB (Chang and Lin, 2011) is selected as 
our classifier since it supports multi-class classi-
fication. The training parameters C (SVM) is set 
to 2.4 according to our previous work on relation 
extraction (Qian et al, 2010). Relation classifica-
tion performance is evaluated using the standard 
Precision (P), Recall (R) and their harmonic av-
erage (F1) as well as deficiency measure (cf. lat-
ter in this section.). Overall performance scores 
are averaged over 10 runs. For each run, 1/40 
and 1/5 randomly selected instances are used as 
the training and test set respectively while the 
remaining instances are used as the unlabeled set 
for further labeling during active learning itera-
tions. 
Methods for Comparison 
For fair comparison, two baseline methods of 
supervised learning are included to augment their 
training sets with labeled instances during itera-
tions. However, these labeled instances are cho-
sen randomly from the corpus. 
SL-MO (Supervised Learning with monolin-
gual labeled instances): only the monolingual 
labeled instances are fed to the SVM classifiers 
for both Chinese and English relation classifica-
tion respectively. The initial training data only 
contain Lc and Le for Chinese and English respec-
tively.  
SL-CR (Supervised Learning with cross-
lingual labeled instances): in addition to mono-
lingual labeled instances (SL-MO), the training 
data for supervised learning contain labeled in-
stances translated from the other language. That 
is, the initial training data contain Lc and Lct for 
Chinese, or Le and Let for English. More impor-
tant, at each iteration not only the labeled in-
stances are added to the training data of its own 
language, but their translated instances are also 
added to the training data of the other language. 
AL-MO (Active Learning with monolingual 
instances): labeled and unlabeled data for active 
learning only contain monolingual instances. No 
translated instances are involved. That is, the 
data contain Lc and Uc for Chinese, or Le and Ue 
for English respectively. This is the normal ac-
tive learning method applied to a single language. 
AL-CR (Active Learning with cross-lingual 
instances): both the manually labeled instances 
and their translated ones are added to the respec-
tive training data. The initial training data con-
tain Lc and Lct for Chinese, or Le and Let for Eng-
lish. At each iteration, the n least confidently 
classified instances in Uc and Ue are labeled and 
added to the Chinese/English training data re-
spectively. Their translated instances in Uet and 
Uct are also added to the English/Chinese training 
data respectively. 
AL-BI (Active Learning with bilingual la-
beled and unlabeled instances): similar to AL-
CR with the exception that the unlabeled in-
stances are chosen not by uncertainty scores in 
one language, but by the joint uncertainty scores 
in two languages. (cf. ?4.4) 
Evaluation Metric 
Although learning curves are often used to evalu-
ate the performance for active learning, it is pref-
erable to quantitatively compare various active 
learning methods using a statistical metric defi-
ciency (Schein and Ungar, 2007) defined as: 
?
?
=
=
?
?= n
i in
n
i in
n
REFFREFF
ALFREFF
REFALdef
1
1
))()((
))()((
),(     (3) 
Where n is the number of iterations involved in 
active learning and Fi is the F1-score of relation 
classification at the ith iteration. REF is the base-
line active learning method and AL is an im-
proved variant of REF, such as AL-CR or AL-
BI. Essentially this deficiency metric measures 
the degree to which REF outperforms AL. Thus, 
smaller deficiency value (i.e. <1.0) indicates AL 
outperforms REF while a larger value (i.e. >1.0) 
indicates AL underperforms REF. 
5.2 Experimental Results and Analysis 
Comparison of overall deficiency 
Table 1 compares the deficiency scores of rela-
tion classification on the Chinese (ACE2005c) 
and English corpora (ACE2005e) for various 
learning methods, i.e., SL-CR, AL-MO, AL-CR 
and AL-BI. Particularly, SL-MO is used as the 
baseline system against which deficiency scores 
for other methods are computed. The batch size n 
is set to 100 and iterations stop after all the unla-
beled instances have run out of. Deficiency 
scores are averaged over 10 runs and the best 
ones are highlighted in bold font. Each run has a 
different test set and a different seed set. 
588
 
 (a) Chinese      (b) English 
Figure 5. Deficiency comparison for different batch sizes 
 
(a) Chinese      (b) English 
Figure 6. Learning curves for different methods 
 
The table shows that among the three active 
learning methods, bilingual active learning (AL-
BI) achieves the best performance for both Chi-
nese and English relation classification. This 
demonstrates that, bilingual active learning with 
jointly selecting the unlabeled instances can not 
only enhance relation classification for its own 
language, but also help relation classification for 
the other language due to the complementary 
nature of relation instances between Chinese and 
English. 
Corpora SL-CR AL-MO AL-CR AL-BI
ACE2005c 0.934 0.383 0.323 0.254
ACE2005e 0.779 0.405 0.298 0.160
Table 1. Deficiency comparison of different 
methods 
The table also shows the consistent utility of 
cross-lingual information for relation classifica-
tion for both languages. When cross-lingual in-
formation is augmented, SL-CR outperforms 
SL-MO and AL-CR outperforms AL-MO. 
Comparison of different batch sizes 
Figure 5(a) and 5(b) illustrate the deficiency 
scores for four learning methods (SL-CR, AL-
MO, AL-CR and AL-BI) against the SL-MO 
method with different batch sizes (n), where pre-
fixes ?C? and ?E? denote Chinese and English 
respectively. The horizontal axes denote the 
range of n (<=1000) while the vertical ones de-
note the deficiency scores. 
The figures show that the deficiency scores 
for three active learning methods run virtually 
parallel with each other while they increase mo-
notonously with the batch size n. This suggests 
that for both Chinese and English AL-BI consis-
tently performs best against other methods across 
a wide range of batch sizes, though the overall 
advantage of three active learning methods gen-
erally diminish. 
Comparison of learning curves 
In order to gain an intuition into how the per-
formance evolves when the labeled instances are 
added into the training data during iterations, we 
depict the learning curves for various learning 
methods on the Chinese and English corpora in 
Fig. 6(a) and 6(b) respectively. The horizontal 
axes denote learning iterations while the vertical 
ones denote F1-scores. For simplicity of illustra-
tion the F1-scores are collected from one of the 
10 runs. 
75
77
79
81
83
85
87
89
91
93
95
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48
C-SL-MO
C-SL-CR
C-AL-MO
C-AL-CR
C-AL-BI
75
77
79
81
83
85
87
89
91
93
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48
E-SL-MO
E-SL-CR
E-AL-MO
E-AL-CR
E-AL-BI
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
100 200 300 400 500 600 700 800 900 1000
C-SL-CR
C-AL-MO
C-AL-CR
C-AL-BI
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
100 200 300 400 500 600 700 800 900 1000
E-SL-CR
E-AL-MO
E-AL-CR
E-AL-BI
589
The figures clearly demonstrate the perform-
ance difference for both languages among five 
methods at the beginning of iterations while F1-
scores converge at the end of iterations. Particu-
larly at the very outset, AL-BI outperforms other 
methods, quickly jumps to a very high point 
comparable to its best performance. However, 
after the 10th iteration the performance scores for 
the three AL variants tend to show trivial differ-
ence probably because most highly informative 
instances have already been added to the training 
data. 
Comparison of annotation scale 
In order to better compare BAL with other AL 
methods Figure 7 zooms out partial data on three 
AL methods in Fig. 6 and rescale the data for 
AL-MO, where ?C? and ?E? denote Chinese and 
English respectively. Likewise, the vertical axis 
denotes F1-scores while the horizontal axis de-
notes the number of instances labeled for AL-
CR and AL-BI. However, for AL-MO that num-
ber is doubled. This figure tries to answer the 
question: to label n respective instances in both 
languages for BAL or to labeled 2n instances in 
just one language for monolingual AL, can the 
former rival the latter? 
80
82
84
86
88
90
92
94
100 200 300 400 500 600 700 800 900 1000
C-AL-MO (2n)
C-AL-CR
C-AL-BI
E-AL-MO (2n)
E-AL-CR
E-AL-BI
 
Figure 7. Comparison of annotation scale among 
three AL methods 
The figure shows that for both Chinese and 
English, when the number of instances (n) to be 
labeled is no greater than 400, AL-BI with n in-
stances can achieve comparable performance 
with AL-MO with 2n instances. It implies that 
when the labeled instances are limited, labeling 
instances, half in one language and half in the 
other for BAL, is competitive against labeling 
the same total number of instances in just one 
language for monolingual AL, not to mention 
that the former can generate two relation extrac-
tors on two languages. 
6 Conclusion 
This paper proposes a bilingual active learning 
paradigm for Chinese and English relation classi-
fication. Given a small number of relation in-
stances and a large number of unlabeled relation 
instances in both languages, we translate both the 
labeled and unlabeled instances in one language 
to the other as pseudo parallel corpora. After en-
tity alignment, these labeled and unlabeled in-
stances in both languages are fed into a bilingual 
active learning engine. Experiments with the task 
of relation classification on the ACE RDC 2005 
Chinese and English corpora show that bilingual 
active learning can significantly outperforms 
monolingual active learning for both Chinese and 
English simultaneously. Moreover, we demon-
strate that BAL across two languages can com-
pete against monolingual AL when the annota-
tion scale is limited, though the overall number 
of labeled instances remains the same. 
For future work, on one hand, we plan to 
combine uncertainty sampling with diversity and 
informativeness measures; on the other hand, we 
intend to combine BAL with semi-supervised 
learning to further reduce human annotation ef-
forts. 
Acknowledgments 
This research is supported by Grants 61373096, 
61305088, 61273320, and 61331011 under the 
National Natural Science Foundation of China; 
Project 2012AA011102 under the ?863? Na-
tional High-Tech Research and Development of 
China; Grant 11KJA520003 under the Education 
Bureau of Jiangsu, China. We would like to 
thank the excellent and insightful comments 
from the three anonymous reviewers. Thanks 
also go to my colleague Dr. Shoushan Li for his 
helpful suggestions. 
Reference 
ACE. 2002-2007. Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/ 
A. Brew, D. Greene, and P. Cunningham. 2010. Using 
crowdsourcing and active learning to track senti-
ment in online media. ECAI?2010: 145?150. 
Y.S. Chan and D. Roth. 2011. Exploiting Syntactico-
Semantic Structures for Relation Extraction. 
ACL?2011: 551-560 
Y.S. Chan and H.T. Ng. 2007. Domain adaptation 
with active learning for word sense disambiguation. 
ACL?2007. 
590
C.C. Chang and C.J. Lin. 2011. LIBSVM: a library 
for support vector machines. ACM Transactions on 
Intelligent Systems and Technology, 2(27):1-27. 
W.X. Che, T. Liu, and S. Li. 2005. Automatic Extrac-
tion of Entity Relation (in Chinese). Journal of 
Chinese Information Processing, 19(2): 1-6. 
J.X. Chen, D.H. Ji, and C. L. Tan. 2006. Relation Ex-
traction using Label Propagation-based Semi-
supervised Learning. ACL/COLING?2006: 129-136. 
A. Culotta and J. Sorensen. 2004. Dependency tree 
kernels for relation extraction. ACL?2004: 423-439.  
A. Culotta and A. McCallum. 2005. Reducing label-
ing effort for stuctured prediction tasks. AAAI?2005: 
746?751. 
S. P. Engelson and I. Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. ACL?1996: 319?326. 
G. Haffari, M. Roy, and A. Sarkar. 2009. Active 
learning for statistical phrase-based machine trans-
lation. NAACL?2009: 415?423. 
G. Haffari and A. Sarkar. 2009. Active learning for 
multilingual statistical machine translation. 
ACL/IJCNLP?2009: 181?189. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from 
Large Corpora. ACL?2004. 
Y.N. Hu, J.G. Shu, L.H. Qian, and Q.M. Qiao. 2013. 
Cross-lingual Relation Extraction based on Ma-
chine Translation (in Chinese). Journal of Chinese 
Information Processing, 27(5): 191-197. 
R. Hwa. 2004. Sample selection for statistical parsing. 
Computational Linguistics, 30(3): 253?276. 
S. Kim, M. Jeong, J. Lee, and G.G. Lee. 2010. A 
Cross-lingual Annotation Projection Approach for 
Relation Detection. COLING?2010: 564-571. 
S. Kim and G.G. Lee. 2012. A Graph-based Cross-
lingual Projection Approach for Weakly Super-
vised Relation Extraction. ACL?2012: 48-53. 
S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee. 
2006. MMR-based active machine learning for bio 
named entity recognition. HLT-NAACL?2006: 69?
72. 
W.J. Li, P. Zhang, F.R. Wei, Y.X. Hou, and Q. Lu. 
2008. A Novel Feature-based Approach to Chinese 
Entity Relation Extraction. ACL?2008: 89-92. 
S.S. Li, S.F. Ju, G.D. Zhou, and X.J. Li. 2012. Active 
learning for imbalanced sentiment classifica-
tion. EMNLP-CoNLL?2012: 139-148. 
B. Lu, C.H. Tan, C. Cardie, and B.K. Tsou. 2011. 
Joint Bilingual Sentiment Classification with 
Unlabeled Parallel Corpora. ACL?2011: 320-330. 
J. Oh, K. Uchimoto, and K. Torisawa. 2009.  Bilin-
gual Co-Training for Monolingual Hyponymy-
Relation Acquisition. ACL?2009: 432-440. 
M. Osborne and J. Baldridge. 2004. Ensemble based 
active learning for parse selection. HLT-NAACL? 
2004: 89?96. 
L.H. Qian, G.D. Zhou, F. Kong, and Q.M. Zhu. 2010. 
Clustering-based Stratified Seed Sampling for 
Semi-Supervised Relation Classification. 
EMNLP2010: 346-355. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D. Qian. 
2008. Exploiting constituent dependencies for tree 
kernel-based semantic relation extraction. COL-
ING?2008: 697-704.  
R. Reichart, K. Tomanek, U. Hahn, and A. Rappoport. 
2008. Multi-task active learning for linguistic an-
notations. ACL?2008: 861-869. 
E. Ringger, P. McClanahan, R. Haertel, G. Busby, M. 
Carmen, J. Carroll, K. Seppi, and D. Lonsdale. 
2007. Active learning for part-of-speech tagging: 
Accelerating corpus annotation. In Proceedings of 
the Linguistic Annotation Workshop at ACL?2007: 
101?108. 
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hidden Markov models for information extrac-
tion. In Proceedings of the International Confer-
ence on Advances in Intelligent Data Analysis 
(CAIDA), pages 309?318. 
A. I. Schein and L. H. Ungar. 2007. Active learning 
for logistic regression: an evaluation. Machine 
Learning, 68(3): 235-265. 
P. Sebastian and M. Lapata. 2009. Cross-lingual an-
notation projection of semantic roles. Journal of 
Artificial Intelligence Research, 36(1): 307-340. 
B. Settles and M. Craven. 2008. An Analysis of Ac-
tive Learning Strategies for Sequence Labeling 
Tasks. EMNLP?2008: 1070?1079. 
D. Shen, J. Zhang, J. Su, G.D. Zhou and C.-L. Tan. 
2004. Multi-criteria-based active learning for 
named entity recognition. ACL?2004. 
K. Tomanek and U. Hahn. 2009. Semi-Supervised 
Active Learning for Sequence Labeling. ACL-
IJCNLP?2009: 1039-1047. 
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts an-
notation costs and maintains reusability of anno-
tated data. EMNLP-CoNLL?2007: 486?495. 
X.J. Wan. 2009. Co-Training for Cross-Lingual Sen-
timent Classification. ACL-AFNLP?2009: 235-243. 
D. Yarowsky and G. Ngai. 2001. Inducing multilin-
gual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. NAACL?2001: 1-8. 
591
D. Yarowsky, G. Ngai, and R. Wicentorski. 2001. 
Inducing multilingual text analysis tools via robust 
projection across aligned corpora. HLT?2001:1-8. 
H.H. Yu, L.H. Qian, G.D. Zhou, and Q.M. Zhu. 2010. 
Chinese Semantic Relation Extraction based on 
Unified Syntactic and Entity Semantic Tree (in 
Chinese). Journal of Chinese Information Process-
ing, 24(5): 17-23. 
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel Methods for Relation Extraction. Journal of 
Machine Learning Research, 3: 1083-1106. 
Z. Zhang. 2004. Weakly-supervised relation classifi-
cation for Information Extraction. CIKM?2004. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree 
Similarity-Based Clustering. IJCNLP?2005: 378-
389.  
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. 
ACL/COLING?2006: 825-832.  
S.B. Zhao and R. Grishman. 2005. Extracting rela-
tions with integrated information using kernel 
methods.  ACL?2005: 419-426. 
G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008. 
Semi-Supervised Learning for Relation Extraction. 
IJCNLP?2008: 32-38. 
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring various knowledge in relation extraction. 
ACL?2005: 427-434.  
J.B. Zhu and E. Hovy. 2007. Active learning for word 
sense disambiguation with methods for addressing 
the class imbalance problem. EMNLP-
CoNLL?2007: 783-790. 
592
Soochow University: Description and Analysis of the Chinese 
Word Sense Induction System for CLP2010 
Hua Xu   Bing Liu   Longhua Qian?   Guodong Zhou 
Natural Language Processing Lab 
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
Email: 
{20094227034,20084227065055,qianlonghua,gdzhou}@suda.edu.cn
 
                                                 
? Corresponding author 
Abstract 
Recent studies on word sense induction 
(WSI) mainly concentrate on European 
languages, Chinese word sense induction 
is becoming popular as it presents a new 
challenge to WSI. In this paper, we 
propose a feature-based approach using 
the spectral clustering algorithm to this 
problem. We also compare various 
clustering algorithms and similarity 
metrics. Experimental results show that 
our system achieves promising 
performance in F-score. 
1 Introduction 
Word sense induction (WSI) is an open problem 
of natural language processing (NLP), which 
governs the process of automatic discovery of 
the possible senses of a word. WSI is similar to 
word sense disambiguation (WSD) both in 
methods employed and in problem encountered. 
In the procedure of WSD, the senses are as-
sumed to be known and the task focuses on 
choosing the correct one for an ambiguous word 
in a context. The main difference between them 
is that the task of WSD generally requires large-
scale manually annotated lexical resources while 
WSI does not. As WSI doesn?t rely on the 
manually annotated corpus, it has become one of 
the most important topics in current NLP re-
search (Pantel and Lin, 2002; Neill, 2002; Rapp, 
2003). Typically, the input to a WSI algorithm is 
a target word to be disambiguated. The task of 
WSI is to distinguish which target words share 
the same meaning when they appear in different 
contexts. Such result can be at the very least 
used as empirically grounded suggestions for 
lexicographers or as input for WSD algorithm. 
Other possible uses include automatic thesaurus 
or ontology construction, machine translation or 
information retrieval. Compared with European 
languages, the study of WSI in Chinese is scarce. 
Furthermore, as Chinese has its special writing 
style and Chinese word senses have their own 
characteristics, the methods that work well in 
English may not perform effectively in Chinese 
and the usefulness of WSI in real-world applica-
tions has yet to be tested and proved. 
The core idea behind word sense induction is 
that contextual information provides important 
cues regarding a word?s meaning. The idea dates 
back to (at least) Firth (1957) (?You shall know 
a word by the company it keeps?), and under-
lies most WSD and lexicon acquisition work to 
date. For example, when the adverb phrase oc-
curring prior to the ambiguous word????, 
then the target word is more likely to be a verb 
and the meaning of which is ?to hold something?; 
Otherwise, if an adjective phrase locates in the 
same position, then it probably means ?confi-
dence? in English. Thus, the words surrounds 
the target word are main contributor to sense 
induction. 
The bake off task 4 on WSI in the first CIPS-
SIGHAN Joint Conference on Chinese Lan-
guage Processing (CLP2010) is intended to 
promote the exchange of ideas among partici-
pants and improve the performance of Chinese 
WSI systems. Generally, our WSI system also 
adopts a clustering algorithm to group the con-
texts of a target word. Differently, after generat-
ing feature vectors of words, we compute a simi-
larity matrix with each cell denoting the similar-
ity between two contexts. Furthermore, the set of 
similarity values of a context with other contexts 
is viewed as another kind of feature vector, 
which we refer to as similarity vector. Both fea-
ture vectors and similarity vectors can be sepa-
rately used as the input to clustering algorithms. 
Experimental results show our system achieves 
good performances on the development dataset 
as well as on the final test dataset provided by 
the CLP2010. 
2 System Description 
This section sequentially describes the architec-
ture of our WSI system and its main components. 
2.1 System Architecture 
Figure 1 shows the architecture of our WSI 
system. The first step is to preprocess the raw 
dataset for feature extraction. After that, we 
extract ?bag of words? from the sentence 
containing a target word (feature extraction) and 
transform them into high-dimension vectors 
(feature vector generation). Then, similarities of 
every two vectors could be computed based on 
the feature vectors (similarity measurement). the 
similarities of an instance can be viewed as 
another vector?similarity vector. Both feature 
vectors and similarity vectors can be served as 
the input for clustering algorithms. Finally, we 
perform three clustering algorithms, namely, k-
means, HAC and spectral clustering.  
Dataset
Preprocess
Feature
Extraction
Vector
Generation
Similarity
Measurement
Similarity
As VectorClustering
WSI
Results
 
Figure 1  Architecture of our Chinese 
WSI system 
2.2 Feature Engineering 
In the task of WSI, the target words with their 
topical context are first transformed into multi-
dimensional vectors with various features, and 
then applying clustering algorithm to detect the 
relevance of each other. 
Corpus Preprocessing 
For each raw file, we first extract each sentence 
embedded in the tag <instance>, including 
the <head> and </head> tags which are used 
to identify the ambiguous word. Then, we put all 
the sentences related to one target word into a 
file, ordered by their instance IDs. The next step 
is word segmentation, which segments each sen-
tence into a sequence of Chinese words and is 
unique for Chinese WSI. Here, we use the soft-
ware from Hylanda1 since it is ready to use and 
considered an efficient word segmentation tool. 
Finally, since we retain the <head> tag in the 
sentence, the <head> and </head> tags are 
usually separated after word segmentation, thus 
we have to restore them in order to correctly lo-
cate the target word during the process of feature 
extraction. 
Feature Extraction 
After word segmentation, for a context of a par-
ticular word, we extract all the words around it 
in the sentence and build a feature vector based 
on a ?bag-of-words? Boolean model. ?Bag-of-
words? means that we don?t consider the order 
of words. Meanwhile, in the Boolean model, 
each word in the context is used to generate a 
feature. This feature will be set to 1 if the word 
appears in the context or 0 if it does not. Finally, 
we get a number of feature vectors, each of them 
corresponds to an instance of the target word. 
One problem with this feature-based method is 
that, since the size of word set may be huge, the 
dimension is also very high, which might lead to 
data sparsity problem.  
Similarity measurement 
One commonly used metric for similarity meas-
urement is cosine similarity, which measures the 
angle between two feature vectors in a high-
dimensional space. Formally, the cosine similar-
ity can be computed as follows: 
cos ,ine similarity ?< > = ?
x yx y
x y
 
where ,x y are two vectors in the vector space 
and x , y are the lengths of  ,x y  respectively. 
                                                 
1 http://www.hylanda.com/
Some clustering algorithms takes feature vec-
tors as the input and use cosine similarity as the 
similarity measurement between two vectors. 
This may lead to performance degradation due 
to data sparsity in feature vectors. To avoid this 
problem, we compute the similarities of every 
two vectors and generate an  similarity 
matrix, where  is the number of all the in-
stances containing the ambiguous word. Gener-
ally, is usually much smaller than the dimen-
sion size and may alleviate the data sparsity 
problem. Moreover, we view every row of this 
matrix (i.e., an ordered set of similarities of an 
instance with other instances) as another kind of 
feature vector. In other words, each instance it-
self is regarded as a feature, and the similarity 
with this instance reflects the weight of the fea-
ture. We call this vector similarity vector, which 
we believe will more properly represent the in-
stance and achieve promising performance. 
*N N
N
N
2.3 Clustering Algorithm 
Clustering is a very popular technique which 
aims to partition a dataset into such subgroups 
that samples in the same group share more simi-
larities than those from different groups. Our 
system explores various cluster algorithms for 
Chinese WSI, including K-means, hierarchical 
agglomerative clustering (HAC), and spectral 
clustering (SC). 
K-means (KM) 
K-means is a very popular method for general 
clustering used to automatically partition a data 
set into k groups. K-means works by assigning 
multidimensional vectors to one of K clusters, 
where is given as a priori. The aim of the al-
gorithm is to minimize the variance of the vec-
tors assigned to each cluster.  
K
K-means proceeds by selecting k  initial clus-
ter centers and then iteratively refining them as 
follows: 
(1) Choose cluster centers to coincide with 
k randomly-chosen patterns or k  ran-
domly defined points. 
k
(2) Assign each pattern to the closest cluster 
center. 
(3) Recompute the cluster centers using the 
current cluster memberships. 
(4) If a convergence criterion is not met, go 
to step 2. 
Hierarchical Agglomerative Clustering (HAC) 
Different from K-means, hierarchical clustering 
creates a hierarchy of clusters which can be 
represented in a tree structure called a 
dendrogram. The root of the tree consists of a 
single cluster containing all objects, and the 
leaves correspond to individual object.  
Typically, hierarchical agglomerative 
clustering (HAC) starts at the leaves and 
successively merges two clusters together as 
long as they have the shortest distance among all 
the pair-wise distances between any two clusters.  
Given a specified number of clusters, the key 
problem is to determine where to cut the hierar-
chical tree into clusters. In this paper, we gener-
ate the final flat cluster structures greedily by 
maximizing the equal distribution of instances 
among different clusters. 
Spectral Clustering (SC) 
Spectral clustering refers to a class of techniques 
which rely on the eigen-structure of a similarity 
matrix to partition points into disjoint clusters 
with points in the same cluster having high simi-
larity and points in different clusters having low 
similarity.  
Compared to the ?traditional algorithms? such 
as K-means or single linkage, spectral clustering 
has many fundamental advantages. Results ob-
tained by spectral clustering often outperform 
the traditional approaches, spectral clustering is 
very simple to implement and can be solved ef-
ficiently by standard linear algebra methods. 
3 System Evaluation 
This section reports the evaluation dataset and 
system performance for our feature-based Chi-
nese WSI system. 
3.1  Dataset and Evaluation Metrics 
We use the CLP2010 bake off task 4 sample 
dataset as our development dataset. There are 
2500 examples containing 50 target words and 
each word has 50 sentences with different mean-
ings. The exact meanings of the target words are 
blind, only the number of the meanings is pro-
vided in the data. We compute the system per-
formance with the sample dataset because it con-
tains the answers of each candidate meaning. 
The test dataset provided by the CLP2010 is 
similar to the sample dataset. It contains 100 
target words and 5000 instances in total. How-
ever, it doesn?t provide the answers. 
The F-score measurement is the same as Zhao 
and Karypis (2005). Given a particular 
class rL of size and a particular cluster  of 
size , suppose  in the cluster  belong to
rn iS
in irn iS rL , 
then the value of this class and cluster is de-
fined to be 
F
2 ( , ) ( ,( , )
( , ) ( , )
r i r i
r i
r i r i
)R L S P L SF L S
R L S P L S
? ?= +  
( , ) /r i ir rR L S n n=  
( , ) /r i ir iP L S n n=  
where ( , )r iR L S is the recall value and  
is the precision value. The F-score of class 
( , )r iP L S
rL is 
the maximum value and F-score value follow: F
( ) max ( , )
ir S r i
F score L F L S? =  
1
( )
c
r
r
r
nF score F score L
n=
? = ??  
where  is the total number of classes and n  is 
the total size. 
c
3.2 Experiment Results 
Table 1 reports the F-score of our feature-based 
Chinese WSI for different feature sets with 
various window sizes using K-means clustering. 
Since there are different results for each run of 
K-means clustering algorithm, we perform 20 
trials and compute their average as the final 
results. The columns denote different window 
size n, that is, the n words before and after the 
target word are extracted as features. Particularly, 
the size of infinity (?) means that all the words 
in the sentence except the target word are 
considered. The rows represent various 
combinations of feature sets and similarity 
measurements, currently, four of which are 
considered as follows: 
F-All: all the words are considered as features 
and from them feature vectors are constructed. 
F-Stop: the top 150 most frequently occurring 
words in the total ?word bags? of the corpus are 
regarded as stop words and thus removed from 
the feature set. Feature vectors are then formed 
from these words. 
S-All: the feature set and the feature vector 
are the same as those of F-All, but instead the 
similarity vector is used for clustering (c.f. Sec-
tion 2.2). 
S-Stop: the feature set and the feature vector 
are the same as those of F-Stop, but instead the 
similarity vector is used for clustering. 
Table 1 Experimental results for differ-
ent feature sets with different window sizes us-
ing K-means clustering 
 
This table shows that S-Stop achieves the best 
performance of 0.7320 in F-score. This suggests 
that for K-means clustering, Chinese WSI can 
benefit much from removing stop words and 
adopting similarity vector. It also shows that: 
Feature/ 
Similarity 3 7 10 ? 
F-All 0.5949 0.6199 0.6320 0.6575
F-Stop 0.6384 0.6500 0.6493 0.6428
S-All 0.5856 0.6044 0.6186 0.6843
S-Stop 0.6532 0.6696 0.6804 0.7320
z As the window size increases, the perform-
ance is almost consistently enhanced. This 
indicates that all the words in the sentence 
more or less help disambiguate the target 
word. 
z Removing stop words consistently improves 
the F-score for both similarity metrics. This 
means some high frequent words do not help 
discriminate the meaning of the target words, 
and further work on feature selection is thus 
encouraged. 
z Similarity vector consistently outperforms 
feature vector for stop-removed features, but 
not so for all-words features. This may be 
due to the fact that, when the window size is 
limited, the influence of frequently occur-
ring stop words is relatively high, thus the 
similarity vector misrepresent the context of 
the target word. On the contrary, when stop 
words are removed or the context is wide, 
the similarity vector can better reflect the 
target word?s context, leading to better per-
formance. 
In order to intuitively explain why the simi-
larity vector is more discriminative than the fea-
ture vector, we take two sentences containing 
the Chinese word ???? (hold, grasp) as an ex-
ample (Figure 2). These two sentences have few 
common words, so clustering via feature vectors 
puts them into different classes. However, since 
the similarities of these two feature vectors with 
other feature vectors are much similar, cluster-
ing via similarity vectors group them into the 
same class.  
 
Figure 2  An example from the dataset 
 
According to the conclusion of the above ex-
periments, it is better to include all the words 
except stop words in the sentence as the features 
in the subsequent experiment. Table 2 lists the 
results using various clustering algorithms with 
this same experimental setting. It shows that the 
spectral clustering algorithm achieves the best 
performance of 0.7692 in F-score for Chinese 
WSI using the S-All setup. Additionally, there 
are some interesting findings: 
mi-
 of 
han 
ing 
this 
nto 
lus-
lly 
er-
re. 
ex-
der 
ers the density information, therefore S-All 
will not significantly improve the perform-
ance. 
 
Feature/ 
Similarity 
KM HAC SC 
F-All 0.6428 0.6280 0.7686 
S-All 0.7320 0.6332 0.7692 
Table 2 Experiments results using dif-
ferent clustering algorithms 
<lexelt item="??" snum="4"> 
<instance id="0012"> 
???????????????????
?????????????????
<head>??</head>???????????
?? 
 </instance>  
<instance id="0015">  
???????????????????
???????????????<head>?
?</head>???????????????
??????????????????? 
</instance>  
</lexelt> 
3.3 Final System Performance 
For the CLP2010 task 4 test dataset which con-
tains 100 target words and 5000 instances in to-
tal, we first extract all the words except stop 
words in a sentence containing the target word, 
then produce the feature vector for each context 
and generate the similarity matrix, finally we 
perform the spectral cluster algorithm. Probably 
because the distribution of the target word in the 
test dataset is different from that in the develop-
ment dataset, the F-score of our system on the 
test dataset is 0.7108, about 0.05 units lower 
than that we got on the sample dataset. 
4 Conclusions and Future Work 
In our Chinese WSI system, we extract all the 
words except stop words in the sentence, con-
struct feature vectors and similarity vectors, and 
apply the spectral clustering algorithm to this 
problem. Experimental results show that our 
simple and efficient system achieve a promising 
result. Moreover, we also compare various clus-
tering algorithms and similarity metrics. We find 
that although the spectral clustering algorithm 
outperforms other clustering algorithms, the K-
means clustering with similarity vectors can also 
achieve comparable results. 
For future work, we will incorporate more 
linguistic features, such as base chunking, parse 
tree feature as well as dependency information 
into our system to further improve the perform-
ance. 
Acknowledgement 
This research is supported by Project 60873150, 
60970056 and 90920004 under the National 
Natural Science Foundation of China. We would z Although SC performs best, KM with si
larity vectors achieves comparable results
0.7320 units in F-score, slightly lower t
that of SC. 
z HAC performs worst among all cluster
algorithms. An observation reveals that 
algorithm always groups the instances i
highly skewed clusters, i.e., one or two c
ters are extremely large while others usua
have only one instance in each cluster. 
z It is surprising that S-All slightly outp
forms F-All by only 0.0006 units in F-sco
The truth is that, as discussed in the first 
periment, KM using F-All doesn?t consi
instance density while S-All does. On the 
contrary, SC identifies the eign-structure in 
the instance space and thus already consid-
also like to thank other contributors in the NLP 
lab at Soochow University. 
References 
Jain A, Murty M. 1999.Flynn P. Data clustering : A 
Review [J]. ACM Computing Surveys,1999,31 
(3) :2642323 
F. Bach and M. Jordan.2004. Learning spectral clus-
tering. In Proc. of NIPS-16. MIT Press, 2004. 
Samuel Brody and Mirella Lapata. 2009. Bayesian 
word sense induction. In Proceedings of the 12th 
Conference of the European Chapter of the ACL 
(EACL 2009), pages 103?111. 
Neill, D. B. 2002. Fully Automatic Word Sense In-
duction by Semantic Clustering. Cambridge Uni-
versity, Master?s Thesis, M.Phil. in Computer 
Speech. 
Agirre, E. and Soroa, A. 2007.  Semeval-2007 task 02: 
Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations:7-12 
Ioannis P. Klapaftis and Suresh Manandhar. 2008. 
Word sense induction using graphs of collocations. 
In Proceedings of the 18th European Conference 
On Artificial Intelligence (ECAI-2008), Patras, 
Greece, July. IOS Press. 
Kannan, R., Vempala, S and Vetta, A. 2004. On clus-
terings: Good, bad and spectral. J. ACM, 51(3), 
497?515. 
Reinhard Rapp.2004. A practical solution to the 
problem of automatic word sense induction. Pro-
ceedings of the ACL 2004 on Interactive poster 
and demonstration sessions, p.26-es, July 21-26, 
2004, Barcelona, Spain 
Bordag, S. 2006. Word sense induction: Triplet-based 
clustering and automatic evaluation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics (EACL, Trento, Italy). 137--144. 
Ying Zhao, and George Karypis.2005. Hierarchical 
Clustering Algorithms for Document Datasets. Da-
ta Mining and Knowledge Discovery, 10, 141?168. 
 
