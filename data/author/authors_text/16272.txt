Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 31?36,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Personalized Normalization for a Multilingual Chat System 
 
 
Ai Ti Aw and Lian Hau Lee 
Human Language Technology 
Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632 
aaiti@i2r.a-star.edu.sg  
 
 
 
Abstract 
This paper describes the personalized 
normalization of a multilingual chat system that 
supports chatting in user defined short-forms or 
abbreviations. One of the major challenges for 
multilingual chat realized through machine 
translation technology is the normalization of 
non-standard, self-created short-forms in the 
chat message to standard words before 
translation. Due to the lack of training data and 
the variations of short-forms used among 
different social communities, it is hard to 
normalize and translate chat messages if user 
uses vocabularies outside the training data and 
create short-forms freely. We develop a 
personalized chat normalizer for English and 
integrate it with a multilingual chat system, 
allowing user to create and use personalized 
short-forms in multilingual chat.  
1 Introduction  
Processing user-generated textual content on social 
media and networking usually encounters 
challenges due to the language used by the online 
community. Though some jargons of the online 
language has made their way into the standard 
dictionary, a large portion of the abbreviations, 
slang and context specific terms are still 
uncommon and only understood within the user 
community. Consequently, content analysis or 
translation techniques developed for a more formal 
genre like news or even conversations cannot 
apply directly and effectively to the social media 
content. In recent years, there are many works (Aw 
et al, 2006; Cook et al, 2009; Han et al, 2011) on 
text normalization to preprocess user generated 
content such as tweets and short messages before 
further processing. The approaches include 
supervised or unsupervised methods based on 
morphological and phonetic variations. However, 
most of the multilingual chat systems on the 
Internet have not yet integrated this feature into 
their systems but requesting users to type in proper 
language so as to have good translation. This is 
because the current techniques are not robust 
enough to model the different characteristics 
featured in the social media content. Most of the 
techniques are developed based on observations 
and assumptions made on certain datasets. It is also 
difficult to unify the language uniqueness among 
different users into a single model.  
We propose a practical and effective method, 
exploiting a personalized dictionary for each user, 
to support the use of user-defined short-forms in a 
multilingual chat system - AsiaSpik. The use of this 
personalized dictionary reduces the reliance on the 
availability and dependency of training data and 
empowers the users with the flexibility and 
interactivity to include and manage their own 
vocabularies during chat.  
2 ASIASPIK System Overview 
AsiaSpik is a web-based multilingual instant 
messaging system that enables online chats written 
in one language to be readable in other languages 
by other users. Figure 1 describes the system 
process. It describes the process flow between 
Chat Client, Chat Server, Translation Bot and 
Normalization Bot whenever Chat Client starts 
chat module. 
When Chat Client starts chat module, the Chat 
Client checks if the normalization option for that 
language used by the user is active and activated. If 
31
so, any message sent by the user will be routed to 
the Normalization Bot for normalization before 
reaching the Chat Server. The Chat Server then 
directs the message to the designated recipients. 
Chat Client at each recipient invokes a translation 
request to the Translation Bot to translate the 
message to the language set by the recipient. This 
allows the same source message to be received by 
different recipients in different target languages.  
 Figure 1. AsiaSpik Chat Process Flow 
 
In this system, we use Openfire Chat Server by 
Ignite Realtime as our Chat Server. We custom 
build a web-based Chat Client to communicate 
with the Chat Server based on Jabber/XMPP to 
receive presence and messaging information. We 
also develop a user management plug-in to 
synchronize and authenticate user login. The 
translation and normalization function used by the 
Translation Bot and Normalization Bot are 
provided through Web Services.  
The Translation Web Service uses in-house 
translation engines and supports the translation 
from Chinese, Malay and Indonesian to English 
and vice versa. Multilingual chat among these 
languages is achieved through pivot translation 
using English as the pivot language. The 
Normalization Web Service supports only English 
normalization. Both web services are running on 
Apache Tomcat web server with Apache Axis2.  
 
3 Personalized Normalization  
Personalized Normalization is the main distinction 
of AsiaSpik among other multilingual chat system.  
It gives the flexibility for user to personalize 
his/her short-forms for messages in English. 
3.1 Related Work 
The traditional text normalization strategy follows 
the noisy channel model (Shannon, 1948). Suppose 
the chat message is C and its corresponding 
standard form is S , the approach aims to find 
)|(maxarg CSP by computing 
)|(maxarg SCP  in which )(SP is usually a 
language model and )|( SCP  is an error model. 
The objective of using model in the chat message 
normalization context is to develop an appropriate 
error model for converting the non-standard and 
unconventional words found in chat messages into 
standard words. 
 
)()|(maxarg)|(maxarg^ SPSCPCSPS
SS
??  
 
Recently, Aw et al (2006) model text message 
normalization as translation from the texting 
language into the standard language. Choudhury et 
al. (2007) model the word-level text generation 
process for SMS messages, by considering 
graphemic/phonetic abbreviations and 
unintentional typos as hidden Markov model 
(HMM) state transitions and emissions, 
respectively. Cook and Stevenson (2009) expand 
the error model by introducing inference from 
different erroneous formation processes, according 
to the sample error distribution. Han and Baldwin 
(2011) use a classifier to detect ill-formed words, 
and generate correction candidates based on 
morphophonemic similarity. These models are 
effective on their experiments conducted, however, 
much works remain to be done to handle the 
diversity and dynamic of content and fast evolution 
of words used in social media and networking. 
As we notice that unlike spelling errors which 
are made mostly unintentionally by the writers, 
abbreviations or slangs found in chat messages are 
introduced intentionally by the senders most of the 
time. This leads us to suggest that if facilities are 
given to users to define their abbreviations, the 
dynamic of the social content and the fast 
32
evolution of words could be well captured and 
managed by the user. In this way, the 
normalization model could be evolved together 
with the social media language and chat message 
could also be personalized for each user 
dynamically and interactively. 
3.2 Personalized Normalization Model 
We employ a simple but effective approach for 
chat normalization. We express normalization 
using a probabilistic model as below 
 
)|(maxarg csPs
sbest
?  
 
and define the probability using a linear 
combination of features  
 
),(exp)|(
1
cshcsP k
m
k
k?
?
? ?  
 
where ),( cshk are two feature functions namely  
the log probability )|( , iji csP of a short-form, ic , 
being normalized to a standard form, jis , ; and the 
language model log probability. k? are weights of 
the feature functions.  
We define )|( , iji csP as a uniform distribution 
computed through a set of dictionary collected 
from corpus, SMS messages and Internet sources. 
A total of 11,119 entries are collected and each 
entry is assigned with an initial probability, 
||
1)|( ,
i
ijis ccsP ?  , where || ic  is the number of 
ic entries defined in the dictionary. We adjust the 
probability manually for some entries that are very 
common and occur more than a certain threshold, 
t , in the NUS SMS corpus (How and Kan, 2005) 
with a higher weight-age, w . This model, together 
with the language model, forms our baseline 
system for chat normalization. 
 
???
???
?
??
???
??
?
 |),(| if     |),(|
|),(|
||
1
|),(| if                         ||
1
)|(
,
,
,
,
,
tcstcs
tcsw
c
tcswccsP
iji
iji
iji
i
iji
i
ijis
 
 
To enable personalized real-time management 
of user-defined abbreviations and short-forms, we 
define a personalized model )|( ,_ ijiiuser csP  for 
each user based on his/her dictionary profile. Each 
personalized model is loaded into the memory 
once the user activates the normalization option. 
Whenever there is a change in the entry, the entry?s 
probability will be re-distributed and updated 
based on the following model. This characterizes 
the AsiaSpik system which supports personalized 
and dynamic chat normalization. 
 
??
?
?
??
?
?
?
?
???
???
?
 SD  if                 M
 1
SD  , SD  if                  1
S  ,c if   )|(
)|( ,
, i,
,_
i
jii
jiijis
ijiiuser
c
scMN
DsMN
NcsP
csP
 
  
 .dictionaryin user  entries  of number  thedenotes  M 
SDin  entries  of number  thedenotesN   
;dictionarydefault  denotes SD  
  where
i
ic
c
 The feature weights in the normalization model 
are optimized by minimum error rate training 
(Och, 2003), which searches for weights 
maximizing the normalization accuracy using a 
small development set. We use standard state-of-
the-art open source tools, Moses (Koehn, 2007), to 
develop the system and the SRI language modeling 
toolkit (Stolcke,2003) to train a trigram language 
model on the English portion of the Europarl 
Corpus (Koehn, 2005). 
3.3 Experiments 
We conducted a small experiment using 134 chat 
messages sent by high school students. Out of 
these messages, 73 short-forms are uncommon and 
not found in our default dictionary. Most of these 
33
short-forms are very irregular and hard to predict 
their standard forms using morphological and 
phonetic similarity. It is also hard to train a 
statistical model if training data is not available. 
We asked the students to define their personal 
abbreviations in the system and run through the 
system with and without the user dictionary. We 
asked them to give a score of 1 if the output is 
acceptable to them as proper English, otherwise a 0 
will be given. We compared the results using both 
the baseline model and the model implemented 
using the same training data as in Aw et al (2006). 
Table 1 shows the number of accepted output 
between the two models. Both models show 
improvement with the use of user dictionary. It 
also shows that it is very critical to have similar 
training data for the targeted domain to have good 
normalization performance. A simple model helps 
if such training data is unavailable. Nevertheless, 
the use of a dictionary driven by the user is an 
alternative to improve the overall performance. 
One reason for the inability of both models to 
capture the variations fully is because many 
messages require some degree of rephrasing in 
addition to insertion and deletion to make it 
readable and acceptable. For example, the ideal 
output for ?haiz, I wanna pontang school? is ?Sigh, 
I do not feel like going to school?, which may not 
be just a normalization problem. 
 
Baseline 
Model 
Baseline  + 
User 
Dictionary 
Aw et al 
(2006) 
Aw et al 
(2006) + 
user 
Dictionary 
40 72 17 42 
Table 1. Number of Correct Normalization Output 
 
In the examples showed in Table 2, ?din? and 
?dnr? are normalized to ?didn?t? and ?do not reply? 
based on the entries captured in the default 
dictionary. With the extension of normalization 
hypotheses in the user dictionary, the system 
produces the correct expansion to ?dinner?.  
 
 
 
 
Chat Message Chat Message 
normalized 
using the 
Default 
dictionary 
Chat Message 
normalized 
with the 
supplement of  
user dictionary 
buy din 4 
urself. 
Buy didn't for 
yourself. 
Buy dinner for 
yourself. 
dun cook dnr 4 
me 2nite 
Don't cook do 
not reply for me 
tonight 
Don't cook 
dinner for me 
tonight 
gtg bb ttyl ttfn Got to go bb ttyl 
ttfn 
Got to go bye 
talk to you later 
bye bye 
I dun feel lyk 
riting 
I don't feel lyk 
riting 
I don't feel like 
writing 
im gng hme 2 
mug 
I'm going hme 
two mug 
I'm going home 
to study 
msg me wh u 
rch 
Message me wh 
you rch 
Message me 
when you reach 
so sian I dun 
wanna do hw 
now 
So sian I don't 
want to do how 
now 
So bored I don't 
want to do 
homework now 
Table 2. Normalized chat messages 
AsiaSpik Multilingual Chat  
 
Figure 2 and Figure 3 show the personal lingo 
defined by two users. Note that expansions for 
?gtg? and ?tgt? are defined differently and 
expanded differently for the two users.  ?Me? in the 
message box indicates the message typed by the 
user while ?Expansion? is the message expanded 
by the system.  
  
Figure 2. Short-forms defined and messages 
expanded for user 1 
34
 Figure 3. Short-forms defined and messages 
expanded for user 2 
 
 
Figure 4 shows the multilingual chat exchange 
between a Malay language user (Mahani) and an 
English user (Keith). The figure shows the 
messages are first expanded to the correct forms 
before translated to the recipient language. 
 
  
Figure 4. Conversion between a Malay user & an 
English user 
4 Conclusions 
AsiaSpik system provides an architecture for 
performing chat normalization for each user such 
that user can chat as usual and does not need to pay 
special attention to type in proper language when 
involving translation for multilingual chat. The 
system aims to overcome the limitations of 
normalizing social media content universally 
through a personalized normalization model. The 
proposed strategy makes user the active contributor 
in defining the chat language and enables the 
system to model the user chat language 
dynamically.  
The normalization approach is a simple 
probabilistic model making use of the 
normalization probability defined for each short-
form and the language model probability. The 
model can be further improved by fine-tuning the 
normalization probability and incorporate other 
feature functions. The baseline model can also be 
further improved with more sophisticated method 
without changing the architecture of the full 
system. 
AsiaSpik is a demonstration system. We would 
like to expand the normalization model to include 
more features and support other languages such as 
Malay and Chinese. We would also like to further 
enhance the system to convert the translated 
English chat messages back to the social media 
language as defined by the user. 
References 
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A 
Phrase-based statistical model for SMS text 
normalization. In Proc. Of the COLING/ACL 2006 
Main Conference Poster Sessions, pages 33-40. 
Sydney.  
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh 
Mukherjee, Sudeshna Sarkar, and Anupam Basu. 
2007. Investigation and modeling of the structure of 
texting language. International Journal on Document 
Analysis and Recognition, 10:157?174. 
Paul Cook and Suzanne Stevenson. 2009. An 
unsupervised model for text message normalization. 
In CALC ?09: Proceedings of the Workshop on 
Computational Approaches to Linguistic Creativity, 
pages 71?78, Boulder, USA. 
Bo Han and Timothy Baldwin. 2011. Leixcal 
Normalisation of Short Text Messages: Makn Sens a 
#twitter. In Proc. Of the 49th Annual Meeting of the 
Association for Computational Linguistics, pages 
368-378, Portland, Oregon, USA. 
Yijue How and Min-Yen Kan. 2005. Optimizing 
predictive text entry for short message service on 
mobile phones. In Proceedings of HCII.  
Philipp Koehn &al. Moses: Open Source Toolkit for 
Statistical Machine Translation, ACL 2007, 
demonstration session.  
Koehn, P. (2005). Europarl: A Parallel Corpus for 
Statistical Machine Translation. In Machine 
Translation Summit X (pp. 79{86). Phuket, Thailand. 
Franz Josef Och. 2003. Minimum error rate training for 
statistical machine translation. In Proceedings of the 
35
41th Annual Meeting of the Association for 
Computational Linguistics, Sapporo, July.  
C. Shannon. 1948. A mathematical theory of 
communication. Bell System Technical Journal 
27(3): 379-423 
A. Stolcke. 2003 SRILM ? an Extensible Language 
Modeling Toolkit. In International Conference on 
Spoken Language Processing, Denver, USA. 
 
36
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 36?44,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
An Unsupervised and Data-Driven Approach for Spell Checking in
Vietnamese OCR-scanned Texts
Cong Duy Vu HOANG & Ai Ti AW
Department of Human Language Technology (HLT)
Institute for Infocomm Research (I2R)
A*STAR, Singapore
{cdvhoang,aaiti}@i2r.a-star.edu.sg
Abstract
OCR (Optical Character Recognition) scan-
ners do not always produce 100% accuracy
in recognizing text documents, leading to
spelling errors that make the texts hard to
process further. This paper presents an in-
vestigation for the task of spell checking
for OCR-scanned text documents. First, we
conduct a detailed analysis on characteris-
tics of spelling errors given by an OCR
scanner. Then, we propose a fully auto-
matic approach combining both error detec-
tion and correction phases within a unique
scheme. The scheme is designed in an un-
supervised & data-driven manner, suitable
for resource-poor languages. Based on the
evaluation on real dataset in Vietnamese
language, our approach gives an acceptable
performance (detection accuracy 86%, cor-
rection accuracy 71%). In addition, we also
give a result analysis to show how accurate
our approach can achieve.
1 Introduction and Related Work
Documents that are only available in print re-
quire scanning from OCR devices for retrieval
or e-archiving purposes (Tseng, 2002; Magdy
and Darwish, 2008). However, OCR scanners
do not always produce 100% accuracy in rec-
ognizing text documents, leading to spelling er-
rors that make the texts texts hard to process fur-
ther. Some factors may cause those errors. For
instance, shape or visual similarity forces OCR
scanners to misunderstand some characters; or in-
put text documents do not have good quality, caus-
ing noises in resulting scanned texts. The task of
spell checking for OCR-scanned text documents
proposed aims to solve the above situation.
Researchers in the literature used to approach
this task for various languages such as: English
(Tong and Evans, 1996; Taghva and Stofsky,
2001; Kolak and Resnik, 2002), Chinese (Zhuang
et al, 2004), Japanese (Nagata, 1996; Nagata,
1998), Arabic (Magdy and Darwish, 2006), and
Thai (Meknavin et al, 1998).
The most common approach is to involve users
for their intervention with computer support.
Taghva and Stofsky (2001) designed an interac-
tive system (called OCRSpell) that assists users as
many interactive features as possible during their
correction, such as: choose word boundary, mem-
orize user-corrected words for future correction,
provide specific prior knowledge about typical er-
rors. For certain applications requiring automa-
tion, the interactive scheme may not work.
Unlike (Taghva and Stofsky, 2001), non-
interactive (or fully automatic) approaches have
been investigated. Such approaches need pre-
specified lexicons & confusion resources (Tong
and Evans, 1996), language-specific knowledge
(Meknavin et al, 1998) or manually-created pho-
netic transformation rules (Hodge and Austin,
2003) to assist correction process.
Other approaches used supervised mecha-
nisms for OCR error correction, such as: statis-
tical language models (Nagata, 1996; Zhuang et
al., 2004; Magdy and Darwish, 2006), noisy chan-
nel model (Kolak and Resnik, 2002). These ap-
proaches performed well but are limited due to
requiring large annotated training data specific to
OCR spell checking in languages that are very
hard to obtain.
Further, research in spell checking for
Vietnamese language has been understudied.
36
Hunspell?spellcheck?vn1 & Aspell2 are inter-
active spell checking tools that work based on
pre-defined dictionaries.
According to our best knowledge, there is no
work in the literature reported the task of spell
checking for Vietnamese OCR-scanned text doc-
uments. In this paper, we approach this task in
terms of 1) fully automatic scheme; 2) without us-
ing any annotated corpora; 3) capable of solving
both non-word & real-word spelling errors simul-
taneously. Such an approach will be beneficial for
a poor-resource language like Vietnamese.
2 Error Characteristics
First of all, we would like to observe and analyse
the characteristics of OCR-induced errors in com-
pared with typographical errors in a real dataset.
2.1 Data Overview
We used a total of 24 samples of Vietnamese
OCR-scanned text documents for our analysis.
Each sample contains real & OCR texts, referring
to texts without & with spelling errors, respec-
tively. Our manual sentence segmentation gives
a result of totally 283 sentences for the above 24
samples, with 103 (good, no errors) and 180 (bad,
errors existed) sentences. Also, the number of syl-
lables3 in real &OCR sentences (over all samples)
are 2392 & 2551, respectively.
2.2 Error Classification
We carried out an in-depth analysis on spelling
errors, identified existing errors, and then man-
ually classified them into three pre-defined error
classes. For each class, we also figured out how
an error is formed.
As a result, we classified OCR-induced spelling
errors into three classes:
Typographic or Non-syllable Errors (Class 1):
refer to incorrect syllables (not included
in a standard dictionary). Normally, at
least one character of a syllable is expected
misspelled.
1http://code.google.com/p/
hunspell-spellcheck-vi/
2http://en.wikipedia.org/wiki/GNU_
Aspell/
3In Vietnamese language, we will use the word ?sylla-
ble? instead of ?token? to mention a unit that is separated by
spaces.
Real-syllable or Context-based Errors (Class 2):
refer to syllables that are correct in terms of
their existence in a standard dictionary but
incorrect in terms of their meaning in the
context of given sentence.
Unexpected Errors (Class 3): are accidentally
formed by unknown operators, such as:
insert non-alphabet characters, do incorrect
upper-/lower- case, split/merge/remove
syllable(s), change syllable orders, . . .
Note that errors in Class 1 & 2 can be formed
by applying one of 4 operators4 (Insertion, Dele-
tion, Substitution, Transposition). Class 3 is ex-
clusive, formed by unexpected operators. Table 1
gives some examples of 3 error classes.
An important note is that an erroneous syllable
can contain errors across different classes. Class
3 can appear with Class 1 or Class 2 but Class 1
never appears with Class 2. For example:
? ho?n (correct) || H?an (incorrect) (Class 3 & 1)
? b?t (correct) || b?t? (incorrect) (Class 3 & 2)
Figure 1: Distribution of operators used in Class
1 (left) & Class 2 (right).
2.3 Error Distribution
Our analysis reveals that there are totally 551 rec-
ognized errors over all 283 sentences. Each error
is classified into three wide classes (Class 1, Class
2, Class 3). Specifically, we also tried to identify
operators used in Class 1 & Class 2. As a result,
we have totally 9 more fine-grained error classes
(1A..1D, 2A..2D, 3)5.
We explored the distribution of 3 error classes
in our analysis. Class 1 distributed the most, fol-
lowing by Class 3 (slightly less) and Class 2.
4Their definitions can be found in (Damerau, 1964).
5A, B, C, and D represent for Insertion, Deletion, Sub-
stitution, and Transposition, respectively. For instance, 1A
means Insertion in Class 1.
37
Class Insertion Deletion Substitution Transpositiona
Class 1 ?p (correct) || ?ip (in-
correct) (?i? inserted)
kh?ng (correct) || kh
(incorrect). (???, ?n?,
and ?g? deleted)
y?u (correct) || ??u
(incorrect). (?y? sub-
stituted by ???)
N.A.
Class 2 l?n (correct) || li?n
(contextually incor-
rect). (?i? inserted)
tr?nh (correct) ||
t?nh (contextually
incorrect). (?r?
deleted)
ngay (correct) || ng?y
(contextually incor-
rect). (?a? substituted
by ???)
N.A.
Class 3 x?c nh?n l? (correct) || x||nha0a (incorrect). 3 syllables were misspelled & accidentally merged.
aOur analysis reveals no examples for this operator.
Table 1: Examples of error classes.
Generally, each class contributed a certain quan-
tity of errors (38%, 37%, & 25%), making the
correction process of errors more challenging. In
addition, there are totally 613 counts for 9 fine-
grained classes (over 551 errors of 283 sentences),
yielding an average & standard deviation 3.41 &
2.78, respectively. Also, one erroneous syllable is
able to contain the number of (fine-grained) error
classes as follows: 1(492), 2(56), 3(3), 4(0) ((N)
is count of cases).
We can also observe more about the distribu-
tion of operators that were used within each error
class in Figure 1. The Substitution operator was
used the most in both Class 1 & Class 2, holding
81% & 97%, respectively. Only a few other oper-
ators (Insertion, Deletion) were used. Specially,
the Transposition operator were not used in both
Class 1&Class 2. This justifies the fact that OCR
scanners normally have ambiguity in recognizing
similar characters.
3 Proposed Approach
The architecture of our proposed approach
(namely (VOSE)) is outlined in Figure 2. Our pur-
pose is to develop VOSE as an unsupervised data-
driven approach. It means VOSE will only use
textual data (un-annotated) to induce the detection
& correction strategies. This makes VOSE unique
and generic to adapt to other languages easily.
In VOSE, potential errors will be detected lo-
cally within each error class and will then be cor-
rected globally under a ranking scheme. Specif-
ically, VOSE implements two different detectors
(Non-syllable Detector & Real-syllable Detec-
tor) for two error groups of Class 1/3 & Class
2, respectively. Then, a corrector combines the
outputs from two above detectors based on rank-
ing scheme to produce the final output. Currently,
VOSE implements two different correctors, a
Contextual Corrector and a Weighting-based
Corrector. Contextual Corrector employs lan-
guage modelling to rank a list of potential can-
didates in the scope of whole sentence whereas
Weighting-based Corrector chooses the best
candidate for each syllable that has the highest
weights. The following will give detailed descrip-
tions for all components developed in VOSE.
3.1 Pre-processor
Pre-processor will take in the input text, do
tokenization & normalization steps. Tokeniza-
tion in Vietnamese is similar to one in En-
glish. Normalization step includes: normal-
ize Vietnamese tone & vowel (e.g. h?a ?>
ho?), standardize upper-/lower- cases, find num-
bers/punctuations/abbreviations, remove noise
characters, . . .
This step also extracts unigrams. Each of them
will then be checked whether they exist in a pre-
built list of unigrams (from large raw text data).
Unigrams that do not exist in the list will be re-
garded as Potential Class 1 & 3 errors and then
turned into Non-syllable Detector. Other uni-
grams will be regarded as Potential Class 2 er-
rors passed into Real-syllable Detector.
3.2 Non-syllable Detector
Non-syllable Detector is to detect errors that do
not exist in a pre-built combined dictionary (Class
1 & 3) and then generate a top-k list of poten-
tial candidates for replacement. A pre-built com-
bined dictionary includes all syllables (unigrams)
extracted from large raw text data.
In VOSE, we propose a novel approach that
uses pattern retrieval technique forNon-syllable
38
Figure 2: Proposed architecture of our approach
Detector. This approach aims to retrieve all n-
gram patterns (n can be 2,3) from textual data,
check approximate similarity with original erro-
neous syllables, and then produce a top list of po-
tential candidates for replacement.
We believe that this approach will be able to
not only handle errors with arbitrary changes on
syllables but also utilize contexts (within 2/3 win-
dow size), making possible replacement candi-
dates more reliable, and more semantically to
some extent.
This idea will be implemented in the N-gram
Engine component.
3.3 Real-syllable Detector
Real-syllable Detector is to detect all possible
real-syllable errors (Class 2) and then produce
the top-K list of potential candidates for replace-
ment. The core idea of Real-syllable Detector is
to measure the cohesion of contexts surrounding a
target syllable to check whether it is possibly erro-
neous or not. The cohesion is measured by counts
& probabilities estimated from textual data.
Assume that a K-size contextual window with a
target syllable at central position is chosen.
s1 s2 ? ? ? [sc] ? ? ? sK?1 sK (K syllables, sc to
be checked, K is an experimental odd value (can
be 3, 5, 7, 9).)
The cohesion of a sequence of syllables sK1 bi-
ased to central syllable sc can be measured by one
of three following formulas:
Formula 1:
cohesion1(s
K
1 ) = log(P (s
K
1 ))
= log(P (sc) ?
K?
i 6=c,i=1
P (si|sc))
(1)
Formula 2:
cohesion2(s
K
1 ) = countexist?(sc?2sc?1sc,
sc?1scsc+1, scsc+1sc+2, sc?1sc, scsc+1)
(2)
Formula 3:
cohesion3(s
K
1 ) = countexist?(sc?2 ? sc,
sc?1sc, sc ? sc+2, scsc+1)
(3)
where:
? cohesion(sK1 ) is cohesion measure of sequence s
K
1 .
39
? P (sc) is estimated from large raw text data com-
puted by c(sc)C , whereas c(sc) is unigram count and C
is total count of all unigrams from data.
? P (si|sc) is computed by:
P (si|sc) =
P (si, sc)
P (sc)
=
c(si, sc, |i? c|)
c(sc)
(4)
where:
? c(si, sc, |i? c|) is a distance-sensitive count of two
unigrams si and sc co-occurred and the gap between
them is |i? c| unigrams.
For Formula 1, if cohesion(sK1 ) < Tc with
Tc is a pre-defined threshold, the target syllable is
possibly erroneous.
For Formula 2, instead of probabilities as in
Formula 1, we use counting on existence of n-
grams within a context. It?s maximum value is 5.
Formula 3 is a generalized version of Formula 2
(the wild-card ?*? means any syllable). It?s maxi-
mum value is 4.
N-gram Engine. The N-gram Engine compo-
nent is very important in VOSE. All detectors &
correctors use it.
Data Structure. It is worthy noting that in or-
der to compute probabilities like c(si, sc, |i? c|)
or query the patterns from data, an efficient data
structure needs to be designed carefully. It MUST
satisfy two criteria: 1) space to suit memory re-
quirements 2) speed to suit real-time speed re-
quirement. In this work,N-gram Engine employs
inverted index (Zobel and Moffat, 2006), a well-
known data structure used in text search engines.
Pattern Retrieval. After detecting poten-
tial errors, both Non-syllable Detector and
Real-syllable Detector use N-gram Engine to
find a set of possible replacement syllables by
querying the textual data using 3-gram patterns
(sc?2sc?1[s?c], sc?1[s
?
c]sc+1, and [s
?
c]sc+1sc+2) or
2-gram patterns (sc?1 [s?c], [s
?
c]sc+1), where [s
?
c] is
a potential candidate. To rank a list of top candi-
dates, we compute the weight for each candidate
using the following formula:
weight(si) = ??Sim(si, s
?
c)+(1??)?Freq(si)
(5)
where:
? Sim(si, s?c) is the string similarity between candi-
date syllable si and erroneous syllable s?c .
? Freq(si) is normalized frequency of si over a re-
trieved list of possible candidates.
? ? is a value to control the weight biased to string
similarity or frequency.
In order to compute the string similarity, we
followed a combined weighted string similarity
(CWSS) computation in (Islam and Inkpen, 2009)
as follows:
Sim(si, s
?
c) = ?1 ?NLCS(si, s
?
c)
+?2 ?NCLCS1(si, s
?
c) + ?3 ?NCLCSn(si, s
?
c)
+?4 ?NCLCSz(si, s
?
c)
(6)
where:
? ?1, ?2, ?3, and ?4 are pre-defined weights for each
similarity computation. Initially, all ? are set equal to
1/4.
? NLCS(si, s?c) is normalized length of longest
common subsequence between si and s?c .
? NCLCS1(si, s?c), NCLCSn(si, s
?
c), and
NCLCSz(si, s?c) is normalized length of maximal
consecutive longest common subsequence between
si and s?c starting from the first character, from any
character, and from the last character, respectively.
? Sim(si, s?c) has its value in range of [0, 1].
We believe that the CWSS method will ob-
tain better performance than standard meth-
ods (e.g. Levenshtein-based String Matching
(Navarro, 2001) or n-gram based similarity (Lin,
1998)) because it can exactly capture more infor-
mation (beginning, body, ending) of incomplete
syllables caused by OCR errors. As a result, this
step will produce a ranked top-k list of potential
candidates for possibly erroneous syllables. In ad-
dition, N-gram Engine also stores computation
utilities relating the language models which are
then provided to Contextual Corrector.
3.4 Corrector
In VOSE, we propose two possible correctors:
Weighting-based Corrector
Given a ranked top-K list of potential can-
didates from Non-syllable Detector and Real-
syllable Detector, Weighting-based Corrector
simply chooses the best candidates based on their
weights (Equation 5) to produce the final output.
Contextual Corrector
Given a ranked top-K list of potential can-
didates from Non-syllable Detector and Real-
syllable Detector, Contextual Corrector glob-
ally ranks the best candidate combination using
language modelling scheme.
40
Specifically, Contextual Corrector employs
the language modelling based scheme which
chooses the combination of candidates (sn1 )
? that
makes PP ((sn1 )
?) maximized over all combina-
tions as follows:
(sn1 )
?
best = argmax(sn1 )? PP ((s
n
1 )
?) (7)
where: PP (.) is a language modelling score or per-
plexity (Jurafsky and Martin, 2008; Koehn, 2010).
In our current implementation, we used Depth-
First Traversal (DFS) strategy to examine over all
combinations. The weakness of DFS strategy is
the explosion of combinations if the number of
nodes (syllables in our case) grows more than 10.
In this case, the speed of DFS-based Contextual
Corrector is getting slow. Future work can con-
sider beam search decoding idea in Statistical
Machine Translation (Koehn, 2010) to adapt for
Contextual Corrector.
3.5 Prior Language-specific Knowledge
SinceVOSE is an unsupervised & data-driven ap-
proach, its performance depends on the quality
and quantity of raw textual data. VOSE?s cur-
rent design allows us to integrate prior language-
specific knowledge easily.
Some possible sources of prior knowledge
could be utilized as follows:
? Vietnamese Character Fuzzy Matching - In
Vietnamese language, some characters look very
similar, forcing OCR scanners mis-recognition.
Thus, we created a manual list of highly similar
characters (as shown in Table 2) and then inte-
grate this into VOSE. Note that this integration
takes place in the process of string similarity com-
putation.
? English Words & Vietnamese Abbrevia-
tions Filtering - In some cases, there exist En-
glish words or Vietnamese abbreviations. VOSE
may suggest wrong replacements for those cases.
Thus, a syllable in either English words or Viet-
namese abbreviations will be ignored in VOSE.
4 Experiments
4.1 Baseline Systems
According to our best knowledge, previous sys-
tems that are able to simultaneously handle both
non-syllable and real-syllable errors do not exist,
especially apply for Vietnamese language. We be-
lieve that VOSE is the first one to do that.
No. Character Similar Characters
1 a {? ? ? ? ? ? ? ?}
2 e {? ? ? ?} + {c}
3 i {? ?} + {l}
4 o {? ? ? ? ?}
5 u {? ? ? ? ?}
6 y {? ?}
7 d {?}
Table 2: Vietnamese similar characters.
4.2 N-gram Extraction Data
In VOSE, we extracted ngrams from the raw tex-
tual data. Table 3 shows data statistics used in our
experiments.
4.3 Evaluation Measure
We used the following measure to evaluate the
performance of VOSE:
- For Detection:
DF =
2?DR?DP
DR+DP
(8)
Where:
? DR (Detection Recall) = the fraction of errors
correctly detected.
? DP (Detection Precision) = the fraction of de-
tected errors that are correct.
? DF (Detection F-Measure) = the combination
of detection recall and precision.
- For Correction:
CF =
2? CR? CP
CR+ CP
(9)
Where:
? CR (Correction Recall) = the fraction of errors
correctly amended.
? CP (Correction Precision) = the fraction of
amended errors that are correct.
? CF (Correction F-Measure) = the combination
of correction recall and precision.
4.4 Results
We carried out our evaluation based on the real
dataset as described in Section 2. In our evalua-
tion, we intend:
? To evaluate whether VOSE can benefit from ad-
dition of more data, meaning that VOSE is actu-
ally a data-driven system.
? To evaluate the effectiveness of language mod-
elling based corrector in compared to weighing
41
N-grams
No Dataset NumOfSents Vocabulary 2-gram 3-gram 4-gram 5-gram
1 DS1 1,328,506 102,945 1,567,045 8,515,894 17,767,103 24,700,815
2 DS2a 2,012,066 169,488 2,175,454 12,610,281 27,961,302 40,295,888
3 DS3b 283 1,546 6,956 9,030 9,671 9,946
4 DS4c 344 1,755 6,583 7,877 8,232 8,383
aincludes DS1 and more
bannotated test data (not included in DS1 & DS2) as described in Section 2
cweb contexts data (not included in others) crawled from the Internet
Table 3: Ngram extraction data statistics.
based corrector.
? To evaluate whether prior knowledge specific
to Vietnamese language can help VOSE.
The overall evaluation result (in terms of detec-
tion & correction accuracy) is shown in Table 4.
In our experiments, all VOSE(s) except of VOSE
6 used contextual corrector (Section 3.4). Also,
Real-syllable Detector (Section 3.3) used Equa-
tion 3 which revealed the best result in our pre-
evaluation (we do not show the results because
spaces do not permit).
We noticed the tone & vowel normalization
step in Pre-processormodule. This step is impor-
tant specific to Vietnamese language. VOSE 2a in
Table 4 shows that VOSE using that step gives a
significant improvement (vs. VOSE 1) in both de-
tection & correction.
We also tried to assess the impact of language
modelling order factor in VOSE. VOSE using 3-
gram language modelling gives the best result
(VOSE 2a vs. VOSE 2b & 2c). Because of this,
we chose 3-gram for next VOSE set-ups.
We experiment how data addition affects
VOSE. First, we used bigger data (DS2) for ngram
extraction and found the significant improvement
(VOSE 3a vs. VOSE 2a). Second, we tried an
interesting set-up in which VOSE utilized ngram
extraction data with annotated test data (Dataset
DS3) only in order to observe the recall ability
of VOSE. Resulting VOSE (VOSE 3b) performed
extremely well.
As discussed in Section 3.5, VOSE allows in-
tegrated prior language-specific knowledge that
helps improve the performance (VOSE 4). This
justifies that statistical method in combined with
such prior knowledge is very effective.
Specifically, for each error in test data, we
crawled the web sentences containing contexts in
which that error occurs (called web contexts). We
added such web contexts into ngram extraction
data. With this strategy, we can improve the per-
formance of VOSE significantly (VOSE 5), ob-
taining the best result. Again, we?ve proved that
more data VOSE has, more accurate it performs.
The result of VOSE 6 is to show the superiority
of VOSE using contextual corrector in compared
with using weighting-based corrector (VOSE 6 vs.
VOSE 4). However, weighting-based corrector
has much faster speed in correction than contex-
tual corrector which is limited due to DFS traver-
sal & language modelling ranking.
Based on the above observations, we have two
following important claims:
? First, the addition of more data in ngram ex-
traction process is really useful for VOSE.
? Second, prior knowledge specific to Viet-
namese language helps to improve the perfor-
mance of VOSE.
? Third, contextual corrector with language mod-
elling is superior than weighting-based corrector
in terms of the accuracy.
4.5 Result Analysis
Based on the best results produced by our ap-
proach (VOSE), we recognize & categorize cases
that VOSE is currently unlikely to detect & cor-
rect properly.
Consecutive Cases (Category 1)
When there are 2 or 3 consecutive errors, their
contexts are limited or lost. This issue will af-
fect the algorithm implemented in VOSE utilizing
the contexts to predict the potential replacements.
VOSE can handle such errors to limited extent.
Merging Cases (Category 2)
In this case, two or more erroneous syllables
are accidentally merged. Currently, VOSE cannot
42
Detection Accuracy Correction Accuracy
Set-up Recall Precision F1 Recall Precision F1 Remark
VOSE 1 0.8782 0.5954 0.7097 0.6849 0.4644 0.5535 w/o TVN + 3-LM + DS1
VOSE 2a 0.8782 0.6552 0.7504 0.6807 0.5078 0.5817 w/ TVN + 3-LM + DS1
VOSE 2b 0.8782 0.6552 0.7504 0.6744 0.5031 0.5763 w/ TVN + 4-LM + DS1
VOSE 2c 0.8782 0.6552 0.7504 0.6765 0.5047 0.5781 w/ TVN + 5-LM + DS1
VOSE 3a 0.8584 0.7342 0.7914 0.6829 0.5841 0.6296 w/ TVN + 3-LM + DS2
VOSE 3b 0.9727 0.9830 0.9778 0.9223 0.9321 0.9271 w/ TVN + 3-LM + DS3
VOSE 4 0.8695 0.7988 0.8327 0.7095 0.6518 0.6794 VOSE 3a + PK
VOSE 5 0.8674 0.8460 0.8565 0.7200 0.7023 0.7110 VOSE 4 + DS4
VOSE 6 0.8695 0.7988 0.8327 0.6337 0.5822 0.6069 VOSE 4 but uses WC
Table 4: Evaluation results. Abbreviations: TVN (Tone & Vowel Normalization); N-LM (N-order
Language Modelling); DS (Dataset); PK (Prior Knowledge); WC (Weighting-based Corrector).
handle such cases. We aim to investigate this in
our future work.
Proper Noun/Abbreviation/Number Cases
(both in English, Vietnamese) (Category 3)
Abbreviations or proper nouns or numbers are
unknown (for VOSE) because they do not appear
in ngram extraction data. If VOSE marks them as
errors, it could not correct them properly.
Ambiguous Cases (Category 4)
Ambiguity can happen in:
? cases in which punctuation marks (e.g. comma,
dot, dash, . . . ) are accidentally added between two
different syllable or within one syllable.
? cases never seen in ngram extraction data.
? cases relating to semantics in Vietnamese.
? cases where one Vietnamese syllable that is
changed incorrectly becomes an English word.
Lost Cases (Category 5)
This case happens when a syllable which is ac-
cidentally lost most of its characters or too short
becomes extremely hard to correct.
Additionally, we conducted to observe the dis-
tribution of the above categories (Figure 3). As
can be seen, Category 4 dominates more than 70%
cases that VOSE has troubles for detection & cor-
rection.
5 Conclusion & Future Work
In this paper, we?ve proposed & developed a new
approach for spell checking task (both detection
and correction) for Vietnamese OCR-scanned text
documents. The approach is designed in an un-
supervised & data-driven manner. Also, it allows
Figure 3: Distribution of categories in the result
of VOSE 4 (left) & VOSE 5 (right).
to integrate the prior language-specific knowledge
easily.
Based on the evaluation on a real dataset,
the system currently offers an acceptable perfor-
mance (best result: detection accuracy 86%, cor-
rection accuracy 71%). With just an amount
of small n-gram extraction data, the obtained re-
sult is very promising. Also, the detailed error
analysis in previous section reveals that cases that
current system VOSE cannot solve are extremely
hard, referring to the problem of semantics-
related ambiguity in Vietnamese language.
Further remarkable point of proposed approach
is that it can perform the detection & correction
processes in real-time manner.
Future works include some directions. First, we
should crawl and add more textual data for n-gram
extraction to improve the performance of current
system. More data VOSE has, more accurate it
performs. Second, we should investigate more on
categories (as discussed earlier) that VOSE could
not resolve well. Last, we also adapt this work for
another language (like English) to assess the gen-
eralization and efficiency of proposed approach.
43
References
Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7:171?176, March.
Victoria J. Hodge and Jim Austin. 2003. A com-
parison of standard spell checking algorithms and
a novel binary neural approach. IEEE Trans. on
Knowl. and Data Eng., 15(5):1073?1081, Septem-
ber.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using google web it 3-grams.
In Proceedings of the 2009 Conference on Empir-
ical Methods in Natural Language Processing: Vol-
ume 3 - Volume 3, EMNLP ?09, pages 1241?1249,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, second
edition, February.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Okan Kolak and Philip Resnik. 2002. Ocr error
correction using a noisy channel model. In Pro-
ceedings of the second international conference on
Human Language Technology Research, HLT ?02,
pages 257?262, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Walid Magdy and Kareem Darwish. 2006. Arabic ocr
error correction using character segment correction,
language modeling, and shallow morphology. In
Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 408?414, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Walid Magdy and Kareem Darwish. 2008. Effect of
ocr error correction on arabic retrieval. Inf. Retr.,
11:405?425, October.
Surapant Meknavin, Boonserm Kijsirikul, Ananlada
Chotimongkol, and Cholwich Nuttee. 1998. Com-
bining trigram and winnow in thai ocr error cor-
rection. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics - Volume 2, ACL ?98, pages
836?842, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Masaaki Nagata. 1996. Context-based spelling cor-
rection for japanese ocr. In Proceedings of the 16th
conference on Computational linguistics - Volume
2, COLING ?96, pages 806?811, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Masaaki Nagata. 1998. Japanese ocr error correction
using character shape similarity and statistical lan-
guage model. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
922?928, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88,
March.
Kazem Taghva and Eric Stofsky. 2001. Ocrspell: an
interactive spelling correction system for ocr errors
in text. International Journal of Document Analysis
and Recognition, 3:2001.
Xian Tong and David A. Evans. 1996. A statistical
approach to automatic ocr error correction in con-
text. In Proceedings of the Fourth Workshop on
Very Large Corpora (WVLC-4, pages 88?100.
Yuen-Hsien Tseng. 2002. Error correction in a chi-
nese ocr test collection. In Proceedings of the 25th
annual international ACM SIGIR conference on Re-
search and development in information retrieval,
SIGIR ?02, pages 429?430, New York, NY, USA.
ACM.
Li Zhuang, Ta Bao, Xioyan Zhu, Chunheng Wang,
and S. Naoi. 2004. A chinese ocr spelling check
approach based on statistical language models. In
Systems, Man and Cybernetics, 2004 IEEE Interna-
tional Conference on, volume 5, pages 4727 ? 4732
vol.5.
Justin Zobel and Alistair Moffat. 2006. Inverted files
for text search engines. ACM Comput. Surv., 38,
July.
44
