Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1290?1298,
Beijing, August 2010
Entity Linking Leveraging
Automatically Generated Annotation 
Wei Zhang?    Jian Su? Chew Lim Tan?   Wen Ting Wang?
?School of Computing 
National University of Singapore 
{z-wei, tancl} 
@comp.nus.edu.sg
? Institute for Infocomm Research 
{sujian, wwang}
@i2r.a-star.edu.sg
Abstract
Entity linking refers entity mentions in a 
document to their representations in a 
knowledge base (KB). In this paper, we 
propose to use additional information 
sources from Wikipedia to find more 
name variations for entity linking task. In 
addition, as manually creating a training 
corpus for entity linking is labor-
intensive and costly, we present a novel 
method to automatically generate a large 
scale corpus annotation for ambiguous 
mentions leveraging on their unambi-
guous synonyms in the document collec-
tion. Then, a binary classifier is trained 
to filter out KB entities that are not simi-
lar to current mentions. This classifier 
not only can effectively reduce the am-
biguities to the existing entities in KB, 
but also be very useful to highlight the 
new entities to KB for the further popu-
lation. Furthermore, we also leverage on 
the Wikipedia documents to provide ad-
ditional information which is not availa-
ble in our generated corpus through a 
domain adaption approach which pro-
vides further performance improve-
ments.  The experiment results show that 
our proposed method outperforms the 
state-of-the-art approaches. 
1 Introduction 
The named entity (NE) ambiguation has raised 
serious problems in many areas, including web 
people search, knowledge base population 
(KBP), and information extraction, because an 
entity (such as Abbott Laboratories, a diversified 
pharmaceuticals health care company) can be 
referred to by multiple mentions (e.g. ?ABT? and 
?Abbott?), and a mention (e.g. ?Abbott?) can be 
shared by different entities (e.g. Abbott Texas: a 
city in United States; Bud Abbott, an American 
actor; and Abbott Laboratories, a diversified 
pharmaceutical health care company).  
Both Web People Search (WePS) task (Artiles 
et al 2007) and Global Entity Detection & Rec-
ognition task (GEDR) in Automatic Content Ex-
traction 2008 (ACE08) disambiguate entity men-
tions by clustering documents with these men-
tions. Each cluster then represents a unique enti-
ty. Recently entity linking has been proposed in 
this field. However, it is quite different from the 
previous tasks.
Given a knowledge base, a document collec-
tion, entity linking task as defined by KBP-091
(McNamee and Dang, 2009) is to determine for 
each name string and the document it appears, 
which knowledge base entity is being referred to, 
or if the entity is a new entity which is not 
present in the reference KB.  
Compared with GEDR and WePS, entity link-
ing has a given entity list (i.e. the reference KB) 
to which we disambiguate the entity mentions. 
Moreover, in document collection, there are new 
entities which are not present in KB and can be 
used for further population. In fact, new entities 
with or without the names in KB cover more 
than half of testing instances. 
1 http://apl.jhu.edu/~paulmac/kbp.html 
1290
Entity linking has been explored by several re-
searchers. Without any training data available, 
most of the previous work ranks the similarity 
between ambiguous mention and candidate enti-
ties through Vector Space Model (VSM). Since 
they always choose the entity with the highest 
rank as the answer, the ranking approaches hard-
ly detect a situation where there may be a new 
entity that is not present in KB. It is also difficult 
to combine bag of words (BOW) with other fea-
tures. For example, to capture the ?category? 
information, the method of Cucerzan (2007) in-
volves a complicated optimization issue and the 
approach has to be simplified for feasible com-
putation, which compromises the accuracy.  Be-
sides unsupervised methods, some supervised 
approaches (Agirre et al 2009, Li et al 2009 and 
McNamee et al 2009) also have been proposed 
recently for entity linking. However, the super-
vised approaches for this problem require large 
amount of training instances. But manually 
creating a corpus is labor-intensive and costly.  
In this paper, we explore how to solve the enti-
ty linking problem. We present a novel method 
that can automatically generate a large scale 
corpus for ambiguous mentions leveraging on 
their unambiguous synonyms in the document 
collection.  A binary classifier based on Support 
Vector Machine (SVM) is trained to filter out 
some candidate entities that are not similar to 
ambiguous mentions. This classifier can effec-
tively reduce the ambiguities to the existing enti-
ties in KB, and it is very useful to highlight the 
new entities to KB for the further population. 
We also leverage on the Wikipedia documents to 
provide additional information which is not 
available in our generated corpus through a do-
main adaption approach which provides further 
performance improvements. Besides, more in-
formation sources for finding more variations 
also contribute to the overall 22.9% accuracy 
improvements on KBP-09 test data over baseline. 
The remainder of the paper is organized as fol-
lows. Section 2 reviews related work for entity 
linking. In Section 3 we detail our algorithm in-
cluding name variation and entity disambigua-
tion. Section 4 describes the experimental setup 
and results. Finally, Section 5 concludes the pa-
per.
2 Related Work 
The crucial component of entity linking is the 
disambiguation process. Raphael et al (2007) 
report a disambiguation algorithm for geography. 
The algorithm ranks the candidates based on the 
manually assigned popularity scores in KB. The 
class with higher popularity will be assigned 
higher score. It causes that the rank of entities 
would never change, such as Lancaster (Califor-
nia) would always have a higher rank than Lan-
caster (UK) for any mentions. However, as the 
popularity scores for the classes change over 
time, it is difficult to accurately assign dynamic 
popularity scores. Cucerzan (2007) proposes a 
disambiguation approach based on vector space 
model for linking ambiguous mention in a doc-
ument with one entity in Wikipedia. The ap-
proach ranks the candidates and chooses the ent-
ity with maximum agreement between the con-
textual information extracted from Wikipedia 
and the context of a document, as well as the 
agreement among the category tags associated 
with the candidate entities. Nguyen and Cao 
(2008) refer the mentions in a document to KIM 
(Popov et al 2004) KB. KIM KB is populated 
with over 40,000 named entities. They represent 
a mention and candidates as vectors of their con-
textual noun phrase and co-occurring NEs, and 
then the similarity is determined by the common 
terms of the vectors and their associated weights. 
For linking mentions in news articles with a Wi-
kipedia-derived KB (KBP-09 data set), Varma et 
al. (2009) rank the entity candidates using a 
search engine. Han and Zhao (2009) rank the 
candidates based on BOW and Wikipedia se-
mantic knowledge similarity. 
All the related work above rank the candidates 
based on the similarity between ambiguous men-
tion and candidate entities. However, the ranking 
approach hardly detects the new entity which is 
not present in KB. 
Some supervised approaches also have been 
proposed. Li et al (2009) and McNamee et al 
(2009) train their models on a small manually 
created data set containing only 1,615 examples. 
But entity linking requires large training data. 
Agirre et al (2009) use Wikipedia to construct 
their training data by utilizing Inter-Wikipedia 
links and the surrounding snippets of text. How-
ever, their training data is created from a         
1291
different domain which does not work well in 
the targeted news article domain.  
3 Approach
In this section we describe our two-stage ap-
proach for entity linking: name variation and 
entity disambiguation. The first stage finds vari-
ations for every entity in the KB and generates 
an entity candidate set for a given query. The 
second stage is entity disambiguation, which 
links an entity mention with the real world entity 
it refers to. 
3.1 Name Variation 
The aim for Name Variation is to build a 
Knowledge Repository of entities that contains 
vast amount of world knowledge of entities like 
name variations, acronyms, confusable names, 
spelling variations, nick names etc. We use 
Wikipedia to build our knowledge repository 
since Wikipedia is the largest encyclopedia in 
the world and surpasses other knowledge bases 
in its coverage of concepts and up-to-date 
content. We obtain useful information from 
Wikipedia by the tool named Java Wikipedia 
Library 2  (Zesch et al 2008), which allows to 
access all information contained in Wikipedia. 
Cucerzan (2007) extracts the name variations 
of an entity by leveraging four knowledge 
sources in Wikipedia: ?entity pages?, ?disam-
biguation pages?  ?redirect pages? and ?anchor 
text?.
Entity page in Wikipedia is uniquely identified 
by its title ? a sequence of words, with the first 
word always capitalized. The title of Entity Page 
represents an unambiguous name variation for 
the entity. A redirect page in Wikipedia is an aid 
to navigation. When a page in Wikipedia is redi-
rected, it means that those set of pages are refer-
ring to the same entity. They often indicate syn-
onym terms, but also can be abbreviations, more 
scientific or more common terms, frequent 
misspellings or alternative spellings etc. Disam-
biguation pages are created only for ambiguous 
mentions which denote two or more entities in 
Wikipedia, typically followed by the word ?dis-
ambiguation? and containing a list of references 
to pages for entities that share the same name. 
This is more useful in extracting the abbrevia-
2 http://www.ukp.tu-darmstadt.de/software/JWPL 
tions of entities, other possible names for an ent-
ity etc. Besides, both outlinks and inlinks in Wi-
kipedia are associated with anchor texts that 
represent name variations for the entities.
Using these four sources above, we extracted 
name variations for every entity in KB to form 
the Knowledge Repository as Cucerzan?s (2007) 
method. For example, the variation set for entity 
E0272065 in KB is {Abbott Laboratories, Ab-
bott Nutrition, Abbott ?}. Finally, we can gen-
erate the entity candidate set for a given query 
using the Knowledge Repository. For example, 
for the query containing ?Abbott?, the entity 
candidate set retrieved is {E0272065, E0064214 
?}.
From our observation, for some queries the re-
trieved candidate set is empty. If the entity for 
the query is a new entity, not present in KB, 
empty candidate set is correct. Otherwise, we 
fail to identify the mention in the query as a var-
iation, commonly because the mention is a miss-
pelling or infrequently used name. So we pro-
pose to use two more sources ?Did You Mean? 
and ?Wikipedia Search Engine? when Cucerzan 
(2007) algorithm returns empty candidate set. 
Our experiment results show that both proposed 
knowledge sources are effective for entity link-
ing. This contributes to a performance improve-
ment on the final entity linking accuracy. 
Did You Mean: The ?did you mean? feature 
of Wikipedia can provide one suggestion for 
misspellings of entities. This feature can help to 
correct the misspellings. For example, ?Abbot 
Nutrition? can be corrected to ?Abbott Nutri-
tion?.
Wikipedia Search Engine: This key word 
based search engine can return a list of relevant 
entity pages of Wikipedia. This feature is more 
useful in extracting infrequently used name. 
Algorithm 1 below presents the approach to 
generate the entity candidate set over the created 
Knowledge Repository. Ref
E
(s) is the entity set 
indexed by mention s retrieved from Knowledge 
Repository.  In Step 8, we use the longest com-
mon subsequence algorithm to measure the simi-
larity between strings s and the title of the entity 
page with highest rank. More details about long-
est common subsequence algorithm can be 
found in Cormen et al (2001). 
1292
Algorithm 1 Candidate Set Generation 
Input: mention s;       
1: if RefE(s) is empty
2:        s??Wikipedia?did you 
           mean?Suggestion 
3:        If s? is not NULL  
4:             s ? s?
5:        else
6:            EntityPageList ? WikipediaSear
               chEngine(s) 
7:            EntityPage?FirstPage of EntityPageL 
               ist 
8:            Sim=Similarity(s,EntityPage.title)
9:            if Sim > Threshold 
10:   s? EntityPage.title
11:          end if 
12: end if 
13: end if 
Output: RefE(s);
3.2 Entity Disambiguation 
The disambiguation component is to link the 
mention in query with the entity it refers to in 
candidate set. If the entity to which the mention 
refers is a new entity which is not present in KB, 
nil will be returned. In this Section, we will de-
scribe the method for automatic data creation, 
domain adaptation from Wikipedia data, and our 
supervised learning approach as well. 
3.2.1 Automatic Data Creation  
The basic idea is to take a document with an un-
ambiguous reference to an entity E1 and replac-
ing it with a phrase which may refer to E1, E2 or 
others.
Observation: Some full names for the entities 
in the world are unambiguous. This phenomenon 
also appears in the given document collection of 
entity linking. The mention ?Abbott Laborato-
ries? appearing at multiple locations in the doc-
ument collection refers to the same entity ?a
pharmaceuticals health care company? in KB.
From this observation, our method takes into 
account the mentions in the Knowledge Reposi-
tory associated with only one entity and we treat 
these mentions as unambiguous name. Let us 
take Abbott Laboratories-{E0272065} in the 
Knowledge Repository as an example. We first 
use an index and search tool to find the docu-
ments with unambiguous mentions. Such as, the 
mention ?Abbott Laboratories? occurs in docu-
ment LDC2009T13 and LDC2007T07 in the 
document collection. The chosen text indexing 
and searching tool is the well-known Apache 
Lucene information retrieval open-source li-
brary3.
Next, to validate the consistency of NE type 
between entities in KB and in document,   we 
run the retrieved documents through a Named 
Entity Recognizer, to tag the named entities in 
the documents. Then we link the document to 
the entity in KB if the document contains a 
named entity whose name exactly matches with 
the unambiguous mention and type (i.e. Person, 
Organization and Geo-Political Entity) exactly 
matches with the type of entity in KB. In this 
example, after Named Entity Recognition, ?Ab-
bott Laboratories? in document LDC2009T13 is
tagged as an Organization which is consistent 
with the entity type of E0272065 in KB. We link 
the ?Abbott Laboratories? occurring in 
LDC2009T13 with entity E0272065.  
Finally, we replace the mention in the selected 
documents with the ambiguous synonyms. For 
example, we replace the mention ?Abbott La-
boratories? in document LDC2009T13 with
?Abbott? where Abbott-{E0064214, 
E0272065?} is an entry in Knowledge Reposi-
tory. ?Abbott? is ambiguous, because it is refer-
ring not only to E0272065, but also to E0064214 
in Knowledge Repository. Then, we can get two 
instances for the created data set as Figure 1, 
where one is positive and the other is negative.  
Figure 1: An instance of the data set 
However, from our studies, we realize some 
limitations on our training data. For example, as 
shown in Figure 1, the negative instance for 
E0272065 and the positive instance for 
3 http://lucene.apache.org 
(Abbott, LDC2009T13)  E0272065    +
(Abbott, LDC2009T13)  E0064214    -
          ? 
                         +   refer to  -  not refer to
1293
E0064214 are not in our created data set. 
However, those instances exist in the current 
document collection. We do not retrieve them 
since there is no unambiguous mention for 
E0064214 in the document collection.   
To reduce the effect of this problem, we pro-
pose to use the Wikipedia data as well, since 
Wikipedia data has training examples for all the 
entities in KB. Articles in Wikipedia often con-
tain mentions of entities that already have a cor-
responding article, and at least the first occur-
rence of the mentions of an entity in a Wikipedia 
article must be linked to its corresponding Wiki-
pedia article, if such an article exists. Therefore, 
if the mention is ambiguous, the hyperlink is 
disambiguating it. Next, we will describe how to 
incorporate Wikipedia data. 
Incorporating Wikipedia Data. The docu-
ment collection for entity linking is commonly 
from other domains, but not Wikipedia. To ben-
efit from Wikipedia data, we introduce a domain 
adaption approach (Daum? III, 2007) which is 
suitable for this work since we have enough 
?target? domain data. The approach is to aug-
ment the feature vectors of the instances. Denote 
by X the input space, and by Y the output space, 
in this case, X is the space of the real vectors 
???? for the instances in data set and Y= {+1,-1} 
is the label. Ds is the Wikipedia domain dataset 
and Dt is our automatically created data set. 
Suppose for simplicity that X=RF for some F > 0 
(RF is the space of F-dimensions). The aug-
mented input space will be defined by ??  =R3F.
Then, define mappings ?s, ?t : X ? ?? for map-
ping the Wikipedia and our created data set re-
spectively.  These are defined as follows:
????? ?? ????? ????? ? ?
????? ?? ??????? ???? ?
Where 0=<0,0,?,0> ?RF is the zero vector. We 
use the simple linear kernel in our experiments. 
However, the following kernelized version can 
help us to gain some insight into the method. K
denotes the dot product of two vectors. 
K(x,x?)=< ?  (x), ?  (x?)>. When the domain is 
the same, we get: ????? ??? ?? ????? ????? ?
?? ????? ????? ? ????? ??? . When they are 
from different domains, we get: ????? ??? ??
????? ????? ?? ???? ???. Putting this togeth-
er, we have: 
?? ? ???
??? ?????????????
???? ???????? ??????
This is an intuitively pleasing result. Loosely 
speaking, this means that data points from our 
created data set have twice as much influence as 
Wikipedia points when making predictions 
about test data from document collection. 
3.2.2 The Disambiguation Framework 
To disambiguate a mention in document collec-
tion, the ranking method is to rank the entities in 
candidate set based on the similarity score. In 
our work, we transform the ranking problem into 
a classification problem: deciding whether a 
mention refers to an entity on an SVM classifier.
If there are 2 or more than 2 candidate entities 
that are assigned positive label by the binary 
classifier, we will use the baseline system (ex-
plained in Section 4.2) to rank the candidates 
and the entity with the highest rank will be cho-
sen.
In the learning framework, the training or test-
ing instance is formed by (query, entity) pair.
For Wikipedia data, (query, entity) is positive if 
there is a hyperlink from the article containing 
the mention in query to the entity, otherwise 
(query, entity) is negative. Our automatically 
created data has been assigned labels in Section 
3.2.1.  Based on the training instances, a binary 
classifier is generated by using particular learn-
ing algorithm.  During disambiguation, (query,
entity) is presented to the classifier which then 
returns a class label.  
Each (query, entity) pair is represented by the 
feature vector using different features and simi-
larity metrics. We chose the following three 
classes of features as they represent a wide range 
of information - lexical features, word-category 
pair, NE type - that have been proved to be ef-
fective in previous works and tasks. We now 
discuss the three categories of features used in 
our framework in details. 
Lexical features. For Bag of Words feature in 
Web People Search, Artiles et al (2009) illu-
strated that noun phrase and n-grams longer than 
2 were not effective in comparison with token-
based features and using bi-grams gives the best 
1294
results only reaching recall 0.7. Thus, we use 
token-based features. The similarity metric we 
choose is cosine (using standard tf.idf weight-
ing). Furthermore, we also take into account the 
co-occurring NEs and represent it in the form of 
token-based features. Then, the single cosine 
similarity feature is based on Co-occurring NEs 
and Bag of Words. 
Word Category Pair. Bunescu (2007) dem-
onstrated that word-category pairs extracted 
from the document and Wikipedia article are a 
good signal for disambiguation. Thus we also 
consider word-category pairs as a feature class, 
i.e., all (w,c) where w is a word from Bag of 
Words of document and c is a category to which 
candidate entity belongs.  
NE Type. This feature is a single binary fea-
ture to guarantee that the type of entity in docu-
ment (i.e. Person, Geo-Political Entity and Or-
ganization) is consistent with the type of entity 
in KB. 
4 Experiments and Discussions 
4.1 Experimental Setup 
    In our study, we use KBP-09 knowledge base 
and document collection for entity linking. In the 
current setting of KBP-09 Data, the KB has been 
generated automatically from Wikipedia. The 
KB contains 818,741 different entities. The doc-
ument collection is mainly composed of news-
wire text from different press agencies. The col-
lection contains 1.3 million documents that span 
from 1994 to the end of 2008. The test data has 
3904 queries across three named entity types: 
Person, Geo-Political Entity and Organization. 
Each query contains a document with an ambi-
guous mention.    
Wikipedia data can be obtained easily from 
the website4 for free research use. It is available 
in the form of database dumps that are released 
periodically. In order to leverage various infor-
mation mentioned in Section 3.1 to derive name 
variations, make use of the links in Wikipedia to 
generate our training corpus and get word cate-
gory information for the disambiguation, we fur-
ther get Wikipedia data directly from the website. 
The version we used in our experiments was re-
leased on Sep. 02, 2009. The automatically 
4 http://download.wikipedia.org   
created corpus (around 10K) was used as the 
training data, and 30K training instances asso-
ciated with the entities in our corpus was derived 
from Wikipedia. 
For pre-processing, we perform sentence 
boundary detection and Chunking derived from 
Stanford parser (Klein and Manning, 2003), 
Named Entity Recognition using a SVM based 
system trained and tested on ACE 2005 with 
92.5(P) 84.3(R) 88.2(F), and coreference resolu-
tion using a SVM based coreference resolver 
trained and tested on ACE 2005 with 79.5%(P), 
66.7%(R) and 72.5%(F).  
We select SVM as the classifier used in this 
paper since SVM can represent the stat-of-the-
art machine learning algorithm. In our imple-
mentation, we use the binary SVMLight devel-
oped by Joachims (1999). The classifier is 
trained with default learning parameters. 
We adopt the measure used in KBP-09 to eva-
luate the performance of entity linking. This 
measure is micro-averaged accuracy: the number 
of correct link divided by the total number of 
queries.
4.2 Baseline Systems 
We build the baseline using the ranking ap-
proach which ranks the candidates based on si-
milarity between mention and candidate entities. 
The entity with the highest rank is chosen. Bag 
of words and co-occurring NEs are represented 
in the form of token-based feature vectors. Then 
tf.idf is employed to calculate similarity between 
feature vectors.  
To make the baseline system with token-
based features state-of-the-art, we conduct a se-
ries of experiments.  Table 1 lists the perfor-
mances of our token-based ranking systems. In 
our experiment, local tokens are text segments 
generated by a text window centered on the 
mention. We set the window size to 55, which is 
the value that was observed to give optimum 
performance for the disambiguation problem 
(Gooi and Allan, 2004). Full tokens and NE are 
all the tokens and named entities co-occurring in 
the text respectively. We notice that tokens of 
the full text as well as the co-occurring named 
entity produce the best baseline performance, 
which we use for the further experiment. 
1295
 Micro-averaged 
Accuracy 
local tokens 60.0 
local tokens + NE 60.6 
full tokens + NE 61.9 
Table 1: Results of the ranking methods 
4.3 Experiment and Result 
As discussed in Section 3.1, we exploit two 
more knowledge sources in Wikipedia: ?did you 
mean? (DYM) and ?Wikipedia search engine? 
(SE) for name variation step. We conduct some 
experiments to compare our name variation me-
thod using Algorithm 1 in Section 3.1 with the 
name variation method of Cucerzan (2007). Ta-
ble 2 shows the comparison results of different 
name variation methods for entity linking. The 
experiments results show that, in entity linking 
task, our name variation method outperforms the 
method of Cucerzan (2007) for both entity dis-
ambiguation methods. 
Name Variation 
Approaches 
Ranking
Method 
Our Disambig-
uation Method 
Cucerzan
(2007) 
60.9 82.2 
+DYM+SE 61.9 83.8 
Table 2: Entity Linking Result for two name 
variation approaches. Column 1 used the base-
line method for entity disambiguation step. Col-
umn 2 used our proposed entity disambiguation 
method.
Table 3 compares the performance of different 
methods for entity linking on the KBP-09 test 
data. Row 1 is the result for baseline system. 
Row 2 and Row 3 show the results training on 
Wikipedia data and our automatically data re-
spectively. Row 4 is the result training on both 
Wikipedia and our created data using the domain 
adaptation method mentioned in Section 3.2.1. It 
shows that our method trained on the automati-
cally generated data alone significantly outper-
forms baseline. Compared Row 3 with Row 2, 
our created data set serves better at training the 
classifier than Wikipedia data. This is due to the 
reason that Wikipedia is a different domain from 
newswire domain. By comparing Row 4 with 
Row 3, we find that by using the domain adapta-
tion method in Section 3.2.1, our method for 
entity linking can be further improved by 1.5%. 
Likely, this is because of the limitation of the 
auto-generated corpus as discussed in Section 
3.2.1. In another hand, Wikipedia can comple-
ment the missing information with the auto-
generated corpus. So combining Wikipedia data 
with our generated data can achieve better result. 
Compared with baseline system using Cucerzan 
(2007) name variation method in Table 2, in to-
tal our proposed method achieves a significant 
22.9% improvement.  
 Micro-averaged Accu-
racy
Baseline 61.9 
Wiki 79.9 
Created Data 82.3 
Wiki? Created Data 83.8 
Table 3: Micro-averaged Accuracy for Entity 
Linking   
     To test the effectiveness of our method to 
deal with new entities not present in KB and ex-
isting entities in KB respectively, we conduct 
some experiments to compare with Baseline.  
Table 4 shows the performances of entity linking 
systems for existing entities (non-NIL) in KB 
and new entity (NIL) which is not present in KB. 
We can see that the binary classifier not only 
effectively reduces the ambiguities to the exist-
ing entities in KB, but also is very useful to 
highlight the new entities to KB for the further 
population. Note that, in baseline system, all the 
new entities are found by the empty candidate 
set of name variation process, while the disam-
biguation component has no contribution.  How-
ever, our approach finds the new entities not on-
ly by the empty candidate set, but also leverag-
ing on disambiguation component which also 
contributes to the performance improvement.  
 non-NIL NIL 
Baseline 72.6  52.4  
Wiki? Created 
Data 
79.2 87.8  
Table 4: Entity Linking on Existing and New 
Entities
1296
Finally, we also compare our method with the 
top 5 systems in KBP-09. Among them, 
Siel_093 (Varma et al 2009) and NLPR_KBP1
(Han and Zhao 2009) use similarity ranking ap-
proach; Stanford_UBC2 (Agirre et al 2009),
QUANTA1 (Li et al 2009) and hltcoe1 (McNa-
mee et al 2009) use supervised approach. From 
the results shown in Figure 2, we observe that 
our method outperforms all the top 5 systems 
and the baseline system of KBP-09. Specifically, 
our method achieves better result than both simi-
larity ranking approaches. This is due to the li-
mitations of the ranking approach which have 
been discussed in Section 2. We also observe 
that our method gets a 5% improvement over 
Stanford_UBC2. This is because they collect 
their training data from Wikipedia which is a 
different domain from document collection of 
entity linking, news articles in this case; while 
our automatic data generation method can create 
a data set from the same domain as the docu-
ment collection. Our system also outperforms 
QUANTA1 and hltcoe1 because they train their 
model on a small manually created data set 
(1,615 examples), while our method can auto-
matically generate a much larger data set. 
Figure 2: A comparison with KBP09 systems 
5 Conclusion
 The purpose of this paper is to explore how 
to leverage the automatically generated large 
scale annotation for entity linking. Traditionally, 
without any training data available, the solution 
is to rank the candidates based on similarity. 
However, it is difficult for the ranking approach 
to detect a new entity that is not present in KB, 
and it is also difficult to combine different fea-
tures. In this paper, we create a large corpus for 
entity linking by an automatic method. A binary 
classifier is then trained to filter out KB entities 
that are not similar to current mentions. We fur-
ther leverage on the Wikipedia documents to 
provide other information which is not available 
in our generated corpus through a domain adap-
tion approach. Furthermore, new information 
sources for finding more variations also contri-
bute to the overall 22.9% accuracy improve-
ments on KBP-09 test data over baseline. 
References  
E. Agirre et al Stanford-UBC at TAC-KBP. In Pro-
ceedings of Test Analysis Conference 2009 (TAC 
09).
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The se-
meval-2007 web evaluation: Establishing a 
benchmark for the web people search task. In Pro-
ceeding of the Fourth International Work-shop on 
Semantic Evaluations (SemEval-2007).
J. Artiles, E. Amigo and J. Gonzalo. 2009. The role 
of named entities in Web People Search. In pro-
ceeding of the 47th Annual Meeting of the Associa-
tion for Computational Linguistics. 
R. Bunescu. 2007. Learning for information extrac-
tion from named entity recognition and disambig-
uation to relation extraction. Ph.D thesis, Universi-
ty of Texas at Austin, 2007. 
T. H. Cormen, et al 2001. Introduction To Algo-
rithms (Second Edition). The MIT Press, Page 
350-355. 
S. Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. Empirical 
Methods in Natural Language Processing, June 
28-30, 2007. 
H. Daum? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics . 
C. H. Gooi and J. Allan. 2004. Cross-document core-
ference on a large scale corpus. In proceedings of 
Human Language Technology Conference North 
American Association for Computational Linguis-
tics Annual Meeting, Boston, MA. 
X. Han and J. Zhao. NLPR_KBP in TAC 2009 KBP 
Track: A Two-Stage Method to Entity Linking. In 
Proceedings of Test Analysis Conference 2009 
(TAC 09).
0.838
0.8217
0.8033
0.7984
0.7884
0.7672
0.571
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
1297
T. Joachims. 1999. Making large-scale SVM learning 
practical. In Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press. 
D. Klein and C. D. Manning. 2003. Fast Exact Infe-
rence with a Factored Model for Natural Language 
Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press, pp. 3-10. 
F. LI et al THU QUANTA at TAC 2009 KBP and 
RTE Track. In Proceedings of Test Analysis Con-
ference 2009 (TAC 09).  
P. McNamee and H. T. Dang. 2009. Overview of the 
TAC 2009 Knowledge Base Population Track. In 
Proceedings of Test Analysis Conference 2009 
(TAC 09).  
P. McNamee et al HLTCOE Approaches to Know-
ledge Base Population at TAC 2009.  In Proceed-
ings of Test Analysis Conference 2009 (TAC 09).  
H. T. Nguyen and T. H. Cao. 2008. Named Entity 
Disambiguation on an Ontology Enriched by Wi-
kipedia. 2008 IEEE International Conference on 
Research, Innovation and Vision for the Future in 
Computing & Communication Technologies. 
B. Popov et al 2004. KIM - a Semantic Platform for 
Information Extraction and Retrieval. In Journal 
of Natural Language Engineering, Vol. 10, Issue 
3-4, Sep 2004, pp. 375-392, Cambridge University 
Press.
V. Raphael, K. Joachim and M. Wolfgang, 2007. 
Towards ontology-based disambiguation of geo-
graphical identifiers. In Proceeding of the 16th
WWW workshop on I3: Identity, Identifiers, Identi-
fications, 2007.  
V. Varma et al 2009. IIIT Hyderabad at TAC 2009. 
In Proceedings of Test Analysis Conference 2009 
(TAC 09).  
T. Zesch, C. Muller and I. Gurevych. 2008. Extrac-
tiong Lexical Semantic Knowledge from Wikipe-
dia and Wiktionary. In Proceedings of the Confe-
rence on Language Resources and Evaluation 
(LREC), 2008.  
1298
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 710?719,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Kernel Based Discourse Relation Recognition with Temporal  
Ordering Information 
 
 
WenTing Wang1                   Jian Su1                   Chew Lim Tan2 
1Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis 
Singapore 138632 
{wwang,sujian}@i2r.a-star.edu.sg 
2Department of Computer Science 
University of Singapore 
Singapore 117417 
tacl@comp.nus.edu.sg 
 
  
 
Abstract 
Syntactic knowledge is important for dis-
course relation recognition. Yet only heu-
ristically selected flat paths and 2-level 
production rules have been used to incor-
porate such information so far. In this 
paper we propose using tree kernel based 
approach to automatically mine the syn-
tactic information from the parse trees for 
discourse analysis, applying kernel func-
tion to the tree structures directly. These 
structural syntactic features, together 
with other normal flat features are incor-
porated into our composite kernel to cap-
ture diverse knowledge for simultaneous 
discourse identification and classification 
for both explicit and implicit relations. 
The experiment shows tree kernel ap-
proach is able to give statistical signifi-
cant improvements over flat syntactic 
path feature. We also illustrate that tree 
kernel approach covers more structure in-
formation than the production rules, 
which allows tree kernel to further incor-
porate information from a higher dimen-
sion space for possible better discrimina-
tion. Besides, we further propose to leve-
rage on temporal ordering information to 
constrain the interpretation of discourse 
relation, which also demonstrate statistic-
al significant improvements for discourse 
relation recognition on PDTB 2.0 for 
both explicit and implicit as well. 
1 Introduction 
Discourse relations capture the internal structure 
and logical relationship of coherent text, includ-
ing Temporal, Causal and Contrastive relations 
etc. The ability of recognizing such relations be-
tween text units including identifying and classi-
fying provides important information to other 
natural language processing systems, such as 
language generation, document summarization, 
and question answering. For example, Causal 
relation can be used to answer more sophisti-
cated, non-factoid ?Why? questions. 
Lee et al (2006) demonstrates that modeling 
discourse structure requires prior linguistic anal-
ysis on syntax. This shows the importance of 
syntactic knowledge to discourse analysis. How-
ever, most of previous work only deploys lexical 
and semantic features (Marcu and Echihabi, 
2002; Pettibone and PonBarry, 2003; Saito et al, 
2006; Ben and James, 2007; Lin et al, 2009; Pit-
ler et al, 2009) with only two exceptions (Ben 
and James, 2007; Lin et al, 2009). Nevertheless, 
Ben and James (2007) only uses flat syntactic 
path connecting connective and arguments in the 
parse tree. The hierarchical structured informa-
tion in the trees is not well preserved in their flat 
syntactic path features. Besides, such a syntactic 
feature selected and defined according to linguis-
tic intuition has its limitation, as it remains un-
clear what kinds of syntactic heuristics are effec-
tive for discourse analysis. 
The more recent work from Lin et al (2009) 
uses 2-level production rules to represent parse 
tree information. Yet it doesn?t cover all the oth-
er sub-trees structural information which can be 
also useful for the recognition. 
In this paper we propose using tree kernel 
based method to automatically mine the syntactic 
710
information from the parse trees for discourse 
analysis, applying kernel function to the parse 
tree structures directly. These structural syntactic 
features, together with other flat features are then 
incorporated into our composite kernel to capture 
diverse knowledge for simultaneous discourse 
identification and classification. The experiment    
shows that tree kernel is able to effectively in-
corporate syntactic structural information and 
produce statistical significant improvements over 
flat syntactic path feature for the recognition of 
both explicit and implicit relation in Penn Dis-
course Treebank (PDTB; Prasad et al, 2008). 
We also illustrate that tree kernel approach cov-
ers more structure information than the produc-
tion rules, which allows tree kernel to further 
work on a higher dimensional space for possible 
better discrimination. 
Besides, inspired by the linguistic study on 
tense and discourse anaphor (Webber, 1988), we 
further propose to incorporate temporal ordering 
information to constrain the interpretation of dis-
course relation, which also demonstrates statis-
tical significant improvements for discourse rela-
tion recognition on PDTB v2.0 for both explicit 
and implicit relations. 
The organization of the rest of the paper is as 
follows. We briefly introduce PDTB in Section 
2. Section 3 gives the related work on tree kernel 
approach in NLP and its difference with produc-
tion rules, and also linguistic study on tense and 
discourse anaphor. Section 4 introduces the 
frame work for discourse recognition, as well as 
the baseline feature space and the SVM classifi-
er. We present our kernel-based method in Sec-
tion 5, and the usage of temporal ordering feature 
in Section 6. Section 7 shows the experiments 
and discussions.  We conclude our works in Sec-
tion 8. 
2 Penn Discourse Tree Bank 
The Penn Discourse Treebank (PDTB) is the 
largest available annotated corpora of discourse 
relations (Prasad et al, 2008) over 2,312 Wall 
Street Journal articles. The PDTB models dis-
course relation in the predicate-argument view, 
where a discourse connective (e.g., but) is treated 
as a predicate taking two text spans as its argu-
ments. The argument that the discourse connec-
tive syntactically bounds to is called Arg2, and 
the other argument is called Arg1. 
The PDTB provides annotations for both ex-
plicit and implicit discourse relations. An explicit 
relation is triggered by an explicit connective. 
Example (1) shows an explicit Contrast relation 
signaled by the discourse connective ?but?. 
 
     (1). Arg1. Yesterday, the retailing and finan-
cial services giant reported a 16% drop in 
third-quarter earnings to $257.5 million, 
or 75 cents a share, from a restated $305 
million, or 80 cents a share, a year earlier. 
             Arg2. But the news was even worse for 
Sears's core U.S. retailing operation, the 
largest in the nation. 
 
In the PDTB, local implicit relations are also 
annotated. The annotators insert a connective 
expression that best conveys the inferred implicit 
relation between adjacent sentences within the 
same paragraph. In Example (2), the annotators 
select ?because? as the most appropriate connec-
tive to express the inferred Causal relation be-
tween the sentences. There is one special label 
AltLex pre-defined for cases where the insertion 
of an Implicit connective to express an inferred 
relation led to a redundancy in the expression of 
the relation. In Example (3), the Causal relation 
derived between sentences is alternatively lexi-
calized by some non-connective expression 
shown in square brackets, so no implicit connec-
tive is inserted. In our experiments, we treat Alt-
Lex Relations the same way as normal Implicit 
relations. 
 
     (2). Arg1. Some have raised their cash posi-
tions to record levels. 
            Arg2. Implicit = Because High cash po-
sitions help buffer a fund when the market 
falls. 
 
     (3). Arg1. Ms. Bartlett?s previous work, 
which earned her an international reputa-
tion in the non-horticultural art world, of-
ten took gardens as its nominal subject. 
             Arg2. [Mayhap this metaphorical con-
nection made] the BPC Fine Arts Com-
mittee think she had a literal green thumb. 
 
The PDTB also captures two non-implicit cas-
es: (a) Entity relation where the relation between 
adjacent sentences is based on entity coherence 
(Knott et al, 2001) as in Example (4); and (b) No 
relation where no discourse or entity-based cohe-
rence relation can be inferred between adjacent 
sentences. 
 
711
    (4).   But for South Garden, the grid was to be 
a 3-D network of masonry or hedge walls 
with real plants inside them. 
              In a Letter to the BPCA, kelly/varnell 
called this ?arbitrary and amateurish.? 
 
Each Explicit, Implicit and AltLex relation is 
annotated with a sense. The senses in PDTB are 
arranged in a three-level hierarchy. The top level 
has four tags representing four major semantic 
classes: Temporal, Contingency, Comparison 
and Expansion. For each class, a second level of 
types is defined to further refine the semantic of 
the class levels. For example, Contingency has 
two types Cause and Condition. A third level of 
subtype specifies the semantic contribution of 
each argument. In our experiments, we use only 
the top level of the sense annotations. 
3 Related Work 
Tree Kernel based Approach in NLP.  While 
the feature based approach may not be able to 
fully utilize the syntactic information in a parse 
tree, an alternative to the feature-based methods, 
tree kernel methods (Haussler, 1999) have been 
proposed to implicitly explore features in a high 
dimensional space by employing a kernel func-
tion to calculate the similarity between two ob-
jects directly. In particular, the kernel methods 
could be very effective at reducing the burden of 
feature engineering for structured objects in NLP 
research (Culotta and Sorensen, 2004). This is 
because a kernel can measure the similarity be-
tween two discrete structured objects by directly 
using the original representation of the objects 
instead of explicitly enumerating their features. 
Indeed, using kernel methods to mine structur-
al knowledge has shown success in some NLP 
applications like parsing (Collins and Duffy, 
2001; Moschitti, 2004) and relation extraction 
(Zelenko et al, 2003; Zhang et al, 2006). How-
ever, to our knowledge, the application of such a 
technique to discourse relation recognition still 
remains unexplored. 
Lin et al (2009) has explored the 2-level pro-
duction rules for discourse analysis. However, 
Figure 1 shows that only 2-level sub-tree struc-
tures (e.g. ?? - ?? ) are covered in production 
rules. Other sub-trees beyond 2-level (e.g. ?? - ?? ) 
are only captured in the tree kernel, which allows 
tree kernel to further leverage on information 
from higher dimension space for possible better 
discrimination. Especially, when there are 
enough training data, this is similar to the study  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
on language modeling that N-gram beyond uni-
gram and bigram further improves the perfor-
mance in large corpus. 
Tense and Temporal Ordering Information.   
Linguistic studies (Webber, 1988) show that a 
tensed clause ??  provides two pieces of semantic 
information: (a) a description of an event (or sit-
uation) ?? ; and (b) a particular configuration of 
the point of event (??), the point of reference 
(??) and the point of speech (??). Both the cha-
racteristics of ??  and the configuration of ??, ?? 
and ?? are critical to interpret the relationship of 
event ??  with other events in the discourse mod-
el. Our observation on temporal ordering infor-
mation is in line with the above, which is also 
incorporated in our discourse analyzer. 
4 The Recognition Framework 
In the learning framework, a training or testing 
instance is formed by a non-overlapping 
clause(s)/sentence(s) pair. Specifically, since im-
plicit relations in PDTB are defined to be local, 
only clauses from adjacent sentences are paired 
for implicit cases. During training, for each dis-
course relation encountered, a positive instance 
is created by pairing the two arguments. Also a 
Figure 1. Different sub-tree sets for ?1 used by 
2-level production rules and convolution tree 
kernel approaches. ?? -??  and ?1  itself are cov-
ered by tree kernel, while only ?? -??  are covered 
by production rules. 
Decomposition 
C 
E 
G 
F 
H 
A 
B 
D 
(?1) A 
B C 
(??) D 
F E 
(??) 
C 
D 
(??) E 
G 
(??) 
F 
H 
(??) 
D 
E 
G 
F 
H 
(??) (??) A 
C 
D 
B 
D 
E 
G 
F 
H 
C 
(?? ) C (??) 
D 
F E 
(??) A 
C 
D 
B 
F E 
712
set of negative instances is formed by paring 
each argument with neighboring non-argument 
clauses or sentences. Based on the training in-
stances, a binary classifier is generated for each 
type using a particular learning algorithm. Dur-
ing resolution, (a) clauses within same sentence 
and sentences within three-sentence spans are 
paired to form an explicit testing instance; and 
(b) neighboring sentences within three-sentence 
spans are paired to form an implicit testing in-
stance. The instance is presented to each explicit 
or implicit relation classifier which then returns a 
class label with a confidence value indicating the 
likelihood that the candidate pair holds a particu-
lar discourse relation. The relation with the high-
est confidence value will be assigned to the pair. 
4.1 Base Features 
In our system, the base features adopted include 
lexical pair, distance and attribution etc. as listed 
in Table 1. All these base features have been 
proved effective for discourse analysis in pre-
vious work. 
 
 
 
4.2 Support Vector Machine 
In theory, any discriminative learning algorithm 
is applicable to learn the classifier for discourse 
analysis. In our study, we use Support Vector 
Machine (Vapnik, 1995) to allow the use of ker-
nels to incorporate the structure feature. 
Suppose the training set ? consists of labeled 
vectors { ?? ,?? }, where ??  is the feature vector 
of a training instance and ??  is its class label. The 
classifier learned by SVM is: 
? ? = ???   ????? ? ?? + ?
?=1
  
where ??  is the learned parameter for a feature 
vector ?? , and ? is another parameter which can 
be derived from ??  . A testing instance ? is clas-
sified as positive if ? ? > 01. 
One advantage of SVM is that we can use tree 
kernel approach to capture syntactic parse tree 
information in a particular high-dimension space. 
In the next section, we will discuss how to use 
kernel to incorporate the more complex structure 
feature. 
5 Incorporating Structural Syntactic 
Information 
A parse tree that covers both discourse argu-
ments could provide us much syntactic informa-
tion related to the pair. Both the syntactic flat 
path connecting connective and arguments and 
the 2-level production rules in the parse tree used 
in previous study can be directly described by the 
tree structure. Other syntactic knowledge that 
may be helpful for discourse resolution could 
also be implicitly represented in the tree. There-
fore, by comparing the common sub-structures 
between two trees we can find out to which level 
two trees contain similar syntactic information, 
which can be done using a convolution tree ker-
nel. 
The value returned from the tree kernel re-
flects the similarity between two instances in 
syntax. Such syntactic similarity can be further 
combined with other flat linguistic features to 
compute the overall similarity between two in-
stances through a composite kernel. And thus an 
SVM classifier can be learned and then used for 
recognition. 
5.1 Structural Syntactic Feature 
Parsing is a sentence level processing. However, 
in many cases two discourse arguments do not 
occur in the same sentence. To present their syn-
tactic properties and relations in a single tree 
structure, we construct a syntax tree for each pa-
ragraph by attaching the parsing trees of all its 
sentences to an upper paragraph node. In this 
paper, we only consider discourse relations with-
in 3 sentences, which only occur within each pa-
                                                 
1 In our task, the result of ? ?  is used as the confidence 
value of the candidate argument pair ? to hold a particular 
discourse relation. 
Feature 
Names 
 Description 
(F1)  cue phrase 
(F2) neighboring punctuation 
(F3)  position of connective if 
presents 
(F4) extents of arguments 
(F5)  relative order of  arguments 
(F6)  distance between  arguments 
(F7)  grammatical role of  arguments 
(F8)  lexical pairs 
(F9) attribution  
Table 1. Base Feature Set 
713
ragraph, thus paragraph parse trees are sufficient. 
Our 3-sentence spans cover 95% discourse rela-
tion cases in PDTB v2.0. 
Having obtained the parse tree of a paragraph, 
we shall consider how to select the appropriate 
portion of the tree as the structured feature for a 
given instance. As each instance is related to two 
arguments, the structured feature at least should 
be able to cover both of these two arguments. 
Generally, the more substructure of the tree is 
included, the more syntactic information would 
be provided, but at the same time the more noisy 
information would likely be introduced. In our 
study, we examine three structured features that 
contain different substructures of the paragraph 
parse tree: 
Min-Expansion This feature records the mi-
nimal structure covering both arguments 
and connective word in the parse tree. It 
only includes the nodes occurring in the 
shortest path connecting Arg1, Arg2 and 
connective, via the nearest commonly 
commanding node. For example, consi-
dering Example (5), Figure 2 illustrates 
the representation of the structured feature 
for this relation instance. Note that the 
two clauses underlined with dashed lines 
are attributions which are not part of the 
relation. 
 
     (5). Arg1. Suppression of the book, Judge 
Oakes observed, would operate as a prior 
restraint and thus involve the First 
Amendment. 
              Arg2. Moreover, and here Judge Oakes 
went to the heart of the question, ?Respon-
sible biographers and historians constantly 
use primary sources, letters, diaries and 
memoranda.? 
 
Simple-Expansion Min-Expansion could, to 
some degree, describe the syntactic rela-
tionships between the connective and ar-
guments. However, the syntactic proper-
ties of the argument pair might not be 
captured, because the tree structure sur-
rounding the argument is not taken into 
consideration. To incorporate such infor-
mation, Simple-Expansion not only con-
tains all the nodes in Min-Expansion, but 
also includes the first-level children of  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
       these nodes2. Figure 3 illustrates such a 
feature for Example (5). We can see that 
the nodes ?PRN? in both sentences are in-
cluded in the feature. 
Full-Expansion This feature focuses on the 
tree structure between two arguments. It 
not only includes all the nodes in Simple-
Expansion, but also the nodes (beneath 
the nearest commanding parent) that cov-
er the words between the two arguments. 
Such a feature keeps the most information 
related to the argument pair. Figure 4 
                                                 
2 We will not expand the nodes denoting the sentences other 
than where the arguments occur. 
Figure 2. Min-Expansion tree built from gol-
den standard parse tree for the explicit dis-
course relation in Example (5). Note that to 
distinguish from other words, we explicitly 
mark up in the structured feature the arguments 
and connective, by appending a string tag 
?Arg1?, ?Arg2? and ?Connective? respective-
ly. 
Figure 3. Simple-Expansion tree for the expli-
cit discourse relation in Example (5).  
714
shows the structure for feature Full-
Expansion of Example (5). As illustrated, 
different from in Simple-Expansion, each 
sub-tree of ?PRN? in each sentence is ful-
ly expanded and all its children nodes are 
included in Full-Expansion. 
 
 
 
 
 
 
 
 
 
 
 
 
 
5.2 Convolution Parse Tree Kernel 
Given the parse tree defined above, we use the 
same convolution tree kernel as described in 
(Collins and Duffy, 2002) and (Moschitti, 2004). 
In general, we can represent a parse tree ? by a 
vector of integer counts of each sub-tree type 
(regardless of its ancestors):  
? ? = (#?? ???????? ?? ???? 1,? , # ??  
     ???????? ?????? ?,? , # ?? ???????? ??   
     ???? ?). 
This results in a very high dimensionality 
since the number of different sub-trees is expo-
nential in its size. Thus, it is computational in-
feasible to directly use the feature vector ?(?). 
To solve the computational issue, a tree kernel 
function is introduced to calculate the dot prod-
uct between the above high dimensional vectors 
efficiently. 
Given two tree segments ?1  and ?2 , the tree 
kernel function is defined:  
   ? ?1 ,?2 = < ? ?1 ,? ?2 > 
                   =  ? ?1  ? ,? ?2 [?]?  
                   =    ?? ?1 ? ??(?2)??2??2?1??1  
where  ?1and ?2 are the sets of all nodes in trees 
?1and ?2, respectively; and ??(?) is the indicator 
function that is 1 iff a subtree of type ?  occurs 
with root at node ? or zero otherwise. (Collins 
and Duffy, 2002) shows that ?(?1 ,?2) is an in-
stance of convolution kernels over tree struc-
tures, and can be computed in ?( ?1 ,  ?2 ) by 
the following recursive definitions: 
            ? ?1 ,?2 =  ?? ?1 ? ??(?2)?                                                                                                   
(1) ? ?1 ,?2 = 0  if ?1  and ?2  do not have the 
same syntactic tag or their children are different; 
(2) else if both ?1 and  ?2 are pre-terminals (i.e. 
POS tags), ? ?1 ,?2 = 1 ? ?; 
(3)  else, ? ?1 ,?2 = 
              ? (1 + ?(??(
?? (?1)
?=1 ?1 , ?), ??(?2 , ?))),                                 
where ??(?1) is the number of the children of 
?1 , ??(?, ?)  is the ?
??  child of node ?  and ? 
(0 < ? < 1) is the decay factor in order to make 
the kernel value less variable with respect to the 
sub-tree sizes. In addition, the recursive rule (3) 
holds because given two nodes with the same 
children, one can construct common sub-trees 
using these children and common sub-trees of 
further offspring. 
    The parse tree kernel counts the number of 
common sub-trees as the syntactic similarity 
measure between two instances. The time com-
plexity for computing this kernel is ?( ?1 ?
 ?2 ). 
5.3 Composite Tree Kernel 
Besides the above convolution parse tree kernel 
? ????  ?1 , ?2 = ?(?1 ,?2) defined to capture the 
syntactic information between two instances ?1 
and ?2, we also use another kernel ? ????  to cap-
ture other flat features, such as base features (de-
scribed in Table 1) and temporal ordering infor-
mation (described in Section 6). In our study, the 
composite kernel is defined in the following 
way: 
? 1 ?1 , ?2 = ? ? ? ????  ?1 , ?2 + 
                                    1 ? ? ? ? ????  ?1 , ?2 . 
Here, ? (?,?) can be normalized by ?  ?, ? =
? ?, ?  ? ?, ? ? ? ?, ?   and ? is the coeffi-
cient. 
6 Using Temporal Ordering Informa-
tion 
In our discourse analyzer, we also add in tem-
poral information to be used as features to pre-
dict discourse relations. This is because both our 
observations and some linguistic studies (Web-
ber, 1988) show that temporal ordering informa-
tion including tense, aspectual and event orders 
between two arguments may constrain the dis-
course relation type. For example, the connective 
Figure 4. Full-Expansion tree for the explicit 
discourse relation in Example (5).  
715
word is the same in both Example (6) and (7), 
but the tense shift from progressive form in 
clause 6.a to simple past form in clause 6.b, indi-
cating that the twisting occurred during the state 
of running the marathon, usually signals a tem-
poral discourse relation; while in Example (7), 
both clauses are in past tense and it is marked as 
a Causal relation. 
 
     (6). a. Yesterday Holly was running a mara-
thon  
            b. when she twisted her ankle. 
 
      (7). a. Use of dispersants was approved 
            b. when a test on the third day showed  
some positive results. 
 
Inspired by the linguistic model from Webber 
(1988) as described in Section 3, we explore the 
temporal order of events in two adjacent sen-
tences for discourse relation interpretation. Here 
event is represented by the head of verb, and the 
temporal order refers to the logical occurrence 
(i.e. before/at/after) between events. For in-
stance, the event ordering in Example (8) can be 
interpreted as:  
     ????? ?????? ??????? ?????(????) . 
 
     8.  a.  John went to the hospital.  
          b. He had broken his ankle on a patch of 
ice. 
 
We notice that the feasible temporal order of 
events differs for different discourse relations. 
For example, in causal relations, cause event 
usually happens before effect event, i.e.           
     ????? ????? ??????? ?????(??????). 
So it is possible to infer a causal relation in 
Example (8) if and only if 8.b is taken to be the 
cause event and 8.a is taken to be the effect 
event. That is, 8.b is taken as happening prior to 
his going into hospital. 
In our experiments, we use the TARSQI3  sys-
tem to identify event, analyze tense and aspectual 
information, and label the temporal order of 
events. Then the tense and temporal ordering 
information is extracted as features for discourse 
relation recognition. 
 
                                                 
3 http://www.isi.edu/tarsqi/ 
7 Experiments and Results 
In this section we provide the results of a set of 
experiments focused on the task of simultaneous 
discourse identification and classification. 
7.1 Experimental Settings 
We experiment on PDTB v2.0 corpus. Besides 
four top-level discourse relations, we also con-
sider Entity and No relations described in Section 
2. We directly use the golden standard parse 
trees in Penn TreeBank. We employ an SVM 
coreference resolver trained and tested on ACE 
2005 with 79.5% Precision, 66.7% Recall and 
72.5% F1 to label coreference mentions of the 
same named entity in an article. For learning, we 
use the binary SVMLight developed by (Joa-
chims, 1998) and Tree Kernel Toolkits devel-
oped by (Moschitti, 2004). All classifiers are 
trained with default learning parameters. 
The performance is evaluated using Accuracy 
which is calculated as follow: 
???????? =
????????????+ ????????????
???
 
Sections 2-22 are used for training and Sec-
tions 23-24 for testing. In this paper, we only 
consider any non-overlapping clauses/sentences 
pair in 3-sentence spans. For training, there were 
14812, 12843 and 4410 instances for Explicit, 
Implicit and Entity+No relations respectively; 
while for testing, the number was 1489, 1167 and 
380. 
7.2 System with Structural Kernel 
Table 2 lists the performance of simultaneous 
identification and classification on level-1 dis-
course senses. In the first row, only base features 
described in Section 4 are used. In the second 
row, we test Ben and James (2007)?s algorithm 
which uses heuristically defined syntactic paths 
and acts as a good baseline to compare with our 
learned-based approach using the structured in-
formation. The last three rows of Table 2 reports 
the results combining base features with three 
syntactic structured features (i.e. Min-Expansion, 
Simple-Expansion and Full-Expansion) de-
scribed in Section 5. 
We can see that all our tree kernels outperform 
the manually constructed flat path feature in all 
three groups including Explicit only, Implicit 
only and All relations, with the accuracy increas-
ing by 1.8%, 6.7% and 3.1% respectively. Espe-
cially, it shows that structural syntactic informa-
tion is more helpful for Implicit cases which is 
generally much harder than Explicit cases. We  
716
 
 
 
 
conduct chi square statistical significance test on 
All relations between flat path approach and 
Simple-Expansion approach, which shows the 
performance improvements are statistical signifi-
cant (? < 0.05) through incorporating tree ker-
nel. This proves that structural syntactic informa-
tion has good predication power for discourse 
analysis in both explicit and implicit relations. 
We also observe that among the three syntactic 
structured features, Min-Expansion and Simple-
Expansion achieve similar performances which 
are better than the result for Full-Expansion. This 
may be due to that most significant information 
is with the arguments and the shortest path con-
necting connectives and arguments. However, 
Full-Expansion that includes more information 
in other branches may introduce too many details 
which are rather tangential to discourse recogni-
tion. Our subsequent reports will focus on Sim-
ple-Expansion, unless otherwise specified. 
As described in Section 5, to compute the 
structural information, parse trees for different 
sentences are connected to form a large tree for a 
paragraph. It would be interesting to find how 
the structured information works for discourse 
relations whose arguments reside in different 
sentences. For this purpose, we test the accuracy 
for discourse relations with the two arguments 
occurring in the same sentence, one-sentence 
apart, and two-sentence apart. Table 3 compares 
the learning systems with/without the structured 
feature present. From the table, for all three cas-
es, the accuracies drop with the increase of the 
distances between the two arguments. However, 
adding the structured information would bring 
consistent improvement against the baselines 
regardless of the number of sentence distance. 
This observation suggests that the structured syn-
tactic information is more helpful for inter-
sentential discourse analysis.  
We also concern about how the structured in-
formation works for identification and classifica-
tion respectively. Table 4 lists the results for the 
two sub-tasks. As shown, with the structured in-
formation incorporated, the system (Base + Tree 
Kernel) can boost the performance of the two 
baselines (Base Features in the first row andBase 
+ Manually selected paths in the second row), for 
both identification and classification respective-
ly. We also observe that the structural syntactic 
information is more helpful for classification task 
which is generally harder than identification. 
This is in line with the intuition that classifica-
tion is generally a much harder task. We find that 
due to the weak modeling of Entity relations, 
many Entity relations which are non-discourse 
relation instances are mis-identified as implicit 
Expansion relations. Nevertheless, it clearly di-
rects our future work. 
 
 
 
 
 
 
 
 
 
 
 
7.3 System with Temporal Ordering Infor-
mation 
To examine the effectiveness of our temporal 
ordering information, we perform experiments 
Features 
 
Accuracy 
Explicit Implicit All 
Base Features 67.1 29 48.6 
Base + Manually 
selected flat path 
features 
70.3 32 52.6 
Base + Tree kernel 
(Min-Expansion) 
71.9 38.6 55.6 
Base + Tree kernel 
(Simple-Expansion) 
72.1 38.7 55.7 
Base + Tree kernel 
(Full-Expansion) 
71.8 38.4 55.4 
Sentence Dis-
tance 
0 
(959) 
1 
(1746) 
2 
(331) 
Base Features 52 49.2 35.5 
Base + Manually 
selected flat path 
features 
56.7 52 43.8 
Base + Tree 
Kernel 
58.3 55.6 49.7 
Tasks Identifica-
tion 
Classifica-
tion 
Base Features 58.6 50.5 
Base + Manually 
selected flat path 
features 
59.7 52.6 
Base + Tree 
Kernel 
63.3 59.3 
Table 3. Results of the syntactic structured kernel 
for discourse relations recognition with argu-
ments in different sentences apart. 
Table 4. Results of the syntactic structured ker-
nel for simultaneous discourse identification and 
classification subtasks. 
Table 2. Results of the syntactic structured ker-
nels on level-1 discourse relation recognition. 
717
on simultaneous identification and classification 
of level-1 discourse relations to compare with 
using only base feature set as baseline. The re-
sults are shown in Table 5.  We observe that the 
use of temporal ordering information increases 
the accuracy by 3%, 3.6% and 3.2% for Explicit, 
Implicit and All groups respectively. We conduct 
chi square statistical significant test on All rela-
tions, which shows the performance improve-
ment is statistical significant (? < 0.05). It indi-
cates that temporal ordering information can 
constrain the discourse relation types inferred 
within a clause(s)/sentence(s) pair for both expli-
cit and implicit relations. 
 
 
 
 
We observe that although temporal ordering 
information is useful in both explicit and implicit 
relation recognition, the contributions of the spe-
cific information are quite different for the two 
cases. In our experiments, we use tense and as-
pectual information for explicit relations, while 
event ordering information is used for implicit 
relations. The reason is explicit connective itself 
provides a strong hint for explicit relation, so 
tense and aspectual analysis which yields a relia-
ble result can provide additional constraints, thus 
can help explicit relation recognition. However, 
event ordering which would inevitably involve 
more noises will adversely affect the explicit re-
lation recognition performance. On the other 
hand, for implicit relations with no explicit con-
nective words, tense and aspectual information 
alone is not enough for discourse analysis. Event 
ordering can provide more necessary information 
to further constrain the inferred relations. 
7.4 Overall Results 
We also evaluate our model which combines 
base features, tree kernel and tense/temporal or-
dering information together on Explicit, Implicit 
and All Relations respectively. The overall re-
sults are shown in Table 6. 
 
 
 
 
 
 
 
 
 
8 Conclusions and Future Works 
The purpose of this paper is to explore how to 
make use of the structural syntactic knowledge to 
do discourse relation recognition. In previous 
work, syntactic information from parse trees is 
represented as a set of heuristically selected flat 
paths or 2-level production rules. However, the 
features defined this way may not necessarily 
capture all useful syntactic information provided 
by the parse trees for discourse analysis. In the 
paper, we propose a kernel-based method to in-
corporate the structural information embedded in 
parse trees. Specifically, we directly utilize the 
syntactic parse tree as a structure feature, and 
then apply kernels to such a feature, together 
with other normal features. The experimental 
results on PDTB v2.0 show that our kernel-based 
approach is able to give statistical significant 
improvement over flat syntactic path method. In 
addition, we also propose to incorporate tempor-
al ordering information to constrain the interpre-
tation of discourse relations, which also demon-
strate statistical significant improvements for 
discourse relation recognition, both explicit and 
implicit. 
In future, we plan to model Entity relations 
which constitute 24% of Implicit+Entity+No re-
lation cases, thus to improve the accuracy of re-
lation detection. 
Reference 
Ben W. and James P. 2007. Automatically Identifying 
the Arguments of Discourse Connectives. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pages 
92-101.  
Culotta A. and Sorensen J. 2004. Dependency Tree 
Kernel for Relation Extraction. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL 2004), pages 423-
429.  
Collins M. and Duffy N. 2001. New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Dis-
Features 
 
Accuracy 
Explicit Implicit All 
Base Features 67.1 29 48.6 
Base + Tem-
poral Ordering 
Information 
70.1 32.6 51.8 
Relations Accuracy 
Explicit 74.2 
Implicit 40.0 
All 57.3 
Table 5. Results of tense and temporal order 
information on level-1 discourse relations. 
Table 6. Overall results for combined model 
(Base  + Tree Kernel + Tense/Temporal). 
718
crete Structures and the Voted Perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2002), 
pages 263-270. 
Collins M. and Duffy N. 2002. Convolution Kernels 
for Natural Language. NIPS-2001. 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz. 
Joachims T.  1999. Making Large-scale SVM Learn-
ing Practical. In Advances in Kernel Methods ? 
Support Vector Learning. MIT Press. 
Knott, A., Oberlander, J., O?Donnel, M., and Mellish, 
C. 2001. Beyond elaboration: the interaction of re-
lations and focus in coherent text. In T. Sanders, J. 
Schilperoord, and W. Spooren, editors, Text Re-
presentation: Linguistic and Psycholinguistics As-
pects, pages 181-196. Benjamins, Amsterdam. 
Lee A., Prasad R., Joshi A., Dinesh N. and Webber  
B. 2006. Complexity of dependencies in discourse: 
are dependencies in discourse more complex than 
in syntax? In Proceedings of the 5th International 
Workshop on Treebanks and Linguistic Theories. 
Prague, Czech Republic, December. 
Lin Z., Kan M. and Ng H. 2009. Recognizing Implicit 
Discourse Relations in the Penn Discourse Tree-
bank. In Proceedings of the 2009 Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP 2009), Singapore, August. 
Marcu D. and Echihabi A. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of the 40th Annual Meeting of ACL, 
pages 368-375. 
Moschitti A. 2004. A Study on Convolution Kernels 
for Shallow Semantic Parsing. In Proceedings of 
the 42th Annual Meeting of the Association for 
Computational Linguistics (ACL 2004), pages 335-
342. 
Pettibone J. and Pon-Barry H. 2003. A Maximum En-
tropy Approach to Recognizing Discourse Rela-
tions in Spoken Language. Working Paper. The 
Stanford Natural Language Processing Group, June 
6. 
Pitler E., Louis A. and Nenkova A. 2009. Automatic 
Sense Predication for Implicit Discourse Relations 
in Text. In Proceedings of the Joint Conference of 
the 47th Annual Meeting of the Association for 
Computational Linguistics and the 4th International 
Joint Conference on Natural Language Processing 
of the Asian Federation of Natural Language 
Processing (ACL-IJCNLP 2009). 
Prasad R., Dinesh N., Lee A., Miltsakaki E., Robaldo 
L., Joshi A. and Webber B. 2008. The Penn Dis-
course TreeBank 2.0. In Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC 2008). 
Saito M., Yamamoto K. and Sekine S. 2006. Using 
phrasal patterns to identify discourse relations. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics (HLT-
NAACL 2006), pages 133?136, New York, USA. 
Vapnik V.  1995. The Nature of Statistical Learning 
Theory. Springer-Verlag, New York. 
Webber Bonnie. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14:61?73. 
Zelenko D., Aone C. and Richardella A. 2003.  Ker-
nel Methods for Relation Extraction.  Journal of 
Machine Learning Research, 3(6):1083-1106. 
Zhang M., Zhang J. and Su J. Exploring Syntactic 
Features for Relation Extraction using a Convolu-
tion Tree Kernel. In Proceedings of the Human 
Language Technology conference - North Ameri-
can chapter of the Association for Computational 
Linguistics annual meeting (HLT-NAACL 2006), 
New York, USA. 
719
