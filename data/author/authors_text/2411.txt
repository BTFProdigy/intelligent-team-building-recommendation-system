Unit Completion for a Computer-aided Translation 
System 
Ph i l ippe  Lang la i s ,  George  Foster  and  Guy  Lapa lme 
RAL I  / D IRO 
Universit6 de Montrea l  
C.P. 6128, succursale Centre-vi l le 
Montra l  (Qubec) ,  Canada,  H3C 3J7 
{ f elipe,f oster, lapalme }@iro. umontreal, ca 
Typing 
Abst ract  
This work is in the context of TRANSTYPE, a sys- 
tem that observes its user as he or she types a trans- 
lation and repeatedly suggests completions for the 
text already entered. The user may either accept, 
modify, or ignore these suggestions. We describe the 
design, implementation, and performance of a pro- 
totype which suggests completions of units of texts 
that are longer than one word. 
1 I n t roduct ion  
TRANSTYPE is part of a project set up to explore 
an appealing solution to Interactive Machine Trans- 
lation (IMT). In constrast to classical IMT systems, 
where the user's role consists mainly of assisting the 
computer to analyse the source text (by answering 
questions about word sense, ellipses, phrasal attach- 
ments, etc), in TRANSTYPE the interaction is direct- 
ly concerned with establishing the target ext. 
Our interactive translation system works as fol- 
lows: a translator selects a sentence and begins typ- 
ing its translation. After each character typed by 
the translator, the system displays a proposed com- 
pletion, which may either be accepted using a spe- 
cial key or rejected by continuing to type. Thus 
the translator remains in control of the translation 
process and the machine must continually adapt it- 
s suggestions in response to his or her input. We 
are currently undertaking a study to measure the 
extent o which our word-completion prototype can 
improve translator productivity. The conclusions of 
this study will be presented elsewhere. 
The first version of TrtANSTYPE (Foster et al, 
1997) only proposed completions for the current 
word. This paper deals with predictions which ex- 
tend to the next several words in the text. The po- 
tential gain from multiple-word predictions can be 
appreciated in the one-sentence translation task re- 
ported in table 1, where a hypothetical user saves 
over 60% of the keystrokes needed to produce a 
translation i a word completion scenario, and about 
85% in a "unit" completion scenario. 
In all the figures that follow, we use different fonts 
to differentiate he various input and output: italics 
are used for the source text, sans-serif for characters 
typed by the user and typewr i te r - l i ke  for charac- 
ters completed by the system. 
The first few lines of the table 1 give an idea of 
how TransType functions. Let us assume the unit s- 
cenario (see column 2 of the table) and suppose that 
the user wants to produce the sentence "Ce projet 
de loi est examin~ ~ la chambre des communes" as a 
translation for the source sentence "This bill is ex- 
amined in the house of commons". The first hypoth- 
esis that the system produces before the user enters 
a character is lo i  (law). As this is not a good guess 
from TRANSTYPE the user types the first character 
(c) of the words he or she wants as a translation. 
Taking this new input into account, TRANSTYPE 
then modifies its proposal so that it is compatible 
whith what the translator has typed. It suggests 
the desired sequence ce projet de Ioi, which the user 
can simply validate by typing a dedicated key. Con- 
tinuing in this way, the user and TRANSTYPE alter- 
nately contribute to the final translation. A screen 
copy of this prototype is provided in figure 1. 
2 The  Core  Eng ine  
The core of TRANSTYPE is a completion engine 
which comprises two main parts: an evaluator which 
assigns probabilistic scores to completion hypotheses 
and a generator which uses the evaluation function 
to select he best candidate for completion. 
2.1 The  Eva luator  
The evaluator is a function p(t\[t', s) which assigns to 
each target-text unit t an estimate of its probability 
given a source text s and the tokens t' which precede 
t in the current ranslation of s. 1 Our approach to 
modeling this distribution is based to a large extent 
on that of the IBM group (Brown et al, 1993), but 
it differs in one significant aspect: whereas the IB- 
M model involves a "noisy channel" decomposition, 
we use a linear combination of separate prediction- 
s from a language model p(tlt ~) and a translation 
model p(tls ). Although the noisy channel technique 
1We assume the existence of a determinist ic  procedure for 
tokenizing the target text. 
135 
This bill is examined in the house of commons 
word-completion task unit-completion task 
ce 
projet 
de 
Ioi 
est 
examin~ 
chambre 
des 
communes 
preL completions 
ce+ / lo i  ? C/' 
p+ /es t ?  p / ro je t  
d+ / t rbs  ? d/e 
I+ / t=~s ? I /o i  
e+ /de ? e / s t  
e+ /en ? e/xamin6 
~+ /par ? ~/ 1~ 
+ /chambre 
de+ /co,~unes ? d/e 
+ /communes 
? de/s 
pref. completions 
c-l- /loJ. ? c/e pro je t  de 1oi 
e+ /de ? e / s t  
ex+ /~ la  chambre des communes. 
+ /b l a  chambre des con~unes 
e/n ? ex /min~ 
Table 1: A one-sentence s ssion illustrating the word- and unit-completion tasks. The first column indicates 
the target words the user is expected to produce. The next two columns indicate respectively the prefixes 
typed by the user and the completions proposed by the system in a word-completion task. The last two 
columns provide the same information for the unit-completion task. The total number of keystrokes for 
both tasks is reported in the last line. + indicates the acceptance key typed by the user. A completion is
denoted by a/13 where a is the typed prefix and 13 the completed part. Completions for different prefixes 
are separated by ?. 
is powerful, it has the disadvantage that p(slt' , t) is 
more expensive to compute than p(tls ) when using 
IBM-style translation models. Since speed is cru- 
cial for our application, we chose to forego the noisy 
channel approach in the work described here. Our 
linear combination model is described as follows: 
pCtlt',s) = pCtlt') a(t ' ,s)  + pCtls) \[1 - exit',s)\] (1) 
? ~ ? ? v J 
language translation 
where a(t', s) E \[0, 1\] are context-dependent inter- 
polation coefficients. For example, the translation 
model could have a higher weight at the start of a 
sentence but the contribution of the language mod- 
el might become more important in the middle or 
the end of the sentence? A study of the weightings 
for these two models is described elsewhere? In the 
work described here we did not use the contribution 
of the language model (that is, a(t' ,  s) = O, V t', s). 
Techniques for weakening the independence as- 
sumptions made by the IBM models 1 and 2 have 
been proposed in recent work (Brown et al, 1993; 
Berger et al, 1996; Och and Weber, 98; Wang and 
Waibel, 98; Wu and Wong, 98). These studies report 
improvements on some specific tasks (task-oriented 
limited vocabulary) which by nature are very differ- 
ent from the task TRANSTYPE is devoted to. Fur- 
thermore, the underlying decoding strategies are too 
time consuming for our application? We therefore 
use a translation model based on the simple linear in- 
terpolation given in equation 2 which combines pre- 
dictions of two translation models - -  Ms and M~ - -  
both based on IBM-like model 2(Brown et al, 1993). 
Ms was trained on single words and Mu, described 
in section 3, was trained on both words and units. 
- -  _ (2 )  
word unit 
where Ps and Pu stand for the probabilities given re- 
spectively by Ms and M~. G(s) represents he new 
sequence of tokens obtained after grouping the to- 
kens of s into units. The grouping operator G is 
illustrated in table 2 and is described in section 3. 
2.2  The  Generator  
The task of the generator is to identify units that 
match the current prefix typed by the user, and pick 
the best candidate according to the evaluator. Due 
to time considerations, the generator introduces a
division of the target vocabulary into two parts: a 
small active component whose contents are always 
searched for a match to the current prefix, and a 
much larger passive part over (380,000 word form- 
s) which comes into play only when no candidates 
are found in the active vocabulary. The active part 
is computed ynamically when a new sentence is s- 
elected by the translator. It is composed of a few 
entities (tokens and units) that are likely to appear 
in the translation. It is a union of the best can- 
didates provided by each model Ms and M~ over 
the set of all possible target tokens (resp. units) 
that have a non-null translation probability of being 
translated by any of the current source tokens (resp. 
units). Table 2 shows the 10 most likely tokens and 
units in the active vocabulary for an example source 
sentence. 
136 
that.  is ? what .  the . p r ime,  minister . said 
? and .  i ? have.  outlined? what .  has .  
happened . since? then . .  
c' - est. ce -que ,  le- premier - ministre, a- 
d i t . , .e t . j ' ,  ai. r4sum4- ce. qui .s ' -  est- 
produit - depuis ? . 
g(s) that is what ? the prime minister said ? , and i 
? have . outlined ? what has happened ? since 
then ? . 
As 
A~ 
? ? ? es t  ? ce  ? m in i s t re  ? que .  e t  ? a ? p remier  
l i e  
ce  qu i  s' es t  p rodu i t  ? e t  je  - c '  es t  ce  que .  vo i l~  
ce  que  ? qu '  es t  - c '  es t  ? ,  e t  ? le p remier  min is t re  
d i sa i t  
Table 2: Role of the generator for a sample pair of 
sentences (t is the translation of s in our corpus). 
G(s) is the sequence of source tokens recasted by 
the grouping operator G. A8 indicates the 10 best 
tokens according to the word model, Au the 10 best 
units according to the unit model. 
3 Mode l ing  Un i t  Assoc ia t ions  
Automatically identifying which source words or 
groups of words will give rise to which target words 
or groups of words is a fundamental problem which 
remains open. In this work, we decided to proceed 
in two steps: a) monolingually identifying roups of 
words that would be better handled as units in a giv- 
en context, and b) mapping the resulting source and 
target units. To train our unit models, we used a 
segment of the Hansard corpus consisting of 15,377 
pairs of sentences, totaling 278,127 english token- 
s (13,543 forms) and 292,865 french tokens (16,399 
forms). 
3.1 F inding Monol ingual  Uni ts  
Finding relevant units in a text has been explored in 
many areas of natural anguage processing. Our ap- 
proach relies on distributional and frequency statis- 
tics computed on each sequence of words found in a 
training corpus. For sake of efficiency, we used the 
suffix array technique to get a compact representa- 
tion of our training corpus. This method allows the 
efficient retrieval of arbitrary length n-grams (Nagao 
and Mori, 94; Haruno et al, 96; Ikehara et al, 96; 
Shimohata et al, 1997; Russell, 1998). 
The literature abounds in measures that can help 
to decide whether words that co-occur are linguisti- 
cally significant or not. In this work, the strength of 
association of a sequence of words w\[ = w l , . . . ,  wn 
is computed by two measures: a likelihood-based one 
p(w'~) (where g is the likelihood ratio given in (Dun- 
ning, 93)) and an entropy-based one e(w'~) (Shimo- 
hata et al, 1997). Letting T stand for the training 
text and m a token: 
p(w~) = argming(w~, uS1  ) (3) 
ie\]l,n\[ 
e(w'~) = 0.5x  +k 
~rnlw,~meT h ( Ireq(w'~ m) k Ir~q(wT) \] 
Intuitively, the first measurement accounts for the 
fact that parts of a sequence of words that should 
be considered as a whole should not appear often by 
themselves. The second one reflects the fact that a 
salient unit should appear in various contexts (i.e. 
should have a high entropy score). 
We implemented a cascade filtering strategy based 
on the likelihood score p, the frequency f ,  the length 
l and the entropy value e of the sequences. A 
first filter (.~"1 (lmin, fmin, Pmin, emin)) removes any 
sequence s for which l (s) < lmin or p(s) < Pmin 
or e(s) < e,nin or f ( s )  < fmin.  A second filter 
(~'2) removes sequences that are included in pre- 
ferred ones. In terms of sequence reduction, apply- 
ing ~1 (2, 2, 5.0, 0.2) on the 81,974 English sequences 
of at least two tokens een at least twice in our train- 
ing corpus, less than 50% of them (39,093) were fil- 
tered: 17,063 (21%) were removed because of their 
low entropy value, 25,818 (31%) because of their low 
likelihood value. 
3.2 Mapping 
Mapping the identified units (tokens or sequences) to 
their equivalents in the other language was achieved 
by training a new translation model (IBM 2) us- 
ing the EM algorithm as described in (Brown et al, 
1993). This required grouping the tokens in our 
training corpus into sequences, on the basis of the 
unit lexicons identified in the previous tep (we will 
refer to the results of this grouping as the sequence- 
based corpus). To deal with overlapping possibilities, 
we used a dynamic programming scheme which opti- 
mized a criterion C given by equation 4 over a set S 
of all units collected for a given language plus all sin- 
gle words. G(w~) is obtained by returning the path 
that maximized B(n) .  We investigated several C- 
criteria and we found C~--a length-based measurc 
to be the most satisfactory. Table 2 shows an output 
of the grouping function. 
Oi l  i=o  
B( i )  = argmax 
/~\[1,i\[ ,w~_les ) + B( i  - I - 1) (4) 
0 i f j<=i  
with: Cl (w~)= j - - i  + l e lse 
137 
source unit (s) 
we have 1748 
we must 720 
this bill 640 
people of canada 282 
mr. speaker : 269 
what is happening 190 
of course , 178 
is it the pleasure of the house to 14 
adopt the 
the world 
child care 
the free trade agreement 
post-secondary education 
the first time 
the canadian aviation safety board 
the next five years 
the people of china 
f(8) target units (\[a,p\]) 
\[nous,0.49\] \[avons,0.41\] \[, nous avons,0.07\] 
\[nous devons,0.61\] \[ilrant,0.19\] [nous,0.14\] 
\[ce projet de 1oi,0.35\] \[projet de loi .,0.21\] [projet de loi,0.18\] 
\[les canadiens,0.26\] \[des canadiens,0.21\] \[la population,0.07\] 
\[m. le prdsident :,0.80\] [a,0.07\] \[h la,0.06\] 
Ice qui se passe,0.21\] Ice qui se,0.16\] [et,0.15\] 
\[dvidemment ,0.26\] \[naturellement,0.08\] \[bien stir,0.08\] 
\[plait-il h la chambre d' adopter,0.49\] \[la motion ?,0.42\] [motion 
?,0.04\] 
201 \[le monde,O.46\] [du monde,O.33\] lie monde entier,O.19\] 
86 lies garderies,O.59\] \[la garde d' enfants,O.23\] \[des services de 
garde d' enfants,O.13\] 
75 \[1' accord de libre-dchange,O.96\] \[la ddcision du gatt,O.04\] 
66 \[1' euseignement postsecondaize,O.75\] \[1' dducation postsec- 
ondaire,O.15\] \[des fonds,O.06\] 
62 \[la premiere fois,l.00\] 
36 lie bureau canadien de la s~urit~ adrienne,O.55\] \[du bureau cana- 
dien de la sdcurit~ adrienne,O.31\] \[1'un,O.14\] 
26 \[au cours des cinq prochaines ann~es,O.53\] \[cinq prochaines an- 
ndes,O.27\] \[25 milliards de d ollars,O.lO\] 
17 \[le peuple chinois,0.38\] \[la population chinoise,0.25\] \[les chi- 
nois,O.13\] 
Table 3: Bilingual associations. The first column indicates a source unit, the second one its frequency in the 
training corpus. The third column reports its 3-best ranked target associations (a being a token or a unit, 
p being the translation probability). The second half of the table reports NP-associations obtained after the 
filter described in the text. 
We investigated three ways of estimating the pa- 
rameters of the unit model. In the first one, El, 
the translation parameters are estimated by apply- 
ing the EM algorithm in a straightforward fashion 
over all entities (tokens and units) present at least 
twice in the sequence-based corpus 2. The two next 
methods filter the probabilities obtained with the Ez 
method. In E2, all probabilities p(tls ) are set to 0 
whenever s is a token (not a unit), thus forcing the 
model to contain only associations between source 
units and target entities (tokens or units). In E3 
any parameter of the model that involves a token 
is removed (that is, p(tls ) = 0 if t or s is a token). 
The resulting model will thus contain only unit as- 
sociations. In both cases, the final probabilities are 
renormalized. Table 3 shows a few entries from a 
unit model (Mu) obtained after 15 iterations of the 
EM-algorithm on a sequence corpus resulting from 
the application of the length-grouping criterion (dr) 
over a lexicon of units whose likelihood score is above 
5.0. The probabilities have been obtained by appli- 
cation of the method E2. 
We found many partially correct associations 
Cover the years/au fils des, we have/nous, etc) that 
illustrate the weakness of decoupling the unit iden- 
tification from the mapping problem. In most cas- 
2The entities een only once are mapped to a special "un- 
known" word 
es however, these associations have a lower proba- 
bility than the good ones. We also found few er- 
ratic associations (the first time/e'dtait, some hon. 
members/t, etc) due to distributional rtifacts. It is 
also interesting to note that the good associations 
we found are not necessary compositional in nature 
(we must/il Iaut, people of canada/les canadiens, of 
eourse/6videmment, etc). 
3.3 F i l ter ing  
One way to increase the precision of the mapping 
process is to impose some linguistic constraints on 
the sequences such as simple noun-phrase contraints 
(Ganssier, 1995; Kupiec, 1993; hua Chen and Chen, 
94; Fung, 1995; Evans and Zhai, 1996). It is also 
possible to focus on non-compositional compounds, 
a key point in bilingual applications (Su et al, 1994; 
Melamed, 1997; Lin, 99). Another interesting ap- 
proach is to restrict sequences to those that do not 
cross constituent boundary patterns (Wu, 1995; Fu- 
ruse and Iida, 96). In this study, we filtered for po- 
tential sequences that are likely to be noun phrases, 
using simple regular expressions over the associated 
part-of-speech tags. An excerpt of the association 
probabilities of a unit model trained considering on- 
ly the NP-sequences i given in table 3. Applying 
this filter (referred to as JrNp in the following) to the 
39,093 english sequences still surviving after previ- 
ous filters ~'1 and ~'2 removes 35,939 of them (92%). 
138 
model spared ok good nu u 
1 baseline - model 1 48.98 0 0 747 0 
2 basel ine - model 2 51.83 0 0 747 0 
3 E1 + ~'1(2, 2, 0, 0.2) 50.98 527 1702 5 626 
4 E1+~'1(2,2,5,0.2)  51.61 596 2149 5 658 
5 E1 + ~-~ (2, 2, 5, 0.2) + 9r2 51.72 633 2265 5 657 
6 E2 + ~'~(2,2,0,0.2) 51.39 514 1551 43 578 
7 ?2 + ~-~ (2, 2, 5, 0.2) 51.99 470 1889 46 614 
8 E2 + ~'~(2,2,5,0.2) + ~'2 52.12 493 1951 46 606 
9 E3 + ~-1(2, 2, 0, 0.2) 51.07 577 1699 43 588 
10 E2 + ~-1(2, 2, 5, 0.2) 51.47 629 2124 46 618 
11 E2+~'~(2 ,2 ,5 ,0 .2 )+~'2  51.68 665 2209 46 615 
12 ~1 -}- .~1 (2, 2, 5, 0.2) -}- .~2 -}- ~:NP 52.83 416 1302 4 564 
13 E2 + ~'1(2, 2, 5, 0.2) + ~NP 53.12 439 1031 228 425 
14 ?2 + ~'~ (2, 2, 5, 0.2) + 5r2 + ~'NP 53.16 458 1052 199 439 
15 ~3 -{- ~ : 0.4 -}- ~-1(2, 2, 5, 0.2) 4- .~NP 53.22 495 1031 228 425 
Table 4: Completion results of several translation models, spared: theoretical proportion of characters 
saved; ok: number of target units accepted by the user; good: number of target units that matched the 
expected whether they were proposed or not; nu: number of sentences for which no target unit was found 
by the translation model; u: number of sentences for which at least one helpful unit has been found by the 
model, but not necessarily proposed. 
More than half of the 3,154 remaining NP-sequences 
contain only two words. 
4 Resu l t s  
We collected completion results on a test corpus 
of 747 sentences (13,386 english tokens and 14,506 
french ones) taken from the Hansard corpus. These 
sentences have been selected randomly among sen- 
tences that have not been used for the training. 
Around 18% of the source and target words are not 
known by the translation model. 
The baseline models (line 1 and 2) are obtained 
without any unit model (i.e. /~ = 1 in equation 2). 
The first one is obtained with an IBM-like model 1 
while the second is an IBM-like model 2. We observe 
that for the pair of languages we considered, model 
2 improves the amount of saved keystrokes of almost 
3% compared to model 1. Therefore we made use of 
alignment probabilities for the other models. 
The three next blocks in table 4 show how the 
parameter estimation method affects performance. 
Training models under the C1 method gives the worst 
results. This results from the fact that the word- 
to-word probabilities trained on the sequence based 
corpus (predicted by Mu in equation 2) are less ac- 
curate than the ones learned from the token based 
corpus. The reason is simply that there are less oc- 
currences of each token, especially if many units are 
identified by the grouping operator. 
In methods C2 and C3, the unit model of equation 
2 only makes predictions pu(tls ) when s is a source u- 
nit, thus lowering the noise compared to method ?1. 
We also observe in these three blocks the influence 
of sequence filtering: the more we filter, the better 
the results. This holds true for all estimation meth- 
ods tried. In the fifth block of table 4 we observe 
the positive influence of the NP-filtering, especially 
when using the third estimation method. 
The best combination we found is reported in line 
15. It outperforms the baseline by around 1.5%. 
This model has been obtained by retaining all se- 
quences een at least two times in the training cor- 
pus for which the likelihood test value was above 5 
and the entropy score above 0.2 (5rl (2, 2, 5, 0.2)). In 
terms of the coverage of this unit model, it is in- 
teresting to note that among the 747 sentences of 
the test session, there were 228 for which the model 
did not propose any units at all. For 425 of the re- 
maining sentences, the model proposed at least one 
helpful (good or partially good) unit. The active vo- 
cabulary for these sentences contained an average of 
around 2.5 good units per sentence, of which only 
half (495) were proposed during the session. The 
fact that this model outperforms others despite it- 
s relatively poor coverage (compared to the others) 
may be explained by the fact that it also removes 
part of the noise introduced by decoupling the i- 
dentification of the salient units from the training 
procedure. Furthermore, as we mentionned earlier, 
the more we filter, the less the grouping scheeme 
presented in equation 4 remains necessary, thus re- 
ducing a possible source of noise. 
The fact that this model outperforms others, de- 
spite its relatively poor coverage, is due to the fact 
139 
E ich le r  C )pt lons  
l am p leased  to  t~ lce  ]par t  in  th i s  debate  tod  W . 
Us ing  rod  W "s techno log ies ,  i t  i s  poss ib le  fo r  a l l  C~m~dia~s  to  
reg is ter  the i r  votes  on  i s s t les  of  pub l i c  spend ing  and  pub l i c  
I )o r ro~v ing .  
II me fa l t  p la le l r  de  prendre  la paro le  au Jourd 'hu i  dana  le cadre  de  ?e  
d~bat .  
Gr~ice  & la  techno log le  moderne ,  toue  lea  Canad len= peuvent  6e  
prononcer  sur  le=;  quest ion= de  d6pen=e== et  d" e rnprunta  de  I" I~tat  . 
Not re  p 
Figure 1: Example of an i teraction i  TRANSTYPE with the source text in the top half of the screen. The 
target text is typed in the bottom half with suggestions given by the menu at the insertion point. 
that it also removes part of the noise that is intro- 
duced by dissociating the identification ofthe salient 
units from the training procedure. ~rthermore, as 
we mentioned earlier, the more we filter, the less the 
grouping scheme presented in equation 4 remains 
necessary, thus further reducing an other possible 
source of noise. 
5 Conclusion 
We have described a prototype system called 
TRANSTYPE which embodies an innovative ap- 
proach to interactive machine translation in which 
the interaction is directly concerned with establish- 
ing the target ext. We proposed and tested a mech- 
anism to enhance TRANSTYPE by having it predic- 
t sequences of words rather than just completions 
for the current word. The results show a modest 
improvement in prediction performance which will 
serve as a baseline for our future investigations. One 
obvious direction for future research is to revise our 
current strategy of decoupling the selection of units 
from their bilingual context. 
Acknowlegments 
TRANSTYPE is a project funded by the Natural Sci- 
ences and Engineering Research Council of Canada. 
We are undebted to Elliott Macklovitch and Pierre 
Isabelle for the fruitful orientations they gave to this 
work. 
References 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent J. Della Pietra. 1996. A maximum entropy 
approach to natural language processing. Compu- 
tational Linguistics, 22(1):39-71. 
Peter F. Brown, Stephen A. Della Pietra, Vincen- 
t Della J. Pietra, and Robert L. Mercer. 1993. 
The mathematics of machine trmaslation: Pa- 
rameter estimation. Computational Linguistics, 
19(2):263-312, June. 
Ted Dunning. 93. Accurate methods for the statis- 
tics of surprise and coincidence. Computational 
Linguistics, 19(1):61-74. 
David A. Evans and Chengxiang Zhai. 1996. Noun- 
phrase analysis in unrestricted text for informa- 
tion retrieval. In Proceedings of the 34th Annu- 
al Meeting of the Association for Computational 
Linguistics, pages 17-24, Santa Cruz, California. 
George Foster, Pierre Isabelle, and Pierre Plamon- 
don. 1997. Target-text Mediated Interactive Ma- 
chine Translation. Machine Translation, 12:175- 
194. 
Pascale Fung. 1995. A pattern matching method for 
finding noun and proper noun translations from 
noisy parallel corpora. In Proceedings ofthe 33rd 
Annual Meeting of the Association for Compu- 
tational Linguistics, pages 236-243, Cambridge, 
Massachusetts. 
Osamu Furuse and Hitoshi Iida. 96. Incremen- 
140 
tal translation utilizing constituent boundray pat- 
terns. In Proceedings of the 16th International 
Conference On Computational Linguistics, pages 
412-417, Copenhagen, Denmark. 
Eric Gaussier. 1995. Modles statistiques et patron- 
s morphosyntaxiques pour l'extraction de lcxiques 
bilingues. Ph.D. thesis, Universit de Paris 7, jan- 
vier. 
Masahiko Haruno, Satoru Ikehara, and Takefumi 
Yamazaki. 96. Learning bilingual collocations by 
word-level sorting. In Proceedings of the 16th In- 
ternational Conference On Computational Lin- 
guistics, pages 525-530, Copenhagen, Denmark. 
Kuang hua Chen and Hsin-Hsi Chen. 94. Extract- 
ing noun phrases from large-scale texts: A hybrid 
approach and its automatic evaluation. In Pro- 
ceedings of the 32nd Annual Meeting of the Asso- 
ciation for Computational Linguistics, pages 234- 
241, Las Cruces, New Mexico. 
Satoru Ikehara, Satoshi Shirai, and Hajine Uchino. 
96. A statistical method for extracting uinterupt- 
ed and interrupted collocations from very large 
corpora. In Proceedings of the 16th International 
Conference On Computational Linguistics, pages 
574-579, Copenhagen, Denmark. 
Julian Kupiec. 1993. An algorithm for finding noun 
phrase correspondences in bilingual corpora. In 
Proceedings of the 31st Annual Meeting of the 
Association for Computational Linguistics, pages 
17-22, Colombus, Ohio. 
Dekang Lin. 99. Automatic identification of non- 
compositional phrases. In Proceedings of the 37th 
Annual Meeting of the Association for Computa- 
tional Linguistics, pages 317-324, College Park, 
Maryland. 
I. Dan Melamed. 1997. Automatic discovery of non- 
compositional coumpounds in parallel data. In 
Proceedings of the 2nd Conference on Empirical 
Methods in Natural Language Processing, pages 
97-108, Providence, RI, August, lst-2nd. 
Makoto Nagao and Shinsuke Mori. 94. A new 
method of n-gram statistics for large number of 
n and automatic extraction of words and phrases 
from large text data of japanese. In Proceedings 
of the 16th International Conference On Com- 
putational Linguistics, volume 1, pages 611-615, 
Copenhagen, Denmark. 
Franz Josef Och and Hans Weber. 98. Improving 
statistical natural anguage translation with cate- 
gories and rules. In Proceedings of the 36th Annu- 
al Meeting of the Association for Computational 
Linguistics, pages 985-989, Montreal, Canada. 
Graham Russell. 1998. Identification of salient to- 
ken sequences. Internal report, RALI, University 
of Montreal, Canada. 
Sayori Shimohata, Toshiyuki Sugio, and Junji 
Nagata. 1997. Retrieving collocations by co- 
occurrences and word order constraints. In Pro- 
ceedings of the 35th Annual Meeting of the Asso- 
ciation for Computational Linguistics, pages 476- 
481, Madrid Spain. 
Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 
1994. A corpus-based approach to automatic om- 
pound extraction. In Proceedings of the 32nd An- 
nual Meeting of the Association for Computation- 
al Linguistics, pages 242-247, Las Cruces, New 
Mexico. 
Ye-Yi Wang and Alex Waibel. 98. Modeling with 
structures in statistical machine translation. In 
Proceedings of the 36th Annual Meeting of the 
Association for Computational Linguistics, vol- 
ume 2, pages 1357-1363, Montreal, Canada. 
Dekai Wu and Hongsing Wong. 98. Machine trans- 
lation with a stochastic grammatical channel. In 
Proceedings of the 36th Annual Meeting of the 
Association for Computational Linguistics, pages 
1408-1414, Montreal, Canada. 
Dekai Wu. 1995. Stochastic inversion transduc- 
tion grammars, with application to segmentation, 
bracketing, and alignment of parallel corpora. In 
Proceedings of the International Joint Conference 
on Artificial Intelligence, volume 2, pages 1328- 
1335, Montreal, Canada. 
141 
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, page 1,
Vancouver, October 2005.
Automatic Detection of Translation Errors: The State of the Art
Graham Russell and Ngoc Tran Nguyen
IIT?ILT, National Research Council Canada
RALI-DIRO, Universite? de Montre?al?
{russell,nguyentt}@iro.umontreal.ca
George Foster
IIT?ILT, National Research Council Canada?
george.foster@nrc-cnrc.gc.ca
1 Background
The demonstration presents TransCheck, a transla-
tion quality-assurance tool developed jointly by the
RALI group at the University of Montreal and the
Interactive Language Technologies section of the
Canadian National Research Council?s Institute for
Information Technology.
The system differs from other similar tools in
the range of error-types targeted, and the underly-
ing mechanisms employed. The demonstration il-
lustrates the operation of the system and gives the
rationale for its design and capabilities. The version
demonstrated accepts input in English and French.
2 System Overview
A modular architecture promotes flexibility (ease
of adaptation to new domains, client requirements
and language pairs) and extensibility (incorporation
of new error-detector components as they become
available).
In a transparent preprocessing stage, source and
target texts are read and aligned. The resulting
stream of alignment regions is passed to a set of in-
dependent error-detection modules, each of which
records errors in a global table for subsequent report
generation. Certain of the error-detection compo-
nents make use of external data in the form of lexical
and other language resources.
3 Translation Errors
The difficulty of general-case translation error detec-
tion is discussed. Several classes of feasible errors
?C.P. 6128, succ. Centre-ville, Montre?al QC, Canada H3C 3J7
?University of Quebec en Outaouais, Lucien Brault Pavilion,
101 St-Jean-Bosco Street, Gatineau QC, Canada K1A 0R6
are identified, and the technological capabilities re-
quired for their successful detection described.
Detection of incorrect terminology usage, for ex-
ample, requires the ability to recognize correspon-
dences between source and target language expres-
sions, and to generalize over different realizations of
a given term; inflection, coordination and anaphora
combine to render inadequate solutions based solely
on simple static lists of term pairs. ?Negative termi-
nology?, covering false friends, deceptive cognates,
Anglicisms, etc., is rather more challenging, and
can benefit from a more precise notion of transla-
tional correspondence. Proper names pose a range of
problems, including referential disambiguation and
varying conventions regarding transliteration, while
a broad class of paralinguistic phenomena (num-
bers, dates, product codes, etc.) raise yet others in
the area of monolingual analysis and translational
equivalence. Omissions and insertions constitute a
final error class; these present particular difficulties
of recognition and interpretation, and are best ad-
dressed heuristically.
The current TransCheck system targets the error
types mentioned above. Each is exemplified and
discussed, together with the elements of language
technology which permit their detection: dictionar-
ies, shallow parsing, alignment, translation models,
etc.
Experience gained in preliminary user trials is
briefly reported and a variety of usage scenarios con-
sidered. Finally, some comparisons are made with
other translation tools, including other proposals for
translation error detection.
1
   
	 TransType: a Computer--Aided Translation Typing System 
Ph i l ippe  Lang la i s  and George  Foster  and Guy  Lapa lme 
RAL I /D IRO - -  Universit@ de Montr@al 
C.P. 6128, succursale Centre-ville 
H3C 3J7 Montr4al, Canada 
Phone:: +1 (514) 343-2145 
Fax: +1 (514) 343-5834 
email: {felipe, foster, lapalme}?iro, umontreal, ca 
Abst ract  
This paper describes the embedding of a sta- 
tistical translation system within a text editor 
to produce TRANSTYPE, a system that watches 
over the user as he or she types a translation and 
repeatedly suggests completions for the text al- 
ready entered. This innovative Embedded Ma- 
chine Translation system is thus a specialized 
means of helping produce high quality transla- 
tions. 
1 In t roduct ion  
TRANSTYPE is a project set up to explore an 
appealing solution to the problem of using In- 
teractive Machine Translation (IMT) as a tool 
for professional or other highly-skilled transla- 
tors. IMT first appeared as part of Kay's MIND 
system (Kay, 1973), where the user's role was 
to help the computer analyze the source text 
by answering questions about word sense, el- 
lipsis, phrasal attachments, etc. Most later 
work on IMT, eg (Blanchon, 1991; Brown and 
Nirenburg, 1990; Maruyama and Watanabe, 
1990; Whitelock et al, 1986), has followed in 
this vein, concentrating on improving the ques- 
tion/answer process by having less questions, 
more friendly ones, etc. Despite progress in 
these endeavors, systems of this sort are gen- 
erally unsuitable as tools for skilled trans\]\[ators 
because the user serves only as an advisor, with 
the MT components keeping overall control over 
the translation process. 
TRANSTYPE originated from the conviction 
that a better approach to IMT for competent 
translators would be to shift the focus of in- 
teraction from the meaning of the source text 
to the form of the target text. This would re- 
lieve the translator of the burden of having to 
provide explicit analyses of the source text and 
allow him to translate naturally, assisted by the 
machine whenever possible. 
In this approach, a translation emerges from 
a series of alternating contributions by human 
and machine. The machine's contributions are 
basically proposals for parts of the target text, 
while the translator's can take many forms, in- 
cluding pieces of target text, corrections to a 
previous machine contribution, hints about the 
nature of the desired translation, etc. In all 
cases, the translator remains directly in control 
of the process: the machine must respect he 
constraints implicit in his contributions, and he 
or she is free to accept, modify, or completely 
ignore its proposals. 
So TRANSTYPE is a specialized text editor 
with an embedded Machine translation engine 
as one of its components. In this project we 
had to address the following problems: how to 
interact with the user and how to find appro- 
priate multi-word units for suggestions that can 
be computed in real time. 
2 The  TransType  mode l  
2.1 User  V iewpo int  
Our interactive translation system is illustrated 
in figure 1 for an English to French translation. 
It works as follows: a translator selects a sen- 
tence and beg!ns typing its translation. After 
each character typed by the translator, the sys- 
tem displays a proposed completion, which may 
either be accepted using a special key or rejected 
by continuing to type. This interface is simple 
and its performance may be measured by the 
proportion of characters or keystrokes aved in 
typing a translation. Note that, throughout this 
process, the translator emains in control, and 
the machine must continually adapt its sugges- 
tions to the translator's input. This differs from 
the usual machine translation set-ups where it is 
the machine that produces the first draft which 
46 
? . ? . : ,  ? , ,  - - - - - - ~  
..... Fich:ier :::= ptions 
? : ' .  " - : . ' .  . . .  " . . :  . . . .  " .11 . . .  I t "  I: am:pleased:to: akepart:m this debate today. 
Usingitoday'S technologies,it:is possiblefOrall ~ad iaqs  to 
. . . .  . a . . . . .  borrowing:. 
. . . . .  ? . . . . . . . . .  :::i~ : 
d~batl.. 
GraCel ~i~la t~chnOIogiemoderne, tousles Can adiehs peuVent se 
prononcer:sur:lesquestions de:depenses: et~:d em runts:de/EZra 
Figure 1: Example of an interaction in TRANSTYPE with the source text in the top half of the 
screen. The target text is typed in the bottom half with suggestions given by the menu at the 
insertion point. 
then has to be corrected by the translator. 
The first version of TRANSTYPE (Foster et 
al., 1997) only proposed completions for the cur- 
rent word. We are now working on predictions 
which extend to the next several words in the 
text. The potential gain from multiple-word 
predictions (Langlais et al, 2000) can be ap- 
preciated in the one-sentence translation task 
reported in table 1, where a hypothetical user 
saves over 60% of the keystrokes needed to pro- 
duce a translation in a word completion sce- 
nario, and about 75% in a "unit" completion 
scenario 
2.2 System V iewpo int  
The core of TRANSTYPE is a completion engine 
which comprises two main parts: an evaluator 
which assigns probabilistic scores to completion 
47 
This bill is very similar to its companion bill which we dealt with yesterday 
in the house of commons 
word-completion task. unit-completion task 
pref. completions pref. completions 
C+ /loi ? c/e pro jet de loi ce 
pro jet 
de 
Ioi 
est 
tr~s 
semblable 
au 
pro jet 
de 
Ioi 
que 
nous 
avons 
examin4 
hier 
la 
chambre 
des 
communes 
ce+ / lOi" C/' 
p+ /est p/rojet 
d+ /tr~s d/e 
I+ /tr~s I/oi 
e+ /de e/st 
t+ /de ? t / r~s  
se+ /de ? s/es 
au+ /loi ? a/vec 
p+ /loi p/rojet 
d+ /loi ? d/e 
I+ /nous ? I/oi 
qu+ /nous ? q /u i  ? 
+ /nous 
av+ /nous 
ex+ /hier 
+ /hier 
se/mblable 
qu/e 
?  v/ons 
? e /n .  ex/amin~ 
~+ /b ie r  ? ~/ la  
+ /chambre 
de+ / communes  ? dle ? 
+ /communes 
de/s 
e+ 
t+ 
se+ 
a+ 
/de e/st 
/de .  t/r~s 
/de ? s/es se/mblable 
/loi ? a/u projet de loi sur 
qu+ /nous ? q /u i  ? qu/e 
+ /nous 
av+ /nous. a/vec, av/ons 
exa+ /& la chambre des communes 
e/n. ex/istence, exa/min~ 
h-F /& la chambre des communes 
h/let 
+ /& la chambre des communes 
106 char. 23 20 accept. 14 11 accept. -t- 1 correc. 
43 keyst rokes  26 keyst rokes  
Table h A one-sentence s ssion illustrating the word- and unit- completion tasks. The first col- 
umn indicates the target words the user is expected to produce. The next two columns indicate 
respectively the prefixes typed by the user and the completions made by the system under a word- 
completion task. The last two columns provide the same information for the unit-completion task. 
The total number of keystrokes for both tasks is reported in the last line. + indicates the accep- 
tance key typed by the user. A Completion is denoted by a/ f l  where a is the typed prefix and fl 
the completed part. Completions for different prefixes are separated by ? . 
hypotheses and a generator which uses the eval- 
uation function to select the best candidate for 
completion. 
2.2.1 The  eva luator  
The evaluator is a function p(t\[t', s) which as- 
signs to each target-text unit t an estimate of 
its probability given a source text s and the to- 
kens t' which precede t in the current ranslation 
of s. Our approach to modeling this distribu- 
tion is based to a large extent on that of the 
IBM group (Brown et al, 1993), but it diflhrs in 
one significant aspect: whereas the IBM model 
involves a "noisy channel" decomposition, we 
use a linear combination of separate predictions 
from a language model p(t\[t') and a transla- 
tion model p(t\[s). Although the noisy channel 
technique is powerful, it has the disadvantage 
that p(s\[t', t) is more expensive to compute than 
p(t\[s) when using IBM-style translation models. 
Since speed is crucial for our application, we 
chose to forego it in the work described here. 
Our linear combination model is fully described 
in (Langlais and Foster, 2000) but can be seen 
as follows: 
48 
p(tlt ' ,s ) = p(tlt' ) A(O(t',s)), (1) 
language 
+ p(tls)\[1-~(O(t',s))! 
translation 
where .~(O(t',s)) e \[0,1\] are context- 
dependent interpolation coefficients. O(t~,s) 
stands for any function which maps t~,s into a 
set of equivalence classes. Intuitively, ),(O(t r, s)) 
should be high when s is more informative than 
t r and low otherwise. For example, the trans- 
lation model could have a higher weight at the 
start of sentence but the contribution of the lan- 
guage model can become more important in the 
middle or the end of the sentence. 
2.2.2 The  language mode l  
We experimented with various simple linear 
combinations of four different French language 
models: a cache model, similar to the cache 
component in Kuhn's model (Kuhn and Mori, 
1990); a unigram model; a trielass model (Der- 
ouault and Merialdo, 1986); and an interpolated 
trigram (Jelinek, 1990). 
We opted for the trigram, which gave signifi- 
cantly better results than the other three mod- 
els. The trigram was trained on the Hansard 
corpus (about 50 million words), with 75% of 
the corpus used for relative-frequency parame- 
ter estimates, and 25% used to reestimate inter- 
polation coefficients. 
2.2.3 The  t rans la t ion  mode l  
Our translation model is based on the linear in- 
terpolation given in equation 2 which combines 
predictions of two translation models - -  Ms and 
Mu - -  both based on an IBM-like model 2 (see 
equation 3). Ms was trained on single words 
and Mu was trained on both words and units. 
p( tls) = Z pt( tls) ,+ (1 - Z).p2 ( (s ) ) 
word unit 
(2) 
where Ps and Pu stand for the probabilities 
given respectively by Ms and M~. ~(s) repre- 
sents the new sequence of tokens obtained after 
grouping the tokens of s into units. 
Both models are based on IBM translation 
model 2 (Brown et al, 1993) which has the 
49 
property that it generates tokens independently. 
The total probability of the ith target-text to- 
ken ti is just the average of the probabilities 
with which it is generated by each source text 
token sj; this is a weighted average that takes 
the distance from the generating token into ac- 
count: 
is1 
p(tils) = ~p( t i l s j )  a(jli, Is\[) 
j=O 
(3) 
where p(ti Is j) is a word-for-word translation 
probability, Isl is the length (counted in tokens) 
o f the  source segment s under translation, and 
a(jli , Is\]) is the a priori alignment probability 
that the target-text token at position i will be 
generated by the source text token at position 
j; this is equal to a constant value of 1~(Is I + 1) 
for model 1. This formula follows the conven- 
tion of (Brown et al, 1993) in letting so des- 
ignate the null state. We modified IBM model 
2 to account for invariant entities such as En- 
glish forms that almost invariably translate into 
French either verbatim or after having under- 
gone a predictable transformation e.g. numbers 
or dates. These forms are very frequent in the 
Hansard corpus. 
2.3 The Generator  
The task of the generator is to identify units 
matching the current prefix typed by the user, 
and pick the best candidate using the evalua- 
tion function. Given the real time constraints 
of an IMT system, we divided the French vocab- 
ulary into two parts: a small active component 
whose contents are always searched for a match 
to the current prefix, and a much larger passive 
part which comes into play only when no candi- 
dates are found in the active vocabulary. Both 
vocabularies are coded as tries. 
The passive vocabulary is a large dictionary 
containing over 380,000 word forms. The ac- 
tive part is computed ynamically when a new 
sentence is selected by the translator. It relies 
on the fact that a small number of words ac- 
count for most of the tokens in a text. It is 
composed of a few entities (tokens and units) 
that are likely to appear in the translation. In 
practice, we found that keeping 500 words and 
50 units yields good performance. 
3 Implementat ion  
From an implementation point of view, the core 
of TransType relies on a flexible object ori- 
ented architecture, which facilitates the integra- 
tion of any model that can predict units (words 
or sequence of words) from what has been al- 
ready typed and the source text being trans- 
lated. This part is written in C?+.  Statisti- 
cal translation and language models have been 
integrated among others into this architecture 
(Lapalme et al, 2000). 
The graphical user interface is implemented 
in Tcl/Tk, a multi-platform script language well 
suited to interfacing problems. It offers all the 
classical functions for text edition plus a pop-up 
menu which contains the more probable words 
or sequences of words that may complete the 
ongoing translation. The proposed completions 
are updated after each keystroke the translator 
enters. 
4 Evaluation 
We have conducted a theoretical evaluation of 
TransType on a word completion task, which 
assumes that a translator carefully observes 
each completion proposed by the system, and 
accepts it as soon as it is correct. Under 
these optimistic onditions, we have shown that 
TransType allows for the production of a trans- 
lation typing less than a third of its characters. 
In order to better grasp the usefulness of 
TRANSTYPE, we also performed a more prac- 
tical evaluation by asking ten translators to 
use the prototype for about one hour to trans- 
late isolated sentences. We first asked them to 
translate without any help from TRANSTYPE 
and then we compared their typing speed with 
TRANSTYPE suggestions turned on. Overall, 
translators liked the concept and found it very 
useful; they all liked the suggestions although 
it seemed to induce a literal style of transla- 
tion. We also asked them if they thought hat 
TRANSTYPE improved their typing speed and 
the majority of them said so; unfortunately the 
figures showed that none of them did so ... The 
typing rates are nevertheless quite good, given 
that the users were new to this environment and 
this style of looking at suggestions while trans- 
lating. But interestingly this practical ew~lua- 
tion confirmed our theoretical evaluation that a- 
translation can be produced with TRANSTYPE 
by typing less than 40% of the characters of a 
translation. Results of this evaluation and com- 
parisons with our theoretical figures are further 
described in (Foster et al, 2000). 
This experiment made us realize that this 
concept of real-time suggestions depends very 
much on the usability of the prototype; we had 
first developed a much simpler editor but its 
limitations were such that the translators found 
it unusable. So we are convinced that the user- 
interface aspects of this prototype should be 
thoroughly studied. But the TRANSTYPE ap- 
proach would be much more useful if it was 
combined with other text editing tasks related 
to translation: for example TRANSTYPE could 
format the translation in the same way as the 
source text, this would be especially useful for 
titles and tables; it would also be possible to 
localize automatically specific entities such as 
dates, numbers and amounts of money. It would 
also be possible to check that some translations 
given by the user are correct with respect with 
some normative usage of words or terminologi- 
cal coherence; these facilities are already part of 
TRANSCHECK, another computer aided transla- 
tion tool prototype developed in our laboratory 
(Jutras, 2000). 
5 Conc lus ion  
We have presented an innovative way of em- 
bedding machine translation by means of a pro- 
totype which implements an appealing interac- 
tive machine translation scenario where the in- 
teraction is mediated via the target text under 
production. Among other advantages, this ap- 
proach relieves the translator of the burden of 
source analyses, and gives him or her direct con- 
trol over the final translation without having to 
resort to post-edition. 
Acknowledgements  
TRANSTYPE is a project funded by the Natu- 
ral Sciences and Engineering Research Council 
of Canada. We are greatly indebted to Elliott 
Macklovitch and Pierre Isabelle for the fruitful 
orientations they gave to this work. 
References 
Herv6 Blanchon. 1991. Probl~mes de 
d@sambigffisation i teractive et TAO per- 
sonnelle. In L 'environnement Traductionnel, 
50 
Journ@es cientifiques du R@seau th@matique 
de recherche "Lexicologie, terminologie, 
traduction", pages 31-48, Mons, April. 
Ralf D. Brown and Sergei Nirenburg. 1990. 
Human-computer interaction for semantic 
disambiguation. In Proceedings off the Inter- 
national Conference on Computational Lin- 
guistics (COLING), pages 42-47, Helsinki, 
Finland, August. 
Peter F. Brown, Stephen A. Della Pietra, Vin- 
cent Della J. Pietra, and Robert L. Mercer. 
1993. The mathematics of machine transla- 
tion: Parameter estimation. Computational 
Linguistics, 19(2):263-312, June. 
A.-M. Derouault and B. Merialdo. 1986. Nat- 
ural language modeling for phoneme-to-text 
transcription. IEEE Transactions on Pattern 
Analysis and Machine Intelligence (PAMI), 
8 (6): 742-749, November. 
George Foster, Pierre Isabelle, and Pierre Pla- 
mondon. 1997. Target-text Mediated Inter- 
active Machine Translation. Machine Trans- 
lation, 12:175-194. 
George Foster, Philippe Langlais, Guy 
Lapalme, Dominique Letarte, Elliott 
Macklovitch, and S@bastien Sauv@. 2000. 
Evaluation of transtype, a computer-aided 
translation typing system: A comparison of 
a theoretical- and a user- oriented evaluation 
procedures. In Conference on Language 
Resources and Evaluation (LREC), page 8 
pages, Athens, Greece, June. 
Frederick Jelinek. 1990. Self-organized lan- 
guage modeling for speech recognition. In 
A. Waibel and K. Lee, editors, Readings in 
Speech Recognition, pages 450-506. Morgan 
Kaufmann, San Mateo, California. 
Jean-Marc Jutras. 2000. An automatic reviser: 
The TransCheck system. In Applied Natu- 
ral Language Processing 2000, page 10 pages, 
Seattle, Washington, May. 
Martin Kay. 1973. The MIND system. In 
R. Rustin, editor, Natural Language Process- 
ing, pages 155-188. Algorithmics Press, New 
York. 
Roland Kuhn and Renato De Mori. 1990. 
A cache-based natural language model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence 
(PAMI), 12(6):570-583, June. 
Philippe Langlais and George Foster. 2000. Us- 
ing context-dependent i terpolation to com- 
bine statistical language and translation 
models for interactive machine translation. 
In Computer-Assisted Information Retrieval, 
Paris, April. 
Philippe Langlais, George Foster, and Guy 
Lapalme. 2000. Unit completion for a 
computer-aided translation typing system. In 
Applied Natural Language Processing 2000, 
page 10 pages, Seattle, Washington, May. 
Guy Lapalme, George Foster, and Philippe 
Langlais. 2000. La programmation rient@e- 
objet pour le d~veloppement de modules de 
langages. In Christophe Dony and Houari A. 
Sahraoui, editors, LMO'O0 - Langages et 
modules ~ objets, pages 139-147, Mont St- 
Hilaire, Qu@bec, 27 Janvier. Hermes Science. 
Conference invit@e. 
Hiroshi Maruyama nd Hideo Watanabe. 1990. 
An interactive Japanese parser for machine 
translation. In Proceedings of the Interna- 
tional Conference on Computational Linguis- 
tics (COLING), pages 257-262, Helsinki, Fin- 
land, August. 
P. J. Whitelock, M. McGee Wood, B. J. Chan- 
dler, N. Holden, and H. J. Horsfall. 1986. 
Strategies for interactive machine transla- 
tion: the experience and implications of the 
UMIST Japanese project. In Proceedings of 
the International Conference on Computa- 
tional Linguistics (COLING), pages 329-334, 
Bonn, West Germany. 
51 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 37-42, Lisbon, Portugal, 2000. 
Incorporating Position Information into a Maximum 
Entropy/Min imum Divergence Translation Model 
George  Foster  
RALI, Universit6 de Montr6al 
fos ter@i ro ,  umontrea l ,  ca 
Abstract  
I describe two methods for incorporating infor- 
mation about the relative positions of bilingual 
word pairs into a Maximum Entropy/Minimum 
Divergence translation model. The better of the 
two achieves over 40% lower test corpus perplex- 
ity than an equivalent combination of a trigram 
language model and the classical IBM transla- 
tion model 2. 
1 Introduct ion 
Statistical Machine Translation (SMT) systems 
use a model ofp(tls), the probability that a text 
s in the source language will translate into a text 
t in the target language, to determine the best 
translation for a given source text. A straight- 
forward way of modeling this distribution is to 
apply a chain-rule xpansion of the form: 
ItL 
p(t lS  ) = Hp(tiltl...ti-l,S), 
i=1 
(i) 
where ti denotes the ith token in t. 1 The objects 
to be modeled in this case belong to the family 
of conditional distributions p(wlhi, s), the prob- 
ability of the ith word in t, given the tokens 
which precede it and the source text. 
The main motivation for modeling p(tls ) in 
terms of p(wlhi ,s) is that it simplifies the "de- 
coding" problem of finding the most likely tar- 
get text. In particular, if hi is known, finding 
the best word at the current position requires 
only a straightforward search through the target 
1This ignores the issue of normalization over target 
texts of all possible lengths, which can be easily enforced 
when desired by using a stop token or a prior distribution 
over lengths. 
vocabulary, and efficient dynamic-programming 
based heuristics can be used to extend this to 
sequences of words. This is very important for 
applications uch as TransType (Foster et al, 
1997; Langlais et al, 2000), where the task is 
to make real-time predictions of the text a hu- 
man translator will type next, based on the 
source text under translation and some prefix 
of the target text that has already been typed. 
The standard "noisy channel" approach used in 
SMT, where p(tls ) c< p(t)p(slt), is generally too 
expensive for such applications because it does 
not permit direct calculation of the probabil- 
ity of a word or sequence of words beginning at 
the current position. Complex and expensive 
search strategies are required to find the best 
target text in this approach (Garcfa-Varea et 
al., 1998; Niessen et al, 1998; Ochet al, 1999; 
Wang and Waibel, 1998). 
The challenge in modeling p(wlhi,s ) is to 
combine two disparate sources of conditioning 
information in an effective way. One obvious 
strategy is to use a linear combination of sep- 
arate language and translation components, of 
the form: 
p(w\[hi, s) -- Ap(w\[hi) + (1 - A)p(w\[i, s). (2) 
where p(w\[hi) is a language model, p(wli , s) is 
a translation model, and A E \[0, 1\] is a com- 
bining weight. However, this appears to be 
a weak technique (Langlais and Foster, 2000), 
even when A is allowed to depend on various 
features of the context (hi, s). 
In previous work (Foster, 2000), I de- 
scribed a Maximum Entropy/Minimum Diver- 
gence (MEMD) model (Berger et al, 1996) 
for p(w\[hi, s) which incorporates a trigram lan- 
guage model and a translation component which 
is an analog of the well-known IBM transla- 
tion model 1 (Brown et al, 1993). This model 
37 
significantly outperforms an equivalent linear 
combination of a trigram and model 1 in test- 
corpus perplexity, despite using several orders of 
magnitude fewer translation parameters. Like 
model 1, its translation component is based only 
on the occurrences in s of words which are po- 
tential translations for w, and does not take 
into account the positions of these words rel- 
ative to w. An obvious enhancement is to in- 
corporate such positional information into the 
MEMD model, thereby making its translation 
component analogous to the IBM model 2. This 
is the problem I address in this paper. 
2 Mode ls  
2.1 L inear  Mode l  
As a baseline for comparison I used a linear com- 
bination as in (2) of a standard interpolated tri- 
gram language model and the IBM translation 
model 2 (IBM2), with the combining weight A 
optimized using the EM algorithm. IBM2 is de- 
rived as follows: 2 
l 
p(wli, s ) = ~p(w, j l i ,  s ) 
j=O 
l 
~p(w\]s j )p( j l i ,  l)
j=O 
where I = \[s\[, and the hidden variable j gives 
the position in s of the (single) source token sj 
assumed to give rise to w, or 0 if there is none. 
The model consists of a set of word-pair param- 
eters p(t\[s) and position parameters p(j\[i,/); in 
model 1 (IBM1) the latter are fixed at 1/(1 + 1), 
as each position, including the empty position 
0, is considered equally likely to contain a trans- 
lation for w. Maximum likelihood estimates for 
these parameters can be obtained with the EM 
algorithm over a bilingual training corpus, as 
described in (Brown et al, 1993). 
2.2 MEMD Mode l  1 
A MEMD model for p(w\[hi, s) has the general 
form: 
p(wlhi, s) = q(w\[hi, s) exp(~ ? f(w, hi, s)) 
Z(hi ,s)  
2Model 2 was originally formulated for p(tls), but 
since target words are predicted independently it can 
also be used for p(wlhi  , s). The only necessary modifica- 
tion in this case is that  the position parameters can no 
longer be conditioned on It\[. 
where q(w\[hi,s) is a reference distribution, 
f(w, hi, s) maps (w, hi, s) into an n-dimensional 
feature vector, (~ is a corresponding vector of 
feature weights (the parameters of the model), 
and Z(hi, s) = ~w q(w\[hi, s) exp((~-f(w, hi)) is 
a normalizing factor. For a given choice of q 
and f, the IIS algorithm (Berger et al, 1996) 
can be used to find maximum likelihood values 
for the parameters ~. It can be shown (Della 
Pietra et al, 1995) that these are the also the 
values which minimize the Kullback-Liebler di- 
vergence D(p\[\[q) between the model and the 
reference distribution under the constraint that 
the expectations of the features (ie, the compo- 
nents of f) with respect o the model must equal 
their expectations with respect o the empirical 
distribution derived from the training corpus. 
Thus the reference distribution serves as a kind 
of prior, and should reflect some initial knowl- 
edge about the true distribution; and the use 
of any feature is justified to the extent that its 
empirical expectation is accurate. 
In the present context, the natural choice for 
the reference distribution q is a trigram lan- 
guage model. To create a MEMD analog to 
IBM model 1 (MEMD1), I used boolean fea- 
tures corresponding to bilingual word pairs: 
1, sEsandt - - - -w 
fst(W,S) = 0, else 
where (s, t) is a (source,target) word pair. Using 
the notational convention that ast is 0 whenever 
the corresponding feature fst does not exist in 
the model, MEMD1 can be written compactly 
as: 
p(wlhi,s) = q(wlhi) exp(~ asw)/Z(hi,s). 
sEs 
Due to the theoretical properties of MEMD 
outlined above, it is necessary to select a sub- 
set of all possible features fst to avoid overfitting 
the training corpus. Using a reduced feature set 
is also computationally advantageous, since the 
time taken to calculate the normalization con- 
stant Z(hi, s) grows linearly with the expected 
number of features which are active per source 
word s E s. This is in contrast o IBM1, where 
use of all available word-pair parameters p(tls )
is standard, and engenders only a very slight 
overfitting effect. In (Foster, 2000) I describe an 
38 
effective technique for selecting MEMD word- 
pair features. 
2.3 MEMD Mode l  2 
IBM2 incorporates position information by in- 
troducing a hidden position variable and mak- 
ing independence hypotheses. This approach is 
not applicable to MEMD models, whose fea- 
tures must capture events which are directly 
observable in the training corpus. 3 It would be 
possible to use pure position features of the form 
fi#, which capture the presence of any word 
pair at position (i, j, l) and are superficially sim- 
ilar to IBM2's position parameters, but these 
would add almost no information to MEMD1. 
On the other hand, features like fstijl, indicat- 
ing the presence of a specific pair (s, t) at posi- 
tion (i, j , /) ,  would cause severe data sparseness 
problems. 
Encoding Posit ions as Feature Values 
A simple solution to this di lemma is to let the 
value of a word-pair feature reflect the current 
position of the pair rather just its presence or 
absence. A reasonable choice for this is the 
value of the corresponding IBM2 position pa- 
rameter p(jli, /): 
fst(W, i, s) = { P(Jsli'o, l), elseS E s and t = w 
where js is the position of s in s, or the most 
likely position according to IBM2 if it occurs 
more than once: 5s = argmaxj:sj=s P(jli, l). Us- 
ing the same convention as in the previous sec- 
tion, the resulting model (MEMD2R) can be 
written: 
q(wlhi) exP(E~es aswP(5~ li, l)) p(wlhi, s ) = 
Z(hi,  s) 
MEMD2R is simple and compact but poses a 
technical difficulty due to its use of real-valued 
features, in that the IIS training algorithm re- 
quires integer or boolean features for efficient 
implemention. Since likelihood is a concave 
function of ~, any hillclimbing method such as 
gradient ascent 4 is guaranteed to find maximum 
3Although it is possible to extend the basic framework 
to allow for embedded Hidden Markov Models (Lalferty, 
1995). 
4I found that the "stochastic" variant of this algo- 
rithm, in which model parameters are updated after each 
training example, gave the best performance. 
likelihood parameter values, but convergence is
slower than IIS and requires tuning a gradient 
step parameter. Unfortunately, apart from this 
problem, MEMD2R also turns out to perform 
slightly worse than MEMD1, as described be- 
low. 
Using Class-based Posi t ion Features 
Since the basic problem with incorporating po- 
sition information is one of insufficient data, a 
natural solution is to try to group word pair and 
position combinations with similar behaviour 
into classes such that the frequency of each 
class in the training corpus is high enough for 
reliable estimation. To do this, I made two 
preliminary assumptions: 1) word pairs with 
similar MEMD1 weights should be grouped to- 
gether; and 2) position configurations with sim- 
ilar IBM2 probabilities hould be grouped to- 
gether. This converts the problem from one 
of finding classes in the five-dimensional space 
(s, t, i, j, l) to one of identifying rectangular ar- 
eas on a 2-dimensional grid where one axis con- 
tains position configurations (i, j, l), ordered by 
p(jli,/); and the other contains word pairs (s, t), 
ordered by ast. To simplify further, I parti- 
tioned both axes so as to approximately bal- 
ance the total corpus frequency of all word pairs 
or position configurations within each parti- 
tion. Thus the only parameters required to com- 
pletely specify a classification are the number of 
position and word-pair partitions. Each combi- 
nation of a position partit ion and a word pair 
partit ion corresponds to a class, and all classes 
can be expected to have roughly the same em- 
pirical counts. 
The model (MEMD2B) based on this scheme 
has one feature for each class; if A designates the 
set of triples (i, j, l) in a position partit ion and 
B designates the set of pairs (s, t) in a word-pair 
partition, then for all A, B there is a feature: 
fA,B(w,i,s) l = ~j=l  5\[(i,j,l) EA A 
(sj,w) B A 
j = )sj\], 
where 5\[X\] is 1 when X is true and 0 other- 
wise. For robustness, I used these position fea- 
tures along with pure MEMDl-style word-pair 
features fst. The weights O~A, s on the position 
features can thus be interpreted as correction 
terms for the pure word-pair weights as,t which 
39 
segment file pairs sentence pairs English tokens French tokens 
train 922 1,639,250 29,547,936 31,826,112 
held-out 1 30 54,758 978,394 1,082,350 
held-out 2 30 59,435 1,111,454 1,241,581 
test 30 53,676 984,809 1,103,320 
Table 1: Corpus segmentation. The train segment was the main training corpus; the held-out 1 
segment was used for combining weights for the trigram and the overall linear model; and the 
held-out 2 segment was used for the MEMD2B partition search. 
reflect the proximity of the words in the pair. p(TIS) -1~IT\], where p is the model being eval- 
The model is: uated, and (S, T) is the test corpus, considered 
.to be a set of statistically independent sentence 
p(w\[hi,s) = q(wlhi)exp(~ses a w + aA(i,j~,O,B(s,t))pair s (s,t). Perplexity is a good indicator of 
Z(hi,s) 
where A(i,Ss,l) gives the partition for the cur- 
rent position, B(s, t) gives the partition for the 
current word pair, and following the usual con- 
vention, aA(i,j~,0,S(s,t) is zero if these are unde- 
fined. 
To find the optimal number of position par- 
titions m and word-pair partitions n, I per- 
formed a greedy search, beginning at a small ini- 
tial point (m, n) and at each iteration training 
two MEMD2B models characterized by (km, n) 
and (m, kn), where k > 1 is a scaling factor 
(note that both these models contain kmn po- 
sition features). The model which gives the 
best performance on a validation corpus is used 
as the starting point for the next iteration. 
Since training MEMD models is very expen- 
sive, to speed up the search I relaxed the con- 
vergence criterion from a training corpus per- 
plexity 5 drop of < .1% (requiring 20-30 IIS it- 
erations) to < .6% (requiring approximately 10 
IIS iterations). I stopped the search when the 
best model's performance on the validation cor- 
pus did not decrease significantly from that of 
the model at the previous tep, indicating that 
overtraining was beginning to occur. 
3 Resu l ts  
I tested the models on the Canadian Hansard 
corpus, with English as the source language 
and French as the target language. After sen- 
tence alignment using the method described 
in (Simard et al, 1992), the corpus was split 
into disjoint segments as shown in table 1. 
To evaluate performance, I used perplexity: 
5Defined in the next section 
l~erformance for the TransType application de- 
scribed in the introduction, and it has also been 
used in the evaluation of full-fledged SMT sys- 
tems (A1-Onaizan et al, 1999). To ensure a fair 
comparison, all models used the same target vo- 
cabulary. For all MEMD models, I used 20,000 
word-pair features selected using the method 
described in (Foster, 2000); this is suboptimal 
but gives reasonably good performance and fa- 
cilitates experimentation. 
Figures 1 and 2 show, respectively, the path 
taken by the MEMD2B partition search, and 
the validation corpus perplexities of each model 
tested during the search. As shown in figure 1, 
the search consisted of 6 iterations. Since on all 
previous iterations no increase in position parti- 
tions beyond the initial value of 10 was selected, 
on the 5th iteration I tried decreasing the num- 
ber of position partitions to 5. This model was 
not selected either, so on the final step only the 
number of word-pair partitions was augmented, 
yielding an optimal combination of 10 position 
partitions and 4000 word-pair partitions. 
Table 2 gives the final results for all mod- 
els. The IBM models tested here incorporate 
a reduced set of 1M word-pair parameters, e- 
lected using the method described in (Foster, 
2000), which gives slightly better test-corpus 
performance than the unrestricted set of all 35M 
word pairs which cooccur within aligned sen- 
tence pairs in the training corpus. 
The basic MEMD1 model (without position 
parameters) attains about 30% lower perplex- 
ity than the model 2 baseline, and MEMD2B 
with an optimal-sized set of position param- 
eters achieves in a further drop of over 10%. 
Interestingly, the difference between IBM1 and 
40 
model word-pair position perplexity improvement 
parameters parameters over baseline 
trigram 
trigram + IBM1 
trigram + IBM2 
MEMD1 
MEMD2R 
MEMD2B 
MEMD2B 
0 
1,000,000 
1,000,000 
20,000 
20,000 
20,000 
20,000 
0 
0 
115,568 
0 
0 
10 x 10 
10 x 4000 
61.0 
43.2 
35.2 
24.5 
28.4 
22.1 
20.2 
O% 
30.4% 
19.3% 
37.2% 
42.6% 
Table 2: Model performances. Linear interpolation is designated with a + sign; and the MEMD2B 
position parameters are given as rex, where m and n are the numbers of position partitions and 
word-pair partitions respectively. 
4000 
2000 
1000 
,500 
250 
\ 
i t I i 
5 10 20 50 
number of posi~on parti~ons 
20.6 
20.4 
20.2 
~. 20 
o~ u 
19.9 
19.6 
19.4 
10 position classes - -~  
20 position classes -~  
50 position dasses -0-- 
5 position classes .x .... 
I 
I I I r 
50 250 500 1000 2000 
word-pair classes 
19.2 
4000 
Figure 1: MEMD2B partition search path, be- 
ginning at the point (10, 10). Arrows out of each 
point show the configurations tested at each it- 
eration. 
IBM2's performance (18.5% lower perplexity for 
IBM2) is about the same as the difference be- 
tween MEMD1 and MEMD2B (17.6% lower for 
MEMD2B). 
4 Conclusion 
This paper deals with the problem of incorpo- 
rating information about the positions of bilin- 
gual word pairs into a MEMD model which is 
analogous to the classical IBM model 1, thereby 
creating a MEMD analog to the IBM model 2. I 
proposed and evaluated two methods for accom- 
plishing this: using IBM2 position parameter 
probabilities as MEMD feature values, which 
was unsuccessful; and adding features which 
Figure 2: Validation corpus perplexities for var- 
ious MEMD2B models. Each connected line in 
this graph corresponds to a vertical column of 
search points in figure 1. 
capture the occurrence of a word-pair with a 
MEMD1 weight that falls into a specific range 
of values at a position to which IBM2 assigns 
a probability in a certain range. The second 
model achieved over 40% lower test perplex- 
ity than a linear combination of a trigram and 
IBM2, despite using several orders of magnitude 
fewer parameters. 
This work represents a novel approach to 
translation modeling which is most appropriate 
for applications like TransType which need to 
make rapid predictions of upcoming text. How- 
ever, it is not inconceivable that it could also 
be used for full-fledged MT. One partial impedi- 
ment to this is that the MEMD framework lacks 
41 
a mechanism equivalant o the EM algorithm 
for estimating probabilities associated with hid- 
den variables. The solution I have proposed 
here can be seen as a first step to investigat- 
ing ways of getting around this problem. 
Acknowledgements  
This work was carried out as part of the 
TransType project at RALI, funded by the Nat- 
ural Sciences and Engineering Research Council 
of Canada. 
References  
Yaser A1-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David 
Yarowsky. 1999. Statistical machine translation: 
Final report, JHU workshop 1999. Technical 
report, The Center for Language and Speech 
Processing, The Johns Hopkins University, 
www.clsp.jhu.edu/ws99/projects/mt/final_report. 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent J. Della Pietra. 1996. A Maximum Entropy 
approach to Natural Language Processing. Com- 
putational Linguistics, 22(1):39-71. 
Peter F. Brown, Stephen A. Della Pietra, Vincent 
Della J. Pietra, and Robert L. Mercer. 1993. 
The mathematics of Machine Translation: Pa- 
rameter estimation. Computational Linguistics, 
19(2):263-312, June. 
S. Della Pietra, V. Della Pietra, and J. Lafferty. 
1995. Inducing features of random fields. Tech- 
nical Report CMU-CS-95-144, CMU. 
George Foster, Pierre Isabelle, and Pierre Plamon- 
don. 1997. Target-text Mediated Interactive Ma- 
chine Translation. Machine Translation, 12:175- 
194. 
George Foster. 2000. A Maximum Entropy / Min- 
imum Divergence translation model. In Proceed- 
ings of the 38th Annual Meeting of the Association 
for Computational Linguistics (ACL-38), Hong 
Kong, October. 
Ismael Garcfa-Varea, Francisco Casacuberta, and 
Hermann Ney. 1998. An iterative, DP-based 
search algorithm for statistical machine trans- 
lation. In Proceedings of the 5th International 
Conference on Spoken Language Processing (IC- 
SLP) 1998, Sydney, Australia, December. pages 
1135-1138. 
John D. Lafferty. 1995. Gibbs-markov models. In 
Computing Science and Statistics: Proceedings of 
the 27th Symposium on the Interface. Interface 
Foundation. 
Ph. Langlais and G. Foster. 2000. Using context- 
dependent interpolation to combine statistical 
language and translation models for interactive 
MT. In Content-Based Multimedia Information 
Access (RIAO), Paris, France, April. 
Ph. Langlais, G. Foster, and G. Lapalme. 2000. Unit 
completion for a computer-aided translation typ- 
ing system. In Proceedings of the 5th Conference 
on Applied Natural Language Processing (ANLP- 
5), Seattle, Washington, May. 
S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 
1998. A DP based search algorithm for statistical 
machine translation. In Proceedings of the 36th 
Annual Meeting of the Association \]or Computa- 
tional Linguistics (ACL) and 17th International 
Conference on Computational Linguistics (COL- 
ING) 1998, pages 960-967, MontrEal, Canada, 
August. 
Franz Jose\] Och, Christoph Tillmann, and Hermann 
Ney. 1999. Improved alignment models for statis- 
tical machine translation. In Proceedings of the 
~nd Conference on Empirical Methods in Natu- 
ral Language Processing (EMNLP), College Park, 
Maryland. 
Michel Simard, George F. Foster, and Pierre Is- 
abelle. 1992. Using cognates to align sentences in
bilingual corpora. In Proceedings of the 4th Con- 
ference on Theoretical and Methodological Is- 
sues in Machine Translation (TMI), Montr@al, 
Qu@bec. 
Ye-yi Wang and Alex Waibel. 1998. Fast decod- 
ing for statistical machine translation. In Proceed- 
ings of the 5th International Conference on Spo- 
ken Language Processing (ICSLP) 1998, Sydney, 
Australia, December, pages 2775-2778. 
42 
User-Friendly Text Prediction for Translators
George Foster and Philippe Langlais and Guy Lapalme
RALI, Universite? de Montre?al
{foster,felipe,lapalme}@iro.umontreal.ca
Abstract
Text prediction is a form of interactive
machine translation that is well suited to
skilled translators. In principle it can as-
sist in the production of a target text with
minimal disruption to a translator?s nor-
mal routine. However, recent evaluations
of a prototype prediction system showed
that it significantly decreased the produc-
tivity of most translators who used it. In
this paper, we analyze the reasons for this
and propose a solution which consists in
seeking predictions that maximize the ex-
pected benefit to the translator, rather than
just trying to anticipate some amount of
upcoming text. Using a model of a ?typ-
ical translator? constructed from data col-
lected in the evaluations of the prediction
prototype, we show that this approach has
the potential to turn text prediction into a
help rather than a hindrance to a translator.
1 Introduction
The idea of using text prediction as a tool for trans-
lators was first introduced by Church and Hovy as
one of many possible applications for ?crummy?
machine translation technology (Church and Hovy,
1993). Text prediction can be seen as a form of in-
teractive MT that is well suited to skilled transla-
tors. Compared to the traditional form of IMT based
on Kay?s original work (Kay, 1973)?in which the
user?s role is to help disambiguate the source text?
prediction is less obtrusive and more natural, allow-
ing the translator to focus on and directly control the
contents of the target text. Predictions can benefit
a translator in several ways: by accelerating typing,
by suggesting translations, and by serving as an im-
plicit check against errors.
The first implementation of a predictive tool for
translators was described in (Foster et al, 1997), in
the form of a simple word-completion system based
on statistical models. Various enhancements to this
were carried out as part of the TransType project
(Langlais et al, 2000), including the addition of a re-
alistic user interface, better models, and the capabil-
ity of predicting multi-word lexical units. In the fi-
nal TransType prototype for English to French trans-
lation, the translator is presented with a short pop-
up menu of predictions after each character typed.
These may be incorporated into the text with a spe-
cial command or rejected by continuing to type nor-
mally.
Although TransType is capable of correctly antic-
ipating over 70% of the characters in a freely-typed
translation (within the domain of its training cor-
pus), this does not mean that users can translate in
70% less time when using the tool. In fact, in a trial
with skilled translators, the users? rate of text pro-
duction declined by an average of 17% as a result
of using TransType (Langlais et al, 2002). There
are two main reasons for this. First, it takes time to
read the system?s proposals, so that in cases where
they are wrong or too short, the net effect will be to
slow the translator down. Second, translators do not
always act ?rationally? when confronted with a pro-
posal; that is, they do not always accept correct pro-
posals and they occasionally accept incorrect ones.
Many of the former cases correspond to translators
simply ignoring proposals altogether, which is un-
derstandable behaviour given the first point.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 148-155.
                         Proceedings of the Conference on Empirical Methods in Natural
This paper describes a new approach to text pre-
diction intended to address these problems. The
main idea is to make predictions that maximize the
expected benefit to the user in each context, rather
than systematically proposing a fixed amount of text
after each character typed. The expected benefit is
estimated from two components: a statistical trans-
lation model that gives the probability that a can-
didate prediction will be correct or incorrect, and a
user model that determines the benefit to the trans-
lator in either case. The user model takes into ac-
count the cost of reading a proposal, as well as the
random nature of the decision to accept it or not.
This approach can be characterized as making fewer
but better predictions: in general, predictions will
be longer in contexts where the translation model is
confident, shorter where it is less so, and absent in
contexts where it is very uncertain.
Other novel aspects of the work we describe here
are the use of a more accurate statistical translation
model than has previously been employed for text
prediction, and the use of a decoder to generate pre-
dictions of arbitrary length, rather than just single
words or lexicalized units as in the TransType pro-
totype. The translation model is based on the max-
imum entropy principle and is designed specifically
for this application.
To evaluate our approach to prediction, we simu-
lated the actions of a translator over a large corpus of
previously-translated text. The result is an increase
of over 10% in translator productivity when using
the predictive tool. This is a considerable improve-
ment over the -17% observed in the TransType trials.
2 The Text Prediction Task
In the basic prediction task, the input to the predictor
is a source sentence s and a prefix h of its translation
(ie, the target text before the current cursor position);
the output is a proposed extension x to h. Figure 1
gives an example. Unlike the TransType prototype,
which proposes a set of single-word (or single-unit)
suggestions, we assume that each prediction consists
of only a single proposal, but one that may span an
arbitrary number of words.
As described above, the goal of the predictor is
to find the prediction x? that maximizes the expected
s: Let us return to serious matters.
t:
h
? ?? ?
On va r
x?
? ?? ?
evenir aux choses se?rieuses.
x: evenir a`
Figure 1: Example of a prediction for English to
French translation. s is the source sentence, h is the
part of its translation that has already been typed,
x? is what the translator wants to type, and x is the
prediction.
benefit to the user:
x? = argmax
x
B(x,h, s), (1)
where B(x,h, s) measures typing time saved. This
obviously depends on how much of x is correct, and
how long it would take to edit it into the desired text.
A major simplifying assumption we make is that the
user edits only by erasing wrong characters from the
end of a proposal. Given a TransType-style interface
where acceptance places the cursor at the end of a
proposal, this is the most common editing method,
and it gives a conservative estimate of the cost at-
tainable by other methods. With this assumption,
the key determinant of edit cost is the length of the
correct prefix of x, so the expected benefit can be
written as:
B(x,h, s) =
l?
k=0
p(k|x,h, s)B(x,h, s, k), (2)
where p(k|x,h, s) is the probability that exactly k
characters from the beginning of x will be correct,
l is the length of x, and B(x,h, s, k) is the benefit
to the user given that the first k characters of x are
correct.
Equations (1) and (2) define three main problems:
estimating the prefix probabilities p(k|x,h, s), esti-
mating the user benefit function B(x,h, s, k), and
searching for x?. The following three sections de-
scribe our solutions to these.
3 Translation Model
The correct-prefix probabilities p(k|x,h, s) are
derived from a word-based statistical translation
model. The first step in the derivation is to con-
vert these into a form that deals explicitly with char-
acter strings. This is accomplished by noting that
p(k|x,h, s) is the probability that the first k charac-
ters of x are correct and that the k + 1th character
(if there is one) is incorrect. For k < l:
p(k|x,h, s) = p(xk1|h, s)? p(x
k+1
1 |h, s)
where xk1 = x1 . . . xk. If k = l, p(k|x,h, s) =
p(x|h, s). Also, p(x01) ? 1.
The next step is to convert string probabilities
into word probabilities. To do this, we assume
that strings map one-to-one into token sequences, so
that:
p(xk1|h, s) ? p(v1, w2, . . . , wm?1, um|h, s),
where v1 is a possibly-empty word suffix, each wi is
a complete word, and um is a possibly empty word
prefix. For example, if x in figure 1 were evenir aux
choses, then x141 would map to v1 = evenir, w2 =
aux, and u3 = cho. The one-to-one assumption is
reasonable given that entries in our lexicon contain
neither whitespace nor internal punctuation.
To model word-sequence probabilities, we apply
the chain rule:
p(v1, w2, . . . , wm?1, um|h, s) =
p(v1|h, s)
m?1?
i=2
p(wi|h, v1, w
i?1
2 , s)?
p(um|h, v1, w
m?1
2 , s). (3)
The probabilities of v1 and um can be expressed in
terms of word probabilities as follows. Letting u1
be the prefix of the word that ends in v1 (eg, r in
figure 1), w1 = u1v1, and h = h?u1:
p(v1|h, s) = p(w1|h?, s)/
?
w:w=u1v
p(w|h?, s),
where the sum is over all words that start with u1.
Similarly:
p(um|h?, w
m?1
1 , s) =
?
w:w=umv
p(w|h?, wm?11 , s). (4)
Thus all factors in (3) can be calculated from
probabilities of the form p(w|h, s) which give the
likelihood that a word w will follow a previous se-
quence of words h in the translation of s.1 This is
the family of distributions we have concentrated on
modeling.
Our model for p(w|h, s) is a log-linear combina-
tion of a trigram language model for p(w|h) and a
maximum-entropy translation model for p(w|s), de-
scribed in (Foster, 2000a; Foster, 2000b). The trans-
lation component is an analog of the IBM model 2
(Brown et al, 1993), with parameters that are op-
timized for use with the trigram. The combined
model is shown in (Foster, 2000a) to have signif-
icantly lower test corpus perplexity than the linear
combination of a trigram and IBM 2 used in the
TransType experiments (Langlais et al, 2002). Both
models supportO(mJV 3) Viterbi-style searches for
the most likely sequence of m words that follows h,
where J is the number of tokens in s and V is the
size of the target-language vocabulary.
Compared to an equivalent noisy-channel combi-
nation of the form p(t)p(s|t), where t is the tar-
get sentence, our model is faster but less accurate.
It is faster because the search problem for noisy-
channel models is NP-complete (Knight, 1999), and
even the fastest dynamic-programming heuristics
used in statistical MT (Niessen et al, 1998; Till-
mann and Ney, 2000), are polynomial in J?for in-
stance O(mJ4V 3) in (Tillmann and Ney, 2000). It
is less accurate because it ignores the alignment rela-
tion between s and h, which is captured by even the
simplest noisy-channel models. Our model is there-
fore suitable for making predictions in real time, but
not for establishing complete translations unassisted
by a human.
3.1 Implementation
The most expensive part of the calculation in equa-
tion (3) is the sum in (4) over all words in the vo-
cabulary, which according to (2) must be carried out
for every character position k in a given prediction
x. We reduce the cost of this by performing sums
only at the end of each sequence of complete tokens
in x (eg, after revenir and revenir aux in the above
example). At these points, probabilities for all pos-
sible prefixes of the next word are calculated in a
1Here we ignore the distinction between previous words that
have been sanctioned by the translator and those that are hy-
pothesized as part of the current prediction.
single recursive pass over the vocabulary and stored
in a trie for later access.
In addition to the exact calculation, we also ex-
perimented with establishing exact probabilities via
p(w|h, s) only at the end of each token in x, and as-
suming that the probabilities of the intervening char-
acters vary linearly between these points. As a re-
sult of this assumption, p(k|x,h, s) = p(xk1|h, s)?
p(xk+11 |h, s) is constant for all k between the end of
one word and the next, and therefore can be factored
out of the sum in equation (2) between these points.
4 User Model
The purpose of the user model is to determine the
expected benefit B(x,h, s, k) to the translator of a
prediction x whose first k characters match the text
that the translator wishes to type. This will depend
on whether the translator decides to accept or reject
the prediction, so the first step in our model is the
following expansion:
B(x,h, s, k) =
?
a?{0,1}
p(a|x,h, s, k)B(x,h, s, k, a),
where p(a|x,h, s, k) is the probability that the trans-
lator accepts or rejects x, B(x,h, s, k, a) is the ben-
efit they derive from doing so, and a is a random
variable that takes on the values 1 for acceptance and
0 for rejection. The first two quantities are the main
elements in the user model, and are described in fol-
lowing sections. The parameters of both were esti-
mated from data collected during the TransType trial
described in (Langlais et al, 2002), which involved
nine accomplished translators using a prototype pre-
diction tool for approximately half an hour each. In
all cases, estimates were made by pooling the data
for all nine translators.
4.1 Acceptance Probability
Ideally, a model for p(a|x,h, s, k) would take into
account whether the user actually reads the proposal
before accepting or rejecting it, eg:
p(a|x,h, s, k) =
?
r?{0,1}
p(a|r,x,h, s, k)p(r|x,h, s, k)
where r is a boolean ?read? variable. However, this
information is hard to extract reliably from the avail-
able data; and even if were obtainable, many of the
?60 ?50 ?40 ?30 ?20 ?10 0 10 20 30 40 50 600
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
pro
ba
bili
ty 
of 
ac
ce
pti
ng
gain (length of correct prefix ? length of incorrect suffix)
raw
smoothed
model
Figure 2: Probability that a prediction will be ac-
cepted versus its gain.
factors which influence whether a user is likely to
read a proposal?such as a record of how many pre-
vious predictions have been accepted?are not avail-
able to the predictor in our formulation. We thus
model p(a|x,h, s, k) directly.
Our model is based on the assumption that the
probability of accepting x depends only on what the
user stands to gain from it, defined according to the
editing scenario given in section 2 as the amount by
which the length of the correct prefix of x exceeds
the length of the incorrect suffix:
p(a|x,h, s, k) ? p(a|2k ? l),
where k?(l?k) = 2k? l is called the gain. For in-
stance, the gain for the prediction in figure 1 would
be 2? 7? 8 = 6. The strongest part of this assump-
tion is dropping the dependence on h, because there
is some evidence from the data that users are more
likely to accept at the beginnings of words. How-
ever, this does not appear to have a severe effect on
the quality of the model.
Figure 2 shows empirical estimates of p(a =
1|2k? l) from the TransType data. There is a certain
amount of noise intrinsic to the estimation proce-
dure, since it is difficult to determine x?, and there-
fore k, reliably from the data in some cases (when
the user is editing the text heavily). Nonetheless, it
is apparent from the plot that gain is a useful abstrac-
0 10 20 30 40 50 600
500
1000
1500
2000
2500
3000
3500
4000
av
er
ag
e t
ime
 to
 ac
cep
t (m
sec
s)
length of proposal (chars)
raw
least?squares fit
0 10 20 30 40 50 600
500
1000
1500
2000
2500
3000
3500
4000
av
er
ag
e t
ime
 to
 re
ject
 (m
sec
s)
length of proposal (chars)
raw
least?squares fit
Figure 3: Time to read and accept or reject proposals versus their length
tion, because the empirical probability of acceptance
is very low when it is less than zero and rises rapidly
as it increases. This relatively clean separation sup-
ports the basic assumption in section 2 that benefit
depends on k.
The points labelled smoothed in figure 2 were
obtained using a sliding-average smoother, and the
model curve was obtained using two-component
Gaussian mixtures to fit the smoothed empirical
likelihoods p(gain|a = 0) and p(gain|a = 1). The
model probabilities are taken from the curve at in-
tegral values. As an example, the probability of ac-
cepting the prediction in figure 1 is about .25.
4.2 Benefit
The benefit B(x,h, s, k, a) is defined as the typing
time the translator saves by accepting or rejecting
a prediction x whose first k characters are correct.
To determine this, we assume that the translator first
reads x, then, if he or she decides to accept, uses a
special command to place the cursor at the end of x
and erases its last l ? k characters. Assuming inde-
pendence from h, s as before, our model is:
B(x, k, a) =
{
?R1(x) + T (x, k)? E(x, k), a = 1
?R0(x), a = 0
where Ra(x) is the cost of reading x when it ul-
timately gets accepted (a= 1) or rejected (a= 0),
T (x, k) is the cost of manually typing xk1 , and
E(x, k) is the edit cost of accepting x and erasing
to the end of its first k characters.
A natural unit for B(x, k, a) is the number of
keystrokes saved, so all elements of the above equa-
tion are converted to this measure. This is straight-
forward in the case of T (x, k) and E(x, k), which
are estimated as k and l ? k + 1 respectively?for
E(x, k), this corresponds to one keystroke for the
command to accept a prediction, and one to erase
each wrong character. This is likely to slightly un-
derestimate the true benefit, because it is usually
harder to type n characters than to erase them.
As in the previous section, read costs are inter-
preted as expected values with respect to the proba-
bility that the user actually does read x, eg, assuming
0 cost for not reading, R0(x) = p(r=1|x)R?0(x),
where R?0(x) is the unknown true cost of reading
and rejecting x. To determine Ra(x), we measured
the average elapsed time in the TransType data from
the point at which a proposal was displayed to the
point at which the next user action occurred?either
an acceptance or some other command signalling a
rejection. Times greater than 5 seconds were treated
as indicating that the translator was distracted and
were filtered out. As shown in figure 3, read times
are much higher for predictions that get accepted, re-
flecting both a more careful perusal by the translator
and the fact the rejected predictions are often simply
ignored.2 In both cases there is a weak linear rela-
2Here the number of characters read was assumed to include
the whole contents of the TransType menu in the case of rejec-
tions, and only the proposal that was ultimately accepted in the
case of acceptances.
tionship between the number of characters read and
the time taken to read them, so we used the least-
squares lines shown as our models. Both plots are
noisy and would benefit from a more sophisticated
psycholinguistic analysis, but they are plausible and
empirically-grounded first approximations.
To convert reading times to keystrokes for the
benefit function we calculated an average time per
keystroke (304 milliseconds) based on sections of
the trial where translators were rapidly typing and
when predictions were not displayed. This gives an
upper bound for the per-keystroke cost of reading?
compare to, for instance, simply dividing the total
time required to produce a text by the number of
characters in it?and therefore results in a conser-
vative estimate of benefit.
To illustrate the complete user model, in the fig-
ure 1 example the benefit of accepting would be
7?2?4.2 = .8 keystrokes and the benefit of reject-
ing would be?.2 keystrokes. Combining these with
the acceptance probability of .25 gives an overall ex-
pected benefit B(x,h, s, k = 7) for this proposal of
0.05 keystrokes.
5 Search
Searching directly through all character strings x
in order to find x? according to equation (1) would
be very expensive. The fact that B(x,h, s) is non-
monotonic in the length of x makes it difficult to or-
ganize efficient dynamic-programming search tech-
niques or use heuristics to prune partial hypotheses.
Because of this, we adopted a fairly radical search
strategy that involves first finding the most likely se-
quence of words of each length, then calculating the
benefit of each of these sequences to determine the
best proposal. The algorithm is:
1. For each length m = 1 . . .M , find the best
word sequence:
w?m = argmax
w1:(w1=u1v), wm2
p(wm1 |h
?, s),
where u1 and h? are as defined in section 3.
2. Convert each w?m to a corresponding character
string x?m.
3. Output x? = argmaxm B(x?m,h, s), or the
empty string if all B(x?m,h, s) are non-
positive.
M average time maximum time
1 0.0012 0.01
2 0.0038 0.23
3 0.0097 0.51
4 0.0184 0.55
5 0.0285 0.57
Table 1: Approximate times in seconds to generate
predictions of maximum word sequence length M ,
on a 1.2GHz processor, for the MEMD model.
In all experiments reported below, M was set to a
maximum of 5 to allow for convenient testing. Step
1 is carried out using a Viterbi beam search. To
speed this up, the search is limited to an active vo-
cabulary of target words likely to appear in transla-
tions of s, defined as the set of all words connected
by some word-pair feature in our translation model
to some word in s. Step 2 is a trivial deterministic
procedure that mainly involves deciding whether or
not to introduce blanks between adjacent words (eg
yes in the case of la + vie, no in the case of l? +
an). This also removes the prefix u1 from the pro-
posal. Step 3 involves a straightforward evaluation
of m strings according to equation (2).
Table 1 shows empirical search timings for vari-
ous values of M , for the MEMD model described
in the next section. Times for the linear model are
similar. Although the maximum times shown would
cause perceptible delays for M > 1, these occur
very rarely, and in practice typing is usually not no-
ticeably impeded when using the TransType inter-
face, even at M = 5.
6 Evaluation
We evaluated the predictor for English to French
translation on a section of the Canadian Hansard
corpus, after training the model on a chronologi-
cally earlier section. The test corpus consisted of
5,020 sentence pairs and approximately 100k words
in each language; details of the training corpus are
given in (Foster, 2000b).
To simulate a translator?s responses to predic-
tions, we relied on the user model, accepting prob-
abilistically according to p(a|x,h, s, k), determin-
ing the associated benefit using B(x,h, s, k, a), and
advancing the cursor k characters in the case of an
config M
1 2 3 4 5
fixed -8.5 -0.4 -3.60 -11.6 -20.8
linear 6.1 9.40 8.8 8.1 7.8
exact 5.3 10.10 10.7 10.0 9.7
corr 5.8 10.7 12.0 12.5 12.6
best 7.9 17.90 24.5 27.7 29.2
fixed -11.5 -9.3 -15.1 -22.0 -28.2
exact 3.0 4.3 5.0 5.2 5.2
best 6.2 12.1 15.4 16.7 17.3
Table 2: Results for different predictor configura-
tions. Numbers give % reductions in keystrokes.
user M
1 2 3 4 5
superman 48.6 53.5 51.8 51.1 50.9
rational 11.7 17.8 17.2 16.4 16.1
real 5.3 10.10 10.7 10.0 9.7
Table 3: Results for different user simulations.
Numbers give % reductions in keystrokes.
acceptance, 1 otherwise. Here k was obtained by
comparing x to the known x? from the test corpus.
It may seem artificial to measure performance ac-
cording to the objective function for the predictor,
but this is biased only to the extent that it misrepre-
sents an actual user?s characteristics. There are two
cases: either the user is a better candidate?types
more slowly, reacts more quickly and rationally?
than assumed by the model, or a worse one. The
predictor will not be optimized in either case, but
the simulation will only overestimate the benefit in
the second case. By being conservative in estimating
the parameters of the user model, we feel we have
minimized the number of translators who would fall
into this category, and thus can hope to obtain real-
istic lower bounds for the average benefit across all
translators.
Table 2 contains results for two different trans-
lation models. The top portion corresponds to the
MEMD2B maximum entropy model described in
(Foster, 2000a); the bottom portion corresponds to
the linear combination of a trigram and IBM 2 used
in the TransType experiments (Langlais et al, 2002).
Columns give the maximum permitted number of
words in predictions. Rows show different predic-
tor configurations: fixed ignores the user model and
makes fixedM -word predictions; linear uses the lin-
ear character-probability estimates described in sec-
tion 3.1; exact uses the exact character-probability
calculation; corr is described below; and best gives
an upper bound on performance by choosing m in
step 3 of the search algorithm so as to maximize
B(x,h, s, k) using the true value of k.
Table 3 illustrates the effects of different compo-
nents of the user model by showing results for sim-
ulated users who read infinitely fast and accept only
predictions having positive benefit (superman); who
read normally but accept like superman (rational);
and who match the standard user model (real). For
each simulation, the predictor optimized benefits for
the corresponding user model.
Several conclusions can be drawn from these re-
sults. First, it is clear that estimating expected bene-
fit is a much better strategy than making fixed-word-
length proposals, since the latter causes an increase
in time for all values of M . In general, making ?ex-
act? estimates of string prefix probabilities works
better than a linear approximation, but the difference
is fairly small.
Second, the MEMD2B model significantly out-
performs the trigram+IBM2 combination, produc-
ing better results for every predictor configuration
tested. The figure of -11.5% in bold corresponds
to the TransType configuration, and corroborates the
validity of the simulation.3
Third, there are large drops in benefit due to read-
ing times and probabilistic acceptance. The biggest
cost is due to reading, which lowers the best possi-
ble keystroke reduction by almost 50% for M = 5.
Probabilistic acceptance causes a further drop of
about 15% for M = 5.
The main disappointment in these results is that
performance peaks at M = 3 rather than continu-
ing to improve as the predictor is allowed to con-
sider longer word sequences. Since the predictor
knows B(x,h, s, k), the most likely cause for this
is that the estimates for p(w?m|h, s) become worse
with increasing m. Significantly, performance lev-
3Although the drop observed with real users was greater at
about 20% (= 17% reduction in speed), there are many dif-
ferences between experimental setups that could account for
the discrepancy. For instance, part of the corpus used for the
TransType trials was drawn from a different domain, which
would adversely affect predictor performance.
els off at three words, just as the search loses di-
rect contact with h through the trigram. To correct
for this, we used modified probabilities of the form
?m p(w?m|h, s), where ?m is a length-specific cor-
rection factor, tuned so as to optimize benefit on a
cross-validation corpus. The results are shown in the
corr row of table 2, for exact character-probability
estimates. In this case, performance improves with
M , reaching a maximum keystroke reduction of
12.6% at M = 5.
7 Conclusion and Future Work
We have described an approach to text prediction for
translators that is based on maximizing the benefit
to the translator according to an explicit user model
whose parameters were set from data collected in
user evaluations of an existing text prediction proto-
type. Using this approach, we demonstrate in sim-
ulated results that our current predictor can reduce
the time required for an average user to type a text
in the domain of our training corpus by over 10%.
We look forward to corroborating this result in tests
with real translators.
There are many ways to build on the work de-
scribed here. The statistical models which are
the backbone of the predictor could be improved
by making them adaptive?taking advantage of the
user?s input?and by adding features to capture the
alignment relation between h and s in such a way as
to preserve the efficient search properties. The user
model could also be made adaptive, and it could be
enriched in many other ways, for instance so as to
capture the propensity of translators to accept at the
beginnings of words.
We feel that the idea of creating explicit user mod-
els to guide the behaviour of interactive systems is
likely to have applications in areas of NLP apart
from translators? tools. For one thing, most of the
approach described here carries over more or less
directly to monolingual text prediction, which is an
important tool for the handicapped (Carlberger et al,
1997). Other possibilities include virtually any ap-
plication where a human and a machine communi-
cate through a language-rich interface.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathematics
of Machine Translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?312, June.
Alice Carlberger, Johan Carlberger, Tina Magnuson,
Sira E. Palazuelos-Cagigas, M. Sharon Hunnicutt, and
Santiago Aguilera Navarro. 1997. Profet, a new gen-
eration of word prediction: an evaluation study. In
Proceedings of the 2nd Workshop on NLP for Commu-
nication Aids, Madrid, Spain, July.
Kenneth W. Church and Eduard H. Hovy. 1993. Good
applications for crummy machine translation. Ma-
chine Translation, 8:239?258.
George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text Mediated Interactive Machine
Translation. Machine Translation, 12:175?194.
George Foster. 2000a. Incorporating position infor-
mation into a Maximum Entropy / Minimum Di-
vergence translation model. In Proceedings of the
4th Computational Natural Language Learning Work-
shop (CoNLL), Lisbon, Portugal, September. ACL
SigNLL.
George Foster. 2000b. A Maximum Entropy / Minimum
Divergence translation model. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Hong Kong, October.
Martin Kay. 1973. The MIND system. In R. Rustin,
editor, Natural Language Processing, pages 155?188.
Algorithmics Press, New York.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs and Discussion, 25(4).
Philippe Langlais, George Foster, and Guy Lapalme.
2000. Unit completion for a computer-aided transla-
tion typing system. Machine Translation, 15(4):267?
294, December.
Philippe Langlais, Guy Lapalme, and Marie Loranger.
2002. TransType: From an idea to a system. Machine
Translation. To Appear.
S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 1998.
A DP based search algorithm for statistical machine
translation. In Proceedings of the 36th Annual Meet-
ing of the ACL and 17th COLING 1998, pages 960?
967, Montre?al, Canada, August.
C. Tillmann and H. Ney. 2000. Word re-ordering and
DP-based search in statistical machine translation. In
Proceedings of the International Conference on Com-
putational Linguistics (COLING) 2000, Saarbrucken,
Luxembourg, Nancy, August.
Confidence Estimation for Translation Prediction
Simona Gandrabur
RALI, Universite? de Montre?al
gandrabu@iro.umontreal.ca
George Foster
RALI, Universite? de Montre?al
foster@iro.umontreal.ca
Abstract
The purpose of this work is to investigate the
use of machine learning approaches for confi-
dence estimation within a statistical machine
translation application. Specifically, we at-
tempt to learn probabilities of correctness for
various model predictions, based on the native
probabilites (i.e. the probabilites given by the
original model) and on features of the current
context. Our experiments were conducted us-
ing three original translation models and two
types of neural nets (single-layer and multi-
layer perceptrons) for the confidence estima-
tion task.
1 Introduction
Most statistical models used in natural language appli-
cations are capable in principle of generating probability
estimates for their outputs. However, in practice, these
estimates are often quite poor and are usually interpreted
simply as scores that are monotonic with probabilities.
There are many contexts where good estimates of true
probabilities are desirable:
? in a decision-theoretic setting, posterior probabili-
ties are required in order to choose the lowest-cost
output for a given input.
? when a collection of different models is available for
some problem, output probabilities provide a princi-
pled and convenient way of combining them; and
? when multiplying conditional probabilities to com-
pute joint distributions, the accuracy of the result
is crucially dependent on the stability of the con-
ditional estimates across different contexts?this is
important for applications like speech recognition
and machine translation that perform searches over
a large space of output sentences, represented as se-
quences of words.
Given a statistical model that produces a probabilistic
score, a straightforward way of obtaining a true probabil-
ity is to use the score as input to another model whose
output is interpreted as the desired probability. The idea
is that the second model can learn how to transform the
base model?s score by observing its performance on new
text, possibly in conjunction with other features. This ap-
proach, which is known as confidence estimation (CE), is
widely used in speech recognition (Guillevic et al, 2002;
Moreno et al, 2001; Sanchis et al, 2003; Stolcke et al,
1997) but is virtually unknown in other areas of natural
language progessing (NLP). 1
The alternatives to confidence estimation are tradi-
tional smoothing techniques such as backing off to sim-
pler models and cross validation, along with careful
marginalization and scaling where applicable to obtain
the desired posterior probabilities. There is some evi-
dence (Wessel et al, 2001) that this approach can give
results that are at least as good as those obtainable with
an external CE model. However, CE as we present it here
is not incompatible with traditional techniques, and has
several practical advantages. First, it can easily incorpo-
rate specialized features that are highly indicative of how
well the base model will perform on a given input, but
that may be of little use for the task of choosing the out-
put. Since such features may be inconvenient to include
in the base model, CE represents a kind of modulariza-
tion, particularly as it may be possible to reuse some fea-
tures for many different problems. Another advantage is
that a CE layer is usually much smaller and easier to train
than the baseline model; this means that it can be used
to rapidly adapt a system?s performance to new domains.
Finally, CE typically concentrates on only the top few hy-
1A recent exception is Manmatha and Sever (2002), who de-
scribe a form of confidence estimation for combining the results
of different query engines in information retrieval.
potheses output by the baseline model, which is an easier
task than estimating a complete distribution. This is es-
pecially true when the hypotheses of interest are drawn
from a joint distribution that may be impossible in prac-
tice to enumerate.
In this paper we describe an application of confidence
estimation to an interactive target-text prediction task in
a translation setting, using two different types of neural
nets: single-layer perceptron (SLPs) and multi-layer per-
ceptrons (MLPs) with 20 hidden units.
The main issues that we investigate here are:
? the benefit that can be gained by using confidence
estimates, in discrimination power and/or over-all
application quality as computed by a simulation that
estimates the benefit to the user;
? the use of different machine learning (ML) tech-
niques for CE;
? the relevance of various confidence features; and
? model combinations: we experiment with various
model combination schemes based on the CE layer
in order to improve the over-all prediction accuracy
of the application.
Among the more interesting results we will present are
the comparisons between the discrimination capacity of
the native probabilities and the probabilities of correct-
ness produced by the CE layer. Depending on the un-
derlying SMT model, we obtained a relative improve-
ment in correct rejection rate (CR) ranging from 3.90%
to 33.09% at a fixed 0.80 correct acceptance rate (CA)
for prediction lengths of up to four words. We also mea-
sured relative improvements of approximately 10% in es-
timated benefit to the user with our application.
In the following section we briefly describe the text
prediction application we are aiming to improve. Next we
outline the CE approach and the evaluation methods we
applied. Finally, we report the results obtained in our ex-
periments and conclude with suggestions for future work.
2 Text Prediction for Translators
The application we are concerned with in this paper is an
interactive text prediction tool for translators. The sys-
tem observes a translator in the process of typing a target
text and, after every character typed, has the opportunity
to display a suggestion about what will come next, based
on the source sentence under translation and the prefix of
its translation that has already been typed. The transla-
tor may incorporate suggestions into the text if they are
helpful, or simply ignore them and keep typing.
Suggestions may range in length from 0 characters to
the end of the target sentence; it is up to the system to
decide how much text to predict in a given context, bal-
ancing the greater potential benefit of longer predictions
against a greater likelihood of being wrong, and a higher
cost to the user (in terms of distraction and editing) if they
are wrong or only partially right.
Our solution to the problem of how much text to pre-
dict is based on a decision-theoretic framework in which
we attempt to find the prediction that maximizes the ex-
pected benefit to the translator in the current context (Fos-
ter et al, 2002b). Formally, we seek:
x? = argmax
x
B(x|h, s), (1)
where x is a prediction about what will follow h in
the translation of a source sentence s, and B(x|h, s)
is the expected benefit in terms of typing time saved.
As described in (Foster et al, 2002b), B(x?
m
|h, s) =
?
l
k=0
p(k|x, h, s)B(x|h, s, k) depends on two main
quantities: the probability p(k|x, h, s) that exactly k
characters from the beginning of x are correct, and
the benefit B(x|h, s, k) to the translator if this is the
case. B(x|h, s, k) is estimated from a model of user
behaviour?based on data collected in user trials of the
tool?that captures the cost of reading a prediction and
performing any necessary editing, as well as the some-
what random nature of people?s decisions to accept. Pre-
diction probabilities p(k|x, h, s) are derived from a statis-
tical translation model for p(w|h, s), the probability that
some word w will follow the target text h in the transla-
tion of a source sentence s.
Because optimizing (1) directly is expensive, we use
a heuristic search procedure to approximate x?. For each
length m from 1 to a fixed maximum of M (4 in this
paper), we perform a Viterbi-like beam search with the
translation model to find the sequence of words w?
m
=
w
1
, . . . , w
m
most likely to follow h. For each such se-
quence, we form a corresponding character sequence x?
m
and evaluate its benefit B(x?
m
, h, s). The final output is
the prediction x?
m
with maximum benefit, or nothing if
all benefit estimates are negative.
To evaluate the system, we simulate a translator?s ac-
tions on a given source text, using an existing transla-
tion as the text the translator wishes to type, and the user
model to determine his or her responses to predictions
and to estimate the resulting benefit. Further details are
given in (Foster et al, 2002b).
2.1 Translation Models
We experimented with three different translation models
for p(w|h, s). All have the property of being fast enough
to support real-time searches for predictions of up to 5
words.
The first model, referred to as Maxent1 below, is a log-
linear combination of a trigram language model with a
maximum entropy translation component that is an ana-
log of the IBM translation model 2 (Brown et al, 1993).
This model is described in (Foster, 2000). Its major weak-
ness is that it does not keep track of which words in the
current source sentence have already been translated, and
hence it is prone to repeating previous suggestions. The
second model, called Maxent2 below, is similar to Max-
ent1 but with the addition of extra parameters to limit this
behaviour (Foster et al, 2002a).
The final model, called Bayes below, is also described
in (Foster et al, 2002a). It is a noisy-channel combination
of a trigram language model and an IBM model 2 for the
source text given target text. This model has roughly the
same theoretical predictive capability as Maxent2, but un-
like the Maxent models it is not discriminatively trained,
and hence its native probability estimates tend to be much
worse than theirs.
2.2 Computing Smoothed Conditional Probabilities
In order to calculate the character-based probabili-
ties p(k|x, h, s) required for estimating expected ben-
efit, we need to know the conditional probabilities
p(w|w
1
, . . . , w
i?1
, h, s) that some word w will follow
w
1
, . . . , w
i?1
in the context (h, s). These are derived
from correctness estimates obtained from our confidence-
estimation layer as follows. As explained below, es-
timates from the CE layer are in the form p(C =
1|w?
m
, h, s), where w?
m
is the most probable prediction
of length m according to the base translation model.
Define a smoothed joint distribution over predictions of
length m as:
p
s
(w
m
|h, s) =
{
p(C = 1|w?
m
, h, s), w
m
= w?
m
p(w
m
|h, s)/z
m
, else
(2)
where p(w
m
|h, s) =
?
m
i=1
p(w
i
|w
1
, . . . , w
i?1
, h, s) is
calculated from the conditional probabilities given by the
base model; and
z
m
=
1 ? p(w?
m
|h, s)
1 ? p(C = 1|w?
m
, h, s)
is a normalization factor. Then the required smoothed
conditional probabilities are estimated from the smoothed
joint distributions in a straightforward way:
p
s
(w|w
1
, . . . , w
i?1
, h, s) =
p
s
(w
1
, . . . , w
i?1
, w|h, s)
p
s
(w
1
, . . . , w
i?1
|h, s)
,
where p(w
1
, . . . , w
i?1
|h, s) ? 1 when i = 1.
3 Confidence Estimation with Neural Nets
Our approach for CE consists in training neural nets to es-
timate the conditional probability of correctness p(C =
1|w?
m
, h, s, {w1
m
, . . . , wn
m
}), where w?
m
= w1
m
is the
most probable prediction of length m from a n-best set
of alternative predictions according to the base model. In
our experiments the prediction length m varies between
1 and 4 and n is at most 5. As the n-best predictions
{w1
m
, . . . , wn
m
} are themselves a function of the context,
we will simply note the conditional probability of cor-
rectness by p(C = 1|w?
m
, h, s).
We experimented with two types of neural nets: single-
layer perceptrons (SLPs) and multi-layer perceptrons
(MLPs) with 20 hidden units. For both, we used a
softmax activation function and gradient descent train-
ing with a negative log-likelihood error function. Given
suitably-behaved class-conditional feature distributions,
this setup is guaranteed to yield estimates of the true pos-
terior probabilities p(C = 1|w?
m
, h, s) (Bishop, 1995).
3.1 Single Layer Neural Nets and Maximum
Entropy Models
It is interesting to note the relation between the SLP and
maximum entropy models. For the problem of estimating
p(y|x) for a set of classes y over a space of input vectors
x, a single-layer neural net with ?softmax? outputs takes
the form:
p(y|x) = exp(~?
y
? x + b)/Z(x)
where ~?
y
is a vector of weights for class y, b is a bias
term, and Z(x) is a normalization factor, the sum over
all classes of the numerator. A maximum entropy model
is a generalization of this in which an arbitrary feature
function f
y
(x) is used to transform the input space as a
function of y:
p(y|x) = exp(~? ? f
y
(x))/Z(x).
Both models are trained by maximum likelihood meth-
ods. Given C classes, the maximum entropy model can
simulate a SLP by dividing its weight vector into C
blocks, each the size of x, then using f
y
(x) to pick out
the yth block:
f
y
(x) = (0
1
, . . . , 0
y?1
, x, 0
y+1
, . . . , 0
C
, 1),
where each 0
i
is a vector of 0?s and the final 1 yields a
bias term.
The advantage of maximum-entropy models is that
their features can depend on the target class. For natural-
language applications where target classes correspond to
words, this produces an economical and powerful repre-
sentation. However, for CE, where the output is binary
(correct or incorrect), this capacity is less interesting. In
fact, there is no a priori reason to use a different set of
features for correct outputs or incorrect ones, so the nat-
ural form of a maxent model for this problem is identical
to a SLP (modulo a bias term). Therefore the experiments
we describe below can be seen as a comparison between
maxent models and neural nets with a hidden layer.
3.2 Confidence Features
The features we use can be divided into three families:
ones designed to capture the intrinsic difficulty of the
source sentence s (for any NLP task); ones intended to
reflect how hard s is to translate in general, and ones in-
tended to reflect how hard s is for the current model to
translate. For the first two families, we used two sets of
values: static ones that depend on s; and dynamic ones
that depend on only those words in s that are deemed
to be still untranslated, as determined by an IBM2 word
alignment between s and h. The features are:
? family 1: trigram perplexity, minimum trigram word
probability, average word frequency, average word
length, and number of words;
? family 2: average number of translations per source
word (according to an independent IBM1), average
IBM1 source word entropy, number of source tokens
still to be translated, number of unknown source to-
kens, ratio of linked to unlinked source words within
the aligned region of the source sentence, and length
of the current target-text prefix; and
? family 3: average number of search hypotheses
pruned (ie outside the beam) per time step, final
search lattice size, active vocabulary size (number
of target words considered in the search), number of
nbest hypotheses, rank of current hypothesis, prob-
ability ratio of best hypothesis to sum of top 5 hy-
potheses, and base model probability of current pre-
diction.
4 Evaluation
Evaluation is performed using test sets of translation pre-
dictions, each tagged as correct or incorrect. A translation
prediction w
m
is tagged as correct if and only if an iden-
tical word sequence is found in the reference translation,
properly aligned. This reflects our application, where we
attempt to match what a particular translator has in mind,
not simply produce any correct translation. We use two
types of evaluation methods: ROC curves and a user sim-
ulation as described above.
4.1 ROC curves
Consider a set of tokens t
i
? D from given domain D.
Each token t
i
is labelled with a tag C(t
i
) = 1 if it is
considered correct or C(t
i
) = 0 if it is false. Consider a
function s : D ? [a, b] that associates a confidence score
s(t) ? [a, b] to any token t
i
? D. s is not necessarily a
probability, it can range over any real interval [a, b].
Given a rejection threshold ? ? [a, b], any token t
i
?
D is rejected if s(t
i
) < ? and it is accepted otherwise.
The correct acceptence rate CA(?) of a threshold ? over
D is the rate of correct tokens t
i
? D with s(t
i
) ? ?.
That is:
CA(?) =
|{t
i
? D | C(t
i
) = 1 ? s(t
i
) ? ?}|
|{t
i
? D | C(t
i
) = 1}|
. (3)
Similarly, the correct rejection rate CR(?) is the rate
of false tokens t
i
such that s(t
i
) < ?:
CR(?) =
|{t
i
? D | C(t
i
) = 0 ? s(t
i
) < ?}|
|{t
i
? D | C(t
i
) = 0}|
. (4)
As ? ranges over [a, b], the value pairs
(CA(?), CR(?)) ? [0, 1] ? [0, 1] define a curve,
called the ROC curve of s over D. The discrimination
capacity of s is given by its capacity to distinguish
correct from false tokens. Consequently, a perfect ROC
curve would describe the square (0, 1), (1, 1), (1, 0).
This is the case whenever there exists a threshold
? ? [a, b] that separates all correct tokens in D from
all the false ones, meaning that the score ranges of
correct, respectively false, tokens don?t overlap. The
worst case scenario, describing a scoring function that
is completely irrelevent for correct/false discrimination,
corresponds to the diagonal (0, 1), (1, 0). Note that the
inverse of the ideal ROC curve, the plot overlapping the
axes (1, 0), (0, 0), (1, 0) is equivalent to its inverse from
a discrimination capacity point of view: it suffices to
invert the rejection algorithm by accepting all tokens that
have a score inferior to the rejection threshold.
In our setting, the tokens are the w?
m
translation predic-
tions and the score function is the conditional probability
p(C = 1|w?
m
, h, s).
In order to easily compare the discrimination capacity
of various scoring functions we use a raw measure, the
integral of the ROC curve, or IROC. A perfect ROC curve
will have an IROC = 1.0 (respectively 0.0 in the inverse
case). The worst case scenario corresponds to an IROC
of 0.5. We also compare various scoring functions by
fixing an operational point at CA = 0.80 and observing
the corresponding CR values.
5 Experimental Set-up
The data for our experiments originates from the Hansard
English-French parallel corpus. In order to generate the
train and test sets, we use 1.3 million (900000 for train-
ing and 400000 for testing purposes) translation predic-
tions for each fixed prediction length of one, two, three
and four words, summing to a total of 5.2 million pre-
diction examples. Each original SMT model experiment
was combined with two different CE model architectures:
MLPs with one hidden layer containing 20 hidden units
and SLP (sometimes also referred to as MLPs with 0 hid-
den units). Moreover, for each (native model, CE model
architecture)-pair, we train five separate CE models: one
Bayes: m = 1, . . . , 4, CA = 0.80
Model IROC CR
native probability 0.8019 0.6604
SLP 0.8357 0.7211
MLP 0.8679 0.7728
Table 1: Comparison of discrimination capacity between
the Bayes prediction model probability and the CE of the
corresponding SLP and MLP on predictions of up to four
words
for each fixed prediction length of one, two, three or four
words, and an additional model for variable prediction
lengths of up to four words.2
6 ROC Evaluations
In this section we report the ROC evaluation results. The
user-model evaluation results are presented in the follow-
ing section.
6.1 CE and Native SMT Probabilites
The first question we wish to address is whether we can
improve the correct/false discrimination capacity by us-
ing the propability of correctness estimated by the CE
model instead of the native probabilites.
For each SMT model we compare the ROC plots,
IROC and CA/CR values obtained by the native proba-
bility and the estimated probability of correctness output
by the corresponding SLPs (also noted as mlp-0-hu) and
the 20 hidden units MLPs on the one-to-four word pre-
diction task.
Results obtained for various length predictions of up
to four words using the Bayes models are summarized in
figure (1)and in table 1 below, and are encouraging. At a
fixed CA of 0.80 we obtain CR increases from 0.6604 for
the native probability to 0.7211 for the SLP and 0.7728
for the MLP. The over-all gain is also evident from the
the relative improvements in IROC obtained by the SLP
and MLP models over the native probability, that are re-
spectively 17.06% and 33.31%. These results are quite
significant.
Note that the improvements obtained in the fixed-
length 4-word-prediction tasks with the Bayes model (fig-
ure (2) and table 2) model are even larger: the relative
improvements on IROC are 32.36% and 50.07% for the
SLP and the MLP, respectively.
However, the results obtained in the Maxent models
are much less positive: the SLP CR actually drops, while
the MLP CR only increases slightly to a 4.80% relative
2Training and testing of the neural nets was done us-
ing the open-source Torch toolkit ((Collobert et al, 2002),
http://www.torch.ch/), which provides efficient C++ implemen-
tations of many ML algorithms.
Figure 1: Bayes: m = 1, . . . , 4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
CA
CR
mlp?20?hu
mlp?0?hu 
orig. prob.
Bayes: m = 4, CA = 0.80
Model IROC CR
native probability 0.7281 0.4998
SLP 0.8161 0.6602
MLP 0.8560 0.7503
Table 2: Comparison of discrimination capacity between
the Bayes prediction model probability and the CE of the
corresponding SLP and MLP on fixed-length predictions
of four words
Figure 2: Bayes: m = 4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
CA
CR
mlp?20?hu
mlp?0?hu 
orig. prob.
Maxent1: m = 1, . . . , 4, CA = 0.80
Model IROC CR
native probability 0.8581 0.7467
SLP 0.8401 0.7142
MLP 0.8636 0.7561
Table 3: Comparison of discrimination capacity between
the Maxent1 prediction model probability and the CE of
the corresponding SLP and MLP on predictions of up to
four words
Maxent2: m = 1, . . . , 4, CA = 0.80
Model IROC CR
native probability 0.8595 0.7479
SLP 0.8352 0.6973
MLP 0.8638 0.7599
Table 4: Comparison of discrimination capacity between
the Maxent2 prediction model probability and the CE of
the corresponding SLP and MLP on predictions of up to
four words
improvement in the CR rate for the Maxent1 model ( ta-
ble 3) and only 3.9% for the Maxent2 model ( table 4).
The results obtained with the two Maxent models are very
similar. We therefore only draw the ROC curve for the
Maxent2 model (figure (3).
It is interesting to note that the native model predic-
tion accuracy didn?t affect the discrimination capacity of
the corresponding probability of correctness of the CE
models. This result is illustrated in table below, where
%C = 1 is the percentage of correct predictions. Even
though the Bayes? model accuracy and IROC is signifi-
cantly lower then the Maxent model?s, the CE IROC val-
ues are almost identical.
6.2 Relevance of Confidence Features
We investigated the relevance of different confidence fea-
tures by using the IROC values of single-feature models
for the 1?4 word prediction task, with both Maxent1 and
Bayes base models.
The group of features that performs best over both
models are the model- and search-dependent features de-
scribed above, followed by the features that capture the
intrinsic difficulty of the source sentence and the target-
prefix. Least valuable are the remaining features that
capture translation difficulty. The single most significant
feature is native probability, followed by the probability
ratio of the best hypothesis, and the prediction length.
Somewhat unsurprisingly, the weaker Bayes models are
much more sensitive to longer translations than the Max-
ent models.
Figure 3: Maxent2: m = 1, . . . , 4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
CA
CR
mlp?20?hu
mlp?0?hu 
orig. prob.
Discrimination vs. prediction length: Maxent2
Prediction %C=1 IROC native IROC CE
length m probability MLP
m = 1 44.78 0.7926 0.7986
m = 2 23.30 0.8074 0.8121
m = 3 13.12 0.8261 0.8245
m = 4 7.74 0.8517 0.8567
m = 1, ..., 4 22.23 0.8595 0.8638
Table 5: Impact of prediction length on discrimination
capacity and accuracy for the Maxent2 prediction model
6.3 Dealing with predictions of various lengths
We compared different approaches for dealing with vari-
ous length predictions: we trained four separate MLPs for
fixed length predictions of one through four words; and a
single MLP over predictions of varying lengths. Results
are given in table 5 and figure (4)
7 Model Combination
In this section we describe how various model combi-
nations schemes affect prediction accuracy. We use the
Bayes and the Maxent2 prediction models: we try to ex-
ploit the fact that these two models, being fundamentally
different, tend to be complementary in some of their re-
sponses. The CE models we use are the corresponding
MLPs, as they clearly outperform the SLPs. The results
presented in table 6 are reported on the variable-length
prediction task for up to four words.
The combination schemes are the following: we run
the two prediction models in parallel and choose one of
the proposed prediction hypotheses according to the fol-
lowing voting criteria:
? Maximum CE vote: choose the prediction with the
highest CE;
Figure 4: Maxent2: m = 1, m = 2, m = 3, m = 4, m =
1, . . . , 4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
CA
CR
m = 1 
m = 2 
m = 3 
m = 4 
m = 1, ?, 4 
Model Combination Prediction Accuracy
Prediction model combination Accuracy
Bayes alone 8.77
Maxent alone 22.23
Max native probability vote combination 17.49
Max CE vote combination 23.86
Optimal combination 27.79
Table 6: Prediction accuracy of the Bayes and Maxent2
model compared with combined model accuracy
? Maximum native probability vote: choose the pre-
diction with the highest native probability.
As a baseline comparison, we use the accuracy of the
individual native prediction models. Then we compute
the maximum gain we can expect with an optimal model
combination strategy, obtained by running an ?oracle?
that always picks the right answer.
The results are very positive: the maximum CE voting
scheme obtains a 29.31% of the maximum possible ac-
curacy gain over the better of the two indiviual models
(Maxent2). Moreover, if we choose the maximum native
probability vote, the overall accuracy actually drops sig-
nificantly. These results are a strong motivation for our
post-prediction confidence estimation approach: by train-
ing an additional CE layer using the same confidence fea-
tures and training data for different underlying prediction
models we obtain more uniform estimates of the proba-
bility of correctness.
8 User-Model Evaluations
As described in section 2, we evaluated the prediction
system as a whole by simulating the actions of a trans-
lator on a given source text and measuring the gain
model base mults SLP MLP best
Bayes 3.2 6.5 6.4 6.4 11.8
ME1 16.6 16.5 18.1 18.3 23.5
ME2 17.4 17.4 19.0 19.3 24.3
Table 7: Percentage of typing time saved for various CE
configurations.
with a user model. In order to abstract away from ap-
proximations made in deriving character-based proba-
bilities p(k|x, h, s) used in the benefit calculation from
word-based probabilities, we employed a specialized user
model. In contrast to the realistic model described in
(Foster et al, 2002b), this assumes that users accept pre-
dictions only at the beginnings of words, and only when
they are correct in their entirety. To reduce variation fur-
ther, it also assumes that the user always accepts a correct
prediction as soon as it is suggested. Thus the model?s
estimates of benefit to the user may be slightly over-
optimistic: the limited opportunities for accepting and
editing must be balanced against the user?s inhumanly
perfect decision-making. However, its main purpose is
not realism but simply to allow for a fair comparison be-
tween the base and the CE models.
Simulations with all three translation models were per-
formed using a 500-sentence test text. At each prediction
point, the benefits associated with best predictions of 1?4
words in length were compared to decide which (if any)
to propose. The results, in terms of percentages of typing
time saved, are shown in table 8: base corresponds to the
base model; mults to length-specific probability multipli-
ers tuned to optimize benefit on a held-out corpus; SLP
and MLP to CE estimates; and best to using an oracle to
pick the length that maximizes benefit.
Although the CE layer provides no gain over the much
simpler probability-multiplier approach for the Bayes
model, the gain for both maxent models is substantial,
around 10% in relative terms and 25% of the theoretical
maximum gain (over the base model) with the MLP and
slightly lower with the SLP.
9 Conclusion
The results obtained in this paper can be summarized in
the following set of questions and answers:
? Can the probabilities of correctness estimated by
the CE layer exceed the native probablities in dis-
crimination capacity? Depending on the underlying
SMT model, we obtained a relative improvement in
correct rejection rate (CR) ranging from 3.90% to
33.09% at a fixed 0.80 (CA) correct acceptance rate
for prediction lengths varying between 1 and 4.
? Can we improve the overall performance of the un-
derlying SMT application using confidence estima-
tion? In simulated results, we found a significant
gain (10% relative) in benefit to a translator due to
the use of a CE layer in two of three translation mod-
els tested.
? Can prediction accuracy of the application be im-
proved using prediction model combinations? A
maximum CE voting scheme yields a 29.31% ac-
curacy improvement of the maximum possible ac-
curacy gain. A similar voting scheme using native
probabilies significantly decreases the accuracy of
the model combination.
? How does the prediction accuracy of the native mod-
els influence the CE accuracy? Prediction accuracy
didn?t prove to be a significant factor in determining
the discrimination capacity of the confidence esti-
mate.
? How does CE accuracy change with various ML ap-
proches? A multi-layer perceptron (MLP) with 20
hidden units significantly outperformed one with 0
hidden units (equivalent to a maxent model for this
application).
? Confidence feature selection: which confidence fea-
tures are more useful and how does their discrimi-
nation capacity vary with different contexts and dif-
ferent native SMT models? Confidence features
based on the original model and the n-best predic-
tion turned out to be the most relevant group of fea-
tured, folowed by features that capture the intrinsic
difficulty of the source text and finally translation-
difficulty-specific features. We also observed inter-
esting variations in relevance as the original models
changed.
Future work will include the search for more relevant
confidence features, such as features based on consenus
over word-lattices ((Mangu et al, 2000)), past perfor-
mance, the use of more appropriate correct/false tagging
methods and experiments with different machine learning
techniques. Finally, we would like to investigate whether
confidence estimation can be used to improve the model
prediction accuray, either by using re-scoring techniques
or using the confidence estimates during search (decod-
ing).
References
Christopher M. Bishop. 1995. Neural Networks for Pat-
tern Recognition. Oxford.
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathematics
of Machine Translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?312, June.
R. Collobert, S. Bengio, and J. Marie?thoz. 2002. Torch:
a modular machine learning software library. Techni-
cal Report IDIAP-RR 02-46, IDIAP.
George Foster, Philippe Langlais, and Guy Lapalme.
2002a. Text prediction with fuzzy alignments. In
Stephen D. Richardson, editor, Proceedings of the 5th
Conference of the Association for Machine Transla-
tion in the Americas, Tiburon, California, October.
Springer-Verlag.
George Foster, Philippe Langlais, and Guy Lapalme.
2002b. User-friendly text prediction for translators.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Philadelphia, PA.
George Foster. 2000. Incorporating position infor-
mation into a Maximum Entropy / Minimum Di-
vergence translation model. In Proceedings of the
4th Computational Natural Language Learning Work-
shop (CoNLL), Lisbon, Portugal, September. ACL
SigNLL.
Didier Guillevic, Simona Gandrabur, and Yves Nor-
mandin. 2002. Robust semantic confidence scoring.
In Proceedings of the 7th International Conference on
Spoken Language Processing (ICSLP) 2002, Denver,
Colorado, September.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding con-
sensus in speech recognition: word error minimization
and other applications of confusion networks. Com-
puter Speech and Language, 14(4):373?400.
R. Manmatha and H. Sever. 2002. A formal approach
to score normalization for meta-search. In M. Mar-
cus, editor, Proceedings of HLT 2002, Second Inter-
national Conference on Human Language Technology
Research, pages 98?103, San Francisco. Morgan Kauf-
mann.
P. Moreno, B. Logan, and B. Raj. 2001. A boosting
approach for confidence scoring. In Eurospeech.
A. Sanchis, A. Juan, and E. Vidal. 2003. A simple hy-
brid aligner for generating lexical correspondences in
parallel texts. In ICASSP 2003, pages 29?35.
A. Stolcke, Y. Koenig, and M. Weintraub. 1997. Explicit
word error minimization in n-best list rescoring. In
Proc. 5th Eur. Conf. Speech Communication and Tech-
nology, volume 1, pages 163?166.
Frank Wessel, Ralf Schlu?ter, Klaus Macherey, and Her-
mann Ney. 2001. Confidence measures for large vo-
cabulary continuous speech recognition. IEEE Trans-
actions on Speech and Audio Processsing, 9(3):288?
298.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 53?61,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Phrasetable Smoothing for Statistical Machine Translation
George Foster and Roland Kuhn and Howard Johnson
National Research Council Canada
Ottawa, Ontario, Canada
firstname.lastname@nrc.gc.ca
Abstract
We discuss different strategies for smooth-
ing the phrasetable in Statistical MT, and
give results over a range of translation set-
tings. We show that any type of smooth-
ing is a better idea than the relative-
frequency estimates that are often used.
The best smoothing techniques yield con-
sistent gains of approximately 1% (abso-
lute) according to the BLEU metric.
1 Introduction
Smoothing is an important technique in statistical
NLP, used to deal with perennial data sparseness
and empirical distributions that overfit the training
corpus. Surprisingly, however, it is rarely men-
tioned in statistical Machine Translation. In par-
ticular, state-of-the-art phrase-based SMT relies
on a phrasetable?a large set of ngram pairs over
the source and target languages, along with their
translation probabilities. This table, which may
contain tens of millions of entries, and phrases of
up to ten words or more, is an excellent candidate
for smoothing. Yet very few publications describe
phrasetable smoothing techniques in detail.
In this paper, we provide the first system-
atic study of smoothing methods for phrase-based
SMT. Although we introduce a few new ideas,
most methods described here were devised by oth-
ers; the main purpose of this paper is not to in-
vent new methods, but to compare methods. In
experiments over many language pairs, we show
that smoothing yields small but consistent gains in
translation performance. We feel that this paper
only scratches the surface: many other combina-
tions of phrasetable smoothing techniques remain
to be tested.
We define a phrasetable as a set of source
phrases (ngrams) s? and their translations t?, along
with associated translation probabilities p(s?|t?) and
p(t?|s?). These conditional distributions are derived
from the joint frequencies c(s?, t?) of source/target
phrase pairs observed in a word-aligned parallel
corpus.
Traditionally, maximum-likelihood estimation
from relative frequencies is used to obtain con-
ditional probabilities (Koehn et al, 2003), eg,
p(s?|t?) = c(s?, t?)/?s? c(s?, t?) (since the estimation
problems for p(s?|t?) and p(t?|s?) are symmetrical,
we will usually refer only to p(s?|t?) for brevity).
The most obvious example of the overfitting this
causes can be seen in phrase pairs whose con-
stituent phrases occur only once in the corpus.
These are assigned conditional probabilities of 1,
higher than the estimated probabilities of pairs for
which much more evidence exists, in the typical
case where the latter have constituents that co-
occur occasionally with other phrases. During de-
coding, overlapping phrase pairs are in direct com-
petition, so estimation biases such as this one in
favour of infrequent pairs have the potential to sig-
nificantly degrade translation quality.
An excellent discussion of smoothing tech-
niques developed for ngram language models
(LMs) may be found in (Chen and Goodman,
1998; Goodman, 2001). Phrasetable smoothing
differs from ngram LM smoothing in the follow-
ing ways:
? Probabilities of individual unseen events are
not important. Because the decoder only
proposes phrase translations that are in the
phrasetable (ie, that have non-zero count), it
never requires estimates for pairs s?, t? having
53
c(s?, t?) = 0.1 However, probability mass is
reserved for the set of unseen translations,
implying that probability mass is subtracted
from the seen translations.
? There is no obvious lower-order distribution
for backoff. One of the most important tech-
niques in ngram LM smoothing is to com-
bine estimates made using the previous n? 1
words with those using only the previous n?i
words, for i = 2 . . . n. This relies on the
fact that closer words are more informative,
which has no direct analog in phrasetable
smoothing.
? The predicted objects are word sequences
(in another language). This contrasts to LM
smoothing where they are single words, and
are thus less amenable to decomposition for
smoothing purposes.
We propose various ways of dealing with these
special features of the phrasetable smoothing
problem, and give evaluations of their perfor-
mance within a phrase-based SMT system.
The paper is structured as follows: section 2
gives a brief description of our phrase-based SMT
system; section 3 presents the smoothing tech-
niques used; section 4 reviews previous work; sec-
tion 5 gives experimental results; and section 6
concludes and discusses future work.
2 Phrase-based Statistical MT
Given a source sentence s, our phrase-based SMT
system tries to find the target sentence t? that is
the most likely translation of s. To make search
more efficient, we use the Viterbi approximation
and seek the most likely combination of t and its
alignment a with s, rather than just the most likely
t:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-
get phrases such that t = t?1 . . . t?K ; s?k are source
phrases such that s = s?j1 . . . s?jK ; and s?k is the
translation of the kth target phrase t?k.
1This is a first approximation; exceptions occur when dif-
ferent phrasetables are used in parallel, and when rules are
used to translate certain classes of entities.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[
?
i
?ifi(s, t,a)
]
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al, 2001) on a development corpus. The
features used in this study are: the length of t;
a single-parameter distortion penalty on phrase
reordering in a, as described in (Koehn et al,
2003); phrase translation model probabilities; and
trigram language model probabilities log p(t), us-
ing Kneser-Ney smoothing as implemented in the
SRILM toolkit (Stolcke, 2002).
Phrase translation model probabilities are fea-
tures of the form:
log p(s|t,a) ?
K
?
k=1
log p(s?k|t?k)
ie, we assume that the phrases s?k specified by a
are conditionally independent, and depend only on
their aligned phrases t?k. The ?forward? phrase
probabilities p(t?|s?) are not used as features, but
only as a filter on the set of possible translations:
for each source phrase s? that matches some ngram
in s, only the 30 top-ranked translations t? accord-
ing to p(t?|s?) are retained.
To derive the joint counts c(s?, t?) from which
p(s?|t?) and p(t?|s?) are estimated, we use the phrase
induction algorithm described in (Koehn et al,
2003), with symmetrized word alignments gener-
ated using IBM model 2 (Brown et al, 1993).
3 Smoothing Techniques
Smoothing involves some recipe for modifying
conditional distributions away from pure relative-
frequency estimates made from joint counts, in or-
der to compensate for data sparsity. In the spirit of
((Hastie et al, 2001), figure 2.11, pg. 38) smooth-
ing can be seen as a way of combining the relative-
frequency estimate, which is a model with high
complexity, high variance, and low bias, with an-
other model with lower complexity, lower vari-
ance, and high bias, in the hope of obtaining bet-
ter performance on new data. There are two main
ingredients in all such recipes: some probability
distribution that is smoother than relative frequen-
cies (ie, that has fewer parameters and is thus less
54
complex) and some technique for combining that
distribution with relative frequency estimates. We
will now discuss both these choices: the distribu-
tion for carrying out smoothing and the combina-
tion technique. In this discussion, we use p?() to
denote relative frequency distributions.
Choice of Smoothing Distribution
One can distinguish between two approaches to
smoothing phrase tables. Black-box techniques do
not look inside phrases but instead treat them as
atomic objects: that is, both the s? and the t? in the
expression p(s?|t?) are treated as units about which
nothing is known except their counts. In contrast,
glass-box methods break phrases down into their
component words.
The black-box approach, which is the sim-
pler of the two, has received little attention in
the SMT literature. An interesting aspect of
this approach is that it allows one to implement
phrasetable smoothing techniques that are analo-
gous to LM smoothing techniques, by treating the
problem of estimating p(s?|t?) as if it were the prob-
lem of estimating a bigram conditional probabil-
ity. In this paper, we give experimental results
for phrasetable smoothing techniques analogous
to Good-Turing, Fixed-Discount, Kneser-Ney, and
Modified Kneser-Ney LM smoothing.
Glass-box methods for phrasetable smoothing
have been described by other authors: see sec-
tion 3.3. These authors decompose p(s?|t?) into a
set of lexical distributions p(s|t?) by making inde-
pendence assumptions about the words s in s?. The
other possibility, which is similar in spirit to ngram
LM lower-order estimates, is to combine estimates
made by replacing words in t? with wildcards, as
proposed in section 3.4.
Choice of Combination Technique
Although we explored a variety of black-box and
glass-box smoothing distributions, we only tried
two combination techniques: linear interpolation,
which we used for black-box smoothing, and log-
linear interpolation, which we used for glass-box
smoothing.
For black-box smoothing, we could have used a
backoff scheme or an interpolation scheme. Back-
off schemes have the form:
p(s?|t?) =
{
ph(s?|t?), c(s?, t?) ? ?
pb(s?|t?), else
where ph(s?|t?) is a higher-order distribution,
pb(s?|t?) is a smooth backoff distribution, and ? is
a threshold above which counts are considered re-
liable. Typically, ? = 1 and ph(s?|t?) is version of
p?(s?|t?) modified to reserve some probability mass
for unseen events.
Interpolation schemes have the general form:
p(s?|t?) = ?(s?, t?)p?(s?|t?) + ?(s?, t?)pb(s?|t?), (1)
where ? and ? are combining coefficients. As
noted in (Chen and Goodman, 1998), a key
difference between interpolation and backoff is
that the former approach uses information from
the smoothing distribution to modify p?(s?|t?) for
higher-frequency events, whereas the latter uses
it only for low-frequency events (most often 0-
frequency events). Since for phrasetable smooth-
ing, better prediction of unseen (zero-count)
events has no direct impact?only seen events are
represented in the phrasetable, and thus hypoth-
esized during decoding?interpolation seemed a
more suitable approach.
For combining relative-frequency estimates
with glass-box smoothing distributions, we em-
ployed loglinear interpolation. This is the tradi-
tional approach for glass-box smoothing (Koehn
et al, 2003; Zens and Ney, 2004). To illustrate the
difference between linear and loglinear interpola-
tion, consider combining two Bernoulli distribu-
tions p1(x) and p2(x) using each method:
plinear(x) = ?p1(x) + (1? ?)p2(x)
ploglin(x) =
p1(x)?p2(x)
p1(x)?p2(x) + q1(x)?q2(x)
where qi(x) = 1 ? pi(x). Setting p2(x) = 0.5
to simulate uniform smoothing gives ploglin(x) =
p1(x)?/(p1(x)? + q1(x)?). This is actually less
smooth than the original distribution p1(x): it pre-
serves extreme values 0 and 1, and makes inter-
mediate values more extreme. On the other hand,
plinear(x) = ?p1(x) + (1 ? ?)/2, which has the
opposite properties: it moderates extreme values
and tends to preserve intermediate values.
An advantage of loglinear interpolation is that
we can tune loglinear weights so as to maximize
the true objective function, for instance BLEU; re-
call that our translation model is itself loglinear,
with weights set to minimize errors. In fact, a lim-
itation of the experiments described in this paper
is that the loglinear weights for the glass-box tech-
niques were optimized for BLEU using Och?s al-
gorithm (Och, 2003), while the linear weights for
55
black-box techniques were set heuristically. Ob-
viously, this gives the glass-box techniques an ad-
vantage when the different smoothing techniques
are compared using BLEU! Implementing an al-
gorithm for optimizing linear weights according to
BLEU is high on our list of priorities.
The preceding discussion implicitly assumes a
single set of counts c(s?, t?) from which conditional
distributions are derived. But, as phrases of differ-
ent lengths are likely to have different statistical
properties, it might be worthwhile to break down
the global phrasetable into separate phrasetables
for each value of |t?| for the purposes of smooth-
ing. Any similar strategy that does not split up
{s?|c(s?, t?) > 0} for any fixed t? can be applied to
any smoothing scheme. This is another idea we
are eager to try soon.
We now describe the individual smoothing
schemes we have implemented. Four of them
are black-box techniques: Good-Turing and three
fixed-discount techniques (fixed-discount inter-
polated with unigram distribution, Kneser-Ney
fixed-discount, and modified Kneser-Ney fixed-
discount). Two of them are glass-box techniques:
Zens-Ney ?noisy-or? and Koehn-Och-Marcu IBM
smoothing. Our experiments tested not only these
individual schemes, but also some loglinear com-
binations of a black-box technique with a glass-
box technique.
3.1 Good-Turing
Good-Turing smoothing is a well-known tech-
nique (Church and Gale, 1991) in which observed
counts c are modified according to the formula:
cg = (c + 1)nc+1/nc (2)
where cg is a modified count value used to replace
c in subsequent relative-frequency estimates, and
nc is the number of events having count c. An
intuitive motivation for this formula is that it ap-
proximates relative-frequency estimates made by
successively leaving out each event in the corpus,
and then averaging the results (Na?das, 1985).
A practical difficulty in implementing Good-
Turing smoothing is that the nc are noisy for large
c. For instance, there may be only one phrase
pair that occurs exactly c = 347, 623 times in a
large corpus, and no pair that occurs c = 347, 624
times, leading to cg(347, 623) = 0, clearly not
what is intended. Our solution to this problem
is based on the technique described in (Church
and Gale, 1991). We first take the log of the ob-
served (c, nc) values, and then use a linear least
squares fit to log nc as a function of log c. To en-
sure that the result stays close to the reliable values
of nc for large c, error terms are weighted by c, ie:
c(log nc? log n?c)2, where n?c are the fitted values.
Our implementation pools all counts c(s?, t?) to-
gether to obtain n?c (we have not yet tried separate
counts based on length of t? as discussed above). It
follows directly from (2) that the total count mass
assigned to unseen phrase pairs is cg(0)n0 = n1,
which we approximate by n?1. This mass is dis-
tributed among contexts t? in proportion to c(t?),
giving final estimates:
p(s?|t?) = cg(s?, t?)?
s cg(s?, t?) + p(t?)n?1
,
where p(t?) = c(t?)/?t? c(t?).
3.2 Fixed-Discount Methods
Fixed-discount methods subtract a fixed discount
D from all non-zero counts, and distribute the re-
sulting probability mass according to a smoothing
distribution (Kneser and Ney, 1995). We use an
interpolated version of fixed-discount proposed by
(Chen and Goodman, 1998) rather than the origi-
nal backoff version. For phrase pairs with non-
zero counts, this distribution has the general form:
p(s?|t?) = c(s?, t?)?D?
s? c(s?, t?)
+ ?(t?)pb(s?|t?), (3)
where pb(s?|t?) is the smoothing distribution. Nor-
malization constraints fix the value of ?(t?):
?(t?) = D n1+(?, t?)/
?
s?
c(s?, t?),
where n1+(?, t?) is the number of phrases s? for
which c(s?, t?) > 0.
We experimented with two choices for the
smoothing distribution pb(s?|t?). The first is a plain
unigram p(s?), and the second is the Kneser-Ney
lower-order distribution:
pb(s?) = n1+(s?, ?)/
?
s?
n1+(s?, ?),
ie, the proportion of unique target phrases that s? is
associated with, where n1+(s?, ?) is defined anal-
ogously to n1+(?, t?). Intuitively, the idea is that
source phrases that co-occur with many different
56
target phrases are more likely to appear in new
contexts.
For both unigram and Kneser-Ney smoothing
distributions, we used a discounting coefficient de-
rived by (Ney et al, 1994) on the basis of a leave-
one-out analysis: D = n1/(n1 + 2n2). For the
Kneser-Ney smoothing distribution, we also tested
the ?Modified Kneser-Ney? extension suggested
in (Chen and Goodman, 1998), in which specific
coefficients Dc are used for small count values
c up to a maximum of three (ie D3 is used for
c ? 3). For c = 2 and c = 3, we used formu-
las given in that paper.
3.3 Lexical Decomposition
The two glass-box techniques that we considered
involve decomposing source phrases with inde-
pendence assumptions. The simplest approach as-
sumes that all source words are conditionally in-
dependent, so that:
p(s?|t?) =
J?
?
j=1
p(sj|t?)
We implemented two variants for p(sj|t?) that
are described in previous work. (Zens and Ney,
2004) describe a ?noisy-or? combination:
p(sj |t?) = 1? p(s?j |t?)
? 1?
I?
?
i=1
(1? p(sj |ti))
where s?j is the probability that sj is not in the
translation of t?, and p(sj|ti) is a lexical proba-
bility. (Zens and Ney, 2004) obtain p(sj|ti) from
smoothed relative-frequency estimates in a word-
aligned corpus. Our implementation simply uses
IBM1 probabilities, which obviate further smooth-
ing.
The noisy-or combination stipulates that sj
should not appear in s? if it is not the translation
of any of the words in t?. The complement of this,
proposed in (Koehn et al, 2005), to say that sj
should appear in s? if it is the translation of at least
one of the words in t?:
p(sj|t?) =
?
i?Aj
p(sj |ti)/|Aj |
where Aj is a set of likely alignment connections
for sj . In our implementation of this method,
we assumed that Aj = {1, . . . , I?}, ie the set of
all connections, and used IBM1 probabilities for
p(s|t).
3.4 Lower-Order Combinations
We mentioned earlier that LM ngrams have a
naturally-ordered sequence of smoothing distribu-
tions, obtained by successively dropping the last
word in the context. For phrasetable smoothing,
because no word in t? is a priori less informative
than any others, there is no exact parallel to this
technique. However, it is clear that estimates made
by replacing particular target (conditioning) words
with wildcards will be smoother than the original
relative frequencies. A simple scheme for combin-
ing them is just to average:
p(s?|t?) =
?
i=I?
c?i (s?, t?)
?
s? c?i (s?, t?)
/I?
where:
c?i (s?, t?) =
?
ti
c(s?, t1 . . . ti . . . tI?).
One might also consider progressively replacing
the least informative remaining word in the target
phrase (using tf-idf or a similar measure).
The same idea could be applied in reverse, by
replacing particular source (conditioned) words
with wildcards. We have not yet implemented
this new glass-box smoothing technique, but it has
considerable appeal. The idea is similar in spirit to
Collins? backoff method for prepositional phrase
attachment (Collins and Brooks, 1995).
4 Related Work
As mentioned previously, (Chen and Goodman,
1998) give a comprehensive survey and evalua-
tion of smoothing techniques for language mod-
eling. As also mentioned previously, there is
relatively little published work on smoothing for
statistical MT. For the IBM models, alignment
probabilities need to be smoothed for combina-
tions of sentence lengths and positions not encoun-
tered in training data (Garc??a-Varea et al, 1998).
Moore (2004) has found that smoothing to cor-
rect overestimated IBM1 lexical probabilities for
rare words can improve word-alignment perfor-
mance. Langlais (2005) reports negative results
for synonym-based smoothing of IBM2 lexical
probabilities prior to extracting phrases for phrase-
based SMT.
For phrase-based SMT, the use of smoothing to
avoid zero probabilities during phrase induction is
reported in (Marcu and Wong, 2002), but no de-
tails are given. As described above, (Zens and
57
Ney, 2004) and (Koehn et al, 2005) use two dif-
ferent variants of glass-box smoothing (which they
call ?lexical smoothing?) over the phrasetable, and
combine the resulting estimates with pure relative-
frequency ones in a loglinear model. Finally, (Cet-
tollo et al, 2005) describes the use of Witten-Bell
smoothing (a black-box technique) for phrasetable
counts, but does not give a comparison to other
methods. As Witten-Bell is reported by (Chen and
Goodman, 1998) to be significantly worse than
Kneser-Ney smoothing, we have not yet tested this
method.
5 Experiments
We carried out experiments in two different set-
tings: broad-coverage ones across six European
language pairs using selected smoothing tech-
niques and relatively small training corpora; and
Chinese to English experiments using all im-
plemented smoothing techniques and large train-
ing corpora. For the black-box techniques,
the smoothed phrase table replaced the original
relative-frequency (RF) phrase table. For the
glass-box techniques, a phrase table (either the
original RF phrase table or its replacement after
black-box smoothing) was interpolated in loglin-
ear fashion with the smoothing glass-box distribu-
tion, with weights set to maximize BLEU on a de-
velopment corpus.
To estimate the significance of the results across
different methods, we used 1000-fold pairwise
bootstrap resampling at the 95% confidence level.
5.1 Broad-Coverage Experiments
In order to measure the benefit of phrasetable
smoothing for relatively small corpora, we used
the data made available for the WMT06 shared
task (WMT, 2006). This exercise is conducted
openly with access to all needed resources and
is thus ideal for benchmarking statistical phrase-
based translation systems on a number of language
pairs.
The WMT06 corpus is based on sentences ex-
tracted from the proceedings of the European Par-
liament. Separate sentence-aligned parallel cor-
pora of about 700,000 sentences (about 150MB)
are provided for the three language pairs hav-
ing one of French, Spanish and German with En-
glish. SRILM language models based on the same
source are also provided for each of the four lan-
guages. We used the provided 2000-sentence dev-
sets for tuning loglinear parameters, and tested on
the 3064-sentence test sets.
Results are shown in table 1 for relative-
frequency (RF), Good-Turing (GT), Kneser-Ney
with 1 (KN1) and 3 (KN3) discount coefficients;
and loglinear combinations of both RF and KN3
phrasetables with Zens-Ney-IBM1 (ZN-IBM1)
smoothed phrasetables (these combinations are
denoted RF+ZN-IBM1 and KN3+ZN-IBM1).
It is apparent from table 1 that any kind of
phrase table smoothing is better than using none;
the minimum improvement is 0.45 BLEU, and
the difference between RF and all other meth-
ods is statistically significant. Also, Kneser-
Ney smoothing gives a statistically significant im-
provement over GT smoothing, with a minimum
gain of 0.30 BLEU. Using more discounting co-
efficients does not appear to help. Smoothing
relative frequencies with an additional Zens-Ney
phrasetable gives about the same gain as Kneser-
Ney smoothing on its own. However, combining
Kneser-Ney with Zens-Ney gives a clear gain over
any other method (statistically significant for all
language pairs except en?es and en?de) demon-
strating that these approaches are complementary.
5.2 Chinese-English Experiments
To test the effects of smoothing with larger
corpora, we ran a set of experiments for
Chinese-English translation using the corpora
distributed for the NIST MT05 evaluation
(www.nist.gov/speech/tests/mt). These are sum-
marized in table 2. Due to the large size of
the out-of-domain UN corpus, we trained one
phrasetable on it, and another on all other parallel
corpora (smoothing was applied to both). We also
used a subset of the English Gigaword corpus to
augment the LM training material.
corpus use sentences
non-UN phrasetable1 + LM 3,164,180
UN phrasetable2 + LM 4,979,345
Gigaword LM 11,681,852
multi-p3 dev 993
eval-04 test 1788
Table 2: Chinese-English Corpora
Table 3 contains results for the Chinese-English
experiments, including fixed-discount with uni-
gram smoothing (FDU), and Koehn-Och-Marcu
smoothing with the IBM1 model (KOM-IBM1)
58
smoothing method fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
RF 25.35 27.25 20.46 27.20 27.18 14.60
GT 25.95 28.07 21.06 27.85 27.96 15.05
KN1 26.83 28.66 21.36 28.62 28.71 15.42
KN3 26.84 28.69 21.53 28.64 28.70 15.40
RF+ZN-IBM1 26.84 28.63 21.32 28.84 28.45 15.44
KN3+ZN-IBM1 27.25 29.30 21.77 29.00 28.86 15.49
Table 1: Broad-coverage results
as described in section 3.3. As with the
broad-coverage experiments, all of the black-box
smoothing techniques do significantly better than
the RF baseline. However, GT appears to work
better in the large-corpus setting: it is statistically
indistinguishable from KN3, and both these meth-
ods are significantly better than all other fixed-
discount variants, among which there is little dif-
ference.
Not surprisingly, the two glass-box methods,
ZN-IBM1 and KOM-IBM1, do poorly when used
on their own. However, in combination with an-
other phrasetable, they yield the best results, ob-
tained by RF+ZN-IBM1 and GT+KOM-IBM1,
which are statistically indistinguishable. In con-
strast to the situation in the broad-coverage set-
ting, these are not significantly better than the
best black-box method (GT) on its own, although
RF+ZN-IBM1 is better than all other glass-box
combinations.
smoothing method BLEU score
RF 29.85
GT 30.66
FDU 30.23
KN1 30.29
KN2 30.13
KN3 30.54
ZN-IBM1 29.55
KOM-IBM1 28.09
RF+ZN-IBM1 30.95
RF+KOM-IBM1 30.10
GT+ZN-IBM1 30.45
GT+KOM-IBM1 30.81
KN3+ZN-IBM1 30.66
Table 3: Chinese-English Results
A striking difference between the broad-
coverage setting and the Chinese-English setting
is that in the former it appears to be beneficial
to apply KN3 smoothing to the phrasetable that
gets combined with the best glass-box phrasetable
(ZN), whereas in the latter setting it does not. To
test whether this was due to corpus size (as the
broad-coverage corpora are around 10% of those
for Chinese-English), we calculated Chinese-
English learning curves for the RF+ZN-IBM1 and
KN3-ZN-IBM1 methods, shown in figure 1. The
results are somewhat inconclusive: although the
KN3+ZN-IBM1 curve is perhaps slightly flatter,
the most obvious characteristic is that this method
appears to be highly sensitive to the particular cor-
pus sample used.
 0.25
 0.255
 0.26
 0.265
 0.27
 0.275
 0.28
 0.285
 0.29
 0.295
 0.3
 0  10  20  30  40  50  60  70  80
B
L
E
U
proportion of corpus
Learning curves for smoothing methods
RF+ZN-IBM1
KN3+ZN-IBM1
Figure 1: Learning curves for two glass-box com-
binations.
6 Conclusion and Future Work
We tested different phrasetable smoothing tech-
niques in two different translation settings: Eu-
ropean language pairs with relatively small cor-
pora, and Chinese to English translation with large
corpora. The smoothing techniques fall into two
59
categories: black-box methods that work only on
phrase-pair counts; and glass-box methods that de-
compose phrase probabilities into lexical proba-
bilities. In our implementation, black-box tech-
niques use linear interpolation to combine relative
frequency estimates with smoothing distributions,
while glass-box techniques are combined in log-
linear fashion with either relative-frequencies or
black-box estimates.
All smoothing techniques tested gave statisti-
cally significant gains over pure relative-frequency
estimates. In the small-corpus setting, the best
technique is a loglinear combination of Kneser-
Ney count smoothing with Zens-Ney glass-box
smoothing; this yields an average gain of 1.6
BLEU points over relative frequencies. In the
large-corpus setting, the best technique is a log-
linear combination of relative-frequency estimates
with Zens-Ney smoothing, with a gain of 1.1
BLEU points. Of the two glass-box smoothing
methods tested, Zens-Ney appears to have a slight
advantage over Koehn-Och-Marcu. Of the black-
box methods tested, Kneser-Ney is clearly bet-
ter for small corpora, but is equivalent to Good-
Turing for larger corpora.
The paper describes several smoothing alterna-
tives which we intend to test in future work:
? Linear versus loglinear combinations (in our
current work, these coincide with the black-
box versus glass-box distinction, making it
impossible to draw conclusions).
? Lower-order distributions as described in sec-
tion 3.4.
? Separate count-smoothing bins based on
phrase length.
7 Acknowledgements
The authors would like to thank their colleague
Michel Simard for stimulating discussions. The
first author would like to thank all his colleagues
for encouraging him to taste a delicacy that was
new to him (shredded paper with maple syrup).
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-
0023. Any opinions, findings and conclusions or
recommendations expressed in this material are
those of the author(s) and do not necessarily re-
flect the views of the Defense Advanced Research
Projects Agency (DARPA).
References
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993. The
mathematics of Machine Translation: Parameter es-
timation. Computational Linguistics, 19(2):263?
312, June.
M. Cettollo, M. Federico, N. Bertoldi, R. Cattoni, and
B. Chen. 2005. A look inside the ITC-irst SMT
system. In Proceedings of MT Summit X, Phuket,
Thailand, September. International Association for
Machine Translation.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University.
K. Church and W. Gale. 1991. A comparison of the
enhanced Good-Turing and deleted estimation meth-
ods for estimating probabilities of English bigrams.
Computer speech and language, 5(1):19?54.
M. Collins and J. Brooks. 1995. Prepositional phrase
attachment through a backed-off model. In Proceed-
ings of the 3rd ACL Workshop on Very Large Cor-
pora (WVLC), Cambridge, Massachusetts.
Ismael Garc??a-Varea, Francisco Casacuberta, and Her-
mann Ney. 1998. An iterative, DP-based search al-
gorithm for statistical machine translation. In Pro-
ceedings of the 5th International Conference on Spo-
ken Language Processing (ICSLP) 1998, volume 4,
pages 1135?1138, Sydney, Australia, December.
Joshua Goodman. 2001. A bit of progress in language
modeling. Computer Speech and Language.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2001. The Elements of Statistical Learning.
Springer.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP) 1995,
pages 181?184, Detroit, Michigan. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Ed-
uard Hovy, editor, Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 127?133, Edmonton, Alberta,
Canada, May. NAACL.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, D. Talbot, and M. White. 2005. Ed-
inburgh system description for the 2005 NIST MT
evaluation. In Proceedings of Machine Translation
Evaluation Workshop.
Philippe Langlais, Guihong Cao, and Fabrizio Gotti.
2005. RALI: SMT shared task system description.
60
In Proceedings of the 2nd ACL workshop on Build-
ing and Using Parallel Texts, pages 137?140, Uni-
versity of Michigan, Ann Arbor, June.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Philadelphia, PA.
Robert C. Moore. 2004. Improving IBM word-
alignment model 1. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Barcelona, July.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 10:1?38.
Arthur Na?das. 1985. On Turing?s formula for
word probabilities. IEEE Transactions on Acous-
tics, Speech and Signal Processing (ASSP), ASSP-
33(6):1415?1417, December.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Meeting of the Association for
Computational Linguistics (ACL), Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Re-
port RC22176, IBM, September.
Andreas Stolcke. 2002. SRILM - an extensi-
ble language modeling toolkit. In Proceedings of
the 7th International Conference on Spoken Lan-
guage Processing (ICSLP) 2002, Denver, Colorado,
September.
WMT. 2006. The NAACL Workshop on Statistical
Machine Translation (www.statmt.org/wmt06), New
York, June.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proceedings of Human Language Technol-
ogy Conference / North American Chapter of the
ACL, Boston, May.
61
 
		Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 913?920
Manchester, August 2008
 
	
	ffProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 967?975, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Translation Quality by Discarding Most of the Phrasetable
J Howard Johnson and Joel Martin
Interactive Information Group
National Research Council Canada
Ottawa, Ontario, Canada
firstname.lastname@nrc.gc.ca
George Foster and Roland Kuhn
Interactive Language Technologies Group
National Research Council Canada
Gatineau, Que?bec, Canada
firstname.lastname@nrc.gc.ca
Abstract
It is possible to reduce the bulk of phrase-
tables for Statistical Machine Translation us-
ing a technique based on the significance
testing of phrase pair co-occurrence in the
parallel corpus. The savings can be quite
substantial (up to 90%) and cause no reduc-
tion in BLEU score. In some cases, an im-
provement in BLEU is obtained at the same
time although the effect is less pronounced
if state-of-the-art phrasetable smoothing is
employed.
1 Introduction
An important part of the process of Statistical Ma-
chine Translation (SMT) involves inferring a large
table of phrase pairs that are translations of each
other from a large corpus of aligned sentences.
These phrase pairs together with estimates of con-
ditional probabilities and useful feature weights,
called collectively a phrasetable, are used to match
a source sentence to produce candidate translations.
The choice of the best translation is made based
on the combination of the probabilities and feature
weights, and much discussion has been made of how
to make the estimates of probabilites, how to smooth
these estimates, and what features are most useful
for discriminating among the translations.
However, a cursory glance at phrasetables pro-
duced often suggests that many of the translations
are wrong or will never be used in any translation.
On the other hand, most obvious ways of reducing
the bulk usually lead to a reduction in translation
quality as measured by BLEU score. This has led to
an impression that these pairs must contribute some-
thing in the grand scheme of things and, certainly,
more data is better than less.
Nonetheless, this bulk comes at a cost. Large ta-
bles lead to large data structures that require more
resources and more time to process and, more im-
portantly, effort directed in handling large tables
could likely be more usefully employed in more fea-
tures or more sophisticated search.
In this paper, we show that it is possible to prune
phrasetables using a straightforward approach based
on significance testing, that this approach does not
adversely affect the quality of translation as mea-
sured by BLEU score, and that savings in terms of
number of discarded phrase pairs can be quite sub-
stantial. Even more surprising, pruning can actu-
ally raise the BLEU score although this phenomenon
is less prominent if state of the art smoothing of
phrasetable probabilities is employed.
Section 2 reviews the basic ideas of Statistical
Machine Translation as well as those of testing sig-
nificance of associations in two by two contingency
tables departing from independence. From this, a
filtering algorithm will be described that keeps only
phrase pairs that pass a significance test. Section 3
outlines a number of experiments that demonstrate
the phenomenon and measure its magnitude. Sec-
tion 4 presents the results of these experiments. The
paper concludes with a summary of what has been
learned and a discussion of continuing work that
builds on these ideas.
967
2 Background Theory
2.1 Our Approach to Statistical Machine
Translation
We define a phrasetable as a set of source phrases (n-
grams) s? and their translations (m-grams) t?, along
with associated translation probabilities p(s?|t?) and
p(t?|s?). These conditional distributions are derived
from the joint frequencies c(s?, t?) of source / tar-
get n,m-grams observed in a word-aligned parallel
corpus. These joint counts are estimated using the
phrase induction algorithm described in (Koehn et
al., 2003), with symmetrized word alignments gen-
erated using IBM model 2 (Brown et al, 1993).
Phrases are limited to 8 tokens in length (n,m ? 8).
Given a source sentence s, our phrase-based SMT
system tries to find the target sentence t? that is the
most likely translation of s. To make search more
efficient, we use the Viterbi approximation and seek
the most likely combination of t and its alignment a
with s, rather than just the most likely t:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-
get phrases such that t = t?1...t?K ; s?k are source
phrases such that s = s?j1 ...s?jK ; and s?k is the trans-
lation of the kth target phrase t?k.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[
?
i
?ifi(s, t,a)
]
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al , 2001) on a development corpus. The
features used are: the length of t; a single-parameter
distortion penalty on phrase reordering in a, as de-
scribed in (Koehn et al, 2003); phrase translation
model probabilities; and 4-gram language model
probabilities log p(t), using Kneser-Ney smooth-
ing as implemented in the SRILM toolkit (Stolcke,
2002).
Phrase translation model probabilities are features
of the form:
log p(s|t,a) ?
K?
k=1
log p(s?k|t?k)
i.e., we assume that the phrases s?k specified by a are
conditionally independent, and depend only on their
aligned phrases t?k.
The ?forward? phrase probabilities p(t?|s?) are not
used as features, but only as a filter on the set of
possible translations: for each source phrase s? that
matches some ngram in s, only the 30 top-ranked
translations t? according to p(t?|s?) are retained. One
of the reviewers has pointed out correctly that tak-
ing only the top 30 translations will interact with the
subject under study; however, this pruning technique
has been used as a way of controlling the width of
our beam search and rebalancing search parameters
would have complicated this study and taken it away
from our standard practice.
The phrase translation model probabilities are
smoothed according to one of several techniques as
described in (Foster et al, 2006) and identified in the
discussion below.
2.2 Significance testing using two by two
contingency tables
Each phrase pair can be thought of as am n,m-gram
(s?, t?) where s? is an n-gram from the source side of
the corpus and t? is an m-gram from the target side
of the corpus.
We then define: C(s?, t?) as the number of parallel
sentences that contain one or more occurrences of
s? on the source side and t? on the target side; C(s?)
the number of parallel sentences that contain one or
more occurrences of s? on the source side; and C(t?)
the number of parallel sentences that contain one or
more occurrences of t? on the target side. Together
with N , the number of parallel sentences, we have
enough information to draw up a two by two contin-
gency table representing the unconditional relation-
ship between s? and t?. This table is shown in Table
1.
A standard statistical technique used to assess the
importance of an association represented by a con-
tingency table involves calculating the probability
that the observed table or one that is more extreme
could occur by chance assuming a model of inde-
pendence. This is called a significance test. Intro-
ductory statistics texts describe one such test called
the Chi-squared test.
There are other tests that more accurately apply
to our small tables with only two rows and columns.
968
Table 1: Two by two contingency table for s? and t?
C(s?, t?) C(s?)? C(s?, t?) C(s?)
C(t?)? C(s?, t?) N ? C(s?)? C(t?) + C(s?, t?) N ? C(s?)
C(t?) N ? C(t?) N
In particular, Fisher?s exact test calculates probabil-
ity of the observed table using the hypergeometric
distibution.
ph(C(s?, t?)) =
(
C(s?)
C(s?, t?)
)(
N ? C(s?)
C(t?)? C(s?, t?)
)
(
N
C(t?)
)
The p-value associated with our observed table is
then calculated by summing probabilities for tables
that have a larger C(s?, t?)).
p-value(C(s?, t?)) =
??
k=C(s?,t?)
ph(k)
This probability is interpreted as the probability
of observing by chance an association that is at least
as strong as the given one and hence its significance.
Agresti (1996) provides an excellent introduction to
this topic and the general ideas of significance test-
ing in contingency tables.
Fisher?s exact test of significance is considered a
gold standard since it represents the precise proba-
bilities under realistic assumptions. Tests such as the
Chi-squared test or the log-likelihood-ratio test (yet
another approximate test of significance) depend on
asymptotic assumptions that are often not valid for
small counts.
Note that the count C(s?, t?) can be larger or
smaller than c(s?, t?) discussed above. In most cases,
it will be larger, because it counts all co-occurrences
of s? with t? rather than just those that respect the
word alignment. It can be smaller though because
multiple co-occurrences can occur within a single
aligned sentence pair and be counted multiple times
in c(s?, t?). On the other hand, C(s?, t?) will not count
all of the possible ways that an n,m-grammatch can
occur within a single sentence pair; it will count the
match only once per sentence pair in which it occurs.
Moore (2004) discusses the use of signifi-
cance testing of word associations using the log-
likelihood-ratio test and Fisher?s exact test. He
shows that Fisher?s exact test is often a practical
method if a number of techniques are followed:
1. approximating the logarithms of factorials us-
ing commonly available numerical approxima-
tions to the log gamma function,
2. using a well-known recurrence for the hyperge-
ometic distribution,
3. noting that few terms usually need to be
summed, and
4. observing that convergence is usually rapid.
2.3 Significance pruning
The idea behind significance pruning of phrasetables
is that not all of the phrase pairs in a phrasetable are
equally supported by the data and that many of the
weakly supported pairs could be removed because:
1. the chance of them occurring again might be
low, and
2. their occurrence in the given corpus may be the
result of an artifact (a combination of effects
where several estimates artificially compensate
for one another). This concept is usually re-
ferred to as overfit since the model fits aspects
of the training data that do not lead to improved
prediction.
Phrase pairs that cannot stand on their own by
demonstrating a certain level of significance are sus-
pect and removing them from the phrasetable may
969
be beneficial in terms of reducing the size of data
structures. This will be shown to be the case in rather
general terms.
Note that this pruning may and quite often will
remove all of the candidate translations for a source
phrase. This might seem to be a bad idea but it must
be remembered that deleting longer phrases will al-
low combinations of shorter phrases to be used and
these might have more and better translations from
the corpus. Here is part of the intuition about how
phrasetable smoothing may interact with phrasetable
pruning: both are discouraging longer but infrequent
phrases from the corpus in favour of combinations of
more frequent, shorter phrases.
Because the probabilities involved below will be
so incredibly tiny, we will work instead with the neg-
ative of the natural logs of the probabilities. Thus
instead of selecting phrase pairs with a p-value less
than exp(?20), we will select phrase pairs with a
negative-log-p-value greater than 20. This has the
advantage of working with ordinary-sized numbers
and the happy convention that bigger means more
pruning.
2.4 C(s?, t?) = 1, 1-1-1 Tables and the ?
Threshold
An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and
each of the component phrases occurs exactly once
in its side of the parallel corpus.
These phrase pairs will be referred to as 1-1-1
phrase pairs and the corresponding tables will be
called 1-1-1 contingency tables because C(s?) = 1,
C(t?) = 1, and C(s?, t?) = 1.
Moore (2004) comments that the p-value for these
tables under Fisher?s exact test is 1/N . Since we are
using thresholds of the negative logarithm of the p-
value, the value ? = log(N) is a useful threshold to
consider.
In particular, ? +  (where  is an appropriately
small positive number) is the smallest threshold that
results in none of the 1-1-1 phrase pairs being in-
cluded. Similarly, ? ?  is the largest threshold that
results in all of the 1-1-1 phrase pairs being included.
Because 1-1-1 phrase pairs can make up a large part
of the phrase table, this is important observation for
its own sake.
Since the contingency table with C(s?, t?) = 1 hav-
ing the greatest significance (lowest p-value) is the
1-1-1 table, using the threshold of ?+  can be used
to exclude all of the phrase pairs occurring exactly
once (C(s?, t?) = 1).
The common strategy of deleting all of the 1-
count phrase pairs is very similar in effect to the use
of the ? +  threshold.
3 Experiments
3.1 WMT06
The corpora used for most of these experiments are
publicly available and have been used for a num-
ber of comparative studies (Workshop on Statisti-
cal Machine Translation, 2006). Provided as part of
the materials for the shared task are parallel corpora
for French?English, Spanish?English, and German?
English as well as language models for English,
French, Spanish, and German. These are all based
on the Europarl resources (Europarl, 2003).
The only change made to these corpora was to
convert them to lowercase and to Unicode UTF-8.
Phrasetables were produced by symmetrizing IBM2
conditional probabilities as described above.
The phrasetables were then used as a list of
n,m-grams for which counts C(s?, t?), C(s?), and
C(t?) were obtained. Negative-log-p-values under
Fisher?s exact test were computed for each of the
phrase pairs in the phrasetable and the entry was
censored if the negative-log-p-value for the test was
below the pruning threshold. The entries that are
kept are ones that are highly significant.
A number of combinations involving many differ-
ent pruning thresholds were considered: no pruning,
10, ??, ?+, 15, 20, 25, 50, 100, and 1000. In ad-
dition, a number of different phrasetable smoothing
algorithms were used: no smoothing, Good-Turing
smoothing, Kneser-Ney 3 parameter smoothing and
the loglinear mixture involving two features called
Zens-Ney (Foster et al, 2006).
3.2 Chinese
To test the effects of significance pruning on larger
corpora, a series of experiments was run on a much
larger corpus based on that distributed for MT06
Chinese?English (NIST MT, 2006). Since the ob-
jective was to assess how the method scaled we used
our preferred phrasetable smoothing technique of
970
1000100101
BLEU by Pruning Threshold
no smoothing
3
3
333 3
3
3
3
GT (+1)
+ +
+++ +
+
+
+
KN3 (+2)
2 2222 2
2
2
2
ZN (+3)
? ???? ?
?
?
?
107
106
105
1000100101
Phrasetable Size by Pruning Threshold
size3 3
333
3
3
3
3
107106105
BLEU by Phrasetable Size
no smoothing
3
3
3333
3
3
3
GT (+1)
++
++++
+
+
+
KN3 (+2)
222222
2
2
2
ZN (+3)
??????
?
?
?
Figure 1: WMT06: Results for French ?? English.
[to separate the curves, graphs for smoothed meth-
ods are shifted by +1, +2, or +3 BLEU points]
Table 2: Corpus Sizes and ? Values
number of
parallel sentences ?
WMT06: fr?? en 688,031 13.4415892
WMT06: es?? en 730,740 13.501813
WMT06: de?? en 751,088 13.5292781
Chinese?English: best 3,164,228 14.9674197
Chinese?English: UN-v2 4,979,345 15.4208089
Zens-Ney and separated our corpus into two phrase-
tables, one based on the UN corpus and the other
based on the best of the remaining parallel corpora
available to us.
Different pruning thresholds were considered: no
pruning, 14, 16, 18, 20, and 25. In addition, another
more aggressive method of pruning was attempted.
Moore points out, correctly, that phrase pairs that oc-
cur in only one sentence pair, (C(s?, t?) = 1 ), are less
reliable and might require more special treatment.
These are all pruned automatically at thresholds of
16 and above but not at threshold of 14. A spe-
cial series of runs was done for threshold 14 with all
of these singletons removed to see whether at these
thresholds it was the significance level or the prun-
ing of phrase pairs with (C(s?, t?) = 1 ) that was more
important. This is identified as 14? in the results.
4 Results
The results of the experiments are described in Ta-
bles 2 through 6.
Table 2 presents the sizes of the various parallel
corpora showing the number of parallel sentences,
N , for each of the experiments, together with the ?
thresholds (? = log(N)).
Table 3 shows the sizes of the phrasetables that
result from the various pruning thresholds described
for the WMT06 data. It is clear that this is extremely
aggressive pruning at the given levels.
Table 4 shows the corresponding phrasetable sizes
for the large corpus Chinese?English data. The
pruning is not as aggressive as for the WMT06 data
but still quite sizeable.
Tables 5 and 6 show the main results for the
WMT06 and the Chinese?English large corpus ex-
periments. To make these results more graphic, Fig-
ure 1 shows the French ?? English data from the
WMT06 results in the form of three graphs. Note
971
Table 3: WMT06: Distinct phrase pairs by pruning threshold
threshold fr?? en es?? en de?? en
none 9,314,165 100% 11,591,013 100% 6,954,243 100%
10 7,999,081 85.9% 10,212,019 88.1% 5,849,593 84.1%
??  6,014,294 64.6% 7,865,072 67.9% 4,357,620 62.7%
? +  1,435,576 15.4% 1,592,655 13.7% 1,163,296 16.7%
15 1,377,375 14.8% 1,533,610 13.2% 1,115,559 16.0%
20 1,152,780 12.4% 1,291,113 11.1% 928,855 13.4%
25 905,201 9.7% 1,000,264 8.6% 732,230 10.5%
50 446,757 4.8% 481,737 4.2% 365,118 5.3%
100 235,132 2.5% 251,999 2.2% 189,655 2.7%
1000 22,873 0.2% 24,070 0.2% 16,467 0.2%
Table 4: Chinese?English: Distinct phrase pairs by pruning threshold
threshold best UN-v2
none 18,858,589 100% 20,228,273 100%
14 7,666,063 40.7% 13,276,885 65.6%
16 4,280,845 22.7% 7,691,660 38.0%
18 4,084,167 21.7% 7,434,939 36.8%
20 3,887,397 20.6% 7,145,827 35.3%
25 3,403,674 18.0% 6,316,795 31.2%
also pruning C(s?, t?) = 1
14? 4,477,920 23.7% 7,917,062 39.1%
that an artificial separation of 1 BLEU point has
been introduced into these graphs to separate them.
Without this, they lie on top of each other and hide
the essential point. In compensation, the scale for
the BLEU co-ordinate has been removed.
These results are summarized in the following
subsections.
4.1 BLEU as a function of threshold
In tables 5 and 6, the largest BLEU score for each
set of runs has been marked in bold font. In addition,
to highlight that there are many near ties for largest
BLEU, all BLEU scores that are within 0.1 of the
best are also marked in bold.
When this is done it becomes clear that pruning
at a level of 20 for the WMT06 runs would not re-
duce BLEU in most cases and in many cases would
actually increase it. A pruning threshold of 20 cor-
responds to discarding roughly 90% of the phrase-
table.
For the Chinese?English large corpus runs, a level
of 16 seems to be about the best with a small in-
crease in BLEU and a 60% ? 70% reduction in the
size of the phrasetable.
4.2 BLEU as a function of depth of pruning
Another view of this can be taken from Tables 5
and 6. The fraction of the phrasetable retained is
a more or less simple function of pruning threshold
as shown in Tables 3 and 4. By including the per-
centages in Tables 5 and 6, we can see that BLEU
goes up as the fraction approaches between 20% and
30%.
This seems to be a relatively stable observation
across the experiments. It is also easily explained by
its strong relationship to pruning threshold.
4.3 Large corpora
Table 6 shows that this is not just a small corpus phe-
nomenon. There is a sizeable benefit both in phrase-
table reduction and a modest improvement to BLEU
even in this case.
4.4 Is this just the same as phrasetable
smoothing?
One question that occurred early on was whether this
improvement in BLEU is somehow related to the
improvement in BLEU that occurs with phrasetable
smoothing.
972
It appears that the answer is, in the main, yes, al-
though there is definitely something else going on.
It is true that the benefit in terms of BLEU is less-
ened for better types of phrasetable smoothing but
the benefit in terms of the reduction in bulk holds. It
is reassuring to see that no harm to BLEU is done by
removing even 80% of the phrasetable.
4.5 Comment about C(s?, t?) = 1
Another question that came up is the role of phrase
pairs that occur only once: C(s?, t?) = 1. In particu-
lar as discussed above, the most significant of these
are the 1-1-1 phrase pairs whose components also
only occur once: C(s?) = 1, and C(t?) = 1. These
phrase pairs are amazingly frequent in the phrase-
tables and are pruned in all of the experiments ex-
cept when pruning threshold is equal to 14.
The Chinese?English large corpus experiments
give us a good opportunity to show that significance
level seems to be more an issue than the case that
C(s?, t?) = 1.
Note that we could have kept the phrase pairs
whose marginal counts were greater than one but
most of these are of lower significance and likely
are pruned already by the threshold. The given con-
figuration was considered the most likely to yield a
benefit and its poor performance led to the whole
idea being put aside.
5 Conclusions and Continuing Work
To sum up, the main conclusions are five in number:
1. Phrasetables produced by the standard Diag-
Andmethod (Koehn et al, 2003) can be aggres-
sively pruned using significance pruning with-
out worsening BLEU.
2. If phrasetable smoothing is not done, the BLEU
score will improve under aggressive signifi-
cance pruning.
3. If phrasetable smoothing is done, the improve-
ment is small or negligible but there is still no
loss on aggressive pruning.
4. The preservation of BLEU score in the pres-
ence of large-scale pruning is a strong effect in
small and moderate size phrasetables, but oc-
curs also in much larger phrasetables.
5. In larger phrasetables based on larger corpora,
the percentage of the table that can be dis-
carded appears to decrease. This is plausible
since a similar effect (a decrease in the benefit
of smoothing) has been noted with phrasetable
smoothing (Foster et al, 2006). Together these
results suggest that, for these corpus sizes, the
increase in the number of strongly supported
phrase pairs is greater than the increase in the
number of poorly supported pairs, which agrees
with intuition.
Although there may be other approaches to prun-
ing that achieve a similar effect, the use of Fisher?s
exact test is mathematically and conceptually one of
the simplest since it asks a question separately for
each phrase pair: ?Considering this phase pair in
isolation of any other analysis on the corpus, could it
have occurred plausibly by purely random processes
inherent in the corpus construction?? If the answer
is ?Yes?, then it is hard to argue that the phrase pair
is an association of general applicability from the
evidence in this corpus alone.
Note that the removal of 1-count phrase pairs is
subsumed by significance pruning with a threshold
greater than ? and many of the other simple ap-
proaches (from an implementation point of view)
are more difficult to justify as simply as the above
significance test. Nonetheless, there remains work
to do in determining if computationally simpler ap-
proaches do as well. Moore?s work suggests that
log-likelihood-ratio would be a cheaper and accurate
enough alternative, for example.
We will now return to the interaction of the se-
lection in our beam search of the top 30 candidates
based on forward conditional probabilities. This will
affect our results but most likely in the following
manner:
1. For very small thresholds, the beam will be-
come much wider and the search will take
much longer. In order to allow the experiments
to complete in a reasonable time, other means
will need to be employed to reduce the choices.
This reduction will also interact with the sig-
nificance pruning but in a less understandable
manner.
2. For large thresholds, there will not be 30
973
choices and so there will be no effect.
3. For intermediate thresholds, the extra prun-
ing might reduce BLEU score but by a small
amount because most of the best choices are
included in the search.
Using thresholds that remove most of the phrase-
table would no doubt qualify as large thresholds so
the question is addressing the true shape of the curve
for smaller thresholds and not at the expected operat-
ing levels. Nonetheless, this is a subject for further
study, especially as we consider alternatives to our
?filter 30? approach for managing beam width.
There are a number of important ways that this
work can and will be continued. The code base for
taking a list of n,m-grams and computing the re-
quired frequencies for signifance evaluation can be
applied to related problems. For example, skip-n-
grams (n-grams that allow for gaps of fixed or vari-
able size) may be studied better using this approach
leading to insight about methods that weakly ap-
proximate patterns.
The original goal of this work was to better un-
derstand the character of phrasetables, and it re-
mains a useful diagnostic technique. It will hope-
fully lead to more understanding of what it takes
to make a good phrasetable especially for languages
that require morphological analysis or segmentation
to produce good tables using standard methods.
The negative-log-p-value promises to be a useful
feature and we are currently evaluating its merits.
6 Acknowledgement
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).?
References
Alan Agresti. 1996. An Introduction to Categorical Data
Analysis. Wiley.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra and Robert L. Mercer. 1993. The Mathemat-
ics of Statistical Machine Translation: Parameter es-
timation. Computational Linguistics, 19(2):263?312,
June.
Philipp Koehn 2003. Europarl: A Mul-
tilingual Corpus for Evaluation of Ma-
chine Translation. Unpublished draft. see
http://www.iccs.inf.ed.ac.uk/?pkoehn
/publications/europarl.pdf
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Machine
Translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, Sydney, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing (ICASSP) 1995, pages
181?184, Detroit, Michigan. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Eduard
Hovy, editor, Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 127?133, Edmonton, Alberta, Canada, May.
NAACL.
Robert C. Moore. 2004. On Log-Likelihood-Ratios and
the Significance of Rare Events. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, Barcelona, Spain.
NIST. 2006. NIST MT Benchmark Test. see
http://www.nist.gov/speech/tests/mt/
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Computa-
tional Linguistics(ACL), Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Report
RC22176, IBM, September.
NAACL Workshop on Statistical Machine Translation.
2006. see http://www.statmt.org/wmt06/
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP) 2002, Denver, Colorado, September.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of Human Language Technology Conference
/ North American Chapter of the ACL, Boston, May.
974
Table 5: WMT06 Results: BLEU by type of smoothing and pruning threshold
threshold phrasetable % fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
relative frequency: no smoothing
none 100% 25.39 27.26 20.74 27.29 27.17 14.71
10 84?88% 25.97 27.81 21.08 27.82 27.71 15.09
??  63?68% 26.32 28.00 21.27 28.11 28.09 15.19
? +  14?17% 26.34 28.27 21.22 28.16 28.08 15.24
15 13?15% 26.36 28.50 21.14 28.20 28.18 15.29
20 11?13% 26.51 28.45 21.36 28.28 28.06 15.28
25 8?10% 26.50 28.38 21.28 28.32 27.97 15.25
50 4?5% 26.26 27.88 20.87 28.05 27.90 15.08
100 2% 25.66 27.07 20.07 27.38 27.11 14.66
1000 0.2% 20.49 21.66 15.23 22.51 22.31 11.36
Good-Turing
none 100% 25.96 28.14 21.17 27.84 27.95 15.13
10 84?88% 26.33 28.33 21.38 28.18 28.27 15.22
??  63?68% 26.54 28.63 21.50 28.36 28.39 15.31
? +  14?17% 26.24 28.49 21.15 28.22 28.16 15.28
15 13?15% 26.48 28.03 21.21 28.27 28.21 15.31
20 11?13% 26.65 28.45 21.41 28.36 28.14 15.25
25 8?10% 26.54 28.56 21.31 28.35 28.04 15.28
50 4?5% 26.26 27.78 20.94 28.07 27.95 15.08
100 2% 25.70 27.07 20.12 27.41 27.13 14.66
1000 0.2% 20.49 21.66 15.52 22.53 22.31 11.37
Kneser-Ney (3 parameter)
none 100% 26.89 28.70 21.78 28.64 28.71 15.50
10 84?88% 26.79 28.78 21.71 28.63 28.41 15.35
15 13?15% 26.49 28.69 21.34 28.60 28.57 15.52
20 11?13% 26.73 28.67 21.54 28.56 28.44 15.41
25 8?10% 26.84 28.70 21.29 28.54 28.21 15.42
50 4?5% 26.44 28.16 20.93 28.17 28.05 15.17
100 2% 25.72 27.27 20.11 27.50 27.26 14.58
1000 0.2% 20.48 21.70 15.28 22.58 22.36 11.33
Zens-Ney
none 100% 26.87 29.07 21.55 28.75 28.54 15.50
10 84?88% 26.81 29.00 21.65 28.72 28.52 15.54
15 13?15% 26.92 28.67 21.74 28.79 28.32 15.44
20 11?13% 26.93 28.47 21.72 28.69 28.42 15.45
25 8?10% 26.85 28.79 21.58 28.59 28.27 15.37
50 4?5% 26.51 27.96 20.96 28.30 27.96 15.27
100 2% 25.82 27.34 20.02 27.57 27.30 14.51
1000 0.2% 20.50 21.76 15.46 22.68 22.33 11.56
Table 6: Chinese Results: BLEU by pruning threshold
threshold phrasetable % nist04 nist05 nist06-GALE nist06-NIST
Zens-Ney Smoothing applied to all phrasetables
none 100% 32.14 30.69 13.06 27.97
14 40?65% 32.66 31.14 13.11 28.35
16 22?38% 32.73 30.97 13.14 28.00
18 21?36% 31.56 30.45 12.49 27.03
20 20?35% 32.00 30.73 12.50 27.33
25 18?31% 30.54 29.58 11.68 26.12
also pruning C(s?, t?) = 1
14? 23?39% 32.08 30.99 12.75 27.66
975
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 25?32,
New York, June 2006. c?2006 Association for Computational Linguistics
Segment Choice Models: Feature-Rich Models for Global  
Distortion in Statistical Machine Translation 
 
 
Roland Kuhn, Denis Yuen, Michel Simard, Patrick Paul,  
George Foster, Eric Joanis, and Howard Johnson 
 
Institute for Information Technology, National Research Council of Canada 
Gatineau, Qu?bec, CANADA  
Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, 
Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com  
 
 
 
 
Abstract 
This paper presents a new approach to 
distortion (phrase reordering) in phrase-
based machine translation (MT). Distor-
tion is modeled as a sequence of choices 
during translation. The approach yields 
trainable, probabilistic distortion models 
that are global: they assign a probability 
to each possible phrase reordering. These 
?segment choice? models (SCMs) can be 
trained on ?segment-aligned? sentence 
pairs; they can be applied during decoding 
or rescoring. The approach yields a metric 
called ?distortion perplexity? (?disperp?) 
for comparing SCMs offline on test data, 
analogous to perplexity for language 
models. A decision-tree-based SCM is 
tested on Chinese-to-English translation, 
and outperforms a baseline distortion 
penalty approach at the 99% confidence 
level. 
1 Introduction: Defining SCMs  
The work presented here was done in the context 
of phrase-based MT (Koehn et al, 2003; Och and 
Ney, 2004). Distortion in phrase-based MT occurs 
when the order of phrases in the source-language 
sentence changes during translation, so the order of 
corresponding phrases in the target-language trans-
lation is different. Some MT systems allow arbi-
trary reordering of phrases, but impose a distortion 
penalty proportional to the difference between the 
new and the original phrase order (Koehn, 2004). 
Some interesting recent research focuses on reor-
dering within a narrow window of phrases (Kumar 
and Byrne, 2005; Tillmann and Zhang, 2005; Till-
mann, 2004). The (Tillmann, 2004) paper intro-
duced lexical features for distortion modeling. A 
recent paper (Collins et al, 2005) shows that major 
gains can be obtained by constructing a parse tree 
for the source sentence and then applying hand-
crafted reordering rules to rewrite the source in 
target-language-like word order prior to MT.  
 
Our model assumes that the source sentence is 
completely segmented prior to distortion. This 
simplifying assumption requires generation of hy-
potheses about the segmentation of the complete 
source sentence during decoding. The model also 
assumes that each translation hypothesis grows in a 
predetermined order. E.g., Koehn?s decoder 
(Koehn 2004) builds each new hypothesis by add-
ing phrases to it left-to-right (order is deterministic 
for the target hypothesis). Our model doesn?t re-
quire this order of operation ? it would support 
right-to-left or inwards-outwards hypothesis con-
struction ? but it does require a predictable order. 
 
One can keep track of how segments in the 
source sentence have been rearranged during de-
coding for a given hypothesis, using what we call a 
?distorted source-language hypothesis? (DSH). A 
similar concept appears in (Collins et al, 2005) 
(this paper?s preoccupations strongly resemble 
25
ours, though our method is completely different: 
we don?t parse the source, and use only automati-
cally generated rules). Figure 1 shows an example 
of a DSH for German-to-English translation (case 
information is removed). Here, German ?ich habe 
das buch gelesen .? is translated into English ?i 
have read the book .? The DSH shows the distor-
tion of the German segments into an English-like 
word order that occurred during translation (we 
tend to use the word ?segment? rather than the 
more linguistically-charged  ?phrase?). 
Figure 1. Example of German-to-English DSH 
From the DSH, one can reconstruct the series of 
segment choices. In Figure 1 - given a left-to-right 
decoder - ?[ich]? was chosen from five candidates 
to be the leftmost segment in the DSH. Next, 
?[habe]? was chosen from four remaining candi-
dates, ?[gelesen]? from three candidates, and ?[das 
buch]? from two candidates. Finally, the decoder 
was forced to choose ?[.]?.   
 
Segment Choice Models (SCMs) assign 
probabilities to segment choices made as the DSH 
is constructed. The available choices at a given 
time are called the ?Remaining Segments? (RS). 
Consider a valid (though stupid) SCM that assigns 
equal probabilities to all segments in the RS. This 
uniform SCM assigns a probability of 1/5! to the 
DSH in Figure 1: the probability of choosing 
?[ich]? from among 5 RS was 1/5, then the 
probability of ?[habe]? among 4 RS was  1/4 , etc. 
The uniform SCM would be of little use to an MT 
system. In the next two sections we describe some 
more informative SCMs, define the ?distortion 
perplexity? (?disperp?) metric for comparing 
SCMs offline on a test corpus, and show how to 
construct this corpus.  
2 Disperp and Distortion Corpora 
2.1 Defining Disperp 
The ultimate reason for choosing one SCM over 
another will be the performance of an MT system 
containing it, as measured by a metric like BLEU 
(Papineni et al, 2002). However, training and 
testing a large-scale MT system for each new SCM 
would be costly. Also, the distortion component?s 
effect on the total score is muffled by other 
components (e.g., the phrase translation and target 
language models). Can we devise a quick 
standalone metric for comparing SCMs? 
 
There is an offline metric for statistical language 
models: perplexity (Jelinek, 1990). By analogy, the 
higher the overall probability a given SCM assigns 
to a test corpus of representative distorted sentence 
hypotheses (DSHs), the better the quality of the 
SCM. To define distortion perplexity (?disperp?), 
let PrM(dk) = the probability an SCM M assigns to 
a DSH for sentence k, dk. If T is a test corpus 
comprising numerous DSHs, the probability of the 
corpus according to M is PrM(T) =   k PrM(dk).  
Let S(T) = total number of segments in T. Then 
disperp(M,T) = PrM(T)-1/S(T). This gives the mean 
number of choices model M allows; the lower the 
disperp for corpus T, the better M is as a model for 
T (a model X that predicts segment choice in T 
perfectly would have disperp(X,T) = 1.0).  
2.2 Some Simple A Priori SCMs 
The uniform SCM assigns to the DSH dk that has 
S(dk) segments the probability 1/[S(dk)!] . We call 
this Model A. Let?s define some other illustrative 
SCMs. Fig. 2 shows a sentence that has 7 segments 
with 10 words (numbered 0-9 by original order). 
Three segments in the source have been used; the 
decoder has a choice of four RS. Which of the RS 
has the highest probability of being chosen? Per-
haps [2 3], because it is the leftmost RS: the ?left-
most? predictor. Or, the last phrase in the DSH will 
be followed by the phrase that originally followed 
it, [8 9]: the ?following? predictor. Or, perhaps 
positions in the source and target should be close, 
so since the next DSH position to be filled is 4, 
phrase [4] should be favoured: the ?parallel? pre-
dictor. 
 
 
Figure 2. Segment choice prediction example 
Model B will be based on the ?leftmost? predic-
tor, giving the leftmost segment in the RS twice the 
probability of the other segments, and giving the 
Original German:   [ich] [habe] [das buch] [gelesen]    [.] 
DSH for German:  [ich] [habe]  [gelesen]    [das buch] [.] 
(English:                [i]     [have]   [read]        [the book] [.]) 
original:  [0 1] [2 3] [4] [5] [6] [7] [8 9] 
DSH:  [0 1] [5] [7],   RS:  [2 3], [4], [6], [8 9] 
26
others uniform probabilities. Model C will be 
based on the ?following? predictor, doubling the 
probability for the segment in the RS whose first 
word was the closest to the last word in the DSH, 
and otherwise assigning uniform probabilities. Fi-
nally, Model D combines ?leftmost? and ?follow-
ing?: where the leftmost and following segments 
are different, both are assigned double the uniform 
probability; if they are the same segment, that 
segment has four times the uniform probability. Of 
course, the factor of 2.0 in these models is arbi-
trary. For Figure 2, probabilities would be: 
? Model A: PrA([2 3])= PrA([4])= PrA([6])= 
PrA([8 9]) = 1/4; 
? Model B: PrB ([2 3])= 2/5, PrB([4])= 
PrB([6])= PrB([8 9]) = 1/5; 
? Model C: PrC ([2 3])= PrC ([4])= PrC([6]) 
= 1/5, PrC([8 9]) = 2/5; 
? Model D: PrD ([2 3]) = PrD([8 9]) = 1/3, 
PrD([4])= PrD([6]) = 1/6.  
 
Finally, let?s define an SCM derived from the 
distortion penalty used by systems based on the 
?following? predictor, as in (Koehn, 2004). Let ai = 
start position of source phrase translated into ith 
target phrase, bi -1= end position of source phrase 
that?s translated into (i-1)th target phrase. Then 
distortion penalty d(ai, bi-1) =   ?ai? bi-1 -1?; the total 
distortion is the product of the phrase distortion 
penalties. This penalty is applied as a kind of non-
normalized probability in the decoder. The value of 
   for given (source, target) languages is optimized 
on development data. 
To turn this penalty into an SCM, penalties are 
normalized into probabilities, at each decoding 
stage; we call the result Model P (for ?penalty?). 
Model P with    = 1.0 is the same as uniform 
Model A. In disperp experiments, Model P with    
optimized on held-out data performs better than 
Models A-D (see Figure 5), suggesting that dis-
perp is a realistic measure.  
Models A-D are models whose parameters were 
all defined a priori; Model P has one trainable pa-
rameter,  . Next, let?s explore distortion models 
with several trainable parameters.  
2.3 Constructing a Distortion Corpus 
To compare SCMs using disperp and to train 
complex SCMs, we need a corpus of representative 
examples of DSHs. There are several ways of ob-
taining such a corpus. For the experiments de-
scribed here, the MT system was first trained on a 
bilingual sentence-aligned corpus. Then, the sys-
tem was run in a second pass over its own training 
corpus, using its phrase table with the standard dis-
tortion penalty to obtain a best-fit phrase alignment 
between each (source, target) sentence pair. Each 
such alignment yields a DSH whose segments are 
aligned with their original positions in the source; 
we call such a source-DSH alignment a ?segment 
alignment?. We now use a leave-one-out procedure 
to ensure that information derived from a given 
sentence pair is not used to segment-align that sen-
tence pair. In our initial experiments we didn?t do 
this, with the result that the segment-aligned cor-
pus underrepresented the case where words or N-
grams not in the phrase table are seen in the source 
sentence during decoding.  
3 A Trainable Decision Tree SCM 
Almost any machine learning technique could be 
used to create a trainable SCM. We implemented 
one based on decision trees (DTs), not because 
DTs necessarily yield the best results but for soft-
ware engineering reasons: DTs are a quick way to 
explore a variety of features, and are easily inter-
preted when grown (so that examining them can 
suggest further features). We grew N DTs, each 
defined by the number of choices available at a 
given moment. The highest-numbered DT has a 
?+? to show it handles N+1 or more choices. E.g., 
if we set N=4, we grow a ?2-choice?, a ?3-choice?, 
a ?4-choice?, and a ?5+-choice tree?. The 2-choice 
tree handles cases where there are 2 segments in 
the RS, assigning a probability to each; the 3-
choice tree handles cases where there are 3 seg-
ments in the RS, etc. The 5+-choice tree is differ-
ent from the others: it handles cases where there 
are 5 segments in the RS to choose from, and 
cases where there are more than 5. The value of N 
is arbitrary; e.g., for N=8, the trees go from ?2-
choice? up to ?9+-choice?.  
Suppose a left-to-right decoder with an N=4 
SCM is translating a sentence with seven phrases. 
Initially, when the DSH is empty, the 5+-choice 
tree assigns probabilities to each of these seven. It 
27
will use the 5+-choice tree twice more, to assign 
probabilities to six RS, then to five. To extend the 
hypothesis, it will then use the 4-choice tree, the 3-
choice tree, and finally the 2-choice tree. Disperps 
for this SCM are calculated on test corpus DSHs in 
the same left-to-right way, using the tree for the 
number of choices in the RS to find the probability 
of each segment choice. 
Segments need labels, so the N-choice DT can 
assign probabilities to the N segments in the RS. 
We currently use a ?following? labeling scheme. 
Let X be the original source position of the last 
word put into the DSH, plus 1. In Figure 2, this 
was word 7, so X=8. In our scheme, the RS seg-
ment whose first word is closest to X is labeled 
?A?; the second-closest segment is labeled ?B?, 
etc. Thus, segments are labeled in order of the 
(Koehn, 2004) penalty; the ?A? segment gets the 
lowest penalty. Ties between segments on the right 
and the left of X are broken by first labeling the 
right segment. In Figure 2, the labels for the RS 
are ?A? = [8 9], ?B? = [6], ?C? = [4], ?D? = [2 3].  
 
 
 
 
 
 
 
 
Figure 3. Some question types for choice DTs 
Figure 3 shows the main types of questions used 
for tree-growing, comprising position questions 
and word-based questions. Position questions 
pertain to location, length, and ordering of seg-
ments. Some position questions ask about the dis-
tance between the first word of a segment and the 
?following? position X: e.g., if the answer to 
?pos(A)-pos(X)=0?? is yes, then segment A comes 
immediately after the last DSH segment in the 
source, and is thus highly likely to be chosen. 
There are also questions relating to the ?leftmost? 
and ?parallel? predictors (above, sec. 2.2). The 
fseg() and bseg() functions count segments in the 
RS from left to right and right to left respectively, 
allowing, e.g., the question whether a given seg-
ment is the second last segment in the RS. The 
only word-based questions currently implemented 
ask whether a given word is contained in a given 
segment (or anywhere in the DSH, or anywhere in 
the RS). This type could be made richer by allow-
ing questions about the position of a given word in 
a given segment, questions about syntax, etc.  
Figure 4 shows an example of a 5+-choice DT. 
The ?+? in its name indicates that it will handle 
cases where there are 5 or more segments in the 
RS. The counts stored in the leaves of this DT rep-
resent the number of training data items that ended 
up there; the counts are used to estimate probabili-
ties. Some smoothing will be done to avoid zero 
probabilities, e.g., for class C in node 3.  
 
Figure 4. Example of a 5+-choice tree 
For ?+? DTs, the label closest to the end of the 
alphabet (?E? in Figure 4) stands for a class that 
can include more than one segment. E.g., if this 
5+-choice DT is used to estimate probabilities for a 
7-segment RS, the segment closest to X is labeled 
?A?, the second closest ?B?, the third closest ?C?, 
and the fourth closest ?D?. That leaves 3 segments, 
all labeled ?E?. The DT shown yields probability 
Pr(E) that one of these three will be chosen. Cur-
rently, we apply a uniform distribution within this 
?furthest from X? class, so the probability of any 
one of the three ?E? segments is estimated as 
Pr(E)/3.  
To train the DTs, we generate data items from 
the second-pass DSH corpus. Each DSH generates 
several data items. E.g., moving across a seven-
segment DSH from left to right, there is an exam-
ple of the seven-choice case, then one of the six-
choice case, etc. Thus, this DSH provides three 
items for training the 5+-choice DT and one item 
     pos(A)-pos(X)<0? 
A:27 B:23 C:20 D:11 E:19  
        today    DSH? 
A:10 B:8 C:10 D:6 E:5 
A:8 B:6 C:0 D:2 E:4 A:2 B:2 C:10 D:4 E:1 
A:17 B:15 C:10 D:5 E:14 
yes no 
yes no 
1. 
3. 
2. 5. 
4. 
1. Position Questions 
Segment Length Questions 
E.g., ?lgth(DSH)<5??, ?lgth(B)=2??, ?lgth(RS)<6??, etc.  
Questions about Original Position 
Let pos(seg) = index of seg?s first word in source sentence 
E.g., ?pos(A)=9??, ?pos(C) <17??, etc.  
Questions With X (?following? word position)  
E.g., ?pos(X)=9??, ?pos(C) ? pos(X) <0??, etc.  
Segment Order Questions  
Let fseg = segment # (forward), bseg = segment # (back-
ward) 
E.g., ?fseg(D) = 1??, ?bseg(A) <5??, etc.  
2. Word-Based Questions  
E.g., ?and   DSH??, ?November   B??, etc.  
28
each for training the 4-choice, 3-choice, and 2-
choice DTs. The DT training method was based on 
Gelfand-Ravishankar-Delp expansion-pruning 
(Gelfand et al, 1991), for DTs whose nodes con-
tain probability distributions (Lazarid?s et al, 
1996).  
4 Disperp Experiments 
We carried out SCM disperp experiments for the 
English-Chinese task, in both directions. That is, 
we trained and tested models both for the distortion 
of English into Chinese-like phrase order, and the 
distortion of Chinese into English-like phrase or-
der. For reasons of space, details about the ?dis-
torted English? experiments won?t be given here. 
Training and development data for the distorted 
Chinese experiments were taken from the NIST 
2005 release of the FBIS corpus of Xinhua news 
stories. The training corpus comprised 62,000 
FBIS segment alignments, and the development 
?dev? corpus comprised a disjoint set of 2,306 
segment alignments from the same FBIS corpus. 
All disperp results are obtained by testing on ?dev? 
corpus. 
 
Distorted Chinese: Models A-D, P, & a four-DT 
Model
1
2
3
4
5
6
7
8
50
0
10
00
20
00
40
00
80
00
16
00
0
32
00
0
62
00
0
# training alignments (log scale)
Di
sp
er
p 
o
n
 
"
de
v
"
 
Model A
Model B
Model C
Model D
Model P (alpha =
0.77)
Four DTs: pos +
100-wd qns
 
Figure 5. Several SCMs for distorted Chinese 
Figure 5 shows disperp results for the models 
described earlier. The y axis begins at 1.0 (mini-
mum value of disperp). The x axis shows number 
of alignments (DSHs) used to train DTs, on a log 
scale. Models A-D are fixed in advance; Model P?s 
single parameter    was optimized once on the en-
tire training set of 62K FBIS alignments (to 0.77) 
rather than separately for each amount of training 
data. Model P, the normalized version of  Koehn?s 
distortion penalty, is superior to Models A-D, and 
the DT-based SCM is superior to Model P.  
The Figure 5 DT-based SCM had four trees (2-
choice, 3-choice, 4-choice, and 5+-choice) with 
position-based and word-based questions. The 
word-based questions involved only the 100 most 
frequent Chinese words in the training corpus. The 
system?s disperp drops from 3.1 to 2.8 as the num-
ber of alignments goes from 500 to 62K. 
Figure 6 examines the effect of allowing word-
based questions. These questions provide a signifi-
cant disperp improvement, which grows with the 
amount of training data. 
Distorted Chinese: effect of allowing word qns 
(four- DT models)
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
50
0
10
00
20
00
40
00
80
00
16
00
0
32
00
0
62
00
0
# training alignments (log scale) 
Di
sp
er
p 
o
n
 
"
de
v
"
 
Four DTs: pos qns
only
Four DTs: pos +
100-wd qns
 
Figure 6. Do word-based questions help? 
In the ?four-DT? results above, examples with 
five or more segments are handled by the same 
?5+-choice? tree. Increasing the number of trees 
allows finer modeling of multi-segment cases 
while spreading the training data more thinly. 
Thus, the optimal number of trees depends on the 
amount of training data. Fixing this amount to 32K 
alignments, we varied the number of trees. Figure 
7 shows that this parameter has a significant im-
pact on disperp, and that questions based on the 
most frequent 100 Chinese words help perform-
ance for any number of trees.  
29
Distorted Chinese: Disperp vs. # of trees (all 
trees grown on 32K alignments)
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3 4 5 6 7 8 9 10 11 12 13 14
# of trees
Di
sp
er
p 
o
n
 
"
de
v"
 
pos qns only
pos + 100-wd qns
 
Figure 7. Varying the number of DTs  
In Figure 8 the number of the most frequent 
Chinese words for questions is varied (for a 13-DT 
system trained on 32K alignments). Most of the 
improvement came from the 8 most frequent 
words, especially from the most frequent, the 
comma ?,?. This behaviour seems to be specific to 
Chinese. In our ?distorted English? experiments, 
questions about the 8 most frequent words also 
gave a significant improvement, but each of the 8 
words had a fairly equal share in the improvement. 
Distorted Chinese: Disperp vs. #words (all trees 
grown on 32K alignments)
2.58
2.6
2.62
2.64
2.66
2.68
2.7
2.72
0 2 8 32 12
8
51
2
# words tried for qns (log scale)
Di
sp
er
p 
o
n
 
"
de
v
"
Performance of 13-
DT system
 
Figure 8. Varying #words (13-DT system) 
Finally, we grew the DT system used for the MT 
experiments: one with 13 trees and questions about 
the 25 most frequent Chinese words, grown on 
88K alignments. Its disperp on the ?dev? used for 
the MT experiments (a different ?dev? from the 
one above ? see Sec. 5.2) was 2.42 vs. 3.48 for the 
baseline Model P system: a 30% drop.  
5 Machine Translation Experiments 
5.1 SCMs for Decoding 
SCMs assume that the source sentence is fully 
segmented throughout decoding. Thus, the system 
must guess the segmentation for the unconsumed 
part of the source (?remaining source?: RS). For 
the results below, we used a simple heuristic: RS is 
broken into one-word segments. In future, we will 
apply a more realistic segmentation model to RS 
(or modify DT training to reflect accurately RS 
treatment during decoding).  
5.2 Chinese-to-English MT Experiments  
The training corpus for the MT system?s phrase 
tables consists of all parallel text available for the 
NIST MT05 Chinese-English evaluation, except 
the Xinhua corpora and part 3 of LDC's ?Multiple-
Translation Chinese Corpus? (MTCCp3). The Eng-
lish language model was trained on the same cor-
pora, plus 250M words from Gigaword. The DT-
based SCM was trained and tuned on a subset of 
this same training corpus (above). The dev corpus 
for optimizing component weights is MTCCp3. 
The experimental results below were obtained by 
testing on the evaluation set for MTeval NIST04.  
Phrase tables were learned from the training cor-
pus using the ?diag-and? method (Koehn et al, 
2003), and using IBM model 2 to produce initial 
word alignments (these authors found this worked 
as well as IBM4). Phrase probabilities were based 
on unsmoothed relative frequencies. The model 
used by the decoder was a log-linear combination 
of a phrase translation model (only in the 
P(source|target) direction), trigram language 
model, word penalty (lexical weighting), an op-
tional segmentation model (in the form of a phrase 
penalty) and distortion model. Weights on the 
components were assigned using the (Och, 2003) 
method for max-BLEU training on the develop-
ment set. The decoder uses a dynamic-
programming beam-search, like the one in (Koehn, 
2004). Future-cost estimates for all distortion mod-
els are assigned using the baseline penalty model. 
5.3 Decoding Results 
30
29,40
29,60
29,80
30,00
30,20
30,40
30,60
30,80
31,00
31,20
no PP PP no PP PP
DP DT
BL
EU
 
sc
o
re
1x beam
4x beam
 
Figure 9. BLEU on NIST04 (95% conf. = ?0.7) 
Figure 9 shows experimental results. The ?DP? 
systems use the distortion penalty in (Koehn, 2004) 
with    optimized on ?dev?, while ?DT? systems 
use the DT-based SCM. ?1x? is the default beam 
width, while ?4x? is a wider beam (our notation 
reflects decoding time, so ?4x? takes four times as 
long as ?1x?). ?PP? denotes presence of the phrase 
penalty component. The advantage of DTs as 
measured by difference between the score of the 
best DT system and the best DP system is 0.75 
BLEU at 1x and 0.5 BLEU at 4x. With a 95% 
bootstrap confidence interval of ?0.7 BLEU (based 
on 1000-fold resampling), the resolution of these 
results is too coarse to draw firm conclusions. 
Thus, we carried out another 1000-fold bootstrap 
resampling test on NIST04, this time for pairwise 
system comparison. Table 1 shows results for 
BLEU comparisons between the systems with the 
default (1x) beam. The entries show how often the 
A system (columns) had a better score than the B 
system (rows), in 1000 observations.  
   A    
vs. B   
DP,  
no PP 
DP, PP DT,  
no PP 
DT, PP 
DP,  
no PP 
x 2.95% 99.45% 99.55% 
DP, PP 97.05% x 99.95% 99.95% 
DT,  
no PP 
0.55% 0.05% x 65.68% 
DT, PP 0.45% 0.05% 34.32% x 
Table 1. Pairwise comparison for 1x systems 
The table shows that both DT-based 1x systems 
performed better than either of the DP systems 
more than 99% of the time (underlined results). 
Though not shown in the table, the same was true 
with 4x beam search. The DT 1x system with a 
phrase penalty had a higher score than the DT 1x 
system without one about 66% of the time. 
6 Summary and Discussion 
In this paper, we presented a new class of probabil-
istic model for distortion, based on the choices 
made during translation. Unlike some recent dis-
tortion models (Kumar and Byrne, 2005; Tillmann 
and Zhang, 2005; Tillmann, 2004) these Segment 
Choice Models (SCMs) allow phrases to be moved 
globally, between any positions in the sentence. 
They also lend themselves to quick offline com-
parison by means of a new metric called disperp. 
We developed a decision-tree (DT) based SCM 
whose parameters were optimized on a ?dev? cor-
pus via disperp. Two variants of the DT system 
were experimentally compared with two systems 
with a distortion penalty on a Chinese-to-English 
task. In pairwise bootstrap comparisons, the sys-
tems with DT-based distortion outperformed the 
penalty-based systems more than 99% of the time. 
The computational cost of training the DTs on 
large quantities of data is comparable to that of 
training phrase tables on the same data - large but 
manageable ? and increases linearly with the 
amount of training data. However, currently there 
is a major problem with DT training: the low pro-
portion of Chinese-English sentence pairs that can 
be fully segment-aligned and thus be used for DT 
training (about 27%). This may result in selection 
bias that impairs performance. We plan to imple-
ment an alignment algorithm with smoothed phrase 
tables (Johnson et al 2006) to achieve segment 
alignment on 100% of the training data. 
Decoding time with the DT-based distortion 
model is roughly proportional to the square of the 
number of tokens in the source sentence. Thus, 
long sentences pose a challenge, particularly dur-
ing the weight optimization step. In experiments on 
other language pairs reported elsewhere (Johnson 
et al 2006), we applied a heuristic: DT training 
and decoding involved source sentences with 60 or 
fewer tokens, while longer sentences were handled 
with the distortion penalty. A more principled ap-
31
proach would be to divide long source sentences 
into chunks not exceeding 60 or so tokens, within 
each of which reordering is allowed, but which 
cannot themselves be reordered.  
The experiments above used a segmentation 
model that was a count of the number of source 
segments (sometimes called ?phrase penalty?), but 
we are currently exploring more sophisticated 
models. Once we have found the best segmentation 
model, we will improve the system?s current na?ve 
single-word segmentation of the remaining source 
sentence during decoding, and construct a more 
accurate future cost function for beam search. An-
other obvious system improvement would be to 
incorporate more advanced word-based features in 
the DTs, such as questions about word classes 
(Tillmann and Zhang 2005, Tillmann 2004).  
We also plan to apply SCMs to rescoring N-best 
lists from the decoder. For rescoring, one could 
apply several SCMs, some with assumptions dif-
fering from those of the decoder. E.g., one could 
apply right-to-left SCMs, or ?distorted target? 
SCMs which assume a target hypothesis generated 
the source sentence, instead of vice versa.  
Finally, we are contemplating an entirely differ-
ent approach to DT-based SCMs for decoding. In 
this approach, only one DT would be used, with 
only two output classes that could be called ?C? 
and ?N?. The input to such a tree would be a par-
ticular segment in the remaining source sentence, 
with contextual information (e.g., the sequence of 
segments already chosen). The DT would estimate 
the probability Pr(C) that the specified segment is 
?chosen? and the probability Pr(N) that it is ?not 
chosen?. This would eliminate the need to guess 
the segmentation of the remaining source sentence.  
References  
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. ?The Mathematics of Statistical Machine 
Translation: Parameter Estimation?. Computational 
Linguistics, 19(2), pp. 263-311.  
 
M. Collins, P. Koehn, and I. Ku   erov?. 2005. ?Clause 
Restructuring for Statistical Machine Translation?. 
Proc. ACL, Ann Arbor, USA, pp. 531-540. 
 
S. Gelfand, C. Ravishankar, and E. Delp. 1991. ?An 
Iterative Growing and Pruning Algorithm for Clas-
sification Tree Design?. IEEE Trans. Patt. Analy. 
Mach. Int. (IEEE PAMI), V. 13, no. 2, pp. 163-174.  
 
F. Jelinek. 1990. ?Self-Organized Language Modeling 
for Speech Recognition? in Readings in Speech 
Recognition (ed. A. Waibel and K. Lee, publ. Mor-
gan Kaufmann), pp. 450-506.  
 
H. Johnson, F. Sadat, G. Foster, R. Kuhn, M. Simard, E. 
Joanis, and S. Larkin. 2006. ?PORTAGE: with 
Smoothed Phrase Tables and Segment Choice Mod-
els?.  Submitted to NAACL 2006 Workshop on Statis-
tical Machine Translation, New York City. 
P. Koehn. 2004. ?Pharaoh: a Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Mod-
els?. Assoc. Machine Trans. Americas (AMTA04). 
 
P. Koehn, F.-J. Och and D. Marcu. 2003. ?Statistical 
Phrase-Based Translation?. Proc. Human Lang. 
Tech. Conf. N. Am. Chapt. Assoc. Comp. Ling. 
(NAACL03), pp. 127-133.  
 
S. Kumar and W. Byrne. 2005. ?Local Phrase Reorder-
ing Models for Statistical Machine Translation?. 
HLT/EMNLP, pp. 161-168, Vancouver, Canada.  
 
A. Lazarid?s, Y. Normandin, and R. Kuhn. 1996. ?Im-
proving Decision Trees for Acoustic Modeling?. 
Int. Conf. Spoken Lang. Proc. (ICSLP96), V. 2, pp. 
1053-1056, Philadelphia, Pennsylvania, USA. 
 
F. Och and H. Ney. 2004. ?The Alignment Template 
Approach to Statistical Machine Translation?. 
Comp. Linguistics, V. 30, Issue 4, pp. 417-449.  
 
Franz Josef Och. 2003. ?Minimum Error Rate Training 
for Statistical Machine  Translation?. Proc. ACL, 
Sapporo, Japan. 
 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
?BLEU: A method for automatic evaluation of ma-
chine translation?. Proc. ACL, pp. 311-318. 
 
C. Tillmann and T. Zhang. 2005. ?A Localized Predic-
tion Model for Statistical Machine Translation?. 
Proc. ACL.  
 
C. Tillmann. 2004. ?A Block Orientation Model for 
Statistical Machine Translation?. HLT/NAACL. 
 
S. Vogel, H. Ney, and C. Tillmann. 1996. ?HMM-Based 
Word Alignment in Statistical Translation?. 
COLING, pp. 836-841. 
32
Adaptive Language and Translation Models
for Interactive Machine Translation
Laurent Nepveu, Guy Lapalme
Philippe Langlais
RALI/DIRO - Universite? de Montre?al,
C.P. 6128, succursale Centre-ville
Montre?al, Que?bec, Canada H3C 3J7
{nepveul,lapalme,felipe}
@iro.umontreal.ca
George Foster
Language Technologies Research Centre
National Research Council Canada
A-1330, 101 rue Saint-Jean Bosco,
Gatineau, Que?bec, Canada K1A 0R6
George.Foster@nrc-cnrc.gc.ca
Abstract
We describe experiments carried out with adaptive
language and translation models in the context of an
interactive computer-assisted translation program.
We developed cache-based language models which
were then extended to the bilingual case for a cache-
based translation model. We present the improve-
ments we obtained in two contexts: in a theoretical
setting, we achieved a drop in perplexity for the new
models and, in a more practical situation simulat-
ing a user working with the system, we showed that
fewer keystrokes would be needed to enter a trans-
lation.
1 Introduction
Cache-based language models were introduced by
Kuhn and de Mori (1990) for the dynamic adap-
tation of speech language models. These models,
inspired by the memory caches on modern com-
puter architectures, are motivated by the principle
of locality which states that a program tends to re-
peatedly use memory cells that are physically close.
Similarly, when speaking or writing, humans tend
to use the same words and phrase constructs from
paragraph to paragraph and from sentence to sen-
tence. This leads us to believe that, when processing
a document, the part of a document that is already
processed (e.g. for speech recognition, translation
or text prediction) gives us very useful information
for future processing in the same document or in
other related documents.
A cache-based language model is a language
model to which is added a smaller model trained
only on the history of the document being pro-
cessed. The history is usually the last N words or
sentences seen in the document.
Kuhn and de Mori (1990) obtained a drop in per-
plexity of nearly 68% when adding an unigram POS
(part-of-speech) cache on a 3g-gram model. Martin
and al. (1997) obtained a drop of nearly 21% when
adding a bigram cache to a trigram model. Clarkson
and Robertson (1997) also obtained similar results
with an exponentially decaying unigram cache.
The major problem with these theoretical results
is that they assume the correctness of the material
entering the cache. In practice, this assumption does
not always hold, and so a cache can sometimes do
more harm than good.
1.1 Interactive translation context
Over the last few years, an interactive machine
translation (IMT) system (Foster et al, 2002) has
been developed which, as the translator is typing,
suggests word and phrase completions that the user
can accept or ignore. The system uses a transla-
tion engine to propose the words or phrases which
it judges the most probable to be immediately typed.
This engine includes a translation model (TM) and
a language model (LM) used jointly to produce pro-
posals that are appropriate translations of source
words and plausible completions of the current text
in the target language. The translator remains in
control of the translation because what is typed by
the user is taken as a constraint to which the model
must continually adapt its completions. Experi-
ments have shown that the use of this system can
save about 50% of the keystrokes needed for enter-
ing a translation. As the translation and language
models are built only once, before the user starts to
work with the system, the translator is often forced
to repeatedly correct similar suggestions from the
system.
The interactive nature of this setup made us be-
lieve that it is a good prospect for dynamic adaptive
modeling. If the dynamic nature of the system can
be disadvantageous for static language and transla-
tion models, it is an incomparable advantage for a
cache based approach because human correction in-
tervenes before words go in the cache. As the trans-
lator is using the system to correctly enter his trans-
lation progressively, we can expect the theoretical
results presented in the literature to be obtainable in
practice in the IMT context.
The first advantage of dynamic adaptation would
be to help the translation engine make better predic-
tions, but it has a further psychological advantage:
as the translator works and potentially corrects the
proposals of the engine, the user would feel that the
software is learning from its errors.
The next section describes the models currently
embedded within our IMT prototype. Section 3 de-
scribes the cache-based adaptation we performed on
the target language model. In section 4, we present
the different types of adaptations we performed on
the translation model. Section 5 then puts the results
in the context of our IMT application. Section 6 dis-
cusses the implications of our experiments and sug-
gests some improvements that could be made to the
system.
2 Current IMT models
The word-based translation model embedded within
the IMT system has been designed by Foster (2000).
It is a Maximum Entropy/Minimum Divergence
(MEMD) translation model (Berger et al, 1996),
which mimics the parameters of the IBM model 2
(Brown et al, 1993) within a log-linear setting.
The resulting model (named MDI2B) is of the
following form, where h is the current target text,
s the source sentence being translated, s a particular
word in s and w the next word to be predicted:
p(w|h, s) =
q(w|h) exp(
?
s?s ?sw + ?AB)
Z(h, s)
(1)
The q distribution represents the prior knowledge
that we have about the true distribution and is mod-
eled by an interpolated trigram in this study. The
? coefficients are the familiar transfer or lexical pa-
rameters, and the ? ones can be understood as their
position dependent correction. Z is a normalizing
factor, the sum of the numerator for every w in the
target vocabulary.
Our baseline model used an interpolated trigram
of the following form as the q distribution:
p(w|h) = ?1(wi?2wi?1) ? ptri(wi|wi?2wi?1)
+ ?2(wi?2wi?1) ? pbi(wi|wi?1)
+ ?3(wi?2wi?1) ? puni(wi)
+ ?4(wi?2wi?1) ? 1|V |+1
where ?1(wi?2wi?1) + ?2(wi?2wi?1) +
?3(wi?2wi?1) + ?4(wi?2wi?1) = 1 and |V | + 1
is the size of the event space (including a special
unknown word).
As mentioned above, the MDI2B model is closely
related to the IBM2 model (Brown et al, 1988). It
contains two classes of features: word pair features
and positional features. The word pair feature func-
tions are defined as follows:
fst(w,h, s) =
{
1 if s ? s and t = w
0 otherwise
This function is on if the predicted word is t and s
is in the current source sentence. Each feature fst
has a corresponding weight ?st (for brevity, this is
defined to be 0 in equation 1 if the pair s, t is not
included in the model).
The positional feature functions are defined as
follows:
fA,B(w, i, s) =
J?
j=1
?[(i, j, J) ? A ? (sj , w) ? B ? j = ??sj ]
where ?[X] is 1 if X is true, otherwise 0; and ??sj
is the position of the occurrence of sj that is clos-
est to i according to an IBM2 model. A is a class
that groups positional (i, j, J) configurations having
similar IBM2 alignment probabilities, in order to re-
duce data sparseness. B is a class of word pairs
having similar weights ?st. Its purpose is to simu-
late the way IBM2 alignment probabilities modulate
IBM1 word-pair probabilities, by allowing the value
of the positional feature weight to depend on the
magnitude of the corresponding word-pair weight.
As with the word pair features, each fA,B has a cor-
responding weight ?AB .
Since feature selection is applied at training time
in order to improve speed, avoid overfitting, and
keep the model compact, the summation in the ex-
ponential term in (1) is only carried out over the set
of active pairs maintained by the model and not over
all pairs as might be inferred from the formulation.
To give an example of how the model works, if
the source sentence is the fruit I am eating is a ba-
nana and we are predicting the word banane follow-
ing the target words: Le fruit que je mange est une,
the active pairs involving banana would be (fruit,
banana) and (banane, banana) since, of all the pairs
(s, t) they would be the only ones kept by the fea-
ture selection algorithm1. The probability of banane
would therefore depend on the weights of those two
pairs, along with position weights which capture the
relative proximity of the words involved.
3 Language model adaptation
We implemented a first monolingual dynamic adap-
tation of this model by inserting a cache compo-
nent in its reference distribution, thus only affect-
ing the q distribution. We obtained similar results
1See (Foster, 2000) for the description of this algorithm.
as for classical ngram models: the unigram cache
model proved to be less efficient than the bigram
one, and the trigram cache suffered from sparsity.
We also tested a model where we interpolated the
three cache models to gain information from each
of the unigram, bigram, and trigram cache mod-
els. For completeness, this generalized model is de-
scribed in equation 2 under the usual constraints that?
i ?i(h) = 1 for all h.
p(w|h) = ?1(h) ? ptri(wi|wi?2wi?1)
+ ?2(h) ? pbi(wi|wi?1)
+ ?3(h) ? puni(wi)
+ ?4(h) ? 1|V |+1
+ ?5(h) ? ptric(wi|wi?2wi?1)
+ ?6(h) ? pbic(wi|wi?1)
+ ?7(h) ? punic(wi)
(2)
Those models were trained from splits of the
Canadian Hansard corpus. The base ngram model
was estimated with a 30M word split of the corpus.
The weighting coefficients of both the base trigram
and the cache models were estimated with an EM
algorithm trained with 1M words.
We tested our models, translating from English
to French, on two corpora of different types: the
first one hansard is a document taken from the
same large corpus that was used for training (the
testing and training corpora were exclusive splits).
The second one sniper, which describes the job
of a sniper, is from another domain characterized
by lexical and phrasal constructions very different
from those used to estimate the probabilities of our
models.
Table 1 shows the perplexity on the hansard
and the sniper corpora. Preliminary experiments
led us to two sizes of cache which seemed promis-
ing: 2000 and 5000 corresponding to the last 2000
and 5000 words seen during the processing of a doc-
ument. The BI column gives the results of the bi-
gram cache model and the 1+2+3 gives the results
of the interpolated cache model which included the
unigram, bigram and trigram cache.
The results show that our models improve the
base static model by 5% on documents supposedly
well known by the models and by more that 52%
on documents that are unknown to the model. Sec-
tion 5 puts these results in the perspective of our
actual IMT system. Note that he addition of a cache
component to a language model involves negligible
extra training time.
Taille BI ? 1+2+3 ?
base hansard=17.6584
2000 16.937 -4.1% 16.840 -4.6%
5000 16.903 -4.3% 16.777 -5.0%
base sniper=135.808
2000 73.936 -45.6% 67.780 -50.1%
5000 70.514 -48.1% 64.204 -52.7%
Table 1: Perplexities of the MDI2B model with a
cache component included in the reference distribu-
tion on the hansard and sniper corpora.
4 Translation model adaptation
With those excellent results in mind, we extended
the idea of dynamic adaptation to the bilingual case
which, to our knowledge, has never been tried be-
fore.
We developed a model called MDI2BCache
which is a MDI2B model to which we added a cache
component based on word pairs. Recall that, when
predicting a word w at a certain point in a document,
the probability depends on the weights of the pairs
(s, w) for each active word s in the current source
sentence. As the prediction of the words of the doc-
ument goes on, our model keeps in a cache each
active pair used for the prediction of each word. In
the example above, if the translator accepts the word
banane, then the two pairs (fruit, banana) and (ba-
nane, banana) will be added to the cache.
We added a new feature to the MEMD model to
take into account the presence of a certain pair in
the recent history of the processed document:
fcache st(w,h, s) =
?
????
????
1 if
?
??
??
s ? s,
t = w,
(s, t) ? cache
?st > p
0 otherwise
We added a threshold value p to the feature func-
tion because while analyzing the pair weights, we
discovered that low weight pairs are usually pairs of
utility words such as conjunctions and punctuation.
We also came to the conclusion that they are not the
kind of words we want to have in the cache, since
their presence in a sentence implies little about their
presence in the next.
The resulting model is of the form:
p(w|h, s) =
q(w|h)exp(
?
s?s ?sw + ?AB + ?sw)
Z(h, s)
Thus, every fcache sw has a corresponding weight
?sw for the calculation of the probability of w.
Size 0.3 ? 0.5 ? 0.7 ?
base One feature weight, no Viterbi orig perp=17.6584
1000 17.5676 -0.51% 17.5756 -0.47% 17.5983 -0.34%
2000 17.5698 -0.50% 17.5766 -0.46% 17.5976 -0.34%
5000 17.5743 -0.48% 17.5776 -0.46% 17.5965 -0.35%
10000 17.5777 -0.46% 17.5791 -0.45% 17.5962 -0.35%
base One feature weight per pair, no Viterbi orig perp=17.6584
1000 17.5817 -0.43% 17.5858 -0.41% 17.6065 -0.29%
2000 17.5933 -0.37% 17.5918 -0.38% 17.6061 -0.30%
5000 17.5849 -0.42% 17.5874 -0.40% 17.6076 -0.29%
10000 17.5890 -0.39% 17.5891 -0.39% 17.6069 -0.29%
base One feature weight, Viterbi orig perp=17.6584
1000 17.5602 -0.56% 17.5697 -0.50% 17.5940 -0.36%
2000 17.5676 -0.51% 17.5695 -0.50% 17.5896 -0.39%
5000 17.5614 -0.55% 17.5687 -0.51% 17.5925 -0.37%
10000 17.5650 -0.53% 17.5687 -0.51% 17.5906 -0.38%
Table 2: MDI2BCache test perplexities. One feature weight, Viterbi alignment version.
4.1 Number of cache features
We implemented two versions of the model, one in
which we estimated only one cache feature weight
for the whole model and another in which we esti-
mated one cache feature weight for every word pair
in the model.
The first model is simpler and is easier to esti-
mate. The assumption is made that every pair in the
model has the same tendency to repeat itself.
The second model doubles the number of word-
pair parameters compared to MDI2B, and thus leads
to a linear increase in training time. Extra training
time is negligible in the first model.
4.2 Word alignment
One of the main difficulties of automatic MT is de-
termining which source word(s) translate to which
target word(s). It is very difficult to do this task
automatically, in part because it is also very diffi-
cult manually. If a pair of sentences are given to
10 translators for alignment, the results would likely
not be identical in all cases. As it is nearly impossi-
ble to determine such an alignment, most translation
models consider every source word to have an effect
on the translation of every target word.
This difficulty shows up in our cache-based
model. When adding word pairs to the cache, we
ideally would like to add only word pairs that were
really in a translation relation in the given sentence.
This is why we also implemented a version of our
model in which a word alignment is first carried out
in order to select good pairs to be added to the cache.
For this purpose, we computed a Viterbi alignment
based on an IBM model 2. This results in a subset of
the good active pairs to be added to the cache. The
Viterbi algorithm gives us a higher confidence level
that the pair of words added to the cache were really
in a translation relation. But it can also lead to word
pairs not added to the cache that should have been
added.
4.3 Results
Table 2 shows the results of the different configura-
tions of the MDI2BCache model. For every config-
uration we trained and tested on splits of the Cana-
dian Hansard with threshold values of 0.3, 0.5, and
0.7 and cache sizes of 1000, 2000, 5000, and 10000.
The top of the table is the version of the model with
only one feature weight without Viterbi alignment.
The middle of the table is the version with one fea-
ture weight per word pair without Viterbi alignment.
Finally, the bottom is for the version with only one
feature weight and a Viterbi alignment made prior
to adding pairs to the cache.
Threshold values of 0.3, 0.5, and 0.7 led to 75%,
50%, and 25% of the pairs considered for addition
to the cache respectively. The results show that the
threshold values of 0.5 and 0.7 are removing too
many pairs. The best results are obtained with a
threshold of 0.3 in all tests. Since the number of
pairs kept in the model appears to vary in proportion
to the threshold value, we did not consider it neces-
sary to use an automatic search algorithm to find an
optimal threshold value. The gain in performance
would have been negligible.
The results also show that having one feature
weight per word pair leads to lower results. This
can be explained by the fact that it is much more
Size 0.3 ? 0.5 ?
base MDI2B=135.808
1000 132.865 -2.17% 132.751 -2.25%
2000 132.771 -2.23% 132.752 -2.25%
5000 132.733 -2.26% 132.628 -2.34%
10000 132.997 -2.07% 132.674 -2.31%
Table 3: MDI2BCache test perplexities. One fea-
ture weight, Viterbi alignment version. Sniper test
difficult to estimate a weight for every pair that one
weight for all pairs. Since we use only thousands of
words in the cache, the training process suffers from
a poor data representation.
The Viterbi alignment seems to be helping the
models. The best results are obtained with the ver-
sion of our model with Viterbi alignment. However,
this gives only a 0.56% percent drop in perplexity.
We then tested our best configuration on the
sniper corpus. Table 3 shows the results. We
dropped threshold value 0.7 and tested only the
model with only one feature weight and a Viterbi
alignment.
Results show that our bilingual cache model
shows improvement (four times higher) in drop of
perplexity when used on documents very different
from the training corpus. In general, results give
lower perplexity than our base model showing that
the bilingual cache is helpful to the model, but the
results are not as good as that the ones obtained in
the unilingual case. Section 6 discusses these results
further.
5 Evaluation of IMT
As stated earlier, drops in perplexity are theoreti-
cal results that have been obtained previously in the
case of unilingual dynamic adaptation but for which
a corresponding level of practical success was rarely
attained because of the cache correctness problem.
To show that the interactive nature of our assisted-
translation application can really benefit from dy-
namic adaptation, we tested our models in a more
realistic translation context. This test consists of
simulating a translator using the IMT system as it
proposes words and phrases and accepting, correct-
ing or rejecting the proposals by trying to reproduce
a given target translation (Foster et al, 2002). The
metric used is the percentage of keystrokes saved
by the use of the system instead of having to type
directly all the target text.
For these simulations, we used only a 10K word
split of the hansard and of the sniper cor-
pus. The reason is that the IMT application poten-
Taille BI ? 1+2+3 ?
base hansard=27.435
2000 27.784 +1.3% 27.719 +1.0%
5000 27.837 +1.5% 27.821 +1.4%
base sniper=9.686
2000 11.404 +15.1% 11.294 +14.2%
5000 11.498 +15.8% 11.623 +16.7%
Table 4: Saved keystrokes raises for the MDI2B
model with cache component in the reference dis-
tribution on the hansard and sniper corpora.
0.3 ?
base hansard=27.4358
1000 27.557 +0.44%
2000 27.531 +0.35%
5000 27.488 +0.18%
10000 27.468 +0.12%
base sniper=9.686
1000 9.896 +2.17%
2000 10.023 +3.48%
5000 9.983 +3.07%
10000 9.957 +2.80%
Table 5: Saved keystrokes raises for the
MDI2BCache model with only one feature
weight and Viterbi alignment on the hansard and
sniper corpora.
tially proposes new completions after every charac-
ter typed by the user. For a 10K word document, it
needs to search about 1 million times for high prob-
ability words and phrases. This leads to relatively
long simulation times, even though predictions are
made at real time speeds.
Table 4 shows the results obtained with the
MDI2B model to which we added a cache compo-
nent for the reference interpolated trigram distribu-
tion.
We can see that the saved keystroke percentages
are proportional to the perplexity drops reported in
section 3. The use of our models raises the saved
keystrokes by nearly 1.5% in the case of well known
documents and by nearly 17% in the case of very
different documents. These are very interesting re-
sults for a potential professional use of TransType.
Table 5 shows an increase in the number of saved
keystrokes: 0.44% on the hansard and 3.5% on
the sniper corpora. Once again, the results are
not as impressive as the ones obtained for the mono-
lingual dynamic adaptation case.
6 Discussion
The results presented in section 3 on language
model adaptation confirmed what had been reported
in the literature: adding a cache component to a lan-
guage model leads to a drop in perplexity. More-
over, we were able to demonstrate that using a
cache-based language model inside a translation
model leads to better performance for the whole
translation model. We obtained drops in perplexity
of 5% on a corpus of the same type as the training
corpus and of 50% on a different one. These theo-
retical results lead to very good practical results. We
were able to increase the saved keystroke percent-
age by 1.5% on the similar corpus as the training
and by nearly 17% on the different corpus. These
results confirm our hypothesis that dynamic adapta-
tion with cache-based language model can be useful
in the context of IMT, particularly for new types of
texts.
Results presented in section 4 on translation
model adaptation show that our approach has led
to drops in perplexity although not as high as we
would have hoped. To understand these disappoint-
ing results, we analyzed the content of the cache for
different configurations of our MDI2BCache model.
base 0.3 viterbi + 0.3
(is,qu?) (to,afin) (offence,crime)
(.,sa) (was,a) (was,e?te?)
(this,,) (UNK,UNK) (very,tre`s)
(all,toutes) (piece,le?gislative) (today,aujourd?hui)
(have,du) (this,ce) (jobs,emploi)
(the,pour) (per,100) (concern,inquie?tude)
(on,du) (that,soient) (skin,peau)
(of,un) (,,,) (there,y)
(we,nous) (?,il) (government,le)
(the,du) (any,tout) (an,un)
18 68 86
Table 6: Cache sampling of different configurations
of MDI2BCache model.
Table 6 shows the results of our sampling. We
tested three model configurations. The first one, in
the first column, was the base MDI2BCache model
which adds all active pairs to the cache. The second
configuration, in the second column, was a thresh-
old value of 0.3 that brings about 75% of the pairs
being added to the cache. The last configuration was
a model with threshold value of 0.3 and a Viterbi
alignment made prior to the addition of pairs in the
cache. The three model configuration were with
only one feature weight. For all three configura-
tions, we took a sample of 10 pairs (shown in table
6) and a sample of 100 pairs. With the second sam-
ple, we manually analyzed each pair and counted
the number of pairs (shown in the last row of the ta-
ble) we believed were useful for the model (words
that are occasionally translations of one another).
The results obtained in section 4 seem to agree
with the current analysis. From left to right in the ta-
ble, the pairs seem to contain more information and
to be more appropriate additions to the cache. The
configuration with Viterbi alignment which contains
86 good pairs clearly seems to be the configuration
with the most interesting pairs.
The problem with such a cache-based translation
model seem to be similar to the balance between
precision and recall in information retrieval. On one
hand, we want to add in the cache every word pair
in which the two words are in translation relation in
the text. We further want to add only the pairs in
which the two words are really in translation rela-
tion in the text. It seems that with our base model,
we add most of the good pairs, but also a lot of bad
ones. With the Viterbi alignment and a threshold
value of 0.3, most of the pairs added are good ones,
but we are probably missing a number of other ap-
propriate ones. This comes back to the task of word
alignment, which is a very difficult task for comput-
ers (Mihalcea and Pedersen, 2003).
Moreover, we would want to add in the cache
only those words for which more than one transla-
tion is possible. For example, the pair (today, au-
jourd?hui), though it is a very useful pair for the
base model, is unlikely to help when added to the
cache. The reason is simple: they are two words
that are always translations of one another, so the
model will have no problem predicting them. This
ideal of precision and recall and of useful pairs in
the cache is obtained by our model with threshold
of 0.3, a Viterbi alignment and a cache size of 1000.
One disadvantage of our bilingual adaptive model
is the way it handles unknown words. In the cache-
based language model, the unknown words were
dealt with normally, i.e. they were added to the
cache and given a certain probability afterwards.
So, if an unknown word was seen in a certain sen-
tence and then later on, it would receive a proba-
bility mass of its own but not the one given to any
unknown word. By having its own probability mass
due to its presence in the cache, such previously un-
known word can be predicted by the model. In the
case of our MDI2BCache model, because we have
not yet implemented an algorithm for guessing the
translations of unknown words, they are simply rep-
resented within the model as UNK words, which
means that the model never learns them.
The results obtained with the sniper corpus
shows us that dynamic adaptation is also more help-
ful for documents that are little known to the model
in the bilingual context. The results are four times
better on the sniper corpus than on the Hansard
testing corpus.
Once again for the bilingual case, the practical
test results in the number of saved keystrokes agree
with the theoretical results of drops in perplexity.
This result shows that bilingual dynamic adaptation
also can be implemented in a practical context and
obtain results similar to the theoretical results.
All things considered, we believe that a cache-
based translation model shows a great potential
for bilingual adaptation and that greater perplexity
drops and keystroke savings could be obtained by
either reengineering the model or by improving the
MDI2BCache model.
6.1 Key improvements to the model
Following the analysis of the results obtained by our
model, we have pointed out some key improvements
that the model would need in order to get better re-
sults. In this list we focus on ways of improving
adaptation strategies for the current model, omitting
other obvious enhancements such as adding phrase
translations.
Unknown word processing Learning new words
would be a very important feature to add to
the model and would lead to better results. We
did not incorporate the processing of unknown
words in the MDI2BCache because the struc-
ture of model did not lend itself to this addi-
tion. Especially with documents such as the
sniper corpus, we believe that this could
be a key improvement for a dynamic adaptive
model.
Better alignment As mentioned before, the ulti-
mate goal for our cache is that it contains only
the pairs present in the perfect alignment. Bet-
ter performance from the alignment would lead
to pairs in the cache closer to this ideal. In this
study we computed Viterbi alignments from an
IBM model 2, because it is very efficient to
compute and also because for training MDI2B,
we do use the IBM model 2. We could consider
also more advanced word alignment models
(Och and Ney, 2000; Lin and Cherry, 2003;
Moore, 2001). To keep the alignment model
simple, we could still use an IBM model 2, but
with the compositionality constraint that has
been shown to give better word alignment than
the Viterbi one (Simard and Langlais, 2003).
Feature weights We implemented two versions of
our model: one with only one feature weight
and another with one feature weight for each
word pair. The second model suffered from
poor data representation and our training algo-
rithm wasn?t able to estimate good cache fea-
ture weights. We think that creating classes
of word pairs, such as it was done for posi-
tional alignment features, would lead to better
results. It would enable the model to take into
account the tendency that a pair has to repeat
itself in a document.
Relative weighting Another key improvement is
that changes to word-pair weights should be
relative to each source word. For example,
if (house, maison) is a pair in the cache, we
would like to favour maison over possible al-
ternatives such as chambre as a translation of
house. In the existing model this is done by
boosting the weight on (house,maison), which
has the undesirable side-effect of making mai-
son more important in the model than transla-
tions of other source words in the current sen-
tence which have not appeared in the cache.
One way of eliminating this behaviour would
be to learn negative weights on alternatives like
(house,chambre) which do not appear in the
cache.
We believe these improvements would better show
the potential of bilingual dynamic adaptation.
7 Conclusion
We have presented dynamic adaptive translation
models using cache-based implementations. We
have shown that monolingual dynamic adaptive
models exhibit good theoretical performance in a
bilingual translation context. We observed that
these theoretical results carry over to practical gains
in the context of an IMT application.
We have developed bilingual dynamic adaptation
through a cache-based translation model. Our re-
sults show the potential of bilingual dynamic adap-
tation. We have given explanations about why the
results obtained are not as high as hoped and pre-
sented some key improvements that should be made
to our model or should be taken into account in the
development of a new model.
We believe that this study reveals the potential for
adaptive interactive machine translation system and
we hope to read similar reports for other implemen-
tations of the same interactive scenario e.g. (Och et
al., 2003).
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy
approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39?71.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Jelinek,
Robert L. Mercer, and Paul Roossin. 1988. A
statistical approach to language translation. In
Proceedings of the International Conference on
Computational Linguistics (COLING), pages 71?
76, Budapest, Hungary, August.
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993.
The mathematics of Machine Translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?312, June.
P.R. Clarkson and P.R. Robertson. 1997. Language
model adaptation using mixtures and an expo-
nentially decaying cache. In IEEE Int. Confer-
ence on Acoustics, Speech, and Signal Process-
ing, Munich.
George Foster, Philippe Langlais, and Guy La-
palme. 2002. User-friendly text prediction
for translators. In 2002 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP 2002), Philadelphia, July. TT2
TransType2.
George Foster. 2000. A Maximum Entropy / Mini-
mum Divergence translation model. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages
37?42, Hong Kong, October.
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recog-
nition. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 12(6):570?
583, June.
Dekang Lin and Colin Cherry. 2003. Proalign:
Shared task system description. In NAACL 2003
Workshop on Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,
pages 11?14, Edmonton Canada, May 31. TT2.
S.C. Martin, J. Liermann, and H. Ney. 1997. Adap-
tative topic-dependent language modelling using
word-based varigrams. In Eurospeech.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In Rada Mihal-
cea and Ted Pedersen, editors, HLT-NAACL 2003
Workshop: Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,
pages 1?10, Edmonton, Alberta, Canada, May
31. Association for Computational Linguistics.
Robert C. Moore. 2001. Towards a simple and
accurate statistical approach to learning transla-
tion relationships among words. In Workshop on
Data-driven Machine Translation, 39th Annual
Meeting and 10th Conference of the European
Chapter, pages 79?86, Toulouse, France. Asso-
ciation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages
440?447, Hong Kong, October.
F.J. Och, R. Zens, and H. Ney. 2003. Efficient
search for interactive statistical machine trans-
lation. In Proceedings of the 10th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 387?
393, Budapest, Hungary, April. TT2.
Michel Simard and Philippe Langlais. 2003. Statis-
tical translation alignment with compositionality
constraints. In NAACL 2003 Workshop on Build-
ing and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 19?22, Ed-
monton Canada, May 31. TT2.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129?132,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
PORTAGE: A Phrase-based Machine Translation System 
 
Fatiha Sadat+, Howard Johnson++, Akakpo Agbago+, George Foster+,               
Roland Kuhn+, Joel Martin++ and Aaron Tikuisis?
 
+ NRC Institute for Information 
Technology 
101 St-Jean-Bosco Street  
Gatineau, QC K1A 0R6, Canada 
++ NRC Institute for Information 
Technology 
1200 Montreal Road  
Ottawa, ON K1A 0R6, Canada 
?University of Waterloo 
200 University Avenue W., 
Waterloo, Ontario, Canada 
 
firstname.lastname@cnrc-nrc.gc.ca aptikuis@uwaterloo.ca  
 
Abstract 
This paper describes the participation of 
the Portage team at NRC Canada in the 
shared task1 of ACL 2005 Workshop on 
Building and Using Parallel Texts. We dis-
cuss Portage, a statistical phrase-based 
machine translation system, and present 
experimental results on the four language 
pairs of the shared task. First, we focus on 
the French-English task using multiple re-
sources and techniques. Then we describe 
our contribution on the Finnish-English, 
Spanish-English and German-English lan-
guage pairs using the provided data for the 
shared task.  
1 Introduction 
The rapid growth of the Internet has led to a rapid 
growth in the need for information exchange among 
different languages. Machine Translation (MT) and 
related technologies have become essential to the 
information flow between speakers of different lan-
guages on the Internet. Statistical Machine Transla-
tion (SMT), a data-driven approach to producing 
translation systems, is becoming a practical solution 
to the longstanding goal of cheap natural language 
processing.  
In this paper, we describe Portage, a statistical 
phrase-based machine translation system, which we 
evaluated on all different language pairs that were 
provided for the shared task.  As Portage is a very 
                                                           
1 http://www.statmt.org/wpt05/mt-shared-task/ 
new system, our main goal in participating in the 
workshop was to test it out on different language 
pairs, and to establish baseline performance for the 
purpose of comparison against other systems and 
against future improvements.  To do this, we used a 
fairly standard configuration for phrase-based SMT, 
described in the next section. 
Of the language pairs in the shared task, French-
English is particularly interesting to us in light of 
Canada?s demographics and policy of official bilin-
gualism. We therefore divided our participation into 
two parts: one stream for French-English and an-
other for Finnish-, German-, and Spanish-English. 
For the French-English stream, we tested the use of 
additional data resources along with hand-coded 
rules for translating numbers and dates. For the 
other streams, we used only the provided resources 
in a purely statistical framework (although we also 
investigated several automatic methods of coping 
with Finnish morphology). 
The remainder of the paper is organized as fol-
lows. Section 2 describes the architecture of the 
Portage system, including its hand-coded rules for 
French-English.  Experimental results for the four 
pairs of languages are reported in Section 3. Section 
4 concludes and gives pointers to future work. 
2 Portage  
Portage operates in three main phases: preprocess-
ing of raw data into tokens, with translation sugges-
tions for some words or phrases generated by rules; 
decoding to produce one or more translation hy-
potheses; and error-driven rescoring to choose the 
best final hypothesis. (A fourth postprocessing 
phase was not needed for the shared task.) 
129
2.1 Preprocessing 
Preprocessing is a necessary first step in order to 
convert raw texts in both source and target lan-
guages into a format suitable for both model train-
ing and decoding (Foster et al, 2003).  For the 
supplied Europarl corpora, we relied on the existing 
segmentation and tokenization, except for French, 
which we manipulated slightly to bring into line 
with our existing conventions (e.g., converting l ? 
an  into l? an).  For the Hansard corpus used to 
supplement our French-English resources (de-
scribed in section 3 below), we used our own 
alignment based on Moore?s algorithm (Moore, 
2002), segmentation, and tokenization procedures. 
Languages with rich morphology are often prob-
lematic for statistical machine translation because 
the available data lacks instances of all possible 
forms of a word to efficiently train a translation sys-
tem. In a language like German, new words can be 
formed by compounding (writing two or more 
words together without a space or a hyphen in be-
tween). Segmentation is a crucial step in preproc-
essing languages such as German and Finnish texts.
In addition to these simple operations, we also 
developed a rule-based component to detect num-
bers and dates in the source text and identify their 
translation in the target text. This component was 
developed on the Hansard corpus, and applied to the 
French-English texts (i.e. Europarl and Hansard), on 
the development data in both languages, and on the 
test data. 
2.2 Decoding 
Decoding is the central phase in SMT, involving a 
search for the hypotheses t that have highest prob-
abilities of being translations of the current source 
sentence s according to a model for P(t|s). Our 
model for P(t|s) is a log-linear combination of four 
main components: one or more trigram language 
models, one or more phrase translation models, a 
distortion model, and a word-length feature. The 
trigram language model is implemented in the 
SRILM toolkit (Stolcke, 2002). The phrase-based 
translation model is similar to the one described in 
(Koehn, 2004), and relies on symmetrized IBM 
model 2 word-alignments for phrase pair induction. 
The distortion model is also very similar to 
Koehn?s, with the exception of a final cost to ac-
count for sentence endings.  
s
To set weights on the components of the log-
linear model, we implemented Och?s algorithm 
(Och, 2003).  This essentially involves generating, 
in an iterative process, a set of nbest translation hy-
potheses that are representative of the entire search 
space for a given set of source sentences. Once this 
is accomplished, a variant of Powell?s algorithm is 
used to find weights that optimize BLEU score 
(Papineni et al 2002) over these hypotheses, com-
pared to reference translations. Unfortunately, our 
implementation of this algorithm converged only 
very slowly to a satisfactory final nbest list, so we 
used two different ad hoc strategies for setting 
weights: choosing the best values encountered dur-
ing
, with the exception of a 
ch as the ability to decode either 
w ards.  
 transla-
 
rent language pairs of the 
sha d t
hared t
- 
 the iterations of Och?s algorithm (French-
English), and a grid search (all other languages).  
To perform the actual translation, we used our 
decoder, Canoe, which implements a dynamic-
programming beam search algorithm based on that 
of Pharaoh (Koehn, 2004). Canoe is input-output 
compatible with Pharaoh
few extensions su
back ards or forw
2.3 Rescoring 
To improve raw output from Canoe, we used a 
rescoring strategy: have Canoe generate a list of 
nbest translations rather than just one, then reorder 
the list using a model trained with Och?s method to 
optimize BLEU score. This is identical to the final 
pass of the algorithm described in the previous sec-
tion, except for the use of a more powerful log-
linear model than would have been feasible to use 
inside the decoder. In addition to the four basic fea-
tures of the initial model, our rescoring model in-
cluded IBM2 model probabilities in both directions 
(i.e., P(s|t) and P(t|s)); and an IBM1-based feature 
designed to detect whether any words in one lan-
guage seemed to be left without satisfactory
tions in the other language. This missing-word
feature was also applied in both directions. 
3 Experiments on the Shared Task 
We conducted experiments and evaluations on 
Portage using the diffe
re ask. The training data was provided for the 
ask as follows:  
Training data of 688,031 sentences in 
French and English. A similarly sized cor-
130
pus is provided for Finnish, Spanish and 
German with matched English translations. 
orpus was used to generate both 
lan
e translations into English, was 
 
 Portage for a comparative study ex-
ploiting and combining different resources and 
tec
 
3. arl corpus 
4. 
rd corpora as training data and 
 
t  mod est  
p icipation at th h-English tas 9.53. 
od D  Decoding+Rescoring
- Development test data of 2,000 sentences in 
the four languages.  
In addition to the provided data, a set of 
6,056,014 sentences extracted from Hansard corpus, 
the official record of Canada?s parliamentary de-
bates, was used in both French and English lan-
guages. This c
guage and translation models for use in decoding 
and rescoring. 
The development test data was split into two 
parts: The first part that includes 1,000 sentences in 
each language with reference translations into Eng-
lish served in the optimization of weights for both 
the decoding and rescoring models. In this study, 
number of n-best lists was set to 1,000. The second 
part, which includes 1,000 sentences in each lan-
guage with referenc
used in the evaluation of the performance of the
translation models. 
3.1 Experiments on the French-English Task 
Our goal for this language pair was to conduct ex-
periments on
hniques:  
1. Method E is based on the Europarl corpus 
as training data, 
2. Method E-H is based on both Europarl and 
Hansard corpora as training data, 
Method E-p is based on the Europ
as training data and parsing numbers and 
dates in the preprocessing phase, 
Method E-H-p is based on both Europarl 
and Hansa
parsing numbers and date in the preprocess-
ing phase. 
Results are shown in Table 1 for the French-
English task. The first column of Table 1 indicates 
the method, the second column gives results for 
decoding with Canoe only, and the third column for 
decoding and rescoring with Canoe. For comparison 
between the four methods, there was an improve-
ment in terms of BLEU scores when using two lan-
guage models and two translation models generated 
from Europarl and Hansard corpora; however, pars-
ing numbers and dates had a negative impact on the
ranslation els. The b  BLEU score for our
art e Frenc k was 2
Meth ecoding
E 27.71 29.22 
E-H 28.71 29.53 
E-p 26.45 28.21 
E-H-p 28.29 28.56 
Ta
ed 
f 
of increased trade within North 
merica but also functions as a good counterpoint 
for French-English. 
 
ble 1. BLEU scores for the French-English test 
sentences  
 
A noteworthy feature of these results is that the 
improvement given by the out-of-domain Hansard 
corpus was very slight. Although we suspect that 
somewhat better performance could have been 
achieved by better weight optimization, this result 
clearly underscores the importance of matching 
training and test domains. A related point is that our 
number and date translation rules actually caused a 
performance drop due to the fact that they were op-
timized for typographical conventions prevalent in 
Hansard, which are quite different from those used 
in Europarl. 
Our best result ranked third in the shared 
WPT05 French-English task , with a difference of 
0.74 in terms of BLEU score from the first rank
participant, and a difference of 0.67 in terms o
BLEU score from the second ranked participant. 
3.2 Experiments on other Pairs of Languages 
The WPT05 workshop provides a good opportunity 
to achieve our benchmarking goals with corpora 
that provide challenging difficulties. German and 
Finnish are languages that make considerable use of 
compounding. Finnish, in addition, has a particu-
larly complex morphology that is organized on 
principles that are quite different from any in Eng-
lish. This results in much longer word forms each of 
which occurs very infrequently. 
Our original intent was to propose a number of pos-
sible statistical approaches to analyzing and split-
ting these word forms and improving our results. 
Since none of these yielded results as good as the 
baseline, we will continue this work until we under-
stand what is really needed. We also care very 
much about translating between French and English 
in Canada and plan to spend a lot of extra effort on 
difficulties that occur in this case. Translation be-
tween Spanish and English is also becoming more 
mportant as a result i
A
131
Language Pair Decoding+Rescoring
Finnish-English 20.95 
German-English 23.21 
Spanish English 29.08 
Ta
and 1.56 in 
m ores, respectively, compared to 
the first ranked participant.   
l 
ation, greater use of morphological 
R
Fr
Meeting of the Association for Computational 
Fr
Statistical Machine Transla-
Ge
id 
Ke
e 
Ki
al Meeting of the Association for Com-
M
ne Trans-
Oc
 of the 40th Annual Meet-
Fr
 Proceedings of 
Ph parl: A multilingual corpusfor 
 P
ation 
Models. In Proceedings of the Association for Ma-
chine Translation in the Americas AMTA 2004. 
ble 2 BLEU scores for the Finnish-English, Ger-
man-English and Spanish-English test sentences  
 
To establish our baseline, the only preprocessing 
we did was lowercasing (using the provided tokeni-
zation). Canoe was run without any special settings, 
although weights for distortion, word penalty, lan-
guage model, and translation model were optimized 
using a grid search, as described above. Rescoring 
was also done, and usually resulted in at least an 
extra BLEU point.  
Our final results are shown in Table 2. Ranks at 
the shared WPT05 Finnish-, German-, and Spanish-
English tasks were assigned as second, third and 
fourth, with differences of 1.06, 1.87 
ter s of BLEU sc
4 Conclusion 
We have reported on our participation in the shared 
task of the ACL 2005 Workshop on Building and 
Using Parallel Texts, conducting evaluations of 
Portage, our statistical machine translation system, 
on all four language pairs. Our best BLEU scores 
for the French-, Finnish-, German-, and Spanish-
English at this stage were 29.5, 20.95, 23.21 and 
29.08, respectively. In total, eleven teams took part 
at the shared task and most of them submitted re-
sults for all pairs of languages.  Our results distin-
guished the NRC team at the third, second, third 
and fourth ranks with slight differences with the 
first ranked participants. 
A major goal of this work was to evaluate Port-
age at its first stage of implementation on different 
pairs of languages. This evaluation has served to 
identify some problems with our system in the areas 
of weight optimization and number and date rules. 
It has also indicated the limits of using out-of-
domain corpora, and the difficulty of morphologi-
cally complex languages like Finnish. 
Current and planned future work includes the 
exploitation of comparable corpora for statistica
machine transl
knowledge, and better features for nbest rescoring. 
eferences 
Andreas Stolcke. 2002. SRILM - an Extensible Language 
Modeling Toolkit. In ICSLP-2002, 901-904. 
anz Josef Och, Hermann Ney. 2000. Improved Statisti-
cal Alignment Models. In Proceedings of the 38th An-
nual 
Linguistics, Hong Kong, China, October 2000, 440-
447. 
anz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, Dragomir Radev. 2004. A Smor-
gasbord of Features for 
tion. In Proceeding of the HLT/NAACL 2004, 
Boston, MA, May 2004. 
orge Foster, Simona Gandrabur, Philippe Langlais, 
Pierre Plamondon, Graham Russell and Michel Si-
mard. 2003. Statistical Machine Translation: Rap
Development with Limited Resources. In Proceedings 
of MT Summit IX 2003, New Orleans, September.  
vin Knight, Ishwar Chander, Matthew Haines, Va-
sileios Hatzivassiloglou, Eduard Hovy, Masayo Iida, 
Steve K. Luk, Richard Whitney, and Kenji Yamada. 
1995. Filling Knowledge Gaps in a Broad-Coverag
MT System. In Proceedings of the International Joint 
Conference on Artificial Intelligence (IJCAI), 1995. 
shore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings of 
the 40th Annu
putational Linguistics ACL, Philadelphia, July 2002, 
pp. 311-318. 
oore, Robert. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Machine Transla-
tion: From Research to Real Users (Proceedings of the 
5th Conference of the Association for Machi
lation in the Americas, Tiburon, California), Springer-
Verlag, Heidelberg, Germany, pp. 135-244. 
h, F. J. and H. Ney. 2002. Discriminative Training 
and Maximum Entropy Models for Statistical Machine 
Translation. In Proceedings
ing of the Association for Computational Linguistics, 
Philadelphia, pp. 295?302. 
anz Josef Och, 2003. Minimum Error Rate Training 
for Statistical Machine Translation. In
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, July. 
ilipp Koehn. 2002. Euro
evaluation of machine translation. Ms., University of 
Southern California. 
hilipp Koehn. 2004. Pharaoh: a Beam Search Decoder 
for Phrase-based Statistical Machine Transl
132
Proceedings of the Workshop on Statistical Machine Translation, pages 134?137,
New York City, June 2006. c?2006 Association for Computational Linguistics
PORTAGE: with Smoothed Phrase Tables
and Segment Choice Models
Howard Johnson
National Research Council
Institute for Information Technology
Interactive Information
1200 Montreal Road
Ottawa, ON, Canada K1A 0R6
Howard.Johnson@cnrc-nrc.gc.ca
Fatiha Sadat, George Foster, Roland Kuhn,
Michel Simard, Eric Joanis and Samuel Larkin
National Research Council
Institute for Information Technology
Interactive Language Technologies
101 St-Jean-Bosco Street
Gatineau, QC, Canada K1A 0R6
firstname.lastname@cnrc-nrc.gc.ca
Abstract
Improvements to Portage and its partici-
pation in the shared task of NAACL 2006
Workshop on Statistical Machine Trans-
lation are described. Promising ideas in
phrase table smoothing and global dis-
tortion using feature-rich models are dis-
cussed as well as numerous improvements
in the software base.
1 Introduction
The statistical machine translation system Portage is
participating in the NAACL 2006 Workshop on Sta-
tistical Machine Translation. This is a good opportu-
nity to do benchmarking against a publicly available
data set and explore the benefits of a number of re-
cently added features.
Section 2 describes the changes that have been
made to Portage in the past year that affect the par-
ticipation in the 2006 shared task. Section 3 outlines
the methods employed for this task and extensions
of it. In Section 4 the results are summarized in tab-
ular form. Following these, there is a conclusions
section that highlights what can be gleaned of value
from these results.
2 Portage
Because this is the second participation of Portage in
such a shared task, a description of the base system
can be found elsewhere (Sadat et al 2005). Briefly,
Portage is a research vehicle and development pro-
totype system exploiting the state-of-the-art in sta-
tistical machine translation (SMT). It uses a custom
built decoder followed by a rescoring module that
adjusts weights based on a number of features de-
fined on the source sentence. We will devote space
to discussing changes made since the 2005 shared
task.
2.1 Phrase-Table Smoothing
Phrase-based SMT relies on conditional distribu-
tions p(s|t) and p(t|s) that are derived from the joint
frequencies c(s, t) of source/target phrase pairs ob-
served in an aligned parallel corpus. Traditionally,
relative-frequency estimation is used to derive con-
ditional distributions, ie p(s|t) = c(s, t)/
?
s c(s, t).
However, relative-frequency estimation has the
well-known problem of favouring rare events. For
instance, any phrase pair whose constituents occur
only once in the corpus will be assigned a probabil-
ity of 1, almost certainly higher than the probabili-
ties of pairs for which much more evidence exists.
During translation, rare pairs can directly compete
with overlapping frequent pairs, so overestimating
their probabilities can significantly degrade perfor-
mance.
To address this problem, we implemented two
simple smoothing strategies. The first is based on
the Good-Turing technique as described in (Church
and Gale, 1991). This replaces each observed joint
frequency c with cg = (c + 1)nc+1/nc, where nc
is the number of distinct pairs with frequency c
(smoothed for large c). It also assigns a total count
mass of n1 to unseen pairs, which we distributed
in proportion to the frequency of each conditioning
134
phrase. The resulting estimates are:
pg(s|t) =
cg(s, t)
?
s cg(s, t) + p(t)n1
,
where p(t) = c(t)/
?
t c(t). The estimates for
pg(t|s) are analogous.
The second strategy is Kneser-Ney smoothing
(Kneser and Ney, 1995), using the interpolated vari-
ant described in (Chen and Goodman., 1998):1
pk(s|t) =
c(s, t) ? D + D n1+(?, t) pk(s)
?
s c(s, t)
where D = n1/(n1 + 2n2), n1+(?, t) is the num-
ber of distinct phrases s with which t co-occurs, and
pk(s) = n1+(s, ?)/
?
s n1+(s, ?), with n1+(s, ?)
analogous to n1+(?, t).
Our approach to phrase-table smoothing contrasts
to previous work (Zens and Ney, 2004) in which
smoothed phrase probabilities are constructed from
word-pair probabilities and combined in a log-linear
model with an unsmoothed phrase-table. We believe
the two approaches are complementary, so a combi-
nation of both would be worth exploring in future
work.
2.2 Feature-Rich DT-based distortion
In a recent paper (Kuhn et al 2006), we presented a
new class of probabilistic ?Segment ChoiceModels?
(SCMs) for distortion in phrase-based systems. In
some situations, SCMs will assign a better distortion
score to a drastic reordering of the source sentence
than to no reordering; in this, SCMs differ from the
conventional penalty-based distortion, which always
favours less rather than more distortion.
We developed a particular kind of SCM based on
decision trees (DTs) containing both questions of a
positional type (e.g., questions about the distance
of a given phrase from the beginning of the source
sentence or from the previously translated phrase)
and word-based questions (e.g., questions about the
presence or absence of given words in a specified
phrase).
The DTs are grown on a corpus consisting of
segment-aligned bilingual sentence pairs. This
1As for Good-Turing smoothing, this formula applies only
to pairs s, t for which c(s, t) > 0, since these are the only ones
considered by the decoder.
segment-aligned corpus is obtained by training a
phrase translation model on a large bilingual cor-
pus and then using it (in conjunction with a distor-
tion penalty) to carry out alignments between the
phrases in the source-language sentence and those
in the corresponding target-language sentence in a
second bilingual corpus. Typically, the first corpus
(on which the phrase translation model is trained) is
the same as the second corpus (on which alignment
is carried out). To avoid overfitting, the alignment
algorithm is leave-one-out: statistics derived from
a particular sentence pair are not used to align that
sentence pair.
Note that the experiments reported in (Kuhn et
al, 2006) focused on translation of Chinese into En-
glish. The interest of the experiments reported here
onWMT data was to see if the feature-rich DT-based
distortion model could be useful for MT between
other language pairs.
3 Application to the Shared Task: Methods
3.1 Restricted Resource Exercise
The first exercise that was done is to replicate the
conditions of 2005 as closely as possible to see the
effects of one year of research and development.
The second exercise was to replicate all three of
these translation exercises using the 2006 language
model, and to do the three exercises of translat-
ing out of English into French, Spanish, and Ger-
man. This was our baseline for other studies. A
third exercise involved modifying the generation
of the phrase-table to incorporate our Good-Turing
smoothing. All six language pairs were re-processed
with these phrase-tables. The improvement in the
results on the devtest set were compelling. This be-
came the baseline for further work. A fourth ex-
ercise involved replacing penalty-based distortion
modelling with the feature-rich decision-tree based
distortion modelling described above. A fifth ex-
ercise involved the use of a Kneser-Ney phrase-
table smoothing algorithm as an alternative to Good-
Turing.
For all of these exercises, 1-best results after de-
coding were calculated as well as rescoring on 1000-
best lists of results using 12 feature functions (13
in the case of decision-tree based distortion mod-
elling). The results submitted for the shared task
135
were the results of the third and fourth exercises
where rescoring had been applied.
3.2 Open Resource Exercise
Our goal in this exercise was to conduct a com-
parative study using additional training data for the
French-English shared task. Results of WPT 2005
showed an improvement of at least 0.3 BLEU point
when exploiting different resources for the French-
English pair of languages. In addition to the training
resources used in WPT 2005 for the French-English
task, i.e. Europarl and Hansard, we used a bilingual
dictionary, Le Grand Dictionnaire Terminologique
(GDT) 2 to train translation models and the English
side of the UN parallel corpus (LDC2004E13) to
train an English language model. Integrating termi-
nological lexicons into a statistical machine transla-
tion engine is not a straightforward operation, since
we cannot expect them to come with attached prob-
abilities. The approach we took consists on view-
ing all translation candidates of each source term or
phrase as equiprobable (Sadat et al 2006).
In total, the data used in this second part of our
contribution to WMT 2006 is described as follows:
(1) A set of 688,031 sentences in French and En-
glish extracted from the Europarl parallel corpus (2)
A set of 6,056,014 sentences in French and English
extracted from the Hansard parallel corpus, the offi-
cial record of Canada?s parliamentary debates. (3) A
set of 701,709 sentences in French and English ex-
tracted from the bilingual dictionary GDT. (4) Lan-
guage models were trained on the French and En-
glish parts of the Europarl and Hansard. We used
the provided Europarl corpus while omitting data
from Q4/2000 (October-December), since it is re-
served for development and test data. (5) An addi-
tional English language model was trained on 128
million words of the UN Parallel corpus.
For the supplied Europarl corpora, we relied on
the existing segmentation and tokenization, except
for French, which we manipulated slightly to bring
into line with our existing conventions (e.g., convert-
ing l ? an into l? an, aujourd ? hui into aujourd?hui).
For the Hansard corpus used to supplement our
French-English resources, we used our own align-
ment based on Moore?s algorithm, segmentation,
2http://www.granddictionnaire.com/
and tokenization procedures. English preprocessing
simply included lower-casing, separating punctua-
tion from words and splitting off ?s.
4 Results
The results are shown in Table 1. The numbers
shown are BLEU scores. The MC rows correspond
to the multi-corpora results described in the open re-
source exercise section above. All other rows are
from the restricted resource exercise.
The devtest results are the scores computed be-
fore the shared-task submission and were used to
drive the choice of direction of the research. The
test results were computed after the shared-task sub-
mission and serve for validation of the conclusions.
We believe that our use of multiple training cor-
pora as well as our re-tokenization for French and
an enhanced language model resulted in our overall
success in the English-French translation track. The
results for the in-domain test data puts our group at
the top of the ranking table drawn by the organizers
(first on Adequacy and fluency and third on BLEU
scores).
5 Conclusion
Benchmarking with same language model and pa-
rameters as WPT05 reproduces the results with a
tiny improvement. The larger language model used
in 2006 for English yields about half a BLEU. Good-
Turing phrase table smoothing yields roughly half
a BLEU point. Kneser-Ney phrase table smooth-
ing yields between a third and half a BLEU point
more than Good-Turing. Decision tree based distor-
tion yields a small improvement for the devtest set
when rescoring was not used but failed to show im-
provement on the test set.
In summary, the results from phrase-table
smoothing are extremely encouraging. On the other
hand, the feature-rich decision tree distortion mod-
elling requires additional work before it provides a
good pay-back. Fortunately we have some encour-
aging avenues under investigation. Clearly there is
more work needed for both of these areas.
Acknowledgements
We wish to thank Aaron Tikuisis and Denis Yuen
for important contributions to the Portage code base
136
Table 1: Restricted and open resource results
fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
devtest: with rescoring
WPT05 29.32 29.08 23.21
LM-2005 29.30 29.21 23.41
LM-2006 29.88 29.54 23.94 30.43 28.81 17.33
GT-PTS 30.35 29.84 24.60 30.89 29.54 17.62
GT-PTS+DT-dist 30.09 29.44 24.62 31.06 29.46 17.84
KN-PTS 30.55 30.12 24.66 31.28 29.90 17.78
MC WPT05 29.63
MC 30.09 31.30
MC+GT-PTS 30.75 31.37
devtest: 1-best after decoding
LM-2006 28.59 28.45 23.22 29.22 28.30 16.94
GT-PTS 29.23 28.91 23.67 30.07 28.86 17.32
GT-PTS+DT-dist 29.48 29.07 23.50 30.22 29.46 17.42
KN-PTS 29.77 29.76 23.27 30.73 29.62 17.78
MC WPT05 28.71
MC 29.63 31.01
MC+GT-PTS 29.90 31.22
test: with rescoring
LM-2006 26.64 28.43 21.33 28.06 28.01 15.19
GT-PTS 27.19 28.95 21.91 28.60 28.83 15.38
GT-PTS+DT-dist 26.84 28.56 21.84 28.56 28.59 15.45
KN-PTS 27.40 29.07 21.98 28.96 29.06 15.64
MC 26.95 29.12
MC+GT-PTS 27.10 29.46
test: 1-best after decoding
LM-2006 25.35 27.25 20.46 27.20 27.18 14.60
GT-PTS 25.95 28.07 21.06 27.85 27.96 15.05
GT-PTS+DT-dist 25.86 28.04 20.74 27.85 27.97 14.92
KN-PTS 26.83 28.66 21.36 28.62 28.71 15.42
MC 26.70 28.74
MC+GT-PTS 26.81 29.03
and the OQLF (Office Que?be?cois de la Langue
Franc?aise) for permission to use the GDT.
References
S. F. Chen and J. T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
K. Church and W. Gale. 1991. A comparison of the en-
hanced Good-Turing and deleted estimation methods
for estimating probabilities of English bigrams. Com-
puter speech and language, 5(1):19?54.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP) 1995, pages 181?184, Detroit, Michi-
gan. IEEE.
R. Kuhn, D. Yuen, M. Simard, G. Foster, P. Paul, E. Joa-
nis and J. H. Johnson. 2006. Segment Choice Models:
Feature-Rich Models for Global Distortion in Statisti-
cal Machine Translation (accepted for publication in
HLT-NAACL conference, to be held June 2006).
F. Sadat, J. H. Johnson, A. Agbago, G. Foster, R. Kuhn,
J. Martin and A. Tikuisis. 2005. PORTAGE: A
Phrase-based Machine Translation System In Proc.
ACL 2005 Workshop on building and using parallel
texts. Ann Arbor, Michigan.
F. Sadat, G. Foster and R. Kuhn. 2006. Syste`me de tra-
duction automatique statistique combinant diffe?rentes
ressources. In Proc. TALN 2006 (Traitement Automa-
tique des Langues Naturelles). Leuven, Belgium, April
10-13, 2006.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conference / North American
Chapter of the ACL, Boston, May.
137
Proceedings of the Second Workshop on Statistical Machine Translation, pages 17?24,
Prague, June 2007. c?2007 Association for Computational Linguistics
Integration of an Arabic Transliteration Module into a Statistical 
Machine Translation System
Mehdi M. Kashani+, Eric Joanis++, Roland Kuhn++, George Foster++, Fred Popowich+
+ School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby, BC V5A 1S6, Canada
mmostafa@sfu.ca
 popowich@sfu.ca
++ NRC Institute for Information Technology
101 St-Jean-Bosco Street 
Gatineau, QC K1A 0R6, Canada
firstname.lastname@cnrc-nrc.gc.ca
Abstract
We provide an in-depth analysis of the in-
tegration of an Arabic-to-English translit-
eration system into a general-purpose 
phrase-based statistical machine translation 
system. We study the integration from dif-
ferent aspects and evaluate the improve-
ment that can be attributed to the integra-
tion using the BLEU metric. Our experi-
ments show that a transliteration module 
can help significantly in the situation where 
the test data is rich with previously unseen 
named entities. We obtain 70% and 53% of 
the theoretical maximum improvement we 
could achieve, as measured by an oracle on 
development and test sets respectively for 
OOV words (out of vocabulary source 
words not appearing in the phrase table).
1 Introduction
Transliteration is the practice of transcribing a 
word or text written in one writing system into an-
other writing system. The most frequent candidates 
for transliteration are person names, locations, or-
ganizations and imported words. The lack of a 
fully comprehensive bilingual dictionary including 
the entries for all named entities (NEs) renders the 
task of transliteration necessary for certain natural 
language processing applications dealing with 
named entities. Two applications where translitera-
tion can be particularly useful are machine transla-
tion (MT) and cross lingual information retrieval. 
While transliteration itself is a relatively well-
studied problem, its effect on the aforementioned 
applications is still under investigation.
Transliteration as a self-contained task has its 
own challenges, but applying it to a real applica-
tion introduces new challenges. In this paper we 
analyze the efficacy of integrating a transliteration 
module into a real MT system and evaluate the 
performance.
When working on a limited domain, given a suf-
ficiently large amount of training data, almost all 
of the words in the unseen data (in the same do-
main) will have appeared in the training corpus. 
But this argument does not hold for NEs, because 
no matter how big the training corpus is, there will 
always be unseen names of people and locations. 
Current MT systems either leave such unknown 
names as they are in the final target text or remove 
them in order to obtain a better evaluation score. 
None of these methods can give the reader who is 
not familiar with the source language any informa-
tion about those out-of-vocabulary (OOV) words, 
especially when the source and target languages 
use different scripts. If these words are not names, 
one can usually guess what they are, by using the 
partial information of other parts of speech. But, in 
the case of names, there is no way to determine the 
individual or location the sentence is talking about. 
So, to improve the usability of a translation, it is 
particularly important to handle NEs well.
The importance of NEs is not yet reflected in the 
evaluation methods used in the MT community, 
the most common of which is the BLEU metric. 
BLEU (Papineni et al 2002) was devised to pro-
vide automatic evaluation of MT output. In this 
metric n-gram similarity of the MT output is com-
puted with one or more references made by human 
17
translators. BLEU does not distinguish between 
different words and gives equal weight to all. In 
this paper, we base our evaluation on the BLEU 
metric and show that using transliteration has im-
pact on it (and in some cases significant impact). 
However, we believe that such integration is more 
important for practical uses of MT than BLEU in-
dicates.
Other than improving readability and raising the 
BLEU score, another advantage of using a translit-
eration system is that having the right translation 
for a name helps the language model select a better 
ordering for other words. For example, our phrase 
table1 does not have any entry for ?????? (Dulles) 
and when running MT system on the plain Arabic 
text we get
and this trip was cancelled [?] by the american 
authorities responsible for security at the airport 
???? .
We ran our MT system twice, once by suggest-
ing ?dallas? and another time ?dulles? as English 
equivalents for ?????? and the decoder generated 
the following sentences, respectively:
and this trip was cancelled [?] by the american 
authorities responsible for security at the airport 
at dallas .
and this trip was cancelled [?] by the american 
authorities responsible for security at dulles air-
port .2
Every statistical MT (SMT) system assigns a 
probability distribution to the words that are seen 
in its parallel training data, including proper names. 
The richer the training data, the higher the chance 
for a given name in the test data to be found in the 
translation tables. In other words, an MT system 
with a relatively rich phrase table is able to trans-
late many of the common names in the test data, 
with all the remaining words being rare and foreign. 
So unlike a self-contained transliteration module, 
which typically deals with a mix of ?easy? and 
                                                
1 A table where the conditional probabilities of target 
phrases given source phrases (and vice versa) is kept.
2 Note that the language model can be trained on more 
text, and hence can know more NEs than the translation 
model does.
?hard? names, the primary use for a transliteration 
module embedded in an SMT system will be to 
deal with the ?hard? names left over after the 
phrase tables have provided translations for the 
?easy? ones. That means that when measuring the 
performance improvements caused by embedding 
a transliteration module in an MT system, one 
must keep in mind that such improvements are dif-
ficult to attain: they are won mainly by correctly 
transliterating ?hard? names. 
Another issue with OOV words is that some of 
them remained untranslated due to misspellings in 
the source text. For example, we encountered 
??????? (?Hthearow?) instead of ??????? 
(?Heathrow?) or ??????? (?Brezer?) instead of 
??????? (?Bremer?) in our development test set. 
Also, evaluation by BLEU (or a similar auto-
matic metric) is problematic. Almost all of the MT 
evaluations use one or more reference translations 
as the gold standard and, using some metrics, they 
give a score to the MT output. The problem with 
NEs is that they usually have more than a single 
equivalent in the target language (especially if they 
don't originally come from the target language) 
which may or may not have been captured in the 
gold standard. So even if the transliteration module 
comes up with a correct interpretation of a name it 
might not receive credit as far as the limited num-
ber of correct names in the references are con-
cerned.
Our first impression was that having more inter-
pretations for a name in the references would raise 
the transliteration module?s chance to generate at 
least one of them, hence improving the perform-
ance. But, in practice, when references do not 
agree on a name?s transliteration that is the sign of 
an ambiguity. In these cases, the transliteration 
module often suggests a correct transliteration that 
the decoder outputs correctly, but which fails to 
receive credit from the BLEU metric because this 
transliteration is not found in the references. As an 
example, for the name ?????????, four references 
came up with four different interpretations: 
swerios, swiriyus, severius, sweires. A quick query 
in Google showed us another four acceptable in-
terpretations (severios, sewerios, sweirios, saw-
erios).
Machine transliteration has been an active re-
search field for quite a while (Al-Onaizan and 
Knight, 2002; AbdulJaleel and Larkey, 2003; Kle-
mentiev and Roth, 2006; Sproat et al 2006) but to 
18
our knowledge there is little published work on 
evaluating transliteration within a real MT system.
The closest work to ours is described in (Hassan 
and Sorensen, 2005) where they have a list of 
names in Arabic and feed this list as the input text 
to their MT system. They evaluate their system in 
three different cases: as a word-based NE transla-
tion, phrase-based NE translation and in presence 
of a transliteration module. Then, they report the 
BLEU score on the final output. Since their text is 
comprised of only NEs, the BLEU increase is quite 
high. Combining all three models, they get a 24.9 
BLEU point increase over the na?ve baseline. The 
difference they report between their best method 
without transliteration and the one including trans-
literation is 8.12 BLEU points for person names 
(their best increase).
In section 2, we introduce different methods for 
incorporating a transliteration module into an MT 
system and justify our choice. In section 3, the 
transliteration module is briefly introduced and we 
explain how we prepared its output for use by the 
MT system. In section 4, an evaluation of the inte-
gration is provided. Finally, section 5 concludes 
the paper.
2 Our Approach
Before going into details of our approach, an 
overview of Portage (Sadat et al 2005), the 
machine translation system that we used for our 
experiments and some of its properties should be 
provided.
Portage is a statistical phrase-based SMT system 
similar to Pharaoh (Koehn et al 2003).  Given a 
source sentence, it tries to find the target sentence 
that maximizes the joint probability of a target sen-
tence and a phrase alignment according to a loglin-
ear model. Features in the loglinear model consist 
of a phrase-based translation model with relative-
frequency and lexical probability estimates; a 4-
gram language model using Kneser-Ney smooth-
ing, trained with the SRILM toolkit; a single-
parameter distortion penalty on phrase reordering; 
and a word-length penalty. Weights on the loglin-
ear features are set using Och's algorithm (Och, 
2003) to maximize the system's BLEU score on a 
development corpus. To generate phrase pairs from 
a parallel corpus, we use the "diag-and" phrase 
induction algorithm described in (Koehn et al 
2003), with symmetrized word alignments gener-
ated using IBM model 2 (Brown et al 1993).
Portage allows the use of SGML-like markup 
for arbitrary entities within the input text. The 
markup can be used to specify translations 
provided by external sources for the entities, such 
as rule-based translations of numbers and dates, or 
a transliteration module for OOVs in our work. 
Many SMT systems have this capability, so 
although the details given here pertain to Portage, 
the techniques described can be used in many 
different SMT systems.
As an example, suppose we already have two 
different transliterations with their probabilities for 
the Arabic name ??????. We can replace every 
occurrence of the ?????? in the Arabic input text 
with the following:
<NAME target="mohammed|mohamed"
prob=".7|.3"> ???? </NAME>
By running Portage on this marked up text, the 
decoder chooses between entries in its own phrase 
table and the marked-up text. One thing that is 
important for our task is that if the entry cannot be 
found in Portage?s phrase tables, it is guaranteed 
that one of the candidates inside the markup will 
be chosen. Even if none of the candidates exist in 
the language model, the decoder still picks one of 
them, because the system assigns a small arbitrary 
probability (we typically use e-18) as unigram 
probability of each unseen word.
We considered four different methods for 
incorporating the transliteration module into the 
MT system. The first and second methods need an 
NE tagger and the other two do not require any 
external tools.
Method 1: use an NE tagger to extract the 
names in the Arabic input text. Then, run the 
transliteration module on them and assign 
probabilities to top candidates. Use the markup 
capability of Portage and replace each name in the 
Arabic text with the SGML-like tag including 
different probabilities for different candidates. 
Feed the marked-up text to Portage to translate.
Method 2: similar to method 1 but instead of 
using the marked-up text, a new phrase table, only 
containing entries for the names in the Arabic input 
text is built and added to Portage?s existing phrase 
tables. A weight is given to this phrase table and 
19
then the decoder uses this phrase table as well as 
its own phrase tables to decide which translation to 
choose when encountering the names in the 
text.  The main difference between methods 1 and 
2 is that in our system, method 2 allows for a bleu-
optimal weight to be learned for the NE phrase 
table, whereas the weight on the rules for method 1 
has to be set by hand.
Method 3: run Portage on the plain Arabic text. 
Extract all untranslated Arabic OOVs and run the 
transliteration module on them. Replace them with 
the top candidate.
Method 4: run Portage on the plain Arabic text. 
Extract all untranslated Arabic OOVs and run the 
transliteration module on them. Replace them with 
SGML-like tags including different probabilities 
for different candidates, as described previously. 
Feed the marked-up text to Portage to translate.
The first two methods need a powerful NE 
tagger with a high recall value. We computed the 
recall value on the development set OOVs using 
two different NE taggers, Tagger A and Tagger B 
(each from a different research group). Taggers A 
and B showed a recall of 33% and 53% respec-
tively, both being low for our purposes. Another 
issue with these two methods is that for many of 
the names the transliteration module will compete 
with the internal phrase table. Our observations 
show that if a name exists in the phrase table, it is 
likely to be translated correctly. In general, 
observed parallel data (i.e. training data) should be 
a more reliable source of information than 
transliteration, encouraging us to use transliteration 
most appropriately as a ?back-off? method. In a 
few cases, the Arabic name is ambiguous with a 
common word and is mistakenly translated as such. 
For example, ????? ??? ???? is an Arabic name that 
should be transliterated as ?Hani Abu Nahl? but 
since ????? also means ?solve?, the MT system 
outputs ?Hani Abu Solve?. The advantage of the 
first two methods is that they can deal with such 
cases. But considering the noise in the NE 
detectors, handling them increases the risk of 
losing already correct translations of other names.
The third method is simple and easy to use but 
not optimal: it does not take advantage of the 
decoder?s internal features (notably the language 
models) and only picks up the highest scoring 
candidate from the transliteration module.
The fourth method only deals with those words 
that the MT system was unable to deal with and 
had to leave untranslated in the final text. 
Therefore whatever suggestions the transliteration 
module makes do not need to compete with the 
internal phrase tables, which is good because we 
expect the phrase tables to be a more reliable 
source of information. It is guaranteed that the 
translation quality will be improved (in the worst 
case, a bad transliteration is still more informative 
than the original word in Arabic script). Moreover, 
unlike the third method, we take advantage of all 
internal decoder features on the second pass. We 
adopt the fourth method for our experiment. The 
following example better illustrates how this 
approach works:
Example: Suppose we have the following sentence 
in the Arabic input text: 
???? ???? ????? ????? ???????.
Portage is run on the Arabic plain text and yields 
the following output:
blair accepts ????? report in full .
The Arabic word ??????? (Hutton) is extracted and 
fed to the transliteration module. The 
transliteration module comes up with some English 
candidates, each with different probabilities as 
estimated by the HMM. They are rescaled (as will 
be explained in section 3) and the following 
markup text will be generated to replace the 
untranslated ??????? in the first plain Arabic 
sentence:
<NAME target="hoton|hutton|authon" 
prob="0.1|0.00028|4.64e-05">?????</NAME> 
Portage is then run on this newly marked up text 
(second pass). From now on, with the additional 
guidance of the language models, it is the 
decoder?s task to decide between different markup 
suggestions. For the above example, the following 
output will be generated:
blair accepts hutton report in full .
20
3 Transliteration System
In this section we provide a brief overview of the 
embedded transliteration system we used for our 
experiment. For the full description refer to 
(Kashani et al 2007).
3.1 Three Phase Transliteration
The transliteration module follows the noisy 
channel framework. The adapted spelling-based 
generative model is similar to (Al-Onaizan and 
Knight, 2002). It consists of three consecutive 
phases, the first two using HMMs and the Viterbi 
algorithm, and the third using a number of 
monolingual dictionaries to match the close entries 
or to filter out some invalid candidates from the 
first two phases.
Since in Arabic, the diacritics are usually 
omitted in writing, a name like ?????? (Mohamed) 
would have an equivalent like ?mhmd? if we only 
take into account the written letters. To address 
this issue, we run Viterbi in two different passes 
(each called a phase), using HMMs trained on data 
prepared in different ways.
In phase 1, the system tries to find the best 
transliterations of the written word, without caring 
about what the hidden diacritics would be (in our 
example, mhmd).
In phase 2, given the Arabic input and the output 
candidates from phase 1, the system fills in the 
possible blanks in between using the character-
based language model (yielding ?mohamed? as a 
possible output, among others).
To prepare the character-level translation model 
for both phases we adopted an approach similar to 
(AbdulJaleel and Larkey, 2003).
In phase 3, the Google unigram model 
(LDC2006T13 from the LDC catalog) is first used 
to filter out the noise (i.e. those candidates that do 
not exist in the Google unigram are removed from 
the candidate list). Then a combination of some 
monolingual dictionaries of person names is used 
to find close matches between their entries and the 
HMM output candidates based on the Levenshtein 
distance metric.
3.2 Task-specific Changes to the Module
Due to the nature of the task at hand and by 
observing the development test set and its 
references, the following major changes became 
necessary:
Removing Part of Phase Three: By observing the 
OOV words in the development test set, we 
realized that having the monolingual dictionary in 
the pipeline and using the Levensthtein distance as 
a metric for adding the closest dictionary entries to 
the final output, does not help much, mainly 
because OOVs are rarely in the dictionary. So, the 
dictionary part not only slows down the execution 
but would also add noise to the final output (by 
adding some entries that probably are not the 
desired outputs). However, we kept the Google 
unigram filtering in the pipeline.
Rescaling HMM Probabilities: Although the 
transliteration module outputs HMM probability 
score for each candidate, and the MT system also 
uses probability scores, in practice the translitera-
tion scores have to be adjusted.  For example, if 
three consecutive candidates have log probabilities 
-40, -42 and -50, the decoder should be given val-
ues with similar differences in scale, comparable 
with the typical differences in its internal features 
(eg. Language Models). Knowing that the entries 
in the internal features usually have exponential 
differences, we adopted the following conversion 
formula:
p'i = 0.1*(pi/pmax)?
Equation 1
where pi = 10(output of HMM for candidate i) and max is the 
best candidate.
We rescale the HMM probability so that the top 
candidate is (arbitrarily) given a probability of p'max
= 0.1.  It immediately follows that the rescaled 
score would be 0.1 * pi / pmax.  Since the decoder
combines its models in a log-linear fashion, we 
apply an exponent ? to the HMM probabilities be-
fore scaling them, as way to control the weight of 
those probabilities in decoding.  This yields equa-
tion 1.  Ideally, we would like the weight ? to be 
optimized the same way other decoder weights are 
optimized, but our decoder does not support this 
yet, so for this work we arbitrarily set the weight to 
? = 0.2, which seems to work well. For the above 
example, the distribution would be 0.1, 0.039 and 0.001.
21
Prefix Detachment: Arabic is a morphologically 
rich language. Even after performing tokenization, 
some words still remain untokenized. If the 
composite word is frequent, there is a chance that it 
exists in the phrase table but many times it does 
not, especially if the main part of that word is a 
named entity. We did not want to delve into the 
details of morphology: we only considered two 
frequent prefixes: ??? (?va? meaning ?and?) and 
???? (?al? determiner in Arabic). If a word starts 
with either of these two prefixes, we detach them 
and run the transliteration module once on the 
detached name and a second time on the whole 
word. The output candidates are merged 
automatically based on their scores, and the 
decoder decides which one to choose.
Keeping the Top 5 HMM Candidates: The 
transliteration module uses the Google unigram 
model to filter out the candidate words that do not 
appear above a certain threshold (200 times) on the 
Internet. This helps eliminate hundreds of 
unwanted sequences of letters. But, we decided to 
keep top-5 candidates on the output list, even if 
they are rejected by the Google unigram model 
because sometimes the transliteration module is
unable to suggest the correct equivalent or in other 
cases the OOV should actually be translated rather 
than transliterated 3 . In these cases, the closest 
literal transliteration will still provide the end user 
more information about the entity than the word in 
Arabic script would.
4 Evaluation
Although there are metrics that directly address NE 
translation performance4, we chose to use BLEU 
because our purpose is to assess NE translation 
within MT, and BLEU is currently the standard 
metric for MT.
                                                
3 This would happen especially for ancient names or 
some names that underwent sophisticated morphologi-
cal transformations (For example, Abraham in English 
and ??????? (Ibrahim) in Arabic).
4 NIST?s NE translation task 
(http://www.nist.gov/speech/tests/ace/index.htm) is an 
example.
4.1 Training Data
We used the data made available for the 2006 
NIST Machine Translation Evaluation. Our bilin-
gual training corpus consisted of 4M sentence pairs
drawn mostly from newswire and UN domains. 
We trained one language model on the English half 
of this corpus (137M running words), and another 
on the English Gigaword corpus (2.3G running 
words). For tuning feature weights, we used LDC's 
"multiple translation part 1" corpus, which contains 
1,043 sentence pairs. 
4.2 Test Data
We used the NIST MT04 evaluation set and the 
NIST MT05 evaluation set as our development and 
blind test sets. The development test set consists of 
1353 sentences, 233 of which contain OOVs. 
Among them 100 sentences have OOVs that are 
actually named entities. The blind test set consists 
of 1056 sentences, 189 of them having OOVs and 
131 of them having OOV named entities. The 
number of sentences for each experiment is 
summarized in table 1.
Whole Text OOV 
Sentences
OOV-NE 
Sentences
Dev test set 1353 233 100
Blind test set 1056 189 131
Table 1: Distribution of sentences in test sets.
4.3 Results
As the baseline, we ran the Portage without the 
transliteration module on development and blind 
test sets. The second column of table 2 shows 
baseline BLEU scores. We applied method 4 as 
outlined in section 2 and computed the BLEU 
score, also in order to compare the results we 
implemented method 3 on the same test sets. The 
BLEU scores obtained from methods 3 and 4 are 
shown in columns 3 and 4 of table 2.
baseline Method 3 Method 4 Oracle
Dev 44.67 44.71 44.83 44.90
Blind 48.56 48.62 48.80 49.01
Table 2: BLEU score on different test sets.
Considering the fact that only a small portion of 
the test set has out-of-vocabulary named entities, 
22
we computed the BLEU score on two different 
sub-portions of the test set: first, on the sentences 
with OOVs; second, only on the sentences 
containing OOV named entities. The BLEU 
increase on different portions of the test set is 
shown in table 3.
baseline Method 4
Dev OOV sentences 39.17 40.02
OOV-NE Sentences 44.56 46.31
blind OOV sentences 43.93 45.07
OOV-NE Sentences 42.32 44.87
Table 3: BLEU score on different 
portions of the test sets.
To set an upper bound on how much applying 
any transliteration module can contribute to the 
overall results, we developed an oracle-like 
dictionary for the OOVs in the test sets, which was 
then used to create a markup Arabic text. By 
feeding this markup input to the MT system we 
obtained the result shown in column 5 of table 2. 
This is the performance our system would achieve 
if it had perfect accuracy in transliteration, 
including correctly guessing what errors the human 
translators made in the references.  Method 4 
achieves 70% of this maximum gain on dev, and 
53% on blind.
5 Conclusion
This paper has described the integration of a trans-
literation module into a state-of-the-art statistical 
machine translation (SMT) system for the Arabic 
to English task. The final version of the translitera-
tion module operates in three phases. First, it gen-
erates English letter sequences corresponding to 
the Arabic letter sequence; for the typical case 
where the Arabic omits diacritics, this often means 
that the English letter sequence is incomplete (e.g., 
vowels are often missing). In the next phase, the 
module tries to guess the missing English letters. 
In the third phase, the module uses a huge collec-
tion of English unigrams to filter out improbable or 
impossible English words and names. We de-
scribed four possible methods for integrating this
module in an SMT system. Two of these methods 
require NE taggers of higher quality than those 
available to us, and were not explored experimen-
tally. Method 3 inserts the top-scoring candidate 
from the transliteration module in the translation 
wherever there was an Arabic OOV in the source. 
Method 4 outputs multiple candidates from the
transliteration module, each with a score; the SMT 
system combines these scores with language model 
scores to decide which candidate will be chosen. In 
our experiments, Method 4 consistently outper-
formed Model 3. Note that although we used 
BLEU as the metric for all experiments in this pa-
per, BLEU greatly understates the importance of
accurate transliteration for many practical SMT 
applications.
References
Nasreen AbdulJaleel and Leah S. Larkey, 2003. Statisti-
cal Transliteration for English-Arabic Cross Lan-
guage Information Retrieval, Proceedings of the 
Twelfth International Conference on Information and 
Knowledge Management, New Orleans, LA
Yaser Al-Onaizan and Kevin Knight, 2002. Machine 
Transliteration of Names in Arabic Text, Proceedings 
of the ACL Workshop on Computational Approaches 
to Semitic Languages 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer, 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation, Computational Linguistics
Hany Hassan and Jeffrey Sorensen, 2005. An Integrated 
Approach for Arabic-English Named Entity Transla-
tion, Proceedings of the ACL Workshop on Compu-
tational Approaches to Semitic Languages (ACL), 
University of Michigan, Ann Arbor
Mehdi M. Kashani, Fred Popowich, and Anoop Sarkar, 
2007. Automatic Transliteration of Proper Nouns 
from Arabic to English, Proceedings of the Second 
Workshop on Computational Approaches to Arabic 
Script-based Languages
Alexandre Klementiev and Dan Roth, 2006. Named 
Entity Transliteration and Discovery from Multilin-
gual Comparable Corpora, COLING-ACL, Sidney, 
Australia
Philipp Koehn, Franz Josef Och, and Daniel Marcu, 
2003. Statistical Phrase-based Translation, In Pro-
ceedings of HLT-NAACL, Edmonton, Canada
Franz Josef Och, 2003. Minimum Error Rate Training 
for Statistical Machine Translation, In Proceedings 
of the 41th Annual Meeting of the Association for 
Computation Linguistics, Sapporo
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu, 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
23
of the 40th Annual Conference of the Association for 
Computational Linguistics (ACL), Philadelphia, PA
Fatiha Sadat, Howard Johnson, Akakpo Agbago, 
George Foster, Roland Kuhn, Aaron Tikuisis, 2005. 
Portage: A Phrase-base Machine Translation System.
In Proceedings of the ACL Workshop on Building 
and Using Parallel Texts, Ann Arbor, Michigan
Richard Sproat, Tao Tao, and ChengXiang Zhai, 2006, 
Named Entity Transliteration with Comparable Cor-
pora, COLING-ACL, Sidney, Australia
24
Proceedings of the Second Workshop on Statistical Machine Translation, pages 128?135,
Prague, June 2007. c?2007 Association for Computational Linguistics
Mixture-Model Adaptation for SMT
George Foster and Roland Kuhn
National Research Council Canada
first.last@nrc.gc.ca
Abstract
We describe a mixture-model approach to
adapting a Statistical Machine Translation
System for new domains, using weights that
depend on text distances to mixture compo-
nents. We investigate a number of variants
on this approach, including cross-domain
versus dynamic adaptation; linear versus
loglinear mixtures; language and transla-
tion model adaptation; different methods of
assigning weights; and granularity of the
source unit being adapted to. The best
methods achieve gains of approximately one
BLEU percentage point over a state-of-the
art non-adapted baseline system.
1 Introduction
Language varies significantly across different gen-
res, topics, styles, etc. This affects empirical mod-
els: a model trained on a corpus of car-repair manu-
als, for instance, will not be well suited to an appli-
cation in the field of tourism. Ideally, models should
be trained on text that is representative of the area
in which they will be used, but such text is not al-
ways available. This is especially the case for bilin-
gual applications, because parallel training corpora
are relatively rare and tend to be drawn from spe-
cific domains such as parliamentary proceedings.
In this paper we address the problem of adapting
a statistical machine translation system by adjust-
ing its parameters based on some information about
a test domain. We assume two basic settings. In
cross-domain adaptation, a small sample of parallel
in-domain text is available, and it is used to optimize
for translating future texts drawn from the same do-
main. In dynamic adaptation, no domain informa-
tion is available ahead of time, and adaptation is
based on the current source text under translation.
Approaches developed for the two settings can be
complementary: an in-domain development corpus
can be used to make broad adjustments, which can
then be fine tuned for individual source texts.
Our method is based on the classical technique
of mixture modeling (Hastie et al, 2001). This
involves dividing the training corpus into different
components, training a model on each part, then
weighting each model appropriately for the current
context. Mixture modeling is a simple framework
that encompasses many different variants, as de-
scribed below. It is naturally fairly low dimensional,
because as the number of sub-models increases, the
amount of text available to train each, and therefore
its reliability, decreases. This makes it suitable for
discriminative SMT training, which is still a chal-
lenge for large parameter sets (Tillmann and Zhang,
2006; Liang et al, 2006).
Techniques for assigning mixture weights depend
on the setting. In cross-domain adaptation, knowl-
edge of both source and target texts in the in-domain
sample can be used to optimize weights directly. In
dynamic adaptation, training poses a problem be-
cause no reference text is available. Our solution
is to construct a multi-domain development sample
for learning parameter settings that are intended to
generalize to new domains (ones not represented in
the sample). We do not learn mixture weights di-
rectly with this method, because there is little hope
128
that these would be well suited to new domains. In-
stead we attempt to learn how weights should be set
as a function of distance. To our knowledge, this ap-
proach to dynamic adaptation for SMT is novel, and
it is one of the main contributions of the paper.
A second contribution is a fairly broad investiga-
tion of the large space of alternatives defined by the
mixture-modeling framework, using a simple genre-
based corpus decomposition. We experimented with
the following choices: cross-domain versus dynamic
adaptation; linear versus loglinear mixtures; lan-
guage and translation model adaptation; various text
distance metrics; different ways of converting dis-
tance metrics into weights; and granularity of the
source unit being adapted to.
The remainder of the paper is structured follows:
section 2 briefly describes our phrase-based SMT
system; section 3 describes mixture-model adapta-
tion; section 4 gives experimental results; section 5
summarizes previous work; and section 6 concludes.
2 Phrase-based Statistical MT
Our baseline is a standard phrase-based SMT sys-
tem (Koehn et al, 2003). Given a source sentence s,
this tries to find the target sentence t? that is the most
likely translation of s, using the Viterbi approxima-
tion:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where alignment a = (s?1, t?1, j1), ..., (s?K , t?K , jK);
t?k are target phrases such that t = t?1 . . . t?K ; s?k are
source phrases such that s = s?j1 . . . s?jK ; and s?k is
the translation of the kth target phrase t?k.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[?
i
?ifi(s, t,a)
]
(1)
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al, 2001) on a development corpus. The
features used in this study are: the length of
t; a single-parameter distortion penalty on phrase
reordering in a, as described in (Koehn et al,
2003); phrase translation model probabilities; and
4-gram language model probabilities log p(t), us-
ing Kneser-Ney smoothing as implemented in the
SRILM toolkit.
Phrase translation model probabilities are features
of the form: log p(s|t,a) ? ?Kk=1 log p(s?k|t?k).
We use two different estimates for the conditional
probabilities p(t?|s?) and p(s?|t?): relative frequencies
and ?lexical? probabilities as described in (Zens and
Ney, 2004). In both cases, the ?forward? phrase
probabilities p(t?|s?) are not used as features, but only
as a filter on the set of possible translations: for each
source phrase s? that matches some ngram in s, only
the 30 top-ranked translations t? according to p(t?|s?)
are retained.
To derive the joint counts c(s?, t?) from which
p(s?|t?) and p(t?|s?) are estimated, we use the phrase in-
duction algorithm described in (Koehn et al, 2003),
with symmetrized word alignments generated using
IBM model 2 (Brown et al, 1993).
3 Mixture-Model Adaptation
Our approach to mixture-model adaptation can be
summarized by the following general algorithm:
1. Split the corpus into different components, ac-
cording to some criterion.
2. Train a model on each corpus component.
3. Weight each model according to its fit with the
test domain:
? For cross-domain adaptation, set param-
eters using a development corpus drawn
from the test domain, and use for all fu-
ture documents.
? For dynamic adaptation, set global param-
eters using a development corpus drawn
from several different domains. Set mix-
ture weights as a function of the distances
from corpus components to the current
source text.
4. Combine weighted component models into a
single global model, and use it to translate as
described in the previous section.
We now describe each aspect of this algorithm in
more detail.
129
3.1 Corpus Decomposition
We partition the corpus into different genres, defined
as being roughly identical to corpus source. This is
the simplest way to exploit heterogeneous training
material for adaptation. An alternative, which we
have not explored, would be to cluster the corpus
automatically according to topic.
3.2 Component Models
We adapt both language and translation model fea-
tures within the overall loglinear combination (1).
To train translation models on each corpus com-
ponent, we used a global IBM2 model for word
alignment (in order to avoid degradation in align-
ment quality due to smaller training corpora), then
extracted component-specific relative frequencies
for phrase pairs. Lexical probabilities were also de-
rived from the global IBM2 model, and were not
adapted.
The procedure for training component-specific
language models on the target halves of each cor-
pus component is identical to the procedure for the
global model described in section 2. In addition to
the component models, we also used a large static
global model.
3.3 Combining Framework
The most commonly-used framework for mixture
models is a linear one:
p(x|h) =
?
c
?cpc(x|h) (2)
where p(x|h) is either a language or translation
model; pc(x|h) is a model trained on component c,
and ?c is the corresponding weight. An alternative,
suggested by the form of the global model, is a log-
linear combination:
p(x|h) =
?
c
pc(x|h)?c
where we write ?c to emphasize that in this case
the mixing parameters are global weights, like the
weights on the other features within the loglinear
model. This is in contrast to linear mixing, where the
combined model p(x|h) receives a loglinear weight,
but the weights on the components do not partici-
pate in the global loglinear combination. One conse-
quence is that it is more difficult to set linear weights
using standard minimum-error training techniques,
which assume only a ?flat? loglinear model.
3.4 Distance Metrics
We used four standard distance metrics to cap-
ture the relation between the current source or tar-
get text q and each corpus component.1 All are
monolingual?they are applied only to source text
or only to target text.
The tf/idf metric commonly used in information
retrieval is defined as cos(vc,vq), where vc and
vq are vectors derived from component c and doc-
ument q, each consisting of elements of the form:
?p?(w) log p?doc(w), where p?(w) is the relative fre-
quency of word w within the component or docu-
ment, and pdoc(w) is the proportion of components
it appears in.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is a technique for implicitly capturing the
semantic properties of texts, based on the use of
Singular Value Decomposition to produce a rank-
reduced approximation of an original matrix of word
and document frequencies. We applied this tech-
nique to all documents in the training corpus (as op-
posed to components), reduced the rank to 100, then
calculated the projections of the component and doc-
ument vectors described in the previous paragraph
into the reduced space.
Perplexity (Jelinek, 1997) is a standard way of
evaluating the quality of a language model on a test
text. We define a perplexity-based distance metric
pc(q)1/|q|, where pc(q) is the probability assigned to
q by an ngram language model trained on compo-
nent c.
The final distance metric, which we call EM, is
based on expressing the probability of q as a word-
level mixture model: p(q) = ?|q|i=1
?
c dcpc(wi|hi),
where q = w1 . . . w|q|, and pc(w|h) is the ngram
probability of w following word sequence h in com-
ponent c. It is straighforward to use the EM algo-
rithm to find the set of weights d?c,?c that maxi-
mizes the likelihood of q. The weight d?c is defined
as the distance to component c. For all experiments
described below, we used a probability difference
threshold of 0.001 as the EM convergence criterion.
1Although we refer to these metrics as distances, most are
in fact proximities, and we use the convention throughout that
higher values mean closer.
130
3.5 Learning Adaptive Parameters
Our focus in this paper is on adaptation via mixture
weights. However, we note that the usual loglinear
parameter tuning described in section 2 can also be
considered adaptation in the cross-domain setting,
because learned preferences for word penalty, rel-
ative LM/TM weighting, etc, will reflect the target
domain. This is not the case for dynamic adapta-
tion, where, in the absence of an in-domain devel-
opment corpus, the only information we can hope to
glean are the weights on adapted models compared
to other features of the system.
The method used for adapting mixture weights
depends on both the combining framework (loglin-
ear versus linear), and the adaptive setting (cross-
domain versus dynamic), as described below.
3.5.1 Setting Loglinear Mixture Weights
When using a loglinear combining framework as
described in section 3.3, mixture weights are set
in the same way as the other loglinear parameters
when performing cross-domain adaptation. Loglin-
ear mixture models were not used for dynamic adap-
tation.
3.5.2 Setting Linear Mixture Weights
For both adaptive settings, linear mixture weights
were set as a function of the distance metrics de-
scribed in section 3.4. Given a set of metrics
{D1, . . . , Dm}, let di,c be the distance from the cur-
rent text to component c according to metric Di. A
simple approach to weighting is to choose a single
metric Di, and set the weights in (2) to be propor-
tional to the corresponding distances:
?c = di,c/
?
c?
di,c? . (3)
Because different distance metrics may capture
complementary information, and because optimal
weights might be a non-linear function of distance,
we also experimented with a linear combination of
metrics transformed using a sigmoid function:
?c =
m?
i=1
?i
1 + exp(ai(bi ? di,c)) (4)
where ?i reflects the relative predictive power of Di,
and the sigmoid parametes ai and bi can be set to
selectively suppress contributions from components
that are far away. Here we assume that ?i absorbs
a normalization constant, so that the ?c?s sum to 1.
In this approach, there are three parameters per dis-
tance metric to learn: ?i, ai, and bi. In general, these
parameters are also specific to the particular model
being adapted, ie the LM or the TM.
To optimize these parameters, we fixed global
loglinear weights at values obtained with Och?s al-
gorithm using representative adapted models based
on a single distance metric in (3), then used the
Downhill Simplex algorithm (Press et al, 2002) to
maximize BLEU score on the development corpus.
For tractability, we followed standard practice with
this technique and considered only monotonic align-
ments when decoding (Zens and Ney, 2004).
The two approaches just described avoid condi-
tioning ?c explicitly on c. This is necessary for
dynamic adaptation, since any genre preferences
learned from the development corpus cannot be ex-
pected to generalize. However, it is not necessary
for cross-domain adaptation, where the genre of the
development corpus is assumed to represent the test
domain. Therefore, we also experimented with us-
ing Downhill Simplex optimization to directly learn
the set of linear weights ?c that yield maximum
BLEU score on the development corpus.
A final variant on setting linear mixture weights is
a hybrid between cross-domain and dynamic adap-
tation. In this approach, both the global loglinear
weights and, if they are being used, the mixture pa-
rameters ?i, ai, bi are set to characterize the test do-
main as in cross-domain adaptation. When trans-
lating, however, distances to the current source text
are used in (3) or (4) instead of distances to the in-
domain development corpus. This obviously limits
the metrics used to ones that depend only on source
text.
4 Experiments
All experiments were run on the NIST MT evalua-
tion 2006 Chinese data set. Table 1 summarizes the
corpora used. The training corpus was divided into
seven components according to genre; in all cases
these were identical to LDC corpora, with the excep-
tion of the Newswire component, which was amal-
gamated from several smaller corpora. The target
131
genre for cross-domain adaptation was newswire,
for which high-quality training material is avail-
able. The cross-domain development set NIST04-
nw is the newswire subset of the NIST 2004 evalu-
ation set, and the dynamic adaptation development
set NIST04-mix is a balanced mixed-genre subset of
NIST 2004. The NIST 2005 evaluation set was used
for testing cross-domain adaptation, and the NIST
2006 evaluation set (both the ?GALE? and ?NIST?
parts) was used to test dynamic adaptation.
Because different development corpora are used
for cross-domain and dynamic adaptation, we
trained one static baseline model for each of these
adaptation settings, on the corresponding develop-
ment set.
All results given in this section are BLEU scores.
role corpus genres sent
train FBIS04 nw 182k
HK Hans proceedings 1,375k
HK Laws legal 475k
HK News press release 740k
Newswire nw 26k
Sinorama news mag 366k
UN proceedings 4,979k
dev NIST04-nw nw 901
NIST04-mix nw, sp, ed 889
test NIST05 nw 1,082
NIST06-GALE nw, ng, bn, bc 2,276
NIST06-NIST nw, ng, bn 1,664
Table 1: Corpora. In the genres column: nw =
newswire, sp = speeches, ed = editorial, ng = news-
group, bn = broadcast news, and bc = broadcast con-
versation.
4.1 Linear versus Loglinear Combination
Table 2 shows a comparison between linear and
loglinear mixing frameworks, with uniform weights
used in the linear mixture. Both types of mixture
model are better than the baseline, but the linear
mixture is slightly better than the loglinear mix-
ture. This is quite surprising, because these results
are on the development set: the loglinear model
tunes its component weights on this set, whereas
the linear model only adjusts global LM and TM
weights. We speculated that this may have been due
to non-smooth component models, and tried various
smoothing schemes, including Kneser-Ney phrase
table smoothing similar to that described in (Foster
et al, 2006), and binary features to indicate phrase-
pair presence within different components. None
helped, however, and we conclude that the problem
is most likely that Och?s algorithm is unable to find
a good maximimum in this setting. Due to this re-
sult, all experiments we describe below involve lin-
ear mixtures only.
combination adapted model
LM TM LM+TM
baseline 30.2 30.2 30.2
loglinear mixture 30.9 31.2 31.4
uniform linear mixture 31.2 31.1 31.8
Table 2: Linear versus loglinear combinations on
NIST04-nw.
4.2 Distance Metrics for Weighting
Table 3 compares the performance of all distance
metrics described in section 3.4 when used on their
own as defined in (3). The difference between them
is fairly small, but appears to be consistent across
LM and TM adaptation and (for the LM metrics)
across source and target side matching. In general,
LM metrics seem to have a slight advantage over the
vector space metrics, with EM being the best overall.
We focus on this metric for most of the experiments
that follow.
metric source text target text
LM TM LM TM
tf/idf 31.3 31.3 31.1 31.1
LSA 31.5 31.6
perplexity 31.6 31.3 31.7 31.5
EM 31.7 31.6 32.1 31.3
Table 3: Distance metrics for linear combination on
the NIST04-nw development set. (Entries in the top
right corner are missing due to lack of time.)
Table 4 shows the performance of the parame-
terized weighting function described by (4), with
source-side EM and LSA metrics as inputs. This
is compared to direct weight optimization, as both
these techniques use Downhill Simplex for param-
eter tuning. Unfortunately, neither is able to beat
132
the performance of the normalized source-side EM
metric on its own (reproduced on the first line from
table 3). In additional tests we verified that this also
holds for the test corpus. We speculate that this dis-
appointing result is due to compromises made in or-
der to run Downhill Simplex efficiently, including
holding global weights fixed, using only a single
starting point, and running with monotone decoding.
weighting LM TM
EM-src, direct 31.7 31.6
EM-src + LSA-src, parameterized 31.0 30.0
direct optimization 31.7 30.2
Table 4: Weighting techniques for linear combina-
tion on the NIST04-nw development set.
4.3 Cross-Domain versus Dynamic Adaptation
Table 5 shows results for cross-domain adaptation,
using the source-side EM metric for linear weight-
ing. Both LM and TM adaptation are effective, with
test-set improvements of approximately 1 BLEU
point over the baseline for LM adaptation and some-
what less for TM adaptation. Performance also im-
proves on the NIST06 out-of-domain test set (al-
though this set includes a newswire portion as well).
However, combined LM and TM adaptation is not
better than LM adaptation on its own, indicating that
the individual adapted models may be capturing the
same information.
model dev test
nist04- nist05 nist06-
nw nist
baseline 30.2 30.3 26.5
EM-src LM 31.7 31.2 27.8
EM-src TM 31.6 30.9 27.3
EM-src LM+TM 32.5 31.2 27.7
Table 5: Cross-Domain adaptation results.
Table 6 contains results for dynamic adaptation,
using the source-side EM metric for linear weight-
ing. In this setting, TM adaptation is much less
effective, not significantly better than the baseline;
performance of combined LM and TM adaptation
is also lower. However, LM adaptation improves
over the baseline by up to a BLEU point. The per-
formance of cross domain adaptation (reproduced
from table 5 on the second line) is slightly better for
the in-domain test set (NIST05), but worse than dy-
namic adaptation on the two mixed-domain sets.
model dev test
nist04- nist05 nist06- nist06-
mix nist gale
baseline 31.9 30.4 27.6 12.9
cross LM n/a 31.2 27.8 12.5
LM 32.8 30.8 28.6 13.4
TM 32.4 30.7 27.6 12.8
LM+TM 33.4 30.8 28.5 13.0
Table 6: Dynamic adaptation results, using src-side
EM distances.
model NIST05
baseline 30.3
cross EM-src LM 31.2
cross EM-src TM 30.9
hybrid EM-src LM 30.9
hybrid EM-src TM 30.7
Table 7: Hybrid adaptation results.
Table 7 shows results for the hybrid approach de-
scribed at the end of section 3.5.2: global weights
are learned on NIST04-nw, but linear weights are
derived dynamically from the current test file. Per-
formance drops slightly compared to pure cross-
domain adaptation, indicating that it may be impor-
tant to have a good fit between global and mixture
weights.
4.4 Source Granularity
The results of the final experiment, to determine the
effects of source granularity on dynamic adaptation,
are shown in table 8. Source-side EM distances are
applied to the whole test set, to genres within the set,
and to each document individually. Global weights
were tuned specifically for each of these conditions.
There appears to be little difference among these ap-
proaches, although genre-based adaptation perhaps
has a slight advantage.
133
granularity dev test
nist04- nist05 nist06- nist06-
mix nist gale
baseline 31.9 30.4 27.6 12.9
file 32.4 30.8 28.6 13.4
genre 32.5 31.1 28.9 13.2
document 32.9 30.9 28.6 13.4
Table 8: The effects of source granularity on dy-
namic adaptation.
5 Related Work
Mixture modeling is a standard technique in ma-
chine learning (Hastie et al, 2001). It has been
widely used to adapt language models for speech
recognition and other applications, for instance us-
ing cross-domain topic mixtures, (Iyer and Osten-
dorf, 1999), dynamic topic mixtures (Kneser and
Steinbiss, 1993), hierachical mixtures (Florian and
Yarowsky, 1999), and cache mixtures (Kuhn and De
Mori, 1990).
Most previous work on adaptive SMT focuses on
the use of IR techniques to identify a relevant sub-
set of the training corpus from which an adapted
model can be learned. Byrne et al(2003) use co-
sine distance from the current source document to
find relevant parallel texts for training an adapted
translation model, with background information for
smoothing alignments. Hildebrand et al(1995) de-
scribe a similar approach, but apply it at the sentence
level, and use it for language model as well as trans-
lation model adaptation. They rely on a perplexity
heuristic to determine an optimal size for the rele-
vant subset. Zhao et al(2004) apply a slightly differ-
ent sentence-level strategy to language model adap-
tation, first generating an nbest list with a baseline
system, then finding similar sentences in a monolin-
gual target-language corpus. This approach has the
advantage of not limiting LM adaptation to a parallel
corpus, but the disadvantage of requiring two trans-
lation passes (one to generate the nbest lists, and an-
other to translate with the adapted model).
Ueffing (2006) describes a self-training approach
that also uses a two-pass algorithm. A baseline sys-
tem generates translations that, after confidence fil-
tering, are used to construct a parallel corpus based
on the test set. Standard phrase-extraction tech-
niques are then applied to extract an adapted phrase
table from the system?s own output.
Finally, Zhang et al(2006) cluster the parallel
training corpus using an algorithm that heuristically
minimizes the average entropy of source-side and
target-side language models over a fixed number of
clusters. Each source sentence is then decoded us-
ing the language model trained on the cluster that
assigns highest likelihood to that sentence.
The work we present here is complementary
to both the IR approaches and Ueffing?s method
because it provides a way of exploiting a pre-
established corpus division. This has the potential
to allow sentences having little surface similarity to
the current source text to contribute statistics that
may be relevant to its translation, for instance by
raising the probability of rare but pertinent words.
Our work can also be seen as extending all previous
approaches in that it assigns weights to components
depending on their degree of relevance, rather than
assuming a binary distinction between relevant and
non-relevant components.
6 Conclusion and Future Work
We have investigated a number of approaches to
mixture-based adaptation using genres for Chi-
nese to English translation. The most successful
is to weight component models in proportion to
maximum-likelihood (EM) weights for the current
text given an ngram language model mixture trained
on corpus components. This resulted in gains of
around one BLEU point. A more sophisticated ap-
proach that attempts to transform and combine mul-
tiple distance metrics did not yield positive results,
probably due to an unsucessful optmization proce-
dure.
Other conclusions are: linear mixtures are more
tractable than loglinear ones; LM-based metrics are
better than VS-based ones; LM adaptation works
well, and adding an adapted TM yields no improve-
ment; cross-domain adaptation is optimal, but dy-
namic adaptation is a good fallback strategy; and
source granularity at the genre level is better than
the document or test-set level.
In future work, we plan to improve the optimiza-
tion procedure for parameterized weight functions.
We will also look at bilingual metrics for cross-
134
domain adaptation, and investigate better combina-
tions of cross-domain and dynamic adaptation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathematics
of Machine Translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?312, June.
W. Byrne, S. Khudanpur, W. Kim, S. Kumar, P. Pecina,
P. Virga, P. Xu, and D. Yarowsky. 2003. The JHU
2003 Chinese-English Machine Translation System.
In MT Summit IX, New Orleans, September.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. JASIS, 41(6):391?407.
Radu Florian and David Yarowsky. 1999. Dynamic non-
local language modeling via hierarchical topic-based
adaptation. In ACL 1999, pages 167?174, College
Park, Maryland, June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In EMNLP 2006, Sydney, Australia.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2001. The Elements of Statistical Learning. Springer.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 1995. Adaptation of the transla-
tion model for statistical machine translation based on
information retrieval. In EAMT 1995, Budapest, May.
R. Iyer and M. Ostendorf. 1999. Modeling long dis-
tance dependence in language: Topic mixtures vs. dy-
namic cache models. In IEEE Trans on Speech and
Language Processing, 1999.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Reinhard Kneser and Volker Steinbiss. 1993. On the
dynamic adaptation of stochastic language models.
In ICASSP 1993, pages 586?589, Minneapolis, Min-
nesota. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
2003, pages 127?133.
Roland Kuhn and Renato De Mori. 1990. A cache-based
natural language model for speech recognition. IEEE
Trans on PAMI, 12(6):570?583, June.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL 2006
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL 2003, Sapporo,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Report
RC22176, IBM, September.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge,
UK.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
ACL 2006.
Nicola Ueffing. 2006. Self-training for machine trans-
lation. In NIPS 2006 Workshop on MLIA, Whistler,
B.C., December.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT/NAACL 2004, Boston, May.
R. Zhang, H. Yamamoto, M. Paul, H. Okuma, K. Yasuda,
Y. Lepage, E. Denoual, D. Mochihashi, A. Finch, and
E. Sumita. 2006. The NiCT-ATR statistical machine
translation system for the IWSLT 2006 evaluation. In
IWSLT 2006.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In COLING
2004, Geneva, August.
135
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 242?249,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Stabilizing Minimum Error Rate Training
George Foster and Roland Kuhn
National Research Council Canada
first.last@nrc.gc.ca
Abstract
The most commonly used method for
training feature weights in statistical ma-
chine translation (SMT) systems is Och?s
minimum error rate training (MERT) pro-
cedure. A well-known problemwith Och?s
procedure is that it tends to be sensitive
to small changes in the system, particu-
larly when the number of features is large.
In this paper, we quantify the stability
of Och?s procedure by supplying different
random seeds to a core component of the
procedure (Powell?s algorithm). We show
that for systems with many features, there
is extensive variation in outcomes, both on
the development data and on the test data.
We analyze the causes of this variation and
propose modifications to the MERT proce-
dure that improve stability while helping
performance on test data.
1 Introduction
Most recent approaches in SMT, eg (Koehn et al,
2003; Chiang, 2005), use a log-linear model to
combine probabilistic features. Minimum Error-
Rate Training (MERT) aims to find the set of log-
linear weights that yields the best translation per-
formance on a development corpus according to
some metric such as BLEU. This is an essen-
tial step in SMT training that can significantly
improve performance on a test corpus compared
to setting weights by hand. MERT is a difficult
problem, however, because calculating BLEU as a
function of log-linear weights requires decoding,
which is an expensive operation. Moreover, be-
cause this function is not differentiable, efficient
gradient-based optimization algorithms cannot be
used.
Och?s procedure is the most widely-used ver-
sion of MERT for SMT (Och, 2003). To reduce
computational cost, it relies on the key technique
of optimizing weights over n-best lists of transla-
tion hypotheses rather than over all possible hy-
potheses. This allows the most probable hypoth-
esis under a given set of weights?and the corre-
sponding BLEU score?to be found by enumer-
ating n-best entries rather than decoding. Some
variant on Powell?s algorithm (Press et al, 2002)
is typically used to maximize BLEU in this set-
ting. The n-best lists are constructed by alternat-
ing decoding and BLEU maximization operations:
decoding adds new hypotheses to the current lists,
then BLEU is maximized over the lists to find new
best weights for the subsequent decoding step, etc.
This process continues until no new hypotheses
are found.
Och?s procedure works well in practice, usually
converging after 10?20 calls to the decoder, far
fewer than would be required to maximize BLEU
directly with a general-purpose optimization algo-
rithm. However, it tends to be sensitive to small
changes in the system, particularly for large fea-
ture sets. This is a well-known problem with
Och?s procedure (Och et al, 2004). It makes it
difficult to assess the contribution of features, be-
cause the measured gain in performance due to a
new feature can depend heavily on the setting of
some apparently unrelated parameter such as the
size of n-best list used. Features with the poten-
tial for statistically significant gains may be re-
jected because Och?s procedure failed to find good
weights for them.
In this paper we attempt to quantify the stabil-
ity of Och?s procedure under different conditions
by measuring the variation in test-set scores across
different random seeds used with Powell?s algo-
rithm. We show that there is extensive variation
for large feature sets, and that it is due to two main
factors: the occasional failure of Och?s procedure
to find a good maximum on the development set,
and the failure of some maxima to generalize to
242
the test set. We analyze the causes of each of these
problems, and propose solutions for improving the
stability of the overall procedure.
2 Previous Work
One possible approach to estimating log-linear
weights on features is to dispense with the n-best
lists employed by Och?s procedure and, instead,
to optimize weights by directly accessing the de-
coder. The disadvantage of this approach is that
far more iterations of decoding of the full devel-
opment set are required. In (Zens and Ney, 2004)
the downhill simplex method is used to estimate
the weights; around 200 iterations are required for
convergence to occur. However, each iteration is
unusually fast, because only monotone decoding
is permitted (i.e., the order of phrases in the tar-
get language mirrors that in the source language).
Similarly, Cettolo and Federico (2004) apply the
simplex method to optimize weights directly using
the decoder. In their experiments on NIST 2003
Chinese-English data, they found about 100 iter-
ations of decoding were required. Although they
obtained consistent and stable performance gains
for MT, these were inferior to the gains yielded
by Och?s procedure in (Och, 2003). Taking Och?s
MERT procedure as a baseline, (Zens et al, 2007)
experiment with different training criteria for SMT
and obtain the best results for a criterion they call
?expected BLEU score?.
Moore and Quirk (2008) share the goal under-
lying our own research: improving, rather than
replacing, Och?s MERT procedure. They focus
on the step in the procedure where the set of fea-
ture weights optimizing BLEU (or some other MT
metric) for an n-best list is estimated. Typically,
several different starting points are tried for this
set of weights; often, one of the starting points is
the best set of weights found for the previous set
of n-best hypotheses. The other starting points are
often chosen randomly. In this paper, Moore and
Quirk look at the best way of generating the ran-
dom starting points; they find that starting points
generated by a random walk from previous max-
ima are superior to those generated from a uni-
form distribution. The criterion used throughout
the paper to judge the performance of MERT is the
BLEU score on the development test set (rather
than, for instance, the variance of that score, or
the BLEU score on held-out test data). Another
contribution of the paper is ingenious methods for
pruning the set of n-best hypotheses at each itera-
tion.
Cer et al(2008) also aim at improving Och?s
MERT. They focus on the search for the best set
of weights for an n-best list that follows choice
of a starting point. They propose a modified ver-
sion of Powell?s in which ?diagonal? directions
are chosen at random. They also modify the ob-
jective function used by Powell?s to reflect the
width of the optima found. They are able to show
that their modified version of MERT outperforms
both a version using Powell?s, and a more heuris-
tic search algorithm devised by Philipp Koehn
that they call Koehn Coordinate Descent, as mea-
sured on the development set and two test data
sets. (Duh and Kirchhoff, 2008) ingeniously uses
MERT as a weak learner in a boosting algorithm
that is applied to the n-best reranking task, with
good results (a gain of about 0.8 BLEU on the test
set).
Recently, some interesting work has been done
on what might be considered a generalization of
Och?s procedure (Macherey et al, 2008). In this
generalization, candidate hypotheses in each iter-
ation of the procedure are represented as lattices,
rather than as n-best lists. This makes it possi-
ble for a far greater proportion of the search space
to be represented: a graph density of 40 arcs per
phrase was used, which corresponds to an n-best
size of more than two octillion (2 ? 1027) entries.
Experimental results for three NIST 2008 tasks
were very encouraging: though BLEU scores for
the lattice variant of Och?s procedure did not typ-
ically exceed those for the n-best variant on de-
velopment data, on test data the lattice variant out-
performed the n-best approach by between 0.6 and
2.5 BLEU points. The convergence behaviour of
the lattice variant was also much smoother than
that of the n-best variant. It would be interesting
to apply some of the insights of the current paper
to the lattice variant of Och?s procedure.
3 Och?s MERT Procedure
Och?s procedure works as follows. First the de-
coder is run using an initial set of weights to gen-
erate n best translations (usually around 100) for
each source sentence. These are added to exist-
ing n-best lists (initially empty). Next, Powell?s
algorithm is used to find the weights that maxi-
mize BLEU score when used to choose the best
hypotheses from the n-best lists. These weights
243
are plugged back into the decoder, and the pro-
cess repeats, nominally until the n-best lists stop
growing, but often in practice until some criterion
of convergence such as minimum weight change
is attained. The weights that give the best BLEU
score when used with the decoder are output.
The point of this procedure is to bypass di-
rect search for the weights that result in maxi-
mum BLEU score, which would involve decoding
using many different sets of weights in order to
find which ones gave the best translations. Och?s
procedure typically runs the decoder only 10?20
times, which is probably at least one order of mag-
nitude fewer than a direct approach. The main
trick is to build up n-best lists that are represen-
tative of the search space, in the sense that a given
set of weights will give approximately the same
BLEU score when used to choose the best hy-
potheses from the n-best lists as it would when de-
coding. By iterating, the algorithm avoids weights
that give good scores on the n-best lists but bad
ones with the decoder, since the bad hypotheses
that are scored highly by such weights will get
added to the n-best lists, thereby preventing the
choice of these weights in future iterations. Unfor-
tunately, there is no corresponding guarantee that
weights which give good scores with the decoder
but bad ones on the nbest lists will get chosen.
Finding the set of weights that maximizes
BLEU score over n-best lists is a relatively easy
problem because candidate weight sets can be
evaluated in time proportional to n (simply cal-
culate the score of each hypothesis according to
the current weight set, then measure BLEU on the
highest scoring hypothesis for each source sen-
tence). Powell?s algorithm basically loops over
each feature in turn, setting its weight to an op-
timum value before moving on.1 Och?s linemax
algorithm is used to perform this optimization effi-
ciently and exactly. However this does not guaran-
tee that Powell?s algorithm will find a global max-
imum, and so Powell?s is typically run with many
different randomly-chosen initial weights in order
to try to find a good maximum.
4 Experimental Setup
The experiments described here were carried out
with a standard phrase-based SMT system (Koehn
1It can also choose to optimize linear combinations of
weights in order to avoid ridges that are not aligned with the
original coordinates, which can be done just as easily.
corpus num sents num Chinese toks
dev1 1506 38,312
dev2 2080 55,159
nist04 1788 53,446
nist06 1664 41,798
Table 1: Development and test corpora.
et al, 2003) employing a log-linear combination
of feature functions. HMM and IBM2 models
were used to perform separate word alignments,
which were symmetrized by the usual ?diag-and?
algorithm prior to phrase extraction. Decoding
used beam search with the cube pruning algorithm
(Huang and Chiang, 2007).
We used two separate log-linear models for
MERT:
? large: 16 phrase-table features, 2 4-gram lan-
guage model features, 1 distortion feature,
and 1 word-count feature (20 features in to-
tal).
? small: 2 phrase-table features, 1 4-gram lan-
guage model feature, 1 distortion feature, and
1 word-count feature (5 features in total).
The phrase-table features for the large model were
derived as follows. Globally-trained HMM and
IBM2 models were each used to extract phrases
from UN and non-UN portions of the training cor-
pora (see below). This produced four separate
phrase tables, each of which was used to generate
both relative-frequency and ?lexical? conditional
phrase-pair probabilities in both directions (target
given source and vice versa). The two language
model features in the large log-linear model were
trained on the UN and non-UN corpora. Phrase-
table features for the small model were derived by
taking the union of the four individual tables, sum-
ming joint counts, then calculating relative fre-
quencies.
All experiments were run using the Chi-
nese/English data made available for NIST?s 2008
MT evaluation. This included approximately 5M
sentence pairs of data from the UN corpus, and
approximatel 4M sentence pairs of other mate-
rial. The English Gigaword corpus was not used
for language model training. Two separate devel-
opment corpora were derived from a mix of the
NIST 2005 evaluation set and some webtext drawn
from the training material (disjoint from the train-
ing set used). The evaluation sets for NIST 2004
244
cfg nist04 nist06
avg ? S avg ? S
S1 31.17 1.09 0.28 26.95 0.90 0.27
S2 31.44 0.22 0.07 27.38 0.71 0.19
L1 33.03 1.09 0.37 29.22 0.97 0.34
L2 33.37 1.49 0.49 29.61 2.14 0.66
Table 2: Test-set BLEU score variation with 10
different random seeds, for small (S) and large (L)
models on dev sets 1 and 2. The avg column gives
the average BLEU score over the 10 runs; ? gives
the difference between the maximum and mini-
mum scores, and S is the standard deviation.
and NIST 2005 corpora were used for testing. Ta-
ble 1 summarizes the sizes of the devtest corpora,
all of which have four reference translations.
5 Measuring the Stability of Och?s
Algorithm
To gauge the response of Och?s algorithm to small
changes in system configuration, we varied the
seed value for initializing the random number gen-
erator used to produce random starting points for
Powell?s algorithm. For each of 10 different seed
values, Och?s algorithm was run for a maximum of
30 iterations2 using 100-best lists. Table 2 shows
the results for the two different log-linear models
described in the previous section.
The two development sets exhibit a similar pat-
tern: the small models appear to be somewhat
more stable, but all models show considerable
variation in test-set BLEU scores. For the large
models, the average difference between best and
worst BLEU scores is almost 1.5% absolute, with
an average standard deviation of almost 0.5%.
Differences of as little as 0.35% are significant at
a 95% confidence level according to paired boot-
strap resampling tests on this data, so these varia-
tions are much too large to be ignored.
The variation in table 2 might result from Och?s
algorithm failing to maximize development-set
BLEU properly on certain runs. Alternatively, it
could be finding different maxima that vary in the
extent to which they generalize to the test sets.
Both of these factors appear to play a role. The
ranges of BLEU scores on the two development
corpora with the large models are 0.86 and 1.3 re-
spectively; the corresponding standard deviations
2Sufficient for effective convergence in all cases we
tested.
dev nist04 nist06 inter
? r ? r ?
dev1 0.18 0.42 -0.27 0.07 0.73
dev2 0.55 0.60 0.73 0.85 0.94
Table 3: Pearson (?) and Spearman rank (r) cor-
relation between dev-set and test-set BLEU scores
for the large log-linear model. The final column
shows nist04/nist06 correlation.
are 0.27 and 0.38. Different runs clearly have sig-
nificantly different degrees of success in maximiz-
ing BLEU.
To test whether the variation in development-
set BLEU scores accounts completely for the vari-
ation in test-set scores, we measured the correla-
tion between them. The results in table 3 show
that this varies considerably across the two de-
velopment and test corpora. Although the rank
correlation is always positive and is in some
cases quite high, there are many examples where
higher development-set scores lead to lower test-
set scores. Interestingly, the correlation between
the two test-set scores (shown in the last column of
the table) is much higher than that between the de-
velopment and test sets. Since the test sets are not
particularly similar to each other, this suggests that
some sets of log-linear weights are in fact overfit-
ting the development corpus.
5.1 Bootstrapping with Random Seeds
The results above indicate that the stability prob-
lems with Och?s MERT can be quite severe, es-
pecially when tuning weights for a fairly large
number of features. However, they also consti-
tute a baseline solution to these problems: run
MERT some number of times with different ran-
dom seeds, then choose the run that achieves the
highest BLEU score on a test set. Since test-
set scores are highly correlated, these weights are
likely to generalize well to new data. Applying
this procedure using the nist04 corpus to choose
weights yields a BLEU increase of 0.69 on nist06
compared to the average value over the 10 runs in
table 2; operating in the reverse direction gives an
increase of 0.37 on nist04.3
3These increases are averages over the increases on each
development set. This comparison is not strictly fair to the
baseline single-MERT procedure, since it relies on a test set
for model selection (using the development set would have
yielded gains of 0.25 for nist06 and 0.27 for nist04). How-
ever, it is fairly typical to select models (involving different
feature sets, etc) using a test set, for later evaluation on a
245
 29
 29.2
 29.4
 29.6
 29.8
 30
 30.2
 30.4
 30.6
 1  2  3  4  5  6  7  8  9  10
number of runs
NIST06 BLEU scores versus number of random runs
dev2
dev1
Figure 1: Results on the nist06 test corpus, using
nist04 to choose best weights from varying num-
bers of MERT runs, averaged over 1000 random
draws. The error bars indicate the magnitude of
the standard deviation.
An obvious drawback to this technique is that
it requires the expensive MERT procedure to be
run many times. To measure the potential gain
from using fewer runs, and to estimate the stability
of the procedure, we used a bootstrap simulation.
For each development set and each n from 1 to 10,
we randomly drew 1000 sets of n runs from the
data used for table 2, then recorded the behaviour
of the nist06 scores that corresponded to the best
nist04 score. The results are plotted in figure 1.
There is no obvious optimal point on the curves,
although 7 runs would be required to reduce the
standard deviation on dev2 (the set with the higher
variance) below 0.35. In the following sections
we evaluate some alternatives that are less com-
putationally expensive. The large model setting is
assumed throughout.
6 Improving Maximization
In this section we address the problem of improv-
ing the maximization procedure over the devel-
opment corpus. In general, we expect that being
able to consistently find higher maxima will lead
to lower variance in test-set scores. Previous work,
eg (Moore and Quirk, 2008; Cer et al, 2008), has
focused on improving the performance of Powell?s
algorithm. The degree to which this is effective de-
pends on how good an approximation the current
n-best lists are to the true search space. As illus-
second, blind, test set. A multi-MERT strategy could be nat-
urally incorporated into such a regime, and seems unlikely to
give rise to substantial bias.
A B C
Figure 2: True objective function (bold curve)
compared to n-best approximation (light curve).
Och?s algorithm can correct for false maxima like
B by adding hypotheses to n-best lists, but may
not find the true global maximum (C), converging
to local peaks like A instead.
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
BL
EU
iter
BLEU scores versus Och iteration
best run
worst run
Figure 3: Development-set BLEU scores after
each Och iteration for two different training runs
on the dev2 corpus.
trated in figure 2, it is possible for the true space to
contain maxima that are absent from the approxi-
mate (n-best) space. Figure 3 gives some evidence
that this happens in practice. It shows the evolu-
tion of decoder BLEU scores with iteration for the
best and worst runs for dev2. Although the worst
run explores a somewhat promising area at itera-
tion 7, it converges soon afterwards in a region that
gives lower true BLEU scores. This is not due to
a failure of Powell?s algorithm, since the scores on
the n-best lists rise monotonically in this range.
We explored various simple strategies for avoid-
ing the kind of local-maximum behaviour exhib-
ited in figure 3. These are orthogonal to improve-
ments to Powell?s algorithm, which was used in
its standard form. Our baseline implementation of
Och?s algorithm calls Powell?s three times start-
ing with each of the three best weight sets from
the previous iteration, then a certain number of
times with randomly-generated weights. The to-
tal number of Powell?s calls is determined by an
algorithm that tries to minimize the probability of
246
a new starting point producing a better maximum.4
The first strategy was simply to re-seed the ran-
dom number generator (based on a given global
seed value) for each iteration of Och?s algorithm.
Our implementation had previously re-used the
same ?random? starting points for Powell?s across
different Och iterations. This is arguably justifi-
able on the grounds that the function to be opti-
mized is different each time.
The second strategy was motivated by the ob-
servation that after the first several iterations of
Och?s algorithm, the starting point that leads to
the best Powell?s result is nearly always one of
the three previous best weight sets rather than a
randomly-generated set. To encourage the algo-
rithm to consider other alternatives, we used the
three best results from all previous Och?s itera-
tions. That is, on iteration n, Powell?s is started
with the three best results from iteration n?1, then
the three best from n?2, and so forth. If more than
3(n ? 1) points are required by the stopping al-
gorithm described above, then they are generated
randomly.
The final strategy is more explicitly aimed at
forcing the algorithm to cover a broader por-
tion of the search space. Rather than choosing
the maximum-BLEU results from Powell?s algo-
rithm for the subsequent decoding step, we choose
weight vectors that yield high BLEU scores and
are dissimilar from previous decoding weights.
Formally:
?? = argmax
??P
w rbleu(?) + (1? w) rdist(?),
where P is the set of all weight vectors returned
by Powell?s on the current iteration, rbleu(?) is
??s BLEU score divided by the highest score for
any vector in P , and rdist(?) is ??s distance to
previous weights divided by the largest distance
for any vector in P . Distance to previous weights
is measured by taking the minimum L2 distance
from ? to any of the decoding weight vectors used
during the previous m Och iterations.
Intuitively, the weight w that controls the im-
portance of BLEU score relative to novelty should
increase gradually as Och?s algorithm progresses
in order to focus the search on the best maxi-
4Whenever a new maximum is encountered, at least the
current number of new starting points must be tried before
stopping, with a minimum of 10 points in total. Experiments
where the total number of starts was fixed at 30 did not pro-
duce significantly different results.
mum found (roughly similar to simulated anneal-
ing search). To accomplish this, w is defined as:
w = 1? a/(iter + b),
where b ? 0 and a ? b + 1 are parameters that
control w?s decay, and iter is the current Och iter-
ation.
Each of the three strategies outlined above was
run using 10 random seeds with both development
corpora. The weight selection strategy was run
with two different sets of values for the a and b
parameters: a = 1, b = 1 and a = 5, b = 9. Each
assigns equal weight to BLEU score and novelty
on the first iteration, but under the first parameter-
ization the weight on novelty decays more swiftly,
to 0.03 by the final iteration compared to to 0.13.
The results are shown in table 4. The best strat-
egy overall appears to be a combination of all three
techniques outlined above. Under the a = 5,
b = 9, m = 3 parametrization for the final (weight
selection) strategy, this improves the development
set scores by an average of approximately 0.4%
BLEU compared to the baseline, while signifi-
cantly reducing the variation across different runs.
Performance of weight selection appears to be
quite insensitive to its parameters: there is no sig-
nificant difference between the a = 1, b = 1 and
a = 5, b = 9 settings. It is possible that further
tuning of these parameters would yield better re-
sults, but this is an expensive procedure; we were
also wary of overfitting. A good fallback is the
first two strategies, which together achieve results
that are almost equivalent to the final gains due to
weight selection.
7 Generalization
As demonstrated in section 5, better performance
on the development set does not necessarily lead
to better performance on the test set: two weight
vectors that give approximately the same dev-set
BLEU score can give very different test-set scores.
We investigated several vectors with this charac-
teristic from the experiments described above, but
were unable to find any intrinsic property that was
a good predictor of test-set performance, perhaps
due to the fact that the weights are scale invari-
ant. We also tried averaging BLEU over boot-
strapped samples of the development corpora, but
this was also not convincingly correlated with test-
set BLEU.
247
strategy dev avg ? S
baseline 1 22.64 0.87 0.27
2 19.11 1.31 0.38
re-seed 1 22.87 0.65 0.21
2 19.37 0.60 0.17
+history 1 22.99 0.43 0.15
2 19.44 0.35 0.11
+sel 1,1,3 1 23.12 0.59 0.19
2 19.53 0.38 0.13
+sel 5,9,3 1 23.11 0.42 0.13
2 19.46 0.44 0.14
Table 4: Performance of various strategies for im-
proving maximization on the dev corpora: base-
line is the baseline used in section 5; re-seed is
random generator re-seeding; history is accumu-
lation of previous best weights as starting point;
and sel a,b,m is the final, weight selection, strat-
egy described in section 6, parameterized by a, b,
and m. Strategies are applied cumulatively, as in-
dicated by the + signs.
An alternate approach was inspired by the reg-
ularization method described in (Cer et al, 2008).
In essence, this uses the average BLEU score from
the points close to a given maximum as a surro-
gate for the BLEU at the maximum, in order to
penalize maxima that are ?narrow? and therefore
more likely to be spurious. While Cer et aluse this
technique while maximizing along a single dimen-
sion within Powell?s algorithm, we apply it over
all dimensions with the vectors output from Pow-
ell?s. Each individual weight is perturbed accord-
ing to a normal distribution (with variance 1e-03),
then the resulting vector is used to calculate BLEU
over the n-best lists. The average score over 10
such perturbed vectors is used to calculate rbleu
in the weight-selection method from the previous
section.
The results from regularized weight selection
are compared to standard weight selection and to
the baseline MERT algorithm in table 5. Regu-
larization appears to have very little effect on the
weight selection approach. This does not neces-
sarily contradict the results of Cer et al since it is
applied in a very different setting. The standard
weight selection technique (in combination with
the re-seeding and history accumulation strate-
gies) gives a systematic improvement in average
test-set BLEU score over the baseline, although it
does not substantially reduce variance.
strategy dev test avg ? S
baseline 1 04 33.03 1.09 0.37
06 29.22 0.97 0.34
2 04 33.37 1.49 0.49
06 29.61 2.14 0.66
(+) sel 5,9,3 1 04 33.43 1.23 0.41
06 29.62 0.98 0.31
2 04 33.95 1.03 0.37
06 30.32 0.88 0.30
+ reg 10 1 04 33.36 1.45 0.49
06 29.56 1.25 0.39
2 04 33.81 0.94 0.28
06 30.17 1.21 0.35
Table 5: Performance of various MERT tech-
niques on the test corpora. (+) sel 5,9,3 is the same
configuration as +sel 5,9,3 in table 4; + reg 10
uses regularized BLEU within this procedure.
8 Conclusion
In this paper, we have investigated the stability
of Och?s MERT algorithm using different random
seeds within Powell?s algorithm to simulate the
effect of small changes to a system. We found
that test-set BLEU scores can vary by 1 percent
or more across 10 runs of Och?s algorithm with
different random seeds. Using a bootstrap analy-
sis, we demonstrate that an effective, though ex-
pensive, way to stabilize MERT would be to run it
many times (at least 7), then choose the weights
that give best results on a held-out corpus. We
propose less expensive simple strategies for avoid-
ing local maxima that systematically improve test-
set BLEU scores averaged over 10 MERT runs, as
well as reducing their variance in some cases. An
attempt to improve on these strategies by regular-
izing BLEU was not effective.
In future work, we plan to integrate improved
variants on Powell?s algorithm, which are orthog-
onal to the investigations reported here.
9 Acknowlegement
This material is partly based upon work supported
by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023. Any opinions, findings and conclu-
sions or recommendations expressed in this ma-
terial are those of the authors and do not neces-
sarily reflect the views of the Defense Advanced
Research Projects Agency (DARPA).
248
References
Daniel Cer, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Regularization and search for minimum
error rate training. In Proceedings of the ACL Work-
shop on Statistical Machine Translation, Columbus,
June. WMT.
Mauro Cettolo and Marcello Federico. 2004. Min-
imum error training of log-linear translation mod-
els. In International Workshop on Spoken Language
Translation, Kyoto, September.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Ann Ar-
bor, Michigan, July.
Kevin Duh and Katrin Kirchhoff. 2008. Beyond log-
linear models: Boosted minimum error rate training
for n-best re-ranking. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Columbus, Ohio, June.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics
(ACL), Prague, Czech Republic, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Ed-
uard Hovy, editor, Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 127?133, Edmonton, Alberta,
Canada, May. NAACL.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine transla-
tion. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Honolulu.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statisti-
cal machine translation. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING) 2008, Manchester, August.
Franz Josef Och, Daniel Gildea, and Sanjeev Khudan-
pur et al 2004. Final report of johns hopkins
2003 summer workshop on syntax for statistical ma-
chine translation (revised version). Technical report,
February 25.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Meeting of the Association for
Computational Linguistics (ACL), Sapporo, July.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
Recipes in C++. Cambridge University Press, Cam-
bridge, UK.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proceedings of Human Language Technol-
ogy Conference / North American Chapter of the
ACL, Boston, May.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Prague, Czech Re-
public.
249
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608?616,
Beijing, August 2010
Phrase Clustering for Smoothing TM Probabilities ? or, How to 
Extract Paraphrases from Phrase Tables  
1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford 
1National Research Council of Canada 
2University of Waterloo 
1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com 
 
Abstract 
This paper describes how to cluster to-
gether the phrases of a phrase-based sta-
tistical machine translation (SMT) sys-
tem, using information in the phrase table 
itself. The clustering is symmetric and 
recursive: it is applied both to source-
language and target-language phrases, 
and the clustering in one language helps 
determine the clustering in the other. The 
phrase clusters have many possible uses. 
This paper looks at one of these uses: 
smoothing the conditional translation 
model (TM) probabilities employed by 
the SMT system. We incorporated 
phrase-cluster-derived probability esti-
mates into a baseline loglinear feature 
combination that included relative fre-
quency and lexically-weighted condition-
al probability estimates. In Chinese-
English (C-E) and French-English (F-E) 
learning curve experiments, we obtained 
a gain over the baseline in 29 of 30 tests, 
with a maximum gain of 0.55 BLEU 
points (though most gains were fairly 
small). The largest gains came with me-
dium (200-400K sentence pairs) rather 
than with small (less than 100K sentence 
pairs) amounts of training data, contrary 
to what one would expect from the pa-
raphrasing literature. We have only be-
gun to explore the original smoothing 
approach described here.  
1 Introduction and Related Work 
The source-language and target-language ?phras-
es? employed by many statistical machine trans-
lation (SMT) systems are anomalous: they are 
arbitrary sequences of contiguous words ex-
tracted by complex heuristics from a bilingual 
corpus, satisfying no formal linguistic criteria. 
Nevertheless, phrase-based systems perform bet-
ter than word-based systems (Koehn 2010, pp. 
127-129). In this paper, we look at what happens 
when we cluster together these anomalous but 
useful entities.  
Here, we apply phrase clustering to obtain bet-
ter estimates for ?backward? probability P(s|t) 
and ?forward? probability P(t|s), where s is a 
source-language phrase, t is a target-language 
phrase, and phrase pair (s,t) was seen at least 
once in training data. The current work is thus 
related to work on smoothing P(s|t) and P(t|s) ? 
see (Foster et al, 2006). The relative frequency 
estimates for P(s|t) and P(t|s) are  
ttstsPRF /#),(#)|( = and stsstPRF /#),(#)|( = , 
where #(s,t) denotes the number of times phrase 
pair (s,t) was observed, etc. These estimates are 
typically smoothed with ?lexical? estimates 
found by breaking phrases s and t into words. 
We adopt a different idea, that of smoothing 
PRF(s|t) and PRF(t|s) with estimates obtained from 
phrases that have similar meanings to s and t. In 
our experiments, the two methods were com-
bined, yielding an improvement over lexical 
smoothing alone ? this indicates they provide 
complementary information. E.g., lexical esti-
mates don?t work well for non-compositional 
phrases like ?kick the bucket? - our method 
might cluster this phrase with ?die? and ?expire? 
and thus provide better smoothing. The research 
that comes closest to ours is the work of 
Schwenk et al (2007) on continuous space N-
gram models, where a neural network is em-
ployed to smooth translation probabilities. How-
ever, both Schwenk et al?s smoothing technique 
608
and the system to which it is applied are quite 
different from ours. 
Phrase clustering is also somewhat related to 
work on paraphrases for SMT. Key papers in this 
area include (Bannard and Callison-Burch, 2005), 
which pioneered the extraction of paraphrases 
from bilingual parallel corpora, (Callison-Burch 
et al, 2006) which showed that paraphrase gen-
eration could improve SMT performance, (Calli-
son-Burch, 2008) and (Zhao et al, 2008) which 
showed how to improve the quality of paraphras-
es, and (Marton et al, 2009) which derived pa-
raphrases from monolingual data using distribu-
tional information. Paraphrases typically help 
SMT systems trained on under 100K sentence 
pairs the most.  
The phrase clustering algorithm in this paper 
outputs groups of source-language and target-
language phrases with similar meanings: paraph-
rases. However, previous work on paraphrases 
for SMT has aimed at finding translations for 
source-language phrases in the system?s input 
that weren?t seen during system training. Our 
approach is completely useless in this situation: 
it only generates new information for target or 
source phrases that are already in the system?s 
phrase table. Thus, we find paraphrases for many 
of the source and target phrases that are in the 
phrase table, while the work cited above looks 
for paraphrases of source phrases that are not in 
the phrase table.  
Our work also differs from most work on pa-
raphrases in that information is extracted not 
from sources outside the SMT system (e.g., pivot 
languages or thesauri) but from the system?s 
phrase table. In this respect if no other, it is simi-
lar to Chiang?s classic work on hierarchical 
phrase-based systems (Chiang, 2005), though 
Chiang was mining a very different type of in-
formation from phrase tables. 
Because of all these differences between work 
on paraphrasing and the phrase clustering ap-
proach, both in terms of the input information 
and where they are best applied, we did not expe-
rimentally compare the two approaches.     
2 Deriving Conditional Probabilities 
from Phrase Clusters 
Given phrase clusters in the source and target 
languages, how would one derive estimates for 
conditional probabilities P(s|t) and P(t|s)? We 
assume that the clustering is ?hard?: each source 
phrase s belongs to exactly one cluster C(s), and 
each target phrase t belongs to exactly one 
cluster C(t). Some of these clusters will contain 
singleton phrases, and others will contain more 
than one phrase. Let ?#? denote the total number 
of observations in the training data associated 
with a phrase or phrase cluster. E.g., suppose the 
English cluster CS contains the three phrases 
?red?, ?dark red?, and ?burgundy?, with 50, 25, 
and 10 observations in the training data 
respectively ? then #(CS) = 85. Also, let #(CS,CT) 
be the number of co-occurrences in the training 
data of source-language cluster CS and target-
language cluster CT.  
The phrase-cluster-based probabilities PPC are: 
)(#
))(),((#
)(#
)(#
))(|)(())(|()|(
tC
tCsC
sC
s
tCsCPsCsPtsPPC
?=
?=
  (1) 
and 
)(#
))(),((#
)(#
)(#
))(|)(())(|()|(
sC
tCsC
tC
t
sCtCPtCtPstPPC
?=
?=
   (2) 
Note that the PPC will often be non-zero where 
the corresponding relative frequency estimates 
PRF were zero (the opposite can?t happen). Also, 
the PPC will be most useful where the phrase be-
ing conditioned on was seldom seen in the train-
ing data. If t was seen 1,000 times during train-
ing, the PRF(s|t) are reliable and don?t need 
smoothing; but if t was seen 6 times,  PPC(s|t) 
may yield valuable extra information. The same 
kind of argument applies to estimation of P(t|s). 
3 Clustering Phrases 
We used only information ?native? to phrase 
tables to cluster phrases. Two types of similarity 
metric between phrases or phrase clusters were 
employed: count-based metrics and edit-based 
metrics. The former are based on phrase co-
occurrence counts; the latter are based on the 
word sequences that make up the phrases. Each 
has its advantages. Count-based metrics can de-
duce from the similar translations of two phrases 
that they have similar meanings, despite dissimi-
larity between the two word sequences ? e.g., 
they can deduce that ?red? and ?burgundy? be-
long in the same cluster. However, these metrics 
are unreliable when total counts are low, since 
phrase co-occurrences are determined by a noisy 
alignment process. Edit-based metrics are inde-
pendent of how often phrases were observed. 
However, sometimes they can be fooled by 
phrases that have similar word sequences but 
different meanings (e.g., ?the dog bit the man? 
609
and ?the man bit the dog?, or ?walk on the 
beach? and ?don?t walk on the beach?). In our 
experiments, we used a combination of count-
based and edit-based metrics to cluster phrases 
(by simply multiplying the metrics together). 
However, we invested most of our effort in per-
fecting the count-based component: our edit-
based metric was fairly na?ve.  
If we rely mainly on count-based similarity 
between phrases to cluster them, and this kind of 
similarity is most reliable when phrases have 
high counts, yet we need phrase-cluster-based 
estimates most for phrases with low counts, 
aren?t we carrying out clustering on the phrases 
that need it least? Our hope was that there is a 
class of phrases with intermediate counts (e.g., 
with 3-15 observations in the training data) that 
can be clustered reliably, but still benefit from 
phrase-cluster-based probability estimates.  
3.1 Count-based clustering: overview  
Figure 1 shows count-based phrase clustering. 
One first arbitrarily picks a language (either 
source or target) and then clusters together some 
of the phrases in that language. One then switch-
es to the other language and clusters phrases in 
that language, then switches back to the first one, 
and so on until enough clustering has taken place.  
Each phrase or phrase cluster is represented by 
the vector of its co-occurrence counts. To calcu-
late the similarity between two phrase clusters, 
one first normalizes their count vectors. At the 
top of Figure 1, source phrase s1 occurred 9 
times: 7 times aligned with target phrase t1, 2 
times aligned with t4. For source similarity com-
putation, the entry for (s1,t1) is normalized to 7/9 
= 0.78 and the entry for (s1,t4) is normalized to 
2/9 = 0.22 (these normalized values are shown in 
brackets and italics after the counts).  
The two most similar normalized vectors at 
the top of Figure 1 are those associated with 
phrases s1 and s2. These phrases are merged by 
adding corresponding counts, yielding a new 
vector associated with the new phrase cluster {s1, 
s2}. In real life, one would now do more source-
language clustering on the source language side; 
in this example, we immediately proceed to tar-
get-language clustering (carried out in target lan-
guage space). Note that the target similarity cal-
culations are affected by the previous source 
clustering (because s1 and s2 are now 
represented by the same coordinate, t3 and t4 are 
now closer than they were in the initial table). In 
this manner, we can iterate back and forth be-
tween the two languages. The final output is a 
table of joint phrase cluster counts, which is used 
to estimate the PPC (see previous section).   
3.2 Count-based clustering: details 
Count-based similarity is computed as follows:   
1. Phrase alignment is a noisy process, so 
we first apply a transformation analogous 
to tf-idf in information retrieval (Salton 
and McGill, 1986) to phrase cluster 
 
Figure 1: Example of phrase clustering 
 
610
counts. For source similarity computation, 
each co-occurrence count #(CS,CT) be-
tween source cluster CS and target cluster 
CT is multiplied by a factor that reflects 
the information content of CT. Let 
#diff(CS) be number of clusters on the 
source side, and let #[CT>0] for a par-
ticular target cluster CT be the number of 
source clusters CS that co-occur with CT. 
Then let 
])0[/#)(log(#),(#),('# >?= TSTSTS CCdiffCCCC .   
Similarly, for target similarity computa-
tion, let 
])0[/#)(log(#),(#),('# >?= STTSTS CCdiffCCCC .   
E.g., in source similarity computation, if 
CT co-occurs with all source clusters, its 
contribution will be set to zero (because 
it carries little information).  
2. We normalize by dividing each vector of 
tf-idf counts ),('# TS CC  by the total num-
ber of observations in the vector. 
3. We compute the similarity between each 
pair of tf-idf vectors using either the co-
sine measure (Salton and McGill, 1986) 
or one of a family of probabilistic metrics 
described below.  
4. We cluster together the most similar vec-
tors; this involves summing the unmodi-
fied counts #(CS,CT) of the vectors (i.e., 
the tf-idf transformation is only applied 
for the purposes of similarity calculation 
and is not retained).  
Now, we?ll describe the probabilistic metrics 
we considered. For a count vector of dimension 
D, u
 
= (u1, u2, ?, uD), define a function 
)/log(...)/log()( 11 ?? ?++?= i iDDi i uuuuuuI u . 
I(u) is a measure of how well the data in u are 
modeled by the normalized vector (u1/?iui,  ?, 
uD/?iui).  Thus, when two count vectors u and v 
are merged (by adding them) we have the follow-
ing measure of the loss in modeling accuracy:  
 
Probability Loss (PL): 
 )()()(),( vuvuvu +?+= IIIPL .   (3) 
 
However, if we choose merges with the lowest 
PL, we will usually merge only vectors with 
small counts. We are more interested in the aver-
age impact of a merge, so we define 
 
Average Probability Loss (APL):  
  )/())()()((),( ?? ++?+= i ii i vuIIIAPL vuvuvu . (4) 
In our initial experiments, APL worked better 
than PL. However, APL had a strange side-effect. 
Most of the phrase clusters it induced made intui-
tive sense, but there were typically three or four 
clusters with large numbers of observations on 
both language sides that grouped together phras-
es with wildly disparate meanings. Why does 
APL induce these ?monster clusters?? 
Consider two count vectors u and v. If ?iui is 
very big and ?ivi is small, then I(u) and I(u + v) 
will be very similar, and APL will be approx-
imately I(v) /[?iui + ?ivi ] which will be close to 
zero. Thus, the decision will probably be made to 
merge u and v, even if they have quite different 
semantics. The resulting cluster, whose counts 
are represented by u + v, is now even bigger and 
even more likely to swallow up other small count 
vectors in the next rounds of merging: it becomes 
a kind of black hole.  
To deal with this problem, we devised another 
metric. Let 
)/log(...)/log()|( 11 ?? ?++?= i iDDi i vvuvvuI vu . 
This is a measure of how well the counts in v 
predict the distribution of counts in u. Then let  
 
Maximum Average Probability Loss (MAPL):  
))|()(,)|()(max(),( ??
+?+?
=
i ii i
v
II
u
IIMAPL vuvvvuuuvu
 .(5) 
 
The first term inside the maximum indicates the 
average probability loss for an observation in u 
when it is modeled by u+v instead of u; similarly, 
the second term indicates the average probability 
loss for an observation in v. If we merge vector 
pairs with the lowest values of MAPL, we will 
never merge vectors in a way that will cause a 
large loss to either of the two parents.  
In practice, we found that all these metrics 
worked better when multiplied by the Dice coef-
ficient based distance. For u and v, this is 
||||
||21),(
vu
vu
vu
+
??
?=Dice , where ?|u|? means 
the number of non-zero count entries in u, and 
?| vu ? |? is the number of count entries that are 
non-zero in u and v. 
3.3 Edit-based similarity 
In most of our experiments, count-based metrics 
were combined with edit-based metrics; we put 
little effort into optimizing the edit metrics. Let 
MCWS stand for ?maximum common word se-
quence?. For phrases p1 and p2, we define  
611
)()(
)),((21),(
21
21
21 plenplen
ppMCWSlenppEdit
+
?
?= .        (6) 
where len() returns the number of  words. This 
metric doesn?t take word identities into account; 
in future work, we may weight differences in-
volving content words more heavily.  
We also defined an edit-based metric for dis-
tance between phrase clusters. Let cluster 1 have 
phrases ?red? (10); ?burgundy? (5); ?resembling 
scarlet? (2) and cluster 2 have ?dark burgundy? 
(7); ?scarlet? (3) (number of observations in 
brackets). What is the edit distance between clus-
ters 1 and 2? We defined the distance as that be-
tween the two phrases with the most observa-
tions in each cluster. Thus, distance between 
clusters 1 and 2 would be Edit(?red?, ?dark bur-
gundy?)=1.0. Other definitions are possible.  
3.4 Examples of phrase clusters 
Figure 2 shows an English phrase cluster learned 
during C-E experiments by a metric combining 
count-based and edit-based information. Each 
phrase is followed by its count in brackets; we 
don?t show phrases with low counts. Since our 
edit distance sees words as atoms (it doesn?t 
know about morphology), the phrases containing 
?emancipating? were clustered with phrases con-
taining ?emancipation? based on count informa-
tion, rather than because of the common stem.  
Figure 3 shows part of a French phrase cluster 
learned during F-E experiments by the same 
mixed metric. The surface forms are quite varied, 
but most of the phrases mean ?to assure or to 
guarantee that something will happen?. An inter-
esting exception is ?pas faire? ? it means not to 
do something (?pas? is negative). This illustrates 
why we need a better edit distance that heavily 
weights negative words.  
 
emancipating (247), emancipate 
(167), emancipate our (73), emanci-
pating thinking (67), emancipate 
our minds (46), further emancipate 
(45), emancipate the (38), emanci-
pate the mind (38), emancipating 
minds (33), emancipate their (32), 
emancipate their minds (27), eman-
cipating our minds (24), emancipat-
ing our (21), emancipate our mind 
(21), further emancipate our (19), 
emancipate our thinking (14), fur-
ther emancipate their (11), emanci-
pating the minds (9), emancipate 
thinking (8), unfettering (8) ...  
 
Figure 2: partial English phrase cluster 
 
garantir que (64), assurer que 
(46), veiller ? ce que (27), afin 
de garantir (24), faire en sorte 
(19), de garantir que (16), afin de 
garantir que (14), faire des (14), 
de veiller ? ce (14), s' assurer 
que (13), de veiller ? ce que (13), 
pour garantir que (13), de faire en 
sorte (8), de faire en sorte que 
(7), ? garantir que (6), pas faire 
(5), de veiller (5)? 
 
Figure 3:  partial French phrase cluster 
4 Experiments  
We carried out experiments on a standard one-
pass phrase-based SMT system with a phrase 
table derived from merged counts of symme-
trized IBM2 and HMM alignments; the system 
has both lexicalized and distance-based distor-
tion components (there is a 7-word distortion 
limit) and employs cube pruning (Huang and 
Chiang, 2007). The baseline is a loglinear feature 
combination that includes language models, the 
distortion components, relative frequency esti-
mators PRF(s|t) and PRF(t|s) and lexical weight 
estimators PLW(s|t) and PLW(t|s). The PLW() com-
ponents are based on (Zens and Ney, 2004); Fos-
ter et al (2006) found this to be the most effec-
tive lexical smoothing technique. The phrase-
cluster-based components PPC(s|t) and PPC(t|s) 
are incorporated as additional loglinear feature 
functions. Weights on feature functions are 
found by lattice MERT (Macherey et al, 2008).  
4.1 Data 
We evaluated our method on C-E and F-E tasks. 
For each pair, we carried out experiments on 
training corpora of different sizes. C-E data were 
from the NIST1 2009 evaluation; all the allowed 
bilingual corpora except the UN corpus, Hong 
Kong Hansard and Hong Kong Law corpus were 
used to estimate the translation model. For C-E, 
we trained two 5-gram language models: the first 
on the English side of the parallel data, and the 
second on the English Gigaword corpus. 
Our C-E development set is made up mainly 
of data from the NIST 2005 test set; it also in-
cludes some balanced-genre web-text from the 
NIST training material. Evaluation was per-
formed on the NIST 2006 and 2008 test sets. 
Table 1 gives figures for training, development 
and test corpora for C-E tasks; |S| is the number 
of sentences, and |W| is the number of words. 
There are four references for dev and test sets. 
                                               
1
 http://www.nist.gov/speech/tests/mt 
612
   Chi Eng 
All parallel 
Train 
|S| 3.3M 
|W| 68.2M 66.5M 
Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics for Chinese-to-English tasks. 
 
 
   Fre Eng 
Train Europarl |S| 1.6M 
|W| 51.3M 46.6M 
Dev 2008 |S| 2,051 
Test 2009 |S| 2,525 
2010 |S| 2,489 
GigaFrEn |S| - 22.5M 
 
Table 2: Statistics for French-to-English tasks. 
 
 
Lang (#sent) C-E (3.3M) F-E (1.6M) 
  #count-1  #other  #count-1  #other 
 
 
Src 
Before 
clustering 
11.3M 5.7M 28.1M 21.2M 
After  
clustering 
11.3M 5.3M 28.1M 19.3M 
#clustered 0 0.4M 0 1.9M 
 
 
Tgt 
Before 
clustering 
11.9M 6.0M 25.6M 20.4M 
After  
clustering 
11.9M 5.6M 25.6M 18.5M 
#clustered 0 0.4M 0 1.9M 
 
Table 3: # phrase classes before & after clustering. 
 
For F-E tasks, we used WMT 20102 F-E track 
data sets. Parallel Europarl data are used for 
training; WMT Newstest 2008 set is the dev set, 
and WMT Newstest 2009 and 2010 are the test 
sets. One reference is provided for each source 
input sentence. Two language models are used in 
this task: one is the English side of the parallel 
data, and the second is the English side of the 
GigaFrEn corpus. Table 2 summarizes the train-
ing, development and test corpora for F-E tasks. 
4.2 Amount of clustering and metric 
For both C-E and E-F, we assumed that phrases 
seen only once in training data couldn?t be clus-
tered reliably, so we prevented these ?count 1? 
phrases from participating in clustering. The key 
                                               
2
 http://www.statmt.org/wmt10/ 
clustering parameter is the number of merge op-
erations per iteration, given as a percentage of 
the number of potential same-language phrase 
pairs satisfying a simple criterion (some overlap 
in translations to the other language). Prelimi-
nary tests involving the FBIS corpus (about 8% 
of the C-E data) caused us to set this parameter at 
5%. For C-E, we first clustered Chinese with this 
5% value, then English with the same amount. 
For F-E, we first clustered French, then English, 
using 5% in both cases.  
Table 3 shows the results. Only 2-4% of the 
total phrases in each language end up in a cluster 
(that?s 6.5-9% of eligible phrases, i.e., of phrases 
that aren?t ?count 1?). However, about 20-25% 
of translation probabilities are smoothed for both 
language pairs. Based on these preliminary tests, 
we decided to use MAPLDiceEdit ??  
( DMAPLEdit ? ) as our metric (though 
CosineEdit ?  was a close runner-up).  
4.3 Results and discussion 
Our evaluation metric is IBM BLEU (Papineni et 
al., 2002), which performs case-insensitive 
matching of n-grams up to n = 4. Our first expe-
riment evaluated the effects of the phrase cluster-
ing features given various amounts of training 
data. Figure 4 gives the BLEU score improve-
ments for the two language pairs, with results for 
each pair averaged over two test sets (training 
data size shown as #sentences). The improve-
ment is largest for medium amounts of training 
data. Since the F-E training data has more words 
per sentence than C-E, the two peaks would have 
been closer together if we?d put #words on the x 
axis: improvements for both tasks peak around 6-
8 M English words. For more details, refer to 
Table 4 and Table 5. The biggest improvement 
is 0.55 BLEU for the NIST06 test. More impor-
tantly, cluster features yield gains in 29 of 30 
experiments. Surprisingly, a reviewer asked if 
we?d done significance tests on the individual 
results shown in Tables 4 and 5. Most likely, 
many of these individual results are insignificant, 
but so what? Based on the tables, the probability 
of the null hypothesis that our method has no 
effect is equivalent to that of tossing a fair coin 
30 times and getting 29 heads (if we adopt an 
independence approximation).  
In the research on paraphrases cited earlier, 
paraphrases tend to be most helpful for small 
amounts of training data. By contrast, our 
approach seems to be most helpful for medium 
amounts of training data (200-400K sentence 
613
pairs). We attribute this to the properties of 
count-based clustering. When there is little 
training data, clustering is unreliable; when there 
is much data, clustering is reliable but unneeded, 
because most relative frequencies are well-
estimated. In between, phrase cluster probability 
estimates are both reliable and useful. 
 
 
 
Figure 4: Average BLEU improvement for C-E and 
F-E tasks (each averaged over two tests) vs. #training 
sent. 
 
Finally, we carried out experiments to see if 
some of our earlier decisions were correct. Were 
we right to use DMAPL instead of cosine as the 
count-based component of our metric? Experi-
ments with DMAPLEdit ?  vs. 
CosineEdit ? on 400K C-E (tested on NIST06 
and NIST08) and on 200K F-E (tested on News-
test2009 and 2010) showed a tiny advantage for 
DMAPLEdit ? of about 0.06 BLEU. So we 
probably didn?t make the wrong decision here 
(though it doesn?t matter much). Were we right 
to include the Edit component? Carrying out ana-
logous experiments with DMAPLEdit ? vs. 
DMAPL, we found that dropping Edit caused a 
loss of 0.1-0.2 BLEU for all four test sets. Here 
again, we made the right decision.  
In a final experiment, we allowed ?count 1? 
phrases to participate in clustering (using 
DMAPLEdit ? ). The resulting C-E system had 
somewhat more clustered phrases than the pre-
vious one (for both Chinese and English, about 
3.5% of phrases were in clusters compared to 
2.5% in the previous system). To our surprise, 
this led to a slight improvement in BLEU: the 
400K C-E system now yielded 30.25 on NIST06 
(up 0.09) and 23.88 on NIST08 (up 0.13). The F-
E system where ?count 1? clustering is allowed 
also had more phrases in clusters than the system 
where it?s prohibited (the former has just under 
10% of French and English phrases in clusters vs. 
 
Data size 
Nist06 Nist08 
Baseline +phrase-clustering Improv. Baseline +phrase-clustering Improv. 
25K 21.66 21.88 0.22 15.80 15.99 0.19 
50K 23.23 23.43 0.20 17.69 17.84 0.15 
100K 25.83 26.24 0.41 20.08 20.27 0.19 
200K 27.80 28.26 0.46 21.28 21.58 0.30 
400K 29.61 30.16 0.55 23.37 23.75 0.38 
800K 30.87 31.17 0.30 24.41 24.65 0.24 
1.6M 32.94 33.10 0.16 25.61 25.72 0.11 
3.3M 33.59 33.64 0.05 26.84 26.85 0.01 
 
Table 4: BLEU(%) scores for C-E with the various training corpora, including baseline results, results for with 
phrase clustering, and the absolute improvements. Corpus size is measured in sentences. 
 
 
Data size 
Newstest2009 Newstest2010 
Baseline +phrase-clustering Improv. Baseline +phrase-clustering Improv. 
25K 20.21 20.37 0.16 20.54 20.73 0.19 
50K 21.25 21.44 0.19 21.95 22.11 0.16 
100K 22.56 22.86 0.30 23.44 23.69 0.25 
200K 23.67 24.02 0.35 24.31 24.71 0.40 
400K 24.36 24.50 0.14 25.28 25.46 0.18 
800K 24.92 24.97 0.05 25.80 25.90 0.10 
1.6M 25.47 25.47 0.00 26.35 26.37 0.02 
 
Table 5: BLEU(%) scores for F-E with the various training corpora, including baseline results without phrase 
clustering feature, results for phrase clustering, and the absolute improvements. 
 
614
4% for the latter). For F-E, the 200K system al-
lowing ?count 1? clustering again yielded a 
slightly higher BLEU: 24.07 on Newstest2009 
and 24.76 on Newstest2010 (up 0.05 in both cas-
es). Thus, our decision not to allow ?count 1? 
phrases to participate in clustering in the Table 4 
and 5 experiments appears to have been a mis-
take. We suspect we can greatly improve han-
dling of ?count 1? phrases ? e.g., by weighting 
the Edit component of the similarity metric more 
heavily when assigning these phrases to clusters.  
5 Conclusion and Future Work 
We have shown that source-language and target-
language phrases in the phrase table can be clus-
tered, and that these clusters can be used to 
smooth ?forward? and ?backward? estimates 
P(t|s) and P(s|t), yielding modest but consistent 
BLEU gains over a baseline that included lexical 
smoothing. Though our experiments were done 
on a phrase-based system, this method could also 
be applied to hierarchical phrase-based SMT and 
syntactic SMT systems. There are several possi-
bilities for future work based on new applica-
tions for phrase clusters: 
? In the experiments above, we used 
phrase clusters to smooth P(t|s) and P(s|t) 
when the pair (s,t) was observed in train-
ing data. However, the phrase clusters 
often give non-zero probabilities for P(t|s) 
and P(s|t) when s and t were both in the 
training data, but didn?t co-occur. We 
could allow the decoder to consider such 
?invented? phrase pairs (s,t).  
? Phrase clusters could be used to con-
struct target language models (LMs) in 
which the basic unit is a phrase cluster 
rather than a word. For instance, a tri-
cluster model would estimate the proba-
bility of phrase p at time i as a function 
of its phrase cluster, Ci(p), and the two 
preceding phrase clusters Ci-1 and Ci-2: 
)|())(|()( 21 ???= iiii CCCfCfP ppp
.  
? Lexicalized distortion models could be 
modified so as to condition distortion 
events on phrase clusters.  
? We could build SMT grammars in which 
the terminals are phrases and the parents 
of terminals are phrase clusters.  
The phrase clustering algorithm described 
above could be improved in several ways: 
? In the above, the edit distance between 
phrases and between phrase clusters was 
crudely defined. If we improve edit dis-
tance, it will have an especially large 
impact on ?count 1? phrases, for which 
count-based metrics are unreliable and 
which are a large proportion of all phras-
es. The edit distance between two phras-
es weighted all words equally: preferably, 
weights for word substitution, insertion, 
or deletion would be learned from purely 
count-derived phrase clusters (content 
words and negative words might have 
heavier weights than other words). The 
edit distance between two phrase clusters 
was defined as the edit distance between 
the phrases with the most observations in 
each cluster. E.g., distance to the phrase 
cluster in Figure 2 is defined as the 
phrase edit distance to ?emancipating?. 
Instead, one could allow a cluster to be 
characterized by (e.g.) up to three phras-
es, and let distance between two clusters 
be the minimum or average pairwise edit 
distance between these characteristic 
phrases.  
? To cluster phrases, we only used infor-
mation derived from phrase tables. In fu-
ture, we could also use the kind of in-
formation used in work on paraphrases, 
such as the context surrounding phrases 
in monolingual corpora, entries in the-
sauri, and information from pivot lan-
guages. 
? The phrase clustering above was ?hard?: 
each phrase in either language belongs to 
exactly one cluster. We could modify 
our algorithms to carry out ?soft? clus-
tering. For instance, we could interpolate 
the probabilities associated with a phrase 
with probabilities from its neighbours.  
? Clustering is a primitive way of finding 
latent structure in the table of joint 
phrase counts. One could apply principal 
component analysis or a related algo-
rithm to this table. 
References 
C. Bannard and C. Callison-Burch. ?Paraphrasing 
with Bilingual Parallel Corpora?. Proc. ACL, pp. 
597-604, Ann Arbor, USA, June 2005.  
C. Callison-Burch, P. Koehn, and M. Osborne. ?Im-
proved Statistical Machine Translation Using Pa-
raphrases?. Proc. HLT/NAACL, pp. 17-24, New 
York City, USA, June 2006.  
615
C. Callison-Burch. ?Syntactic Constraints on Paraph-
rases Extracted from Parallel Corpora?. Proc. 
EMNLP, pp. 196-205, Honolulu, USA, October 
2008.  
D. Chiang. ?A hierarchical phrase-based model for 
statistical machine translation?. Proc. ACL, pp. 
263-270, Ann Arbor, USA, June 2005.  
G. Foster, R. Kuhn, and H. Johnson. ?Phrasetable 
smoothing for statistical machine translation?. 
Proc. EMNLP, pp. 53-61, Sydney, Australia, July 
2006.  
L. Huang and D. Chiang. ?Forest Rescoring: Faster 
Decoding with Integrated Language Models?. 
Proc. ACL, pp.  144-151, Prague, Czech Republic, 
June 2007.  
P. Koehn. 2010. Statistical Machine Translation. 
Cambridge University Press, Cambridge, UK.  
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 
?Lattice-based Minimum Error Rate Training for 
Statistical Machine Translation?. Proc. EMNLP, 
pp. 725-734, Honolulu, USA, October 2008.  
Y. Marton, C. Callison-Burch, and Philip Resnik. 
?Improved Statistical Machine Translation Using 
Monolingually-Derived Paraphrases?. Proc. 
EMNLP, pp. 381-390, Singapore, August 2009.  
K. Papineni, S. Roukos, T. Ward, and W. Zhu. ?Bleu: 
a method for automatic evaluation of machine 
translation?. Proc. ACL, pp. 311?318, Philadel-
phia, July 2002.  
G. Salton and M. McGill. 1986. Introduction to Mod-
ern Information Retrieval.  McGraw-Hill Inc., New 
York, USA. 
H. Schwenk, M. Costa-juss?, and J. Fonollosa. 
?Smooth Bilingual N-gram Translation?. Proc. 
Joint EMNLP/CoNLL, pp. 430-438, Prague, Czech 
Republic, June 2007.  
R.  Zens and H. Ney. ?Improvements in phrase-based 
statistical machine translation?. Proc. ACL/HLT, 
pp. 257-264, Boston, USA, May 2004. 
S. Zhao, H. Wang, T. Liu, and S. Li. ?Pivot Approach 
for Extracting Paraphrase Patterns from Bilingual 
Corpora?. Proc. ACL/HLT, pp. 780-788, Colum-
bus, USA, June 2008.  
 
616
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451?459,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Crown in Right of Canada.
Discriminative Instance Weighting for Domain Adaptation
in Statistical Machine Translation
George Foster and Cyril Goutte and Roland Kuhn
National Research Council Canada
283 Alexandre-Tache? Blvd
Gatineau, QC J8X 3X7
first.last@nrc.gc.ca
Abstract
We describe a new approach to SMT adapta-
tion that weights out-of-domain phrase pairs
according to their relevance to the target do-
main, determined by both how similar to it
they appear to be, and whether they belong to
general language or not. This extends previ-
ous work on discriminative weighting by us-
ing a finer granularity, focusing on the prop-
erties of instances rather than corpus com-
ponents, and using a simpler training proce-
dure. We incorporate instance weighting into
a mixture-model framework, and find that it
yields consistent improvements over a wide
range of baselines.
1 Introduction
Domain adaptation is a common concern when op-
timizing empirical NLP applications. Even when
there is training data available in the domain of inter-
est, there is often additional data from other domains
that could in principle be used to improve perfor-
mance. Realizing gains in practice can be challeng-
ing, however, particularly when the target domain is
distant from the background data. For developers
of Statistical Machine Translation (SMT) systems,
an additional complication is the heterogeneous na-
ture of SMT components (word-alignment model,
language model, translation model, etc.), which pre-
cludes a single universal approach to adaptation.
In this paper we study the problem of us-
ing a parallel corpus from a background domain
(OUT) to improve performance on a target do-
main (IN) for which a smaller amount of parallel
training material?though adequate for reasonable
performance?is also available. This is a standard
adaptation problem for SMT. It is difficult when IN
and OUT are dissimilar, as they are in the cases we
study. For simplicity, we assume that OUT is ho-
mogeneous. The techniques we develop can be ex-
tended in a relatively straightforward manner to the
more general case when OUT consists of multiple
sub-domains.
There is a fairly large body of work on SMT
adaptation. We introduce several new ideas. First,
we aim to explicitly characterize examples from
OUT as belonging to general language or not. Pre-
vious approaches have tried to find examples that
are similar to the target domain. This is less ef-
fective in our setting, where IN and OUT are dis-
parate. The idea of distinguishing between general
and domain-specific examples is due to Daume? and
Marcu (2006), who used a maximum-entropy model
with latent variables to capture the degree of speci-
ficity. Daume? (2007) applies a related idea in a
simpler way, by splitting features into general and
domain-specific versions. This highly effective ap-
proach is not directly applicable to the multinomial
models used for core SMT components, which have
no natural method for combining split features, so
we rely on an instance-weighting approach (Jiang
and Zhai, 2007) to downweight domain-specific ex-
amples in OUT. Within this framework, we use fea-
tures intended to capture degree of generality, in-
cluding the output from an SVM classifier that uses
the intersection between IN and OUT as positive ex-
amples.
Our second contribution is to apply instance
451
weighting at the level of phrase pairs. Sentence
pairs are the natural instances for SMT, but sen-
tences often contain a mix of domain-specific and
general language. For instance, the sentence Sim-
ilar improvements in haemoglobin levels were re-
ported in the scientific literature for other epoetins
would likely be considered domain-specific despite
the presence of general phrases like were reported
in. Phrase-level granularity distinguishes our work
from previous work by Matsoukas et al(2009), who
weight sentences according to sub-corpus and genre
membership.
Finally, we make some improvements to baseline
approaches. We train linear mixture models for con-
ditional phrase pair probabilities over IN and OUT
so as to maximize the likelihood of an empirical
joint phrase-pair distribution extracted from a de-
velopment set. This is a simple and effective alter-
native to setting weights discriminatively to maxi-
mize a metric such as BLEU. A similar maximum-
likelihood approach was used by Foster and Kuhn
(2007), but for language models only. For compar-
ison to information-retrieval inspired baselines, eg
(Lu? et al, 2007), we select sentences from OUT
using language model perplexities from IN. This
is a straightforward technique that is arguably bet-
ter suited to the adaptation task than the standard
method of treating representative IN sentences as
queries, then pooling the match results.
The paper is structured as follows. Section 2 de-
scribes our baseline techniques for SMT adaptation,
and section 3 describes the instance-weighting ap-
proach. Experiments are presented in section 4. Sec-
tion 5 covers relevant previous work on SMT adap-
tation, and section 6 concludes.
2 Baseline SMT Adaptation Techniques
Standard SMT systems have a hierarchical param-
eter structure: top-level log-linear weights are used
to combine a small set of complex features, inter-
preted as log probabilities, many of which have their
own internal parameters and objectives. The top-
level weights are trained to maximize a metric such
as BLEU on a small development set of approxi-
mately 1000 sentence pairs. Thus, provided at least
this amount of IN data is available?as it is in our
setting?adapting these weights is straightforward.
We focus here instead on adapting the two most im-
portant features: the language model (LM), which
estimates the probability p(w|h) of a target word w
following an ngram h; and the translation models
(TM) p(s|t) and p(t|s), which give the probability
of source phrase s translating to target phrase t, and
vice versa. We do not adapt the alignment procedure
for generating the phrase table from which the TM
distributions are derived.
2.1 Simple Baselines
The natural baseline approach is to concatenate data
from IN and OUT. Its success depends on the two
domains being relatively close, and on the OUT cor-
pus not being so large as to overwhelm the contribu-
tion of IN.
When OUT is large and distinct, its contribution
can be controlled by training separate IN and OUT
models, and weighting their combination. An easy
way to achieve this is to put the domain-specific
LMs and TMs into the top-level log-linear model
and learn optimal weights with MERT (Och, 2003).
This has the potential drawback of increasing the
number of features, which can make MERT less sta-
ble (Foster and Kuhn, 2009).
2.2 Linear Combinations
Apart fromMERT difficulties, a conceptual problem
with log-linear combination is that it multiplies fea-
ture probabilities, essentially forcing different fea-
tures to agree on high-scoring candidates. This is
appropriate in cases where it is sanctioned by Bayes?
law, such as multiplying LM and TM probabilities,
but for adaptation a more suitable framework is of-
ten a mixture model in which each event may be
generated from some domain. This leads to a linear
combination of domain-specific probabilities, with
weights in [0, 1], normalized to sum to 1.
Linear weights are difficult to incorporate into the
standard MERT procedure because they are ?hid-
den? within a top-level probability that represents
the linear combination.1 Following previous work
(Foster and Kuhn, 2007), we circumvent this prob-
lem by choosing weights to optimize corpus log-
likelihood, which is roughly speaking the training
criterion used by the LM and TM themselves.
1This precludes the use of exact line-maximization within
Powell?s algorithm (Och, 2003), for instance.
452
For the LM, adaptive weights are set as follows:
?? = argmax
?
?
w,h
p?(w, h) log
?
i
?ipi(w|h), (1)
where ? is a weight vector containing an element ?i
for each domain (just IN and OUT in our case), pi
are the corresponding domain-specific models, and
p?(w, h) is an empirical distribution from a target-
language training corpus?we used the IN dev set
for this.
It is not immediately obvious how to formulate an
equivalent to equation (1) for an adapted TM, be-
cause there is no well-defined objective for learning
TMs from parallel corpora. This has led previous
workers to adopt ad hoc linear weighting schemes
(Finch and Sumita, 2008; Foster and Kuhn, 2007;
Lu? et al, 2007). However, we note that the final con-
ditional estimates p(s|t) from a given phrase table
maximize the likelihood of joint empirical phrase
pair counts over a word-aligned corpus. This sug-
gests a direct parallel to (1):
?? = argmax
?
?
s,t
p?(s, t) log
?
i
?ipi(s|t), (2)
where p?(s, t) is a joint empirical distribution ex-
tracted from the IN dev set using the standard pro-
cedure.2
An alternative form of linear combination is a
maximum a posteriori (MAP) combination (Bacchi-
ani et al, 2004). For the TM, this is:
p(s|t) = cI(s, t) + ? po(s|t)
cI(t) + ?
, (3)
where cI(s, t) is the count in the IN phrase table of
pair (s, t), po(s|t) is its probability under the OUT
TM, and cI(t) =
?
s? cI(s?, t). This is motivated by
taking ? po(s|t) to be the parameters of a Dirich-
let prior on phrase probabilities, then maximizing
posterior estimates p(s|t) given the IN corpus. In-
tuitively, it places more weight on OUT when less
evidence from IN is available. To set ?, we used the
same criterion as for ?, over a dev corpus:
?? = argmax
?
?
s,t
p?(s, t) log cI(s, t) + ? po(s|t)
cI(t) + ?
.
2Using non-adapted IBM models trained on all available IN
and OUT data.
TheMAP combination was used for TM probabil-
ities only, in part due to a technical difficulty in for-
mulating coherent counts when using standard LM
smoothing techniques (Kneser and Ney, 1995).3
2.3 Sentence Selection
Motivated by information retrieval, a number of
approaches choose ?relevant? sentence pairs from
OUT by matching individual source sentences from
IN (Hildebrand et al, 2005; Lu? et al, 2007), or
individual target hypotheses (Zhao et al, 2004).
The matching sentence pairs are then added to the
IN corpus, and the system is re-trained. Although
matching is done at the sentence level, this informa-
tion is subsequently discarded when all matches are
pooled.
To approximate these baselines, we implemented
a very simple sentence selection algorithm in which
parallel sentence pairs from OUT are ranked by the
perplexity of their target half according to the IN lan-
guage model. The number of top-ranked pairs to re-
tain is chosen to optimize dev-set BLEU score.
3 Instance Weighting
The sentence-selection approach is crude in that it
imposes a binary distinction between useful and
non-useful parts of OUT. Matsoukas et al(2009)
generalize it by learning weights on sentence pairs
that are used when estimating relative-frequency
phrase-pair probabilities. The weight on each sen-
tence is a value in [0, 1] computed by a perceptron
with Boolean features that indicate collection and
genre membership.
We extend the Matsoukas et alapproach in sev-
eral ways. First, we learn weights on individual
phrase pairs rather than sentences. Intuitively, as
suggested by the example in the introduction, this
is the right granularity to capture domain effects.
Second, rather than relying on a division of the cor-
pus into manually-assigned portions, we use features
intended to capture the usefulness of each phrase
pair. Finally, we incorporate the instance-weighting
model into a general linear combination, and learn
weights and mixing parameters simultaneously.
3Bacchiani et al(2004) solve this problem by reconstitut-
ing joint counts from smoothed conditional estimates and un-
smoothed marginals, but this seems somewhat unsatisfactory.
453
3.1 Model
The overall adapted TM is a combination of the
form:
p(s|t) = ?t pI(s|t) + (1? ?t) po(s|t), (4)
where pI(s|t) is derived from the IN corpus us-
ing relative-frequency estimates, and po(s|t) is an
instance-weighted model derived from the OUT cor-
pus. This combination generalizes (2) and (3): we
use either ?t = ? to obtain a fixed-weight linear
combination, or ?t = cI(t)/(cI(t) + ?) to obtain a
MAP combination.
We model po(s|t) using a MAP criterion over
weighted phrase-pair counts:
po(s|t) =
c?(s, t) + ?u(s|t)?
s? c?(s?, t) + ?
(5)
where c?(s, t) is a modified count for pair (s, t)
in OUT, u(s|t) is a prior distribution, and ? is a
prior weight. The original OUT counts co(s, t) are
weighted by a logistic function w?(s, t):
c?(s, t) = co(s, t) w?(s, t) (6)
= co(s, t) [1 + exp(?
?
i
?ifi(s, t))]?1,
where each fi(s, t) is a feature intended to charac-
terize the usefulness of (s, t), weighted by ?i.
The mixing parameters and feature weights (col-
lectively ?) are optimized simultaneously using dev-
set maximum likelihood as before:
?? = argmax
?
?
s,t
p?(s, t) log p(s|t;?). (7)
This is a somewhat less direct objective than used
by Matsoukas et al who make an iterative approxi-
mation to expected TER. However, it is robust, effi-
cient, and easy to implement.4
To perform the maximization in (7), we used
the popular L-BFGS algorithm (Liu and Nocedal,
1989), which requires gradient information. Drop-
ping the conditioning on ? for brevity, and let-
ting c??(s, t) = c?(s, t) + ?u(s|t), and c??(t) =
4Note that the probabilities in (7) need only be evaluated
over the support of p?(s, t), which is quite small when this dis-
tribution is derived from a dev set. Maximizing (7) is thus much
faster than a typical MERT run.
?
s? c??(s?, t):
? log p(s|t)
??t
= kt
[
pI(s|t)
p(s|t) ?
po(s|t)
p(s|t)
]
? log p(s|t)
??
= 1? ?t
p(s|t)
[
u(s|t)
c??(t)
?
c??(s, t)
c??(t)2
]
? log p(s|t)
??i
= 1? ?t
p(s|t)
[
c??i(s, t)
c??(t)
?
c??(s, t)c??i(t)
c??(t)2
]
where:
kt =
{
1 fixed weight
?cI(t)/(cI(t) + ?)2 MAP
c??i(s, t) = fi(s, t)(1? w?(s, t))c?(s, t)
and:
c??i(t) =
?
s?
c??i(s
?, t).
3.2 Interpretation and Variants
To motivate weighting joint OUT counts as in (6),
we begin with the ?ideal? objective for setting
multinomial phrase probabilities ? = {p(s|t),?st},
which is the likelihood with respect to the true IN
distribution pI?(s, t). Jiang and Zhai (2007) sug-
gest the following derivation, making use of the true
OUT distribution po?(s, t):
?? = argmax
?
?
s,t
pI?(s, t) log p?(s|t) (8)
= argmax
?
?
s,t
pI?(s, t)
po?(s, t)
po?(s, t) log p?(s|t)
? argmax
?
?
s,t
pI?(s, t)
po?(s, t)
co(s, t) log p?(s|t),
where co(s, t) are the counts from OUT, as in (6).
This has solutions:
p??(s|t) =
pI?(s, t)
po?(s, t)
co(s, t)/
?
s?
pI?(s?, t)
po?(s?, t)
co(s?, t),
and from the similarity to (5), assuming ? = 0, we
see that w?(s, t) can be interpreted as approximat-
ing pI?(s, t)/po?(s, t). The logistic function, whose
outputs are in [0, 1], forces pI?(s, t) ? po?(s, t). This
is not unreasonable given the application to phrase
pairs fromOUT, but it suggests that an interesting al-
ternative might be to use a plain log-linear weighting
454
function exp(?i ?ifi(s, t)), with outputs in [0,?].
We have not yet tried this.
An alternate approximation to (8) would be to let
w?(s, t) directly approximate pI?(s, t). With the ad-
ditional assumption that (s, t) can be restricted to the
support of co(s, t), this is equivalent to a ?flat? alter-
native to (6) in which each non-zero co(s, t) is set to
one. This variant is tested in the experiments below.
A final alternate approach would be to combine
weighted joint frequencies rather than conditional
estimates, ie: cI(s, t) + w?(s, t)co(, s, t), suitably
normalized.5 Such an approach could be simulated
by a MAP-style combination in which separate ?(t)
values were maintained for each t. This would make
the model more powerful, but at the cost of having
to learn to downweight OUT separately for each t,
which we suspect would require more training data
for reliable performance. We have not explored this
strategy.
3.3 Simple Features
We used 22 features for the logistic weighting
model, divided into two groups: one intended to re-
flect the degree to which a phrase pair belongs to
general language, and one intended to capture simi-
larity to the IN domain.
The 14 general-language features embody
straightforward cues: frequency, ?centrality? as
reflected in model scores, and lack of burstiness.
They are:
? total number of tokens in the phrase pair (1);
? OUT corpus frequency (1);
? OUT-corpus frequencies of rarest source and
target words (2);
? perplexities for OUT IBM1 models, in both di-
rections (2);
? average and minimum source and target word
?document frequencies? in the OUT corpus,
using successive 100-line pseudo-documents6
(4); and
5We are grateful to an anonymous reviewer for pointing this
out.
6One of our experimental settings lacks document bound-
aries, and we used this approximation in both settings for con-
sistency.
? average and minimum source and target word
values from the OUT corpus of the following
statistic, intended to reflect degree of burstiness
(higher values indicate less bursty behaviour):
g/(L ? L/(l + 1) + (), where g is the sum
over all sentences containing the word of the
distance (number of sentences) to the nearest
sentence that also contains the word, L is the
total number of sentences, l is the number of
sentences that contain the word, and ( is a small
constant (4).
The 8 similarity-to-IN features are based on word
frequencies and scores from various models trained
on the IN corpus:
? 1gram and 2gram source and target perplexities
according to the IN LM (4);7
? source and target OOV counts with respect to
IN (2); and
? perplexities for IN IBM1 models, in both direc-
tions (2).
To avoid numerical problems, each feature was
normalized by subtracting its mean and dividing by
its standard deviation.
3.4 SVM Feature
In addition to using the simple features directly, we
also trained an SVM classifier with these features
to distinguish between IN and OUT phrase pairs.
Phrase tables were extracted from the IN and OUT
training corpora (not the dev as was used for instance
weighting models), and phrase pairs in the intersec-
tion of the IN and OUT phrase tables were used as
positive examples, with two alternate definitions of
negative examples:
1. Pairs from OUT that are not in IN, but whose
source phrase is.
2. Pairs from OUT that are not in IN, but whose
source phrase is, and where the intersection of
IN and OUT translations for that source phrase
is empty.
7In the case of the Chinese experiments below, source LMs
were trained using text segmented with the LDC segmenter, as
were the other Chinese models in our system.
455
The classifier trained using the 2nd definition had
higher accuracy on a development set. We used it to
score all phrase pairs in the OUT table, in order to
provide a feature for the instance-weighting model.
4 Experiments
4.1 Corpora and System
We carried out translation experiments in two dif-
ferent settings. The first setting uses the Euro-
pean Medicines Agency (EMEA) corpus (Tiede-
mann, 2009) as IN, and the Europarl (EP) cor-
pus (www.statmt.org/europarl) as OUT,
for English/French translation in both directions.
The dev and test sets were randomly chosen from
the EMEA corpus. Figure 1 shows sample sentences
from these domains, which are widely divergent.
The second setting uses the news-related sub-
corpora for the NIST09 MT Chinese to English
evaluation8 as IN, and the remaining NIST paral-
lel Chinese/English corpora (UN, Hong Kong Laws,
and Hong Kong Hansard) as OUT. The dev cor-
pus was taken from the NIST05 evaluation set, aug-
mented with some randomly-selected material re-
served from the training set. The NIST06 and
NIST08 evaluation sets were used for testing. (Thus
the domain of the dev and test corpora matches IN.)
Compared to the EMEA/EP setting, the two do-
mains in the NIST setting are less homogeneous and
more similar to each other; there is also considerably
more IN text available.
The corpora for both settings are summarized in
table 1.
corpus sentence pairs
Europarl 1,328,360
EMEA train 11,770
EMEA dev 1,533
EMEA test 1,522
NIST OUT 6,677,729
NIST IN train 2,103,827
NIST IN dev 1,894
NIST06 test 1,664
NIST08 test 1,357
Table 1: Corpora
8www.itl.nist.gov/iad/mig//tests/mt/2009
The reference medicine for Silapo is
EPREX/ERYPO, which contains epoetin alfa.
Le me?dicament de re?fe?rence de Silapo est
EPREX/ERYPO, qui contient de l?e?poe?tine alfa.
?
I would also like to point out to commissioner Liika-
nen that it is not easy to take a matter to a national
court.
Je voudrais pre?ciser, a` l?adresse du commissaire
Liikanen, qu?il n?est pas aise? de recourir aux tri-
bunaux nationaux.
Figure 1: Sentence pairs from EMEA (top) and Europarl
text.
We used a standard one-pass phrase-based sys-
tem (Koehn et al, 2003), with the following fea-
tures: relative-frequency TM probabilities in both
directions; a 4-gram LM with Kneser-Ney smooth-
ing; word-displacement distortion model; and word
count. Feature weights were set using Och?s MERT
algorithm (Och, 2003). The corpus was word-
aligned using both HMM and IBM2 models, and the
phrase table was the union of phrases extracted from
these separate alignments, with a length limit of 7.
It was filtered to retain the top 30 translations for
each source phrase using the TM part of the current
log-linear model.
4.2 Results
Table 2 shows results for both settings and all meth-
ods described in sections 2 and 3. The 1st block
contains the simple baselines from section 2.1. The
natural baseline (baseline) outperforms the pure IN
system only for EMEA/EP fren. Log-linear combi-
nation (loglin) improves on this in all cases, and also
beats the pure IN system.
The 2nd block contains the IR system, which was
tuned by selecting text in multiples of the size of the
EMEA training corpus, according to dev set perfor-
mance. This significantly underperforms log-linear
combination.
The 3rd block contains the mixture baselines. The
linear LM (lin lm), TM (lin tm) and MAP TM (map
tm) used with non-adapted counterparts perform in
all cases slightly worse than the log-linear combi-
nation, which adapts both LM and TM components.
However, when the linear LM is combined with a
456
method EMEA/EP NIST
fren enfr nst06 nst08
in 32.77 31.98 27.65 21.65
out 20.42 17.41 19.85 15.71
baseline 33.61 31.15 26.93 21.01
loglin 35.94 32.62 28.09 21.85
ir 33.75 31.91 ?? ??
lin lm 35.61 31.55 28.02 21.68
lin tm 35.32 32.52 27.16 21.32
map tm 35.15 31.99 27.20 21.17
lm+lin tm 36.42 33.49 27.83 22.03
lm+map tm 36.28 33.31 28.05 22.11
iw all 36.55 33.73 28.74 22.28
iw all map 37.01 33.90 30.04 23.76
iw all flat 36.50 33.42 28.31 22.13
iw gen map 36.98 33.75 29.81 23.56
iw sim map 36.82 33.68 29.66 23.53
iw svm map 36.79 33.67 ?? ??
Table 2: Results, for EMEA/EP translation into English
(fren) and French (enfr); and for NIST Chinese to En-
glish translation with NIST06 and NIST08 evaluation
sets. Numbers are BLEU scores.
linear TM (lm+lin tm) or MAP TM (lm+map TM),
the results are much better than a log-linear com-
bination for the EMEA setting, and on a par for
NIST. This is consistent with the nature of these two
settings: log-linear combination, which effectively
takes the intersection of IN and OUT, does relatively
better on NIST, where the domains are broader and
closer together. Somewhat surprisingly, there do not
appear to be large systematic differences between
linear and MAP combinations.
The 4th block contains instance-weighting mod-
els trained on all features, used within a MAP TM
combination, and with a linear LM mixture. The
iw all map variant uses a non-0 ? weight on a uni-
form prior in po(s|t), and outperforms a version
with ? = 0 (iw all) and the ?flattened? variant de-
scribed in section 3.2. Clearly, retaining the origi-
nal frequencies is important for good performance,
and globally smoothing the final weighted frequen-
cies is crucial. This best instance-weighting model
beats the equivalant model without instance weights
by between 0.6 BLEU and 1.8 BLEU, and beats the
log-linear baseline by a large margin.
The final block in table 2 shows models trained
on feature subsets and on the SVM feature described
in 3.4. The general-language features have a slight
advantage over the similarity features, and both are
better than the SVM feature.
5 Related Work
We have already mentioned the closely related work
by Matsoukas et al(2009) on discriminative cor-
pus weighting, and Jiang and Zhai (2007) on (non-
discriminative) instance weighting. It is difficult to
directly compare the Matsoukas et alresults with
ours, since our out-of-domain corpus is homoge-
neous; given heterogeneous training data, however,
it would be trivial to include Matsoukas-style iden-
tity features in our instance-weighting model. Al-
though these authors report better gains than ours,
they are with respect to a non-adapted baseline. Fi-
nally, we note that Jiang?s instance-weighting frame-
work is broader than we have presented above, en-
compassing among other possibilities the use of un-
labelled IN data, which is applicable to SMT settings
where source-only IN corpora are available.
It is also worth pointing out a connection with
Daume??s (2007) work that splits each feature into
domain-specific and general copies. At first glance,
this seems only peripherally related to our work,
since the specific/general distinction is made for fea-
tures rather than instances. However, for multino-
mial models like our LMs and TMs, there is a one to
one correspondence between instances and features,
eg the correspondence between a phrase pair (s, t)
and its conditional multinomial probability p(s|t).
As mentioned above, it is not obvious how to ap-
ply Daume??s approach to multinomials, which do
not have a mechanism for combining split features.
Recent work by Finkel and Manning (2009) which
re-casts Daume??s approach in a hierarchical MAP
framework may be applicable to this problem.
Moving beyond directly related work, major
themes in SMT adaptation include the IR (Hilde-
brand et al, 2005; Lu? et al, 2007; Zhao et al,
2004) and mixture (Finch and Sumita, 2008; Fos-
ter and Kuhn, 2007; Koehn and Schroeder, 2007; Lu?
et al, 2007) approaches for LMs and TMs described
above, as well as methods for exploiting monolin-
gual in-domain text, typically by translating it auto-
matically and then performing self training (Bertoldi
457
and Federico, 2009; Ueffing et al, 2007; Schwenk
and Senellart, 2009). There has also been some
work on adapting the word alignment model prior to
phrase extraction (Civera and Juan, 2007; Wu et al,
2005), and on dynamically choosing a dev set (Xu
et al, 2007). Other work includes transferring latent
topic distributions from source to target language for
LM adaptation, (Tam et al, 2007) and adapting fea-
tures at the sentence level to different categories of
sentence (Finch and Sumita, 2008).
6 Conclusion
In this paper we have proposed an approach for
instance-weighting phrase pairs in an out-of-domain
corpus in order to improve in-domain performance.
Each out-of-domain phrase pair is characterized by
a set of simple features intended to reflect how use-
ful it will be. The features are weighted within a
logistic model to give an overall weight that is ap-
plied to the phrase pair?s frequency prior to making
MAP-smoothed relative-frequency estimates (dif-
ferent weights are learned for each conditioning
direction). These estimates are in turn combined
linearly with relative-frequency estimates from an
in-domain phrase table. Mixing, smoothing, and
instance-feature weights are learned at the same time
using an efficient maximum-likelihood procedure
that relies on only a small in-domain development
corpus.
We obtained positive results using a very sim-
ple phrase-based system in two different adaptation
settings: using English/French Europarl to improve
a performance on a small, specialized medical do-
main; and using non-news portions of the NIST09
training material to improve performance on the
news-related corpora. In both cases, the instance-
weighting approach improved over a wide range of
baselines, giving gains of over 2 BLEU points over
the best non-adapted baseline, and gains of between
0.6 and 1.8 over an equivalent mixture model (with
an identical training procedure but without instance
weighting).
In future work we plan to try this approach with
more competitive SMT systems, and to extend in-
stance weighting to other standard SMT components
such as the LM, lexical phrase weights, and lexical-
ized distortion. We will also directly compare with
a baseline similar to the Matsoukas et alapproach in
order to measure the benefit from weighting phrase
pairs (or ngrams) rather than full sentences. Finally,
we intend to explore more sophisticated instance-
weighting features for capturing the degree of gen-
erality of phrase pairs.
References
ACL. 2007. Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL),
Prague, Czech Republic, June.
Michel Bacchiani, Brian Roark, and Murat Saraclar.
2004. Language model adaptation with MAP esti-
mation and the perceptron algorithm. In NAACL04
(NAA, 2004).
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In WMT09 (WMT, 2009).
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in Statistical Machine Translation with mixture mod-
elling. In WMT07 (WMT, 2007).
Hal Daume? III and Daniel Marcu. 2006. Domain Adap-
tation for Statistical Classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In ACL-07 (ACL, 2007).
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine translation.
In Proceedings of the ACL Workshop on Statistical
Machine Translation, Columbus, June. WMT.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian domain adaptation. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL), Boulder, June.
NAACL.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In WMT07 (WMT, 2007).
George Foster and Roland Kuhn. 2009. Stabilizing min-
imum error rate training. In WMT09 (WMT, 2009).
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th EAMT
Conference, Budapest, May.
Jing Jiang and ChengXiang Zhai. 2007. Instance
Weighting for Domain Adaptation in NLP. In ACL-
07 (ACL, 2007).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
458
Speech, and Signal Processing (ICASSP) 1995, pages
181?184, Detroit, Michigan. IEEE.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL), pages 127?133,
Edmonton, May. NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. In Proceedings
of the 2007 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Prague, Czech
Republic.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
NAACL. 2004. Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Boston, May.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Sapporo, July. ACL.
Holger Schwenk and Jean Senellart. 2009. Translation
model adaptation for an arabic/french news translation
system by lightly-supervised training. In Proceedings
of MT Summit XII, Ottawa, Canada, September. Inter-
national Association for Machine Translation.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-LSA Based LM Adaptation for Spoken Lan-
guage Translation. In ACL-07 (ACL, 2007).
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In ACL-07 (ACL, 2007).
WMT. 2007. Proceedings of the ACL Workshop on Sta-
tistical Machine Translation, Prague, June.
WMT. 2009. Proceedings of the 4th Workshop on Statis-
tical Machine Translation, Athens, March.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005.
Alignment model adaptation for domain-specific word
alignment. In Proceedings of the 43th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Ann Arbor, Michigan, July. ACL.
Jia Xu, Yonggang Deng, Yuqing Gao, and Hermann Ney.
2007. Domain dependent statistical machine transla-
tion. In MT Summit XI, Copenhagen, September.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING) 2004, Geneva, August.
459
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427?436,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Batch Tuning Strategies for Statistical Machine Translation
Colin Cherry and George Foster
National Research Council Canada
{Colin.Cherry,George.Foster}@nrc-cnrc.gc.ca
Abstract
There has been a proliferation of recent work
on SMT tuning algorithms capable of han-
dling larger feature sets than the traditional
MERT approach. We analyze a number of
these algorithms in terms of their sentence-
level loss functions, which motivates several
new approaches, including a Structured SVM.
We perform empirical comparisons of eight
different tuning strategies, including MERT,
in a variety of settings. Among other results,
we find that a simple and efficient batch ver-
sion of MIRA performs at least as well as
training online, and consistently outperforms
other options.
1 Introduction
The availability of linear models and discriminative
tuning algorithms has been a huge boon to statis-
tical machine translation (SMT), allowing the field
to move beyond the constraints of generative noisy
channels (Och and Ney, 2002). The ability to opti-
mize these models according to an error metric has
become a standard assumption in SMT, due to the
wide-spread adoption ofMinimum Error Rate Train-
ing or MERT (Och, 2003). However, MERT has
trouble scaling to more than 30 features, which has
led to a surge in research on tuning schemes that can
handle high-dimensional feature spaces.
These methods fall into a number of broad cate-
gories. Minimum risk approaches (Och, 2003; Smith
and Eisner, 2006) have been quietly capable of han-
dling many features for some time, but have yet to
see widespread adoption. Online methods (Liang
et al, 2006; Watanabe et al, 2007), are recognized
to be effective, but require substantial implementa-
tion efforts due to difficulties with parallelization.
Pairwise ranking (Shen et al, 2004; Hopkins and
May, 2011) recasts tuning as classification, and can
be very easy to implement, as it fits nicely into the
established MERT infrastructure.
The MERT algorithm optimizes linear weights
relative to a collection of k-best lists or lattices,
which provide an approximation to the true search
space. This optimization is wrapped in an outer
loop that iterates between optimizing weights and
re-decoding with those weights to enhance the ap-
proximation. Our primary contribution is to empiri-
cally compare eight tuning algorithms and variants,
focusing on methods that work within MERT?s es-
tablished outer loop. This is the first comparison to
include all three categories of optimizer.
Furthermore, we introduce three tuners that have
not been previously tested. In particular, we
test variants of Chiang et al?s (2008) hope-fear
MIRA that use k-best or lattice-approximated search
spaces, producing a Batch MIRA that outperforms
a popular mechanism for parallelizing online learn-
ers. We also investigate the direct optimization of
hinge loss on k-best lists, through the use of a Struc-
tured SVM (Tsochantaridis et al, 2004). We review
and organize the existing tuning literature, provid-
ing sentence-level loss functions for minimum risk,
online and pairwise training. Finally, since random-
ization plays a different role in each tuner, we also
suggest a new method for testing an optimizer?s sta-
bility (Clark et al, 2011), which sub-samples the
tuning set instead of varying a random seed.
2 Background
We begin by establishing some notation. We view
our training set as a list of triples [f,R, E ]ni=1, where
f is a source-language sentence, R is a set of target-
language reference sentences, and E is the set of
427
all reachable hypotheses; that is, each e ? Ei is a
target-language derivation that can be decoded from
fi. The function ~hi(e) describes e?s relationship to
its source fi using features that decompose into the
decoder. A linear model ~w scores derivations ac-
cording to their features, meaning that the decoder
solves:
ei(~w) = argmax
e?Ei
~w ? ~hi(e) (1)
Assuming we wish to optimize our decoder?s BLEU
score (Papineni et al, 2002), the natural objec-
tive of learning would be to find a ~w such that
BLEU([e(~w), R]n1 ) is maximal. In most machine
learning papers, this would be the point where we
would say, ?unfortunately, this objective is unfeasi-
ble.? But in SMT, we have been happily optimizing
exactly this objective for years using MERT.
However, it is now acknowledged that the MERT
approach is not feasible for more than 30 or so fea-
tures. This is due to two main factors:
1. MERT?s parameter search slows and becomes
less effective as the number of features rises,
stopping it from finding good training scores.
2. BLEU is a scale invariant objective: one can
scale ~w by any positive constant and receive the
same BLEU score.1 This causes MERT to re-
sist standard mechanisms of regularization that
aim to keep ||~w|| small.
The problems with MERT can be addressed
through the use of surrogate loss functions. In this
paper, we focus on linear losses that decompose over
training examples. Using Ri and Ei, each loss `i(~w)
indicates how poorly ~w performs on the ith training
example. This requires a sentence-level approxima-
tion of BLEU, which we re-encode into a cost ?i(e)
on derivations, where a high cost indicates that e re-
ceives a low BLEU score. Unless otherwise stated,
we will assume the use of sentence BLEU with add-
1 smoothing (Lin and Och, 2004). The learners dif-
fer in their definition of ` and ?, and in how they
employ their loss functions to tune their weights.
1This is true of any evaluation metric that considers only the
ranking of hypotheses and not their model scores; ie, it is true
of all common MT metrics.
2.1 Margin Infused Relaxed Algorithm
First employed in SMT by Watanabe et al (2007),
and refined by Chiang et al (2008; 2009), the Mar-
gin Infused Relaxed Algorithm (MIRA) employs a
structured hinge loss:
`i(~w) = max
e?Ei
[
?i(e) + ~w ?
(
~hi(e) ? ~hi(e
?
i )
)]
(2)
where e?i is an oracle derivation, and cost is de-
fined as ?i(e) = BLEUi(e?i ) ? BLEUi(e), so that
?i(e?i ) = 0. The loss `i(~w) is 0 only if ~w separates
each e ? Ei from e?i by a margin proportional to their
BLEU differentials.
MIRA is an instance of online learning, repeating
the following steps: visit an example i, decode ac-
cording to ~w, and update ~w to reduce `i(~w). Each
update makes the smallest change to ~w (subject to a
step-size cap C) that will separate the oracle from a
number of negative hypotheses. The work of Cram-
mer et al (2006) shows that updating away from a
single ?fear? hypothesis that maximizes (2) admits
a closed-form update that performs well. Let e?i be
the e ? Ei that maximizes `i(~w); the update can be
performed in two steps:
?t = min
[
C, `i(~wt)
||~hi(e?i )?
~hi(e?i)||
2
]
~wt+1 = ~wt + ?t
(~hi(e?i ) ? ~hi(e
?
i)
)
(3)
To improve generalization, the average of all
weights seen during learning is used on unseen data.
Chiang et al (2008) take advantage of MIRA?s
online nature to modify each update to better suit
SMT. The cost ?i is defined using a pseudo-
corpus BLEU that tracks the n-gram statistics of
the model-best derivations from the last few up-
dates. This modified cost matches corpus BLEU
better than add-1 smoothing, but it also makes ?i
time-dependent: each update for an example i will
be in the context of a different pseudo-corpus. The
oracle e?i also shifts with each update to ~w, as it
is defined as a ?hope? derivation, which maximizes
~w ? ~hi(e) + BLEUi(e). Hope updating ensures that
MIRA aims for ambitious, reachable derivations.
In our implementation, we make a number of
small, empirically verified deviations from Chiang
et al (2008). These include the above-mentioned
use of a single hope and fear hypothesis, and the use
428
of hope hypotheses (as opposed to model-best hy-
potheses) to build the pseudo-corpus for calculating
BLEUi. These changes were observed to be neu-
tral with respect to translation quality, but resulted in
faster running time and simplified implementation.
2.2 Direct Optimization
With the exception of MIRA, the tuning approaches
discussed in this paper are direct optimizers. That is,
each solves the following optimization problem:
~w? = argmin
~w
?
2
||~w||2 +
?
i
`i(~w) (4)
where the first term provides regularization,
weighted by ?. Throughout this paper, (4) is
optimized with respect to a fixed approximation
of the decoder?s true search space, represented as
a collection of k-best lists. The various methods
differ in their definition of loss and in how they
optimize their objective.
Without the complications added by hope decod-
ing and a time-dependent cost function, unmodified
MIRA can be shown to be carrying out dual coordi-
nate descent for an SVM training objective (Martins
et al, 2010). However, exactly what objective hope-
fear MIRA is optimizing remains an open question.
Gimpel and Smith (2012) discuss these issues in
greater detail, while also providing an interpretable
alternative to MIRA.
2.3 Pairwise Ranking Optimization
Introduced by Hopkins and May (2011), Pairwise
Ranking Optimization (PRO) aims to handle large
feature sets inside the traditional MERT architec-
ture. That is, PRO employs a growing approxima-
tion of Ei by aggregating the k-best hypotheses from
a series of increasingly refined models. This archi-
tecture is desirable, as most groups have infrastruc-
ture to k-best decode their tuning sets in parallel.
For a given approximate E?i, PRO creates a sam-
ple Si of (eg, eb) pairs, such that BLEUi(eg) >
BLEUi(eb). It then uses a binary classifier to sep-
arate each pair. We describe the resulting loss in
terms of an SVM classifier, to highlight similarities
with MIRA. In terms of (4), PRO defines
`i(~w) =
?
(eg ,eb)?Si
2
(
1 + ~w ?
(~hi(eb) ? ~hi(eg)
))+
where (x)+ = max(0, x). The hinge loss is multi-
plied by 2 to account for PRO?s use of two examples
(positive and negative) for each sampled pair. This
sum of hinge-losses is 0 only if each pair is separated
by a model score of 1. Given [S]ni=1, this convex
objective can be optimized using any binary SVM.2
Unlike MIRA, the margin here is fixed to 1; cost en-
ters into PRO through its sampling routine, which
performs a large uniform sample and then selects a
subset of pairs with large BLEU differentials.
The PRO loss uses a sum over pairs in place of
MIRA?s max, which allows PRO to bypass oracle
selection, and to optimize with off-the-shelf classi-
fiers. This sum is potentially a weakness, as PRO
receives credit for each correctly ordered pair in its
sample, and these pairs are not equally relevant to
the final BLEU score.
2.4 Minimum Risk Training
Minimum risk training (MR) interprets ~w as a prob-
abilistic model, and optimizes expected BLEU. We
focus on expected sentence costs (Och, 2003; Zens
et al, 2007; Li and Eisner, 2009), as this risk is sim-
ple to optimize and fits nicely into our mathemati-
cal framework. Variants that use the expected suffi-
cient statistics of BLEU also exist (Smith and Eisner,
2006; Pauls et al, 2009; Rosti et al, 2011).
We again assume a MERT-like tuning architec-
ture. Let ?i(e) = ?BLEUi(e) and let
`i(~w) = EP~w [?i(e)] =
?
e?E?i
[
exp(~w ? ~hi(e))?i(e)
]
?
e??E?i
exp(~w ? ~hi(e?))
This expected cost becomes increasingly small as
greater probability mass is placed on derivations
with high BLEU scores. This smooth, non-convex
objective can be solved to a local minimum using
gradient-based optimizers; we have found stochastic
gradient descent to be quite effective (Bottou, 2010).
Like PRO, MR requires no oracle derivation, and
fits nicely into the established MERT architecture.
The expectations needed to calculate the gradient
EP~w
[
~hi(e)?i(e)
]
? EP~w [?i(e)]EP~w
[
~hi(e)
]
2Hopkins and May (2011) advocate a maximum-entropy
version of PRO, which is what we evaluate in our empirical
comparison. It can be obtained using a logit loss `i(~w) =
P
g,b 2 log
?
1 + exp
?
~w ?
`~hi(eb) ? ~hi(eg)
???
.
429
are trivial to extract from a k-best list of derivations.
Each downward step along this gradient moves the
model toward likely derivations, and away from
likely derivations that incur high costs.
3 Novel Methods
We have reviewed three tuning methods, all of which
address MERT?s weakness with large features by us-
ing surrogate loss functions. Additionally, MIRA
has the following advantages over PRO and MR:
1. Loss is optimized using the true Ei, as opposed
to an approximate search space E?i.
2. Sentence BLEU is calculated with a fluid
pseudo-corpus, instead of add-1 smoothing.
Both of these advantages come at a cost: oper-
ating on the true Ei sacrifices easy parallelization,
while using a fluid pseudo-corpus creates an unsta-
ble learning objective. We develop two large-margin
tuners that explore these trade-offs.
3.1 Batch MIRA
Online training makes it possible to learn with the
decoder in the loop, forgoing the need to approxi-
mate the search space, but it is not necessarily con-
venient to do so. Online algorithms are notoriously
difficult to parallelize, as they assume each example
is visited in sequence. Parallelization is important
for efficient SMT tuning, as decoding is still rela-
tively expensive.
The parallel online updates suggested by Chi-
ang et al (2008) involve substantial inter-process
communication, which may not be easily supported
by all clusters. McDonald et al (2010) suggest
a simpler distributed strategy that is amenable to
map-reduce-like frameworks, which interleaves on-
line training on shards with weight averaging across
shards. This strategy has been adopted by Moses
(Hasler et al, 2011), and it is the one we adopt in
our MIRA implementation.
However, online training using the decoder may
not be necessary for good performance. The success
of MERT, PRO and MR indicates that their shared
search approximation is actually quite reasonable.
Therefore, we propose Batch MIRA, which sits ex-
actly where MERT sits in the standard tuning archi-
tecture, greatly simplifying parallelization:
1. Parallel Decode: [E? ?]n1 = k-best([f, E ]
n
1 , ~w)
2. Aggregate: [E? ]n1 = [E? ]
n
1 ? [E?
?]n1
3. Train: ~w = BatchMIRA([f,R, E? ]n1 , ~w)
4. Repeat
where BatchMIRA() trains the SMT-adapted MIRA
algorithm to completion on the current approxima-
tion E? , without parallelization.3 The only change we
make to MIRA is to replace the hope-fear decoding
of sentences with the hope-fear re-ranking of k-best
lists. Despite its lack of parallelization, each call to
BatchMIRA() is extremely fast, as SMT tuning sets
are small enough to load all of [E? ]n1 into memory. We
test two Batch MIRA variants, which differ in their
representation of E? . Pseudo-code that covers both is
provided in Algorithm 1. Note that if we set E? = E ,
Algorithm 1 also describes online MIRA.
Batch k-best MIRA inherits all of the MERT archi-
tecture. It is very easy to implement; the hope-fear
decoding steps can by carried out by simply evaluat-
ing BLEU score and model score for each hypothe-
sis in the k-best list.
Batch Lattice MIRA replaces k-best decoding in
step 1 with decoding to lattices. To enable loading
all of the lattices into memory at once, we prune to
a density of 50 edges per reference word. The hope-
fear decoding step requires the same oracle lattice
decoding algorithms as online MIRA (Chiang et al,
2008). The lattice aggregation in the outer loop can
be kept reasonable by aggregating only those paths
corresponding to hope or fear derivations.
3.2 Structured SVM
While MIRA takes a series of local hinge-loss re-
ducing steps, it is also possible to directly minimize
the sum of hinge-losses using a batch algorithm, cre-
ating a structured SVM (Tsochantaridis et al, 2004).
To avoid fixing an oracle before optimization begins,
we adapt Yu and Joachim?s (2009) latent SVM to
our task, which allows the oracle derivation for each
sentence to vary during training. Again we assume a
MERT-like architecture, which approximates E with
an E? constructed from aggregated k-best lists.
Inspired by the local oracle of Liang et al (2006),
we define E?i? to be an oracle set:
E?i? = {e|BLEUi(e) is maximal}.
3In our implementation, BatchMIRA() trains for J = 30
passes over [E? ]n1 .
430
Algorithm 1 BatchMIRA
input [f,R, E? ]n1 , ~w, max epochs J , step cap C,
and pseudo-corpus decay ?.
init Pseudo-corpus BG to small positive counts.
init t = 1; ~wt = ~w
for j from 1 to J do
for i from 1 to n in random order do
// Hope-fear decode in E?i
e?t = argmaxe?E?i
[
~wt ? ~hi(e) + BLEUi(e)
]
e?t = argmaxe?E?i
[
~wt ? ~hi(e) ? BLEUi(e)
]
// Update weights
?t = BLEUi(e?t ) ? BLEUi(e
?
t)
?t = min
[
C,
?t+~wt?
(
~hi(e?t)?~hi(e
?
t )
)
||~hi(e?t )?
~hi(e?t)||
2
]
~wt+1 = ~wt + ?t
(~hi(e?t ) ? ~hi(e
?
i)
)
// Update statistics
BG = ?BG+ BLEU stats for e?t and Ri
t = t + 1
end for
~wavgj =
1
nj
?nj
t?=1 ~wt?
end for
return ~wavgj that maximizes training BLEU
Cost is also defined in terms of the maximal BLEU,
?i(e) = max
e??E?i
[
BLEUi(e
?)
]
? BLEUi(e).
Finally, loss is defined as:
`i(~w) = maxe?E?i
[
?i(e) + ~w ? ~hi(e)
? maxe?i ?E?i?
(
~w ? ~hi(e?i )
)]
This loss is 0 only if some hypothesis in the oracle
set is separated from all others by a margin propor-
tional to their BLEUi differentials.
With loss defined in this manner, we can mini-
mize (4) to local minimum by using an alternating
training procedure. For each example i, we select
a fixed e?i ? E?i? that maximizes model score; that
is, ~w is used to break ties in BLEU for oracle selec-
tion. With the oracle fixed, the objective becomes
a standard structured SVM objective, which can be
minimized using a cutting-plane algorithm, as de-
scribed by Tsochantaridis et al (2004). After doing
so, we can drive the loss lower still by iterating this
process: re-select each oracle (breaking ties with the
new ~w), then re-optimize ~w. We do so 10 times. We
were surprised by the impact of these additional iter-
ations on the final loss; for some sentences, E?i? can
be quite large.
Despite the fact that both algorithms use a struc-
tured hinge loss, there are several differences be-
tween our SVM and MIRA. The SVM has an ex-
plicit regularization term ? that is factored into its
global objective, while MIRA regularizes implicitly
by taking small steps. The SVM requires a stable
objective to optimize, meaning that it must forgo the
pseudo-corpus used by MIRA to calculate ?i; in-
stead, the SVM uses an interpolated sentence-level
BLEU (Liang et al, 2006).4 Finally, MIRA?s oracle
is selected with hope decoding. With a sufficiently
large ~w, any e ? E? can potentially become the ora-
cle. In contrast, the SVM?s local oracle is selected
from a small set E??, which was done to more closely
match the assumptions of the Latent SVM.
To solve the necessary quadratic programming
sub-problems, we use a multiclass SVM similar to
LIBLINEAR (Hsieh et al, 2008). Like Batch MIRA
and PRO, the actual optimization is very fast, as the
cutting plane converges quickly and all of [E? ]n1 can
be loaded into memory at once.
3.3 Qualitative Summary
We have reviewed three tuning methods and intro-
duced three tuning methods. All six methods em-
ploy sentence-level loss functions, which in turn em-
ploy sentence-level BLEU approximations. Except
for online MIRA, all methods plug nicely into the
existing MERT architecture. These methods can be
split into two groups: MIRA variants (online, batch
k-best, batch lattice), and direct optimizers (PRO,
MR and SVM). The MIRA variants use pseudo-
corpus BLEU in place of smoothed BLEU, and
provide access to richer hypothesis spaces through
the use of online training or lattices.5 The direct
optimizers have access to a tunable regularization
parameter ?, and do not require special purpose
code for hope and fear lattice decoding. Batch
4SVM training with interpolated BLEU outperformed add-1
BLEU in preliminary testing. A comparison of different BLEU
approximations under different tuning objectives would be an
interesting path for future work.
5MR approaches that use lattices (Li and Eisner, 2009;
Pauls et al, 2009; Rosti et al, 2011) or the complete search
space (Arun et al, 2010) exist, but are not tested here.
431
k-best MIRA straddles the two groups, benefiting
from pseudo-corpus BLEU and easy implementa-
tion, while being restricted to a k-best list.
4 Experimental Design
We evaluated the six tuning strategies described
in this paper, along with two MERT baselines,
on three language pairs
(
French-English (Fr-En),
English-French (En-Fr) and Chinese-English (Zh-
En)
)
, across three different feature-set sizes. Each
setting was run five times over randomized variants
to improve reliability. To cope with the resulting
large number of configurations, we ran all experi-
ments using an efficient phrase-based decoder simi-
lar to Moses (Koehn et al, 2007).
All tuning methods that use an approximate E? per-
form 15 iterations of the outer loop and return the
weights that achieve the best development BLEU
score. When present, ? was coarsely tuned (trying 3
values differing by magnitudes of 10) in our large-
feature Chinese-English setting.
? kb-mert : k-best MERT with 20 random
restarts. All k-best methods use k = 100.
? lb-mert : Lattice MERT (Machery et al, 2008)
using unpruned lattices and aggregating only
those paths on the line search?s upper envelope.
? mira : Online MIRA (?2.1). All MIRA vari-
ants use a pseudo-corpus decay ? = 0.999 and
C = 0.01. Online parallelization follows Mc-
Donald et al (2010), using 8 shards. We tested
20, 15, 10, 8 and 5 shards during development.
? lb-mira : Batch Lattice MIRA (?3.1).
? kb-mira : Batch k-best MIRA (?3.1).
? pro : PRO (?2.3) follows Hopkins and May
(2011); however, we were unable to find set-
tings that performed well in general. Reported
results use MegaM6 with a maximum of 30 it-
erations (as is done in Moses; the early stop-
ping provides a form of regularization) for our
six English/French tests, and MegaM with 100
iterations and a reduced initial uniform sam-
ple (50 pairs instead of 5000) for our three En-
glish/Chinese tests.
? mr : MR as described in ?2.4. We employ a
learning rate of ?0/(1 + ?0?t) for stochastic
6Available at www.cs.utah.edu/?hal/megam/
corpus sentences words (en) words (fr)
train 2,928,759 60,482,232 68,578,459
dev 2,002 40,094 44,603
test1 2,148 42,503 48,064
test2 2,166 44,701 49,986
Table 1: Hansard Corpus (English/French)
corpus sentences words (zh) words (en)
train1 6,677,729 200,706,469 213,175,586
train2 3,378,230 69,232,098 66,510,420
dev 1,506 38,233 40,260
nist04 1,788 53,439 59,944
nist06 1,664 41,782 46,140
nist08 1,357 35,369 42,039
Table 2: NIST09 Corpus (Chinese-English). Train1 cor-
responds to the UN and Hong Kong sub-corpora; train2
to all others.
gradient descent, with ?0 tuned to optimize the
training loss achieved after one epoch (Bottou,
2010). Upon reaching a local optimum, we re-
shuffle our data, re-tune our learning rate, and
re-start from the optimum, repeating this pro-
cess 5 times. We do not sharpen our distribu-
tion with a temperature or otherwise control for
entropy; instead, we trust ? = 50 to maintain a
reasonable distribution.
? svm : Structured SVM (?3.2) with ? = 1000.
4.1 Data
Systems for English/French were trained on Cana-
dian Hansard data (years 2001?2009) summarized
in table 1.7 The dev and test sets were chosen
randomly from among the most recent 5 days of
Hansard transcripts.
The system for Zh-En was trained on data from
the NIST 2009 Chinese MT evaluation, summarized
in table 2. The dev set was taken from the NIST
05 evaluation set, augmented with some material re-
served from other NIST corpora. The NIST 04, 06,
and 08 evaluation sets were used for testing.
4.2 SMT Features
For all language pairs, phrases were extracted with
a length limit of 7 from separate word alignments
7This corpus will be distributed on request.
432
template max fren enfr zhen
tgt unal 50 50 50 31
count bin 11 11 11 11
word pair 6724 1298 1291 1664
length bin 63 63 63 63
total 6848 1422 1415 1769
Table 3: Sparse feature templates used in Big.
performed by IBM2 and HMM models and sym-
metrized using diag-and (Koehn et al, 2003). Con-
ditional phrase probabilities in both directions were
estimated from relative frequencies, and from lexical
probabilities (Zens and Ney, 2004). Language mod-
els were estimated with Kneser-Ney smoothing us-
ing SRILM. Six-feature lexicalized distortion mod-
els were estimated and applied as in Moses.
For each language pair, we defined roughly equiv-
alent systems (exactly equivalent for En-Fr and Fr-
En, which are mirror images) for each of three
nested feature sets: Small, Medium, and Big.
The Small set defines a minimal 7-feature sys-
tem intended to be within easy reach of all tuning
strategies. It comprises 4 TM features, one LM, and
length and distortion features. For the Chinese sys-
tem, the LM is a 5-gram trained on the NIST09 Gi-
gaword corpus; for English/French, it is a 4-gram
trained on the target half of the parallel Hansard.
The Medium set is a more competitive 18-feature
system. It adds 4 TM features, one LM, and 6 lex-
icalized distortion features. For Zh-En, Small?s TM
(trained on both train1 and train2 in table 2) is re-
placed by 2 separate TMs from these sub-corpora;
for En/Fr, the extra TM (4 features) comes from a
forced-decoding alignment of the training corpus, as
proposed by Wuebker et al (2010). For Zh-En, the
extra LM is a 4-gram trained on the target half of the
parallel corpus; for En/Fr, it is a 4-gram trained on
5m sentences of similar parliamentary data.
The Big set adds sparse Boolean features to
Medium, for a maximum of 6,848 features. We used
sparse feature templates that are equivalent to the
PBMT set described in (Hopkins and May, 2011):
tgt unal picks out each of the 50 most frequent tar-
get words to appear unaligned in the phrase table;
count bin uniquely bins joint phrase pair counts with
upper bounds 1,2,4,8,16,32,64,128,1k,10k,?; word
pair fires when each of the 80 most frequent words
in each language appear aligned 1-1 to each other, to
some other word, or not 1-1; and length bin captures
each possible phrase length and length pair. Table 3
summarizes the feature templates, showing the max-
imum number of features each can generate, and the
number of features that received non-zero weights in
the final model tuned by MR for each language pair.
Feature weights are initialized to 1.0 for each of
the TM, LM and distortion penalty features. All
other weights are initialized to 0.0.
4.3 Stability Testing
We follow Clark et al(2011), and perform multiple
randomized replications of each experiment. How-
ever, their method of using different random seeds
is not applicable in our context, since randomization
does not play the same role for all tuning methods.
Our solution was to randomly draw and fix four dif-
ferent sub-samples of each dev set, retaining each
sentence with a probability of 0.9. For each tuning
method and setting, we then optimize on the origi-
nal dev and all sub-samples. The resulting standard
deviations provide an indication of stability.
5 Results
The results of our survey of tuning methods can be
seen in Tables 4, 5 and 6. Results are averaged over
test sets (2 for Fr/En, 3 for Zh/En), and over 5 sub-
sampled runs per test set. The SD column reports the
standard deviation of the average test score across
the 5 sub-samples.
It may be dismaying to see only small score
improvements when transitioning from Medium to
Big. This is partially due to the fact that our Big fea-
ture set affects only phrase-table scores. Our phrase
tables are already strong, through our use of large
data or leave-one-out forced decoding. The impor-
tant baseline when assessing the utility of a method
is Medium k-best MERT. In all language pairs, our
Big systems generally outperform this baseline by
0.4 BLEU points. It is interesting to note that most
methods achieve the bulk of this improvement on the
Medium feature set.8 This indicates that MERT be-
gins to show some problems even in an 18-feature
8One can see the same phenomenon in the results of Hop-
kins and May (2011) as well.
433
Table 4: French to English Translation (Fr-En)
Small Medium Big
Tune Test SD Tune Test SD Tune Test SD
kb-mert 40.50 39.94 0.04 40.75 40.29 0.13 n/a n/a n/a
lb-mert 40.52 39.93 0.11 40.93 40.39 0.08 n/a n/a n/a
mira 40.38 39.94 0.04 40.64 40.59 0.06 41.02 40.74 0.05
kb-mira 40.46 39.97 0.05 40.92 40.64 0.12 41.46 40.75 0.08
lb-mira 40.44 39.98 0.06 40.94 40.65 0.06 41.59 40.78 0.09
pro 40.11 40.05 0.05 40.16 40.07 0.08 40.55 40.21 0.24
mr 40.24 39.88 0.05 40.70 40.57 0.14 41.18 40.60 0.08
svm 40.05 40.20 0.03 40.60 40.56 0.08 41.32 40.52 0.07
Table 5: English to French Translation (En-Fr)
Small Medium Big
Tune Test SD Tune Test SD Tune Test SD
kb-mert 40.47 39.72 0.06 40.70 40.02 0.11 n/a n/a n/a
lb-mert 40.45 39.76 0.08 40.90 40.13 0.10 n/a n/a n/a
mira 40.36 39.83 0.03 40.78 40.44 0.02 40.89 40.45 0.05
kb-mira 40.44 39.83 0.02 40.94 40.35 0.06 41.48 40.52 0.06
lb-mira 40.45 39.83 0.02 41.05 40.45 0.04 41.65 40.59 0.07
pro 40.17 39.57 0.15 40.30 40.01 0.04 40.75 40.22 0.17
mr 40.31 39.65 0.04 40.94 40.30 0.13 41.45 40.47 0.10
svm 39.99 39.55 0.03 40.40 39.96 0.05 41.00 40.21 0.03
Table 6: Chinese to English Translation (Zh-En)
Small Medium Big
Tune Test SD Tune Test SD Tune Test SD
kb-mert 23.97 29.65 0.06 25.74 31.58 0.42 n/a n/a n/a
lb-mert 24.18 29.48 0.15 26.42 32.39 0.22 n/a n/a n/a
mira 23.98 29.54 0.01 26.23 32.58 0.08 25.99 32.52 0.08
kb-mira 24.10 29.51 0.06 26.28 32.50 0.12 26.18 32.61 0.14
lb-mira 24.13 29.59 0.05 26.43 32.77 0.06 26.40 32.82 0.18
pro 23.25 28.74 0.24 25.80 32.42 0.20 26.49 32.18 0.40
mr 23.87 29.55 0.09 26.26 32.52 0.12 26.42 32.79 0.15
svm 23.59 28.91 0.05 26.26 32.70 0.05 27.23 33.04 0.12
40
41
42
43
44
45
lb-mirasvmmr
40
40.2
40.4
40.6
40.8
lb-mira svmmr
Tune Test
Figure 1: French-English test of regularization with an over-fitting feature set. lb-mira varies C ={1, 1e-1, 1e-2, 1e-3}, its default
C is 1e-2; svm varies ? ={1e2, 1e3, 1e4, 1e5}, its default ? is 1e3; mr varies ? ={5, 5e1, 5e2, 5e3}, its default ? is 5e1.
434
setting, which can be mitigated through the use of
Lattice MERT.
When examining score differentials, recall that
the reported scores average over multiple test sets
and sub-sampled tuning runs. Using Small features,
all of the tested methods are mostly indistinguish-
able, but as we move to Medium and Big, Batch
Lattice MIRA emerges as our method of choice. It
is the top scoring system in all Medium settings,
and in two of three Big settings (in Big Zh-En, the
SVM comes first, with batch lattice MIRA placing
second). However, all of the MIRA variants per-
form similarly, though our implementation of on-
line MIRA is an order of magnitude slower, mostly
due to its small number of shards. It is interest-
ing that our batch lattice variant consistently outper-
forms online MIRA. We attribute this to our paral-
lelization strategy, Chiang et al?s (2008) more com-
plex solution may perform better.
There may be settings where an explicit regular-
ization parameter is desirable, thus we also make a
recommendation among the direct optimizers (PRO,
MR and SVM). Though these systems all tend to
show a fair amount of variance across language and
feature sets (likely due to their use sentence-level
BLEU), MR performs the most consistently, and is
always within 0.2 of batch lattice MIRA.
The SVM?s performance on Big Zh-En is an in-
triguing outlier in our results. Note that it not only
performs best on the test set, but also achieves the
best tuning score by a large margin. We suspect
we have simply found a setting where interpolated
BLEU and our choice of ? work particularly well.
We intend to investigate this case to see if this level
of success can be replicated consistently, perhaps
through improved sentence BLEU approximation or
improved oracle selection.
5.1 Impact of Regularization
One main difference between MIRA and the direct
optimizers is the availability of an explicit regular-
ization term ?. To measure the impact of this param-
eter, we designed a feature set explicitly for over-
fitting. This set uses our Big Fr-En features, with the
count bin template modified to distinguish each joint
count observed in the tuning set. These new fea-
tures, which expand the set to 20k+ features, should
generalize poorly.
We tested MR and SVM on our Fr-En data us-
ing this feature set, varying their respective regular-
ization parameters by factors of 10. We compared
this to Batch Lattice MIRA?s step-size cap C, which
controls its regularization (Martins et al, 2010). The
results are shown in Figure 1. Looking at the tuning
scores, one can see that ? affords much greater con-
trol over tuning performance than MIRA?s C. Look-
ing at test scores, MIRA?s narrow band of regular-
ization appears to be just about right; however, there
is no reason to expect this to always be the case.
6 Conclusion
We have presented three new, large-margin tuning
methods for SMT that can handle thousands of fea-
tures. Batch lattice and k-best MIRA carry out their
online training within approximated search spaces,
reducing costs in terms of both implementation and
training time. The Structured SVM optimizes a sum
of hinge losses directly, exposing an explicit reg-
ularization term. We have organized the literature
on tuning, and carried out an extensive comparison
of linear-loss SMT tuners. Our experiments show
Batch Lattice MIRA to be the most consistent of the
tested methods. In the future, we intend to inves-
tigate improved sentence-BLEU approximations to
help narrow the gap between MIRA and the direct
optimizers.
Acknowledgements
Thanks to Mark Hopkins, Zhifei Li and Jonathan
May for their advice while implementing the meth-
ods in this review, and to Kevin Gimpel, Roland
Kuhn and the anonymous reviewers for their valu-
able comments on an earlier draft.
References
Abhishek Arun, Barry Haddow, and Philipp Koehn.
2010. A unified approach to minimum risk training
and decoding. In Proceedings of the Joint Workshop
on Statistical Machine Translation and MetricsMATR,
pages 365?374.
Leon Bottou. 2010. Large-scale machine learning with
stochastic gradient descent. In International Confer-
ence on Computational Statistics, pages 177?187.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, pages 224?233.
435
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In HLT-NAACL, pages 218?226.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL, pages 176?181.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
HLT-NAACL, Montreal, Canada, June.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2011.
Margin infused relaxed algorithm for moses. The
Prague Bulletin of Mathematical Linguistics, 96:69?
78.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In EMNLP, pages 1352?1362.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In ICML.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL,
pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180, Prague, Czech Republic, June.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40?51.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In COLING-ACL,
pages 761?768.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In COLING, pages 501?507.
WolfgangMachery, Franz Josef Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725?734.
Andre? F. T. Martins, Kevin Gimpel, Noah A. Smith,
Eric P. Xing, Pedro M. Q. Aguiar, and Ma?rio A. T.
Figueiredo. 2010. Learning structured classifiers with
dual coordinate descent. Technical Report CMU-ML-
10-109, Carnegie Mellon University.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In ACL, pages 456?464.
Franz Joseph Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302,
Philadelphia, PA, July.
Franz Joseph Och. 2003. Minimum error rate training
for statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
Adam Pauls, John Denero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In EMNLP, pages 1418?1427.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected bleu training for
graphs: BBN system description for WMT11 system
combination task. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 159?
165.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
HLT-NAACL, pages 177?184, Boston, Massachusetts,
May 2 - May 7.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In COLING-
ACL, pages 787?794.
Ioannis Tsochantaridis, Thomas Hofman, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In ICML, pages 823?830.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764?773.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
ICML.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In HLT-
NAACL, pages 257?264, Boston, USA, May.
Richard Zens, Sa?sa Hasan, and Hermann Ney. 2007. A
systematic comparison of training criteria for statisti-
cal machine translation. In EMNLP, pages 524?532.
436
Proceedings of NAACL-HLT 2013, pages 938?946,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Adaptation of Reordering Models for Statistical Machine Translation
Boxing Chen, George Foster and Roland Kuhn
National Research Council Canada
first.last@nrc-cnrc.gc.ca
Abstract
Previous research on domain adaptation (DA)
for statistical machine translation (SMT) has
mainly focused on the translation model (TM)
and the language model (LM). To the best of
our knowledge, there is no previous work on
reordering model (RM) adaptation for phrase-
based SMT. In this paper, we demonstrate
that mixture model adaptation of a lexical-
ized RM can significantly improve SMT per-
formance, even when the system already con-
tains a domain-adapted TM and LM. We find
that, surprisingly, different training corpora
can vary widely in their reordering character-
istics for particular phrase pairs. Furthermore,
particular training corpora may be highly suit-
able for training the TM or the LM, but unsuit-
able for training the RM, or vice versa, so mix-
ture weights for these models should be esti-
mated separately. An additional contribution
of the paper is to propose two improvements
to mixture model adaptation: smoothing the
in-domain sample, and weighting instances
by document frequency. Applied to mixture
RMs in our experiments, these techniques (es-
pecially smoothing) yield significant perfor-
mance improvements.
1 Introduction
A phrase-based statistical machine translation
(SMT) system typically has three main components:
a translation model (TM) that contains information
about how to translate word sequences (phrases)
from the source language to the target language,
a language model (LM) that contains information
about probable word sequences in the target lan-
guage, and a reordering model (RM) that indicates
how the order of words in the source sentence is
likely to influence the order of words in the target
sentence. The TM and the RM are trained on parallel
data, and the LM is trained on target-language data.
Usage of language and therefore the best translation
practice differs widely across genres, topics, and di-
alects, and even depends on a particular author?s or
publication?s style; the word ?domain? is often used
to indicate a particular combination of all these fac-
tors. Unless there is a perfect match between the
training data domain and the (test) domain in which
the SMT system will be used, one can often get bet-
ter performance by adapting the system to the test
domain.
In offline domain adaptation, the system is pro-
vided with a sample of translated sentences from
the test domain prior to deployment. In a popular
variant of offline adaptation, linear mixture model
adaptation, each training corpus is used to gener-
ate a separate model component that forms part of
a linear combination, and the sample is used to as-
sign a weight to each component (Foster and Kuhn,
2007). If the sample resembles some of the corpora
more than others, those corpora will receive higher
weights in the combination.
Previous research on domain adaptation for SMT
has focused on the TM and the LM. Such research
is easily motivated: translations across domains are
unreliable. For example, the Chinese translation
of the English word ?mouse? would most likely be
?laoshu ??? if the topic is the animal; if the topic
is computer hardware, its translation would most
938
likely be ?shubiao???. However, when the trans-
lation is for people in Taiwan, even when the topic
is computer hardware, its translation would more
likely be ?huashu ???. It is intuitively obvious
why TM and LM adaptation would be helpful here.
By contrast, it is not at all obvious that RM model
adaptation will improve SMT performace. One
would expect reordering behaviour to be characteris-
tic of a particular language pair, but not of particular
domains. At most, one might think that reordering
is lexicalized?perhaps, (for instance) in translating
from Chinese to English, or from Arabic to English,
there are certain words whose English translations
tend to undergo long-distance movement from their
original positions, while others stay close to their
original positions. However, one would not expect
a particular Chinese adverb or a particular Arabic
noun to undergo long-distance movement when be-
ing translated into English in one domain, but not in
others. Nevertheless, that is what we observe: see
section 5 below.
This paper shows that RM adaptation improves
the performance of our phrase-based SMT system.
In our implementation, the RM is adapted by means
of a linear mixture model, but it is likely that other
forms of RM adaptation would also work. We ob-
tain even more effective RM adaptation by smooth-
ing the in-domain sample and by weighting orienta-
tion counts by the document frequency of the phrase
pair. Both improvements could be applied to the TM
or the LM as well, though we have not done so.
Finally, the paper analyzes reordering to see why
RM adaptation works. There seem to be two fac-
tors at work. First, the reordering behaviour of
words and phrases often differs dramatically from
one bilingual corpus to another. Second, there are
corpora (for instance, comparable corpora and bilin-
gual lexicons) which may contain very valuable in-
formation for the TM, but which are poor sources
of RM information; RM adaptation downweights in-
formation from these corpora significantly, and thus
improves the overall quality of the RM.
2 Reordering Model
In early SMT systems, such as (Koehn, 2004),
changes in word order when a sentence is trans-
lated were modeled by means of a penalty that is in-
curred when the decoder chooses, as the next source
phrase to be translated, a phrase that does not imme-
diately follow the previously translated source sen-
tence. Thus, the system penalizes deviations from
monotone order, with the magnitude of the penalty
being proportional to distance in the source sentence
between the end of the previously translated source
phrase and the start of the newly chosen source
phrase.
Many SMT systems, including our own, still use
this distance-based penalty as a feature. However,
starting with (Tillmann and Zhang, 2005; Koehn
et al, 2005), a more sophisticated type of reorder-
ing model has often been adopted as well, and has
yielded consistent performance gains. This type of
RM typically identifies three possible orientations
for a newly chosen source phrase: monotone (M),
swap (S), and discontinuous (D). The M orientation
occurs when the newly chosen phrase is immedi-
ately to the right of the previously translated phrase
in the source sentence, the S orientation occurs when
the new phrase is immediately to the left of the pre-
vious phrase, and the D orientation covers all other
cases.1 This type of RM is lexicalized: the estimated
probabilities of M, S and D depend on the source-
language and target-language words in both the pre-
vious phrase pair and the newly chosen one.
Galley and Manning (2008) proposed a ?hierar-
chical? lexicalized RM in which the orientation (M,
S, or D) is determined not by individual phrase pairs,
but by blocks. A block is the largest contiguous se-
quence of phrase pairs that satisfies the phrase pair
consistency requirement of having no external links.
Thus, classification of the orientation of a newly
chosen phrase as M, S, or D is carried out as if the
decoder always chose the longest possible source
phrase in the past, and will choose the longest pos-
sible source phrase in the future.
The RM used in this paper is hierarchical and lex-
icalized. For a given phrase pair (f , e), we estimate
the probabilities that it will be in an M, S, or D ori-
entation o with respect to the previous phrase pair
and the following phrase pair (two separate distri-
butions). Orientation counts c(o, f, e) are obtained
from a word-aligned corpus using the method de-
1Some researchers have distinguished between left and right
versions of the D orientation, but this 4-orientation scheme has
not yielded significant gains over the 3-orientation one.
939
scribed in (Cherry et al, 2012), and corresponding
probabilities p(o|f, e) are estimated using recursive
MAP smoothing:
p(o|f, e) =
c(o, f, e) + ?f p(o|f) + ?e p(o|e)
c(f, e) + ?f + ?e
p(o|f) =
c(o, f) + ?g p(o)
c(f) + ?g
p(o) =
c(o) + ?u/3
c(?) + ?u
, (1)
where p(o|e) is defined analogously to p(o|f), and
the four smoothing parameters ?e, ?f , ?g, and ?u
are set to values that minimize the perplexity of the
resulting model on held-out data.
During decoding, orientations with respect to the
previous context are obtained from a shift-reduce
parser, and orientations with respect to following
context are approximated using the coverage vector
(Cherry et al, 2012).
3 RM Adaptation
3.1 Linear mixture model
Following previous work (Foster and Kuhn, 2007;
Foster et al, 2010), we adopt the linear mixture
model technique for RM adaptation. This technique
trains separate models for each training corpus, then
learns weights for each of the models and combines
the weighted component models into a single model.
If we have N sub-corpora, the global reordering
model probabilities p(o|f, e) are computed as in (2):
p(o|f, e) =
N?
i=1
?i pi(o|f, e) (2)
where pi(o|f, e) is the reordering model trained on
sub-corpus i, and ?i is its weight.
Following (Foster et al, 2010), we use the EM
algorithm to learn the weights that maximize the
probability of phrase-pair orientations in the devel-
opment set (in-domain data):
?? = argmax
?
?
o,f,e
p?(o, f, e) log
N?
i=1
?i pi(o|f, e)
(3)
where p?(o, f, e) is the empirical distribution of
counts in the dev set (proportional to c(o, f, e)). Two
separate sets of mixing weights are learned: one for
the distribution with respect to the previous phrase
pair, and one for the next phrase pair.
3.2 Development set smoothing
In Equation 3, p?(o, f, e) is extracted from the in-
domain development set. Since dev sets for SMT
systems are typically small (1,000-3,000 sentences),
we apply smoothing to this RM. We first obtain
a smoothed conditional distribution p(o|f, e) using
the MAP technique described above, then multiply
by the empirical marginal p?(e, f) to obtain a final
smoothed joint distribution p(o, f, e).
There is nothing about this idea that limits it to
the RM: smoothing could be applied to the statistics
in the dev that are used to estimate a mixture TM
or LM, in order to mitigate over-fitting. However,
we note that, compared to the TM, the over-fitting
problem is likely to be more acute for the RM, since
it splits counts for each phrase pair into three cate-
gories.
3.3 Document-frequency weighting
Mixture models, like the RM in this paper, depend
on the existence of multiple training corpora, with
each sub-corpus nominally representing a domain.
A recent paper suggests that some phrase pairs be-
long to general language, while others are domain-
specific (Foster et al, 2010). If a phrase pair exists
in all training corpora, it probably belongs to general
language; on the other hand, if it appears in only
one or two training corpora, it is more likely to be
domain-specific.
We were interested in seeing whether information
about domain-specificity could improve the estima-
tion of mixture RM weights. The intuition is that
phrase pairs that belong to general language should
contribute more to determining sub-corpus weights,
since they are the ones whose reordering behaviour
is most likely to shift with domain. To capture this
intuition, we multiplied the empirical distribution in
(3) by the following factor, inspired by the standard
document-frequency formula:
D(f, e) = log(DF (f, e) +K), (4)
where DF (f, e) is the number of sub-corpora
that (f, e) appears in, and K is an empirically-
determined smoothing term.
940
corpus # segs # en tok % genres
fbis 250K 10.5M 3.7 nw
financial 90K 2.5M 0.9 financial
gale bc 79K 1.3M 0.5 bc
gale bn 75K 1.8M 0.6 bn ng
gale nw 25K 696K 0.2 nw
gale wl 24K 596K 0.2 wl
hkh 1.3M 39.5M 14.0 Hansard
hkl 400K 9.3M 3.3 legal
hkn 702K 16.6M 5.9 nw
isi 558K 18.0M 6.4 nw
lex&ne 1.3M 2.0M 0.7 lexicon
others nw 146K 5.2M 1.8 nw
sinorama 282K 10.0M 3.5 nw
un 5.0M 164M 58.2 un
TOTAL 10.1M 283M 100.0 (all)
devtest
tune 1,506 161K nw wl
NIST06 1,664 189K nw bn ng
NIST08 1,357 164K nw wl
Table 1: NIST Chinese-English data. In the gen-
res column: nw=newswire, bc=broadcast conversa-
tion, bn=broadcast news, wl=weblog, ng=newsgroup,
un=United Nations proceedings.
4 Experiments
4.1 Data setting
We carried out experiments in two different settings,
both involving data from NIST Open MT 2012.2
The first setting uses data from the Chinese to En-
glish constrained track, comprising 283M English
tokens. We manually identified 14 sub-corpora on
the basis of genres and origins. Table 1 summarizes
the statistics and genres of all the training corpora
and the development and test sets; for the training
corpora, we show their size in number of words as
a percentage of all training data. Most training cor-
pora consist of parallel sentence pairs. The isi and
lex&ne corpora are exceptions: the former is ex-
tracted from comparable data, while the latter is a
lexicon that includes many named entities. The de-
velopment set (tune) was taken from the NIST 2005
evaluation set, augmented with some web-genre ma-
terial reserved from other NIST corpora.
2http://www.nist.gov/itl/iad/mig/openmt12.cfm
corpus # segs # en toks % genres
gale bc 57K 1.6M 3.3 bc
gale bn 45K 1.2M 2.5 bn
gale ng 21K 491K 1.0 ng
gale nw 17K 659K 1.4 nw
gale wl 24K 590K 1.2 wl
isi 1,124K 34.7M 72.6 nw
other nw 224K 8.7M 18.2 nw
TOTAL 1,512K 47.8M 100.0 (all)
devtest
NIST06 1,664 202K nw wl
NIST08 1,360 205K nw wl
NIST09 1,313 187K nw wl
Table 2: NIST Arabic-English data. In the gen-
res column: nw=newswire, bc=broadcast conversation,
bn=broadcase news, ng=newsgroup, wl=weblog.
The second setting uses NIST 2012 Arabic to En-
glish data, but excluding the UN data. There are
about 47.8 million English running words in these
training data. We manually grouped the training data
into 7 groups according to genre and origin. Ta-
ble 2 summarizes the statistics and genres of all the
training corpora and the development and test sets.
Note that for this language pair, the comparable isi
data represent a large proportion of the training data:
72% of the English words. We use the evaluation
sets from NIST 2006, 2008, and 2009 as our devel-
opment set and two test sets, respectively.
4.2 System
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et al,
2007). The corpus was word-aligned using IBM2,
HMM, and IBM4 models, and the phrase table was
the union of phrase pairs extracted from these sepa-
rate alignments, with a length limit of 7. The trans-
lation model was smoothed in both directions with
KN smoothing (Chen et al, 2011). The DF smooth-
ing term K in equation 4 was set to 0.1 using held-
out optimization. We use the hierarchical lexical-
ized RM described above, with a distortion limit of
7. Other features include lexical weighting in both
directions, word count, a distance-based RM, a 4-
gram LM trained on the target side of the parallel
data, and a 6-gram English Gigaword LM. The sys-
941
system Chinese Arabic
baseline 31.7 46.8
baseline+loglin 29.6 45.9
RMA 31.8 47.7**
RMA+DF 32.2* 47.9**
RMA+dev smoothing 32.3* 48.3**
RMA+dev smoothing+DF 32.8** 48.2**
Table 3: Results for variants of RM adaptation.
system Chinese Arabic
LM+TM adaptation 33.2 47.7
+RMA+dev-smoothing+DF 33.5 48.4**
Table 4: RM adaptation improves over a baseline con-
taining adapted LMs and TMs.
tem was tuned with batch lattice MIRA (Cherry and
Foster, 2012).
4.3 Results
For our main baseline, we simply concatenate all
training data. We also tried augmenting this with
separate log-linear features corresponding to sub-
corpus-specific RMs. Our metric is case-insensitvie
IBM BLEU-4 (Papineni et al, 2002); we report
BLEU scores averaged across both test sets. Follow-
ing (Koehn, 2004), we use the bootstrap-resampling
test to do significance testing. In tables 3 to 5, *
and ** denote significant gains over the baseline at
p < 0.05 and p < 0.01 levels, respectively.
Table 3 shows that reordering model adaptation
helps in both data settings. Adding either document-
frequency weighting (equation 4) or dev-set smooth-
ing makes the improvement significant in both set-
tings. Using both techniques together yields highly
significant improvements.
Our second experiment measures the improve-
ment from RM adaptation over a baseline that
includes adapted LMs and TMs. We use the
same technique?linear mixtures with EM-tuned
weights?to adapt these models. Table 4 shows that
adapting the RM gives gains over this strong base-
line for both language pairs; improvements are sig-
nificant in the case of Arabic to English.
The third experiment breaks down the gains in the
last line of table 4 by individual adapted model. As
shown in table 5, RM adaptation yielded the largest
system Chinese Arabic
baseline 31.7 46.8
LM adaptation 32.1* 47.0
TM adaptation 33.0** 47.5**
RM adaptation 32.8** 48.2**
Table 5: Comparison of LM, TM, and RM adaptation.
improvement on Arabic, while TM adaptation did
best on Chinese. Surprisingly, both methods sig-
nificantly outperformed LM adaptation, which only
achieved significant gains over the baseline for Chi-
nese.
5 Analysis
Why does RM adaptation work? Intuitively, one
would think that reordering behaviour for a given
phrase pair should not be much affected by domain,
making RM adaptation pointless. That is probably
why (as far as we know) no-one has tried it before.
In this section, we describe three factors that account
for at least part of the observed gains.
5.1 Weighting by corpus quality
One answer to the above question is that some cor-
pora are better for training RMs than others. Fur-
thermore, corpora that are good for training the LM
or TM are not necessarily good for training the RM,
and vice versa. Tables 6 and 7 illustrate this. These
list the weights assigned to various sub-corpora for
LM, TM, and RM mixture models.
The weights assigned to the isi sub-corpus in par-
ticular exhibit a striking pattern. These are high in
the LM mixtures, moderate in the TM mixtures, and
very low in the RM mixtures. When one considers
that isi contains 72.6% of the English words in the
Arabic training data (see table 2), its weight of 0.01
in the RM mixture is remarkable.
On reflection, it makes sense that EM would as-
sign weights in the order it does. The isi corpus
consists of comparable data: sentence pairs whose
source- and target-language sides are similar, but of-
ten not mutual translations. These are a valuable
source of in-domain n-grams for the LM; a some-
what noisy source of in-domain phrase pairs for the
TM; and an unreliable source of re-ordering patterns
for the RM. Figure 1 shows this. Although the two
942
LM TM RM
isi (0.23) un (0.29) un (0.21)
gale nw (0.11) fbis (0.15) gale nw (0.13)
un (0.11) hkh (0.10) lex&ne (0.12)
sino. (0.09) gale nw (0.09) hkh (0.08)
fbis (0.08) gale bn (0.07) fbis (0.08)
fin. (0.07) oth nw (0.06) gale bn (0.08)
oth nw (0.07) sino. (0.06) gale wl (0.06)
gale bn (0.07) isi (0.05) gale bc (0.06)
gale wl (0.06) hkn (0.04) hkn (0.04)
hkh (0.06) fin. (0.04) fin. (0.04)
hkn (0.03) gale bc (0.03) oth nw (0.03)
gale bc (0.02) gale wl (0.02) hkl (0.03)
lex&ne (0.00) lex&ne (0.00) isi (0.01)
hkl (0.00) hkl (0.00) sino. (0.01)
Table 6: Chinese-English sub-corpora for LM, TM, and
RM mixture models, ordered by mixture weight.
LM TM RM
isi (0.41) isi (0.35) gale bc (0.21)
oth nw (0.19) oth nw (0.29) gale ng (0.20)
gale ng (0.15) gale bc (0.10) gale nw (0.20)
gale wl (0.09) gale ng (0.08) oth nw (0.13)
gale nw (0.07) gale bn (0.07) gale ng (0.12)
gale bc (0.05) gale nw (0.07) gale wb (0.11)
gale bn (0.02) gale wl (0.05) isi (0.01)
Table 7: Arabic-English sub-corpora for LM, TM, and
RM mixture models, ordered by mixture weight.
sides of the comparable data are similar, they give
the misleading impression that the phrases labeled
1, 2, 3 in the Chinese source should be reordered as
2, 3, 1 in English. We show a reference translation
of the Chinese source (not found in the comparable
data) that reorders the phrases as 1, 3, 2.
Thus, RM adaptation allows the RM to learn that
certain corpora whose reordering information is of
lower quality corpora should have lower weights.
The optimal weights for corpora inside an RM may
be different from the optimal weights inside a TM or
LM.
5.2 Weighting by domain match
So is this all that RM adaptation does: downweight
poor-quality data? We believe there is more to
RM adaptation than that. Specifically, even if one
 REF: The American list of goods that would incur tariffs in retaliation would certainly not be accepted by the Chinese government.  SRC: ??(1) ? ?? ??? ??(2) ?? ?? ? ??  ?(3)?  TGT: And the Chinese(2) side would certainly not accept(3)  the unreasonable demands put forward by the Americans(1) concerning the protection of intellectual property rights .  
Figure 1: Example of sentence pair from comparable
data; underlined words with the same number are trans-
lations of each other
Corpus M S D Count
fbis 0.50 0.07 0.43 685
financial 0.32 0.28 0.41 65
gale bc 0.60 0.10 0.31 50
gale bn 0.47 0.15 0.37 109
gale nw 0.51 0.05 0.44 326
gale wl 0.42 0.26 0.32 52
hkh 0.29 0.23 0.48 130
hkl 0.28 0.16 0.56 263
hkn 0.30 0.27 0.43 241
isi 0.24 0.16 0.60 240
lex&ne 0.94 0.03 0.02 1
others nw 0.29 0.16 0.55 23
sinorama 0.44 0.07 0.49 110
un 0.37 0.10 0.53 15
dev 0.46 0.24 0.31 11
Table 8: Orientation frequencies for the phrase pair ??
? immediately?, with respect to the previous phrase.
considers only high-quality data for training RMs
(ignoring comparable data, etc.) one sees differ-
ences in reordering behaviour between different do-
mains. This isn?t just because of differences in word
frequencies between domains, because we observe
domain-dependent differences in reordering for the
same phrase pair. Two examples are given below:
one Chinese-English, one Arabic-English.
Table 8 shows reordering data for the phrase
pair ??? immediately? in various corpora. No-
tice the strong difference in behaviour between the
three Hong Kong corpora?hkh, hkl and hkn?and
some of the other corpora, for instance fbis. In the
943
Corpus M S D Count
gale bc 0.50 0.27 0.22 233
gale bn 0.56 0.21 0.23 226
gale ng 0.51 0.13 0.37 295
gale nw 0.47 0.20 0.33 167
gale wl 0.56 0.18 0.26 127
isi 0.50 0.06 0.44 5502
other nw 0.50 0.16 0.34 1450
dev 0.75 0.12 0.13 52
Table 9: Orientation frequencies for the phrase pair
?work AlEml? with respect to the previous phrase.
Hong Kong corpora, immediately is much less likely
(probability of around 0.3) to be associated with a
monotone (M) orientation than it is in fbis (proba-
bility of 0.5). This phrase pair is relatively frequent
in both corpora, so this disparity seems too great to
be due to chance.
Table 9 shows reordering behaviour for the phrase
pair ?work AlEml?3 across different sub-corpora.
As in the Chinese example, there appear to be sig-
nificant differences in reordering patterns for cer-
tain corpora. For instance, gale bc swaps this well-
attested phrase pair twice as often (probability of
0.27) as gale ng (probability of 0.13).
For Chinese, it is possible that dialect plays a role
in reordering behaviour. In theory, Mandarin Chi-
nese is a single language which is quite different,
especially in spoken form, from other languages of
China such as Cantonese, Hokkien, Shanghainese,
and so on. In practice, many speakers of Mandarin
may be unconsciously influenced by other languages
that they speak, or by other languages that they don?t
speak but that have an influence over people they in-
teract with frequently. Word order can be affected
by this: the Mandarin of Mainland China, Hong
Kong and Taiwan sometimes has slightly different
word order. Hong Kong Mandarin can be somewhat
influenced by Cantonese, and Taiwan Mandarin by
Hokkien. For instance, if a verb is modified by an
adverb in Mandarin, the standard word order is ?ad-
verb verb?. However, since in Cantonese, ?verb ad-
verb? is a more common word order, speakers and
writers of Mandarin in Hong Kong may adopt the
3We represent the Arabic word AlEml in its Buckwalter
transliteration.
  	
 

 	 

Figure 2: An example of different word ordering in Man-
darin from different area.
?verb adverb? order in that language as well. Figure
2 shows how a different word order in the Mandarin
source affects reordering when translating into En-
glish. Perhaps in situations where different training
corpora represent different dialects, RM adaptation
involves an element of dialect adaptation. We are ea-
ger to test this hypothesis for Arabic?different di-
alects of Arabic are much more different from each
other than dialects of Mandarin, and reordering is
often one of the differences?but we do not have ac-
cess to Arabic training, dev, and test data in which
the dialects are clearly separated.
It is possible that RM adaptation also has an el-
ement of genre adaptation. We have not yet been
able to confirm or refute this hypothesis. However,
whatever is causing the corpus-dependent reorder-
ing patterns for particular phrase pairs shown in the
two tables above, it is clear that they may explain
the performance improvements we observe for RM
adaptation in our experiments.
5.3 Penalizing highly-specific phrase pairs
In section 3.3 we described our strategy for giving
general (high document-frequency) phrase pairs that
occur in the dev set more influence in determining
mixing weights. An artifact of our implementation
applies a similar strategy to the probability estimates
for all phrase pairs in the model. This is that 0 prob-
abilities are assigned to all orientations whenever a
phrase pair is absent from a particular sub-corpus.
Thus, for example, a pair (f, e) that occurs only
in sub-corpus iwill receive a probability p(o|f, e) =
?i pi(o|f, e) in the mixture model (equation 2).
Since ?i ? 1, this amounts to a penalty on pairs
that occur in few sub-corpora, especially ones with
low mixture weights.
The resulting mixture model is deficient (non-
944
normalized), but easy to fix by backing off to a
global distribution such as p(o) in equation 1. How-
ever, we found that this ?fix? caused large drops in
performance, for instance from the Arabic BLEU
score of 48.3 reported in table 3 to 46.0. We there-
fore retained the original strategy, which can be seen
as a form of instance weighting. Moreover, it is one
that is particularly effective in the RM, since, com-
pared to a similar strategy in the TM (which we also
employ), it applies to whole phrase pairs and results
in much larger penalties.
6 Related work
Domain adaptation is an active topic in the NLP re-
search community. Its application to SMT systems
has recently received considerable attention. Previ-
ous work on SMT adaptation has mainly focused
on translation model (TM) and language model
(LM) adaptation. Approaches that have been tried
for SMT model adaptation include mixture models,
transductive learning, data selection, data weighting,
and phrase sense disambiguation.
Research on mixture models has considered both
linear and log-linear mixtures. Both were studied
in (Foster and Kuhn, 2007), which concluded that
the best approach was to combine sub-models of
the same type (for instance, several different TMs
or several different LMs) linearly, while combining
models of different types (for instance, a mixture
TM with a mixture LM) log-linearly. (Koehn and
Schroeder, 2007), instead, opted for combining the
sub-models directly in the SMT log-linear frame-
work.
In transductive learning, an MT system trained on
general domain data is used to translate in-domain
monolingual data. The resulting bilingual sentence
pairs are then used as additional training data (Ueff-
ing et al, 2007; Chen et al, 2008; Schwenk, 2008;
Bertoldi and Federico, 2009).
Data selection approaches (Zhao et al, 2004; Lu?
et al, 2007; Moore and Lewis, 2010; Axelrod et
al., 2011) search for bilingual sentence pairs that are
similar to the in-domain ?dev? data, then add them
to the training data. The selection criteria are typi-
cally related to the TM, though the newly found data
will be used for training not only the TM but also the
LM and RM.
Data weighting approaches (Matsoukas et al,
2009; Foster et al, 2010; Huang and Xiang, 2010;
Phillips and Brown, 2011; Sennrich, 2012) use a
rich feature set to decide on weights for the train-
ing data, at the sentence or phrase pair level. For
instance, a sentence from a corpus whose domain is
far from that of the dev set would typically receive
a low weight, but sentences in this corpus that ap-
pear to be of a general nature might receive higher
weights.
The 2012 JHU workshop on Domain Adapta-
tion for MT 4 proposed phrase sense disambiguation
(PSD) for translation model adaptation. In this ap-
proach, the context of a phrase helps the system to
find the appropriate translation.
All of the above work focuses on either TM or
LM domain adaptation.
7 Conclusions
In this paper, we adapt the lexicalized reordering
model (RM) of an SMT system to the domain in
which the system will operate using a mixture model
approach. Domain adaptation of translation mod-
els (TMs) and language models (LMs) has become
common for SMT systems, but to our knowledge
this is the first attempt in the literature to adapt the
RM. Our experiments demonstrate that RM adap-
tation can significantly improve translation quality,
even when the system already has TM and LM adap-
tation. We also experimented with two modifica-
tions to linear mixture model adaptation: dev set
smoothing and weighting orientation counts with
document frequency of phrase pairs. Both ideas
are potentially applicable to TM and LM adaptation.
Dev set smoothing, in particular, seems to improve
the performance of RM adaptation significantly. Fi-
nally, we investigate why RM adaptation helps SMT
performance. Three factors seem to be important:
downweighting information from corpora that are
less suitable for modeling reordering (such as com-
parable corpora), dialect/genre effects, and implicit
instance weighting.
4http://www.clsp.jhu.edu/workshops/archive/ws-
12/groups/dasmt
945
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In EMNLP 2011.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the 4th
Workshop on Statistical Machine Translation, Athens,
March. WMT.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for smt self-
enhancement. In ACL 2008.
Boxing Chen, Roland Kuhn, George Foster, and Howard
Johnson. 2011. Unpacking and transforming feature
functions: New ways to smooth phrase tables. In MT
Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In WMT 2012.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the ACL Work-
shop on Statistical Machine Translation, Prague, June.
WMT.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Boston.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model. In
EMNLP 2008, pages 848?856, Hawaii, October.
Fei Huang and Bing Xiang. 2010. Feature-rich discrimi-
native phrase rescoring for SMT. In COLING 2010.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, D. Talbot, and M. White. 2005. Edin-
burgh system description for the 2005 NIST MT eval-
uation. In Proceedings of Machine Translation Evalu-
ation Workshop.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL 2007, Demon-
stration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the 6th Conference of the As-
sociation for Machine Translation in the Americas,
Georgetown University, Washington D.C., October.
Springer-Verlag.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. In Proceedings
of the 2007 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Prague, Czech
Republic.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL
2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311?318, Philadel-
phia, July. ACL.
Aaron B. Phillips and Ralf D. Brown. 2011. Training
machine translation with a second-order taylor approx-
imation of weighted translation instances. In MT Sum-
mit 2011.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT 2008.
Rico Sennrich. 2012. Mixture-modeling with unsuper-
vised clusters for domain adaptation in statistical ma-
chine translation. In EACL 2012.
Christoph Tillmann and Tong Zhang. 2005. A localized
prediction model for statistical machine translation. In
Proceedings of the 43th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Ann Ar-
bor, Michigan, July. ACL.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Prague, Czech Republic, June. ACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING) 2004, Geneva, August.
946
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 834?843,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bilingual Sense Similarity for Statistical Machine Translation 
 
 
Boxing Chen, George Foster and Roland Kuhn 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca 
 
  
 
Abstract 
 
This paper proposes new algorithms to com-
pute the sense similarity between two units 
(words, phrases, rules, etc.) from parallel cor-
pora. The sense similarity scores are computed 
by using the vector space model.  We then ap-
ply the algorithms to statistical machine trans-
lation by computing the sense similarity be-
tween the source and target side of translation 
rule pairs. Similarity scores are used as addi-
tional features of the translation model to im-
prove translation performance. Significant im-
provements are obtained over a state-of-the-art 
hierarchical phrase-based machine translation 
system. 
1 Introduction 
The sense of a term can generally be inferred 
from its context. The underlying idea is that a 
term is characterized by the contexts it co-occurs 
with. This is also well known as the Distribu-
tional Hypothesis (Harris, 1954): terms occurring 
in similar contexts tend to have similar mean-
ings. There has been a lot of work to compute the 
sense similarity between terms based on their 
distribution in a corpus, such as (Hindle, 1990; 
Lund and Burgess, 1996; Landauer and Dumais, 
1997; Lin, 1998; Turney, 2001; Pantel and Lin, 
2002; Pado and Lapata, 2007).  
In the work just cited, a common procedure is 
followed. Given two terms to be compared, one 
first extracts various features for each term from 
their contexts in a corpus and forms a vector 
space model (VSM); then, one computes their 
similarity by using similarity functions. The fea-
tures include words within a surface window of a 
fixed size (Lund and Burgess, 1996), grammati-
cal dependencies (Lin, 1998; Pantel and Lin 
2002; Pado and Lapata, 2007), etc.  The similari-
ty function which has been most widely used is 
cosine distance (Salton and McGill, 1983); other 
similarity functions include Euclidean distance, 
City Block distance (Bullinaria and Levy; 2007), 
and Dice and Jaccard coefficients (Frakes and 
Baeza-Yates, 1992), etc. Measures of monolin-
gual sense similarity have been widely used in 
many applications, such as synonym recognizing 
(Landauer and Dumais, 1997), word clustering 
(Pantel and Lin 2002), word sense disambigua-
tion (Yuret and Yatbaz 2009), etc. 
Use of the vector space model to compute  
sense similarity has also been adapted to the mul-
tilingual condition,  based on the assumption that 
two terms with similar meanings often occur in 
comparable contexts across languages. Fung 
(1998) and Rapp (1999) adopted VSM for the 
application of extracting translation pairs from 
comparable or even unrelated corpora. The vec-
tors in different languages are first mapped to a 
common space using an initial bilingual dictio-
nary, and then compared. 
However, there is no previous work that uses 
the VSM to compute sense similarity for terms 
from parallel corpora. The sense similarities, i.e. 
the translation probabilities in a translation mod-
el, for units from parallel corpora are mainly 
based on the co-occurrence counts of the two 
units. Therefore, questions emerge: how good is 
the sense similarity computed via VSM for two 
units from parallel corpora? Is it useful for multi-
lingual applications, such as statistical machine 
translation (SMT)? 
In this paper, we try to answer these questions, 
focusing on sense similarity applied to the SMT 
task. For this task, translation rules are heuristi-
cally extracted from automatically word-aligned 
sentence pairs. Due to noise in the training cor-
pus or wrong word alignment, the source and 
target sides of some rules are not semantically 
equivalent, as can be seen from the following 
834
real examples which are taken from the rule table 
built on our training data (Section 5.1): 
?? ? X ?? ||| one of X (*) 
?? ? X ?? ||| one of X in the world    
?? ?? ||| many citizens 
?? ?? ||| many hong kong residents (*) 
The source and target sides of the rules with (*) 
at the end are not semantically equivalent; it 
seems likely that measuring the semantic similar-
ity from their context between the source and 
target sides of rules might be helpful to machine 
translation. 
In this work, we first propose new algorithms 
to compute the sense similarity between two 
units (unit here includes word, phrase, rule, etc.) 
in different languages by using their contexts. 
Second, we use the sense similarities between the 
source and target sides of a translation rule to 
improve statistical machine translation perfor-
mance.  
This work attempts to measure directly the 
sense similarity for units from different languag-
es by comparing their contexts1. Our contribution 
includes proposing new bilingual sense similarity 
algorithms and applying them to machine trans-
lation. 
We chose a hierarchical phrase-based SMT 
system as our baseline; thus, the units involved 
in computation of sense similarities are hierar-
chical rules. 
2 Hierarchical phrase-based MT system 
The hierarchical phrase-based translation method 
(Chiang, 2005; Chiang, 2007) is a formal syntax-
based translation modeling method; its transla-
tion model is a weighted synchronous context 
free grammar (SCFG). No explicit linguistic syn-
tactic information appears in the model. An 
SCFG rule has the following form: 
~,,???X
 
where X is a non-terminal symbol shared by all 
the rules; each rule has at most two non-
terminals. ?  (? ) is a source (target) string con-
sisting of terminal and non-terminal symbols. ~  
defines a one-to-one correspondence between 
non-terminals in ?  and ? . 
                                               
1
 There has been a lot of work (more details in Section 7) on 
applying word sense disambiguation (WSD) techniques in 
SMT for translation selection. However, WSD techniques 
for SMT do so indirectly, using source-side context to help 
select a particular translation for a source rule. 
 source target 
Ini. phr. ? ?? ? ?? he attended the meeting 
Rule 1 
Context 1 
? ?? ? X1 
?? 
he attended X1 
the, meeting 
Rule 2 
Context 2 
?? 
?, ??, ? 
the meeting 
he, attended 
Rule 3 
Context 3 
? X1?? 
??, ? 
he X1 the meeting 
attended 
Rule 4 
Context 4 
?? ? 
?,?? 
attended 
he, the, meeting 
 
Figure 1: example of hierarchical rule pairs and their 
context features. 
 
Rule frequencies are counted during rule ex-
traction over word-aligned sentence pairs, and 
they are normalized to estimate features on rules. 
Following (Chiang, 2005; Chiang, 2007), 4 fea-
tures are computed for each rule: 
? )|( ??P  and )|( ??P  are direct and in-
verse rule-based conditional probabilities; 
? )|( ??wP  and )|( ??wP are direct and in-
verse lexical weights (Koehn et al, 2003). 
Empirically, this method has yielded better 
performance on language pairs such as Chinese-
English than the phrase-based method because it 
permits phrases with gaps; it generalizes the 
normal phrase-based models in a way that allows 
long-distance reordering (Chiang, 2005; Chiang, 
2007). We use the Joshua implementation of the 
method for decoding (Li et al, 2009). 
3 Bag-of-Words Vector Space Model 
To compute the sense similarity via VSM, we 
follow the previous work (Lin, 1998) and 
represent the source and target side of a rule by 
feature vectors. In our work, each feature corres-
ponds to a context word which co-occurs with 
the translation rule. 
3.1 Context Features 
In the hierarchical phrase-based translation me-
thod, the translation rules are extracted by ab-
stracting some words from an initial phrase pair 
(Chiang, 2005). Consider a rule with non-
terminals on the source and target side; for a giv-
en instance of the rule (a particular phrase pair in 
the training corpus), the context will be the 
words instantiating the non-terminals. In turn, the 
context for the sub-phrases that instantiate the 
non-terminals will be the words in the remainder 
of the phrase pair. For example in Figure 1, if we 
835
have an initial phrase pair ? ?? ? ?? ||| he 
attended the meeting, and we extract four rules 
from this initial phrase: ? ?? ? X1 ||| he at-
tended X1, ?? ||| the meeting, ? X1?? ||| he 
X1 the meeting, and?? ? ||| attended. There-
fore, the and meeting are context features of tar-
get pattern he attended X1; he and attended are 
the context features of the meeting; attended is 
the context feature of he X1 the meeting;  also he, 
the and meeting are the context feature of at-
tended (in each case, there are also source-side 
context features).  
3.2 Bag-of-Words Model 
For each side of a translation rule pair, its context 
words are all collected from the training data, 
and two ?bags-of-words? which consist of col-
lections of source and target context words co-
occurring with the rule?s source and target sides 
are created. 
},...,,{
},...,,{
21
21
Je
If
eeeB
fffB
=
=
                        (1) 
where )1( Iifi ??  are source context words 
which co-occur with the source side of rule ? , 
and )1( Jje j ??  are target context words 
which co-occur with the target side of rule ? . 
Therefore, we can represent source and target 
sides of the rule by vectors fv
v
  and ev
v
 as in Eq-
uation (2): 
},...,,{
},...,,{
21
21
J
I
eeee
ffff
wwwv
wwwv
=
=
v
v
                     (2) 
where 
ifw  and jew are values for each source 
and target context feature; normally, these values 
are based on the counts of the words in the cor-
responding bags.  
3.3 Feature Weighting Schemes 
We use pointwise mutual information (Church et 
al., 1990) to compute the feature values. Let c 
( fBc ? or eBc ?  ) be a context word and 
),( crF  be the frequency count of a rule r (?  or 
? ) co-occurring with the context word c. The 
pointwise mutual information ),( crMI  is de-
fined as: 
N
cF
N
rF
N
crF
crMIcrw )(log)(log
),(log
),(),(
?
==
          (3) 
where N is the total frequency counts of all rules 
and their context words. Since we are using this 
value as a weight, following (Turney, 2001), we 
drop log, N and )(rF . Thus (3) simplifies to:  
)(
),(),(
cF
crF
crw =
                     (4) 
It can be seen as an estimate of )|( crP , the em-
pirical probability of observing r given c. 
A problem with )|( crP  is that it is biased 
towards infrequent words/features. We therefore 
smooth ),( crw  with add-k smoothing: 
kRcF
kcrF
kcrF
kcrF
crw R
i
i
+
+
=
+
+
=
?
=
)(
),(
)),((
),(),(
1
  (5) 
where k is a tunable global smoothing constant, 
and R is the number of rules. 
4 Similarity Functions 
There are many possibilities for calculating simi-
larities between bags-of-words in different lan-
guages. We consider IBM model 1 probabilities 
and cosine distance similarity functions. 
4.1 IBM Model 1 Probabilities 
For the IBM model 1 similarity function, we take 
the geometric mean of symmetrized conditional 
IBM model 1 (Brown et al, 1993) bag probabili-
ties, as in Equation (6). 
))|()|((),( feef BBPBBPsqrtsim ?=??       (6) 
To compute )|( ef BBP , IBM model 1 as-
sumes that all source words are conditionally 
independent, so that: 
 ?
=
=
I
i
eief BfpBBP
1
)|()|(                (7) 
To compute, we use a ?Noisy-OR? combina-
tion which has shown better performance than 
standard IBM model 1 probability, as described 
in (Zens and Ney, 2004): 
)|(1)|( eiei BfpBfp ?=                       (8) 
?
=
???
J
j
jiei efpBfp
1
))|(1(1)|(          (9) 
where )|( ei Bfp  is the probability that if  is not 
in the translation of eB , and  is the IBM model 1 
probability. 
4.2 Vector Space Mapping 
A common way to calculate semantic similarity 
is by vector space cosine distance; we will also 
836
use this similarity function in our algorithm. 
However, the two vectors in Equation (2) cannot 
be directly compared because the axes of their 
spaces represent different words in different lan-
guages, and also their dimensions I and J are not 
assured to be the same. Therefore, we need to 
first map a vector into the space of the other vec-
tor, so that the similarity can be calculated. Fung 
(1998) and Rapp (1999) map the vector one-
dimension-to-one-dimension (a context word is a 
dimension in each vector space) from one lan-
guage to another language via an initial bilingual 
dictionary. We follow (Zhao et al, 2004) to do 
vector space mapping.  
Our goal is ? given a source pattern ? to dis-
tinguish between the senses of its associated tar-
get patterns. Therefore, we map all vectors in 
target language into the vector space in the 
source language. What we want is a representa-
tion 
av
v
 in the source language space of the target 
vector 
ev
v
. To get 
av
v
, we can let ifaw , the weight 
of the ith source feature, be a linear combination 
over target features. That is to say, given a 
source feature weight for fi, each target feature 
weight is linked to it with some probability. So 
that we can calculate a transformed vector from 
the target vectors by calculating weights if
aw  us-
ing a translation lexicon: 
?
=
=
J
j
eji
f
a j
i wefw
1
)|Pr(                    (10) 
where )|( ji efp  is a lexical probability (we use 
IBM model 1 probability). Now the source vec-
tor and the mapped vector av
v
 have the same di-
mensions as shown in (11): 
},...,,{
},...,,{
21
21
I
I
f
a
f
a
f
aa
ffff
wwwv
wwwv
=
=
v
v
                   (11) 
4.3 Na?ve Cosine Distance Similarity 
The standard cosine distance is defined as the 
inner product of the two vectors fv
v
 and av
v
 nor-
malized by their norms. Based on Equation (10) 
and (11), it is easy to derive the similarity as fol-
lows: 
)()(
)|Pr(
||||),cos(),(
1
2
1
2
1 1
??
??
==
= =
=
?
?
==
I
i
f
a
I
I
f
I
i
J
j
ejif
af
af
af
i
i
ji
wsqrtwsqrt
wefw
vv
vv
vvsim vv
vv
vv??
         (12) 
where I and J are the number of the words in 
source and target bag-of-words; 
ifw  and jew are 
values of source and target features; ifaw  is the 
transformed weight mapped from all target fea-
tures to the source dimension at word fi. 
4.4 Improved Similarity Function 
To incorporate more information than the origi-
nal similarity functions ? IBM model 1 proba-
bilities in Equation (6) and na?ve cosine distance 
similarity function in Equation (12) ? we refine 
the similarity function and propose a new algo-
rithm.  
As shown in Figure 2, suppose that we have a 
rule pair ),( ?? . fullfC  and fulleC  are the contexts 
extracted according to the definition in section 3 
from the full training data for ?
 
and for ? , re-
spectively. coocfC and cooceC  are the contexts for 
?
 
  and ?   when ?
 
and ? co-occur. Obviously, 
they satisfy the constraints: fullf
cooc
f CC ?  and  
full
e
cooc
e CC ? .  Therefore, the original similarity 
functions are to compare the two context vectors 
built on full training data directly, as shown in 
Equation (13). 
),(),( fullefullf CCsimsim =??             (13) 
Then, we propose a new similarity function as 
follows: 
321 ),(),(),(
),(
???
??
cooc
e
full
e
cooc
e
cooc
f
cooc
f
full
f CCsimCCsimCCsim
sim
??
=
(14) 
where the parameters i? (i=1,2,3) can be tuned 
via minimal error rate training (MERT) (Och, 
2003). 
 
 
 
 
 
 
 
 
 
 
Figure 2: contexts for rule ?
 
  and ? . 
 
A unit?s sense is defined by all its contexts in 
the whole training data; it may have a lot of dif-
ferent senses in the whole training data. Howev-
er, when it is linked with another unit in the other 
language, its sense pool is constrained and is just 
?  
?  
full
fC  coocfC  
   
full
eC  cooceC  
837
a subset of the whole sense set. ),( coocffullf CCsim  
is the metric which evaluates the similarity be-
tween the whole sense pool of ?  and the sense 
pool when ?  co-occurs with ? ; 
),( coocefulle CCsim  is the analogous similarity me-
tric for ? . They range from 0 to 1. These two 
metrics both evaluate the similarity for two vec-
tors in the same language, so using cosine dis-
tance to compute the similarity is straightfor-
ward. And we can set a relatively large size for 
the vector, since it is not necessary to do vector 
mapping as the vectors are in the same language. 
),( coocecoocf CCsim  computes the similarity between 
the context vectors when ?
 
and ? co-occur. We 
may compute ),( coocecoocf CCsim by using IBM 
model 1 probability and cosine distance similari-
ty functions as Equation (6) and (12). Therefore, 
on top of the degree of bilingual semantic simi-
larity between a source and a target translation 
unit, we have also incorporated the monolingual 
semantic similarity between all occurrences of a 
source or target unit, and that unit?s occurrence 
as part of the given rule, into the sense similarity 
measure. 
5 Experiments 
We evaluate the algorithm of bilingual sense si-
milarity via machine translation. The sense simi-
larity scores are used as feature functions in the 
translation model. 
5.1 Data 
We evaluated with different language pairs: Chi-
nese-to-English, and German-to-English. For 
Chinese-to-English tasks, we carried out the ex-
periments in two data conditions. The first one is 
the large data condition, based on training data 
for the NIST 2  2009 evaluation Chinese-to-
English track. In particular, all the allowed bilin-
gual corpora except the UN corpus and Hong 
Kong Hansard corpus have been used for esti-
mating the translation model. The second one is 
the small data condition where only the FBIS3 
corpus is used to train the translation model. We 
trained two language models: the first one is a 4-
gram LM which is estimated on the target side of 
the texts used in the large data condition. The 
second LM is a 5-gram LM trained on the so-
                                               
2
 http://www.nist.gov/speech/tests/mt 
3
 LDC2003E14 
called English Gigaword corpus. Both language 
models are used for both tasks. 
We carried out experiments for translating 
Chinese to English. We use the same develop-
ment and test sets for the two data conditions. 
We first created a development set which used 
mainly data from the NIST 2005 test set, and 
also some balanced-genre web-text from the 
NIST training material. Evaluation was per-
formed on the NIST 2006 and 2008 test sets. Ta-
ble 1 gives figures for training, development and 
test corpora; |S| is the number of the sentences, 
and |W| is the number of running words. Four 
references are provided for all dev and test sets. 
 
   Chi Eng 
 
Parallel 
Train 
Large 
Data 
|S| 3,322K 
|W| 64.2M 62.6M 
Small 
Data 
|S| 245K 
|W| 9.0M 10.5M 
   Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics of training, dev, and test sets for 
Chinese-to-English task. 
 
For German-to-English tasks, we used WMT 
20064 data sets. The parallel training data con-
tains 21 million target words; both the dev set 
and test set contain 2000 sentences; one refer-
ence is provided for each source input sentence. 
Only the target-language half of the parallel 
training data are used to train the language model 
in this task.  
5.2 Results 
For the baseline, we train the translation model 
by following (Chiang, 2005; Chiang, 2007) and 
our decoder is Joshua5, an open-source hierar-
chical phrase-based machine translation system 
written in Java. Our evaluation metric is IBM 
BLEU (Papineni et al, 2002), which performs 
case-insensitive matching of n-grams up to n = 4. 
Following (Koehn, 2004), we use the bootstrap-
resampling test to do significance testing. 
By observing the results on dev set in the addi-
tional experiments, we first set the smoothing 
constant k in Equation (5) to 0.5. 
Then, we need to set the sizes of the vectors to 
balance the computing time and translation accu-
                                               
4
 http://www.statmt.org/wmt06/ 
5
 http://www.cs.jhu.edu/~ccb/joshua/index.html 
838
racy, i.e., we keep only the top N context words 
with the highest feature value for each side of a 
rule 6 . In the following, we use ?Alg1? to 
represent the original similarity functions which 
compare the two context vectors built on full 
training data, as in Equation (13); while we use 
?Alg2? to represent the improved similarity as in 
Equation (14). ?IBM? represents IBM model 1 
probabilities, and ?COS? represents cosine dis-
tance similarity function. 
After carrying out a series of additional expe-
riments on the small data condition and observ-
ing the results on the dev set, we set the size of 
the vector to 500 for Alg1; while for Alg2, we 
set the sizes of fullfC  and fulleC N1 to 1000, and the 
sizes of coocfC  and cooceC N2 to 100.  
The sizes of the vectors in Alg2 are set in the 
following process: first, we set N2 to 500 and let 
N1  range from 500 to 3,000, we observed that the 
dev set got best performance when N1 was 1000; 
then we set N1 to 1000 and let N1 range from 50 
to 1000, we got best performance when N1 =100. 
We use this setting as the default setting in all 
remaining experiments. 
 
Algorithm NIST?06 NIST?08 
Baseline 27.4 21.2 
Alg1 IBM 27.8* 21.5 
Alg1 COS 27.8* 21.5 
Alg2 IBM 27.9* 21.6* 
Alg2 COS 28.1** 21.7* 
 
Table 2: Results (BLEU%) of small data Chinese-to-
English NIST task. Alg1 represents the original simi-
larity functions as in Equation (13); while Alg2 
represents the improved similarity as in Equation 
(14). IBM represents IBM model 1 probability, and 
COS represents cosine distance similarity function. * 
or ** means result is significantly better than the 
baseline (p < 0.05 or p < 0.01, respectively). 
 
 Ch-En De-En 
Algorithm NIST?06 NIST?08 Test?06 
Baseline 31.0 23.8 26.9 
Alg2 IBM 31.5* 24.5** 27.2* 
Alg2 COS 31.6** 24.5** 27.3* 
 
Table 3: Results (BLEU%) of large data Chinese-to-
English NIST task and German-to-English WMT 
task. 
                                               
6
 We have also conducted additional experiments by remov-
ing the stop words from the context vectors; however, we 
did not observe any consistent improvement. So we filter 
the context vectors by only considering the feature values. 
Table 2 compares the performance of Alg1 
and Alg2 on the Chinese-to-English small data 
condition. Both Alg1 and Alg2 improved the 
performance over the baseline, and Alg2 ob-
tained slight and consistent improvements over 
Alg1. The improved similarity function Alg2 
makes it possible to incorporate monolingual 
semantic similarity on top of the bilingual se-
mantic similarity, thus it may improve the accu-
racy of the similarity estimate. Alg2 significantly 
improved the performance over the baseline. The 
Alg2 cosine similarity function got 0.7 BLEU-
score (p<0.01) improvement over the baseline 
for NIST 2006 test set, and a 0.5 BLEU-score 
(p<0.05) for NIST 2008 test set. 
Table 3 reports the performance of Alg2 on 
Chinese-to-English NIST large data condition 
and German-to-English WMT task. We can see 
that IBM model 1 and cosine distance similarity 
function both obtained significant improvement 
on all test sets of the two tasks. The two similari-
ty functions obtained comparable results. 
6 Analysis and Discussion 
6.1 Effect of Single Features 
In Alg2, the similarity score consists of three 
parts as in Equation (14): ),( coocffullf CCsim , 
),( coocefulle CCsim , and ),( coocecoocf CCsim ; where  
),( coocecoocf CCsim  could be computed by IBM mod-
el 1 probabilities ),( coocecoocfIBM CCsim  or cosine dis-
tance similarity function ),( coocecoocfCOS CCsim . 
Therefore, our first study is to determine which 
one of the above four features has the most im-
pact on the result. Table 4 shows the results ob-
tained by using each of the 4 features. First, we 
can see that ),( coocecoocfIBM CCsim  always gives a 
better improvement than ),( coocecoocfCOS CCsim . This 
is because  ),( coocecoocfIBM CCsim  scores are more 
diverse than the latter when the number of con-
text features is small (there are many rules that 
have only a few contexts.) For an extreme exam-
ple, suppose that there is only one context word 
in each vector of source and target context fea-
tures, and the translation probability of the two 
context words is not 0. In this case, 
),( coocecoocfIBM CCsim   reflects the translation proba-
bility of the context word pair, while 
),( coocecoocfCOS CCsim  is always 1.  
   Second, ),( coocffullf CCsim  and ),( coocefulle CCsim   
also give some improvements even when used 
839
independently. For a possible explanation, con-
sider the following example. The Chinese word 
?? ? can translate to ?red?, ?communist?, or 
?hong? (the transliteration of ?, when it is used 
in a person?s name).  Since these translations are 
likely to be associated with very different source 
contexts, each will have a low ),( coocffullf CCsim  
score.  Another Chinese word ?? may translate 
into synonymous words, such as ?brook?, 
?stream?, and ?rivulet?, each of which will have 
a high  ),( coocffullf CCsim  score. Clearly, ? is a 
more ?dangerous? word than??, since choos-
ing the wrong translation for it would be a bad 
mistake. But if the two words have similar trans-
lation distributions, the system cannot distinguish 
between them. The monolingual similarity scores 
give it the ability to avoid ?dangerous? words, 
and choose alternatives (such as larger phrase 
translations) when available. 
Third, the similarity function of Alg2 consis-
tently achieved further improvement by incorpo-
rating the monolingual similarities computed for 
the source and target side. This confirms the ef-
fectiveness of our algorithm. 
 
 CE_LD CE_SD 
testset (NIST) ?06 ?08 ?06 ?08 
Baseline 31.0 23.8 27.4 21.2 
),( coocffullf CCsim  31.1 24.3 27.5 21.3 
),( coocefulle CCsim  31.1 23.9 27.9 21.5 
),( coocecoocfIBM CCsim  31.4 24.3 27.9 21.5 
),( coocecoocfCOS CCsim  31.2 23.9 27.7 21.4 
Alg2 IBM 31.5 24.5 27.9 21.6 
Alg2 COS 31.6 24.5 28.1 21.7 
 
Table 4: Results (BLEU%) of Chinese-to-English 
large data (CE_LD) and small data (CE_SD) NIST 
task by applying one feature. 
6.2 Effect of Combining the Two Similari-
ties 
We then combine the two similarity scores by 
using both of them as features to see if we could 
obtain further improvement. In practice, we use 
the four features in Table 4 together.  
Table 5 reports the results on the small data 
condition. We observed further improvement on 
dev set, but failed to get the same improvements 
on test sets or even lost performance. Since the 
IBM+COS configuration has one extra feature, it 
is possible that it overfits the dev set. 
 
Algorithm Dev NIST?06 NIST?08 
Baseline 20.2 27.4 21.2 
Alg2 IBM 20.5 27.9 21.6 
Alg2 COS 20.6 28.1 21.7 
Alg2 IBM+COS 20.8 27.9 21.5 
 
Table 5: Results (BLEU%) for combination of two 
similarity scores. Further improvement was only ob-
tained on dev set but not on test sets. 
6.3 Comparison with Simple Contextual 
Features 
Now, we try to answer the question: can the si-
milarity features computed by the function in 
Equation (14) be replaced with some other sim-
ple features? We did additional experiments on 
small data Chinese-to-English task to test the 
following features: (15) and (16) represent the 
sum of the counts of the context words in Cfull, 
while (17) represents the proportion of words in 
the context of ?  that appeared in the context of 
the rule ( ?? , ); similarly, (18) is related to the 
properties of the words in the context of ? . 
? ?= fullfi Cf if fFN ),()( ??              (15) 
? ?= fullej Ce je eFN ),()( ??                (16) 
)(
),(
),(
?
?
??
f
Cf i
f N
fF
E
cooc
fi? ?
=           (17) 
)(
),(
),( ?
?
??
e
Ce j
e N
eF
E
cooc
ej? ?
=           (18)   
where ),( ifF ?  and ),( jeF ?  are the frequency 
counts of rule ?  or ?   co-occurring with the 
context word if  or je   respectively. 
 
Feature Dev NIST?06 NIST?08 
Baseline 20.2 27.4 21.2 
+Nf 20.5 27.6 21.4 
+Ne 20.5 27.5 21.3 
+Ef 20.4 27.5 21.2 
+Ee 20.4 27.3 21.2 
+Nf+Ne 20.5 27.5 21.3 
 
Table 6: Results (BLEU%) of using simple features 
based on context on small data NIST task. Some im-
provements are obtained on dev set, but there was no 
significant effect on the test sets. 
 
Table 6 shows results obtained by adding the 
above features to the system for the small data 
840
condition. Although all these features have ob-
tained some improvements on dev set, there was 
no significant effect on the test sets. This means 
simple features based on context, such as the 
sum of the counts of the context features, are not 
as helpful as the sense similarity computed by 
Equation (14). 
6.4 Null Context Feature 
There are two cases where no context word can 
be extracted according to the definition of con-
text in Section 3.1. The first case is when a rule 
pair is always a full sentence-pair in the training 
data. The second case is when for some rule 
pairs, either their source or target contexts are 
out of the span limit of the initial phrase, so that 
we cannot extract contexts for those rule-pairs. 
For Chinese-to-English NIST task, there are 
about 1% of the rules that do not have contexts; 
for German-to-English task, this number is about 
0.4%. We assign a uniform number as their bi-
lingual sense similarity score, and this number is 
tuned through MERT. We call it the null context 
feature. It is included in all the results reported 
from Table 2 to Table 6. In Table 7, we show the 
weight of the null context feature tuned by run-
ning MERT in the experiments reported in Sec-
tion 5.2. We can learn that penalties always dis-
courage using those rules which have no context 
to be extracted.  
 
 
Alg. 
Task 
CE_SD CE_LD DE 
Alg2 IBM -0.09 -0.37 -0.15 
Alg2 COS -0.59 -0.42 -0.36 
 
Table 7: Weight learned for employing the null con-
text feature. CE_SD, CE_LD and DE are Chinese-to-
English small data task, large data task and German-
to-English task respectively. 
6.5 Discussion 
Our aim in this paper is to characterize the se-
mantic similarity of bilingual hierarchical rules. 
We can make several observations concerning 
our features: 
1) Rules that are largely syntactic in nature, 
such as ? X ||| the X of, will have very diffuse 
?meanings? and therefore lower similarity 
scores. It could be that the gains we obtained 
come simply from biasing the system against 
such rules. However, the results in table 6 show 
that this is unlikely to be the case: features that 
just count context words help very little. 
2) In addition to bilingual similarity, Alg2 re-
lies on the degree of monolingual similarity be-
tween the sense of a source or target unit within a 
rule, and the sense of the unit in general. This has 
a bias in favor of less ambiguous rules, i.e. rules 
involving only units with closely related mean-
ings. Although this bias is helpful on its own, 
possibly due to the mechanism we outline in sec-
tion 6.1, it appears to have a synergistic effect 
when used along with the bilingual similarity 
feature. 
3) Finally, we note that many of the features 
we use for capturing similarity, such as the con-
text ?the, of? for instantiations of X in the unit 
the X of, are arguably more syntactic than seman-
tic. Thus, like other ?semantic? approaches, ours 
can be seen as blending syntactic and semantic 
information. 
7 Related Work 
There has been extensive work on incorporating 
semantics into SMT. Key papers by Carpuat and 
Wu (2007) and Chan et al(2007) showed that 
word-sense disambiguation (WSD) techniques 
relying on source-language context can be effec-
tive in selecting translations in phrase-based and 
hierarchical SMT. More recent work has aimed 
at incorporating richer disambiguating features 
into the SMT log-linear model (Gimpel and 
Smith, 2008; Chiang et al 2009); predicting co-
herent sets of target words rather than individual 
phrase translations (Bangalore et al 2009; Maus-
er et al 2009); and selecting applicable rules in 
hierarchical (He et al 2008) and syntactic (Liu et 
al, 2008) translation, relying on source as well as 
target context. Work by Wu and Fung (2009) 
breaks new ground in attempting to match se-
mantic roles derived from a semantic parser 
across source and target languages. 
Our work is different from all the above ap-
proaches in that we attempt to discriminate 
among hierarchical rules based on: 1) the degree 
of bilingual semantic similarity between source 
and target translation units; and 2) the monolin-
gual semantic similarity between occurrences of 
source or target units as part of the given rule, 
and in general. In another words, WSD explicitly 
tries to choose a translation given the current 
source context, while our work rates rule pairs 
independent of the current context. 
8 Conclusions and Future Work 
In this paper, we have proposed an approach that 
uses the vector space model to compute the sense 
841
similarity for terms from parallel corpora and 
applied it to statistical machine translation. We 
saw that the bilingual sense similarity computed 
by our algorithm led to significant improve-
ments. Therefore, we can answer the questions 
proposed in Section 1. We have shown that the 
sense similarity computed between units from 
parallel corpora by means of our algorithm is 
helpful for at least one multilingual application: 
statistical machine translation. 
Finally, although we described and evaluated 
bilingual sense similarity algorithms applied to a 
hierarchical phrase-based system, this method is 
also suitable for syntax-based MT systems and 
phrase-based MT systems. The only difference is 
the definition of the context. For a syntax-based 
system, the context of a rule could be defined 
similarly to the way it was defined in the work 
described above. For a phrase-based system, the 
context of a phrase could be defined as its sur-
rounding words in a given size window. In our 
future work, we may try this algorithm on syn-
tax-based MT systems and phrase-based MT sys-
tems with different context features. It would 
also be possible to use this technique during 
training of an SMT system ? for instance, to im-
prove the bilingual word alignment or reduce the 
training data noise. 
References  
S. Bangalore, S. Kanthak, and P. Haffner. 2009. Sta-
tistical Machine Translation through Global Lexi-
cal Selection and Sentence Reconstruction. In: 
Goutte et al(ed.), Learning Machine Translation. 
MIT Press. 
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & 
R. L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2) 263-312. 
J. Bullinaria and J. Levy. 2007. Extracting semantic 
representations from word co-occurrence statistics: 
A computational study. Behavior Research Me-
thods, 39 (3), 510?526. 
M. Carpuat and D. Wu. 2007. Improving Statistical 
Machine Translation using Word Sense Disambig-
uation. In:  Proceedings of EMNLP, Prague. 
M. Carpuat. 2009. One Translation per Discourse. In:  
Proceedings of NAACL HLT Workshop on Se-
mantic Evaluations, Boulder, CO. 
Y. Chan, H. Ng and D. Chiang. 2007. Word Sense 
Disambiguation Improves Statistical Machine 
Translation. In:  Proceedings of ACL, Prague. 
D. Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In: Proceedings 
of ACL, pp. 263?270. 
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics. 33(2):201?228. 
D. Chiang, W. Wang and K. Knight. 2009. 11,001 
new features for statistical machine translation. In: 
Proc. NAACL HLT, pp. 218?226. 
K. W. Church and P. Hanks. 1990. Word association 
norms, mutual information, and lexicography. 
Computational Linguistics, 16(1):22?29. 
W. B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structure and Algo-
rithms. Prentice Hall. 
P. Fung. 1998. A statistical view on bilingual lexicon 
extraction: From parallel corpora to non-parallel 
corpora. In: Proceedings of AMTA, pp. 1?17. Oct. 
Langhorne, PA, USA. 
J. Gimenez and L. Marquez. 2009. Discriminative 
Phrase Selection for SMT. In: Goutte et al(ed.), 
Learning Machine Translation. MIT Press. 
K. Gimpel and N. A. Smith. 2008. Rich Source-Side 
Context for Statistical Machine Translation. In: 
Proceedings of WMT, Columbus, OH. 
Z. Harris. 1954. Distributional structure. Word, 
10(23): 146-162. 
Z. He, Q. Liu, and S. Lin. 2008. Improving Statistical 
Machine Translation using Lexicalized Rule Selec-
tion. In: Proceedings of COLING, Manchester, 
UK. 
D. Hindle. 1990. Noun classification from predicate-
argument structures. In: Proceedings of ACL. pp. 
268-275. Pittsburgh, PA. 
P. Koehn, F. Och, D. Marcu. 2003. Statistical Phrase-
Based Translation. In: Proceedings of HLT-
NAACL. pp. 127-133, Edmonton, Canada 
P.  Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In:  Proceedings of 
EMNLP, pp. 388?395. July, Barcelona, Spain. 
T. Landauer and S. T. Dumais. 1997. A solution to 
Plato?s problem: The Latent Semantic Analysis 
theory of the acquisition, induction, and representa-
tion of knowledge. Psychological Review. 104:211-
240. 
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. 
Khudanpur, L. Schwartz, W. Thornton, J. Weese 
and O. Zaidan, 2009. Joshua: An Open Source 
Toolkit for Parsing-based Machine Translation. In: 
Proceedings of the WMT.  March. Athens, Greece. 
D. Lin. 1998. Automatic retrieval and clustering of 
similar words. In: Proceedings of COLING/ACL-
98. pp. 768-774. Montreal, Canada.  
842
Q. Liu, Z. He, Y. Liu and S. Lin. 2008. Maximum 
Entropy based Rule Selection Model for Syntax-
based Statistical Machine Translation. In: Proceed-
ings of EMNLP, Honolulu, Hawaii. 
K. Lund, and C. Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28 (2), 203?208. 
A. Mauser, S. Hasan and H. Ney. 2009. Extending 
Statistical Machine Translation with Discrimina-
tive and Trigger-Based Lexicon Models. In: Pro-
ceedings of EMNLP, Singapore. 
F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL. 
Sapporo, Japan. 
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational 
Linguistics, 33 (2), 161?199. 
P. Pantel and D. Lin. 2002. Discovering word senses 
from text. In: Proceedings of ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, 
pp. 613?619. Edmonton, Canada. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, pp. 311?
318. July. Philadelphia, PA, USA. 
R. Rapp. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
Corpora. In: Proceedings of ACL, pp. 519?526. 
June. Maryland. 
G. Salton and M. J. McGill. 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill, New 
York. 
P. Turney. 2001. Mining the Web for synonyms: 
PMI-IR versus LSA on TOEFL. In: Proceedings of 
the Twelfth European Conference on Machine 
Learning, pp. 491?502, Berlin, Germany.  
D. Wu and P. Fung. 2009. Semantic Roles for SMT: 
A Hybrid Two-Pass Model. In: Proceedings of 
NAACL/HLT, Boulder, CO. 
D. Yuret and M. A. Yatbaz. 2009. The Noisy Channel 
Model for Unsupervised Word Sense Disambigua-
tion. In: Computational Linguistics. Vol. 1(1) 1-18. 
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In: Proceed-
ings of NAACL-HLT. Boston, MA. 
B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004. 
Phrase pair rescoring with term weighting for sta-
tistical machine translation. In Proceedings of 
EMNLP, pp. 206?213. July. Barcelona, Spain. 
 
843
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940?949,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Mixing Multiple Translation Models in Statistical Machine Translation
Majid Razmara1 George Foster2 Baskaran Sankaran1 Anoop Sarkar1
1 Simon Fraser University, 8888 University Dr., Burnaby, BC, Canada
{razmara,baskaran,anoop}@sfu.ca
2 National Research Council Canada, 283 Alexandre-Tache? Blvd, Gatineau, QC, Canada
george.foster@nrc.gc.ca
Abstract
Statistical machine translation is often faced
with the problem of combining training data
from many diverse sources into a single trans-
lation model which then has to translate sen-
tences in a new domain. We propose a novel
approach, ensemble decoding, which com-
bines a number of translation systems dynam-
ically at the decoding step. In this paper,
we evaluate performance on a domain adap-
tation setting where we translate sentences
from the medical domain. Our experimental
results show that ensemble decoding outper-
forms various strong baselines including mix-
ture models, the current state-of-the-art for do-
main adaptation in machine translation.
1 Introduction
Statistical machine translation (SMT) systems re-
quire large parallel corpora in order to be able to
obtain a reasonable translation quality. In statisti-
cal learning theory, it is assumed that the training
and test datasets are drawn from the same distribu-
tion, or in other words, they are from the same do-
main. However, bilingual corpora are only available
in very limited domains and building bilingual re-
sources in a new domain is usually very expensive.
It is an interesting question whether a model that is
trained on an existing large bilingual corpus in a spe-
cific domain can be adapted to another domain for
which little parallel data is present. Domain adap-
tation techniques aim at finding ways to adjust an
out-of-domain (OUT) model to represent a target do-
main (in-domain or IN).
Common techniques for model adaptation adapt
two main components of contemporary state-of-the-
art SMT systems: the language model and the trans-
lation model. However, language model adapta-
tion is a more straight-forward problem compared to
translation model adaptation, because various mea-
sures such as perplexity of adapted language models
can be easily computed on data in the target domain.
As a result, language model adaptation has been well
studied in various work (Clarkson and Robinson,
1997; Seymore and Rosenfeld, 1997; Bacchiani and
Roark, 2003; Eck et al, 2004) both for speech recog-
nition and for machine translation. It is also easier to
obtain monolingual data in the target domain, com-
pared to bilingual data which is required for transla-
tion model adaptation. In this paper, we focused on
adapting only the translation model by fixing a lan-
guage model for all the experiments. We expect do-
main adaptation for machine translation can be im-
proved further by combining orthogonal techniques
for translation model adaptation combined with lan-
guage model adaptation.
In this paper, a new approach for adapting the
translation model is proposed. We use a novel sys-
tem combination approach called ensemble decod-
ing in order to combine two or more translation
models with the goal of constructing a system that
outperforms all the component models. The strength
of this system combination method is that the sys-
tems are combined in the decoder. This enables
the decoder to pick the best hypotheses for each
span of the input. The main applications of en-
semble models are domain adaptation, domain mix-
ing and system combination. We have modified
Kriya (Sankaran et al, 2012), an in-house imple-
mentation of hierarchical phrase-based translation
system (Chiang, 2005), to implement ensemble de-
coding using multiple translation models.
We compare the results of ensemble decoding
with a number of baselines for domain adaptation.
In addition to the basic approach of concatenation of
in-domain and out-of-domain data, we also trained
a log-linear mixture model (Foster and Kuhn, 2007)
940
as well as the linear mixture model of (Foster et al,
2010) for conditional phrase-pair probabilities over
IN and OUT. Furthermore, within the framework of
ensemble decoding, we study and evaluate various
methods for combining translation tables.
2 Baselines
The natural baseline for model adaption is to con-
catenate the IN and OUT data into a single paral-
lel corpus and train a model on it. In addition to
this baseline, we have experimented with two more
sophisticated baselines which are based on mixture
techniques.
2.1 Log-Linear Mixture
Log-linear translation model (TM) mixtures are of
the form:
p(e?|f?) ? exp
( M?
m
?m log pm(e?|f?)
)
where m ranges over IN and OUT, pm(e?|f?) is an
estimate from a component phrase table, and each
?m is a weight in the top-level log-linear model, set
so as to maximize dev-set BLEU using minimum
error rate training (Och, 2003). We learn separate
weights for relative-frequency and lexical estimates
for both pm(e?|f?) and pm(f? |e?). Thus, for 2 compo-
nent models (from IN and OUT training corpora),
there are 4 ? 2 = 8 TM weights to tune. Whenever
a phrase pair does not appear in a component phrase
table, we set the corresponding pm(e?|f?) to a small
epsilon value.
2.2 Linear Mixture
Linear TM mixtures are of the form:
p(e?|f?) =
M?
m
?mpm(e?|f?)
Our technique for setting ?m is similar to that
outlined in Foster et al (2010). We first extract a
joint phrase-pair distribution p?(e?, f?) from the de-
velopment set using standard techniques (HMM
word alignment with grow-diag-and symmeteriza-
tion (Koehn et al, 2003)). We then find the set
of weights ?? that minimize the cross-entropy of the
mixture p(e?|f?) with respect to p?(e?, f?):
?? = argmax
?
?
e?,f?
p?(e?, f?) log
M?
m
?mpm(e?|f?)
For efficiency and stability, we use the EM algo-
rithm to find ??, rather than L-BFGS as in (Foster et
al., 2010). Whenever a phrase pair does not appear
in a component phrase table, we set the correspond-
ing pm(e?|f?) to 0; pairs in p?(e?, f?) that do not appear
in at least one component table are discarded. We
learn separate linear mixtures for relative-frequency
and lexical estimates for both p(e?|f?) and p(f? |e?).
These four features then appear in the top-level
model as usual ? there is no runtime cost for the lin-
ear mixture.
3 Ensemble Decoding
Ensemble decoding is a way to combine the exper-
tise of different models in one single model. The
current implementation is able to combine hierar-
chical phrase-based systems (Chiang, 2005) as well
as phrase-based translation systems (Koehn et al,
2003). However, the method can be easily extended
to support combining a number of heterogeneous
translation systems e.g. phrase-based, hierarchical
phrase-based, and/or syntax-based systems. This
section explains how such models can be combined
during the decoding.
Given a number of translation models which are
already trained and tuned, the ensemble decoder
uses hypotheses constructed from all of the models
in order to translate a sentence. We use the bottom-
up CKY parsing algorithm for decoding. For each
sentence, a CKY chart is constructed. The cells of
the CKY chart are populated with appropriate rules
from all the phrase tables of different components.
As in the Hiero SMT system (Chiang, 2005), the
cells which span up to a certain length (i.e. the max-
imum span length) are populated from the phrase-
tables and the rest of the chart uses glue rules as de-
fined in (Chiang, 2005).
The rules suggested from the component models
are combined in a single set. Some of the rules may
be unique and others may be common with other
component model rule sets, though with different
scores. Therefore, we need to combine the scores
of such common rules and assign a single score to
941
them. Depending on the mixture operation used for
combining the scores, we would get different mix-
ture scores. The choice of mixture operation will be
discussed in Section 3.1.
Figure 1 illustrates how the CKY chart is filled
with the rules. Each cell, covering a span, is popu-
lated with rules from all component models as well
as from cells covering a sub-span of it.
In the typical log-linear model SMT, the posterior
probability for each phrase pair (e?, f?) is given by:
p(e? | f?) ? exp
(
?
i
wi?i(e?, f?)
? ?? ?
w??
)
Ensemble decoding uses the same framework for
each individual system. Therefore, the score of a
phrase-pair (e?, f?) in the ensemble model is:
p(e? | f?) ? exp
(
w1 ? ?1? ?? ?
1st model
? w2 ? ?2? ?? ?
2nd model
? ? ? ?
)
where? denotes the mixture operation between two
or more model scores.
3.1 Mixture Operations
Mixture operations receive two or more scores
(probabilities) and return the mixture score (prob-
ability). In this section, we explore different options
for mixture operation and discuss some of the char-
acteristics of these mixture operations.
? Weighted Sum (wsum): in wsum the ensemble
probability is proportional to the weighted sum
of all individual model probabilities (i.e. linear
mixture).
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component mod-
els, M is the total number of them and ?i is the
weight for component i.
? Weighted Max (wmax): where the ensemble
score is the weighted max of all model scores.
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
? Model Switching (Switch): in model switch-
ing, each cell in the CKY chart gets populated
only by rules from one of the models and the
other models? rules are discarded. This is based
on the hypothesis that each component model
is an expert on certain parts of sentence. In this
method, we need to define a binary indicator
function ?(f? ,m) for each span and component
model to specify rules of which model to retain
for each span.
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each cell,
?(f? , n), could be based on:
? Max: for each cell, the model that has the
highest weighted best-rule score wins:
?(f? , n) = ?n max
e
(wn ? ?n(e?, f?))
? Sum: Instead of comparing only the
scores of the best rules, the model with
the highest weighted sum of the probabil-
ities of the rules wins. This sum has to
take into account the translation table limit
(ttl), on the number of rules suggested by
each model for each cell:
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
computed as:
p(e? | f?) =
M?
m
?(f? ,m) pm(e? | f?)
? Product (prod): in Product models or Prod-
uct of Experts (Hinton, 1999), the probability
of the ensemble model or a rule is computed as
the product of the probabilities of all compo-
nents (or equally the sum of log-probabilities,
i.e. log-linear mixture). Product models can
also make use of weights to control the contri-
bution of each component. These models are
942
Figure 1: The cells in the CKY chart are populated using rules from all component models and sub-span cells.
generally known as Logarithmic Opinion Pools
(LOPs) where:
p(e? | f?) ? exp
(
M?
m
?m (wm ? ?m)
)
Product models have been used in combining
LMs and TMs in SMT as well as some other
NLP tasks such as ensemble parsing (Petrov,
2010).
Each of these mixture operations has a specific
property that makes it work in specific domain adap-
tation or system combination scenarios. For in-
stance, LOPs may not be optimal for domain adapta-
tion in the setting where there are two or more mod-
els trained on heterogeneous corpora. As discussed
in (Smith et al, 2005), LOPs work best when all the
models accuracies are high and close to each other
with some degree of diversity. LOPs give veto power
to any of the component models and this perfectly
works for settings such as the one in (Petrov, 2010)
where a number of parsers are trained by changing
the randomization seeds but having the same base
parser and using the same training set. They no-
ticed that parsers trained using different randomiza-
tion seeds have high accuracies but there are some
diversities among them and they used product mod-
els for their advantage to get an even better parser.
We assume that each of the models is expert in some
parts and so they do not necessarily agree on cor-
rect hypotheses. In other words, product models (or
LOPs) tend to have intersection-style effects while
we are more interested in union-style effects.
In Section 4.2, we compare the BLEU scores of
different mixture operations on a French-English ex-
perimental setup.
3.2 Normalization
Since in log-linear models, the model scores are
not normalized to form probability distributions, the
scores that different models assign to each phrase-
pair may not be in the same scale. Therefore, mixing
their scores might wash out the information in one
(or some) of the models. We experimented with two
different ways to deal with this normalization issue.
A practical but inexact heuristic is to normalize the
scores over a shorter list. So the list of rules coming
from each model for a cell in CKY chart is normal-
ized before getting mixed with other phrase-table
rules. However, experiments showed changing the
scores with the normalized scores hurts the BLEU
score radically. So we use the normalized scores
only for pruning and the actual scores are intact.
We could also globally normalize the scores to ob-
tain posterior probabilities using the inside-outside
algorithm. However, we did not try it as the BLEU
scores we got using the normalization heuristic was
not promissing and it would impose a cost in de-
coding as well. More investigation on this issue has
been left for future work.
A more principled way is to systematically find
the most appropriate model weights that can avoid
this problem by scaling the scores properly. We
used a publicly available toolkit, CONDOR (Van-
den Berghen and Bersini, 2005), a direct optimizer
based on Powell?s algorithm, that does not require
943
explicit gradient information for the objective func-
tion. Component weights for each mixture operation
are optimized on the dev-set using CONDOR.
4 Experiments & Results
4.1 Experimental Setup
We carried out translation experiments using the Eu-
ropean Medicines Agency (EMEA) corpus (Tiede-
mann, 2009) as IN, and the Europarl (EP) corpus1 as
OUT, for French to English translation. The dev and
test sets were randomly chosen from the EMEA cor-
pus.2 The details of datasets used are summarized in
Table 1.
Dataset Sents
Words
French English
EMEA 11770 168K 144K
Europarl 1.3M 40M 37M
Dev 1533 29K 25K
Test 1522 29K 25K
Table 1: Training, dev and test sets for EMEA.
For the mixture baselines, we used a standard
one-pass phrase-based system (Koehn et al, 2003),
Portage (Sadat et al, 2005), with the following 7
features: relative-frequency and lexical translation
model (TM) probabilities in both directions; word-
displacement distortion model; language model
(LM) and word count. The corpus was word-aligned
using both HMM and IBM2 models, and the phrase
table was the union of phrases extracted from these
separate alignments, with a length limit of 7. It
was filtered to retain the top 20 translations for each
source phrase using the TM part of the current log-
linear model.
For ensemble decoding, we modified an in-house
implementation of hierarchical phrase-based sys-
tem, Kriya (Sankaran et al, 2012) which uses the
same features mentioned in (Chiang, 2005): for-
ward and backward relative-frequency and lexical
TM probabilities; LM; word, phrase and glue-rules
penalty. GIZA++(Och and Ney, 2000) has been used
for word alignment with phrase length limit of 7.
In both systems, feature weights were optimized
using MERT (Och, 2003) and with a 5-gram lan-
1www.statmt.org/europarl
2Please contact the authors to access the data-sets.
guage model and Kneser-Ney smoothing was used
in all the experiments. We used SRILM (Stolcke,
2002) as the langugage model toolkit. Fixing the
language model allows us to compare various trans-
lation model combination techniques.
4.2 Results
Table 2 shows the results of the baselines. The first
group are the baseline results on the phrase-based
system discussed in Section 2 and the second group
are those of our hierarchical MT system. Since the
Hiero baselines results were substantially better than
those of the phrase-based model, we also imple-
mented the best-performing baseline, linear mixture,
in our Hiero-style MT system and in fact it achieves
the hights BLEU score among all the baselines as
shown in Table 2. This baseline is run three times
the score is averaged over the BLEU scores with
standard deviation of 0.34.
Baseline PBS Hiero
IN 31.84 33.69
OUT 24.08 25.32
IN + OUT 31.75 33.76
LOGLIN 32.21 ?
LINMIX 33.81 35.57
Table 2: The results of various baselines implemented in
a phrase-based (PBS) and a Hiero SMT on EMEA.
Table 3 shows the results of ensemble decoding
with different mixture operations and model weight
settings. Each mixture operation has been evalu-
ated on the test-set by setting the component weights
uniformly (denoted by uniform) and by tuning the
weights using CONDOR (denoted by tuned) on a
held-out set. The tuned scores (3rd column in Ta-
ble 3) are averages of three runs with different initial
points as in Clark et al (2011). We also reported the
BLEU scores when we applied the span-wise nor-
malization heuristic. All of these mixture operations
were able to significantly improve over the concate-
nation baseline. In particular, Switching:Max could
gain up to 2.2 BLEU points over the concatenation
baseline and 0.39 BLEU points over the best per-
forming baseline (i.e. linear mixture model imple-
mented in Hiero) which is statistically significant
based on Clark et al (2011) (p = 0.02).
Prod when using with uniform weights gets the
944
Mixture Operation Uniform Tuned Norm.
WMAX 35.39 35.47 (s=0.03) 35.47
WSUM 35.35 35.53 (s=0.04) 35.45
SWITCHING:MAX 35.93 35.96 (s=0.01) 32.62
SWITCHING:SUM 34.90 34.72 (s=0.23) 34.90
PROD 33.93 35.24 (s=0.05) 35.02
Table 3: The results of ensemble decoding on EMEA for Fr2En when using uniform weights, tuned weights and
normalization heuristic. The tuned BLEU scores are averaged over three runs with multiple initial points, as in (Clark
et al, 2011), with the standard deviations in brackets .
lowest score among the mixture operations, how-
ever after tuning, it learns to bias the weights to-
wards one of the models and hence improves by
1.31 BLEU points. Although Switching:Sum outper-
forms the concatenation baseline, it is substantially
worse than other mixture operations. One explana-
tion that Switching:Max is the best performing op-
eration and Switching:Sum is the worst one, despite
their similarities, is that Switching:Max prefers more
peaked distributions while Switching:Sum favours a
model that has fewer hypotheses for each span.
An interesting observation based on the results in
Table 3 is that uniform weights are doing reasonably
well given that the component weights are not opti-
mized and therefore model scores may not be in the
same scope (refer to discussion in ?3.2). We suspect
this is because a single LM is shared between both
models. This shared component controls the vari-
ance of the weights in the two models when com-
bined with the standard L-1 normalization of each
model?s weights and hence prohibits models to have
too varied scores for the same input. Though, it may
not be the case when multiple LMs are used which
are not shared.
Two sample sentences from the EMEA test-set
along with their translations by the IN, OUT and En-
semble models are shown in Figure 2. The boxes
show how the Ensemble model is able to use n-
grams from the IN and OUT models to construct
a better translation than both of them. In the first
example, there are two OOVs one for each of the
IN and OUT models. Our approach is able to re-
solve the OOV issues by taking advantage of the
other model?s presence. Similarly, the second exam-
ple shows how ensemble decoding improves lexical
choices as well as word re-orderings.
5 Related Work
5.1 Domain Adaptation
Early approaches to domain adaptation involved in-
formation retrieval techniques where sentence pairs
related to the target domain were retrieved from the
training corpus using IR methods (Eck et al, 2004;
Hildebrand et al, 2005). Foster et al (2010), how-
ever, uses a different approach to select related sen-
tences from OUT. They use language model per-
plexities from IN to select relavant sentences from
OUT. These sentences are used to enrich the IN
training set.
Other domain adaptation methods involve tech-
niques that distinguish between general and domain-
specific examples (Daume? and Marcu, 2006). Jiang
and Zhai (2007) introduce a general instance weight-
ing framework for model adaptation. This approach
tries to penalize misleading training instances from
OUT and assign more weight to IN-like instances
than OUT instances. Foster et al (2010) propose a
similar method for machine translation that uses fea-
tures to capture degrees of generality. Particularly,
they include the output from an SVM classifier that
uses the intersection between IN and OUT as pos-
itive examples. Unlike previous work on instance
weighting in machine translation, they use phrase-
level instances instead of sentences.
A large body of work uses interpolation tech-
niques to create a single TM/LM from interpolating
a number of LMs/TMs. Two famous examples of
such methods are linear mixtures and log-linear mix-
tures (Koehn and Schroeder, 2007; Civera and Juan,
2007; Foster and Kuhn, 2007) which were used as
baselines and discussed in Section 2. Other meth-
ods include using self-training techniques to exploit
monolingual in-domain data (Ueffing et al, 2007;
945
SOURCE ame?norrhe?e , menstruations irre?gulie`res
REF amenorrhoea , irregular menstruation
IN amenorrhoea , menstruations irre?gulie`res
OUT ame?norrhe?e , irregular menstruation
ENSEMBLE amenorrhoea , irregular menstruation
SOURCE le traitement par naglazyme doit e?tre supervise? par un me?decin ayant l? expe?rience de
la prise en charge des patients atteints de mps vi ou d? une autre maladie me?tabolique
he?re?ditaire .
REF naglazyme treatment should be supervised by a physician experienced in the manage-
ment of patients with mps vi or other inherited metabolic diseases .
IN naglazyme treatment should be supervise? by a doctor the with
in the management of patients with mps vi or other hereditary metabolic disease .
OUT naglazyme ?s treatment must be supervised by a doctor with the experience of the care
of patients with mps vi. or another disease hereditary metabolic .
ENSEMBLE naglazyme treatment should be supervised by a physician experienced
in the management of patients with mps vi or other hereditary metabolic disease .
Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems.
Bertoldi and Federico, 2009). In this approach, a
system is trained on the parallel OUT and IN data
and it is used to translate the monolingual IN data
set. Iteratively, most confident sentence pairs are se-
lected and added to the training corpus on which a
new system is trained.
5.2 System Combination
Tackling the model adaptation problem using sys-
tem combination approaches has been experimented
in various work (Koehn and Schroeder, 2007; Hilde-
brand and Vogel, 2009). Among these approaches
are sentence-based, phrase-based and word-based
output combination methods. In a similar approach,
Koehn and Schroeder (2007) use a feature of the fac-
tored translation model framework in Moses SMT
system (Koehn and Schroeder, 2007) to use multiple
alternative decoding paths. Two decoding paths, one
for each translation table (IN and OUT), were used
during decoding. The weights are set with minimum
error rate training (Och, 2003).
Our work is closely related to Koehn and
Schroeder (2007) but uses a different approach to
deal with multiple translation tables. The Moses
SMT system implements (Koehn and Schroeder,
2007) and can treat multiple translation tables in
two different ways: intersection and union. In in-
tersection, for each span only the hypotheses would
be used that are present in all phrase tables. For
each set of hypothesis with the same source and
target phrases, a new hypothesis is created whose
feature-set is the union of feature sets of all corre-
sponding hypotheses. Union, on the other hand, uses
hypotheses from all the phrase tables. The feature
set of these hypotheses are expanded to include one
feature set for each table. However, for the corre-
sponding feature values of those phrase-tables that
did not have a particular phrase-pair, a default log
probability value of 0 is assumed (Bertoldi and Fed-
erico, 2009) which is counter-intuitive as it boosts
the score of hypotheses with phrase-pairs that do not
belong to all of the translation tables.
Our approach is different from Koehn and
Schroeder (2007) in a number of ways. Firstly, un-
like the multi-table support of Moses which only
supports phrase-based translation table combination,
our approach supports ensembles of both hierarchi-
cal and phrase-based systems. With little modifica-
tion, it can also support ensemble of syntax-based
systems with the other two state-of-the-art SMT sys-
946
tems. Secondly, our combining method uses the
union option, but instead of preserving the features
of all phrase-tables, it only combines their scores
using various mixture operations. This enables us
to experiment with a number of different opera-
tions as opposed to sticking to only one combination
method. Finally, by avoiding increasing the number
of features we can add as many translation models
as we need without serious performance drop. In
addition, MERT would not be an appropriate opti-
mizer when the number of features increases a cer-
tain amount (Chiang et al, 2008).
Our approach differs from the model combina-
tion approach of DeNero et al (2010), a generaliza-
tion of consensus or minimum Bayes risk decoding
where the search space consists of those of multi-
ple systems, in that model combination uses forest
of derivations of all component models to do the
combination. In other words, it requires all compo-
nent models to fully decode each sentence, compute
n-gram expectations from each component model
and calculate posterior probabilities over transla-
tion derivations. While, in our approach we only
use partial hypotheses from component models and
the derivation forest is constructed by the ensemble
model. A major difference is that in the model com-
bination approach the component search spaces are
conjoined and they are not intermingled as opposed
to our approach where these search spaces are inter-
mixed on spans. This enables us to generate new
sentences that cannot be generated by component
models. Furthermore, various combination methods
can be explored in our approach. Finally, main tech-
niques used in this work are orthogonal to our ap-
proach such as Minimum Bayes Risk decoding, us-
ing n-gram features and tuning using MERT.
Finally, our work is most similar to that of
Liu et al (2009) where max-derivation and max-
translation decoding have been used. Max-
derivation finds a derivation with highest score and
max-translation finds the highest scoring translation
by summing the score of all derivations with the
same yield. The combination can be done in two
levels: translation-level and derivation-level. Their
derivation-level max-translation decoding is similar
to our ensemble decoding with wsum as the mixture
operation. We did not restrict ourself to this par-
ticular mixture operation and experimented with a
number of different mixing techniques and as Ta-
ble 3 shows we could improve over wsum in our
experimental setup. Liu et al (2009) used a mod-
ified version of MERT to tune max-translation de-
coding weights, while we use a two-step approach
using MERT for tuning each component model sep-
arately and then using CONDOR to tune component
weights on top of them.
6 Conclusion & Future Work
In this paper, we presented a new approach for do-
main adaptation using ensemble decoding. In this
approach a number of MT systems are combined at
decoding time in order to form an ensemble model.
The model combination can be done using various
mixture operations. We showed that this approach
can gain up to 2.2 BLEU points over its concatena-
tion baseline and 0.39 BLEU points over a powerful
mixture model.
Future work includes extending this approach to
use multiple translation models with multiple lan-
guage models in ensemble decoding. Different
mixture operations can be investigated and the be-
haviour of each operation can be studied in more
details. We will also add capability of support-
ing syntax-based ensemble decoding and experi-
ment how a phrase-based system can benefit from
syntax information present in a syntax-aware MT
system. Furthermore, ensemble decoding can be ap-
plied on domain mixing settings in which develop-
ment sets and test sets include sentences from dif-
ferent domains and genres, and this is a very suit-
able setting for an ensemble model which can adapt
to new domains at test time. In addition, we can
extend our approach by applying some of the tech-
niques used in other system combination approaches
such as consensus decoding, using n-gram features,
tuning using forest-based MERT, among other pos-
sible extensions.
Acknowledgments
This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Fac-
ulty Award to the last author. We would like to
thank Philipp Koehn and the anonymous reviewers
for their valuable comments. We also thank the de-
velopers of GIZA++ and Condor which we used for
our experiments.
947
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In Acoustics, Speech, and
Signal Processing, 2003. Proceedings. (ICASSP ?03).
2003 IEEE International Conference on, volume 1,
pages I?224 ? I?227 vol.1, april.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 182?189, Stroudsburg, PA, USA. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. ACL.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Mor-
ristown, NJ, USA. ACL.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, StatMT ?07, pages
177?180, Stroudsburg, PA, USA. ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume 2,
HLT ?11, pages 176?181. ACL.
P. Clarkson and A. Robinson. 1997. Language model
adaptation using mixtures and an exponentially decay-
ing cache. In Proceedings of the 1997 IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP ?97)-Volume 2 - Volume 2,
ICASSP ?97, pages 799?, Washington, DC, USA.
IEEE Computer Society.
Hal Daume?, III and Daniel Marcu. 2006. Domain
adaptation for statistical classifiers. J. Artif. Int. Res.,
26:101?126, May.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 975?983, Stroudsburg, PA, USA. ACL.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In In Pro-
ceedings of LREC.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for smt. In Proceedings of the Second
Workshop on Statistical Machine Translation, StatMT
?07, pages 128?135, Stroudsburg, PA, USA. ACL.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 451?
459, Stroudsburg, PA, USA. ACL.
Almut Silja Hildebrand and Stephan Vogel. 2009. CMU
system combination for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 47?50, Stroudsburg, PA, USA.
ACL.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th EAMT
2005, Budapest, Hungary, May.
Geoffrey E. Hinton. 1999. Products of experts. In Artifi-
cial Neural Networks, 1999. ICANN 99. Ninth Interna-
tional Conference on (Conf. Publ. No. 470), volume 1,
pages 1?6.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264?271, Prague, Czech
Republic, June. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 224?
227, Stroudsburg, PA, USA. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, pages 127?133, Edmonton, May.
NAACL.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ?09, pages
576?584, Stroudsburg, PA, USA. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual Meet-
ing of the ACL, pages 440?447, Hongkong, China, Oc-
tober.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the ACL, Sapporo, July. ACL.
948
Slav Petrov. 2010. Products of random latent variable
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 19?27, Stroudsburg, PA, USA. ACL.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Joel Martin, and Aaron Tikuisis. 2005.
Portage: A phrase-based machine translation system.
In In Proceedings of the ACL Worskhop on Building
and Using Parallel Texts, Ann Arbor. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, 97(97), April.
Kristie Seymore and Ronald Rosenfeld. 1997. Us-
ing story topics for language model adaptation. In
George Kokkinakis, Nikos Fakotakis, and Evangelos
Dermatas, editors, EUROSPEECH. ISCA.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 18?25, Stroudsburg, PA, USA. ACL.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286.
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 25?32, Prague, Czech Republic, June. ACL.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175,
September.
949
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285?1293,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Vector Space Model for Adaptation in Statistical Machine Translation
Boxing Chen, Roland Kuhn and George Foster
National Research Council Canada
first.last@nrc-cnrc.gc.ca
Abstract
This paper proposes a new approach to
domain adaptation in statistical machine
translation (SMT) based on a vector space
model (VSM). The general idea is first to
create a vector profile for the in-domain
development (?dev?) set. This profile
might, for instance, be a vector with a di-
mensionality equal to the number of train-
ing subcorpora; each entry in the vector re-
flects the contribution of a particular sub-
corpus to all the phrase pairs that can be
extracted from the dev set. Then, for
each phrase pair extracted from the train-
ing data, we create a vector with features
defined in the same way, and calculate its
similarity score with the vector represent-
ing the dev set. Thus, we obtain a de-
coding feature whose value represents the
phrase pair?s closeness to the dev. This is
a simple, computationally cheap form of
instance weighting for phrase pairs. Ex-
periments on large scale NIST evaluation
data show improvements over strong base-
lines: +1.8 BLEU on Arabic to English
and +1.4 BLEU on Chinese to English
over a non-adapted baseline, and signifi-
cant improvements in most circumstances
over baselines with linear mixture model
adaptation. An informal analysis suggests
that VSM adaptation may help in making
a good choice among words with the same
meaning, on the basis of style and genre.
1 Introduction
The translation models of a statistical machine
translation (SMT) system are trained on parallel
data. Usage of language and therefore the best
translation practice differs widely across genres,
topics, and dialects, and even depends on a partic-
ular author?s or publication?s style; the word ?do-
main? is often used to indicate a particular combi-
nation of all these factors. Unless there is a per-
fect match between the training data domain and
the (test) domain in which the SMT system will
be used, one can often get better performance by
adapting the system to the test domain.
Domain adaptation is an active topic in the nat-
ural language processing (NLP) research commu-
nity. Its application to SMT systems has recently
received considerable attention. Approaches that
have been tried for SMT model adaptation include
mixture models, transductive learning, data selec-
tion, instance weighting, and phrase sense disam-
biguation, etc.
Research on mixture models has considered
both linear and log-linear mixtures. Both were
studied in (Foster and Kuhn, 2007), which con-
cluded that the best approach was to combine sub-
models of the same type (for instance, several
different TMs or several different LMs) linearly,
while combining models of different types (for in-
stance, a mixture TM with a mixture LM) log-
linearly. (Koehn and Schroeder, 2007), instead,
opted for combining the sub-models directly in the
SMT log-linear framework.
In transductive learning, an MT system trained
on general domain data is used to translate in-
domain monolingual data. The resulting bilingual
sentence pairs are then used as additional train-
ing data (Ueffing et al, 2007; Chen et al, 2008;
Schwenk, 2008; Bertoldi and Federico, 2009).
Data selection approaches (Zhao et al, 2004;
Hildebrand et al, 2005; Lu? et al, 2007; Moore
and Lewis, 2010; Axelrod et al, 2011) search for
bilingual sentence pairs that are similar to the in-
domain ?dev? data, then add them to the training
data.
Instance weighting approaches (Matsoukas et
al., 2009; Foster et al, 2010; Huang and Xiang,
2010; Phillips and Brown, 2011; Sennrich, 2012)1285
typically use a rich feature set to decide on weights
for the training data, at the sentence or phrase pair
level. For example, a sentence from a subcorpus
whose domain is far from that of the dev set would
typically receive a low weight, but sentences in
this subcorpus that appear to be of a general na-
ture might receive higher weights.
The 2012 JHU workshop on Domain Adapta-
tion for MT 1 proposed phrase sense disambigua-
tion (PSD) for translation model adaptation. In
this approach, the context of a phrase helps the
system to find the appropriate translation.
In this paper, we propose a new instance weight-
ing approach to domain adaptation based on a vec-
tor space model (VSM). As in (Foster et al, 2010),
this approach works at the level of phrase pairs.
However, the VSM approach is simpler and more
straightforward. Instead of using word-based fea-
tures and a computationally expensive training
procedure, we capture the distributional properties
of each phrase pair directly, representing it as a
vector in a space which also contains a representa-
tion of the dev set. The similarity between a given
phrase pair?s vector and the dev set vector be-
comes a feature for the decoder. It rewards phrase
pairs that are in some sense closer to those found
in the dev set, and punishes the rest. In initial ex-
periments, we tried three different similarity func-
tions: Bhattacharyya coefficient, Jensen-Shannon
divergency, and cosine measure. They all enabled
VSM adaptation to beat the non-adaptive baseline,
but Bhattacharyya similarity worked best, so we
adopted it for the remaining experiments.
The vector space used by VSM adaptation can
be defined in various ways. In the experiments
described below, we chose a definition that mea-
sures the contribution (to counts of a given phrase
pair, or to counts of all phrase pairs in the dev
set) of each training subcorpus. Thus, the vari-
ant of VSM adaptation tested here bears a super-
ficial resemblance to domain adaptation based on
mixture models for TMs, as in (Foster and Kuhn,
2007), in that both approaches rely on information
about the subcorpora from which the data origi-
nate. However, a key difference is that in this pa-
per we explicitly capture each phrase pair?s dis-
tribution across subcorpora, and compare it to the
aggregated distribution of phrase pairs in the dev
set. In mixture models, a phrase pair?s distribu-
1http://www.clsp.jhu.edu/workshops/archive/ws-
12/groups/dasmt
tion across subcorpora is captured only implicitly,
by probabilities that reflect the prevalence of the
pair within each subcorpus. Thus, VSM adapta-
tion occurs at a much finer granularity than mix-
ture model adaptation. More fundamentally, there
is nothing about the VSM idea that obliges us to
define the vector space in terms of subcorpora.
For instance, we could cluster the words in the
source language into S clusters, and the words in
the target language into T clusters. Then, treat-
ing the dev set and each phrase pair as a pair of
bags of words (a source bag and a target bag) one
could represent each as a vector of dimension S +
T, with entries calculated from the counts associ-
ated with the S + T clusters (in a way similar to
that described for phrase pairs below). The (dev,
phrase pair) similarity would then be independent
of the subcorpora. One can think of several other
ways of defining the vector space that might yield
even better results than those reported here. Thus,
VSM adaptation is not limited to the variant of it
that we tested in our experiments.
2 Vector space model adaptation
Vector space models (VSMs) have been widely
applied in many information retrieval and natural
language processing applications. For instance, to
compute the sense similarity between terms, many
researchers extract features for each term from its
context in a corpus, define a VSM and then ap-
ply similarity functions (Hindle, 1990; Lund and
Burgess, 1996; Lin, 1998; Turney, 2001).
In our experiments, we exploited the fact that
the training data come from a set of subcorpora.
For instance, the Chinese-English training data are
made up of 14 subcorpora (see section 3 below).
Suppose we have C subcorpora. The domain vec-
tor for a phrase-pair (f, e) is defined as
V (f, e) =< w1(f, e), ...wi(f, e), ..., wC(f, e) >,
(1)
where wi(f, e) is a standard tf ? idf weight, i.e.
wi(f, e) = tfi (f, e) ? idf (f, e) . (2)
To avoid a bias towards longer corpora, we nor-
malize the raw joint count ci(f, e) in the corpus
si by dividing by the maximum raw count of any
phrase pair extracted in the corpus si. Let1286
tfi (f, e) =
ci (f, e)
max {ci (fj , ek) , (fj , ek) ? si}
.
(3)
The idf (f, e) is the inverse document fre-
quency: a measure of whether the phrase-pair
(f, e) is common or rare across all subcorpora. We
use the standard formula:
idf (f, e) = log
( C
df (f, e) + ?
)
, (4)
where df(f, e) is the number of subcorpora that
(f, e) appears in, and ? is an empirically deter-
mined smoothing term.
For the in-domain dev set, we first run word
alignment and phrases extracting in the usual way
for the dev set, then sum the distribution of each
phrase pair (fj , ek) extracted from the dev data
across subcorpora to represent its domain informa-
tion. The dev vector is thus
V (dev) =< w1(dev), . . . , wC(dev) >, (5)
where
wi(dev) =
j=J?
j=0
k=K?
k=0
cdev (fj , ek)wi(fj , ek) (6)
J,K are the total numbers of source/target
phrases extracted from the dev data respectively.
cdev (fj , ek) is the joint count of phrase pair fj , ek
found in the dev set.
The vector can also be built with other features
of the phrase pair. For instance, we could replace
the raw joint count ci(f, e) in Equation 3 with the
raw marginal count of phrase pairs (f, e). There-
fore, even within the variant of VSM adaptation
we focus on in this paper, where the definition of
the vector space is based on the existence of sub-
corpora, one could utilize other definitions of the
vectors of the similarity function than those we uti-
lized in our experiments.
2.1 Vector similarity functions
VSM uses the similarity score between the vec-
tor representing the in-domain dev set and the vec-
tor representing each phrase pair as a decoder fea-
ture. There are many similarity functions we could
have employed for this purpose (Cha, 2007). We
tested three commonly-used functions: the Bhat-
tacharyya coefficient (BC) (Bhattacharyya, 1943;
Kazama et al, 2010), the Jensen-Shannon diver-
gence (JSD), and the cosine measure. According
to (Cha, 2007), these belong to three different fam-
ilies of similarity functions: the Fidelity family,
the Shannon?s entropy family, and the inner Prod-
uct family respectively. It was BC similarity that
yielded the best performance, and that we ended
up using in subsequent experiments.
To map the BC score onto a range from 0 to
1, we first normalize each weight in the vector by
dividing it by the sum of the weights. Thus, we get
the probability distribution of a phrase pair or the
phrase pairs in the dev data across all subcorpora:
pi(f, e) =
wi(f, e)?j=C
j=1 wj(f, e)
(7)
pi(dev) =
wi(dev)?j=C
j=1 wj(dev)
(8)
To further improve the similarity score, we ap-
ply absolute discounting smoothing when calcu-
lating the probability distributions pi(f, e). We
subtract a discounting value ? from the non-zero
pi(f, e), and equally allocate the remaining proba-
bility mass to the zero probabilities. We carry out
the same smoothing for the probability distribu-
tions pi(dev). The smoothing constant ? is deter-
mined empirically on held-out data.
The Bhattacharyya coefficient (BC) is defined
as follows:
BC(dev; f, e) =
i=C?
i=0
?
pi(dev) ? pi(f, e) (9)
The other two similarity functions we also
tested are JSD and cosine (Cos). They are defined
as follows:
JSD(dev; f, e) = (10)
1
2[
i=C?
i=1
pi(dev) log
2pi(dev)
pi(dev) + pi(f, e)
+
i=C?
i=1
pi(f, e) log
2pi(f, e)
pi(dev) + pi(f, e)
]
Cos(dev; f, e) =
?
i pi(dev) ? pi (f, e)??
i p2i (dev)
??
i p2i (f, e)
(11)1287
corpus # segs # en tok % genres
fbis 250K 10.5M 3.7 nw
financial 90K 2.5M 0.9 fin
gale bc 79K 1.3M 0.5 bc
gale bn 75K 1.8M 0.6 bn ng
gale nw 25K 696K 0.2 nw
gale wl 24K 596K 0.2 wl
hkh 1.3M 39.5M 14.0 hans
hkl 400K 9.3M 3.3 legal
hkn 702K 16.6M 5.9 nw
isi 558K 18.0M 6.4 nw
lex&ne 1.3M 2.0M 0.7 lex
other nw 146K 5.2M 1.8 nw
sinorama 282K 10.0M 3.5 nw
un 5.0M 164M 58.2 un
TOTAL 10.1M 283M 100.0 (all)
devtest
tune 1,506 161K nw wl
NIST06 1,664 189K nw bng
NIST08 1,357 164K nw wl
Table 1: NIST Chinese-English data. In the
genres column: nw=newswire, bc=broadcast
conversation, bn=broadcast news, wl=weblog,
ng=newsgroup, un=UN proc., bng = bn & ng.
3 Experiments
3.1 Data setting
We carried out experiments in two different set-
tings, both involving data from NIST Open MT
2012.2 The first setting is based on data from
the Chinese to English constrained track, compris-
ing about 283 million English running words. We
manually grouped the training data into 14 corpora
according to genre and origin. Table 1 summa-
rizes information about the training, development
and test sets; we show the sizes of the training sub-
corpora in number of words as a percentage of all
training data. Most training subcorpora consist of
parallel sentence pairs. The isi and lex&ne cor-
pora are exceptions: the former is extracted from
comparable data, while the latter is a lexicon that
includes many named entities. The development
set (tune) was taken from the NIST 2005 evalua-
tion set, augmented with some web-genre material
reserved from other NIST corpora.
The second setting uses NIST 2012 Arabic to
English data, but excludes the UN data. There are
about 47.8 million English running words in these
2http://www.nist.gov/itl/iad/mig/openmt12.cfm
corpus # segs # en toks % gen
gale bc 57K 1.6M 3.3 bc
gale bn 45K 1.2M 2.5 bn
gale ng 21K 491K 1.0 ng
gale nw 17K 659K 1.4 nw
gale wl 24K 590K 1.2 wl
isi 1,124K 34.7M 72.6 nw
other nw 224K 8.7M 18.2 nw
TOTAL 1,512K 47.8M 100.0 (all)
devtest
NIST06 1,664 202K nwl
NIST08 1,360 205K nwl
NIST09 1,313 187K nwl
Table 2: NIST Arabic-English data. In the gen
(genres) column: nw=newswire, bc=broadcast
conversation, bn=broadcast news, ng=newsgroup,
wl=weblog, nwl = nw & wl.
training data. We manually grouped the training
data into 7 groups according to genre and origin.
Table 2 summarizes information about the train-
ing, development and test sets. Note that for this
language pair, the comparable isi data represent a
large proportion of the training data: 72% of the
English words. We use the evaluation sets from
NIST 2006, 2008, and 2009 as our development
set and two test sets, respectively.
3.2 System
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007). Each corpus was word-aligned using
IBM2, HMM, and IBM4 models, and the phrase
table was the union of phrase pairs extracted from
these separate alignments, with a length limit of
7. The translation model (TM) was smoothed in
both directions with KN smoothing (Chen et al,
2011). We use the hierarchical lexicalized reorder-
ing model (RM) (Galley and Manning, 2008), with
a distortion limit of 7. Other features include lex-
ical weighting in both directions, word count, a
distance-based RM, a 4-gram LM trained on the
target side of the parallel data, and a 6-gram En-
glish Gigaword LM. The system was tuned with
batch lattice MIRA (Cherry and Foster, 2012).
3.3 Results
For the baseline, we simply concatenate all train-
ing data. We have also compared our approach
to two widely used TM domain adaptation ap-1288
proaches. One is the log-linear combination
of TMs trained on each subcorpus (Koehn and
Schroeder, 2007), with weights of each model
tuned under minimal error rate training using
MIRA. The other is a linear combination of TMs
trained on each subcorpus, with the weights of
each model learned with an EM algorithm to max-
imize the likelihood of joint empirical phrase pair
counts for in-domain dev data. For details, refer to
(Foster and Kuhn, 2007).
The value of ? and ? (see Eq 4 and Section 2.1)
are determined by the performance on the dev
set of the Arabic-to-English system. For both
Arabic-to-English and Chinese-to-English exper-
iment, these values obtained on Arabic dev were
used to obtain the results below: ? was set to 8,
and ? was set to 0.01. (Later, we ran an exper-
iment on Chinese-to-English with ? and ? tuned
specifically for that language pair, but the perfor-
mance for the Chinese-English system only im-
proved by a tiny, insignificant amount).
Our metric is case-insensitive IBM BLEU (Pa-
pineni et al, 2002), which performs matching of
n-grams up to n = 4; we report BLEU scores av-
eraged across both test sets NIST06 and NIST08
for Chinese; NIST08 and NIST09 for Arabic.
Following (Koehn, 2004), we use the bootstrap-
resampling test to do significance testing. In ta-
bles 3 to 5, * and ** denote significant gains over
the baseline at p < 0.05 and p < 0.01 levels, re-
spectively.
We first compare the performance of differ-
ent similarity functions: cosine (COS), Jensen-
Shannon divergence (JSD) and Bhattacharyya co-
efficient (BC). The results are shown in Table 3.
All three functions obtained improvements. Both
COS and BC yield statistically significant im-
provements over the baseline, with BC performing
better than COS by a further statistically signifi-
cant margin. The Bhattacharyya coefficient is ex-
plicitly designed to measure the overlap between
the probability distributions of two statistical sam-
ples or populations, which is precisely what we are
trying to do here: we are trying to reward phrase
pairs whose distribution is similar to that of the
dev set. Thus, its superior performance in these
experiments is not unexpected.
In the next set of experiments, we compared
VSM adaptation using the BC similarity function
with the baseline which concatenates all training
data and with log-linear and linear TM mixtures
system Chinese Arabic
baseline 31.7 46.8
COS 32.3* 47.8**
JSD 32.1 47.1
BC 33.0** 48.4**
Table 3: Comparison of different similarity func-
tions. * and ** denote significant gains over the
baseline at p < 0.05 and p < 0.01 levels, respec-
tively.
system Chinese Arabic
baseline 31.7 46.8
loglinear tm 28.4 44.5
linear tm 32.7** 47.5**
vsm, BC 33.0** 48.4**
Table 4: Results for variants of adaptation.
whose components are based on subcorpora. Ta-
ble 4 shows that log-linear combination performs
worse than the baseline: the tuning algorithm
failed to optimize the log-linear combination even
on dev set. For Chinese, the BLEU score of the
dev set on the baseline system is 27.3, while on
the log-linear combination system, it is 24.0; for
Arabic, the BLEU score of the dev set on the base-
line system is 46.8, while on the log-linear com-
bination system, it is 45.4. We also tried adding
the global model to the loglinear combination and
it didn?t improve over the baseline for either lan-
guage pair. Linear mixture was significantly better
than the baseline at the p < 0.01 level for both lan-
guage pairs. Since our approach, VSM, performed
better than the linear mixture for both pairs, it is of
course also significantly better than the baseline at
the p < 0.01 level.
This raises the question: is VSM performance
significantly better than that of a linear mixture of
TMs? The answer (not shown in the table) is that
for Arabic to English, VSM performance is bet-
ter than linear mixture at the p < 0.01 level. For
Chinese to English, the argument for the superi-
ority of VSM over linear mixture is less convinc-
ing: there is significance at the p < 0.05 for one
of the two test sets (NIST06) but not for the other
(NIST08). At any rate, these results establish that
VSM adaptation is clearly superior to linear mix-
ture TM adaptation, for one of the two language
pairs.
In Table 4, the VSM results are based on the1289
system Chinese Arabic
baseline 31.7 46.8
linear tm 32.7** 47.5**
vsm, joint 33.0** 48.4**
vsm, src-marginal 32.2* 47.3*
vsm, tgt-marginal 32.6** 47.6**
vsm, src+tgt (2 feat.) 32.7** 48.2**
vsm, joint+src (2 feat.) 32.9** 48.4**
vsm, joint+tgt (2 feat.) 32.9** 48.4**
vsm, joint+src+tgt (3 feat.) 33.1** 48.6**
Table 5: Results for adaptation based on joint or
maginal counts.
vector of the joint counts of the phrase pair. In
the next experiment, we replace the joint counts
with the source or target marginal counts. In Ta-
ble 5, we first show the results based on source
and target marginal counts, then the results of us-
ing feature sets drawn from three decoder VSM
features: a joint count feature, a source marginal
count feature, and a target marginal count fea-
ture. For instance, the last row shows the results
when all three features are used (with their weights
tuned by MIRA). It looks as though the source and
target marginal counts contain useful information.
The best performance is obtained by combining all
three sources of information. The 3-feature ver-
sion of VSM yields +1.8 BLEU over the baseline
for Arabic to English, and +1.4 BLEU for Chinese
to English.
When we compared two sets of results in Ta-
ble 4, the joint count version of VSM and lin-
ear mixture of TMs, we found that for Arabic to
English, VSM performance is better than linear
mixture at the p < 0.01 level; the Chinese to
English significance test was inconclusive (VSM
found to be superior to linear mixture at p < 0.05
for NIST06 but not for NIST08). We now have
somewhat better results for the 3-feature version
of VSM shown in Table 5. How do these new re-
sults affect the VSM vs. linear mixture compari-
son? Naturally, the conclusions for Arabic don?t
change. For Chinese, 3-feature VSM is now su-
perior to linear mixture at p < 0.01 on NIST06
test set, but 3-feature VSM still doesn?t have a sta-
tistically significant edge over linear mixture on
NIST08 test set. A fair summary would be that 3-
feature VSM adaptation is decisively superior to
linear mixture adaptation for Arabic to English,
and highly competitive with linear mixture adap-
tation for Chinese to English.
Our last set of experiments examined the ques-
tion: when added to a system that already has
some form of linear mixture model adaptation,
does VSM improve performance? In (Foster and
Kuhn, 2007), two kinds of linear mixture were de-
scribed: linear mixture of language models (LMs),
and linear mixture of translation models (TMs).
Some of the results reported above involved lin-
ear TM mixtures, but none of them involved lin-
ear LM mixtures. Table 6 shows the results of
different combinations of VSM and mixture mod-
els. * and ** denote significant gains over the row
no vsm at p < 0.05 and p < 0.01 levels, re-
spectively. This means that in the table, the base-
line within each box containing three results is the
topmost result in the box. For instance, with an
initial Chinese system that employs linear mixture
LM adaptation (lin-lm) and has a BLEU of 32.1,
adding 1-feature VSM adaptation (+vsm, joint)
improves performance to 33.1 (improvement sig-
nificant at p < 0.01), while adding 3-feature VSM
instead (+vsm, 3 feat.) improves performance to
33.2 (also significant at p < 0.01). For Arabic, in-
cluding either form of VSM adaptation always im-
proves performance with significance at p < 0.01,
even over a system including both linear TM and
linear LM adaptation. For Chinese, adding VSM
still always yields an improvement, but the im-
provement is not significant if linear TM adapta-
tion is already in the system. These results show
that combining VSM adaptation and either or both
kinds of linear mixture adaptation never hurts per-
formance, and often improves it by a significant
amount.
3.4 Informal Data Analysis
To get an intuition for how VSM adaptation im-
proves BLEU scores, we compared outputs from
the baseline and VSM-adapted system (?vsm,
joint? in Table 5) on the Chinese test data. We
focused on examples where the two systems had
translated the same source-language (Chinese)
phrase s differently, and where the target-language
(English) translation of s chosen by the VSM-
adapted system, tV , had a higher Bhattacharyya
score for similarity with the dev set than did the
phrase that was chosen by the baseline system, tB .
Thus, we ignored differences in the two transla-
tions that might have been due to the secondary
effects of VSM adaptation (such as a different tar-1290
no-lin-adap lin-lm lin-tm lin-lm+lin-tm
no vsm 31.7 32.1 32.7 33.1
Chinese +vsm, joint 33.0** 33.1** 33.0 33.3
+vsm, 3 feat. 33.1** 33.2** 33.1 33.4
no vsm 46.8 47.0 47.5 47.7
Arabic +vsm, joint 48.4** 48.7** 48.6** 48.8**
+vsm, 3 feat. 48.6** 48.8** 48.7** 48.9**
Table 6: Results of combining VSM and linear mixture adaptation. ?lin-lm? is linear language model
adaptation, ?lin-tm? is linear translation model adaptation. * and ** denote significant gains over the row
?no vsm? at p < 0.05 and p < 0.01 levels, respectively.
get phrase being preferred by the language model
in the VSM-adapted system from the one preferred
in the baseline system because of a Bhattacharyya-
mediated change in the phrase preceding it).
An interesting pattern soon emerged: the VSM-
adapted system seems to be better than the base-
line at choosing among synonyms in a way that is
appropriate to the genre or style of a text. For in-
stance, where the text to be translated is from an
informal genre such as weblog, the VSM-adapted
system will often pick an informal word where the
baseline picks a formal word with the same or sim-
ilar meaning, and vice versa where the text to be
translated is from a more formal genre. To our
surprise, we saw few examples where the VSM-
adapted system did a better job than the baseline of
choosing between two words with different mean-
ing, but we saw many examples where the VSM-
adapted system did a better job than the baseline
of choosing between two words that both have the
same meaning according to considerations of style
and genre.
Two examples are shown in Table 7. In the
first example, the first two lines show that VSM
finds that the Chinese-English phrase pair (??,
assaulted) has a Bhattacharyya (BC) similarity of
0.556163 to the dev set, while the phrase pair (?
?, beat) has a BC similarity of 0.780787 to the
dev. In this situation, the VSM-adapted system
thus prefers ?beat? to ?assaulted? as a translation
for ??. The next four lines show the source
sentence (SRC), the reference (REF), the baseline
output (BSL), and the output of the VSM-adapted
system. Note that the result of VSM adaptation is
that the rather formal word ?assaulted? is replaced
by its informal near-synonym ?beat? in the trans-
lation of an informal weblog text.
?apprehend? might be preferable to ?arrest? in
a legal text. However, it looks as though the
VSM-adapted system has learned from the dev
that among synonyms, those more characteristic
of news stories than of legal texts should be cho-
sen: it therefore picks ?arrest? over its synonym
?apprehend?.
What follows is a partial list of pairs of phrases
(all single words) from our system?s outputs,
where the baseline chose the first member of a pair
and the VSM-adapted system chose the second
member of the pair to translate the same Chinese
phrase into English (because the second word
yields a better BC score for the dev set we used).
It will be seen that nearly all of the pairs involve
synonyms or near-synonyms rather than words
with radically different senses (one exception
below is ?center? vs ?heart?). Instead, the differ-
ences between the two words tend to be related to
genre or style: gunmen-mobsters, champion-star,
updated-latest, caricatures-cartoons, spill-leakage,
hiv-aids, inkling-clues, behaviour-actions, deceit-
trick, brazen-shameless, aristocratic-noble,
circumvent-avoid, attack-criticized, descent-born,
hasten-quickly, precipice-cliff, center-heart,
blessing-approval, imminent-approaching,
stormed-rushed, etc.
4 Conclusions and future work
This paper proposed a new approach to domain
adaptation in statistical machine translation, based
on vector space models (VSMs). This approach
measures the similarity between a vector repre-
senting a particular phrase pair in the phrase ta-
ble and a vector representing the dev set, yield-
ing a feature associated with that phrase pair that
will be used by the decoder. The approach is
simple, easy to implement, and computationally
cheap. For the two language pairs we looked
at, it provided a large performance improvement
over a non-adaptive baseline, and also compared1291
1 phrase ??? assaulted (0.556163)
pairs ??? beat (0.780787)
SRC ...???????????...
REF ... those local ruffians and hooligans who beat up villagers ...
BSL ... those who assaulted the villagers land hooligans ...
VSM ... hooligans who beat the villagers ...
2 phrase ??? apprehend (0.286533)
pairs ??? arrest (0.603342)
SRC ... ?????????????
REF ... catch the killers and bring them to justice .
BSL ... apprehend the perpetrators and bring them to justice .
VSM ... arrest the perpetrators and bring them to justice .
Table 7: Examples show that VSM chooses translations according to considerations of style and genre.
favourably with linear mixture adaptation tech-
niques.
Furthermore, VSM adaptation can be exploited
in a number of different ways, which we have only
begun to explore. In our experiments, we based
the vector space on subcorpora defined by the na-
ture of the training data. This was done purely
out of convenience: there are many, many ways to
define a vector space in this situation. An obvi-
ous and appealing one, which we intend to try in
future, is a vector space based on a bag-of-words
topic model. A feature derived from this topic-
related vector space might complement some fea-
tures derived from the subcorpora which we ex-
plored in the experiments above, and which seem
to exploit information related to genre and style.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In EMNLP 2011.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th Workshop on Statistical Machine Translation,
Athens, March. WMT.
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bulletin of the Calcutta
Mathematical Society, 35:99?109.
Sung-Hyuk Cha. 2007. Comprehensive survey on dis-
tance/similarity measures between probability den-
sity functions. International Journal of Mathe-
matical Models ind Methods in Applied Sciences,
1(4):300?307.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for smt self-
enhancement. In ACL 2008.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In MT Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
ACL Workshop on Statistical Machine Translation,
Prague, June. WMT.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Boston.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model.
In EMNLP 2008, pages 848?856, Hawaii, October.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
EAMT Conference, Budapest, May.
Donald Hindle. 1990. Noun classification from predi-
cate.argument structures. In Proceedings of the 28th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 268?275, Pittsburgh,
PA, June. ACL.
Fei Huang and Bing Xiang. 2010. Feature-rich dis-
criminative phrase rescoring for SMT. In COLING
2010.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A1292
bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 247?256, Uppsala, Swe-
den, July. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL 2007,
Demonstration Session.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Barcelona, Spain.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768?774, Montreal, Quebec, Canada.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Pro-
ceedings of the 2007 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
Prague, Czech Republic.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods Instru-
ments and Computers, 28(2):203?208.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL
2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, July. ACL.
Aaron B. Phillips and Ralf D. Brown. 2011. Train-
ing machine translation with a second-order taylor
approximation of weighted translation instances. In
MT Summit 2011.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT 2008.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In EACL 2012.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Twelfth European
Conference on Machine Learning, page 491?502,
Berlin, Germany.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Prague, Czech Republic, June.
ACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING) 2004, Geneva, Au-
gust.
1293
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 11?16,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Fast Consensus Hypothesis Regeneration for Machine Translation 
 
Boxing Chen, George Foster and Roland Kuhn 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca 
 
  
 
Abstract 
This paper presents a fast consensus hy-
pothesis regeneration approach for ma-
chine translation. It combines the advan-
tages of feature-based fast consensus de-
coding and hypothesis regeneration.  Our 
approach is more efficient than previous 
work on hypothesis regeneration, and it 
explores a wider search space than con-
sensus decoding, resulting in improved 
performance.  Experimental results show 
consistent improvements across language 
pairs, and an improvement of up to 0.72 
BLEU is obtained over a competitive 
single-pass baseline on the Chinese-to-
English NIST task. 
1 Introduction 
State-of-the-art statistical machine translation 
(SMT) systems are often described as a two-pass 
process. In the first pass, decoding algorithms are 
applied to generate either a translation N-best list 
or a translation forest.  Then in the second pass, 
various re-ranking algorithms are adopted to 
compute the final translation. The re-ranking al-
gorithms include rescoring (Och et al, 2004) and 
Minimum Bayes-Risk (MBR) decoding (Kumar 
and Byrne, 2004; Zhang and Gildea, 2008; 
Tromble et al, 2008). Rescoring uses more so-
phisticated additional feature functions to score 
the hypotheses. MBR decoding directly incorpo-
rates the evaluation metrics (i.e., loss function), 
into the decision criterion, so it is effective in 
tuning the MT performance for a specific loss 
function. In particular, sentence-level BLEU loss 
function gives gains on BLEU (Kumar and 
Byrne, 2004).  
The na?ve MBR algorithm computes the loss 
function between every pair of k hypotheses, 
needing O(k2) comparisons. Therefore, only 
small number k is applicable. Very recently, De-
Nero et al (2009) proposed a fast consensus de-
coding (FCD) algorithm in which the similarity 
scores are computed based on the feature expec-
tations over the translation N-best list or transla-
tion forest. It is equivalent to MBR decoding 
when using a linear similarity function, such as 
unigram precision.  
Re-ranking approaches improve performance 
on an N-best list whose contents are fixed. A   
complementary strategy is to augment the con-
tents of an N-best list in order to broaden the 
search space. Chen et al(2008) have proposed a 
three-pass SMT process, in which a hypothesis 
regeneration pass is added between the decoding 
and rescoring passes. New hypotheses are gener-
ated based on the original N-best hypotheses 
through n-gram expansion, confusion-network 
decoding or re-decoding. All three hypothesis 
regeneration methods obtained decent and com-
parable improvements in conjunction with the 
same rescoring model. However, since the final 
translation candidates in this approach are pro-
duced from different methods, local feature func-
tions (such as translation models and reordering 
models) of each hypothesis are not directly com-
parable and rescoring must exploit rich global 
feature functions to compensate for the loss of 
local feature functions. Thus this approach is de-
pendent on the use of computationally expensive 
features for rescoring, which makes it inefficient.  
In this paper, we propose a fast consensus hy-
pothesis regeneration method that combines the 
advantages of feature-based fast consensus de-
coding and hypothesis regeneration. That is, we 
integrate the feature-based similarity/loss func-
tion based on evaluation metrics such as BLEU 
score into the hypothesis regeneration procedure 
to score the partial hypotheses in the beam search 
and compute the final translations. Thus, our ap-
proach is more efficient than the original three-
pass hypothesis regeneration. Moreover, our ap-
proach explores more search space than consen-
11
sus decoding, giving it an advantage over the 
latter. 
In particular, we extend linear corpus BLEU 
(Tromble et al, 2008) to n-gram expectation-
based linear BLEU, then further extend the n-
gram expectation computed on full-length hypo-
theses to n-gram expectation computed on fixed-
length partial hypotheses. Finally, we extend the 
hypothesis regeneration with forward n-gram 
expansion to bidirectional n-gram expansion in-
cluding both the forward and backward n-gram 
expansion. Experimental results show consistent 
improvements over the baseline across language 
pairs, and up to 0.72 BLEU points are obtained 
from a competitive baseline on the Chinese-to-
English NIST task. 
2 Fast Consensus Hypothesis Regenera-
tion 
Since the three hypothesis regeneration methods 
with n-gram expansion, confusion network de-
coding and re-decoding produce very similar per-
formance (Chen et al, 2008), we consider only 
n-gram expansion method in this paper. N-gram 
expansion can (almost) fully exploit the search 
space of target strings which can be generated by 
an n-gram language model trained on the N-best 
hypotheses (Chen et al, 2007). 
2.1 Hypothesis regeneration with bidirec-
tional n-gram expansion 
N-gram expansion (Chen et al, 2007) works as 
follows: firstly, train an n-gram language model 
based on the translation N-best list or translation 
forest; secondly, expand each partial hypothesis 
by appending a word via overlapped (n-1)-grams 
until the partial hypothesis reaches the sentence 
ending symbol. In each expanding step, the par-
tial hypotheses are pruned through a beam-search 
algorithm with scoring functions. 
Duchateau et al (2001) shows that the back-
ward language model contains information com-
plementary to the information in the forward 
language model. Hence, on top of the forward n-
gram expansion used in (Chen et al, 2008), we 
further introduce backward n-gram expansion to 
the hypothesis regeneration procedure. Backward 
n-gram expansion involves letting the partial hy-
potheses start from the last words that appeared 
in the translation N-best list and having the ex-
pansion go from right to left. 
Figure 1 gives an example of backward n-
gram expansion. The second row shows bi-grams 
which are extracted from the original hypotheses 
in the first row. The third row shows how a par-
tial hypothesis is expanded via backward n-gram 
expansion method. The fourth row lists some 
new hypotheses generated by backward n-gram 
expansion which do not exist in the original hy-
pothesis list. 
 
 
original 
 hypotheses 
about weeks' work . 
one week's work 
about one week's 
about a week work 
about one week work 
bi-grams about weeks', weeks' work, ?, 
about one, ?,  week work. 
backward 
n-gram 
 expansion 
partial hyp.    week's work 
n-gram one week's  
new partial hyp. one week's work 
 
 
new 
 hypotheses 
about one week's work 
about week's work 
one weeks' work . 
one week's work . 
one week's work . 
 
Figure 1: Example of original hypotheses; bi-grams 
collected from them; backward expanding a partial 
hypothesis via an overlapped n-1-gram; and new hy-
potheses generated through backward n-gram expan-
sion. 
2.2 Feature-based scoring functions 
To speed up the search, the partial hypotheses 
are pruned via beam-search in each expanding 
step. Therefore, the scoring functions applied 
with the beam-search algorithm are very impor-
tant. In (Chen et al, 2008), more than 10 addi-
tional global features are computed to rank the 
partial hypothesis list, and this is not an efficient 
way. In this paper, we propose to directly incor-
porate the evaluation metrics such as BLEU 
score to rank the candidates. The scoring func-
tions of this work are derived from the method of 
lattice Minimum Bayes-risk (MBR) decoding 
(Tromble et al, 2008) and fast consensus decod-
ing (DeNero et al, 2009), which were originally 
inspired from N-best MBR decoding (Kumar and 
Byrne, 2004). 
From a set of translation candidates E, MBR 
decoding chooses the translation that has the 
least expected loss with respect to other candi-
dates. Given a hypothesis set E, under the proba-
bility model )|( feP , MBR computes the trans-
lation e~  as follows: 
 
12
)|(),(minarg~ fePeeLe
EeEe
??= ?
???
        (1) 
 
where f is the source sentence, ),( eeL ?  is the loss 
function of two translations e and e? . 
Suppose that we are interested in maximizing 
the BLEU score (Papineni et al, 2002) to optim-
ize the translation performance. The loss func-
tion is defined as ),(1),( eeBLEUeeL ??=? ,  
then the MBR objective can be re-written as 
 
)|(),(maxarg~ fePeeBLEUe
EeEe
??= ?
???
         (2) 
 
E represents the space of the translations. For 
N-best MBR decoding, this space is the N-best 
list produced by a baseline decoder (Kumar and 
Byrne, 2004). For lattice MBR decoding, this 
space is the set of candidates encoded in the lat-
tice (Tromble et al, 2008). Here, with hypothesis 
regeneration, this space includes: 1) the transla-
tions produced by the baseline decoder either in 
an N-best list or encoded in a translation lattice, 
and 2) the translations created by hypothesis re-
generation. 
However, BLEU score is not linear with the 
length of the hypothesis, which makes the scor-
ing process for each expanding step of hypothe-
sis regeneration very slow. To further speed up 
the beam search procedure, we use an extension 
of a linear function of a Taylor approximation to 
the logarithm of corpus BLEU which was devel-
oped by (Tromble et al, 2008).  The original 
BLEU score of two hypotheses e and e? are 
computed as follows. 
 
)),(log(
4
1
exp(),(),(
4
1
?
=
???=?
n
n
eePeeeeBLEU ?    (3) 
 
where ),( eePn ?  is the precision of n-grams in the 
hypothesis e given e? and  ),( ee ??  is a brevity 
penalty. Let |e| denote the length of e. The corpus 
log-BLEU gain is defined as follows: 
 
)),(log(
4
1)||
||1,0min()),(log(
4
1
?
=
?+
?
?=?
n
n eeP
e
e
eeBLEU  (4) 
 
Therefore, the first-order Taylor approxima-
tion to the logarithm of corpus BLEU is shown 
in Equation (5). 
 
?
=
??+=?
4
1
0 ),(4
1||),(
n
nn
eeceeeG ??                    (5) 
where ),( eecn ? are the counts of the matched n-
grams and 
n?  ( 40 ?? n ) are constant weights 
estimated with held-out data.  
Suppose we have computed the expected n-
gram counts from the N-best list or translation 
forest. Then we may extend linear corpus BLEU 
in (5) to n-gram expectation-based linear corpus 
BLEU to score the partial hypotheses h. That is 
 
? ?
= ?
??+=
4
1
0 ),()],'([4
1||)',(
n Tt
nnn
n
thtecEhehG ???       (6) 
 
where ),( thn?  are n-gram indicator functions that 
equal 1 if n-gram t  appears in h  and 0 other-
wise; )],'([ tecE n  ( 41 ?? n ) are the real-valued 
n-gram expectations. Different from lattice MBR 
decoding, n-gram expectations in this work are 
computed over the original translation N-best list 
or translation forest; 
nT  ( 41 ?? n ) are the sets of 
n-grams collected from translation N-best list or 
translation forest. Then we make a further exten-
sion: the expectations of the n-gram counts for 
each expanding step are computed over the par-
tial translations. The lengths of all partial hypo-
theses are the same in each n-gram expanding 
step. For instance, in the 5th n-gram expanding 
step, the lengths of all the partial hypotheses are 
5 words. Therefore, we use n-gram count expec-
tations computed over partial original transla-
tions that only contain the first 5 words. The rea-
son is that this solution contains more informa-
tion about word orderings, since some n-grams 
appear more than others at the beginning of the 
translations while they may appear with the same 
or even lower frequencies than others in the full 
translations.  
Once the expanding process of hypothesis re-
generation is finished, we use a more precise 
BLEU metric to score all the translation candi-
dates. We extend BLEU score in (3) to n-gram 
expectation-based BLEU. That is: 
 
?
?
?
?
?
?
?
?
?
?
+???
?
???
?
?=
=
? ?
?
=
?
?
4
1 ),(
)]),'([),,(min(
log
4
1
||
|]'[|1,0minexp
)',()(
n
Tt
n
Tt
nn
n
n
thc
tecEthc
h
eE
ehBLEUhScore
                                                        (7) 
 
where ),( thcn  is the count of  n-gram t in the 
hypothesis h. The step of choosing the final 
translation is the same as fast consensus decod-
ing (DeNero et al, 2009): first we compute n-
13
gram feature expectations, and then we choose 
the translation that is most similar to the others 
via expected similarity according to feature-
based BLEU score as shown in (7). The differ-
ence is the space of translations: the space of fast 
consensus decoding is the same as MBR decod-
ing, while the space of hypothesis regeneration is 
enlarged by the new translations produced via n-
gram expansion. 
2.3 Fast consensus hypothesis regeneration 
We first generate two new hypothesis lists via 
forward and backward n-gram expansion using 
the scoring function in Equation (6). Then we 
choose a final translation using the scoring func-
tion in Equation (7) from the union of the origi-
nal hypotheses and newly generated hypotheses. 
The original hypotheses are from the N-best list 
or extracted from the translation forest. The new 
hypotheses are generated by forward or back-
ward n-gram expansion or are the union of both 
two new hypothesis lists (this is called ?bi-
directional n-gram expansion?). 
3 Experimental Results 
We carried out experiments based on translation 
N-best lists generated by a state-of-the-art 
phrase-based statistical machine translation sys-
tem, similar to (Koehn et al, 2007). In detail, the 
phrase table is derived from merged counts of 
symmetrized IBM2 and HMM alignments; the 
system has both lexicalized and distance-based 
distortion components (there is a 7-word distor-
tion limit) and employs cube pruning (Huang and 
Chiang, 2007). The baseline is a log-linear fea-
ture combination that includes language models, 
the distortion components, translation model, 
phrase and word penalties. Weights on feature 
functions are found by lattice MERT (Macherey 
et al, 2008). 
3.1 Data 
We evaluated with different language pairs: Chi-
nese-to-English, and German-to-English. Chi-
nese-to-English tasks are based on training data 
for the NIST 1  2009 evaluation Chinese-to-
English track. All the allowed bilingual corpora 
have been used for estimating the translation 
model. We trained two language models: the first 
one is a 5-gram LM which is estimated on the 
target side of the parallel data. The second is a 5-
                                               
1
 http://www.nist.gov/speech/tests/mt 
gram LM trained on the so-called English Giga-
word corpus. 
 
   Chi Eng 
Parallel 
Train 
Large 
Data 
|S| 10.1M 
|W| 270.0M 279.1M 
   Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics of training, dev, and test sets for 
Chinese-to-English task. 
 
We carried out experiments for translating 
Chinese to English. We first created a develop-
ment set which used mainly data from the NIST 
2005 test set, and also some balanced-genre web-
text from the NIST training material. Evaluation 
was performed on the NIST 2006 and 2008 test 
sets. Table 1 gives figures for training, develop-
ment and test corpora; |S| is the number of the 
sentences, and |W| is the size of running words. 
Four references are provided for all dev and test 
sets. 
For German-to-English tasks, we used WMT 
20062 data sets. The parallel training data con-
tains about 1 million sentence pairs and includes 
21 million target words; both the dev set and test 
set contain 2000 sentences; one reference is pro-
vided for each source input sentence. Only the 
target-language half of the parallel training data 
are used to train the language model in this task. 
3.2 Results 
Our evaluation metric is IBM BLEU (Papineni et 
al., 2002), which performs case-insensitive 
matching of n-grams up to n = 4.  
Our first experiment was carried out over 
1000-best lists on Chinese-to-English task. For 
comparison, we also conducted experiments with 
rescoring (two-pass) and three-pass hypothesis 
regeneration with only forward n-gram expan-
sion as proposed in (Chen et al, 2008). In the 
?rescoring? and ?three-pass? systems, we used 
the same rescoring model. There are 21 rescoring 
features in total, mainly translation lexicon 
scores from IBM and HMM models, posterior 
probabilities for words, n-grams, and sentence 
length, and language models, etc. For a complete 
description, please refer to (Ueffing et al, 2007). 
The results in BLEU-4 are reported in Table 2. 
 
                                               
2
 http://www.statmt.org/wmt06/ 
14
testset NIST?06 NIST?08 
baseline 35.70 28.60 
rescoring 36.01 28.97 
three-pass 35.98 28.99 
FCD 36.00 29.10 
Fwd. 36.13 29.19 
Bwd. 36.11 29.20 
Bid. 36.20 29.28 
 
Table 2: Translation performances in BLEU-4(%) 
over 1000-best lists for Chinese-to-English task: ?res-
coring? represents the results of rescoring; ?three-
pass?, three-pass hypothesis regeneration with for-
ward n-gram expansion; ?FCD?, fast consensus de-
coding; ?Fwd?, the results of hypothesis regeneration 
with forward n-gram expansion; ?Bwd?, backward n-
gram expansion; and ?Bid?, bi-directional n-gram 
expansion. 
 
Firstly, rescoring improved performance over 
the baseline by 0.3-0.4 BLEU point. Three-pass 
hypothesis regeneration with only forward n-
gram expansion (?three-pass? in Table 2) ob-
tained almost the same improvements as rescor-
ing. Three-pass hypothesis regeneration exploits 
more hypotheses than rescoring, while rescoring 
involves more scoring feature functions than the 
former. They reached a balance in this experi-
ment. Then, fast consensus decoding (?FCD? in 
Table 2) obtains 0.3-0.5 BLEU point improve-
ments over the baseline. Both forward and back-
ward n-gram expansion (?Fwd.? and ?Bwd.? in 
Table 2) improved about 0.1 BLEU point over 
the results of consensus decoding. Fast consen-
sus hypothesis regeneration (Fwd. and Bwd. in 
Table 2) got better improvements than three-pass 
hypothesis regeneration (?three-pass? in Table 2) 
by 0.1-0.2 BLEU point. Finally, combining hy-
pothesis lists from forward and backward n-gram 
expansion (?Bid.? in Table 2), further slight 
gains were obtained. 
 
testset Average time 
three-pass 3h 54m 
Fwd. 25m 
Bwd. 28m 
Bid. 40m 
 
Table 3: Average processing time of NIST?06 and 
NIST?08 test sets used in different systems. Times 
include n-best list regeneration and re-ranking. 
 
Moreover, fast consensus hypothesis regenera-
tion is much faster than the three-pass one, be-
cause the former only needs to compute one fea-
ture, while the latter needs to compute more than 
20 additional features. In this experiment, the 
former is about 10 times faster than the latter in 
terms of processing time, as shown in Table 3. 
 
In our second experiment, we set the size of 
N-best list N equal to 10,000 for both Chinese-to-
English and German-to-English tasks. The re-
sults are reported in Table 4. The same trend as 
in the first experiment can also be observed in 
this experiment. It is worth noticing that enlarg-
ing the size of the N-best list from 1000 to 
10,000 did not change the performance signifi-
cantly. Bi-directional n-gram expansion obtained 
improvements of 0.24 BLEU-score for WMT 
2006 de-en test set; 0.55 for NIST 2006 test set; 
and 0.72 for NIST 2008 test set over the base-
line. 
 
Lang. ch-en de-en 
testset NIST?06 NIST?08 Test2006 
baseline 35.70 28.60 26.92 
FCD 36.03 29.08 27.03 
Fwd. 36.16 29.25 27.11 
Bwd. 36.17 29.22 27.12 
Bid. 36.25 29.32 27.16 
 
Table 4: Translation performances in BLEU-4 (%) 
over 10K-best lists. 
 
We then tested the effect of the extension ac-
cording to which the expectations over n-gram 
counts are computed on partial hypotheses rather 
than whole candidate translations as described in 
Section 2.2. As shown in Table 5, we got tiny 
improvements on both test sets by computing the 
expectations over n-gram counts on partial hypo-
theses. 
 
testset NIST?06 NIST?08 
full 36.11 29.14 
partial 36.13 29.19 
 
Table 5: Translation performances in BLEU-4 (%) 
over 1000-best lists for Chinese-to-English task: 
?full? represents expectations over n-gram counts that 
are computed on whole hypotheses; ?partial? 
represents expectations over n-gram counts that are 
computed on partial hypotheses. 
3.3 Discussion  
To speed up the search, the partial hypotheses in 
each expanding step are pruned. When pruning is 
applied, forward and backward n-gram expan-
sion would generate different new hypothesis 
lists. Let us look back at the example in Figure 1.  
15
Given 5 original hypotheses in Figure 1, if we set 
the beam size equal to 5 (the size of the original 
hypotheses), the forward and backward n-gram 
expansion generated different new hypothesis 
lists, as shown in Figure 2. 
 
forward backward 
one week's work . 
about week's work 
one week's work . 
about one week's work 
 
Figure 2: Different new hypothesis lists generated by 
forward and backward n-gram expansion. 
 
For bi-directional n-gram expansion, the cho-
sen translation for a source sentence comes from 
the decoder 94% of the time for WMT 2006 test 
set, 90% for NIST test sets; it comes from for-
ward n-gram expansion 2% of the time for WMT 
2006 test set, 4% for NIST test sets; it comes 
from backward n-gram expansion 4% of the time 
for WMT 2006 test set, 6% for NIST test sets. 
This proves bidirectional n-gram expansion is a 
good way of enlarging the search space. 
4 Conclusions and Future Work 
We have proposed a fast consensus hypothesis 
regeneration approach for machine translation. It 
combines the advantages of feature-based con-
sensus decoding and hypothesis regeneration. 
This approach is more efficient than previous 
work on hypothesis regeneration, and it explores 
a wider search space than consensus decoding, 
resulting in improved performance.  Experiments 
showed consistent improvements across lan-
guage pairs. 
Instead of N-best lists, translation lattices or 
forests have been shown to be effective for MBR 
decoding (Zhang and Gildea, 2008; Tromble et 
al., 2008), and DeNero et al (2009) showed how 
to compute expectations of n-grams from a trans-
lation forest. Therefore, our future work may 
involve hypothesis regeneration using an n-gram 
language model trained on the translation forest. 
References  
B. Chen, M. Federico and M. Cettolo. 2007. Better N-
best Translations through Generative n-gram Lan-
guage Models. In: Proceedings of MT Summit XI. 
Copenhagen, Denmark. September. 
B. Chen, M. Zhang, A. Aw, and H. Li. 2008. Regene-
rating Hypotheses for Statistical Machine Transla-
tion. In: Proceedings of COLING. pp105-112. 
Manchester, UK, August. 
J. DeNero, D. Chiang and K. Knight. 2009. Fast Con-
sensus Decoding over Translation Forests. In: Pro-
ceedings of ACL. Singapore, August. 
J. Duchateau, K. Demuynck, and P. Wambacq. 2001. 
Confidence scoring based on backward language 
models. In: Proceedings of ICASSP 2001. Salt 
Lake City, Utah, USA, May. 
L. Huang and D. Chiang. 2007. Forest Rescoring: 
Faster Decoding with Integrated Language Models. 
In: Proceedings of ACL. pp. 144-151, Prague, 
Czech Republic, June.  
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In: Proceedings of 
ACL. pp. 177-180, Prague, Czech Republic. 
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk 
decoding for statistical machine translation. In: 
Proceedings of NAACL. Boston, MA, May. 
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 
2008. Lattice-based Minimum Error Rate Training 
for Statistical Machine Translation. In: Proceed-
ings of EMNLP. pp. 725-734, Honolulu, USA, 
October. 
F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL. 
Sapporo, Japan. July. 
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. 
Eng, V. Jain, Z. Jin, and D. Radev. 2004. A Smor-
gasbord of Features for Statistical Machine Trans-
lation. In: Proceedings of NAACL. Boston. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
BLEU: A method for automatic evaluation of ma-
chine translation. In: Proceedings of the ACL 2002. 
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 
2008. Lattice minimum Bayes-risk decoding for 
statistical machine translation. In: Proceedings of 
EMNLP. Hawaii, US. October. 
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.  
2007. NRC?s Portage system for WMT 2007. In: 
Proceedings of ACL Workshop on SMT. Prague, 
Czech Republic, June. 
H. Zhang and D. Gildea. 2008. Efficient multipass 
decoding for synchronous context free grammars. 
In: Proceedings of ACL. Columbus, US. June. 
16
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127?132,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Lessons from NRC?s Portage System at WMT 2010 
 
 
Samuel Larkin, Boxing Chen, George Foster, Ulrich Germann, Eric Joanis, 
Howard Johnson, and Roland Kuhn  
National Research Council of Canada (NRC) 
Gatineau, Qu?bec, Canada. 
Firstname.Lastname@cnrc-nrc.gc.ca 
 
  
 
Abstract 
 
NRC?s Portage system participated in the Eng-
lish-French (E-F) and French-English (F-E) 
translation tasks of the ACL WMT 2010 eval-
uation. The most notable improvement over 
earlier versions of Portage is an efficient im-
plementation of lattice MERT. While Portage 
has typically performed well in Chinese to 
English MT evaluations, most recently in the 
NIST09 evaluation, our participation in WMT 
2010 revealed some interesting differences be-
tween Chinese-English and E-F/F-E transla-
tion, and alerted us to certain weak spots in 
our system. Most of this paper discusses the 
problems we found in our system and ways of 
fixing them. We learned several lessons that 
we think will be of general interest.  
1 Introduction 
Portage, the statistical machine translation sys-
tem of the National Research Council of Canada 
(NRC), is a two-pass phrase-based system. The 
translation tasks to which it is most often applied 
are Chinese to English, English to French (hen-
ceforth ?E-F?), and French to English (hence-
forth ?F-E?): in recent years we worked on Chi-
nese-English translation for the GALE project 
and for NIST evaluations, and English and 
French are Canada?s two official languages. In 
WMT 2010, Portage scored 28.5 BLEU (un-
cased) for F-E, but only 27.0 BLEU (uncased) 
for E-F. For both language pairs, Portage tru-
ecasing caused a loss of 1.4 BLEU; other WMT 
systems typically lost around 1.0 BLEU after 
truecasing. In Canada, about 80% of translations 
between English and French are from English to 
French, so we would have preferred better results 
for that direction. This paper first describes the 
version of Portage that participated in WMT 
2010. It then analyzes problems with the system 
and describes the solutions we found for some of 
them.  
2 Portage system description 
2.1 Core engine and training data 
The NRC system uses a standard two-pass 
phrase-based approach. Major features in the 
first-pass loglinear model include phrase tables 
derived from symmetrized IBM2 alignments and 
symmetrized HMM alignments, a distance-based 
distortion model, a lexicalized distortion model, 
and language models (LMs) that can be either 
static or else dynamic mixtures. Each phrase ta-
ble used was a merged one, created by separately 
training an IBM2-based and an HMM-based 
joint count table on the same data and then add-
ing the counts. Each includes relative frequency 
estimates and lexical estimates (based on Zens 
and Ney, 2004) of forward and backward condi-
tional probabilities. The lexicalized distortion 
probabilities are also obtained by adding IBM2 
and HMM counts. They involve 6 features (mo-
notone, swap and discontinuous features for fol-
lowing and preceding phrase) and are condi-
tioned on phrase pairs in a model similar to that 
of Moses (Koehn et al, 2005); a MAP-based 
backoff smoothing scheme is used to combat 
data sparseness when estimating these probabili-
ties. Dynamic mixture LMs are linear mixtures 
of ngram models trained on parallel sub-corpora 
with weights set to minimize perplexity of the 
current source text as described in (Foster and 
Kuhn, 2007); henceforth, we?ll call them ?dy-
namic LMs?.  
Decoding uses the cube-pruning algorithm of 
(Huang and Chiang, 2007) with a 7-word distor-
tion limit. Contrary to the usual implementation 
of distortion limits, we allow a new phrase to end 
127
more than 7 words past the first non-covered 
word, as long as the new phrase starts within 7 
words from the first non-covered word. Notwith-
standing the distortion limit, contiguous phrases 
can always be swapped. Out-of-vocabulary 
(OOV) source words are passed through un-
changed to the target. Loglinear weights are 
tuned with Och's max-BLEU algorithm over lat-
tices (Macherey et al, 2008); more details about 
lattice MERT are given in the next section. The 
second pass rescores 1000-best lists produced by 
the first pass, with additional features including 
various LM and IBM-model probabilities; ngram, 
length, and reordering posterior probabilities and 
frequencies; and quote and parenthesis mismatch 
indicators. To improve the quality of the maxima 
found by MERT when using large sets of partial-
ly-overlapping rescoring features, we use greedy 
feature selection, first expanding from a baseline 
set, then pruning. 
We restricted our training data to data that was 
directly available through the workshop's web-
site; we didn?t use the LDC resources mentioned 
on the website (e.g., French Gigaword, English 
Gigaword). Below, ?mono? refers to all mono-
lingual data (Europarl, news-commentary, and 
shuffle); ?mono? English is roughly three times 
bigger than ?mono? French (50.6 M lines in 
?mono? English, 17.7 M lines in ?mono? French). 
?Domain? refers to all WMT parallel training 
data except GigaFrEn (i.e., Europarl, news-
commentary, and UN).   
2.2 Preprocessing and postprocessing 
We used our own English and French pre- and 
post-processing tools, rather than those available 
from the WMT web site. For training, all English 
and French text is tokenized with a language-
specific tokenizer and then mapped to lowercase. 
Truecasing uses an HMM approach, with lexical 
probabilities derived from ?mono? and transition 
probabilities from a 3-gram LM trained on tru-
ecase ?mono?. A subsequent rule-based pass ca-
pitalizes sentence-initial words. A final detokeni-
zation step undoes the tokenization. 
2.3 System configurations for WMT 2010 
In the weeks preceding the evaluation, we tried 
several ways of arranging the resources available 
to us. We picked the configurations that gave the 
highest BLEU scores on WMT2009 Newstest. 
We found that tuning with lattice MERT rather 
than N-best MERT allowed us to employ more 
parameters and obtain better results.  
E-F system components: 
1. Phrase table trained on ?domain?;  
2. Phrase table trained on GigaFrEn;  
3. Lexicalized distortion model trained on 
?domain?;  
4. Distance-based distortion model; 
5. 5-gram French LM trained on ?mono?;  
6. 4-gram LM trained on French half of 
GigaFrEn;  
7. Dynamic LM composed of 4 LMs, each 
trained on the French half of a parallel 
corpus (5-gram LM trained on ?domain?, 
4-gram LM on GigaFrEn, 5-gram LM on 
news-commentary and 5-gram LM on 
UN). 
 
The F-E system is a mirror image of the E-F sys-
tem.  
3 Details of lattice MERT (LMERT) 
Our system?s implementation of LMERT (Ma-
cherey et al, 2008) is the most notable recent 
change in our system. As more and more features 
are included in the loglinear model, especially if 
they are correlated, N-best MERT (Och, 2003) 
shows more and more instability, because of 
convergence to local optima (Foster and Kuhn, 
2009). We had been looking for methods that 
promise more stability and better convergence. 
LMERT seemed to fit the bill. It optimizes over 
the complete lattice of candidate translations af-
ter a decoding run. This avoids some of the prob-
lems of N-best lists, which lack variety, leading 
to poor local optima and the need for many de-
coder runs. 
Though the algorithm is straightforward and is 
highly parallelizable, attention must be paid to 
space and time resource issues during implemen-
tation. Lattices output by our decoder were large 
and needed to be shrunk dramatically for the al-
gorithm to function well. Fortunately, this could 
be achieved via the finite state equivalence algo-
rithm for minimizing deterministic finite state 
machines. The second helpful idea was to sepa-
rate out the features that were a function of the 
phrase associated with an arc (e.g., translation 
length and translation model probability fea-
tures). These features could then be stored in a 
smaller phrase-feature table. Features associated 
with language or distortion models could be han-
dled in a larger transition-feature table. 
The above ideas, plus careful coding of data 
structures, brought the memory footprint down 
sufficiently to allow us to use complete lattices 
from the decoder and optimize over the complete 
128
development set for NIST09 Chinese-English. 
However, combining lattices between decoder 
runs again resulted in excessive memory re-
quirements. We achieved acceptable perfor-
mance by searching only the lattice from the lat-
est decoder run; perhaps information from earlier 
runs, though critical for convergence in N-best 
MERT, isn?t as important for LMERT.  
Until a reviewer suggested it, we had not 
thought of pruning lattices to a specified graph 
density as a solution for our memory problems. 
This is referred to in a single sentence in (Ma-
cherey et al, 2008), which does not specify its 
implementation or its impact on performance, 
and is an option of OpenFst (we didn?t use 
OpenFst). We will certainly experiment with lat-
tice pruning in future.  
Powell's algorithm (PA), which is at the core 
of MERT, has good convergence when features 
are mostly independent and do not depart much 
from a simple coordinate search; it can run into 
problems when there are many correlated fea-
tures (as with multiple translation and language 
models). Figure 1 shows the kind of case where 
PA works well. The contours of the function be-
ing optimized are relatively smooth, facilitating 
learning of new search directions from gradients. 
Figure 2 shows a more difficult case: there is 
a single optimum, but noise dominates and PA 
has difficulty finding new directions. Search of-
ten iterates over the original co-ordinates, miss-
ing optima that are nearby but in directions not 
discoverable from local gradients. Probes in ran-
dom directions can do better than iteration over 
the same directions (this is similar to the method 
proposed for N-best MERT by Cer et al, 2008). 
Each 1-dimensional MERT optimization is exact, 
so if our probe stabs a region with better scores, 
it will be discovered. Figures 1 and 2 only hint 
at the problem: in reality, 2-dimensional search 
isn?t a problem. The difficulties occur as the di-
mension grows: in high dimensions, it is more 
important to get good directions and they are 
harder to find. 
For WMT 2010, we crafted a compromise 
with the best properties of PA, yet alowing for a 
more aggressive search in more directions. We 
start with PA. As long as PA is adding new di-
rection vectors, it is continued. When PA stops 
adding new directions, random rotation (ortho-
gonal transformation) of the coordinates is per-
formed and PA is restarted in the new space. PA 
almost always fails to introduce new directions 
within the new coordinates, then fails again, so 
another set of random coordinates is chosen. This 
process repeats until convergence. In future 
work, we will look at incorporating random res-
tarts into the algorithm as additional insurance 
against premature convergence.  
Our LMERT implementation has room for 
improvement: it may still run into over-fitting 
problems with many correlated features. Howev-
er, during preparation for the evaluation, we no-
ticed that LMERT converged better than N-best 
MERT, allowing models with more features and 
higher BLEU to be chosen.  
After the WMT submission, we discovered 
that our LMERT implementation had a bug; our 
submission was tuned with this buggy LMERT. 
Comparison between our E-F submission tuned 
with N-best MERT and the same system tuned 
with bug-fixed LMERT shows BLEU gains of 
+1.5-3.5 for LMERT (on dev, WMT2009, and 
WMT2010, with no rescoring). However, N-best 
MERT performed very poorly in this particular 
case; we usually obtain a gain due to LMERT of 
+0.2-1.0 (e.g., for the submitted F-E system).  
 
 
Figure 1: Convergence for PA (Smooth Feature 
Space)  
 
 
Figure 2: Convergence for PA with Random Rotation 
(Rough Feature Space) 
129
4 Problems and Solutions 
4.1 Fixing LMERT  
Just after the evaluation, we noticed a discrepan-
cy for E-F between BLEU scores computed dur-
ing LMERT optimization and scores from the 1-
best list immediately after decoding. Our 
LMERT code had a bug that garbled any ac-
cented word in the version of the French refer-
ence in memory; previous LMERT experiments 
had English as target language, so the bug hadn?t 
showed up. The bug didn?t affect characters in 
the 7-bit ASCII set, such as English ones, only 
accented characters. Words in candidate transla-
tions were not garbled, so correct translations 
with accents received a lower BLEU score than 
they should have. As Table 1 shows, this bug 
cost us about 0.5 BLEU for WMT 2010 E-F after 
rescoring (according to NRC?s internal version 
of BLEU, which differs slightly from WMT?s 
BLEU). Despite this bug, the system tuned with 
buggy LMERT (and submitted) was still better 
than the best system we obtained with N-best 
MERT. The bug didn?t affect F-E scores.  
 
 Dev WMT2009 WMT2010 
LMERT (bug) 25.26 26.85 27.55 
LMERT 
 (no bug) 
25.43 26.89 28.07 
 
Table 1: LMERT bug fix (E-F BLEU after rescoring) 
4.2 Fixing odd translations 
After the evaluation, we carefully studied the 
system outputs on the WMT 2010 test data, par-
ticularly for E-F. Apart from truecasing errors, 
we noticed two kinds of bad behaviour: transla-
tions of proper names and apparent passthrough 
of English words to the French side.  
Examples of E-F translations of proper names 
from our WMT 2010 submission (each from a 
different sentence): 
 
Mr. Onderka ? M. Roman, Luk?? Marvan ? G. 
Luk??, Janey ? The, Janette Tozer ? Janette, 
Aysel Tugluk ? joints tugluk, Tawa Hallae ? 
Ottawa, Oleson ?  production,  Alcobendas ?  ; 
 
When the LMERT bug was fixed, some but 
not all of these bad translations were corrected 
(e.g., 3 of the 8 examples above were corrected). 
Our system passes OOV words through un-
changed. Thus, the names above aren?t OOVs, 
but words that occur rarely in the training data, 
and for which bad alignments have a dispropor-
tionate effect. We realized that when a source 
word begins with a capital, that may be a signal 
that it should be passed through. We thus de-
signed a passthrough feature function that applies 
to all capitalized forms not at the start of a sen-
tence (and also to forms at the sentence start if 
they?re capitalized elsewhere). Sequences of one 
or more capitalized forms are grouped into a 
phrase suggestion (e.g., Barack Obama ? bar-
rack obama) which competes with phrase table 
entries and is assigned a weight by MERT. 
The passthrough feature function yields a tiny 
improvement over the E-F system with the bug-
fixed LMERT on the dev corpus (WMT2008): 
+0.06 BLEU (without rescoring). It yields a larg-
er improvement on our test corpus: +0.27 BLEU 
(without rescoring). Furthermore, it corrects all 
the examples from the WMT 2010 test shown 
above (after the LMERT bug fix 5 of the 8 ex-
amples above still had problems, but when the 
passthrough function is incorporated all of them 
go away). Though the BLEU gain is small, we 
are happy to have almost eradicated this type of 
error, which human beings find very annoying.  
The opposite type of error is apparent pass-
through. For instance, ?we?re? appeared 12 times 
in the WMT 2010 test data, and was translated 6 
times into French as ?we?re? - even though better 
translations had higher forward probabilities. The 
source of the problem is the backward probabili-
ty P(E=?we?re?|F=?we?re?), which is 1.0; the 
backward probabilities for valid French transla-
tions of ?we?re? are lower. Because of the high 
probability P(E=?we?re?|F=?we?re?) within the 
loglinear combination, the decoder often chooses 
?we?re? as the French translation of ?we?re?. 
The (E=?we?re?, F=?we?re?) pair in WMT 
2010 phrase tables arose from two sentence pairs 
where the ?French? translation of an English sen-
tence is a copy of that English sentence. In both, 
the original English sentence contains ?we?re?. 
Naturally, the English words on the ?French? 
side are word-aligned with their identical twins 
on the English side. Generally, if the training 
data has sentence pairs where the ?French? sen-
tence contains words from the English sentence, 
those words will get high backward probabilities 
of being translated as themselves. This problem 
may not show up as an apparent passthrough; 
instead, it may cause MERT to lower the weight 
of the backward probability component, thus 
hurting performance.  
We estimated English contamination of the 
French side of the parallel training data by ma-
130
nually inspecting a random sample of ?French? 
sentences containing common English function 
words. Manual inspection is needed for accurate 
estimation: a legitimate French sentence might 
contain mostly English words if, e.g., it is short 
and cites the title of an English work (this 
wouldn?t count as contamination). The degree of 
contamination is roughly 0.05% for Europarl, 
0.5% for news-commentary, 0.5% for UN, and 
1% for GigaFrEn (in these corpora the French is 
also contaminated by other languages, particular-
ly German). Foreign contamination of English 
for these corpora appears to be much less fre-
quent.  
Contamination can take strange forms. We ex-
pected to see English sentences copied over in-
tact to the French side, and we did, but we did 
not expect to see so many ?French? sentences 
that interleaved short English word sequences 
with short French word sequences, apparently 
because text with an English and a French col-
umn had been copied by taking lines from alter-
nate columns. We found many of these inter-
leaved ?French? sentences, and found some of 
them in exactly this form on the Web (i.e., the 
corruption didn?t occur during WMT data collec-
tion). The details may not matter: whenever the 
?French? training sentence contains words from 
its English twin, there can be serious damage via 
backward probabilities. 
To test this hypothesis, we filtered all parallel 
and monolingual training data for the E-F system 
with a language guessing tool called text_cat 
(Cavnar and Trenkle, 1994). From parallel data, 
we filtered out sentence pairs whose French side 
had a high probability of not being French; from 
LM training data, sentences with a high non-
French probability. We set the filtering level by 
inspecting the guesser?s assessment of news-
commentary sentences, choosing a rather aggres-
sive level that eliminated 0.7% of news-
commentary sentence pairs. We used the same 
level to filter Europarl (0.8% of sentence pairs 
removed), UN (3.4%), GigaFrEn (4.7%), and 
?mono? (4.3% of sentences).  
 
 Dev WMT2009 WMT2010 
Baseline 25.23 26.47 27.72 
Filtered 25.45 26.66 27.98 
 
Table 2: Data filtering (E-F BLEU, no rescoring) 
 
Table 2 shows the results: a small but consis-
tent gain (about +0.2 BLEU without rescoring). 
We have not yet confirmed the hypothesis that 
copies of source-language words in the paired 
target sentence within training data can damage 
system performance via backward probabilities.  
4.3 Fixing problems with LM training   
Post-evaluation, we realized that our arrange-
ment of the training data for the LMs for both 
language directions was flawed. The grouping 
together of disparate corpora in ?mono? and 
?domain? didn?t allow higher-quality, truly in-
domain corpora to be weighted more heavily 
(e.g., the news corpora should have higher 
weights than Europarl, but they are lumped to-
gether in ?mono?). There are also potentially 
harmful overlaps between LMs (e.g., GigaFrEn 
is used both inside and outside the dynamic LM).  
We trained a new set of French LMs for the E-
F system, which replaced all the French LMs 
(#5-7) described in section 2.3 in the E-F system: 
1. 5-gram LM trained on news-commentary 
and shuffle;  
2. Dynamic LM based on 4 5-gram LMs 
trained on French side of parallel data 
(LM trained on GigaFrEn, LM on UN, 
LM on Europarl, and LM on news-
commentary). 
We did not apply the passthrough function or 
language filtering (section 4.2) to any of the 
training data for any component (LMs, TMs, dis-
tortion models) of this system; we did use the 
bug-fixed version of LMERT (section 4.1). 
The experiments with these new French LMs 
for the E-F system yielded a small decrease of 
NRC BLEU on dev (-0.15) and small increases 
on WMT Newstest 2009 and Newstest 2010 
(+0.2 and +0.4 respectively without rescoring). 
We didn?t do F-E experiments of this type.  
4.4 Pooling improvements   
The improvements above were (individual un-
cased E-F BLEU gains without rescoring in 
brackets): LMERT bug fix (about +0.5); pass-
through feature function (+0.1-0.3); language 
filtering for French (+0.2). There was also a 
small gain on test data by rearranging E-F LM 
training data, though the loss on ?dev? suggests 
this may be a statistical fluctuation. We built 
these four improvements into the evaluation E-F 
system, along with quote normalization: in all 
training and test data, diverse single quotes were 
mapped onto the ascii single quote, and diverse 
double quotes were mapped onto the ascii double 
quote. The average result on WMT2009 and 
WMT2010 was +1.7 BLEU points compared to 
the original system, so there may be synergy be-
131
tween the improvements. The original system 
had gained +0.3 from rescoring, while the final 
improved system only gained +0.1 from rescor-
ing: a post-evaluation rescored gain of +1.5.  
An experiment in which we dropped lexica-
lized distortion from the improved system 
showed that this component yields about +0.2 
BLEU. Much earlier, when we were still training 
systems with N-best MERT, incorporation of the 
6-feature lexicalized distortion often caused 
scores to go down (by as much as 2.8 BLEU). 
This illustrates how LMERT can make incorpo-
ration of many more features worthwhile.  
4.5 Fixing truecasing  
Our truecaser doesn?t work as well as truecasers 
of other WMT groups: we lost 1.4 BLEU by tru-
ecasing in both language directions, while others 
lost 1.0 or less. To improve our truecaser, we 
tried: 1. Training it on all relevant data and 2. 
Collecting 3-gram case-pattern statistics instead 
of unigrams. Neither of these helped significant-
ly. One way of improving the truecaser would be 
to let case information from source words influ-
ence the case of the corresponding target words. 
Alternatively, one of the reviewers stated that 
several labs involved in WMT have no separate 
truecaser and simply train on truecase text. We 
had previously tried this approach for NIST Chi-
nese-English and discarded it because of its poor 
performance. We are currently re-trying it on 
WMT data; if it works better than having a sepa-
rate truecaser, this was yet another area where 
lessons from Chinese-English were misleading. 
5 Lessons  
LMERT is an improvement over N-best MERT. 
The submitted system was one for which N-best 
MERT happened to work very badly, so we got 
ridiculously large gains of +1.5-3.5 BLEU for 
non-buggy LMERT over N-best MERT. These 
results are outliers: in experiments with similar 
configurations, we typically get +0.2-1.0 for 
LMERT over N-best MERT. Post-evaluation, 
four minor improvements ? a case-based pass-
through function, language filtering, LM rear-
rangement, and quote normalization ? collective-
ly gave a nice improvement. Nothing we tried 
helped truecaser performance significantly, 
though we have some ideas on how to proceed. 
We learned some lessons from WMT 2010. 
Always test your system on the relevant lan-
guage pair. Our original version of LMERT was 
developed on Chinese-English and worked well 
there, but had a bug that surfaced only when the 
target language had accents.  
European language pairs are more porous to 
information than Chinese-English. Our WMT 
system reflected design decisions for Chinese-
English, and thus didn?t exploit case information 
in the source: it passed through OOVs to the tar-
get, but didn?t pass through upper-case words 
that are likely to be proper nouns.  
It is beneficial to remove foreign-language 
contamination from the training data.  
When entering an evaluation one hasn?t parti-
cipated in for several years, always read system 
papers from the previous year. Some of the 
WMT 2008 system papers mention passthrough 
of some non-OOVs, filtering out of noisy train-
ing data, and using the case of a source word to 
predict the case of the corresponding target word. 
References  
William Cavnar and John Trenkle. 1994. N-Gram-
Based Text Categorization. Proc. Symposium on 
Document Analysis and Information Retrieval, 
UNLV Publications/Reprographics, pp. 161-175. 
Daniel Cer, Daniel Jurafsky, and Christopher D. 
Manning. 2008. Regularization and search for min-
imum error rate training. Proc. Workshop on 
SMT, pp. 26-34. 
George Foster and Roland Kuhn. 2009. Stabilizing 
Minimum Error Rate Training. Proc. Workshop 
on SMT, pp. 242-249. 
George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. Proc. Workshop on 
SMT, pp. 128-135. 
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language 
Models.  Proc. ACL, pp.  144-151. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Transcription Evalua-
tion. MT Eval. Workshop. 
Wolfgang Macherey, Franz Josef Och, Ignacio Thay-
er, and Jakob Uszkoreit. 2008. Lattice-based Min-
imum Error Rate Training for Statistical Machine-
Translation. Conf. EMNLP, pp. 725-734. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation.  Proc. ACL, 
pp. 160-167.  
Richard Zens and Hermann Ney. 2004. Improvements 
in Phrase-Based Statistical Machine Translation. 
Proc. HLT/NAACL, pp. 257-264. 
132
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 59?63,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Improving AMBER, an MT Evaluation Metric 
 
Boxing Chen, Roland Kuhn and George Foster 
 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
 
{Boxing.Chen, Roland.Kuhn, George.Foster}@nrc.ca 
 
  
Abstract 
A recent paper described a new machine 
translation evaluation metric, AMBER. This 
paper describes two changes to AMBER. The 
first one is incorporation of a new ordering 
penalty; the second one is the use of the 
downhill simplex algorithm to tune the 
weights for the components of AMBER. We 
tested the impact of the two changes, using 
data from the WMT metrics task. Each of the 
changes by itself improved the performance of 
AMBER, and the two together yielded even 
greater improvement, which in some cases 
was more than additive. The new version of 
AMBER clearly outperforms BLEU in terms 
of correlation with human judgment.  
1 Introduction 
AMBER is a machine translation evaluation metric 
first described in (Chen and Kuhn, 2011). It is de-
signed to have the advantages of BLEU (Papineni 
et al, 2002), such as nearly complete language 
independence and rapid computability, while at-
taining even higher correlation with human judg-
ment. According to the paper just cited: ?It can be 
thought of as a weighted combination of dozens of 
computationally cheap features based on word sur-
face forms for evaluating MT quality?. Many re-
cently defined machine translation metrics seek to 
exploit deeper sources of knowledge than are 
available to BLEU, such as external lexical and 
syntactic resources. Unlike these and like BLEU, 
AMBER relies entirely on matching surface forms 
in tokens in the hypothesis and reference, thus sac-
rificing depth of knowledge for simplicity and 
speed.  
In this paper, we describe two improvements to 
AMBER. The first is a new ordering penalty called 
?v? developed in (Chen et al, 2012). The second 
remedies a weakness in the 2011 version of 
AMBER  by carrying out automatic, rather than 
manual, tuning of this metric?s free parameters; we 
now use the simplex algorithm to do the tuning. 
2 AMBER 
AMBER is the product of a score and a penalty, as 
in Equation (1); in this, it resembles BLEU. How-
ever, both the score part and the penalty part are 
more sophisticated than in BLEU. The score part 
(Equation 2) is enriched by incorporating the 
weighted average of n-gram precisions (AvgP), the 
F-measure derived from the arithmetic averages of 
precision and recall (Fmean), and the arithmetic 
average of F-measure of precision and recall for 
each n-gram (AvgF). The penalty part is a 
weighted product of several different penalties 
(Equation 3). Our original AMBER paper (Chen 
and Kuhn, 2011) describes the ten penalties used at 
that time; two of these penalties, the normalized 
Spearman?s correlation penalty and the normalized 
Kendall?s correlation penalty, model word reorder-
ing.  
 
penaltyscoreAMBER ?=                 (1)  
AvgF
FmeanAvgPscore
???+
?+?=
)1( 21
21
??
??
  
      (2) 
?
=
=
P
i
w
i
ipenpenalty
1
                           (3) 
where 1?  and 2?  are weights of each score com-
ponent; wi is the weight of each penalty peni. 
59
In addition to the more complex score and pen-
alty factors, AMBER differs from BLEU in two 
other ways: 
? Not only fixed n-grams, but three different 
kinds of flexible n-grams, are used in com-
puting scores and penalties.  
? The AMBER score can be computed with 
different types of text preprocessing, i.e. 
different combinations of several text pre-
processing techniques: lowercasing, to-
kenization, stemming, word splitting, etc. 8 
types were tried in (Chen and Kuhn, 2011). 
When using more than one type, the final 
score is computed as an average over runs, 
one run per type. In the experiments re-
ported below, we averaged over two types 
of preprocessing. 
3 Improvements to AMBER 
3.1   Ordering penalty v 
We use a simple matching algorithm (Isozaki et 
al., 2010) to do 1-1 word alignment between the 
hypothesis and the reference.  
After word alignment, represent the reference by 
a list of normalized positions of those of its words 
that were aligned with words in the hypothesis, and 
represent the hypothesis by a list of positions for 
the corresponding words in the reference. For both 
lists, unaligned words are ignored. E.g., let P1 = 
reference, P2 = hypothesis: 
P1: 11p  
2
1p  
3
1p  
4
1p  ? 
ip1  ? 
np1  
 P2: 12p  
2
2p  
3
2p  
4
2p  ? 
ip2  ? 
np2
 
Suppose we have 
Ref: in the winter of 2010 , I visited Paris 
Hyp: I visited Paris in 2010 ?s winter 
Then we obtain 
P1: 1 2 3 4 5 6  (the 2nd word ?the?, 4th 
word ?of? and 6th word ?,? in the reference 
are not aligned to any word in the 
hypothesis. Thus, their positions are not in 
P1, so the positions of the matching words 
?in winter 2010 I visited Paris? are nor-
malized to 1 2 3 4 5 6) 
P2: 4 5 6 1 3 2 (the word ??s? was 
unaligned).  
The ordering metric v is computed from two 
distance measures. The first is absolute 
permutation distance: 
?
=
?=
n
i
ii ppPPDIST
1
21211 ||),(               (4) 
Let       
2/)1(
),(1 2111 +?= nn
PPDIST
?                     (5)                  
v1 ranges from 0 to 1; a larger value means more 
similarity between the two permutations. This 
metric is similar to Spearman?s ? (Spearman, 
1904). However, we have found that ? punishes 
long-distance reordering too heavily. For instance, 
1?
 
is more tolerant than ? of the movement of 
?recently? in this example:  
Ref: Recently , I visited Paris 
Hyp: I visited Paris recently  
P1: 1 2 3 4 
P2: 2 3 4 1 
Its 2.0-1 1)4(16
)9116(1
?==
?
+++? ; however, its  
4.0-1 1)/24(4 3111 == + +++1v . 
Inspired by HMM word alignment (Vogel et al, 
1996), our second distance measure is based on 
jump width. This punishes only once a sequence of 
words that moves a long distance with the internal 
word order conserved, rather than on every word. 
In the following, only two groups of words have 
moved, so the jump width punishment is light: 
Ref: In the winter of 2010, I visited Paris 
Hyp: I visited Paris in the winter of 2010  
The second distance measure is 
?
=
??
???=
n
i
iiii ppppPPDIST
1
1
22
1
11212 |)()(|),(   (6) 
where we set 001 =p  and 0
0
2 =p . Let 
1
),(1 2 2122
?
?=
n
PPDIST
v                     (7) 
As with v1, v2 is also from 0 to 1, and larger values 
indicate more similar permutations. The ordering 
measure vs is the harmonic mean of v1 and v2 (Chen 
et al, 2012):  
)11(2 21 /v/v/vs +=
 
.                     (8) 
 In (Chen et al, 2012) we found this to be slightly 
more effective than the geometric mean. vs in (8) is 
computed at segment level. We compute document 
level ordering vD with a weighted arithmetic mean:  
60
?
?
=
=
?
= l
s s
l
s ss
D
Rlen
Rlenv
v
1
1
)(
)(
                    (9) 
where l is the number of segments of the 
document, and len(R) is the length of the reference 
after text preprocessing. vs is the segment-level 
ordering penalty. 
Recall that the penalty part of AMBER is the 
weighted product of several component penalties. 
In the original version of AMBER, there were 10 
component penalties. In the new version, v is in-
corporated as an additional, 11th weighted penalty 
in (3). Thus, the new version of AMBER incorpo-
rates three reordering penalties: Spearman?s 
correlation, Kendall?s correlation, and v. Note that 
v is also incorporated in a tuning metric we recent-
ly devised (Chen et al, 2012).   
3.2   Automatic tuning 
In (Chen and Kuhn, 2011), we manually set the 17 
free parameters of AMBER (see section 3.2 of that 
paper). In the experiments reported below, we 
tuned the 18 free parameters ? the original 17 plus 
the ordering metric v described in the previous sec-
tion - automatically, using the downhill simplex 
method of (Nelder and Mead, 1965) as described 
in (Press et al, 2002). This is a multidimensional 
optimization technique inspired by geometrical 
considerations that has shown good performance in 
a variety of applications.  
4 Experiments 
The experiments are carried out on WMT metric 
task data: specifically, the WMT 2008, WMT 
2009, WMT 2010, WMT 2011 all-to-English, and 
English-to-all submissions. The languages ?all? 
(?xx? in Table 1) include French, Spanish, German 
and Czech. Table 1 summarizes the statistics for 
these data sets. 
 
Set Year Lang. #system #sent-pair 
Test1 2008 xx-En 43 7,804 
Test2 2009 xx-En 45 15,087 
Test3 2009 en-Ex 40 14,563 
Test4 2010 xx-En 53 15,964 
Test5 2010 en-xx 32 18,508 
Test6 2011 xx-En 78 16,120 
Test7 2011 en-xx 94 23,209 
 
Table 1: Statistics of the WMT dev and test sets. 
 
We used 2008 and 2011 data as dev sets, 2009 
and 2010 data as test sets. Spearman?s rank 
correlation coefficient ? was employed to measure 
correlation of the metric with system-level human 
judgments of translation. The human judgment 
score was based on the ?Rank? only, i.e., how 
often the translations of the system were rated as 
better than those from other systems (Callison-
Burch et al, 2008). Thus, BLEU and the new ver-
sion of AMBER were evaluated on how well their 
rankings correlated with the human ones. For the 
segment level, we followed (Callison-Burch et al, 
2010) in using Kendall?s rank correlation 
coefficient ?. 
In what follows, ?AMBER1? will denote a vari-
ant of AMBER as described in (Chen and Kuhn, 
2011). Specifically, it is the variant AMBER(1,4) ? 
that is, the variant in which results are averaged 
over two runs with the following preprocessing: 
1. A run with tokenization and lower-casing 
2. A run in which tokenization and lower-
casing are followed by the word splitting. 
Each word with more than 4 letters is seg-
mented into two sub-words, with one being 
the first 4 letters and the other the last 2 let-
ters. If the word has 5 letters, the 4th letter 
appears twice: e.g., ?gangs? becomes 
?gang? + ?gs?. If the word has more than 6 
letters, the middle part is thrown away.  
The second run above requires some explana-
tion. Recall that in AMBER, we wish to avoid use 
of external resources such as stemmers and mor-
phological analyzers, and we aim at maximal lan-
guage independence. Here, we are doing a kind of 
?poor man?s morphological analysis?. The first 
four letters of a word are an approximation of its 
stem, and the last two letters typically carry at least 
some information about number, gender, case, etc. 
Some information is lost, but on the other hand, 
when we use the metric for a new language (or at 
least, a new Indo-European language) we know 
that it will extract at least some of the information 
hidden inside morphologically complex words. 
The results shown in Tables 2-4 compare the 
correlation of variants of AMBER with human 
judgment; Table 5 compares the best version of 
AMBER (AMBER2) with BLEU. For instance, to 
calculate segment-level correlations using 
61
Kendall?s ?, we carried out 33,071 paired compari-
sons for out-of-English and 31,051 paired compar-
isons for into-English. The resulting ? was 
calculated per system, then averaged for each con-
dition (out-of-English and into-English) to obtain 
one out-of-English value and one into-English val-
ue. 
First, we compared the performance of 
AMBER1 with a version of AMBER1 that in-
cludes the new reordering penalty v, at the system 
and segment levels. The results are shown in Table 
2. The greatest impact of v is on ?out of English? at 
the segment level, but none of the results are par-
ticularly impressive.  
 
 AMBER1 +v Change 
Into-En 
System 
0.860 0.862 0.002 
(+0.2%) 
Into-En 
Segment 
0.178 0.180 0.002 
 (+1.1%) 
Out-of-En 
System 
0.637 0.637 0 
 (0%) 
Out-of-En 
Segment 
0.167 0.170 0.003 
(+1.8%) 
 
Table 2: Correlation with human judgment for 
AMBER1 vs. (AMBER1 including v). 
 
Second, we compared the performance of manu-
ally tuned AMBER1 with AMBER1 whose param-
eters were tuned by the simplex method. The 
tuning was run four times on the dev set, once for 
each possible combination of into/out-of English 
and system/segment level. Table 3 shows the re-
sults on the test set. This change had a greater im-
pact, especially on the segment level. 
 
 AMBER1 +Simplex Change 
Into-En 
 System 
0.860 0.862 0.002 
(+0.2%) 
Into-En 
Segment 
0.178 0.184 0.006  
(+3.4%) 
Out-of-En 
 System 
0.637 0.637 0 
 (0%) 
Out-of-En  
Segment 
0.167 0.182 0.015 
(+9.0%) 
 
Table 3: Correlation with human judgment for 
AMBER1 vs. simplex-tuned AMBER1. 
 
Then, we compared the performance of 
AMBER1 with AMBER1 that contains v and that 
has been tuned by the simplex method. We will 
denote the new version of AMBER containing 
both changes ?AMBER2?. It will be seen from 
Table 4 that AMBER2 is a major improvement 
over AMBER1 at the segment level. In the case of 
?into English? at the segment level, the impact of 
the two changes seems to have been synergistic: 
adding together the percentage improvements due 
to v and simplex from Tables 2 and 3, one would 
have expected an improvement of 4.5% for both 
changes together, but the actual improvement was 
6.2%. Furthermore, there was no improvement at 
the system level for ?out of English? when either 
change was tried separately, but there was a small 
improvement when the two changes were com-
bined.  
 
 AMBER1 AMBER2 Change 
Into-En 
System 
0.860 0.870 0.010 
(+1.2%) 
Into-En 
Segment 
0.178 0.189 0.011 
(+6.2%) 
Out-of-En 
System 
0.637 0.642 0.005 
(+0.8%) 
Out-of-En 
Segment 
0.167 0.184 0.017 
(+10.2%) 
 
Table 4: Correlation with human judgment for 
AMBER1 vs. AMBER2. 
 
Of course, the most important question is: does 
the new version of AMBER (AMBER2) perform 
better than BLEU? Table 5 answers this question 
(the version of BLEU used here was smoothed 
BLEU (mteval-v13a)). There is a clear advantage 
for AMBER2 over BLEU at both the system and 
segment levels, for both ?into English? and ?out of 
English?.  
 
 BLEU AMBER2 Change 
Into-En 
System 
0.773 0.870 0.097 
(+12.5%) 
Into-En 
Segment 
0.154 0.189 0.035 
(+22.7%) 
Out-of-En 
System 
0.574 0.642 0.068 
(+11.8%) 
Out-of-En 
Segment 
0.149 0.184 0.035 
(+23.5%) 
 
Table 5: Correlation with human judgment for 
 BLEU vs. AMBER2. 
 
62
5 Conclusion 
We have made two changes to AMBER, a metric 
described in (Chen and Kuhn, 2011). In our exper-
iments, the new version of AMBER was shown to 
be an improvement on the original version in terms 
of correlation with human judgment. Furthermore, 
it outperformed BLEU by about 12% at the system 
level and about 23% at the segment level.  
A good evaluation metric is not necessarily a 
good tuning metric, and vice versa. In parallel with 
our work on AMBER for evaluation, we have also 
been exploring a machine translation tuning metric 
called PORT (Chen et al, 2012). AMBER and 
PORT differ in many details, but they share the 
same underlying philosophy: to exploit surface 
similarities between hypothesis and references 
even more thoroughly than BLEU does, rather than 
to invoke external resources with richer linguistic 
knowledge. So far, the results for PORT have been 
just as encouraging as the ones for AMBER re-
ported here.  
Reference 
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, M. 
Przybocki and O. Zaidan. 2010. Findings of the 2010 
Joint Workshop on Statistical Machine Translation 
and Metrics for Machine Translation. In Proceedings 
of WMT. 
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz and 
J. Schroeder. 2008. Further Meta-Evaluation of Ma-
chine Translation. In Proceedings of WMT. 
B. Chen, R. Kuhn, and S. Larkin. 2012. PORT:  a Preci-
sion-Order-Recall MT Evaluation Metric for Tuning. 
Accepted for publication in Proceedings of ACL. 
B. Chen and R. Kuhn. 2011. AMBER: a Modified 
BLEU, Enhanced Ranking Metric. In Proceedings of 
the Sixth Workshop on Statistical Machine Transla-
tion, Edinburgh, Scotland.  
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada. 
2010. Automatic Evaluation of Translation Quality 
for Distant Language Pairs. In Proceedings of 
EMNLP.  
J. Nelder and R. Mead. 1965. A simplex method for 
function minimization. Computer Journal V. 7, pages 
308?313. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL. 
W. Press, S. Teukolsky, W. Vetterling and B. Flannery. 
2002. Numerical Recipes in C++. Cambridge Uni-
versity Press, Cambridge, UK.  
C. Spearman. 1904. The proof and measurement of as-
sociation between two things. In American Journal of 
Psychology, V. 15, pages 72?101. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based 
word alignment in statistical translation. In Proceed-
ings of COLING. 
63
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 499?509,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Linear Mixture Models for Robust Machine Translation
Marine Carpuat, Cyril Goutte and George Foster
Multilingual Text Processing
National Research Council
Ottawa, ON K1A0R6, Canada
firstname.lastname@nrc.ca
Abstract
As larger and more diverse parallel texts
become available, how can we lever-
age heterogeneous data to train robust
machine translation systems that achieve
good translation quality on various test
domains? This challenge has been ad-
dressed so far by repurposing techniques
developed for domain adaptation, such
as linear mixture models which combine
estimates learned on homogeneous sub-
domains. However, learning from large
heterogeneous corpora is quite different
from standard adaptation tasks with clear
domain distinctions. In this paper, we
show that linear mixture models can re-
liably improve translation quality in very
heterogeneous training conditions, even
if the mixtures do not use any domain
knowledge and attempt to learn generic
models rather than adapt them to the tar-
get domain. This surprising finding opens
new perspectives for using mixture mod-
els in machine translation beyond clear cut
domain adaptation tasks.
1 Introduction
While machine translation tasks used to be de-
fined by drawing training and test data from a sin-
gle well-defined domain, current systems have to
deal with increasingly heterogeneous data, both at
training and at test time. As larger and more di-
verse parallel texts become available, how can we
leverage heterogeneous data to train statistical ma-
chine translation (SMT) systems that achieve good
translation quality on various test domains?
So far, this challenge has been addressed by re-
purposing techniques developed for more clear-cut
domain adaptation scenarios, such as linear mix-
ture models (Koehn and Schroeder, 2007; Foster
and Kuhn, 2007; Sennrich, 2012b). Instead of es-
timating models on the whole training corpus at
once, linear mixture models are built as follows:
(1) partition the training corpus into homogeneous
domain-based component, (2) train one model per
component, (3) linearly mix models using weights
learned to adapt to the test domain, (4) replace re-
sulting model in translation system.
In this paper, we aim to gain a better under-
standing of the benefits of linear mixture models in
heterogeneous data conditions, by examining key
untested assumptions:
? Should mixture component capture domain
information? Previous work assumes that
training data should be organized into do-
mains. When manual domain distinctions are
not available, previous work uses clustering
approaches to approximate manual domain
distinctions (Sennrich, 2012a). However, it
is unclear whether it is necessary to use or
mimic domain distinctions in order to define
mixture components.
? Mixture models are usually assumed to im-
prove translation quality by giving more
weight to parts of the training corpus that are
more relevant to the test domain. Is this intu-
ition still valid in our more complex hetero-
geneous training conditions? If not, how do
mixture models affect translation probability
estimates?
In order to answer these questions, we propose
to study several variants of linear mixture mod-
els that reflect different modeling assumptions and
different levels of domain knowledge. We first
499
consider two methods for setting mixture weights:
adaptation to the test domain via maximum like-
lihood, and uniform mixtures that make no as-
sumption about the domain of interest (Section 2).
Then, we will describe a wide range of tech-
niques that can be used to define mixture com-
ponents (Section 3). Again, these techniques re-
flect opposite modeling assumptions: manually
defined domains and automatic clusters attempt to
organize heterogeneous training sets into homo-
geneous groups that represent distinct domains,
while random samples capture no domain infor-
mation and simply provide different views of the
training set.
We present an empirical investigation of all the
variations outlined above using a strong system
trained on large and diverse training corpora, for
two language pairs and two distinct test domains.
Our results show that linear mixtures reliably and
robustly improve the quality of machine transla-
tion (Section 5). While they were originally de-
veloped for domain adaptation tasks, linear mix-
tures that have no domain knowledge can perform
as well as traditional mixtures meant to perform
domain adaptation. This suggests that improve-
ments do not stem from domain modeling per se,
but from better generic estimates from the hetero-
geneous training data. Further analysis shows that
the linear mixture estimates are very different from
estimates obtained using more explicit smoothing
schemes (Section 6).
2 Linear Mixtures for Translation
Models
Does domain knowledge yield better translation
quality when learning linear mixture weights for
the translation model of a phrase-based MT sys-
tem? We leave the study of linear mixtures for
language and reordering models for future work.
2.1 Maximum Likelihood Mixtures
In the standard domain adaptation scenario, the
linear mixture combines translation probabilities
learned on distinct sub-domains in the training
corpus. The conditional translation probability of
phrase t given s is defined as:
p(t|s) =
K
?
k=1
?
k
p
k
(t|s) (1)
where p
k
(t|s) is a conditional translation proba-
bility learned on subset k of the training corpus.
Note that for all phrase pairs (s, t) that are not ob-
served in component k of the training corpus, we
will have p
k
(t|s) = 0. As a result, the resulting
distributions are not normalized.
The weights ?
k
are learned to adapt the transla-
tion model to a development set, which represents
the domain of interest. First, we extract all phrase
pairs from the development set, using the same
technique used to extract phrases from the training
set as part of standard phrase-based MT training.
This yields a joint distribution p?(s, t), which can
be used to define a maximum likelihood objective:
?
? = argmax
?
?
s,t
p?(s, t) log
K
?
k=1
?
k
p
k
(s|t). (2)
We use the Expectation Maximization algo-
rithm to solve this maximization problem.
2.2 Uniform Mixtures
We will consider uniform mixtures where all com-
ponents are weighted equally:
p(t|s) =
1
K
K
?
k=1
p
k
(t|s). (3)
In contrast with maximum likelihood mixtures,
uniform mixtures are not meant to adapt the trans-
lation model to a specific test domain. Instead,
they combine estimates learned on various subsets
of the data in the hope of obtaining a better es-
timate of the translation probability distributions
from the (possibly heterogeneous) training domain
as a whole.
2.3 Why Not Use Loglinear Mixtures?
In current machine translation systems, there are
two straightforward ways to combine estimates
from heterogneous training data: linear and loglin-
ear mixtures. We argue that linear mixtures are a
better model for combining domain-specific prob-
abilities, since they sum translation probabilities,
while loglinear mixtures multiply probabilities. In
a loglinear mixture, a translation candidate t for a
phrase s will only be scored highly if all compo-
nents agree that it is highly probable. In contrast,
in a linear mixture, t can be a top translation can-
didate overall even if it is not a preferred transla-
tion in some of the components. When the train-
ing data is very heterogeneous, linear mixtures are
therefore preferable.
500
Previous work provides empirical evidence sup-
porting this. For instance, Foster et al. (2010)
found that linear mixtures outperform log linear
mixtures when adapting a French-English system
to the medical domain, as well as on a Chinese-
English NIST translation task.
2.4 Estimating Conditional Translation
Probabilities
Within each mixture component, we extract all
phrase-pairs, compute relative frequencies, and
use Kneser-Ney smoothing (Chen et al., 2011) to
produce the final estimate of conditional transla-
tion probabilities p
k
(t|s). Per-component proba-
bilities are then combined in Eq. 1 and 3. Simi-
larly, baseline translation probabilities are learned
using Kneser-Ney smoothed frequencies collected
on the entire training set.
3 Defining Mixture Components
We assume that the heterogeneous training corpus
can be split into basic elements that will be or-
ganized in various ways to define the K mixture
components. Basic components could be docu-
ments or sets of sentences defined along various
criteria. Sennrich (2012a) show that using iso-
lated sentences as basic elements might not pro-
vide sufficient information, as smoothing com-
ponent assignments using neighboring sentences
benefits translation quality. In our experiments,
basic elements are sets of parallel sentences which
share the same provenance, genre and dialect, as
we will see in Section 4.
We consider four very different ways of defin-
ing mixture components by grouping the basic
corpus elements: (1) manual partition of the train-
ing corpus into domains, (2) automatically learn-
ing homogeneous domains using text clustering
algorithms, (3) random partitioning, (4) sampling
with replacement.
3.1 Manually Defined Domains
Heterogeneous training data is usually grouped
into domains manually using provenance informa-
tion. In most previous work, such domain dis-
tinctions are very clear and easy to define. For
instance, Haddow (2013) uses European parlia-
ment proceedings to improve translation of text
in the movie subtitles and News Commentary do-
mains; Sennrich (2012a) aims to translate Alpine
Club reports using components trained on Euro-
pean parliament proceedings and movie subtitles.
Foster et al. (2010) work with a slightly differ-
ent setting when defining mixture components for
the NIST Chinese-English translation task: while
there is no single obvious ?in-domain? component
in the NIST training set, homogeneous domains
can still be defined in a straightforward fashion
based on the provenance of the data (e.g., Hong
Kong Hansards vs. Hong Kong Law vs. News ar-
ticles from FBIS, etc.). We take a similar approach
in our experiments. However, we will see that
since our training data is very heterogeneous, we
take into account other dimensions beyond prove-
nance, such as genre and dialect information (Sec-
tion 4).
3.2 Induced Domains Using Automatic
Clustering Algorithms
We propose to use automatic text clustering tech-
niques to organize basic elements into homoge-
neous clusters that are seen as sub-domains. In our
experiments, we apply clustering algorithms to the
target (English) side of the corpus only.
Each corpus element is transformed into a
vector-space format by constructing a tf.idf vector
representation. After indexing, we filter out stop-
words as well as words occuring in a single doc-
ument. We then weight each word token by the
log of its frequency in the document, combined
with an inverse document frequency (Salton and
McGill, 1983) followed by a normalization to unit
length. The cosine similarity between each pair
of elements is obtained by simply computing the
scalar product, resulting in aN?N similarity ma-
trix, where N is the number of corpus elements.
For clustering, we used Ward?s hierarchical
clustering algorithm (Ward, 1963). We start with
one cluster per corpus element, i.e. N clusters.
From the similarity matrix, we identify the two
most similar clusters and merge them into a sin-
gle one, resulting in N ?1 clusters. The similarity
matrix is updated using Ward?s method to form a
(N?1)?(N?1) similarity matrix. The process is
repeated on the new set of clusters, until we reach
the target number of clusters K.
3.3 Random Partitioning
We consider random partitions of the training cor-
pus. They are generated by using a random num-
ber generator to assign each basic element to one
of K clusters. Resulting components therefore do
not capture any domain information. Each com-
501
Arabic-English Training Conditions
segs src en
train 8.5M 262M 207M
Test Domain 1: Webforum
segs src en
dev (tune) 4.1k 66k 72k
web1 (eval) 2.2k 35k 38k
web2 (eval) 2.4k 37k 40k
Test Domain 2: News
segs src en
dev (tune) 1664 54k 51k
news (eval) 813 32k 29k
Table 1: Statistics for Arabic-English data: Num-
ber of segments (segs), source tokens (src) and En-
glish tokens (en) for each corpus. For English dev
and test sets, word counts averaged across 2 refer-
ences.
ponent can potentially be as heterogeneous as the
full training set.
3.4 Random Sampling with Replacement
All previous techniques assume that the training
corpus should be partitioned into distinct clus-
ters. We now consider mixture components that
break this assumption, and simply represent sev-
eral, possibly overlapping, views of the training
corpus. They are defined by sampling basic corpus
elements uniformly with replacement. This ap-
proach simply requires defining a number of sam-
ples K and the size n of each sample. We set the
sample size n to the average size of the manual
clusters. We do not fix K in advance: in order to
provide a fair comparison with corpus partitioning
techniques where components achieve coverage of
the entire training set by definition, we keep gen-
erating samples until all basic elements have been
used, and use all resulting K components.
When using uniform linear mixtures, this ap-
proach is similar to bootstrap aggregating (bag-
ging) for regression (Breiman, 1996), where a
more stable model is learned by averaging K es-
timates obtained by sampling the training set uni-
formly and with replacement.
4 Experiment Settings
We evaluate our linear mixture models on two
different language pairs, Arabic-English and
Chinese-English, and two different test domains.
Chinese-English Training Conditions
segs src en
train 11M 234M 253M
Test Domain 1: Webforum
segs src en
dev (tune) 2.7k 61k 77k
web1 (eval) 1.4k 31k 38k
web2 (eval) 1.2k 29k 36k
Test Domain 2: News
segs src en
dev (tune) 1.7k 39k 24k
news (eval) 0.7k 19k 19k
Table 2: Statistics for Chinese-English data: Num-
ber of segments (segs), source tokens (src) and En-
glish tokens (en) for each corpus. For English dev
and test sets, word counts averaged across 4 refer-
ences.
4.1 Training Conditions
We use the large-scale heterogeneous training con-
ditions defined in the DARPA BOLT project. Data
statistics for both language pairs are given in Ta-
bles 1 and 2. Training corpora cover a wide variety
of sources, genres, dialects, domains, topics.
For instance, for the Arabic task, the training
corpus is originally bundled into 48 files repre-
senting different provenance and epochs. The
data spans 15 genres (defined based on data
provenance, they range from lexicon to newswire,
United Nations, and many variants of web data
such as webforum, weblog, newsgroup, etc.) and
4 automatically tagged dialects (Egyptian, Levan-
tine, Modern Standard Arabic, and untagged). The
distribution along each of these dimensions is very
unbalanced, and each corpus file often contains
text in more than one genre, epoch or dialect.
As a result, we divide the large training corpus
into basic elements, based on the available meta-
data. We define basic corpus elements as a subset
of sentences from the same provenance (i.e. cor-
pus file), dialect and genre. For Arabic, splitting
the original 48 files along these dimensions yields
82 basic elements. Similarly, the Chinese data was
split into a set of 101 basic elements, using genre,
dialects, as well as time span information to split
the original files. Figure 1 shows the wide range of
component sizes in the Arabic and Chinese collec-
tion. For Arabice, notice that several components
are very small, from 6 lines and 90 words to 5.3
million lines and 137M words.
502
0 20 40 60 801
e+0
1
1e+
05
Arabic corpus components
corpus component
#lin
es 
/ #w
ord
s
l
lllll
ll
lllllllllllll
l
llllllllllllllllllll
lllllll
l
lllllll
lllllll
l
lll
l
ll
l
l
l
l
l
ll
l
ll
l
l #lines  #words   
0 20 40 60 80 100
1e+
03
1e+
05
1e+
07
Chinese corpus components
corpus component
#lin
es 
/ #w
ord
s
l
l
llllll
llllllllllll
llllllllllllll
lllllll
lllll
lll
l
lll
llll
l
l
l
lll
l
llllllllllll
llllllllllllll
llll
l
l
lll
l
l
l #lines  #words   
Figure 1: Sizes of the 82 Arabic-English (top)
and 101 Chinese-English (bottom) corpus compo-
nents.
4.2 Definition of Mixture Components
Manual partitions were created first by the system
developers, based on intuitions on the nature of the
test domain and manual inspection of the training
data. The main goal was to group data into com-
ponents that are large enough to reliably estimate
translation probabilities, but small enough to be
homogeneous. This resulted in K
m
= 10 clusters
for Arabic, and K
m
= 17 for Chinese.
Automatic partitions are created as described
in Section 3. Preliminary experiments with the
hierarchical agglomerative clustering algorithm
showed that the number of clusters used did not
have a big impact on translation quality,
1
so we
will only present results that use the same num-
ber of clusters as in the manual partitions (10 for
Arabic and 17 for Chinese).
Results for random partitions are averaged
across experiments run with four random seeds.
4.3 Test Domains
We consider two test domains, as described in Ta-
bles 1 and 2: webforum and news.
The webforum test domain is defined by devel-
opment test sets made available through BOLT. It
1
We tried K = {2, 4, . . . , 18, 20} for Arabic and K =
{12, 14, . . . , 20} for Chinese, plus all basic components.
contains very informal text drawn from online dis-
cussion of various topics. Taking these data sets
as the definition of the target domain, there is no
single obvious in-domain section of the training
corpus. For instance, for Arabic, the dev set sen-
tences are almost exclusively written in the Egyp-
tian dialect. Therefore, Egyptian webforum data
is presumably the closest to the test domain, but
Egyptian weblogs or mixed-dialect broadcast con-
versations could potentially be useful as well.
We also test the Arabic and Chinese systems on
the news domain. The goal of these experiments is
to evaluate the robustness of linear mixtures across
different test domains. We use publicly available
test sets from the NIST evaluation. The dev set
used to learn maximum likelihood mixtures and
tune the translation system is the NIST section of
the 2006 test set. We evaluate system performance
on the newswire section of the NIST 2008 test set.
4.4 Machine Translation System
We use an in-house implementation of a Phrase-
based Statistical Machine Translation system
(Koehn et al., 2007) to build strong baseline sys-
tems for both language pairs. Translation hypothe-
ses are scored according to the following features:
? 4 phrase-table scores: Kneser-Ney smoothed
phrasal translation probabilites and lexical
weights, in both translation directions (Chen
et al., 2011)
2
? 6 hierarchical lexicalized reordering scores
(Galley and Manning, 2008)
? a word penalty, and a word-displacement dis-
tortion penalty
? a Good-Turing smoothed 4-gram language
model trained on the Gigaword corpus,
Kneser-Ney smoothed 5-gram models trained
on the English side of the training corpus, and
an additional 5-gram model trained on mono-
lingual webforum data.
Weights for these features are learned using a
batch version of the MIRA algorithm (Chiang,
2012). Phrase pairs are extracted from several
word alignments of the training set: HMM, IBM2,
and IBM4. Word alignments are kept constant
across all experiments.
We apply our linear mixture models to both
translation probability scores, in each direction.
The reordering and language models are not
2
The Arabic-English system uses 6 additional binary fea-
tures which fire if a phrase-pair was generated by one of the
3 word alignment methods in each translation direction.
503
Test domain Webforum
Arabic eval Forum1 Forum2
Linear mix 39.67 40.60
Loglinear mix 37.53 38.80
Chinese eval Forum1 Forum2
Linear mix 30.17 26.86
Loglinear mix 27.65 23.78
Table 3: Impact of mixture type on translation
quality as measured by BLEU.
adapted. Note that systems used to translate the
web1 and web2 test sets are always tuned on the
webforum tuning set, while systems used to trans-
late data in the news domain are tuned on a news
development set. The relevant tuning set is also
used for learning maximum likelkihood mixtures
when appropriate.
5 Findings: Impact on Translation
Quality
5.1 Linear vs. Loglinear Mixtures
Before focusing exclusively on linear mixtures,
we confirm that they outperform loglinear mix-
tures. This comparison was conducted on the web-
forum domain, using manually defined domains
as components. For linear mixtures, we trained
the weights using maximum likelihood. Loglin-
ear mixture weights are trained by MIRA. Table 3
shows that linear mixtures yield consistently and
significantly higher BLEU scores than loglinear
mixtures, which is consistent with existing results
(Foster et al., 2010, inter alia).
5.2 Impact of Mixture Components
We now focus on linear mixtures and measure the
impact on translation quality of the various com-
ponent types described in Section 3. In all cases,
mixtures weights are estimated by maximum like-
lihood. Results are summarized in Table 4 for both
Arabic and Chinese.
The main result is that all mixture models con-
sidered significantly improve on the ?no mix?
baseline for both languages. Directly using the
101 basic elements for Chinese and the 82 basic
elements for Arabic significantly improves on the
baseline. Grouping the basic elements into coarser
clusters can further improve BLEU. For Arabic,
automatic partitioning (randomly or by clustering)
yields better BLEU scores than manual partition-
Test domain Webforum News
Arabic eval web1 web2 news
Cluster domains 40.11 40.60 57.95
Random partition 40.43 40.63 57.78
Random sample 39.94 40.36 57.85
Manual domains 39.67 40.60 57.63
Basic elements 39.83 40.63 57.57
No mix 38.64 39.21 56.59
Chinese eval web1 web2 news
Cluster domains 29.82 26.34 37.22
Random partition 29.50 26.21 36.83
Random sample 29.47 26.17 36.70
Manual domains 30.17 26.86 36.90
Basic elements 29.29 26.25 36.17
No mix 28.61 25.63 35.96
Table 4: Impact of mixture component definition
on BLEU score: there is no clear benefit to explic-
itly modeling domains.
ing, while the manual and cluster-based domains
yield the highest BLEU scores for Chinese.
5.3 Impact of Mixture Weights
Does domain knowledge yield better translation
quality when learning linear mixture weights? We
answer this question by comparing the transla-
tion quality obtained with maximum likelihood
vs. uniform mixtures. The maximum likelihood
weights are set once per domain, using the rele-
vant domain development set, while the uniform
mixture is the same across all test domains.
Table 5 shows that maximum likelihood
weights generally have a slight advantage over
uniform weights, especially in the Webforum do-
main. On ?basic elements? in Arabic, the gain is
a massive 5 BLEU points, which we attribute to
the fact that, as shown in Figure 1, there are many
more very small components in Arabic. Those get
a disproportionate influence in the uniform mix-
ture, hurting the overall performance. On the other
hand, the uniform mixture performs better in the
News domain. This might be explained by the fact
that the tune and test sets are more distant in News
than in Webforum, as suggested by the fact that the
tuning BLEU scores are not as good at predicing
test BLEU rankings in the news domain as in the
webforum domain.
Overall, the difference in performance between
the best linear mixture and the ?no mix? baseline
is 1.4 to 1.6 BLEU on Arabic, and 0.7 to 1.3 BLEU
504
on Chinese. By comparison, the delta between the
two weight setting approaches (maximum likeli-
hood vs. uniform), depending on the partition-
ing technique, is below 0.4 BLEU for Arabic (ex-
cept for Basic elements, +3.6 BLEU) and below
0.57 BLEU for Chinese. It is therefore clear that
the gain from using linear mixtures is much larger
than the influence of the mixture weight setting,
except in the one specific case discussed above.
Taken together, these results show that lin-
ear mixtures can reliably and robustly improve
the quality of machine translation. But surpris-
ingly, linear mixtures that have no domain knowl-
edge (random partition + uniform weights) can
sometimes perform as well as traditional mixtures
meant to perform domain adaptation. This sug-
gests that improvements cannot be only explained
by improved domain modeling.
Test domain Webforum News
Arabic eval web1 web2 news
Cluster domains 40.11 40.60 57.95
w/ uniform mix 39.63 40.15 58.21
Random partition 40.43 40.63 57.78
w/ uniform mix 40.31 40.15 58.18
Random sample 39.94 40.36 58.06
w/ uniform mix 39.88 40.56 58.65
Manual domains 39.67 40.60 57.63
w/ uniform mix 39.93 40.18 58.00
Basic elements 39.83 40.63 57.57
w/ uniform mix 34.84 35.82 58.46
No mix 38.64 39.21 56.59
Chinese eval web1 web2 news
Cluster domains 29.82 26.34 37.22
w/ uniform mix 29.44 25.94 37.47
Random partition 29.50 26.21 36.83
w/ uniform mix 29.43 25.89 36.95
Random sample 29.47 26.17 36.70
w/ uniform mix 28.47 25.54 36.61
Manual domains 30.17 26.86 36.90
w/ uniform mix 29.25 26.36 36.95
Basic elements 29.29 26.25 36.17
w/ uniform mix 29.23 25.81 36.63
No mix 28.61 25.63 35.96
Table 5: Impact of linear mixture weights on trans-
lation quality as measured by BLEU: using do-
main knowledge when setting weights has an un-
reliable impact.
6 Findings: Impact on Translation
Probability Estimates
Thus far, all our experiments have measured the
impact of different types of linear mixtures on
overall translation quality. But what is the im-
pact of these various estimations methods on the
learned phrasal translation probability distribu-
tions themselves? More specifically, how do trans-
lation probabilities estimated using linear mixtures
differ from global ?no mix? estimates? If linear
mixtures do not only capture domain knowledge
as suggested by Section 5, do they simply perform
a form of smoothing? If so, how does this im-
plicit smoothing compare to more explicit smooth-
ing schemes for translation probabilities?
6.1 How do linear mixtures affect translation
probabilities?
Let us compare translation probabilities estimated
directly on the entire corpus P
nomix
(t|s), with lin-
ear mixtures p
mix
(t|s) =
?
K
k=1
?
k
p
k
(t|s). The
difference between p
mix
(t|s) and p
nomix
(t|s) is
hard to represent analytically in the general case,
but studying a few particular cases can help us gain
a better understanding.
First, we observe that linear mixtures scale
down the contribution of component-specific
source phrases. Assume that the phrase s oc-
curs only once in the training corpus, with trans-
lation t. By definition, there is a single mixture
component k such that p
mix
(t|s) = ?
k
, which is
likely to be smaller than p
nomix
(t|s) = 1. In the
slightly more general case where s occurs more
than once, but always in the same component k,
then p
mix
(t|s) = ?
k
p
nomix
(t|s), which has no im-
pact on the ranking of translation candidates for s,
but yields a smaller feature value for the decoder.
Second, let us consider the case of very fre-
quent ?general language? phrases. They should
have roughly the same translation distributions in
all mixture components: If the p
k
(t|s) distribu-
tions are the same in each component, the ?
k
val-
ues learned do not matter, they have no impact on
p
mix
(t|s) = p
nomix
(t|s).
In between these extremes, the impact of linear
mixtures depends on the frequency and ambiguity
of translation candidates t across mixture compo-
nents. For instance, let us assume that the mixture
components are somehow defined such that they
partition the translate candidates t of a phrase s
into separate clusters. In that case, for each t, there
505
4 8 16 32 64 128 512 2048 8192 32768
0.00
0.02
0.04
0.06
0.08
0.10
frequency bin
JSdiv
nomix unsmoothed vs. nomix
Figure 2: Comparing translation probability dis-
tributions with and without Kneser Ney smooth-
ing for Chinese phrase-tables: boxplots of Jensen-
Shannon divergences binned by source phrase fre-
quency. For instance, the box and whisker at x = 8
represent the distribution of the values of Jensen-
Shannon divergence between the unsmoothed and
smoothed translation probability distribution for
all Chinese phrases seen between 5 and 8 times
during phrase extraction.
is a k such that p
k
(t|s) = p
nomix
(t|s). The rank-
ing of translation candidates t for s according to
p
mix
(t|s) can be very different from p
nomix
(t|s),
as controlled by the ? values used.
6.2 Smoothing Effects
As a basis for comparison, let us analyze the
difference between unsmoothed relative frequen-
cies and smoothed translation probabilities using
a conventional smoothing scheme. We focus on
the Kneser-Ney smoothing scheme (Chen et al.,
2011), since it is used to smooth translation prob-
abilities in the ?nomix? baseline as well as in all
mixture components.
For seen phrase pairs (with f(s, t) > 0), the
difference between Kneser-Ney estimates p
kn
(t|s)
and relative frequency estimates p
rf
(t|s) can be
written as:
p
rf
(t|s)? p
kn
(t|s) =
D
f(s)
?
D ? n(s) ? p
b
(t)
f(s)
(4)
where D is a discount coefficient, f(s) is the raw
frequency for source phrase s, n(s) is the num-
ber of translation candidates for s in the phrase-
table, p
b
(t) is a back-off distribution proportional
to n(t). The first term is a discount that increases
when s is rare, while the second term adds some
probability mass back, based on the frequency and
degree of ambiguity of the target phrase t. There-
fore, Kneser-Ney smoothing has primarily a dis-
count effect, applied on rare source phrases. In ad-
dition, for more frequent and ambiguous phrases,
the relative frequency can be adjusted up or down
depending on how ambiguous s and t are.
Overall, there are some similarities between the
impact of Kneser-Ney smoothing and linear mix-
tures, since one can expect that the translation
distributions will diverge more from global rela-
tive frequencies for rare phrases than for frequent
phrases. However, the discounting / down-scaling
effects are controlled by very different parameters
in linear mixtures than in Kneser-New smoothing.
In order to better understand these differences in
practice, an empirical analysis is required.
6.3 Empirical Comparison
How do linear mixtures and smoothing affect
translation probabilities p(t|s) in practice? We
use the Jensen-Shannon divergence (Lin, 1991) to
quantify the distance between (a) various mixture
model estimates and (b) the global smoothed rela-
tive frequency estimates used in our baseline ?no
mix? experiments. In addition, we also compare
the Kneser-Ney smoothed translation probabilities
with unsmoothed relative frequencies, in order to
highlight the difference between standard smooth-
ing techniques and linear mixture models.
Figures 2 and 3 show the distributions of di-
vergence values by source phrase frequencies for
Chinese-English phrase-tables. The divergence
from the global estimate is the largest for rare
phrases in all cases, as expected based on previous
Sections. However, the Figures also highlight the
different behavior of linear mixtures compared to
Kneser-Ney smoothing. The divergence values are
much higher overall for the linear mixtures than
for smoothing (note that the difference in range
on the y axis in Figure 2 vs. Figure 3). In addi-
tion, linear mixtures have a large impact on trans-
lation probabilities not only on the rarest source
phrases but also on relatively frequent phrases: in
Figure 3, the median Jensen-Shannon divergence
remains high for source phrases extracted up to
128 times from the training set
3
, while the median
value drops significantly as the frequency range in-
3
Recall that we use multiple word alignment methods, so
extraction counts are summed across all alignment methods.
506
4 8 16 32 64 128 512 2048 8192 32768
0.0
0.1
0.2
0.3
0.4
0.5
frequency bin
JSdiv
mix manual domains vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
0.0
0.1
0.2
0.3
0.4
0.5
frequency bin
JSdiv
mix cluster domains vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
0.0
0.1
0.2
0.3
0.4
0.5
frequency bin
JSdiv
mix random partitions vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
0.0
0.1
0.2
0.3
0.4
0.5
frequency bin
JSdiv
avg manual domains vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
0.0
0.1
0.2
0.3
0.4
0.5
frequency bin
JSdiv
avg cluster domains vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
0.0
0.1
0.2
0.3
0.4
0.5
frequency bin
JSdiv
avg random partitions vs. nomix
Figure 3: Comparing translation probability distributions of mixtures vs. ?nomix? on Chinese webforum
data, including EM weights (top row) and uniform weights (bottom row).
creases in Figure 2. In addition, uniform mixtures
have an even higher impact on frequent phrases
than mixtures based on EM weights.
Furthermore, the nature of mixture components
used has a visible impact on the divergence distri-
butions in Figure 3: random partitions yield lower
divergences for very frequent source phrases.
Overall, the linear mixtures result in very differ-
ent translation probability distributions than global
estimates, including smoothed estimates. This
suggests that standard smoothing techniques can
be improved when learning from heterogeneous
training data, and that mixture components are
beneficial even when they do not explicitly cap-
ture domain distinctions.
7 Related work
Most previous work on domain adaptation in ma-
chine translation presupposes a clear-cut distinc-
tion between in-domain and out-of-domain data
(Koehn and Schroeder, 2007; Foster and Kuhn,
2007; Duh et al., 2010; Bisazza et al., 2011; Had-
dow and Koehn, 2012; Sennrich, 2012b; Haddow
and Koehn, 2012; Clark et al., 2012, among many
others). We focused instead on a different less-
studied question: how can we leverage training
data drawn from a wide variety of sources, genres,
time periods, to translate a domain represented by
a small development set?
Many approaches focus on mapping the test do-
main to a single subset of the training data. In con-
trast, we show that the test domain can be flex-
ibly represented by a mixture of many compo-
nents. Yamamoto and Sumita (2007) cluster the
parallel data using bilingual representations, and
assign data to a single cluster at test time. Wang
et al. (2012) show how to detect a known domain
at test time in order to configure a generic transla-
tion system with domain-specific feature weights.
Others select a subset of training data that is rele-
vant to the test domain, using e.g., IR techniques
(Hildebrand et al., 2005) or language model cross-
entropy (Axelrod et al., 2011).
Closer to this work, Sennrich (2012a) proposes
a sentence-level clustering approach to automati-
cally recover domain distinctions in a heteoroge-
neous corpus obtained by concatenating data from
a small number of very distant domains. The tar-
507
get domain was Alpine Club reports, while out
of domain data sets comprised European parlia-
ment proceedings and movie subtitles. We address
training conditions where the dimensions for or-
ganizing the training data are not as clear-cut, and
show that partitions that do not attempt to mimick
domain distinctions can improve translation qual-
ity. It would be interesting to see whether our con-
clusion holds in these more artificial training set-
tings, and whether sentence-level corpus organiza-
tion could help translation quality in our settings.
Finally, recent work shows that linear mixture
weights can be optimized for BLEU, either di-
rectly (Haddow, 2013), or by simulating discrim-
inative training (Foster et al., 2013). In this pa-
per, we limited our studies to maximum likelihood
and uniform mixtures, however, the various mix-
ture component definitions proposed here can also
be applied when maximizing BLEU.
8 Conclusion
We have presented an extensive study of lin-
ear mixtures for training translation models on
very heterogeneous data on Arabic-English and
Chinese-English translation tasks. In addition, we
evaluated the robustness of our models across two
distinct domains on the Arabic-English task.
Our results show that linear mixtures reliably
and robustly improve the quality of machine trans-
lation. Improvements on the mixture-free base-
line system range from 0.7 to 1.6 BLEU points
depending on the components and weights used.
While linear mixture translation models were orig-
inally proposed for domain adaptation tasks, we
showed that linear mixtures that have no domain
knowledge can perform as well or better than tra-
ditional mixtures meant to perform domain adap-
tation. This suggests that improvements with lin-
ear mixture models do not only stem from giving
more weight to sections of the training data that
are relevant to the test domain, as is assumed in
a standard domain adaptation task. Improvements
also come from averaging better generic estimates
from the heterogeneous training data. In other
words, in heterogeneous training settings, linear
mixture models improve translation quality even
though they do not perform domain adaptation.
Finally, we show that while linear mixtures can
be viewed as a smoothing technique, linear mix-
ture estimates do not diverge from global estimates
in the same way as Kneser-Ney smoothed transla-
tion probabilities. In particular, while smoothing
primarily has a large discounting effect for rare
source phrases, linear mixtures yield differences
in translation probabilities for phrases with a wider
range of frequencies.
These surprising results encourage us to rethink
the use of mixture models, and opens up new ways
of conceptualizing learning from heterogeneous
data beyond domain adaptation. In future work,
we will extend this study by varying the gran-
ularity of basic elements used to define mixture
components, including sentences and phrases, and
will explore how they compare with more general
smoothing techniques.
Acknowledgments
This research was supported in part by DARPA
contract HR0011-12-C-0014 under subcontract to
Raytheon BBN Technologies. The authors would
like to thank the reviewers and the PORTAGE
group at the National Research Council.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 355?362.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based SMT adaptation. International Work-
shop on Spoken Language Translation (IWSLT).
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In Proceedings of Machine Translation Sum-
mit.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal
of Machine Learning Research, 13(1):1159?1187,
April.
Jonathan H. Clark, Alon Lavie, and Chris Dyer. 2012.
One system, many domains: Open-domain statisti-
cal machine translation via feature augmentation. In
Proceedings of the Conference of the Association for
Machine Translation in the Americas.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation in
statistical machine translation.
508
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing.
George Foster, Boxing Chen, and Roland Kuhn. 2013.
Simulating discriminative training for linear mixture
adaptation in statistical machine translation. In Pro-
ceedings of the XIV Machine Translation Summit,
pages 183?190.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 848?856.
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on SMT systems. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, pages 422?432.
Barry Haddow. 2013. Applying pairwise ranked opti-
misation to improve the interpolation of translation
models. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 342?347.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In European Association
for Machine Translation.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Trans-
lation. In Workshop on Statistical Machine Transla-
tion.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), demonstration session.
Jianhua Lin. 1991. Divergence measures based
on the shannon entropy. IEEE Trans. Inf. Theor.,
37(1):145?151, September.
Rico Sennrich. 2012a. Mixture-modeling with un-
supervised clusters for domain adaptation in statis-
tical machine translation. In 16th Conference of
the European Association for Machine Translation
(EAMT).
Rico Sennrich. 2012b. Perplexity minimization for
translation model adaptation in statistical machine
tra. In Thirteenth Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL).
Wei Wang, Klaus Macherey, Wolfgang Macherey,
Franz Och, and Peng Xu. 2012. Improved do-
main adaptation for statistical machine translation.
In 10th biennial conference of the Association for
Machine Translation in the Americas (AMTA).
Hirofumi Yamamoto and Eiichiro Sumita. 2007. Bilin-
gual cluster based models for statistical machine
translation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 514?523.
509
